Mach Learn 84:137–169
DOI 10.1007/s10994-011-5235-x
Reinforcement learning in feedback control
Challenges and benchmarks from technical process control
Roland Hafner · Martin Riedmiller
Received: 26 February 2010 / Revised: 3 January 2011 / Accepted: 8 January 2011 /
Published online: 27 February 2011
© The Author(s) 2011
Abstract Technical process control is a highly interesting area of application serving a high
practical impact. Since classical controller design is, in general, a demanding job, this area
constitutes a highly attractive domain for the application of learning approaches—in particular, reinforcement learning (RL) methods. RL provides concepts for learning controllers
that, by cleverly exploiting information from interactions with the process, can acquire highquality control behaviour from scratch.
This article focuses on the presentation of four typical benchmark problems whilst highlighting important and challenging aspects of technical process control: nonlinear dynamics;
varying set-points; long-term dynamic effects; inﬂuence of external variables; and the primacy of precision. We propose performance measures for controller quality that apply both
to classical control design and learning controllers, measuring precision, speed, and stability
of the controller. A second set of key-ﬁgures describes the performance from the perspective of a learning approach while providing information about the efﬁciency of the method
with respect to the learning effort needed. For all four benchmark problems, extensive and
detailed information is provided with which to carry out the evaluations outlined in this
A close evaluation of our own RL learning scheme, NFQCA (Neural Fitted Q Iteration
with Continuous Actions), in acordance with the proposed scheme on all four benchmarks,
thereby provides performance ﬁgures on both control quality and learning behavior.
Keywords Reinforcement learning · Feedback control · Benchmarks · Nonlinear control
1 Introduction
Reinforcement learning (RL) aims at learning control policies in situations where the available training information is basically provided in terms of judging success or failure of the
Editors: S. Whiteson and M. Littman.
R. Hafner () · M. Riedmiller
Machine Learning Lab, Albert-Ludwigs University Freiburg, Freiburg im Breisgau, Germany
e-mail: 
M. Riedmiller
e-mail: 
Mach Learn 84:137–169
observed behaviour . Because this is a very general scenario, a wide
range of different application areas can be addressed. Successful applications are known
from such different areas as game playing , routing , dispatching and scheduling ,
robot control , and autonomic computing
 (to name but a few).
In this article, we particularly focus on applications originating from the area of technical process control; the design of high quality controllers is an essential requirement for the
operation of nearly every high-level technical system. Application examples can be found in
complex technical systems such as aircraft, magnetic levitation trains, and chemical plants,
but also in objects of daily life such as air conditioners and computer drives . However, the design of
good controllers is a tedious and demanding job; the classical controller design procedure
incorporates careful analysis of the process dynamics, the building of an abstract mathematical model, and ﬁnally, the derivation of a control law that meets certain design criteria.
In contrast to the classical design process, reinforcement learning is geared towards learning appropriate closed-loop controllers by simply interacting with the process and incrementally improving control behaviour. The promise of such an approach is tempting: instead of
being developed in a time consuming design process, the controller learns the whole of its
behaviour by interaction—in the most extreme case completely from scratch. Moreover, the
same underlying learning principle can be applied to a wide range of different process types:
linear and nonlinear systems; deterministic and stochastic systems; single input/output and
multi input/output systems.
Learning the complete control law from scratch is also the underlying scenario for the
control tasks presented here—meant as a challenge for the reinforcement learning algorithms under examination. Of course, in a practical application, it is often advisable to use
as much prior knowledge as available, and for example to start with the best controller that
can be designed in a classical way and use the learning controller then to improve over the
inadequacies of the designed controller.
With the advent of increasingly efﬁcient RL methods, one also observes a growing
number of successful control applications, e.g. helicopter control , car
driving , learning of robot behaviours , control of soccer robots , and engine
control . The bibtex collection of Csaba Szepesvari provides a nice overview
over successful applications .
This article complements this line of research on technical process control by providing
four different and challenging benchmark systems that are easily accessible for comparison
of different learning and non-learning control design approaches. The system equations of
each benchmark system are completely speciﬁed in the appendix and can be easily implemented.
Benchmarking learning controllers has a long tradition and
has gained an increasing amount of interest in the last couple of years (also see the website at www.rl-competition.org). In parallel, software environments such as RL-Bench or CLSquare have been developed that support easy study and standardized benchmarking
of tasks in many different domains. This article suggests both new benchmarks and new
performance measures, which address the needs from the area of technical process control. Also, some domain-speciﬁc requirements are introduced (e.g. changing setpoints), that
may be considered to play an important role in the further development of the benchmark
environments.
Mach Learn 84:137–169
Using performance measures that can be applied by both classical controller design methods as well as by learning methods, this article is also intended as a contribution with which
to pave the way for comparisons between classical control design techniques and learning
1.1 Contributions
This article presents four challenging tasks from the area of technical process control. The
tasks were selected to particularly highlight ﬁve central aspects of technical process control:
the need for nonlinear control laws; the ability to cope with long-range dynamic effects;
the need for highly precise control laws; the ability to cope with the inﬂuence of external variables; and the ability to deal with changing set-points. All of the presented systems
are particularly interesting from the viewpoint of classical controller design. We therefore
hope to stimulate discussion between classical control engineers and machine learning researchers. Whenever available, we will present a classical control solution as a reference. To
further encourage the link to classical controller design, we distinguish between two types
of performance measures: (a) the performance of the controller is measured in terms of
key ﬁgures considering precision, speed, and stability of the closed-loop system and (b) the
performance of the learning algorithm, measured by key ﬁgures describing the efﬁciency
of the learning system with respect to the amount of data needed. Classical controller designs can therefore be compared with learning approaches at least on the level of controller
performance.
For the four benchmarks, we will report the results of our own reinforcement learning
controller scheme called NFQCA (Neural Fitted Q-Iteration with Continuous Actions). Besides results obtained from classical control approaches, we will primarily give a report of
the results obtained from our own method. We do not make comparison to other learning
methods on purpose, since the explicit purpose of this article is to provide a solid base for
benchmarking in technical process control, and not to show the superiority of our method
over others. By concentrating on making performance ﬁgures clear and easy to compute, we
hope to encourage other teams to apply their algorithms and to publish their results in a similar manner. We believe, that a procedure, where everybody is advocating his or her favourite
algorithm and produce the according ﬁgures, will lead to the most fair comparison.
In particular, the article makes the following contributions:
– introduction of four benchmark tasks, highlighting various important aspects of typical
technical control tasks
– tasks are easy to re-implement (all equations are published in the Appendix)
– suggestions for quantitative performance measures of controller quality, which can be
easily determined, general enough for a wide range of control approaches
– suggestions for quantitative performance measures of learning behaviour, which can be
easily determined general enough for a wide range of learning approaches
– description of our learning control scheme NFQCA
– systematic evaluation and reporting of NFQCA on all four tasks
As mentioned, it is not our intention to show the superiority of our learning control
scheme NFQCA over other approaches. Therefore, no ﬁgures for other learning schemes
are given. Furthermore, the classical controllers are among the best solutions that we found,
but most likely, better solutions exist and hopefully will be advocated by classical control
engineers. It is the intention of the article to serve as a starting point for benchmarking both
learning and classical controller designs on challenging control tasks.
Mach Learn 84:137–169
2 Learning feedback control for technical processes
The classical feedback-control loop describes the application-speciﬁc inﬂuence of a control
device on a controlled process. Within this interaction loop the control device applies appropriate control actions, u, to bring the controlled process variables, y, in close proximity
to external set-points or reference inputs, w. A deviation of the controlled process variables
from the set-point can occur due to external disturbances on the process and/or to the external change of the set-point. The decision of the control device is based on information that
is fed back from the process. In this article we refer to the feedback control schematic depicted in Fig. 1. The controller is reduced to a control law that can be computed on a digital
device, e.g. a computer or microcontroller. All physical properties of the devices involved
in the feedback control loop are integrated in a process that exhibits a certain composite
behaviour. Furthermore, it is assumed that both the process and controller can communicate
digital information in discrete time intervals. In each time interval the process communicates a process state, χ (a vector of measured process variables), to the controller that then
answers with a vector of control actions. In this way, the resulting control problem can be
treated as a state controller where the controlled process variables, y, are contained in the
observed process state, χ.
In this article we focus explicitly on processes that can be described as time discrete
dynamic systems of the form given in (1).
χt+1 = f (χt,ut,σt)
It is therefore possible that the dynamic of the process contains strong nonlinearities. This
allows for the formulation of a broad range of challenging control applications. Furthermore,
the system equation, f , can be subject to a reasonable bounded noise parameter, σ. Assuming this formulation we address one of the most general forms of time discrete, dynamic
Using this formulation, we place some restrictions on the considered dynamic systems
and the resulting control problems to set up the control benchmarks for reinforcement learning. As the system equation f does not change over time, we consider autonomous nonlinear
time discrete systems. Furthermore, the process variables are assumed to be fully observable
and can be observed without noticeable time delay. Dropping these restrictions results in interesting research topics that could also be addressed by reinforcement learning, but which
are beyond the focus of the benchmarks presented in this article. However, the presented
benchmarks can be extended easily for these settings making them even more challenging.
Fig. 1 The schematic of the closed loop control for learning state based feedback control. A process subsumes all physical characteristics of the controlled process, while the controller is an algorithm that is executed on a computer or microcontroller
Mach Learn 84:137–169
To solve a control problem it is necessary to ﬁnd an appropriate control law, ut =
π(χt,wt), such that the closed dynamic system, χt+1 = f (χt,π(χt,wt),σt), exhibits certain
properties. A broad range of standardized methods to design such control laws are provided
by the classical control theory. However, for the design of the controller a high amount of a
priori knowledge must be collected concerning the properties of the process and its dynamic.
This process is known as “system identiﬁcation” , which is a separate research area with roots in many different ﬁelds of research, including statistics, statistical learning, physics, machine learning, and others. One
possibility is to use parametric models that are based on the physical properties of the
process (white-box models) and to ﬁt the model parameters to data from the real process.
Another possibility is to use a regression directly on measured data from the process with
nonparametric models (black-box models), e.g. neural networks . Inbetween these two approaches exists a wide range of grey-models, which in turn, combine
the two approaches. Because the quality of the system identiﬁcation inﬂuences the quality of
the designed controller, this step must be performed with a degree of precision. For parametric models, a large degree of expert knowledge of the process is required to be successful.
Using nonparametric models, the process of ﬁtting the model is a complex task, which results depend mainly upon the experience with the chosen method and the experiment design
in order to collect the appropriate data from the real process.
For linear dynamic systems there exists a broad range of standardized methods for the
controller synthesis from an accurate model. For nonlinear systems the methods are less
standardized. To design an appropriate control law for a nonlinear system, either the solution
will ignore some properties of the process (by treating it as a linear system), or a very high
amount of expert knowledge is required .
To overcome the loss of controller quality by a poor system model identiﬁcation, the
research ﬁeld of robust control exists explicitly to deal with uncertainty in
the design. Robust control methods are designed to guarantee a robust solution so long as
uncertain parameters or disturbances are within some speciﬁed range, however, they do not
provide an optimal solution.
The main idea of learning feedback control is to have an intelligent controller component that acts in the standard feedback control loop and learns to control the process by
experience made by interaction. Using reinforcement learning methods, a controller can be
learned with only a small amount of a priori knowledge of the process (see Sect. 3). Furthermore, the identiﬁcation of the classical controller design is implicitly incorporated into
the learning method. This has several additional advantages: (1) The controller learns with
the real process behaviour and thus does not suffer from model inaccuracy or simpliﬁcations
made in the design process. (2) In contrast to the nonparametric identiﬁcation models, the
problem of where to sample data is also incorporated within the learning process and can be
concentrated on regions important to the control application. Also, in contrast to the robust
control approach, the learning controller can concentrate on the stochastic behaviour of the
process and does not suffer from uncertain parameter estimations of the model.
In our point of view, feedback control is perfectly suited for serving as a test-bed to
compare the capability of learning controllers—both against classical controllers as well as
against other learning methods. For this comparison three different aspects are important:
– the controller performance that can be achieved by a speciﬁc method
– the experience that is required to gain information of the process in form of interaction
time with the process
– the amount of a priori knowledge that is required to apply the learning or the design
Mach Learn 84:137–169
A classical nonlinear controller design would require an extremely high amount of a
priori knowledge in form of the system model and an expert knowledge of the synthesis
process. While the amount of experience is very small—required only at identiﬁcation—the
controller performance will depend on the quality of the design, however, can be expected
to be rather high. In contrast to this, a reinforcement learning approach can be expected
to require much less a priori knowledge, but on the other hand, an increased number of
interactions with the process. In the following section we will propose measures for the
controller performance and the required amount of interactions to deﬁne a suitable test-bed
for benchmarking. Not only is the focus placed on the benchmarking of RL setups against
one another, but also against other existing setups.
As it is not possible to specify the required amount of a priori knowledge in any concrete measure, we do not try to identify this amount quantitatively. Instead, we encourage
researchers of the different disciplines to accompany their publication of benchmark results
with a qualitative description of the required a priori knowledge used.
2.1 Evaluation of controller performance
Classical control theory does not lack tools with which to analyze the control quality of
linear controllers applied to linear processes. As we wish to address nonlinear process dynamics and nonlinear controllers, these tools can not be used to serve as an evaluation criterion. Therefore, an appropriate choice for an evaluation criterion of a controller at any given
process must originate from the nonlinear dynamic system analysis.
For the analysis in this context, it is necessary to analyze the behaviour of a process
dynamic, χ(t + 1) = f (χ(t),u(t)), in combination with a certain control law, u(t) =
π(χ(t),w), at a particular set-point, w.
Thus, we are required to analyze a closed dynamic system of the form χ(t + 1) =
h(χ(t)). For this system we are interested in the dynamics of the control deviation, e(t) =
w −y(t), over time where y(t) comprise the subset of controlled process variables in χ(t).
We assume that there is a solution for the given set-point and thus the dynamic system has
an equilibrium point χe with the corresponding control deviation ee = 0. For the analysis,
we are interested in the stability properties of the equilibrium point ee. This means we want
to know what happens to a control deviation, e(t), for t > t0, corresponding to the initial
condition χ(t0) and e(t0)! = 0.
Control theory knows different classes of stability (referred to as stability in the sense of
Lyapunov), e.g. the equilibrium point ee(t) is called:
(uniformly) stable if for any ϵ > 0, there exists δ(ϵ) > 0, such that |e(t0)| < δ(ϵ) →|e(t)| <
ϵ for all t > t0.
(uniformly) asymptotic stable if it is (uniformly) stable and there exists δ > 0 independent
of t, such that for all ϵ > 0 there exists N(ϵ) > 0, such that |e(t0)| < δ →e(t) < ϵ for all
t > t0 + N(ϵ).
(uniformly) exponentially stable if for any ϵ > 0 there exists δ(ϵ) > 0, such that |e(t0)| <
δ(ϵ) →e(t) < ϵe−a(t−t0) for all t > t0 for some a > 0.
These deﬁnitions of stability include the intuitive idea that the control deviation is bounded,
but can be made arbitrarily small through restriction of the initial condition. Asymptotic
stability requires the system to converge against the equilibrium point whereas exponential
stability requires at least an exponential rate of convergence. In practical applications with
perturbations and noise, these concepts of stability are not practicable. Furthermore, when
dealing with learning controllers, the control law, u(t) = π(χ(t),w), is often represented
Mach Learn 84:137–169
Fig. 2 The schematic diagram of
a UUB stable controlled system,
where the controlled variable is
stable in the tolerance range of a
certain set-point
by approximation schemes. Due to numerical issues and approximation errors we can not
guarantee that e(t) can be made arbitrarily small by starting close enough to the set-point.
What we would like to expect from a learning feedback controller is a concept originating
from boundedness . We say the solution is
uniformly ultimately bounded (UUB) if there exists a compact set U ⊂Rn, such that for
all e(t0) ∈U there exists an ϵ > 0 and a number N(ϵ,e(t0)), such that |e(t)| < ϵ for all
t ≥t0 + N(ϵ,e(t0)).
In other words, if the process is started from the initial process state, χ0, it is ensured
that, after some maximum number of time-steps, the controlled variables reach a tolerance
range around the set-point and do not leave it again (see Fig. 2). To deduce an evaluation
criterion for the controller performance from this deﬁnition, we deﬁne a benchmark-speciﬁc
tolerance range, μ. The evaluation of the controller can be achieved by interaction with
the process on trajectories of length T . If N is measured for various initial process states
and set-points, we obtain a measure for the time-optimality of the controller. Because this
criterion evaluates only the time-optimality of the controller, a second criterion is required
that provides information about the preciseness of the controller in the tolerance range.
We assume that time-optimality and controller precision are good general measures of
control quality. For the evaluation from a ﬁxed starting point, χ0, with a ﬁxed set-point, w,
– N(χ0,w) as the number of time-steps after the controlled process variable enters the
tolerance range around the set-point w (and does not leave it again). More formally, N is
the smallest t for which |yt+k −w| < μ for all k ∈0,...,T −t holds (N = T if no such t
– e∞as the mean absolute off-set the controlled process variable has from the set-point
after a certain number of time-steps Nmax
e∞(χ0,w) =
To evaluate the controller performance over a broad range of working conditions, we
take the mean values of these measurements over J runs, each with a predeﬁned start
point and set-point. In this way we are able to obtain a measure representing the average performance of the controller for steps in the set-point over the whole working range:
0 ,wj) and ¯N = 1
These evaluation criterion are deﬁned on single steps of the set-point. In many applications there are certain kinds of set-point trajectories of special importance, e.g. rapidly
changing, or even continuously changing, set-points. To evaluate the overall performance of
the controller we deﬁne an additional criterion:
Mach Learn 84:137–169
– eT (χ0,w(t)) as the mean absolute off-set that the controlled process variable has from the
set-point on a predeﬁned reference trajectory w(t)
|yt −w(t)|
This criterion is deﬁned on a separate trajectory to which the controller is applied. Typically,
this trajectory has a much longer duration, Ttraj, than the other measures as well as an
application-speciﬁc form.
The parameters that describe the evaluation of the controller performance in a certain
benchmark can be speciﬁed by the following: the trajectory length, T ; the start of the evaluation period for determining the remaining control deviation, Nmax; a set of J start point
and set-point tuples B = {(χj
0 ,wj),j = 1,...,J} for evaluating the preciseness, ¯e∞, and
the time-optimality, ¯N. In addition, the reference set-point trajectory length Ttraj, the initial
process state for the trajectory, χ0, and a set-point trajectory, w(t), deﬁne the evaluation of
the overall performance of the controller eT .
2.2 Evaluation of learning performance
Especially for learning controllers the pure evaluation of the controller performance that can
be learned is not sufﬁcient. An additional measure is required that describes quantitatively
how much effort is required to learn the controller in the given task. Especially for learning
feedback control the typical application that should come in reach for learning controllers
are real devices or machines. For this application it is a central key feature how long the
controller is occupied with learning, how much wear and tear the learning procedure causes
and when the system can be productive again.
To characterize learning performance, we propose a strict separation of two phases,
namely a learning phase and an evaluation phase. The evaluation phase is used to determine the performance of the controller for comparison. Therefore, in the evaluation phase,
the conditions are exactly speciﬁed, e.g. the set of starting states, the length of the trajectories, changes in set-points, etc. For the learning phase, these settings might be chosen
individually to ﬁt the needs of the learning method at hand.
Learning performance then is reported in the number of interactions that are required
during the learning phase to reach a certain performance level. The rationale behind this is
that the lower the number of interactions required for learning, the easier it is to bring the
controller into a corresponding real-world application.
Another interesting ﬁgure is the time, that a controller spends interacting with the process
in a corresponding real world scenario. To identify this number, we simply have to multiply
the number of interactions with the length of the control interval. This number, the actual
interaction time with the process, is also provided in the following experiments.
3 Reinforcement learning controllers
In this chapter we brieﬂy describe our approach in formulating the various requirements of
technical process control within the framework of a reinforcement learning controller. Our
approach is based on batch learning of a Q-value function based on experiences of state
transitions. In particular, we discuss learning design issues such as the choice of the immediate cost function and the choice of the inputs with which to realize a set-point controller.
Furthermore, we present our learning scheme for learning neural network based controllers
with continuous action values (NFQCA).
Mach Learn 84:137–169
3.1 An RL formulation for feedback control
Our approach is based on the formulation of the control problem as a Markov Decision
Process (MDP) . In this setting a learning agent interacts with its environment in discrete time steps. In every time step, t, the agent observes
a state xt ∈X ⊂Rn from the environment and chooses an action ut ∈U ⊂Rm, based on
its current policy π(xt) : X →U. In the subsequent time-step, t + 1, the state of the environment is assumed to change according to a transition probability P (xt+1|xt,ut). The
successor state can be observed by the agent, accompanied by an immediate cost signal
c(xt,ut). The task is to ﬁnd an optimal control law or policy, π∗, that minimizes the expected accumulated cost, ∞
t=0 c(xt,ut), for every initial starting point x0. For simplicity,
we assume that the vector of process variables χ can be observed.
Dealing with multiple set-points
An important characteristic of feedback controllers is
that they have to cope with varying set-points for the target values of the process variables.
Whereas the majority of typical RL tasks is characterized by a single goal state (or by a
single goal region, respectively), here, the set-points that determine the goal states may vary
continuously, and therefore an inﬁnite number of goal states must be managed.
As the controller learns by interaction with the control process it has to generalize from
samples of interaction—not only over the state and action space, but also over different setpoints. By consequence, the information of the current set-point has to be integrated in the
state of the MDP. In general, there are different possibilities to represent this information in
the MDP state. In our setup the state of the MDP is given as xt = [χt,et]: a combination of
the process variables, χt, and the recent control deviation, et = wt −yt.
If we consider a constant set-point, w, the transition probabilities, P (xt+1|xt,ut), for the
MDP are entirely deﬁned by the dynamic, χt+1 = f (χt,ut,σt), of the process. A problem
arises if we wish to allow changing setpoints wt. The change of the set-point from wt to
wt+1 has a direct impact on the MDP state and, hence, on the observed transitions. If the
set-point is not frequently changed, this impact could be modelled as additional noise that
will not exhibit too large an effect on the learning agent. But if the set-point is changed
frequently—or even continuously—this will violate the Markov Property of the modelled MDP. To prevent this we can add a kind of sample and hold
element for the set-point within the interaction loop of the controller. Upon execution of
the controller in every time-step, t, the control action, ut, is computed using the recent setpoint, wt, as ut = π([χt,wt −yt]). In the subsequent time-step the controller observes the
process state χt+1 and builds a transition (xt,ut,xt+1) for the MDP using set-point wt as
([χt,wt −yt],ut,[χt+1,wt −yt+1]). In other words, we build a consistent transition for
the MDP that does not include a change in set-point. Using these transitions we can train a
controller that will compute the optimal control action for a process state and set-point, under
the assumption that the set-point will not change in the subsequent time-step. This behaviour
agrees completely with the functionality of feedback control wherein the controller reacts
only to the control deviation of the recent time-step.
Specifying the immediate costs
One of our most important design objectives is to incorporate as little prior knowledge into the speciﬁcation for the learning controller as possible. In
other words, we are always looking for generic settings that can be applied to a wide range
of tasks without tuning. A very general choice for the immediate cost function is given by
the following deﬁnition, that only considers the error between the desired and the actual
Mach Learn 84:137–169
value to determine the immediate costs:
c(x,u) = c(e) =
Here, C is a positive constant value while μ deﬁnes the tolerance of the target region and
therefore determines the expected precision. This formulation of the cost signal is a good
representation of the UUB stability claims. The learning agent will optimize its policy to
reach the tolerance range in a minimum number of time-steps and strives for forever staying
within. Because all states within the tolerance range are not punished, we can expect timeoptimal and stable control, but not precise control. To achieve a more precise control law,
the cost function can be reﬁned while retaining the advantages of the above deﬁnition. In (5)
a smooth and differentiable cost function is given and enables the agent to learn precise and
time-optimal control .
c(x,u) = c(e)
= tanh2(|e| ∗w) ∗C
w = tanh−1
In Fig. 3a the original cost function (dashed line) and the precise time-optimal cost function are plotted in a one-dimensional setting. A typical resulting value function for the two
different cost functions is illustrated in Fig. 3b. In 3b-I a typical effect of the original formulation on the optimal value function is depicted. The accute steps of the direct cost function
results in similarly accute steps in the optimal value function. Especially when using approximation schemes, these sharp steps in the value function are a problem and result in a high
approximation error. In contrast to this, the smooth immediate cost function of (5) ehibits
an additional advantage: the resulting optimal value function is smooth and can therefore be
approximated much more accurately.
3.2 Neural reinforcement learning controllers
In the following section we provide the basic idea of a recently developed value function
based RL algorithm—the Neural Fitted Q-Iteration for Continuous Actions (NFQCA)—
Fig. 3 (Color online) (a) A sketch of the deﬁnition of the direct cost signal over a one dimensional control deviation, e, with tolerance range, μ. Pure time-optimal deﬁnition: dashed dotted (black); precise time-optimal
deﬁnition: solid (red). (b) An exemplary visualisation of typical forms of value functions that results from the
deﬁnitions of the direct cost functions. I: the pure time-optimal deﬁnition; II: the precise time-optimal tanh2
Mach Learn 84:137–169
which is used to generate the evaluations for the benchmarks we introduce for learning
feedback control. Within the scope of this article we provide a general description of the
algorithm and its properties to accompany the benchmark results and in a forthcoming paper speciﬁcally devoted to this algorithm).
The NFQCA algorithm is a expansion of the Neural Fitted Q-Iteration (NFQ) algorithm
 that was developed especially within the
context of reinforcement learning for feedback control applications. It was developed to
overcome one of the main shortcomings of the NFQ algorithm, namely the restriction of
NFQ to discrete actions. To overcome this problem NFQCA is designed as a ﬁtted actor
critic algorithm for continuous state and action spaces, based on the principles of NFQ.
NFQ utilizes a learning agent that interacts with its environment in discrete time-steps.
In every time-step the agent observes the state, x, of the environment, chooses an action, u
(based on its recent policy π(x)), and observes a successor state x′ in the subsequent timestep. All of the experience is stored in form of the observed transitions in a dataset, D, with
entries d = (x,u,x′). NFQ is an iterative algorithm that represents the Q-function in form of a neural network, Q(x,u,wq), with weights
wq. In a certain iteration step, k, a new target value ˆQx,u = c(x,u) + minb Qk(x′,b) is
computed for each transition sample in the dataset, D, using the standard Q-update function.
From this information we are able to build a training set, P , with entries (pinput,ptarget) =
((x,u), ˆQx,u). Using the training set P we can apply an efﬁcient epoch-based supervised
learning method with which to adjust the weights of the neural Q-function for the iteration
k + 1 of the algorithm. In our approach we use Resilient Propagation (RProp) as an epoch-based learning method that proved to be very robust with
respect to the choice of the learning parameters and the topology of the network.1
With discrete actions, the standard Q-learning rule can be applied directly. With continuous actions this simply is not possible. The main reason for this is that we can not
directly ﬁnd the action with the minimal Q-value for a given state in the neural Q-function.
To overcome this problem, in NFQCA—in addition to the neural Q-function, that serves
as the representation of the critic—the actor is explicitly represented by a neural policy function, π(x,wπ), with weights, wπ. In iteration step k of the NFQCA algorithm
we assume that the recent policy πk represents the greedy evaluation of the Q-function:
πk(x) ≈argminu(Qk(x,u)). With this assumption we can formulate the Q-update without
a minimization step over all actions as ˆQ(x,u) = c(x,u) + Qk(x′,πk(x′)). Analogous to
NFQ, with this update rule a training set can be built to adjust the weights, wq, for Qk + 1
with RProp. After updating the neural Q-function, the weights of the neural policy function
wπ must be updated so that it represents a greedy evaluation of the updated Q-function,
according to the base assumption of the critic update. In NFQCA again, a gradient descend algorithm is used to adjust the weights of the policy. For a set of states, each state,
x, is propagated forward through the policy network. The same state x and the policy output π(x,wπ) are then propagated forward through the neural Q-function. As a property of
neural networks, the partial derivatives of the Q-function with respect to the action inputs
of the network, ∂Q(x,π(x,wπ ),wq)
, can afterwards be computed by backpropagation through
the neural Q-function. These partial derivatives can be propagated backwards through the
neural policy function to compute ∂Q(x,π(x,wπ ),wq)
: the partial derivatives of the Q-function
1A comparison of different optimized Backpropagation algorithms for supervised training of neural networks
can be found in literature, e.g. Schiffmann et al. ; in context of supervised learning in RL see Hafner
Mach Learn 84:137–169
with respect to the weights of the policy net. With these gradients and a set of states x, e.g.
the states stored in the dataset, D, RProp, as an epoch based gradient descend scheme, can
be applied to the weights of the policy net.
This basic idea of propagating the gradient of a neural Q-function through the policy
network can also be found in earlier approaches . However, with NFQCA we combine this idea with a
model-free batch reinforcement learning approach based on conventional Q-learning. The
resulting method is very efﬁcient with respect to the number of required interactions, and is
able to learn high quality continuous control laws. Also, the actor critic setting of NFQCA
outperforms a pure gradient-based search of argminu(Qk(x,u)) using ∂Q(x,u)
in a single
neural Q-function for general control applications . One of the advantages of
NFQCA is that a gradient-based search in a neural Q-function requires an iterative procedure
of propagating values forward and backward through the network for a sufﬁcient number
of search steps, starting from a number of different initial values to prevent local minimas.
Hence even for a modest network and state-action size the computation of the policy requires
several milliseconds, where we can compute the policy with NFQCA in just one propagation
step. This allows high control frequencies (up to a few kilo-hertz) that are required for many
real-time control applications.
Learning feedback control in this article uses an explorative learning process for both
learning algorithms. The learning process has two phases which alternate. In the ﬁrst phase,
the process is controlled by the recent policy for a certain number of time-steps and the observed transitions are added to the dataset, D. In the second phase, an update iteration of the
learning algorithm is applied using the recent experience contained in D. Using this procedure the learning process can be started with an empty dataset, D, and randomly initialized
neural functions to learn a policy for the given task.
4 Benchmark environments for technical process control
A crucial point for the selection of the four benchmarks presented in the following, is that
they are interesting and challenging both from the perspective of classical control theory and
from the perspective of machine learning. In particular, for three of the four benchmarks, an
analytically designed controller is known, which can be used as a reference for controller
performance. Furthermore, the benchmarks are selected, since they explicitly shed light on
one or more of the key properties, that are essential in the domain of technical process
control. The key properties examined here are listed in the following:
1. The existence of an external setpoint, that can take arbitrary values, is a general requirement of applications in technical process control. Up to now, in the RL research community external setpoints only play a minor role (if at all).
2. The request of a highly accurate control behaviour, that reaches the given setpoint with
high precision. For a learning controller, this is a considerable challenge, which might
for example require the use of continuous actions.
3. The presence of nonlinear system behaviour, where the application of RL can have a
considerable advantage, compared to classical controller design methods.
4. The presence of long-range dynamic effects, where the system has to be controlled over
several hundred of time-steps to reach a target state. This is particularly challenging for
RL controllers based on dynamic programming methods. In particular, value function
based approaches as they are used here must be able to accurately estimate path costs
over a long range of control steps.
Mach Learn 84:137–169
Table 1 Proposed benchmark tasks and to what extent they shed light on the respective properties
Underwater
Levitation
long-range
5. The presence of external system variables, that can not be inﬂuenced by the controller
but represent noise in form of some external environmental changes the controller has to
cope with.
Table 1 gives a quick overview over the tasks presented in the following and to which
extent the tasks reﬂect the key properties. A more detailed description of the challenges is
given in the following subsections.
We are aware, that many more interesting features for potential benchmark tasks can be
identiﬁed, like high-dimensional state spaces, multi-dimensional actions, time-delay of sensor information, or partial observability of state variables. To keep a reasonable focus, we
concentrated here on the ﬁve properties of the list above. However, it is straight-forward to
extend the proposed benchmarks to introduce features like time-delay or partial observability. The performance ﬁgures presented in this article might then serve as a reference for the
‘ideal’ case.
4.1 Underwater vehicle
The ﬁrst benchmark problem has only a loose connection to a real system, and is a kind
of synthetic problem setup that is especially designed to show interesting properties for
learning feedback control. As the process state has only one dimension, the structure of
the benchmark setup is relatively simple. Nevertheless, the dynamic of the benchmark has
highly nonlinear properties. An aspect of this worth noting is that, for the time-optimal and
precise control of the problem at each state, an appropriate continuous control action must
be chosen carefully.
We concentrate on the velocity control of a virtual, miniature underwater vehicle that is
driven by a propeller. The only process variable is the velocity, v, of the vehicle submerged
in water. The mass, m, and drag coefﬁcient, c, of the vehicle are assumed not to be constant.
Instead they are replaced by the equivalent mass function, m(v), and equivalent drag coefﬁcient, c(v), that represent the complex dynamic motion effects of a vehicle in a ﬂuid .
Furthermore we assume the control action u to inﬂuence the thrust produced by the
propeller e.g. thought of the velocity (or ideal thrust) of the propeller that inﬂuences the
effective thrust. The effective thrust that acts on the vehicle is computed by a coefﬁcient,
k(v,u), that represents the efﬁciency the propeller exhibits at certain vehicle speeds and
Mach Learn 84:137–169
Fig. 4 A plot of the dynamic of
the underwater vehicle. The
resulting acceleration,
˙v = f (v,u), is plotted over the
velocity, v, and control action, u.
As illustrated, the dynamic of the
system is highly nonlinear and
shows intriguing properties for
control applications
Table 2 Characterization of the
process for the velocity control of
the underwater vehicle
velocity of vehicle
desired velocity
ideal thrust
Fig. 5 (Color online) Comparison of a bang-bang controller and a nearly optimal controller. The velocity of
the underwater vehicle is started from −4 m/s and should be controlled to 3 m/s. The solid line in the upper
part of the plot (black) shows the velocity of the vehicle when controlled by the bang-bang controller with
actions plotted as solid line in the lower part of the plot (magenta). The dashed dotted line (red) when controlled by the nearly optimal controller with actions plotted as a dashed line (blue). The bang-bag controller
is slower in reaching the set-point and can not reach it precisely
control actions. In Fig. 4 the dynamic of the underwater vehicle is given as a plot of the
resulting acceleration, ˙v = f (v,u), when applying a certain control action, u, at certain
vehicle velocities, v. For the benchmark we simulate the process dynamic (see A.1.1) over
a time interval of 0.03 s.
4.1.1 Control challenge
For the underwater vehicle, the control task is to appropriately control its velocity (see Table 2). Based on the dynamics of the system in Fig. 4, it follows that a true time-optimal and
precise control can not be achieved by a bang-bang controller with minimal and maximal
control actions (see Fig. 5). If the controller is expected to generate the maximum possible
Mach Learn 84:137–169
Table 3 Characterization of the
benchmark parameters for
evaluation
150 time-steps (4.5 seconds)
50 time-steps (after 1.5 seconds)
v uniformly distributed ∈[−5 m/s, 5 m/s]
uniformly distributed ∈[−3 m/s, 3 m/s]
800 time-steps (24 seconds)
step and continuously changing
Table 4 Parameters for learning
the velocity control of the
underwater vehicle
velocity of vehicle
control deviation
ideal thrust
{±30, ±15, 0}
3-6-1 (600 epochs RProp)
2-15-1 (400 epochs RProp)
acceleration, it must very carefully choose an appropriate control action at every velocity
the vehicle is travelling. In this point of view the benchmark is an example for a control
task that has a highly nonlinear dynamic behaviour and requires a continuous control law,
if precise and time-optimal control are to be achieved. It is therefore an excellent challenge
for learning approaches that are able to deal with continuous actions.
4.1.2 Benchmark environment
In order to evaluate ¯N and ¯e∞, the controllers are tested on a set of 50 trajectories. Each
trajectory has a length of 150 time-steps (4.5 seconds) and is started with a uniformly distributed initial velocity. At the beginning of every trajectory, a set-point between −3 m/s
and 3 m/s is chosen and kept constant over the whole trajectory (see Table 3).
To evaluate eT a reference trajectory is deﬁned for the vehicle, starting with v0 = 0 m/s.
The reference trajectory has several parts, combining steps and continuously changing characteristics (see Fig. 7 and Table 18).
4.1.3 Benchmark results
Learning with NFQCA and NFQ (parameters in Table 4) is done using interaction trajectories with a length of 50 time-steps, where each trajectory is started from a uniformly
distributed initial velocity, v0 ∈[−5 m/s, 5 m/s], with a uniformly distributed set-point,
vd ∈[−3 m/s, 3 m/s].
The learning curve for NFQCA is depicted in Fig. 6. After only a few iterations (approx.
30 iterations), the controller is much better in the time criterion, ¯N, as a reference bang-bang
controller. In the subsequent iterations the controller improves ¯N, but also ¯e∞and eT . We
Mach Learn 84:137–169
Fig. 6 Learning curves for NFQCA on the underwater vehicle benchmark task. In every iteration 50 interaction samples with the process are collected using the recently learned controller. Afterwards, an update of
the controller is performed with all samples collected up to that point. The black-dotted line shows the value,
¯N = 20.04, of a bang-bang controller (with actions: −30 and 30)
Table 5 Benchmark evaluation for the underwater vehicle control challenge. Smaller values in the evaluation
criteria mean better performance of the controller. For comparison, the results of a bang-bang controller with
minimal and maximal actions are shown
Controller
report the results of the controller with the lowest value for the time-criterion, ¯N, as the best
controller. If multiple controllers have the same value in ¯N, the one with the lowest value ¯e∞
is reported. For NFQCA, the best controller was learned after 100 interaction trajectories.
This corresponds to 5,000 interaction steps with the process (or 2.5 minutes of interaction
with the corresponding real-time process). With NFQ and 5 discrete actions, however, we
needed 140 iterations and interaction trajectories until the best controller was learned (7,000
interactions or 3.5 minutes of interaction with the real-time process). As the benchmark
results in Table 5 show, by using continuous actions, the controller is better in the time
criterion, ¯N, and also more precise (¯e∞). Furthermore, the overall quality of the controller
on the set-point trajectory clearly shows the beneﬁt of continuous actions (eT ).
In Fig. 7 the learned NFQCA controller is shown on the reference trajectory. With
NFQCA, a smooth control law can be learned that controls the velocity such that it follows
the set-point very closely.
4.2 Pitch control
This control benchmark refers to an autopilot that controls the pitch of a Boeing airliner
aircraft. It is taken from a collection of detailed control examples where several
classical reference controllers are explained for educational purposes. Though the equations
governing the motion of an aircraft are a very complicated set of six non-linear differential
equations, under certain assumptions they can be decoupled and linearised. Here we will
focus on the longitudinal and linearised problem of pitch control of the aicraft in a steady
cruise at constant altitude and velocity. This implies that we can disregard the effects of
Mach Learn 84:137–169
Fig. 7 NFQCA controller on a set-point trajectory of the underwater vehicle task
Fig. 8 Schematic of the pitch control process. X denotes the base coordinate axis along the main axis of the
thrust, drag, and lift in the dynamic of the aircraft. We also assume that any change in the
pitch angle does not change the speed of the aircraft under any circumstances.
Figure 8 shows the schematic of the benchmark process. We assume the base coordinate
system to run along the main axis of the aircraft (denoted by X in Fig. 8). Also the aircraft is
assumed to move with constant velocity, V , under an varying angle of attack, α, with respect
to the main axis. By setting the elevator deﬂection angle, δ, a controller can inﬂuence the
angle of attack, the pitch rate, q, and the pitch angle, θ, of the aircraft.
4.2.1 Control challenge
The control task here is to provide appropriate elevator deﬂection angles, δ, to allow the
pitch angle, θ, to come as close as possible to the current set-point, θd (see Table 6). The
range of set-points, θd, is restricted between ±0.5 rad, while the model dynamics provide a
reasonable approximation of the real behaviour of the aircraft. For a learning state controller
we have a controller state vector, x, that contains at least three of the following process
variables: α, q and θ. As the process dynamic is linear and does not depend on the recent
pitch angle, we can replace θ with e = θd −θ in the controller state representation. By con-
Mach Learn 84:137–169
Table 6 Characterization of the
pitch control process
angle of attack
pitch rate
pitch angle
desired pitch angle
∈[−0.5,0.5]
elevator deﬂection angle
∈[−1.4,1.4]
Table 7 Characterization of the
benchmark parameters for
evaluation
300 time-steps (15 seconds)
50 time-steps (after 2.5 seconds)
constant = (0,0,0)
uniformly distributed ∈[−0.5 rad,0.5 rad]
1000 time-steps (50 seconds)
step and continuously changing (see A.2.2)
sequence we have a three-dimensional controller state, x = (α,q,e), and a one-dimensional
controller action, u = δ.
Though linear, the process dynamic exhibits an interesting property for learning controllers. By changing the elevator deﬂection angle, δ, the controller can rapidly change the
pitch-angle of the aircraft. The angle of attack, α, however, has a very slow dynamic that
requires several hundred time-steps to adapt to a control action. Consequently, a good controller only requires a maximum of 20 time-steps to bring the controlled process variable,
θ, very close to an arbitrary set-point, θd, within the entire working range. However, after
reaching the set-point, the controller has to actively keep the controlled process variable as
close as possible to the set-point while the process variable, α, slowly changes over several
hundred time-steps. Because the length of the trajectories to the desired and stable states are
very long, this represents a particularly challenging environment for RL controllers.
4.2.2 Benchmark environment
In order to evaluate the precision and time-optimality of the controllers in terms of ¯N and
¯e∞, they are tested on a set of 50 trajectories. Each trajectory has a length of 300 timesteps (15 seconds) and is started at χ0 = (0,0,0). This corresponds to an aircraft in a steady
state where the angle of attack, pitch rate, and pitch angle are all zero. Preceding every
trajectory, a set-point between −0.5 rad and 0.5 rad is chosen and kept constant over the
entire trajectory (see Table 7).
To evaluate the overall performance of the controller in a typical situation, the criterion
eT is determined on a predeﬁned set-point trajectory. For this trajectory the process is again
started in the state χ0 = (0,0,0). The set-point trajectory is made up of four parts (see
Fig. 11). In the ﬁrst phase, the set-point is kept constant at a value of 0 for 1 second. A perfect
controller will keep the starting state in this phase. Afterwards, the set-point is changed to
a value of −0.2 for a duration of 6 seconds, in order to represent a typical step that is not
Mach Learn 84:137–169
long enough to reach the steady state in the slow process variable. The next part is a linear
change of the set-point from −0.2 to 0.2 over 13 seconds, followed by a constant value of
0.2 for the remainder of the trajectory.
4.2.3 Benchmark results
In CTM a LQR controller design for the pitch control challenge is represented that
can serve as a reference controller for benchmarking the performance of learning controllers.
For a controller state, x = (α,q,θ −θd), the control law, δ = −Kx, and certain values for
R and Q, the LQR controller design yields a gain vector K = (−0.6435,169.6950,7.0711)
 .
For learning a controller with NFQCA and NFQ (parameters in Table 8), interaction trajectories with a length of 300 time-steps (15 seconds) are made. Each interaction trajectory
starts from χ0 = (0,0,0) under a uniformly distributed set-point θd ∈[−0.5 rad, 0.5 rad].
The learning curve for NFQCA is depicted in Fig. 9. After only 14 interaction trajectories and iterations of NFQCA, the resulting controller exhibits a comparable quality to the
LQR controller in the criteria eT on the reference trajectory. The best controller (as outlined in 4.1.3) is learned after 174 interaction trajectories (or iterations of NFQCA). This
Table 8 Parameters for learning
the pitch control of the aircraft
angle of attack
pitch rate
control deviation
(0, 0, 0.06)
elevator deﬂection angle
[−1.4,1.4]
{±1.4, ±0.5, ±0.2, ±0.1, 0}
4-15-1 (600 epochs RProp)
3-5-1 (400 epochs RProp)
Fig. 9 Learning curves for NFQCA on the pitch control benchmark task. In every iteration 300 interaction samples with the process are collected using the recent learned controller. Afterwards an update of the
controller is done with all samples collected so far
Mach Learn 84:137–169
Table 9 Benchmark evaluation
for the pitch control challenge
Controller
Fig. 10 (Color online) NFQCA result on a set-point step of 0.2. The controller learned with NFQCA on a
single set-point step of 0.2 rad. The solid (red) line is the pitch angle of the aircraft. As dashed dotted (black)
line the angle of attack is plotted that has a very slow dynamic and is stabilizing after more than 25 seconds
corresponds to 52,200 interaction steps (or 43.5 minutes of interaction with the real-time
process). With NFQ we obtain comparable values for the best controller, that was learned
after 215 iterations (64,500 interaction steps or 53.75 minutes of interaction with the realtime process). As listed in Table 9, the learning controllers perform better in the criteria ¯N
than the classically designed LQR controller, where the precision in ¯e∞is comparable. This
is primarily due to a low rise time and an overshoot of the classical LQR controller. In order
to achieve a controller design with low rise time and less overshoot, one must search and
discover other parameters with which to design a more efﬁcient LQR controller for this control task. In contrast, with the learning controller we need not search for this parameters, as
we specify what the controller should achieve and not how it should be achieved. In Fig. 10
a trajectory of the learned NFQCA controller is shown for the pitch control task under a single set-point change. The controlled variable reaches the set-point very quickly with nearly
no overshoot. After the set-point is reached, the slow process variable (dashed dotted line)
requires more than 25 seconds to stabilize. During this time, the controller has to adapt the
control action within a small range such that the controlled process variable is stabilised in
close proximity to the set-point. In Fig. 11 the learned controller is shown on the benchmark
reference trajectory. As shown, the controller is able to follow the set-point steps as well as
the continuous change of the set-points. The evaluation of single set-point changes on each
trajectory evaluates the behaviour of the controller starting in a balanced system. In contrast to this, the evaluation of the reference trajectory shows the behaviour of the controller
in a broader working range—a range wherein the controller receives a set-point change in
unbalanced situations as well.
Mach Learn 84:137–169
Fig. 11 (Color online) The controller learned with NFQCA on the benchmark reference trajectory. The solid
(red) line is the pitch angle of the aircraft that follows the reference trajectory. As dashed dotted (black) line
the angle of attack is plotted that has a very slow dynamic
Fig. 12 The process setup for
the electromagnetic levitation of
a steel ball. Here a computer is
depicted as commanding an
adjustable voltage source with
which to apply a voltage to the
coil. The distance between the
steel ball and the steel base plate
is measured by a laser sensor
4.3 Magnetic levitation of a steel ball
The technique of contact-less positioning of an object with ferromagnetic properties in
a controlled electromagnetic ﬁeld has a wide range of technical applications. Examples
for such applications can be found in contact-less bearings, magnetic levitation trains, or
contact-less positioning for precise measuring. When it comes to these kinds of applications, the swift and precise positioning of the object represent both the highest priority for
nonlinear control design and its most challenging task.
There are many different technologies and setups with which to realise magnetic levitation systems. The system we introduce here representing a benchmark for reinforcement
learning feedback control, is a standardized one-dimensional levitation model used to develop nonlinear controllers . The schematic in
Fig. 12 shows the setup of the process. A solenoid can apply forces to a steel ball with
mass, M, that is positioned on a steel plate. The control action is the voltage, u, that is
applied to the solenoid. The characteristic of the solenoid and the generated electromagnetic ﬁeld is deﬁned by the parameters R, x∞, L∞and
. For contact-less measuring of
the position and velocity of the steel ball a laser sensor is placed under the steel plate. The
process variables are given by the position, d, of the steel ball (as the length of the air-gap
between the solenoid and the ball), the velocity of the ball, ˙d, and the current in the coil of
the solenoid (see Table 10).
Mach Learn 84:137–169
Table 10 The process variables,
controlled process variables,
set-point, and control action for
the magnetic levitation control
position of steel ball
∈[0.000,0.013]
velocity of steel ball
current in coil
desired position of steel ball
∈[0.000,0.013]
applied voltage to coil
4.3.1 Control challenge
The quick and precise positioning of the steel ball by applying voltages to the solenoid poses
a difﬁcult nonlinear control problem. The open-loop dynamic behaviour is extremely unstable and has a very fast system dynamic. Also, due to the magnetic properties, the amount of
nonlinear dynamics is extremely high. Linear controllers can only be developed for single
working points and are only valid within a very small proximity to that point. Therefore, advanced nonlinear control design concepts must be carried out with a high amount of design
effort and expert knowledge .
For reinforcement learning controllers the amount of nonlinear behaviour in the system
dynamics is challenging. Within the control law, and given the strong discontinuities required to lift the ball from the steel plate and to stop it in a time-optimal way at a desired
position, these discontinuities are not easy to learn, as a sequence of a few, nearly optimal
actions are required. As the open-loop system is extremely unstable, an already levitating
ball will easily pass to the upper or lower position if the control law is not appropriate—also
for some of the time-steps. For RL methods a good exploration scheme is required that does
not lead to the degenerated stable points at the steel plate or solenoid.
4.3.2 Benchmark environment
For the benchmark setup (see Table 12), in every trajectory the process is started with the
ball placed at rest on the steel plate (d = 13 mm, ˙d = 0) with no current in the coil. To
follow the procedure in Yang and Minashima , a constant voltage of 15 V is applied
to the coil for the duration of 0.5 seconds. This voltage is applied in a pre-run phase before
each trajectory starts. This initialization phase ensures that the current is in an admissible
range and that magnetic saturation is reached. After the initialization phase the controller
observes the recent state of the system (the initialisation phase only changes I, not position
or velocity of the ball) and the control loop is started.
For the evaluation of ¯N and ¯e∞of the controllers, 50 trajectories are executed, each
with a uniformly distributed set-point, dd ∈[0.000 m,0.013 m]. Each of the trajectories is
50 time-steps (2 seconds) in duration. To evaluate the overall behaviour the controllers are
tested on a reference trajectory with 4,000 time-steps (16 seconds). This reference trajectory
starts with the ball resting at the base plate. Every 80 time-steps (0.32 seconds) the set-point
is changed (chosen uniformly distributed ∈[0.000 m,0.013 m]) and stays constant for the
subsequent 80 time-steps (see Fig. 15).
Mach Learn 84:137–169
Fig. 13 Learning curves for NFQCA on the magnetic levitation benchmark task. In every iteration 160
interaction samples with the process are collected using the recent learned controller. Afterwards an update
of the controller is done with all samples collected so far
Table 11 The learning
parameters for the magnetic
levitation challenge
position of ball
velocity of ball
current in coil
control deviation
(–, –, –, 0)
(0, 0, 0, 0.002)
voltage applied to coil
5-15-20-1 (600 epochs RProp)
4-15-1 (1000 epochs RProp)
Table 12 Characterization of the
benchmark parameters for
evaluation
500 time-steps (2 seconds)
50 time-steps (after 0.2 seconds)
constant = (0.013,0,0)
uniformly distributed ∈[0.000 m,0.013 m]
4000 time-steps (16 seconds)
change every 80 time-steps,
uniformly distributed ∈[0.000 m,0.013 m]
4.3.3 Benchmark results
For learning a controller with NFQCA (parameters in Table 11) interaction trajectories with
a length of 160 time-steps (0.64 seconds) are made. Each interaction trajectory starts with
the ball resting at the steel plate. The set-points for the trajectories are drawn uniformly
∈[0.000 m,0.013 m]. Unlike the other interaction trajectories, the set-point is kept constant
Mach Learn 84:137–169
Fig. 14 Examples of interaction trajectories of the learning controller (NFQCA) at different stages (number
of interaction sequences) of learning
Fig. 15 The learned NFQCA controller applied on the ﬁrst 4 seconds of the reference trajectory with random
changing setpoints
Table 13 Benchmark evaluation
of the magnetic levitation
controller
for 80 time-steps, after which it is changed again. This enables the controller to acquire
more experience within the entire working range of the process. In Fig. 14, nine interaction
trajectories during learning are shown after different numbers of iterations with NFQCA. In
the ﬁrst 20 interaction trajectories the agent has not yet learned to levitate the ball and thus
bounces it to the limits of its range of motion. When the learning process continues and more
experience is collected alongside additional iterations of NFQCA, the controller improves
and learns the positioning of the ball at the desired positions. The learning curve for NFQCA
is depicted in Fig. 13. After only 92 iterations and interaction trajectories, NFQCA generated
the best (as outlined in 4.1.3) controller. This corresponds to 14,720 interaction steps (or 0.98
minutes interaction with the real-time process). As referenced in Table 13, the precision of
the learned controller is very high. In Fig. 15 the ﬁrst 4 seconds of the learned controller
applied to the reference trajectory are shown.
4.4 Heating coil
The control challenge of the heating coil belongs to the set of “Heating, Ventilation, and
Air Conditioning” (HVAC) problems. HVAC problems have received much attention in past
and recent years , as the existing methods for control
Mach Learn 84:137–169
Fig. 16 A typical setup of a Heating Ventilation and Cooling task. The schematic references to the benchmark setup used in this article
leave signiﬁcant room for improvement. The system dynamic of an HVAC problem typically
shows highly nonlinear properties and varies widely at different operating points. In most
cases linear model will fail here as a sufﬁcient representation of the real plant dynamics. In
addition, the different components of a typical HVAC system (heating coils, fans, valves,
etc.) can not be modelled as separate systems as their dynamics are highly coupled. As
a consequence, linear control laws are prone to having highly variable gains at different
operation points that are difﬁcult to determine with classical control design methods.
Besides the nonlinear dynamic properties of the HVAC process, a very interesting aspect of this is the inﬂuence of environment on the dynamic of the process. For example,
the change of weather condition, the desired scheduling, or human behaviour can be measured, but hardly predicted. This makes it challenging to determine a control law even with
advanced classical control methods. This is the case because these techniques make assumptions about the underlying dynamics and form of the system. The process used as
control challenge here is based on a nonlinear model developed in Underwood and Crawford that was adapted to a real HVAC system by Andrew et al. in Anderson et al.
 . A schematic diagram of the process is shown in Fig. 16. The dynamic of the process
depends on three internal process variables: the ﬂow rate of the incoming boiler water, fw,
the temperature of the incoming boiler water, Two, and the temperature of the out coming
air. This internal process variables are directly inﬂuenced by the controller. In addition the
process dynamics depends on three external process variables: the temperature of the input
air, Tai, the temperature of the water that goes back to the boiler, Twi, and the ﬂow rate of
the incoming air, fa. These external process variables cannot be inﬂuenced by the controller
as they represent the inﬂuence of the environment on the process.
4.4.1 Control challenge
The control task is to set appropriate openings for the control valve, c, such as to have the
output air temperature reach as closely as possible to the current set-point, Td, under different and changing environmental conditions (see Table 14). The change of environmental
conditions acts as noise on the system dynamic. As a speciality in this setup, the current
Mach Learn 84:137–169
Table 14 The process variables,
controlled process variables,
set-point and control action for
the heating coil control challenge
ﬂow rate boiler water
temperature boiler water in
temperature air out
temperature air in
temperature boiler water out
ﬂow rate air in
desired temperature
∈[0.000,0.013]
opening of valve
∈ 
Table 15 Characterization of the
benchmark parameters for
evaluation
200 time-steps
50 time-steps
fw = 0.128 Two = 43.24 Tao = 40.1
uniformly distributed ∈ 
500 time-steps (16 seconds)
three steps from 40C to 45C
external process variables are not inﬂuenced by the controller, however, can be measured.
This scenario represents a nonlinear process dynamic with a high amount of noise. From the
perspective of both RL and classical control, it is a challenging task to produce time-optimal
and robust controllers that work precisely within a wide range of operating points.
4.4.2 Benchmark environment
For the benchmark setting (see Table 15), the variables, Tai, Twi, and, fa, are modiﬁed by
random walk every 30 time-steps in order to model the disturbances and changing conditions
that would occur in actual heating and air-conditioning systems. The admissible ranges for
the random walk are 4 ≤Tai ≤12C, 73 ≤Twi ≤81C and 0.7 ≤fa ≤0.9 kg/s.
For the evaluation of ¯N and ¯e∞of the controllers, 50 trajectories are executed, each with
a uniformly distributed set-point, Td ∈[40C,45C]. Each of the trajectories is 200 time-steps
long and starts with fw = 0.128, Two = 43.24, and Tao = 40.1. The external process variables are chosen randomly within the admissible range. To evaluate the overall behaviour,
the controllers are tested on a reference trajectory with 500 time-steps. This reference trajectory starts with the same initial conditions as the other evaluation trajectories.
4.4.3 Benchmark results
The learning curve for NFQCA is depicted in Fig. 17. Learning a controller with NFQCA
(parameters in Table 16) requires only 163 interaction trajectories and iteration of the algorithm. This corresponds to 32,600 interaction steps. In Table 17, the performance of the
learned controller is compared to a reference PI controller developed in Kretchmar .
In Fig. 18 the reference PI controller is shown when executed on the reference trajectory. In
Mach Learn 84:137–169
Fig. 17 Learning curves for NFQCA on the heating coil benchmark task. In every iteration 200 interaction samples with the process are collected using the recent learned controller. Afterwards an update of the
controller is done with all samples collected so far
Table 16 The learning
parameters for the heating coil
control task
low rate boiler water
temperature boiler water in
temperature air out
temperature air in
temperature boiler water out
ﬂow rate air in
T dao −Tao
control deviation
valve state
8-10-10-1 (600 epochs RProp)
4-15-1 (1000 epochs RProp)
Table 17 Benchmark evaluation
of the heating coil control
controller
contrast to the learned controller in Fig. 19, the PI controller has a high overshoot and reacts
on the external noise very slow. The controller learned with NFQCA exhibits a very good,
time-optimal behaviour and can react on the external changes very quickly.
5 Discussion
The contribution of this article is the presentation of four benchmarking scenarios for reinforcement learning in the ﬁeld of technical process control. From the perspective of both
classical control and reinforcement learning, the presented benchmark settings exhibit intriguing and challenging attributes. For the benchmark, all of the information is provided
Mach Learn 84:137–169
Fig. 18 PI controller result on the reference trajectory of the heating coil control challenge
Fig. 19 NFQCA controller result on the reference trajectory of the heating coil control challenge
such as to allow and encourage the implementation of the benchmark settings and, therefore,
the cataloguing of results using other methods. The presented benchmark environments and
evaluation setups will be implemented in the next ofﬁcial release of our software package
for benchmarking the “Closed Loop System Simulator (CLSquare)”.2
The proposed quantitative performance measures for the quality of the learned controller
and the learning performance allows for a comparison between different control methods.
The comparison of application-speciﬁc, classical control methods against learning controllers, as well as the comparison of different learning methods, is possible.
2Available at www.ml.uni-freiburg.de.
Mach Learn 84:137–169
By reporting the results of our own reinforcement learning algorithm—the Neural Fitted
Q-Iteration with Continuous Actions (NFQCA)—on the four presented benchmark problems, a baseline for other benchmarking results is given. The evaluation of the NFQCA algorithm within the benchmarks showed a very efﬁcient learning behaviour. With an amount
of interaction that falls within the range of a few hundred (or even under one hundred) of
interaction trajectories (corresponding to minutes of interaction on a real time process), the
application to real-world systems and problems comes into view. The benchmark results
show that we are able to learn high quality, continuous and nonlinear control laws with
NFQCA. It is certainly worth noting that what the benchmarks showed, is that we can learn
these high quality, continuous control laws with the same amount of interactions that are
needed for the NFQ algorithm with discrete actions.
Appendix: Benchmark details
A.1 Underwater vehicle
A.1.1 System dynamics
The system dynamic of the underwatervehicle is given by the dynamic equation:
˙v = f (v,u) = u · k(v,u) −c(v) · v · |v|
c(v) = 1.2 + 0.2 · sin(|v|)
m(v) = 3.0 + 1.5 · sin(|v|)
k(v,u) = −0.5 · tanh[(|(c · v · |v|) −u| −30.0) · 0.1] + 0.5
For simulating the magnetic levitation system we use the Runge Kutta 4 (RK4) numeric
integration scheme with 2 intermediate steps.
A.1.2 Benchmark details
To evaluate a controller in the underwater vehicle benchmark the reference trajectory is
depicted in Table 18.
A.2 Pitch control
A.2.1 System dynamics
Given the process state, χ = [α,q,θ]T , and control action, δ, the dynamic of the process is
described by:
˙α = −0.313α + 56.7q + 0.232δ
˙q = −0.0139α −0.426q + 0.0203δ
For simulating the pitch control system we use the Runge Kutta 4 (RK4) numeric integration
scheme with 10 intermediate steps.
Mach Learn 84:137–169
Table 18 A characterization of
the reference trajectory for the
underwater vehicle. The
reference trajectory is a linear
interpolation between these
points—constant when no point
behind is given (compare Fig. 7)
Table 19 A characterization of
the reference trajectory for the
pitch control benchmark. The
reference trajectory is a linear
interpolation between these
points—constant when no point
behind is given (compare Fig. 11)
A.2.2 Benchmark details
The reference trajectory is deﬁned by linear interpolations with intermediate points, given
in Table 19.
A.3 Magnetic levitation of a steel ball
A.3.1 System dynamics
Given the process state, χ = [χ1,χ2,χ3]T = [d, ˙d,I]T , and control action, u, the dynamic
of the process is described by:
α(χ) = g −
2M(x∞+ χ1)2
χ3(ξχ2 −R(x∞+ χ1)2)
ξ(x∞+ χ1) + L∞(x∞+ χ1)2
ξ + L∞(x∞+ χ1)
Mach Learn 84:137–169
Table 20 The simulation
parameters for the magnetic
levitation system
mass of steel ball
electrical resistance
coil parameter
coil parameter
L∞= 0.8052
coil parameter
ξ = 0.001599
process action
u ∈[−60,60]
control interval
t = 0.004
For simulating the magnetic levitation system we use the Runge Kutta 4 (RK4) numeric
integration scheme with 2 intermediate steps and the system parameters given in Table 20.
A.4 Heating coil
A.4.1 System dynamics
Given the process state, χ = [χ1,χ2,χ3,χ4,χ5,χ6]T = [fw,Two,Tao,Tai,Twi,fa]T , and
control action, u, the time discrete dynamic of the internal process variables is described
χ1(t + 1) = 6.7210−10u(t)3 −2.3010−6u(t)2 + 2.1810−3u(t) −0.2823
χ2(t + 1) = χ2(t) + 0.649χ1(t)χ5(t) −0.649χ1(t)χ2(t)
−0.012χ5(t + 1) −0.012χ2(t) + 0.023χ4(t + 1) + 0.104χ1(t)χ4(t + 1)
−0.052χ1(t)χ5(t + 1) −0.052χ1(t)χ2(t) + 0.028χ6(t + 1)χ4(t + 1)
−0.014χ6(t + 1)χ5(t + 1) −0.014χ6(t + 1)χ2(t)
χ3(t + 1) = χ3(t) + 0.197 ∗χ6(t + 1) ∗χ4(t + 1) −0.197 ∗χ6(t + 1) ∗χ3(t)
+ 0.016 ∗χ5(t + 1) + 0.016 ∗χ2(t) −0.032 ∗χ4(t + 1)
+ 0.077 ∗χ1(t) ∗χ5(t + 1) + 0.077 ∗χ1(t) ∗χ2(t)
−0.015 ∗χ1(t) ∗χ4(t + 1) + 0.022 ∗χ6(t + 1) ∗χ5(t + 1)
+ 0.022 ∗χ6(t + 1) ∗χ2(t) −0.045 ∗χ6(t + 1) ∗χ4(t + 1)
+ 0.206 ∗χ4(t) −0.206 ∗χ4(t + 1)
The dynamic of the external process variables are functions over time, χ4(t), χ5(t), χ6(t),
and are dependent on the benchmark setting.