A general reinforcement learning algorithm that
masters chess, shogi and Go through self-play
David Silver,1,2∗Thomas Hubert,1∗Julian Schrittwieser,1∗
Ioannis Antonoglou,1,2 Matthew Lai,1 Arthur Guez,1 Marc Lanctot,1
Laurent Sifre,1 Dharshan Kumaran,1,2 Thore Graepel,1,2
Timothy Lillicrap,1 Karen Simonyan,1 Demis Hassabis1
1DeepMind, 6 Pancras Square, London N1C 4AG.
2University College London, Gower Street, London WC1E 6BT.
∗These authors contributed equally to this work.
The game of chess is the longest-studied domain in the history of artiﬁcial intelligence.
The strongest programs are based on a combination of sophisticated search techniques,
domain-speciﬁc adaptations, and handcrafted evaluation functions that have been reﬁned
by human experts over several decades. By contrast, the AlphaGo Zero program recently
achieved superhuman performance in the game of Go by reinforcement learning from selfplay. In this paper, we generalize this approach into a single AlphaZero algorithm that can
achieve superhuman performance in many challenging games. Starting from random play
and given no domain knowledge except the game rules, AlphaZero convincingly defeated
a world champion program in the games of chess and shogi (Japanese chess) as well as Go.
The study of computer chess is as old as computer science itself. Charles Babbage, Alan
Turing, Claude Shannon, and John von Neumann devised hardware, algorithms and theory to
analyse and play the game of chess. Chess subsequently became a grand challenge task for
a generation of artiﬁcial intelligence researchers, culminating in high-performance computer
chess programs that play at a super-human level (1,2). However, these systems are highly tuned
to their domain, and cannot be generalized to other games without substantial human effort,
whereas general game-playing systems (3,4) remain comparatively weak.
A long-standing ambition of artiﬁcial intelligence has been to create programs that can instead learn for themselves from ﬁrst principles (5, 6). Recently, the AlphaGo Zero algorithm
achieved superhuman performance in the game of Go, by representing Go knowledge using
deep convolutional neural networks (7,8), trained solely by reinforcement learning from games
of self-play (9). In this paper, we introduce AlphaZero: a more generic version of the AlphaGo
Zero algorithm that accomodates, without special-casing, to a broader class of game rules. We
apply AlphaZero to the games of chess and shogi as well as Go, using the same algorithm and
network architecture for all three games. Our results demonstrate that a general-purpose reinforcement learning algorithm can learn, tabula rasa – without domain-speciﬁc human knowledge or data, as evidenced by the same algorithm succeeding in multiple domains – superhuman
performance across multiple challenging games.
A landmark for artiﬁcial intelligence was achieved in 1997 when Deep Blue defeated the
human world chess champion (1). Computer chess programs continued to progress steadily
beyond human level in the following two decades. These programs evaluate positions using
handcrafted features and carefully tuned weights, constructed by strong human players and
programmers, combined with a high-performance alpha-beta search that expands a vast search
tree using a large number of clever heuristics and domain-speciﬁc adaptations. In (10) we
describe these augmentations, focusing on the 2016 Top Chess Engine Championship (TCEC)
season 9 world-champion Stockﬁsh (11); other strong chess programs, including Deep Blue,
use very similar architectures (1,12).
In terms of game tree complexity, shogi is a substantially harder game than chess (13, 14):
it is played on a larger board with a wider variety of pieces; any captured opponent piece
switches sides and may subsequently be dropped anywhere on the board. The strongest shogi
programs, such as the 2017 Computer Shogi Association (CSA) world-champion Elmo, have
only recently defeated human champions (15). These programs use an algorithm similar to
those used by computer chess programs, again based on a highly optimized alpha-beta search
engine with many domain-speciﬁc adaptations.
AlphaZero replaces the handcrafted knowledge and domain-speciﬁc augmentations used in
traditional game-playing programs with deep neural networks, a general-purpose reinforcement
learning algorithm, and a general-purpose tree search algorithm.
Instead of a handcrafted evaluation function and move ordering heuristics, AlphaZero uses
a deep neural network (p, v) = fθ(s) with parameters θ. This neural network fθ(s) takes the
board position s as an input and outputs a vector of move probabilities p with components
pa = Pr(a|s) for each action a, and a scalar value v estimating the expected outcome z of
the game from position s, v ≈E[z|s]. AlphaZero learns these move probabilities and value
estimates entirely from self-play; these are then used to guide its search in future games.
Instead of an alpha-beta search with domain-speciﬁc enhancements, AlphaZero uses a generalpurpose Monte Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root state sroot until a leaf state is reached.
Each simulation proceeds by selecting in each state s a move a with low visit count (not previously frequently explored), high move probability and high value (averaged over the leaf states
of simulations that selected a from s) according to the current neural network fθ. The search
returns a vector πππ representing a probability distribution over moves, πa = Pr(a|sroot).
The parameters θ of the deep neural network in AlphaZero are trained by reinforcement
learning from self-play games, starting from randomly initialized parameters θ. Each game is
played by running an MCTS search from the current position sroot = st at turn t, and then
selecting a move, at ∼πππt, either proportionally (for exploration) or greedily (for exploitation)
with respect to the visit counts at the root state. At the end of the game, the terminal position
sT is scored according to the rules of the game to compute the game outcome z: −1 for a loss,
0 for a draw, and +1 for a win. The neural network parameters θ are updated to minimize the
error between the predicted outcome vt and the game outcome z, and to maximize the similarity
of the policy vector pt to the search probabilities πππt. Speciﬁcally, the parameters θ are adjusted
by gradient descent on a loss function l that sums over mean-squared error and cross-entropy
(p, v) = fθ(s),
l = (z −v)2 −πππ⊤log p + c||θ||2,
where c is a parameter controlling the level of L2 weight regularization. The updated parameters
are used in subsequent games of self-play.
The AlphaZero algorithm described in this paper (see (10) for pseudocode) differs from the
original AlphaGo Zero algorithm in several respects.
AlphaGo Zero estimated and optimized the probability of winning, exploiting the fact that
Go games have a binary win or loss outcome. However, both chess and shogi may end in drawn
outcomes; it is believed that the optimal solution to chess is a draw (16–18). AlphaZero instead
estimates and optimizes the expected outcome.
The rules of Go are invariant to rotation and reﬂection. This fact was exploited in AlphaGo
and AlphaGo Zero in two ways. First, training data were augmented by generating eight symmetries for each position. Second, during MCTS, board positions were transformed by using a
randomly selected rotation or reﬂection before being evaluated by the neural network, so that
the Monte Carlo evaluation was averaged over different biases. To accommodate a broader class
of games, AlphaZero does not assume symmetry; the rules of chess and shogi are asymmetric
(e.g. pawns only move forward, and castling is different on kingside and queenside). AlphaZero
does not augment the training data and does not transform the board position during MCTS.
In AlphaGo Zero, self-play games were generated by the best player from all previous iterations. After each iteration of training, the performance of the new player was measured against
the best player; if the new player won by a margin of 55% then it replaced the best player. By
contrast, AlphaZero simply maintains a single neural network that is updated continually, rather
than waiting for an iteration to complete. Self-play games are always generated by using the
latest parameters for this neural network.
Like AlphaGo Zero, the board state is encoded by spatial planes based only on the basic
rules for each game. The actions are encoded by either spatial planes or a ﬂat vector, again
based only on the basic rules for each game (10).
AlphaGo Zero used a convolutional neural network architecture that is particularly wellsuited to Go: the rules of the game are translationally invariant (matching the weight sharing
structure of convolutional networks) and are deﬁned in terms of liberties corresponding to the
adjacencies between points on the board (matching the local structure of convolutional networks). By contrast, the rules of chess and shogi are position-dependent (e.g. pawns may
Training AlphaZero for 700,000 steps. Elo ratings were computed from games
between different players where each player was given one second per move. (A) Performance
of AlphaZero in chess, compared with the 2016 TCEC world-champion program Stockﬁsh. (B)
Performance of AlphaZero in shogi, compared with the 2017 CSA world-champion program
Elmo. (C) Performance of AlphaZero in Go, compared with AlphaGo Lee and AlphaGo Zero
(20 blocks over 3 days).
move two steps forward from the second rank and promote on the eighth rank) and include
long-range interactions (e.g. the queen may traverse the board in one move). Despite these
differences, AlphaZero uses the same convolutional network architecture as AlphaGo Zero for
chess, shogi and Go.
The hyperparameters of AlphaGo Zero were tuned by Bayesian optimization. In AlphaZero
we reuse the same hyperparameters, algorithm settings and network architecture for all games
without game-speciﬁc tuning. The only exceptions are the exploration noise and the learning
rate schedule (see (10) for further details).
We trained separate instances of AlphaZero for chess, shogi and Go. Training proceeded for
700,000 steps (in mini-batches of 4,096 training positions) starting from randomly initialized
parameters. During training only, 5,000 ﬁrst-generation tensor processing units (TPUs) (19)
were used to generate self-play games, and 16 second-generation TPUs were used to train the
neural networks. Training lasted for approximately 9 hours in chess, 12 hours in shogi and 13
days in Go (see table S3) (20). Further details of the training procedure are provided in (10).
Figure 1 shows the performance of AlphaZero during self-play reinforcement learning, as
a function of training steps, on an Elo (21) scale (22). In chess, AlphaZero ﬁrst outperformed
Stockﬁsh after just 4 hours (300,000 steps); in shogi, AlphaZero ﬁrst outperformed Elmo after
2 hours (110,000 steps); and in Go, AlphaZero ﬁrst outperformed AlphaGo Lee (9) after 30
hours (74,000 steps). The training algorithm achieved similar performance in all independent
runs (see ﬁg. S3), suggesting that the high performance of AlphaZero’s training algorithm is
repeatable.
We evaluated the fully trained instances of AlphaZero against Stockﬁsh, Elmo and the previous version of AlphaGo Zero in chess, shogi and Go respectively. Each program was run on
the hardware for which it was designed (23): Stockﬁsh and Elmo used 44 central processing
unit (CPU) cores (as in the TCEC world championship), whereas AlphaZero and AlphaGo Zero
used a single machine with four ﬁrst-generation TPUs and 44 CPU cores (24). The chess match
was played against the 2016 TCEC (season 9) world champion Stockﬁsh (see (10) for details).
The shogi match was played against the 2017 CSA world champion version of Elmo (10). The
Go match was played against the previously published version of AlphaGo Zero (also trained
for 700,000 steps (25)). All matches were played using time controls of 3 hours per game, plus
an additional 15 seconds for each move.
In Go, AlphaZero defeated AlphaGo Zero (9), winning 61% of games. This demonstrates
that a general approach can recover the performance of an algorithm that exploited board symmetries to generate eight times as much data (see also ﬁg. S1).
In chess, AlphaZero defeated Stockﬁsh, winning 155 games and losing 6 games out of 1,000
(Fig. 2). To verify the robustness of AlphaZero, we played additional matches that started from
common human openings (Fig. 3). AlphaZero defeated Stockﬁsh in each opening, suggesting
that AlphaZero has mastered a wide spectrum of chess play. The frequency plots in Fig. 3 and
the timeline in ﬁg. S2 show that common human openings were independently discovered and
played frequently by AlphaZero during self-play training. We also played a match that started
from the set of opening positions used in the 2016 TCEC world championship; AlphaZero won
convincingly in this match too (26) (see ﬁg. S4). We played additional matches against the
most recent development version of Stockﬁsh (27), and a variant of Stockﬁsh that uses a strong
opening book (28). AlphaZero won all matches by a large margin (Fig. 2).
Table S6 shows 20 chess games played by AlphaZero in its matches against Stockﬁsh. In
several games AlphaZero sacriﬁced pieces for long-term strategic advantage, suggesting that it
has a more ﬂuid, context-dependent positional evaluation than the rule-based evaluations used
by previous chess programs.
In shogi, AlphaZero defeated Elmo, winning 98.2% of games when playing black, and
91.2% overall. We also played a match under the faster time controls used in the 2017 CSA
world championship, and against another state-of-the-art shogi program (29); AlphaZero again
won both matches by a wide margin (Fig. 2).
Table S7 shows 10 shogi games played by AlphaZero in its matches against Elmo. The
frequency plots in Fig. 3 and the timeline in ﬁg. S2 show that AlphaZero frequently plays one
of the two most common human openings, but rarely plays the second, deviating on the very
ﬁrst move.
AlphaZero searches just 60,000 positions per second in chess and shogi, compared with 60
million for Stockﬁsh and 25 million for Elmo (table S4). AlphaZero may compensate for the
lower number of evaluations by using its deep neural network to focus much more selectively
on the most promising variations (Fig. 4 provides an example from the match against Stockﬁsh)
– arguably a more “human-like” approach to search, as originally proposed by Shannon (30).
AlphaZero also defeated Stockﬁsh when given 1/10 as much thinking time as its opponent
(i.e. searching ~ 1/10, 000 as many positions), and won 46% of games against Elmo when
given 1/100 as much time (i.e. searching ~ 1/40, 000 as many positions), see Fig. 2. The high
performance of AlphaZero, using MCTS, calls into question the widely held belief (31,32) that
alpha-beta search is inherently superior in these domains.
The game of chess represented the pinnacle of artiﬁcial intelligence research over several
decades. State-of-the-art programs are based on powerful engines that search many millions
of positions, leveraging handcrafted domain expertise and sophisticated domain adaptations.
AlphaZero is a generic reinforcement learning and search algorithm – originally devised for the
game of Go – that achieved superior results within a few hours, searching 1/1, 000 as many positions, given no domain knowledge except the rules of chess. Furthermore, the same algorithm
was applied without modiﬁcation to the more challenging game of shogi, again outperforming
state-of-the-art programs within a few hours. These results bring us a step closer to fulﬁlling
a longstanding ambition of artiﬁcial intelligence (3): a general games playing system that can
learn to master any game.