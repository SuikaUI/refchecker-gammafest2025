Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4323–4332,
Hong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics
Patient Knowledge Distillation for BERT Model Compression
Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu
Microsoft Dynamics 365 AI Research
{Siqi.Sun,Yu.Cheng,Zhe.Gan,jingjl}@microsoft.com
Pre-trained language models such as BERT
have proven to be highly effective for natural language processing (NLP) tasks.
However, the high demand for computing resources
in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training,
we propose a Patient Knowledge Distillation
approach to compress an original large model
(teacher) into an equally-effective lightweight
shallow network (student). Different from previous knowledge distillation methods, which
only use the output from the last layer of the
teacher network for distillation, our student
model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: (i) PKD-Last: learning from the last k
layers; and (ii) PKD-Skip: learning from every k layers.
These two patient distillation
schemes enable the exploitation of rich information in the teacher’s hidden layers, and encourage the student model to patiently learn
from and imitate the teacher through a multilayer distillation process.
Empirically, this
translates into improved results on multiple
NLP tasks with signiﬁcant gain in training ef-
ﬁciency, without sacriﬁcing model accuracy.1
Introduction
Language model pre-training has proven to be
highly effective in learning universal language
representations from large-scale unlabeled data.
ELMo , GPT and BERT have
achieved great success in many NLP tasks, such as
sentiment classiﬁcation , natural language inference , and
question answering .
1Code will be avialable at 
intersun/PKD-for-BERT-Model-Compression.
Despite its empirical success, BERT’s computational efﬁciency is a widely recognized issue because of its large number of parameters. For example, the original BERT-Base model has 12 layers
and 110 million parameters. Training from scratch
typically takes four days on 4 to 16 Cloud TPUs.
Even ﬁne-tuning the pre-trained model with taskspeciﬁc dataset may take several hours to ﬁnish
one epoch. Thus, reducing computational costs for
such models is crucial for their application in practice, where computational resources are limited.
Motivated by this, we investigate the redundancy issue of learned parameters in large-scale
pre-trained models, and propose a new model
compression approach, Patient Knowledge Distillation (Patient-KD), to compress original teacher
(e.g., BERT) into a lightweight student model
without performance sacriﬁce. In our approach,
the teacher model outputs probability logits and
predicts labels for the training samples (extendable to additional unannotated samples), and the
student model learns from the teacher network to
mimic the teacher’s prediction.
Different from previous knowledge distillation
methods , we adopt a patient learning mechanism: instead of learning parameters from only the last layer of the teacher,
we encourage the student model to extract knowledge also from previous layers of the teacher network. We call this ‘Patient Knowledge Distillation’. This patient learner has the advantage of distilling rich information through the deep structure
of the teacher network for multi-layer knowledge
distillation.
We also propose two different strategies for the
distillation process: (i) PKD-Last: the student
learns from the last k layers of the teacher, under
the assumption that the top layers of the original
network contain the most informative knowledge
to teach the student; and (ii) PKD-Skip: the student learns from every k layers of the teacher, suggesting that the lower layers of the teacher network
also contain important information and should be
passed along for incremental distillation.
We evaluate the proposed approach on several NLP tasks, including Sentiment Classiﬁcation, Paraphrase Similarity Matching, Natural Language Inference, and Machine Reading
Comprehension. Experiments on seven datasets
across these four tasks demonstrate that the proposed Patient-KD approach achieves superior performance and better generalization than standard
knowledge distillation methods , with signiﬁcant gain in training efﬁciency
and storage reduction while maintaining comparable model accuracy to original large models. To
the authors’ best knowledge, this is the ﬁrst known
effort for BERT model compression.
Related Work
Language Model Pre-training
Pre-training has
been widely applied to universal language representation learning. Previous work can be divided
into two main categories: (i) feature-based approach; (ii) ﬁne-tuning approach.
Feature-based methods mainly focus on learning:
(i) context-independent word representation ,
GloVe , FastText ); (ii) sentence-level representation ; Conneau
et al. ; Logeswaran and Lee ); and
(iii) contextualized word representation , ELMo ). Speciﬁcally, ELMo 
learns high-quality, deep contextualized word representation using bidirectional language model,
which can be directly plugged into standard NLU
models for performance boosting.
On the other hand, ﬁne-tuning approaches
GPT ,
BERT ) on a large corpus with an unsupervised objective, and then ﬁne-tune the model
with in-domain labeled data for downstream
applications . Speciﬁcally, BERT is a large-scale
language model consisting of multiple layers
of Transformer blocks .
BERT-Base has 12 layers of Transformer and 110
million parameters, while BERT-Large has 24
layers of Transformer and 330 million parameters.
By pre-training via masked language modeling
and next sentence prediction, BERT has achieved
state-of-the-art performance on a wide-range of
NLU tasks, such as the GLUE benchmark and SQuAD .
However, these modern pre-trained language
models contain millions of parameters, which hinders their application in practice where computational resource is limited. In this paper, we aim
at addressing this critical and challenging problem, taking BERT as an example, i.e., how to
compress a large BERT model into a shallower
one without sacriﬁcing performance. Besides, the
proposed approach can also be applied to other
large-scale pre-trained language models, such as
recently proposed XLNet and
RoBERTa .
Model Compression & Knowledge Distillation
Our focus is model compression, i.e., making deep
neural networks more compact . A similar line of work has
focused on accelerating deep network inference at
test time and reducing model
training time .
A conventional understanding is that a large
number of connections (weights) is necessary for
training deep networks .
However, once the network has
been trained, there will be a high degree of parameter redundancy. Network pruning , in which network connections are reduced or sparsiﬁed, is one common strategy for model compression. Another direction is weight quantization , in which connection weights
are constrained to a set of discrete values, allowing
weights to be represented by fewer bits. However,
most of these pruning and quantization approaches
perform on convolutional networks. Only a few
work are designed for rich structural information
such as deep language models 
aims to compress a network with a large set
of parameters into a compact and fast-to-execute
model. This can be achieved by training a compact model to imitate the soft output of a larger
model. Romero et al. further demonstrated
that intermediate representations learned by the
large model can serve as hints to improve the training process and the ﬁnal performance of the compact model. Chen et al. introduced techniques for efﬁciently transferring knowledge from
an existing network to a deeper or wider network.
More recently, Liu et al. used knowledge
from ensemble models to improve single model
performance on NLU tasks.
Tan et al. 
tried knowledge distillation for multilingual translation. Different from the above efforts, we investigate the problem of compressing large-scale language models, and propose a novel patient knowledge distillation approach to effectively transferring knowledge from a teacher to a student model.
Patient Knowledge Distillation
In this section, we ﬁrst introduce a vanilla knowledge distillation method for BERT compression
(Section 3.1), then present the proposed Patient
Knowledge Distillation (Section 3.2) in details.
Problem Deﬁnition
The original large teacher
network is represented by a function f(x; θ),
where x is the input to the network, and θ denotes
the model parameters. The goal of knowledge distillation is to learn a new set of parameters θ′ for
a shallower student network g(x; θ′), such that the
student network achieves similar performance to
the teacher, with much lower computational cost.
Our strategy is to force the student model to imitate outputs from the teacher model on the training
dataset with a deﬁned objective LKD.
Distillation Objective
In our setting, the teacher f(x; θ) is deﬁned as a
deep bidirectional encoder, e.g., BERT, and the
student g(x; θ′) is a lightweight model with fewer
layers. For simplicity, we use BERTk to denote
a model with k layers of Transformers. Following the original BERT paper ,
we also use BERT-Base and BERT-Large to denote BERT12 and BERT24, respectively.
Assume {xi, yi}N
i=1 are N training samples,
where xi is the i-th input instance for BERT,
and yi is the corresponding ground-truth label.
BERT ﬁrst computes a contextualized embedding
hi = BERT(xi) ∈Rd. Then, a softmax layer
ˆyi = P(yi|xi) = softmax(Whi) for classiﬁcation is applied to the embedding of BERT output,
where W is a weight matrix to be learned.
To apply knowledge distillation, ﬁrst we need to
train a teacher network. For example, to train a 12layer BERT-Base as the teacher model, the learned
parameters are denoted as:
ˆθt = arg min
CE(xi, yi; [θBERT12, W])
where the superscript t denotes parameters in
the teacher model, [N] denotes set {1, 2, . . . , N},
denotes the cross-entropy loss for the
teacher training, and θBERT12 denotes parameters
of BERT12.
The output probability for any given input xi
can be formulated as:
ˆyi = P t(yi|xi) = softmax
W · BERT12(xi; ˆθt)
where P t(·|·) denotes the probability output from
the teacher.
ˆyi is ﬁxed as soft labels, and T
is the temperature used in KD, which controls
how much to rely on the teacher’s soft predictions. A higher temperature produces a more diverse probability distribution over classes . Similarly, let θs denote parameters
to be learned for the student model, and P s(·|·)
denote the corresponding probability output from
the student model. Thus, the distance between the
teacher’s prediction and the student’s prediction
can be deﬁned as:
P t(yi = c|xi; ˆθt)·
log P s(yi = c|xi; θs)
where c is a class label and C denotes the set of
class labels.
Besides encouraging the student model to imitate the teacher’s behavior, we can also ﬁne-tune
the student model on target tasks, where taskspeciﬁc cross-entropy loss is included for model
1[yi = c]·
log P s(yi = c|xi; θs)
Thus, the ﬁnal objective function for knowledge
distillation can be formulated as:
LKD = (1 −α)Ls
where α is the hyper-parameter that balances the
importance of the cross-entropy loss and the distillation loss.
Figure 1: Model architecture of the proposed Patient
Knowledge Distillation approach to BERT model compression. (Left) PKD-Skip: the student network learns
the teacher’s outputs in every 2 layers. (Right) PKD-
Last: the student learns the teacher’s outputs from the
last 6 layers. Trm: Transformer.
Patient Teacher for Model Compression
Using a weighted combination of ground-truth labels and soft predictions from the last layer of the
teacher network, the student network can achieve
comparable performance to the teacher model on
the training set.
However, with the number of
epochs increasing, the student model learned with
this vanilla KD framework quickly reaches saturation on the test set (see Figure 2 in Section 4).
One hypothesis is that overﬁtting during knowledge distillation may lead to poor generalization.
To mitigate this issue, instead of forcing the student to learn only from the logits of the last layer,
we propose a “patient” teacher-student mechanism to distill knowledge from the teacher’s intermediate layers as well. Speciﬁcally, we investigate two patient distillation strategies: (i) PKD-
Skip: the student learns from every k layers of the
teacher (Figure 1: Left); and (ii) PKD-Last: the
student learns from the last k layers of the teacher
(Figure 1: Right).
Learning from the hidden states of all the tokens is computationally expensive, and may introduce noise. In the original BERT implementation
 , prediction is performed by
only using the output from the last layer’s [CLS]
In some variants of BERT, like SDNet
 , a weighted average of all layers’ [CLS] embeddings is applied. In general,
the ﬁnal logit can be computed based on hﬁnal =
j∈[k] wjhj, where wj could be either learned parameters or a pre-deﬁned hyper-parameter, hj is
the embedding of [CLS] from the hidden layer
j, and k is the number of hidden layers. Derived
from this, if the compressed model can learn from
the representation of [CLS] in the teacher’s intermediate layers for any given input, it has the potential of gaining a generalization ability similar to
the teacher model.
Motivated by this, in our Patient-KD framework, the student is cultivated to imitate the representations only for the [CLS] token in the intermediate layers, following the intuition aforementioned that the [CLS] token is important in predicting the ﬁnal labels. For an input xi, the outputs
of the [CLS] tokens for all the layers are denoted
hi = [hi,1, hi,2, . . . , hi,k] = BERTk(xi) ∈Rk×d
We denote the set of intermediate layers to distill knowledge from as Ipt. Take distilling from
BERT12 to BERT6 as an example. For the PKD-
Skip strategy, Ipt = {2, 4, 6, 8, 10}; and for the
PKD-Last strategy, Ipt = {7, 8, 9, 10, 11}. Note
that k = 5 for both cases, because the output from
the last layer (e.g., Layer 12 for BERT-Base) is
omitted since its hidden states are connected to the
softmax layer, which is already included in the KD
loss deﬁned in Eqn. (5). In general, for BERT student with n layers, k always equals to n −1.
The additional training loss introduced by the
patient teacher is deﬁned as the mean-square loss
between the normalized hidden states:
i,Ipt(j)||2
where M denotes the number of layers in the student network, N is the number of training samples, and the superscripts s and t in h indicate the
student and the teacher model, respectively. Combined with the KD loss introduced in Section 3.1,
the ﬁnal objective function can be formulated as:
LPKD = (1 −α)Ls
CE + αLDS + βLPT
where β is another hyper-parameter that weights
the importance of the features for distillation in the
intermediate layers.
Experiments
In this section, we describe our experiments on applying the proposed Patient-KD approach to four
different NLP tasks. Details on the datasets and
experimental results are provided in the following
sub-sections.
We evaluate our proposed approach on Sentiment
Classiﬁcation, Paraphrase Similarity Matching,
Natural Language Inference, and Machine Reading Comprehension tasks.
For Sentiment Classiﬁcation, we test on Stanford Sentiment Treebank (SST-2)
 .
For Paraphrase Similarity Matching, we use Microsoft Research Paraphrase Corpus (MRPC)
 and Quora Question Pairs (QQP)2
For Natural Language Inference, we
evaluate on Multi-Genre Natural Language Inference (MNLI) , QNLI3 , and Recognizing Textual Entailment (RTE).
More speciﬁcally, SST-2 is a movie review
dataset with binary annotations, where the binary label indicates positive and negative reviews.
MRPC contains pairs of sentences and
corresponding labels, which indicate the semantic
equivalence relationship between each pair. QQP
is designed to predict whether a pair of questions
is duplicate or not, provided by a popular online question-answering website Quora. MNLI is
a multi-domain NLI task for predicting whether
a given premise-hypothesis pair is entailment,
contradiction or neural.
Its test and development datasets are further divided into in-domain
(MNLI-m) and cross-domain (MNLI-mm) splits
to evaluate the generality of tested models. QNLI
is a task for predicting whether a question-answer
pair is entailment or not. Finally, RTE is based
on a series of textual entailment challenges, created by General Language Understanding Evaluation (GLUE) benchmark .
For the Machine Reading Comprehension task,
we evaluate on RACE , a largescale dataset collected from English exams, containing 25,137 passages and 87,866 questions. For
each question, four candidate answers are pro-
2 
Question-Pairs
3The dataset is derived from Stanford Question Answer
Dataset (SQuAD).
vided, only one of which is correct. The dataset is
further divided into RACE-M and RACE-H, containing exam questions for middle school and high
school students.
Baselines and Training Details
For experiments on the GLUE benchmark, since
all the tasks can be considered as sentence (or
sentence-pair) classiﬁcation, we use the same architecture in the original BERT , and ﬁne-tune each task independently.
For experiments on RACE, we denote the input passage as P, the question as q, and the four
answers as a1, . . . , a4. We ﬁrst concatenate the
tokens in q and each ai, and arrange the input of
BERT as [CLS] P [SEP] q+ai [SEP] for each
input pair (P, q + ai), where [CLS] and [SEP]
are the special tokens used in the original BERT.
In this way, we can obtain a single logit value for
each ai. At last, a softmax layer is placed on top of
these four logits to obtain the normalized probability of each answer ai being correct, which is then
used to compute the cross-entropy loss for modeling training.
We ﬁne-tune BERT-Base (denoted as BERT12)
as the teacher model to compute soft labels for
each task independently, where the pretrained
model weights are obtained from Google’s ofﬁcial BERT’s repo4, and use 3 and 6 layers of
Transformers as the student models (BERT3 and
BERT6), respectively. We initialize BERTk with
the ﬁrst k layers of parameters from pre-trained
BERT-Base, where k ∈{3, 6}. To validate the
effectiveness of our proposed approach, we ﬁrst
conduct direct ﬁne-tuning on each task without using any soft labels. In order to reduce the hyperparameter search space, we ﬁx the number of hidden units in the ﬁnal softmax layer as 768, the
batch size as 32, and the number of epochs as 4
for all the experiments, with a learning rate from
{5e-5, 2e-5, 1e-5}. The model with the best validation accuracy is selected for each setting.
Besides direct ﬁne-tuning, we further implement a vanilla KD method on all the tasks by
optimizing the objective function in Eqn.
We set the temperature T as {5, 10, 20}, α =
{0.2, 0.5, 0.7}, and perform grid search over T, α
and learning rate, to select the model with the best
validation accuracy. For our proposed Patient-KD
approach, we conduct additional search over β
4 
BERT12 (Google)
BERT12 (Teacher)
Table 1: Results from the GLUE test server. The best results for 3-layer and 6-layer models are in-bold. Google’s
submission results are obtained from ofﬁcial GLUE leaderboard. BERT12 (Teacher) is our own implementation
of the BERT teacher model. FT represents direct ﬁne-tuning on each dataset without using knowledge distillation.
KD represents using a vanilla knowledge distillation method. And PKD represents our proposed Patient-KD-Skip
approach. Results show that PKD-Skip outperforms the baselines on almost all the datasets except for MRPC. The
numbers under each dataset indicate the corresponding number of training samples.
Figure 2: Accuracy on the training and dev sets of QNLI and MNLI datasets, by directly applying vanilla knowledge distillation (KD) and the proposed Patient-KD-Skip. The teacher and the student networks are BERT12 and
BERT6, respectively. The student network learned with vanilla KD quickly saturates on the dev set, while the
proposed Patient-KD starts to plateau only in a later stage.
from {10, 100, 500, 1000} on all the tasks. Since
there are so many hyper-parameters to learn for
Patient KD, we ﬁx α and T to the values used
in the model with the best performance from the
vanilla KD experiments, and only search over β
and learning rate.
Experimental Results
We submitted our model predictions to the ofﬁcial GLUE evaluation server to obtain results on
the test data. Results are summarized in Table 1.
Compared to direct ﬁne-tuning and vanilla KD,
our Patient-KD models with BERT3 and BERT6
students perform the best on almost all the tasks
except MRPC. For MNLI-m and MNLI-mm, our
6-layer model improves 1.1% and 1.3% over ﬁnetune (FT) baselines; for QNLI and QQP, even
though the gap between BERT6-KD and BERT12
teacher is relatively small, our approach still succeeded in improving over both FT and KD baselines and further closing the gap between the student and the teacher models.
Furthermore, in 5 tasks out of 7 (SST-2 (-2.3%
compared to BERT-Base teacher), QQP (-0.1%),
MNLI-m (-2.2%), MNLI-mm (-1.8%), and QNLI
(-1.4%)), the proposed 6-layer student coached by
the patient teacher achieved similar performance
to the original BERT-Base, demonstrating the effectiveness of our approach.
Interestingly, all
those 5 tasks have more than 60k training samples, which indicates that our method tends to perform better when there is a large amount of training data.
For the QQP task, we can further reduce the
model size to 3 layers, where BERT3-PKD can
still have a similar performance to the teacher
The learning curves on the QNLI and
MNLI datasets are provided in Figure 2. The student model learned with vanilla KD quickly saturated on the dev set, while the proposed Patient-
BERT6 (PKD-Last)
BERT6 (PKD-Skip)
Table 2: Performance comparison between PKD-Last and PKD-Skip on GLUE benchmark.
KD keeps learning from the teacher and improving
accuracy, only starting to plateau in a later stage.
For the MRPC dataset, one hypothesis for the
reason on vanilla KD outperforming our model is
that the lack of enough training samples may lead
to overﬁtting on the dev set. To further investigate,
we repeat the experiments three times and compute the average accuracy on the dev set. We observe that ﬁne-tuning and vanilla KD have a mean
dev accuracy of 82.23% and 82.84%, respectively.
Our proposed method has a higher mean dev accuracy of 83.46%, hence indicating that our Patient-
KD method slightly overﬁtted to the dev set of
MRPC due to the small amount of training data.
This can also be observed on the performance gap
between teacher and student on RTE in Table 5,
which also has a small training set.
We further investigate the performance gain
from two different patient teacher designs: PKD-
Last vs. PKD-Skip. Results of both PKD variants
on the GLUE benchmark (with BERT6 as the student) are summarized in Table 2. Although both
strategies achieved improvement over the vanilla
KD baseline (see Table 1), PKD-Skip performs
slightly better than PKD-Last. Presumably, this
might be due to the fact that distilling information across every k layers captures more diverse
representations of richer semantics from low-level
to high-level, while focusing on the last k layers
tends to capture relatively homogeneous semantic
information.
Results on RACE are reported in Table 3, which
shows that the Vanilla KD method outperforms direct ﬁne-tuning by 4.42%, and our proposed patient teacher achieves further 1.6% performance
lift, which again demonstrates the effectiveness of
Patient-KD.
Analysis of Model Efﬁciency
We have demonstrated that the proposed Patient-
KD method can effectively compress BERT12 into
BERT6 models without performance sacriﬁce. In
this section, we further investigate the efﬁciency of
Patient-KD on storage saving and inference-time
speedup. Parameter statistics and inference time
RACE RACE-M RACE-H
BERT12 (Leaderboard) 65.00
BERT12 (Teacher)
BERT6-PKD-Skip
Results on RACE test set.
(Leaderboard) denotes results extracted from the of-
ﬁcial leaderboard ( 
/data/RACE_leaderboard). BERT12 (Teacher)
is our own implementation. Results of BERT3 are not
included due to the large gap between the teacher and
the BERT3 student.
are summarized in Table 4. All the models share
the same embedding layer with 24 millon parameters that map a 30k-word vocabulary to a 768dimensional vector, which leads to 1.64 and 2.4
times of machine memory saving from BERT6 and
BERT3, respectively.
To test the inference speed, we ran experiments
on 105k samples from QNLI training set . Inference is performed on
a single Titan RTX GPU with batch size set to
128, maximum sequence length set to 128, and
FP16 activated. The inference time for the embedding layer is negligible compared to the Transformer layers. Results in Table 4 show that the
proposed Patient-KD approach achieves an almost
linear speedup, 1.94 and 3.73 times for BERT6 and
BERT3, respectively.
Does a Better Teacher Help?
To evaluate the effectiveness of the teacher model
in our Patient-KD framework, we conduct additional experiments to measure the difference
between BERT-Base teacher and BERT-Large
teacher for model compression.
Each Transformer layer in BERT-Large has
12.6 million parameters, which is much larger
than the Transformer layer used in BERT-Base.
For a compressed BERT model with 6 layers,
BERT6 with BERT-Base Transformer (denoted as
BERT6[Base]) has only 67.0 million parameters,
# Param (Emb)
# Params (Trm)
Total Params
Inference Time (s)
45.7M (2.40×)
27.35 (3.73×)
67.0M (1.64×)
52.51 (1.94×)
101.89 (1×)
Table 4: The number of parameters and inference time for BERT3, BERT6 and BERT12. Parameters in Transformers (Trm) grow linearly with the increase of layers. Note that the summation of # Param (Emb) and # Param (Trm)
does not exactly equal to Total Params, because there is another softmax layer with 0.6M parameters.
BERT12 (Teacher)
BERT24 (Teacher)
BERT6[Base]-KD
BERT6[Base]-KD
BERT6[Large]-KD
BERT6[Large]-PKD
Table 5: Performance comparison with different teacher and student models. BERT6[Base]/[Large] denotes a
BERT6 model with a BERT-Base/Large Transformer in each layer. For PKD, we use the PKD-Skip architecture.
while BERT6 with BERT-Large Transformer (denoted as BERT6[Large]) has 108.4 million parameters. Since the size of the [CLS] token embedding is different between BERT-Large and BERT-
Base, we cannot directly compute the patient
teacher loss (7) for BERT6[Base] when BERT-
Large is used as teacher.
Hence, in the case
where the teacher is BERT-Large and the student
is BERT6[Base], we only conduct experiments in
the vanilla KD setting.
Results are summarized in Table 5. When the
teacher changes from BERT12 to BERT24 (i.e.,
Setting #1 vs.
#2), there is not much difference between the students’ performance. Specifically, BERT12 teacher performs better on SST-2,
QQP and QNLI, while BERT24 performs better on
MNLI-m, MNLI-mm and RTE. Presumably, distilling knowledge from a larger teacher requires a
larger training dataset, thus better results are observed on MNLI-m and MNLI-mm.
We also report results on using BERT-Large
as the teacher and BERT6[Large] as the student.
Interestingly, when comparing Setting #1
with #3, BERT6[Large] performs much worse
than BERT6[Base] even though a better teacher
is used in the former case.
The BERT6[Large]
student also has 1.6 times more parameters than
BERT6[Base]. One intuition behind this is that the
compression ratio for the BERT6[Large] model is
4:1 (24:6), which is larger than the ratio used for
the BERT6[Base] model (2:1 (12:6)). The higher
compression ratio renders it more challenging for
the student model to absorb important weights.
When comparing Setting # 2 and #3, we observe that even when the same large teacher is
used, BERT6[Large] still performs worse than
BERT6[Base].
Presumably, this may be due
to initialization mismatch.
Ideally, we should
pre-train BERT6[Large] and BERT6[Base] from
scratch, and use the weights learned from the pretraining step for weight initialization in KD training. However, due to computational limits of training BERT6 from scratch, we only initialize the student model with the ﬁrst six layers of BERT12 or
BERT24. Therefore, the ﬁrst six layers of BERT24
may not be able to capture high-level features,
leading to worse KD performance.
Finally, when comparing Setting #3 vs.
where for setting #4 we use Patient-KD-Skip instead of vanilla KD, we observe a performance
gain on almost all the tasks, which indicates
Patient-KD is a generic approach independent of
the selection of the teacher model (BERT12 or
Conclusion
In this paper, we propose a novel approach to
compressing a large BERT model into a shallow one via Patient Knowledge Distillation. To
fully utilize the rich information in deep structure of the teacher network, our Patient-KD approach encourages the student model to patiently
learn from the teacher through a multi-layer distillation process. Extensive experiments over four
NLP tasks demonstrate the effectiveness of our
proposed model.
For future work, we plan to pre-train BERT
from scratch to address the initialization mismatch
issue, and potentially modify the proposed method
such that it could also help during pre-training.
Designing more sophisticated distance metrics for
loss functions is another exploration direction. We
will also investigate Patient-KD in more complex settings such as multi-task learning and meta