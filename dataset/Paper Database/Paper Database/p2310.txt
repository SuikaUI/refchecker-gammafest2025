ReBNet: Residual Binarized Neural Network
Mohammad Ghasemzadeh, Mohammad Samragh, and Farinaz Koushanfar
Department of Electrical and Computer Engineering, University of California San Diego
{mghasemzadeh, msamragh, farinaz}@ucsd.edu
Abstract—This
end-to-end
framework for training reconﬁgurable binary neural networks
on software and developing efﬁcient accelerators for execution on FPGA. Binary neural networks offer an intriguing
opportunity for deploying large-scale deep learning models on
resource-constrained devices. Binarization reduces the memory
footprint and replaces the power-hungry matrix-multiplication
with light-weight XnorPopcount operations. However, binary
networks suffer from a degraded accuracy compared to their
ﬁxed-point counterparts. We show that the state-of-the-art
methods for optimizing binary networks accuracy, signiﬁcantly increase the implementation cost and complexity. To
compensate for the degraded accuracy while adhering to the
simplicity of binary networks, we devise the ﬁrst reconﬁgurable
scheme that can adjust the classiﬁcation accuracy based on the
application. Our proposition improves the classiﬁcation accuracy by representing features with multiple levels of residual
binarization. Unlike previous methods, our approach does not
exacerbate the area cost of the hardware accelerator. Instead,
it provides a tradeoff between throughput and accuracy while
the area overhead of multi-level binarization is negligible.
Keywords-Deep neural networks, Reconﬁgurable computing,
Domain-customized computing, Binary neural network, Residual binarization.
I. INTRODUCTION
Convolutional Neural Networks (CNNs) are widely used
in a variety of machine learning applications, many of which
are deployed on embedded devices , , . With the
swarm of emerging intelligent applications, development of
real-time and low-power hardware accelerators is especially
critical for resource-limited settings. A line of research
has therefore been focused on the development of FPGA
accelerators for execution of CNN applications , , .
Although the building blocks of CNNs are highly parallelizable, the high computational complexity and memory footprint of these models are barriers to efﬁcient implementation.
A number of prior works have focused on reducing the
computational complexity and memory footprint of CNNs
by trimming the redundancies of the model prior to designing an accelerator. Examples of such optimization techniques
include tensor decomposition , , , parameter quantization , , , sparse convolutions , , and
training binary neural networks , .
Among the above optimization techniques, binary networks result in two particular beneﬁts: (i) They reduce
the memory footprint compared to models with ﬁxed-point
parameters; this is especially important since memory access
plays an essential role in the execution of CNNs on resourceconstrained platforms. (ii) Binary CNNs replace the powerhungry multiplications with simple XNOR operations ,
 , signiﬁcantly reducing the runtime and energy consumption. Consequent to the aforementioned beneﬁts, the
dataﬂow architecture of binary CNNs is remarkably simpler
than their ﬁxed-point counterparts.
There are several remaining challenges for training binary
CNNs. First, the training phase of binary neural networks is
often slow and the ﬁnal classiﬁcation accuracy is typically
lower than the model with full-precision parameters. It has
been shown that the loss of accuracy can be partially evaded
by training binary networks that have wider layers .
Nevertheless, this method for accuracy enhancement diminishes the performance gains of binarization. In another
effort, authors of XNOR-net tried to improve the
accuracy of the binary CNN using scaling factors that are
computed by averaging the features during inference. This
method, however, sacriﬁces the simplicity of the binary
CNN accelerator by adding extra full-precision calculations
to the computation ﬂow of binary CNN layers. Similarly,
the approach in involves multiple rounds of computing
the average absolute value of input activations which incurs
an excessive computation cost during the inference phase.
Ternary neural networks , may surpass binary
CNNs in terms of the inference accuracy; however, they are
deprived of the beneﬁts of simple XNOR operations.
We argue that a practical solution for binary CNNs should
possess two main properties: (i) The accuracy of the binary
model should be comparable to its full-precision counterpart.
(ii) The proposed method to improve the accuracy should
not compromise the low overhead and accelerator scalability
beneﬁts of binary networks. This paper proposes ReBNet,
an end-to-end framework for training reconﬁgurable binary
CNNs in software and developing efﬁcient hardware accelerators for execution on FPGA. We introduce the novel
concept of multi-level binary CNNs and design algorithms
for learning such models. Building upon this idea, we
design and implement a scalable FPGA accelerator for
binary CNNs. The beneﬁt of our approach over existing
binarization methods is that the number of binarization levels
can be adjusted for different applications without signiﬁcant
hardware modiﬁcation.
In ReBNet, the weight parameters of CNN layers are
represented with 1-level binarized values, while a multi-level
residual binarization scheme is learned for the activation
units. As such, the memory footprint of the parameters in
ReBNet is the same as that of a single-level Binary CNN.
We show that the accuracy of ReBNet can be improved by
using 2 or 3 levels of residual binarization with a negligible
area overhead. The design of ReBNet residual binarization is
 
compatible with the standard XnorPopcount operations; the
underlying computations involving the feature vectors can
be decomposed into a number of standard XnorPopcount
operations. As such, ReBNet provides scalability for the
design of binary CNNs. The contributions of this paper are
summarized as follows:
• Proposing residual binarization as a reconﬁgurable dimension of binary CNNs. We devise an activation
function with few scaling factors that are used for
residual binarization. We also introduce a method for
training the scaling factors.
• Development of an Application Programming Interface
(API) for training multi-level binarized CNNs1.
• Creation of a hardware library for implementation of
different CNN layers using ReBNet methodology. The
library allows users to conﬁgure the parallelism in each
CNN layer using high-level parameters1.
• Performing proof-of-concept evaluations on four benchmarks on three FPGA platforms.
II. PRELIMINARIES
In this section, we outline the operations of binary CNNs
and their hardware implementation. Speciﬁcally, we brieﬂy
describe the FPGA design proposed by . Please refer to
the original paper for a more detailed explanation.
A. Binary CNN Operations
Neural networks are composed of multiple convolution,
fully-connected, activation, batch-normalization, and maxpooling layers. Binarization enables the use of a simpler
equivalent for each layer as explained in this section.
Binary dot product: The computational complexity of
neural networks is mostly owing to the convolution and
fully-connected layers. Both layers can be broken into a
number of dot products between input features and weight
parameters. A dot product accumulates the element-wise
products of a feature vector ⃗x and a weight vector ⃗w:
dot(⃗x,⃗w) = ∑
⃗x[i]×⃗w[i]
In case of binary CNNs, the elements of ⃗x and ⃗w are
restricted to binary values ±γx and ±γw, respectively. The
dot product of these vectors can be efﬁciently computed
using XnorPopcount operations as suggested in , .
Let ⃗x = γx ⃗sx and ⃗w = γw ⃗sw, where {γx,γw} are scalar values
and {⃗sx,⃗sw} are sign vectors whose elements are either +1
or −1. If we encode the sign values (−1 →0 and +1 →1),
we obtain binary vectors {⃗bx,⃗bw}. The dot product between
⃗x and ⃗w can be computed as:
dot(⃗w,⃗x) = γxγw dot(⃗sx,⃗sw) = γxγw XnorPopcount(⃗bx, ⃗bw)
equivalence
dot(⃗sx,⃗sw)
XnorPopcount(⃗bx, ⃗bw) using an example.
Binary activation: The binary activation function encodes
an input y using a single bit based on the sign of y. Therefore,
the hardware implementation only requires a comparator.
1Codes are available at 
Figure 1: The equivalence of dot product (top) and
XnorPopcount (bottom) operations. In the popcount operation, p is the number of set bits and N is the size of the
Figure 2: The equivalence of max-pooling operation over
binarized features (left) and OR operation over encoded
features (right).
Binary batch-normalization: It is often useful to normalize
the result of the dot product y = dot(⃗x,⃗w) before feeding it
to the binary activation function described above. A batchnormalization layer converts each input y into α × y −β,
where α and β are the parameters of the layer. Authors
of suggest combining batch-normalization and binaryactivation layers into a single thresholding layer. The cascade
of the two layers computes the following:
output = Sign(α ×y−β) = Sign(y−β
Therefore, the combination of the two layers only requires
a comparison with the threshold value β
Binary max-pooling: A max-pooling layer computes the
maximum of features over sliding windows of the input
feature map. The max-pooling operation can be performed
by element-wise OR over the binary features. Figure 2
depicts the equivalence between max-pooling over ﬁxedpoint features ⃗x ∈{+γ,−γ}N and element-wise OR over
binary features ⃗bx ∈{0,1}N.
B. Hardware Implementation and Parallelism
Matrix multiplication: Authors of propose the ﬂow
diagram of Figure 3-(a) to implement different layers of
binary CNNs on FPGA. The sliding window unit (SWU)
scans the input feature maps of convolution layers and
feeds appropriate values to the corresponding matrix vector threshold unit (MVTU). Both convolution and fullyconnected layers are implemented using the MVTU, which
realizes matrix-vector multiplication, batch normalization,
and binary activation.
Parallelism: The MTVU offers two levels of parallelism for
matrix-vector multiplication as depicted in Figure 3-(b,c).
First, each MTVU has a number of processing elements
(PEs) that compute multiple output neurons in parallel; each
PE is responsible for a single dot product between two
binarized vectors. Second, each PE breaks the corresponding
Max-Pooling
Binary Features
Input Image Stream
Input Vector Buffer
Output Vector Buffer
Output Image Stream
SIMD Lanes (S)
Accumulator
Output Vector
Input Vector
Figure 3: (a) Computation ﬂow of FINN accelerator.
(b) An MVTU with “P” processing elements, each with
SIMD-width of “S”. (c) Architecture of a processing unit.
“S” is the SIMD-width and “T” is the ﬁxed-point bitwidth.
operation into a number of sub-operations, each of which
performed on SIMD-width binary words.
III. OVERVIEW
The global ﬂow of ReBNet API is presented in Figure 4.
The user provides the CNN architecture to the software
library, which trains the binary CNN with a speciﬁed number
of residual binarization levels. She/he also provides the
network description using our hardware library, along with
the parallelism factors for the hardware accelerator. Based
on these parallelism factors (PE-count and SIMD-width),
the binary network parameters are re-aligned and stored
appropriately to be loaded into the hardware accelerator. The
bitﬁle is then generated using the hardware library.
In this section, we ﬁrst describe residual binarization as
a reconﬁgurable dimension in the design of binary CNNs.
We next discuss the training methodology for residual binary
networks. Finally, we elaborate on our hardware accelerator.
A. Residual Binarization
Imposing binary constraints on weights and activations
of a neural network limits the model’s ability to provide
the inference accuracy that a ﬂoating-point or ﬁxed-point
counterpart would achieve. To address this issue, we propose
a multi-level binarization scheme where the residual errors
are sequentially binarized to increase the numerical precision
of the approximation.
Multi-level residual binarization: Figure 5 presents the
procedure to approximate a ﬁxed-point input x with a multilevel binarized value e. For an M-level binarization scheme,
Software Library
(Training Binary CNNs)
Hardware Library
(Execution on FPGA)
CNN Architecture
(Keras Description)
CNN Architecture
(Vivado HLS Description)
Re-align CNN Parameters
Layer-wise Parallelism
Parameters
Figure 4: The global ﬂow of ReBNet software training and
hardware accelerator synthesis.
Figure 5: Schematic ﬂow for computing an M-level residual
binary approximate eM and the corresponding encoded bits
{b1,b2,··· ,bM}. As one goes deeper in levels, the estimation
becomes more accurate. In this ﬁgure, we drop the subscript
x from γxi and represent them as γi for simplicity.
there exist M scaling factors {γ1,γ2,...,γM}. The ﬁrst level
of binarization takes input x and approximates it by either
+γ1 or −γ1 based on the sign of x, then computes the residual error r2 = x−γ1 ·Sign(x). The second level of binarization approximates r2 by either +γ2 or −γ2 based on the sign
of r2, then computes the residual error r3 = r2−γ2·Sign(r2).
Repeating this process for M-times results in an M-level
binarization for the input value. More formally, an input x
can be approximated as e = ∑M
i=1 γi · Sign(ri) with ri being
the i-th residual error. In ReBNet, the same set of scaling
factors is used for all features corresponding to a certain
CNN layer; therefore, the features can be encoded using
M bits. Algorithm 1 presents the procedure for computing
encoded bits {b1,b2,··· ,bn}.
Algorithm 1 M-level residual encoding algorithm
inputs: γ1,γ2,...,γM, x
outputs: b1,b2,...,bM
2: for i = 1...M do
bi ←Binarize(Sign(r))
r ←r −Sign(r)×γi
5: end for
Residual binary activation function: Similar to previous
works which use the Sign function as the activation function,
in this paper we use the residual binarization. The difference
between our approach and the single-bit approach is illustrated in Figure 6. The activation function includes a set of
scaling factors {γ1,γ2,...,γM} that should be learned during
the training phase.
Multi-level XnorPopcount: In ReBNet, the dot product of
Figure 6: Illustration of binarized activation function. (a)
Conventional 1-level binarization. (b) Residual binarization
with two levels. Note that the γ parameters are universal
across all features (activations) of a particular layer.
an M-level residual-binarized feature vector ⃗e and a vector
of binary weights ⃗w can be rendered using M subsequent
XnorPopcount operations. Let ⃗e = ∑M
i=1 γei⃗sei and ⃗w = γw⃗sw,
where⃗sei denotes the i-th residual sign vector of the features
and ⃗sw is the sign vector of the weight vector. The dot
product between ⃗e and ⃗w is computed as:
dot(⃗w,⃗e)
i=1 γei⃗sei,γw⃗sw)
i=1 γeiγw dot(⃗sei,⃗sw)
i=1 γeiγw XnorPopcount(⃗bei,⃗bw),
where {⃗bei, ⃗bw} are the binary encodings corresponding to {⃗sei,⃗sw}, respectively. Note that the subsequent
XnorPopcount operations can be performed sequentially on
the same hardware accelerator, providing a tradeoff between
runtime and approximation accuracy.
B. Training residual binary CNNs
Training neural networks is generally performed in two
steps. First, the output layer of the neural network is
computed and a cost function is derived. This step is called
forward propagation. In the second step, known as backward
propagation, the gradient of the cost function with respect
to the CNN parameters is computed and the parameters are
updated accordingly to minimize the cost function.
For binary neural networks, the forward propagation step
is performed using the binary approximations of the parameters and the features. In the backward propagation step,
however, the full-precision parameters are updated. Once
the network is trained, the full-precision parameters are
binarized and used for efﬁcient inference.
In this section, we derive the gradients for training residual
binarized parameters in CNNs. Let L denote the cost
function of the neural network. Consider a full-precision
feature (weight) x approximated by a single binary value
e = γ · Sign(x). The derivatives of the cost function with
respect to γ is computed as follows:
∂e ×Sign(x)
Similarly for x:
∂Sign(x) × ∂Sign(x)
∂e ×γ ×1|x|≤1
where the derivative term ∂Sign(x)
is approximated as sug-
Figure 7: The baseline sliding window unit (top), and
our M-bit sliding window unit (bottom).
gested in :
In a multi-level binarization scheme, input x is approximated as e = ∑i γi ·Sign(ri) with ri denoting the i −th
residual error. The gradients can be computed similar to the
derivatives above:
∂e ×Sign(ri)
∂e ×∑i γi ×1|ri|≤1
Note that the training phase of ReBNet is performed on
a ﬂoating point processor (e.g., CPU or GPU) and the
parameters have full-precision values in this phase. After
training, the binary approximates are loaded into the binary
hardware accelerator.
C. Hardware Accelerator
ReBNet includes a high-level synthesis library for FPGA
implementation of binary CNNs. The accelerator architecture is inspired by the one described in Figure 3 but provides
a paradigm shift in term of overhead and scaling properties.
Unlike the previous works that only accommodate single-bit
binary CNNs, our accelerator offers a reconﬁgurable design
that enjoys residual binarization with minimal hardware
modiﬁcation. In this section, we discuss different components of ReBNet accelerator and compare them with those
of the baseline accelerator discussed in Section II-B.
Sliding window unit (SWU): Figure 7 depicts a highlevel overview of the SWU. It slides the (dashed) windows
through the feature maps, converts them into a binary vector,
and splits the binary vector into S-bit words (S is the SIMDwidth), which are sequentially sent to the MVTU. In the case
of M-level residual binarization, the SWU still sends S-bit
words but this time it transfers M such words sequentially.
This approach enables a scalable design with a ﬁxed SIMDwidth; therefore, the reconﬁgurability of M incurs negligible
hardware overhead while the runtime grows linearly with M.
Matrix vector threshold unit (MVTU): This unit is responsible for computing the neuron outputs in convolution
and fully-connected layers. Similar to the baseline accelerator in Section II-B, our MVTU offers two levels of
Coefficients
(This Layer)
Input Vector
(Algorithm 1)
Coefficients
(Next Layer)
Index 2 = i
Output Vector
Figure 8: Architecture of processing element in ReBNet.
“S” is the SIMD-width, “T” is the ﬁxed-point bitwidth in
computations, and “M” is the number of residual levels.
parallelism. The internal architecture of PE is shown in
Figure 8. Compared to the baseline unit of Figure 3-(c), our
PE maintains multiple accumulators to store the popcount
values corresponding to each of the M residual binarization
levels. Once all M popcounts are accumulated, they are
multiplied by the corresponding scaling factors (γi) and
summed together via the MAC unit. Batch normalization
is implemented using the threshold memory. Finally, the
encoder module computes the M-bit residual representation
fed to the next layer based on Algorithm 1.
Max-pooling: The original 1-bit binarization allows us
to implement max-pooling layers using simple OR operations. For M-level binarization, however, max-pooling layers
should be implemented using comparators since performing Boolean OR over the binary encodings is no longer
equivalent to performing max-pooling over the features.
Nevertheless, the pooling operation can be performed over
the encoded values directly. Assume full-precision values ex
and ey, with M-bit binary encodings bx and by, respectively.
Considering ordered positive γi values (i.e. γ1 > γ2 > ··· >
γl > 0), one can conclude that if bx < by then ex < ey;
therefore, instead of comparing the original values (ex,ey),
we compare the binary encodings (bx,by) which require
small comparators.
IV. EXPERIMENTS
We implement our Software API using Keras library.
Our hardware API is implemented in Vivado HLS and
the synthesis reports (resource utilization and latency) for
the FPGA accelerator are obtained using Vivado Design
Suite . We compare ReBNet with the prior art in terms of
accuracy, FPGA resource utilization, execution (inference)
time on the FPGA accelerator, and scalability. Proof-ofconcept evaluations are performed for four datasets: CIFAR-
10, SVHN, MNIST, and ILSVRC-2012 (Imagenet). Our
hardware results are compared against the FINN design ,
which uses the training method of Binarynet . The FINN
paper only evaluates the ﬁrst three applications. For the
last dataset (Imagenet), we implemented the corresponding
FINN accelerator using their open-source library. Throughout this section, M denotes the number of residual binarizations and T = 24 is the ﬁxed-point bitwidth of features.
We implement a small network consisting only fullyconnected layers for MNIST, which we call Arch-1. The
CIFAR-10 and SVHN datasets are evaluated on a mediumsized convolutional neural network named Arch-2. Finally,
the Imagenet dataset is evaluated on a relatively large network called Arch-3. Table I outlines the three neural network
architectures and the corresponding parallelism factors for
different layers. The parallelism factors only affect hardware
performance and have no effect on the CNN accuracy. For
Arch-1 and Arch-2, we set the parallelism factors exactly
the same as the baselines in the FINN design. For Arch-3,
we conﬁgure the parallelism factors ourselves. Each of these
architectures is implemented on a different FPGA evaluation
board outlined in Table II.
Table I: Network architectures for evaluation benchmarks.
C64(P,S) denotes a convolution with 64 output channels;
the kernel size of the convolution is 3×3 and the stride is 1
unless stated otherwise. D512(P,S) means a fully-connected
layer with 512 outputs. The numbers (P,S) represent the two
parallelism factors (PE-count, SIMD-width) for the layers.
MP2 stands for 2 × 2 max pooling, BN represents batch
normalization. Residual Binarization is shown using RB.
CNN Architecture
784 (input)- D256(16,64)- BN- RB- D256(32,16)- BN-
RB- D256(16,32)- BN- RB- D10(16,4)- BN- Softmax
3×32×32 (input)- C64(16,3)- BN- RB- C64(32,32)-
BN- RB- MP2- C128(16,32)- BN- RB- C128(16,32)-
BN- RB- MP2- C256(4,32)- BN- RB- C256(1,32)- BN-
RB- D512(1,4)- BN- RB- D512(1,8)- BN- RB- D10(4,1)-
BN- Softmax
3×224×224 (input)- C96(32,33)∗- BN- RB- MP3-
C256(64,25)∗∗- BN- RB- MP3- C384(64,54)- BN- RB-
C384(64,36)- BN- RB- C256(64,36)- BN- RB- MP3-
D4096(128,64)- BN- RB- D4096(128,64)- BN- RB-
D1000(40,50)- BN- Softmax
∗Stride is 4 for this layer, ﬁlter size is 11×11.
∗∗Filter size is 5×5 for this layer.
A. Accuracy and Throughput Tradeoff
The accuracy of a binary CNN depends on the network
architecture and the number of training epochs. The training
phase is performed on a computer, not the FPGA accelerator.
However, we provide the learning curve of ReBNet with
different numbers of residual levels to show that residual binarization increases the convergence speed of binary CNNs.
Figure 9 presents the learning curves (accuracy versus
epoch) for the four applications. As can be seen, increasing
M improves both the convergence speed and the ﬁnal achievable accuracy. On the other hand, a higher M requires more
computation time during the execution (after the accelerator
is implemented on FPGA). Table III compares our accuracy
and throughput with the conventional binarization method
proposed in and implemented (on FPGA) by the FINN
design . Even with 1 level of binarization, our method
surpasses the FINN design in terms of accuracy, which is
due to the trainable scaling factors of ReBNet. As can be
seen, we can improve the accuracy of ReBNet by increasing
Table II: Platform details in terms of block ram (BRAM),
DSP, ﬂip-ﬂop (FF), and look-up table (LUT) resources.
Application
Virtex VCU108
CIFAR-10 & SVHN
Zynq ZC702
Spartan XC7S50
(b) CIFAR-10
(d) Imagenet
Figure 9: Top-1 accuracy of ReBNet for different benchmarks. M denotes the number of residual binarization levels. MNIST,
CIFAR-10, SVHN, and Imagenet models are trained for 200, 200, 50, and 100 epochs, respectively.
Table III: Comparison of accuracy and throughput between
the FINN design and ReBNet. The baseline accuracy for
the ﬁrst three datasets is reported by . The authors
of XNOR-net implement Binarynet and report
the top-1 accuracy of 27.9% for Imagenet. As M grows,
the accuracy of ReBNet is increased. Note that for CNN
applications, even incremental accuracy gains are of importance . The clock frequency of our designs is 200Mhz.
Accuracy (%)
Throughput (samples/sec)
Accuracy (%)
Throughput (samples/sec)
Accuracy (%)
Throughput (samples/sec)
Accuracy (%)
Throughput (samples/sec)
M, which, in turn, reduces the throughput. We evaluate the
extra hardware cost of having M ≥2 in the next section.
B. Hardware Evaluation and Reconﬁgurability
Figure 10-(a, b, c) compares the resource utilization of
ReBNet with the baseline FINN design for each of the
three network architectures. Indeed, compared to the less
accurate FINN design, ReBNet has an added area cost
mainly due to the design reconﬁgurability. Recall the PE
design in Figure 8; The MAC block, the thresholding unit,
and the encoder unit altogether require extra computation
resources, which explains why ReBNet has extra DSP
utilization compared to FINN. The BRAM, FF, and LUT
utilizations of ReBNet increase with M because the PEs of
the design would require more accumulators and a more
complex control logic. Recall that the FINN design has lower
accuracy compared to ReBNet (See Table III). In fact, If we
were to enhance the accuracy of FINN (by training wider
networks), the increase in resource utilizations would exceed
the limitations of the board as discussed in Section IV-C.
Figure 10-(d) compares the latency (runtime) of ReBNet
with FINN. The latency of our accelerator with M = 1 is
the same as that of FINN; this is due to the fact that the
runtime overhead of the extra modules of ReBNet, i.e., the
MAC and encoder modules, is negligible compared to the
XnorPopcount operations. Overall, the latency of ReBNet
grows linearly with M since the number of XnorPopcount
operations (for a single dot product) is equal to M (see
Equation 4). As such, ReBNet offers a tradeoff between
accuracy and latency.
C. Effect of CNN size
As we demonstrated before, ReBNet improves the accuracy using more residual binarization levels. Another method
for enhancing the accuracy is to train a wide network as
analyzed in . In our notation, a reference network is
widened by the scale S if all hidden layers are expanded
by a factor of S, i.e., the widened network has S-times
more output channels in convolution layers and S-times
more output neurons in fully-connected layers. Consider the
CIFAR-10 architecture Arch-2 in Table I. In Figure 11, we
depict the accuracy of the Arch-2 network with M = 1 that
is widened with different scales (variable S). The accuracies
of the 2-level and 3-level binarization schemes correspond
to the reference Arch-2 with no widening (S = 1). It can be
seen that the 1-level network should be widened by scales
of 2.25 and 2.75 to achieve the same accuracy as the 2-level
and 3-level networks, respectively.
In Figure 12, we compare the reference 2-level and 3level residual binary networks with their widened 1-level
counterparts that achieve the same accuracies. In this Figure,
the resource utilization (i.e., BRAM, DSP, FF, and LUT)
are normalized by the maximum resources available on the
FPGA platform and the latency is normalized by that of the
reference CNN (i.e., S = 1 and M = 1). It is worth noting that
widening a network by a factor of S increases the computation and memory burden by a factor of S2 while adding to
the number of residual levels M incurs a linear overhead. As
such, widening the network explodes the resource utilization
and exceeds the platform constraints. In addition, the latency
of the wide CNN increases quadratically with S whereas in
ReBNet the latency increases linearly with M. Therefore,
ReBNet achieves higher throughput and lower resource
utilization for a certain classiﬁcation accuracy.
D. Effect of Online Scaling Factors
Authors of XNOR-net suggest computing the scaling
factors during the execution. Here, we show that such computations add signiﬁcant hardware cost to the binary CNN
accelerator. We assume that the runtime and DSP utilization
overhead of online scaling factor computation are negligible.
This section only considers the excessive memory footprint
of online scaling factors. Consider a convolution layer with
K × K ﬁlters and input image IH×H×F to be binarized
using online scaling factors. Recall the SWU module that
is responsible for generating input vectors for the MVTU.
The SWU is implemented using streaming buffers, meaning
Figure 10: (a, b, c) Normalized hardware utilization and (d) latency of ReBNet compared to FINN. The utilizations are
normalized by maximum resources available on each platform. The latencies are normalized by the latency of FINN. In
ReBNet, the hardware overhead of reconﬁgurability is minimal and latency grows linearly with M. Note that the accuracy of
FINN in all benchmarks is lower than ReBNet with M = 1 and as we increase M, the accuracy is improved (see Table III).
FINN, however, incurs excessive area and latency costs to increase the accuracy and thus is not scalable (see Figure 12).
Figure 11: Effect of network widening on the accuracy of the
CIFAR-10 benchmark. The widened CNNs are trained for
200 epochs and the best accuracy is accounted. Although
wider single-level networks are capable of achieving the
same accuracy as multi-level networks, the hardware cost
and latency of wider networks are signiﬁcantly larger than
those of multi-level CNNs (see Figure 12).
Figure 12: Hardware cost and latency comparison of residual
binarized CNN and widened CNN with the same accuracy.
(a) Arch-2 widened by a scale of 2.25 compared to Arch-2
with 2-level binarization. (b) Arch-2 widened by a scale of
2.75 compared to Arch-2 with 3-level binarization. Network
widening explodes the resource utlization and latency of
the accelerator while ReBNet offers a scalable solution by
providing a tradeoff between accuracy and latency.
that it does not store the whole input image at once. Instead,
it only buffers one row of sliding windows at a time. If the
features are binary, then a buffer of size KHF sufﬁces in the
SWU. In the case of XNOR-net, assuming that the ﬁxedpoint values are represented with T bits, the binarization
operation requires two additional buffers of size KHFT (for
ﬁxed-point features) and KHT (for scaling factors). The ratio
of the memory footprint with and without online scaling
factor computation is as follows:
MemXNOR−net
= KHF+KHFT+KHT
Table IV: Memory utilization of the baseline binary CNN
accelerator and estimated overhead of online scaling factor
computation suggested by XNOR-net . The bitwidth of
ﬁxed-point features is T = 24 in our designs.
FF Utilization without XNOR-net(%)
FF Utilization with XNOR-net (%)
BRAM Utilization without XNOR-net (%)
BRAM Utilization with XNOR-net (%)
where T is the ﬁxed-point representation bitwidth and F is
the number of input channels. For a single SWU, if the ﬂip-
ﬂop and/or BRAM utilization is P%, the overhead would be
F )P. Table IV presents the estimated memory overhead
for two of our architectures that include convolution layers.
The overall overhead is obtained by summing the overheads
corresponding to all SWUs: ∑L
i=2 (T + T
Fi )Pi, where L is the
total number of convolution layers. Note that the ﬁrst layer
is not considered in this summation because its input is not
binarized and does not need scaling factor computation.
V. RELATED WORK
Training CNNs with binary weights and/or activations has
been the subject of very recent works , , , .
The authors of Binaryconnect suggest a probabilistic
methodology that leverages the full-precision weights to
generate binary representatives during forward pass while
in the back-propagation the full-precision weights are updated. The authors of introduced binarization of both
weights and activation of CNNs. The authors also suggest
replacing the costly dot products by XnorPopcount operations. XNOR-net proposes to use scale factors during
training, which results in an improved accuracy. The authors
of XNOR-net do not provide a hardware accelerator for
their binarized CNN. Although XNOR-net achieves higher
accuracy compared to the available literature, it sacriﬁces
the simplicity of the hardware accelerator for binary CNNs
due to two reasons: (i) It utilizes multiple scaling factors for
each parameter set (e.g. one scaling factor for each column
of a weight matrix), which would increase the memory
footprint and logic utilization. (ii) The online computation
of the scaling factors for the activations requires a signiﬁcant
number of full-precision operations.
The aforementioned works propose optimization solutions
that enable the use of binarized values in CNNs which, in
turn, enable the design of simple and efﬁcient hardware
accelerators. The downside of these works is that, aside
from changing the architecture of the CNN , they do not
offer any other reconﬁgurability in their designs. Providing
an easily reconﬁgurable architecture is the key to adapting
the accelerator to the application requirements.
In a separate research track, the reconﬁgurability of
CNN accelerators has been investigated. Using adaptive
low bitwidth representations for compressing the parameters
and/or simplifying the pertinent arithmetic operations is
investigated in , , . The proposed solutions,
however, do not enjoy the same simpliﬁed XnorPopcount
operations as in binarized CNNs. Considering the aforementioned works, the reconﬁgurability and scalability of binary
CNN accelerators require further investigation. To the best
of our knowledge, ReBNet is the ﬁrst scalable binary CNN
solution that embeds reconﬁgurability and, at the same time,
enjoys the beneﬁts of binarized CNNs.
VI. CONCLUSION
This paper introduces ReBNet, a novel reconﬁgurable
binarization scheme which aims to improve the convergence
rate and the ﬁnal accuracy of binary CNNs. Many existing
works have tried to compensate for the accuracy loss of
binary CNNs but did not consider hardware implications of
their proposals. As a result, they suffer from a degraded performance and resource efﬁciency. We argue that a practical
and scalable effort towards enhancing the accuracy of binary
CNNs should not add signiﬁcant ﬁxed-point operations
and/or memory overhead to the computation ﬂow of binary
CNNs. In this paper, we evaluated the hardware cost of
two of the state-of-the-art methods (XNOR-net and widenetworks) for enhancing the accuracy of binary CNNs and
showed that their implementation overhead is considerable.
Unlike prior methods, ReBNet does not sacriﬁce the simplicity of the hardware architecture to achieve a higher accuracy.
Our work is accompanied by an API that facilitates training
and design of residual binary networks. The API is opensource to foster the research in reconﬁgurable machinelearning on FPGA platforms.