An Analytic Solution to Discrete Bayesian Reinforcement Learning
Pascal Poupart
 
Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada
Nikos Vlassis
 
Informatics Institute, University of Amsterdam, Amsterdam, The Netherlands
Jesse Hoey
 
Dept. of Computer Science, University of Toronto, Toronto, Ontario, Canada
Kevin Regan
 
Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada
Reinforcement learning (RL) was originally
proposed as a framework to allow agents to
learn in an online fashion as they interact
with their environment.
Existing RL algorithms come short of achieving this goal because the amount of exploration required is
often too costly and/or too time consuming for online learning.
As a result, RL is
mostly used for oﬄine learning in simulated
environments. We propose a new algorithm,
called BEETLE, for eﬀective online learning
that is computationally eﬃcient while minimizing the amount of exploration. We take
a Bayesian model-based approach, framing
RL as a partially observable Markov decision
process. Our two main contributions are the
analytical derivation that the optimal value
function is the upper envelope of a set of multivariate polynomials, and an eﬃcient pointbased value iteration algorithm that exploits
this simple parameterization.
1. Introduction
Over the years, reinforcement learning (RL) has emerged as a dominant framework
for simultaneous planning and learning under uncertainty. Many problems of sequential decision making
with unknown action eﬀects can be solved by rein-
Appearing in Proceedings of the 23 rd International Conference
on Machine Learning, Pittsburgh, PA, 2006. Copyright 2006 by
the author(s)/owner(s).
forcement learning , helicopter control ,
backgammon playing ).
Interestingly, even though RL can, in theory, enable
an agent to plan and learn online, in practice, RL is
mostly used in simulation to learn oﬄine. Model-free
algorithms, which directly learn an optimal policy (or
value function), tend to have slow convergence, requiring too many trials to be used for online learning. In
application domains where each state transition has a
cost or some state transitions may lead to severe losses
(e.g., helicopter crash, mobile robot collision), online
learning with model-free RL is not realistic. In contrast, model-based approaches can incorporate prior
knowledge to mitigate severe losses, speed up convergence and reduce the number of trials. Model-based
approaches, especially Bayesian ones, can also optimally tradeoﬀexploration and exploitation. However
model-based approaches tend to be much more complicated and computationally intensive, making them
impractical for online learning.
In this paper, we derive an analytic solution to
Bayesian model-based RL. While it is well known that
Bayesian RL can be cast as a partially observable
Markov decision process (POMDP) , the
lack of a convenient parameterization for the optimal
value function is one of the causes of the poor scalability of Bayesian RL algorithms. We show that for
discrete Bayesian RL, the optimal value function is parameterized by a set of multivariate polynomials. This
parameterization allows us to derive an eﬃcient of-
ﬂine approximate policy optimization technique. Even
though this optimization is done oﬄine, learning is really done online as originally intended in the RL frame-
An Analytic Solution to Discrete Bayesian Reinforcement Learning
work. Furthermore, online learning is not computationally intensive since it requires only belief monitoring. This removes the main concern that practitioners
traditionally have with model-based approaches.
The paper is organized as follows. Sect. 2 reviews the
POMDP formulation of Bayesian RL and how to do
belief monitoring. Sect. 3 demonstrates that the optimal value function is the upper envelope of a set of
multivariate polynomials. Sect. 4 presents an eﬃcient
algorithm called Beetle that exploits this parameterization. Sect. 5 demonstrates empirically the Beetle
algorithm on a toy problem and a realistic assistive
technology task. Finally, Sect. 6 concludes.
2. POMDP formulation of Bayesian RL
A Markov decision process (MDP) can be formally
deﬁned by a tuple ⟨S, A, T , R⟩, where S is the set
of states s, A is the set of actions a, T (s, a, s′) =
Pr(s′|s, a) encodes the probability that state s′ is
reached when action a is executed in state s, and
R(s, a, s′) encodes the reward earned when state s′ is
reached after executing action a in state s. A policy
π : S →A is a mapping from states to actions.
The problem of reinforcement learning (RL) consists of
ﬁnding an optimal policy for an MDP with a partially
or completely unknown transition function. In this paper, we analytically derive a simple parameterization
of the optimal value function for the Bayesian modelbased approach.
Bayesian learning proceeds as follows. Pick a prior distribution encoding the learner’s
initial belief about the possible values of each unknown
parameter. Then, whenever a sampled realization of
the unknown parameter is observed, update the belief
to reﬂect the observed data. In the context of reinforcement learning, each unknown transition probability T (s′, a, s) is an unknown parameter θa
s,s′. Since
these are probabilities, the parameters θs,s′
take values
in the -interval.
We can then formulate Bayesian model-based RL
as a partially observable Markov decision process
(POMDP) , which is formally described
by a tuple ⟨SP , AP , OP , TP , ZP , RP ⟩.
S × {θs,s′
} is the set of states composed of the cross
product of the MDP states s with the unknown parameters θs,s′
. Since the MDP states are discrete and
the unknown parameters are continuous, the POMDP
state space SP is hybrid. The action space AP = A
is the same as the underlying MDP. The observation space OP = S consists of the MDP state space
since it is fully observable.
The transition function
TP (s, θ, a, s′, θ′) = Pr(s′, θ′|s, θ, a) can be factored in
two conditional distributions for the MDP states (i.e.,
Pr(s′|s, θs,s′
, a) = θs,s′
) and the unknown parameters
(i.e., Pr(θ′|θ) = δθ(θ′) where δθ(θ′) is a Kronecker
delta with value 1 when θ′ = θ and value 0 otherwise).
This Kronecker delta essentially denotes the
assumption that unknown parameters are stationary
(i.e., θ does not change).
The observation function
ZP (s′, θ′, a, o) = Pr(o|s′, θ′, a) indicates the probability of making an observation o when state s′, θ′ is
reached after executing action a. Since the observations are the MDP states, then Pr(o|s′, θ′, a) = δs′(o).
The reward function RP (s, θ, a, s′, θ′) = R(s, a, s′) is
the same as the underlying MDP reward function since
it doesn’t depend on the unknown parameters θ, θ′.
Based on this POMDP formulation, we can learn the
transition model θ by belief monitoring. At each time
step, the belief (or probability density) b(θ) = Pr(θ)
over all unknown parameters θs,s′
is updated based on
the observed transition s, a, s′ using Bayes’ theorem:
kb(θ) Pr(s′|θ, s, a)
kb(θ)θs,s′
In practice, belief monitoring can be performed easily
when the prior and the posterior belong to the same
family of distributions.
If the prior b is a product
of Dirichlets then the posterior bs,s′
is also a product of Dirichlets since Dirichlets are conjugate priors
of multinomials .
A Dirichlet distribution D(p; n) = kΠipni−1
over a multinomial p
is parameterized by positive numbers ni, such that
ni −1 can be interpreted as the number of times that
the pi-probability event has been observed. Since the
unknown transition model θ is made up of one unknown distribution θs
a per s, a pair, let the prior be
b(θ) = Πs,aD(θs
a) such that ns
a is a vector of hyperparameters ns,s′
. The posterior obtained after transition ˆs, ˆa, ˆs′ is:
a + δˆs,ˆa,ˆs′(s, a, s′))
Here δˆs,ˆa,ˆs′(s, a, s′) is a Kronecker delta that returns
1 when s = ˆs, a = ˆa, s′ = ˆs′, and 0 otherwise. In
practice, belief monitoring is as simple as incrementing the hyperparameter corresponding to the observed
transition.
3. Policy Optimization
We now explain how optimal policies and value functions can be derived. Sect. 3.1 reviews Bellman’s equation for Bayesian RL. Sect. 3.2 explains how optimal POMDP solutions naturally optimize the exploration/exploitation tradeoﬀ. Sect. 3.3 shows that the
optimal value function in Bayesian RL is parameterized by a set of multivariate polynomials. This is a
An Analytic Solution to Discrete Bayesian Reinforcement Learning
key result that will be the basis of the Beetle algorithm proposed in Sect. 4.
3.1. Bellman’s Equation
In POMDPs, policies π are mappings from belief states
to actions (i.e., π(b) = a).
The value V π of a policy π is measured by the expected discounted sum
of the rewards earned while executing it: V π(b) =
t=0 γtR(bt, π(bt), bt+1).
An optimal policy π∗has
the highest value in all belief states (i.e., V π∗(b) ≥
V π(b) ∀π, b) and its value function V ∗(b) satisﬁes Bellman’s equation:
V ∗(b) = max
Pr(o|b, a)[R(b, a, bo
a) + γV ∗ showed that the optimal value function of POMDPs with discrete states is
piecewise linear and convex. More precisely, it corresponds to the upper envelope of a (possibly inﬁnite)
set of linear segments α(b) called α-vectors.
Recall that for Bayesian RL, states are partly discrete
and partly continuous, however following Duﬀ ,
Bellman’s equation can be re-written
s (b) = max
Pr(o|s, b, a)[R(s, b, a, s′, bo
Using the fact that rewards do not depend on b nor
a and that observations correspond to the physical
states s′ in Bayesian RL, Bellman’s equation can be
simpliﬁed to
s (b) = max
Pr(s′|s, b, a)[R(s, a, s′) + γV ∗
Here b is the current belief in θ and bs,s′
is the revised
belief state according to Eq. 4.
3.2. Exploration/Exploitation Tradeoﬀ
The POMDP formulation of Bayesian RL allows one to
naturally optimize the exploration/exploitation tradeoﬀ. Bellman’s equation helps to understand why this
is the case. Pure exploitation selects the action that
maximizes total rewards based on b only, disregarding
the fact that valuable information may be gained by
observing the outcome of the action chosen. More precisely, policy optimization by pure exploitation would
select actions according to the following equation,
which diﬀers from Eq. 7 only by the use of b instead
in the right hand side:
s (b) = max
Pr(s′|s, b, a)[R(s, a, s′) + γV ∗
At run time, the agent would continually perform belief monitoring to update the belief state, but the action chosen at each step would simply take into account the current belief state since the outcome of
future actions hasn’t been observed yet. While this
may seem reasonable, it is suboptimal. Even though
the outcome of future actions cannot be observed yet,
we can hypothesize future action outcomes and take
them into account by conditional planning. This is precisely what Bellman’s equation (Eq. 7) achieves since
all possible updated belief states bs,s′
are considered
with probabilities corresponding to the likelihood of
reaching s′. Hence, Bellman’s equation optimizes the
sum of the rewards that can be derived based on the
information available in b (e.g., exploitation) as well
as the information gained in the future by observing
the outcome of the actions selected (e.g., exploration).
Alternatively, we can also argue that an optimal policy
of the POMDP formulation of Bayesian RL optimizes
the exploration/exploitation tradeoﬀsimply based on
the fact that such a policy maximizes the expected
total return.
3.3. Value Function Parameterization
Recall that the optimal value function of POMDPs
with discrete states is piecewise linear and convex . More precisely, it
corresponds to the upper envelope of a (possibly in-
ﬁnite) set Γ of linear segments α(b) called α-vectors
(i.e., V ∗(b) = maxα∈Γ α(b)). In Bayesian RL, despite
the hybrid nature of the state space, the piecewise linear and convex property still holds as demonstrated by
Duﬀ and Porta et al. . The optimal value
function corresponds to the upper envelope of a set Γ
of linear segments called α-functions due to the continuous nature of θ (i.e., V ∗
s (b) = maxα∈Γ αs(b)). Here
α can be deﬁned as a linear function of b subscripted
by s (i.e., αs(b)) or as a function of θ subscripted by s
(i.e., αs(θ)) such that αs(b) =
θ b(θ)αs(θ)dθ. Hence,
value functions in Bayesian RL can also be represented
by a set of α-functions, however it is unknown how to
parameterize α-functions in such a way that this parameterization be closed under Bellman backups. Due
to the lack of a convenient parameterization, practitioners have had diﬃculty developing eﬃcient and accurate algorithms. To date, several approximate algorithms based on conﬁdence intervals , Normal-Gamma distributions , linear combinations of
hyperparameters and sampling have been
proposed, but they tend to be computationally intensive at run time, preventing online learning or to make
drastic approximations such as myopically optimizing
the policy.
In a recent paper, Porta et al.
 derived that
the parameterization of α-functions for continuous
POMDPs with Gaussian dynamics is a linear combina-
An Analytic Solution to Discrete Bayesian Reinforcement Learning
tion of Gaussian functions. Similarly, we analytically
derive that α-functions in Bayesian RL are multivariate polynomials (Theorem 1). Based on this parameterization, we propose an eﬃcient nonmyopic approximate algorithm called Beetle in Sect. 4.
Before establishing our main theorem, let us ﬁrst review the Bellman backup operator and the updating of the α-functions .
Suppose that
the optimal value function V k
s (b) for k steps-to-go
is composed of a set Γk of α-functions such that
s (b) = maxα∈Γk αs(b).
Using Bellman’s equation,
we can compute by dynamic programming the best
set Γk+1 representing the optimal value function V k+1
with k + 1 stages-to-go. First we rewrite Bellman’s
equation (Eq. 7) by substituting V k for the maximum
over the α-functions in Γk:
Pr(s′|s, b, a)[R(s, a, s′) + γ max
α∈Γk αs′(bs,s′
Then we decompose Bellman’s equation in 3 steps.
The ﬁrst step (Eq. 9) ﬁnds the maximal α-function
for each a and s′. The second step (Eq. 10) ﬁnds the
best action a. The third step (Eq. 11) performs the
actual Bellman backup using the maximal action and
α-functions.
b,a = argmax
α∈Γk αs′(bs,s′
b = argmax
Pr(s′|s, b, a)[R(s, a, s′) + γαs,s′
b,a (bs,s′
Pr(s′|s, b, as
b)[R(s, as
b, s′) + γαs,s′
We can further rewrite the third step (Eq. 11) by using
α-functions in terms of θ (instead of b) and expanding
the belief state bs,s′
Pr(s′|s, b, as
b)[R(s, as
b, s′) + γ
b (θ)αs,s′
b(θ) Pr(s′|s, θ, as
b)[R(s, as
b, s′) + γαs,s′
Pr(s′|s, θ, as
b)[R(s, as
b, s′) + γαs,s′
Since the expression in the outer square brackets is a
function of s and θ, let’s use it as the deﬁnition of an
α-function in Γk+1:
Pr(s′|s, θ, as
b)[R(s, as
b, s′) + γαs,s′
Hence for every b we can deﬁne such an α-function and
together they form the set Γk+1. Since each αb,s was
deﬁned by using the optimal action and α-functions
in Γk, then each αb,s is necessarily optimal at b and
we can inroduce a max over all α-functions without
changing anything:
b(θ)αb,s(θ)dθ
α∈Γk+1 αs(b)
We are now ready to prove our main theorem:
Theorem 1 α-functions in Bayesian RL are multivariate polynomials.
Proof: We give a proof by induction. Initially, Γ0 consists of a single α-function that assigns 0 to all belief
states. This α-function is a trivial multivariate polynomial. Assuming α-functions in Γk are multivariate
polynomials, we show that αb,s in Eq. 15 is also a multivariate polynomial.
In Eq. 15, we can substitute Pr(s′|s, θ, a) by θs,s′
where for simplicity we use a to denote the optimal action as
b,a (θ) = !
i ci,s′µi,s′(θ) where
µi,s′(θ) = Πˆs,ˆa,ˆs′(θˆs,ˆs′
ˆa,i is a monomial over the parameter space, with non-negative powers λ (one for
each parameter).
The indices s, a, s′ have a “hat”
to distinguish them from those used in the deﬁnition
of αb,s. The reward R(s, a, s′) can also be written as
a degenerate constant monomial cs′µs′(θ) such that
cs′ = R(s, a, s′) and µs′(θ) = Πˆs,ˆa,ˆs′(θˆs,ˆs′
)0. Eq. 15
then reads:
[cs′µs′(θ) + γ
ci,s′µi,s′(θ)]
We can absorb θs,s′
into the monomials by incrementing the appropriate powers. If we write
µ′(θ) = Πˆs,ˆa,ˆs′(θˆs,ˆs′
+δs,a,s′ (ˆs,ˆa,ˆs′))
then Equation 15 reads:
which is again a multivariate polynomial. ◀
4. The Beetle Algorithm
Since multivariate polynomials form a closed representation for α-functions under Bellman backups, we propose a simple and eﬃcient point-based value iteration
algorithm for Bayesian RL called Beetle (i.e., Bayesian
Exploration Exploitation Tradeoﬀin LEarning).
4.1. Point-based value iteration
The Beetle algorithm is an extension of the Perseus
algorithm for Bayesian RL.
First, a set of reachable s, b pairs is sampled by simulating several runs of a default or random policy. Then
(approximate) value iteration is done by performing
point-based backups at those sampled s, b pairs based
on Eq. 9, 10 and 15. For a given s, b pair, the best αfunction for each a, s′ is computed according to Eq. 9.
An Analytic Solution to Discrete Bayesian Reinforcement Learning
Then, the optimal action is computed according to
Eq. 10. A new α-function is constructed according to
Eq. 15. This new α-function is represented very simply
by the non-negative powers λ of its monomial terms.
As is, Beetle suﬀers from an important source of intractability. At each backup, the number of terms of
the multivariate polynomial of the resulting α-function
grows signiﬁcantly. More precisely, in Eq. 21, the number of monomials is multiplied by O(|S|), which yields
a number of monomials that grows exponentially with
the planning horizon.
4.2. α-function Projection
In order to mitigate the exponential growth in the
number of monomials, after each Bellman backup we
project each new α-function onto a multivariate polynomial with a smaller number of monomials.
Intuitively, ﬁnding a good projection can be cast as an
optimization problem where we would like to simultaneously minimize the error at each θ. For instance,
when projecting an α-function onto a linear combination of monomial basis functions (i.e., !
i ciφi(θ)),
minimizing an Ln norm yields:
ciφi(θ)|ndθ
If we use a Euclidean norm, the optimal coeﬃcients
ci can be found analytically by solving a system of
linear equations Ax = d where Ai,j =
θ φi(θ)φj(θ)dθ,
θ φi(θ)α(θ)dθ and xj = cj.
Alternatively, since α-functions can be deﬁned with respect to θ or b, we can also devise a projection scheme
that minimizes error at some belief points instead of
When using an Ln norm, this can be done
by minimizing
b |α(b) −!
i ciφi(b)|ndb. Since the integral over b is generally diﬃcult to compute and in
many domains only a small region of belief space is
visited, minimizing the error only at a sample B of
reachable belief points is more practical:
If we use a Euclidean norm, the optimal coeﬃcients ci
can be found analytically by solving a system of linear
equations Ax = d where Ai,j = !
b∈B φi(b)φj(b), di =
b∈B φi(b)α(b) and xj = cj.
In practice, the optimization of Eq. 22 with a Euclidean norm is faster computationally and approximates uniformly at all θ’s. In contrast, if a good set
B of reachable belief states is used, then the optimization of Eq. 23 may yield a better approximation since
it focuses on the reachable belief states.
the optimization of Eq. 23 indirectly minimizes error
at all θ’s in a weighted fashion since belief points are
densities over θ, whereas the optimization of Eq. 22
minimizes error at all θ’s uniformly.
Ideally, we would like to pick basis functions as close as
possible to the monomials of α-functions. When comparing the equations for belief monitoring (Eq. 4) and
backing up α-functions (Eq. 11) it is interesting to note
that in both cases, powers are incremented with each
s, a, s′ transition. Hence belief states and α-functions
are both made up of similar monomials. Hence we propose to use the set of reachable belief states generated
at the beginning of the Beetle algorithm as the set of
basis functions.
Note that a ﬁxed basis set also allows us to precompute several operations to reduce computation
during point-based backups. More precisely, for each
point-based backup, the α-functions of the previous
step are all deﬁned with respect to the same components (but diﬀerent coeﬃcients).
Then the actual backup always transforms those components in
the same way by incrementing some hyperparameters. Hence we can pre-compute the projection of each
backed-up component.
So we can represent α-functions in a very compact way
just by a column vector ˜α corresponding to the coef-
ﬁcients of the ﬁxed basis functions. We can also precompute a projected transition function ˜T s,s′
form for each s, a, s′. Similarly we can pre-compute
the projection of the reward function and store basis coeﬃcients in column vectors ˜Rs,s′
for each s, a, s′.
Altogether, point-based backups can be performed by
simple matrix operations.
For instance, Eq. 15 becomes
+ γ ˜αs,s′
4.3. Parameter Tying
In classic reinforcement learning, the entire transition
dynamics are unknown. With the above Bayesian RL
formulation, this leads to an unknown distribution θs
for every state-action pair. When the number of states
and actions are large, the amount of computation and
the amount of interaction with the environment both
become prohibitive.
Fortunately, in practice, the transition dynamics are
rarely completely unknown.
Sometimes, just a few
transition probabilities are unknown.
In other situations, several unknown transition probabilities are
known to be the same (allowing parameter tying).
More generally, the transition dynamics may be jointly
expressed as a function of a small number of parameters (e.g. factored models). In addition to being able
to encode the uncertainty with a small number of unknowns, the amount of interaction for online learning
may be signiﬁcantly reduced by starting with informative priors, that is prior distributions skewed towards
a small range of values (i.e., low entropy).
An Analytic Solution to Discrete Bayesian Reinforcement Learning
Note that the Beetle algorithm can be used directly
when unknown parameters are tied. We simply have
one θi per diﬀerent unknown distribution.
the transition dynamics are factored using a dynamic
Bayesian network representation, our Beetle algorithm
can again be used directly. In this case, the unknowns
are the conditional probability distributions. Hence,
we have one θi per unknown conditional distribution.
Note that the probability of transitioning to s′ from s
when executing a is now the product of several conditional probabilities. Hence, for each observed transition s, a, s′, we increment several powers, one per conditional probability table, during belief monitoring as
well as point-based backups. In all cases, α-functions
remain multivariate polynomials.
4.4. Reward Function
So far we have assumed that the reward function is
known. We argue that this is not a restriction. The
Beetle algorithm can still learn reward functions with
a ﬁnite number of possible values. By considering a
factored model, we can treat the reward signal r as
a state variable.
The reward function R(s, a, s′) =
r can then be encoded as a conditional probability
distribution Pr(r|s, a, s′) which can be learned like all
the other conditional probability distributions. In the
case of a continuous reward signal, a suﬃciently ﬁne
discretization should provide enough accuracy.
4.5. Discussion
Traditionally, Bayesian RL was considered too complex and intractable to be of practical use. This paper
actually shows that the optimal value function has a
simple analytical form consisting of a set of multivariate polynomials. This analytical form allows us to derive an eﬃcient point-based value iteration algorithm.
As a result, we can optimize a policy oﬄine.
optimization can be eﬃcient as long as the number
of unknowns remains small. As argued in the previous section, the transition dynamics of many problems
can be encoded with few parameters by tying parameters together or using a factored model. Note that
for eﬀective online learning, what really matters is the
computation time while executing the policy, not the
time for oﬄine optimization. In many domains such as
robotics, elevator control and assistive technologies, it
is quite acceptable to have a computer in the lab spend
a few hours to optimize the policy by running Beetle before downloading it into an agent for execution.
However, at run time, actions must often be selected
in a fraction of a second for realtime execution. Beetle can easily achieve this since belief monitoring and
action selection are not computationally intensive.
While policy optimization is done oﬄine, it is important to realize that learning is really done online. The
policy computed consists of a mapping from statebelief pairs to actions. Even though this mapping is
ﬁxed throughout its execution, the belief states change
with each state transition. Recall that belief monitoring is essentially the process by which the unknown
transition dynamics are learned.
Hence, the policy
indirectly adapts with each belief update. The main
drawback of oﬄine policy optimization is that the precomputed policy must cater to as many scenarios as
In theory, it should prescribe an optimal
action for every belief state, but this is usually intractable. Hence, we have to settle for a suboptimal
policy that is hopefully good at the belief states that
are more likely to be visited, and hopefully generalizes well over the remaining belief states via the use
of α-functions.
To that eﬀect, point-based value iteration concentrates its eﬀort on ﬁnding good actions
at a sample of reachable belief states. Note that this
idea is also used in classic RL approaches with value
function approximation.
5. Experiments
We consider two problems. The ﬁrst is the toy “chain”
problem used in ,
while the second comes from a realistic assistive technology scenario . In both problems,
we experiment with varying degrees of parameter tying and we evaluate our methods by comparing them
to two heuristic methods:
EXPLOIT This is a strictly online method with no
oﬄine optimization, which purely exploits its current
belief at each step. We simply monitor the belief state
online and pick the best action by solving the MDP
for the expected model (i.e., average belief). While this
method is simple, it tends to be slow at run time since
an MDP must be solved between each action executed
and it suﬀers from a lack of exploration.
DISCRETE POMDP An alternative to Beetle is to
discretize the unknown distributions θ in N values and
to build a discrete POMDP, which can be solved using Perseus . The drawback of
this approach is the exponential explosion of the state
space, which consists of the cross product of the physical states with N discrete values for each unknown distribution (i.e, O(|S|N k) for k unknown distributions).
5.1. Problem Descriptions
Fig. 1(a) shows the “chain” problem from , in which the agent has two
An Analytic Solution to Discrete Bayesian Reinforcement Learning
b,2 b,2 b,2
Figure 1. (a) Chain problem showing the action, reward for each
transition (b) plansteps for the handwashing problem.
actions a,b which cause transitions between ﬁve states.
At each time step, the agent “slips” and performs the
opposite action with probability pslip = 0.2.
A more realistic problem domain is concerned with
assisting persons with cognitive disabilities complete
activities of daily living such as handwashing. We consider a simpliﬁed version of the system developed by
Boger et al. that gives audio prompts to help
users wash their hands with minimal assistance from a
caregiver. Since the level of independence varies widely
depending on each user, a major issue is to learn user
characteristics that inﬂuence their ability to carry out
the task. Note that those characteristics can only be
learned online as the system interacts with each user.
Framed as a Bayesian RL problem, we want the system
to learn user types as quickly as possible since it can
be quite frustrating for users to be given inappropriate
prompts. The states of the handwashing problem can
be grouped into nine plansteps shown in Fig. 1(b). The
system has two actions available: to do nothing, or to
issue an audio prompt corresponding to the current
planstep. A user of the system will exhibit certain behaviors: doing nothing, doing the best possible action
at a planstep, doing the second best action (if there are
two choices), or regressing (e.g. putting soap on their
hands after they are clean at planstep=g). Each user
has some distribution over these behaviors, which may
depend on both the current planstep, and the action
of the system. Typically, the system’s prompt will increase the probability that the user will perform the
best action for a planstep.
5.2. Results
Table 1 shows the results from the chain and handwashing problems. We experimented with 3 structural
priors referred as tied, semi and full. The full version corresponds to the extreme (and perhaps rare)
case where the dynamics are completely unknown. In
navigation scenarios such as the chain problem, the
eﬀects of each action are usually known up to some
noise term (i.e., the slip probability). Similarly, in assistive scenarios such as handwashing, system eﬀects
are usually known (or can be learned through simulation) except for user behaviors. Hence, more realistic
encodings of the chain and handwashing problems assume that the transition dynamics are known except
for the slip and behavior probabilities, which are state
and action independent in the tied version, while action dependent in the semi-tied version.
We report the expected total return (averaged over
500 runs) with standard deviation for the ﬁrst 1000
steps (without any discounting) for the exploit and
discrete POMDP heuristics, and Beetle. In all cases,
30 Bellman iterations were performed and 2000 belief points were sampled for Beetle and the discrete
POMDP heuristic.
The ﬁrst 200 (linearly independent) belief points were selected as basis functions for
The initial belief state is a uniform Dirichlet. For the discrete POMDP heuristic, the continuous
space of each unknown distribution θs
a was discretized
into 100 grid points selected at random uniformly.
The optimal return given the true model is reported
as a (utopic) upper bound. Beetle found near optimal
policies for the tied and semi-tied versions, while doing poorly on the full version. Since the dynamics are
completely unknown in the full version, Beetle has
trouble pre-computing a policy that is good for all possible models. Beetle found statistically equivalent or
better policies when compared to the discrete POMDP
heuristic, which found very good policies for the tied
and semi-tied versions, while running out of memory
for the full versions. This conﬁrms that discretizing is
impractical for problems with many free parameters.
The exploit heuristic ﬁnds provably optimal policies
for the tied version since there is no exploration required (i.e., the unknown distributions are tied across
all actions). However, exploration is required for the
semi-tied and full versions, which explains the suboptimal performance of the exploit heuristic. For further comparison, Dearden et al.
 and Strens
 report results for several other Bayesian RL
heuristics on the chain full problem, the best of
which, “Bayesian DP” (similar to the exploit heuristic
in that actions are selected greedily with respect to a
model sampled from the current belief instead of the
expected model) scored 3158 ± 31.
The running times for our Matlab implementation of
Beetle are reported in the last two columns. We also
wrote a C implementation (which is almost complete
at the time of publication) that achieves running times
one to two orders of magnitude faster. The second last
An Analytic Solution to Discrete Bayesian Reinforcement Learning
Beetle time (minutes)
precomputation
optimization
chain tied
chain semi
3257 ± 124
chain full
handw tied
handw semi
handw full
Table 1. Expected total reward for chain and handwashing problems. na-m indicates insuﬃcient memory.
Table 2. Expected total reward for varying priors
column indicates the time used to precompute projected transition and reward functions by minimizing
error with respect to all θ′s (Eq. 22). The last column reports the time to optimize by Beetle with the
projected transition and reward functions. Recall that
precomputation and optimization times are borne of-
ﬂine and therefore are in an acceptable range. Action
selection takes less than 0.3 seconds.
We also tested Beetle with informative priors in Table 2. Instead of starting Beetle with a uniform prior
(i.e, counts set to 1), we tried more informative priors
by varying a parameter k from 0 to 30. That is, the
Dirichlet counts are set to 1 plus k times the probabilities of the true model. As k increases, the conﬁdence
in the true model increases.
In scenarios where we
have some belief about the transition probabilities, but
we are not completely sure, we can reduce the model
uncertainty by using such an informative prior. On
the problems for which Beetle didn’t ﬁnd a near optimal policy with a uniform prior, Table 2 shows that
increasingly informative priors generally improve Beetle’s performance since it can focus on ﬁnding a good
policy for a smaller range of likely models.
6. Conclusion
In this paper, we have shown that optimal value
functions for Bayesian RL are parameterized by
sets of multivariate polynomials, and exploited this
parameterization to develop an eﬀective algorithm
called Beetle.
It naturally optimizes the exploration/exploitation tradeoﬀ. It allows practitioners to
easily encode prior knowledge, which permits Beetle
to focus only on the truly unknown parts of the dynamics, reducing the amount of exploration necessary.
Furthermore, online eﬃciency is achieved by precomputing oﬄine a policy and doing only action selection
and belief monitoring at run time. Overall, this work
represents an important step towards the development
of eﬀective online RL algorithms.
We plan to extend this work on Bayesian RL in several directions, including continuous state, action and
observation spaces, partially observable domains and
multi-agent systems. We also plan to explore how to
handle and possibly learn non-stationary dynamics.