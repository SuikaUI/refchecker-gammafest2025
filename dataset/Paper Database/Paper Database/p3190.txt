Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 106–111,
Beijing, China, July 26-31, 2015. c⃝2015 Association for Computational Linguistics
A Distributed Representation Based Query Expansion Approach for
Image Captioning
Semih Yagcioglu1
Erkut Erdem1
Aykut Erdem1
Ruket C¸ akıcı2
1 Hacettepe University Computer Vision Lab (HUCVL)
Dept. of Computer Engineering, Hacettepe University, Ankara, TURKEY
 , {erkut,aykut}@cs.hacettepe.edu.tr
2 Dept. of Computer Engineering, Middle East Technical University, Ankara, TURKEY
 
In this paper, we propose a novel query expansion approach for improving transferbased automatic image captioning.
core idea of our method is to translate the
given visual query into a distributional semantics based form, which is generated
by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three
image captioning benchmark datasets, we
show that our approach provides more accurate results compared to the state-of-theart data-driven methods in terms of both
automatic metrics and subjective evaluation.
Introduction
Automatic image captioning is a fast growing area
of research which lies at the intersection of computer vision and natural language processing and
refers to the problem of generating natural language descriptions from images. In the literature,
there are a variety of image captioning models that
can be categorized into three main groups as summarized below.
The ﬁrst line of approaches attempts to generate novel captions directly from images . Speciﬁcally, they borrow techniques
from computer vision such as object detectors and
scene/attribute classiﬁers, exploit their outputs to
extract the visual content of the input image and
then generate the caption through surface realization. More recently, a particular set of generative
approaches have emerged over the last few years,
which depends on deep neural networks .
In general, these studies combine convolutional neural
networks (CNNs) with recurrent neural networks
(RNNs) to generate a description for a given image.
The studies in the second group aim at learning
joint representations of images and captions . They employ certain machine learning
techniques to form a common embedding space
for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool
of captions to ﬁnd the most proper caption for a
given image.
The last group of works, on the other hand,
follows a data-driven approach and treats image
captioning as a caption transfer problem . For a
given image, these methods ﬁrst search for visually similar images and then use the captions of the
retrieved images to provide a description, which
makes them much easier to implement compared
to the other two classes of approaches.
The success of these data-driven approaches depends directly on the amount of data available and
the quality of the retrieval set. Clearly, the image features and the corresponding similarity measures used in retrieval play a signiﬁcant role here
but, as investigated in , what
makes this particularly difﬁcult is that while describing an image humans do not explicitly mention every detail. That is, some parts of an image
are more salient than the others. Hence, one also
needs to bridge the semantic gap between what is
there in the image and what people say when describing it.
As a step towards achieving this goal, in this paper, we introduce a novel automatic query expansion approach for image captioning to retrieve semantically more relevant captions. As illustrated
in Fig. 1, we swap modalities at our query expan-
Initial ranking
c1: A man climbs up a snowy
: A boy in orange jacket
appears unhappy.
: A person wearing a red
jacket climbs a snowy hill.
Query image Iq
Visually similar images
Query expansion using
distributed representations
Final ranking
transferred caption
c2: A boy in orange jacket
appears unhappy.
c1: A man climbs up a snowy
c5: A person wearing a red
jacket climbs a snowy hill.
Figure 1: A system overview of the proposed query expansion approach for image captioning.
sion step and synthesize a new query, based on
distributional representations of the captions of
the images visually similar to the input image.
Through comprehensive experiments over three
benchmark datasets, we show that our model improves upon existing methods and produces captions more appropriate to the query image.
Related Work
As mentioned earlier, a number of studies pose image captioning as a caption transfer problem by
relying on the assumption that visually similar images generally contain very similar captions. The
pioneering work in this category is the im2text
model by Ordonez et al. , which suggests
a two-step retrieval process to transfer a caption to
a given query image. The ﬁrst step, which provides a baseline for the follow-up caption transfer
approaches, is to ﬁnd visually similar images in
terms of some global image features. In the second
step, according to the retrieved captions, speciﬁc
detectors and classiﬁers are applied to images to
construct a semantic representation, which is then
used to re-rank the associated captions.
Kuznetsova et al. proposed performing
multiple retrievals for each detected visual element in the query image and then combining the
relevant parts of the retrieved captions to generate the output caption. Patterson et al. extended the baseline model by replacing global features with automatically extracted scene attributes,
and showed the importance of scene information
in caption transfer. Mason and Charniak 
formulated caption transfer as an extractive summarization problem and proposed to perform the
re-ranking step by means of a word frequencybased representations of captions. More recently,
Mitchell et al. proposed to select the caption that best describes the remaining descriptions of the retrieved similar images wrt an n-gram
overlap-based sentence similarity measure.
In this paper, we take a new perspective to
data-driven image captioning by proposing a novel
query expansion step based on compositional distributed semantics to improve the results.
approach uses the weighted average of the distributed representations of retrieved captions to expand the original query in order to obtain captions
that are semantically more related to the visual
content of the input image.
Our Approach
In this section, we describe the steps of the proposed method in more detail.
Retrieving Visually Similar Images
Representing Images.
Data-driven approaches
such as ours rely heavily on the quality of the initial retrieval, which makes having a good visual
feature of utmost importance. In our study, we
use the recently proposed Caffe deep learning features , trained on ImageNet, which
have been proven to be effective in many computer
vision problems. Speciﬁcally, we use the activations from the seventh hidden layer (fc7), resulting
in a 4096-dimensional feature vector.
Adaptive Neighborhood Selection.
our expanded query by using the distributed representations of the captions associated with the
retrieved images, and thus, having no outliers is
also an important factor for the effectiveness of
the approach. For this, instead of using a ﬁxed
neighborhood, we adopt an adaptive strategy to select the initial candidate set of image-caption pairs
{(Ii, ci)}.
For a query image Iq, we utilize a ratio test and
only consider the candidates that fall within a radius deﬁned by the distance score of the query im-
age to the nearest training image Iclosest, as
N(Iq) = {(Ii, ci) | dist(Iq, Ii) ≤(1 + ϵ)dist(Iq, Iclosest),
Iclosest = arg min dist(Iq, Ii), Ii ∈T }
where dist denotes the Euclidean distance between two feature vectors, N represents the candidate set based on the adaptive neighborhood, T is
the training set, and ϵ is a positive scalar value1.
Query Expansion Based on Distributed
Representations
Representing Words and Captions.
study, we build our query expansion model on
the distributional models of semantics where the
meanings of words are represented with vectors
that characterize the set of contexts they occur in a
corpus. Existing approaches to distributional semantics can be grouped into two, as count and
predict-based models . In our
experiments, we tested our approach using two recent models, namely word2vec and GloVe , and
found out that the predict-based model of Mikolov
et al. performs better in our case.
To move from word level to caption level,
we take the simple addition based compositional
model described in and
form the vector representation of a caption as the
sum of the vectors of its constituent words. Note
that here we only use the non-stop words in the
Query Expansion.
For a query image Iq, we
ﬁrst retrieve visually similar images from a large
dataset of captioned images. In our query expansion step, we swap modalities and construct a new
query based on the distributed representations of
captions. In particular, we expand the original visual query with a new textual query based on the
weighted average of the vectors of the retrieved
captions as follows:
sim(Iq, Ii) · c j
where N and M respectively denote the total number of image-caption pairs in the candidate set N
and the number of reference captions associated
with each training image, and sim(Iq, Ii) refers to
the visual similarity score of the image Ii to the
1The adaptive neighborhood parameter ϵ was emprically
set to 0.15.
query image Iq2 which is used to give more importance to the captions of images visually more
close to the query image.
Then, we re-rank the candidate captions by estimating the cosine distance between the distributed
representation of the captions and the expanded
query vector q, and ﬁnally transfer the closest caption as the description of the input image.
Experimental Setup and Evaluation
In the following, we give the details about our experimental setup.
Corpus. We estimated the distributed representation of words based on the captions of the MS
COCO dataset, containing 620K
As a preprocessing step, all captions
in the corpus were lowercased, and stripped from
punctuation.
In the training of word vectors, we used 500 dimensional vectors obtained with both GloVe and word2vec models. The minimum word count was set
to 5, and the window size was set to 10. Although
these two methods seem to produce comparable
results, we found out that word2vec gives better
results in our case, and thus we only report our results with word2vec model.
Datasets. In our experiments, we used the popular
Flickr8K , Flickr30K , MS COCO datasets,
containing 8K, 30K and 123K images, respectively. Each image in these datasets comes with
5 captions annotated by different people. For each
dataset, we utilized the corresponding validation
split to optimize the parameters of our method, and
used the test split for evaluation where we considered all the image-caption pairs in the training and
the validation splits as our knowledge base.
Although Flickr8K, and Flickr30K datasets
have been in use for a while, MS COCO dataset
is under active development and might be subject
to change. Here, we report our results with version
1.0 of MS COCO dataset where we used the train,
validation and test splits provided by .
We compared our proposed approach against
the adapted baseline model (VC) of im2text which corresponds to using
the caption of the nearest visually similar im-
2We deﬁne sim(Iq, Ii) = 1 −dist(Iq, Ii)/Z where Z is
a normalization constant.
a black and white dog is playing
or ﬁghting with a brown dog in
a man is sitting on a blue bench
with a blue blanket covering his
a man in a white shirt and sunglasses is holding hands with a
woman wearing a red shirt outside
one brown and black pigmented
bird sitting on a tree branch
a dog looks behind itself
a girl looks at a woman s face
a woman and her two dogs are
walking down the street
a tree with many leaves around
a brown and white dog jumping over a red yellow and white
a father feeding his child on the
a girl is skipping across the road
in front of a white truck
a black bear climbing a tree in
forest area
a brown and white dog jumps
over a dog hurdle
a man in a black shirt and his
little girl wearing orange are
sharing a treat
a girl jumps rope in a parking
a bird standing on a tree branch
in a wooded area
a brown and white sheltie leaping over a rail
a man and a girl sit on the
ground and eat
a girl is in a parking lot jumping
a painted sign of a blue bird in
a tree in the woods
Figure 2: Some example input images and the generated descriptions.
Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
age, and the word frequency-based approaches of
Mason and Charniak (MC-SB and MC-
KL). We also provide the human agreement results
(HUMAN) by comparing one groundtruth caption
against the rest.
For a fair comparison with the MC-SB and MC-
KL models and the
baseline approach VC, we used the same image
similarity metric and training splits in retrieving
visually similar images for all models. For human agreement, we had ﬁve groundtruth image
captions, thus we determine the human agreement
score by following a leave-one-out strategy. For
display purposes, we selected one description randomly from the available ﬁve groundtruth captions
in the ﬁgures.
Automatic Evaluation.
We evaluated our approach with a range of existing metrics, which
are thoroughly discussed in . We used smoothed
BLEU for benchmarking
We also provided the scores of ME-
TEOR and the recently proposed CIDEr metric , which has been shown to correlate well
with the human judgments in and , respectively3.
Human Evaluation. We designed a subjective experiment to measure how relevant the transferred
caption is to a given image using a setup similar
to those of 4. In this experiment, we provided
human annotators an image and a candidate description where it is rated according to a scale of
1 to 5 (5: perfect, 4: almost perfect, 3: 70-80%
good, 2: 50-70% good, 1: totally bad) for its relevancy. We experimented on a randomly selected
set of 100 images from our test set and evaluated
our captions as well as those of the competing approaches.
3We collected METEOR and BLEU scores via MultEval and for CIDEr scores we used the
authors’ publicly available code.
4We used CrowdFlower and at least 5 different human annotators for each question.
Table 2: Human judgment scores on a scale of 1 to 5.
Results and Discussion
In Figure 2, we present sample results obtained
with our framework, MC-SB, MC-KL and VC
models along with the groundtruth caption. We
provide the quantitative results based on automatic
evaluation measures and human judgment scores
in Table 1 and Table 2, respectively.
Our ﬁndings indicate that our query expansion
approach which is based on distributed representations of captions gives results better than those of
VC, MC-SB and MC-KL models. Although our
method makes a modest improvement compared
to the human scores we believe that there is still a
big gap between the human baseline, which align
well with the recently held MS COCO 2015 Captioning Challenge results.
One limitation in this work is the Out-of-
Vocabulary (OOV) words, which is around 1% on
average for the benchmark datasets. We omit them
in our calculations, since there is no practical way
to map word vectors for the OOV words, as they
are not included in the training of the word embeddings. Another limitation is that this approach
currently does not incorporate the syntactic structures in captions, therefore the position of a word
in a caption does not make any difference in the
representation, i.e. “a man with a hat is holding a
dog” and “a man is holding a dog with a hat” are
represented with the same vector. This limitation
is illustrated in Fig. 3, where the closest caption
from retrieval set contains similar scene elements
but does not depict the scene well.
Conclusion
In this paper, we present a novel query expansion
approach for image captioning, in which we utilize a distributional model of meaning for sentences.
Extensive experimental results on three
well-established benchmark datasets have demonstrated that our approach outperforms the state-ofthe art data-driven approaches. Our future plans
focus on incorporating other cues in images, and
a man wearing a santa hat holding a dog posing for a picture
a boy is holding a dog that is
wearing a hat
Figure 3: Limitation. A query image on the left and its
actual caption, a proposed caption on the right along with its
actual image.
considering the syntactic structures in image descriptions.
Acknowledgments
This study was supported in part by The Scientiﬁc
and Technological Research Council of Turkey
(TUBITAK), with award no 113E116.