Front. Comput. Sci., 2021, 0(0): 1–19
 
RESEARCH ARTICLE
Graph Convolution Machine for Context-aware
Recommender System
Jiancan Wu1, Xiangnan He(B)1, Xiang Wang2, Qifan Wang3, Weijian Chen1,
Jianxun Lian4, Xing Xie4
School of Information Science and Technology, University of Science and Technology of China, Hefei
230026, China
5 Prince George’s Park, National University of Singapore, Singapore 118404, Singapore
Google Research, Mountain View, CA 94043, USA
Microsoft Research Asia, Beijing 100190, China
© Higher Education Press 2021
The latest advance in recommendation
shows that better user and item representations can
be learned via performing graph convolutions on
the user-item interaction graph. However, such ﬁnding is mostly restricted to the collaborative ﬁltering
(CF) scenario, where the interaction contexts are
not available. In this work, we extend the advantages of graph convolutions to context-aware recommender system (CARS, which represents a generic
type of models that can handle various side information). We propose Graph Convolution Machine
(GCM), an end-to-end framework that consists of
three components: an encoder, graph convolution
(GC) layers, and a decoder. The encoder projects
users, items, and contexts into embedding vectors,
which are passed to the GC layers that reﬁne user
and item embeddings with context-aware graph convolutions on the user-item graph. The decoder digests the reﬁned embeddings to output the prediction score by considering the interactions among
user, item, and context embeddings. We conduct
experiments on three real-world datasets from Yelp
and Amazon, validating the effectiveness of GCM
and the beneﬁts of performing graph convolutions
for CARS. Our implementations are available at
 
Context-Aware Recommender Systems,
Graph Convolution
Introduction
Recommendation has become a pervasive service
in today’s Web, serving as an important tool to al-
Received month dd, yyyy; accepted month dd, yyyy
E-mail: 
 
Front. Comput. Sci., 2021, 0(0): 1–19
leviate information overload and improve user experience. The key data source for building a recommendation service is user-item interactions, e.g.,
clicks and purchases, which spawn wide research
efforts on collaborative ﬁltering (CF) that
leverage the interaction data only to predict user
preference. Recently, inspired by the success of
graph neural networks (GNNs) , researchers
have attempted to employ GNNs on recommendation in which CF signals are exhibited as high-order
connectivity . While CF provides a universal
solution for recommendation, it falls short in utilizing the side information of interaction contexts.
In many scenarios, the current contexts could have
a strong impact on user choice. For example, in
restaurant recommendation, the current time and
location can effectively ﬁlter out unsuitable candidates; in E-commerce, the click behaviors in recent
sessions provide strong signal on user next purchase.
As such, it is important to develop context-aware
recommender system (CARS) that can effectively
integrate contexts (and possibly other side information like user proﬁles and item attributes) into user
preference prediction .
Inspired by the matrix completion view of CF,
early research naturally extended the problem of
CARS to tensor completion , which however
suffers from high complexity. Later on, Rendle proposed factorization machine (FM) , which to
the ﬁrst time addressed CARS from the view of
standard supervised learning. Speciﬁcally, it converts all information related to an interaction to a
feature vector via multi-hot encoding, modeling the
second-order feature interactions to predict the interaction label. Due to its generality and effectiveness,
FM soon becomes a prevalent solution for CARS
and is followed by many work. For example, in the
era of deep learning, Wide&Deep and Deep
Crossing replaced the second-order interaction
modeling with a neural network for implicit interaction modeling; recently, Neural FM , Attentional FM , xDeepFM , and Convolutional
FM extended FM with various kinds of neural
networks to enhance its expressiveness.
Summarizing existing CARS models, we can
ﬁnd a common drawback: they follow the standard supervised learning scheme that ignores the
relationship among data instances. This may limit
the model’s effectiveness in capturing the CF effect, since it needs to consider multiple interactions
simultaneously to recognize the CF patterns. An
evidence is from the neural graph collaborative ﬁltering (NGCF) work , which demonstrates that
connecting the interactions in the predictive model
signiﬁcantly improves the embedding quality for
CF. Since in CARS user-item interactions still play
an important role by reﬂecting user preference, it
is reasonable to believe that properly modeling the
relationship among interactions can improve the
model quality. Moreover, the recent neural networkbased methods like xDeepFM and Convolutional FM suffer from low efﬁciency in online
serving, since each candidate item needs be scored
separately with the deep model architecture that
models complex feature interactions, which could
be very time-consuming.
In this work, we aim to propose new CARS model
by addressing the above-mentioned limitations. Firstly,
we cast the data in CARS as an attributed user-item
graph, where the side information of users and items
are represented as node features, and the contexts
are represented as edge features (Figure 1). Secondly, we propose an end-to-end model that consists
of three components: an encoder, graph convolution
(GC) layers, and a decoder (Figure 2). The encoder
projects users, items, and contexts into embedding
Jiancan Wu et al. Graph Convolution Machine for Context-aware Recommender System
vectors; the GC layers then exploit the interactions
to reﬁne the embeddings via performing graph convolutions; lastly, the decoder models the interactions
among embeddings via FM to output the prediction
score. After the model is trained, the reﬁned embeddings by GC layers can be pre-computed before
serving. As such, the time complexity of online
serving is the same as FM, being much more efﬁcient than the recent neural network methods. We
summarize the contributions of this work as follows:
• We highlight the limitation of the mainstream
supervised learning schemes and the necessity of exploiting the relationship among data
instances in the predictive model of CARS.
• We propose a new model named Graph Convolution Machine (GCM), unifying the strengths
of graph convolution network and factorization machine for CARS.
• We conduct extensive experiments on three
real-world datasets which demonstrate the effectiveness and efﬁciency of GCM.
Related Work
Context-aware Recommendation
Extensive studies on context-aware recommender
system (CARS) have been conducted
and achieved great success. Learning informative
representations, based on user-item interactions (e.g.,
clicks, purchases) and contextual features (e.g., location, time, last purchase), has been a central theme
of research on CARS. Towards this end, modeling
interactions among different features is showing
promise. Early, factorization machine (FM) 
embeds each feature into a vector representation,
and utilizes inner product to capture their pairwise
relationships (e.g., the second-order feature interactions). Due to its generality and effectiveness,
FM becomes a prevalent solution for CARS. Many
works resort to this paradigm, such as FFM .
Recent works leverage deep neural networks to model higher-order feature interactions, so as to generate better representations and
enhance recommendation performance. For example, NFM proposes a bilinear interaction operation which uses a sum pooling over the pair-wise
dot-product of feature vectors; AFM learns the
importance of each feature interaction via the attention mechanism; xDeepFM extends the Cross
Network to the Compressed Interaction Network (CIN) which models high-order interactions
explicitly at vector-wise level; while Convolutional
FM models second-order interaction with outer
product, forming an interaction cube, then applying
3D convolution to learn high-order interactions. It’s
worth mentioning that another research line close
to CARS is the CTR (Click Through Rate) prediction , which also focuses on modeling the
complex feature interactions. The key difference
lies in the evaluation protocol: most CARS models
adopt top-k recommendation protocols, while CTR
prediction models measure log loss or AUC metrics
on positive/negative samples.
Despite effectiveness, we argue that present works
treat user interactions as isolated data instances,
while forgoing their relationships (e.g., user behaviors happened at the same time and location
are highly likely to reﬂect user preferences). This
would easily lead to suboptimal representations and
limit the performance. We hence aim to explore
relationships among user behaviors in this work.
Graph Neural Networks for Recommendation
Another relevant research line is to leverage graph
neural networks (GNNs) for recommendation. In
particular, GNN models exploit graph struc-
Front. Comput. Sci., 2021, 0(0): 1–19
ture to guide the representation learning. The basic idea is the embedding propagation mechanism,
which aggregates the embeddings of neighbors to
update the target node’s embedding. By recursively
performing such propagations, the information from
multi-hop neighbors is encoded into the representation of target node. GNN models has been widely
used in many fundamental tasks due to their strong
representation ability, spanning from node classiﬁcation , link prediction , to graph classiﬁcation , and achieved remarkable improvements.
Inspired by their success, researchers have attempts to employ GNNs on recommendation. Recent works on collaborative ﬁltering (CF), such as
NGCF , GC-MC , SpectralCF and Pin-
Sage , reorganize historical user behaviors in the
form of a user-item bipartite graph, exhibit CF signals as high-order connectivity, and encode such signals into representations. For CTR prediction task,
Fi-GNN takes multi-ﬁeld features into consideration by constructing feature graph for each instance
and converting the task of modeling feature interactions among ﬁelds into modeling node interactions
on the feature graph; GIN models implicit
user intention by the multi-layered intention diffusion and aggregation on the co-occurrence click
relationship graph; builds the multi-relational
item graph and applies GNN to capture complex
transition relations between items in user bahavior sequences. Moreover, GNN models have also
been employed on other recommendation tasks, including social recommendation , sequential
recommendation , and knowledge-aware recommendation . As such, aggregating useful
information from multi-hop neighbors is able to
achieve better expressiveness, than single ID embeddings. Hence, it is reasonable to believe that
graph learning is a promising solution to properly
model the relationships among interactions.
Problem Deﬁnition
We divide the data used for CARS into four types:
users, items, contexts, and interactions. Following , we deﬁne context as the information that
is associated with an interaction, e.g., the current location, time, previous click, etc. Figure 1 illustrates
the data in CARS, where the main data is the useritem-context interaction tensor. In the sparse tensor,
each nonzero entry (u, i, c) denotes that the user u
has interacted with the item i under the context c;
we give such entries a label of 1, i.e., yuic = 1. Each
u, i, c is respectively associated with a multi-hot feature vector u, i, and c, which contain the features
that describe the user, item, and context. For example, u includes static user proﬁles like gender and
interested tags, i includes static item attributes like
category and price, and c includes dynamic contexts
like the current location of the user and the time.
Given such data, we convert it to the form of attributed user-item bipartite graph that has the same
representation power. Speciﬁcally, each vertex represents a user or an item, and each edge represents
the interaction between the connected user and item.
Each vertex or edge is associated with a feature vector u, i, or c. Note that there may exist multiple
edges between a user-item pair, since a user may
interact with the same item multiple times under different contexts. We denote all edges in the graph as
the set Y = {(u, i, c)|yuic = 1}, the neighbors of the
user u as the set Nu = {(i, c)|yuic = 1}, and neighbors
of the item i as the set Ni = {(u, c)|yuic = 1}.
We formulate the problem of CARS as:
Input: User-item-context interactions {(u, i, c)|yuic =
1}, feature vectors of users {u}, items {i}, and
contexts {c}.
Jiancan Wu et al. Graph Convolution Machine for Context-aware Recommender System
Interaction
The data used for building a CARS. The mixture data
of interaction tensor and user/item/context feature matrices
are converted to an attributed user-item bipartite graph without
loss of ﬁdelity.
Output: Prediction function f : u, i, c →R, which
takes the feature vector of a user, an item, and
a context as the input, and outputs a real value
that estimates how likely the user will interact
with the item under the context.
Graph Convolution Machine (GCM)
We present our method in this section. We ﬁrst describe the predictive model, followed by the model
complexity analyses and optimization details.
Predictive Model
Figure 2 illustrates the model framework, which
consists of three components: an encoder, graph
convolution layers, and a decoder. We next describe
each component one by one.
The input to the encoder has three ﬁelds: user-ﬁeld
features u, item-ﬁeld features i, and the context-
ﬁeld features c. We include the ID feature into
the user-ﬁeld and item-ﬁeld features, since it helps
to differentiate users (items) when their proﬁles
(attributes) are the same1). For each nonzero feature,
1)Note that there is no need to include ID into the context-
ﬁeld features, since a context c and its features c are one-to-one
we associate it with an embedding vector, resulting
in a set of embeddings to describe the input user,
item, and context, respectively. We then pool the set
of user (and item) ﬁeld into a vector, so as to feed
the vector into the the following GC layers to reﬁne
the user (and item) representations. Speciﬁcally, we
adopt average pooling, that is,
where |u| denotes the number of nonzero features
in u, and P ∈RU×D is the embedding matrix for
user features, where U denotes the number of total
user features and D denotes the embedding size.
u denotes the initial representation vector for u.
Similarly, we get the initial representation vector
for item i as q(0)
Note that other pooling mechanisms can be applied here, such as the attention-based pooling which learns varying weights for feature
embeddings. However, we tried that and ﬁnd it
does not improve the performance. Thus we keep
the simplest average pooling and avoid introducing additional parameters. Since we do not update
the context representation in the following GC layers, we do not perform pooling on the context ﬁeld.
We denote the set of context-ﬁeld embeddings as
Vc = {vs|s ∈c}, where s ∈c denotes the nonzero
feature in c and vs denotes the embedding vector
for context feature s. The encoder outputs p(0)
and Vc, which are fed into the next component of
GC layers.
Graph Convolution Layers
This is the core component of GCM, designed to address the limitation of existing supervised learningbased CARS models. It reﬁnes p(0)
exploiting holistic user-item interaction data, which
Front. Comput. Sci., 2021, 0(0): 1–19
Graph Convolution
The Graph Convolution Machine model.
can augment the user and item representations with
explicit collaborative ﬁltering signal . The GC on
user-item graph is typically formulated as a message
propagation framework:
i ); q(l+1)
where p(l)
u and q(l)
i denote the reﬁned user representation and item representation of the l-th GC layer,
respectively, and g(·) is a self-deﬁned function. Recursively conducting such message propagation relates the representation of a user with her high-order
neighbors, e.g., ﬁrst-order for interacted items and
second-order for co-interacted users, which is bene-
ﬁcial for collaborative ﬁltering; and the same logic
applies to item representation.
However, the standard GC does not consider the
features on edges. In our constructed user-item
graph, the edges between a user and an item carry
the context features, which are important to understand the context-dependent interaction patterns.
For example, a user may prefer bars on Friday, and
a restaurant is more popular on lunch time. As such,
better user and item representations can be obtained
if the context features can be properly integrated
into the GC.
To this end, we propose a new GC operation that
incorporates the edge features of contexts:
Next we explain the rationality of the GC of the
user side, since the item side can be interpreted in
the same way. Here |Nu| denotes the number of
edges connected with the user u, and the coefﬁcient
√|Nu| is a normalization term to avoid the scale of
embedding values increasing with the GC. We incorporate the context features by averaging their
embeddings and adding to the connected user embedding. Through this way, we build the connection
between a user with both her interacted item and
the interacted context. It is expected to capture the
Jiancan Wu et al. Graph Convolution Machine for Context-aware Recommender System
effect that if a user likes to choose an item under a
certain context, then the similarity among their representations is similar. Note that we have tried more
complicated mechanisms like incorporating the pairwise interactions among Vc and q(l)
i , and using a
MLP to capture high-order interactions. However
these ways do not lead to performance improvements. Thus we use this simple average operation,
which is easy to interpret and train (no additional
parameters are introduced).
By stacking multiple such GC layers, a user (or
an item) representation can be reﬁned by its multihop neighbors. Since the representation of different
layers carry different semantics, we next combine
the representations of all layers to form a more comprehensive representation:
where αl denotes the weight of the l-th layer representation, which can be treated as hyper-parameter
and tuned via a grid search with the constraint that
αl ≥0 and PL
l=0 αl = 1. However, the workload of
tuning them increases exponentially, as the GCN
goes deep. In our experiments, we ﬁnd that setting
αl to 1/(L + 1) leads to satisfactory performance
in general. Therefore, we ﬁx αl to 1/(L + 1) for
simplicity. A possible extension is to learn αl, e.g.,
designing attention mechanism or optimizing them
on the validation data. We leave this extension as
future work, since it is not the focus of this work.
In what follows, we provide the matrix form of
GC layers for implementation. Let the user-item
interaction matrix be Rui ∈RN×M, where N and M
denotes the number of users and items. Each entry
rui ∈Rui is the number of times user u interacts with
item i. Similarly, we utilize Ruc ∈RN×K and Ric ∈
RM×K to denote user-context interaction matrix and
item-context interaction matrix respectively, where
K is the number of contexts. Then we deﬁne the
adjacency matrix of user-item-context graph as


where 0 is all-zero matrix, I is identity matrix. Let
D be diagonal degree matrix of A, that is, the t-th
diagonal element Dtt = P
j At j. The normalized
adjacency matrix can be expressed as
Then, we get the matrix form of the layer-wise propagation rule which is equivalent to Equation (3):
E(l) = ˆAE(l−1),
where E(l) ∈R(N+M+K)×D is the concatenate of user,
item and context embedding matrix. E(0) is set as
the concatenate matrix of encoded embedding tables
from Encoder, which can be expressed as
E(0) = [p(0)
u1 , · · · p(0)
user embeddings
i1 , · · · q(0)
item embeddings
rc1, · · · rcK
context embeddings
Lastly, we get the ﬁnal embedding matrix
E = α0E(0) + α1E(1) + α2E(2) · · · + αLE(L)
= α0E(0) + α1 ˆAE(0) + α2 ˆA2E(0) + · · · + αL ˆALE(0).
The GC layers output reﬁned representation of user
pu and item qi, and keep the embeddings of context features unchanged. The role of the decoder
Front. Comput. Sci., 2021, 0(0): 1–19
is to output the prediction score by taking in the
representations. The standard choice of decoder is
multi-layer perceptron (MLP), which however falls
short here since it only models feature interactions
in an implicit way. In CARS, explicitly modeling
the interactions between features is known to be
important for user preference estimation . For
example, the classic factorization machine (FM)
models the pairwise interactions between feature
embeddings and has long been a competitive model
Inspired by the simplicity (linear model) and the
effectiveness of FM, we adopt it as the decoder of
GCM. The idea is to explicitly model the pairwise
interactions between the (reﬁned) representations of
user, item, and contexts with inner product. Speciﬁcally, let the set of vectors V be Vc ∪{pu, qi}, the
decoder outputs the prediction score as:
Here the self-interactions vT
s vs are excluded since
they are useless for the prediction. The bias terms
for each user, item, and context feature are omitted
for clarity.
Note that our FM-based decoder slightly differs
from the vanilla FM, which models the interactions
between the embeddings of all input features. Here
we project each user (item) into a vector, rather than
retaining the embeddings of her (its) features. An
advantage is that this way abandons the internal
interactions of user-ﬁeld (item-ﬁeld) features, shedding more light on the interactions between user
(item) and context features, which is as expected.
Model Complexity Analyses
We analyze the complexity of GCM from two aspects: the number of trainable parameters and the
time complexity.
All trainable parameters come from the encoder
layer, i.e., the embeddings of input features, since
the GC layers and the decoder layer introduce no
parameters to train. Let the feature number for the
user ﬁeld, item ﬁeld, and context ﬁeld as U, I, and C,
respectively, and the embedding size be D. Then the
embedding layer costs (U + I + C) × D parameters.
This demonstrates the low model complexity of
GCM, since the number of trainable parameters is
the same as FM — the most simple embeddingbased CARS model.
For model training, since the complexity of the
encoder plus the decoder is the same as that of FM,
we analyze the additional time complexity caused
by the GC layers. We implement the training in the
batch-wise matrix form. Assume a batch contains
all interactions. Then performing one GC layer
takes time O((|Y| + N + M)D), where N and M
denote the number of users and items, respectively.
This complexity increases linearly with the number
of GC layers.
After the model is trained, we perform one pass
of GC layers to obtain the reﬁned representations
of all users and items, which can be done ofﬂine before online serving. As such, during online serving,
we only need to execute the decoder, which has the
same time complexity of FM. This is much faster
than the recently emerging deep neural networkbased CARS models like xDeepFM and Convolutional FM . Table 1 shows the model inference time of evaluating 1000 Yelp-OH users in
which each interaction has 10 nonzero features of
embedding size 64 and batch size is 4000. The test-
Jiancan Wu et al. Graph Convolution Machine for Context-aware Recommender System
Model inference time of evaluating 1,000 Yelp-
OH users (14 million interactions and 10 nonzero features
per interaction).
Convolutional FM
ing platform is GeFore GTX 1080Ti with 16GB
memory CPU. As can be seen, GCM takes similar
time as FM, being 24.5 and 157.7 times faster than
xDeepFM and Convolutional FM, respectively.
Optimization
To optimize model parameters, we opt for the pointwise log loss, which is a common choice in recommender system . In each training epoch,
we randomly sample non-observed interactions for
each instance in Y to form the negative set Y−—
that is, for each observed instance (u, i, c) ∈Y of
Yelp (or Amazon) dataset, we randomly match 4
(or 2) items from the item pool that user u has not
interacted under context c. Then we minimize the
following objective function:
log σ(ˆyuic)−
(u,i,c)∈Y−
log (1 −σ(ˆyuic))+λ ∥Θ∥2
where σ(·) is the sigmoid function, λ controls the
L2 regularization to prevent over-ﬁtting. The optimization is done by mini-batch Adam .
Experiments
We evaluate experiments on three benchmark datasets,
aiming to answer the following research questions:
• RQ1: Compared with the state-of-the-art models, how does GCM perform w.r.t. top-k recommendation?
• RQ2: How do different settings (e.g., depth
of layer, modeling of context features, design
of decoder) affect GCM?
Statistics of the datasets. We omit ID feature
when counting the number of user and item features
Amazon-book
#User Feature
#Item Feature
#Context Feature
• RQ3: How do the representation learning
beneﬁt from multiple interactions among users,
items and contexts for item cold start issue?
Experimental Settings
Dataset Description
To demonstrate the effectiveness of GCM, we conduct experiments on three datasets from Yelp and
Amazon, which are publicly available and vary in
domain and size. We summarize the statistics of
datasets in Table 2.
• Yelp: This dataset is released by Yelp and
records users’ reviews on local businesses
like bars and restaurants. In particular, we
extract records happened in two different areas of USA — North Carolina, Ohio States
— to construct datasets, termed Yelp-NC and
Yelp-OH respectively.
• Amazon: Amazon review data is widely used
in recommendation . We select book subset from the collection in this work, and term
it Amaon-book.
In what follows, we brieﬂy introduce the features of users, items, and contexts. Speciﬁcally, for
Yelp-NC and Yelp-OH, each user proﬁle includes
yelping since year2) and average stars, while the
pre-existing features of items are composed of three
2)We only keep the year of the yelping since ﬁeld which
indicates the time the user joined Yelp.
Front. Comput. Sci., 2021, 0(0): 1–19
attributes: city, stars and is open. We treat each
review record as an observed instance, and collect
city3), month, hour, day of week and last purchase
as its context feature. For Amaon-book, the static
features of items are composed of two attributes:
price and brand. Similarly, each review record is
treated as an observed instance, and year, month,
day, day of week and last purchase are collected as
its context feature. Moreover, for all datasets, the
10-core setting is adopted to ensure data quality, i.e.,
retaining users with at least ten interactions.
For each user, we select the last interaction record
to constitute the test set, while the remains are
served as the training set. To emphasize model
capability in recommending novel items for a user,
we further ﬁlter the training set if the user-item pairs
have appeared in the test set.
Evaluation Metrics
In the evaluation phase, for each user in the test set,
we view all items that she has not consumed before as recommendation candidates. Each method
outputs a ranking list over the candidates. We then
adopt two widely-used protocols to evaluate the
quality of ranking lists: Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). In
particular, HR@K measures whether the test item
is in the top-K positions of the recommended list,
whereas NDCG@K assigns higher scores to the
top-ranked items. In our experiments, we report the
results of K = 10 and K = 50.
We compare our GCM with several methods as follows:
3)The context feature city means which city does the interaction happen on. It is set as the city of the interacted
• MF : This exploits the user-item interactions only to learn user and item embeddings,
while forgoing the context features.
• LightGCN : Such model is the state-ofthe-art GNN-based CF recommender, which
incorporates high-order connectivity in useritem interaction graph into embeddings, while
neglecting context features.
• FM : This takes into account all information related to an interaction by converting all
information to a feature vector then modeling second-order feature interaction to predict
user preference.
• NFM : This model leverages a MLP to
capture nonlinear and high-order interaction
among user, item, and context features.
• xDeepFM : This is a recent neural FM
model which combines explicit and implicit
high-order feature interactions.
• GIN : This is a graph-based model which
mines user intention by applying implicit intention propagation and attention mechanism
on commodity similarity graph.
Fi-GNN is a recent work on click-through
rate prediction with graph neural network, which
is highly relevant with our work. It differs from
GCM in graph construction — it builds a feature
graph for each interaction, rather than the user-item
graph. As a graph needs be built for each interaction
to obtain its prediction, the method is very slow
in evaluation since all recommendation candidates
need be scored. As such, this method is not suitable
for our all-ranking CARS evaluation, and we do not
further compare with it. The Convolutional FM is
not compared for the same reason (see Table 1 for
model inference time).
Jiancan Wu et al. Graph Convolution Machine for Context-aware Recommender System
Overall Performance Comparison. The bold indicates the best result, while the second-best performance is underlined.
Amazon-book
Parameter Settings
We implement our GCM model and all baselines
in Tensorﬂow and will release our code upon acceptance. We apply the mini-batch Adam to optimize all models, the learning rate and batch size are
set to 0.001 and 2048 respectively. A grid search
is conducted for conﬁrming optimal hyperparameters: for LightGCN, the number of gcn layers is
searched in {1, 2, 3, 4}; for NFM, the number of hidden layers is set to 1, the dropout rate is tuned in
{0.9, 0.8, · · · , 0.1} for bi-interaction layer and hidden layer respectively; for xDeepFM, the number of
cross layers is searched in {1, 2, 3} with neuron number per layer in {10, 20, 50, 100, 200}, the number of
DNN layers is same to that of cross layers, while the
neuron number per layer is set to 100; for GIN, the
length of previous records is 1 since we only keep
the last purchase information in the datasets, the
depth parameter is searched in {1, 2, 3, 4}, the number of neighbor nodes is tuned in {10, 20}, the neighbor is selected by the Top-N function according to
the edge weight (for nodes with few neighbors, we
randomly sample from unconnected nodes as their
potential neighbors), a 5-layer full-connection perception with ReLU activation is adopted as the setting in ; for the proposed GCM, we search the
model depth L amongst {1, 2, 3} with αl = 1/(L+1),
and adopt average pooling to generate the ﬁnal re-
ﬁned representations of GC layers. For all models,
the coefﬁcient of L2 regularization term is searched
in {10−5, 10−4, 10−3, 10−2, 10−1}. Moreover, we set
the embedding size to 64 and train all models from
the scratch.
Performance Comparison (RQ1)
We report the empirical results of all models in
Table 3 and have the following observations:
• Clearly, MF achieves the worst performance
on three datasets, indicating that modeling
user-item pairs as isolated instances limits the
representation ability severely. LightGCN obtains consistent improvements over MF. We
attribute such improvements to the modeling
of user-item connectivity. However, neither
MF nor LightGCN takes the context features
into consideration, ignoring important factors
and being insufﬁcient for CARS.
• FM, NFM and xDeepFM consistently outperform MF and LightGCN across all cases.
This is reasonable since they incorporate context features into the representation learning,
so as to achieve better expressiveness and
Front. Comput. Sci., 2021, 0(0): 1–19
help to solve the data sparsity issue; Among
them, NFM and xDeepFM perform better
than FM by a large margin since they model
more complex feature interactions: NFM employs MLP on user, item, and context features
to capture their nonlinear and complex interactions, while xDeepFM learns high-order
feature interactions in a more explicit way
through a CIN network. This veriﬁes that
simply linear functions (e.g., inner product
adopted by MF and LightGCN) might limit
the representation learning and interaction
• GIN is the strongest baseline in all cases except for NDCG@10 and NDCG@50 in Yelp-
NC. Such improvements is mainly because of
GIN’s capability to model user intention by
applying message propagation in commodity
similarity graph, which also verify the necessity of bridging the relationship among data
instances.
• GCM consistently outperforms all baselines
w.r.t. all measures. In particular, GCM achieves
noticeable improvements over the strongest
baselines w.r.t. HR@10 by 20.78%, 14.93%,
and 3.08%, in Yelp-NC, Yelp-OH, and Amazonbook, respectively. From t-test, we can ﬁnd pvalue < 0.05 across the board, indicating that
the improvements of GCM over the strongest
baseline are statistically signiﬁcant. We attribute such improvements to: 1) GCM employs the embedding propagation over the
attributed graph to distill useful information
from neighbors and connected edges, thus
improving the representation ability; 2) Comparing with GIN which only propagates item
embedding in the graph, GCM integrates the
representations of users, items and contexts
into the graph for information propagation,
which may results in a more uniﬁed representations; and 3) Having established the reﬁned
representations, GCM further adopts FM to
explicitly model the feature interactions.
Study of GCM (RQ2)
We next report ablation studies to verify the rationality of some designs in GCM, i.e., analyzing the
inﬂuence of model depth, context modeling, normalization term, and decoder.
Impact of Model Depth
As GC is the core of GCM and stacking more GC
layers is expected to augment the user and item
representations with information propagated from
multi-hop neighbors, we investigate how the number of GC layers affects the performance. In particular, we search the number of GC layers, L, in the
range of {0, 1, 2, 3} and report the empirical results
in Figure 3.
We use GCM-1 to represent the model with one
GC layer, and similar notations for others. We have
several ﬁndings:
• GCM-0 disables the embedding propagation
over user-item attributed graph and downgrades to a FM-like linear model, thereby
achieving poor performance. This again justi-
ﬁes the importance of GC layers.
• Obviously, increasing the number of GC layers results in better performance from L = 0
to 2. In particular, GCM-2 performs better
than GCM-1 in both datasets. It is reasonable since the signals passing from multi-hop
neighbors (e.g., the second-order connectivity between behaviorally similar users or copurchased items) are encoded into user and
Jiancan Wu et al. Graph Convolution Machine for Context-aware Recommender System
GCM-GC-MLP
(a) Yelp-NC
GCM-GC-MLP
(b) Yelp-OH
The impact of depth and propagation rule in GC.
item representations of GCM-2, while GCM-
1 only exploits personal history to enrich representations. This observation is consistent
with that in NGCF . We also tried to stack
more GC layers (i.e., GCM-3), ﬁnding improvement degrades and over-smoothing issue. This suggests that GCM beneﬁts from
the ﬁrst- and second-order neighbors most,
but may suffer from degradation when higherorder neighbors are involved.
Impact of Context Modeling
One major contribution of GCM is to organize the
context features as edges in the attributed user-item
graph. We hence perform ablation study, to demonstrate the rationality and effectiveness of this design.
In particular, we build three different propagation
rules for the GC layers of GCM: 1) GCM-GC-C
removes the context features from the attributed
graph and keeps the vanilla user-item interaction
graph only; 2) GCM-GC-MLP ﬁrst replaces the addition operation with concatenation in Equation (3),
then generates the message vector through a MLP;
3) GCM-GC-HP encodes the Hadamard product of
the representations of neighboring node and their
connected edge into the message during message
passing. We show the comparison results in Figure 3 and have the following observations.
• Modeling context features as the edges endows GCM with better generalization ability. In particular, GCM-GC-C performs worst
among four competing methods in all cases,
demonstrating the necessity of modeling context features when performing message propagation.
• GCM-GC-MLP consistently achieves better
recommendation accuracy than GCM-GC-HP.
One possible reason is that equipped with
MLP, GCM-GC-MLP can model non-linear
and high-order feature interactions, resulting
in better representation ability.
• On both datasets, the best performance is always achieved by 2-layers GCM, which again
justiﬁes the rationality of our context modeling strategy. Mathematically, GCM can be
viewed as a special case of GCM-GC-MLP
in which the learnt weights are the concatenation of two identity matrix. However, as
more trainable parameters are involved, it
would require more data to learn the function
which may encounter difﬁculty in actually
learning process, especially for the notorious
problem of data sparsity in recommendation
system. .
• Jointly analyzing Table 3 and Figure 3, we
ﬁnd that GCM-GC-C without considering
contexts achieves better performance than
other baselines in Yelp-NC and comparable
performance in Yelp-OH. This empirically
suggests that propagating embeddings over
interaction graphs is of importance to generate high-quality representations.
Impact of normalization term
For convenience, we only present the variants of
the GC of the user side, since the same logic can be
applied to the item side. In GCM, we employ sqrt
Front. Comput. Sci., 2021, 0(0): 1–19
The variants of GCM with different normalization
terms and decoders
normalization term
√|Nu| on each neighbor embedding when performing neighborhood aggregation.
To verify its rationality, we explore two different
variants and report their empirical results here. The
ﬁrst variant uses symmetric normalization term, i.e.,
√|Nu| √|Ni|, which is a common choice in GCN-based
models , we term it GCM-sym. The other variant uses L1 normalization, i.e.,
|Nu|, we term it GCM-
L1. Table 4 shows the results of the 2-layer GCM.
We have the following observations:
• The best setting in general is using sqrt normalization term on single side (i.e., the current degign of GCM). Adding additional regularization coefﬁcients will greatly drop the
performance.
• The smaller the normalization term, the worse
the performance. To understand this observation, we can see the following inequalities:
√|Nu| √|Ni| >
√|Nu| √|Nu| =
|Nu|. The second inequality is due to |Nu| > |Ni| on everage
in Yelp-NC and Yelp-OH.
Impact of Decoder
Having applied GC layers, we equip GCM with
a decoder to model the pairwise interactions between the reﬁned representations of users, items,
and contexts. Here we investigate the role of such
decoder. Towards this end, we compare GCM with
three variants: 1) GCM-APC, that performs average
pooling on context features before pair-wise inner
product; 2) GCM-MLP, that replaces the decoder
with MLP; and 3) GCM-MF, that replaces FM with
inner product on user and item representations. Table 4 shows the comparison of results. There are
several observations:
• Clearly, modeling feature interactions in the
decoder enhances the predictive results. In
particular, GCM, GCM-APC and GCM-MLP
perform consistently better than GCM-MF,
which relies only on the inner product of user
and item representations.
• While having encoded context features into
user and item representations via GC layer,
GCM and GCM-APC highlight their inﬂuence in an explicit fashion, while GCM-MLP
models the feature interactions in a rather implicit way. The better performances of GCM
and GCM-APC again verify the rationality
and effectiveness of FM-based decoder. In
addition, after performing average pooling on
context features, GCM-APC degenerates the
performance by a noticeable margin, the reason is that GCM endows higher weights on
feature interaction between context ﬁeld and
user (item) ﬁeld, which is the core of CARS.
Performance w.r.t. Item Popularity (RQ3)
To alleviate the issue of item cold start of CF models,
taking side information into account is an auxiliary
strategy go beyond modeling user-item interaction.
In the proposed GCM, We apply gc layers to capture
high-order connectivity on user-item graph, which
breaks down the independent interaction assumption of non-graph-based methods. We argue that
such connectivity is a potential side information for
cold-start issue. To verify this viewpoint, we split
the test set according to the popularity (the number
Jiancan Wu et al. Graph Convolution Machine for Context-aware Recommender System
of interaction records) of the target item, and report
the performance of MF , GCM-0 and GCM in
Figure 4. We have the following observations:
• MF performs poorly at unpopular items, which
indicates the item cold-start issue for CF models. GCM-0 has signiﬁcant improvements in
recommending uncommon items by introducing side information and modeling feature
interactions. Our GCM can further improve
performance by 20%-30%. We attribute such
improvements to modeling high-order connectivity since gnn increases the possibility of
unpopular item being exposed through highorder links, thereby expanding the training
data of unpopular items.
• For popular items, MF achieves comparable
performance with GCM, even prevails over
GCM in Yelp-NC. The possible reason is that
the data of popular items occupies the majority of the training data, making MF adopt a
cautious strategy — biased to recommending
generally accepted items. Instead, GCM will
recommend items that are more niche but still
consistent with user’s taste.
• The gain brought by gcn decreases as the popularity of items increases. This shows that as
the number of neighbors increases, gcn may
suffer from over-smoothing, since these items
have too many audiences, causing collecting
information from neighbor nodes will also
bring in noises.
Conclusion and Future Work
In this work, we emphasize the importance of exploiting multiple interactions in CARS. Towards
this end, we ﬁrst convert the features of users, items,
and contexts into an attributed graph involving the
(a) Yelp-NC
(b) Yelp-OH
Performances with respect to item popularity.
contexts as edges between user and item nodes. We
then develop a new model, GCM, which captures
the interactions among multiple user behaviors via
graph neural networks, and then models the interactions among features of individual behavior via
factorization machine. To demonstrate the effectiveness of GCM, we test it on three public datasets, and
it shows signiﬁcant improvements over the state-ofthe-art CF and CARS baselines. Extensive experiments also are conducted to verify the rationality
of attributed graph and offer insights into how the
representations beneﬁt from such graph learning.
Organizing user behaviors with contextual information in graphs is a promising direction to build an
effective context-aware recommender. It helps build
strong representations for users and items. However, GCM simply uniﬁes all context features as an
edge, neglecting dynamic characteristics of some
contexts (e.g., time) and hardly capturing dynamic
preference of users . In future, we plan to build
Front. Comput. Sci., 2021, 0(0): 1–19
dynamic graphs based on contextual information,
instead of one static graph, and devise a dynamic
graph neural network. Furthermore, rich side information is beneﬁcial for explaining diverse intents
behind user behaviors . We hence plan to model
user-item relationships at a granular level of user intents to generate disentangled representations .