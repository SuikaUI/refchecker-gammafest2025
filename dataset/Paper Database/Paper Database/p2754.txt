YOLOv3: An Incremental Improvement
Joseph Redmon
Ali Farhadi
University of Washington
We present some updates to YOLO! We made a bunch
of little design changes to make it better. We also trained
this new network that’s pretty swell. It’s a little bigger than
last time but more accurate.
It’s still fast though, don’t
worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP,
as accurate as SSD but three times faster. When we look
at the old .5 IOU mAP detection metric YOLOv3 is quite
good. It achieves 57.9 AP50 in 51 ms on a Titan X, compared to 57.5 AP50 in 198 ms by RetinaNet, similar performance but 3.8× faster. As always, all the code is online at
 
1. Introduction
Sometimes you just kinda phone it in for a year, you
know? I didn’t do a whole lot of research this year. Spent
a lot of time on Twitter. Played around with GANs a little.
I had a little momentum left over from last year ; I
managed to make some improvements to YOLO. But, honestly, nothing like super interesting, just a bunch of small
changes that make it better. I also helped out with other
people’s research a little.
Actually, that’s what brings us here today.
a camera-ready deadline and we need to cite some of
the random updates I made to YOLO but we don’t have a
source. So get ready for a TECH REPORT!
The great thing about tech reports is that they don’t need
intros, y’all know why we’re here. So the end of this introduction will signpost for the rest of the paper. First we’ll tell
you what the deal is with YOLOv3. Then we’ll tell you how
we do. We’ll also tell you about some things we tried that
didn’t work. Finally we’ll contemplate what this all means.
2. The Deal
So here’s the deal with YOLOv3: We mostly took good
ideas from other people. We also trained a new classiﬁer
network that’s better than the other ones. We’ll just take
you through the whole system from scratch so you can understand it all.
inference time (ms)
RetinaNet-50
RetinaNet-101
[B] SSD321
[C] DSSD321
[E] SSD513
[F] DSSD513
[G] FPN FRCN
RetinaNet-50-500
RetinaNet-101-500
RetinaNet-101-800
YOLOv3-320
YOLOv3-416
YOLOv3-608
Figure 1. We adapt this ﬁgure from the Focal Loss paper .
YOLOv3 runs signiﬁcantly faster than other detection methods
with comparable performance. Times from either an M40 or Titan
X, they are basically the same GPU.
2.1. Bounding Box Prediction
Following YOLO9000 our system predicts bounding
boxes using dimension clusters as anchor boxes . The
network predicts 4 coordinates for each bounding box, tx,
ty, tw, th. If the cell is offset from the top left corner of the
image by (cx, cy) and the bounding box prior has width and
height pw, ph, then the predictions correspond to:
bx = σ(tx) + cx
by = σ(ty) + cy
bw = pwetw
bh = pheth
During training we use sum of squared error loss. If the
ground truth for some coordinate prediction is ˆt* our gradient is the ground truth value (computed from the ground
truth box) minus our prediction: ˆt* −t*. This ground truth
value can be easily computed by inverting the equations
YOLOv3 predicts an objectness score for each bounding
box using logistic regression. This should be 1 if the bounding box prior overlaps a ground truth object by more than
any other bounding box prior. If the bounding box prior
 
bx=σ(tx)+cx
by=σ(ty)+cy
Figure 2. Bounding boxes with dimension priors and location
prediction. We predict the width and height of the box as offsets
from cluster centroids. We predict the center coordinates of the
box relative to the location of ﬁlter application using a sigmoid
function. This ﬁgure blatantly self-plagiarized from .
is not the best but does overlap a ground truth object by
more than some threshold we ignore the prediction, following . We use the threshold of .5. Unlike our system
only assigns one bounding box prior for each ground truth
object. If a bounding box prior is not assigned to a ground
truth object it incurs no loss for coordinate or class predictions, only objectness.
2.2. Class Prediction
Each box predicts the classes the bounding box may contain using multilabel classiﬁcation. We do not use a softmax
as we have found it is unnecessary for good performance,
instead we simply use independent logistic classiﬁers. During training we use binary cross-entropy loss for the class
predictions.
This formulation helps when we move to more complex
domains like the Open Images Dataset . In this dataset
there are many overlapping labels (i.e. Woman and Person).
Using a softmax imposes the assumption that each box has
exactly one class which is often not the case. A multilabel
approach better models the data.
2.3. Predictions Across Scales
YOLOv3 predicts boxes at 3 different scales. Our system extracts features from those scales using a similar concept to feature pyramid networks . From our base feature extractor we add several convolutional layers. The last
of these predicts a 3-d tensor encoding bounding box, objectness, and class predictions.
In our experiments with
COCO we predict 3 boxes at each scale so the tensor is
N × N × [3 ∗(4 + 1 + 80)] for the 4 bounding box offsets,
1 objectness prediction, and 80 class predictions.
Next we take the feature map from 2 layers previous and
upsample it by 2×. We also take a feature map from earlier
in the network and merge it with our upsampled features
using concatenation. This method allows us to get more
meaningful semantic information from the upsampled features and ﬁner-grained information from the earlier feature
map. We then add a few more convolutional layers to process this combined feature map, and eventually predict a
similar tensor, although now twice the size.
We perform the same design one more time to predict
boxes for the ﬁnal scale. Thus our predictions for the 3rd
scale beneﬁt from all the prior computation as well as ﬁnegrained features from early on in the network.
We still use k-means clustering to determine our bounding box priors.
We just sort of chose 9 clusters and 3
scales arbitrarily and then divide up the clusters evenly
across scales. On the COCO dataset the 9 clusters were:
(10×13), (16×30), (33×23), (30×61), (62×45), (59×
119), (116 × 90), (156 × 198), (373 × 326).
2.4. Feature Extractor
We use a new network for performing feature extraction.
Our new network is a hybrid approach between the network
used in YOLOv2, Darknet-19, and that newfangled residual
network stuff. Our network uses successive 3 × 3 and 1 × 1
convolutional layers but now has some shortcut connections
as well and is signiﬁcantly larger. It has 53 convolutional
layers so we call it.... wait for it..... Darknet-53!
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Convolutional
Table 1. Darknet-53.
This new network is much more powerful than Darknet-
19 but still more efﬁcient than ResNet-101 or ResNet-152.
Here are some ImageNet results:
Darknet-19 
ResNet-101 
ResNet-152 
Darknet-53
Table 2. Comparison of backbones. Accuracy, billions of operations, billion ﬂoating point operations per second, and FPS for
various networks.
Each network is trained with identical settings and tested
at 256×256, single crop accuracy. Run times are measured
on a Titan X at 256 × 256. Thus Darknet-53 performs on
par with state-of-the-art classiﬁers but with fewer ﬂoating
point operations and more speed. Darknet-53 is better than
ResNet-101 and 1.5× faster. Darknet-53 has similar performance to ResNet-152 and is 2× faster.
Darknet-53 also achieves the highest measured ﬂoating
point operations per second. This means the network structure better utilizes the GPU, making it more efﬁcient to evaluate and thus faster. That’s mostly because ResNets have
just way too many layers and aren’t very efﬁcient.
2.5. Training
We still train on full images with no hard negative mining
or any of that stuff. We use multi-scale training, lots of data
augmentation, batch normalization, all the standard stuff.
We use the Darknet neural network framework for training
and testing .
3. How We Do
YOLOv3 is pretty good! See table 3. In terms of COCOs
weird average mean AP metric it is on par with the SSD
variants but is 3× faster. It is still quite a bit behind other
Two-stage methods
Faster R-CNN+++ 
ResNet-101-C4
Faster R-CNN w FPN 
ResNet-101-FPN
Faster R-CNN by G-RMI 
Inception-ResNet-v2 
Faster R-CNN w TDM 
Inception-ResNet-v2-TDM
One-stage methods
YOLOv2 
DarkNet-19 
SSD513 
ResNet-101-SSD
DSSD513 
ResNet-101-DSSD
RetinaNet 
ResNet-101-FPN
RetinaNet 
ResNeXt-101-FPN
YOLOv3 608 × 608
Darknet-53
Table 3. I’m seriously just stealing all these tables from they take soooo long to make from scratch. Ok, YOLOv3 is doing alright.
Keep in mind that RetinaNet has like 3.8× longer to process an image. YOLOv3 is much better than SSD variants and comparable to
state-of-the-art models on the AP50 metric.
models like RetinaNet in this metric though.
However, when we look at the “old” detection metric of
mAP at IOU= .5 (or AP50 in the chart) YOLOv3 is very
strong. It is almost on par with RetinaNet and far above
the SSD variants. This indicates that YOLOv3 is a very
strong detector that excels at producing decent boxes for objects. However, performance drops signiﬁcantly as the IOU
threshold increases indicating YOLOv3 struggles to get the
boxes perfectly aligned with the object.
In the past YOLO struggled with small objects. However, now we see a reversal in that trend. With the new
multi-scale predictions we see YOLOv3 has relatively high
APS performance. However, it has comparatively worse
performance on medium and larger size objects. More investigation is needed to get to the bottom of this.
When we plot accuracy vs speed on the AP50 metric (see
ﬁgure 5) we see YOLOv3 has signiﬁcant beneﬁts over other
detection systems. Namely, it’s faster and better.
4. Things We Tried That Didn’t Work
We tried lots of stuff while we were working on
YOLOv3. A lot of it didn’t work. Here’s the stuff we can
Anchor box x, y offset predictions. We tried using the
normal anchor box prediction mechanism where you predict the x, y offset as a multiple of the box width or height
using a linear activation. We found this formulation decreased model stability and didn’t work very well.
Linear x, y predictions instead of logistic. We tried
using a linear activation to directly predict the x, y offset
instead of the logistic activation. This led to a couple point
drop in mAP.
Focal loss. We tried using focal loss. It dropped our
mAP about 2 points. YOLOv3 may already be robust to
the problem focal loss is trying to solve because it has separate objectness predictions and conditional class predictions. Thus for most examples there is no loss from the
class predictions? Or something? We aren’t totally sure.
inference time (ms)
COCO mAP-50
RetinaNet-50
RetinaNet-101
[B] SSD321
[C] DSSD321
[E] SSD513
[F] DSSD513
[G] FPN FRCN
RetinaNet-50-500
RetinaNet-101-500
RetinaNet-101-800
YOLOv3-320
YOLOv3-416
YOLOv3-608
Figure 3. Again adapted from the , this time displaying speed/accuracy tradeoff on the mAP at .5 IOU metric. You can tell YOLOv3 is
good because it’s very high and far to the left. Can you cite your own paper? Guess who’s going to try, this guy → . Oh, I forgot, we
also ﬁx a data loading bug in YOLOv2, that helped by like 2 mAP. Just sneaking this in here to not throw off layout.
Dual IOU thresholds and truth assignment. Faster R-
CNN uses two IOU thresholds during training. If a prediction overlaps the ground truth by .7 it is as a positive example, by [.3−.7] it is ignored, less than .3 for all ground truth
objects it is a negative example. We tried a similar strategy
but couldn’t get good results.
We quite like our current formulation, it seems to be at
a local optima at least. It is possible that some of these
techniques could eventually produce good results, perhaps
they just need some tuning to stabilize the training.
5. What This All Means
YOLOv3 is a good detector. It’s fast, it’s accurate. It’s
not as great on the COCO average AP between .5 and .95
IOU metric. But it’s very good on the old detection metric
of .5 IOU.
Why did we switch metrics anyway?
The original
COCO paper just has this cryptic sentence: “A full discussion of evaluation metrics will be added once the evaluation
server is complete”. Russakovsky et al report that that humans have a hard time distinguishing an IOU of .3 from .5!
“Training humans to visually inspect a bounding box with
IOU of 0.3 and distinguish it from one with IOU 0.5 is surprisingly difﬁcult.” If humans have a hard time telling
the difference, how much does it matter?
But maybe a better question is: “What are we going to
do with these detectors now that we have them?” A lot of
the people doing this research are at Google and Facebook.
I guess at least we know the technology is in good hands
and deﬁnitely won’t be used to harvest your personal information and sell it to.... wait, you’re saying that’s exactly
what it will be used for?? Oh.
Well the other people heavily funding vision research are
the military and they’ve never done anything horrible like
killing lots of people with new technology oh wait.....1
I have a lot of hope that most of the people using computer vision are just doing happy, good stuff with it, like
counting the number of zebras in a national park , or
tracking their cat as it wanders around their house . But
computer vision is already being put to questionable use and
as researchers we have a responsibility to at least consider
the harm our work might be doing and think of ways to mitigate it. We owe the world that much.
In closing, do not @ me. (Because I ﬁnally quit Twitter).
1The author is funded by the Ofﬁce of Naval Research and Google.