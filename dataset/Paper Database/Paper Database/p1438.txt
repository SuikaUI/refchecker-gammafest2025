Omni-Scale Feature Learning for Person Re-Identiﬁcation
Kaiyang Zhou1
Yongxin Yang1
Andrea Cavallaro2
Tao Xiang1,3
1University of Surrey
2Queen Mary University of London
3Samsung AI Center, Cambridge
{k.zhou, yongxin.yang, t.xiang}@surrey.ac.uk
 
As an instance-level recognition problem, person reidentiﬁcation (re-ID) relies on discriminative features,
which not only capture different spatial scales but also
encapsulate an arbitrary combination of multiple scales.
We call features of both homogeneous and heterogeneous
scales omni-scale features. In this paper, a novel deep re-ID
CNN is designed, termed omni-scale network (OSNet), for
omni-scale feature learning. This is achieved by designing a
residual block composed of multiple convolutional streams,
each detecting features at a certain scale. Importantly, a
novel uniﬁed aggregation gate is introduced to dynamically
fuse multi-scale features with input-dependent channel-wise
weights. To efﬁciently learn spatial-channel correlations
and avoid overﬁtting, the building block uses pointwise
and depthwise convolutions. By stacking such block layerby-layer, our OSNet is extremely lightweight and can be
trained from scratch on existing re-ID benchmarks. Despite its small model size, OSNet achieves state-of-the-art
performance on six person re-ID datasets, outperforming
most large-sized models, often by a clear margin. Code
and models are available at: 
KaiyangZhou/deep-person-reid.
1. Introduction
Person re-identiﬁcation (re-ID), a fundamental task in
distributed multi-camera surveillance, aims to match people
appearing in different non-overlapping camera views. As an
instance-level recognition problem, person re-ID faces two
major challenges as illustrated in Fig. 1. First, the intraclass (instance/identity) variations are typically big due to
the changes of camera viewing conditions. For instance,
both people in Figs. 1(a) and (b) carry a backpack; the view
change across cameras (frontal to back) brings large appearance changes in the backpack area, making matching the
same person difﬁcult. Second, there are also small interclass variations – people in public space often wear similar
Figure 1: Person re-ID is a hard problem, as exempliﬁed by
the four triplets of images above. Each sub-ﬁgure shows,
from left to right, the query image, a true match and an impostor/false match.
clothes; from a distance as typically in surveillance videos,
they can look incredibly similar (see the impostors for all
four people in Fig. 1).
To overcome these two challenges, key to re-ID is to
learn discriminative features. We argue that such features
need to be of omni-scale, deﬁned as the combination of variable homogeneous scales and heterogeneous scales, each of
which is composed of a mixture of multiple scales. The
need for omni-scale features is evident from Fig. 1.
match people and distinguish them from impostors, features
corresponding small local regions (e.g. shoes, glasses) and
global whole body regions are equally important. For example, given the query image in Fig. 1(a) (left), looking at
the global-scale features (e.g. young man, a white T-shirt
+ grey shorts combo) would narrow down the search to
the true match (middle) and an impostor (right). Now the
local-scale features come into play. The shoe region gives
away the fact that the person on the right is an impostor
(trainers vs. sandals). However, for more challenging cases,
even features of variable homogeneous scales would not be
enough and more complicated and richer features that span
multiple scales are required. For instance, to eliminate the
impostor in Fig. 1(d) (right), one needs features that repre-
 
sent a white T-shirt with a speciﬁc logo in the front. Note
that the logo is not distinctive on its own – without the white
T-shirt as context, it can be confused with many other patterns. Similarly, the white T-shirt is likely everywhere in
summer (e.g. Fig. 1(a)). It is however the unique combination, captured by heterogeneous-scale features spanning
both small (logo size) and medium (upper body size) scales,
that makes the features most effective.
Nevertheless, none of the existing re-ID models addresses omni-scale feature learning. In recent years, deep
convolutional neural networks (CNNs) have been widely
used in person re-ID to learn discriminative features . However, most of the CNNs
adopted, such as ResNet , were originally designed for
object category-level recognition tasks that are fundamentally different from the instance-level recognition task in re-
ID. For the latter, omni-scale features are more important,
as explained earlier. A few attempts at learning multi-scale
features also exist . Yet, none has the ability to learn
features of both homogeneous and heterogeneous scales.
In this paper, we present OSNet, a novel CNN architecture designed for learning omni-scale feature representations. The underpinning building block consists of multiple
convolutional streams with different receptive ﬁeld sizes1
(see Fig. 2). The feature scale that each stream focuses on is
determined by exponent, a new dimension factor that is linearly increased across streams to ensure that various scales
are captured in each block. Critically, the resulting multiscale feature maps are dynamically fused by channel-wise
weights that are generated by a uniﬁed aggregation gate
(AG). The AG is a mini-network sharing parameters across
all streams with a number of desirable properties for effective model training. With the trainable AG, the generated
channel-wise weights become input-dependent, hence the
dynamic scale fusion. This novel AG design allows the network to learn omni-scale feature representations: depending on the speciﬁc input image, the gate could focus on a
single scale by assigning a dominant weight to a particular
stream or scale; alternatively, it can pick and mix and thus
produce heterogeneous scales.
Apart from omni-scale feature learning, another key design principle adopted in OSNet is to construct a lightweight
network. This brings a couple of beneﬁts: (1) re-ID datasets
are often of moderate size due to the difﬁculties in collecting
across-camera matched person images. A lightweight network with a small number of parameters is thus less prone
to overﬁtting. (2) In a large-scale surveillance application
(e.g. city-wide surveillance using thousands of cameras),
the most practical way for re-ID is to perform feature extraction at the camera end. Instead of sending the raw videos to
a central server, only the extracted features need to be sent.
For on-device processing, small re-ID networks are clearly
1We use scale and receptive ﬁeld interchangeably.
Channel-wise Adaptive Aggregation
Multi-scale
Omni-scale
Pixels across channels have
homogeneous scales
Pixels across channels have
heterogeneous scales
Figure 2: A schematic of the proposed building block for
OSNet. R: Receptive ﬁeld size.
preferred. To this end, in our building block, we factorise
standard convolutions with pointwise and depthwise convolutions . The contributions of this work are thus
both the concept of omni-scale feature learning and an effective and efﬁcient implementation of it in OSNet. The end
result is a lightweight re-ID model that is more than one order of magnitude smaller than the popular ResNet50-based
models, but performs better: OSNet achieves state-of-theart performance on six person re-ID datasets, beating much
larger networks, often by a clear margin. We also demonstrate the effectiveness of OSNet on object category recognition tasks, namely CIFAR and ImageNet , and a
multi-label person attribute recognition task. The results
suggest that omni-scale feature learning is useful beyond instance recognition and can be considered for a broad range
of visual recognition tasks. Code and pre-trained models
are available in Torchreid 2.
2. Related Work
Deep re-ID architectures.
Most existing deep re-ID
CNNs borrow architectures designed for generic object categorisation problems, such as
ImageNet 1K object classiﬁcation. Recently, some architectural modiﬁcations are introduced to reﬂect the fact that images in re-ID datasets contain instances of only one object
category (i.e., person) that mostly stand upright. To exploit
the upright body pose, add auxiliary supervision signals to features pooled horizontally from the last
convolutional feature maps. devise attention
mechanisms to focus feature learning on the foreground person regions. In , body part-speciﬁc
CNNs are learned by means of off-the-shelf pose detectors.
2 
deep-person-reid
In , CNNs are branched to learn representations of global and local image regions. In ,
multi-level features extracted at different layers are combined. However, none of these re-ID networks learns multiscale features explicitly at each layer of the networks as in
our OSNet – they typically rely on an external pose model
and/or hand-pick speciﬁc layers for multi-scale learning.
Moreover, heterogeneous-scale features computed from a
mixture of different scales are not considered.
Multi-scale feature learning. As far as we know, the concept of omni-scale deep feature learning has never been
introduced before. Nonetheless, the importance of multiscale feature learning has been recognised recently and the
multi-stream building block design has also been adopted.
Compared to a number of re-ID networks with multistream building blocks , OSNet is signiﬁcantly different.
Speciﬁcally the layer design in is based on
ResNeXt , where each stream learns features at the
same scale, while our streams in each block have different
scales. Different to , the network in is built on Inception , where multiple streams were originally designed for low computational cost with handcrafted mixture
of convolution and pooling layers. In contrast, our building
block uses a scale-controlling factor to diversify the spatial
scales to be captured. Moreover, fuses multi-stream
features with learnable but ﬁxed-once-learned streamwise
weights only at the ﬁnal block. Whereas we fuse multi-scale
features within each building block using dynamic (inputdependent) channel-wise weights to learn combinations of
multi-scale patterns. Therefore, only our OSNet is capable of learning omni-scale features with each feature channel potentially capturing discriminative features of either a
single scale or a weighted mixture of multiple scales. Our
experiments (see Sec. 4.1) show that OSNet signiﬁcantly
outperforms the models in .
Lightweight network designs. With embedded AI becoming topical, lightweight CNN design has attracted increasing attention. SqueezeNet compresses feature dimensions using 1×1 convolutions. IGCNet , ResNeXt 
and CondenseNet leverage group convolutions. Xception and MobileNet series are based on depthwise separable convolutions. Dense 1 × 1 convolutions are
grouped with channel shufﬂing in ShufﬂeNet . In terms
of lightweight design, our OSNet is similar to MobileNet
by employing factorised convolutions, with some modiﬁcations that empirically work better for omni-scale feature
3. Omni-Scale Feature Learning
In this section, we present OSNet, which specialises in
learning omni-scale feature representations for the person
re-ID task. We start with the factorised convolutional layer
and then introduce the omni-scale residual block and the
DW Conv 3x3
Figure 3: (a) Standard 3 × 3 convolution. (b) Lite 3 × 3
convolution. DW: Depth-Wise.
uniﬁed aggregation gate.
3.1. Depthwise Separable Convolutions
To reduce the number of parameters, we adopt the depthwise separable convolutions .
The basic idea is
to divide a convolution layer ReLU(w ∗x) with kernel
w ∈Rk×k×c×c′ into two separate layers ReLU((v◦u)∗x)
with depthwise kernel u ∈Rk×k×1×c′ and pointwise kernel v ∈R1×1×c×c′, where ∗denotes convolution, k the kernel size, c the input channel width and c′ the output channel
width. Given an input tensor x ∈Rh×w×c of height h and
width w, the computational cost is reduced from h·w·k2·c·c′
to h · w · (k2 + c) · c′, and the number of parameters from
k2 · c · c′ to (k2 + c) · c′. In our implementation, we use
ReLU((u◦v)∗x) (pointwise →depthwise instead of depthwise →pointwise), which turns out to be more effective for
omni-scale feature learning3. We call such layer Lite 3 × 3
hereafter. The implementation is shown in Fig. 3.
3.2. Omni-Scale Residual Block
The building block in our architecture is the residual
bottleneck , equipped with the Lite 3 × 3 layer (see
Fig. 4(a)). Given an input x, this bottleneck aims to learn a
residual ˜x with a mapping function F, i.e.
y = x + ˜x,
˜x = F(x),
where F represents a Lite 3 × 3 layer that learns singlescale features (scale = 3). Note that here the 1 × 1 layers
are ignored in notation as they are used to manipulate feature dimension and do not contribute to the aggregation of
spatial information .
Multi-scale feature learning. To achieve multi-scale feature learning, we extend the residual function F by introducing a new dimension, exponent t, which represents the
scale of the feature. For F t, with t > 1, we stack t Lite
3 × 3 layers, and this results in a receptive ﬁeld of size
(2t + 1) × (2t + 1). Then, the residual to be learned, ˜x,
is the sum of incremental scales of representations up to T:
3The subtle difference between the two orders is when the channel
width is increased: pointwise →depthwise increases the channel width
before spatial aggregation.
Figure 4: (a) Baseline bottleneck. (b) Proposed bottleneck.
AG: Aggregation Gate. The ﬁrst/last 1 × 1 layers are used
to reduce/restore feature dimension.
When T = 1, Eq. 2 reduces to Eq. 1 (see Fig. 4(a)). In
this paper, our bottleneck is set with T = 4 (i.e. the largest
receptive ﬁeld is 9 × 9) as shown in Fig. 4(b). The shortcut
connection allows features at smaller scales learned in the
current layer to be preserved effectively in the next layers,
thus enabling the ﬁnal features to capture a whole range of
spatial scales.
Uniﬁed aggregation gate. So far, each stream can give
us features of a speciﬁc scale, i.e., they are scale homogeneous. To learn omni-scale features, we propose to combine
the outputs of different streams in a dynamic way, i.e., different weights are assigned to different scales according to
the input image, rather than being ﬁxed after training. More
speciﬁcally, the dynamic scale-fusion is achieved by a novel
aggregation gate (AG), which is a learnable neural network.
Let xt denote F t(x), the omni-scale residual ˜x is obtained by
G(xt) ⊙xt,
xt ≜F t(x),
where G(xt) is a vector with length spanning the entire
channel dimension of xt and ⊙denotes the Hadamard
product. G is implemented as a mini-network composed
of a non-parametric global average pooling layer and
a multi-layer perceptron (MLP) with one ReLU-activated
hidden layer, followed by the sigmoid activation. To reduce
parameter overhead, we follow to reduce the hidden dimension of the MLP with a reduction ratio, which is
set to 16.
It is worth pointing out that, in contrast to using a single
scalar-output function that provides a coarse scale-fusion,
we choose to use channel-wise weights, i.e., the output of
the AG network G(xt) is a vector rather a scalar for the tth stream. This design results in a more ﬁne-grained fusion
that tunes each feature channel. In addition, the weights are
dynamically computed by being conditioned on the input
data. This is crucial for re-ID as the test images contain
people of different identities from those in training; thus
an adaptive/input-dependent feature-scale fusion strategy is
more desirable.
Note that in our architecture, the AG is shared for all feature streams in the same omni-scale residual block (dashed
box in Fig. 4(b)). This is similar in spirit to the convolution
ﬁlter parameter sharing in CNNs, resulting in a number of
advantages. First, the number of parameters is independent
of T (number of streams), thus the model becomes more
scalable. Second, unifying AG (sharing the same AG module across streams) has a nice property while performing
backpropagation. Concretely, suppose the network is supervised by a loss function L which is differentiable and the
gradient ∂L
∂˜x can be computed; the gradient w.r.t G, based on
The second term in Eq. 4 indicates that the supervision
signals from all streams are gathered together to guide the
learning of G.
This desirable property disappears when
each stream has its own gate.
3.3. Network Architecture
OSNet is constructed by simply stacking the proposed
lightweight bottleneck layer-by-layer without any effort to
customise the blocks at different depths (stages) of the
The detailed network architecture is shown in
Table 1. For comparison, the same network architecture
with standard convolutions has 6.9 million parameters and
3,384.9 million mult-add operations, which are 3× larger
than our OSNet with the Lite 3×3 convolution layer design.
The standard OSNet in Table 1 can be easily scaled up or
down in practice, to balance model size, computational cost
and performance. To this end, we use a width multiplier4
and an image resolution multiplier, following .
Relation to prior architectures.
In terms of multistream design, OSNet is related to Inception and
ResNeXt , but has crucial differences in several aspects. First, the multi-stream design in OSNet strictly follows the scale-incremental principle dictated by the exponent (Eq. 2).
Speciﬁcally, different streams have different receptive ﬁelds but are built with the same Lite 3 × 3
layers (Fig. 4(b)).
Such a design is more effective at
capturing a wide range of scales.
In contrast, Inception
was originally designed to have low computational costs
by sharing computations with multiple streams. Therefore
its structure, which includes mixed operations of convolution and pooling, was handcrafted. ResNeXt has multi-
4Width multiplier with magnitude smaller than 1 works on all layers in
OSNet except the last FC layer whose feature dimension is ﬁxed to 512.
128×64, 64
7×7 conv, stride 2
3×3 max pool, stride 2
64×32, 256
bottleneck × 2
transition
64×32, 256
32×16, 256
2×2 average pool, stride 2
32×16, 384
bottleneck × 2
transition
32×16, 384
2×2 average pool, stride 2
bottleneck × 2
global average pool
Table 1: Architecture of OSNet with input image size 256×
ple equal-scale streams thus learning representations at the
same scale.
Second, Inception/ResNeXt aggregates features by concatenation/addition while OSNet uses a uni-
ﬁed AG (Eq. 3), which facilitates the learning of combinations of multi-scale features. Critically, it means that the
fusion is dynamic and adaptive to each individual input image. Therefore, OSNet’s architecture is fundamentally different from that of Inception/ResNeXt in nature. Third, OS-
Net uses factorised convolutions and thus the building block
and subsequently the whole network is lightweight. Compared with SENet , OSNet is conceptually different.
Concretely, SENet aims to re-calibrate the feature channels by re-scaling the activation values for a single stream,
whereas OSNet is designed to selectively fuse multiple feature streams of different receptive ﬁeld sizes in order to
learn omni-scale features (see Fig. 2).
4. Experiments
4.1. Evaluation on Person Re-Identiﬁcation
Datasets and settings.
We conduct experiments on six
widely used person re-ID datasets:
Market1501 ,
DukeMTMC-reID
MSMT17 , VIPeR and GRID .
dataset statistics are provided in Table 2. The ﬁrst four are
considered as ‘big’ datasets even though their sizes (around
30K training images for the largest MSMT17) are fairly
moderate; while VIPeR and GRID are generally too small
to train without using those big datasets for pre-training. For
CUHK03, we use the 767/700 split with the detected
images. For VIPeR and GRID, we ﬁrst train a single OS-
Net from scratch using training images from Market1501,
CUHK03, Duke and MSMT17 (Mix4), and then perform
ﬁne-tuning. Following , the results on VIPeR and GRID
are averaged over 10 random splits. Such a ﬁne-tuning strat-
# IDs (T-Q-G)
# images (T-Q-G)
Market1501
751-750-751
12936-3368-15913
767-700-700
7365-1400-5332
702-702-1110
16522-2228-17661
1041-3060-3060
30248-11659-82161
316-316-316
632-632-632
125-125-900
250-125-900
Table 2: Dataset statistics. T: Train. Q: Query. G: Gallery.
egy has been commonly adopted by other deep learning approaches . Cumulative matching characteristics (CMC) Rank-1 accuracy and mAP are used as
evaluation metrics.
Implementation details. A classiﬁcation layer (linear FC
+ softmax) is mounted on the top of OSNet. Training follows the standard classiﬁcation paradigm where each person identity is regarded as a unique class. Similar to ,
cross entropy loss with label smoothing is used for
supervision.
For fair comparison against existing models, we implement two versions of OSNet. One is trained
from scratch and the other is ﬁne-tuned from ImageNet pretrained weights. Person matching is based on the ℓ2 distance of 512-D feature vectors extracted from the last FC
layer (see Table 1). Batch size and weight decay are set to
64 and 5e-4 respectively. For training from scratch, SGD
is used to train the network for 350 epochs. The learning
rate starts from 0.065 and is decayed by 0.1 at 150, 225
and 300 epochs. Data augmentation includes random ﬂip,
random crop and random patch5. For ﬁne-tuning, we train
the network with AMSGrad and initial learning rate of
0.0015 for 150 epochs. The learning rate is decayed by 0.1
every 60 epochs. During the ﬁrst 10 epochs, the ImageNet
pre-trained base network is frozen and only the randomly
initialised classiﬁer is open for training. Images are resized
to 256 × 128. Data augmentation includes random ﬂip and
random erasing . The code is based on Torchreid .
Results on big re-ID datasets. From Table 3, we have
the following observations. (1) OSNet achieves state-ofthe-art performance on all datasets, outperforming most
published methods by a clear margin. It is evident from
Table 3 that the performance on re-ID benchmarks, especially Market1501 and Duke, has been saturated lately.
Therefore, the improvements obtained by OSNet are signiﬁcant.
Crucially, the improvements are achieved with
much smaller model size – most existing state-of-the-art re-
ID models employ a ResNet50 backbone, which has more
than 24 million parameters (considering their extra customised modules), while our OSNet has only 2.2 million
parameters. This veriﬁes the effectiveness of omni-scale
feature learning for re-ID achieved by an extremely com-
5RandomPatch works by (1) constructing a patch pool that stores randomly extracted image patches and (2) pasting a random patch selected
from the patch pool onto an input image at random position.
Publication
Market1501
ShufﬂeNet†‡ 
MobileNetV2†‡ 
MobileNetV2
BraidNet† 
OSNet† (ours)
PNGAN 
FDGAN 
NeurIPS’18
DuATM 
Bilinear 
DeepCRF 
SGGNN 
Mancs 
AANet 
IANet 
DGNet 
OSNet (ours)
Table 3: Results (%) on big re-ID datasets. It is clear that OSNet achieves state-of-the-art performance on all datasets,
surpassing most published methods by a clear margin. It is noteworthy that OSNet has only 2.2 million parameters, which
are far less than the current best-performing ResNet-based methods. -: not available. †: model trained from scratch. ‡:
reproduced by us. (Best and second best results in red and blue respectively)
pact network. As OSNet is orthogonal to some methods,
such as the image generation based DGNet , they can
be potentially combined to further boost the re-ID performance. (2) OSNet yields strong performance with or without ImageNet pre-training. Among the very few existing
lightweight re-ID models that can be trained from scratch
(HAN and BraidNet), OSNet exhibits huge advantages. At
R1, OSNet beats HAN/BraidNet by 2.4%/9.9% on Market1501 and 4.2%/8.3% on Duke. The margins at mAP are
even larger. In addition, general-purpose lightweight CNNs
are also compared without ImageNet pre-training. Table 3
shows that OSNet surpasses the popular MobileNetV2 and
ShufﬂeNet by large margins on all datasets.
all three networks have similar model sizes.
These results thus demonstrate the versatility of our OSNet:
enables effective feature tuning from generic object categorisation tasks and offers robustness against model over-
ﬁtting when trained from scratch on datasets of moderate
sizes. (3) Compared with re-ID models that deploy a multiscale/multi-stream architecture, namely those with a Inception or ResNeXt backbone , OSNet is
clearly superior. As analysed in Sec. 3, this is attributed to
the unique ability of OSNet to learn heterogeneous-scale
features by combining multiple homogeneous-scale features with the dynamic AG.
Results on small re-ID datasets. VIPeR and GRID are
very challenging datasets for deep re-ID approaches because they have only hundreds of training images - training
on the large re-ID datasets and ﬁne-tuning on them is thus
necessary. Table 4 compares OSNet with six state-of-theart deep re-ID methods. On VIPeR, it can be observed that
OSNet outperforms the alternatives by a signiﬁcant margin
– more than 11.4% at R1. GRID is much more challenging
than VIPeR because it has only 125 training identities (250
images) and extra distractors. Further, it was captured by
real (operational) analogue CCTV cameras installed in busy
public spaces. JLML is currently the best published
method on GRID. It is noted that OSNet is marginally better than JLML on GRID. Overall, the strong performance of
OSNet on these two small datasets is indicative of its practical usefulness in real-world applications where collecting
large-scale training data is unscalable.
Ablation experiments. Table 5 evaluates our architectural
design choices where our primary model is model 1. T is
the stream cardinality in Eq. 2. (1) vs. standard convolutions: Factorising convolutions reduces the R1 marginally
by 0.4% (model 2 vs. 1). This means our architecture design maintains the representational power even though the
model size is reduced by more than 3×. (2) vs. ResNeXt-like
design: OSNet is transformed into a ResNeXt-like archi-
MuDeep 
DeepAlign 
Spindle 
HydraPlus-Net 
OSNet (ours)
Table 4: Comparison with deep learning approaches on
VIPeR and GRID. Only Rank-1 accuracy (%) is reported.
-: not available.
Architecture
Market1501
T = 4 + uniﬁed AG (primary model)
T = 4 w/ full conv + uniﬁed AG
T = 4 (same depth) + uniﬁed AG
T = 4 + concatenation
T = 4 + addition
T = 4 + separate AGs
T = 4 + uniﬁed AG (stream-wise)
T = 4 + learned-and-ﬁxed gates
T = 2 + uniﬁed AG
T = 3 + uniﬁed AG
Table 5: Ablation study on architectural design choices.
tecture by making all streams homogeneous in depth while
preserving the uniﬁed AG, which refers to model 3. We
observe that this variant is clearly outperformed by the primary model, with 1.9%/3.1% difference in R1/mAP. This
further validates the necessity of our omni-scale design. (3)
Multi-scale fusion strategy: To justify our design of the uni-
ﬁed AG, we conduct experiments by changing the way how
features of different scales are aggregated. The baselines
are concatenation (model 4) and addition (model 5). The
primary model is better than the two baselines by more
than 1.6%/2.8% at R1/mAP. Nevertheless, models 4 and
5 are still much better than the single-scale architecture
(model 9). (4) Uniﬁed AG vs. separate AGs: When separate AGs are learned for each feature stream, the model
size is increased and the nice property in gradient computation (Eq. 4) is lost. Empirically, unifying AG improves by
0.7%/0.8% at R1/mAP (model 1 vs. 6), despite having less
parameters. (5) Channel-wise gates vs. stream-wise gates:
By turning the channel-wise gates into stream-wise gates
(model 7), both the R1 and the mAP decline by 1%. As
feature channels encapsulate sophisticated correlations and
can represent numerous visual concepts , it is advantageous to use channel-speciﬁc weights. (6) Dynamic gates
vs. static gates: In model 8, feature streams are fused by
static (learned-and-then-ﬁxed) channel-wise gates to mimic
the design in . As a result, the R1/mAP drops off by
2.0%/3.5% compared with that of dynamic gates (primary
Market1501
Table 6: Results (%) of varying width multiplier β and resolution multiplier γ for OSNet. For input size, γ = 0.75:
192 × 96; γ = 0.5: 128 × 64; γ = 0.25: 64 × 32.
model). Therefore, adapting the scale fusion for individual
input images is essential. (7) Evaluation on stream cardinality: The results are substantially improved from T = 1
(model 9) to T = 2 (model 10) and gradually progress to
T = 4 (model 1).
Model shrinking hyper-parameters. We can trade-off between model size, computations and performance by adjusting the width multiplier β and the image resolution multiplier γ. Table 6 shows that by keeping one multiplier ﬁxed
and shrinking the other, the R1 drops off smoothly. It is
worth noting that 92.2% R1 accuracy is obtained by a much
shrunken version of OSNet with merely 0.2M parameters
and 82M mult-adds (β = 0.25). Compared with the results in Table 3, we can see that the shrunken OSNet is still
very competitive against the latest proposed models, most
of which are 100× bigger in size. This indicates that OSNet
has a great potential for efﬁcient deployment in resourceconstrained devices such as a surveillance camera with an
AI processor.
Visualisation of uniﬁed aggregation gate. As the gating
vectors produced by the AG inherently encode the way how
the omni-scale feature streams are aggregated, we can understand what the AG sub-network has learned by visualising images of similar gating vectors.
To this end, we
concatenate the gating vectors of four streams in the last
bottleneck, perform k-means clustering on test images of
Mix4, and select top-15 images closest to the cluster centres.
Fig. 5 shows four example clusters where images
within the same cluster exhibit similar patterns, i.e., combinations of global-scale and local-scale appearance.
Visualisation of attention. To understand how our designs
(b) Male + black jacket + blue jeans.
(c) Back bags + yellow T-shirt + black shorts.
(d) Green T-shirt.
(a) Hoody + back bag.
Figure 5: Image clusters of similar gating vectors. The visualisation shows that our uniﬁed aggregation gate is capable of
learning the combination of homogeneous and heterogeneous scales in a dynamic manner.
Figure 6: Each triplet contains, from left to right, original image, activation map of OSNet and activation map of
single-scale baseline. These images indicate that OSNet can
detect subtle differences between visually similar persons.
help OSNet learn discriminative features, we visualise the
activations of the last convolutional feature maps to investigate where the network focuses on to extract features, i.e.
attention. Following , the activation maps are computed
as the sum of absolute-valued feature maps along the channel dimension followed by a spatial ℓ2 normalisation. Fig. 6
compares the activation maps of OSNet and the single-scale
baseline (model 9 in Table 5). It is clear that OSNet can
capture the local discriminative patterns of Person A (e.g.,
the clothing logo) which distinguish Person A from Person
B. In contrast, the single-scale model over-concentrates on
the face region, which is unreliable for re-ID due to the low
resolution of surveillance images. Therefore, this qualitative result shows that our multi-scale design and uniﬁed aggregation gate enable OSNet to identify subtle differences
between visually similar persons – a vital requirement for
accurate re-ID.
4.2. Evaluation on Person Attribute Recognition
Although person attribute recognition is a categoryrecognition problem, it is closely related to the person re-ID
problem in that omni-scale feature learning is also critical:
some attributes such as ‘view angle’ are global; others such
as ‘wearing glasses’ are local; heterogeneous-scale features
are also needed for recognising attributes such as ‘age’.
Datasets and settings. We use PA-100K , the largest
person attribute recognition dataset. PA-100K contains 80K
training images and 10K test images. Each image is annotated with 26 attributes, e.g., male/female, wearing glasses,
carrying hand bag. Following , we adopt ﬁve evaluation
metrics, including mean Accuracy (mA), and four instancebased metrics, namely Accuracy (Acc), Precision (Prec),
DeepMar 
HydraPlusNet 
Table 7: Results (%) on pedestrian attribute recognition.
Female: 93.4%
Age 18-60: 99.9%
Front: 52.5%
Short Sleeve: 100.0%
Upper Logo: 94.5%
Shorts: 99.9%
Female: 95.0%
Age 18-60: 99.9%
Side: 10.7%
Shoulder Bag: 99.9%
Long Sleeve: 99.7%
Trousers: 98.3%
Age18-60: 99.8%
Back: 95.7%
Glasses: 96.4%
Long Sleeve: 91.8%
Trousers: 99.9%
Figure 7: Likelihoods on ground-truth attributes predicted
by OSNet. Correct/incorrect classiﬁcations based on threshold 50% are shown in green/red.
Recall (Rec) and F1-score (F1). Please refer to for the
detailed deﬁnitions.
Implementation details. A sigmoid-activated attribute prediction layer is added on the top of OSNet.
 , we use the weighted multi-label classiﬁcation loss
for supervision. For data augmentation, we adopt random
translation and mirroring. OSNet is trained from scratch
with SGD, momentum of 0.9 and initial learning rate of
0.065 for 50 epochs. The learning rate is decayed by 0.1
at 30 and 40 epochs.
Results. Table 7 compares OSNet with two state-of-the-art
methods on PA-100K. It can be seen that OSNet
outperforms both alternatives on all ﬁve evaluation metrics.
Fig. 7 provides some qualitative results. It shows that OS-
Net is particularly strong at predicting attributes that can
only be inferred by examining features of heterogeneous
scales such as age and gender.
4.3. Evaluation on CIFAR
Datasets and settings. CIFAR10/100 has 50K training
images and 10K test images, each with the size of 32 × 32.
OSNet is trained following the setting in . Apart
from the default OSNet in Table 1, a deeper version is
constructed by increasing the number of staged bottlenecks
from 2-2-2 to 3-8-6. Error rate is reported as the metric.
Results. Table 8 compares OSNet with a number of state-
Depth # params CIFAR10 CIFAR100
pre-act ResNet 
pre-act ResNet 
Wide ResNet 
Wide ResNet 
DenseNet 
DenseNet 
Table 8: Error rates (%) on CIFAR datasets. All methods
here use translation and mirroring for data augmentation.
Pointwise and depthwise convolutions are counted as separate layers.
Architecture
T = 4 + addition
T = 4 + uniﬁed AG
Table 9: Ablation study on OSNet on CIFAR10/100.
SqueezeNet 
MobileNetV1 
MobileNetV1 
MobileNetV1 
ShufﬂeNet 
ShufﬂeNet 
ShufﬂeNet 
MobileNetV2 
MobileNetV2 
OSNet (ours)
OSNet (ours)
OSNet (ours)
Table 10: Single-crop top1 accuracy (%) on ImageNet-2012
validation set. β: width multiplier. M: Million.
of-the-art object recognition models. The results suggest
that, although OSNet is originally designed for ﬁne-grained
object instance recognition task in re-ID, it is also highly
competitive on object category recognition tasks.
that CIFAR100 is more difﬁcult than CIFAR10 because
it contains ten times fewer training images per class (500
vs. 5,000). However, OSNet’s performance on CIFAR100
is stronger, indicating that it is better at capturing useful patterns with limited data, hence its excellent performance on
the data-scarce re-ID benchmarks.
Ablation study.
We compare our primary model with
model 9 (single-scale baseline in Table 5) and model 5 (four
streams + addition) on CIFAR10/100. Table 9 shows that
both omni-scale feature learning and uniﬁed AG contribute
positively to the overall performance of OSNet.
4.4. Evaluation on ImageNet
In this section, the results on the larger-scale ImageNet
1K category dataset are presented.
Implementation. OSNet is trained with SGD, initial learning rate of 0.4, batch size of 1024 and weight decay of 4e-
5 for 120 epochs. For data augmentation, we use random
224 × 224 crops on 256 × 256 images and random mirroring. To benchmark, we report single-crop6 top1 accuracy
on the LSVRC-2012 validation set .
Table 10 shows that OSNet outperforms the
alternative lightweight models by a clear margin.
particular OSNet×1.0 surpasses MobiltNetV2×1.0 by
3.5% and MobiltNetV2×1.4 by 0.8%.
It is noteworthy that MobiltNetV2×1.4 is around 2.5× larger than
our OSNet×1.0.
OSNet×0.75 performs on par with
ShufﬂeNet×2.0 and outperforms ShufﬂeNet×1.5/×1.0 by
2.0%/5.9%. These results give a strong indication that OS-
Net has a great potential for a broad range of visual recognition tasks. Note that although the model size is smaller, our
OSNet does have a higher number of mult-adds operations
than its main competitors. This is mainly due to the multistream design. However, if both model size and number of
Multi-Adds need to be small for a certain application, we
can reduce the latter by introducing pointwise convolutions
with group convolutions and channel shufﬂing . The
overall results on CIFAR and ImageNet show that omniscale feature learning is beneﬁcial beyond re-ID and should
be considered for a broad range of visual recognition tasks.
5. Conclusion
We presented OSNet, a lightweight CNN architecture
that is capable of learning omni-scale feature representations. Extensive experiments on six person re-ID datasets
demonstrated that OSNet achieved state-of-the-art performance, despite its lightweight design. The superior performance on object categorisation tasks and a multi-label attribute recognition task further suggested that OSNet is of
wide interest to visual recognition beyond re-ID.
Supplementary
The results in the main paper have been presented at
ICCV’19. In this supplementary, we show additional results
to further demonstrate the stength of OSNet.
A. A Strong Backbone for Cross-Domain Re-
In this section, we construct a strong backbone model
for cross-domain re-ID based on OSNet. Following ,
we add instance normalisation (IN) to the lower layers
(conv1, conv2) in OSNet. Speciﬁcally, IN is inserted
6224 × 224 centre crop from 256 × 256.
Target: Duke
Target: Market1501
Market1501 + Duke (U)
Duke + Market1501 (U)
Market1501 + Duke (U)
Duke + Market1501 (U)
TJ-AIDL 
Market1501 + Duke (U)
Duke + Market1501 (U)
ATNet 
Market1501 + Duke (U)
Duke + Market1501 (U)
CamStyle 
Market1501 + Duke (U)
Duke + Market1501 (U)
Market1501 + Duke (U)
Duke + Market1501 (U)
Market1501 + Duke (U)
Duke + Market1501 (U)
OSNet-IBN (ours)
Market1501
MSMT17+Duke (U)
MSMT17+Market1501 (U)
MSMT17+Duke (U)
MSMT17+Market1501 (U)
OSNet-IBN (ours)
Table 11: Cross-domain re-ID results. It is worth noting that OSNet-IBN (highlighted rows), without using any target data,
can achieve competitive performance with state-of-the-art unsupervised domain adaptation re-ID methods. U: Unlabelled.
after the residual connection and before the ReLU function
in a bottleneck. It has been shown in that IN can improve the generalisation performance on cross-domain semantic segmentation tasks. Here we apply the same idea to
OSNet and show that we can build a strong backbone model
for cross-domain re-ID. We call this new network OSNet-
Settings. Following the recent works ,
we choose Market1501 and Duke as the target datasets. The
source dataset is either Market1501, Duke or MSMT177.
Models are trained on labelled source data and directly
tested on target data.
Implementation details. Similar to the conventional setting, we use cross-entropy loss as the objective function.
We train OSNet-IBN with AMSGrad , batch size of 64,
weight decay of 5e-4 and initial learning rate of 0.0015 for
150 epochs. The learning rate is decayed by 0.1 every 60
epochs. During the ﬁrst 10 epochs, only the randomly initialised classiﬁcation layer is open for training while the
ImageNet pre-trained base network is frozen. All images
are resized to 256 × 128. Data augmentation includes random ﬂip and color jittering. We observed that random erasing dramatically decreased the cross-domain results so
we did not use it.
Results. Table 11 compares OSNet-IBN with current stateof-the-art unsupervised domain adaptation (UDA) methods. It is clear that OSNet-IBN achieves highly competitive
performance or even better results than some UDA methods on the target datasets, despite only using source data
for training. In particular, on Market1501→Duke (at R1),
OSNet-IBN beats all the UDA methods except ECN; on
MSMT17→Duke, OSNet-IBN performs on par with MAR;
on MSMT17→Market1501, OSNet-IBN obtains compara-
7Following , all 126,441 images of 4,101 identities in MSMT17
are used for training.
ble results with MAR and PAUL. These results demonstrate
that our OSNet-IBN, with a minor modiﬁcation, can be used
as a strong backbone model for cross-domain re-ID8.
B. Training Recipes for Practitioners
We investigate some training methods in order to further improve OSNet’s performance. We not only show the
methods that work, but also discuss what do not work in our
experiments.
Implementation.
We train the baseline OSNet following , where the main difference compared with the conference version is the use of cosine annealing strategy 
to decay the learning rate. For image matching, we use cosine distance. To make sure the result is convincing, we run
every experiment with 3 different random seeds and report
the mean and standard deviation. We choose Market1501
and Duke for benchmarking.
Dataset-speciﬁc normalisation parameters. Most re-ID
papers used the ImageNet mean and standard deviation
for pixel normalisation, without justifying whether using
dataset-speciﬁc statistics is a better choice. Typically, images from re-ID datasets exhibit drastic differences compared with the natural images from ImageNet, e.g., the person images for re-ID are usually of poor quality and blurred.
Therefore, using the statistics from re-ID dataset for pixel
normalisation seems to make more sense. However, Table 12a shows that the difference in performance is subtle,
suggesting that collecting dataset-speciﬁc statistics might
be unnecessary. In practice, we do, however, encourage
practitioners to try both ways for their own datasets.
Will larger input size help?
Table 12b shows that using larger input size improves the performance, but only
marginally. This is because OSNet can learn omni-scale
8See for an improved OSNet-IBN (called OSNet-AIN ) which
achieves better cross-domain performance via neural architecture search.
Mean & std from
Market1501
94.6±0.1 86.5±0.2
88.6±0.3 76.6±0.1
Re-ID dataset
94.4±0.0 86.3±0.1
88.5±0.1 76.5±0.2
(a) Pixel normalisation parameters
Input size
Market1501
94.6±0.1 86.5±0.2
88.6±0.3 76.6±0.1
94.9±0.1 86.9±0.1
88.5±0.5 76.8±0.2
(b) Input size
Market1501
94.6±0.1 86.5±0.2
88.6±0.3 76.6±0.1
94.6±0.1 86.4±0.1
88.5±0.5 76.5±0.3
94.5±0.1 86.5±0.2
88.7±0.2 76.6±0.0
94.7±0.1 86.4±0.3
88.4±0.1 76.7±0.2
94.7±0.1 86.6±0.2
88.3±0.2 76.7±0.2
(c) Regularisation with entropy maximisation: LID −
λeLEntropy
Market1501
94.6±0.1 86.5±0.2
88.6±0.3 76.6±0.1
w/ DML model-1
94.7±0.1 87.2±0.0
88.4±0.4 77.3±0.2
w/ DML model-2
94.8±0.1 87.3±0.0
88.5±0.8 77.3±0.2
w/ DML model-1+2
94.9±0.1 87.8±0.0
88.7±0.6 78.0±0.2
(d) Deep mutual learning and model ensemble
Market1501
94.6±0.1 86.5±0.2
88.6±0.3 76.6±0.1
94.7±0.4 86.7±0.2
88.3±0.3 76.9±0.1
95.5±0.1 87.2±0.0
88.6±0.2 77.3±0.1
94.9±0.1 86.9±0.1
88.4±0.1 76.8±0.4
(e) Auxiliary loss with hard example-mining triplet
loss: LID + λtLTriplet
Market1501
λt = 0 w/o DML
94.6±0.1 86.5±0.2
88.6±0.3 76.6±0.1
λt = 0.5 + DML model-1
95.7±0.1 88.1±0.2
89.1±0.2 78.4±0.0
λt = 0.5 + DML model-2
95.5±0.2 88.0±0.1
89.6±0.3 78.5±0.2
λt = 0.5 + DML model-1+2
95.7±0.1 88.7±0.1
89.0±0.0 79.5±0.1
(f) LTriplet + deep mutual learning + model ensemble
Table 12: Investigation on various training methods for improving OSNet’s performance. All experiments are run for 3 times
with different random seeds. Note that the implementation follows , which is slightly different from the conference
features, which are insensitive to the input size. Considering that using 320×160 increases the ﬂops from 978.9M to
1,529.3M, we suggest using 256 × 128.
Entropy maximisation. As re-ID datasets are small-scale,
we add a entropy maximisation term to further regularise the network (this term penalises conﬁdent predictions). The results are shown in Table 12c where we observe that this new regularisation term, with various balancing weights, has little effect on the performance.
Deep mutual learning (DML). Zhang et al. has
shown that DML can achieve notable improvement for re-
ID (when using MobileNet ). We apply DML to training OSNet and report the results in Table 12d. It is clear
that DML improves the mAP. This indicates that features
learned with DML are more discriminative. As DML trains
two networks simultaneously, it is natural to try model ensemble with these two networks. The results (last row in
Table 12d) show clear improvements on both rank-1 and
mAP. Note that when doing ensemble, we concatenate the
features rather than performing mean-pooling. The latter
makes more sense for classiﬁcation tasks but not for retrieval tasks where features are used.
Auxiliary loss. Several recent re-ID approaches 
adopt a multi-loss training strategy, e.g., using both crossentropy loss and triplet loss . We investigate such training strategy for OSNet where a balancing weight λt is added
to scale the triplet loss (see the caption of Table 12e). Table 12e shows that the triplet loss improves the performance
when λt is carefully tuned. In practice, we encourage practitioners to use the cross-entropy loss as the main objective and the triplet loss as an auxiliary loss with a balancing
weight (which needs to be tuned).
Combination.
We combine the effective training techniques, i.e. DML and auxiliary loss learning (with the triplet
loss), and show the results in Table 12f. It can be observed
that the improvement is larger than that of using either technique alone. The best performance is obtained by fusing the
two DML-trained models.
Therefore, we suggest training OSNet with crossentropy loss + triplet loss (λt = 0.5 as the rule of thumb) +
DML and testing with model ensemble.