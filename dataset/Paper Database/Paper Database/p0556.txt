This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
Credit Card Fraud Detection: A Realistic Modeling
and a Novel Learning Strategy
Andrea Dal Pozzolo, Giacomo Boracchi, Olivier Caelen, Cesare Alippi, Fellow, IEEE,
and Gianluca Bontempi, Senior Member, IEEE
Abstract—Detecting frauds in credit card transactions is
perhaps one of the best testbeds for computational intelligence
algorithms. In fact, this problem involves a number of relevant
challenges, namely: concept drift (customers’ habits evolve and
fraudsters change their strategies over time), class imbalance
(genuine transactions far outnumber frauds), and veriﬁcation
latency (only a small set of transactions are timely checked by
investigators). However, the vast majority of learning algorithms
that have been proposed for fraud detection rely on assumptions
that hardly hold in a real-world fraud-detection system (FDS).
This lack of realism concerns two main aspects: 1) the way
and timing with which supervised information is provided and
2) the measures used to assess fraud-detection performance.
This paper has three major contributions. First, we propose,
with the help of our industrial partner, a formalization of the
fraud-detection problem that realistically describes the operating
conditions of FDSs that everyday analyze massive streams of
credit card transactions. We also illustrate the most appropriate
performance measures to be used for fraud-detection purposes.
Second, we design and assess a novel learning strategy that effectively addresses class imbalance, concept drift, and veriﬁcation
latency. Third, in our experiments, we demonstrate the impact
of class unbalance and concept drift in a real-world data stream
containing more than 75 million transactions, authorized over a
time window of three years.
Terms—Concept
detection, learning in nonstationary environments, unbalanced
classiﬁcation.
I. INTRODUCTION
REDIT card fraud detection is a relevant problem that
draws the attention of machine-learning and computational intelligence communities, where a large number of
automatic solutions have been proposed , , , ,
Manuscript received December 8, 2015; revised August 26, 2016 and
February 28, 2017; accepted July 24, 2017. The work of A. Dal Pozzolo
was supported by the Doctiris Scholarship. The work of G. Bontempi
was supported by BridgeIRIS and BruFence funded by Innoviris, Belgium.
(Corresponding author: Andrea Dal Pozzolo.)
A. Dal Pozzolo and G. Bontempi are with the Machine Learning Group,
Computer Science Department, Université Libre de Bruxelles, 1050 Brussels,
Belgium (e-mail: ; ).
G. Boracchi
with the Dipartimento
di Elettronica,
Informazione
Bioingegneria,
Politecnico
 ).
O. Caelen is with the R&D High Processing & Volume Team, Worldline,
1130 Brussels, Belgium (e-mail: ).
C. Alippi is with the Dipartimento
di Elettronica,
Informazione
Bioingegneria, Politecnico di Milano, 20133 Milan, Italy, and also with
the Università della Svizzera Italiana, 6900 Lugano, Switzerland (e-mail:
 ).
Color versions of one or more of the ﬁgures in this paper are available
online at 
Digital Object Identiﬁer 10.1109/TNNLS.2017.2736643
 , , , , , . In fact, this problem
appears to be particularly challenging from a learning perspective, since it is characterized at the same time by class
imbalance , , namely, genuine transactions far outnumber frauds, and concept drift , , namely, transactions
might change their statistical properties over time. These,
however, are not the only challenges characterizing learning
problems in a real-world fraud-detection system (FDS).
In a real-world FDS, the massive stream of payment
requests is quickly scanned by automatic tools that determine which transactions to authorize. Classiﬁers are typically
employed to analyze all the authorized transactions and alert
the most suspicious ones. Alerts are then inspected by professional investigators that contact the cardholders to determine
the true nature (either genuine or fraudulent) of each alerted
transaction. By doing this, investigators provide a feedback
to the system in the form of labeled transactions, which
can be used to train or update the classiﬁer, in order to
preserve (or eventually improve) the fraud-detection performance over time. The vast majority of transactions cannot be
veriﬁed by investigators for obvious time and cost constraints.
These transactions remain unlabeled until customers discover
and report frauds, or until a sufﬁcient amount of time has
elapsed such that nondisputed transactions are considered
Thus, in practice, most of supervised samples are provided
with a substantial delay, a problem known as veriﬁcation
latency . The only recent supervised information made
available to update the classiﬁer is provided through the alert–
feedback interaction. Most papers in the literature ignore
the veriﬁcation latency as well as the alert–feedback
interaction, and unrealistically assume that the label of each
transaction is regularly made available to the FDS, e.g., on
a daily basis (see , , , , , , , ).
However, these aspects have to be considered when designing
a real-world FDS, since veriﬁcation latency is harmful when
concept drift occurs, and the alert–feedback interaction is
responsible of a sort of sample selection bias (SSB) that
injects further differences between the distribution of training
and test data.
Another important difference between what is typically done
in the literature and the real-world operating conditions of
Fraud-Detection System (FDS) concerns the measures used
to assess the fraud-detection performance. Most often, global
ranking measures , , , like the area under the
ROC curve (AUC), or cost-based measures , , are
2162-237X © 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See for more information.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
Scheme illustrating the layers of control in an FDS. Our focus is mainly on the DDM and the alert–feedback interaction, which regulates the way
recent supervised samples are provided.
used, but these ignore the fact that only few alerts can be
controlled everyday, and that companies are very concerned
of the precision of the generated alerts.
The main contributions of this paper are as follows.
1) We describe the mechanisms regulating a real-world
FDS, and provide a formal model of the articulated
classiﬁcation problem to be addressed in fraud detection.
2) We introduce the performance measures that are considered in a real-world FDS.
3) Within this sound and realistic model, we propose an
effective learning strategy for addressing the above challenges, including the veriﬁcation latency and the alert–
feedback interaction. This learning strategy is tested on
a large number of credit card transactions.
This paper is organized as follows. We ﬁrst detail the operating conditions of a real-world FDS in Section II, and then
in Section III model the articulated fraud-detection problem
and present the most suitable performance measures. In particular, we deem that it is most appropriate to assess the
number of detected fraudulent transactions (or cards) over the
maximum number of transactions (or cards) that investigators can check. The main challenges raising when training
a classiﬁer for fraud-detection purposes are then discussed
in Section IV. Section V introduces the proposed learning
strategy, which consists in separately training different classi-
ﬁers from feedbacks and delayed supervised samples, and then
aggregating their predictions. This strategy, inspired by the
different nature of feedbacks and delayed supervised samples,
is shown to be particularly effective in FDS using sliding
window or ensemble of classiﬁers. We validate our claims in
experiments (Section VI) on more than 75 million e-commerce
credit card transactions acquired over three years, which are
also analyzed to observe the impact of class imbalance and
concept drift in real-world transaction streams.
Our work builds upon , which is signiﬁcantly extended
by describing in detail the real-world operating conditions of
an FDS and by analyzing the SSB introduced by the alert–
feedback interaction. Furthermore, the experimental section
has been largely updated and completed by presenting additional analysis over two large data sets.
II. REAL-WORLD FDS
Here we describe the main peculiarities and the operating
conditions of a real-world FDS, inspired by the one routinely
used by our industrial partner. Fig. 1 illustrates the ﬁve layers
of control typically employed in an FDS: 1) the terminal;
2) the transaction-blocking rules; 3) the scoring rules; 4) the
data-driven model (DDM); and 5) the investigators.
Layers 1)–4) fully implement automatic controls, while
layer 5) is the only one requiring human intervention.
A. Layers of Controls in an FDS
1) Terminal: The terminal represents the ﬁrst control layer
in an FDS and performs conventional security checks on all the
payment requests . Security checks include controlling the
PIN code (possible only in case of cards provided with chip),
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
DAL POZZOLO et al.: CREDIT CARD FRAUD DETECTION
the number of attempts, the card status (either active or
blocked), the balance available, and the expenditure limit.
In case of online transactions, these operations have to be
performed in real time (response has to be provided in a few
milliseconds), during which the terminal queries a server of
the card issuing company. Requests that do not pass any of
these controls are denied, while the others become transaction
requests that are processed by the second layer of control.
2) Transaction-Blocking Rules: Transaction-blocking rules
are if-then (-else) statements meant to block transaction
requests that are clearly perceived as frauds. These rules use
the few information available when the payment is requested,
without analyzing historical records or cardholder proﬁle.
An example of blocking rule could be “IF internet transactions AND unsecured Web site THEN deny the transaction.”1
In practice, several transaction-blocking rules are simultaneously executed, and transactions ﬁring any of these rules
are blocked (though cards are not deactivated). Transactionblocking rules are manually designed by the investigator and,
as such, are expert-driven components of the FDS. To guarantee real-time operations and avoid blocking many genuine
transactions, blocking rules should be: 1) quick to compute and
2) very precise, namely, should raise very few false alarms.
All transactions passing blocking rules are ﬁnally authorized. However, the fraud-detection activity continues after
having enriched transaction data with aggregated features
used to compare the current purchase against the previous
ones and the cardholder proﬁle. These aggregated features
include, for instance, the average expenditure, the average
number of transactions in the same day, or the location of
the previous purchases. The process of computing aggregated
features is referred to as feature augmentation and is described
in Section II-B. Augmented features and current transaction
data are stacked in a feature vector that is supposed to be
informative for determining whether the authorized transaction
is fraudulent or genuine. The following layers of the FDS
operate on this feature vector.
3) Scoring Rules: Scoring rules are also expert-driven models that are expressed as if-then (-else) statements. However,
these operate on feature vectors and assign a score to each
authorized transaction: the larger the score, the more likely
the transaction to be a fraud. Scoring rules are manually
designed by investigators, which arbitrarily deﬁne their associated scores. An example of scoring rule can be “IF previous
transaction in a different continent AND less than 1 h from
the previous transaction THEN fraud score = 0.95.”
Unfortunately, scoring rules can detect only fraudulent
strategies that have already been discovered by investigators,
and that exhibit patterns involving few components of the
feature vectors. Moreover, scoring rules are rather subjective,
since different experts design different rules.
4) Data Driven Model (DDM): This layer is purely data
driven and adopts a classiﬁer or another statistical model to
estimate the probability for each feature vector being a fraud.
This probability is used as the fraud score associated with the
1These rules are conﬁdential and we cannot disclose any of them. Here we
provide a realistic example to illustrate which sort of information can be used
in these rules.
authorized transactions. Thus, the DDM is trained from a set
of labeled transactions and cannot be interpreted or manually
modiﬁed by investigators. An effective DDM is expected to
detect fraudulent patterns by simultaneously analyzing multiple components of the feature vector, possibly through nonlinear expressions. Therefore, the DDM is expected to ﬁnd frauds
according to rules that go beyond investigator experience, and
that do not necessarily correspond to interpretable rules.
This paper focuses on this component of the FDS and
proposes a strategy to design, train, and update the DDM to
improve fraud-detection performance. Transactions associated
with feature vectors that have either received a large fraud
score or a high probability of being a fraud generate alerts.
Only a limited number of alerted transactions are reported to
the investigators, which represent the ﬁnal layer of control.
5) Investigators: Investigators are professionals experienced
in analyzing credit card transactions and are responsible of the
expert-driven layers of the FDS. In particular, investigators
design transaction-blocking and scoring rules.
Investigators are also in charge of controlling alerts raised
by the scoring rules and the DDM, to determine whether
these correspond to frauds or false alarms. In particular,
they visualize all the alerted transactions in a case management tool, where all the information about the transaction is
reported, including the assigned scores/probabilities, which in
practice indicate how risky each transaction is. Investigators
call cardholders and, after having veriﬁed, assign the label
“genuine” or “fraudulent” to the alerted transaction, and return
this information to the FDS. In the following, we refer to
these labeled transactions as feedbacks and use the term
alert–feedback interaction to describe this mechanism yielding
supervised information in a real-world FDS.
Any card that is found victim of a fraud is immediately
blocked, to prevent further fraudulent activities. Typically,
investigators check all the recent transactions from a compromised card, which means that each detected fraud can
potentially generate more than one feedback, not necessarily corresponding to alerts or frauds. In a real-world FDS,
investigators can only check few alerts per day as this
process can be long and tedious. Therefore, the primary goal
of a DDM is to return precise alerts, as investigators might
ignore further alerts when too many false alarms are reported.
B. Features Augmentation
Any transaction request is described by few variables such
as the merchant ID, cardholder ID, purchase amount, date, and
time. All transaction requests passing the blocking rules are
entered in a database containing all recent authorized transactions, where the feature-augmentation process starts. During
feature augmentation, a speciﬁc set of aggregated features
associated with each authorized transactions is computed, to
provide additional information about the purchase and better
discriminate frauds from genuine transactions. Examples of
aggregated features are the average expenditure of the customer every week/month, the average number of transactions
per day or in the same shop, the average transaction amount,
and the location of the last purchases , , , ,
 , . Van Vlasselaer et al. show that additional
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
informative features can be extracted from the social networks
connecting the cardholders with merchants/shops.
Aggregated features are very informative, as they summarize the recent cardholder activities. Thus, they allow to alert
transactions that are not suspicious by themselves but might
be unusual compared with the shopping habits of the speciﬁc
cardholder. Features augmentation can be computationally
expensive, and aggregated features are often precomputed
ofﬂine for each cardholder on the basis of historical transactions. Aggregated features are stacked with the transaction
data in the feature vector.
C. Supervised Information
Investigators’ feedbacks are the most recent supervised
information made available to the FDS, but represent only
a small fraction of the transactions processed every day .
Additional labeled transactions are provided by cardholders
that directly dispute unauthorized transactions , . The
timing of disputed transactions can vary substantially, since
cardholders have different habits when checking the transcript
of credit card sent by the bank. Moreover, checking disputed
transactions entails some necessary administrative procedures
that might introduce substantial delays.
All other transactions remain unlabeled: these can be either
genuine transactions or frauds that were missed by the FDS
and ignored by the cardholders. However, after a certain
number of days have passed without cardholder dispute, all
the unreported transactions are considered genuine by default,
and inserted in the training set of the DDM.
Overall, there are two types of supervised information:
1) feedbacks provided by investigators that are limited in number but refer to recent transactions and 2) delayed supervised
transactions, which are the vast majority for which the labels
become available after several days (e.g., one month). This
latter includes both disputed and nondisputed transactions.
D. System Update
Customers’ spending behavior evolves and fraudsters continuously design new attacks, and thus their strategies also
change over time. It is then necessary to constantly update
the FDS to guarantee satisfactory performance. Expert-driven
systems are regularly updated by investigators who add ad hoc
(transaction-blocking or scoring) rules to counteract the onset
of new fraudulent activities and remove those rules liable of
too many false alerts. However, investigators cannot modify
the DDM, since it is not interpretable and can be only
updated (e.g., retrained) on the basis of recent supervised
information, as shown in Fig. 1. This operation typically
requires a large number of labeled transactions; therefore,
though investigators steadily provide feedbacks during the day,
the classiﬁer is usually updated/re-trained only once, notably at
the end of the day, when a sufﬁcient number of feedbacks are
available.
III. PROBLEM FORMULATION
Here, we model the classiﬁcation problem to be addressed
in a real-world FDS, providing a formal description of the
alert–feedback interaction and presenting suitable performance
measures. The proposed learning strategy (Section V) and our
experiments (Section VI) are built upon this model.
Let xi denote the feature vector associated with the ith
authorized transaction and yi ∈{+, −} be the corresponding
class, where + denotes a fraud and −a genuine transaction.
To cope with the time-variant nature of the transaction stream,
a classiﬁer K is updated (or newly retrained) every day.
In particular, we denote by Kt−1 the classiﬁer that is trained on
supervised transactions available up to day t −1. The classiﬁer
Kt−1 is then used to process the set of transactions Tt that
have been authorized at day t. We denote by PKt−1(+|xi)
the posterior of Kt−1, namely, the probability for xi to be a
fraud according to Kt−1. Investigators check only few highrisk transactions. Thus, we model alerts as the k-most risky
transactions, namely
At = {xi ∈Tt s.t. r(xi) ≤k}
where r(xi) ∈{1, . . . , |Tt|} is the rank of xi according
to PKt(+|xi), and k
0 is the maximum number of
alerts that can be checked by investigators.2 As discussed in
Section II-A.5, investigators contact the cardholders and provide supervised samples to the FDS in the form of feedbacks.
In particular, feedbacks include all recent transactions from
the controlled cards, which we model as
Ft = {(xi, yi) s.t. xi is from cards(At)}
where cards(At) denotes the set of cards having at least a
transaction in At. The number of feedbacks, i.e., |Ft|, depends
on the number of transactions associated with the k controlled
After a certain veriﬁcation latency, the labels of all the
transactions are provided to the FDS, since, as discussed in
Section II-C, nondisputed transactions are considered genuine.
For the sake of simplicity, we assume a constant veriﬁcation
latency of δ days, such that at day t, the labels of all the
transactions authorized at day t −δ are provided. We refer to
these as delayed supervised samples
Dt−δ = {(xi, yi),
xi ∈Tt−δ}.
Note that Ft−δ ⊂Dt−δ since transactions at day t−δ obviously
include those that have been alerted. Fig. 2 illustrates the
different types of supervised information available in an FDS.
It is worth mentioning that, despite our formal description
includes several aspects and details that have been so far
ignored in the fraud-detection literature, this is still a simpliﬁed model. In fact, alerts in a real-world FDS are typically
raised online while transactions are being processed, without
having to rank all transactions in Tt. Similarly, the delayed
supervised couples do not come all at once, as each disputed
transactions might take less (or possibly more) than δ days.
Notwithstanding, we deem that our formulation takes into
account the aspects of a real-world FDS that are the most
important ones from a learning perspective, which include
alerts, alter–feedback interaction, and veriﬁcation latency.
We further comment that in principle, since the classiﬁer
2Throughout this paper, we denote by |·| the cardinality of a set.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
DAL POZZOLO et al.: CREDIT CARD FRAUD DETECTION
Fig. 2. Supervised samples available at the end of day t include: 1) feedbacks
[F(·)] and 2) delayed couples [D(·)] occurred before t −δ days. In this plot,
we have assumed δ = 7. Patterns indicate different labels, and the size of
these regions indicates balanced/unbalanced class proportions.
analyzes each feature vector xi independently, it does not alert
cards receiving several risky transactions until any of these
enters in the pool of the alerts (1). However, these situations
are particularly relevant for investigators, and can be handled either by suitable scoring rules or feature augmentation,
adding, for instance, a component that keeps track of the scores
of recent transactions.
Fraud-detection performance can be conveniently assessed
in terms of the alert precision Pk(t), which is deﬁned as
Pk(t) = |TPk(t)|
where TPk(t) = {(xi, yi) such that xi ∈At, yi = +}. Thus,
Pk(t) is the proportion of frauds in the alerts At. Though
the classiﬁer independently processes each feature vector, the
alert precision would be more realistically measured in terms
of cards rather than authorized transactions. In fact, multiple
transactions in At from the same card should be counted as a
single alert, since investigators check all the recent transactions
when contacting cardholders. This implies that k depends
on the maximum number of cards that the investigators can
control. In this context, it is more informative to measure the
detection performance at the card level, such that multiple
fraudulent transactions from the same card count as a single
correct detection. Thus, we introduce CPk, the card precision,
as the proportion of fraudulent cards detected in the k cards
controlled by the investigators
CPk(t) = |C+
t denotes the set of fraudulent cards correctly detected
at day t, namely, fraudulent cards having reported at least
one alert. To correctly account for those days where less than
k cards are fraudulent, we deﬁne the normalized CPk(t) as
NCPk(t) = CPk(t)
with (t) =
where (t) is the maximum value of CPk(t) and γt is the
number of fraudulent cards at day t. From (6), we have that
NCPk(t) takes values in the range , while CPk(t) is in
 when γt > k and in [0, (γt/k)] otherwise. For example,
if at day t, we have correctly detected 40 fraudulent cards
t | = 40) out of the k = 100 cards checked by investigators,
and the overall number of fraudulent cards is 50 (γt = 50),
then CPk(t) = 0.4 while NCPk(t) = (0.4)/(0.5) = 0.8.
Note that, since (t) does not depend on the speciﬁc classi-
ﬁer Kt−1 adopted, when algorithm “A” is better than algorithm
“B” in terms of CPk, “A” is also better than “B” in terms of
NCPk. Moreover, because of veriﬁcation latency, the number
of fraudulent cards in day t (i.e., γt) can be only computed
after few days, and therefore NCPk cannot be computed in
real time. Thus, we recommend using CPk for assessing the
running performance, while NCPk for backtesting, e.g., when
testing different FDS conﬁgurations, as in Section VI-F.
IV. RELATED WORKS
A. Data-Driven Approaches in Credit Card Fraud Detection
Both supervised , , and unsupervised ,
 , methods have been proposed for credit card frauddetection purposes. Unsupervised methods consist in outlier/
anomaly detection techniques that consider as a fraud any
transaction that does not conform with the majority. Remarkably, an unsupervised DDM in an FDS can be directly con-
ﬁgured from unlabeled transactions. A well-known method is
peer group analysis , which clusters customers according
to their proﬁle and identiﬁes frauds as transactions departing
from the typical cardholder’s behavior (also see
 ). The
typical cardholder’s behavior has also been modeled by means
of self-organizing maps , , .
Supervised methods are by far the most popular in fraud
detection, and exploit labeled transactions for training a
classiﬁer. Frauds are detected by classifying feature vectors
of the authorized transactions or possibly by analyzing the
posterior of the classiﬁer . Several classiﬁcation algorithms
have been tested on credit card transactions to detect frauds,
including neural networks , , , logistic regression , association rules , support vector machines ,
modiﬁed Fisher discriminant analysis , and decision
trees , , . Several studies have reported random
forest (RF) to achieve the best performance , , ,
 , : this is one of the reasons why we adopt RFs in our
experiments.
B. Performance Measure for Fraud Detection
The typical performance measure for fraud-detection problems is the AUC , , . AUC can be estimated
by means of the Mann–Whitney statistic and its value
can be interpreted as the probability that a classiﬁer ranks
frauds higher than genuine transactions . Another ranking
measure frequently used in fraud detection is average precision , which corresponds to the area under the precision–
recall curve. While these measures are widely used in detection
cost-based
speciﬁcally
designed for fraud-detection purposes. Cost-based measures , , quantify the monetary loss of a fraud by
means of a cost matrix that associates a cost with each entry
of the confusion matrix. Elkan shows that a cost matrix
may be misleading because the minimum/maximum loss of
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
the problem can change over time. To avoid this problem,
normalized cost or savings are used to assess the
performance with respect to the maximum loss.
We argue that performance measures should also account
for the investigators availability, as they have to check all the
alerts raised by the FDS. Given the limited time investigators
have, only a few alerts can be veriﬁed every day, and thus an
effective FDS should provide investigators a small number of
reliable alerts. This is the reason why we have introduced the
alert-precision measures described in Section III.
C. Major Challenges to be Addressed in a Real-World FDS
As anticipated in Section I, the major challenges to be
addressed when designing an FDS include: 1) handling the
class imbalance, since legitimate transactions far outnumber
the fraudulent ones; 2) handling the concept drift since the
statistical properties of both frauds and genuine transactions
evolve with time; and 3) operating with a small number
of recent supervised transactions, provided in the form of
investigators’ feedback.
1) Class Imbalance: Class distribution is extremely unbalanced in credit card transactions, since frauds are typically less
than 1% of the overall transactions, as shown in and 
and in our analysis (see Table I). Learning under class
imbalance has lately received a lot of attention, since traditional learning methods yield classiﬁers that are poorly
performing on the minority class, which is deﬁnitively the
class of interest in detection problems. Several techniques
have been proposed to deal with class imbalance, and for
a comprehensive overview, we refer the reader to . The
two main approaches for dealing with class imbalance are:
1) sampling methods and 2) cost-based methods. Sampling
methods are used to balance the class distribution in the
training set before running a traditional learning algorithm,
while cost-based methods modify the learning algorithm to
assign a larger misclassiﬁcation cost to the minority class .
Sampling methods are divided in undersampling, which
balance the class proportions in the training set by removing
samples from the majority class, and oversampling ones, which
achieve the same goal by replicating training samples of
the minority class . Advanced oversampling methods like
SMOTE generate synthetic training instances from the
minority class by interpolation, instead of sample replication.
Cost-based methods do not need to balance the proportion
of training data, as they take into account different losses
for classiﬁcation errors on samples belonging to the minority
and majority class. In credit card fraud detection, the cost
of a missed fraud is often assumed to be proportional to the
transaction amount , , , and this assigns a larger
misclassiﬁcation cost to frauds, thus steering the classiﬁer to
prefer false alerts rather than taking the risk of missing a fraud.
As a consequence, these algorithms might generate many false
positives while investigators require precise alerts.
2) Concept Drift: There are two main factors introducing
changes/evolutions in the stream of credit card transactions,
which in the literature are typically referred to as concept
drift , . At ﬁrst, genuine transactions evolve because
cardholders typically change their spending behaviors over
time (e.g., during holidays, they purchase more and differently from the rest of the year). Second, frauds change over
time, since new fraudulent activities are perpetrated. In our
experiments (see Section VI-D), we observe the evolving
nature of credit card transactions in two large data sets of
real-world e-commerce transactions. Learning under concept
drift is one of the major challenges that data-driven methods
have to face, since classiﬁers operating in these conditions
have in practice to autonomously identify the most relevant
up-to-date supervised information while ignoring the obsolete
one. Concept drift adaptation approaches can be divided into
two families: 1) active adaptation and 2) passive adaptation.
Active approaches , , , , use a changedetection test or other statistical triggers to monitor the
incoming data by analyzing the classiﬁcation error and/or the
data distribution . As soon as a change in the incoming
data is detected, adaptation is activated and the classiﬁer
is updated/retrained on recent supervised samples that are
considered to be coherent with the current state of the process.
As such, active approaches are mostly suited when the data
distribution changes abruptly, and the process generating the
data shifts through a sequence of stationary states.
In passive approaches, the classiﬁer is continuously updated
when new supervised samples become available, without
involving any explicit triggering mechanism. Ensemble methods , , , , and classiﬁers trained over
a sliding window of the recent supervised samples (like
STAGGER and FLORA ) are probably the most
extensively investigated passive solutions. Passive approaches
are the more suitable ones in gradually drifting environments, and when the supervised information is provided in
When data streams are characterized by both concept drift
and unbalanced distributions, adaptation is often achieved
by combining ensemble methods and resampling techniques , , . An alternative approach consists
in propagating the training samples of the minority class
over time , possibly undersampling the majority class.
Chen and He proposed REA that propagates examples
only from the minority class that belongs to the current
3) Alert–Feedback
Interaction
Bias: The majority of classiﬁers used for credit card fraud
detection in the literature (see , , ) are tested
in experiments where transaction labels are supposed to be
available the very next day since the transaction is authorized.
In a real-world FDS (Section II-C), the only recent supervised
information is the feedbacks Ft, provided by investigators,
while the vast majority of transactions authorized everyday do
not receive a label in a short time (|Ft| << |Tt|). Feedbacks
are not representative of the transactions processed everyday
for two main reasons: 1) feedbacks contain transactions that
are characterized by a high probability of being frauds and
2) the proportion of frauds in the feedbacks is different from
the proportion of frauds occurring everyday. Thus, feedbacks
represent a sort of biased training set: this problem evokes
what is known in the literature as SSB .
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
DAL POZZOLO et al.: CREDIT CARD FRAUD DETECTION
A biased training set may hinder the performance of
learning algorithms, since training data do not match the
distribution of the test ones. The reader can refer to for
a survey on Sample Selection Bias (SSB). Here we simply
mention that there are three different types of SSBs! (SSBs!):
class-prior bias, feature bias (also called covariate shift), and
complete bias. A standard remedy to SSB is importance
weighting , , , namely, semisupervised reweighting
techniques that assign larger weights to those training samples
that more closely resemble the data distribution in the test
set. The main idea of importance weighting is to reduce the
inﬂuence of the most biased samples in the learning process.
Ensembles of classiﬁers have also been proposed to correct
The interaction between the FDS (raising alerts) and the
investigators (providing true labels) recalls the active learning scenario , where it is possible to select few—very
informative—samples and query their labels to an oracle
which in the FDS would be the investigators. However, this is
not feasible in a real-world FDS, since investigators have to
focus on the most suspicious transactions to detect the largest
number of frauds. Requests to check (possibly genuine) transactions for obtaining informative samples would be ignored.
Considering the limited number of transactions investigators
can check, addressing these questions would necessarily imply
that some high-risk transaction is not being controlled, with
the consequent loss in detection performance.
V. PROPOSED LEARNING STRATEGY
It is important to stress that feedbacks (Ft) and delayed
samples (Dt−δ) are very different sets of supervised samples. The ﬁrst difference is quite evident: Ft provides recent
up-to-date information while Dt−δ might be already obsolete
for training a classiﬁer that is meant to analyze transactions
that will be authorized the next day. The second difference
concerns the percentage of frauds in Ft and Dt−δ: while
the class proportion in Dt−δ is heavily skewed toward the
genuine class (see the proportions of frauds in Table I), the
number of frauds in Ft actually depends on the detection
performance of Kt−1, and high precision values might even
result in Ft skewed toward frauds. The third, and probably the
most subtle, difference is that supervised couples in Ft are not
independently drawn, but are instead transactions from cards
selected by Kt−1 as those that are most likely to have been
frauded. As such, Ft is affected by SSB and any classiﬁer
trained on Ft would in principle learn how to label transactions
that are most likely to be fraudulent. Thus, this might not be in
principle precise on the vast majority of genuine transactions.
Our intuition is that feedbacks and delayed samples are
representative of two different classiﬁcation problems, and
thus they have to be separately handled. Therefore, our learning strategy consists in training a classiﬁer exclusively on
feedbacks (i.e., Ft) and a classiﬁer exclusively on delayed
supervised samples (i.e., Dt), and by aggregating their posterior probabilities when deﬁning PKt(+|xi) to determine which
transactions to alert.
In the following, we detail the proposed learning strategy,
where adaptation is performed according to a passive approach
and the classiﬁer is updated everyday on a batch containing
the latest supervised couples available, either feedbacks or
delayed samples. As in Section III, we consider a constant
veriﬁcation latency of δ days. In particular, to process the
transactions authorized at day t + 1, we rely on Q days
of feedbacks {Ft, . . . , Ft−(Q−1)}, and M days of delayed
supervised samples {Dt−δ, . . . , Dt−(δ+M−1)}, and these latter
obviously include the feedbacks received in the same days
(i.e., Fi ⊂Di, i ≤t −δ). Our learning strategy, which
is detailed in Algorithm 1, consists in separately training a
classiﬁer Ft on feedbacks
Ft = TRAIN({Ft, . . . , Ft−(Q−1)})
and a classiﬁer on delayed supervised samples
Dt = TRAIN({Dt−δ, . . . , Dt−(δ+M−1)})
and to detect frauds by the aggregation classiﬁer At, whose
posterior probability is deﬁned as
PAt (+|x) = αPFt (+|x) + (1 −α)PDt (+|x)
where 0 ≤α ≤1 is the weight parameter that balances the
contribution of Ft and Dt. Thus, the posterior probability of
the classiﬁer Kt, which alerts the transactions authorized at
day t + 1, is given by (9).
The parameters Q and M that, respectively, deﬁne how
many days of feedbacks and delayed supervised samples are
used for training our classiﬁers have to be deﬁned considering
the overall number of feedbacks and the percentage of frauds.
The training set of Ft approximately contains Q ·|Ft| samples
(a different number of feedbacks might be provided everyday)
and this has to be a sufﬁciently large number to train a
classiﬁer addressing quite a challenging classiﬁcation problem
in high dimensions. However, Q cannot be made arbitrarily
large, not to include old feedbacks. Similar considerations hold
when setting M, the considered number of days containing
delayed transactions, which have to include a sufﬁcient number
of frauds. Note that it is nevertheless possible to include in the
training set of Ft feedbacks received before δ days (Q ≥δ)
and in particular in our experiments we used Q = δ + M.
The rationale behind the proposed learning strategy is
twofold. At ﬁrst, by training a classiﬁer (7) exclusively on
feedbacks, we guarantee larger relevance to these supervised
samples, which would be otherwise outnumbered by the
delayed supervised samples. Second, we alert only those
transactions that both Ft and Dt consider most probably
frauds: this follows from the fact that, in practice, because
of the large number of transactions processed everyday, alerts
correspond to values of PAt that are very close to one. Let
us recall that Ft, thus also At, is affected by SSB due to
alert–feedback interaction. The only training samples that are
not affected by SSB are the delayed supervised samples that,
however, might be obsolete because of concept drift.
A. Implementation of the Proposed Learning Strategy
In our experiments, we implement the proposed learning
strategy in two different scenarios, which correspond to two
mainstream approaches for learning Dt. In the former, Dt is
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
Algorithm 1 Proposed Learning Strategy
Require: M and Q, i.e., the number of days of delayed
samples and feedbacks to use, respectively; Ft and Dt
classiﬁers previously trained.
Tt+1 ←transactions at day t + 1.
for each transaction x ∈Tt+1 do
compute PFt(+, x)
compute PDt(+, x)
compute PAt(+, x) as in (9)
rank Tt+1 according to PAt (+, ·),
generate alerts At.
if update the classiﬁer then
Ft+1 ←feedbacks from cards alerted in At.
Ft+1 ←TRAIN({Ft+1, . . . , Ft−Q})
Dt+1−δ ←transactions authorized at t + 1 −δ
Dt+1 ←TRAIN({Dt+1−δ, . . . , Dt−(δ+M)})
return Ft, Dt and At deﬁned as in (9).
a sliding window classiﬁer as in and , which we
denote by W D
t , while in the latter, Dt is an ensemble of
classiﬁers similar to and , which we denote by E D
Both the classiﬁers W D
are trained on delayed samples {Dt−δ, . . . , Dt−(δ+M−1)}. However, while W D
a unique model to this purpose, E D
is an ensemble of M
classiﬁers {M1, M2, . . . , MM} where each individual classiﬁer Mi is trained on delayed samples of a different day, i.e.,
Dt−δ−i, i = 0, . . . , M−1. The posterior PE D
t (+|x) is obtained
by averaging the posterior probabilities of the individual
classiﬁers, i.e., PE D
t (+|x) = (M
i PMi(+|x))/(M).
In the sliding window case, the proposed learning strategy consists in analyzing the posterior of the classiﬁer
t , which aggregates Ft and W D
t , i.e., PAW
αPFt (+|x) + (1 −α)PW D
t (+|x) as in (9). The benchmark
to compare against AW
is the classiﬁer Wt, which is trained
on all the supervised transactions referring to the same
time interval (thus mixing delayed samples and feedbacks):
{Ft, . . . , Ft−(δ−1), Dt−δ, . . . , Dt−(δ+M−1)}.
Similarly, in the ensemble case, the proposed learning
strategy consists in analyzing the posterior of the classiﬁer
t , which is obtained by aggregating the posteriors of Ft
t , i.e., PAEt (+|x) = αPFt (+|x) + (1 −α)PE D
as in (9). The benchmark to compare against AE
classiﬁer Et, whose individuals are {M1, M2, . . . , MM, Ft},
and whose posterior PEt (+|x) is estimated by averaging the
posterior probabilities of all its individuals, i.e., PEt (+|x) =
i PMi(+|x) + PFt (+|x))/(M + 1).
In both aggregations AW
t , we set α = 0.5 to give
equal contribution to the feedback and delayed classiﬁer, as
better discussed in Section VI-F. For all the base classiﬁers
involved (i.e., Ft, W D
t , Wt, Mi, i = 1, . . . , M), we adopt
RF having 100 tree each. Each tree is trained on a
balanced bootstrap sample, obtained by randomly undersampling the majority class while preserving all the minority class
samples in the corresponding training set. In this way, each tree
is trained on randomly selected genuine transactions and on the
same fraud examples. This undersampling strategy allows one
to learn trees with balanced distribution and to exploit many
subsets of the majority class. At the same time, the training
times of these classiﬁers are reasonably low. A drawback of
undersampling is that we potentially remove relevant training
samples from the data set, though this problem is mitigated
by the fact that we learn 100 different trees for each base
classiﬁer.
VI. EXPERIMENTS
Our experiments are organized as follows. In Section VI-A,
we describe data sets and in Section VI-B, we detail the
experimental settings. Section VI-C presents our ﬁrst experiment that uses the classiﬁers described in Section V-A to
assess the effectiveness of the proposed learning strategy.
In the second experiment (Section VI-D), we analyze more
than 54 millions of credit card transactions acquired over
10 months, and show that this stream is seriously affected
by concept drift. Then, to investigate the adaptation ability
of the proposed learning strategy, we synthetically introduce
an abrupt concept drift in speciﬁc locations of the transaction
stream, and assess the classiﬁcation performance. In the third
experiment (Section VI-E), we investigate the sample-selection
bias introduced by the alert–feedback interaction, and we show
that importance weighting —a conventional technique to
correct SSB—is not effective on training sets of feedbacks.
Finally, in Section VI-F, we discuss the most important
parameters inﬂuencing the proposed learning strategy.
A. Our Data Sets
We use two large data set of online e-commerce transactions
from European credit card holders, provided by our industrial
partner. Even though these transactions are not initiated from
a physical terminal, they undergo the same process described
in Fig. 1. In Table I, we provide all the information about these
data sets, which we denote as 2013 and 2014–2015, and in
particular, we stress the extreme class imbalance since frauds
account for about 0.2% of all transactions. As shown in Fig. 3,
the number of frauds per day varies signiﬁcantly over time, and
there are more fraudulent transactions than fraudulent cards,
indicating that sometimes multiple frauds are perpetrated on
the same card. The 2013 data set has also been used in 
and part of this data set have been suitably anonymized and
made publicly available for download .
To reliably assess the fraud-detection performance in terms
of Pk, we have removed the CARD_ID component from all the
feature vectors. This is very important when testing a classiﬁer
on a data set of historical transactions, since a classiﬁer that
receives in input the variable CARD_ID might learn this as
a discriminative feature to detect multiple frauds from the
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
DAL POZZOLO et al.: CREDIT CARD FRAUD DETECTION
Number of fraudulent transactions and cards per day in the data sets
described in Table I. It emerges that there are more fraudulent transactions
than cards, meaning that some cards have received more than a fraud.
(a) Number of fraudulent transactions. (b) Number of fraudulent cards.
same card in different days (thus providing too optimistic
performance). However, in a real-world FDS, it is not possible
to have multiple frauds from the same card after having
detected the ﬁrst one since, as discussed in Section II, that card
is immediately blocked. A different option would be to remove
all transactions of the same card after having detected the ﬁrst
fraud. However, this would reduce the number of available
frauds, further worsening the class imbalance in our data set.
Therefore, we consider the CARD_ID exclusively to compute
the aggregated features, and do not include it in the feature
B. Experimental Settings
In agreement with our industrial partner, we assumed that
investigators can check up to 100 cards alerted by the DDM
every day. Thus, Ft is everyday trained over Q days containing
each alerted transactions from 100 different cardholders. Let us
recall that feedbacks depend on the actual classiﬁer requesting
the labels. As such, the training set of Ft might be different
when used in At and when used standalone: in fact, in the
former case, alerts also depend on the posterior of Dt, while
in the latter, alerts are uniquely determined by Ft.
We assess the overall fraud-detection performance in our
data sets both by averaging daily performance measures
(Pk, CPk, and AUC) and also by analyzing the sum of
classiﬁers’ ranks in each day. In particular, in each day j,
we rank the S tested classiﬁers from the best to the least
performing one, and denote by rK, j ∈{1, . . . , S} the rank
of the classiﬁer K on day j: when K is the best classiﬁer,
its rank is maximum, i.e., rK, j = S, while when it is the
worst, rK, j = 1. As recommended in , we perform a
Friedman test and reject the null hypothesis that all the
Supervised information used by the classiﬁers considered in our
experiments. In this illustrative example, we set δ = 7, M
Q = 7 + 2 = 9. (a) Pooling together all labeled transactions. (b) Separating
feedbacks and delayed samples.
classiﬁers achieve the same performance. Then, we deﬁne a
global ranking by summing all the daily ranks (see Table II):
the larger the sum of ranks, the better the classiﬁer, and we
use paired t-tests to determine whether the differences in the
global ranking are signiﬁcant. In practice, for each pair of
classiﬁers K and H, a t-test is used to compare their ranks
over all days (i.e., rK, j −rH, j, j ∈{1, . . ., J}), being J the
number of days.3
Each experiment is repeated 10 times to reduce performance’s variability, and when comparing classiﬁers on
multiple days, we omit the index t from the classiﬁer notation. In most of our experiments, we consider one week of
veriﬁcation latency (δ = 7) and M = 8, such that the
overall number of feedbacks used is Q = M + δ = 15.
In Section VI-F, we repeat the experiments considering a
longer veriﬁcation latency δ = 15 and M = 15, Q = 30.
C. Separating Feedbacks From Delayed Supervised Samples
To assess the effectiveness of the proposed learning strategy,
we compare the performance of the proposed classiﬁers AW
(resp. AE) against the corresponding benchmarks introduced
in Section V-A and the classiﬁers used to deﬁne their posteriors, i.e., F and W D (resp. E D). Fig. 4 illustrates the training
set involved when using AW
and the related classiﬁers, while
Table III summarizes the most important parameters and the
training samples used by the considered classiﬁers.
In this experiment, we have also included the ideal classiﬁer
Rt that is trained on all transactions authorized between
day t and t−δ. This classiﬁer is considered an ideal counterpart
3Since the training sets used in consecutive days are largely overlapping,
the ranks are not independent and this might affect the signiﬁcance of the
tests. In fact, when a classiﬁer is outperforming others in one day, it is also
likely the same happens during the next few days. However, this is a standard
post hoc analysis for nonparametric tests, such as the Friedman test .
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
FRAUD-DETECTION PERFORMANCE WHEN USING 15 DAYS OF TRANSACTIONS (δ = 7, M = 8, AND Q = 15)
CLASSIFIERS CONSIDERED IN OUR EXPERIMENTS
of sliding window classiﬁers, which unrealistically assume that
investigators can everyday assign the correct label to each
authorized transaction. In particular, the training set of Rt is
not inﬂuenced by the alert–feedback interaction.
Table II shows the average Pk, CPk, and AUC over all
the batches for the two data sets separately. The columns
comparison report the results of the paired t-test on the ranks
described above. Classiﬁers having the same letter cannot
be considered signiﬁcantly different. In both data sets, AW
outperforms W in terms of Pk and CPk, and this indicates that
separating feedbacks and delayed samples is indeed a good
learning strategy. The same result holds for the considered
ensembles, i.e., AE and E. Since both AE and E average
the posteriors of their individuals, their difference consists
only in the aggregation weights: in AE, 50% of the total
weight is assigned to PF(+|x) and the remaining 50% is
equally distributed to the other individuals. In contrast, in E
all the individuals contribute equally. The same relation does
not hold between AW and W, which are updated in a
sliding-window manner. However, also in this case, we can
conclude that feedbacks are very informative and need to be
carefully considered to increase the alert precision. This is also
conﬁrmed by the fact that F outperforms both W D and W.
As a general comment, we note that CPk is typically lower
than Pk, since often multiple frauds are perpetrated on the
same card.
Table II also reports the results in terms of AUC, a global
ranking measure that evaluates the classiﬁer’s posteriors over
all the instances and not only in the top k (differently from CPk
and Pk). In terms of AUC, the ideal classiﬁer R is signiﬁcantly
better than AW and F is by far the worse, indicating that F
is not effective when ranking all the transactions.
We interpret these results as follows: when the goal is
to obtain an accurate ranking of the most suspicious cards
(i.e., maximize CPk), we should assign larger weights to those
transactions that are as risky as those we want to predict, hence
using AW. On the contrary, a classiﬁer trained on all daily
transactions (which are mostly genuine) is better at ranking
all the transactions, as it emerges from the area under the
ROC curve (AUC) of R. In Table II, we can also see that
R outperforms W D in terms of Pk, CPk, and AUC. This
result suggests that the stream of credit card transactions is
nonstationary. In fact, both the training sets of R and W D
contain all transactions authorized in δ = 7 and M = 8
consecutive days, respectively. Their major difference is that
R is trained on the most recent transactions, while transactions
in W D come with a lag of δ days. The fact that R outperforms
W D indicates that the most recent transactions are more
informative to detect frauds in the next days, and thus that
the distribution of transaction is nonstationary.
The standard deviations of Pk and CPk, reported in Table II,
are rather high in particular if compared with those of the
AUC. As we discussed in Section III, and as shown in Fig. 5,
the values of CPk (and Pk as well) are heavily inﬂuenced by
the number of frauds occurring every day. Since this number
heavily ﬂuctuates over time (see Fig. 3), it is reasonable to
expect such a large dispersion. We remark that the comparison
between classiﬁers in Table II indicates that differences in
terms of performance are always signiﬁcant, despite such
a large standard deviation. Note that the values of NCPk
(see Table IV) are less affected by such ﬂuctuations.
D. Concept Drift
In this section, we ﬁrst analyze the 2014–2015 data set
that contains more than 54 million transaction authorized
over 10 months and show that this stream is affected by
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
DAL POZZOLO et al.: CREDIT CARD FRAUD DETECTION
AVERAGE NCPk WHEN k ≥100 IN THE 2013 DATA SET (δ = 15)
(a) Values of CPk for S, W D, and AW on data set 2014–2015.
(b) Number of fraudulent cards on the same period. For the visualization
sake, these values have averaged over a sliding window of 15 days. The peak
of CPk in (a) corresponds to the peak in number of fraudulent cards in (b).
This result conﬁrms that the classiﬁers become more precise in those days
characterized by a large number of fraudulent cards.
concept drift. To this purpose, we use a static classiﬁer St,
which is initially trained on M days and never updated (such
that it initially coincides with W D
t ), and compare it against
(which is instead regularly updated) and AW
also leverages updated supervised samples). In a stationary
classiﬁcation problem, the two classiﬁers S and W D would
perform similarly. The fact that St outperforms W D
[see Fig. 5(a)] conﬁrms that this data set is affected by concept
drift. While it might not sound surprising that the stream of
credit card transactions is nonstationary, ours is, to the best
of our knowledge, the ﬁrst analysis on the impact of concept
drift on such a large transaction data set.
Fig. 5(a) also shows that the proposed AW always achieves
superior performance in terms of CPk, demonstrating a better
adaptation to concept drift. It is worth noting that the performances of all the classiﬁers in Fig. 5(a) ﬂuctuate quite heavily
and report a peak during February 2015. This is indeed the
month having the largest number of fraudulent cards in our
data set [which is reported in Fig. 5(b)]. In contrast, during
October 2014 (the period exhibiting the lowest number of
fraudulent cards in our data set), all the classiﬁers achieve low
values of CPk. Thus, Fig. 5 conﬁrms that the alert precision
heavily depends on the number of fraudulent cards in a day.
To further investigate the adaptation performance of AW in
nonstationary environments, we assess its adaptation abilities
with respect to an artiﬁcially introduced concept drift. In particular, we artiﬁcially introduce changes at known locations,
adding an abrupt drift on top of the (gradual) one affecting
the transaction stream, which we have previously discussed.
As in , we prepared 10 short streams by juxtaposing
transactions authorized in two nonconsecutive months. Each
of these streams contains an abrupt concept drift in the middle,
which should be more clearly perceivable when the time
distance between the juxtaposed months increases. To assess
the adaptation ability of the proposed learning strategy, we
compare the performance of AW and W D in terms of CPk.
In particular, we measure the relative performance loss due
to concept drift as the difference between the CPk in the ﬁrst
and the second month, divided by the value of CPk in the ﬁrst
month. Our experiments show that on these 10 data sets the
CPk of AW decays of 7.7%, while the CPk of W D decays
of 12.5%, conﬁrming the superior adaptation performance of
the proposed learning strategy.
E. Sample Selection Bias due to Alert–Feedback Interaction
Here we investigate whether importance weighting ,
a mainstream solution to correct SSB, can successfully compensate for the SSB introduced by the alert–feedback interaction. To this purpose, we consider the feedback classiﬁer Ft,
as this is the one mainly affected by the SSB due to
alert–feedback interaction, and use a weight-sensitive implementation of the RFs based on conditional inference trees .
Importance weighting , , consists of reweighting each training sample in Ft using the following weight:
P(s = 1|x, y)
where s is a selection variable that associates with each sample
in Tt the value 1 if the transaction is in Ft and 0 otherwise.
Thus, P(s = 1|x, y) corresponds to the probability for a
sample (x, y) to be in the training set Ft. The deﬁnition of
weights in (10) follows from the Bayes theorem and the fact
that it is possible to express (as in ) the unbiased join
distribution P(x, y) with respect to the biased joint distribution
P(x, y|s = 1) as:
P(s = 1|x, y)P(x, y|s = 1) = wP(x, y|s = 1).
Table V reports the performance achieved when correcting
the SSB using weights provided by (10) and it emerges
that these are lower than the performance achieved by F
in Table II. Importance weighting does not actually improve
the performance of F, which we interpret as a failure when
compensating for the SSB introduced by the alert–feedback
interaction.
We believe that importance weighting turns to be ineffective
since P(s = 1|x, y) and P(+|x) in (10) are highly correlated,
due to the alert–feedback interaction. It means that the more
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
AVERAGE Pk, CPk, AND AUC FOR Ft WHEN Q = 15
AVERAGE CPk WHEN USING 30 DAYS (δ = 15, M = 15, AND Q = 30)
a transaction is likely to be considered risky, the larger the
probability P(s = 1|x, y) and the lower is its weight in (10),
accordingly. Thus, importance weighting lowers the inﬂuence
of those samples within the feedbacks that are more likely to
be a fraud, and this negatively impacts on the alert precision.
As a sanity check, we repeated this experiment in a framework where recent supervised samples are not provided by the
alert–feedback interaction but are randomly selected (in the
same number and class proportions of the above experiment)
among transactions having amount larger than AC500. This
form of SSB is referred to as covariate shift , , ,
since we have P(s|y, x) = P(s|x), i.e., given the input x,
the selection variable s is independent of the class y. In this
case, importance weighting was able to correctly compensate
for this bias, and the debiased classiﬁer outperforms a similar
classiﬁer trained without correcting the SSB.
F. Inﬂuence of Parameters
Here we show how the performances of Ft and AW
inﬂuenced by: 1) the number of feedback days considered to
train our classiﬁers (i.e., Q); 2) the number of cards that are
everyday controlled by investigators; and 3) the parameter α
that regulates the aggregation classiﬁer in (9). To this purpose,
we consider δ = 15 days of veriﬁcation latency, such that
Ft is trained on 30 days of feedbacks (Q = 30, M = 15,
and δ = 15) and the delayed supervised samples come after
15 days. Table VI shows that F is better in terms of CPk when
it is trained using Q = 30 days of feedbacks than Q = 15
(see Table II). The same holds for AW, as a consequence of
the superior performance achieved by F. Therefore, the larger
amount of feedbacks used during training well compensate in
this case the increase in veriﬁcation latency.
We repeat this experiment by considering a larger number
of feedbacks per day, to show how this parameter inﬂuences
the performance of F and AW. In Table IV, we assume
that investigators are able to check more than 100 cards,
and report the fraud-detection performance in terms of NCPk
to properly assess the alert precision when more cards can
be controlled. This result conﬁrms that having more feedbacks guarantees superior fraud-detection performance. This
analysis can be considered as a guideline for companies that
have to decide whether the costs of hiring more investigators are compensated by the expected improvement in the
fraud-detection performance.
Another important parameter in our learning strategy is α,
which balances the contribution of the feedback and delayed
classiﬁers in (9). This was empirically set to 0.5 after having
investigated multiple strategies to make this parameter adaptive on a daily basis. Our idea was to take into account the
precision (or other performance measures) achieved during day
t −1 by the Ft−1 and Dt−1, and then assign weights to Ft and
Dt accordingly (the best the classiﬁer was during day t −1,
the larger the weight during day t). Unfortunately, none of the
implemented solutions seemed to outperform the average of
the two posteriors, i.e., αt = 0.5 ∀t.
Thus, we ran an extensive simulation on the sliding window
solution, where we tested everyday αt ∈{0.1, 0.2, . . . , 0.9}
and then we choose α∗
t as the one yielding the aggregation
performing at best in terms of Pk. Such an optimal selection
of weights is of course not feasible in a real-world FDS,
as it would necessary require to request feedbacks for each
αt ∈{0.1, 0.2, . . ., 0.9}. However, setting everyday α∗
minimal improvements with respect to setting αt = 0.5 ∀t.
This can be explained by the fact that α∗
had a peaked
distribution about 0.5, having mean(α∗
t ) ≈0.52. The Pk
value was steadily decreasing when approaching α = 0.1 and
α = 0.9, indicating that extreme values of α are very seldom
the best option. In these extreme cases, At approaches either
Dt or Ft (which are shown not to be the best options) and
the classiﬁer receiving the lowest weight has little chances
to request feedbacks in order to improve its performance and
increase its weight.
VII. CONCLUSION
The majority of works addressing the fraud-detection problem in credit card transactions (see , , ) unrealistically assume that the class of each transaction is immediately
provided for training the classiﬁer. Here we analyze in detail
the real-world working conditions of FDS and provide a formal
description of the articulated classiﬁcation problem involved.
In particular, we have described the alert–feedback interaction,
which is the mechanism providing recent supervised samples
to train/update the classiﬁer. We also claim that, in contrast to
traditional performance measures used in the literature, in a
real-world FDS, the precision of the reported alerts is probably
the most meaningful one, since investigators can check only
few alerts.
Our experiments on two vast data sets of real-world transactions show that, in order to get precise alerts, it is mandatory
to assign larger importance to feedbacks during the learning
problem. Not surprisingly, feedbacks play a central role in
the proposed learning strategy, which consists in separately
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
DAL POZZOLO et al.: CREDIT CARD FRAUD DETECTION
training a classiﬁer on feedbacks and a classiﬁer on delayed
supervised samples, and then aggregating their posteriors to
identify alerts. Our experiments also show that solutions that
lower the inﬂuence of feedbacks in the learning process
(e.g., classiﬁers that mix feedbacks and delayed supervised
samples or that implement instance weighting schemes) are
often returning less precise alerts.
Future work concerns the study of adaptive and possibly
nonlinear aggregation methods for the classiﬁers trained on
feedbacks and delayed supervised samples. We also expect to
further increase the alert precision by implementing a learning
to rank approach that would be speciﬁcally designed to
replace the linear aggregation of the posterior probabilities.
Finally, a very promising research direction concerns semisupervised learning methods , for exploiting in the
learning process also few recent unlabeled transactions.