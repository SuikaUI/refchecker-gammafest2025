Density-aware Single Image De-raining using a Multi-stream Dense Network
Vishal M. Patel
Department of Electrical and Computer Engineering
Rutgers University, Piscataway, NJ 08854
{he.zhang92,vishal.m.patel}@rutgers.edu
Single image rain streak removal is an extremely challenging problem due to the presence of non-uniform
rain densities in images.
We present a novel densityaware multi-stream densely connected convolutional neural
network-based algorithm, called DID-MDN, for joint rain
density estimation and de-raining. The proposed method
enables the network itself to automatically determine the
rain-density information and then efﬁciently remove the
corresponding rain-streaks guided by the estimated raindensity label. To better characterize rain-streaks with different scales and shapes, a multi-stream densely connected
de-raining network is proposed which efﬁciently leverages
features from different scales. Furthermore, a new dataset
containing images with rain-density labels is created and
used to train the proposed density-aware network. Extensive experiments on synthetic and real datasets demonstrate
that the proposed method achieves signiﬁcant improvements
over the recent state-of-the-art methods. In addition, an ablation study is performed to demonstrate the improvements
obtained by different modules in the proposed method. Code
can be found at: 
1. Introduction
In many applications such as drone-based video surveillance and self driving cars, one has to process images and
videos containing undesirable artifacts such as rain, snow,
and fog. Furthermore, the performance of many computer
vision systems often degrades when they are presented with
images containing some of these artifacts. Hence, it is important to develop algorithms that can automatically remove
these artifacts. In this paper, we address the problem of
rain streak removal from a single image. Various methods
have been proposed in the literature to address this problem
 .
One of the main limitations of the existing single image de-raining methods is that they are designed to deal
with certain types of rainy images and they do not effec-
Figure 1: Image de-raining results. (a) Input rainy image. (b)
Result from Fu et al. . (c) DID-MDN. (d) Input rainy image.
(e) Result from Li et al. . (f) DID-MDN. Note that tends to
over de-rain the image while tends to under de-rain the image.
tively consider various shapes, scales and density of rain
drops into their algorithms. State-of-the-art de-raining algorithms such as often tend to over de-rain or under
de-rain the image if the rain condition present in the test image is not properly considered during training. For example,
when a rainy image shown in Fig. 1(a) is de-rained using the
method of Fu et al. , it tends to remove some important
parts in the de-rained image such as the right arm of the
person, as shown in Fig. 1(b). Similarly, when is used
to de-rain the image shown in Fig. 1(d), it tends to under
de-rain the image and leaves some rain streaks in the output
de-rained image. Hence, more adaptive and efﬁcient methods, that can deal with different rain density levels present
in the image, are needed.
One possible solution to this problem is to build a very
large training dataset with sufﬁcient rain conditions containing various rain-density levels with different orientations
and scales. This has been achieved by Fu et al. and
Yang et al. , where they synthesize a novel large-scale
dataset consisting of rainy images with various conditions
 
and they train a single network based on this dataset for image de-raining. However, one drawback of this approach is
that a single network may not be capable enough to learn
all types of variations present in the training samples. It can
be observed from Fig. 1 that both methods tend to either
over de-rain or under de-rain results. Alternative solution
to this problem is to learn a density-speciﬁc model for deraining. However, this solution lacks ﬂexibility in practical
de-raining as the density label information is needed for a
given rainy image to determine which network to choose for
de-raining.
In order to address these issues, we propose a novel
Density-aware Image De-raining method using a Multistream Dense Network (DID-MDN) that can automatically
determine the rain-density information (i.e. heavy, medium
or light) present in the input image (see Fig. 2). The proposed method consists of two main stages: rain-density
classiﬁcation and rain streak removal. To accurately estimate the rain-density level, a new residual-aware classiﬁer
that makes use of the residual component in the rainy image for density classiﬁcation is proposed in this paper. The
rain streak removal algorithm is based on a multi-stream
densely-connected network that takes into account the distinct scale and shape information of rain streaks.
the rain-density level is estimated, we fuse the estimated
density information into our ﬁnal multi-stream denselyconnected network to get the ﬁnal de-rained output. Furthermore, to efﬁciently train the proposed network, a largescale dataset consisting of 12,000 images with different
rain-density levels/labels (i.e. heavy, medium and light) is
synthesized. Fig. 1(c) & (d) present sample results from our
network, where one can clearly see that DID-MDN does not
over de-rain or under de-rain the image and is able to provide better results as compared to and .
This paper makes the following contributions:
1. A novel DID-MDN method which automatically determines the rain-density information and then efﬁciently
removes the corresponding rain-streaks guided by the
estimated rain-density label is proposed.
2. Based on the observation that residual can be used as a
better feature representation in characterizing the raindensity information, a novel residual-aware classiﬁer
to efﬁciently determine the density-level of a given
rainy image is proposed in this paper.
3. A new synthetic dataset consisting of 12,000 training
images with rain-density labels and 1,200 test images
is synthesized. To the best of our knowledge, this is
the ﬁrst dataset that contains the rain-density label information. Although the network is trained on our synthetic dataset, it generalizes well to real-world rainy
4. Extensive experiments are conducted on three highly
challenging datasets (two synthetic and one realworld) and comparisons are performed against several
recent state-of-the-art approaches. Furthermore, an ablation study is conducted to demonstrate the effects of
different modules in the proposed network.
2. Background and Related Work
In this section, we brieﬂy review several recent related
works on single image de-raining and multi-scale feature
aggregation.
2.1. Single Image De-raining
Mathematically, a rainy image y can be modeled as a linear combination of a rain-streak component r with a clean
background image x, as follows
y = x + r.
In single image de-raining, given y the goal is to recover
x. As can be observed from (1) that image de-raining is
a highly ill-posed problem. Unlike video-based methods
 , which leverage temporal information in removing rain components, prior-based methods have been
proposed in the literature to deal with this problem. These
include sparse coding-based methods , lowrank representation-based methods and GMM-based
(gaussian mixture model) methods . One of the limitations of some of these prior-based methods is that they often
tend to over-smooth the image details .
Recently, due to the immense success of deep learning in
both high-level and low-level vision tasks , several CNN-based methods have also been proposed
for image de-raining . In these methods, the idea
is to learn a mapping between input rainy images and their
corresponding ground truths using a CNN structure.
2.2. Multi-scale Feature Aggregation
It has been observed that combining convolutional features at different levels (scales) can lead to a better representation of an object in the image and its surrounding
context . For instance, to efﬁciently leverage features obtained from different scales, the FCN (fully
convolutional network) method uses skip-connections
and adds high-level prediction layers to intermediate layers
to generate pixel-wise prediction results at multiple resolutions. Similarly, the U-Net architecture consists of
a contracting path to capture the context and a symmetric
expanding path that enables the precise localization. The
HED model employs deeply supervised structures, and
automatically learns rich hierarchical representations that
are fused to resolve the challenging ambiguity in edge and
object boundary detection. Multi-scale features have also
been leveraged in various applications such as semantic segmentation , face-alignment , visual tracking 
Figure 2: An overview of the proposed DID-MDN method. The proposed network contains two modules: (a) residual-aware rain-density
classiﬁer, and (b) multi-stream densely-connected de-raining network. The goal of the residual-aware rain-density classiﬁer is to determine
the rain-density level given a rainy image. On the other hand, the multi-stream densely-connected de-raining network is designed to
efﬁciently remove the rain streaks from the rainy images guided by the estimated rain-density information.
crowd-counting , action recognition , depth estimation , single image dehazing and also in single
image de-raining . Similar to , we also leverage a
multi-stream network to capture the rain-streak components
with different scales and shapes. However, rather than using two convolutional layers with different dilation factors
to combine features from different scales, we leverage the
densely-connected block as the building module and
then we connect features from each block together for the
ﬁnal rain-streak removal. The ablation study demonstrates
the effectiveness of our proposed network compared with
the structure proposed in .
3. Proposed Method
The proposed DID-MDN architecture mainly consists
of two modules: (a) residual-aware rain-density classiﬁer,
and (b) multi-stream densely connected de-raining network.
The residual-aware rain-density classiﬁer aims to determine
the rain-density level given a rainy image. On the other
hand, the multi-stream densely connected de-raining network is designed to efﬁciently remove the rain streaks from
the rainy images guided by the estimated rain-density information. The entire network architecture of the proposed
DID-MDN method is shown in Fig. 2.
3.1. Residual-aware Rain-density Classiﬁer
As discussed above, even though some of the previous methods achieve signiﬁcant improvements on the deraining performance, they often tend to over de-rain or under de-rain the image. This is mainly due to the fact that
a single network may not be sufﬁcient enough to learn different rain-densities occurring in practice. We believe that
incorporating density level information into the network can
beneﬁt the overall learning procedure and hence can guarantee better generalization to different rain conditions .
Similar observations have also been made in , where
they use two different priors to characterize light rain and
heavy rain, respectively. Unlike using two priors to characterize different rain-density conditions , the rain-density
label estimated from a CNN classiﬁer is used for guiding the
de-raining process. To accurately estimate the density information given a rainy input image, a residual-aware raindensity classiﬁer is proposed, where the residual information is leveraged to better represent the rain features. In
addition, to train the classier, a large-scale synthetic dataset
consisting of 12,000 rainy images with density labels is synthesized. Note that there are only three types of classes (i.e.
labels) present in the dataset and they correspond to low,
medium and high density.
One common strategy in training a new classiﬁer is to
ﬁne-tune a pre-deﬁned model such as VGG-16 , Res-net
 or Dense-net on the newly introduced dataset. One
of the fundamental reasons to leverage a ﬁne-tune strategy
for the new dataset is that discriminative features encoded
in these pre-deﬁned models can be beneﬁcial in accelerating the training and it can also guarantee better generalization. However, we observed that directly ﬁne-tuning such a
‘deep’ model on our task is not an efﬁcient solution. This is
mainly due to the fact that high-level features (deeper part)
of a CNN tend to pay more attention to localize the discriminative objects in the input image . Hence, relatively
small rain-streaks may not be localized well in these highlevel features. In other words, the rain-streak information
may be lost in the high-level features and hence may degrade the overall classiﬁcation performance. As a result, it
is important to come up with a better feature representation
to effectively characterize rain-streaks (i.e. rain-density).
From (1), one can regard r = y −x as the residual component which can be used to characterize the rain-density.
To estimate the residual component (ˆr) from the observation y, a multi-stream dense-net (without the label fusion
part) using the new dataset with heavy-density is trained.
Then, the estimated residual is regarded as the input to
train the ﬁnal classiﬁer. In this way, the residual estimation
part can be regarded as the feature extraction procedure 1,
which is discussed in Section 3.2. The classiﬁcation part
is mainly composed of three convolutional layers (Conv)
with kernel size 3 × 3, one average pooling (AP) layer
with kernel size 9×9 and two fully-connected layers (FC).
Details of the classiﬁer are as follows:
Conv(3,24)-Conv(24,64)-Conv(64,24)-AP-
FC(127896,512)-FC(512,3),
where (3,24) means that the input consists of 3 channels and
the output consists of 24 channels. Note that the ﬁnal layer
consists of a set of 3 neurons indicating the rain-density
class of the input image (i.e.
low, medium, high).
ablation study, discussed in Section 4.3, is conducted to
demonstrate the effectiveness of proposed residual-aware
classiﬁer as compared with the VGG-16 model.
Loss for the Residual-aware Classiﬁer:.
To efﬁciently
train the classiﬁer, a two-stage training protocol is leveraged. A residual feature extraction network is ﬁrstly trained
to estimate the residual part of the given rainy image, then
a classiﬁcation sub-network is trained using the estimated
residual as the input and is optimized via the ground truth
labels (rain-density). Finally, the two stages (feature extraction and classiﬁcation) are jointly optimized. The overall
loss function used to train the residual-aware classier is as
L = LE,r + LC,
where LE,r indicates the per-pixel Euclidean-loss to estimate the residual component and LC indicates the crossentropy loss for rain-density classiﬁcation.
3.2. Multi-stream Dense Network
It is well-known that different rainy images contain rainstreaks with different scales and shapes. Considering the
1Classiﬁcaiton network can be regarded as two parts: 1.Feature extractor and 2. Classifer
Figure 3: Sample images containing rain-streaks with various
scales and shapes.(a) contains smaller rain-streaks, (b) contains
longer rain-streaks.
images shown in Fig. 3, the rainy image in Fig. 3 (a) contains smaller rain-streaks, which can be captured by smallscale features (with smaller receptive ﬁelds), while the image in Fig. 3 (b) contains longer rain-streaks, which can
be captured by large-scale features (with larger receptive
ﬁelds). Hence, we believe that combining features from different scales can be a more efﬁcient way to capture various
rain streak components .
Based on this observation and motivated by the success
of using multi-scale features for single image de-raining
 , a more efﬁcient multi-stream densely-connected network to estimate the rain-streak components is proposed,
where each stream is built on the dense-block introduced in
 with different kernel sizes (different receptive ﬁelds).
These multi-stream blocks are denoted by Dense1 (7 × 7),
Dense2 (5 × 5), and Dense3 (3 × 3), in yellow, green and
blue blocks, respectively in Fig. 2. In addition, to further
improve the information ﬂow among different blocks and
to leverage features from each dense-block in estimating
the rain streak components, a modiﬁed connectivity is introduced, where all the features from each block are concatenated together for rain-streak estimation. Rather than leveraging only two convolutional layers in each stream , we
create short paths among features from different scales to
strengthen feature aggregation and to obtain better convergence. To demonstrate the effectiveness of our proposed
multi-stream network compared with the multi-scale structure proposed in , an ablation study is conducted, which
is described in Section 4.
To leverage the rain-density information to guide the deraining process, the up-sampled label map 2 is concatenated
with the rain streak features from all three streams. Then,
the concatenated features are used to estimate the residual
(ˆr) rain-streak information. In addition, the residual is subtracted from the input rainy image to estimate the coarse
de-rained image.
Finally, to further reﬁne the estimated
2For example, if the label is 1, then the corresponding up-sampled
label-map is of the same dimension as the output features from each stream
and all the pixel values of the label map are 1.
coarse de-rained image and make sure better details well
preserved, another two convolutional layers with ReLU are
adopted as the ﬁnal reﬁnement.
There are six dense-blocks in each stream. Mathematically, each stream can be represented as
sj = cat[DB1, DB2, ..., DB6],
where cat indicates concatenation, DBi, i = 1, · · · 6 denotes the output from the ith dense block, and sj, j = 1, 2, 3
denotes the jth stream. Furthermore, we adopt different
transition layer combinations3 and kernel sizes in each
stream. Details of each stream are as follows:
Dense1: three transition-down layers, three transition-up
layers and kernel size 7 × 7.
two transition-down layers, two no-sampling
transition layers, two transition-up layers and kernel size
one transition-down layer, four no-sampling
transition layers, one transition-up layer and kernel size
Note that each dense-block is followed by a transition layer.
Fig 4 presents an overview of the ﬁrst stream, Dense1.
Figure 4: Details of the ﬁrst stream Dense1.
Loss for the De-raining Network:. Motivated by the observation that CNN feature-based loss can better improve
the semantic edge information and to further enhance the visual quality of the estimated de-rained image
 , we also leverage a weighted combination of pixelwise Euclidean loss and the feature-based loss. The loss
for training the multi-stream densely connected network is
as follows
L = LE,r + LE,d + λF LF ,
where LE,d represents the per-pixel Euclidean loss function
to reconstruct the de-rained image and LF is the featurebased loss for the de-rained image, deﬁned as
CWH ∥F(ˆx)c,w,h −F(x)c,w,h∥2
where F represents a non-linear CNN transformation and ˆx
is the recovered de-rained image. Here, we have assumed
that the features are of size w × h with c channels. In our
method, we compute the feature loss from the layer relu1 2
of the VGG-16 model .
3The transition layer can function as up-sample transition, downsample transition or no-sampling transition .
3.3. Testing
During testing, the rain-density label information using
the proposed residual-aware classiﬁer is estimated. Then,
the up-sampled label-map with the corresponding input image are fed into the multi-stream network to get the ﬁnal
de-rained image.
4. Experimental Results
In this section, we present the experimental details and
evaluation results on both synthetic and real-world datasets.
De-raining performance on the synthetic data is evaluated
in terms of PSNR and SSIM . Performance of different
methods on real-world images is evaluated visually since
the ground truth images are not available. The proposed
DID-MDN method is compared with the following recent
state-of-the-art methods: (a) Discriminative sparse codingbased method (DSC) (ICCV’15), (b) Gaussian mixture
model (GMM) based method (CVPR’16), (c) CNN
method (CNN) (TIP’17), (d) Joint Rain Detection and
Removal (JORDER) method (CVPR’17), (e) Deep detailed Network method (DDN) (CVPR’17), and (f) Joint
Bi-layer Optimization (JBO) method (ICCV’17).
4.1. Synthetic Dataset
Even though there exist several large-scale synthetic
datasets , they lack the availability of the corresponding rain-density label information for each synthetic
rainy image. Hence, we develop a new dataset, denoted
as Train1, consisting of 12,000 images, where each image
is assigned a label based on its corresponding rain-density
There are three rain-density labels present in the
dataset (e.g. light, medium and heavy). There are roughly
4,000 images per rain-density level in the dataset. Similarly,
we also synthesize a new test set, denoted as Test1, which
consists of a total of 1,200 images. It is ensured that each
dataset contains rain streaks with different orientations and
scales. Images are synthesized using Photoshop. We modify the noise level introduced in step 3 of 4 to generate different rain-density images, where light, medium and heavy
rain conditions correspond to the noise levels 5% ∼35%,
35% ∼65%, and 65% ∼95%, respectively 5. Sample synthesized images under these three conditions are shown in
Fig 5. To better test the generalization capability of the proposed method, we also randomly sample 1,000 images from
the synthetic dataset provided by Fu as another testing
set, denoted as Test2.
4 
5The reason why we use three labels is that during our experiments, we
found that having more than three rain-density levels does not signiﬁcantly
improve the performance. Hence, we only use three labels (heavy, medium
and light) in the experiments.
Table 1: Quantitative results evaluated in terms of average SSIM and PSNR (dB) (SSIM/PSNR).
DSC (ICCV’15)
GMM (CVPR’16)
CNN (TIP’17)
JORDER (CVPR’17)
DDN (CVPR’17)
JBO (ICCV’17)
0.7781/21.15
0.7896/21.44
0.8352/22.75
0.8422/22.07
0.8622/24.32
0.8978/ 27.33
0.8522/23.05
0.9087/ 27.95
0.7695/19.31
0.7825/20.08
0.8105/20.66
0.8289/19.73
0.8405/22.26
0.8851/25.63
0.8356/22.45
0.9092/ 26.0745
Figure 5: Samples synthetic images in three different conditions.
Table 2: Quantitative results compared with three baseline conﬁgurations on Test1.
Yang-Multi 
Multi-no-label
Table 3: Accuracy of rain-density estimation evaluated on Test1.
VGG-16 
Residual-aware
4.2. Training Details
During training, a 512 × 512 image is randomly
cropped from the input image (or its horizontal ﬂip) of size
586×586. Adam is used as optimization algorithm with a
mini-batch size of 1. The learning rate starts from 0.001
and is divided by 10 after 20 epoch. The models are trained
for up to 80×12000 iterations.
We use a weight decay
of 0.0001 and a momentum of 0.9.
The entire network
is trained using the Pytorch framework. During training,
we set λF = 1. All the parameters are deﬁned via crossvalidation using the validation set.
4.3. Ablation Study
The ﬁrst ablation study is conducted to demonstrate the
effectiveness of the proposed residual-aware classiﬁer compared to the VGG-16 model. The two classiﬁers are
trained using our synthesized training samples Train1 and
tested on the Test1 set. The classiﬁcation accuracy corresponding to both classiﬁers on Test1 is tabulated in Table 3.
It can be observed that the proposed residual-aware classi-
ﬁer is more accurate than the VGG-16 model for predicting
the rain-density levels.
In the second ablation study, we demonstrate the effectiveness of different modules in our method by conducting
the following experiments:
• Single: A single-stream densely connected network
(Dense2) without the procedure of label fusion.
• Yang-Multi 6: Multi-stream network trained without the procedure of label fusion.
• Multi-no-label: Multi-stream densely connected network trained without the procedure of label fusion.
• DID-MDN (our):
Multi-stream Densely-connected
network trained with the procedure of estimated label
The average PSNR and SSIM results evaluated on Test1
are tabulated in Table 2. As shown in Fig. 6, even though
the single stream network and Yang’s multi-stream network
 are able to successfully remove the rain streak components, they both tend to over de-rain the image with the
blurry output. The multi-stream network without label fusion is unable to accurately estimate the rain-density level
and hence it tends to leave some rain streaks in the derained image (especially observed from the derained-part
around the light). In contrast, the proposed multi-stream
network with label fusion approach is capable of removing
rain streaks while preserving the background details. Similar observations can be made using the quantitative results
as shown in Table 2.
Results on Two Synthetic Datasets
We compare quantitative and qualitative performance of
different methods on the test images from the two synthetic
datasets - Test1 and Test2. Quantitative results corresponding to different methods are tabulated in Table 1. It can
be clearly observed that the proposed DID-MDN is able to
achieve superior quantitative performance.
To visually demonstrate the improvements obtained by
the proposed method on the synthetic dataset, results on two
sample images selected from Test2 and one sample chosen
from our newly synthesized Test1 are presented in Figure 7.
Note that we selectively sample images from all three conditions to show that our method performs well under different variations 7. While the JORDER method is able
to remove some parts of the rain-streaks, it still tends to
leave some rain-streaks in the de-rained images. Similar results are also observed from . Even though the method
6To better demonstrate the effectiveness of our proposed muli-stream
network compared with the state-of-the-art multi-scale structure proposed
in , we replace our multi-stream dense-net part with the multi-scale
structured in and keep all the other parts the same.
7Due to space limitations and for better comparisons, we only show
the results corresponding to the most recent state-of-the-art methods in the main paper. More results corresponding to the other methods
 can be found in Supplementary Material.
PSNR: 16.47
SSIM: 0.51
PSNR: 22.87
SSIM: 0.8215
PSNR: 23.02
SSIM: 0.8213
Yang-Multi 
PSNR: 23.47
SSIM: 0.8233
Multi-no-label
PSNR: 24.88
SSIM: 0.8623
Ground Truth
Figure 6: Results of ablation study on a synthetic image.
PSNR: 17.27
SSIM: 0.8257
PSNR:21.89
SSIM: 0.9007
PSNR: 25.30
SSIM:0.9455
PSNR: 20.72
SSIM: 0.8885
PSNR: 25.95
SSIM: 0.9605
PSNR:19.31
SSIM: 0.7256
PSNR:22.28
SSIM: 0.8199
PSNR:26.88
SSIM:0.8814
PSNR: 21.42
SSIM:0.7878
PSNR: 29.88
SSIM:0.9252
PSNR: 20.74
SSIM:0.7992
PSNR:24.20
SSIM:0.8502
JORDER (CVPR’17)
PSNR:29.44
SSIM:0.9429
DDN (CVPR’17)
PSNR:25.32
SSIM: 0.8922
JBO (ICCV’17)
PSNR:29.84
SSIM:0.9482
Ground Truth
Figure 7: Rain-streak removal results on sample images from the synthetic datasets Test1 and Test2.
of Fu et al. is able to remove the rain-streak, especially in the medium and light rain conditions, it tends to
remove some important details as well, such as ﬂower details, as shown in the second row and window structures
as shown in the third row (Details can be better observed
via zooming-in the ﬁgure). Overall, the proposed method
is able to preserve better details while effectively removing
the rain-streak components.
Results on Real-World Images
The performance of the proposed method is also evaluated
on many real-world images downloaded from the Internet
and also real-world images published by the authors of . The de-raining results are shown in Fig 8.
As before, previous methods either tend to under de-rain
or over de-rain the images. In contrast, the proposed method
achieves better results in terms of effectively removing rain
streaks while preserving the image details. In addition, it
can be observed that the proposed method is able to deal
with different types of rain conditions, such as heavy rain
shown in the second row of Fig 8 and medium rain shown
in the ﬁfth row of Fig 8. Furthermore, the proposed method
can effectively deal with rain-streaks containing different
shapes and scales such as small round rain streaks shown in
the third row in Fig 8 and long-thin rain-streak in the second
row in Fig 8. Overall, the results evaluated on real-world
images captured from different rain conditions demonstrate
the effectiveness and the robustness of the proposed DID-
JORDER (CVPR’17)
DDN (CVPR’17)
JBO (ICCV’17)
Figure 8: Rain-streak removal results on sample real-world images.
MDN method. More results can be found in Supplementary
Running Time Comparisons
Running time comparisons are shown in the table below. It
can be observed that the testing time of the proposed DID-
MDN is comparable to the DDN method. On average, it
takes about 0.3s to de-rain an image of size 512 × 512.
Table 4: Running time (in seconds) for different methods averaged on 1000 images with size 512×512.
JORDER (GPU)
DID-MDN (GPU)
5. Conclusion
In this paper, we propose a novel density-aware image
deraining method with multi-stream densely connected network (DID-MDN) for jointly rain-density estimation and
deraining. In comparison to existing approaches which attempt to solve the de-raining problem using a single network to learn to remove rain streaks with different densities
(heavy, medium and light), we investigated the use of estimated rain-density label for guiding the synthesis of the derained image. To efﬁciently predict the rain-density label, a
residual-aware rain-density classier is proposed in this paper. Detailed experiments and comparisons are performed
on two synthetic and one real-world datasets to demonstrate
that the proposed DID-MDN method signiﬁcantly outperforms many recent state-of-the-art methods. Additionally,
the proposed DID-MDN method is compared against baseline conﬁgurations to illustrate the performance gains obtained by each module.