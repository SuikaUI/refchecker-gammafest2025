SwinIR: Image Restoration Using Swin Transformer
Jingyun Liang1 Jiezhang Cao1 Guolei Sun1 Kai Zhang1,* Luc Van Gool1,2 Radu Timofte1
1Computer Vision Lab, ETH Zurich, Switzerland
2KU Leuven, Belgium
{jinliang, jiezcao, guosun, kai.zhang, vangool, timofter}@vision.ee.ethz.ch
 
Image restoration is a long-standing low-level vision
problem that aims to restore high-quality images from lowquality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are
based on convolutional neural networks, few attempts have
been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer.
SwinIR consists of
three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular,
the deep feature extraction module is composed of several
residual Swin Transformer blocks (RSTB), each of which
has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical,
lightweight and real-world image super-resolution), image
denoising (including grayscale and color image denoising)
and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art
methods on different tasks by up to 0.14∼0.45dB, while the
total number of parameters can be reduced by up to 67%.
1. Introduction
Image restoration, such as image super-resolution (SR),
image denoising and JPEG compression artifact reduction,
aims to reconstruct the high-quality clean image from its
low-quality degraded counterpart.
Since several revolutionary work , convolutional neural networks (CNN) have become the primary workhorse for image restoration .
Most CNN-based methods focus on elaborate architecture designs such as residual learning and dense
connections .
Although the performance is signiﬁcantly improved compared with traditional model-based
*Corresponding author.
Number of Parameters
EDSR (CVPR2017)
RNAN (ICLR2019)
OISR (CVPR2019)
RDN (CVPR2018)
RCAN (ECCV2018)
IGNN (NeurIPS2020)
HAN (ECCV2020)
NLSA (CVPR2021)
IPT (CVPR2021)
SwinIR (ours)
Figure 1: PSNR results v.s the total number of parameters of different methods for image SR (×4) on Set5 .
methods , they generally suffer from two basic
problems that stem from the basic convolution layer. First,
the interactions between images and convolution kernels are
content-independent. Using the same convolution kernel to
restore different image regions may not be the best choice.
Second, under the principle of local processing, convolution
is not effective for long-range dependency modelling.
As an alternative to CNN, Transformer designs a
self-attention mechanism to capture global interactions between contexts and has shown promising performance in
several vision problems . However, vision
Transformers for image restoration usually divide
the input image into patches with ﬁxed size (e.g., 48×48)
and process each patch independently. Such a strategy inevitably gives rise to two drawbacks. First, border pixels
cannot utilize neighbouring pixels that are out of the patch
for image restoration. Second, the restored image may introduce border artifacts around each patch. While this problem can be alleviated by patch overlapping, it would introduce extra computational burden.
Recently, Swin Transformer has shown great
promise as it integrates the advantages of both CNN and
Transformer.
On the one hand, it has the advantage of
CNN to process image with large size due to the local attention mechanism. On the other hand, it has the advantage
of Transformer to model long-range dependency with the
shifted window scheme.
 
In this paper, we propose an image restoration model,
namely SwinIR, based on Swin Transformer. More specifically, SwinIR consists of three modules: shallow feature
extraction, deep feature extraction and high-quality image
reconstruction modules. Shallow feature extraction module
uses a convolution layer to extract shallow feature, which
is directly transmitted to the reconstruction module so as to
preserve low-frequency information. Deep feature extraction module is mainly composed of residual Swin Transformer blocks (RSTB), each of which utilizes several Swin
Transformer layers for local attention and cross-window interaction. In addition, we add a convolution layer at the
end of the block for feature enhancement and use a residual connection to provide a shortcut for feature aggregation.
Finally, both shallow and deep features are fused in the reconstruction module for high-quality image reconstruction.
Compared with prevalent CNN-based image restoration
models, Transformer-based SwinIR has several beneﬁts: (1)
content-based interactions between image content and attention weights, which can be interpreted as spatially varying convolution . (2) long-range dependency
modelling are enabled by the shifted window mechanism.
(3) better performance with less parameters. For example,
as shown in Fig. 1, SwinIR achieves better PSNR with less
parameters compared with existing image SR methods.
2. Related Work
2.1. Image Restoration
Compared to traditional image restoration methods which are generally model-based, learningbased methods, especially CNN-based methods, have become more popular due to their impressive performance.
They often learn mappings between low-quality and highquality images from large-scale paired datasets. Since pioneering work SRCNN (for image SR), DnCNN 
(for image denoising) and ARCNN (for JPEG compression artifact reduction), a ﬂurry of CNN-based models have been proposed to improve model representation
ability by using more elaborate neural network architecture designs, such as residual block , dense
block and others . Some
of them have exploited the attention mechanism inside the
CNN framework, such as channel attention ,
non-local attention and adaptive patch aggregation .
2.2. Vision Transformer
Recently, natural language processing model Transformer has gained much popularity in the computer
vision community.
When used in vision problems such
as image classiﬁcation , object detection , segmentation 
and crowd counting , it learns to attend to important image regions by exploring the global interactions between different regions. Due to its impressive performance,
Transformer has also been introduced for image restoration . Chen et al. proposed a backbone model
IPT for various restoration problems based on the standard Transformer. However, IPT relies on large number of
parameters (over 115.5M parameters), large-scale datasets
(over 1.1M images) and multi-task learning for good performance. Cao et al. proposed VSR-Transformer that uses
the self-attention mechanism for better feature fusion in
video SR, but image features are still extracted from CNN.
Besides, both IPT and VSR-Transformer are patch-wise attention, which may be improper for image restoration. In
addition, a concurrent work proposed a U-shaped architecture based on the Swin Transformer .
3.1. Network Architecture
As shown in Fig. 2, SwinIR consists of three modules:
shallow feature extraction, deep feature extraction and highquality (HQ) image reconstruction modules. We employ the
same feature extraction modules for all restoration tasks, but
use different reconstruction modules for different tasks.
Shallow and deep feature extraction.
Given a lowquality (LQ) input ILQ ∈RH×W ×Cin (H, W and Cin are
the image height, width and input channel number, respectively), we use a 3 × 3 convolutional layer HSF(·) to extract
shallow feature F0 ∈RH×W ×C as
F0 = HSF(ILQ),
where C is the feature channel number. The convolution
layer is good at early visual processing, leading to more
stable optimization and better results . It also provides
a simple way to map the input image space to a higher
dimensional feature space. Then, we extract deep feature
FDF ∈RH×W ×C from F0 as
FDF = HDF(F0),
where HDF(·) is the deep feature extraction module and it
contains K residual Swin Transformer blocks (RSTB) and
a 3 × 3 convolutional layer. More speciﬁcally, intermediate
features F1, F2, . . . , FK and the output deep feature FDF are
extracted block by block as
Fi = HRSTBi(Fi−1),
i = 1, 2, . . . , K,
FDF = HCONV(FK),
where HRSTBi(·) denotes the i-th RSTB and HCONV is the
last convolutional layer. Using a convolutional layer at the
Shallow Feature
Extraction
Deep Feature Extraction
Reconstruction
(a) Residual Swin Transformer Block (RSTB)
(b) Swin Transformer Layer (STL)
Figure 2: The architecture of the proposed SwinIR for image restoration.
end of feature extraction can bring the inductive bias of the
convolution operation into the Transformer-based network,
and lay a better foundation for the later aggregation of shallow and deep features.
Image reconstruction.
Taking image SR as an example,
we reconstruct the high-quality image IRHQ by aggregating
shallow and deep features as
IRHQ = HREC(F0 + FDF),
where HREC(·) is the function of the reconstruction module. Shallow feature mainly contain low-frequencies, while
deep feature focus on recovering lost high-frequencies.
With a long skip connection, SwinIR can transmit the lowfrequency information directly to the reconstruction module, which can help deep feature extraction module focus
on high-frequency information and stabilize training. For
the implementation of reconstruction module, we use the
sub-pixel convolution layer to upsample the feature.
For tasks that do not need upsampling, such as image
denoising and JPEG compression artifact reduction, a single
convolution layer is used for reconstruction. Besides, we
use residual learning to reconstruct the residual between the
LQ and the HQ image instead of the HQ image. This is
formulated as
IRHQ = HSwinIR(ILQ) + ILQ,
where HSwinIR(·) denotes the function of SwinIR.
Loss function.
For image SR, we optimize the parameters
of SwinIR by minimizing the L1 pixel loss
L = ∥IRHQ −IHQ∥1,
where IRHQ is obtained by taking ILQ as the input of SwinIR,
and IHQ is the corresponding ground-truth HQ image. For
classical and lightweight image SR, we only use the naive
L1 pixel loss as same as previous work to show the effectiveness of the proposed network. For real-world image SR,
we use a combination of pixel loss, GAN loss and perceptual loss to improve visual quality.
For image denoising and JPEG compression artifact reduction, we use the Charbonnier loss 
∥IRHQ −IHQ∥2 + ϵ2,
where ϵ is a constant that is empirically set to 10−3.
3.2. Residual Swin Transformer Block
As shown in Fig. 2(a), the residual Swin Transformer
block (RSTB) is a residual block with Swin Transformer
layers (STL) and convolutional layers. Given the input feature Fi,0 of the i-th RSTB, we ﬁrst extract intermediate features Fi,1, Fi,2, . . . , Fi,L by L Swin Transformer layers as
Fi,j = HSTLi,j(Fi,j−1),
j = 1, 2, . . . , L,
where HSTLi,j(·) is the j-th Swin Transformer layer in the
i-th RSTB. Then, we add a convolutional layer before the
residual connection. The output of RSTB is formulated as
Fi,out = HCONVi(Fi,L) + Fi,0,
where HCONVi(·) is the convolutional layer in the i-th
RSTB. This design has two beneﬁts. First, although Transformer can be viewed as a speciﬁc instantiation of spatially
varying convolution , covolutional layers with spatially invariant ﬁlters can enhance the translational equivariance of SwinIR. Second, the residual connection provides a
identity-based connection from different blocks to the reconstruction module, allowing the aggregation of different
levels of features.
Transformer
Transformer
(STL) is based on the standard multi-head selfattention of the original Transformer layer . The main
differences lie in local attention and the shifted window
mechanism. As shown in Fig. 2(b), given an input of size
H × W × C, Swin Transformer ﬁrst reshapes the input to
M 2 × M 2 × C feature by partitioning the input into
non-overlapping M × M local windows, where HW
M 2 is the
total number of windows. Then, it computes the standard
self-attention separately for each window (i.e., local attention). For a local window feature X ∈RM 2×C, the query,
key and value matrices Q, K and V are computed as
where PQ, PK and PV are projection matrices that are
shared across different windows.
Generally, we have
Q, K, V ∈RM 2×d. The attention matrix is thus computed
by the self-attention mechanism in a local window as
Attention(Q, K, V ) = SoftMax(QKT /
d + B)V, (11)
where B is the learnable relative positional encoding. In
practice, following , we perform the attention function
for h times in parallel and concatenate the results for multihead self-attention (MSA).
Next, a multi-layer perceptron (MLP) that has two fullyconnected layers with GELU non-linearity between them is
used for further feature transformations. The LayerNorm
(LN) layer is added before both MSA and MLP, and the
residual connection is employed for both modules.
whole process is formulated as
X = MSA(LN(X)) + X,
X = MLP(LN(X)) + X.
However, when the partition is ﬁxed for different layers, there is no connection across local windows. Therefore, regular and shifted window partitioning are used alternately to enable cross-window connections , where
shifted window partitioning means shifting the feature by
2 ⌋) pixels before partitioning.
4. Experiments
4.1. Experimental Setup
For classical image SR, real-world image SR, image
denoising and JPEG compression artifact reduction, the
RSTB number, STL number, window size, channel number and attention head number are generally set to 6, 6,
8, 180 and 6, respectively. One exception is that the window size is set to 7 for JPEG compression artifact reduction, as we observe signiﬁcant performance drop when using 8, possibly because JPEG encoding uses 8 × 8 image
For lightweight image SR, we decrease RSTB
number and channel number to 4 and 60, respectively. Following , when self-ensemble strategy is used
in testing, we mark the model with a symbol “+”, e.g.,
SwinIR+. Due to page limit, training and evaluation details
are provided in the supplementary.
4.2. Ablation Study and Discussion
For ablation study, we train SwinIR on DIV2K for
classical image SR (×2) and test it on Manga109 .
Impact of channel number, RSTB number and STL
We show the effects of channel number, RSTB
number and STL number in a RSTB on model performance
in Figs. 3(a), 3(b) and 3(c), respectively. It is observed that
the PSNR is positively correlated with these three hyperparameters. For channel number, although the performance
keeps increasing, the total number of parameters grows
quadratically. To balance the performance and model size,
we choose 180 as the channel number in rest experiments.
As for RSTB number and layer number, the performance
gain becomes saturated gradually. We choose 6 for both of
them to obtain a relatively small model.
Impact of patch size and training image number; model
convergence comparison.
We compare the proposed
SwinIR with a representative CNN-based model RCAN to
compare the difference of Transformer-based and CNNbased models. From Fig.
3(d), one can see that SwinIR
performs better than RCAN on different patch sizes, and
the PSNR gain becomes larger when the patch size is larger.
Fig. 3(e) shows the impact of the number of training images.
Extra images from Flickr2K are used in training when the
percentage is larger than 100% (800 images). There are
two observations. First, as expected, the performance of
SwinIR rises with the training image number. Second, different from the observation in IPT that Transformer-based
models are heavily relied on large amount of training data,
SwinIR achieves better results than CNN-based models using the same training data, even when the dataset is small
(i.e., 25%, 200 images). We also plot the PSNR during
training for both SwinIR and RCAN in Fig. 3(f). It is clear
that SwinIR converges faster and better than RCAN, which
is contradictory to previous observations that Transformerbased models often suffer from slow model convergence.
Impact of residual connection and convolution layer in
Table 1 shows four residual connection variants
in RSTB: no residual connection, using 1 × 1 convolution layer, using 3 × 3 convolution layer and using three
3 × 3 convolution layers (channel number of the intermediate layer is set to one fourth of network channel number). From the table, we can have following observations.
First, the residual connection in RSTB is important as it
improves the PSNR by 0.16dB. Second, using 1 × 1 convolution brings little improvement maybe because it cannot
Channel Number
RSTB Number
Layer Number in a RSTB
Training Patch Size
RCAN (CNN-based)
Percentage of Used Images
RCAN (CNN-based)
Training Iterations
RCAN (CNN-based)
Figure 3: Ablation study on different settings of SwinIR. Results are tested on Manga109 for image SR (×2).
Table 1: Ablation study on RSTB design.
No residual
1 × 1 conv
3 × 3 conv
Three 3 × 3 conv
extract local neighbouring information as 3×3 convolution
does. Third, although using three 3 × 3 convolution layers can reduce the number of parameters, the performance
drops slightly.
4.3. Results on Image SR
Classical image SR.
Table 2 shows the quantitative comparisons between SwinIR (middle size) and state-of-the-art
methods: DBPN , RCAN , RRDB , SAN ,
IGNN , HAN , NLSA and IPT . As one
can see, when trained on DIV2K, SwinIR achieves best
performance on almost all ﬁve benchmark datasets for all
scale factors. The maximum PSNR gain reaches 0.26dB
on Manga109 for scale factor 4.
Note that RCAN and
HAN introduce channel and spatial attention, IGNN proposes adaptive patch feature aggregation, and NLSA is
based on the non-local attention mechanism. However, all
these CNN-based attention mechanisms perform worse than
the proposed Transformer-based SwinIR, which indicates
the effectiveness of the proposed model. When we train
SwinIR on a larger dataset (DIV2K+Flickr2K), the performance further increases by a large margin (up to 0.47dB),
achieving better accuracy than the same Transformer-based
model IPT, even though IPT utilizes ImageNet (more than
1.3M images) in training and has huge number of parameters (115.5M). In contrast, SwinIR has a small number
of parameters (11.8M) even compared with state-of-the-art
CNN-based models (15.4∼44.3M). As for runtime, representative CNN-based model RCAN, IPT and SwinIR take
about 0.2, 4.5s and 1.1s to test on a 1, 024 × 1, 024 image, respectively. Visual comparisons are show in Fig. 4.
SwinIR can restore high-frequency details and alleviate the
blurring artifacts, resulting in sharp and natural edges. In
contrast, most CNN-based methods produces blurry images
or even incorrect textures. IPT generates better images compared with CNN-based methods, but it suffers from image
distortions and border artifact.
Lightweight image SR.
We also provide comparison of
SwinIR (small size) with state-of-the-art lightweight image SR methods: CARN , FALSR-A , IMDN ,
LAPAR-A and LatticeNet . In addition to PSNR
and SSIM, we also report the total numbers of parameters and multiply-accumulate operations (evaluated on a
1280×720 HQ image) to compare the model size and computational complexity of different models. As shown in Table 3, SwinIR outperforms competitive methods by a PSNR
margin of up to 0.53dB on different benchmark datasets,
with similar total numbers of parameters and multiplyaccumulate operations. This indicates that the SwinIR architecture is highly efﬁcient for image restoration.
Real-world image SR.
The ultimate goal of image SR
is for real-world applications. Recently, Zhang et al. 
proposed a practical degradation model BSRGAN for realworld image SR and achieved surprising results in real
scenarios1.
To test the performance of SwinIR for realworld SR, we re-train SwinIR by using the same degradation model as BSRGAN for low-quality image synthesis.
Since there is no ground-truth high-quality images, we only provide visual comparison with representative bicubic model ESRGAN and state-of-the-art realworld image SR models RealSR , BSRGAN and
Real-ESRGAN .
As shown in Fig. 5, SwinIR produces visually pleasing images with clear and sharp edges,
whereas other compared methods may suffer from unsatisfactory artifacts. In addition, to exploit the full potential of SwinIR for real applications, we further propose a
1 
Table 2: Quantitative comparison (average PSNR/SSIM) with state-of-the-art methods for classical image SR on benchmark datasets. Best and second best performance are in red and blue colors, respectively. Results on ×8 are provided in
supplementary.
Set14 
BSD100 
Urban100 
Manga109 
IGNN 
SwinIR (Ours)
SwinIR+ (Ours)
DIV2K+Flickr2K
SwinIR (Ours)
DIV2K+Flickr2K
SwinIR+ (Ours)
DIV2K+Flickr2K
IGNN 
SwinIR (Ours)
SwinIR+ (Ours)
SwinIR (Ours)
DIV2K+Flickr2K
SwinIR+ (Ours)
DIV2K+Flickr2K
IGNN 
SwinIR (Ours)
SwinIR+ (Ours)
DIV2K+Flickr2K
DIV2K+Flickr2K
SwinIR (Ours)
DIV2K+Flickr2K
SwinIR+ (Ours)
DIV2K+Flickr2K
Urban100 (4×):img 012
IGNN 
SwinIR (ours)
Figure 4: Visual comparison of bicubic image SR (×4) methods. Compared images are derived from . Best viewed by zooming.
Table 3: Quantitative comparison (average PSNR/SSIM) with state-of-the-art methods for lightweight image SR on benchmark datasets. Best and second best performance are in red and blue colors, respectively.
#Mult-Adds
Set14 
BSD100 
Urban100 
Manga109 
FALSR-A 
LAPAR-A 
LatticeNet 
SwinIR (Ours)
LAPAR-A 
LatticeNet 
SwinIR (Ours)
LAPAR-A 
LatticeNet 
SwinIR (Ours)
ESRGAN 
RealSR 
BSRGAN 
Real-ESRGAN 
SwinIR (ours)
Figure 5: Visual comparison of real-world image SR (×4) methods on real-world images.
Quantitative
comparison
PSNR/SSIM/PSNR-B)
state-of-the-art
JPEG compression artifact reduction on benchmark datasets.
Best and second best performance are in red and
blue colors, respectively.
ARCNN 
DnCNN-3 
DRUNet 
SwinIR (ours)
29.03/0.7929/28.76
29.40/0.8026/29.13
29.84/0.8370/29.43
29.96/0.8178/29.62
30.00/0.8188/-
30.16/0.8234/29.81
30.27/0.8249/29.95
31.15/0.8517/30.59
31.63/0.8610/31.19
31.98/0.8850/31.37
32.11/0.8693/31.57
32.15/0.8699/-
32.39/0.8734/31.80
32.52/0.8748/31.99
32.51/0.8806/31.98
32.91/0.8861/32.38
33.22/0.9070/32.42
33.38/0.8924/32.68
33.43/0.8930/-
33.59/0.8949/32.82
33.73/0.8961/33.03
33.32/0.8953/32.79
33.77/0.9003/33.20
34.27/0.9061/33.4
34.27/0.9061/-
34.41/0.9075/33.51
34.52/0.9082/33.66
28.96/0.8076/28.77
29.19/0.8123/28.90
29.53/0.8400/29.15
29.63/0.8239/29.25
29.67/0.8247/-
29.79/0.8278/29.48
29.86/0.8287/29.50
31.29/0.8733/30.79
31.59/0.8802/31.07
31.86/0.9010/31.27
32.03/0.8877/31.44
32.07/0.8882/-
32.17/0.8899/31.69
32.25/0.8909/31.70
32.67/0.9043/32.22
32.98/0.9090/32.34
33.23/0.9250/32.50
33.45/0.9149/32.71
33.51/0.9153/-
33.59/0.9166/32.99
33.69/0.9174/33.01
33.63/0.9198/33.14
33.96/0.9247/33.28
34.47/0.9299/33.66
34.51/0.9302/-
34.58/0.9312/33.93
34.67/0.9317/33.88
large model and train it on much larger datasets. Experiments show that it can deal with more complex corruptions and achieves even better performance on real-world
images than the current model. Due to page limit, the details
are given in our project page 
JingyunLiang/SwinIR.
4.4. Results on JPEG Compression Artifact Reduction
Table 4 shows the comparison of SwinIR with stateof-the-art JPEG compression artifact reduction methods:
ARCNN , DnCNN-3 , QGAC , RNAN ,
RDN and DRUNet . All of compared methods
are CNN-based models. Following , we test different methods on two benchmark datasets (Classic5 
and LIVE1 ) for JPEG quality factors 10, 20, 30 and
40. As we can see, the proposed SwinIR has average PSNR
gains of at least 0.11dB and 0.07dB on two testing datasets
for different quality factors. Besides, compared with the
previous best model DRUNet, SwinIR only has 11.5M parameters, while DRUNet is a large model that has 32.7M
parameters.
4.5. Results on Image Denoising
We show grayscale and color image denoising results in Table 5 and Table 6,
respectively.
Compared methods include traditional models BM3D 
and WNNM , CNN-based models DnCNN , IR-
CNN , FFDNet , N3Net , NLRN , FOC-
Net , RNAN , MWCNN and DRUNet .
Following , the compared noise levels include 15,
25 and 50. As one can see, our model achieves better performance than all compared methods. In particular, it surpasses the state-of-the-art model DRUNet by up to 0.3dB
on the large Urban100 dataset that has 100 high-resolution
testing images. It is worth pointing out that SwinIR only
has 12.0M parameters, whereas DRUNet has 32.7M parameters. This indicates that the SwinIR architecture is highly
efﬁcient in learning feature representations for restoration.
The visual comparison for grayscale and color image denoising of different methods are shown in Figs. 6 and 7.
As we can see, our method can remove heavy noise corruption and preserve high-frequency image details, resulting in sharper edges and more natural textures. By contrast,
other methods suffer from either over-smoothness or oversharpness, and cannot recover rich textures.
5. Conclusion
In this paper, we propose a Swin Transformer-based image restoration model SwinIR. The model is composed of
three parts: shallow feature extraction, deep feature extrac-
Table 5: Quantitative comparison (average PSNR) with state-of-the-art methods for grayscale image denoising on benchmark datasets. Best and second best performance are in red and blue colors, respectively.
SwinIR (ours)
Table 6: Quantitative comparison (average PSNR) with state-of-the-art methods for color image denoising on benchmark
datasets. Best and second best performance are in red and blue colors, respectively.
SwinIR (ours)
DnCNN 
FFDNet 
DRUNet 
SwinIR (ours)
Figure 6: Visual comparison of grayscale image denoising (noise level 50) methods on image “Monarch” from Set12 . Compared
images are derived from .
DnCNN 
FFDNet 
DRUNet 
SwinIR (ours)
Figure 7: Visual comparison of color image denoising (noise level 50) methods on image “163085” from CBSD68 . Compared images
are derived from .
tion and HR reconstruction modules. In particular, we use a
stack of residual Swin Transformer blocks (RSTB) for deep
feature extraction, and each RSTB is composed of Swin
Transformer layers, convolution layer and a residual connection. Extensive experiments show that SwinIR achieves
state-of-the-art performance on three representative image
restoration tasks and six different settings: classic image
SR, lightweight image SR, real-world image SR, grayscale
image denoising, color image denoising and JPEG compression artifact reduction, which demonstrates the effectiveness and generalizability of the proposed SwinIR. In the
future, we will extend the model to other restoration tasks
such as image deblurring and deraining.
Acknowledgements
This paper was partially supported
by the ETH Zurich Fund (OK), a Huawei Technologies Oy
(Finland) project, the China Scholarship Council and an
Amazon AWS grant. Special thanks goes to Yijue Chen.