Journal of Artiﬁcial Intelligence Research 27 203-233
Submitted 01/06; published 10/06
Active Learning with Multiple Views
Ion Muslea
 
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, CA 90292
Steven Minton
 
Fetch Technologies, Inc.
2041 Rosecrans Ave., Suite 245
El Segundo, CA 90245
Craig A. Knoblock
 
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292
Active learners alleviate the burden of labeling large amounts of data by detecting and
asking the user to label only the most informative examples in the domain. We focus here
on active learning for multi-view domains, in which there are several disjoint subsets of
features (views), each of which is suﬃcient to learn the target concept. In this paper we
make several contributions. First, we introduce Co-Testing, which is the ﬁrst approach
to multi-view active learning. Second, we extend the multi-view learning framework by
also exploiting weak views, which are adequate only for learning a concept that is more
general/speciﬁc than the target concept.
Finally, we empirically show that Co-Testing
outperforms existing active learners on a variety of real world domains such as wrapper
induction, Web page classiﬁcation, advertisement removal, and discourse tree parsing.
Introduction
Labeling the training data for a machine learning algorithm is a tedious, time consuming,
error prone process; furthermore, in some application domains, the labeling of each example
may also be extremely expensive (e.g., it may require running costly laboratory tests).
Active learning algorithms cope with this problem by detecting and asking the user to label only the
most informative examples in the domain, thus reducing the user’s involvement in the data
labeling process.
In this paper, we introduce Co-Testing, an active learning technique for multi-view
learning tasks; i.e., tasks that have several disjoint subsets of features (views), each of
which is suﬃcient to learn the concepts of interest. For instance, Web page classiﬁcation
is a multi-view task because Web pages can be classiﬁed based on the words that appear
either in the documents or in the hyperlinks pointing to them ;
similarly, one can classify segments of televised broadcast based either on the video or on
the audio information, or one can perform speech recognition based on either sound or lip
motion features .
c⃝2006 AI Access Foundation. All rights reserved.
Muslea, Minton, & Knoblock
Co-Testing is a two-step iterative algorithm that requires as input a few labeled and
many unlabeled examples.
First, Co-Testing uses the few labeled examples to learn a
hypothesis in each view. Then it applies the learned hypotheses to all unlabeled examples
and detects the set of contention points (i.e., unlabeled examples on which the views predict
a diﬀerent label); ﬁnally, it queries (i.e., asks the user to label) one of the contention
points, adds the newly labeled example to the training set, and repeats the whole process.
Intuitively, Co-Testing relies on the following observation: if, for an unlabeled example,
the hypotheses learned in each view predict a diﬀerent label, at least one of them makes a
mistake on that particular prediction. By asking the user to label such a contention point,
Co-Testing is guaranteed to provide useful information for the view that made the mistake.
In this paper we make several contributions. First, we introduce Co-Testing, a family of
active learners for multi-view learning tasks. Second, we extend the traditional multi-view
learning framework by also allowing the use of weak views, in which one can adequately learn
only a concept that is strictly more general or more speciﬁc than the target concept (all
previous multi-view work makes the strong view assumption that each view is adequate for
learning the target concept). Last but not least, we show that, in practice, Co-Testing clearly
outperforms existing active learners on a variety of real world domains such as wrapper
induction, Web page classiﬁcation, advertisement removal, and discourse tree parsing.
Compared with previous work, Co-Testing is unique in several ways:
1. existing multi-view approaches , which also use a small set of labeled and a large set of unlabeled
examples, are based on the idea of bootstrapping the views from each other. In contrast, Co-Testing is the ﬁrst algorithm that exploits multiple views for active learning
purposes. Furthermore, Co-Testing allows the simultaneous use of strong and weak
views without additional data engineering costs.
2. existing active learners, which pool all domain features together, are typically designed
to exploit some properties speciﬁc to a particular (type of) base learner (i.e., the algorithm used to learn the target concept); for example, uncertainty reduction methods
assume that the base learner provides a reliable estimate of its conﬁdence in each
prediction. In contrast, Co-Testing uses the multiple views to detect the contention
points, among which it chooses the next query. This approach has several advantages:
- it converges quickly to the target concept because it is based on the idea of learning
from mistakes (remember that each contention point is guaranteed to represent
a mistake in at least one of the views). In contrast, existing active learners often
times query examples that are classiﬁed correctly, but with a low conﬁdence.
- in its simplest form (i.e., Naive Co-Testing, which is described in section 4), it
makes no assumptions about the properties of the base learner. More precisely,
by simply querying an arbitrary contention point, Co-Testing is guaranteed to
provide “the mistaken view” with a highly informative example.
- by considering only the contention points as query candidates, it allows the use of
query selection heuristics that - computationally - are too expensive to be applied
to the entire set of unlabeled examples.
Active Learning with Multiple Views
The remainder of the paper is organized as follows. First, we introduce the concepts
and notation, followed by a comprehensive survey of the literature on active and multi-view
learning. Then we formally introduce the Co-Testing family of algorithms and we present
our empirical evaluation on a variety of real-world domains.
Preliminaries: Terminology and Notation
For any given learning task, the set of all possible domain examples is called the instance
space and is denoted by X. Any x ∈X represents a particular example or instance. In this
paper we are concerned mostly with examples that are represented as feature vectors that
store the values of the various attributes or features that describe the example.
The concept to be learned is called the target concept, and it can be seen as a function
c : X →{l1, l2, . . . , lN} that classiﬁes any instance x as a member of one of the N classes of
interest l1, l2, . . . , lN. In order to learn the target concept, the user provides a set of training
examples, each of which consists of an instance x ∈X and its label, c(x). The notation
⟨x, c(x)⟩denotes such a training example. The symbol L is used to denote the set of labeled
training examples (also known as the training set).
Given a training set L for the target concept c, an inductive learning algorithm L
searches for a function h : X →{l1, l2, . . . , lN} such that ∀x ∈X, h(x) = c(x). The learner
L searches for h within the set H of all possible hypotheses, which is (typically) determined
by the person who designs the learning algorithm. A hypothesis h is consistent with the
training set L if and only if ∀⟨x, c(x)⟩∈L, h(x) = c(x). Finally, the version space V SH,L
represents the subset of hypotheses in H that are consistent with the training set L.
By deﬁnition, a passive learning algorithm takes as input a randomly chosen training
set L. In contrast, active learning algorithms have the ability to choose the examples in L.
That is, they detect the most informative examples in the instance space X and ask the
user to label only them; the examples that are chosen for labeling are called queries. In
this paper we focus on selective sampling algorithms, which are active learners that choose
the queries from a given working set of unlabeled examples U (we use the notation ⟨x, ?⟩
to denote an unlabeled examples). In this paper the terms active learning and selective
sampling are used interchangeably.
In the traditional, single-view machine learning scenario, a learner has access to the entire
set of domain features. By contrast, in the multi-view setting one can partition the domain’s
features in subsets (views) that are suﬃcient for learning the target concept. Existing multiview learners are semi-supervised algorithms: they exploit unlabeled examples to boost the
accuracy of the classiﬁers learned in each view by bootstrapping the views from each other.
In multi-view learning, an example x is described by a diﬀerent set of features in each
view. For example, in a domain with k views V1, V2, . . . Vk, a labeled example can be seen
as a tuple ⟨x1, x2, . . . , xk, l⟩, where l is its label, and x1, x2, . . . , xk are its descriptions in
the k views. Similarly, a k-view unlabeled example is denoted by ⟨x1, x2, . . . , xk, ?⟩. For
any example x, Vi(x) denotes the descriptions xi of x in Vi. Similarly, Vi(L) consists of the
descriptions in Vi of all the examples in L.
Muslea, Minton, & Knoblock
Background on Active and Multi-view Learning
Active learning can be seen as a natural development from the earlier work on optimum
experimental design . In the early 1980s, the machine learning community
started recognizing the advantages of inductive systems that are capable of querying their
instructors. For example, in order to detect errors in Prolog programs, the Algorithmic
Debugging System was allowed to ask the user several types of queries.
Similarly, concept learning systems such as Marvin and cat
 used queries as an integral part of their respective learning strategies.
Our literature review below is structured as follows. First, we discuss the early, mostly
theoretical results on query construction. Then we focus on selective sampling algorithms,
which select as the next query one of the unlabeled examples from the working set. Finally,
we conclude by reviewing the existing multi-view learning algorithms.
Active Learning by Query Construction
The earliest approaches to formalizing active learning appeared in the seminal papers of
Angluin and Valiant , who focused on exact concept induction and
learning in the pac framework, respectively. This theoretic work focused on learning classes
of concepts such as regular sets, monotone dnf expressions, and µ−expressions. Besides
membership queries such as “is this an example of the target concept?,” Angluin also used more
sophisticated types of queries such as equivalence queries (“is this concept equivalent with the
target concept?”) or superset queries (“is this concept a superset of the target concept?”).
These early active learners took a constructive approach to query generation in the
sense that each query is (artiﬁcially) constructed by setting the values of the attributes
so that the query is as informative as possible. In practice, this may raise some serious
problems; for example, consider a hand-writing recognizer that must discriminate between
the 10 digits . In this scenario, an informative query may consist of an
image that represents a “fusion” of two similarly-looking digits, such as “3” and “5.” When
presented with such an image, a user cannot label it properly because it does not represent
a recognizable digit.
Consequently, a query is “wasted” on a totally irrelevant image.
Similar situations appear in many real world tasks such as text classiﬁcation, information
extraction, or speech recognition: whenever the active learner artiﬁcially builds a query for
such a domain, it is highly unlikely that the newly created object has any meaning for the
human user.
Despite this practical applicability issue, the constructive approach to active learning
leads to interesting theoretical insights about the merits of various types of queries. For
example, researchers considered learning with:
- incomplete queries, for which the query’s answer may be “I don’t know.” ;
- malicious queries, for which the answer to the queries may be erroneous .
New learning problems were also considered, from unrestricted dnf expressions and unions of boxes
Active Learning with Multiple Views
 to tree patterns and Horn clauses . Researchers also reported results on
applying active learning to neural networks and for combining declarative bias (prior
knowledge) and active learning .
Selective Sampling
Selective sampling represents an alternative active learning approach. It typically applies to
classiﬁcation tasks in which the learner has access to a large number of unlabeled examples.
In this scenario, rather than constructing an informative query, the active learner asks the
user to label one of the existing unlabeled examples. Depending on the source of unlabeled
examples, there are two main types of sampling algorithms: stream- and pool- based.
The former assumes that the active learner has access to an (inﬁnite) stream of unlabeled
examples ; as successive examples are presented to it, the active learner
must decide which of them should be labeled by the user. In contrast, in the pool-based
scenario , the learner is presented with a working set of unlabeled
examples; in order to make a query, the active learner goes through the entire pool and
selects the example to be labeled next.
Based on the criterion used to select the next query, selective sampling algorithms fall
under three main categories:
- uncertainty reduction: the system queries the example on which the current hypothesis
makes the least conﬁdent prediction;
- expected-error minimization: the system queries the example that maximizes the expected
reduction in classiﬁcation error;
- version space reduction: the system queries the example that, once labeled, removes as
much as possible of the version space.
The uncertainty reduction approach to selective sampling works as follows: ﬁrst, one uses
the labeled examples to learn a classiﬁer; then the system queries the unlabeled example
on which this classiﬁer makes the least conﬁdent prediction. This straightforward idea can
be applied to any base learner for which one can reliably estimate the conﬁdence of its
predictions. Conﬁdence-estimation heuristics were proposed for a variety of base learners
such as logistic regression , partially hidden
Markov Models , support vector machines , and inductive logic programming .
The second, more sophisticated approach to selective sampling, expected-error minimization, is based on the statistically optimal solution to the active learning problem. In
this scenario, the intuition is to query the unlabeled example that minimizes the error rate
of the (future) classiﬁer on the test set. Even though for some (extremely simple) base
learners one can ﬁnd such optimal queries , this is not
Muslea, Minton, & Knoblock
true for most inductive learners. Consequently, researchers proposed methods to estimate
the error reduction for various types of base learners. For example, Roy and McCallum
 use a sample estimation method for the Naive Bayes classiﬁer; similar approaches
were also described for parameter learning in Bayesian nets and for
nearest neighbor classiﬁers .
The heuristic approach to expected-error minimization can be summarized as follows.
First, one chooses a loss function that is used to estimate the
future error rate. Then each unlabeled example x in the working set is considered as the
possible next query, and the system estimates the expected reduction of the error rate for
each possible label that x may take. Finally, the system queries the unlabeled example that
leads to the largest estimated reduction in the error rate.
Finally, a typical version space reduction active learner works as follows: it generates
a committee of several hypotheses, and it queries the unlabeled examples on which the
disagreement within the committee is the greatest. In a two-class learning problem, this
strategy translates into making queries that remove approximately half of the version space.
Depending on the method used to generate the committee, one can distinguish several types
of active learners:
- Query-by-Committee selects a committee by randomly sampling hypotheses from the
version space. Query-by-Committee was applied to a variety of base learners such as
perceptrons , Naive Bayes , and Winnow . Furthermore, Argamon-Engelson and Dagan introduce an extension to Query-by-Committee for Bayesian learning. In the
Bayesian framework, one can create the committee by sampling classiﬁers according
to their posterior distributions; that is, the better a hypothesis explains the training
data, the more likely it is to be sampled. The main limitation of Query-by-Committee
is that it can be applied only to base learners for which it is feasible to randomly sample hypotheses from the version space.
- sg-net creates a 2-hypothesis committee that consists of a “mostgeneral” and a “most-speciﬁc” classiﬁer. These two hypotheses are generated by modifying the base learner so that it learns a classiﬁer that labels as many as possible of
the unlabeled examples in the working set as positive or negative, respectively. This
approach has an obvious drawback: it requires the user to modify the base learner so
that it can generate “most-general” and “most-speciﬁc” classiﬁers.
- Active-Decorate can be seen as both a generalization and an
improvement of sg-net. It generates a large and diverse committee by successively
augmenting the original training set with additional sets of artiﬁcially-generated examples. More precisely, it generates artiﬁcial examples in keeping with the distribution
of the instance space; then it applies the current committee to each such example, and
it labels the artiﬁcial example with the label that contradicts most of the committee’s
predictions. A new classiﬁer is learned from this augmented dataset, and then the
entire process is repeated until the desired committee size is reached. Active-Decorate
was successfully used for domains with nominal and numeric features, but it is unclear
how it could be applied to domains such as text classiﬁcation or extraction, where
generating the artiﬁcial examples may be problematic.
Active Learning with Multiple Views
- Query-by-Bagging and Query-by-Boosting create the committee by using the well-known bagging and boosting 
algorithms, respectively. These algorithms were introduced for the c4.5 base learner,
for which both bagging and boosting are known to work extremely well.
In general, committee-based sampling tends to be associated with the version space
reduction approach. However, for base learners such as support vector machines, one can
use a single hypothesis to make queries that remove (approximately) half of the version
space . Conversely, committee-based sampling can also be seen as
relying on the uncertainty reduction principle: after all, the unlabeled example on which
the disagreement within the committee is the greatest can be also seen as the example that
has the least certain classiﬁcation.
Multi-view, Semi-supervised Learning
As already mentioned, Blum and Mitchell provided the ﬁrst formalization of learning
in the multi-view framework. Previously, this topic was largely ignored, though the idea
clearly shows up in applications such as word sense disambiguation and
speech recognition . Blum and Mitchell proved that two independent, compatible views can be used to pac-learn a concept based on few
labeled and many unlabeled examples. They also introduced Co-Training, which is the ﬁrst
general-purpose, multi-view algorithm.
Collins and Singer proposed a version of Co-Training that is biased towards
learning hypotheses that predict the same label on most of the unlabeled examples. They
introduce an explicit objective function that measures the compatibility of the learned
hypotheses and use a boosting algorithm to optimize this objective function. In a related
paper , the authors provide pac-like guarantees
for this novel Co-Training algorithm (the assumption is, again, that the views are both
independent and compatible). Intuitively, Dasgupta et al. show that the ratio of
contention points to unlabeled examples is an upper-bound on the error rate of the classiﬁers
learned in the two views.
Abney extends the work of Dasgupta et al. by relaxing the view independence
assumption. More precisely, the author shows that even with views that are weakly dependent, the ratio of contention points to unlabeled examples still represents an upper-bound
on the two view’s error rate. Unfortunately, this paper introduces just a theoretical deﬁnition for the weak dependence of the views, without providing an intuitive explanation of its
practical consequences.
Researchers proposed two main types of extensions to the original Co-Training algorithm: modiﬁcations of the actual algorithm and changes aiming to extend its practical
applicability. The former cover a wide variety of scenarios:
- Co-EM uses Expectation Maximization
 for multi-view learning. Co-EM can be seen as the
closest implementation of the theoretical framework proposed by Blum and Mitchell
Muslea, Minton, & Knoblock
- Ghani uses Error-Correcting Output Codes to allow Co-Training and Co-EM to
scale up to problems with a large number of classes.
- Corrected Co-Training asks the user to manually correct the
labels of the bootstrapped examples. This approach is motivated by the observation
that the quality of the bootstrapped data is crucial for Co-Training’s convergence.
- Co-Boost and Greedy Agreement are Co-Training
algorithms that explicitly aim to minimize the number of contention points.
The second group of extensions to Co-Training is motivated by the fact that, in practice,
one also encounters many problems for which there is no straightforward way to split the
features in two views.
In order to cope with this problem, Goldman and Zhou 
advocate the use of multiple biases instead of multiple views. The authors introduce an
algorithm similar to Co-Training, which bootstraps from each other hypotheses learned by
two diﬀerent base learners; this approach relies on the assumption that the base learners
generate hypotheses that partition the instance space into equivalence classes. In a recent
paper, Zhou and Goldman
 use the idea of a multi-biased committee for active
learning; i.e., they use various types base learners to obtain a diverse committee, and then
query the examples on which this committee disagree the most.
Within the multi-view framework, Nigam and Ghani show that, for “bag-ofwords” text classiﬁcation, one can create two views by arbitrarily splitting the original set
of features into two sub-sets. Such an approach ﬁts well the text classiﬁcation domain, in
which the features are abundant, but it is unlikely to work on other types of problems.
An alternative solution is proposed by Raskutti, Ferra, and Kowalczyk , where the
authors create a second view that consists of a variety of features that measure the examples’
similarity with the N largest clusters in the domain. Finally, Muslea, Minton, and Knoblock
 propose a meta-learning approach that uses past experiences to predict whether
the given views are appropriate for a new, unseen learning task.
The Co-Testing Family of Algorithms
In this section, we discuss in detail the Co-Testing family of algorithms. As we already
mentioned, Co-Testing can be seen as a two-step iterative process: ﬁrst, it uses a few
labeled examples to learn a hypothesis in each view; then it queries an unlabeled example
for which the views predict diﬀerent labels. After adding the queried example to the training
set, the entire process is repeated for a number of iterations.
The remainder of this section is organized as follows: ﬁrst, we formally present the Co-
Testing family of algorithms and we discuss several of its members. Then we introduce the
concepts of strong and weak views, and we analyze how Co-Testing can exploit both types
of views (previous multi-view learners could only use strong views). Finally, we compare
and contrast Co-Testing to the related approaches.
The Family of Algorithms
Table 1 provides a formal description on the Co-Testing family of algorithms. The input
consists of k views V1, V2, . . . , Vk, a base learner L, and the sets L and U of labeled and
Active Learning with Multiple Views
Table 1: The Co-Testing family of algorithms: repeatedly learn a classiﬁer in each view and
query an example on which they predict diﬀerent labels.
- a base learner L
- a learning domain with features V = {a1, a2, . . . , aN}
- k views V1, V2, . . . , Vk such that V =
Vi and ∀i, j ∈{1, 2, . . . , k}, i ̸= j, Vi ∩Vj = Ø
- the sets L and U of labeled and unlabeled examples, respectively
- number N of queries to be made
- LOOP for N iterations
- use L to learn the classiﬁers h1, h2, . . . , hk in the views V1, V2, . . . , Vk, respectively
- let ContentionPoints = { ⟨x1, x2, . . . , xk, ?⟩∈U | ∃i, j hi(xi) ̸= hj(xj) }
- let ⟨x1, x2, . . . , xk, ?⟩= SelectQuery(ContentionPoints)
- remove ⟨x1, x2, . . . , xk, ?⟩from U and ask for its label l
- add ⟨x1, x2, . . . , xk, l⟩to L
- hOUT = CreateOutputHypothesis( h1, h2, . . . , hk )
unlabeled examples, respectively. Co-Testing algorithms work as follows: ﬁrst, they learn
the classiﬁers h1, h2, . . . , hk by applying the algorithm L to the projection of the examples in
L onto each view. Then they apply h1, h2, . . . , hk to all unlabeled examples in U and create
the set of contention points, which consists of all unlabeled examples for which at least two
of these hypotheses predict a diﬀerent label. Finally, they query one of the contention points
and then repeat the whole process for a number of iterations. After making the allowed
number of queries, Co-Testing creates an output hypothesis that is used to make the actual
predictions.
The various members of the Co-Testing family diﬀer from each other with two respects:
the strategy used to select the next query, and the manner in which the output hypothesis
is constructed. In other words, each Co-Testing algorithm is uniquely deﬁned by the choice
of the functions SelectQuery() and CreateOutputHypothesis().
In this paper we consider three types of query selection strategies:
- naive: choose at random one of the contention points.
This straightforward strategy
is appropriate for base learners that lack the capability of reliably estimating the
conﬁdence of their predictions. As this naive query selection strategy is independent
of both the domain and the base learner properties, it follows that it can be used for
solving any multi-view learning task.
- aggressive: choose as query the contention point Q on which the least conﬁdent of the
hypotheses h1, h2, . . . , hk makes the most conﬁdent prediction; more formally,
Muslea, Minton, & Knoblock
x∈ContentionPoints
i∈{1,2,...,k} Confidence(hi(x))
Aggressive Co-Testing is designed for high accuracy domains, in which there is little
or no noise. On such domains, discovering unlabeled examples that are misclassiﬁed
“with high conﬁdence” translates into queries that remove signiﬁcantly more than half
of the version space.
- conservative: choose the contention point on which the conﬁdence of the predictions
made by h1, h2, . . . , hk is as close as possible (ideally, they would be equally conﬁdent
in predicting diﬀerent labels); that is,
x∈ContentionPoints
f∈{h1,...,hk} Confidence(f(x)) −
g∈{h1,...,hk} Confidence(g(x))
Conservative Co-Testing is appropriate for noisy domains, where the aggressive strategy may end up querying mostly noisy examples.
Creating the output hypothesis also allows the user to choose from a variety of alternatives, such as:
- weighted vote: combines the vote of each hypothesis, weighted by the conﬁdence of their
respective predictions.
hOUT (x) = arg max
g ∈{h1, . . . , hk}
Confidence(g(x))
- majority vote: Co-Testing chooses the label that was predicted by most of the hypotheses
learned in the k views.
hOUT (x) = arg max
g ∈{h1, . . . , hk}
This strategy is appropriate when there are at least three views, and the base learner
cannot reliably estimate the conﬁdence of its predictions.
- winner-takes-all: the output hypothesis is the one learned in the view that makes the
smallest number of mistakes over the N queries. This is the most obvious solution for
2-view learning tasks in which the base learner cannot (reliably) estimate the conﬁdence of its predictions. If we denote by Mistakes(h1), Mistakes(h2), . . . , Mistakes(hk)
the number of mistakes made by the hypotheses learned in the k views on the N
queries, then
hOUT (x) =
g∈{h1,...,hk}
Mistakes(g)
Active Learning with Multiple Views
Learning with Strong and Weak Views
In the original multi-view setting , one makes
the strong views assumption that each view is suﬃcient to learn the target concept. However,
in practice, one also encounters views in which one can accurately learn only a concept that
is strictly more general or more speciﬁc than the concept of interest . This is often the case in domains that involve hierarchical classiﬁcation,
such as information extraction or email classiﬁcation. For example, it may be extremely
easy to discriminate (with a high accuracy) between work and personal emails based solely
on the email’s sender; however, this same information may be insuﬃcient for predicting the
work or personal sub-folder in which the email should be stored.
We introduce now the notion of a weak view, in which one can accurately learn only a
concept that is strictly more general or more speciﬁc than the target concept. Note that
learning in a weak view is qualitatively diﬀerent from learning “an approximation” of the
target concept: the latter represents learning with imperfect features, while the former
typically refers to a (easily) learnable concept that is a strict generalization/specialization
of the target concept (note that, in the real world, imperfect features and noisy labels aﬀect
learning in both strong and weak views).
In the context of learning with strong and weak views, we redeﬁne contention points
as the unlabeled examples on which the strong views predict a diﬀerent label. This is a
necessary step because of two reasons: ﬁrst, as the weak view is inadequate for learning the
target concept, it typically disagrees with the strong views on a large number of unlabeled
examples; in turn, this would increase the number of contention points and skew their
distribution. Second, we are not interested in ﬁxing the mistakes made by a weak view, but
rather in using this view as an additional information source that allows faster learning in
the strong views (i.e., from fewer examples).
Even though the weak views are inadequate for learning the target concept, they can
be exploited by Co-Testing both in the SelectQuery() and CreateOutputHypothesis()
functions. In particular, weak views are extremely useful for domains that have only two
strong views:
- the weak view can be used in CreateOutputHypothesis() as a tie-breaker when the
two strong views predict a diﬀerent label.
- SelectQuery() can be designed so that, ideally, each query would represent a mistake in
both strong views. This can be done by ﬁrst detecting the contention points - if any
- on which the weak view disagrees with both strong views; among these, the next
query is the one on which the weak view makes the most conﬁdent prediction. Such
queries are likely to represent a mistake in both strong views, rather than in just one
of them; in turn, this implies simultaneous large cuts in both strong version spaces,
thus leading to faster convergence.
In section 5.2.2 we describe a Co-Testing algorithm that exploits strong and weak views
for wrapper induction domains .
Note that learning from strong and weak views clearly extends beyond wrapper induction
tasks: for example, the idea of exploiting complementary information sources (i.e., diﬀerent
Muslea, Minton, & Knoblock
types of features) appears in the two multi-strategy learners that are discussed in section 4.3.2.
Co-Testing vs. Related Approaches
As we already mentioned in section 3.3, existing multi-view approaches are typically semisupervised learners that bootstrap the views from each other. The two exceptions interleave Co-Testing and Co-EM
 , thus combining the best of both worlds: semi-supervised learning
provides the active learner with more accurate hypotheses, which lead to more informative queries; active learning provides the semi-supervised learner with a more informative
training set, thus leading to faster convergence.
Co-Testing vs. Existing Active Learners
Intuitively, Co-Testing can be seen as a committee-based active learner that generates a
committee that consists of one hypothesis in each view. Also note that Co-Testing can be
combined with virtually any of the existing single-view active learners: among the contention
points, Co-Testing can select the next query based on any of the heuristics discussed in
section 3.2.
There are two main diﬀerences between Co-Testing and other active learners:
- except for Co-Testing and its variants
 , all
other active learners work in the single-view framework (i.e., they pool together all
the domain features).
- single-view active learners are typically designed for a particular (class of) base learner(s).
For example, Query-by-Committee assumes
that one can randomly sample hypotheses from the version space, while Uncertainty
Sampling relies on the base learner’s
ability to reliably evaluate the conﬁdence of its predictions. In contrast, the basic
idea of Co-Testing (i.e., querying contention points) applies to any multi-view problem,
independently of the base learner to be used.
The Co-Testing approach to active learning has both advantages and disadvantages.
On one hand, Co-Testing cannot be applied to problems that do not have at least two
views. On the other hand, for any multi-view problem, Co-Testing can be used with the
best base learner for that particular task. In contrast, in the single-view framework, one
often must either create a new active learning method for a particular base learner or, even
worse, modify an existing base learner so that it can be used in conjunction with an existing
sampling algorithm.
To illustrate this last point, let us brieﬂy consider learning for information extraction,
where the goal is to use machine learning for extracting relevant strings from a collection
of documents (e.g., extract the perpetrators, weapons, and victims from a corpus of news
stories on terrorist attacks). As information extraction is diﬀerent in nature from a typical
classiﬁcation task, existing active learners cannot be applied in a straightforward manner:
- for information extraction from free text (ie), the existing active learners are crafted based on heuristics
Active Learning with Multiple Views
speciﬁc to their respective base learners, rapier, whisk, and Partially Hidden Markov
Models. An alternative is discussed by Finn and Kushmerick , who explore
a variety of ie-speciﬁc heuristics that can be used for active learning purposes and
analyze the trade-oﬀs related to using these heuristics.
- for wrapper induction, where the goal is to extract data from Web pages that share
the same underlying structure, there are no reported results for applying (singleview) active learning. This is because typical wrapper induction algorithms are base learners that lack the
properties exploited by the single-view active learners reviewed in section 3.2.: they are
determinist learners that are noise sensitive, provide no conﬁdence in their predictions,
and make no mistakes on the training set.
In contrast, Co-Testing applies naturally to both wrapper induction and information extraction from free text . This is due to the
fact that Co-Testing does not rely on the base learner’s properties to identify its highly
informative set of candidate queries; instead, it focuses on the contention points, which, by
deﬁnition, are guaranteed to represent mistakes in some of the views.
Exploiting Weak Views
We brieﬂy discuss now two learning tasks that can be seen as learning from strong and
weak views, even though they were not formalized as such, and the views were not used
for active learning. An additional application domain with strong and weak views, wrapper
induction, is discussed at length in section 5.2.
The discotex system was designed to extract job titles,
salaries, locations, etc from computer science job postings to the newsgroup austin.jobs.
discotex proceeds in four steps: ﬁrst, it uses rapier to learn
extraction rules for each item of interest. Second, it applies the learned rules to a large,
unlabeled corpus of job postings and creates a database that is populated with the extracted
data. Third, by text mining this database, discotex learns to predict the value of each item
based on the values of the other ﬁelds; e.g., it may discover that “IF the job requires c++
and corba THEN the development platforms include Windows.” Finally, when the system
is deployed and the rapier rules fail to extract an item, the mined rules are used to predict
the item’s content.
In this scenario, the rapier rules represent the strong view because they are suﬃcient
for extracting the data of interest. In contrast, the mined rules represent the weak view
because they cannot be learned or used by themselves. Furthermore, as discotex discards
all but the most accurate of the mined rules, which are highly-speciﬁc, it follows that this
weak view is used to learn concepts that are more speciﬁc than the target concept. Nahm
and Mooney show that the mined rules improve the extraction accuracy by capturing
information that complements the rapier extraction rules.
Another domain with strong and weak views is presented by Kushmerick et al. .
The learning task here is to classify the lines of text on a business card as a person’s name,
aﬃliation, address, phone number, etc. In this domain, the strong view consists of the
words that appear on each line, based on which a Naive Bayes text classiﬁer is learned.
In the weak view, one can exploit the relative order of the lines on the card by learning a
Muslea, Minton, & Knoblock
Hidden Markov Model that predicts the probability of a particular ordering of the lines on
the business card (e.g., name followed by address, followed by phone number).
This weak view deﬁnes a class of concepts that is more general than the target concept:
all line orderings are possible, even though they are not equally probable. Even though
the order of the text lines cannot be used by itself to accurately classify the lines, when
combined with the strong view, the ordering information leads to a classiﬁer that clearly
outperforms the stand-alone strong view .
Note that both approaches above use the strong and weak views for passive, rather
than active learning. That is, given a ﬁxed set of labeled and no unlabeled examples, these
algorithms learn one weak and one strong hypothesis that are then used to craft a domainspeciﬁc predictor that outperforms each individual hypothesis. In contrast, Co-Testing is
an active learner that seamlessly integrates weak and strong hypotheses without requiring
additional, domain-speciﬁc data engineering.
Empirical Validation
In this section we empirically compare Co-Testing with other state of the art learners. Our
goal is to test the following hypothesis: given a multi-view learning problem, Co-Testing
converges faster than its single-view counterparts.
We begin by presenting the results on three real-world classiﬁcation domains: Webpage classiﬁcation, discourse tree parsings, and advertisement removal. Then we focus on
an important industrial application, wrapper induction , in which the goal is to learn rules that extract the relevant data
from a collection of documents (e.g., extract book titles and prices from a Web site).
The results for classiﬁcation and wrapper induction are analyzed separately because:
- for each of the three classiﬁcation tasks, there are only two strong views that are available;
in contrast, for wrapper induction we have two strong and one weak views, which
allows us to explore a wider range of options.
- for each classiﬁcation domain, there is exactly one available dataset.
In contrast, for
wrapper induction we use a testbed of 33 distinct tasks. This imbalance in the number
of available datasets requires diﬀerent presentation styles for the results.
- in contrast to typical classiﬁcation, a major requirement for wrapper induction is to learn
(close to) 100%-accurate extraction rules from just a handful of examples . This requirement leads to signiﬁcant diﬀerences in both the experimental setup and the interpretation of the results (e.g., results that are excellent for
most classiﬁcation tasks may be unacceptable for wrapper induction).
Co-Testing for Classiﬁcation
We begin our empirical study by using three classiﬁcation tasks to compare Co-Testing with
existing active learners. We ﬁrst introduce these three domains and their respective views;
then we discuss the learners used in the evaluation and analyze the experimental results.
Active Learning with Multiple Views
Co-Testing
Single-view Algorithms
Hypothesis
conservative
Table 2: The algorithms used for classiﬁcation. The last ﬁve columns denote Query-by-
Committee/-Bagging/-Boosting, Uncertainty Sampling and Random Sampling.
The Views used by Co-Testing
We applied Co-Testing to three real-world classiﬁcation domains for which there is a natural,
intuitive way to create two views:
- ad is a classiﬁcation problem with two classes, 1500 attributes, and
3279 examples. In ad, images that appear in Web pages are classiﬁed into ads and
non-ads. The view V1 consists of all textual features that describe the image; e.g.,
1-grams and 2-grams from the caption, from the url of the page that contains the
image, from the url of the page the image points to, etc. In turn, V2 describes the
properties of the image itself: length, width, aspect ratio, and “origin” (i.e., are the
image and the page that contains it coming from the same Web server?).
- courses is a domain with two classes, 2206 features, and 1042
examples. The learning task consists of classifying Web pages as course homepages
and other pages. In courses the two views consist of words that appear in the page
itself and words that appear in hyperlinks pointing to them, respectively.
- tf is a classiﬁcation problem with seven classes, 99
features and 11,193 examples. In the context of a machine translation system, it uses
the shift-reduce parsing paradigm to learn how to rewrite Japanese discourse trees as
English-like discourse trees. In this case, V1 uses features speciﬁc to the shift-reduce
parser: the elements in the input list and the partial trees in the stack. V2 consists of
features speciﬁc to the Japanese tree given as input.
The Algorithms used in the Evaluation
Table 2 shows the learners used in this empirical comparison. We have implemented all
active learners as extensions of the MLC++ library . For each domain, we choose the base learner as follows: after applying all primitive
learners in MLC++ on the dataset (10-fold cross-validation), we select the one that obtains
the best performance. More precisely, we are using the following base learners: ib for ad, Naive Bayes for courses, and mc4, which is an
implementation of c4.5, for tf.
Muslea, Minton, & Knoblock
The ﬁve single-view algorithms from Table 2 use all available features (i.e., V1 ∪V2)
to learn the target concept.1 On all three domains, Random Sampling (Rnd) is used as
strawman; Query-by-Bagging and -Boosting, denoted by qBag and qBst, are also run
on all three domains. In contrast, Uncertainty Sampling (US) is applied only on ad and
courses because mc4, which is the base learner for tf, does not provide an estimate of the
conﬁdence of its prediction.
As there is no known method for randomly sampling from the ib or mc4 version spaces,
Query-by-Committee (QBC) is not applied to ad and tf. However, we apply QBC to
courses by borrowing an idea from McCallum and Nigam : we create the committee by sampling hypotheses according to the (Gamma) distribution of the Naive Bayes
parameters estimated from the training set L.
For Query-by-Committee we use a typical 2-hypothesis committee.
For Query-by-
Bagging and -Boosting, we use a relatively small 5-hypothesis committees because of the
cpu constraints: the running time increases linearly with the number of learned hypotheses,
and, in some domains, it takes more than 50 cpu hours to complete the experiments even
with the 5-hypothesis committees.
Because of the limitations of their respective base learners (i.e., the above-mentioned
issue of estimating the conﬁdence in each prediction), for ad and tf we use Naive Co-Testing
with a winner-takes-all output hypothesis; that is, each query is randomly selected among
the contention points, and the output hypothesis is the one learned in the view that makes
the fewest mistakes on the queries. In contrast, for courses we follow the methodology
from the original Co-Training paper , where the output hypothesis
consists of the weighted vote of the classiﬁers learned in each view.
On courses we investigate two of the Co-Testing query selection strategies: naive and
conservative. The third, aggressive query selection strategy is not appropriate for courses
because the “hyperlink view” is signiﬁcantly less accurate than the other one (after all, one
rarely encounters more than a handful of words in a hyperlink). Consequently, most of
the high-conﬁdence contention points are “unﬁxable mistakes” in the hyperlink view, which
means that even after seeing the correct label, they cannot be classiﬁed correctly in that
The Experimental Results
The learners’ performance is evaluated based on 10-fold, stratiﬁed cross validation. On ad,
each algorithm starts with 150 randomly chosen examples and makes 10 queries after each
of the 40 learning episodes, for a total of 550 labeled examples. On courses, the algorithms
start with 6 randomly chosen examples and make one query after each of the 175 learning
episodes. Finally, on tf the algorithms start with 110 randomly chosen examples and make
20 queries after each of the 100 learning episodes.
Figures 1 and 2 display the learning curves of the various algorithms on ad, tf, and
course. On all three domains, Co-Testing reaches the highest accuracy (i.e., smallest error
rate). Table 3 summarizes the statistical signiﬁcance results (t-test conﬁdence of at least
1. In a preliminary experiment, we have also ran the algorithms of the individual views. The results on
V1 and V2 were either worse then those on V1 ∪V2 or the diﬀerences were statistically insigniﬁcant.
Consequently, for sake of simplicity, we decided to show here only the single-view results for V1 ∪V2.
Active Learning with Multiple Views
error rate (%)
labeled examples
Naive Co-Testing
Uncertainty Sampling
error rate (%)
labeled examples
Naive Co-Testing
error rate (%)
labeled examples
Naive Co-Testing
Figure 1: Empirical results on the ad and tf problems
Muslea, Minton, & Knoblock
error rate (%)
labeled examples
Conservative Co-Testing
Uncertainty Sampling
error rate (%)
labeled examples
Conservative Co-Testing
error rate (%)
labeled examples
Conservative Co-Testing
Naive Co-Testing
Figure 2: Empirical results on the courses problem
Active Learning with Multiple Views
Naive Co-Testing
Conservative Co-Testing
Random Sampling
Uncertainty Sampling
Query-by-Committee
Query-by-Bagging
Query-by-Boosting
Naive Co-Testing
Table 3: Statistical signiﬁcance results in the empirical (pair-wise) comparison of the various
algorithms on the three domains.
95%) obtained in a pair-wise comparison of the various algorithms. These comparisons are
performed on the right-most half of each learning curve (i.e., towards convergence). The
best way to explain the results in Table 3 is via examples: the results of comparing Naive
Co-Testing and Random Sampling on ad appear in the ﬁrst three columns of the ﬁrst
row. The three numbers (i.e., 0, 0, and 19) mean that on (all) 19 comparison points Naive
Co-Testing outperforms Random Sampling in a statistically signiﬁcant manner. Similarly,
comparing Naive and Conservative Co-Testing on courses (the last three columns on the
last row) leads to the following results: on 28 of the comparison points Conservative Co-
Testing outperforms Naive Co-Testing in a statistically signiﬁcant manner; on 21 other
points the diﬀerences are statistically insigniﬁcant; ﬁnally, on no comparison point Naive
Co-Testing outperforms its Conservative counterpart.
The results in Table 3 can be summarized as follows. First of all, no single-view algorithm
outperforms Co-Testing in a statistically signiﬁcant manner on any of the comparison points.
Furthermore, except for the comparison with Query-by-Bagging and -Boosting on ad, where
the diﬀerence in accuracy is statistically insigniﬁcant on almost all comparison points, Co-
Testing clearly outperform all algorithms on all domains.
Finally, let us brieﬂy comment on applying multi-view, semi-supervised learners to the
three tasks above. As mentioned in section 3.3, such algorithms bootstrap the views from
each other by training each view on the examples labeled with high-conﬁdence by the
other view. For ad and tf, we could not use multi-view, semi-supervised learning because
the base learners ib and mc4 do not provide a (reliable) estimate of the conﬁdence in
their predictions.
More precisely, mc4 provides no estimate at all, while ib’s estimates
are extremely poor when the training data is scarce (e.g., see the poor performance of
Uncertainty Sampling on ad, where it barely outperforms Random Sampling).
On courses, we have applied both Co-Training and Co-EM in conjunction with the
Naive Bayes base learner. Both these multi-view learners reach their maximum accuracy
(close to 95%) based on solely 12 labeled and 933 unlabeled examples, after which their
Muslea, Minton, & Knoblock
Name:<i>Gino’s</i><p>Phone:<i> (800)111-1717 </i><p>Cuisine: …
Figure 3: Both the forward and backward rules detect the beginning of the phone number.
performance does not improve in a statistically signiﬁcant manner.2 As shown in Figure 2,
when the training data is scarce (i.e., under 40 labeled examples), Co-Testing’s accuracy is
less than 95%; however, after making additional queries, Co-Testing reaches 98% accuracy,
while Co-Training and Co-EM remain at 95% even when trained on 180 labeled and 765
unlabeled examples.
These results are consistent with the diﬀerent goals of active and
semi-supervised learning: the former focuses on learning the perfect target concept from
a minimal amount of labeled data, while the latter uses unlabeled examples to boost the
accuracy of a hypothesis learned from just a handful of labeled examples.
Co-Testing for Wrapper Induction
We focus now on a diﬀerent type of learning application, wrapper induction , in which the goal is to learn rules that extract relevant sub-strings
from a collection of documents.
Wrapper induction is a key component of commercial
systems that integrate data from a variety of Web-based information sources.
The Views used by Co-Testing
Consider the illustrative task of extracting phone numbers from documents similar to the
fragment in Figure 3. To ﬁnd where the phone number begins,3 one can use the rule
R1 = SkipTo( Phone:<i> )
This rule is applied forward, from the beginning of the page, and it ignores everything until
it ﬁnds the string Phone:<i>. Note that such forward-going rules do not represent the only
way to detect where the phone number begins: an alternative approach is to use the rule
R2 = BackTo( Cuisine ) BackTo( ( Number ) )
which is applied backward, from the end of the document. R2 ignores everything until it
ﬁnds “Cuisine” and then, again, skips to the ﬁrst number between parentheses.
Forward and backward rules such as R1 and R2 can be learned from user-provided
examples by the state of the art wrapper induction system stalker ,
which we use as base learner for Co-Testing. Intuitively, stalker creates a forward or a
2. A recent paper shows that - for text classiﬁcation - svm is more appropriate
than Naive Bayes as base learner for Co-EM, though not necessarily for Co-Training. As the MLC++
library does not provide svm as base learner, we could not compare our results with those by Brefeld
and Scheﬀer , where Co-EM + svm reaches 99% accuracy based on 12 labeled and 933 unlabeled
examples. However, in all fairness, it is unlikely that Co-Testing could lead to an even faster convergence.
3. As shown by Muslea et al. , the end of the phone number can be found in a similar manner.
Active Learning with Multiple Views
backward rule that consumes all the tokens that precede or follow the extraction point,
respectively. It follows that rules such as R1 and R2 represent descriptions of the same
concept (i.e., beginning of phone number) that are learned in two diﬀerent views: the
sequences of tokens that precede and follow the beginning of the item, respectively. These
views are strong views because each of them is suﬃcient to accurately extract the items of
interest .
In addition to these two views, which rely mostly on the context of the item to be
extracted (i.e., the text surrounding the item), one can use a third view that describes
the content of the item to be extracted. For example, phone numbers can be described
by the simple grammar: “( Number ) Number - Number”; similarly, most urls start with
“ end with “.html”, and contain no html tags.
Such a content-based view is a weak view because it often represents a concept more
general than the target one. For example, the phone number grammar above cannot discriminate between the home, oﬃce, cell, and fax numbers that appear within the same Web
page; similarly, the url grammar cannot distinguish between the urls of interest (e.g., a
product’s review) and all the other ones (e.g., advertisements).
In this weak view, we use as base learner a version of DataPro that is described elsewhere . DataPro learns - from
positives examples only - the “prototypes” of the items to be extracted; i.e., it ﬁnds statistically signiﬁcant sequences of tokens that (1) are highly unlikely to have been generated
by chance and (2) describe the content of many of the positive examples. The features used
by this base learner consist of the length range (in tokens) of the seen examples, the token
types that appear in the training set (e.g., Number, AllCaps, etc), and the start- and endpattern (e.g., “ and “AlphaNum .html”, respectively).
The Algorithms used in the Evaluation
As the extraction rules learned in the two strong views do not provide any estimate of
the conﬁdence of the extractions, the only Co-Testing algorithm that can be implemented
based solely on the forward and backward views is Naive Co-Testing with a winner-takes-all
output hypothesis:
- each query is randomly chosen (Naive Co-Testing) among the contention points, which
are the documents from which the learned rules extract diﬀerent strings.
- the output hypothesis is the rule learned in the view that makes the fewest mistakes over
the allowed number of queries (i.e., winner-takes-all).
Given the additional, content-based view, we can also implement an aggressive version
of Co-Testing for wrapper induction:
- the contention points are, again, the unlabeled examples on which the rules learned in
the strong views do not extract the same string.
- the aggressive query selection strategy works by selecting the contention point for which
the hypothesis learned in the weak view is maximally conﬁdent that both stalker
rules are extracting incorrect strings. More formally, for each contention point, let s1
and s2 be the strings extracted by the strong views; let us also denote by n1 and n2
Muslea, Minton, & Knoblock
the number of constraints learned in the weak views that are violated by s1 and s2.
Using this notation, the next query is the contention point for which min(n1, n2) has
the largest value.
- the output hypothesis is obtained by the following majority voting scheme: if the strings
extracted by the strong views are identical, then they represent the extracted item;
otherwise the result is the one of these two strings that violates fewer of the constraints
learned in the weak view.
In the empirical evaluation below, we compare these two Co-Testing algorithms with
Random Sampling and Query-by-Bagging.
The former is used as strawman, while the
latter is the only general-purpose active learner that can be applied in a straightforward
manner to wrapper induction (for details, see the discussion in section 4.3.1).
existing multi-view, semi-supervised learners cannot be used for wrapper induction because
the base learners do not provide an estimate in the conﬁdence of each extraction; and even
if such an estimate could be obtained, wrapper induction algorithms are extremely sensitive
to mislabeled examples, which would make the bootstrapping process unacceptably brittle.
In this paper, the implementation of Random Sampling is identical with that of Naive
Co-Testing with winner takes all, except that it randomly queries one of the unlabeled
examples from the working set.
For Query-by-Bagging, the committee of hypotheses is
created by repeatedly re-sampling (with substitution) the examples in the original training
set L. We use a relatively small committee (i.e., 10 extraction rules) because when learning
from a handful of examples, re-sampling with replacement leads to just a few distinct
training sets. In order to make a fair comparison with Co-Testing, we run Query-by-Bagging
once in each strong view and report the best of the obtained results.
The Experimental Results
In our empirical comparison, we use the 33 most diﬃcult wrapper induction tasks from the
testbed introduced by Kushmerick . These tasks, which were previously used in
the literature , are brieﬂy described in Table 4. We use
20-fold cross-validation to compare the performance of Naive and Aggressive Co-Testing,
Random Sampling, and Query-by-Bagging on the 33 tasks. Each algorithm starts with two
randomly chosen examples and then makes 18 successive queries.
The results below can be summarized as follows: for 12 tasks, only the two Co-Testing
algorithms learn 100% accurate rules; for another 18 tasks, Co-Testing and at least another
algorithm reach 100% accuracy, but Co-Testing requires the smallest number of queries.
Finally, on the remaining three tasks, no algorithm learns a 100% accurate rule.
Figure 4 shows the aggregate performance of the four algorithms over the 33 tasks. In
each of the six graphs, the X axis shows the number of queries made by the algorithm, while
the Y axis shows the number of tasks for which a 100% accurate rule was learned based on
exactly X queries. As mentioned earlier, all algorithms start with 2 random examples and
make 18 additional queries, for a total of 20 labeled examples. By convention, the rightmost point on the X axis, which is labeled “19 queries”, represents the number of tasks that
require more than the allowed 18 queries to learn a 100% accurate rule. This additional
“19 queries” data-point allows us to summarize the results without dramatically extending
the X axis beyond 18 queries: as for some of the extraction tasks Random Sampling and
Active Learning with Multiple Views
AllPolitics
University
Arrival Time
Travel Net.
Availability
Organization
Democratic
Party Online
Languages for
Translation
us Tax Code
Web Server
Cycling www
Person Name
Table 4: The 33 wrapper induction tasks used in the empirical evaluation.
Query-by-Bagging need hundreds of queries to learn the correct rules, the histograms would
become diﬃcult to read if the entire X axis were shown.
As shown in Figure 4, the two Co-Testing algorithms clearly outperform their single-view
counterparts, with Aggressive Co-Testing doing signiﬁcantly better than Naive Co-Testing
(the results are statistically signiﬁcant with a conﬁdence of at least 99%). Aggressive Co-
Testing learns 100%-accurate rules on 30 of the 33 tasks; for all these tasks, the extraction
rules are learned from at most seven queries. Naive Co-Testing learns 100% accurate rules
on 28 of the 33 tasks.
On 26 of these 28 tasks, the extraction rules are learned based
on at most six queries. In contrast, Random Sampling and Query-by-Bagging learn 100%
accurate rules for only seven and twelve of the tasks, respectively. In other words, both
Co-Testing algorithms learn the correct target concept for more than twice as many tasks
than Query-by-Bagging or Random Sampling.
We must emphasize the power of Aggressive Co-Testing on high-accuracy tasks such as
wrapper induction: for 11 of the 33 tasks, a single, “aggressively-chosen” query is suﬃcient
to learn the correct extraction rule. In contrast, Naive Co-Testing converges in a single
query on just four of the 33 tasks, while the other two learners never converge in a single
For the three tasks on which Aggressive Co-Testing does not learn 100% accurate rules,
the failure is due to the fact that one of the views is signiﬁcantly less accurate than the other
one. This leads to a majority of contention points that are mislabeled by the “bad view,”
which - in turn - skews the distribution of the queries towards mistakes of the “bad view.”
Consequently, Co-Testing’s performance suﬀers because such queries are uninformative for
Muslea, Minton, & Knoblock
extraction task that converged
Aggressive Co-Testing
extraction task that converged
Naive Co-Testing
extraction task that converged
Query-by-Bagging (FB)
extraction task that converged
Random Sampling
Figure 4: Convergence results for the 33 wrapper induction tasks.
both views: the “good view” makes the correct prediction on them, while the “bad view” is
inadequate to learn the target concept. In order to cope with this problem, we introduced
a view validation algorithm that predicts whether the views are
appropriate for a particular task.
Finally, let us brieﬂy compare the results above with the ones obtained by wien , which is the only wrapper induction system that was evaluated on all the
extraction tasks used here. As the two experimental setups are not identical (i.e., crossvalidation vs.
random splits) this is just an informal comparison; however, it puts our
results into perspective by contrasting Co-Testing with another state of the art approach
to wrapper induction.
The results can be summarized as follows: wien fails on 18 of the 33 task; these 18 tasks
include the three for which Aggressive and Naive Co-Testing failed to learn perfect rules.
On the remaining 15 tasks, wien requires between 25 and 90 examples4 to learn the correct
rule. For the same 15 tasks, both Aggressive and Naive Co-Testing learn 100% accurate
rules based on at most eight examples (two random plus at most six queries).
4. In the wien framework, an example consists of a document in which all items of interest are labeled. For
example, a page that contains a list of 100 names, all labeled, represents a single labeled example. In
contrast, for stalker the same labeled document represents 100 distinct labeled examples. In order to
compare the wien and stalker results, we convert the wien data to stalker-like data by multiplying
the number of labeled wien pages by the average number of item occurrences in each page.
Active Learning with Multiple Views
Conclusion
In this paper we introduce Co-Testing, which is an active learning technique for multi-view
learning tasks. This novel approach to active learning is based on the idea of learning from
mistakes; i.e., Co-Testing queries unlabeled examples on which the views predict a diﬀerent
label (such contention points are guaranteed to represent mistakes made in one of the views).
We have analyzed several members of the Co-Testing family (e.g., Naive, Conservative and
Aggressive Co-Testing). We have also introduced and evaluated a Co-Testing algorithm
that simultaneously exploits both strong and weak views.
Our empirical results show that Co-Testing is a powerful approach to active learning. Our experiments use four extremely diﬀerent base learners (i.e., stalker, ib, Naive
Bayes, and mc4) on four diﬀerent types of domains: wrapper induction, text classiﬁcation (courses), ad removal (ad), and discourse tree parsing (tf). In all these scenarios,
Co-Testing clearly outperforms the single-view, state of the art active learning algorithms.
Furthermore, except for Query-by-Bagging, Co-Testing is the only algorithm that can be
applied to all the problems considered in the empirical evaluation. In contrast to Queryby-Bagging, which has a poor performance on courses and wrapper induction, Co-Testing
obtains the highest accuracy among the considered algorithms.
Co-Testing’s success is due to its ability to discover the mistakes made in each view. As
each contention point represents a mistake (i.e., an erroneous prediction) in at least one of
the views, it follows that each query is extremely informative for the view that misclassiﬁed
that example; that is, mistakes are more informative than correctly labeled examples. This
is particularly true for base learners such as stalker, which do not improve the current
hypothesis unless they are provided with examples of misclassiﬁed instances.
As a limitation, Co-Testing can be applied only to multi-view tasks; that is, unless the
user can provide two views, Co-Testing cannot be used at all. However, researchers have
shown that besides the four problems above, multiple views exist in a variety of real world
problems, such as named entity classiﬁcation , statistical parsing
 , speech recognition , word sense disambiguation
 , or base noun phrase bracketing .
The other concern about Co-Testing is related to the potential violations of the two
multi-view assumptions, which require that the views are both uncorrelated and compatible.
For example, in the case of correlated views, the hypotheses learned in each view may be so
similar that there are no contention points among which to select the next query. In terms
of view incompatibility, remember that, for three of the 33 wrapper induction tasks, one of
the views was so inaccurate that the Co-Testing could not outperform Random Sampling.
In two companion papers we have proposed practical solutions
for both these problems.
Acknowledgments
This research is based upon work supported in part by the National Science Foundation
under Award No. IIS-0324955 and grant number 0090978, in part by the Defense Advanced
Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No. NBCHD030010, and in part by the Air Force
Muslea, Minton, & Knoblock
Oﬃce of Scientiﬁc Research under grant number FA9550-04-1-0105. The U.S.Government is
authorized to reproduce and distribute reports for Governmental purposes notwithstanding
any copy right annotation thereon. The views and conclusions contained herein are those of
the authors and should not be interpreted as necessarily representing the oﬃcial policies or
endorsements, either expressed or implied, of any of the above organizations or any person
connected with them.