Encoder-Decoder with Atrous Separable
Convolution for Semantic Image Segmentation
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroﬀ, and
Hartwig Adam
Google Inc.
{lcchen, yukun, gpapan, fschroff, hadam}@google.com
Abstract. Spatial pyramid pooling module or encode-decoder structure
are used in deep neural networks for semantic segmentation task. The
former networks are able to encode multi-scale contextual information by
probing the incoming features with ﬁlters or pooling operations at multiple rates and multiple eﬀective ﬁelds-of-view, while the latter networks
can capture sharper object boundaries by gradually recovering the spatial
information. In this work, we propose to combine the advantages from
both methods. Speciﬁcally, our proposed model, DeepLabv3+, extends
DeepLabv3 by adding a simple yet eﬀective decoder module to reﬁne the
segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution
to both Atrous Spatial Pyramid Pooling and decoder modules, resulting
in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes
datasets, achieving the test set performance of 89.0% and 82.1% without
any post-processing. Our paper is accompanied with a publicly available
reference implementation of the proposed models in Tensorﬂow at https:
//github.com/tensorflow/models/tree/master/research/deeplab.
Keywords: Semantic image segmentation, spatial pyramid pooling, encoderdecoder, and depthwise separable convolution.
Introduction
Semantic segmentation with the goal to assign semantic labels to every pixel in an
image is one of the fundamental topics in computer vision. Deep convolutional neural networks based on the Fully Convolutional Neural
Network show striking improvement over systems relying on hand-crafted
features on benchmark tasks. In this work, we consider two
types of neural networks that use spatial pyramid pooling module or
encoder-decoder structure for semantic segmentation, where the former
one captures rich contextual information by pooling features at diﬀerent resolution while the latter one is able to obtain sharp object boundaries.
In order to capture the contextual information at multiple scales, DeepLabv3
 applies several parallel atrous convolution with diﬀerent rates (called Atrous
 
L.-C Chen, Y. Zhu, G. Papandreou, F. Schroﬀ, and H. Adam
Spatial Pyramid Pooling
Prediction
Prediction
Spatial Pyramid Pooling
Prediction
(a) Spatial Pyramid Pooling
(b) Encoder-Decoder
(c) Encoder-Decoder with Atrous Conv
Fig. 1. We improve DeepLabv3, which employs the spatial pyramid pooling module (a),
with the encoder-decoder structure (b). The proposed model, DeepLabv3+, contains
rich semantic information from the encoder module, while the detailed object boundaries are recovered by the simple yet eﬀective decoder module. The encoder module
allows us to extract features at an arbitrary resolution by applying atrous convolution.
Spatial Pyramid Pooling, or ASPP), while PSPNet performs pooling operations at diﬀerent grid scales. Even though rich semantic information is encoded in
the last feature map, detailed information related to object boundaries is missing
due to the pooling or convolutions with striding operations within the network
backbone. This could be alleviated by applying the atrous convolution to extract
denser feature maps. However, given the design of state-of-art neural networks
 and limited GPU memory, it is computationally prohibitive to extract output feature maps that are 8, or even 4 times smaller than the input
resolution. Taking ResNet-101 for example, when applying atrous convolution to extract output features that are 16 times smaller than input resolution,
features within the last 3 residual blocks (9 layers) have to be dilated. Even
worse, 26 residual blocks (78 layers!) will be aﬀected if output features that are
8 times smaller than input are desired. Thus, it is computationally intensive if
denser output features are extracted for this type of models. On the other hand,
encoder-decoder models lend themselves to faster computation (since no
features are dilated) in the encoder path and gradually recover sharp object
boundaries in the decoder path. Attempting to combine the advantages from
both methods, we propose to enrich the encoder module in the encoder-decoder
networks by incorporating the multi-scale contextual information.
In particular, our proposed model, called DeepLabv3+, extends DeepLabv3
 by adding a simple yet eﬀective decoder module to recover the object boundaries, as illustrated in Fig. 1. The rich semantic information is encoded in the
output of DeepLabv3, with atrous convolution allowing one to control the density of the encoder features, depending on the budget of computation resources.
Furthermore, the decoder module allows detailed object boundary recovery.
Motivated by the recent success of depthwise separable convolution ,
we also explore this operation and show improvement in terms of both speed and
accuracy by adapting the Xception model , similar to , for the task of
DeepLabv3+: Encoder-Decoder with Atrous Separable Convolution
semantic segmentation, and applying the atrous separable convolution to both
the ASPP and decoder modules. Finally, we demonstrate the eﬀectiveness of the
proposed model on PASCAL VOC 2012 and Cityscapes datasts and attain the
test set performance of 89.0% and 82.1% without any post-processing, setting a
new state-of-the-art.
In summary, our contributions are:
– We propose a novel encoder-decoder structure which employs DeepLabv3 as
a powerful encoder module and a simple yet eﬀective decoder module.
– In our structure, one can arbitrarily control the resolution of extracted encoder features by atrous convolution to trade-oﬀprecision and runtime,
which is not possible with existing encoder-decoder models.
– We adapt the Xception model for the segmentation task and apply depthwise
separable convolution to both ASPP module and decoder module, resulting
in a faster and stronger encoder-decoder network.
– Our proposed model attains a new state-of-art performance on PASCAL
VOC 2012 and Cityscapes datasets. We also provide detailed analysis of
design choices and model variants.
– We make our Tensorﬂow-based implementation of the proposed model publicly available at 
research/deeplab.
Related Work
Models based on Fully Convolutional Networks (FCNs) have demonstrated
signiﬁcant improvement on several segmentation benchmarks . There
are several model variants proposed to exploit the contextual information for
segmentation , including those that employ multi-scale
inputs (i.e., image pyramid) or those that adopt probabilistic
graphical models (such as DenseCRF with eﬃcient inference algorithm )
 . In this work, we mainly discuss about the
models that use spatial pyramid pooling and encoder-decoder structure.
Spatial pyramid pooling: Models, such as PSPNet or DeepLab ,
perform spatial pyramid pooling at several grid scales (including imagelevel pooling ) or apply several parallel atrous convolution with diﬀerent
rates (called Atrous Spatial Pyramid Pooling, or ASPP). These models have
shown promising results on several segmentation benchmarks by exploiting the
multi-scale information.
Encoder-decoder: The encoder-decoder networks have been successfully
applied to many computer vision tasks, including human pose estimation , object detection , and semantic segmentation .
Typically, the encoder-decoder networks contain (1) an encoder module that
gradually reduces the feature maps and captures higher semantic information,
and (2) a decoder module that gradually recovers the spatial information. Building on top of this idea, we propose to use DeepLabv3 as the encoder module
and add a simple yet eﬀective decoder module to obtain sharper segmentations.
L.-C Chen, Y. Zhu, G. Papandreou, F. Schroﬀ, and H. Adam
Atrous Conv
Prediction
Fig. 2. Our proposed DeepLabv3+ extends DeepLabv3 by employing a encoderdecoder structure. The encoder module encodes multi-scale contextual information by
applying atrous convolution at multiple scales, while the simple yet eﬀective decoder
module reﬁnes the segmentation results along object boundaries.
Depthwise separable convolution: Depthwise separable convolution 
or group convolution , a powerful operation to reduce the computation cost
and number of parameters while maintaining similar (or slightly better) performance. This operation has been adopted in many recent neural network designs
 . In particular, we explore the Xception model , similar
to for their COCO 2017 detection challenge submission, and show improvement in terms of both accuracy and speed for the task of semantic segmentation.
In this section, we brieﬂy introduce atrous convolution and depthwise separable convolution . We then review DeepLabv3 
which is used as our encoder module before discussing the proposed decoder
module appended to the encoder output. We also present a modiﬁed Xception
model which further improves the performance with faster computation.
Encoder-Decoder with Atrous Convolution
Atrous convolution: Atrous convolution, a powerful tool that allows us to explicitly control the resolution of features computed by deep convolutional neural
networks and adjust ﬁlter’s ﬁeld-of-view in order to capture multi-scale information, generalizes standard convolution operation. In the case of two-dimensional
signals, for each location i on the output feature map y and a convolution ﬁlter
w, atrous convolution is applied over the input feature map x as follows:
DeepLabv3+: Encoder-Decoder with Atrous Separable Convolution
(a) Depthwise conv.
(b) Pointwise conv.
(c) Atrous depthwise conv.
Fig. 3. 3 × 3 Depthwise separable convolution decomposes a standard convolution into
(a) a depthwise convolution (applying a single ﬁlter for each input channel) and (b) a
pointwise convolution (combining the outputs from depthwise convolution across channels). In this work, we explore atrous separable convolution where atrous convolution
is adopted in the depthwise convolution, as shown in (c) with rate = 2.
x[i + r · k]w[k]
where the atrous rate r determines the stride with which we sample the input
signal. We refer interested readers to for more details. Note that standard
convolution is a special case in which rate r = 1. The ﬁlter’s ﬁeld-of-view is
adaptively modiﬁed by changing the rate value.
Depthwise separable convolution: Depthwise separable convolution, factorizing a standard convolution into a depthwise convolution followed by a pointwise convolution (i.e., 1 × 1 convolution), drastically reduces computation complexity. Speciﬁcally, the depthwise convolution performs a spatial convolution
independently for each input channel, while the pointwise convolution is employed to combine the output from the depthwise convolution. In the TensorFlow
 implementation of depthwise separable convolution, atrous convolution has
been supported in the depthwise convolution (i.e., the spatial convolution), as
illustrated in Fig. 3. In this work, we refer the resulting convolution as atrous
separable convolution, and found that atrous separable convolution signiﬁcantly
reduces the computation complexity of proposed model while maintaining similar (or better) performance.
DeepLabv3 as encoder: DeepLabv3 employs atrous convolution 
to extract the features computed by deep convolutional neural networks at an
arbitrary resolution. Here, we denote output stride as the ratio of input image
spatial resolution to the ﬁnal output resolution (before global pooling or fullyconnected layer). For the task of image classiﬁcation, the spatial resolution of the
ﬁnal feature maps is usually 32 times smaller than the input image resolution and
thus output stride = 32. For the task of semantic segmentation, one can adopt
output stride = 16 (or 8) for denser feature extraction by removing the striding
in the last one (or two) block(s) and applying the atrous convolution correspondingly (e.g., we apply rate = 2 and rate = 4 to the last two blocks respectively
for output stride = 8). Additionally, DeepLabv3 augments the Atrous Spatial
Pyramid Pooling module, which probes convolutional features at multiple scales
by applying atrous convolution with diﬀerent rates, with the image-level fea-
L.-C Chen, Y. Zhu, G. Papandreou, F. Schroﬀ, and H. Adam
tures . We use the last feature map before logits in the original DeepLabv3
as the encoder output in our proposed encoder-decoder structure. Note the encoder output feature map contains 256 channels and rich semantic information.
Besides, one could extract features at an arbitrary resolution by applying the
atrous convolution, depending on the computation budget.
Proposed decoder: The encoder features from DeepLabv3 are usually computed with output stride = 16. In the work of , the features are bilinearly
upsampled by a factor of 16, which could be considered a naive decoder module.
However, this naive decoder module may not successfully recover object segmentation details. We thus propose a simple yet eﬀective decoder module, as
illustrated in Fig. 2. The encoder features are ﬁrst bilinearly upsampled by a
factor of 4 and then concatenated with the corresponding low-level features 
from the network backbone that have the same spatial resolution (e.g., Conv2
before striding in ResNet-101 ). We apply another 1 × 1 convolution on the
low-level features to reduce the number of channels, since the corresponding lowlevel features usually contain a large number of channels (e.g., 256 or 512) which
may outweigh the importance of the rich encoder features (only 256 channels in
our model) and make the training harder. After the concatenation, we apply a
few 3 × 3 convolutions to reﬁne the features followed by another simple bilinear
upsampling by a factor of 4. We show in Sec. 4 that using output stride = 16
for the encoder module strikes the best trade-oﬀbetween speed and accuracy.
The performance is marginally improved when using output stride = 8 for the
encoder module at the cost of extra computation complexity.
Modiﬁed Aligned Xception
The Xception model has shown promising image classiﬁcation results on ImageNet with fast computation. More recently, the MSRA team modiﬁes
the Xception model (called Aligned Xception) and further pushes the performance in the task of object detection. Motivated by these ﬁndings, we work in
the same direction to adapt the Xception model for the task of semantic image
segmentation. In particular, we make a few more changes on top of MSRA’s
modiﬁcations, namely (1) deeper Xception same as in except that we do
not modify the entry ﬂow network structure for fast computation and memory
eﬃciency, (2) all max pooling operations are replaced by depthwise separable
convolution with striding, which enables us to apply atrous separable convolution to extract feature maps at an arbitrary resolution (another option is to
extend the atrous algorithm to max pooling operations), and (3) extra batch
normalization and ReLU activation are added after each 3 × 3 depthwise
convolution, similar to MobileNet design . See Fig. 4 for details.
Experimental Evaluation
We employ ImageNet-1k pretrained ResNet-101 or modiﬁed aligned
Xception to extract dense feature maps by atrous convolution. Our implementation is built on TensorFlow and is made publicly available.
DeepLabv3+: Encoder-Decoder with Atrous Separable Convolution
Conv 32, 3x3, stride 2
Conv 64, 3x3
Sep Conv 128, 3x3
Sep Conv 128, 3x3
Sep Conv 128, 3x3, stride 2
Conv 128, 1x1
Sep Conv 256, 3x3
Sep Conv 256, 3x3
Sep Conv 256, 3x3, stride 2
Conv 256, 1x1
Sep Conv 728, 3x3
Sep Conv 728, 3x3
Sep Conv 728, 3x3, stride 2
Conv 728, 1x1
Entry flow
Sep Conv 728, 3x3
Sep Conv 728, 3x3
Sep Conv 728, 3x3
Middle flow
Repeat 16 times
Sep Conv 728, 3x3
Sep Conv 1024, 3x3
Sep Conv 1024, 3x3, stride 2
Conv 1024, 1x1
Sep Conv 1536, 3x3
Sep Conv 1536, 3x3
Sep Conv 2048, 3x3
Fig. 4. We modify the Xception as follows: (1) more layers (same as MSRA’s modiﬁcation except the changes in Entry ﬂow), (2) all the max pooling operations are replaced
by depthwise separable convolutions with striding, and (3) extra batch normalization
and ReLU are added after each 3 × 3 depthwise convolution, similar to MobileNet.
The proposed models are evaluated on the PASCAL VOC 2012 semantic
segmentation benchmark which contains 20 foreground object classes and one
background class. The original dataset contains 1, 464 (train), 1, 449 (val), and
1, 456 (test) pixel-level annotated images. We augment the dataset by the extra
annotations provided by , resulting in 10, 582 (trainaug) training images.
The performance is measured in terms of pixel intersection-over-union averaged
across the 21 classes (mIOU).
We follow the same training protocol as in and refer the interested readers
to for details. In short, we employ the same learning rate schedule (i.e.,
“poly” policy and same initial learning rate 0.007), crop size 513 × 513,
ﬁne-tuning batch normalization parameters when output stride = 16, and
random scale data augmentation during training. Note that we also include batch
normalization parameters in the proposed decoder module. Our proposed model
is trained end-to-end without piecewise pretraining of each component.
L.-C Chen, Y. Zhu, G. Papandreou, F. Schroﬀ, and H. Adam
Decoder Design Choices
We deﬁne “DeepLabv3 feature map” as the last feature map computed by
DeepLabv3 (i.e., the features containing ASPP features and image-level features), and [k × k, f] as a convolution operation with kernel k × k and f ﬁlters.
When employing output stride = 16, ResNet-101 based DeepLabv3 bilinearly upsamples the logits by 16 during both training and evaluation. This
simple bilinear upsampling could be considered as a naive decoder design, attaining the performance of 77.21% on PASCAL VOC 2012 val set and is 1.2%
better than not using this naive decoder during training (i.e., downsampling
groundtruth during training). To improve over this naive baseline, our proposed
model “DeepLabv3+” adds the decoder module on top of the encoder output, as
shown in Fig. 2. In the decoder module, we consider three places for diﬀerent design choices, namely (1) the 1×1 convolution used to reduce the channels of the
low-level feature map from the encoder module, (2) the 3 × 3 convolution used
to obtain sharper segmentation results, and (3) what encoder low-level features
should be used.
To evaluate the eﬀect of the 1 × 1 convolution in the decoder module, we
employ [3 × 3, 256] and the Conv2 features from ResNet-101 network backbone,
i.e., the last feature map in res2x residual block (to be concrete, we use the
feature map before striding). As shown in Tab. 1, reducing the channels of the
low-level feature map from the encoder module to either 48 or 32 leads to better
performance. We thus adopt [1 × 1, 48] for channel reduction.
We then design the 3 × 3 convolution structure for the decoder module and
report the ﬁndings in Tab. 2. We ﬁnd that after concatenating the Conv2 feature
map (before striding) with DeepLabv3 feature map, it is more eﬀective to employ
two 3×3 convolution with 256 ﬁlters than using simply one or three convolutions.
Changing the number of ﬁlters from 256 to 128 or the kernel size from 3 × 3 to
1×1 degrades performance. We also experiment with the case where both Conv2
and Conv3 feature maps are exploited in the decoder module. In this case, the
decoder feature map are gradually upsampled by 2, concatenated with Conv3
ﬁrst and then Conv2, and each will be reﬁned by the [3 × 3, 256] operation. The
whole decoding procedure is then similar to the U-Net/SegNet design .
However, we have not observed signiﬁcant improvement. Thus, in the end, we
adopt the very simple yet eﬀective decoder module: the concatenation of the
DeepLabv3 feature map and the channel-reduced Conv2 feature map are reﬁned
by two [3 × 3, 256] operations. Note that our proposed DeepLabv3+ model has
output stride = 4. We do not pursue further denser output feature map (i.e.,
output stride < 4) given the limited GPU resources.
ResNet-101 as Network Backbone
To compare the model variants in terms of both accuracy and speed, we report
mIOU and Multiply-Adds in Tab. 3 when using ResNet-101 as network
backbone in the proposed DeepLabv3+ model. Thanks to atrous convolution, we
DeepLabv3+: Encoder-Decoder with Atrous Separable Convolution
77.61% 77.92% 78.16% 78.21% 77.94%
Table 1. PASCAL VOC 2012 val set. Eﬀect of decoder 1 × 1 convolution used to
reduce the channels of low-level feature map from the encoder module. We ﬁx the
other components in the decoder structure as using [3 × 3, 256] and Conv2.
3 × 3 Conv
Conv2 Conv3
[3 × 3, 256]
[3 × 3, 256] × 2 78.85%
[3 × 3, 256] × 3 78.02%
[3 × 3, 128]
[1 × 1, 256]
[3 × 3, 256]
Table 2. Eﬀect of decoder structure when ﬁxing [1 × 1, 48] to reduce the encoder
feature channels. We found that it is most eﬀective to use the Conv2 (before striding)
feature map and two extra [3 × 3, 256] operations. Performance on VOC 2012 val set.
Decoder MS Flip mIOU Multiply-Adds
train OS eval OS
Table 3. Inference strategy on the PASCAL VOC 2012 val set using ResNet-101.
train OS: The output stride used during training. eval OS: The output stride used
during evaluation. Decoder: Employing the proposed decoder structure. MS: Multiscale inputs during evaluation. Flip: Adding left-right ﬂipped inputs.
are able to obtain features at diﬀerent resolutions during training and evaluation
using a single model.
L.-C Chen, Y. Zhu, G. Papandreou, F. Schroﬀ, and H. Adam
Top-1 Error Top-5 Error
Reproduced ResNet-101
Modiﬁed Xception
Table 4. Single-model error rates on ImageNet-1K validation set.
Baseline: The ﬁrst row block in Tab. 3 contains the results from showing
that extracting denser feature maps during evaluation (i.e., eval output stride =
8) and adopting multi-scale inputs increases performance. Besides, adding leftright ﬂipped inputs doubles the computation complexity with only marginal
performance improvement.
Adding decoder: The second row block in Tab. 3 contains the results when
adopting the proposed decoder structure. The performance is improved from
77.21% to 78.85% or 78.51% to 79.35% when using eval output stride = 16 or 8,
respectively, at the cost of about 20B extra computation overhead. The performance is further improved when using multi-scale and left-right ﬂipped inputs.
Coarser feature maps: We also experiment with the case when using
train output stride = 32 (i.e., no atrous convolution at all during training) for
fast computation. As shown in the third row block in Tab. 3, adding the decoder
brings about 2% improvement while only 74.20B Multiply-Adds are required.
However, the performance is always about 1% to 1.5% below the case in which
we employ train output stride = 16 and diﬀerent eval output stride values. We
thus prefer using output stride = 16 or 8 during training or evaluation depending
on the complexity budget.
Xception as Network Backbone
We further employ the more powerful Xception as network backbone. Following , we make a few more changes, as described in Sec. 3.2.
ImageNet pretraining: The proposed Xception network is pretrained on
ImageNet-1k dataset with similar training protocol in . Speciﬁcally, we
adopt Nesterov momentum optimizer with momentum = 0.9, initial learning
rate = 0.05, rate decay = 0.94 every 2 epochs, and weight decay 4e −5. We
use asynchronous training with 50 GPUs and each GPU has batch size 32 with
image size 299×299. We did not tune the hyper-parameters very hard as the goal
is to pretrain the model on ImageNet for semantic segmentation. We report the
single-model error rates on the validation set in Tab. 4 along with the baseline
reproduced ResNet-101 under the same training protocol. We have observed
0.75% and 0.29% performance degradation for Top1 and Top5 accuracy when
not adding the extra batch normalization and ReLU after each 3 × 3 depthwise
convolution in the modiﬁed Xception.
The results of using the proposed Xception as network backbone for semantic
segmentation are reported in Tab. 5.
Baseline: We ﬁrst report the results without using the proposed decoder in
the ﬁrst row block in Tab. 5, which shows that employing Xception as network
DeepLabv3+: Encoder-Decoder with Atrous Separable Convolution
backbone improves the performance by about 2% when train output stride =
eval output stride = 16 over the case where ResNet-101 is used. Further improvement can also be obtained by using eval output stride = 8, multi-scale
inputs during inference and adding left-right ﬂipped inputs. Note that we do not
employ the multi-grid method , which we found does not improve the
performance.
Adding decoder: As shown in the second row block in Tab. 5, adding
decoder brings about 0.8% improvement when using eval output stride = 16 for
all the diﬀerent inference strategies. The improvement becomes less when using
eval output stride = 8.
Using depthwise separable convolution: Motivated by the eﬃcient computation of depthwise separable convolution, we further adopt it in the ASPP
and the decoder modules. As shown in the third row block in Tab. 5, the computation complexity in terms of Multiply-Adds is signiﬁcantly reduced by 33%
to 41%, while similar mIOU performance is obtained.
Pretraining on COCO: For comparison with other state-of-art models, we
further pretrain our proposed DeepLabv3+ model on MS-COCO dataset ,
which yields about extra 2% improvement for all diﬀerent inference strategies.
Pretraining on JFT: Similar to , we also employ the proposed Xception
model that has been pretrained on both ImageNet-1k and JFT-300M dataset
 , which brings extra 0.8% to 1% improvement.
Test set results: Since the computation complexity is not considered in the
benchmark evaluation, we thus opt for the best performance model and train it
with output stride = 8 and frozen batch normalization parameters. In the end,
our ‘DeepLabv3+’ achieves the performance of 87.8% and 89.0% without and
with JFT dataset pretraining.
Qualitative results: We provide visual results of our best model in Fig. 6.
As shown in the ﬁgure, our model is able to segment objects very well without
any post-processing.
Failure mode: As shown in the last row of Fig. 6, our model has diﬃculty
in segmenting (a) sofa vs. chair, (b) heavily occluded objects, and (c) objects
with rare view.
Improvement along Object Boundaries
In this subsection, we evaluate the segmentation accuracy with the trimap experiment to quantify the accuracy of the proposed decoder module near
object boundaries. Speciﬁcally, we apply the morphological dilation on ‘void’ label annotations on val set, which typically occurs around object boundaries. We
then compute the mean IOU for those pixels that are within the dilated band
(called trimap) of ‘void’ labels. As shown in Fig. 5 (a), employing the proposed
decoder for both ResNet-101 and Xception network backbones improves
the performance compared to the naive bilinear upsampling. The improvement
is more signiﬁcant when the dilated band is narrow. We have observed 4.8%
and 5.4% mIOU improvement for ResNet-101 and Xception respectively at the
L.-C Chen, Y. Zhu, G. Papandreou, F. Schroﬀ, and H. Adam
Decoder MS Flip SC COCO JFT mIOU Multiply-Adds
train OS eval OS
Table 5. Inference strategy on the PASCAL VOC 2012 val set when using modiﬁed Xception. train OS: The output stride used during training. eval OS: The
output stride used during evaluation. Decoder: Employing the proposed decoder structure. MS: Multi-scale inputs during evaluation. Flip: Adding left-right ﬂipped inputs.
SC: Adopting depthwise separable convolution for both ASPP and decoder modules.
COCO: Models pretrained on MS-COCO. JFT: Models pretrained on JFT.
smallest trimap width as shown in the ﬁgure. We also visualize the eﬀect of
employing the proposed decoder in Fig. 5 (b).
Experimental Results on Cityscapes
In this section, we experiment DeepLabv3+ on the Cityscapes dataset , a
large-scale dataset containing high quality pixel-level annotations of 5000 images
(2975, 500, and 1525 for the training, validation, and test sets respectively) and
about 20000 coarsely annotated images.
As shown in Tab. 7 (a), employing the proposed Xception model as network
backbone (denoted as X-65) on top of DeepLabv3 , which includes the ASPP
DeepLabv3+: Encoder-Decoder with Atrous Separable Convolution
Deep Layer Cascade (LC) 
TuSimple 
Large Kernel Matters 
Multipath-ReﬁneNet 
ResNet-38 MS COCO 
PSPNet 
IDW-CNN 
CASIA IVA SDN 
DeepLabv3 
DeepLabv3-JFT 
DeepLabv3+ (Xception)
DeepLabv3+ (Xception-JFT)
Table 6. PASCAL VOC 2012 test set results with top-performing models.
mean IOU (%)
Trimap Width (pixels)
Xception w/ Decoder
ResNet−101 w/ Decoder
Xception w/ BU
ResNet−101 w/ BU
w/ Decoder
(a) mIOU vs. Trimap width
(b) Decoder eﬀect
Fig. 5. (a) mIOU as a function of trimap band width around the object boundaries
when employing train output stride = eval output stride = 16. BU: Bilinear upsampling. (b) Qualitative eﬀect of employing the proposed decoder module compared with
the naive bilinear upsampling (denoted as BU). In the examples, we adopt Xception
as feature extractor and train output stride = eval output stride = 16.
module and image-level features , attains the performance of 77.33% on the
validation set. Adding the proposed decoder module signiﬁcantly improves the
performance to 78.79% (1.46% improvement). We notice that removing the augmented image-level feature improves the performance to 79.14%, showing that
in DeepLab model, the image-level features are more eﬀective on the PASCAL
VOC 2012 dataset. We also discover that on the Cityscapes dataset, it is eﬀective to increase more layers in the entry ﬂow in the Xception , the same as
what did for the object detection task. The resulting model building on top
of the deeper network backbone (denoted as X-71 in the table), attains the best
performance of 79.55% on the validation set.
After ﬁnding the best model variant on val set, we then further ﬁne-tune
the model on the coarse annotations in order to compete with other state-of-art
L.-C Chen, Y. Zhu, G. Papandreou, F. Schroﬀ, and H. Adam
Fig. 6. Visualization results on val set. The last row shows a failure mode.
Backbone Decoder ASPP Image-Level mIOU
ResNet-38 
PSPNet 
Mapillary 
DeepLabv3+
(a) val set results
(b) test set results
Table 7. (a) DeepLabv3+ on the Cityscapes val set when trained with train ﬁne set.
(b) DeepLabv3+ on Cityscapes test set. Coarse: Use train extra set (coarse annotations) as well. Only a few top models are listed in this table.
models. As shown in Tab. 7 (b), our proposed DeepLabv3+ attains a performance
of 82.1% on the test set, setting a new state-of-art performance on Cityscapes.
Conclusion
Our proposed model “DeepLabv3+” employs the encoder-decoder structure where
DeepLabv3 is used to encode the rich contextual information and a simple yet
eﬀective decoder module is adopted to recover the object boundaries. One could
also apply the atrous convolution to extract the encoder features at an arbitrary
resolution, depending on the available computation resources. We also explore
the Xception model and atrous separable convolution to make the proposed
model faster and stronger. Finally, our experimental results show that the proposed model sets a new state-of-the-art performance on PASCAL VOC 2012 and
Cityscapes datasets.
Acknowledgments We would like to acknowledge the valuable discussions
with Haozhi Qi and Jifeng Dai about Aligned Xception, the feedback from Chen
Sun, and the support from Google Mobile Vision team.
DeepLabv3+: Encoder-Decoder with Atrous Separable Convolution