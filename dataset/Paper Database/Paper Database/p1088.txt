MIT Open Access Articles
On the use of deep learning for computational imaging
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Barbastathis, George et al., "On the use of deep learning for computational imaging."
Optica 6, 8 : p. 921-943 doi. 10.1364/OPTICA.6.000921 ©2019 Authors
As Published: 
Publisher: The Optical Society
Persistent URL: 
Version: Final published version: final published article, as it appeared in a journal, conference
proceedings, or other formally published context
Terms of Use: Article is made available in accordance with the publisher's policy and may be
subject to US copyright law. Please refer to the publisher's site for terms of use.
On the use of deep learning for computational
GEORGE BARBASTATHIS,1,2,* AYDOGAN OZCAN,3
AND GUOHAI SITU4,5
1Department of Mechanical Engineering, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge,
Massachusetts 02139-4301, USA
2Singapore-MIT Alliance for Research and Technology (SMART) Centre, 1 Create Way, Singapore 138602, Singapore
3Department of Electrical & Computer Engineering, and Department of Bioengineering, University of California at Los Angeles, Engineering IV Building,
Los Angeles, California 90095-1594, USA
4Shanghai Institute of Optics and Fine Mechanics, Chinese Academy of Sciences, Shanghai 201800, China
5University of the Chinese Academy of Sciences, Beijing 100049, China
*Corresponding author: 
Received 13 March 2019; revised 16 May 2019; accepted 16 May 2019 (Doc. ID 362339); published 25 July 2019
Since their inception in the 1930–1960s, the research disciplines of computational imaging and machine learning have
followed parallel tracks and, during the last two decades, experienced explosive growth drawing on similar progress in
mathematical optimization and computing hardware. While these developments have always been to the benefit of
image interpretation and machine vision, only recently has it become evident that machine learning architectures, and
deep neural networks in particular, can be effective for computational image formation, aside from interpretation. The
deep learning approach has proven to be especially attractive when the measurement is noisy and the measurement
operator ill posed or uncertain. Examples reviewed here are: super-resolution; lensless retrieval of phase and complex
amplitude from intensity; photon-limited scenes, including ghost imaging; and imaging through scatter. In this paper,
we cast these works in a common framework. We relate the deep-learning-inspired solutions to the original computational imaging formulation and use the relationship to derive design insights, principles, and caveats of more general
applicability. We also explore how the machine learning process is aided by the physics of imaging when ill posedness
and uncertainties become particularly severe. It is hoped that the present unifying exposition will stimulate further
progress in this promising field of research.
© 2019 Optical Society of America under the terms of the OSA Open Access
Publishing Agreement
 
1. INTRODUCTION
Computational imaging (CI) is a class of imaging systems that,
starting from an imperfect physical measurement and prior
knowledge about the class of objects or scenes being imaged,
deliver estimates of a specific object or scene presented to the imaging system . This is shown schematically in Fig. 1. The
physical measurement, typically of light intensity on a digital
camera, i.e., sampled on a pixel grid, we will refer to as raw measurement, or raw intensity image. The analytical relationship between the object and raw intensity image is the forward operator,
whereas the analytical expression of prior knowledge is the regularizer, or simply prior. The premise on the existence of the regularizer is that, in general, any object that we measure in a natural
scene obeys patterns that introduce correlations in the object’s
spatial features ; these patterns, if discovered, should help to
reduce uncertainty in the recovery of the object, i.e., to reduce
ill posedness. As we will review in some detail in Section 2, in
a CI system, the estimate of the object is generally obtained by
solving an optimization problem. The functional to be optimized
promotes competition between fitting the raw measurement
according to the forward operator, and observing the prior.
We refer to the optimization outcome as object estimate, scene
estimate, solution to the inverse problem or, simply, image.
The motivation for employing computation instead of using
the raw intensity image directly is that the latter may not be
readily interpretable. The information may be there, but incomplete or hidden; so further processing is needed to complement
and reveal it. The CI way of thinking also opens up new directions
in optical design because it removes the traditional requirement
that the raw image itself must observe some metric of spatial conformity and fidelity with respect to the object. Thus, it is no
longer necessary to fully correct all the aberrations before forming
the raw image; some of the correction may be assigned to the
computational components of the system. Perhaps more importantly, with CI, it becomes possible to create optical systems producing raw images that are deliberately spatially non-conforming
to their respective scenes. The intent would then be after computation to form representations that are not readily available in
the traditional spatially conforming intensity images: tomography,
i.e., reconstructing the interior of a volume from projections; and
2334-2536/19/080921-23 Journal © 2019 Optical Society of America
Review Article
Vol. 6, No. 8 / August 2019 / Optica
quantitative phase retrieval, i.e., extracting the phase of the optical
field from the raw intensity, are both classical examples of this CI
mode of operation.
Machine learning (ML) is a class of function interpolation algorithms that are informed by examples of the function being
interpolated . ML architectures are very general, e.g., they
may be layered, recursive, etc. Later in this review, we will see that
it is even possible to include explicit physical models as part of the
computation. The specific architecture of interest here is based on
the neural network (NN), a multilayered computational geometry. Each layer is composed of simple nonlinear processing units,
also referred to as activation units (or elements); and each unit
receives its inputs as weighted sums from the previous layer (except the very first layer, whose inputs are the quantities we wish
the NN to process.) Until about two decades ago, students were
advised to design NNs with up to three layers: the input layer, the
hidden layer, and the output layer. Recent progress in ML has
demonstrated the superiority of architectures with many more
than three layers, referred to as deep NNs (DNNs) .
Figure 2 is a simplified schematic diagram of the multi-layered
DNN architecture.
The process of adjusting the connection weights is known as
training and is generally implemented as an optimization routine.
In the supervised training mode, which is of most interest here,
the weights are adjusted by minimizing the training loss function
(TLF), a metric of the difference between the actual DNN output
and its desired output. The former is determined by known DNN
input–output pairs, the training examples. Thus, already the use
of optimization emerges as a broad common ground between CI
and ML. However, there is an important difference: in traditional
CI, the optimization routine must be computed for each imaging
operation, whereas in ML, the optimization is executed only
during the training phase of the DNN. Imaging operations using
ML, thus, are generally very fast because the trained DNN is a
feed-forward computational architecture. Typical computation
times may be from a few minutes to a few hours per frame
for typical CI optimizations; several hours or days for DNN
training; and milliseconds per frame for a trained DNN.
Thus, as a rule, the ML approach yields images at rates approaching real time, even in common laptop computer or smartphone
processors.
During the past few years, a number of researchers have shown
convincingly that the ML formulation is not only computationally efficient, but it also yields high-quality solutions in several CI
problems. In this approach, shown in Fig. 3, the raw intensity
image is fed into a computational engine specifically incorporating ML components, i.e., multilayered structures as in Fig. 2 and
trained from examples—taking the place of the generic computational engine in Fig. 1. CI problems so solved have included lensless imaging, imaging through scatter, bandwidth- or samplinglimited imaging (also referred to as “super-resolution”), and
extremely noisy imaging, e.g., under the constraint of very low
photon counts. With few early exceptions , the ML architectures used in these CI problems have relied on DNNs. The main
purpose of this paper is to review these developments, providing a
common framework for understanding and improving upon
them as well as developing underlying design principles and
intuition.
It is important early on to make a clear distinction between
image interpretation versus image formation (or image transformation, as it is referred to in the computer vision research
General computational imaging system. The illumination
source or source array is shaped by the condenser optics according to
the operator H i before reaching the object f . The radiation is subsequently shaped again by the imaging optics with collection operator
H c, and the intensity g is sampled by the digital camera. The signal
g is then processed by the CI algorithm, which takes into account prior
knowledge Φ about the class of objects being imaged (regularizer) in addition to the physical models H i, H c. The result of the computation is
the image ˆf . For detailed notation and description, please see Section 2.
Simplified schematic of a generic deep neural network. The
input layer consists of the components of the input vector u. The dark
circles denote the activation (nonlinear) elements, whereas wl
weight connecting the j-th activation element in layer l with the
k-th activation element in layer l  1. The last L-th layer produces
an estimate of the output vector v. The quality of the approximation
relating input to output depends on the training; please see Section 3.
CI architecture of Fig. 1 with an ML engine producing the
image. As we see in Section 4, the ML engine generally includes a multilayered architecture such as the one shown in Fig. 2 and is informed on
the physics of the illumination and collection optics, H i, H c, respectively,
and the prior Φ. The three components H i, H c, Φ or their combinations
are incorporated in the ML engine either explicitly as approximants
(Section 3.F) or implicitly through training with examples.
Review Article
Vol. 6, No. 8 / August 2019 / Optica
community), and emphasize that the work presented here is exclusive to the latter. Methods for the former typically are agnostic
to the physics of imaging, instead assuming that the input data to
the ML algorithm are a vector or matrix of pixel values produced
by a generic imaging system. These data are then used to arrive at
a scalar or low-dimensional vector representation expressing classification or prediction outcomes e.g., identification of objects or
faces in the scene, translating written text found in the scene, etc.
The images used as input to the ML algorithm are assumed to
meet the same spatial conformity and fidelity criteria that a human performing the image interpretation task would have demanded. Traditional image interpretation, in other words,
aims to directly replace the human’s function with an algorithm,
with the algorithm operating on input similar to what the human’s
visual system would (or should) be receiving.
In contrast, the ML algorithms we are concerned with here are
cognizant of imaging physics but agnostic to the contents of the
object or scene, except to the degree that the contents provide
regularizing priors. ML executes a regression operation: given an
incomplete representation of the scene in the raw intensity measured by the camera, ML replaces the raw scene with one of
higher spatial fidelity. In the process, information such as the
phase of the field, which in the measured raw intensity may have
been hidden or incomplete, is revealed. The user of the MLproduced images may well be a human demanding a guarantee
that the images meet certain standards of fidelity to the true scene;
or the images could be fed to another ML algorithm of the more
common image interpretation type designed for a similar kind of
spatial fidelity as the human would have required.
When the raw intensity data on the camera are not readily
usable by a human or a traditional image interpretation algorithm
for the purposes of recognition, an interesting dilemma occurs:
does it make sense to first form an image of high spatial fidelity
and then feed it to the image interpretation algorithm? Or should
one design the image interpretation algorithm anew, operating
directly on the incomplete raw intensity data? (For the human
user, there is of course no choice: spatial fidelity is imperative.)
The second approach has often been proven to be productive,
e.g., for digital holographic particle localization and tracking
 and diseased cell identification ; and recognition
of handwritten characters through multimode fibers . In this
paper, we will not take a position on this dilemma or discuss it any
further; yet, we wish to point out that, in our opinion, the dilemma is one of the least explored and most fascinating in the
rapidly progressing field of merging ML into CI.
After these introductory remarks, in Section 2, we briefly summarize the principles of CI to establish notation and a common
framework. With similar goals, we summarize ML architectures
and design principles in Section 3, emphasizing those that have
proven to be most useful for CI to-date and the principles for
including physical knowledge of the forward operator into the
ML computation. In Section 4, we first describe how ML principles apply specifically to CI, and then in Sections 4.A–4.D, respectively, we review the related literature on the following
canonical CI problems: super-resolution, i.e., recovery of lost
or suppressed spatial frequencies; lensless phase retrieval from
intensity; imaging of dark scenes; and imaging through scatter.
The use of ML in magnetic resonance imaging (MRI) and
x-ray tomography has been reviewed elsewhere in detail
 ; and overviews specific to the use of ML in computer
vision-related reconstruction problems and quantitative
phase imaging have also been published. Concluding remarks and further suggestions for future work are given in
Section 5.
2. OVERVIEW OF COMPUTATIONAL IMAGING
A. General Formulation
Referring to Fig. 1, let f denote the object or scene that the imaging system’s user wishes to retrieve. To avoid complications that
are beyond the scope of this review, we will assume that even
though objects are generally continuous, a discrete representation
suffices . Therefore, f is a vector or matrix matching the
spatial dimension where the object is sampled. Light–object interaction is denoted by the illumination operator H i, whereas the
collection operator H c models propagation through the rest of the
optical system.
How f , H i, and H c are constructed depends on the state of
the illumination and how light is scattered by the object and
optical elements in the system. Possible models for H c are thin
transparency, Born or Rytov expansions, beam propagation, transmission matrix, etc. If multiple scattering is present in the optical
system, e.g., a strong diffuser with f denoting the index of refraction distribution, then H c becomes random and nonlinear.
In Section 4.D, we discuss how using ML improves image quality
in the latter most challenging case.
The illumination operator H i is often designed to improve the
condition of the inverse problem. Structured and coded illumination have been important for CI since the 1960s for improving
resolution and sectioning contrast , and more recently for
phase retrieval . Another interesting instance is ghost
imaging, whose ML implementation we discuss in Section 4.C.
Similarly, coded aperture approaches are effective for designing
the pupil function in H c to improve performance .
If the thin object approximation is valid, H if is simply a
multiplication whose outcome serves as input to the collection
optics. For quasi-monochromatic spatially incoherent illumination, f is the modulus-squared of the complex transmittance
as a function of lateral position, and H if is the intensity immediately after the object plane. For monochromatic spatially coherent
light, f is the thin object’s complex transmittance, and H if is the
optical field. Sections 4.A and 4.B describe the ML implementations of super-resolution and quantitative phase retrieval, respectively, where these assumptions hold.
The output of the collection optics is optical intensity g,
sampled and digitized at the output (camera) plane. After aggregating the illumination and collection models into the forward
operator H  H cH i, the noiseless measurement model is
Since the measurements are by necessity discrete, g is arranged
into a matrix of the appropriate dimension or rastered into a
one-dimensional vector. For a single raw intensity image, g
may be up to two dimensional; however, if scanning is involved
(as, e.g., in computed tomography where multiple projections are
obtained with the object rotated at various angles), then g must be
augmented accordingly. In ghost imaging, g is a one-dimensional
intensity signal whose length equals the number of illumination
patterns applied on the object.
Review Article
Vol. 6, No. 8 / August 2019 / Optica
Uncertainty in the measurements and/or the forward operator
is the main challenge in inverse problems. Typically, an optical
measurement is subject to signal-dependent Poisson statistics
due to the random arrival of signal photons, and additive signal-independent statistics due to thermal electrons in the detector
circuitry. Thus, the deterministic model (1) should be replaced by
g  PfHf g  T :
Here, P generates a Poisson random process with arrival rate
equal to its argument; and T is the thermal random process often
modeled as additive white Gaussian noise (AWGN). In realistic
sensors, noise may originate from multiple causes, such as environmental disturbances. For large photon counts, signal quantization is also modeled as AWGN. The nature of the noise
underlies many choices in our subsequent developments, e.g.,
the form of the Tikhonov functional (3) and the related
Wiener filter . Extremely noisy imaging conditions where
ML can be of value are discussed in Section 4.C.
B. Linear Inverse Problems, Regularization, and
For linear forward operators H, the image is obtained by minimizing the Tikhonov functional
ˆf  argmin
2  αΦf g,
where k · k2 denotes the L2 norm. The first term expresses fitness,
i.e., matching in the least-squares sense the measurement to the
forward model for the assumed object. The fitness term is constructed for AWGN errors, even though it is often used with more
general noise models (2). The regularization parameter α expresses our relative belief in the measurement fitness versus
our prior knowledge. Setting α  0 to obtain the image from
the fitness term yields only the pseudo-inverse solution, or its
Moore–Penrose improvement . The results are often
prone to artifacts and seldom satisfactory, due to ill posedness
in the forward operator H. To improve, the second regularizing
term Φf  is meant to compete with the fitness term, by driving
the estimate ˆf to also match prior knowledge about the class of
objects being imaged.
Andrey Tikhonov proposed the regularizer
Φf   kf k2
The solution to the inverse problem is then obtained explicitly as
ˆf  H T H  α1−1H T g,
where H T is the transpose of H, and 1 is the unit tensor matching
the dimension of H T H. For α  0, this expression reduces to the
pseudo-inverse solution, in imaging also known as direct deconvolution. Equation (5) is quite similar to the Wiener filter if
α is set to equal the noise-to-signal ratio. In the Tikhonov–Wiener
solution, improvement over the pseudo-inverse is expected because ˆf includes amplified noise, and keeping the L2 norm to a
low value competes with the amplification. Unfortunately, due to
the action of H T, the estimate ˆf is typically low-pass filtered.
Instead of (4), modern regularizers Φ· promote sparsity in
the object f or its transformed version Sf . The linear transform
S that converts the object to a sparse representation is called the
sparsifier. Sparse or compressive sensing and imaging schemes
 are intuitively justified because we know that most signals, especially images and video, are compressible; therefore, the
degrees of freedom expressed by most images of interest are far
fewer than the raw number of pixels implies. Moreover, if the
raw images are corrupted by noise, S most likely will sparsify
the signal but not the noise; as a result, the noise should be penalized and vulnerable to removal by the regularizer.
Compressed sensing is realized with either a fixed or learned set
of basis functions. A common example of a fixed set is wavelet
decomposition, which is highly redundant and, therefore, typically results in a highly sparse representation of the signal. This is also the reason that wavelets are the basis of the JPEG
compressed image format . Another popular alternative is the
class of nonlinear diffusion and Weikert operators . The
special case of
∂TV ≡k∇x,yf k2,
known as total variation (TV), rests on the notion that most images become sparse when edge enhanced. TV was used in one of
the earliest demonstrations of compressed sensing in tomography
 , and subsequently for other CI problems including confocal
microscopy , phase retrieval , holographic superlocalization , and phase tomography .
Early instances of the idea that sparse representations may instead be learned from examples were motivated by the power spectral density of natural images and analogies with the primate
visual cortex . This led to the concept of a dictionary operator D, ideally chosen such that the transformation
results in a representation whose L0 norm k ˜f k0 is minimized
 , i.e., a representation that is sparsest. Dictionaries have
been used extensively in CI and inverse problems, e.g., for image
upsampling , a form of super-resolution—see Section 4.A.
Sparsity-promoting regularizers typically involve non-differentiable metrics, e.g., L1 , so their iterative optimization requires
the use of proximal gradients . Common algorithms include
lasso , iterative shrinkage and thresholding algorithm (ISTA)
 and its variants fast ISTA (FISTA) and TwIST ,
message passing , coordinate descent (CoD) , alternating direction method of multipliers (ADMM) , and Adam
 . Shortcomings in the iterative procedures specified by these
algorithms, ISTA and CoD especially, inspired the idea of training
the nonlinearities in a DNN to approximate the sparse regularizer
flexibility
computational
efficiency
 . We will pick up on this point in Section 3.F.
3. OVERVIEW OF NEURAL NETWORKS
A. Neural Network Fundamentals
The generic computational architecture of a NN is shown in
Fig. 2. Its purpose is to learn and implement the map relating
the input vector u to the output vector v. The standard NN consists of layers, and each layer consists of several simple activation
units that are connected to the units in the preceding and succeeding layers. The number of layers L is the depth of the
NN. The input vector is fed into the input layer l  1, and
the output vector emerges from the output layer l  L; the
counter l is used to enumerate the layers. We will use the notation
as the input–output relationship of the NN. The networks considered in this review for computational image formation range in
Review Article
Vol. 6, No. 8 / August 2019 / Optica
depth from L ∼10 to 20. For pattern recognition tasks, e.g.,
ImageNet, it is not uncommon to have L in excess of 50.
The number of units in each layer is the layer’s width. Unit
progressions.
Classification tasks generally produce representations of much
lowerdimension than that of the input images; therefore, the width
decreases progressively toward the output, following the contracting architecture in Fig. 4(a). Up-sampling tasks, as in the image super-resolution examples that we discuss in Section 4.A,
require output dimension larger than the input, so expanding architectures such as Fig. 4(b) may be considered. The concatenation
of the two is the encoder–decoder architecture in Fig. 4(c). The
unit widths progressively decrease, forming a compressed (encoded) representation of the input near the waist of the structure,
and then progressively increase again to produce the final reconstructed (decoded) image. In the encoder–decoder structure, skip
connections are also used to transfer information directly between
layers of the same width, bypassing the encoded channels. The
U-net architecture (see also Section 3.D and Fig. 7) was an
early successful convolutional implementation of the encoder–
decoder principle, and has been the most common implementation of ML approaches for CI to-date.
The layers are implemented as arrays of identical activation
units. Each activation unit expresses a simple nonlinear function,
typically the same for all units, which we will denote as r·. With
the exception of the first layer, the inputs to the units are weighted
sums of the outputs of the units in the previous layer. Let sl
denote the output of the k-th unit in the l-th layer, for
l  2, …, L. This is obtained as
where the summation takes place over all the units j of the previous layer, and wl−1
is included as a bias term. For the first layer,
where uk denotes the k-th element of the input vector u. Similarly,
at the last layer,
The coefficients wl
kj , l  1, …L −1 are the weights, and the procedure of specifying the weights’ values is referred to as training
the NN. It involves an optimization process over the training
examples, as we will see below. We will denote the collection
of all weights, arranged in the appropriate data structure, as W .
The nonlinearity r· in modern NNs is most commonly
implemented as the function
rξ  max0, ξ 
otherwise:
This is known as the rectifying linear unit (ReLU). Sigmoidal
functions, e.g., tanh·, used to be popular but have the problem
of stagnating derivatives for large arguments, hampering training
progress, as we will see later. This realization motivated the adoption of ReLU in modern practice and, arguably,
is one of the most significant factors making deep networks
trainable.
B. Training and Testing Neural Networks
The power of NNs to perform demanding computational tasks is
drawn from the complex connectivity between very simple activation units. The training process determines the connectivity
from examples, and can be supervised or unsupervised. The
supervised mode has generally been used for CI tasks, though unsupervised training has also been proposed . After
training, performance is evaluated from test examples that were
never presented during training.
The supervised training mode requires examples of inputs u
and the corresponding precisely known outputs ˜v. In practice,
one starts from a database of available examples and splits them
to training examples, validation examples, and test examples. The
training examples are used to specify the network weights; the
validation examples are used to determine when to stop training;
and the test examples are never to be used during the training
process, only to evaluate it.
Suppose N such pairs of examples fun, ˜vng, n  1, …, N
are available for training. The TLF is defined as the distance
between the desired outputs ˜vn and the actual outputs vn
summed over all training examples as
Lfun, ˜vngn1,…,N ≡
ENNun, ˜vn,
where E·, · is the distance metric. Common TLF choices for CI
problems are discussed in Section E.
Training specifies the network weights W so as to minimize
the TLF. Formally,
Simplified schematics of layer width progression strategies
for DNNs. (a) Contracting architecture. (b) Expanding architecture.
(c) Encoder–decoder architecture.
Review Article
Vol. 6, No. 8 / August 2019 / Optica
W  argmin
Lfun, ˜vngn1,…,N ,
W is the optimal set of weights. L depends on the weights
W implicitly through the input–output relationships of the NN
and its nonlinear units, Eqs. (8) and (9), respectively. Since the
activation functions are often non-differentiable, as in the ReLU
(12), the optimization is carried out iteratively as a proximal
gradient descent procedure. The iteration step t is called the training epoch, and the weights w evolve from each training epoch t to
the next as
wt  1  wt  η
Here, · denotes the proximal gradient, and η is the learning rate.
If η is too small, convergence is too slow; if it is too large, the
descent may oscillate around the minimum, also slowing progress
down. Thus, an ideal value for the training rate exists, typically
found by experimentation. At the beginning of training t  0,
the weights may be initialized randomly or according to other
schemes, if the problem justifies it; see , Section 8.4.
In Fig. 2 and Eq. (9), it is evident that the input–output
relationship NN· involves the computation of nested (implicit)
instances of the nonlinearity r·, and therefore, implementing the
derivative in (15) will require multiple applications of the chain
rule. This results in the computational procedure known as back
propagation (or back-prop.) To avoid excessive computational cost when dealing with large networks and training datasets, it is common to follow stochastic optimization approaches
 using small, randomly chosen subsets of the dataset in
each iteration. The optimizations are implemented in multidimensional numerical platforms such as TensorFlow or
Theano , and standard optimization libraries such as Adam
 or ML-specific libraries, e.g., Caffe , CNTK ,
and Keras .
If M pairs of test examples fum, ˜vmg, m  1, …, M are
available, the test error is computed as
Ltestfum, ˜vmgm1,…,M ≡
EtestNNum, ˜vm:
The test error metric Etest·, · quantifies the ability of the ML
engine to generalize, i.e., to produce correct outputs v even for
inputs u it has never seen before. Etest·, · need not be the same
as the training metric E; in fact, interesting conclusions are often
drawn by comparing performance in different metrics. Of course,
if the test metric is different from the training metric, there is no
guarantee that test performance will be monotonic with training
performance.
Even if the test metric is the same as the training metric,
generally the two do not evolve in the same way during training.
Recall that test examples are not supposed to be used in any way
during training; however, the test error may be monitored and
plotted as a function of training epoch t, and typically its evolution compared to the training error is as shown in Fig. 5. The
reason test error begins to increase after a certain training duration
is that overtraining results in overfitting: network function becomes so specific to the training examples that it damages generalization. It is tempting to use the test error evolution to
determine the optimum training duration topt; however, that is
not permissible because it would contaminate the integrity of
the test examples. This is the reason we set aside the third set
of validation examples; their only purpose is to monitor error
on them, and stop training just before this validation error starts
to increase. Assuming that all three sets of training, test, and validation examples have been drawn so that they are statistically
representative of the class of objects of interest, there is a reasonable guarantee that topt for validation and test error will be
The generalization ability of a computational learning architecture depends on many factors, including: the network architecture, i.e., the depth and the width of each layer; the choice
of nonlinearity (ReLU generally performs better than sigmoids,
as we saw); the quality of examples, i.e., if they were drawn representatively enough from the distribution of interest; and on the
accuracy of the numerical algorithm used to implement Eq. (14).
The design of NNs involves a certain degree of intuition and art,
and is beyond the scope of the present review. Detailed strategies
are given in textbooks, e.g., .
C. Weight Regularization
Overtraining and overfitting relate to the complexity of the model
being learned vis-à-vis the complexity of the NN. Here, we use the
term complexity in the context of degrees of freedom in a computational model . For learning models, in particular,
model complexity is known as Vapnik–Chervonenkis (VC) dimension , and it should match the complexity of
the computational task. Unfortunately, the VC dimension itself
is seldom directly computable except for the simplest ML
architectures.
In practice, degrees of freedom in deep learning models are
controlled by placing restrictions on the weights during the training process. This strategy is called weight regularization, and it is
motivated by arguments similar to those in Section 2.B in the
context of inverse problems. For example, weight decay 
modifies the training functional (14) as
˜W  argmin
Lfun, ˜vngn1,…,N  αkW k2
We recognize this as formally identical to the Tikhonov regularizer (4). The L2 term suppresses weight values overall. This
restricts the degrees of freedom the NN has to learn and improves
generalization. It can be shown from (17), e.g., –section 9.2,
that in the absence of error, i.e., if TLF  0, then the weights
decay exponentially:
Behavior of the training, test, and validation error as function
of training epoch t.
Review Article
Vol. 6, No. 8 / August 2019 / Optica
wt ∼w0e−ηαt;
hence, the name of the method.
More drastic is weight pruning: degrees of freedom are restricted by setting all weights below a certain threshold to zero,
or by eliminating the activation units associated with weak
weights . Recent improvements are Dropout 
and DropConnect , where instead multiple “thinned” versions of the network with shared weights are trained, each with
subsets of activations or weights, respectively, randomly removed.
The subnetworks are then recombined into a single network for
testing. More complicated Bayesian strategies sample models according to their posterior distributions given the training
data. Alternatively, generalizability can be improved with a sparsity-promoting TLF, e.g., the minimum absolute error (MAE);
see Section 3.E. One way to restrict weights appropriately for
many CI circumstances is to exploit invariances in the object class,
as in the convolutional model that we discuss in Section 3.D.
D. Convolutional Neural Networks
Certain tasks, such as speech and image processing, are naturally
invariant to temporal and spatial shifts, respectively. This may be
exploited to regularize the weights through convolutional architectures . The convolutional NN (CNN) principle limits the spatial range on the next layer, i.e., the neighborhood where
each unit is allowed to influence, and make the weights spatially
repeating. Thus, (9) is replaced by
where now ζl−1
is an intermediate output of the l −1-th layer,
r is the ReLU nonlinearity Eq. (12), and M is a small integer.
Typically, M  1, i.e., each unit influences only three immediate
neighbors in the intermediate layer. Equation (19) needs trivial
modification to remain valid at the layer edges.
In the encoder–decoder DNN architecture in Fig. 4(c), convolutions may be used to reduce the dimension of signal representation on the encoder side, i.e., to the left of the DNN waist,
and increase it on the decoder side. The convolutional encoding
principle is shown in Fig. 6. The intermediate outputs ζl−1
the l −1-th layer are first fed into a pooling layer, which combines neighboring ζ's to reduce the size of the l-th layer, typically
by a factor of two. To avoid overcomplicating the notation, we
will describe only the 1D case here; notation in higher dimensions
becomes tensorial without altering the concept. Common choices
for the pooling function are
stride: sl
k  ζl−1
max pooling: sl
2k−1, ζl−1
average pooling: sl
k  ζl−1
2k−1  ζl−1
Stride is the simplest, and computationally most efficient, since
half the convolutions need not even be carried out. Max pooling
allows dominant features to propagate downstream without being
influenced by their neighbors or by small shifts within a spatial
domain. A word of caution on terminology: in the ML literature,
this property is referred to as “invariance,” whereas what standard
optics textbooks define as shift invariance, the ML literature
refers to as “equivariance;” see , Sections 9.2 and 9.3.
A realistic implementation of an encoder–decoder type convolutional network from the first U-net realization is shown in
Fig. 7. The original purpose of U-net was image segmentation,
but similar ML architectures have found wide use in CI problems,
as we will see in Section 4. The layer width contracts through 3 ×
3 convolutions and pooling (max, in this case) in the earlier encoding stages of the network. Multiple channels of convolutions
are added to ensure information is not throttled by the contraction. Starting from input image size 572 × 572, by the deepest
layer, the signal has contracted to 30 × 30 but with 1024 parallel
convolution channels. In the subsequent decoding stages, upsampling is achieved through 2 × 2 convolutions and concatenating
with same-width layers from the contracting path through the
skip connections.
Subsequently, residual learning , or ResNet, was introduced to facilitate learning in very deep NNs, including U-nets.
In the example illustrated in Fig. 8, let sl denote the vector of
inputs (activation pattern) arriving at the l-th activation (ReLU)
layer, and let W denote the (nonlinear) map between sl−1 and
Convolutional encoding principle between layers l −1 and l
of decreasing width. Shift-invariant weights of limited range feed into
activation units, e.g., ReLU, followed by a pooling layer.
Detailed implementation of the encoder–decoder architecture,
Fig. 4(c), and the convolutional principle, Fig. 6, in the original U-net
that was built for image segmentation (after , reprinted with
permission from Springer).
Review Article
Vol. 6, No. 8 / August 2019 / Optica
sl1. Since sl−1 is added through the unit-weight (dashed line)
connection before the l  2-nd activation layer, W need learn
only the difference sl1 −sl−1 rather than the complete map
sl−1 →sl1. With the residual method, it became possible
for networks of depth in excess of ∼50 to be trained and generalize well, whereas previously there had been a tendency in very
deep networks to get “stuck” in local TLF minima with large
training error.
E. Training Loss Functions
The most obvious TLF choices are the L2 (minimum square error,
MSE) and L1 (MAE) metrics. They are defined, respectively, as
EMSEv, ˜v 
vp −˜vp2,
EMAEv, ˜v 
jvp −˜vpj:
The summations take place over all the pixels p required to describe the object, and the equations are often also normalized for
the number of pixels P2 and/or the peak or average signal energies. As noted in the compressive imaging discussion in Section 2.
B, the MAE promotes sparsity in the solution that minimizes the
functional. Thus, the MAE acts effectively as a weight regularizer.
These metrics are pixel-wise, i.e., they do not take into account
what happens in each pixel’s neighborhood. For images, especially
of natural objects, local correlations are prevalent, and neglecting
them may lead to unnatural results. Instead, correlative metrics
such as the negative Pearson correlation coefficient (NPCC) and
the structural similarity image measure (SSIM) take local
structure explicitly into account.
Given two arbitrary signals a, b of length P, let us define the
and covariance
a −haib −hbi:
Here, as in (23) and (24), the summations are over all pixels. The
NPCC is defined as
ENPCCv, ˜v  −
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
CvvC ˜v ˜v
The negative sign maintains the optimization process as a minimization. Since Eq. (25) is essentially a (negative) dot product, it
preserves structural similarity between the operands better than
the MSE or MAE metrics. The Pearson metric originates from
statistics, where it was proposed for measuring evolutionary correlations between subpopulations . It has been disparaged as a metric for secure image comparison , yet it still has
found some use in robotic vision . In ML-based computational image inversion, it was found to do a better job than
MAE to promote simultaneously weight sparsity and objects’
spatial sparsity .
The (N)PCC may be viewed as a simplification of the more
general SSIM index :
ESSIMv, ˜v 
2hvih ˜vi  c12Cv ˜v  c2
hvi2  h ˜vi2  c1C2vv  C2
˜v ˜v  c2 :
The SSIM index is often used in evaluating DNN performance
on test examples after training. The coefficients c1, c2 stabilize the
denominators and are chosen proportionally to the square of the
dynamic range of the signals.
Sparsity is also promoted by the cross-entropy metric, inspired
by information theory–see, e.g., Sections 2.4 and 8.1–8.7,
and also known for its utility in solving combinatorial optimization problems . Typically, cross-entropy in ML is computed by first binarizing the training ground truth f
reconstruction data ˆf and then penalizing deviations from one
of the probability χ of agreement between f and ˆf according
to 
σf  log χ  1 −σf  log1 −χ,
∀available instances k of the data f k:
Modifications, e.g., focal loss , have also been proposed.
A related metric is the generative adversarial gain (payoff)
 . In this case, we assume that outputs v and inputs u are
drawn from probability distributions pdata and pmodel, respectively.
For CI, these should be interpreted as the prior distribution according to which objects f are generated and the probability of
the optical system generating raw intensity images g, respectively.
Then, two DNNs are used: the generator G and the discriminator
D. The generator is given inputs u and produces samples v, while
the discriminator emits a probability dv that v belongs to the
training set. The two networks are then set to compete against
each other as the generator tries to minimize and the discriminator to maximize the discriminator’s payoff:
EDisc−P  EVv∼pdata log dv  EVv∼pmodel1 −log dv:
Here, EV· denotes the expectation value, estimated over the
available training examples u, ˜v. The adversarial model was
originally proposed for embedding physics into ML engines
for inversion, as discussed relating to Fig. 10(b) in Section 3.F.
Unfortunately, it is often the case that while TLFs or their
combinations decrease during training, the visual fidelity of
reconstructed images becomes worse. This phenomenon is well
known in inverse problems literature . The frustrating
Simplified schematic of residual learning through bypass
connections with unit weight (dashed lines) .
Review Article
Vol. 6, No. 8 / August 2019 / Optica
disagreement between numerical metrics and image quality perception by human observers is not easy to resolve. Recently, computer vision researchers have started using Amazon’s Mechanical
Turk platform for that purpose .
To make image evaluation content-aware, a perceptual metric
 has been proposed. The principle is shown in Fig. 9.
During training, each training object f is accompanied by a content target f c and/or a style target f s. The training architecture
consists of two DNNs: DNN1 for the image reconstruction task,
i.e., for producing an image ˆf given the raw measurement g; and
DNN2, whose job is to compute the perceptual loss function L.
DNN2 is structured as 16-layer VGG . The VGG-16 is
pre-trained to recognize objects in images, e.g., ImageNet ;
during the training of DNN1, DNN2 remains fixed.
For an image reconstruction task, such as super-resolution
(Section 4.A) the content target to be used in Fig. 9 is the object
itself, i.e., f c  f . Let slu denote the vector of activation outputs of the l-th layer when the input to DNN2 is u. The vectors
slf  are encoded representations of objects f ; indeed, it has
been shown that using a regularized optimization routine similar
to (3) these encoded representations may be “inverted” to reproduce estimates of their respective inputs f . The key insight
 is that these encoded representations also capture the
perceptual difference between the ideal reconstruction f and an
imperfect reconstruction ˆf produced by DNN1. Accordingly, the
perceptual feature loss is defined as
EPFL ˆf , f  
μlksl ˆf  −slf k2
where the coefficients μl are for weighing the features from different layers (deeper layers tend not to preserve texture or fine shape
details) and for normalizing according to the layer size (feature
maps from deeper layers are vectors of smaller dimension in
the VGG-16 network.) The training architecture in Fig. 9 can
also account for style losses, such as texture and color .
The content-specific perceptual loss derived from activation layers
within a VGG network may also be combined with an adversarial
loss, used by a discriminator network that evaluates the fit of the
reconstruction with natural images .
It is also common to use mixtures of error metrics for training,
i.e., of the form
E  μ1E1  μ2E2    
with coefficients μ1, μ2, … to be chosen according to the relative
confidence the designer wishes to assign to the respective metrics
E1, E2, … For example, Eq. (30) is typically modified to mix
content- and style-sensitive perceptual losses, as well as other metrics such as MAE. There are no universal strategies for constructing the mixtures, and fine-tuning is often required.
F. Physics Priors
Unlike abstract classification, e.g., face recognition and customer
taste prediction, in CI, the input g and intended output ˆf ≈f of
the NN are related by the known physics of the imaging system,
i.e., by the operator H. Physical knowledge should be useful; how
then to best incorporate it into an ML engine for imaging?
One possibility is to not incorporate it at all, as depicted in
Fig. 10(a). In this end-to-end engine, the raw intensity measurement g is input directly to the DNN, and the image ˆf is the direct
output; in the notation of Section 3.A, Eq. (8):
ˆf  NNg:
The burden then is on the training examples to teach the NN the
forward operator H as well as the prior Φ that, as we saw in
Section 2.B, is needed to regularize the problem. Despite this high
burden, the end-to-end approach has been successful in several CI
problems except when ill posedness or uncertainty become too
high; see Section 4.C.
A definitive approach to incorporate the physical model
(forward operator) into the ML engine originated from sparse representations, Section 2.B. Learned ISTA (LISTA), in particular,
Training an image-forming DNN1 with a perceptual loss function generated by a content- and style-sensitive DNN2 (after ).
Ways to implement the ML engine in Fig. 3, with or without
physical priors. (a) End-to-end ML engine. (b) Recurrent physics-informed ML engine . (c) Cascaded physics-informed ML engine
 . (d) Single-pass physics-informed ML engine. Here, H  H T
if the forward operator H is linear; otherwise, even a crude approximation
to the nonlinear inverse has often been sufficient.
Review Article
Vol. 6, No. 8 / August 2019 / Optica
was an early proposal to incorporate a DNN into dictionary training for a Tikhonov–Wiener functional (3) .
Let us denote as H T the operator conjugate to H and PΦ the
proximal gradient operator corresponding to αΦ. Following
Mardani et al. , the proximal gradient descent approach
is expressed as
ˆf m1  PΦ
ˆf m  αH T 
g −H ˆf m o
αH T g  1 −αH T H ˆf m
Here, f m is the m-th iterate, and α is a (small) step size for the
iterative update (32). Expression (33) suggests the recurrent
architecture shown in Fig. 10(b), where
N  1 −αH T H
denotes the projection operator onto the null space of H, and the
proximal gradient operator PΦ has been replaced by a recurrent
DNN. In principle, if the available examples succeed in training
the DNN to the priors and the parameter α, then the DNN can
replace the job of the regularizer and its proximal gradient. In the
original recurrent implementation, the DNN in Fig. 10(b) was
the generator, the output
was fed into a discriminator
DNN, and the two DNNs were trained according to the adversarial model (see also Section 3.E.).
In practice, instead of the recurrent architecture in Fig. 10(b), it
is easier to implement the unfolded, or cascaded ML engine shown
in Fig. 10(c). In this version, the measurement g first produces an
estimate ˆf 0 according to the conjugate operator H T only. We will
refer to ˆf 0 as the zeroth-order approximant, or simply the approximant. This approximant result is correct within the conjugate operator’s range. To recover information outside the range, i.e., from
the conjugate operator’s null space, the approximant is passed consecutively through M cascaded DNNs. The output at the m-th
stage is projected onto the null space by the operator N, and
the zeroth-order approximant is added [see Eq. (33)] to produce
the m-th-order approximant. The final image is the M-th-order
approximant ˆf M. Note also that in this architecture, there is
no discriminator; the DNN cascade is trained on an appropriately
chosen loss function (see Section 3.E.).
The advantage of the cascaded ML engine is that it is faithful
to the original Tikhonov–Wiener formulation. The forward operator is explicitly included while the NNs, in a sense, are replacing the regularizer, and the prior knowledge is now learned from
examples rather than explicitly defined. One disadvantage is that
training M DNNs increases the complexity of the learning task
and the associated risks of undertraining (not enough training
examples) or overtraining (too many degrees of freedom).
A compromise is the single-pass ML engine in Fig. 10(d).
Here, an approximate inverse operator H produces the single
approximant f 0. The single DNN is trained to receive f 0 as
input and produce the image ˆf as output directly, rather than
its projection onto the null space. In practice, the single-pass
approach has proven to be robust and reliable even for CI problems with high ill posedness or uncertainty, as we will see in
Sections 4.A–4.C. Last, it is important to note that H must
be linear for Eqs. (32) and (33) to have a fixed point; only then
are Figs. 10(b) and 10(c) valid. If this is the case, then the singlepass approximant should be f 0  H g  H T g. For nonlinear
inverse problems, such as phase retrieval and imaging through
strong scatter, Sections 4.B and 4.D, respectively, we will see that
end-to-end or single-pass engines work well even with crude
approximations to H .
4. COMPUTATIONAL IMAGING REALIZATIONS
WITH MACHINE LEARNING
The strategy for using ML for computational image formation is
broadly described as follows:
(1) Obtain a database of physical realizations of objects and
their corresponding raw intensity images through the instrument
of interest. For example, such a physical database may be built by
using an alternative imaging instrument considered accurate
enough to be trusted as ground truth; or by displaying objects
from a publicly available abstract database, e.g., ImageNet
 on a spatial light modulator (SLM) as phase or intensity;
or by rigorous simulation of the forward operator and associated
noise processes.
(2) Decide
regularization
TLF, and physical priors according to the principles of
Sections 3.C–3.F, and then train the NN from the training and
validation subsets of the database, as described in Section 3.B.
(3) Test the ML engine for generalization by measuring a
TLF, same as training or different, for on the test example subset
of the database.
Generating the physical database for CI tasks can be hardware
intensive and time consuming. Moreover, the quality of training
depends on the examples chosen; therefore, care must be taken
that they be representative of the class of objects the ML architecture would be used on after training. For highly ill-posed and
noisy tasks, it is generally better to restrict the class so that
stronger priors are learned. In such cases, it is also a good strategy
to adopt an ML engine including the forward operator explicitly,
rather than end-to-end (Section 3.F and Fig. 10).
This section is organized according to the type of forward operator the ML algorithm is called upon to invert, and the severity of
the ill-posed problem: super-resolution in photography, machine
vision, and optical microscopy in Section 4.A; complex amplitude
retrieval from intensity in Section 4.B; attenuation and phase
retrieval under extremely low photon counts in Section 4.C;
and imaging through diffusers in Section 4.D. We also wish to remind the reader of three recent and more focused reviews on
tomography , computer-vision related reconstruction problems , and quantitative phase imaging .
A. Super-Resolution
The two-point resolution problem was first posed by Airy 
and Lord Rayleigh . In modern optical imaging systems, resolution is understood to be limited by mainly two factors: undersampling by the camera, whence super-resolution should be taken
to mean upsampling; and blur by the optics or camera motion, in
which case super-resolution means deblurring. Both situations or
their combination lead to a singular or severely ill-posed inverse
problem due to suppression or loss of entire spatial frequency
bands; therefore, they have attracted significant research interest,
including some of the earliest uses of ML in the CI context.
A comprehensive review of methods for super-resolution in the
sense of upsampling, based on a single image, is in . To our
knowledge, the first-ever effort to use a DNN in the same context
was by Dong et al. . The key insight, as with LISTA
Review Article
Vol. 6, No. 8 / August 2019 / Optica
(Section 3.F), was that dictionary-based sparse representations for
upsampling could equivalently be learned by DNNs.
Both approaches similarly start by extracting compressed feature
maps and then expanding these maps to a higher sampling rate.
The difference is that sparse coding solvers are iterative; whereas,
as we also pointed out in Section 1, with the ML approach, the
iterative scheme takes place during training only; the trained ML
engine operation is feed-forward and, thus, very fast. To combine
super-resolution with motion compensation, a spatio-temporal
CNN has been proposed, where, rather than simple images, the
inputs are blocks consisting of multiple frames from video .
The ML approach to the super-resolution problem also served
as motivation and testing ground for the perceptual TLF
 (Section 3.E). The structure of the downsampling kernel was exploited in using the cascaded ML engine architecture in Fig. 10(c) with M  4. Figure 11 is a representative
result showing the evolution of the image estimates along the ML
cascade, as well as their spatial spectra. It is interesting that, by the
final stage, the ML engine has succeeded in both suppressing
high-frequency artifacts due to undersampling and boosting
low frequency components to make the reconstruction appear
Turning to inverse problems dominated by blur, early work
 used a perceptron network with two hidden layers and a
sigmoidal activation function to compensate for static blur caused
by Gaussian and rectangular kernels, as well as motion blur .
Two years later, Sun Jiao et al. showed that a CNN can
learn to compensate even when the motion blur kernel across
the image is non-uniform. This was accomplished by feeding
the CNN with rotated patches containing simple object features,
such that the network learned to predict the direction of motion.
In optical microscopy, blur is typically caused by aberrations
and diffraction . More than 100 years of research, tracing
back to Airy and Rayleigh’s observations, have been oriented toward modifying the optical hardware—in our language, designing
the illumination and collection operators—to compensate for the
blur and obtain sharp images of objects down to sub-micrometer
size. Thorough review of this literature is beyond the present scope;
we just point out the culmination of optical super-resolution
methods with the 2014 Nobel Prize in Chemistry .
Stochastic optical reconstruction microscopy (STORM) and fluorescence photoactivation localization microscopy (PALM) for single molecule imaging and localization are
examples of co-designing the illumination operator H i and the
computational inverse to achieve performance vastly better than
an unaided microscope could do.
Computationally, the blur kernel can be compensated for
through iterative blind deconvolution or learned from
examples . A DNN-based solution to the inverse problem
was proposed for the first time, to our knowledge, by Rivenson
et al. in a wide-field microscope. The approach and results
are summarized in Fig. 12. For training, the samples were imaged
twice, once with a 40 × 0.95 NA objective lens and again with a
Evolution of the image estimate along the cascaded ML
engine in Fig. 10(c) (reprinted with permission from , Fig. 10 ).
Top row, left to right, and in the notation used in the present review:
undersampled raw intensity image g, ˆf 0, ˆf 1, ˆf 2, and ˆf ≡ˆf 3.
Bottom row: spatial Fourier transform magnitude of the corresponding
images from the top row.
Deep learning microscopy (adapted from , Figs. 1 and 5, with permission). (a) Training of the end-to-end ML engine with lowresolution (downsampled and blurred) inputs to the DNN produced by a 40 × 0.95 NA objective lens and high-resolution outputs produced by a
100 × 1.4 NA objective lens. (b) Operation of the trained DNN engine, with the test outputs successfully upsampled and deblurred.
(c) Resolution test demonstrating the DNN with inputs from the 40 × 0.95 NA objective lens achieving in testing performance comparable to the
raw performance of the 100 × 1.4 NA objective lens the DNN was trained with.
Review Article
Vol. 6, No. 8 / August 2019 / Optica
100 × 1.4 NA objective lens. The training goal was such that with
the 40 × 0.95 NA raw images as input g, the DNN would produce estimates ˆf matching the 100 × 1.4 NA images, i.e., the
latter were taken to approximate the true objects f . The number
of pixels in the high-resolution images was 2.52 × the number
of pixels in the low-resolution representation. Of course, the lowresolution images were also subject to stronger blur due to the
lower-NA objective lens. Therefore, the inverse algorithm had
to perform both upsampling and deblurring in this case. The
ML engine was of the end-to-end type, as in Fig. 10(a), implemented as convolutional DNN with pyramidal progression for
upsampling. The TLF was a mixture of the MSE metric (23) and
a TV-like ∂2
TV [Eq. (6)] penalty. Since then, ML has been shown
to improve the resolution of fluorescence microscopy , as
well as single-molecule STORM imaging and 3D localization .
B. Quantitative Phase Retrieval and Lensless Imaging
The forward operator relating the complex amplitude of an object
to the raw intensity image at the exit plane of an optical system is
nonlinear. Classical iterative solutions are the Gerchberg–Saxton
algorithm ; the input–output algorithm, originally proposed by Fienup and subsequent variants ; and
the gradient descent or its variants, steepest descent and
conjugate gradient . This inverse problem has attracted considerable attention because of its importance in retrieving the
shape or optical density of transparent samples with visible light
 and x rays .
In the case of weak scattering, the problem may be linearized
through a quasi-hydrodynamic approximation leading to the
transport of intensity equation (TIE) formulation .
Alternatively, if a reference beam is provided in the optical system,
the measurement may be interpreted as a digital hologram ,
and the object may be reconstructed by a computational backpropagation algorithm (not to be confused with the
back-propagation algorithm for NN training, Section 3.B.)
Ptychography captures measurements effectively in the phase
(Wigner) space, where the problem is linearized, by modulating
the illumination with a quadratic phase and structuring it so that
it is confined and scanned in either space or angle
 . Due to the difficulty of the phase retrieval inverse
problem, compressive priors have often been used to regularize
it in its various linear forms, including digital holography
 , TIE , and Wigner deconvolution ptychography .
When the linearization assumptions do not apply or regularization priors are not explicitly available, an ML engine may
instead be applied directly on the nonlinear inverse problem. To
our knowledge, this investigation was first attempted by Sinha
et al. with binary pure phase objects , and subsequently with
multi-level pure phase objects . Representative results are
shown in Fig. 13. The phase objects were displayed on a reflective
SLM, and the light propagated in free space until intensity sampling by the camera. The ML engine of the end-to-end type
[Fig. 10(a)] was of the convolutional DNN type with residuals.
Training was carried out by drawing objects from standard databases, Faces-LFW, and ImageNet, converting each object’s grayscale intensity to a phase signal in the range 0, π, and then
displaying that signal on the SLM. Because of the relatively large
range of phase modulation, linearizing assumptions would have
been invalid in this arrangement.
Retrieval of the complex amplitude, i.e., of both the magnitude and phase, of biological samples using ML in the digital
holography (DH) arrangement was reported by Rivenson et al.
 ; see Fig. 14. The samples used in the experiments were
from breast tissue, Papanicolaou (Pap) smears, and blood smears.
In this case, the ML engine used a single-pass physics-informed
preprocessor, as in Fig. 10(d), with the approximant H implemented as the (optical) backpropagation algorithm. The DNN
was of the convolutional type. Training was carried out using
Phase extraction neural network (PhENN) (reprinted from , Fig. 5, with permission). Columns I and II are the ground truth pixel values
driving the SLM and corresponding phase images f produced by the SLM, according to calibration. Groups of columns III-V, VI-VIII, and IX-XI are
different propagation distances z  37.5 cm, 67.5 cm, and 97.5 cm, respectively. Columns III, VI, IX are the raw images g captured by the camera.
Columns IV, VII, X are reconstructions ˆf when PhENN was trained with Faces-LFW , whereas columns V, VIII, XI are estimates ˆf when PhENN
was trained with ImageNet . The rows represent different databases the PhENN was tested on as a, Faces-LFW (disjoint from training); b, ImageNet
(disjoint from training); c, Characters ; d, MNIST ; e, Faces-ATT ; f, CIFAR .
Review Article
Vol. 6, No. 8 / August 2019 / Optica
up to eight holograms to produce accurate estimates of the samples’ phase profiles. After training, the ML engine was able, with a
single hologram input, to match imaging quality, in terms of
SSIM (Section 3.E) of traditional algorithms that would have required two to three times as many holograms, and was faster as
well by a factor of three to four times.
The 3D image formation properties of phase retrieval have also
been investigated as depth prediction with robust automatic focusing , image reconstruction with a single-shot in-line
hologram , extended depth of field , and transparent 3D sample reconstruction from diffraction images obtained at
multiple angles (phase tomography) . In the results shown in
Fig. 15 from some early work , the single-pass physicsassisted ML engine was trained with physical objects at various
depths, and out-performed both single-height and multi-height
(optical) back propagation in terms of depth invariance.
Resolution improvements in digital holographic microscopy were
obtained through aberration correction and background rejection
 instead of the compressive approach in . Adversarial
payoff was recently used in holographic microscopy with
resolution below the nominal pixel size and ptychography
249 ] using SSIM and MAE, respectively, as TLFs.
In the same context as strongly scattering phase objects, ML
was used early on in an intriguing alternative to our general architecture in Fig. 3 where the object’s 3D refractive index distribution, instead of output by the ML engine, is stored as the weights
in a multilayered deep network . This is possible because of the formal analogy between the beam-propagation
method and the NN back-propagation weight training
method, and is implemented by training the DNN to match
the illumination fields from multiple angles as input and the corresponding raw intensity images as outputs. This technique recently inspired a similar solution to the simpler problem of a
thin phase object with phase delays stored in a single layer
and raw intensity measurements obtained according to the
Fourier ptychography scheme .
The problem of lateral resolution in phase retrieval algorithms
is interesting, because performance depends on the optics
as well as the inverse algorithm. In regularized algorithms, e.g.,
Gerchberg–Saxton, gradient descent, or compressed holography,
Retrieval of phase and amplitude with a physics-informed ML engine (reprinted with permission from Springer , Fig. 2 ). (a)–(h) Pap
smear and (j)–(p) breast tissue section reconstructions. (a), (i) Zoom-ins to the (optical) backpropagation results from a single intensity image; (h),
(p) corresponding bright-field microscopy images, shown for comparison. (b), (c), (j), (k) (Optical) backpropagation results showing artifacts due
to twin image and self-interference effects; (d), (e), (l), (m) corresponding ML engine reconstructions from a single hologram, showing quality comparable
to (f), (g), (n), (o) traditional reconstructions from eight holograms.
Physics-informed DNN trained with samples at different
propagation distances of up to
100 μm from the focal plane, exhibiting
improved depth invariance compared to (optical) backpropagation and
multi-height phase retrieval (MH-PR) (reprinted with permission ,
Fig. 3). The results are from a human breast tissue sample, captured at
different propagation distances dz. Distances marked in red exceed the
training range of the DNN. Scale bar  20 μm.
Review Article
Vol. 6, No. 8 / August 2019 / Optica
the effectiveness of the computation depends on how faithful the
prior is to the relevant class of phase objects. Similarly, in MLbased phase retrieval, one would expect qualities of the example
database, such as the spatial power spectral density (PSD), to have
a similar effect. In fact, due to the nonlinearities of the training
and reconstruction processes, spatial frequencies exhibiting higher
prevalence in the training database may become so dominant that
they inhibit successful reconstruction of less prevalent spatial
frequencies. Invariably, lower spatial frequencies are more popular, especially in datasets of natural images due to the well-known
inverse-square PSD law . The result is that the ML-reconstructed phase estimates are strongly low-pass filtered . One
way around that problem is to violate the training prior, e.g., by
spectrally pre-modulating the examples to flatten the PSD. This
results in resolution improvement, at the expense of some edgeenhancement artifacts, as shown in Fig. 16. Alternatively, the lowand high-frequency bands can be processed by two separate NNs
 and recombined by a third NN, all three networks trained
separately . The idea can be thought of as a computational
analogue to the HiLo super-resolution method ,
and in , it was applied to both super-resolution and phase
retrieval.
C. Imaging of Dark Scenes
The challenges associated with super-resolution and phase
retrieval become much exacerbated when the photon budget is
tight or other sources of noise are strong. This is because deconvolutions, in general, tend to amplify noise artifacts . In standard photography, histogram equalization and gamma correction
are automatically applied by modern high-end digital cameras and
even in smartphones; however, “grainy” images and color distortion still occur. In more challenging situations, a variety of more
sophisticated denoising algorithms utilizing compressed sensing
and local feature representations have been investigated and
benchmarked . What these algorithms exploit, with
varying success, is that natural images are characterized by the
prior of strong correlation structure, which should persist even
under noise fluctuations that much exceed the signal. Understood
in this sense, ML presents itself as an attractive option to learn the
correlation structures and then recover high-resolution content
from the noisy raw images.
The first use of a CNN for monochrome Poisson denoising, to
our knowledge, was by Remez et al. . More recently, a convolutional network of the U-net type was trained to operate on all
three color channels under illumination and exposure conditions
that, to the naked eye, make the raw images appear entirely dark
while histogram- and gamma-corrected reconstructions are severely color distorted ; see Fig. 17. The authors created a
see-in-the-dark (SID) dataset of short-exposure images, coupled
with their respective long-exposure images, for training and testing; and used Amazon’s Mechanical Turk platform for perceptual
image evaluation by humans . They also report that, unlike
other related works, neither skip connections in U-net nor generative adversarial training led to any improvement in their reconstructions.
ML has also been shown to be effective for phase retrieval from
intensity data with extremely low photon counts . The recurrent and cascaded engines in Figs. 10(b) and 10(c) are not,
strictly speaking, applicable because this inverse problem is nonlinear. An end-to-end DNN, as in Fig. 10(a), yielded acceptable
results only with moderate photon flux. For lower flux, the singlepass engine in Fig. 10(d) was used. The approximant H was
obtained by running the standard Gerchberg–Saxton algorithm
up to a single iteration. With noisy data, as expected, the estimates
ˆf 0 produced by H were rather poor, as seen in Figs. 18(c) and
18(g); yet with this as input, the DNN was capable of significantly improving its output; compare Figs. 18(d), 18(h) and
18(e), 18(i), respectively. It is also worth noting that running
the Gerchberg–Saxton algorithm to more iterations produced discernible improvements neither to its own reconstructions nor as
an alternative approximant to the DNN.
A radically different approach to imaging under extremely low
light flux is computational ghost imaging. Interferometric ghost
imaging originated in quantum optics , but it was soon discovered theoretically and experimentally 
that it has a classical analogue with pseudo-thermal light. The
raw intensity image is obtained not by a camera but by a spatially
Resolution improvement by spectral premodulation of
PhENN (from , Figs. 5, 8, and 9; reprinted with permission).
(a) Two-phase point-object resolution target, implemented on the SLM;
(b) PhENN image, not resolving phase point-objects separated by three
pixels; (c) with spectral pre-modulation, the three pixel-separated phasepoint objects are resolved; (d) sample test image from the ImageNet database; (e) PheNN reconstruction; (f) PhENN reconstruction with spectral
pre-modulation, showing sharper features but also edge-enhancement
artifacts.
ML applied to reconstruct a severely photon-limited scene
(after , Fig. 5; reprinted with permission). (a) Signal captured by
a Fujifilm X-T2 camera with ISO 800, aperture f/7.1, and exposure
of 1/30 s, with illuminance of approximately 1 lux; (b) reconstruction
using denoising, deblurring, and enhancement, which the authors refer
to as “traditional image processing pipeline”; (d) reconstruction obtained
using the radiational image processing pipeline on a Sony α7SII camera;
(e) reconstruction obtained using BM3D , considered as benchmark for denoising; (c), (f) ML engine reconstructions of the respective
dark images.
Review Article
Vol. 6, No. 8 / August 2019 / Optica
integrating (bucket) detector on one arm of the interferometer,
while an SLM displays random phase patterns, and a pinhole detector is scanned laterally on the other arm. The image is obtained
by correlating the two measurements and removing a background
term. Following a dispute about the explanation of pseudo-thermal ghost imaging , it was shown that in the classical
case, one arm of the interferometer can be removed and replaced
by computation . Because the bucket detector collects all
available photons, ghost imaging methods are attractive for applications where the available illumination is extremely weak, much
less than a few photons per pixel and in harsh environments
 . However, to reconstruct images with a good signal-tonoise-ratio (SNR), usually a long acquisition time is needed
 . Therefore, the sampling ratio
β  #displayed SLM patterns
#reconstructed pixels
needs to be kept relatively low. Compressive priors have also been
used with some success to improve the condition of the ghost
inverse problem .
Lyu et al. used deep learning with the single-pass
physics-informed engine [Fig. 10(d)] and approximant H
computed according to the original computational ghost imaging
 . Due to the low sampling rate and the noisy nature of the
raw measurements, the approximant reconstructions ˆf 0 were
corrupted and unrecognizable. However, when these
were used as input to the DNN, high-quality final estimates ˆf
were obtained even with sampling rates β as low as 5%, as shown
in Fig. 19.
D. Imaging in the Presence of Strong Scattering
Imaging through diffuse media is a classical challenging
inverse problem with significant practical applications ranging
from non-invasive medical imaging through tissue to autonomous
navigation of vehicles in foggy conditions. The noisy statistical
inverse model formulation (2) must now be reinterpreted with
the forward operator H itself becoming random. When f is
the index of refraction of the strongly scattering medium itself,
then H is also nonlinear. Not surprisingly, this topic has attracted
considerable attention in the literature, with most attempts generally belonging to one of two categories. The first is to characterize the diffuse medium H, assuming it is accessible and static,
through (incomplete) measurement of the operator H, which
in this context is referred to as transmission matrix .
The alternative is to characterize statistical similarities between
moments of H. The second-order moment, or speckle correlations, are known as the memory effect. The idea originated in
the context of electron propagation in disordered conductors
 and of course is also valid for the analogous problem of
optical disordered media .
The first-ever, to our knowledge, use of a non-dictionary
(Section 2.B) ML algorithm for imaging through diffusers was
by Horisaki et al. . The authors created an experimental configuration explicitly non-suited to the transmission matrix formulation by sandwiching two SLMs, both displaying the same
image, between three acrylic diffuser plates. The optical system
was illuminated coherently, and it was lensless, i.e., without imaging optics on the beam path. Examples for training were drawn
from the Caltech Computer Vision Database . The ML engine was not implemented as a DNN, but rather as an extension
of support vector machines known as support vector regression (SVR) .
In SVR, the image is expressed in linear form as
γnKgn, g  f 0,
where K·, · is the kernel function , f 0 is a bias vector
obtained by an efficient form of sequential minimal optimization
 , and the coefficients γn are the Lagrange multipliers in a
convex optimization problem involving the ε-insensitive
ML applied to quantitative phase retrieval on a severely photon-limited signal (after , Fig. 2; reprinted with permission).
(a) Ground truth f for a test example; (b), (f) raw intensity signal at
photon flux of 1,050 photons/pixel; (c), (g) corresponding outputs ˆf 0
of the approximant H of Fig. 10(d), implemented as a single-iteration
Gerchberg–Saxton algorithm; (d), (h) corresponding reconstructions by
the end-to-end ML engine of Fig. 10(a); (e), (i) corresponding reconstructions by physics-informed single-pass ML engine according to
Fig. 10(d). In (a) and reconstructions (c)–(e, (g)–(i), grayscale tone
represents phase delay 0,2π.
Experimental computational ghost image reconstructions
with sampling ratio β  0.1 (left-hand side group) and β  0.05
(right-hand side group) (after ; reprinted with permission from
Springer Nature). Top row: ground truth; second-row: basic ghost
reconstruction according to ; third row: deep learning ghost reconstructions using the images in the second row as approximants ˆf 0 to the
physics-informed ML engine of Fig. 10(d); last row: compressive ghost
reconstructions according to .
Review Article
Vol. 6, No. 8 / August 2019 / Optica
Eε−insv, ˜v 
if jv −˜vj ≤ε,
jv −˜vj −ε,
otherwise:
This metric is also known as a soft margin TLF. An early proposal
for kernel functions was polynomials . Horisaki et al. 
instead used the shift-invariant Gaussian radial basis function kernel. The results are shown in Figs. 20 and 21. The SVR engine
trained with faces does a good job reconstructing test objects that
are faces also. However, when the test objects are not faces, the
SVR still “hallucinates” face reconstructions.
Deep learning solutions to the problem were first presented in
 and , using end-to-end fully connected and residualconvolutional (CNN) architectures, respectively. Results are
shown in Figs. 22 and 23. The fully connected solution is
motivated by the physical fact that when light propagates through
a strongly scattering medium, every object pixel influences every
raw image pixel in shift non-invariant fashion. However, the large
number of connections creates risks of undertraining and overfitting, and limits the space-bandwidth product (SBP) of the reconstructions due to limited computational resources. On the other
hand, the CNN trained with NPCC loss function , despite being designed for situations when limited range of influence and shift invariance constraints are valid, Section 3.D,
does a surprisingly good job at learning shift variance—through
the ReLU nonlinearities and pooling operations, presumably—
and achieves larger SBP. Both methods work well with spatially
sparse datasets, e.g., handwritten numerical digits, and Latin and
Chinese characters. Compared to Horisaki et al. , the deep
architectures perform comparably well with spatially dense datasets of restricted content, e.g., faces, and also hallucinate when
tested outside their learned priors.
Approaches are all meant to be used with
the specific diffuser they were trained with. It was recently shown
that for spatially sparse datasets, it is possible to train a single
DNN to image through arbitrary diffusers belonging to a class
of similar statistics, by adopting a cross-entropy TLF (27),
Section 3.E, and with the training procedure including samples
Learning-based imaging through diffusers using support vector regression (SVR) (after , Fig. 3; reprinted with permission).
(a) Training examples from a database of faces and (b) their corresponding speckle patterns; (c) test face examples and their corresponding
(d) speckle patterns, (e) test reconstructions using SVR, and (f) test
reconstructions using pattern matching.
Generalization study of the SVR method (after , Fig. 5;
reprinted with permission). (a) Test non-face examples and their corresponding (b) speckle patterns, (c) test reconstructions using SVR, and
(d) test reconstructions using pattern matching.
Learning-based imaging through optically thick diffusers using a fully connected DNN with MAE TLF (after , Fig. 3; reprinted
with permission). First row: speckle patterns input to the DNN; second
row: corresponding ground-truth images selected among a database of
English letters; third row: corresponding reconstructions by the DNN.
Learning-based imaging through diffusers using IDiffNet: a
residual-convolutional DNN with NPCC TLF (after , Fig. 6; reprinted with permission). Columns I, II; ground truth pixel values and
fields modulated by the SLM, after calibration; III-VI: results with 600grit diffuser. III are the raw images, IV-VI test reconstructions from
IDiffNet trained with Faces-LFW , ImageNet , and
MNIST datasets, respectively. Rows correspond to the dataset the
test image is drawn from, as (a) Faces-LFW, (b) ImageNet, (c) Characters
 , (d) MNIST, (e) Faces-ATT , (f) CIFAR .
Review Article
Vol. 6, No. 8 / August 2019 / Optica
of the same training object imaged with randomly chosen diffusers from among the class . It has also been shown that
imaging through multi-core fibers can be relatively tolerant to
variability in the transmission matrix, in terms of both image
reconstruction quality and recognition of simple patterns, e.g.,
characters from the MNIST database .
Non-line-of-sight (NLOS) imaging, recognition, and tracking
belong to a related class of problems, because capturing details
about objects in such cases must rely on scattering, typically of
light pulses or spatially incoherent light .
Convolutional DNNs have been found to be useful for improving
gesture classification , and person identification and threedimensional localization ; in the latter case even with a
single-photon, single-pixel detector only.
5. CONCLUDING REMARKS
The diverse collection of ML flavors adopted and problems
tackled by the CI community in a relatively brief time period,
mostly since ∼2010 , indicate that the basic idea of doing
at least partially the job of Tikhonov–Wiener optimization by
DNN holds much promise. A significant increase in the rate
of related publications is evident—we had trouble keeping up
while crafting the present review—and is likely to accelerate,
at least in the near future. As we saw in Section 4, in many cases,
ML algorithms have been discovered to offer new insights or
substantial performance improvements on previous CI approaches, mostly compressive sensing based, whereas in other
cases, particular challenges associated with acute CI problems
have prompted innovations in ML architectures themselves. This
productive interplay is likely to benefit both disciplines in the
long run, especially because of the strong connection they share
through optimization theory and practice.
Beyond these bidirectional improvements, we wish to single
out and return to the open question of image interpretation versus
image formation that we touched upon in Section 1. It is possible
that image-forming instruments aided by ML will continue doing
just that, producing high-quality images for consumption by humans and image interpretation algorithms that require high-quality images as their input. It is also possible that, to some degree,
image formation and interpretation will fuse, resulting in algorithms that interpret the raw data directly, without bothering
to form high-quality images; or other algorithms might turn
out to be useful by falling somewhere in between these two extremes. We saw already that DH and imaging through multimode
fiber bundles have emerged as fertile ground for this kind of crossover . In our view, further developments in that direction
will carry significant fundamental value, as existing algorithms
and insights, originating as they have from either side of formation or interpretation, will likely have to be at least partially reconsidered. Practical implications are also to be expected if pattern
recognition and image interpretation, rather than relying mostly
on human-recognizable optical intensity patterns as it does presently, become more directly linked to the complete electromagnetic representation of the optical field and its interaction with
matter, i.e., the broadest definition of CI.
Intelligence Advanced Research Projects Activity
(IARPA) (FA8650-17-C-9113); Chinese Academy of Sciences
(CAS) (QYZDB-SSW-JSC002); Chinesisch-Deutsche Zentrum
für Wissenschaftsförderung (CDZ) (GZ1931); National Research
Foundation Singapore (NRF) (SMART Centre).
Acknowledgment.
The authors are grateful to Zhang
Zhengyun, Alexandre Goy, Berthold K. P. Horn, and Lei Tian
for helpful discussions and extensive comments on earlier versions
of the paper.