Representation Learning by Learning to Count
Mehdi Noroozi1
Hamed Pirsiavash2
Paolo Favaro1
University of Bern1
University of Maryland, Baltimore County2
{noroozi,favaro}@inf.unibe.ch
{ }
We introduce a novel method for representation learning
that uses an artiﬁcial supervision signal based on counting visual primitives. This supervision signal is obtained
from an equivariance relation, which does not require any
manual annotation. We relate transformations of images to
transformations of the representations. More speciﬁcally,
we look for the representation that satisﬁes such relation
rather than the transformations that match a given representation. In this paper, we use two image transformations
in the context of counting: scaling and tiling.
transformation exploits the fact that the number of visual
primitives should be invariant to scale. The second transformation allows us to equate the total number of visual
primitives in each tile to that in the whole image. These two
transformations are combined in one constraint and used to
train a neural network with a contrastive loss.
The proposed task produces representations that perform on par or
exceed the state of the art in transfer learning benchmarks.
1. Introduction
We are interested in learning representations (features)
that are discriminative for semantic image understanding
tasks such as classiﬁcation, detection, and segmentation. A
common approach to obtain such features is to use supervised learning. However, this requires manual annotation
of images, which is costly, time-consuming, and prone to
errors. In contrast, unsupervised or self-supervised feature
learning methods exploiting unlabeled data can be much
more scalable and ﬂexible.
Some recent feature learning methods, in the so-called
self-supervised learning paradigm, have managed to avoid
annotation by deﬁning a task which provides a supervision
signal. For example, some methods recover color from gray
scale images and vice versa , recover a whole
patch from the surrounding pixels , or recover the relative location of patches . These methods use information already present in the data as supervision signal so that
Figure 1: The number of visual primitives in the whole image should match the sum of the number of visual primitives
in each tile (dashed red boxes).
supervised learning tools can be used. A rationale behind
self-supervised learning is that pretext tasks that relate the
most to the ﬁnal problems (e.g., classiﬁcation and detection)
will be more likely to build relevant representations.
As a novel candidate pretext task, we propose counting
visual primitives. It requires discriminative features, which
can be useful to classiﬁcation, and it can be formulated via
detection. To obtain a supervision signal useful to learn to
count, we exploit the following property: If we partition an
image into non-overlapping regions, the number of visual
primitives in each region should sum up to the number of
primitives in the original image (see the example in Fig. 1).
We make the hypothesis that the model needs to disentangle
the image into high-level factors of variation, such that the
complex relation between the original image and its regions
is translated to a simple arithmetic operation . Our
experimental results validate this hypothesis both qualitatively and quantitatively.
While in this work we focus on a speciﬁc combination
of transformations, one can consider more general relationships (i.e., beyond counting, scaling, and tiling) as superarXiv:1708.06734v1 [cs.CV] 22 Aug 2017
vision signals. The same procedure that we introduce is
therefore applicable to a broader range of tasks as long as
it is possible to express the transformation in feature space
caused by a transformation in image space .
Our contributions are: 1) We introduce a novel method
to learn representations from data without manual annotation; 2) We propose exploiting counting as a pretext task and
demonstrate its relation to counting visual primitives; 3) We
show that the proposed methodology learns representations
that perform on par or exceed the state of the art in standard
transfer learning benchmarks.
2. Prior Work
In this work we propose to learn a representation without
relying on annotation, a problem that is typically addressed
via unsupervised learning. An example of this approach is
the autoencoder , which reconstructs data by mapping it to a low-dimensional feature vector. A recent alternative approach is self-supervised learning, which is a
technique that substitutes the labels for a task with artiﬁcial
or surrogate ones. In our work such artiﬁcial labels are provided by a counting constraint. In many instances, this technique can be seen as recasting the unsupervised learning
problem of ﬁnding p(x) = p(x1, x2), where x⊤= [x⊤
is a random variable, as a partly supervised one of ﬁnding
p(x2|x1), so that we can write p(x1, x2) = p(x2|x1)p(x1)
eq. (5.1) in ).
In our context, the data sample
x collects all available information, which can be just an
image, but might also include egomotion measurements,
sound, and so on. In the literature, self-supervised methods
do not recover a model for the probability function p(x1),
since p(x2|x1) is sufﬁcient to obtain a representation of x.
Most methods are then organized based on the choice of x1
and x2, where x2 deﬁnes the surrogate labels. Below we
brieﬂy summarize methods based on their choice for x2,
which leads to a regression or classiﬁcation problem.
Regression. In recent work Pathak et al. choose as
surrogate label x2 a region of pixels in an image (e.g., the
central patch) and use the remaining pixels in the image as
x1. The model used for p(x2|x1) is based on generative
adversarial networks . Other related work 
maps images to the Lab (luminance and opponent colors)
space, and then uses the opponent colors as labels x2 and
the luminance as x1. Zhang et al. combine this choice
to the opposite task of predicting the grayscale image from
the opponent colors and outperform prior work.
Classiﬁcation. Doersch et al. and Noroozi & Favaro 
deﬁne a categorical problem where the surrogate labels are
the relative positions of patches. Other recent works use as
surrogate labels ego-motion , temporal ordering in
video , sound , and physical interaction .
In contrast to these works, here we introduce a different
formulation to arrive at a supervision signal. We deﬁne the
counting relationship “having the same number of visual
primitives” between two images. We use the fact that this
relationship is satisﬁed by two identical images undergoing
certain transformations, but not by two different images (although they might, with very low probability). Thus, we
are able to assign a binary label (same or different number
of visual primitives) to pairs of images. We are not aware
of any other self-supervised method that uses this method
to obtain the surrogate labels. In contrast, Wang and Gupta
 impose relationships between triplets of different images obtained through tracking. Notice that also Reed et al.
 exploit an explicit relationship between features. However, they rely on labeling that would reveal the relationship
between different input images. Instead, we only exploit
the structure of images and relate different parts of the same
image to each other. Due to the above counting relationship
our work relates also to object counting, which we revise
here below.
Object counting. In comparison to other semantic tasks,
counting has received little attention in the computer vision
community. Most effort has been devoted to counting just
one category and only recently it was applied to multiple
categories in a scene. Counting is usually addressed as a
supervised task, where a model is trained on annotated images. The counting prediction can be provided as an object
density map or simply as the number of counted
objects . There are methods to count humans in crowds
 , cars , and penguins . Some recent works
count common objects in the scene without relying on object localization .
In this work, we are not interested in the task of counting per se. As mentioned earlier on, counting is used as a
pretext task to learn a representation. Moreover, we do not
use labels about the number of objects during training.
3. Transforming Images to Transform Features
One way to characterize a feature of interest is to describe how it should vary as a function of changes in the
input data. For example, a feature that counts visual primitives should not be affected by scale, 2D translation, and
2D rotation changes of the input image. Other relationships
might indicate instead that a feature should increase its values as a result of some transformation of the input. For
example, the magnitude of the feature for counting visual
primitives applied to half of an image should be smaller than
when applied to the whole image. In general, we propose to
learn a deep representation by using the known relationship
between input and output transformations as a supervisory
signal. To formalize these concepts, we ﬁrst need to introduce some notation.
Let us denote a color image with x ∈Rm×n×3, where
m × n is the size in pixels and there are 3 color channels (RGB). We deﬁne a family of image transformations
G ≜{G1, . . . , GJ}, where Gj : Rm×n×3 7→Rp×q×3,
with j = 1, . . . , J, that take images x and map them
to images of p × q pixels.
Let us also deﬁne a feature
φ : Rp×q×3 7→Rk mapping the transformed image to some
k-dimensional vector. Finally, we deﬁne a feature transformation g : Rk × · · · × Rk 7→Rk that takes J features and
maps them to another feature. Given the image transformation family G and g, we learn the feature φ by using the
following relationship as an artiﬁcial supervisory signal
g (φ(G1 ◦x), . . . , φ(GJ ◦x)) = 0
In this work, the transformation family consists of the
downsampling operator D, with a downsampling factor of
2, and the tiling operator Tj, where j = 1, . . . , 4, which extracts the j−th tile from a 2 × 2 grid of tiles. Notice that
these two transformations produce images of the same size.
Thus, we can set G ≡{D, T1, . . . , T4}. We also deﬁne
our desired relation between counting features on the transformed images as g(d, t1, . . . , t4) = d −P4
j=1 tj. This
can be written explicitly as
We use eq. (2) as our main building block to learn features
φ that can count visual primitives.
This relationship has a bearing also on equivariance
Equivariance, however, is typically deﬁned as the
property of a given feature.
In this work we invert this
logic by ﬁxing the transformations and by ﬁnding instead a
representation satisfying those transformations. Moreover,
equivariance has restrictions on the type of transformations
applied to the inputs and the features.
Notice that we have no simple way to control the scale
at which our counting features work. It could count object parts, whole objects, object groups, or any combination
thereof. This choice might depend on the number of elements of the counting vector φ, on the loss function used
for training, and on the type of data used for training.
4. Learning to Count
We use convolutional neural networks to obtain our representation. In principle, our network could be trained with
color images x from a large database (e.g., ImageNet 
or COCO ) using an l2 loss based on eq. (2), for example,
φ(D ◦x) −P4
j=1 φ(Tj ◦x)
However, this loss has φ(z) = 0, ∀z, as its trivial solution. To avoid such a scenario, we use a contrastive loss ,
where we also enforce that the counting feature should be
φ(T1 ◦x) φ(T2 ◦x)
φ(T3 ◦x) φ(T4 ◦x)
max{0, M −|c −t|2}
Figure 2: Training AlexNet to learn to count. The proposed architecture uses a siamese arrangement so that we
simultaneously produce features for 4 tiles and a downsampled image. We also compute the feature from a randomly
chosen downsampled image (D ◦y) as a contrastive term.
different between two randomly chosen different images.
Therefore, for any x ̸= y, we would like to minimize
ℓcon(x, y) =
φ(D ◦x) −P4
j=1 φ(Tj ◦x)
φ(D ◦y) −P4
j=1 φ(Tj ◦x)
where in our experiments the constant scalar M = 10.
Least effort bias. A bias of the system is that it can easily
satisfy the constraint (3) by learning to count as few visual
primitives as possible. Thus, many entries of the feature
mapping may collapse to zero. This effect is observed in the
ﬁnal trained network. In Fig. 3, we show the average of features computed over the ImageNet validation set. There are
only 30 and 44 non zero entries out of 1000 after training on
ImageNet and on COCO respectively. Despite the sparsity
of the features, our transfer learning experiments show that
the features in the hidden layers (conv1-conv5) perform
very well on several benchmarks. In our formulation (4),
the contrastive term limits the effects of the least effort bias.
Indeed, features that count very few visual primitives cannot differentiate much the content across different images.
Therefore, the contrastive term will introduce a tradeoff that
average magnitude
Figure 3: Average response of our trained network on
the ImageNet validation set. Despite its sparsity (30 non
zero entries), the hidden representation in the trained network performs well when transferred to the classiﬁcation,
detection and segmentation tasks.
will push features towards counting as many primitives as
is needed to differentiate images from each other.
Network architecture. In principle, the choice of the architecture is arbitrary. For ease of comparison with stateof-the-art methods when transferring to classiﬁcation and
detection tasks, we adopt the AlexNet architecture as
commonly done in other self-supervised learning methods.
We use the ﬁrst 5 convolutional layers from AlexNet followed by three fully connected layers ((3×3×256)×4096,
4096 × 4096, and 4096 × 1000), and ReLU units. Note that
1000 is the number of elements that we want to count. We
use ReLU in the end since we want the counting vector to be
all positive. Our input is 114 × 114 pixels to handle smaller
tiles. Because all the features are the same, training with
the loss function in eq. 4 is equivalent to training a 6-way
siamese network, as shown in Fig. 2.
5. Experiments
We ﬁrst present the evaluations of our learned representation in the standard transfer learning benchmarks. Then,
we perform ablation studies on our proposed method to
show quantitatively the impact of our techniques to prevent
poor representations. Finally, we analyze the learned representation through some quantitative and qualitative experiments to get a better insight into what has been learned. We
call the activation of the last layer of our network, on which
the loss (4) is deﬁned, the counting vector. We evaluate
whether each unit in the counting vector is counting some
visual primitive or not. Our model is based on AlexNet 
in all experiments. In our tables we use boldface for the top
performer and underline the second top performer.
Implementation Details.
We use caffe with the default weight regularization settings to train our network.
The learning rate is set to be quite low to avoid divergence.
We begin with a learning rate of 10−4 and drop it by a factor of 0.9 every 10K iterations. An important step is to normalize the input by subtracting the mean intensity value and
dividing the zero-mean images by their standard deviation.
Supervised 
Context 
Context ∗
Jigsaw 
ego-motion 
ego-motion ∗
Adversarial ∗
ContextEncoder 
Sound 
Sound ∗
Video 
Video ∗
Colorization ∗
Split-Brain ∗
ColorProxy 
WatchingObjectsMove 
Table 1: Evaluation of transfer learning on PASCAL.
Classiﬁcation and detection are evaluated on PASCAL VOC
2007 in the frameworks introduced in and respectively. Both tasks are evaluated using mean average precision (mAP) as a performance measure. Segmentation is
evaluated on PASCAL VOC 2012 in the framework of ,
which reports mean intersection over union (mIoU). (*) denotes the use of the data initialization method .
5.1. Transfer Learning Evaluation
We evaluate our learned representation on the detection, classiﬁcation, and segmentation tasks on the PASCAL
dataset as well as the classiﬁcation task on the ImageNet
dataset. We train our counting network on the 1.3M images from the training set of ImageNet. We use images of
114×114 pixels as input. Since we transfer only the convolutional layers, it has no effect on the transferred models and
evaluation. A new version of has been released ,
where the standard AlexNet is used for transfer learning.
All the numbers in our comparisons are from that version.
Fine-tuning on PASCAL
In this set of experiments, we ﬁne-tune our network on
the PASCAL VOC 2007 and VOC 2012 datasets, which
are standard benchmarks for representation learning. Finetuning is based on established frameworks for object classiﬁcation , detection and segmentation tasks.
The classiﬁcation task is a multi-class classiﬁcation problem, which predicts the presence or absence of 20 object
The detection task involves locating objects by
specifying a bounding box around them. Segmentation assigns the label of an object class to each pixel in the image. As shown in Table 1, we either outperform previous
methods or achieve the second best performance. Notice
Supervised 
Context 
Jigsaw 
ContextEncoder 
Adversarial 
Colorization 
Split-Brain 
Table 2: ImageNet classiﬁcation with a linear classiﬁer.
We use the publicly available code and conﬁguration of
 . Every column shows the top-1 accuracy of AlexNet
on the classiﬁcation task. The learned weights from conv1
up to the displayed layer are frozen. The features of each
layer are spatially resized until there are fewer than 9K dimensions left. A fully connected layer followed by softmax
is trained on a 1000-way object classiﬁcation task.
Places labels 
ImageNet labels 
Context 
Jigsaw 
Context encoder 
Sound 
Adversarial 
Colorization 
Split-Brain 
Table 3: Places classiﬁcation with a linear classiﬁer. We
use the same setting as in Table 2 except that to evaluate
generalization across datasets, the model is pretrained on
ImageNet (with no labels) and then tested with frozen layers
on Places (with labels). The last layer has 205 neurons for
scene categories.
that while classiﬁcation and detection are evaluated on VOC
2007, segmentation is evaluated on VOC 2012. Unfortunately, we did not get any performance boost when using
the method of Kr¨ahenb¨uhl et al. .
Linear Classiﬁcation on Places and ImageNet
As introduced by Zhang et al. , we train a linear classiﬁer on top of the frozen layers on ImageNet and
Places datasets. The results of these experiments are
shown in Tables 2 and 3. Our method achieves a performance comparable to the other state-of-the-art methods on
the ImageNet dataset and shows a signiﬁcant improvement
on the Places dataset. Training and testing a method on the
same dataset type, although with separate sets and no labels,
Interpolation
performance
Table 4: Ablation studies. We train the counting task under different interpolation methods, training size/color, and
feature dimensions, and compare the performance of the
learned representations on the detection task on the PAS-
CAL VOC 2007 dataset.
may be affected by dataset bias. To have a better assessment of the generalization properties of all the competing
methods, we suggest (as shown in Table 3) using the ImageNet dataset for training and the Places benchmark for testing (or vice versa). Our method archives state-of-the-art results with the conv1-conv4 layers on the Places dataset.
Interestingly, the performance of our conv1 layer is even
higher than the one obtained with supervised learning
when trained either on ImageNet or Places labels. The values for all the other methods in Tables 2 and 3 are taken
form except for , which we report for the ﬁrst time.
5.2. Ablation Studies
To show the effectiveness of our proposed method, in Table 4 we compare its performance on the detection task on
PASCAL VOC 2007 under different training scenarios. The
ﬁrst three rows illustrate some simple comparisons based
on feature and dataset size. The ﬁrst row shows the impact of the counting vector length. As discussed earlier on,
the network tends to generate sparse counting features. We
train the network on ImageNet with only 20 elements in
the counting vector. This leads to a small drop in the performance, thus showing little sensitivity with respect to feature
length. We also train the network with a smaller set of training images. The results show that our method is sensitive to
the size of the training set. This shows that the counting
task is non-trivial and requires a large training dataset.
The remaining rows in Table 4 illustrate a more advanced
analysis of the counting task. An important part of the design of the learning procedure is the identiﬁcation of trivial
solutions, i.e., solutions that would not result in a useful
representation and that the neural network could converge
to. By identifying such trivial learning scenarios, we can
train\test
Table 5: Learning the downsampling style. The ﬁrst column and row show the downsampling methods used during the training and testing time respectively. The values in
the ﬁrst block show the pairwise error metric in eq. (6) between corresponding downsampling methods. The last column shows the standard deviation of the error metric across
different downsampling methods at test time.
provide suitable countermeasures. We now discuss possible
shortcuts that the network could use to solve the counting
task and also the techniques that we use to avoid them.
A ﬁrst potential problem is that the neural network
learns trivial features such as low-level texture statistics histograms. For example, a special case is color histograms.
This representation is undesirable because it would be semantically agnostic (or very weak) and therefore we would
not expect it to transfer well to classiﬁcation and detection.
In general, these histograms would not satisfy eq. (2). However, if the neural network could tell tiles apart from downsampled images, then it could apply a customized scaling
factor to the histograms in the two cases and satisfy eq. (2).
In other words, the network might learn the following degenerate feature
if z is a tile
if z is downsampled.
Notice that this feature would satisfy the ﬁrst term in eq. (2).
The second (contrastive) term would also be easily satis-
ﬁed since different images have typically different low-level
texture histograms. We discuss below scenarios when this
might happen and present our solutions towards reducing
the likelihood of trivial learning.
The network recognizes the downsampling style. During training, we randomly crop a 224 × 224 region from
a 256 × 256 image. Next, we downsample the whole image by a factor of 2. The downsampling style, e.g., bilinear,
bicubic, and Lanczos, may leave artifacts in images that the
network may learn to recognize. To make the identiﬁcation of the downsampling method difﬁcult, at each stochastic gradient descent iteration, we randomly pick either the
bicubic, bilinear, lanczos, or the area method as deﬁned in
OpenCV . As shown in Table 4, the randomization of
different downsampling methods signiﬁcantly improves the
detection performance by at least 2.2%.
In Table 5, we perform another experiment that clearly
shows that network learns the downsampling style.
train our network by using only one downsampling method.
Then, we test the network on the pretext task by using only
one (possibly different) method. If the network has learned
to detect the downsampling method, then it will perform
poorly at test time when using a different one. As an error
metric, we use the ﬁrst term in the loss function normalized
by the average of the norm of the feature vector. More precisely, the error when the network is trained with the i-th
downsampling style and tested on the j-th one is
p=1 φi  Tp ◦x) −φi(Dj ◦x
x |φi (Di ◦x)|2
where φi denotes the counting vector of the network trained
with the i-th downsampling method. Dj denotes the downsampling transformation using the j-th method. Tp is the
tiling transformation that gives the p-th tile.
Table 5 collects all the computed errors. The element
in row i and column j shows the pairwise error metric eij.
The last column shows the standard deviation of this error
metric across different downsampling methods. A higher
value means that the network is sensitive to the downsampling method. This experiment clearly shows that the network learns the downsampling style. Another observation
that can be made based on the similarity of the errors, is that
the pairs (linear, area) and (cubic, lanczos) leave similar artifacts in downsampling.
The network recognizes chromatic aberration. The presence of chromatic aberration and its undesirable effects on
learning have been pointed out by Doersch et al. . Chromatic aberration is a relative shift between the color channels that increases in the outward radial direction. Hence,
our network can use this property to tell tiles apart from
the dowsampled images. In fact, tiles will have a strongly
diagonal chromatic aberration, while the downsampled image will have a radial aberration. We already reduce its effect by choosing the central region in the very ﬁrst cropping preprocessing. To further reduce its effect, we train
the network with both color and grayscale images (obtained
by replicating the average color across all 3 channels). In
training, we randomly choose color images 33% of the time
and grayscale images 67% of the time. This choice is consistent across all the terms in the loss function (i.e., all tiles
and downsampled images are either colored or grayscale).
While this choice does not completely solve the issue, it
does improve the performance of the model. We ﬁnd that
completely eliminating the color from images leads to a loss
in performance in transfer learning (see Table 4).
5.3. Analysis
We use visualization and nearest neighbor search to see
what visual primitives our trained network counts. Ideally,
these visual primitives should capture high-level concepts
Figure 4: Examples of activating/ignored images. (a) and (b) show the top 16 images with the highest and lowest counting
feature magnitude from the validation set of ImageNet. (c) and (d) show the top 16 images with the highest and lowest
counting feature magnitude from the test set of COCO.
Figure 5: Image croppings of increasing size. The number
of visual primitives should increase going from left to right.
counting vector magnitude
Figure 6: Counting evaluation on ImageNet. On the abscissa we report the scale of the cropped region and on the
ordinate the corresponding average and standard deviation
of the counting vector magnitude.
like objects or object parts rather than low-level concepts
like edges and corners. In fact, detecting simple corners
will not go a long way in semantic scene understanding. To
avoid dataset bias, we train our model on ImageNet (with
no labeles) and show the results on COCO dataset.
Quantitative Analysis
We illustrate quantitatively the relation between the magnitude of the counting vector and the number of objects.
Rather than counting exactly the number of speciﬁc objects, we introduce a simple method to rank images based
on how many objects they contain. The method is based on
cropping an image with larger and larger regions which are
then rescaled to the same size through downsampling (see
Fig. 5). We build two sets of 100 images each. We assign
images yielding the highest and lowest feature magnitude
into two different sets. We randomly crop 10 regions with
an area between 50%−95% of each image and compute the
corresponding counting vector. The mean and the standard
deviation of the counting vector magnitude of the cropped
images for each set is shown in Fig 6. We observe that
our feature does not count low-level texture, and is instead
more sensitive to composite images. A better understanding
of this observation needs futher investigation.
Qualitative Analysis
Activating/Ignored images. In Fig 4, we show blocks of
16 images ranked based on the magnitude of the counting vector. We observe that images with the lowest feature
norms are textures without any high-level visual primitives.
In contrast, images with the highest feature response mostly
contain multiple object instances or a large object. For this
experiment we use the validation or the test set of the dataset
that the network has been trained on, so the network has not
seen these images during training.
Nearest neighbor search.
To qualitatively evaluate our
learned representation, for some validation images, we visualize their nearest neighbors in the training set in Fig. 7.
Given a query image, the retrieval is obtained as a ranking of the Euclidean distance between the counting vector
of the query image and the counting vector of images in
the dataset. Smaller values indicate higher afﬁnity. Fig. 7
shows that the retrieved results share a similar scene outline
and are semantically related to the query images. Note that
we perform retrieval in the counting space, which is the last
layer of our network. This is different from the analogous
experiment in which performs the retrieval in the intermediate layers. This result can be seen as an evidence
that our initial hypothesis, that the counting vectors capture
high level visual primitives, was true.
Neuron activations. To visualize what each single counting neuron (i.e., feature element) has learned, we rank im-
Figure 7: Nearest neighbor retrievals. Left: COCO retrievals. Right: ImageNet retrievals. In both datasets, the leftmost
column (with a red border) shows the queries and the other columns show the top matching images sorted with increasing
Euclidean distance in our counting feature space from left to right. On the bottom 3 rows, we show the failure retrieval cases.
Note that the matches share a similar content and scene outline.
Figure 8: Blocks of the 8 most activating images for 4 neurons of our network trained on ImageNet (top row) and COCO
(bottom row). The counting neurons are sensitive to semantically similar images. Interestingly, dominant concepts in each
dataset, e.g., dogs in ImageNet and persons playing baseball in COCO, emerge in our counting vector.
ages not seen during training based on the magnitude of
their neuron responses. We do this experiment on the validation set of ImageNet and the test set of COCO. In Fig. 8,
we show the top 8 most activating images for 4 neurons out
of 30 active ones on ImageNet and out of 44 active ones on
COCO. We observe that these neurons seem to cluster images that share the same scene layout and general content.
6. Conclusions
We have presented a novel representation learning
method that does not rely on annotated data. We used counting as a pretext task, which we formalized as a constraint
that relates the “counted” visual primitives in tiles of an image to those counted in its downsampled version. This constraint was used to train a neural network with a contrastive
loss. Our experiments show that the learned features count
non-trivial semantic content, qualitatively cluster images
with similar scene outline, and outperform previous state
of the art methods on transfer learning benchmarks. Our
framework can be further extended to other tasks and transformations in addition to being combined with partially labeled data in a semi-supervised learning method.
Acknowledgements. We thank Attila Szab´o for insightful discussions about unsupervised learning and relations
based on equivariance. Paolo Favaro acknowledges support from the Swiss National Science Foundation on project
200021 149227. Hamed Pirsiavash acknowledges support
from GE Global Research.