I N S T I T U T E
How to Make Causal Inferences with
Time-Series Cross-Sectional Data
under Selection on Observables
Matthew Blackwell
Adam Glynn
Working Paper
SERIES 2018:67
THE VARIETIES OF DEMOCRACY INSTITUTE
Varieties of Democracy (V–Dem) is a new approach to conceptualization and measurement of democracy. The headquarters – the V-Dem Institute – is based at the University of
Gothenburg with 17 staff. The project includes a worldwide team with six Principal Investigators, 14 Project Managers, 30 Regional Managers, 170 Country Coordinators, Research
Assistants, and 3,000 Country Experts. The V-Dem project is one of the largest ever social
science research-oriented data collection programs.
Please address comments and/or queries for information to:
V–Dem Institute
Department of Political Science
University of Gothenburg
Sprängkullsgatan 19, PO Box 711
SE 40530 Gothenburg
E-mail: 
V–Dem Working Papers are available in electronic format at www.v-dem.net.
Copyright ©2018 by authors. All rights reserved.
How to Make Causal Inferences with Time-Series
Cross-Sectional Data under Selection on
Observables*
Matthew Blackwell†
Adam Glynn‡
April 26, 2018
*We are grateful to Neal Beck, Jake Bowers, Patrick Brandt, Simo Goshev, and Cyrus Samii for helpful
advice and feedback and Elisha Cohen for research support. Any remaining errors are our own. This research
project was supported by Riksbankens Jubileumsfond, Grant M13-0559:1, PI: Staffan I. Lindberg, V-Dem Institute, University of Gothenburg, Sweden; by Knut and Alice Wallenberg Foundation to Wallenberg Academy
Fellow Staffan I. Lindberg, Grant 2013.0166, V-Dem Institute, University of Gothenburg, Sweden; by European Research Council, Grant 724191, PI: Staffan I. Lindberg, V-Dem Institute, University of Gothenburg,
Sweden; as well as by internal grants from the Vice-Chancellor’s office, the Dean of the College of Social
Sciences, and the Department of Political Science at University of Gothenburg.
†Department of Government and Institute for Quantitative Social Science, Harvard University, 1737 Cambridge St, ma 02138. web: email: 
‡Department of Political Science, Emory University, 327 Tarbutton Hall, 1555 Dickey Drive, Atlanta, ga
30322 email: 
Repeated measurements of the same countries, people, or groups over time are vital to many
fields of political science. These measurements, sometimes called time-series cross-sectional
(TSCS) data, allow researchers to estimate a broad set of causal quantities, including contemporaneous and lagged treatment effects. Unfortunately, popular methods for TSCS data
can only produce valid inferences for lagged effects under very strong assumptions. In this
paper, we use potential outcomes to define causal quantities of interest in this settings and
clarify how standard models like the autoregressive distributed lag model can produce biased estimates of these quantities due to post-treatment conditioning. We then describe two
estimation strategies that avoid these post-treatment biases—inverse probability weighting
and structural nested mean models—and show via simulations that they can outperform
standard approaches in small sample settings. We illustrate these methods in a study of how
welfare spending affects terrorism.
Introduction
Many inquiries in political science involve the study of repeated measurements of the same
countries, people, or groups at several points in time. This type of data, sometimes called
time-series cross-sectional (TSCS) data, allows researchers to draw on a larger pool of information when estimating causal effects. TSCS data also give researchers the power to ask
a richer set of questions than data with a single measurement for each unit . Using this data, researchers can move past the narrowest contemporaneous questions—what are the effects of a single event—and instead ask how the history
of a process affects the political world. Unfortunately, the most common approaches to
modeling TSCS data require strict assumptions to estimate the effect of treatment histories
without bias and make it difficult to understand the nature of the counterfactual comparisons.
This paper makes three contributions to the study of TSCS data. Our first contribution is
to define counterfactual causal effects and discuss the assumptions needed to identify them
nonparametrically. We relate these quantities of interest to common quantities in the TSCS
literature, like impulse responses, and show how to derive them from the parameters of a
common TSCS model, the autoregressive distributed lag (ADL) model. These treatment
effects can be nonparametrically identified under a key selection-on-observables assumption
called sequential ignorability; unfortunately, however, many common TSCS approaches rely
on more stringent assumptions, including a lack of causal feedback between the treatment
and time-varying covariates. This feedback, for example, might involve a country’s level of
welfare spending affecting domestic terrorism, which in turn might affect future levels of
spending. We argue that this type of feedback is common in TSCS settings. While we focus
on a selection-on-observables assumption in this paper, we discuss the tradeoffs with this
choice compared to standard fixed-effects methods, noting that the latter may also rule out
this type of dynamic feedback.
Our second contribution is to provide an introduction to two methods from biostatistics
that can estimate the effect of treatment histories without bias and under weaker assumptions than common TSCS models. We focus on two methods: (1) structural nested mean models
or SNMMs and (2) marginal structural models with inverse probability of treatment
weighting or MSMs with IPTWs . These models allow for consistent estimation of lagged effects of treatment by paying careful attention to
the causal ordering of the treatment, the outcome, and the time-varying covariates. The
SNMM approach generalizes the standard regression modeling of ADLs and often imply
very simple and intuitive multi-step estimators. The MSM approach focuses on modeling
the treatment process to develop weights that adjust for confounding in simple weighted
regression models. Both of these approaches have the ability to incorporate weaker modeling assumptions than traditional TSCS models. We describe the modeling choices involved
and provide guidance on how to implement these methods.
Our third contribution is to show how traditional models like the ADL are biased for
lagged treatment effects in common TSCS settings, while MSMs and SNMMs are not. This
bias arises from the time-varying covariates—researchers must control for them to accurately estimate contemporaneous effects, but they induce post-treatment bias for lagged effects. Thus, ADL models can only consistently estimate lagged effects when time-varying
covariates are unaffected by past treatment. SNMMs and MSMs, on the other hand, can
estimate these effects even when such feedback exists. We provide simulation evidence that
this type of feedback can lead to significant bias in ADL models compared to the SNMM
and MSM approaches. Overall, these latter methods could be promising for TSCS scholars,
especially those who are interested longer-term effects.
This paper proceeds as follows. Section 2 clarifies the causal quantities of interest available with TSCS data and shows how they relate to parameters from traditional TSCS models.
Causal assumptions are a key part of any TSCS analysis and we discuss them in Section 3.
Section 4 discusses post-treatment bias stemming from traditional TSCS approaches, and
Section 5 introduces the SNMM and MSM approaches which avoid this post-treatment bias
and shows how to estimate causal effects using these methodologies. We present simulation
evidence of how these methods outperform traditional TSCS models in small samples in
Section 6. In Section 7, we present an empirical illustration of each approach, based on Burgoon , investigating the connection between welfare spending and terrorism. Finally,
Section 8 concludes with thoughts on both the limitations of these approaches and avenues
for future research.
Causal quantities of interest in TSCS data
At their most basic, TSCS data consists of a treatment (or main independent variable of
interest), an outcome, and some covariates all measured for the same units at various points
in time. In our empirical setting below, we focus on a dataset of countries with the number
of terrorist incidents as an outcome and domestic welfare spending as a binary treatment.
With one time period, only one causal comparison exists: a country has either high or low
levels of welfare spending. As we gather data on these countries over time, there are more
counterfactual comparisons to investigate. How does the history of welfare spending affect
the incidence of terrorism? Does the spending regime today only affect terrorism today or
does the recent history matter as well? The variation over time provides the opportunity
and the challenge of answering these complex questions.
To fix ideas, let Xit be the treatment for unit i in time period t. For simplicity, we focus
first on the case of a binary treatment so that Xit = 1 if the unit is treated in period t and
Xit = 0 if the unit is untreated in period t (it is straightforward to generalize them to arbitrary
treatment types). In our running example, Xit = 1 would represent a country that had high
welfare spending in year t and Xit = 0 would be a country with low welfare spending. We
collect all of the treatments for a given unit into a treatment history, Xi = (Xi1, . . . , XiT), where T
is the number of time periods in the study. For example, we might have a country that always
had high spending, (1,1, . . . ,1), or a country that always had low spending, (0,0, . . . ,0). We
refer to the partial treatment history up to t as Xi,1:t = (Xi1, . . . , Xit), with x1:t as a possible
particular realization of this random vector. We define Zit, Zi,1:t, and z1:t similarly for a
set of time-varying covariates that are causally prior to the treatment at time t such as the
government capability, population size, and whether or not the country is in a conflict.
The goal is to estimate causal effects of the treatment on an outcome, Yit, that also varies
over time. In our running example, Yit is the number of terrorist incidents in a given country
in a given year. We take a counterfactual approach and define potential outcomes for each
time period, Yit(x1:t) .1 This potential outcome represents the
incidence of terrorism that would occur in country i in year t if i had followed history of
welfare spending equal to x1:t. Obviously, for any country in any year, we only observe
one of these potential outcomes since a country cannot follow multiple histories of welfare
spending over the same time window. To connect the potential outcomes to the observed
outcomes, we make the standard consistency asssumption. Namely, we assume that the observed
outcome and the potential outcome are the same for the observed history: Yit = Yit(x1:t)
when Xi,1:t = x1:t.
To create a common playing field for all the methods we evaluate, we limit ourselves
to making causal inferences about the time window observed in the data—that is, we want
to study the effect of welfare spending on terrorism for the years in our data set. Under
certain assumptions like stationarity of the covariates and error terms, many TSCS methods
can make inferences about the long-term effects beyond the end of the study. This extrapolation is, of course, required with a single time series, but with the multiple units we have
in TSCS data, we have the ability to focus our inferences on a particular window and avoid
1The definition of potential outcomes in this manner implicitly assumes the usual stable unit treatment
value assumption (SUTVA) . This assumption is questionable for the many comparative politics
and international relations applications, but we avoid discussing this complication in this paper in order to
focus on the issues regarding TSCS data. Implicit in our definition of the potential outcomes is that outcomes
at time t only depend on past values of treatment, not future values .
these assumptions about the time-series processes. We view this as a conservative approach
because all methods for handling TSCS should be able to generate sensible estimates of
causal effects in the period under study. Of course, there is a tradeoff with this approach:
we cannot study some common TSCS estimands like the long-run multiplier that are based
on time-series analysis. We discuss this estimand in particular in the supplemental materials.
Given our focus on a fixed time window, we will define expectations over cross-sectional
units and consider asymptotic properties of the estimators as the number of these units
grows (rather than the length of the time series). Of course, asymptotics are only useful
in how they guide our analyses in the real world of finite samples, and we may worry that
“large-N, fixed-T” asymptotic results may not provide a reliable approximation when N
and T are roughly the same size, as is often the case for TSCS data. Fortunately, as we
show in the simulation studies of Section 6, our analysis of the various TSCS estimators
holds even when N and T are small and close in size. Thus, we do not see the choices of
“fixed time-window” versus “time-series analysis” or large-N versus large-T asymptotics to
be consequential to the conclusions we draw.
The effect of a treatment history
For an individual country, the causal effect of a particular history of welfare spending, x1:t,
relative to some other history of spending, x′
1:t, is the difference Yit(x1:t) −Yit(x′
1:t). That
is, it is the difference in the potential or counterfactual level of terrorism when the country
follows history x1:t minus the counterfactual outcome when it follows history x′
1:t. Given the
number of possible treatment histories, there can be numerous causal effects to investigate,
even with a simple binary treatment. As the length of time under study grows, so does
the number of possible comparisons. In fact, with a binary treatment there are 2t different
potential outcomes for the outcome in period t. This large number of potential outcomes
allows for a very large number of comparisons and a host of causal questions: does the
stability of spending over time matter for the impact on the incidence of terrorism? Is there
a cumulative impact of welfare spending or is it only the current level that matters?
These individual-level causal effects are difficult to identify without strong assumptions,
so we often focus on estimating the average causal effect of a treatment history :
τ(x1:t, x′
1:t) = E[Yit(x1:t) −Yit(x′
Here, the expectations are over the units so that this quantity is the average difference in
outcomes between the world where all units had history x1:t and the world where all units
Figure 1: Directed acyclic graph (DAG) of a typical TSCS data. Dotted red lines are the causal
pathways that constitute the average causal effect of a treatment history at time t.
had history x′
1:t. For example, we might be interested in the effect of a country having
always high welfare spending versus a country always having low spending levels. Thus, this
quantity considers the effect of treatment at time t, but also the effect of all lagged values
of the treatment as well. A graphical depiction of the pathways contained in τ(x1:t, x′
presented in Figure 1, where the red arrows correspond to components of the effect. These
arrows represent all of the effects of Xit, Xi,t−1, Xi,t−2, and so on, that end up at Yit. Note that
many of these effects flow through the time-varying covariates, Zit. This point complicates
the estimation of causal effects in this setting and we return to it below.
Marginal effects of recent treatments
As mentioned above, there are numerous possible treatment histories to compare when
estimating causal effects. This can be daunting for applied researchers who may only be interested in the effects of the first few lags of welfare spending. Furthermore, any particular
treatment history may not be well-represented in the data if the number of time periods is
moderate. To avoid these problems, we introduce causal quantities that focus on recent values of treatment and average over more distant lags. We define the potential outcomes just
intervening on treatment the last j periods as Yit(xt−j:t) = Yit(Xi,1:t−j−1, xt−j:t). This “marginal”
potential outcome represents the potential or counterfactual level of terrorism in country i
if we let welfare spending run its natural course up to t −j −1 and just set the last j lags of
spending to xt−j:t.2
With this definition in hand, we can define one important quantity of interest, the contemporaneous effect of treatment (CET) of Xit on Yit:
τc(t) = E[Yit(Xi,1:t−1,1) −Yit(Xi,1:t−1,0)],
= E[Yit(1) −Yit(0)],
2See Shephard and Bojinov for a similar approach to defining recent effects in time-series data.
Figure 2: DAG of a TSCS setting where the dotted red line represents the contemporaneous effect
of treatment at time t.
Here we have switched from potential outcomes that depend on the entire history to potential outcomes that only depend on treatment in time t. The CET reflects the effect of
treatment in period t on the outcome in period t, averaging across all of the treatment histories up to period t. Thus, it would be the expected effect of switching a random country
from low levels of welfare spending to high levels in period t. A graphical depiction of a
CET is presented in Figure 2, where the red arrow corresponds to component of the effect.
It is common in pooled TSCS analyses to assume that this effect is constant over time so
that τc(t) = τc.
Researchers are also often interested in how more distant changes to treatment affect
the outcome. Thus, we define the lagged effect of treatment, which is the marginal effect
of treatment in time t −1 on the outcome in time t, holding treatment at time t fixed:
E[Yit(1,0) −Yit(0,0)]. More generally, the j-step lagged effect is defined as follows:
τl(t, j) = E[Yit(Xi,1:t−j−1,1,0j) −Yit(Xi,1:t−j−1,0,0j)],
= E[Yit(1,0j) −Yit(0j+1)],
where 0s is a vector of s zero values. For example, the two-step lagged effect would be
E[Yit(1,0,0) −Yit(0,0,0)] and represents the effect of welfare spending two years ago on
terrorism today holding the intervening welfare spending fixed at low levels. A graphical
depiction of the one-step lagged effect is presented in Figure 3, where again the red arrows
correspond to component of the effect. These effects are similar to a common quantity of
interest in both time-series and TSCS applications called the impulse response .
Another common quantity of interest in the TSCS literature is the step response, which
is the culmulative effect of a permanent shift in treatment status on some future outcome
 . The step response function, or
SRF, describes how this effect varies by time period and distance between the shift and the
Figure 3: DAG of a panel setting where the dotted red lines represent the paths that constitute the
lagged effect of treatment at time t −1 on the outcome at time t.
τs(t, j) = E[Yit(1j) −Yit(0j)],
where 1s has a similar definition to 0s. Thus, τs(t, j) is the effect of a j periods of treatment at
time t −j on the outcome at time t. Without further assumptions, there are separate lagged
effects and step responses for each pair of periods. As we discuss next, traditional modeling
of TSCS data imposes restrictions on the data-generating processes in part to summarize
this large number of effects with a few parameters.
Relationship to traditional TSCS models
The potential outcomes and causal effects defined above are completely nonparametric
in the sense that they impose no restrictions on the distribution of Yit. To situate these
quantities in the TSCS literature, it is helpful to see how they are parameterized in a particular
TSCS model. One general model that encompasses many different possible specifications
is called an autoregressive distributed lag (ADL) model:3
Yit = β0 + αYi,t−1 + β1Xit + β2Xi,t−1 + εit,
where εit are i.i.d. errors, independent of Xis for all t and s. The key features of such a model
are the presence of lagged independent and dependent variables and the exogeneity of the
independent variables. This model for the outcome would imply the following form for the
potential outcomes:
Yit(x1:t) = β0 + αYi,t−1(x1:t−1) + β1xt + β2xt−1 + εit.
In this form, it is clear to see what TSCS scholars have long pointed out: causal effects are
complicated with lagged dependent variables since a change in xt−1 can have both a direct
3For introductions to modeling choices for TSCS data in political science, see De Boef and Keele 
and Beck and Katz .
effect on Yit and an indirect effect through Yi,t−1. This is why even seemingly simple TSCS
models such as the ADL imply quite complicated expressions for long-run effects.
The ADL model also has implications for the various causal quantities, both short-term
and long-term. The coefficient on the contemporaneous treatment, β1, is constant over time
and does not depend on past values of the treatment, so it is equal to the CET, τc(t) = β1.
One can derive the lagged effects from different combinations of α, β1, and β2:
τl(t,0) = β1,
τl(t,1) = αβ1 + β2,
τl(t,2) = α2β1 + αβ2.
Note that these lagged effects are constant across t. The step response, on the other hand,
has a stronger impact because it accumulates the impulse responses over time:
τs(t,0) = β1,
τs(t,1) = β1 + αβ1 + β2,
τs(t,2) = β1 + αβ1 + β2 + α2β1 + αβ2.
Note that the step response here is just the sum of all previous lagged effects. It is clear that
one benefit of such a TSCS model is to summarize a broad set of estimands with just a few
parameters. This helps to simplify the complexity of the TSCS setting while introducing the
possibility of bias if this model is incorrect or misspecified.
Causal assumptions and designs in TSCS data
Under what assumptions are the above causal quantities identified? When we have repeated
measurements on the outcome-treatment relationship, there are a number of assumptions
we could invoke in order to identify causal effects. In this section we discuss several of
these assumptions. We focus on cross-sectional assumptions given our fixed time-window
approach. That is, we make no assumptions on the time-series processes such as stationarity
even though imposing these types of assumptions will not materially affect our conclusions
about the bias of traditional TSCS methods. This result is confirmed in the simulations of
Section 6, where the data generating process is stationary and the biases we describe below
still occur.
Baseline randomized treatments
A powerful, if rare, research design for TSCS data is one that randomly assigns the entire
history of treatment, X1:T, at time t = 0. Under this assumption, treatment at time t cannot
be affected by, say, previous values of the outcome or time-varying covariates. In terms of
potential outcomes, the baseline randomized treatment history assumption is:
{Yit(x1:t) : t = 1, . . . , T} ⊥⊥Xi,1:T|Zi0,
where A ⊥⊥B|C is defined as “A is independent of B conditional on C.” This assumes
that the entire history of welfare spending is independent of all potential levels of terrorism,
possibly conditional on baseline (that is, time-invariant) covariates. Hernán, Brumback and
Robins called Xi,1:T causally exogeneous under this assumption. The lack of time-varying
covariates or past values of Yit on the right-hand side of the conditioning bar in (12) implies that these variables do not confound the relationship between the treatment and the
outcome. For example, this assumes there are no time-varying covariates that affect both
welfare spending and the number of terrorist incidents. Thus, baseline randomization relies
on strong assumptions that are rarely satisfied outside of randomized experiments and is
unsuitable for most observational TSCS studies.4
Baseline randomization is closely related to exogeneity assumptions in linear TSCS models. For example, suppose we had the following distributed lag model with no autoregressive
component:
Yit = β0 + β1Xit + β2Xi,t−1 + ηit
Here, baseline randomization of the treatment history implies the usual identifying assumption in linear TSCS models, strict exogeneity of the errors:
E[ηit|Xi,1:T] = E[ηit] = 0.
This is a mean independence assumption about the relationship between the errors, ηit, and
the treatment history, Xi,1:T.
Sequentially randomized treatments
Beginning with Robins , scholars in epidemiology have expanded the potential outcomes framework to handle weaker identifying assumptions than baseline randomization.
4A notable exception are experiments with a panel design that randomize rollout of a treatment .
These innovations centered on sequentially randomized experiments, where at each period,
Xit was randomized conditional on the past values of the treatment and time-varying covariates (including past values of the outcome). Under this sequential ignorability assumption,
the treatment is randomly assigned not at the beginning of the process, but at each point in
time and can be affected by the past values of the covariates and the outcome.
At its core, sequential ignorability assumes there is some function or subset of the observed history up to time t, Vit = g(Xi,1:t−1, Yi,1:t−1, Zi,1:t), that is sufficient to satisfy no unmeasured confounders for the effect of Xit on future outcomes. Formally, the assumption
states that, conditional on this set of variables, Vit, the treatment at time t is independent of
the potential outcomes at time t:
Assumption 1 (Sequential Ignorability). For every treatment history x1:T and periods t,
{Yis(x1:s) : s = t, , . . . , T} ⊥⊥Xit|Vit.
For example, a researcher might assume that sequential ignorability for current welfare
spending holds conditional on lagged levels of terrorism, lagged welfare spending, and some
contemporaenous covariates, so that Vit = {Yi,t−1, Xi,t−1, Zit}. Unlike baseline randomization and strict exogeneity, it allows for observed time-varying covariates like conflict status
and lagged values of terrorism to confound the relationship between welfare spending and
current terrorism levels, so long as we have measures of these confounders. Furthermore,
these time-varying covariates can be affected by past values of welfare spending.
In the context of traditional TSCS models such as (4), sequential ignorability implies the
sequential exogeneity assumption:
E[εit|Xi,1:t, Zi,1:t, Yi,1:t−1] = E[εit|Xit, Vit] = 0.
According to the model in (4), the time-varying covariates here would include the lagged
dependent variable. This assumption states that the errors of the TSCS model are mean
independent of welfare spending at time t given the conditioning set that depends on the
history of the data up to t. Thus, this allows the errors for levels of terrorism to be related
to future values of welfare spending.
Sequential ignorability weakens baseline randomization to allow for feedback between
the treatment status and the time-varying covariates, including lagged outcomes. For instance, sequential ignorability allows for the welfare spending of a country to impact future
levels of terrorism and for this terrorism to affect future welfare spending. Thus, in this
dynamic case, treatments can affect the covariates and so the covariates also have poten-
tial responses: Zit(x1:t−1). This dynamic feedback implies that the lagged treatment may
have both a direct effect on the outcome and an indirect effect through this covariate. For
example, welfare spending might directly affect terrorism by reducing resentment among
potential terrorists, but it might also have an indirect effect if it helps to increase levels of
state capacity which could, in turn, help combat future terrorism.
In TSCS models, the lagged dependent variable, or LDV, is often included in the above
time-varying conditioning set, Vit, to assess the dynamics of the time-series process or to
capture the effects of longer lags of treatment in a simple manner.5 In either case, sequential ignorability would allow the LDV to have an effect on the treatment history as well, but
baseline randomization would not. For instance, welfare spending may have a strong effect
on terrorism levels which, in turn, affect future welfare spending. Under this type of feedback, a lagged dependent variable must be in the conditioning set Vit and strict exogeneity
will be violated.
Unmeasured confounding and fixed effects assumptions
Sequential ignorability is a selection-on-observables assumption—the researcher must be
able to choose a (time-varying) conditioning set to eliminate any confounding. A oft-cited
benefit of having repeated observations is that it allows scholars to estimate causal effects in
spite of time-constant unmeasured confounders. Linear fixed effects models, for instance,
have the benefit of adjusting for all time-constant covariates, measured or unmeasured. This
would be very helpful if, for instance, each country had its own baseline level of welfare
spending that was determined by factors correlated with terrorist attacks but the year-toyear variation in spending within a country was exogeneous. At first glance, this ability to
avoid time-constant omitted variable bias appears to be a huge benefit.
Unfortunately, these fixed effects estimation strategies require within-unit baseline randomization to identify any quantity other than the contemporaneous effect of treatment . Specifically, standard fixed effects models assume that previous values of covariates like GDP growth or lagged terrorist attacks (that is, the LDV) have
no impact on the current value of welfare spending. Thus, to estimate any effects of lagged
treatment, fixed effects models would allow for time-constant unmeasured confounding but
would also rule out a large number of TSCS applications where there is feedback between the
covariates and the treatment. Furthermore, the assumptions of fixed-effects-style models
in nonlinear settings can impose strong restrictions on over-time variation in the treatment
5In certain parametric models, the LDV can be interpreted as summarizing the effects of the entire history
of treatment. More generally, the LDV may effectively block confounding for contemporaneous treatment
even if it has no causal effect on the current outcome.
and outcome . For these reasons, and because there is a large
TSCS literature in political science that relies on selection-on-observables assumptions, we
focus on situations where sequential ignorability holds. We return to the avenues for future
research on fixed effects models in this setting in the conclusion.
The post-treatment bias of traditional TSCS models
Under sequential ignorability, standard TSCS models like the ADL model in Section 2.3
can become biased for common TSCS estimands. The basic problem with these models
is that sequential ignorability allows for the possibility of post-treatment bias when estimating lagged effects in the ADL model. While this problem is well known in statistics
 , we review it here in
the context of TSCS models to highlight the potential for biased and inconsistent estimators.
The root of the bias in the ADL approach is the nature of time-varying covariates, Zit.
Under the assumption of baseline randomization, there is no need to control or adjust
for these covariates beyond the baseline covariates, Zi0, because treatment is assigned at
baseline—future covariates cannot confound past treatment assignment. The ADL approach thrives in this setting. But when baseline randomization is implausible, as we argue
is true in most TSCS settings, we will typically require conditioning on these covariates to
obtain credible causal estimates. And this conditioning on Zit is what can create large biases
in the ADL approach.
To demonstrate the potential for bias, we focus on a simple case where we are only
interested in the first two lags of treatment and sequential ignorability assumption holds with
Vit = {Yi,t−1, Zit, Xi,t−1}. This means that treatment is randomly assigned conditional on the
contemporaneous value of the time-varying covariate and the lagged values of the outcome
and the treatment. Given this setting, the ADL approach would model the outcome as
Yit = β0 + αYi,t−1 + β1Xit + β2Xi,t−1 + Z′
itδ + εit.
Assuming this functional form is correct and assuming that εit are independent and identically distributed, this model would consistently estimate the contemporaneous effect of
treatment, Xit, given the sequential ignorability assumption. But what about the effect of
lagged treatment? In the ADL approach, one would combine the coefficients as bαbβ1 + bβ2.
The problem with this approach is that, if Zit is affected by Xi,t−1, then Zit will be posttreatment and in many cases induce bias in the estimation of bβ2 . Why not simply omit Zit from our model? Because this would
bias the estimates of the contemporary treatment effect, bβ1 due to omitted variable bias.6
In this setting, there is no way to estimate the direct effect of lagged treatment without bias with a single ADL model. Unfortunately, even weakening the parametric modeling assumptions via matching or generalized additive models will fail to overcome this
problem—it is inherent to the data generating process . These biases exist even in favorable settings for the ADL, such as when the outcome is stationary and
treatment effects are constant over time. Furthermore, as discussed above, standard fixed
effects models cannot eliminate this bias because it involves time-dependent causal feedback. Traditional approaches can only avoid the bias under special circumstances such as
when treatment is randomly assigned at baseline or when the time-varying covariates are
completely unaffected by treatment. Both of these assumptions lack plausibility in TSCS
settings, which is why many TSCS studies control for time-varying covariates. Below we
demonstrate this bias in simulations, but we first turn to two methods from biostatistics
that can avoid these biases.
Two methods for estimating the effect of treatment
If the traditional ADL model is biased in the presence of time-varying covariates, how can we
proceed with estimating both contemporaneous and lagged effect of treatment in the TSCS
setting? In this section, we show how to estimate the causal quantities of interest in Section 2
under sequential ignorability using two approaches developed in biostatistics to specifically
address this potential for bias in this type of setting. The first approach is based on structural
nested mean models (SNMMs), which, in their simplest form, represent an extension of
the ADL approach to avoid the post-treatment bias described above. The second class of
estimators, based on marginal structural models (MSM) and inverse probability of treatment
weighting (IPTW), is semiparametric the sense that it models the treatment history, but leaves
the relationship between the outcome and the time-varying covariates unspecified. Because
of this, MSMs have the advantage of being robust to our ability or inability to model the
outcome. We focus our attention on these two broad classes of models because they are
commonly used approaches that both (a) avoid post-treatment bias in this setting and (b)
do not require the parametric modeling of time-varying covariates.
6A second issue is that ADL models often only include conditioning variables to identify the contemporaneous effect, not any lagged effects of treatment. Thus, the effect of Xi,t−1 might also suffer from omitted
variable bias. This issue can be more easily corrected by including the proper condition set, Vi,t−1, in the
One modeling choice that is common to all of these approaches, including the ADL, is
the choice of causal lag length. Should we attempt to estimate the effect of the entire history of welfare spending on terrorist incidents with potential outcome Yit(x1:t)? Or should
we only investigate the contemporaneous and first lagged effects with potential outcome
Yit(xt−1, xt)? As we discussed above, we can always focus on effects that marginalize over
lags of treatment beyond the scope of our investigation. Thus, this choice of lag length is
less about the “correct” specification and more about choosing what question the researcher
wants to answer. A separate question is what variables and their lags need to be included in
the various models in order for our answers to be correct. We discuss the details of what
needs to be controlled for and when in our discussion of each estimator.
Structural nested mean models
Our first class of models, called structural nested mean models, can be seen as an extension
of the ADL approach that allows for estimation of lagged effects in a relatively straightforward manner . At their most general, these models focus on parameterizing a conditional version of the lagged effects (that is, the impulse response function):7
bt(x1:t, j) = E[Yit(x1:t−j,0j) −Yit(x1:t−j−1,0j+1)|X1:t−j = x1:t−j].
Robins refers to these impulse responses as “blip-down functions.” This function
gives the effect of a change from 0 to xt−j in terms of welfare spending on levels of terrorism
at time t, conditional on the treatment history up to time t−j. Inference in SNMMs focuses
on estimating the causal parameters of this function. The conditional mean of the outcome
given the covariates needs to be estimated as part of this approach, but this is seen as a
nuisance function rather than the object of direct interest.
Given the chosen lag length to study, a researcher must only specify the parameters of
the impulse response up to that many lags. If we chose a lag length of 1, for example, then
we might parameterize the impulse response function as:
bt(x1:t, j; γ) = γjxt−j,
Here, γj is the impulse effect of a one-unit change of welfare spending at lag j on levels of
terrorism which does not depend on the past treatment history, x1:t−1 or the time period t.
7Because of focus on being faithful to the ADL setup, we assume that the lagged effects are constant
across levels of the time-varying confounders as is standard in ADL models. One can include interactions
with these variables, though SNMMs then require additional models for Zit. See Robins 
for more details.
Keeping the desired lag length, we could generalize this specification and have an impulse
response that depended on past values of the treatment:
bt(x1:t, j; γ) = γ1jxt−j + γ2jxt−jxt−j−1,
where γ2j captures the interaction between contemporaneous and lagged values of welfare
spending. Note that, given the definition of the impulse response, if xt = 0, then bt = 0
since this would be comparing the average effect of a change from 0 to 0. Choosing this
function is similar to modeling Xi,t−j in a regression—it requires the analyst to decide what
nonlinearities or interactions are important to include for the effect of treatment. If Yit is
not continuous, it is possible to choose an alternative functional form (such as one that uses
a log link) that restricts the effects to the proper scale .
Note that the non-interactive impulse response function in (19) can be seen as an alternative parameterization of the ADL (1,1) in (4). When j = 0 in (19) and an ADL (1,1) model
holds, then the contemporaneous effect of γ0 corresponds to the β1 parameter from the
ADL model. When j = 1 in (19) and an ADL (1,1) model holds, then the impulse response
effect of γ1 corresponds to the αβ1 + β2 combination of parameters from the ADL model.
We derive this connection in more detail below, but one important difference can be seen
in this example. The SNMM approach directly models the impulse response effects while
the ADL model recreates the impulse response effects from all constituent path effects.
The key to the SNMM identification approach is that problems of post-treatment bias
can be avoided by using a transformation of the outcome that leads to easy estimation of
each conditional impulse responses (γj). This transformation is
it = Yit −
bt(Xi,1:t, s),
which, under the modeling assumptions of equation (19), would be
it = Yit −
These transformed outcomes are called the blipped-down or demediated outcomes. For example,
the first blipped-down outcome, which we will use to estimate first lagged effect, subtracts
the contemporaneous effect for each unit off of the outcome, eY1
it = Yit −γ0Xit. Intuitively,
this transformation subtracts off the effect of j lags of treatment, creating an estimate of
the counterfactual level of terrorism at time t if welfare spending had been set to 0 for j
periods before t. Robins and Robins show that, under sequential ignorability,
the transformed outcome, eYj
it, has the same expectation as this counterfactual, Yit(x1:t−j,0j),
conditional on the past. Thus, we can use the relationship between eYj
it and Xi,t−j as an estimate of the j-step lagged effect of treatment, which can be used to create eYj+1
and estimate
the lagged effect for j + 1. This recursive structure of the modeling is what gives SNMM
the “nested” moniker.
We focus on one approach to estimating the parameters called sequential g-estimation in the
biostatistics literature .8 This approach is similar to an extension of the
standard ADL model in the sense that it requires modeling the conditional mean of the
(transformed) outcome to estimate the effect of each lag under study. In particular, for lag
j the researcher must specify a linear regression of eYj
it on the variables in the assumed impulse response function, bt(x1:t, j; γ) and whatever covariates are needed to satisfy sequential
ignorability.
For example, suppose we focused on the contemporaneous effect and the first lagged
effect of welfare spending and we adopted the simple impulse response bt(x1:t, j; γ) = γjxt−j
for both of these effects. As in Section 4, we assume that sequential ignorability held conditional on Vit = {Xi,t−1, Yi,t−1, Zit}. Sequential g-estimation involves the following steps:
1. For j = 0, we would regress the untransformed outcome on {Xit, Xi,t−1, Yi,t−1, Zit}, just
as we would for the ADL model. If the modeling is correctly specified (as we would
assume with the ADL approach), the coefficient on Xit in this regression will provide
an estimate of the blip-down parameter, γ0 (the contemporaneous effect).
2. We would use bγ0 to construct the one-lag blipped-down outcome, eY1
i,t = Yit −bγ0Xit.
3. This blipped-down outcome would be regressed on {Xi,t−1, Xi,t−2, Yi,t−2, Zi,t−1} to estimate the next blip-down parameter, γ1.
If more than two lags are desired, we could use bγ1 to construct the second set of blippeddown outcomes, eY2
i,t−bγ1Xi,t−1, which could then be regressed on {Xi,t−2, Xi,t−3, Yi,t−3, Zi,t−2}
to estimate γ2. This iteration can continue for as many lags as desired. What this approach
avoids is ever estimating a causal effect while including a post-treatment covariate for that
effect. That is, when estimating the effect of welfare spending at lag j, only variables causally
prior to welfare spending at that point are included in the regression. Standard errors for
all of the estimated effects can be estimated using a consistent variance estimator presented
in the Supplemental Materials or via a block bootstrap.
8See Acharya, Blackwell and Sen for an introduction to this method in political science.
This sequential g-estimation approach requires the correct specification of the relationship between the (transformed) outcome and the covariate and treatment histories. It thus
requires a similar regression model to the ADL approach described above. More complicated SNMM estimators can incorporate a model for the treatment process, providing some
robustness to the modeling choices for the outcome. These estimators are consistent for
the parameters of the SNMM when either the model for the (transformed) outcome or the
model for the treatment process is correctly specified. This property is called double robustness because there are “two shots” to achieve consistency. Vansteelandt and Joffe 
provides a review of these methods for SNMMs.
Relationship to the ADL model
As we mentioned in Section 4, the ADL approach and the sequential g-estimation version
of SNMM presented above are very similar when the time-varying covariates, Zit, are not
affected by treatment. One intuition for this result is that the ADL model and the SNMM
with linear model are equivalent when there are no covariates aside from the LDV. To see
this, suppose that the ADL model in (4) is correct and perform the first transformation
from step 2 above, noting, as above, that the contemporaneous effect is the same for both
models γ0 = β1:
Yit −γ0Xit = Yit −β1Xit
= β0 + αYi,t−1 + β2Xi,t−1 + εit
= β0 + α(β0 + αYi,t−2 + β1Xi,t−1 + β2Xi,t−2 + εi,t−1) + β2Xi,t−1 + εit
= (β0 + αβ0) + α2Yi,t−2 + (αβ1 + β2)
Xi,t−1 + αβ2Xi,t−2 + (αεi,t−1 + εit)
From this, we can see that the coefficient on Xi,t−1 for this transformed outcome is simply
the impulse response at lag 1, which is exactly the quantity that the SNMM targets. Given
the ADL and SNMM assumptions above, this quantity will be αβ1 + β2 for the ADL model
and γ1 for the SNMM. Of course, this correspondence will continue for all lagged effects
and Table 1 shows how the two sets of quantities relate for various lags.
Furthermore, in the Supplemental Materials we show that the sequential g-estimation estimator with no covariates except a lagged dependent variable is nearly mechanically equivalent to a traditional ADL estimator with one lag. The difference is that the traditional
ADL model relies on an assumption that the contemporaneous effect is constant over time,
whereas sequential g-estimation relaxes this assumption. This provides an useful interpretation of the ADL model in terms of counterfactual causal effects. It is important to note,
α2β1 + αβ2
α3β1 + α2β2
α4β1 + α3β2
Table 1: The lagged effects, or impulse responses, under the ADL (1,1) in (4) and SNMM in (19).
however, that this equivalence also relies on the form of the ADL model, which uses only
three parameters regardless of the number of lags, while the SNMM in this version uses
a new parameter for every lag. Additionally, the equivalence disappears once there is an
additional time-varying covariate (Zit) in the model.
Marginal structural models
One potential downside of the SNMM approach is that it requires the analyst to correctly
model the relationship between the time-varying covariates and the outcome. This can be
difficult when the outcome is a complicated process and there is little theoretical guidance
for specifying the outcome-covariate relationships. An alternative that relies instead on
modeling the treatment-covariate relationship is called a marginal structural model or MSM
 .9 To specify an MSM, we first choose a potential
outcome lag length to study and write a model for the marginal mean of those potential
outcomes in terms of the treatment history. At the most general, then, an MSM would be
the following:
E[Yit(x1:t)] = g(x1:t;β),
where the function g operates similarly to a link function in a generalized linear model.10
These models are similar to the impulse response functions in the SNMM approach, bt, because they provide structure for the treatment-outcome relationship. For instance, suppose
that we were focused on the contemporaneous effect and the effect of the first two lags
and so we had to model E[Yit(xt−2:t)] = g(xt−2:t;β), marginalizing over further lags and other
covariates. If Yit were continuous, as in the case of the number of terrorist incidents, we
9For a detailed introduction to and application of MSMs in political science, see Blackwell .
10These marginal structural models are similar in spirit to transfer functions the context of pure time-series
data .
might take g to be linear and focus on the additive effects of each period of treatment:11
g(xt−2:t;β) = β0 + β1xt + β2xt−1 + β3xt−2.
If Yit were binary, we might instead assume g to have a logistic form:
g(xt−2:t;β) =
exp(β0 + β1xt + β2xt−1 + β3xt−2)
1 + exp(β0 + β1xt + β2xt−1 + β3xt−2).
In both of these cases, we have restricted our attention to the last three periods of treatment
and so we cannot answer questions about longer-term effects with these models. On the
other hand, as we increase the number of lags under study, the number of parameters needed
to summarize the effects grows and the model can become unwieldy. Thus, we may consider
focusing on the effect of the cumulative number of treated periods, ∑t
s=1 xis. This allows for
the entire history of treatment to affect the outcome in a structured, low-dimensional way.
Under any of these models, the average causal effect becomes:
τ(x1:t, x′
1:t) = g(x1:t;β) −g(x′
Of course, the MSM specification will place restrictions on the average casual effects. A
MSM that is a function of only the cumulative treatment, for instance, implies that τ(x1:t, x′
0 if x1:t and x′
1:t have the same number of treated periods, even if their sequence differs.
How can a researcher estimate an MSM? If one blindly follows model (28) and regresses
Yit on {Xi,t−2, Xi,t−1, Xit} using ordinary least squares, there will be omitted variable bias in
the estimated coefficients. But as we have seen above, simply including time-varying covariates in these models can lead to post-treatment bias. Fortunately, the causal parameters
of these models are estimable using an inverse probability of treatment weighting (IPTW)
approach where we adjust for time-varying covariates using the propensity score weights,
not the outcome model itself, avoiding post-treatment bias . The weighting balances the distribution of the time-varying covariates across
values of the treatments, so that omitting these variables in the reweighted data produces
no omitted variable bias.
To use IPTW, a researcher must develop a model for the probability of treatment in
period t given the variables that satisfy sequential ignorability. For example, suppose that
sequential ignorability holds conditional on some conditioning set Vit. If Xit is binary, then
we must obtain a consistent estimate of πt(v) = Pr[Xit = 1 | Vit = v]. This might be a pooled
11When the treatment is binary and the chosen lag length is short, we can relax the linearity assumption
here by saturating the modeling with all interactions between the periods under study.
logit, a generalized additive model with a flexible functional form, a boosted regression
 , or a covariate-balancing propensity score (CBPS)
model . The IPTW approach requires this model to provide
consistent estimates of the conditional predicted probability of treatment.12 In spite of
this requirement, some methods for propensity score estimation such as CBPS have good
finite-sample properties in the face of model misspecification .
We use the predicted probabilities from this treatment model to construct weights for
each country-year. For example, suppose that Vit included lagged levels of terrorism, Yi,t−1,
lagged welfare spending, Xi,t−1, and a set of time-varying covariates, Zit. Then, for a binary
treatment, we would construct the weights as:
Pr[Xit | Xi,t−1; bγ]
Pr[Xit | Zit, Yi,t−1, Xi,t−1; bα]
The denominator of each term in the product is the predicted probability of observing
unit i’s observed treatment status in time t (Xit), conditional on the covariates that satisfy
sequential ignorability.13 When we multiply this over time, it is the probability of seeing
this unit’s treatment history conditional on the past. The numerators here are the marginal
probability of the observed treatment history and stabilize the weights to make sure they
are not too variable which can lead to poor finite sample performance . For instance, to construct this numerator we might run a pooled logistic regression
of welfare spending in year t on welfare spending in year t −1, omitting any time-varying
covariates or lagged dependent variables. While this choice of numerator is not required for
consistency of the estimator (it can be replaced with 1, for instance), it can help to stabilized
weights that are highly variable and thus increase efficiency.
Under these assumptions, the expectation of Yit conditional on Xi,1:t in the reweighted
data is equal to the MSM:
ESW[Yit|Xi,1:t = x1:t] = E[Yit(x1:t)].
Here ESW[·] is the expectation in the reweighted data. For example, if we used the linear
MSM in (28), then we can estimate the causal parameters of MSM by running a weighted
least squares regression of the outcome, Yit on {Xi,t−2, Xi,t−1, Xit} with d
SWit as the weights.
12This requirement makes it difficult to apply IPTW to fixed-effects settings with binary treatments since
estimating the unit-specific models would face an incidental parameters problem, at least for a fixed time
13To ensure the weights are well-defined, the conditional probability of treatment given the past must be
bounded away from 0 and 1. In the biostatistics literature, this assumption is called positivity and is similar to
the overlap condition in the matching literature.
If sequential ignorability holds, the coefficients on the components of Xi,1:t from this regression will have a causal interpretation, though they may depend on the particular modeling
choices of the MSM . Standard errors can be estimated via a block bootstrap of units. Note that, unlike the ADL and SNMMs, this approach
does not require a model for the relationship between the time-varying covariates and the
Finally, when the conditional probability of treatment is close to 0 or 1, the IPTW approach can have large and unstable weights, leading to high variance and sometimes small
sample biases . SNMMs, on the other hand, tend to be more
stable in this setting. And while MSMs and SNMMs can accommodate general types of
covariates, SNMMs also tend to be more stable when the treatment is continuous since
weighting by a continuous density (as would be required with IPTW) is sensitive to small
perturbations in the data .
Modeling checklist
In this section, we review the key modeling choices required to implement these methods.
Causal lag length:
First, one must choose the lag length to study. At the most general,
one can investigate the effect of an entire treatment history, but these are usually too highly
dimensional to study without further assumptions. In MSMs, one can reduce this dimensionality by assuming that treatment history only affects the outcome through the average
level of treatment or the cumulative amount of treatment up to time t. Alternatively, a
researcher can focus on the marginal effects of the last j lags of treatment.
Conditioning set for sequential ignorability:
Separate from the question of what to study,
is the question of what covariates to choose so that the question can be answered. Sequential
ignorability is an assumption about conditional independence: welfare spending is independent of the potential outcomes conditional on past treatment and some set of baseline and
time-varying covariates. Thus, scholars must choose a set of covariates for each time period
that blocks all confounding for the treatment-outcome relationship—that is, there must be
no omitted variables after controlling for that conditioning set. In the context of welfare
spending, these covariates might include: lagged welfare spending (Xi,t−1) and the lagged
terrorist activity (Yi,t−1), time-varying economic factors like GDP growth unemployment
(Zit), and baseline characteristics such as region of the world (Zi0). This conditioning set of
variables will be included in the models for the outcome in the SNMM approach or the in
the models for the treatment in the MSM approach.
Modeling treatment or outcome:
In all of the methods described in this paper, the analyst
must specify the functional form of how the treatment history and outcome relate. In
the SNMM approach, this is done through the IRF or blip-down functions, while in the
MSM, this is done through the specification of the MSM itself. To actually estimate these
models, however, a researcher must additionally model either the relationship between the
outcome and the covariates in the conditioning set (in the ADL or SNMM approaches) or
the relationship between the treatment and the covariates in the conditioning set (in the
MSM approach). Because the quality of the casual estimates depends on this modeling, we
encourage researchers to choose the approach for which more substantive knowledge can
be mustered to help with the modeling task. For example, suppose we were estimating the
effect of central bank interest rate changes on support for incumbent candidates. It may
be easier to model the central bank interest rate changes if we have detailed information
on central bank deliberations about changes that help us specify the model. In other cases,
there may be more substantive information about the outcome model.
Functional form assumptions:
Finally, in either model that is chosen, the analyst must
correctly specify the model in the sense that the functional form assumed for the variables
in the conditioning set is correct. This may require, for instance, taking the natural log of
population, including a squared term for GDP growth to allow for a nonlinear relationship,
or including an interaction between two important covariates. This task is common to all
modeling strategies and is not unique to the current setting. A researcher can weaken these
modeling assumptions by replacing linear or generalized linear models with generalized additive models that allow the functional forms of chosen covariates to be estimated along
with the other model parameters .14 Finally, we note that all of
these models, including the ADL, assume the correct periodization and causal structure in
the data. For instance, we must know that Zit is in fact causally prior to Xit even though they
could be measured in the same time period. Thus, a significant amount of subject-matter
expertise may be required to ensure these specifications are correct. For the purposes of
our discussion, however, these issues are common to all TSCS methods and will not affect
the comparison between methods.
14A growing literature has developed several approaches to flexibly estimating linear (and sometimes generalized linear) models that would reduce the modeling burden on the researcher even further. These models
include sparse additive models , kernal regularized least squares , and generalized boosted models .
Figure 4: Direct acyclic graph of the simulation study. Dotted red line represents the key causal
quantity varied in the simulations, whether past treatment affects future covariates. Dashed lines
represent unmeasured confounding.
Simulation evidence
To investigate the small sample properties of the various estimators, we conducted a simulation study of a TSCS setting with a treatment, an outcome, and a single covariate, all
time-varying. We describe the simulation in more detail in the Supplemental Materials, but
the main causal relationships in the design are displayed in Figure 4. Here, the treatment
history only has a contemporaneous effects— lagged treatments, Xi,1:t−1, and outcomes,
Yi,1:t−1, have no direct or indirect effect on current outcomes, Yit, that don’t go through
current treatment, Xit. The treatment-outcome relationship is confounded due to a timeconstant unmeasured confounder, Ui, but conditioning on {Yi,t−1, Zit} can block this confounding and ensure sequential ignorability for Xit. Finally, the distribution of {Yit, Xit, Zit}
is Markovian and stationary within each unit, which should be an ideal setting for the ADL
To show how the causal structure can affect the performance of the estimators, we
consider two scenarios that vary the feedback between the treatment and the time-varying
covariate. In the first, we allow for lagged treatment to affect future covariates so that
Xi,t−1 →Zit, and in the second, we close this path. These two scenarios represent when
the time-varying counfounder, Zit, is post-treatment to lagged treatment and when it is not.
Unfortunately, when Zit is post-treatment, conditioning on it will induce post-treatment bias
for the effect of lagged treatment, because conditioning will open a backdoor path from Zit
through Ui to Yit. However, we must condition on Zit to remove the omitted variable bias
for contemporaneous treatment. This is the dilemma that traditional TSCS models like the
ADL model cannot solve, because a single model cannot simultaneously control for Zit and
not control for Zit.
We generate data from this model varying numbers of time periods and units and focus
on the lagged effect of treatment, E[Yit(1,0)−Yit(0,0)], which in this case is 0. We compared
several methods for estimating this quantity: (1) an ADL as in Section 4 with estimate
bαbβ1 + bβ2; (2) an SNMM sequential g-estimation with additive linear models for the outcome
for each lag; (3) a linear, additive MSM with g(xt−1, xt;β) = β0 + β1xt + β2xt−1; and (4)
a raw model with no controls that only includes Xit and Xi,t−1.15 For reference, we also
compare these estimators to the infeasible estimator that simply takes the sample average
of Yit(0,1) −Yit(0,0) across all unit-periods.
Figure 5 shows the results of these simulation. The left column shows the root mean
squared error (RMSE) of the various estimators when the time-varying confounder, Zit, is
affected by past treatment. While the SNMM and MSM approaches have roughly similar
estimation error across different sample sizes, they vastly outperform the ADL approach.
The high RMSE of the ADL approach that persists across sample sizes is due to a large
degree of post-treatment bias on the coefficient on Xi,t−1 due to conditioning on Zit. This
bias propagates to the ADL computation of the total effect of lagged treatment. The ADL
model even performs worse that a model that has significant omitted variable bias due to
excluding all time-varying covariates from the model (labelled “Raw” in the figure). These
results hold even though the DGP here is stationary and the sample size and the number
of time periods are small and similar in size, meaning that they are unlikely to depend on a
“large-N, small-T” setting.
The right column of Figure 5 shows the results when the time-varying confounder is
not affected by treatment. Here, the ADL has lower estimation error than any of the other
methods, slightly beating out SNMM. The ADL model performs well in this setting since
the lagged dependent variable is the only variable affected by past treatment. As we show in
the Supplemental Materials, in this case the ADL model is essentially a correctly specified
SNMM. This correct specification breaks down when time-varying covariates are affected by
treatment. Given the robustness of SNMM to this feature of the casual process and given
the similarity in modeling choices for the SNMM and ADL approaches, we recommend
using the SNMM as a working replacement for the ADL model whenever lagged effects are
of interest.16
Empirical illustration: Welfare spending and terrorism
Burgoon studied the effect of domestic welfare spending on terrorist activity within
15For each of these approaches except the last, we include the relevant covariates, correctly specified in
terms of their functional form. In the Supplemental Materials, we weaken use misspecified functional forms
for all models and the results are qualitatively similar.
16The ADL approach is also biased when omitting Zit, but including Yi,t−1 (results not reported here).
There is no permutation of controls that eliminate the bias of ADL when Zit is affected by Xi,t−1.
Z endogeneous (T = 20)
Sample Size
Z exogeneous (T = 20)
Sample Size
Z endogeneous (T = 50)
Sample Size
Z exogeneous (T = 50)
Sample Size
SNMM (t-1)
Truth (t-1)
Figure 5: Simulation results when the time-varying confounder is post-treatment (left column) and
when the time-varying confounder is not post-treatment (right column). Points represent the root
mean squared error (RMSE) of each estimator for the lagged effect of treatment.
countries and used TSCS data to show that increasing spending leads to lower levels of
terrorist activity within a country. But how does the timing of this spending matter? Can
we assess the effects of lagged government spending on future values of terrorist activity?
We apply the models of this paper to show how they differ from traditional approaches to
answering these questions.
To do this, we closely follow the specification of Burgoon . The dependent variable is the number of transnational terrorist incidents occurring in a country, omitting purely
domestic terrorism such as the Oklahoma City bombing in the United States. Burgoon
 uses a negative binomial regression model to estimate the effect of contemporaneous spending, whereas we use a linear model. To account for overdispersion, we use the
square root of the number of transnational terrorist incidents as our dependent variable.
This approach recovers very similar substantive results as that of Burgoon .
A first step for any of the methods we describe in the paper is to choose a conditioning
set of covariates that can satisfy sequential ignorability. Given that Burgoon interprets the effect of spending in a causal fashion, we follow this selection-on-observables
approach and assume that the control variables in the paper’s models are sufficient to satisfy sequential ignorability. These include a set of regional and year dummies as baseline
covariates and the following time-varying covariates: a lagged dependent variable, left-party
control of government, Polity score and its lag, log population, a measure of government
capability, whether the country is in a conflict, and the amount of trade logged. In this context, the sequential ignorability assumption states that welfare spending is exogeneous with
respect to terrorism conditional on previous terrorist incidents, the time-varying covariates,
and region and year fixed effects. Note that if there were unmeasured confounding beyond
these controls, the estimates of causal effects in this application could be biased. One could,
however, perform a sensitivity analysis to determine how much of the estimated effect disappears under various departures from sequential ignorability .
To begin, we compare how the ADL and the SNMM approaches differ in terms of their
estimates in this context. For the SNMM, we assume each lag has a simple additive effect as
in (19), γjxt−j, with no interactions between treatment and lagged treatment. For the ADL
model, we use the specification described above while including a lag of treatment in the
model to allow for some flexibility in the lag structure. We use the formulas for calculating
lagged effects from an ADL model, as described in Section 2.3. This ADL regression is also
the first-stage regression for our sequential g-estimation approach, since under sequential
ignorability, it can estimate the contemporaneous effect of treatment. Note that the functional form assumptions for the ADL and the SNMM approach are the same, so that any
differences between these methods are likely due to the above biases of the ADL model. We
Impulse Response Function
Lag of Effect
Effect of Govt Spending on Terrorist Incidents
Step Response Function
Lag of Effect
Cumulative Effect of Gov't Spending
Figure 6: Left: Estimated effect of government spending on the terrorist incidents at various lags,
along with 95% confidence intervals. Right: implied step response function at various lags. Data
from Burgoon .
focus on a lag length of four years for comparing the SNMM and ADL approaches. Finally,
we use the consistent and cluster-robust variance estimator in the Supplemental Materials
for the SNMM and a standard cluster-robust variance estimator for the ADL.
Figure 6 shows the estimated contemporaneous and lagged effect of welfare spending
on terrorist activity. For instance, one-year lag has bγ1 for the SNMM, estimated from a
regression of the blipped-down outcome on lagged treatment and its conditioning set, and
bαbβ1 + bβ2 for the ADL approach. The two approaches are equivalent for the contemporaneous effect but differ in their estimates of the lagged effect. Both methods show a significant
and negative effect on lagged spending, but the coefficient from the SNMM approach is
about 60% larger in magnitude than the ADL approach. These differences continue with
the lags—the effect of the second and third lags are 60% greater in the ADL approach,
whereas the effect of the fourth lag is almost double the magnitude for SNMMs. These
differences lead to large differences in the estimated cumulative effect of the step response
function at the end of four years, with the SNMM estimate almost 40% larger in magnitude.
Why do these differences between the SNMM and the ADL occur? Differences in
assumptions about functional form of the covariates are ruled out since the SNMM and
ADL models handle these covariates in the exact same way. Furthermore, each of the two
approaches rely on a similar assumption about no unmeasured confounding. We believe
that the difference between these two approaches is in the post-treatment bias induced by
conditioning on the time-varying controls in the ADL approach. In this case, it is highly
unlikely that the time-varying covariates are exogenous to welfare spending. For example,
one time-varying covariate is the proportion of the government held by left-wing parties.
It would be unreasonable to assume that past values of welfare spending are unrelated to
future electoral prospects of leftist parties, as would have to be the case for the ADL model
to be correct in this case. Indeed, if we regress proportion of the government controlled
by leftist parties on lagged welfare spending and the conditioning set for lagged spending,
there is a statistically significant and positive coefficient on lagged welfare spending. Thus,
it does appear that post-treatment bias could loom large in the estimated effects of the ADL
In the above analysis, we focused on a lag length of 4, even though the data run from
1978 until 1995. Can we learn more about the effects of the history of welfare spending on
terrorism? To do this, we turn to marginal structural models where we can develop models
that summarize the effects of the entire treatment history in low dimensions. We have seen
that lagged welfare spending appears to have an effect and so we may want to know if
having a long history of spending also decreases incidences of terrorism. To implement
this, we first create a binary measure of welfare spending, X∗
it, that is 1 if the country-year
had spending (as a function of its GDP) above the global average and 0 if the spending was
below the average. We then specify the following MSM:
i,1:t)] = β0 + β1x∗
Here, the mean of the potential outcomes is a function of the contemporaneous level of
spending and the number of lagged periods that have above-average spending. We focus
on this simple model, though it is possible to include further lags or interactions between
different parts of the history.
We use three approaches to estimating the parameters of the MSM. First, we take the
standard ADL-like approach of including the entire set of baseline and time-varying covariates in a regression model. Second, we run the same regression with only the baseline
covariates. As we have discussed above, the first of these approaches is likely to produce
post-treatment bias and the second is likely to produce omitted variable bias. We compare these to a third approach that uses the IPTW method described above. To create the
weights, we fit a logistic regression of the binary treatment on the first two lags of treatment,
the cumulative sum of treatment through t−3, and the baseline and time-varying covariates
described above. We use predicted probabilities from this model to create the weights as
Effect of Cumulative Lagged Welfare Spending on Terrorism
(a) Control for TVCs
(b) Omit TVCs
Figure 7: Estimated effect of the cumulative number of high welfare spending years through t −1 on
terrorism incidents in year t, fixing welfare spending in year t. The three approaches are (a) when
controlling for time-varying covariates, (b) when omitting those variables, and (c) using IPTW. Lines
are 95% confidence intervals based on a block bootstrap with 1,000 replications.
in (31), which we use a in WLS regression of the above MSM. We use a block bootstrap
to estimate standard errors and trim the weights at 10 to help guard against highly unstable
weights .
Figure 7 shows the results of these models. Both of the traditional approaches estimate
a relatively small negative effect of lagged welfare spending on terrorism. The IPTW approach, on the other hand, shows a much larger negative and statistically significant impact,
which is consistent with the results of the analysis from both the SNMM and ADL models
above. It is interesting to note that the implied post-treatment and omitted variable biases in
the first and second models, respectively, are in the same direction. This agreement tempts
us to confirm the approximate validity of their results; after all, a natural intuition would be
that the true effect must be between these two estimates. Unfortunately, this intuition, while
natural, is incorrect. The biases of both approaches can be in the same direction, negating
their usefulness as bounds . Finally, we note that the rather large increase
the standard errors in the IPTW approach is driven in part by large weights due to predicted
probabilities being close to 0 or 1. This can happen with slowly-changing treatments and is
one reason to prefer an SNMM approach in this setting.
Conclusions, drawbacks, and future research
Repeated measurements over time of countries, people, or governments expand the scope
of causal inference methods. TSCS data allow us to estimate both contemporaneous effects and the effects of more distant lags of treatment. But with an expanded scope comes
complications. The usual TSCS regression methods break down for lagged effects. Nevertheless, we have shown that two approaches developed in biostatistics can overcome these
difficulties and recover effect estimates across a wide variety of settings.
Both SNMMs and MSMs have their own drawbacks, of course. Even though sequential ignorability nonparametrically identifies any average causal effect of a treatment history,
both approaches will almost always depend on modeling to estimate these effects since the
covariates needed to justify such an assumption will be highly dimensional. While these
modeling assumptions can be weakened to some extent through generalized additive models or other semiparametric techniques, there will always be some degree of model dependence that follows from these approaches. Another problem is that sequential ignorability
is a strong, untestable assumption that might be violated. One approach to mitigating this
problem is to conduct a formal sensitivity analysis using the methods of either Blackwell
 or relying the bias formulas presented in Acharya, Blackwell and Sen . These
sensitivity analyses can give researchers a sense of how reliant their results are on sequential
ignorability holding.
In this paper, we focused on the usual sequential ignorability assumption as commonly
invoked in epidemiology. Many TSCS applications in political science rely on a “fixed effects” assumption that there is time-constant, unmeasured heterogeneity in units. Linear
models can easily handle these types of assumptions, though nonlinear fixed effects models
pose greater difficulties. Estimating the above causal quantities with these models, however, remains elusive except under strong assumptions like baseline randomization . A valuable direction for future work would be to develop fixed effects methods that could estimate causal effects under a within-unit version
of sequential ignorability.