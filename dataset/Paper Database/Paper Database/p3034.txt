IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
A Comprehensive Survey of Graph Embedding:
Problems, Techniques and Applications
Hongyun Cai, Vincent W. Zheng, and Kevin Chen-Chuan Chang
Abstract—Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph
analytics provides users a deeper understanding of what is behind the data, and thus can beneﬁt a lot of useful applications such as
node classiﬁcation, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation
and space cost. Graph embedding is an effective yet efﬁcient way to solve the graph analytics problem. It converts the graph data into
a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we
conduct a comprehensive review of the literature in graph embedding. We ﬁrst introduce the formal deﬁnition of graph embedding as
well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in
different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we
summarize the applications that graph embedding enables and suggest four promising future research directions in terms of
computation efﬁciency, problem settings, techniques and application scenarios.
Index Terms—Graph embedding, graph analytics, graph embedding survey, network embedding
INTRODUCTION
RAPHS naturally exist in a wide diversity of realworld scenarios, e.g., social graph/diffusion graph in
social media networks, citation graph in research areas,
user interest graph in electronic commerce area, knowledge graph etc. Analysing these graphs provides insights
into how to make good use of the information hidden
in graphs, and thus has received signiﬁcant attention in
the last few decades. Effective graph analytics can beneﬁt a lot of applications, such as node classiﬁcation ,
node clustering , node retrieval/recommendation , link
prediction , etc. For example, by analysing the graph
constructed based on user interactions in a social network
(e.g., retweet/comment/follow in Twitter), we can classify
users, detect communities, recommend friends, and predict
whether an interaction will happen between two users.
Although graph analytics is practical and essential, most
existing graph analytics methods suffer the high computation and space cost. A lot of research efforts have been
devoted to conducting the expensive graph analytics efﬁciently. Examples include the distributed graph data processing framework (e.g., GraphX , GraphLab ), new
space-efﬁcient graph storage which accelerate the I/O and
computation cost , and so on.
In addition to the above strategies, graph embedding
provides an effective yet efﬁcient way to solve the graph
analytics problem. Speciﬁcally, graph embedding converts
a graph into a low dimensional space in which the graph
information is preserved. By representing a graph as a
(or a set of) low dimensional vector(s), graph algorithms
H. Cai is with Advanced Digital Sciences Center, Singapore. E-mail:
 .
V. Zheng is with Advanced Digital Sciences Center, Singapore. Email:
 .
K. Chang is with University of Illinois at Urbana-Champaign, USA.
Email: .
can then be computed efﬁciently. There are different types
of graphs (e.g., homogeneous graph, heterogeneous graph,
attribute graph, etc), so the input of graph embedding
varies in different scenarios. The output of graph embedding is a low-dimensional vector representing a part of
the graph (or a whole graph). Fig. 1 shows a toy example of embedding a graph into a 2D space in different
granularities. I.e., according to different needs, we may
represent a node/edge/substructure/whole-graph as a lowdimensional vector. More details about different types of
graph embedding input and output are provided in Sec. 3.
In the early 2000s, graph embedding algorithms were
mainly designed to reduce the high dimensionality of the
non-relational data by assuming the data lie in a low
dimensional manifold. Given a set of non-relational highdimensional data features, a similarity graph is constructed
based on the pairwise feature similarity. Then, each node
in the graph is embedded into a low-dimensional space
where connected nodes are closer to each other. Examples
of this line of researches are introduced in Sec. 4.1. Since
2010, with the proliferation of graph in various ﬁelds,
research in graph embedding started to take a graph as
the input and leverage the auxiliary information (if any)
to facilitate the embedding. On the one hand, some of
them focus on representing a part of the graph (e.g., node,
edge, substructure) (Figs. 1(b)-1(d)) as one vector. To obtain
such embedding, they either adopt the state-of-the-art deep
learning techniques (Sec. 4.2) or design an objective function
to optimize the edge reconstruction probability (Sec. 4.3).
On the other hand, there is also some work concentrating
on embedding the whole graph as one vector (Fig. 1(e)) for
graph level applications. Graph kernels (Sec. 4.4) are usually
designed to meet this need.
The problem of graph embedding is related to two
traditional research problems, i.e., graph analytics and
representation learning . Particularly, graph embedding
 
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
(a) Input Graph G1
0.0 1.5 3.0
(b) Node Embedding
0.0 1.5 3.0
(c) Edge Embedding
0.0 1.5 3.0
(d) Substructure Embedding
0.0 1.5 3.0
(e) Whole-Graph Embedding
Fig. 1. A toy example of embedding a graph into 2D space with different granularities. G{1,2,3} denotes the substructure containing node v1, v2, v3.
aims to represent a graph as low dimensional vectors while
the graph structures are preserved. On the one hand, graph
analytics aims to mine useful information from graph data.
On the other hand, representation learning obtains data
representations that make it easier to extract useful information when building classiﬁers or other predictors . Graph
embedding lies in the overlap of the two problems and
focuses on learning the low-dimensional representations.
Note that we distinguish graph representation learning
and graph embedding in this survey. Graph representation
learning does not require the learned representations to be
low dimensional. For example, represents each node as
a vector with dimensionality equals to the number of nodes
in the input graph. Every dimension denotes the geodesic
distance of a node to each other node in the graph.
Embedding graphs into low dimensional spaces is not
a trivial task. The challenges of graph embedding depend
on the problem setting, which consists of embedding input
and embedding output. In this survey, we divide the input
graph into four categories, including homogeneous graph,
heterogeneous graph, graph with auxiliary information and graph
constructed from non-relational data. Different types of embedding input carry different information to be preserved
in the embedded space and thus pose different challenges
to the problem of graph embedding. For example, when
embedding a graph with structural information only, the
connections between nodes are the target to be preserved.
However, for a graph with node label or attribute information, the auxiliary information provides graph property
from other perspectives, and thus may also be considered
during the embedding. Unlike embedding input which is
given and ﬁxed, the embedding output is task driven. For
example, the most common type of embedding output is
node embedding which represents close nodes as similar
vectors. Node embedding can beneﬁt node related tasks
such as node classiﬁcation, node clustering, etc. However, in
some cases, the tasks may be related to higher granularity
of a graph e.g., node pairs, subgraph, whole graph. Hence,
the ﬁrst challenge in terms of embedding output is to ﬁnd a
suitable embedding output type for the application of interest. We categorize four types of graph embedding output,
including node embedding, edge embedding, hybrid embedding
and whole-graph embedding. Different output granularities
have different criteria for a “good” embedding and face
different challenges. For example, a good node embedding
preserves the similarity to its neighbouring nodes in the
embedded space. In contrast, a good whole-graph embedding
represents a whole graph as a vector so that the graph-level
similarity is preserved.
In observations of the challenges faced in different problem settings, we propose two taxonomies of graph embedding work, by categorizing graph embedding literature
based on the problem settings and the embedding techniques. These two taxonomies correspond to what challenges exist in graph embedding and how existing studies
address these challenges. In particular, we ﬁrst introduce
different settings of graph embedding problem as well as
the challenges faced in each setting. Then we describe how
existing studies address these challenges in their work,
including their insights and their technical solutions.
Note that although a few attempts have been made to
survey graph embedding ( , , ), they have the following two limitations. First, they usually propose only one
taxonomy of graph embedding techniques. None of them
analyzed graph embedding work from the perspective of
problem setting, nor did they summarize the challenges in
each setting. Second, only a limited number of related work
are covered in existing graph embedding surveys. E.g., 
mainly introduces twelve representative graph embedding
algorithms, and focuses on knowledge graph embedding only. Moreover, there is no analysis on the insight
behind each graph embedding technique. A comprehensive
review of existing graph embedding work and a high level
abstraction of the insight for each embedding technique can
foster the future researches in the ﬁeld.
Our Contributions
Below, we summarize our major contributions in this survey.
• We propose a taxonomy of graph embedding based on
problem settings and summarize the challenges faced in
each setting. We are the ﬁrst to categorize graph embedding
work based on problem setting, which brings new perspectives to understanding existing work.
• We provide a detailed analysis of graph embedding techniques. Compared to existing graph embedding surveys,
we not only investigate a more comprehensive set of graph
embedding work, but also present a summary of the insights
behind each technique. In contrast to simply listing how the
graph embedding was solved in the past, the summarized
insights answer the questions of why the graph embedding
can be solved in a certain way. This can serve as an insightful
guideline for future research.
• We systematically categorize the applications that graph
embedding enables and divide the applications as node
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
Graph Embedding Problem Settings
Graph Embedding Input
Graph Embedding Output
Homogeneous Graph
Heterogeneous Graph
Graph with Auxiliary Information
Graph Constructed from Non-relational Data
Node Embedding
Edge Embedding
Hybrid Embedding
Whole-Graph Embedding
Graph Embedding Techniques
Matrix Factorization
Deep Learning
Graph Laplacian Eigenmaps
Node Proximity Matrix Factorization
Maximize Edge Reconstruct Probability
Minimize Distance-based Loss
With Random Walk
Without Random Walk
Minimize Margin-based Ranking Loss
Based on Graphlet
Edge Reconstruction
Graph Kernel
Generative Model
Based on Subtree Patterns
Based on Random Walks
Embed Graph into Latent Space
Incorporate Semantics for Embedding
Fig. 2. Graph embedding taxonomies by problems and techniques.
related, edge related and graph related. For each category,
we present detailed application scenarios as the reference.
• We suggest four promising future research directions in
the ﬁeld of graph embedding in terms of computational ef-
ﬁciency, problem settings, solution techniques and applications. For each direction, we provide a thorough analysis of
its disadvantages (deﬁciency) in current work and propose
future research direction(s).
Organization of The Survey
The rest of this survey is organized as follows. In Sec. 2,
we introduce the deﬁnitions of the basic concepts required
to understand the graph embedding problem, and then
provide a formal problem deﬁnition of graph embedding. In
the next two sections, we provide two taxonomies of graph
embedding, where the taxonomy structures are illustrated
in Fig. 2. Sec. 3 compares the related work based on the
problem settings and summarizes the challenges faced in
each setting. In Sec. 4, we categorize the literature based on
the embedding techniques.The insights behind each technique are abstracted, and a detailed comparison of different
techniques is provided at the end. After that, we present
the applications that graph embedding enables in Sec. 5. We
then discuss four potential future research directions in Sec.
6 and concludes this survey in Sec. 7.
PROBLEM FORMALIZATION
In this section, we ﬁrst introduce the deﬁnition of the basic
concepts in graph embedding, and then provide a formal
deﬁnition of the graph embedding problem.
Notation and Deﬁnition
The detailed descriptions of the notations used in this survey can be found in Table 1.
Deﬁnition 1. A graph is G = (V, E), where v ∈V is a
node and e ∈E is an edge. G is associated with a node
type mapping function fv : V →T v and an edge type
mapping function fe : E →T e.
T v and T e denote the set of node types and edge types,
respectively. Each node vi ∈V belongs to one particular
type, i.e., fv(vi) ∈T v. Similarly, for eij ∈E, fe(eij) ∈T e.
Notations used in this paper.
Descriptions
The cardinality of a set
G = (V, E)
Graph G with nodes set V and edges set E
ˆG = ( ˆV , ˆ
A substructure of graph G, where ˆV ⊆V, ˆ
A node vi ∈V and an edge eij ∈E connecting vi and vj
The adjacent matrix of G
The i-th row vector of matrix A
The i-th row and j-th column in matrix A
fv(vi), fe(eij) Type of node vi and type of edge eij
The node type set and edge type set
The k nearest neighbours of node vi
X ∈R|V |×N
A feature matrix, each row Xi is a N-dimensional vector for vi
yi, yij, y ˆ
The embedding of node vi, edge eij, and structure ˆG
The dimensionality of the embedding
< h, r, t >
A knowledge graph triplet, with head entity h,
tail entity t and the relation between them r
First- and second-order proximity between node vi and vj
An information cascade
Gc = (V c, Ec) A cascade graph which adopts the cascade c
isSupervisorOf
isFriendOf
Fig. 3. A toy example of knowledge graph.
Deﬁnition 2. A homogeneous graph Ghomo = (V, E) is a
graph in which |T v| = |T e| = 1. All nodes in G belong
to a single type and all edges belong to one single type.
Deﬁnition 3. A heterogeneous graph Ghete = (V, E) is a
graph in which |T v| > 1 and/or |T e| > 1.
Deﬁnition 4. A knowledge graph Gknow = (V, E) is a
directed graph whose nodes are entities and edges are
subject-property-object triple facts. Each edge of the form
(head entity, relation, tail entity) (denoted as < h, r, t >)
indicates a relationship of r from entity h to entity t.
h, t ∈V are entities and r ∈E is the relation. In this survey,
we call < h, r, t > a knowledge graph triplet. For example,
in Fig. 3, there are two triplets: < Alice, isFriendOf, Bob >
and < Bob, isSupervisorOf, Chris >. Note that the entities and relations in a knowledge graph are usually of
different types , . Hence, knowledge graph can be
viewed as an instance of the heterogeneous graph.
The following proximity measures are usually adopted
to quantify the graph property to be preserved in the embedded space. The ﬁrst-order proximity is the local pairwise
similarity between only the nodes connected by edges. It
compares the direct connection strength between a node
pair. Formally,
Deﬁnition 5. The ﬁrst-order proximity between node vi and
node vj is the weight of the edge eij, i.e., Ai,j.
Two nodes are more similar if they are connected by an
edge with larger weight. Denote the ﬁrst-order proximity
between node vi and vj as s(1)
ij , we have s(1)
= Ai,j. Let
i2 , · · · , s(1)
i|V |] denote the ﬁrst-order proximity
between vi and other nodes. Take the graph in Fig. 1(a) as an
example, the ﬁrst order between v1 and v2 is the weight of
edge e12, denoted as s(1)
12 = 1.2. And s(1)
records the weight
of edges connecting v1 and other nodes in the graph, i.e.,
= [0, 1.2, 1.5, 0, 0, 0, 0, 0, 0].
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
The second-order proximity compares the similarity of
the nodes’ neighbourhood structures. The more similar two
nodes’ neighbourhoods are, the larger the second-order
proximity value between them. Formally,
Deﬁnition 6. The second-order proximity s(2)
ij between node
vi and vj is a similarity between vi’s neighbourhood s(1)
and vj’s neighborhood s(1)
Again, take Fig. 1(a) as an example: s(2)
similarity between s(1)
2 . As introduced before,
= [0, 1.2, 1.5, 0, 0, 0, 0, 0, 0] and s(1)
= [1.2, 0, 0.8, 0, 0,
0, 0, 0, 0].
similarities
cosine(s(1)
2 ) = 0.43 and s(2)
15 = cosine(s(1)
We can see that the second-order proximity between v1 and
v5 equals to zero as v1 and v5 do not share any common
1-hop neighbour. v1 and v2 share a common neighbour v3,
hence their second-order proximity s(2)
12 is larger than zero.
The higher-order proximity can be deﬁned likewise. For
example, the k-th-order proximity between node vi and
vj is the similarity between s(k−1)
and s(k−1)
. Note that
sometimes the higher-order proximities are also deﬁned using some other metrics, e.g., Katz Index, Rooted PageRank,
Adamic Adar, etc .
It is worth noting that, in some work, the ﬁrst-order and
second-order proximities are empirically calculated based
on the joint probability and conditional probability of two
nodes. More details are discussed in Sect. 4.3.2.
Problem 1. Graph embedding: Given the input of a graph G
= (V, E), and a predeﬁned dimensionality of the embedding d (d ≪|V |), the problem of graph embedding is to
convert G into a d-dimensional space, in which the graph
property is preserved as much as possible. The graph
property can be quantiﬁed using proximity measures
such as the ﬁrst- and higher-order proximity. Each graph
is represented as either a d-dimensional vector (for a
whole graph) or a set of d-dimensional vectors with each
vector representing the embedding of part of the graph
(e.g., node, edge, substructure).
Fig. 1 shows a toy example of graph embedding with d =
2. Given an input graph (Fig. 1(a)), the graph embedding
algorithms are applied to convert a node (Fig. 1(b))/ edge
(Fig. 1(c)), substructure (Fig. 1(d))/ whole-graph (Fig. 1(e))
as a 2D vector (i.e., a point in a 2D space). In the next two
sections, we provide two taxonomies of graph embedding,
by categorizing the graph embedding literature based on
problem settings and embedding techniques respectively.
PROBLEM SETTINGS OF GRAPH EMBEDDING
In this section, we compare existing graph embedding work
from the perspective of problem setting, which consists of
the embedding input and the embedding output. For each
setting, we ﬁrst introduce different types of graph embedding input or output, and then summarize the challenges
faced in each setting at the end.
We start with graph embedding input. As a graph embedding setting consists of both input and output, we use
node embedding as an example embedding output setting
during the introduction of different types of input. The
(a) A weighted graph.
(b) A directed graph.
Fig. 4. Examples of weighted and directed graphs.
reason is that although there exist various types of embedding output, the majority of graph embedding studies
focus on node embedding, i.e., embedding nodes to a low
dimensional space where the node similarity in the input
graph is preserved. More details about node embedding and
other types of embedding output are presented in Sec. 3.2.
Graph Embedding Input
The input of graph embedding is a graph. In this survey, we
divide graph embedding input into four categories: homogeneous graph, heterogeneous graph, graph with auxiliary
information and constructed graph. Each type of graph
poses different challenges to graph embedding. Next, we
introduce these four types of input graphs and summarize
the challenges faced in each input setting.
Homogeneous Graph
The ﬁrst category of input graph is the homogeneous graph
(Def. 2), in which both nodes and edges belong to a single
type respectively. The homogeneous graph can be further
categorized as the weighted (or directed) and unweighted
(or undirected) graph as the example shown in Fig. 4.
Undirected and unweighted homogeneous graph is the
most basic graph embedding input setting. A number of
studies work under this setting, e.g., , , , , .
They treat all nodes and edges equally, as only the basic
structural information of the input graph is available.
Intuitively, the weights and directions of the edges provide more information about the graph, and may help
represent the graph more accurately in the embedded space.
For example, in Fig. 4(a), v1 should be embedded closer to
v2 than v3 because the weight of the edge e1,2 is higher.
Similarly, v4 in Fig. 4(b) should be embedded closer to v5
than v6 as v4 and v5 are connected in both direction. The
above information is lost in the unweighted and undirected
graph. Noticing the advantages of exploiting the weight and
direction property of the graph edges, the graph embedding community starts to explore the weighted and/or the
directed graph. Some of them focus on only one graph property, i.e., either edge weight or edge direction. On the one
hand, the weighted graph is considered in , , ,
 , , . Nodes connected by higher-weighted edges
are embedded closer to each other. However, their work
is still limited to undirected graphs. On the other hand,
some work distinguishes directions of the edges during the
embedding process and preserve the direction information
in the embedded space. One example of the directed graph
is the social network graph, e.g, . Each user has both
followership and followeeship with other users. However,
the weight information is unavailable for the social user
links. Recently, a more general graph embedding algorithm
is proposed, in which both weight and direction properties
are considered. In other words, these algorithms (e.g., ,
 , ) can process both directed and undirected, as well
as weighted and unweighted graph.
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
Graph Embedding Algorithms for cQA sites
GE Algorithm Links Exploited
user-user, user-question
user-user, user-question, question-answer
user-user, question-answer, user-answer
users’ asymmetric following links, a ordered tuple (i, j, k, o, p)
Challenge: How to capture the diversity of connectivity patterns
observed in graphs? Since only structural information is available in homogeneous graphs, the challenge of homogeneous
graph embedding lies in how to preserve these connectivity
patterns observed in the input graphs during embedding.
Heterogeneous Graph
The second category of input is the heterogeneous graph
(Def. 3), which mainly exist in the three scenarios below.
Community-based Question Answering (cQA) sites.
cQA is an Internet-based crowdsourcing service that enables
users to post questions on a website, which are then answered by other users . Intuitively, there are different
types of nodes in a cQA graph, e.g., question, answer, user.
Existing cQA graph embedding methods distinguish from
each other in terms of the links they exploit as summarized
in Table 2, where (i, j, k, o, p) denotes that the j-th answer
provided by user k obtains more votes (i.e., thumb-ups) than
the o-th answer of user p for question i.
Multimedia Networks. A multimedia network is a network containing multimedia data, e.g., image, text, etc. For
example, both and embed the graphs containing
two types of nodes (image and text) and three types of links
(the co-occurrence of image-image, text-text and imagetext). processes a social curation with user node and
image node. It exploits user-image links to embed users
and images into the same space so that they can be directly
compared for image recommendation. In , a click graph
is considered which contains images and text queries. The
image-query edge indicates a click of an image given a
query, where the click count serves as the edge weight.
Knowledge Graphs. In a knowledge graph (Def. 4),
the entities (nodes) and relations (edges) are usually of
different types. For example, in a ﬁlm related knowledge
graph constructed from Freebase , the types of entities
can be “director”, “actor”, “ﬁlm”, etc. The types of relations
can be “produce”, “direct”, “act in”. A lot of efforts have been
devoted to embeding knowledge graphs (e.g., , ,
 ). We will introduce them in details in Sec. 4.3.3.
Other heterogeneous graphs also exist. For instance, 
and work on the mobility data graph, in which the
station (s), role (r) and company (c) nodes are connected by
three types of links (s-s, s-r, s-c). embeds a Wikipedia
graph with three types of nodes (entity (e), category (c)
and word (w)) and three types of edges (e-e, e-c, w-w).
In addition to the above graphs, there are some general
heterogeneous graphs in which the types of nodes and
edges are not speciﬁcally deﬁned , , .
Challenge: How to explore global consistency between different
types of objects, and how to deal with the imbalances of objects
belonging to different types, if any? Different types of objects
(e.g., nodes, edges) are embedded into the same space in
heterogeneous graph embedding. How to explore the global
Comparison of Different Types of Auxiliary Information in Graphs
Auxiliary Information
Description
categorical value of a node/edge, e.g., class information
categorical or continuous value of a node/edge,
e.g., property information
node feature
text or image feature for a node
information propagation the paths of how the information is propagated in graphs
knowledge base
text associated with or facts between knowledge concepts
consistency between them is a problem. Moreover, there
may exist imbalance between objects of different types. This
data skewness should be considered in embedding.
Graph with Auxiliary Information
The third category of input graph contains auxiliary information of a node/edge/whole-graph in addition to the
structural relations of nodes (i.e., E). Generally, there are ﬁve
different types of auxiliary information as listed in Table 3.
Label: Nodes with different labels should be embedded
far away from each other. In order to achieve this, 
and jointly optimize the embedding objective function
together with a classiﬁer function. puts a penalty on the
similarity between nodes with different labels. considers node labels and edge labels when calculating different
graph kernels. and embed a knowledge graph, in
which the entity (node) has a semantic category. embeds
a more complicated knowledge graph with the entity categories in a hierarchical structure, e.g., the category “book”
has two sub-categories “author” and “written work”.
Attribute: In contrast to a label, an attribute value can be
discrete or continuous. For example, embeds a graph
with discrete node attribute value (e.g., the atomic number
in a molecule). In contrast, represents the node attribute
as a continuous high-dimensional vector (e.g., user attribute
features in social networks). deals with both discrete
and continuous attributes for nodes and edges.
Node feature: Most node features are text, which are
provided either as a feature vector for each node , 
or as a document , , , . For the latter, the
documents are further processed to extract feature vectors
using techniques such as bag-of-words , topic modelling
 , , or treating “word” as one type of node . Other
types of node features, such as image features , are also
possible. Node features enhance the graph embedding performance by providing rich and unstructured information,
which is available in many real-world graphs. Moreover, it
makes inductive graph embedding possible .
Information propagation: An example of information
propagation is “retweet” in Twitter. In , given a data
graph G = (V, E), a cascade graph Gc = (V c, Ec) is
constructed for each cascade c, where V c are the nodes
that have adopted c and Ec are the edges with both ends
in V c. They then embed Gc to predict the increment of
cascade size. Differently, aims to embed the users and
content information, such that the similarity between their
embedding indicates a diffusion probability. Topo-LSTM
 considers a cascade as not merely a sequence of nodes,
but a dynamic directed acyclic graphs for embedding.
Knowledge base: The popular knowledge bases include
Wikipedia , Freebase , YAGO , DBpedia , etc.
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
Take Wikipedia as an example, the concepts are entities
proposed by users and text is the article associated with the
entity. uses knowledge base to learn a social knowledge
graph from a social network by linking each social network
user to a given set of knowledge concepts. represents
queries and documents in the entity space (provided by a
knowledge base) so that the academic search engine can
understand the meaning of research concepts in queries.
Other types of auxiliary information include user checkin data (user-location) , user item preference ranking
list , etc. Note that the auxiliary information is not just
limited to one type. For instance, and consider
both label and node feature information. utilizes node
contents and labels to assist the graph embedding process.
Challenge: How to incorporate the rich and unstructured information so that the learnt embeddings are both representing the
topological structure and discriminative in terms of the auxiliary
information? The auxiliary information helps to deﬁne node
similarity in addition to graph structural information. The
challenges of embedding graph with auxiliary information
is how to combine these two information sources to deﬁne
the node similarity to be preserved.
Graph Constructed from Non-relational Data
As the last category of input graph is not provided, but
constructed from the non-relational input data by different
strategies. This usually happens when the input data is
assumed to lie in a low dimensional manifold.
In most cases, the input is a feature matrix X ∈R|V |×N
where each row Xi is an N-dimensional feature vector
for the i-th training instance. A similarity matrix S is
constructed by calculating Sij using the similarity between
(Xi, Xj). There are usually two ways to construct a graph
from S. A straightforward way is to directly treat S as the
adjacency matrix A of an invisible graph . However, 
is based on the Euclidean distance and it does not consider
the neighbouring nodes when calculating Sij. If X lies on
or near a curved manifold, the distance between Xi and
Xj over the manifold is much larger than their Euclidean
distance . To address these issues, other methods (e.g.,
 , , ) construct a K nearest neighbour (KNN)
graph from S ﬁrst and estimate the adjacency matrix A
based on the KNN graph. For example, Isomap incorporates the geodesic distance in A. It ﬁrst constructs a KNN
graph from S, and then ﬁnds the shortest path between two
nodes as the geodesic distance between them. To reduce the
cost of KNN graph construction (O(|V |2)), constructs
an Anchor graph instead, whose cost is O(|V |) in terms of
both time and space consumption. They ﬁrst obtain a set of
clustering centers as virtual anchors and ﬁnd the K nearest
anchors of each node for anchor graph construction.
Another way of graph construction is to establish edges
between nodes based on the nodes’ co-occurrence. For example, to facilitate image related applications (e.g., image
segmentation, image classiﬁcation), researchers (e.g., ,
 , ) construct a graph from each image by treating
pixels as nodes and the spatial relations between pixels
as edges. extracts three types of nodes (location, time
and message) from the GTMS record and therefore forms
six types of edges between these nodes. generates a
graph using entity mention, target type and text feature as
nodes, and establishes three kinds of edges: mention-type,
mention-feature and type-type.
In addition to the above pairwise similarity based and
node co-occurrence based methods, other graph construction strategies have been designed for different purposes.
For example, constructs an intrinsic graph to capture the
intraclass compactness, and a penalty graph to characterize
the interclass separability. The former is constructed by
connecting each data point with its neighbours of the same
class, while the latter connects the marginal points across
different classes. constructs a signed graph to exploit
the label information. Two nodes are connected by a positive
edge if they belong to the same class, and a negative edge
if they are from two classes. includes all instances with
a common label into one hyperedge to capture their joint
similarity. In , two feedback graphs are constructed to
gather together relevant pairs and keep away irrelevant
ones after embedding. In the positive graph, two nodes are
connected if they are both relevant. In the negative graph,
two nodes are connected only when one node is relevant
and the other is irrelevant.
Challenge: How to construct a graph that encodes the pairwise
relations between instances and how to preserve the generated
node proximity matrix in the embedded space? The ﬁrst challenge faced by embedding graphs constructed from nonrelational data is how to compute the relations between the
non-relational data and construct such a graph. After the
graph is constructed, the challenge becomes the same as in
other input graphs, i.e., how to preserve the node proximity
of the constructed graph in the embedded space.
Graph Embedding Output
The output of graph embedding is a (set of) low dimensional
vector(s) representing (part of) a graph. Based on the output
granularity, we divide graph embedding output into four
categories, including node embedding, edge embedding,
hybrid embedding and whole-graph embedding. Different
types of embedding facilitate different applications.
Unlike embedding input which is ﬁxed and given, the
embedding output is task driven. For example, node embedding can beneﬁt a wide variety of node related graph
analysis tasks. By representing each node as a vector, the
node related tasks such as node clustering, node classiﬁcation, can be performed efﬁciently in terms of both time
and space. However, graph analytics tasks are not always at
node level. In some scenarios, the tasks may be related to
higher granularity of a graph, such as node pairs, subgraph,
or even a whole graph. Hence, the ﬁrst challenge in terms of
embedding output is how to ﬁnd a suitable type of embedding
output which meets the needs of the speciﬁc application task.
Node Embedding
As the most common embedding output setting, node
embedding represents each node as a vector in a low dimensional space. Nodes that are “close” in the graph are
embedded to have similar vector representations. The differences between various graph embedding methods lie in
how they deﬁne the “closeness” between two nodes. Firstorder proximity (Def. 5) and second-order proximity are two commonly adopted metrics for pairwise node
similarity calculation. In some work, higher-order proximity
is also explored to certain extent. For example, captures
the k-step (k = 1, 2, 3, · · · ) neighbours relations in their
embedding. Both and consider two nodes belonging
to the same community as embedded closer.
Challenge: How to deﬁne the pairwise node proximity in various types of input graph and how to encode the proximity in
the learnt embeddings? The challenges of node embedding
mainly come from deﬁning the node proximity in the input
graph. In Sec 3.1, we have elaborated the challenges of node
embedding with different types of input graphs.
Next, we will introduce other types of embedding output
as well as the new challenges posed by these outputs.
Edge Embedding
In contrast to node embedding, edge embedding aims to
represent an edge as a low-dimensional vector. Edge embedding is useful in the following two scenarios.
Firstly, knowledge graph embedding (e.g., , , 
) learns embedding for both nodes and edges. Each edge
is a triplet < h, r, t > (Def. 4). The embedding is learnt to
preserve r between h and t in the embedded space, so that
a missing entity/relation can be correctly predicted given
the other two components in < h, r, t >. Secondly, some
work (e.g., , ) embeds a node pair as a vector feature
to either make the node pair comparable to other nodes
or predict the existence of a link between two nodes. For
instance, proposes a content-social inﬂuential feature to
predict user-user interaction probability given a content. It
embeds both the user pairs and content in the same space.
 embeds a pair of nodes using a bootstrapping approach
over the node embedding, to facilitate the prediction of
whether a link exists between two nodes in a graph.
In summary, edge embedding beneﬁts edge (/node
pairs) related graph analysis, such as link prediction, knowledge graph entity/relation prediction, etc.
Challenge: How to deﬁne the edge-level similarity and how to
model the asymmetric property of the edges, if any? The edge
proximity is different from node proximity as an edge contains a pair of nodes and usually denotes the pairwise node
relation. Moreover, unlike nodes, edges may be directed.
This asymmetric property should be encoded in the learnt
edge representations.
Hybrid Embedding
Hybrid embedding is the embedding of a combination of
different types of graph components, e.g, node + edge (i.e.,
substructure), node + community.
Substructure embedding has been studied in a quantity
of work. For example, embeds the graph structure
between two possibly distant nodes to support semantic
proximity search. learns the embedding for subgraphs
(e.g., graphlets) so as to deﬁne the graph kernels for graph
classiﬁcation. utilizes a knowledge base to enrich the
information about the answer. It embeds both path and
subgraph from the question entity to the answer entity.
Compared to subgraph embedding, community embedding has only attracted limited attention. proposes to
consider a community-aware proximity for node embedding, such that a node’s embedding is similar to its community’s embedding. ComE also jointly solves node embedding, community detection and community embedding
together. Rather than representing a community as a vector,
it deﬁnes each community embedding as a multivariate
Gaussian distribution so as to characterize how its member
nodes are distributed.
The embedding of substructure or community can also
be derived by aggregating the individual node and edge
embedding inside it. However, such a kind of “indirect”
approach is not optimized to represent the structure. Moreover, node embedding and community embedding can
reinforce each other. Better node embedding is learnt by
incorporating the community-aware high-order proximity,
while better communities are detected when more accurate
node embedding is generated.
Challenge: How to generate the target substructure and how to
embed different types of graph components in one common space?
In contrast to other types of embedding output, the target to
embed in hybrid embedding (e.g., subgraph, community) is
not given. Hence the ﬁrst challenge is how to generate such
kind of embedding target structure. Furthermore, different
types of targets (e.g., community, node) may be embedded
in one common space simultaneously. How to address the
heterogeneity of the embedding target types is a problem.
Whole-Graph Embedding
The last type of output is the embedding of a whole graph
usually for small graphs, such as proteins, molecules, etc.
In this case, a graph is represented as one vector and two
similar graphs are embedded to be closer.
Whole-graph embedding beneﬁts the graph classiﬁcation task by providing a straightforward and efﬁcient solution for calculating graph similarities , , . To
establish a compromise between the embedding time (efﬁciency) and the ability to preserve information (expressiveness), designs a hierarchical graph embedding framework. It thinks that accurate understanding of the global
graph information requires the processing of substructures
in different scales. A graph pyramid is formed where each
level is a summarized graph at different scales. The graph
is embedded at all levels and then concatenated into one
vector. learns the embedding for a whole cascade graph,
and then trains a multi-layer perceptron to predict the
increment of the size of the cascade graph in the future.
Challenge: How to capture the properties of a whole graph and
how to make a trade-off between expressiveness and efﬁciency?
Embedding a whole graph requires capturing the property
of a whole graph and is thus more time consuming compared to other types of embedding. The key challenge of
whole-graph embedding is how to make a choice between
the expressive power of the learnt embedding and the
efﬁciency of the embedding algorithm.
GRAPH EMBEDDING TECHNIQUES
In this section, we categorize graph embedding methods
based on the techniques used. Generally, graph embedding
aims to represent a graph in a low dimensional space which
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
preserves as much graph property information as possible. The differences between different graph embedding
algorithms lie in how they deﬁne the graph property to
be preserved. Different algorithms have different insights
of the node(/edge/substructure/whole-graph) similarities
and how to preserve them in the embedded space. Next,
we will introduce the insight of each graph embedding
technique, as well as how they quantify the graph property
and solve the graph embedding problem.
Matrix Factorization
Matrix factorization based graph embedding represent
graph property (e.g., node pairwise similarity) in the form of
a matrix and factorize this matrix to obtain node embedding
 . The pioneering studies in graph embedding usually
solve graph embedding in this way. In most cases, the input
is a graph constructed from non-relational high dimensional
data features as introduced in Sec. 3.1.4. And the output is a
set of node embedding (Sec. 3.2.1). The problem of graph
embedding can thus be treated as a structure-preserving
dimensionality reduction problem which assumes the input
data lie in a low dimensional manifold. There are two types
of matrix factorization based graph embedding. One is to
factorize graph Laplacian eigenmaps, and the other is to
directly factorize the node proximity matrix.
Graph Laplacian Eigenmaps
Insight: The graph property to be preserved can be interpreted as
pairwise node similarities. Thus, a larger penalty is imposed if two
nodes with larger similarity are embedded far apart.
Based on the above insight, the optimal embedding y can
be derived by the below objective function .
y∗= arg min
i̸=j(yi −yj
2Wij) = arg min yT Ly,
where Wij is the “deﬁned” similarity between node vi and
vj; L = D −W is the graph Laplacian. D is the diagonal
matrix where Dii = P
j̸=i Wij. The bigger the value of Dii,
the more important yi is . A constraint yT Dy = 1 is
usually imposed on Eq. 1 to remove an arbitrary scaling
factor in the embedding. Eq. 1 then reduces to:
yT Dy=1 yT Ly = arg min yT Ly
yT Dy = arg max yT Wy
The optimal y’s are the eigenvectors corresponding to the
maximum eigenvalue of the eigenproblem Wy = λDy.
The above graph embedding is transductive because it
can only embed the nodes that exist in the training set.
In practice, it might also need to embed the new coming
nodes that have not been seen in training. One solution is to
design a linear function y = XT a so that the embedding
can be derived as long as the node feature is provided.
Consequently, for inductive graph embedding, Eq. 1 becomes
ﬁnding the optimal a in the below objective function:
a∗=arg min
i̸=j ∥aTXi−aTXj∥
2Wij =arg min aTXLXTa.
Similar to Eq. 2, by adding the constraint aT XDXT a =
1, the problem in Eq. 3 becomes:
a∗=arg min aT XLXT a
aT XDXT a = arg max aT XWXT a
aT XDXT a .
The optimal a’s are eigenvectors with the maximum eigenvalues in solving XWXT a = λXDXT a.
The differences of existing studies mainly lie in how they
calculate the pairwise node similarity Wij, and whether they
use a linear function y = XT a or not. Some attempts ,
 have been made to summarize existing Laplacian eigenmaps based graph embedding methods using a general
framework. But their surveys only cover a limited quantity
of work. In Table 4, we summarize existing Laplacian eigenmaps based graph embedding studies and compare how
they calculate W and what objective function they adopt.
The initial study MDS directly adopted the Euclidean distance between two feature vectors Xi and Xj as
Wij. Eq. 2 is used to ﬁnd the optimal embedding y’s. MDS
does not consider the neighbourhood of nodes, i.e., any
pair of training instances are considered as connected. The
follow-up studies (e.g., , , , ) overcome this
problem by ﬁrst constructing a k nearest neighbour (KNN)
graph from the data feature. Each node is only connected
with its top k similar neighbours. After that, different methods are utilized to calculate the similarity matrix W so as to
preserve as much desired graph property as possible. Some
more advanced models are design recently. For example,
AgLPP introduces an anchor graph to signiﬁcantly
improve the efﬁciency of earlier matrix factorization model
LPP. LGRM learns a local regression model to grasp
the graph structure and a global regression term for out-ofsample data extrapolation. Finally, different from previous
works preserving local geometry, LSE uses local spline
regression to preserve global geometry.
When auxiliary information (e.g., label, attribute) is
available, the objective function is adjusted to preserve
the richer information. E.g., constructs an adjacency
graph W and a labelled graph W SR. The objective function consists of two parts, one focuses on preserving the
local geometric structure of the datasets as in LPP ,
and the other tries to get the embedding with the best
class separability on the labelled training data. Similarly,
 also constructs two graphs, an adjacency graph W
which encodes local geometric structures and a feedback
relational graph W ARE that encodes the pairwise relations
in users’ relevance feedbacks. RF-Semi-NMF-PCA simultaneously consider clustering, dimensionality reduction
and graph embedding by constructing an objective function
that consists of three components: PCA, k-means and graph
Laplacian regularization.
Some other work thinks that W cannot be constructed
by easily enumerating pairwise node relationships. Instead,
they adopt semideﬁnite programming (SDP) to learn W.
Speciﬁcally, SDP aims to ﬁnd an inner product matrix
that maximizes the pairwise distances between any two
inputs which are not connected in the graph while preserving the nearest neighbors distances. MVU constructs
such matrix and then applies MDS on the learned
inner product matrix. proves that regularized LPP is
equivalent to regularized SR if W is symmetric, doubly
stochastic, PSD and with rank p. It constructs such kind of
similarity matrix so as to solve LPP liked problem efﬁciently.
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
Graph Laplacian eigenmaps based graph embedding.
GE Algorithm
Objective Function
Wij = Euclidean distance (Xi, Xj)
Isomap 
KNN, Wij is the sum of edge weights along the shortest path between vi and vj
KNN, Wij = exp(
KNN, Wij = exp(
AgLPP 
anchor graph, W = ZΛ−1ZT , Λkk = P Zik, Zik =
j Kσ(Xi,Uj )
a∗= arg min aT ULUT a
KNN, Wij = exp(
y∗= arg min
yT (Lle+µLg)y
KNN, Wij = exp(
−ρ2(XiXj )
−γ, Xi ∈F +&Xj ∈F +
1, L(Xi) ̸= L(Xj)
0, otherwise
a∗= arg max aT XLARE XT a
F + denotes the images relevant to a query, γ controls the unbalanced feedback
KNN, Wij =
1, L(Xi) = L(Xj)
0, L(Xi) ̸= L(Xj)
Wij, otherwise
 1/lr, L(Xi) = L(Xj) = Cr
0, otherwise
a∗= arg max
aT XW SRXT a
aT X(DSR+L)XT a
Cr is the r-th class, lr=|Xi ∈X : L(Xi) = Cr|
S = I −L, where L is normalized hypergraph Laplacian
a∗= arg max tr(aT XSXT a), s.t. aT XXT a = Ik
KNN, W ∗= arg max tr(W ), s.t. W ≥0,P
ij Wij = 0 and ∀i, j,
where Wii −2Wij + Wjj = ∥Xi −Xj∥2
KNN, Wij =
 1/lr, L(Xi) = L(Xj) = Cr
−1, L(Xi) ̸= L(Xj)
Cr is the r-th class, lr=|Xi ∈X : L(Xi) = Cr|
NSHLRR 
normal graph: KNN, Wij = 1
hypergraph: W (e) is the weight of a hyperedge e
y∗= arg min P
∥yi −yj∥2 W (e)
0, otherwise
, d(e) = P
v∈V h(v, e)
∥Xi−Xm+1∥2
2−∥Xi−Xj ∥2
k∥Xi−Xk+1∥2
m=1 ∥Xi−Xk∥2
Wij min(∥yi −yj∥p
KNN, Wij = exp(−
Eq. 4 +(must-link and cannot link constraints)
RF-Semi-NMF-PCA KNN , Wij = 1
Eq. 2 +O(PCA) + O(kmeans)
Node Proximity Matrix Factorization
In addition to solving the above generalized eigenvalue
problem, another line of studies tries to directly factorize
node proximity matrix.
Insight: Node proximity can be approximated in a lowdimensional space using matrix factorization. The objective of preserving node proximity is to minimize the loss of approximation.
Given the node proximity matrix W, the objective is:
min ∥W −Y Y cT ∥,
where Y ∈R|V |×d is the node embedding, and Y c ∈R|V |×d
is the embedding for the context nodes .
Eq. 5 aims to ﬁnd an optimal rank-d approximation of
the proximity matrix W (d is the dimensionality of the embedding). One popular solution is to apply SVD (Singular
Value Decomposition) on W . Formally,
i=1 σiuiuc
i=1 σiuiuc
where {σ1, σ2, · · · , σ|V |} are the singular values sorted in
descending order, ui and uc
i are singular vectors of σi. The
optimal embedding is obtained using the largest d singular
values and corresponding singular vectors as follows:
Y = [√σ1u1, · · · , √σdud],
Y c = [√σ1uc
1, · · · , √σduc
Depending on whether the asymmetric property is preserved or not, the embedding of node i is either yi = Yi ,
 , or the concatenation of Yi and Y c
i , i.e., yi = [Yi, Y c
 . There exist other solutions for Eq. 5, such as regularized Gaussian matrix factorization , low-rank matrix
factorization , and adding other regularizers to enforce
more constraints . We summarize all the node proximity
matrix factorization based graph embedding in Table 5.
Summary: Matrix Factorization (MF) is mostly used to
embed a graph constructed from non-relational data (Sec.
3.1.4) for node embedding (Sec. 3.2.1), which is the typical
setting of graph Laplacian eigenmap problems. MF is also
used to embed homogeneous graphs , (Sec. 3.1.1).
Deep Learning
Deep learning (DL) has shown outstanding performance
in a wide variety of research ﬁelds, such as computer
vision, language modeling, etc. DL based graph embedding
applies DL models on graphs. These models are either a
direct adoption from other ﬁelds or a new neural network
model speciﬁcally designed for embedding graph data. The
input is either paths sampled from a graph or the whole
graph itself. Consequently, we divide the DL based graph
embedding into two categories based on whether random
walk is adopted to sample paths from a graph.
DL based Graph Embedding with Random Walk
Insight: The second-order proximity in a graph can be preserved
in the embedded space by maximizing the probability of observing
the neighbourhood of a node conditioned on its embedding.
In the ﬁrst category of deep learning based graph embedding, a graph is represented as a set of random walk
paths sampled from it. The deep learning methods are then
applied to the sampled paths for graph embedding which
preserves graph properties carried by the paths.
In view of the above insight, DeepWalk adopts a
neural language model (SkipGram) for graph embedding.
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
Node proximity matrix factorization based graph embedding. O(∗) denotes the objective function; e.g., O(SVM classiﬁer) denotes the objective
function of a SVM classiﬁer.
GE Algorithm W
Objective Function
 1, eij ∈E
0, otherwise
KNN, W ∗= arg max
k≥0 tr(W ˆ
A), s.t. Dij > (1 −ˆ
A is a connectivity matrix that describes local pairwise distances
HOPE 
Katz Index W = (I −βA)−1βA; Personalized Pagerank W = (1 −α)(I −αP )−1
Common neighbors W = A2; Adamic-Adar W = A(1/ P
j(Aij + Aji))A
GraRep 
|V | ), where ˆ
A = D−1S, Dij =
p Aip, i = j
Eq. 5 with text feature matrix
y∗= arg min P
eij ∈E (Aij−< yi, yj >)2 + λ
Eq. 5 + O(SVM classiﬁer)
O(MMDW) + (1st order proximity constraint)
KNN, W ∗= arg min tr(W (Pd
i=d+1 υiυT
where υi is eigenvector of a pairwise distance matrix, d is embedding dimensionality
W = s(1) + 5s(2)
Eq. 5 + O(community detection)
+ (community proximity constraint)
W = Z∆−1ZT , where Z∗=
i 1=1,zi≥0
j=1 ∥Xi −uj∥2
2zij + γ Pm
a∗= arg min ∥aT X −Fp∥2
whereFp is the top p eigenvectors of W
KNN, W ∗= arg min P
y∗= arg min P
RESCAL Wijk =
 1, (hi, rj, tk) exists
0, otherwise
∥Wk −Y RkY T ∥2
F + λ(∥Y ∥2
FONPE 
KNN, W ∗= arg min P
min ∥F −F W T ∥2
F + β∥P T X −F ∥2
F , s.t. P T P = I
SikpGram aims to maximize the co-occurrence probability among the words that appear within a window w.
DeepWalk ﬁrst samples a set of paths from the input graph
using truncated random walk (i.e., uniformly sample a
neighbour of the last visited node until the maximum length
is reached). Each path sampled from the graph corresponds
to a sentence from the corpus, where a node corresponds
to a word. Then SkipGram is applied on the paths to maximize the probability of observing a node’s neighbourhood
conditioned on its embedding. In this way, nodes with similar neighbourhoods (having large second-order proximity
values) share similar embedding. The objective function of
DeepWalk is as follows:
miny −log P({vi−w, · · · , vi−1, vi+1, · · · , vi+w}|yi),
where w is the window size which restricts the size of
random walk context. SkipGram removes the ordering constraint, and Eq. 8 is transformed to:
miny −log P
−w≤j≤w P(vi+j|yi),
where P(vi+j|yi) is deﬁned using the softmax function:
P(vi+j|yi) =
k=1 exp(yT
Note that calculating Eq. 10 is not feasible as the normalization factor (i.e., the summation over all inner product
with every node in a graph) is expensive. There are usually
two solutions to approximate the full softmax: hierarchical
softmax and negative sampling .
Hierarchical softmax: To efﬁciently solve Eq. 10, a binary
tree is constructed in which the nodes are assigned to the
leaves. Instead of enumerating all nodes as in Eq. 10, only
the path from the root to the corresponding leaf needs to be
evaluated. The optimization problem becomes maximizing
the probability of a speciﬁc path in the tree. Suppose the
path to leaf vi is a sequence of nodes (b0, b1, · · · , blog(|V |)),
where b0 = root, blog(|V |) = vi. Eq. 10 then becomes:
P(vi+j|yi) = Qlog(|V |)
where P(bt) is a binary classiﬁer: P(bt|vi) = σ(yT
btyi). σ(·)
denotes the sigmoid function. ybt is the embedding of tree
node bt’s parent. The hierarchical softmax reduces time
complexity of SkipGram from O(|V |2) to O(|V |log(|V |)).
Negative sampling: The key idea of negative sampling
is to distinguish the target node from noises using logistic regression. I.e., for a node vi, we want to distinguish
its neighbour vi+j from other nodes. A noise distribution
Pn(vi) is designed to draw the negative samples for node
vi. Each log P(vi+j|yi) in Eq. 9 is then calculated as:
i+jyi) + PK
t=1 Evt∼Pn[log σ(−yT
where K is the number of negative nodes that are sampled.
Pn(vi) is a noise distribution, e.g., a uniform distribution
|V |, ∀vi ∈V ). The time complexity of SkipGram
with negative sampling is O(K|V |).
The success of DeepWalk motivates many subsequent studies which apply deep learning models (e.g., Skip-
Gram or Long-Short Term Memory (LSTM) ) on the
sampled paths for graph embedding. We summarize them
in Table 6. As shown in the table, most studies follow the
idea of DeepWalk but change the settings of either random
walk sampling methods ( , , , ) or proximity
(Def. 5 and Def. 6) to be preserved ( , , , ,
 ). designs meta-path-based random walks to deal
with heterogeneous graphs and a heterogeneous SkipGram
which maximizes the probability of having the hetegeneous
context for a given node. Apart from SkipGram, LSTM is
another popular deep learning model adopted in graph
embedding. Note that SkipGram can only embed one single node. However, sometimes we may need to embed a
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
Deep Learning based graph embedding with random walk paths.
GE Algorithm
Ransom Walk Methods
Preserved Proximity DL Model
DeepWalk 
truncated random walk
truncated random walk
2nd (word-image)
GenVector 
truncated random walk
(user-user
concept-concept)
Constrained
DeepWalk 
sampling with edge weight 2nd
hierarchical
truncated random walk
2nd + class identity
TriDNR 
truncated random walk
word & label)
node2vec 
UPP-SNE 
truncated random walk
(user-user
proﬁle-proﬁle)
Planetoid 
sampling node pairs by labels and structure
2nd + label identity
sampling direct neighbours
sampling 
2nd (by graphlet)
(Eqs. 11–12 )
metapath2vec
meta-path based random
heterogeneous
ProxEmbed 
truncate random walk
node ranking tuples
truncate random walk
2nd + QA ranking
truncated random walk
2nd + user-question
quality ranking
DeepCas 
Markov chain based random walk
information cascade
MRW-MN 
truncated random walk
+ cross-modal
feature difference
sequence of nodes as a ﬁxed length vector, e.g., represent a
sentence (i.e., a sequence of words) as one vector. LSTM is
then adopted in such scenarios to embed a node sequence.
For example, and embed the sentences from questions/answers in cQA sites, and embeds a sequence
of nodes between two nodes for proximity embedding. A
ranking loss function is optimized in these work to preserve
the ranking scores in the training data. In , GRU 
(i.e., a recurrent neural network model similar to LSTM) is
used to embed information cascade paths.
DL based Graph Embedding without Random Walk
Insight:The multi-layered learning architecture is a robust and
effective solution to encode the graph into a low dimensional space.
The second class of deep learning based graph embedding methods applies deep models on a whole graph (or a
proximity matrix of a whole graph) directly. Below are some
popular deep learning models used in graph embedding.
Autoencoder: An autoencoder aims to minimize the
reconstruction error of the output and input by its encoder
and decoder. Both encoder and decoder contain multiple
nonlinear functions. The encoder maps input data to a representation space and the decoder maps the representation
space to a reconstruction space. The idea of adopting autoencoder for graph embedding is similar to node proximity
matrix factorization (Sec. 4.1.2) in terms of neighbourhood
preservation. Speciﬁcally, the adjacency matrix captures a
node’s neighbourhood. If we input the adjacency matrix to
an autoencoder, the reconstruction process will make the
nodes with similar neighbourhood have similar embedding.
Deep Neural Network: As a popular deep learning
model, Convolutional Neural Network (CNN) and its variants have been widely adopted in graph embedding. On
Deep learning based graph embedding without random walk paths.
GE Algorithm Deep Learning Model
Model Input
autoencoder
stacked denoising autoencoder
sparse autoencoder
node sequence
SCNN 
Spectral CNN
Spectral CNN with smooth
spectral multipliers
MoNet 
Mixture model network
ChebNet 
Graph CNN a.k.a. ChebNet
Graph Convolutional Network
Graph Neural Network
adapted Graph Neural Network molecules graph
GGS-NNs adapted Graph Neural Network graph
graph with image and text
a hierarchical deep model
social curation network
ProjE 
a neural network model
knowledge graph
TIGraNet Graph Convolutional Network
graph constructed from images
the one hand, some of them directly use the original CNN
model designed for Euclidean domains and reformat input
graphs to ﬁt it. E.g., uses graph labelling to select
a ﬁxed-length node sequence from a graph and then assembles nodes’ neighbourhood to learn a neighbourhood
representation with the CNN model. On the other hand,
some other work attempts to generalize the deep neural
model to non-Euclidean domains (e.g., graphs). summarizes the representative studies in their survey. Generally,
the differences between these approaches lie in the way
they formulate a convolution-like operation on graphs. One
way is to emulate the Convolution Theorem to deﬁne the
convolution in the spectral domain , . Another is
to treat the convolution as neighborhood matching in the
spatial domain , , .
Others: There are some other types of deep learning
based graph embedding methods. E.g., proposes DUIF,
which uses a hierarchical softmax as a forward propagation
to maximize the modularity. HNE utilizes deep learning
techniques to capture the interactions between heterogeneous components, e.g., CNN for image and FC layers for
text. ProjE designs a neural network with a combination
layer and a projection layer. It deﬁnes a pointwise loss
(similar to multi-class classiﬁcation) and a listwise loss (i.e.,
softmax regression loss) for knowledge graph embedding.
We summarize all deep learning based graph embedding
methods (random walk free) in Table 7, and compare the
models they use as well as the input for each model.
Summary: Due to its robustness and effectiveness, deep
learning has been widely used in graph embedding. Three
types of input graphs (except for graph constructed from
non-relational data (Sec. 3.1.4)) and all the four types of
embedding output have been observed in deep learning
based graph embedding methods.
Edge Reconstruction based Optimization
Overall Insight: The edges established based on node embedding
should be as similar to those in the input graph as possible.
The third category of graph embedding techniques directly optimizes an edge reconstruction based objective
functions, by either maximizing edge reconstruction probability or minimizing edge reconstruction loss. The later is
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
further divided into distance-based loss and margin-based
ranking loss. Next, we introduce the three types one by one.
Maximizing Edge Reconstruction Probability
Insight: Good node embedding maximizes the probability of
generating the observed edges in a graph.
Good node embedding should be able to re-establish
edges in the original input graph. This can be realized by
maximizing the probability of generating all observed edges
(i.e., node pairwise proximity) using node embedding.
The direct edge between a node pair vi and vj indicates
their ﬁrst-order proximity, which can be calculated as the joint
probability using the embedding of vi and vj:
p(1)(vi, vj) =
1+exp(−yiT yj).
The above ﬁrst-order proximity exists between any pair of
connected nodes in a graph. To learn the embedding, we
maximize the log-likelihood of observing these proximities
in a graph. The objective function is then deﬁned as:
max = max P
eij∈E log p(1)(vi, vj).
Similarly, second-order proximity of vi and vj is the conditional probability of vj generated by vi using yi and yj:
p(2)(vj|vi) =
k=1 exp(yT
It can be interpreted as the probability of a random walk in
a graph which starts from vi and ends with vj. Hence the
graph embedding objective function is:
max = max P
{vi,vj}∈P log p(2)(vj|vi),
where P is a set of {start node, end node} in the paths
sampled from the graph, i.e., the two end nodes from each
sampled path. This simulates the second-order proximity
as the probability of a random walk starting from the
start node and ending with the end node.
Minimizing Distance-based Loss
Insight: The node proximity calculated based on node embedding
should be as close to the node proximity calculated based on the
observed edges as possible.
Speciﬁcally, node proximity can be calculated based on
node embedding or empirically calculated based on observed edges. Minimizing the differences between the two
types of proximities preserves the corresponding proximity.
For the ﬁrst-order proximity, it can be computed using
node embedding as deﬁned in Eq. 13. The empirical probability is ˆp(1)(vi, vj) = Aij/ P
eij∈E Aij, where Aij is the
weight of edge eij. The smaller the distance between p(1)
and ˆp(1) is, the better ﬁrst-order proximity is preserved.
Adopting KL-divergence as the distance function to calculate the differences between p(1) and ˆp(1) and omitting some
constants, the objective function to preserve the ﬁrst-order
proximity in graph embedding is:
min = min −P
eij∈E Aij log p(1)(vi, vj).
Similarly, the second-order proximity of vi and vj is the
conditional probability of vj generated by node vi (Eq.
15). The empirical probability of ˆp(2)(vi|vj) is calculated as
ˆp(2)(vj|vi) = Aij/di, where di = P
eik∈E Aik is the outdegree (or degree in the case of undirected graph) of node
Edge reconstruction based graph embedding. O∗
vj ) refers to
one of Eq. 14, Eq. 16 ∼Eq. 19; e.g., O(2)
min(word-label) refers to Eq. 18
with a word node and a label node. T v
vi denote the type of node vi.
Objectives
max(node, node)
Orank(node, neighbour node) + O(attribute loss)
min(word, word) + O(2)
min(word, document) +
min(word, label)
max(node, node))
GraphEmbed
min(word, location) + O(2)
min(time, location) +
min(location, location) + O(2)
min(time, time)
 , 
min(station,
min(station,
min(destination, boarding)
Orank(mention-type) + O(2)
min(mention, feature) +
min(type, type)
min(node, node) + O(anchor align)
min(node, other nodes in one hyperedge)
max(node, neighbour context)+O(2)
max(node, path
context)+O(2)
max(node, edge context)
max(user pair, diffused content)
max(entity,
max(entity,
max(entity, words) + O(2)
max(entity, venue)
min(node, node) + O(2)
min(node, node))
O(AUC ranking) + O(2)
max(node, node) + O(2)
node context)
Orank(question, answer)
vi. Similar to Eq. 10, it is very expensive to calculate Eq.
15 and negative sampling is again adopted for approximate
computation to improve the efﬁciency. By minimizing the
KL divergence between p(2)(vj|vi) and ˆp(2)(vj|vi), the objective function to preserve second-order proximity is:
min = min −P
eij∈E Aij log p(2)(vj|vi).
Minimizing Margin-based Ranking Loss
In the margin-based ranking loss optimization, the edges of
the input graph indicate the relevance between a node pair.
Some nodes in the graph are usually associated with a set
of relevant nodes. E.g., in a cQA site, a set of answers are
marked as relevant to a given question. The insight of the
loss is straightforward.
Insight: A node’s embedding is more similar to the embedding of
relevant nodes than that of any other irrelevant node.
Denote s(vi, vj) as the similarity score for node vi and vj,
as the set of nodes relevant to vi and V −
as the irrelevant
nodes set. The margin-based ranking loss is deﬁned as:
Orank = min
max{0, γ−s(vi, v+
i )+s(vi, v−
where γ is the margin. Minimizing the loss rank encourages
a large margin between s(vi, v+
i ) and s(vi, v−
i ), and thus
enforces vi to be embedded closer to its relevant nodes than
to any other irrelevant nodes.
In Table 8, we summarize existing edge reconstruction
based graph embedding methods, based on their objective
functions and preserved node proximity. In general, most
methods use one of the above objective functions (Eq. 14,
Eq. 16 ∼Eq. 19). optimizes an AUC ranking loss, which
is a substitution loss for margin based ranking loss (Eq. 19).
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
Knowledge graph embedding using margin-based ranking loss.
GE Algorithm
Energy Function fr(h, t)
TransE 
∥h + r −t∥l1
∥Mrhh + r −Mrtt∥
TransR 
∥hMr + r −tMr∥2
CTransR 
∥hMr + rc −tMr∥2
2 + α∥rc −r∥2
TransH 
r hwr) + dr −(t −wT
SePLi 
2 ∥Wieih + bi −eit∥2
TransD 
∥Mrhh + r −Mrtt∥2
TranSparse 
r )h + r −M t
m-TransH 
ρ∈M(Rr) ar(ρ)Pnr (t(ρ)) + br∥2, t ∈N M(Rr)
DKRL 
∥hd + r −td∥+ ∥hd + r −ts∥+ ∥hs + r −td∥
ManifoldE 
Sphere: ∥ϕ(h) + ϕ(r) −ϕ(t)∥2
Hyperplane: (ϕ(h) + ϕ(rhead))T (ϕ(t) + ϕ(rtail))
ϕ is the mapping function to Hilbert space
TransA 
∥h + r −t∥
puTransE 
∥h + r −t∥
KGE-LDA 
∥h + r −t∥l1
∥Ruh −Rut∥l1
SME linear
(Wu1r + Wu2h + bu)T (Wv1r + Wv2t + bv)
SME bilinear
(Wu1r + Wu2h + bu)T (Wv1r + Wv2t + bv)
−λ∥e −sT es∥2
2, S(sh, st) =
r tanh(hT Wrt + Wrhh + Wrtt + br)
HOLE 
rT (h ⋆t), where ⋆is circular correlation
MTransE 
∥h + r −t∥l1
Note that when another task is simultaneously optimized
during graph embedding, that task-speciﬁc objective will be
incorporated in the overall objective. For instance, aims
to align two graphs. Hence an network alignment objective
function is optimized together with O(2)
min (Eq. 18).
It is worth noting that most of the existing knowledge
graph embedding methods choose to optimize marginbased ranking loss. Recall that a knowledge graph G consists
of a set of triplets < h, r, t > denoting the head entity h is
linked to a tail entity t by a relation r. Embedding G can
be interpreted as preserving the ranking of a true triplet
< h, r, t > over a false triplet < h′, r, t′ > that does not exist
in G. Particularly, in knowledge graph embedding, similar
to s(vi, v+
i ) in Eq. 19, an energy function is designed for
a triplet < h, r, t > as fr(h, t). There is a slight difference
between these two functions. s(vi, v+
i ) denotes the similarity score between the embedding of node vi and v+
fr(h, t) is the distance score of the embedding of h and t in
terms of relation r. One example of fr(h, t) is ∥h + r −t∥l1,
where relationships are represented as translations in the embedded space . Other options of fr(h, t) are summarized
in Table 9. Consequently, for knowledge graph embedding,
Eq. 19 becomes:
rank = min
<h,r,t>∈S,
<h′,r,t′>/∈S
max{0, γ + fr(h, t) −fr(h′, t′)},
where S is the triples in the input knowledge graph. Existing knowledge graph embedding methods mainly optimize
Eq. 20 in their work. The difference among them is how they
deﬁne fr(h, t) as summarized in Table 9. More details about
the related work in knowledge graph embedding has been
thoroughly reviewed in .
Note that some studies jointly optimize the ranking loss
(Eq. 20) and other objectives to preserve more information.
E.g., SSP optimizes a topic model loss together with
Eq. 20 to utilize textual node descriptions for embedding.
 categorizes monolingual relations and uses linear
transformation to learn cross-lingual alignment for entities
and relations. There also exists some work which deﬁnes a
matching degree score rather than an energy function for a
triplet < h, r, t >. E.g., deﬁnes a bilinear score function
h Wrvt It adds a normality constraint and a commutativity constraint to impose analogical structures among the
embedding. ComplEx extends embedding to complex
domain and deﬁnes the real part of vT
h Wrvt as the score.
Summary: Edge reconstruction based optimization is applicable for most graph embedding settings. As far as can be
observed, only graph constructed from non-relational data
(Sec. 3.1.4) and whole-graph embedding (Sec. 3.2.4) have
not been tried. The reason is that reconstructing manually
constructed edges is not as intuitive as in other graphs.
Moreover, as this technique focuses on the directly observed
local edges, it is not suitable for whole-graph embedding.
Graph Kernel
Insight: The whole graph structure can be represented as a
vector containing the counts of elementary substructures that are
decomposed from it.
Graph kernel is an instance of R-convolution kernels
 , which is a generic way of deﬁning kernels on discrete
compound objects by recursively decomposing structured
objects into “atomic” substructures and comparing all pairs
of them . The graph kernel represents each graph as
a vector, and two graphs are compared using an inner
product of the two vectors. There are generally three types
of “atomic” substructures deﬁned in graph kernel.
Graphlet. A graphlet is an induced and non-isomorphic
subgraph of size-k . Suppose graph G is decomposed
into a set of graphlets {G1, G2, · · · , Gd}. Then G is embedded as a d-dimensional vector (denoted as yG) of normalized
counts. The i-th dimension of yG is the frequency of the
graphlet Gi occurring in G.
Subtree Patterns. In this kernel, a graph is decomposed
as its subtree patterns. One example is Weisfeiler-Lehman
subtree . In particular, a relabelling iteration process is
conducted on a labelled graph (i.e., a graph with discrete
node labels). At each iteration, a multiset label is generated based on the label of a node and its neighbours. The
new generated multiset label is a compressed label which
denotes the subtree patterns, which is then used for the
next iteration. Based on Weisfeiler-Lehman test of graph
isomorphism, counting the occurrence of labels in a graph is
equivalent to counting the corresponding subtree structures.
Suppose h iterations of relabelling are performed on graph
G. Its embedding yG contains h blocks. The i-th dimension
in the j-th block of yG is the frequency of the i-th label being
assigned to a node in the j-th iteration.
Random Walks. In the third type of graph kernels, a graph
is decomposed into random walks or paths and represented
as the counts of occurrence of random walks or paths
 in it. Take paths as an example, suppose graph G is
decomposed into d shortest paths. Denote the i-th path as
a triplet < ls
i , ni >, where ls
i are the labels of the
starting and ending nodes, ni is the length of the path. Then
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
G is represented as a d-dimensional vector yG where the i-th
dimension is the frequency of the i-th triplet occurring in G.
Summary: A graph kernel is designed for whole-graph
embedding (Sec. 3.2.4) only as it captures the global property of a whole graph. The type of input graph is usually
a homogeneous graph (Sec. 3.1.1) or a graph with
auxiliary information (Sec. 3.1.3) .
Generative Model
A generative model can be deﬁned by specifying the joint
distribution of the input features and the class labels,
conditioned on a set of parameters . An example is
Latent Dirichlet Allocation (LDA), in which a document is
interpreted as a distribution over topics, and a topic is a
distribution over words . There are the following two
ways to adopt a generative model for graph embedding.
Embed Graph Into The Latent Semantic Space
Insight: Nodes are embedded into a latent semantic space where
the distances among nodes explain the observed graph structure.
The ﬁrst type of generative model based graph embedding methods directly embeds a graph in the latent space.
Each node is represented as a vector of the latent variables.
In other words, it views the observed graph as generated
by a model. E.g., in LDA, documents are embedded in a
“topic” space where documents with similar words have
similar topic vector representations. designs a LDAlike model to embed a location-based social network (LBSN)
graph. Speciﬁcally, the input is locations (documents), each
of which contains a set of users (words) who visited that
location. Users visit the same location (words appearing in
the same document) due to some activities (topics). Then a
model is designed to represent a location as a distribution
over activities, where each activity has an attractiveness distribution over users. Consequently, both user and location
are represented as a vector in the “activity” space.
Incorporate Latent Semantics for Graph Embedding
Insight: Nodes which are close in the graph and having similar
semantics should be embedded closer. The node semantics can be
detected from node descriptions via a generative model.
In this line of methods, latent semantics are used to
leverage auxiliary node information for graph embedding.
The embedding is decided not only by the graph structure
information but also by the latent semantics discovered
from other sources of node information. For example, 
proposes a uniﬁed framework which jointly integrates topic
modelling and graph embedding. Its principle is that if
two nodes are close in the embedded space, they will
also share similar topic distribution. A mapping function
from the embedded space to the topic semantic space is
designed so as to correlate the two spaces. proposes a
generative model (Bayesian non-parametric inﬁnite mixture
embedding model) to address the issue of multiple relation
semantics in knowledge graph embedding. It discovers the
latent semantics of a relation and leverages a mixture of
relation components for embedding. embeds a knowledge graph from both the knowledge graph triplets and the
textual descriptions of entities and relations. It learns the
semantic representation of text using topic modelling and
restricts the triplet embedding in the semantic subspace.
The difference between the above two directions of
methods is that the embedded space is the latent space in
the ﬁrst way. In contrast, in the second way, the latent space
is used to integrate information from different sources, and
help to embed a graph to another space.
Summary: Generative model can be used for both node
embedding (Sec. 3.2.1) and edge embedding (Sec. 3.2.2)
 . As it considers node semantics, the input graph is
usually a heterogeneous graph (Sec. 3.1.2 ) or a graph
with auxiliary information (Sec. 3.1.3) .
Hybrid Techniques and Others
Sometimes multiple techniques are combined in one study.
For example, learns edge-based embedding via minimizing the margin-based ranking loss (Sec. 4.3), and learns
attribute-based embedding by matrix factorization (Sect.
4.1). optimizes a margin-based ranking loss (Sec. 4.3)
with matrix factorization based loss (Sec. 4.1) as regularization terms. uses LSTM (Sec. 4.2) to learn sentences embedding in cQAs and a margin-based ranking loss
(Sec. 4.3) to incorporate friendship relations. adopts
CBOW/SkipGram (Sec. 4.2) for knowledge graph entity embedding, and then ﬁne-tune the embedding by minimizing
a margin-based ranking loss (Sec. 4.3). use word2vec
(Sec. 4.2) to embed the textual context and TransH (Sec.
4.3) to embed the entity/relations so that the rich context
information is utilized in knowledge graph embedding.
 leverages the heterogeneous information in a knowledge base to improve recommendation performance. It uses
TransR (Sec. 4.3) for network embedding, and uses autoencoders for textual and visual embedding (Sec. 4.2). Finally,
a generative framework (Sec. 4.5) is proposed to integrate
collaborative ﬁltering with items semantic representations.
Apart from the introduced ﬁve categories of techniques,
there exist other approaches. presents embedding of a
graph by its distances to prototype graphs. ﬁrst embeds
a few landmark nodes using their pairwise shortest path
distances. Then other nodes are embedded so that their
distances to a subset of landmarks are as close as possible to the real shortest paths. jointly optimizes a linkbased loss (maximizing the likelihood of observing a node’s
neighbours instead of its non-neighbours) and an attributebased loss (learning a linear projection based on link-based
representation). KR-EAR distinguishes the relations in
a knowledge graph as attribute-based and relation-based.
It constructs a relational triple encoder (TransE, TransR) to
embed the correlations between entities and relations, and
an attributional triple encoder to embed the correlations
between entities and attributes. Struct2vec considers
the structral identify of nodes by a hierarchical metric for
node embedding. provides a fast embedding approach
by approximating the higher-order proximity matrices.
We now summarize and compare all the ﬁve categories
of introduced graph embedding techniques in Table 10 in
terms of their advantages and disadvantages.
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
Comparison of graph embedding techniques.
Subcategory
Advantages
Disadvantages
matrix factorization graph Laplacian eigenaps
consider global node proximity
large time and space consumption
node proximity matrix factorization
deep learning
with random walk
effective and robust,
a) only consider local context within a path
no feature engineering
b) hard to ﬁnd optimal sampling strategy
without random walk
high computation cost
edge reconstruction
maximize edge reconstruct probability
optimization using only observed local
minimize distance-based loss
relatively efﬁcient training
information, i.e., edges (1-hop neighbour)
minimize margin-based ranking loss
or ranked node pairs
graph kernel
based on graphlet
efﬁcient, only counting the desired atom
a) substructures are not independent
based on subtree patterns
substructure
b) embedding dimensionality grows
based on random walks
exponentially
generative model
embed graph in the latent space
interpretable embedding
a) hard to justify the choice of distribution
incorporate latent semantics for graph embedding naturally leverage multiple information sources b) require a large amount of training data
Matrix factorization based graph embedding learns the
representations based on the statistics of global pairwise
similarities. Hence it can outperform deep learning based
graph embedding (random walk involved) in certain tasks
as the latter relies on separate local context windows ,
 . However, either the proximity matrix construction or
the eigendecomposition of the matrix is time and space
consuming , making matrix factorization inefﬁcient
and unscalable for large graphs.
Deep Learning (DL) has shown promising results among
different graph embedding methods. We consider DL as
suitable for graph embedding, because of its capability of
automatically identifying useful representations from complex graph structures. For example, DL with random walk
(e.g., DeepWalk , node2vec , metapath2vec ) can
automatically exploit the neighbourhood structure through
sampled paths on the graph. DL without random walk can
model variable-sized subgraph structures in homogeneous
graphs (e.g., GCN , struc2vec , GraphSAGE ),
or rich interactions among ﬂexible-typed nodes in heterogeneous graphs (e.g., HNE , TransE , ProxEmbed ),
as useful representations. On the other hand, DL also has
its limitations. For DL with random walks, it typically considers a nodes local neighbours within the same path and
thus overlooks the global structure information. Moreover,
it is difﬁcult to ﬁnd an ‘optimal sampling strategy as the
embedding and path sampling are not jointly optimized in
a uniﬁed framework. For DL without random walks, the
computational cost is usually high. The traditional deep
learning architectures assume the input data on 1D or 2D
grid to take advantage of GPU . However, graphs do
not have such a grid structure, and thus require different
solutions to improve the efﬁciency .
Edge reconstruction based graph embedding optimizes
an objective function based on the observed edges or ranking triplets. It is more efﬁcient compared to the previous
two categories of graph embedding. However, this line
of methods are trained using the directly observed local
information, and thus the obtained embedding lacks the
awareness of the global graph structure.
Graph kernel based graph embedding converts a graph
into one single vector to facilitate the graph level analytic
tasks such as graph classiﬁcation. It is more efﬁcient than
other categories of techniques as it only needs to enumerate
the desired atomic substructures in a graph. However, such
“bag-of-structure” based methods have two limitations .
Firstly, the substructures are not independent. For example,
the graphlet of size k+1 can be derived from size k graphlet
by adding a new node and some edges. This means there
exist redundant information in the graph representation.
Secondly, the embedding dimensionality usually grows exponentially when the size of the substructure grows, leading
to a sparse problem in the embedding.
Generative model based graph embedding can naturally
leverage information from different sources (e.g., graph
structure, node attribute) in a uniﬁed model. Directly embedding graphs into the latent semantic space generates the
embedding that can be interpreted using the semantics. But
the assumption of modelling the observation using certain
distributions is hard to justify. Moreover, the generative
method needs a large amount of training data to estimate
a proper model which ﬁts the data. Hence it may not work
well for small graphs or a small number of graphs.
APPLICATIONS
Graph embedding beneﬁts a wide variety of graph analytics
applications as the vector representations can be processed
efﬁciently in both time and space. In this section, we categorize the graph embedding enabled applications as node
related, edge related and graph related.
Node Related Applications
Node Classiﬁcation
Node classiﬁcation is to assign a class label to each node in
a graph based on the rules learnt from the labelled nodes.
Intuitively, “similar” nodes have the same labels. It is one
of the most common applications discussed in graph embedding literatures. In general, each node is embedded as a
low-dimensional vector. Node classiﬁcation is conducted by
applying a classiﬁer on the set of labelled node embedding
for training. The example classiﬁers include SVM ( , ,
 , , , , , , , , , , , ),
logistic regression ( , , , , , , , ,
 , , ) and k-nearest neighbour classiﬁcation ( ,
 ). Then given the embedding of an unlabelled node,
the trained classiﬁer can predict its class label. In contrast
to the above sequential processing of ﬁrst node embedding
then node classiﬁcation, some other work designs a uniﬁed framework to jointly optimize
graph embedding and node classiﬁcation, which learns a
classiﬁcation-speciﬁc representation for each node.
Node Clustering
Node clustering aims to group similar nodes together, so
that nodes in the same group are more similar to each other
than those in other groups. As an unsupervised algorithm,
it is applicable when the node labels are unavailable. After
representing nodes as vectors, the traditional clustering
algorithms can then be applied on the node embedding.
Most existing work , , , , , , adopts
k-means as the clustering algorithm. In contrast, and
 jointly optimize clustering and graph embedding in one
objective to learn a clustering-speciﬁc node representation.
Node Recommendation/Retrieval/Ranking
The task of node recommendation is to recommend top K
nodes of interest to a given node based on certain criteria
such as similarity , , , , , . In realworld scenarios, there are various types of recommended
node, such as research interests for researchers , items
for customers , , images for curation network users
 , friends for social network users , and documents for
a query . It is also popular in community-based question
answering. Given a question, they predict the relative rank
of users ( , ) or answers ( , ). In proximity
search , , they rank the nodes of a particular type
(e.g., “user”) for a given query node (e.g., “Bob”) and a
proximity class (e.g., “schoolmate”), e.g., ranking users who
are the schoolmates of Bob. And there is some work focusing
on cross-modal retrieval , , , , e.g., keywordbased image/video search.
A speciﬁc application which is popularly discussed in
knowledge graph embedding is entity ranking , ,
 , , . Recall that a knowledge graph consists of
a set of triplets < h, r, t >. Entity ranking aims to rank the
correct missing entities given the other two components in
a triplet higher than the false entities. E.g., it returns the true
h’s among all the candidate entities given r and t, or returns
the true t’s given r and h.
Edge Related Applications
Next we introduce edge related applications in which an
edge or a node pair is involved.
Link Prediction
Graph embedding aims to represent a graph with lowdimensional vectors, but interestingly its output vectors can
also help infer the graph structure. In practice, graphs are
often incomplete; e.g., in social networks, friendship links
can be missing between two users who actually know each
other. In graph embedding, the low-dimensional vectors are
expected to preserve different orders of network proximity
(e.g., DeepWalk , LINE ), as well as different scales of
structural similarity (e.g., GCN , struc2vec ). Hence,
these vectors encode rich information about the network
structure, and they can be used to predict missing links in
the incomplete graph. Most attempts on graph embedding
driven link prediction are on homogeneous graphs , ,
 , . For example, predicts the friendship relation
between two users. Relatively fewer graph embedding work
deals with heterogeneous graph link prediction. For example, on a heterogeneous social graph, ProxEmbed tries
to predict the missing links of certain semantic types (e.g.,
schoolmates) between two users, based on the embedding of
their connecting paths on the graph. D2AGE solves the
same problem by embedding two users connecting directed
acyclic graph structure.
Triple Classiﬁcation
Triplet classiﬁcation , , , , , , , 
is a speciﬁc application for knowledge graph. It aims to
classify whether an unseen triplet < h, r, t > is correct or
not, i.e., whether the relation between h and t is r.
Graph Related Applications
Graph Classiﬁcation
Graph classiﬁcation assigns a class label to a whole graph.
This is important when the graph is the unit of data. For
example, in , each graph is a chemical compound, an
organic molecule or a protein structure. In most cases,
whole-graph embedding is applied to calculate graph level
similarity , , , , . Recently, some work
starts to match node embedding for graph similarity ,
 . Each graph is represented as a set of node embedding
vectors. Graphs are compared based on two sets of node
embedding. decomposes a graph into a set of substructures and then embed each substructure as a vector
and compare graphs via substructure similarities.
Visualization
Graph visualization generates visualizations of a graph on
a low dimensional space , , , , , .
Usually, for visualization purpose, all nodes are embedded
as 2D vectors and then plotted in a 2D space with different
colours indicating nodes’ categories. It provides a vivid
demonstration of whether nodes belonging to the same
category are embedded closer to each other.
Other Applications
Above are some general applications that are commonly
discussed in existing work. Depending on the information
carried in the input graph, more speciﬁc applications may
exist. Below are some example scenarios.
Knowledge graph related: and extract relational
fact from large-scale plain text. extracts medical entities from text. links natural language text with entities in a knowledge graph. focuses on de-duplicating
entities that are equivalent in a knowledge graph. 
jointly embeds entity mentions, text and entity types to
estimate the true type-path for each mention from its noisy
candidate type set. E.g., the candidate types for “Trump”
are {person, politician, businessman, artist, actor}. For the
mention “Trump” in sentence “Republican presidential candidate Donald Trump spoke during a campaign event in
Rock Hill.”, only {person, politician} are correct types.
Multimedia network related: embeds the geotagged social media (GTSM) records <time, location,
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017
message> which enables them to recover the missing component from a GTSM triplet given the other two. It can also
classify the GTSM records, e.g., whether a check-in record is
related to “Food” or “Shop”. uses graph embedding to
reduce data dimensionality for face recognition. maps
images into a semantic manifold that faithfully grasps users’
preferences to facilitate content-based image retrieval.
Information propagation related: predicts the increment of a cascade size after a given time interval. 
predicts the propagation user and identiﬁes the domain
expert by embedding the social interaction graph.
Social networks alignment: Both and learn
node embedding to align users across different social networks, i.e., to predict whether two user accounts in two
different social networks are owned by the same user.
Image related: Some work embeds graphs constructed
from images, and then use the embedding for image classi-
ﬁcation ( , ), image clustering , image segmentation , pattern recognition , and so on.
FUTURE DIRECTIONS
In this section, we summarize four future directions for the
ﬁeld of graph embedding, including computation efﬁciency,
problem settings, techniques and application scenarios.
Computation. The deep architecture, which takes the geometric input (e.g., graph), suffers the low efﬁciency problem.
Traditional deep learning models (designed for Euclidean
domains) utilize the modern GPU to optimize their efﬁciency by assuming that the input data are on a 1D or
2D grid. However, graphs do not have such a kind of grid
structure and thus the deep architecture designed for graph
embedding needs to seek alternative solutions to improve
the model efﬁciency. suggested that the computational
paradigms developed for large-scale graph processing can
be adopted to facilitate efﬁciency improvement in deep
learning models for graph embedding.
Problem settings. The dynamic graph is one promising
setting for graph embedding. Graphs are not always static,
especially in real life scenarios, e.g., social graphs in Twitter,
citation graphs in DBLP. Graphs can be dynamic in terms
of graph structure or node/edge information. On the one
hand, the graph structure may evolve over time, i.e., new
nodes/edges appear while some old nodes/edges disappear. On the other hand, the nodes/edges may be described
by some time-varying information. Existing graph embedding mainly focuses on embedding the static graph and
the settings of dynamic graph embedding are overlooked.
Unlike static graph embedding, the techniques for dynamic
graphs need to be scalable and better to be incremental so
as to deal with the dynamic changes efﬁciently. This makes
most of the existing graph embedding methods, which
suffer from the low efﬁciency problem, not suitable anymore. How to design effective graph embedding methods
in dynamic domains remains an open question.
Techniques. Structure awareness is important for edge
reconstruction based graph embedding. Current edge reconstruction based graph embedding methods are mainly
based on the edges only, e.g., 1-hope neighbours in a general graph, a ranked triplet (< h, r, t > in a knowledge
graph, and (vi, v+
i ) in a cQA graph. Single edges only
provide local neighbourhood information to calculate the
ﬁrst- and second-order proximity. The global structure of
a graph (e.g., paths, tree, subgraph patterns) is omitted.
Intuitively, a substructure contains richer information than
one single edge. Some work attempts to explore the path
information in knowledge graph embedding ( , , ,
 ). However, most of them use deep learning models
( , , ) which suffer the low efﬁciency issue as
discussed earlier. How to design the non-deep learning
based methods that can take advantage of the expressive
power of graph structure is a question. provides one
example solution. It minimizes both pairwise and longrange loss to capture pairwise relations and long-range
interactions between entities. Note that in addition to the
list/path structure, there are various kinds of substructures
which carry different structure information. For example,
SPE has tried to introduce a subgraph-augmented path
structure for embedding the proximity between two nodes
in a heterogeneous graph, and it shows better performance
than embedding simple paths for semantic search tasks. In
general, an efﬁcient structure-aware graph embedding optimization solution, together with the substructure sampling
strategy, is in needed.
Applications. Graph embedding has been applied in many
different applications. It is an effective way to learn the
representations of data with consideration of their relations. Moreover, it can convert data instances from different
sources/platforms/views into one common space so that
they are directly comparable. For example, , , use
graph embedding for cross-modal retrieval, such as contentbased image retrieval, keyword-based image/video search.
The advantages of using graph embedding for representation learning is that the graph manifold of the training
data instances are preserved in the representations and can
further beneﬁt the follow-up applications. Consequently,
graph embedding can beneﬁt the tasks which assume the
input data instances are correlated with certain relations
(i.e., connected by certain links). It is of great importance
to exploring the application scenarios which beneﬁt from
graph embedding, as it provides effective solutions to the
conventional problems from a different perspective.
CONCLUSIONS
In this survey, we conduct a comprehensive review of the literature in graph embedding. We provide a formal deﬁnition
to the problem of graph embedding and introduce some basic concepts. More importantly, we propose two taxonomies
of graph embedding, categorizing existing work based on
problem settings and embedding techniques respectively.
In terms of problem setting taxonomy, we introduce four
types of embedding input and four types of embedding
output and summarize the challenges faced in each setting. For embedding technique taxonomy, we introduce the
work in each category and compare them in terms of their
advantages and disadvantages. After that, we summarize
the applications that graph embedding enables. Finally, we
suggest four promising future research directions in the
ﬁeld of graph embedding in terms of computation efﬁciency,
problem settings, techniques and application scenarios.
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, SEPT 2017