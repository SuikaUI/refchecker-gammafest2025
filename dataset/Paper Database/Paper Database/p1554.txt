Learning Decision Trees from Dynamic Data Streams
Jo˜ao Gama
LIACC, FEP - University of Porto
Rua de Ceuta 118-6, 4050 Porto, Portugal
 
Pedro Medas
LIACC - University of Porto
Rua de Ceuta 118-6, 4050 Porto, Portugal
 
Abstract: This paper presents a system for induction of forest of functional trees
from data streams able to detect concept drift. The Ultra Fast Forest of Trees (UFFT)
is an incremental algorithm, which works online, processing each example in constant
time, and performing a single scan over the training examples. It uses analytical techniques to choose the splitting criteria, and the information gain to estimate the merit
of each possible splitting-test. For multi-class problems the algorithm builds a binary
tree for each possible pair of classes, leading to a forest of trees. Decision nodes and
leaves contain naive-Bayes classiﬁers playing diﬀerent roles during the induction process. Naive-Bayes in leaves are used to classify test examples. Naive-Bayes in inner
nodes play two diﬀerent roles. They can be used as multivariate splitting-tests if chosen by the splitting criteria, and used to detect changes in the class-distribution of the
examples that traverse the node. When a change in the class-distribution is detected,
all the sub-tree rooted at that node will be pruned. The use of naive-Bayes classiﬁers at
leaves to classify test examples, the use of splitting-tests based on the outcome of naive-
Bayes, and the use of naive-Bayes classiﬁers at decision nodes to detect changes in the
distribution of the examples are directly obtained from the suﬃcient statistics required
to compute the splitting criteria, without no additional computations. This aspect is
a main advantage in the context of high-speed data streams. This methodology was
tested with artiﬁcial and real-world data sets. The experimental results show a very
good performance in comparison to a batch decision tree learner, and high capacity to
detect drift in the distribution of the examples.
Key Words: Data streams, Incremental Decision trees, Concept Drift
Category: H.2.8, I.2.6, I.5.2
Introduction
Many sources produce data continuously. Examples include telephone record
calls, customer click streams, large sets of web pages, multimedia data, and
sets of retail chain transactions. These sources are called data streams. In data
streams training examples come over time, usually one at a time. In the
authors present desirable properties for learning in data streams: incrementality,
online learning, constant time to process each example, single scan over the
training set, take drift into account. In these situations is highly unprovable the
Journal of Universal Computer Science, vol. 11, no. 8 , 1353-1366
submitted: 5/3/05, accepted: 5/5/05, appeared: 28/8/05 © J.UCS
assumption that the examples are generated at random according to a stationary
probability distribution. At least in complex systems and for large time periods,
we should expect changes in the distribution of the examples. A natural approach
for this incremental tasks are adaptive learning algorithms, incremental learning
algorithms that take into account concept drift.
In this paper we present UFFT, an algorithm that generates forest of functional trees for data streams. The main contributions of this work include a fast
method to choose the cut point for splitting tests, use of multivariate splitting
tests, the use of functional leaves to classify test cases, and the ability to detect
concept drift. These aspects are integrated in the sense that the suﬃcient statistics needed by the splitting criteria are the only statistics used in the functional
leaves, multivariate splitting tests, and in the drift detection method. The paper
is organized as follows. In the next section we present related work in the areas
of incremental decision-tree induction and concept drift detection. In Section 3,
we present the main issues of our algorithm. The system has been implemented,
and evaluated in a set of benchmark problems. Preliminary results are presented
in Section 4. In the last section we resume the main contributions of this paper,
and point out some future work.
Related Work
In this section we analyze related work in two dimensions. One dimension is
related to methods dealing with concept drift. The other dimension is related to
the induction of decision trees from data streams.
In the literature of machine learning, several methods have been presented
to deal with time changing concepts . The two basic
methods are based on temporal windows where the window ﬁxes the training
set for the learning algorithm and weighting examples that ages the examples,
shrinking the importance of the oldest examples. These basic methods can be
combined and used together. Both weighting and time window forgetting systems are used for incremental learning. A method to dynamically choose the
set of old examples that will be used to learn the new concept faces several
diﬃculties. It has to select enough examples to the learner algorithm and also
to keep old data from disturbing the learning process, when older data have a
diﬀerent probability distribution from the new concept. A larger set of examples
allows a better generalization if no concept drift happened since the examples
arrived . The systems using weighting examples use partial memory to select the more recent examples, and therefore probably within the new context.
Repeated examples are assigned more weight. The older examples, according
to some threshold, are forgotten and only the newer ones are used to learn the
new concept model . When a drift concept occurs the older examples become
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
irrelevant. We can apply a time window on the training examples to learn the
new concept description only from the most recent examples. The time window
can be improved by adapting its size. Widmer and Klinkenberg present
several methods to choose a time window dynamically adjusting the size using
heuristics to track the learning process. The methods select the time window to
include only examples on the current target concept. Klinkenberg in presents
a method to automatically select the time window size in order to minimize the
generalization error. Kubat and Widmer describe a system that adapts to
drift in continuous domains. Klinkenberg shows the application of several
methods of handling concept drift with an adaptive time window on the training
data, by selecting representative training examples or by weighting the training
examples. Those systems automatically adjust the window size, the example
selection and the example weighting to minimize the estimated generalization
Concept drift in the context of data streams appears for example in .
H. Wang et al. train ensembles of batch learners from sequential chunks of data
and use error estimates on the test data under the time-evolving environment. G.
Hulten and P.Domingos have proposed a method to scale-up learning algorithms
to very-large databases . They have presented system VFDT , a very fast
decision tree algorithm for data-streams described by nominal attributes. The
main innovation in VFDT is the use of the Hoeﬀding bound to decide when
a leaf should be expanded to a decision node. The work of VFDT, has been
extended with the ability to detect changes in the underlying distribution of the
examples. CVFDT is a system for mining decision trees from time-changing
data streams. CVFDT works by keeping its model consistent with a sliding
window of the most recent examples. When a new example arrives it increments
the counts corresponding to the new example and decrements the counts to the
oldest example in the window which is now forgotten. Each node in the tree
maintains the suﬃcient statistics. Periodically, the splitting-test is recomputed.
If a new test is chosen, the CVFDT starts growing an alternate sub-tree. The
old one is replaced only when the new one becomes more accurate.
Ultra-Fast Forest Trees - UFFT
UFFT is an algorithm for supervised classiﬁcation learning that generates a
forest of binary trees. The algorithm is incremental, processing each example in
constant time, and works online. UFFT is designed for continuous data. It uses
analytical techniques to choose the splitting criteria, and the information gain
to estimate the merit of each possible splitting-test. For multi-class problems,
the algorithm builds a binary tree for each possible pair of classes leading to a
forest-of-trees. During the training phase the algorithm maintains a short term
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
memory. Given a data stream, a limited number of the most recent examples
are maintained in a data structure that supports constant time insertion and
deletion. When a test is installed, a leaf is transformed into a decision node with
two descendant leaves. The suﬃcient statistics of the leaf are initialized with the
examples in the short term memory that will fall at that leaf. The UFFT has
shown good results with several large and medium size problems. In this work
we incorporate in UFFT system the ability to support Concept Drift Detection.
To detect concept drift we maintain, at each inner node, a naive-Bayes classiﬁer trained with the examples that traverse the node. Statistical theory
guarantees that for stationary distribution of the examples, the online error
of naive-Bayes will decrease; when the distribution function of the examples
changes, the online error of the naive-Bayes at the node will increase. In that
case we decide that the test installed at this node is not appropriate for the actual distribution of the examples. When this occurs the sub-tree rooted at this
node will be pruned. The algorithm forgets the suﬃcient statistics and learns
the new concept with only the examples in the new concept. The drift detection method will always check the stability of the distribution function of the
examples at each decision node. In the following sections we provide detailed
information about the most relevant aspects of the system.
Algorithm Details
The Splitting Criteria
The UFFT starts with a single leaf. When a splitting test is installed at a leaf,
the leaf becomes a decision node, and two descendant leaves are generated. The
splitting test has two possible outcomes each conducting to a diﬀerent leaf. The
value True is associated with one branch and the value False, with the other. The
splitting tests are over a numerical attribute and are of the form attributei ≤
valuej. We use the analytical method for split point selection presented in .
We choose, for all numerical attributes, the most promising valuej. The only
suﬃcient statistics required are the mean and variance per class of each numerical
attribute. This is a major advantage over other approaches, as the exhaustive
method used in C4.5 and in VFDTc , because all the necessary statistics
are computed on the ﬂy. This is a desirable property on the treatment of huge
data streams because it guarantees constant time processing each example.
The analytical method uses a modiﬁed form of quadratic discriminant analysis to include diﬀerent variances on the two classes1. This analysis assumes that
the distribution of the values of an attribute follows a normal distribution for
both classes. Let φ(¯x, σ) =
be the normal density function,
1 The reader should note that in UFFT any n-class problem is decomposed into n(n−
1)/2 two-class problems.
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
where ¯x and σ2 are the sample mean and variance of the class. The class mean
and variance for the normal density function are estimated from the sample set
of examples at the node. The quadratic discriminant splits the X-axis into three
intervals (−∞, d1), (d1, d2), (d2, ∞) where d1 and d2 are the possible roots of
the equation p(−)φ{(¯x−, σ−)} = p(+)φ{(¯x+, σ+)} where p(i) denotes the estimated probability than an example belongs to class i. We pretend a binary
split, so we use the root closer to the sample means of both classes. Let d be
that root. The splitting test candidate for each numeric attribute i will be of
the form Atti ≤di. To choose the best splitting test from the candidate list we
use an heuristic method. We use the information gain to choose, from all the
splitting point candidates, the best splitting test. To compute the information
gain we need to construct a contingency table with the distribution per class of
the number of examples lesser and greater than di:
Atti ≤di Atti > di
The information kept by the tree is not suﬃcient to compute the exact number
of examples for each entry in the contingency table. Doing that would require to
maintain information about all the examples at each leaf. With the assumption
of normality, we can compute the probability of observing a value less or greater
than di. From these probabilities and the distribution of examples per class at
the leaf we populate the contingency table. The splitting test with the maximum
information gain is chosen. This method only requires that we maintain the mean
and standard deviation for each class per attribute. Both quantities are easily
maintained incrementally. In the authors presented an extension to VFDT to
deal with continuous attributes. They use a Btree to store continuous attributevalues with complexity O(nlog(n)). The complexity of the method proposed here
is O(n). This is why we denote our algorithm as ultra-fast. Once the merit of
each splitting has been evaluated, we have to decide on the expansion of the
tree. This problem is discussed in the next section.
From Leaf to Decision Node
To expand the tree, a test attributei ≤di is installed in a leaf, and the leaf
becomes a decision node with two new descendant leaves. To expand a leaf two
conditions must be satisﬁed. The ﬁrst one requires the information gain of the
selected splitting test to be positive. That is, there is a gain in expanding the leaf
against not expanding. The second condition, it must exist statistical support in
favor of the best splitting test which is asserted using the Hoeﬀding bound as in
VFDT . When new nodes are created, the short term memory is used. This
is described in the following section.
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
Short Term Memory
The short term memory maintains a limited number of the most recent examples.
These examples are used to update the statistics at the new leaves when they are
created. The examples in the short term memory traverse the tree. Only those
that reach the new leaves will update the suﬃcient statistics of the tree. The data
structure used in our algorithm supports constant time insertion of elements at
the beginning of the sequence and constant time removal of elements at the end
of the sequence.
Functional Leaves
To classify an unlabeled example, the example traverses the tree from the root
to a leaf. It follows the path established, at each decision node, by the splitting
test at the appropriate attribute-value. The leaf reached classiﬁes the example.
The classiﬁcation method is a naive-Bayes classiﬁer. The use of the naive-Bayes
classiﬁers at the tree leaves does not enter any overhead in the training phase.
At each leaf we maintain suﬃcient statistics to compute the information gain.
These are the necessary statistics to compute the conditional probabilities of
P(xi|Class) assuming that the attribute values follow, for each class, a normal
distribution. Let l be the number of attributes, and φ(¯x, σ) denotes the standard
normal density function for the values of attribute i that belong to a given class.
Assuming that the attributes are independent given the class, the Bayes rule
will classify an example in the class that maximizes the a posteriori conditional
probability, given by: P(Ci|x) ∝log(Pr(Ci)) + l
k=1 log(φ(¯xi
k)). There is a
simple motivation for this option. UFFT only changes a leaf to a decision node
when there is a suﬃcient number of examples to support the change. Usually
hundreds or even thousands of examples are required. To classify a test example,
the majority class strategy only use the information about class distributions and
does not look for the attribute-values. It uses only a small part of the available
information, a crude approximation to the distribution of the examples. On the
other hand, naive-Bayes takes into account not only the prior distribution of the
classes, but also the conditional probabilities of the attribute-values given the
class. In this way, there is a much better exploitation of the information available
at each leaf .
Forest of Trees
The splitting criterion only applies to two class problems. Most of real-world
problems are multi-class. In the original paper and for a batch-learning scenario, this problem was solved using, at each decision node, a 2-means cluster algorithm to group the classes into two super-classes. Obviously, the cluster method can not be applied in the context of learning from data streams.
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
We propose another methodology based on round-robin classiﬁcation . The
round-robin classiﬁcation technique decomposes a multi-class problem into k binary problems, that is, each pair of classes deﬁnes a two-class problem. In 
the author shows the advantages of this method to solve n-class problems. The
UFFT algorithm builds a binary tree for each possible pair of classes. For example, in a three class problem (A,B, and C) the algorithm grows a forest of binary
trees, one for each pair: A-B, B-C, and A-C. In the general case of n classes,
the algorithm grows a forest of
binary trees. When a new example is
received during the tree growing phase each tree will receive the example if the
class attached to it is one of the two classes in the tree label. Each example is
used to train several trees and neither tree will get all examples. The short term
memory is common to all trees in the forest. When a leaf in a particular tree
becomes a decision node, only the examples corresponding to this tree are used
to initialize the new leaves.
Fusion of Classiﬁers
When doing classiﬁcation of a test example, the algorithm sends the example to
all trees in the forest. The example will traverse the tree from root to leaf and
the classiﬁcation is registered. Each tree in the forest makes a prediction. This
prediction takes the form of a probability class distribution. Taking into account
the classes that each tree discriminates, these probabilities are aggregated using
the sum rule . The most probable class is used to classify the example. Note
that some examples will be forced to be classiﬁed erroneously by some of the binary base classiﬁers, because each classiﬁer must label all examples as belonging
to one of the two classes it was trained on.
Functional Inner-nodes
When evaluating the splitting-criteria the merit of the best attributes could be
closed enough that the diﬀerence in gains does not satisfy the Hoeﬀding bound.
This aspect has been pointed out in . In VFDT, the authors propose the use
of a user deﬁned constant, τ, that can decide towards a split (given that ϵ < τ),
even when the Hoeﬀding bound is not satisﬁed. In UFFT when there is a tie in
the evaluation of the merit of tests based on single attributes, the system starts
trying more complex splitting tests .
As we have shown, the suﬃcient statistics for the splitting-criteria can be
directly used to construct a naive-Bayes classiﬁer. The idea of functional inner
nodes is to install splitting-tests based on the predictions of the naive-Bayes
classiﬁer build at that node.
Suppose that we observe a leaf where the diﬀerence in gain between the two
best attributes does not satisﬁes the Hoeﬀding bound. Since the ﬁrst tie, when a
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
new training example falls at this leaf, it will be classiﬁed using the naive-Bayes
derived from the suﬃcient statistics. Those predictions are used to populate a
2 × 2 contingency table, where each cell nij contains the number of examples
from class i that naive Bayes predict class j.
In the next evaluation we evaluate also, in addiction to the evaluation of all
the original attributes, the information gain of the contingency table obtained
by the naive-Bayes predictions. This evaluation corresponds to consider a new
attribute: the naive-Bayes predictions. If this implicit attribute is the best attribute in terms of information gain, and the diﬀerence with respect to the second
best satisﬁes the Hoeﬀding bound, then the leaf becomes a decision node with
two outcomes: the naive-Bayes predictions. UFFT uses naive-Bayes classiﬁers at
leaves. When considering splitting-tests based on naive-Bayes we must consider
the advantage of splitting versus not splitting. For example, if the predictions of
naive-Bayes are accurate, the corresponding gain will be high. In such cases, we
don’t need to expand the leaf, avoiding too much structure and overﬁtting. After
a ﬁrst tie, we only expand a leaf, if the gain of the naive-Bayes predictions is less
than a user deﬁned threshold. In the experiments described below the threshold
was set to 0.5. Naive Bayes classiﬁers use all attributes to make predictions. This
aspect could be negative in the presence of irrelevant attributes. In UFFT we
only consider splitting-tests based on naive-Bayes classiﬁers after the ﬁrst tie.
This aspect can be used to select the most informative attributes to use with
naive-Bayes.
Concept Drift Detection
The UFFT algorithm maintains, at each node of all decision trees, a naive-
Bayes classiﬁer. Those classiﬁers were constructed using the suﬃcient statistics
needed to evaluate the splitting criteria when that node was a leaf. When the leaf
becomes a node the naive-Bayes classiﬁer will classify the examples that traverse
the node. The basic idea of the drift detection method is to control this online
error-rate. If the distribution of the examples that traverse a node is stationary,
the error rate of naive-Bayes decreases. If there is a change on the distribution
of the examples the naive-Bayes error will increase . When the system detect
an increase of the naive-Bayes error in a given node, an indication of a change
in the distribution of the examples, this suggest that the splitting-test that has
been installed at this node is no longer appropriate. In such cases, all the subtree
rooted at that node is pruned, and the node becomes a leaf. All the suﬃcient
statistics of the leaf are initialized using the examples in the new context from
the short term memory. We designate as context a set of contiguous examples
where the distribution is stationary, assuming that the data stream is a set of
contexts. The goal of the method is to detect when in the sequence of examples
of the data stream there is a change from one context to another.
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
When a new training example becomes available, it will cross the corresponding binary decision trees from the root node till a leaf. At each node, the naive
Bayes installed at that node classiﬁes the example. The example will be correctly
or incorrectly classiﬁed. For a set of examples the error is a random variable from
Bernoulli trials. The Binomial distribution gives the general form of the probability for the random variable that represents the number of errors in a sample
of n examples. We use the following estimator for the true error of the classiﬁcation function pi ≡(errori/i) where i is the number of examples and errori
is the number of examples misclassiﬁed, both measured in the actual context.
The estimate of error has a variance. The standard deviation for a Binomial is
given by si ≡
, where i is the number of examples observed within the
present context. For suﬃcient large values of the example size, the Binomial distribution is closely approximated by a Normal distribution with the same mean
and variance. Considering that the probability distribution is unchanged when
the context is static, then the 1 −α/2 conﬁdence interval for p with n > 30 examples is approximately pi ± α ∗si. The parameter α depends on the conﬁdence
level. In our experiments the conﬁdence level for drift has been set to 99%. The
drift detection method manages two registers during the training of the learning
algorithm, pmin and smin. Every time a new example i is processed those values
are updated when pi + si is lower than pmin + smin.
We use a warning level to deﬁne the optimal size of the context window. The
context window will contain the old examples that are on the new context and
a minimal number of examples on the old context. Suppose that in the sequence
of examples that traverse a node, there is an example i with correspondent pi
and si. The warning level is reached if pi +si ≥pmin +1.5 ∗smin. The drift level
is reached if pi +si ≥pmin +3∗smin. Suppose a sequence of examples where the
naive-Bayes error increases reaching the warning level at example kw, and the
drift level at example kd. This is an indicator of a change in the distribution of
the examples. A new context is declared starting in example kw, and the node is
pruned becoming a leaf. The suﬃcient statistics of the leaf are initialized with the
examples in the short term memory whose time stamp is greater than kw. It is
possible to observe an increase of the error reaching the warning level, followed
by a decrease. We assume that such situations corresponds to a false alarm,
without changing the context. With this method of learning and forgetting we
ensure a way to continuously keep a model better adapted to the present context.
The method uses the information already available to the learning algorithm and
does not require additional computational resources.
An advantage of this method is it continuously monitors the online error of
naive Bayes. It can detect changes in the class-distribution of the examples at
any time. All decision nodes contain naive Bayes to detect changes in the classdistribution of the examples that traverse the node, that correspond to detect
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
shifts in diﬀerent regions of the instance space. Nodes near the root should be
able to detect abrupt changes in the distribution of the examples, while deeper
nodes should detect smoothed changes.
All the main characteristics of UFFT are due to the splitting criteria. All
the statistics required by the splitting criteria can be computed incrementally.
Moreover we can directly derive naive Bayes classiﬁers from the suﬃcient statistics. Naive Bayes classiﬁers are used in leaves to classify test examples, are used
in inner decision nodes to detect drift and can be used in splitting tests. It is
known that naive Bayes is a low-variance classiﬁer. This property is relevant
mainly when the naive Bayes acts as splitting test and in the drift detection.
Experimental Work
Stationary Data
The experimental work has been done using the Waveform, LED and Balance
datasets available at the UCI repository . There are two Waveform problems,
both with three classes. The ﬁrst problem is deﬁned by 21 numerical attributes.
The second one contains 40 attributes. It is known that the optimal Bayes error
is 14%. The LED problem has 24 binary attributes (17 are irrelevant) and 10
classes. The optimal Bayes error is 26%. The Balance problem has 4 attributes
and 3 classes. The choice of these datasets was motivated by the existence of
dataset generators at the UCI repository that could simulate streams of data.
For all the problems we generate training sets of a varying number of examples, starting from 50k till 1500k. The test set contains 100k examples. UFFT
generates a model from the training set, seeing each example once. The generated model classiﬁes the test examples. The UFFT algorithm was used with
parameters values δ = 0.05, τ = 0.001, nmin = 300, and buﬀer size of 1000. All
algorithms ran on a Centrino at 1.5GHz with 512 MB of RAM and using Linux
For comparative purposes, we use C4.5, the state of the art in decision tree
learning. It is a batch algorithm that requires that all the data should ﬁt in memory. All the data is successively re-used at decision nodes in order to choose the
splitting test. At each decision node, continuous attributes should be sorted, an
operation with complexity of O(n log n). We conducted a set of experiments comparing UFFT against C4.5. Both algorithms learn on the same training dataset,
and the generated models are evaluated on the same test set. Detailed results
are presented in Table 1. UFFT is orders of magnitude faster that C4.5 generating simpler (in terms of the number of decision nodes) models, with similar
performance. The advantage of using multivariate splitting tests is evident in
waveform datasets. The diﬀerences in performance appears in columns Version
1 and Default in Table 1. In these datasets the observed improvement is about
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
Error Rate
Training Time
No Drift Version 1 Default
Balance dataset - 4 Attributes
41 315 1 1
822 1355 1 1
2949 2051 1 1 16223
Waveform dataset - 21 Attributes
43 83 65 33787
10802 113 73 1 61841
Waveform dataset - 40 Attributes
59 53 75 435233
45 49 1 80331
LED dataset - 24 Attributes
Table 1: Learning curves for the datasets under study. For UFFT we present the
results of three versions: disabling drift (No Drift), disabling the use of naive
Bayes in splitting tests (Version 1) and enabling the use of naive Bayes and drift
detection (Default). For Led dataset the ﬁgures of tree size refer to the mean of
all 45 trees.
5%. These datasets are generated by a stationary distribution. Nevertheless there
are signals of false alarms drift detection. They never appear in the root node
but in deeper nodes in the tree. The impact in performance is reduced or even
Non-Stationary Data
For illustrative purposes we evaluate UFFT in the SEA concepts, previously
used in to evaluate the ability to detect concept drift. Table 2(a) presents
the average error-rate of 30 runs of UFFT setting on/oﬀthe ability of drift
detection. The results are diﬀerent at a signiﬁcance level of 99%. They clear
indicate the beneﬁts of using drift detection in this dataset. For reference we
also present the results of CVFDT.
The Electricity Market Dataset was collected from the Australian NSW Elec-
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
(a)Sea Dataset
(b)Electricity market Dataset
Upper Bound
Drift No Drift
Bound All Data Last Year
Variance 1.26
Last Week 14.5
Table 2: Error-rates on drift detection problems.
tricity Market. In this market, the prices are not ﬁxed and are aﬀected by demand
and supply of the market. The prices in this market are set every ﬁve minutes.
The class label identiﬁes the change of the price related to a moving average of
the last 24 hours. The goal of the problem is to predict if the price will increase
or decrease. From the original dataset we design two experiments. In one of the
experiments, the test set is the last day (48 examples); in the other, the test
set is the last week (336 examples). For each problem, we detect a lower bound
and an upper bound of the error using a batch decision tree learner. The upper
bound use ad-hoc heuristics to choose the training set. One heuristic use all
the training data; the other heuristic use only the last year training examples.
When predicting the last day, the error-rate of Rpart2 using all the training set
is 18.7%, when restricting the training set to the last year, the error decrease
to 12.5%. To compute the lower-bound we perform an exhaustive search for the
best training set that produces lower error rate in the last day of the training
set. The results appear in Table 2(b).
The experiments using UFFT with drift detection exhibit a performance
similar to the lower-bound using exhaustive search. This is an indication of the
quality of the results. The advantage of using a drift detection method is the
ability to automatically choose the set of training examples. This is a real world
dataset where we do not know where the context is changing.
Conclusions and Future Work
This work presents an incremental learning algorithm appropriate for processing
high-speed numerical data streams. The main contributions of this work are the
ability to use multivariate splitting tests, and the ability to adapt the decision
model to concept drift. While the former has impact in the performance of the
system, the latter extends the range of applications to dynamic environments.
The UFFT system can process new examples as they arrive, performing
a single scan of the training data. The method to choose the cut point for
splitting tests is based on quadratic discriminant analysis. It has complexity
2 The version of Cart implemented in R .
Gama J., Medas P.: Learning Decision Trees from Dynamic Data Streams
O(#examples). The suﬃcient statistics required by the analytical method can
be computed in an incremental way, guaranteeing constant time to process each
example. This analytical method is restricted to two-class problems. We use a
forest of binary trees to solve problems with more than 2 classes. Other contributions of this work are the use of a short-term memory to initialize new leaves,
and the use of functional leaves to classify test cases.
An important aspect in this work, is the ability to detect changes in the
distribution of the examples. To detect concept drift, we maintain, at each inner
node, a naive-Bayes classiﬁer trained with the examples that cross the node.
While the distribution of the examples is stationary, the online error of naive-
Bayes will decrease. When the distribution changes, the naive-Bayes online error
will increase. In that case the test installed at this node is no more appropriate
for the actual distribution of the examples. When this occurs all the subtree
rooted at this node will be pruned. The pruning corresponds to forget older
examples. The empirical evaluation, using stationary data, shows that UFFT is
competitive to the state of the art in batch decision tree learning, using much
less computational resources. There are two main factors that justiﬁes the overall
good performance of the system. One is the use of more powerful classiﬁcation
strategies at tree leaves. The other is the ability to use multivariate splits. The
experimental results using non-stationary data, suggest that the system exhibit
fast reaction to changes in the concept to learn. The performance of the system
indicates that there is a good adaptation of the decision model to the actual
distribution of the examples. We should stress that the use of naive-Bayes classi-
ﬁers at leaves to classify test examples, the use of naive-Bayes as splitting-tests,
and the use of naive-Bayes classiﬁers at decision nodes to detect changes in the
distribution of the examples are directly obtained from the suﬃcient statistics
required to compute the splitting criteria, without no additional computations.
This aspect is a main advantage in the context of high-speed data streams.
Acknowledgments
This work was developed in the context of projects RETINAE, and ALESII
 .