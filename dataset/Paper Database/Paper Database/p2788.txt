Low-Shot Learning with Imprinted Weights
Matthew Brown
David G. Lowe
Human vision is able to immediately recognize novel visual categories after seeing just one or a few training examples. We describe how to add a similar capability to
ConvNet classiﬁers by directly setting the ﬁnal layer weights
from novel training examples during low-shot learning. We
call this process weight imprinting as it directly sets weights
for a new category based on an appropriately scaled copy of
the embedding layer activations for that training example.
The imprinting process provides a valuable complement to
training with stochastic gradient descent, as it provides immediate good classiﬁcation performance and an initialization for any further ﬁne-tuning in the future. We show how
this imprinting process is related to proxy-based embeddings. However, it differs in that only a single imprinted
weight vector is learned for each novel category, rather
than relying on a nearest-neighbor distance to training instances as typically used with embedding methods. Our experiments show that using averaging of imprinted weights
provides better generalization than using nearest-neighbor
instance embeddings.
1. Introduction
Human vision can immediately recognize new categories
after a person is shown just one or a few examples .
For instance, humans can recognize a new face from a photo
of an unknown person and new objects or ﬁne-grained categories from a few examples by implicitly drawing connections from previously acquired knowledge. Although
deep neural networks trained on millions of images have
in some cases exceeded human performance in large-scale
image recognition , under an open-world setting with
emerging new categories it remains a challenging problem
how to continuously expand the capability of an intelligent
agent from limited new samples, also known as low-shot
Embedding methods have a natural representation
for low-shot learning, as new categories can be added simply by pushing data examples through the network and per-
∗The majority of the work was done while interning at Google.
forming a nearest neighbor algorithm on the result .
It has long been realized in the semantic embedding literature that the activations of the penultimate layer of a
ConvNet classiﬁer can also be thought of as an embedding
vector, which is a connection we further develop in this paper. ConvNets are the preferred solution for achieving the
highest classiﬁcation performance, and the softmax crossentropy loss is faster to train than the objectives typically
used in embedding methods, such as triplet loss.
In this paper, we attempt to combine the best properties of ConvNet classiﬁers1 with embedding approaches for
solving the low-shot learning problem. Inspired by the use
of embeddings as proxies or agents for individual object classes, we argue that embedding vectors can be
effectively compared to weights in the last linear layer of
ConvNet classiﬁers. Our approach, called imprinting, is to
compute these activations from a training image for a new
object category and use an appropriately scaled version of
these activation values as the ﬁnal layer weights for the new
category while leaving the weights of existing categories
unchanged. This is extended to multiple training examples
by incrementally averaging the activation vectors computed
from the new training images, which our experiments ﬁnd
to outperform nearest-neighbor classiﬁcation as used with
embedding approaches.
We consider a low-shot learning scenario where a learner
initially trained on base classes with abundant samples is
then exposed to previously unseen novel classes with a limited amount of training data for each category . The goal
is to have a learner that performs well on the combined set
of classes. This setup aligns with human recognition which
continuously learns new concepts during a lifetime.
Existing approaches exhibit characteristics that render
them infeasible for resource-limited environments such as
mobile devices and robots. For example, training a deep
ConvNet classiﬁer with stochastic gradient descent requires
an extensive ﬁne-tuning process that cycles through all prior
training data together with examples from additional categories .
Alternatively, semantic embedding methods
1In this paper we use the term “ConvNet classiﬁers” to refer to convolutional neural networks trained with the softmax cross-entropy loss for
classiﬁcation tasks.
 
such as can immediately remember new
examples and use them for recognition without retraining.
However, semantic embeddings are difﬁcult to train due to
the computationally expensive hard-negative mining step
and these methods require storing all the embedding vectors of encountered examples at test time for nearest neighbor retrieval or classiﬁcation.
We demonstrate that the imprinted weights enable instant learning in low-shot object recognition with a single
new example. Moreover, since the resulting model after imprinting remains in the same parametric form as ConvNets,
ﬁne-tuning via backpropagation can be applied when more
training samples are available and when iterative optimization is affordable.
Experiments show that the imprinted
weights provide a better starting point than the usual random initialization for ﬁne-tuning all network weights and
result in better ﬁnal classiﬁcation results for low-shot categories. Our imprinting method provides a potential model
for immediate recognition in biological vision as well as a
useful approach for on-line updates for novel training data,
as in a mobile device or robot.
The remainder of the paper is organized as follows. In
Section 2, we discuss related work. Section 3 discusses the
connections between embedding training and classiﬁcation.
Section 4 describes our approach. Then we provide implementation details and evaluate our approach with experiments in Sections 5 and 6. Section 7 concludes the paper.
2. Related Work
Metric Learning. Metric learning has been successfully
used to recognize faces of new identities and ﬁnegrained objects . The idea is to learn
a mapping from inputs to vectors in an embedding space
where the inputs of the same identity or category are closer
than those of different identities or categories. Once the
mapping is learned, at test time a nearest neighbors method
can be used for retrieval and classiﬁcation for new categories that are unseen during training.
Contrastive loss minimizes the distances between inputs with the same label while keeping the distances between inputs with different labels far apart. Rather than
minimizing absolute distances, recent approaches formulate
objectives focusing on relative distances. FaceNet optimizes a triplet loss and develops an online negative mining strategy to form triplets within a mini-batch. Instead of
penalizing violating instance-based triplets independently,
alternative loss functions regulate the global structure of the
embedding space. Magnet loss optimizes the distribution of different classes by clustering the examples using kmeans and representing classes with centroids. Lifted structured loss incorporates all pair-wise relations within a
mini-batch instead of forming triplets. The N-pair loss 
requires each batch to have examples from N categories for
improved computational efﬁciency. All these methods require some online or ofﬂine batch generation step to form
informative batches to speed up training. Structured clustering loss optimizes a clustering quality metric globally
in the embeddings space.
The Proxy-NCA loss demonstrates faster convergence without requiring batch generation by assigning
trainable proxies to each category, which we will describe in
more detail in Section 3. NormFace explores a similar
idea with all feature vectors normalized. The embedding
can generalize to unseen categories, however the nearest
neighbor model needs to store the embeddings of all reference points during testing. In our work, we retain the
parametric form of ConvNet models and demonstrate that
semantic embeddings can be used to imprint weights in the
ﬁnal layer. As a result, our approach has the same convergence advantages as and during training, yet does
not require storing embeddings for each training example or
using nearest-neighbor search during inference.
One-shot and Low-shot Learning. One-shot or lowshot learning aims at training models with only one or a
few training examples. The siamese network uses two
network streams to extract features from a pair of images
and regress the inputs to a similarity score between two feature vectors. Matching networks learn a neural network
that maps a small support set of images from unseen categories and an unlabeled example to its label. Prototypical
networks use the mean embeddings of new examples as
prototypes, but the embedding space is local with respect to
the support classes due to the episodic scheme. These works
formulate the low-shot learning problem as classifying an
image among a number of unseen classes characterized by
the support images; a query image and a support set must
be provided together every time at inference. However, this
evaluation setup does not align with human vision and many
real-world applications where a learner grows its capability as it encounters more categories and training samples.
In contrast, we consider an alternative setup similar to 
which focuses on the overall performance of the learner on
a combined set of categories including base classes represented by abundant examples together with novel low-shot
classes. Hariharan and Girshick train a multi-layer perceptron to generate additional feature vectors from a single example by drawing an analogy with seen examples.
Their method retrains the last linear classiﬁer at the lowshot training stage, whereas our approach allows instant
performance gain on novel classes without retraining. More
similar to our work is , which trains parameter predictors for novel categories from activations. However, our
method directly imprints weights from activations, which is
made possible by architecture modiﬁcations that introduce
a normalization layer.
3. Metric Learning and Softmax Classiﬁers
In this section, we discuss the connection between a
proxy-based objective used in embedding training and softmax cross-entropy loss. Based on these observations, we
then describe our method for extending ConvNet classiﬁers
to new classes in the next section.
3.1. Proxy-based Embedding Training
Recent work has blurred the divide between triplet-based
embedding training and softmax classiﬁcation. For example, Neighborhood Components Analysis learns a distance metric with a softmax-like loss,
LNCA(x, y, Z) = −log
exp(−d(x, y))
z∈Z exp(−d(x, z))
which makes points x, y with the same label closer than examples z with different labels under the squared Euclidean
distance d(x, y) = ||x −y||2
2. Movshovitz-Attias et al. 
reformulated the loss by assigning proxies p(·) to training
examples according to the class labels
Lproxy(x) ≜LNCA(x, p(x), p(Z))
exp(−d(x, p(x)))
p(z)∈p(Z) exp(−d(x, p(z))),
where p(Z) is a set of all negative proxies. This formulation
allows sampling anchor points x, rather than triplets, for
each mini-batch and results in faster convergence than other
objectives.
3.2. Connections to Softmax Classiﬁers
We will now discuss the connections between metric
learning and softmax classiﬁers.
We consider the case
that each class has exactly one proxy and the proxy of
a data point is determined statically according to its label.
Concretely, let C be the set of category labels and
P = {p1, p2, . . . , p|C|} be the set of trainable proxies, then
the proxy of every point x is p(x) = pc(x) where c(x) ∈C
is the class label of x. We argue that the proxies pc are
comparable to weights wc in softmax classiﬁers.
To see this, we assume point vectors and proxy vectors
are normalized to the same length. It follows that minimizing the squared Euclidean distance between a point x and its
proxy p(x) is equivalent to maximizing the inner-product,
or equivalently cosine similarity, of the corresponding unit
min d(x, p(x)) ≜min ||x −p(x)||2
2 = max x⊤p(x), (3)
since ||u−v||2
2 = 2−2u⊤v for unit vectors u, v ∈RD. Substituting the squared Euclidean distance with inner product
in Eq. 2, the resulting loss can be written as
L(x, c(x)) = −log
exp(x⊤pc(x))
c∈C exp(x⊤pc),
Deep ConvNet
imprinting
Extended classiﬁer
Base classiﬁer
Embedding Extractor
Nomalization
Figure 1. The overall architecture of imprinting. After a base classiﬁer is trained, the embedding vectors of new low-shot examples
are used to imprint weights for new classes in the extended classi-
which is comparable to the softmax cross-entropy loss used
for training classiﬁers
Lsoftmax(x, c(x)) = −log exp(x⊤wc(x) + bc(x))
c∈C exp(x⊤wc + bc),
with bias terms bc = 0 for all c ∈C.
4. Imprinting
Given the conceptual similarity of normalized embedding vectors and ﬁnal layer weights as discussed above, it
seems natural that we should be able to set network weights
for a novel class immediately from a single exemplar. In
the following, we outline our proposed method to do this,
which we call imprinting. In essence, imprinting exploits
the symmetry between normalized inputs and weights in a
fully connected layer, copying the embedding activations
for a novel exemplar into a new set of network weights.
To demonstrate this method, we focus on a two-stage
low-shot classiﬁcation problem where a learner is trained on
a set of base classes with abundant training samples in the
ﬁrst stage and then grows its capability to additional novel
classes for which only one or a few examples are available
in the second stage.
4.1. Model Architecture
Our model consists of two parts. First, an embedding
extractor φ : RN →RD, parameterized by a convolutional
neural network, maps an input image x ∈RN to a Ddimensional embedding vector φ(x). Different from standard ConvNet classiﬁer architectures, we add an L2 normalization layer at the end of the embedding extractor so that
the output embedding has unit length, i.e. ||φ(x)||2 = 1.
Second, a softmax classiﬁer f(φ(x)) maps the embedding
into unnormalized logit scores followed by a softmax activation that produces a probability distribution across all
categories
fi(φ(x)) =
where wi is the i-th column of the weight matrix normalized
to unit length. No bias term is used in this layer.
We view each column of the weight matrix as a template
of the corresponding category. Unlike in where only
the embedding extractor part is used during test time with
the auxiliary proxies thrown away, we keep the entirety of
the network. In the forward pass, the last layer in our model
computes the inner product between the embedding of the
input image φ(x) and all the templates wi. With embeddings and templates normalized to unit lengths, the resulting prediction is equivalent to ﬁnding the nearest template
in the embedding space in terms of squared Euclidean distance
ˆy = arg max
c φ(x) = arg min
c∈C d(φ(x), wc).
Compared with non-parametric nearest neighbor models,
however, our classiﬁer only contains one template per class
rather than storing a large set of reference data points.
Normalization. Normalizing embeddings and columns
of the weight matrix in the last layer to unit lengths is an
important architectural design in our model.
Geometrically, normalized embeddings and weights lie on a highdimensional sphere. In contrast, existing deep neural networks normally encourage activations to have zero mean
and unit variance within mini-batches or layers for
optimization reasons while they do not address the scale differences between neuron activations and weights. In our
model, as a result of normalizing embeddings and columns
of the weight matrix, the magnitude differences do not affect the prediction as long as the angle between the normalized vectors remains the same, since the inner product
i φ(x) ∈[−1, 1] now measures cosine similarity. Recent
work in cosine normalization discusses a similar idea
of replacing the inner product with a cosine similarity for
bounded activations and stable training, while we arrive at
this design from a different direction. In particular, this establishes a symmetric relationship between normalized embeddings and weights, which enables us to treat them interchangeably.
Scale factor. The cosine similarity w⊤
i φ(x) ∈[−1, 1]
can prevent the normalized probability of the correct class
from reaching close to 1 when applying softmax activation.
For example, consider for an input x the inner product producing 1 for the correct category and producing the minimum possible value −1 for the incorrect categories, the normalized probability is p(yi|x) = e1/[e1 + (|C| −1)e−1] =
0.069, assuming a total of |C| = 100 categories. In consequence, it fails to produce a distribution close to the one-hot
encoding of the ground truth label and therefore imposes a
lower bound on the cross-entropy loss. This effect becomes
more severe as the number of categories increases. To alleviate this problem, we adapt a scaling factor in our model
as discussed by Wang et al. . Concretely, we modify
Eq. 6 by adding a trainable scalar s shared across all classes
to scale the inner product
fi(φ(x)) =
We also experimented with the option of using an adaptive
scale factor per class, but we did not observe signiﬁcant effects on classiﬁcation accuracy compared to our use of a
single global scale factor.
In summary, our model architecture is similar to standard ConvNet classiﬁers except for two differences. The
normalized embeddings and weights introduce a symmetric relationship that allows us to treat them interchangeably.
The scaled inner product at the ﬁnal layer enables training
the entire model with the cross-entropy loss in the same way
that standard ConvNet classiﬁers are trained. Next, we discuss how to extend such a classiﬁer to novel categories by
leveraging the symmetry between embeddings and weights.
4.2. Weight Imprinting
Inspired by the effectiveness of embeddings in retrieving and recognizing objects from unseen classes in metric
learning, our proposed imprinting method is to directly set
the ﬁnal layer weights for new classes from the embeddings
of training exemplars. Consider a single training sample
x+ from a novel class, our method computes the embedding φ(x+) and uses it to set a new column in the weight
matrix for the new class, i.e. w+ = φ(x+). Figure 1 illustrates this idea of extending the ﬁnal layer weight matrix
of a trained classiﬁer by imprinting additional columns for
new categories.
Intuitively, one can think of the imprinting operation as
remembering the semantic embeddings of low-shot examples as the templates for new classes. Figure 2 illustrates
the change of the decision boundaries after a new weight
column is imprinted. The underlying assumption is that test
examples from new classes are closer to the corresponding
training examples, even if only one or a few are observed,
than to instances of other classes in the embedding space.
Notably, this desired property coincides with metric learning objectives such as triplet loss. The proxy-based loss,
from which we have derived our method, upper bounds the
instance-based triplet loss .
Average embedding. If n > 1 examples {x(i)
available for a new class, we compute new weights by averaging the normalized embeddings ˜w+ = 1
i=1 φ(x(i)
Figure 2. Illustration of imprinting in the normalized embedding
space. (a) Before imprinting, the decision boundaries are determined by the trained weights. (b) With imprinting, the embedding
of an example (the yellow point) from a novel class deﬁnes a new
and re-normalizing the resulting vector to unit length w+ =
˜w+/|| ˜w+||. In practice, the averaging operation can also
be applied to the embeddings computed from the randomly
augmented versions of the original low-shot training examples.
Fine-tuning. Since our model architecture has the same
differentiable form as ordinary ConvNet classiﬁers, a ﬁnetuning step can be applied after new weights are imprinted.
The average embedding strategy assumes that examples
from each novel class have a unimodal distribution in the
embedding space. This may not hold for every novel class
since the learned embedding space could be biased towards features that are salient and discriminative among
base classes. However, ﬁne-tuning (using backpropagation
to further optimize the network weights) should move the
embedding space towards having unimodal distribution for
the new class.
5. Implementation Details
The implementation details are comparable to 
For training, all the convolutional layers are
initialized from ConvNet classiﬁers pre-trained on the
ImageNet dataset .
InceptionV1 is used in our
experiments. The parameters of the fully-connected layers
producing the embedding and unnormalized logit scores are
initialized randomly. L2 normalization is used for embedding vectors and weights in the last layer along the embedding dimension. Input images are resized to 256×256 and
cropped to 224×224. Intensity is scaled to [−1, 1]. During training, we augment inputs with random cropping and
random horizontal ﬂipping. The learning rate is 0.0001 for
pre-trained layers; a 10× multiplier is used for randomly
initialized layers. We apply exponential decay every four
epochs with decay rate 0.94. The RMSProp optimizer is
used with momentum 0.9. During testing, input patches are
cropped from the center.
6. Experiments
We empirically evaluate the classiﬁers containing imprinted weights. We ﬁrst describe the overall protocols, then
we present results on the CUB-200-2011 dataset.
6.1. Data Splits
The CUB-200-2011 dataset contains 200 ﬁnegrained categories of birds with 11,788 images. We use the
train/test split provided by the dataset. In addition, we treat
the ﬁrst 100 classes as base classes where all the training examples (about 30 images per class on average) from these
categories are used to train a base classiﬁer. The remaining
100 classes are treated as novel classes where only n examples from the training split are used for low-shot learning.
We experiment with a range of sizes n = 1, 2, 5, 10, 20 of
novel exemplars for the low-shot training split. During testing, the original test split that includes both base and novel
classes is used. We measure the top-1 classiﬁcation accuracy of the ﬁnal classiﬁer on all categories. To show the
effect of weight imprinting for low-shot categories, we also
report the performance on the test examples from the novel
classes only.
6.2. Models and Conﬁguration Variants
Imprinting. To obtain imprinted models, we compute
embeddings of novel examples and set novel weights in the
ﬁnal layer directly. When more than one novel example is
available for a class, the mean of the normalized embeddings is used. The basic conﬁguration (Imprinting) uses
only the novel examples in their original forms. Alternatively, we experiment with random augmentation (Imprinting+Aug). Five augmented versions are generated for each
novel example by random cropping and horizontal ﬂipping,
followed by averaging the embedded vectors. Both variants require only forward-pass computation of a trained embedding extractor without any iterative optimization. We
compare these imprinting variants against a model initialization consisting of random novel weights without ﬁnetuning (Rand-noFT), which also involves zero backpropagation. Random weights are generated with a Xavier uniform initializer .
Fine-tuning. To demonstrate that imprinted weights can
be used as better initializations than random values, we apply ﬁne-tuning to the imprinting model (Imprinting+FT)
and to the model with random novel weights (Rand+FT),
respectively.
In both cases, we ﬁne-tune the entire network end-to-end.
We use only low-shot examples from
novel classes in addition to all training examples from base
classes. When the distribution across all classes is imbalanced, we oversample the novel classes such that all the
classes are sampled uniformly for each mini-batch. Random data augmentation is also applied.
Rand-noFT2
Imprinting
Imprinting + Aug
Imprinting + FT
AllClassJoint
Generator + Classiﬁer 
Matching Networks 
Table 1. 200-way top-1 accuracy for novel-class examples in
CUB-200-2011.
Imprinting provides good immediate performance without ﬁne tuning. Adding data augmentation (Imprinting+Aug) does not give signiﬁcant further beneﬁt. The second
block of 3 rows shows the results of ﬁne tuning, for which the
imprinting initialization retains an advantage. This remains true
even when compared to training all classes from scratch (All-
ClassJoint). The ﬁnal 2 rows provide comparisons with previous
Imprinting
Imprinting + Aug
Imprinting + FT
AllClassJoint
Generator + Classiﬁer 
Matching Networks 
Table 2. 200-way top-1 accuracy measured across examples in all
classes (100 base plus 100 novel classes) of CUB-200-2011. Imprinting retains similar advantages for rapid learning and initialization of ﬁne-tuning as seen in Table 1.
Jointly-trained ConvNet classiﬁer. For comparison, we
train a ConvNet classiﬁer for base and novel classes jointly
without a separate low-shot learning phase (AllClassJoint).
The same data splits and preprocessing pipeline are used
as in the ﬁne-tuning cases. This model does not normalize
embeddings or weights.
Other low-shot methods.
We also apply the feature
generator and matching networks to our normalized
embeddings trained with the softmax loss for comparison.
6.3. Results
Tables 1 and 2 show the top-1 accuracy of 200-way classiﬁcation for novel examples and all examples in CUB-200-
2011, respectively. Without any backpropagation, the imprinted weights computed from one-shot examples instantly
provide good classiﬁcation performance: 21.26% on novel
classes and 44.75% on all classes. Imprinting using the average of multiple augmented exemplars (Imprinting+Aug),
using the same random ﬂips and crops as for base class
training, does not give a signiﬁcant improvement in perfor-
2Rand-noFT is listed for easy comparison. Strictly, the header n =
1, . . . , 20 does not apply, since low-shot examples are not used.
Top-1 Accuracy (%)
Imprinting (1 shot)
Imprinting (2 shot)
Imprinting (5 shot)
Imprinting (10 shot)
Imprinting (20 shot)
Rand (1 shot)
Rand (2 shot)
Rand (5 shot)
Rand (10 shot)
Rand (20 shot)
Figure 3. Accuracy of ﬁne-tuned models on novel classes for the
ﬁrst 40 epochs of training. Table 1 lists results after 112 epochs.
Top-1 Accuracy (%)
Imprinting (1 shot)
Imprinting (2 shot)
Imprinting (5 shot)
Imprinting (10 shot)
Imprinting (20 shot)
Rand (1 shot)
Rand (2 shot)
Rand (5 shot)
Rand (10 shot)
Rand (20 shot)
Figure 4. Accuracy of ﬁne-tuned models measured over all classes
(100 base plus 100 novel classes) for the ﬁrst 40 epochs of training.
Table 2 lists results after 112 epochs.
mance. We conjecture this is because the embedding extractor has been trained on the base classes to be invariant
to the applied transformations.
When ﬁne-tuning the network weights with backpropagation, models initialized with imprinted weights (Imprint-
Predictions
categories
Predictions
Figure 5. (a) A subset of exemplars used for 1-shot training of novel classes sorted by their recall@1 scores as shown below each exemplar.
High-performing categories tend to exhibit more distinctive colors, shapes, and/or textures. (b) Randomly selected success and failure cases
predicted by a 1-shot imprinted model on CUB-200-2011. Test images and the 1-shot exemplar whose embedding was used to imprint the
predicted class are shown in separate rows. Correct and wrong predictions are marked with green and red borders, respectively.
ing+FT) take less time to converge and achieve better ﬁnal accuracies than randomly initialized models, especially
when limited low-shot examples are used. Figures 3 and 4
plot evaluation accuracy of the ﬁne-tunned models in the
ﬁrst 40 epochs on novel classes and all classes, respectively.
Accuracies in Tables 1 and 2 are recorded after
around 112 epochs. For cases n = 1, 2, the performance of
imprinted weights is close to saturation and ﬁne-tuning for
more epochs can lead to degraded evaluation accuracies on
novel classes, which we conjecture is due to overﬁtting on
the 1 or 2 examples. The results show that the imprinted initialization can lead to better results for low-shot categories
even when training from scratch on the entire dataset, as
with AllClassJoint.
The classiﬁer using generated features has a similar
performance to imprinting for n = 1. While the matching
network outperforms the feature generator as n increases,
we observe a performance gap when compared with imprinting. For our tests we modiﬁed the matching network
to perform 200-way classiﬁcation instead of 5-way .
Figure 5 shows some sampled results following training of novel categories from the 1-shot imprinted model
on CUB-200-2011. The top row shows randomly selected
novel categories sorted by their classiﬁcation accuracy as
given below each exemplar.
As might be expected, the
highest-performing categories tend to exhibit more distinctive features of color, texture, and/or shape. In Figure 5(b)
we show randomly selected success and failure cases predicted by the 1-shot imprinted model. The learned embeddings demonstrate an ability to generalize to different viewpoints, poses, and backgrounds from the single training example for each new category.
Transfer Learning with Imprinted Weights. We show
that imprinting beneﬁts transfer learning in general.
transfer a trained classiﬁer to a new set of classes, we substitute the ﬁnal layer parameters with the mean embeddings of
examples from new classes. The only difference between
our approach and standard transfer learning approaches is
that we initialize the new weights by imprinting rather than
with random values. Note that the imprinting process requires little cost in terms of computation. Table 3 shows
the top-1 classiﬁcation accuracy of the imprinted model on
the new classes. Random initialization yields an accuracy
of 0.85% while the models using imprinted weights have
accuracies from 26.76% up to 52.25% as the number of
training examples increases.
Applying random augmentation (Imprinting+Aug) does not impact the performance
signiﬁcantly. Additional ﬁne-tuning improves the performance. When novel training data is scarce (n = 1, 2, 5),
starting from the imprinted weights (Imprinting+FT) outperforms ﬁne-tuning from random weights (Rand+FT) by
a large margin. With more training examples, ﬁne-tuning
from imprinted weights converges to similar accuracy as
when starting from random weights.
Comparison with Nearest Neighbors. As discussed in
Section 2, the usual approach used in metric learning has
been to store all exemplar embeddings and use the nearest
neighbor algorithm for classiﬁcation. Therefore, we comn =
Imprinting
Imprinting + Aug
Imprinting + FT
Table 3. Top-1 accuracy for transfer learning on CUB-200-2011
using 1–20 examples for computing imprinted weights. The imprinted weights provide good immediate performance while also
providing better ﬁnal classiﬁcation accuracy for 1 to 5 shot learning following ﬁne tuning.
Examples per novel class
Top-1 Accuracy (%)
Imprinting (softmax loss)
ProxyNCA (softmax + NN)
Imprinting (LiftedStruct loss)
LiftedStruct + NN
Figure 6. Top-1 accuracy of 100-way classiﬁcation on novel
classes of CUB-200-2011. Imprinting averaged embeddings with
a softmax loss (blue bars) outperforms storing all individual embeddings with a nearest-neighbor classiﬁer (green). By comparison, embeddings trained with the lifted structured loss do not perform as well as with the softmax loss (red and pink).
pare our approach of using averaged embeddings with using a nearest-neighbor classiﬁer where the embeddings of
n low-shot training examples from each novel class form
the population set. When there is only one training example per class, n = 1, the imprinted classiﬁer is equivalent
to the nearest neighbor classiﬁer. When n > 1, the size
of the imprinted classiﬁer remains constant, whereas the
size of the nearest neighbor classiﬁer grows linearly as n
increases. Note that storing all embeddings trained with the
softmax loss in a nearest-neighbor classiﬁer is equivalent to
a special case of Proxy-NCA using one proxy per class.
Perhaps surprisingly, the averaged embeddings perform
better than storing all individual embeddings (Figure 6). We
conjecture that the averaging operation reduces potentially
noisy dimensions in the embedding to focus on those that
are more consistent for that category. Although the averaging may not seem to be the optimal choice in cases where
the distribution of novel class examples has multiple modalities in the embedding space, we do not observe this in our
experiments. When the embedding space is ﬁrst trained on
the base classes, lower layers of the network will have been
trained to bring multiple modalities together for feature in-
Dimension of embeddings
Top-1 Accuracy (%)
AllClassJoint
Imprinting
Dimension of embeddings
Top-1 Accuracy (%)
AllClassJoint
Imprinting
Figure 7. Classiﬁcation accuracies for Imprinting and All-
ClassJoint with different embedding dimensionalities under 1-shot
and 5-shot settings, respectively.
puts to the ﬁnal linear layer. Moreover, keeping a single
embedding for each class in the imprinted classiﬁer has additional beneﬁts since this standard form allows ﬁne-tuning
the embedding space with backpropagation and reduces test
time computation and memory requirements.
Comparison with Lifted Structured Loss. Imprinted
weights and Proxy-NCA are both trained with the softmax
cross-entropy loss. Alternatively, we compare with embeddings trained with the lifted structured loss , which is
a generalization of the widely used triplet loss. Figure 6
shows that the softmax loss performs better in our experiments than the lifted structured loss. However, the lifted
structured loss can also beneﬁt from imprinting averaged
embeddings rather than a nearest-neighbor classiﬁer.
Embedding Dimensionality.
We use 64-dimensional
embeddings in all the experiments above. Empirically we
experimented with various settings D = 64, 128, 256, 512
for the imprinting model and the jointly-trained ConvNet
Classiﬁer (Figure 7). Increasing the dimensionality does
not appear to have signiﬁcant effects on the results.
7. Conclusions
This paper has presented a new method, weight imprinting, that directly sets the ﬁnal layer weights of a ConvNet
classiﬁer for novel low-shot categories. This is a valuable
complement to stochastic gradient descent, as it provides
instant good classiﬁcation performance on novel categories
while allowing for further ﬁne tuning when time permits.
The key change that is made to the ConvNet architecture
is a normalization layer with a scaling factor that allows
activations computed for novel training examples to be directly copied (imprinted) as ﬁnal layer weights. When multiple low-shot examples are presented, the computed activations for additional examples are averaged with the existing weights, which our experiments show to perform better
than the nearest-neighbor approach typically used with embedding methods. An area for future research is whether
the imprinting approach can also be used for more rapid
training of other network layers, such as when encountering novel lower-level features.