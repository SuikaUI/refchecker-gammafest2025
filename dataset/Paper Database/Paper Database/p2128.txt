Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 968–988
August 1–6, 2021. ©2021 Association for Computational Linguistics
A Survey of Data Augmentation Approaches for NLP
Steven Y. Feng∗, 1 Varun Gangal∗, 1 Jason Wei†, 2 Sarath Chandar,3
Soroush Vosoughi,4 Teruko Mitamura,1 Eduard Hovy1
1Carnegie Mellon University, 2Google Research
3Mila - Quebec AI Institute, 4Dartmouth College
{syfeng,vgangal,teruko,hovy}@cs.cmu.edu
 
 
 
Data augmentation has recently seen increased
interest in NLP due to more work in lowresource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data.
Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language
data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in
a structured manner. We ﬁrst introduce and
motivate data augmentation for NLP, and then
discuss major methodologically representative
approaches.
Next, we highlight techniques
that are used for popular NLP applications and
tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape
of existing literature in data augmentation for
NLP and motivate additional work in this area.
We also present a GitHub repository with a paper list that will be continuously updated at
 
Introduction
Data augmentation (DA) refers to strategies for increasing the diversity of training examples without
explicitly collecting new data. It has received active
attention in recent machine learning (ML) research
in the form of well-received, general-purpose techniques such as UDA (3.1) and
MIXUP (3.2). These are often
ﬁrst explored in computer vision (CV), and DA’s
adaptation for natural language processing (NLP)
seems secondary and comparatively underexplored,
perhaps due to challenges presented by the discrete
nature of language, which rules out continuous
noising and makes it hard to maintain invariance.
∗Equal contribution by the two authors.
† AI Resident.
Figure 1: Weekly Google Trends scores for the search
term "data augmentation", with a control, uneventful
ML search term ("minibatch") for comparison.
Despite these challenges, there has been increased interest and demand for DA for NLP. As
NLP grows due to off-the-shelf availability of large
pretrained models, there are increasingly more
tasks and domains to explore. Many of these are
low-resource, and have a paucity of training examples, creating many use-cases for which DA can
play an important role. Particularly, for many nonclassiﬁcation NLP tasks such as span-based tasks
and generation, DA research is relatively sparse
despite their ubiquity in real-world settings.
Our paper aims to sensitize the NLP community
towards this growing area of work, which has also
seen increasing interest in ML overall (as seen in
Figure 1). As interest and work on this topic continue to increase, this is an opportune time for a
paper of our kind to (i) give a bird’s eye view of
DA for NLP, and (ii) identify key challenges to
effectively motivate and orient interest in this area.
To the best of our knowledge, this is the ﬁrst survey
to take a detailed look at DA methods for NLP.1
This paper is structured as follows.
1Liu et al. present a smaller-scale text data augmentation survey that is concise and focused. Our work serves
as a more comprehensive survey with larger coverage and is
more up-to-date.
2 discusses what DA is, its goals and trade-offs,
and why it works. Section 3 describes popular
methodologically representative DA techniques for
NLP—which we categorize into rule-based (3.1),
example interpolation-based (3.2), or model-based
(3.3). Section 4 discusses useful NLP applications
for DA, including low-resource languages (4.1),
mitigating bias (4.2), ﬁxing class imbalance (4.3),
few-shot learning (4.4), and adversarial examples
(4.5). Section 5 describes DA methods for common
NLP tasks including summarization (5.1), question
answering (5.2), sequence tagging tasks (5.3), parsing tasks (5.4), grammatical error correction (5.5),
neural machine translation (5.6), data-to-text NLG
(5.7), open-ended and conditional text generation
(5.8), dialogue (5.9), and multimodal tasks (5.10).
Finally, Section 6 discusses challenges and future
directions in DA for NLP. Appendix A lists useful
blog posts and code repositories.
Through this work, we hope to emulate past papers which have surveyed DA methods for other
types of data, such as images , faces , and time
series . We hope to draw
further attention, elicit broader interest, and motivate additional work in DA, particularly for NLP.
Background
What is data augmentation? Data augmentation
(DA) encompasses methods of increasing training
data diversity without directly collecting more data.
Most strategies either add slightly modiﬁed copies
of existing data or create synthetic data, aiming for
the augmented data to act as a regularizer and reduce overﬁtting when training ML models . DA has been commonly used in
CV, where techniques like cropping, ﬂipping, and
color jittering are a standard component of model
training. In NLP, where the input space is discrete,
how to generate effective augmented examples that
capture the desired invariances is less obvious.
What are the goals and trade-offs?
challenges associated with text, many DA techniques for NLP have been proposed, ranging from
rule-based manipulations to
more complicated generative approaches . As DA aims to provide an alternative to
collecting more data, an ideal DA technique should
be both easy-to-implement and improve model performance. Most offer trade-offs between these two.
Rule-based techniques are easy-to-implement
but usually offer incremental performance improvements . Techniques leveraging trained models may be more costly to implement but introduce
more data variation, leading to better performance
boosts. Model-based techniques customized for
downstream tasks can have strong effects on performance but be difﬁcult to develop and utilize.
Further, the distribution of augmented data
should neither be too similar nor too different from
the original. This may lead to greater overﬁtting
or poor performance through training on examples
not representative of the given domain, respectively.
Effective DA approaches should aim for a balance.
Divergence-based unsupervised procedure to preemptively choose among DA heuristics, rather than
a typical "run-all-heuristics" comparison, which
can be very time and cost intensive.
Interpretation of DA
Dao et al. note that
"data augmentation is typically performed in an adhoc manner with little understanding of the underlying theoretical principles", and claim the typical
explanation of DA as regularization to be insufﬁcient. Overall, there indeed appears to be a lack of
research on why exactly DA works. Existing work
on this topic is mainly surface-level, and rarely
investigates the theoretical underpinnings and principles. We discuss this challenge more in §6, and
highlight some of the existing work below.
Bishop show training with noised examples is reducible to Tikhonov regularization (subsumes L2). Rajput et al. show that DA can
increase the positive margin for classiﬁers, but only
when augmenting exponentially many examples
for common DA methods.
Dao et al. think of DA transformations
as kernels, and ﬁnd two ways DA helps: averaging
of features and variance regularization. Chen et al.
 show that DA leads to variance reduction
by averaging over orbits of the group that keep the
data distribution approximately invariant.
Techniques & Methods
We now discuss some methodologically representative DA techniques which are relevant to all tasks
via the extensibility of their formulation.2
2Table 1 compares several DA methods by various aspects
relating to their applicability, dependencies, and requirements.
Figure 2: Dependency tree morphing DA applied to a
Turkish sentence, ¸Sahin and Steedman 
Rule-Based Techniques
Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model
components. Feature space DA approaches generate augmented examples in the model’s feature
space rather than input data. Many few-shot learning approaches leverage estimated feature
space "analogy" transformations between examples of known classes to augment for novel classes
(see §4.4).
Paschali et al. use iterative
afﬁne transformations and projections to maximally
"stretch" an example along the class-manifold.
Wei and Zou propose EASY DATA AUG-
MENTATION (EDA), a set of token-level random
perturbation operations including random insertion,
deletion, and swap. They show improved performance on many text classiﬁcation tasks. UDA show how supervised DA methods can
be exploited for unsupervised data through consistency training on (x, DA(x)) pairs.
For paraphrase identiﬁcation, Chen et al. 
construct a signed graph over the data, with individual sentences as nodes and pair labels as signed
edges. They use balance theory and transitivity
to infer augmented sentence pairs from this graph.
Motivated by image cropping and rotation, ¸Sahin
and Steedman propose dependency tree morphing. For dependency-annotated sentences, children of the same parent are swapped (à la rotation)
or some deleted (à la cropping), as seen in Figure 2.
This is most beneﬁcial for language families with
rich case marking systems (e.g. Baltic and Slavic).
Example Interpolation Techniques
Another class of DA techniques, pioneered by
MIXUP , interpolates the inputs and labels of two or more real examples. This
class of techniques is also sometimes referred to as
Mixed Sample Data Augmentation (MSDA). Ensuing work has explored interpolating inner components ,
more general mixing schemes , and
adding adversaries .
Another class of extensions of MIXUP which has
been growing in the vision community attempts to
fuse raw input image pairs together into a single
input image, rather than improve the continuous interpolation mechanism. Examples of this paradigm
include CUTMIX , CUTOUT and COPY-PASTE . For instance, CUTMIX replaces a
small sub-region of Image A with a patch sampled
from Image B, with the labels mixed in proportion
to sub-region sizes. There is potential to borrow
ideas and inspiration from these works for NLP,
e.g. for multimodal work involving both images
and text (see "Multimodal challenges" in §6).
A bottleneck to using MIXUP for NLP tasks
was the requirement of continuous inputs. This has
been overcome by mixing embeddings or higher
hidden layers . Later variants
propose speech-tailored mixing schemes and interpolation with adversarial
examples , among others.
SEQ2MIXUP generalizes
MIXUP for sequence transduction tasks in two
ways - the "hard" version samples a binary mask
(from a Bernoulli with a β(α, α) prior) and picks
from one of two sequences at each token position,
while the "soft" version softly interpolates between
sequences based on a coefﬁcient sampled from
β(α, α). The "soft" version is found to outperform
the "hard" version and earlier interpolation-based
techniques like SWITCHOUT .
Model-Based Techniques
Seq2seq and language models have also been used
for DA. The popular BACKTRANSLATION method
 translates a sequence into
another language and then back into the original
language. Kumar et al. train seq2seq models with their proposed method DiPS which learns
to generate diverse paraphrases of input text using
a modiﬁed decoder with a submodular objective,
Figure 3: Contextual Augmentation, Kobayashi 
and show its effectiveness as DA for several classi-
ﬁcation tasks. Pretrained language models such as
RNNs and transformers have also been used for augmentation.
Kobayashi generate augmented examples
by replacing words with others randomly drawn
according to the recurrent language model’s distribution based on the current context (illustration in Figure 3). Yang et al. propose G-
DAUGc which generates synthetic examples using
pretrained transformer language models, and selects the most informative and diverse set for augmentation. Gao et al. advocate retaining the
full distribution through "soft" augmented examples, showing gains on machine translation.
Nie et al. augment word representations
with a context-sensitive attention-based mixture of
their semantic neighbors from a pretrained embedding space, and show its effectiveness for NER
on social media text. Inspired by denoising autoencoders, Ng et al. use a corrupt-andreconstruct approach, with the corruption function
q(x′|x) masking an arbitrary number of word positions and the reconstruction function r(x|x′) unmasking them using BERT .
Their approach works well on domain-shifted test
sets across 9 datasets on sentiment, NLI, and NMT.
Feng et al. propose a task called SEMAN-
TIC TEXT EXCHANGE (STE) which involves adjusting the overall semantics of a text to ﬁt the
context of a new word/phrase that is inserted called
the replacement entity (RE). They do so by using a
system called SMERTI and a masked LM approach.
While not proposed directly for DA, it can be used
as such, as investigated in Feng et al. .
Rather than starting from an existing example and modifying it, some model-based DA approaches directly estimate a generative process
from the training set and sample from it. Anaby-
Tavor et al. learn a label-conditioned generator by ﬁnetuning GPT-2 
on the training data, using this to generate candidate examples per class. A classiﬁer trained on the
original training set is then used to select top k candidate examples which conﬁdently belong to the
respective class for augmentation. Quteineh et al.
 use a similar label-conditioned GPT-2 generation method, and demonstrate its effectiveness
as a DA method in an active learning setup.
Other approaches include syntactic or controlled
paraphrasing , document or story-level paraphrasing , augmenting misclassiﬁed examples , BERT cross-encoder
labeling of new inputs , and
guided generation using large-scale generative language models . Models can
also learn to combine together simpler DA primitives or add
human-in-the-loop .
Applications
In this section, we discuss several DA methods for
some common NLP applications.2
Low-Resource Languages
Low-resource languages are an important and challenging application for DA, typically for neural
machine translation (NMT). Techniques using external knowledge such as WordNet 
may be difﬁcult to use effectively here.3 There
are ways to leverage high-resource languages for
low-resource languages, particularly if they have
similar linguistic properties. Xia et al. use
this approach to improve low-resource NMT.
Li et al. use backtranslation and selflearning to generate augmented training data. Inspired by work in CV, Fadaee et al. generate additional training examples that contain lowfrequency (rare) words in synthetically created contexts. Qin et al. present a DA framework to
generate multi-lingual code-switching data to ﬁnetune multilingual-BERT. It encourages the alignment of representations from source and multiple
target languages once by mixing their context information. They see improved performance across
5 tasks with 19 languages.
3Low-resource language challenges discussed more in §6.
Ext.Know Pretrained
Preprocess
Task-Agnostic
SYNONYM REPLACEMENT 
RANDOM DELETION 
RANDOM SWAP 
BACKTRANSLATION 
SCPN 
SEMANTIC TEXT EXCHANGE 
CONTEXTUALAUG 
LAMBADA 
GECA 
SEQMIXUP 
SWITCHOUT 
EMIX 
Emb/Hidden
SPEECHMIX 
Emb/Hidden Speech/Audio
MIXTEXT 
Emb/Hidden
SIGNEDGRAPH 
DTREEMORPH 
Sub2 
Substructural
DAGA 
Input+Label
WN-HYPERS 
SYNTHETIC NOISE 
UEDIN-MS (DA part) 
NONCE 
XLDA 
SEQMIX 
Input+Label
SLOT-SUB-LM 
UBT & TBT 
SOFT CONTEXTUAL DA 
Emb/Hidden
DATA DIVERSIFICATION 
DIPS 
AUGMENTED SBERT 
Input+Label Sentence Pairs
Table 1: Comparing a selection of DA methods by various aspects relating to their applicability, dependencies, and
requirements. Ext.Know, KWE, tok, const, and dep stand for External Knowledge, keyword extraction, tokenization, constituency parsing, and dependency parsing, respectively. Ext.Know refers to whether the DA method requires external knowledge (e.g. WordNet) and Pretrained if it requires a pretrained model (e.g. BERT). Preprocess
denotes preprocessing required, Level denotes the depth at which data is modiﬁed by the DA, and Task-Agnostic
refers to whether the DA method can be applied to different tasks. See Appendix B for further explanation.
Mitigating Bias
Zhao et al. attempt to mitigate gender
bias in coreference resolution by creating an augmented dataset identical to the original but biased
towards the underrepresented gender (using gender swapping of entities such as replacing "he"
with "she") and train on the union of the two
datasets. Lu et al. formally propose COUN-
TERFACTUAL DA (CDA) for gender bias mitigation, which involves causal interventions that break
associations between gendered and gender-neutral
words. Zmigrod et al. and Hall Maudslay
et al. propose further improvements to CDA.
Moosavi et al. augment training sentences
with their corresponding predicate-argument structures, improving the robustness of transformer models against various types of biases.
Fixing Class Imbalance
Fixing class imbalance typically involves a combination of undersampling and oversampling. SYN-
THETIC MINORITY OVERSAMPLING TECHNIQUE
(SMOTE) , which generates augmented minority class examples through
interpolation, still remains popular . MULTILABEL SMOTE (MLSMOTE)
 modiﬁes SMOTE to balance
classes for multi-label classiﬁcation, where classi-
ﬁers predict more than one class at the same time.
Other techniques such as EDA 
can possibly be used for oversampling as well.
Few-Shot Learning
DA methods can ease few-shot learning by adding
more examples for novel classes introduced in the
few-shot phase. Hariharan and Girshick 
use learned analogy transformations φ(z1, z2, x)
between example pairs from a non-novel class
z1 →z2 to generate augmented examples x →x′
for novel classes. Schwartz et al. generalize
this to beyond just linear offsets, through their "∆network" autoencoder which learns the distribution
P(z2|z1, C) from all y∗
z2 = C pairs, where
C is a class and y is the ground-truth labelling
function. Both these methods are applied only on
image tasks, but their theoretical formulations are
generally applicable, and hence we discuss them.
Kumar et al. apply these and other
DA methods for few-shot learning of novel intent
classes in task-oriented dialog. Wei et al. 
show that data augmentation facilitates curriculum
learning for training triplet networks for few-shot
text classiﬁcation. Lee et al. use T5 to generate additional examples for data-scarce classes.
Adversarial Examples (AVEs)
Adversarial examples can be generated using
innocuous label-preserving transformations (e.g.
paraphrasing) that fool state-of-the-art NLP models, as shown in Jia et al. . Speciﬁcally,
they add sentences with distractor spans to passages to construct AVEs for span-based QA. Zhang
et al. construct AVEs for paraphrase detection using word swapping. Kang et al. 
and Glockner et al. create AVEs for textual
entailment using WordNet relations.
In this section, we discuss several DA works
for common NLP tasks.2 We focus on nonclassiﬁcation tasks as classiﬁcation is worked on
by default, and well covered in earlier sections (e.g.
§3 and §4). Numerous previously mentioned DA
techniques, e.g. , have been used
or can be used for text classiﬁcation tasks.
Summarization
Fabbri et al. investigate backtranslation as a
DA method for few-shot abstractive summarization
with the use of a consistency loss inspired by UDA.
Parida and Motlicek propose an iterative DA
approach for abstractive summarization that uses a
mix of synthetic and real data, where the former is
generated from Common Crawl. Zhu et al. 
introduce a query-focused summarization dataset collected using Wikipedia called
WIKIREF which can be used for DA. Pasunuru et al.
 use DA methods to construct two training
datasets for Query-focused Multi-Document Summarization (QMDS) called QMDSCNN and QMD-
SIR by modifying CNN/DM 
and mining search-query logs, respectively.
Question Answering (QA)
Longpre et al. investigate various DA and
sampling techniques for domain-agnostic QA including paraphrasing by backtranslation. Yang
et al. propose a DA method using distant
supervision to improve BERT ﬁnetuning for opendomain QA. Riabi et al. leverage Question
Generation models to produce augmented examples for zero-shot cross-lingual QA. Singh et al.
 propose XLDA, or CROSS-LINGUAL DA,
which substitutes a portion of the input text with
its translation in another language, improving performance across multiple languages on NLI tasks
including the SQuAD QA task. Asai and Hajishirzi
 use logical and linguistic knowledge to generate additional training data to improve the accuracy and consistency of QA responses by models.
Yu et al. introduce a new QA architecture
called QANet that shows improved performance
on SQuAD when combined with augmented data
generated using backtranslation.
Sequence Tagging Tasks
Ding et al. propose DAGA, a two-step DA
process. First, a language model over sequences of
tags and words linearized as per a certain scheme is
learned. Second, sequences are sampled from this
language model and de-linearized to generate new
examples. ¸Sahin and Steedman , discussed
in §3.1, use dependency tree morphing (Figure 2)
to generate additional training examples on the
downstream task of part-of-speech (POS) tagging.
Dai and Adel modify DA techniques proposed for sentence-level tasks for named entity
recognition (NER), including label-wise token and
synonym replacement, and show improved performance using both recurrent and transformer models.
Zhang et al. propose a DA method based
on MIXUP called SEQMIX for active sequence labeling by augmenting queried samples, showing
improvements on NER and Event Detection.
Parsing Tasks
Jia and Liang propose DATA RECOMBINA-
TION for injecting task-speciﬁc priors to neural semantic parsers. A synchronous context-free grammar (SCFG) is induced from training data, and
new "recombinant" examples are sampled. Yu et al.
 introduce GRAPPA, a pretraining approach
for table semantic parsing, and generate synthetic
question-SQL pairs via an SCFG. Andreas 
use compositionality to construct synthetic examples for downstream tasks like semantic parsing.
Fragments of original examples are replaced with
fragments from other examples in similar contexts.
Vania et al. investigate DA for lowresource dependency parsing including dependency
tree morphing from ¸Sahin and Steedman 
(Figure 2) and modiﬁed nonce sentence generation from Gulordava et al. , which replaces
content words with other words of the same POS,
morphological features, and dependency labels.
Grammatical Error Correction (GEC)
Lack of parallel data is typically a barrier for GEC.
Various works have thus looked at DA methods
for GEC. We discuss some here, and more can be
found in Table 2 in Appendix C.
There is work that makes use of additional resources.
Boyd use German edits from
Wikipedia revision history and use those relating
to GEC as augmented training data. Zhang et al.
 explore multi-task transfer, or the use of
annotated data from other tasks.
There is also work that adds synthetic errors to
noise the text. Wang et al. investigate two
approaches: token-level perturbations and training
error generation models with a ﬁltering strategy
to keep generations with sufﬁcient errors. Grundkiewicz et al. use confusion sets generated
by a spellchecker for noising. Choe et al. 
learn error patterns from small annotated samples
along with POS-speciﬁc noising.
There have also been approaches to improve the
diversity of generated errors. Wan et al. 
investigate noising through editing the latent representations of grammatical sentences, and Xie et al.
 use a neural sequence transduction model
and beam search noising procedures.
Neural Machine Translation (NMT)
There are many works which have investigated DA
for NMT. We highlighted some in §3 and §4.1,
e.g. . We discuss some further ones here,
and more can be found in Table 3 in Appendix C.
Wang et al. propose SWITCHOUT, a
DA method that randomly replaces words in both
source and target sentences with other random
words from their corresponding vocabularies. Gao
et al. introduce SOFT CONTEXTUAL DA
that softly augments randomly chosen words in a
sentence using a contextual mixture of multiple
related words over the vocabulary. Nguyen et al.
 propose DATA DIVERSIFICATION which
merges original training data with the predictions
of several forward and backward models.
Data-to-Text NLG
Data-to-text NLG refers to tasks which require generating natural language descriptions of structured
or semi-structured data inputs, e.g. game score
tables . Randomly perturbing game score values without invalidating overall
game outcome is one DA strategy explored in game
summary generation .
Two popular recent benchmarks are E2E-NLG
 and WebNLG . Both involve generation from structured
inputs - meaning representation (MR) sequences
and triple sequences, respectively. Montella et al.
 show performance gains on WebNLG by
DA using Wikipedia sentences as targets and
parsed OpenIE triples as inputs.
Tandon et al.
 propose DA for E2E-NLG based on permuting the input MR sequence. Kedzie and McKeown inject Gaussian noise into a trained
decoder’s hidden states and sample diverse augmented examples from it. This sample-augmentretrain loop helps performance on E2E-NLG.
Open-Ended & Conditional Generation
There has been limited work on DA for open-ended
and conditional text generation. Feng et al. 
experiment with a suite of DA methods for ﬁnetuning GPT-2 on a low-resource domain in attempts
to improve the quality of generated continuations,
which they call GENAUG. They ﬁnd that WN-
HYPERS (WordNet hypernym replacement of keywords) and SYNTHETIC NOISE (randomly perturbing non-terminal characters in words) are useful,
and the quality of generated text improves to a peak
at ≈3x the original amount of training data.
Most DA approaches for dialogue focus on taskoriented dialogue. We outline some below, and
more can be found in Table 4 in Appendix C.
Quan and Xiong present sentence and
word-level DA approaches for end-to-end taskoriented dialogue. Louvan and Magnini 
propose LIGHTWEIGHT AUGMENTATION, a set of
word-span and sentence-level DA methods for lowresource slot ﬁlling and intent classiﬁcation.
Hou et al. present a seq2seq DA framework to augment dialogue utterances for dialogue
language understanding , including a diversity rank to produce diverse utterances. Zhang et al. propose MADA to
generate diverse responses using the property that
several valid responses exist for a dialogue context.
There is also DA work for spoken dialogue. Hou
et al. , Kim et al. , Zhao et al. ,
and Yoo et al. investigate DA methods for dialogue and spoken language understanding (SLU),
including generative latent variable models.
Multimodal Tasks
DA techniques have also been proposed for multimodal tasks where aligned data for multiple modalities is required. We look at ones that involve language or text. Some are discussed below, and more
can be found in Table 5 in Appendix C.
Beginning with speech, Wang et al. propose a DA method to improve the robustness of
downstream dialogue models to speech recognition
errors. Wiesner et al. and Renduchintala
et al. propose DA methods for end-to-end
automatic speech recognition (ASR).
Looking at images or video, Xu et al. 
learn a cross-modality matching network to produce synthetic image-text pairs for multimodal classiﬁers. Atliha and Šešok explore DA methods such as synonym replacement and contextualized word embeddings augmentation using BERT
for image captioning. Kaﬂe et al. , Yokota
and Nakayama , and Tang et al. propose methods for visual QA including question
generation and adversarial examples.
Challenges & Future Directions
Looking forward, data augmentation faces substantial challenges, speciﬁcally for NLP, and with these
challenges, new opportunities for future work arise.
Dissonance between empirical novelties and
theoretical narrative:
There appears to be a conspicuous lack of research on why DA works. Most
studies might show empirically that a DA technique
works and provide some intuition, but it is currently
challenging to measure the goodness of a technique
without resorting to a full-scale experiment. A recent work in vision 
has proposed that afﬁnity (the distributional shift
caused by DA) and diversity (the complexity of the
augmentation) can predict DA performance, but it
is unclear how these results might translate to NLP.
Minimal beneﬁt for pretrained models on indomain data:
With the popularization of large
pretrained language models, it has recently come to
light that a couple of previously effective DA techniques for certain text classiﬁcation tasks in English
 provide
little beneﬁt for models like BERT and RoBERTa,
which already achieve high performance on indomain text classiﬁcation .
One hypothesis for this could be that using simple
DA techniques provides little beneﬁt when ﬁnetuning large pretrained transformers on tasks for which
examples are well-represented in the pretraining
data, but DA methods could still be effective when
ﬁnetuning on tasks for which examples are scarce
or out-of-domain compared with the training data.
Further work could study under which scenarios
data augmentation for large pretrained models is
likely to be effective.
Multimodal challenges:
While there has been
increased work in multimodal DA, as discussed in
§5.10, effective DA methods for multiple modalities has been challenging. Many works focus on
augmenting a single modality or multiple ones separately. For example, there is potential to further
explore simultaneous image and text augmentation
for image captioning, such as a combination of
CUTMIX and caption editing.
Span-based tasks
offer unique DA challenges
as there are typically many correlated classiﬁcation
decisions. For example, random token replacement
may be a locally acceptable DA method but possibly disrupt coreference chains for latter sentences.
DA techniques here must take into account dependencies between different locations in the text.
Working in specialized domains
such as those
with domain-speciﬁc vocabulary and jargon (e.g.
medicine) can present challenges. Many pretrained
models and external knowledge (e.g. WordNet)
cannot be effectively used. Studies have shown
that DA becomes less beneﬁcial when applied to
out-of-domain data, likely because the distribution
of augmented data can substantially differ from the
original data .
Working with low-resource languages
present similar difﬁculties as specialized domains.
Further, DA techniques successful in the highresource scenario may not be effective for lowresource languages that are of a different language
family or very distinctive in linguistic and typological terms. For example, those which are language
isolates or lack high-resource cognates.
vision-inspired
techniques:
many NLP DA methods have been inspired by analogous approaches in CV, there is potential for drawing further connections. Many CV DA techniques
motivated by real-world invariances (e.g. many
angles of looking at the same object) may have
similar NLP interpretations. For instance, grayscaling could translate to toning down aspects of the
text (e.g. plural to singular, "awesome" →"good").
Morphing a dependency tree could be analogous
to rotating an image, and paraphrasing techniques
may be analogous to changing perspective. For example, negative data augmentation (NDA) involves creating out-of-distribution
samples. It has so far been exclusively explored for
CV, but could be investigated for text.
Self-supervised learning:
More recently, DA
has been increasingly used as a key component
of self-supervised learning, particularly in vision
 . In NLP, BART showed that predicting deleted tokens as a
pretraining task can achieve similar performance as
the masked LM, and ELECTRA 
found that pretraining by predicting corrupted tokens outperforms BERT given the same model size,
data, and compute. We expect future work will
continue exploring how to effectively manipulate
text for both pretraining and downstream tasks.
Ofﬂine versus online data augmentation:
CV, standard techniques such as cropping, color
jittering, and rotations are typically done stochastically, allowing for DA to be incorporated elegantly
into the training pipeline. In NLP, however, it is unclear how to include a lightweight code module to
apply DA stochastically. This is because DA techniques for NLP often leverage external resources
(e.g. a word dictionary for token substitution or a
translation model for backtranslation) that are not
easily transferable across model training pipelines.
Thus, a common practice for DA in NLP is simply
to generate augmented data ofﬂine and store it as
additional data to be loaded during training.4 Future work on a lightweight module for online DA
4See Appendix D.
in NLP could be fruitful, though another challenge
will be determining when such a module will be
helpful, which—compared with CV, where the invariances being imposed are well-accepted—can
vary substantially across NLP tasks.
Lack of uniﬁcation
is a challenge for the current literature on data augmentation for NLP, and
popular methods are often presented in an auxiliary fashion. Whereas there are well-accepted
frameworks for DA for CV ), there are no such "generalized" DA
techniques for NLP. Further, we believe that DA
research would beneﬁt from the establishment of
standard and uniﬁed benchmark tasks and datasets
to compare different augmentation methods.
Good data augmentation practices
would help
make DA work more accessible and reproducible
to the NLP and ML communities.
uniﬁed benchmark tasks, datasets, and frameworks/libraries mentioned above, other good practices include making code and augmented datasets
publicly available, reporting variation among results (e.g. standard deviation across random seeds),
and more standardized evaluation procedures. Further, transparent hyperparameter analysis, explicitly stating failure cases of proposed techniques,
and discussion of the intuition and theory behind
them would further improve the transparency and
interpretability of DA techniques.
Conclusion
In this paper, we presented a comprehensive and
structured survey of data augmentation for natural language processing (NLP). We provided a
background about data augmentation and how it
works, discussed major methodologically representative data augmentation techniques for NLP, and
touched upon data augmentation techniques for
popular NLP applications and tasks. Finally, we
outlined current challenges and directions for future research, and showed that there is much room
for further exploration. Overall, we hope our paper
can serve as a guide for NLP researchers to decide
on which data augmentation techniques to use, and
inspire additional interest and work in this area.
Please see the corresponding GitHub repository at