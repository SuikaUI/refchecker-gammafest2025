Hierarchical Diffusion Models for Two-Choice Response Times
Joachim Vandekerckhove and Francis Tuerlinckx
University of Leuven
Michael D. Lee
University of California, Irvine
Two-choice response times are a common type of data, and much research has been devoted to the
development of process models for such data. However, the practical application of these models is
notoriously complicated, and ﬂexible methods are largely nonexistent. We combine a popular model for
choice response times—the Wiener diffusion process—with techniques from psychometrics in order to
construct a hierarchical diffusion model. Chief among these techniques is the application of random
effects, with which we allow for unexplained variability among participants, items, or other experimental
units. These techniques lead to a modeling framework that is highly ﬂexible and easy to work with.
Among the many novel models this statistical framework provides are a multilevel diffusion model,
regression diffusion models, and a large family of explanatory diffusion models. We provide examples
and the necessary computer code.
Keywords: response time, psychometrics, hierarchical, random effects, diffusion model
Supplemental materials: 
In his 1957 presidential address at the 65th annual business meeting of the American Psychological Association, Lee Cronbach drew
a captivating sketch of the state of psychology at the time. He focused
on the two distinct disciplines that then existed in the ﬁeld of scientiﬁc
psychology. On the one side, there was the experimental discipline,
which concerned itself with the systematic manipulation of conditions
in order to observe the consequences. On the other side, there was the
correlational discipline, which focused on the study of preexisting
differences between individuals or groups. Cronbach saw many potential contributions of these disciplines to one another and argued
that the time and opportunity had come for the two dissociated ﬁelds
to crossbreed: “We are free at last to look up from our own bedazzling
treasure, to cast properly covetous glances upon the scientiﬁc wealth
of our neighbor discipline. Trading has already been resumed, with
beneﬁt to both parties” . Two decades
onward, Cronbach saw the hybrid discipline ﬂourishing across
several domains.
In the area of measurement of psychological processes, there
exists a schism similar to the one Cronbach pointed out in his
presidential address. Psychological measurement and individual
differences are studied in the domain of psychometrics, whereas
cognitive processes are the stuff of the more nomothetic mathematical psychology. In both areas, statistical models are used
extensively. There are common models based on the (general)
linear model, such as analysis of variance (ANOVA) and regression, but we focus on more advanced, nonlinear techniques.
Experimental psychology has, for a long time, made use of
process models to describe interesting psychological phenomena
in various ﬁelds. Some famous examples are Sternberg’s 
sequential exhaustive search model for visual search and memory
scanning, Atkinson and Shiffrin’s multistore model for
memory, multinomial processing tree models for categorical responses ,
and the general family of sequential sampling models for choice
response times . One property shared by these process models is that
they give detailed accounts of underlying response processes. Such
models are typically applied to data from single participants, and
they are very successful in ﬁtting empirical data.
In the correlational area, however, measurement models are
dominant. Most well known among these is the factor analysis
(FA) model, but models from item response theory (IRT) belong to
this class as well. In the past decade, a lot of work has appeared
showing the relationships between FA, IRT, and multilevel models. Rijmen, Tuerlinckx, De Boeck, and Kuppens showed
that many IRT models are generalized linear mixed models and
that the rest are nonlinear mixed models . Skrondal and Rabe-Hasketh offered an encompassing framework for FA models, IRT models,
and multilevel models , Department of Psychology, University of Leuven, Leuven, Belgium; Francis Tuerlinckx, Department of Psychology,
University of Leuven; Michael D. Lee, Department of Cognitive Sciences,
University of California, Irvine.
This research was supported by Grants GOA/00/02–ZKA4511, GOA/
2005/04–ZKB3312, and IUAP P5/24 to Francis Tuerlinckx and Joachim
Vandekerckhove; Grant K.2.215.07.N.01 to Joachim Vandekerckhove; and
KULeuven/BOF Senior Fellowship SF/08/015 to Michael D. Lee.
The authors are indebted to Philip Smith for his insightful comments and
to Gilles Dutilh, Roger Ratcliff, Jeff Rouder, and Eric-Jan Wagenmakers
for sharing their data with us. This research was conducted utilizing
high-performance computational resources provided by the University of
Leuven. We also thank Microsoft Corporation and Dell for generously
providing us with additional computing resources.
Correspondence concerning this article should be addressed to Joachim
Vandekerckhove, Department of Psychology, University of Leuven, Tiensestraat 102 B3713, B–3000 Leuven, Belgium. E-mail: joachim.vandekerckhove@
psy.kuleuven.be
Psychological Methods
2011, Vol. 16, No. 1, 44–62
© 2011 American Psychological Association
1082-989X/11/$12.00
DOI: 10.1037/a0021765
models). The models that originated in correlational research are
used to model individual differences. Often such models are less
detailed and more general than the models discussed in the previous paragraph, but they are able to locate the main sources of
individual differences.
Recently, some convergence between the experimental and the
correlational areas has emerged. Batchelder and Riefer introduced the concept of cognitive psychometrics. In cognitive psychometrics, models from cognitive psychology are used to capture speciﬁc interesting aspects of the data.
These models typically assume that the data have been gathered
with a speciﬁc paradigm (e.g., that they are binary choice response
times). Although this necessarily makes the models less general
than multipurpose statistical models, it provides the advantage of
offering substantive insight into the data. Furthermore, ideas of
hierarchical modeling have recently been introduced into the area
of cognitive modeling, most notably by Rouder and colleagues
 , who used hierarchical models as
a statistical framework for inference, and also by Tenenbaum and
colleagues , who used hierarchical models as an account of the
organization of human cognition.
Extending cognitive models to hierarchical models (or vice versa)
is an important part of the trading between disciplines that Cronbach
 advocated. The beneﬁts of the trade do go both ways: By
extending process models hierarchically, experimental psychologists
who use these models can take between-subjects variability into
account and are in a better position to explain such interindividual
differences. Correlational psychologists, on the other hand, could
apply measurement models that are built upon ﬁrmly validated process models, often grounded in substantive theory.
In the present article, we aim to integrate both traditions further by
extending hierarchically an important and popular process model: the
diffusion model for two-choice response times. Even though choosing
the diffusion model as our measurement level bears with it a number
of implementation difﬁculties, we choose this model because of the
interesting psychological interpretation of its parameters, which we
explain in the next section. Additionally, choice response times—the
combination of reaction time (RT) and accuracy data—are ubiquitous
in experimental psychology, and we believe that a hierarchical extension of the diffusion model could be of considerable value to the ﬁeld.
In addition, a Bayesian approach is taken to ﬁt the hierarchical
extension of the diffusion model. Details on the practical implementation are provided as well.
In the sections that follow, we introduce the diffusion model for
two-choice response times and then provide a detailed account of
the hierarchical extension to the diffusion model. Then we describe
two sample applications. We conclude with a discussion of our
approach and of further possible applications.
The Diffusion Model
The diffusion model as a process for speeded decisions starts
from the basic principle of accumulation of information . When an individual is asked to make
a binary choice on the basis of an available stimulus, the assumption is that evidence from the stimulus is accumulated over (continuous) time and that a decision is made as soon as an upper or
lower boundary is reached. Which boundary is reached determines
which response is given. The basic form of this model is often
referred to as the Wiener diffusion model with absorbing boundaries.
Figure 1 depicts the Wiener diffusion process and shows the
main parameters of the process. On the vertical axis there are the
boundary separation 1 indicating the evidence required to make a
response (i.e., speed–accuracy trade-off) and the initial bias ,
indicating the a priori status of the evidence counter as a proportion of . If  is less than 0.5, this indicates bias for the response
represented by the lower boundary. The absolute value of the
starting position is   init, but we will generally not use this
parameter. The arrow represents the average rate of information
uptake, or drift rate , which indicates the average amount of
evidence that the observer receives from the stimulus at each
sampling. (The amount of variability in these samples, which
makes the process stochastic, is a scaling constant that is typically
set to 0.1 in the literature.) Finally, the short dashed line indicates
the nondecision time , the time used for everything except making
a decision (i.e., encoding the stimulus and physically executing the
response). Table 1 gives a summary of the parameters and their
classical interpretations.
The diffusion model owes much of its current popularity to the
work of Ratcliff and colleagues . An important contribution Ratcliff made was to
incorporate trial-to-trial variance into the Wiener diffusion model,
so that the parameters , , and  are not constant but vary from
trial to trial. This conceptually signiﬁcant extension has performed
so remarkably well in the analysis of two-choice response time
data that it is now sometimes referred to as the Ratcliff diffusion
model . It has successfully been applied to data from experiments in
many different ﬁelds, such as memory , letter matching , lexical decision
 , signal detection ,
visual search , and perceptual judgment
 . The Ratcliff diffusion model is also one of
few models that succeed in explaining all of the “benchmark”
characteristic aspects of two-choice response time data—such as
different response time distributions for correct and error responses, both of them positively skewed and the relation between
their means dependent on parameters, with some minimum value
below which there is no mass. In addition, the model has passed
selective inﬂuence tests for its main parameters , in which experimental manipulations are shown to
affect only the relevant model parameters (e.g., changing from
speed to accuracy instructions affects only the boundary separation
parameter). Fitting the model to empirical data has become a topic
1 Throughout, we use Greek letters to indicate unobserved parameters
and Latin letters for running indices or observed variables.
HIERARCHICAL DIFFUSION MODELS
of research in its own right .
For our purposes, however, an important aspect of the diffusion
model is that there is a mathematically tractable solution for the
bivariate probability density function (PDF) of the response time
and accuracy. In other words, it is possible to deﬁne explicitly a
four-parameter density function, the “Wiener PDF,” that describes
the predictions of the model, given only the four parameters
described in Table 1. The mathematical form of this PDF is given
in the Supplementary Materials.
Finally, it should be kept in mind that, as with all statistical
models, application of the diffusion model requires the user to
assume that the process described here is the real process that
brings about each individual response by a participant to a stimulus. If, for example, the experimental paradigm allows for selfcorrecting processes (e.g., a participant second-guessing a response), then one of the process assumptions of the diffusion
model is violated and the model should not be applied.
A Hierarchical Framework for the Diffusion Model
Motivation
There are several motivations for making a hierarchical extension of a substantively generated model such as the diffusion
model. The ﬁrst and most important motivation is the fact that
traditional applications of the diffusion model have been restricted to single participants , and there has generally been no motivation to model
interindividual differences in the decision process. The dearth
of investigation into individual differences when applying process models is reminiscent of the schism between the experimental and correlational subdisciplines that Cronbach pointed out.
More recently, however, the diffusion model has been applied to study individual differences . The typical approach in such cases
is to run multistep analyses: In a ﬁrst step a speciﬁc model is
ﬁtted to data from each individual, and then inferences regarding individual differences are made on the basis of summary
measures of the parameter estimates. An example of this approach can be found in Klauer et al. , in which individual
participants’ parameter estimates are subjected to second-stage
analysis using ANOVA.
However, data do not always allow for separate analyses per
individual: Estimating the diffusion model’s parameters typically requires a large number of data points , and in
many experimental contexts it may be impractical or even impossible
to obtain many data points within each participant. In particular, when
studying higher level cognitive processes or emotions the stimulus
material may simply not allow for the generation of hundreds of trials
or for presenting stimuli more than once . Often, however, there are many participants in the sample. In cases such as these,
it is natural to be interested in individual differences, but it is impossible to analyze the data separately for each participant, and the
multistep procedure cannot be applied.
Another problem with the multistep procedures is that one may
want to constrain parameters to be equal across participants. In this
case, an analysis needs to involve all participants simultaneously,
allowing some of the parameters to differ and others to be equal.
However, such an approach may lead to a prohibitively large
number of parameters. As will be argued in the following sections,
a hierarchical approach may offer a solution by formalizing individual differences in a speciﬁc process model framework.
Response A
Response B
Sample Path
A graphical illustration of the Wiener diffusion model.   boundary separation indicating the
evidence required to make a response;   initial bias indicating the a priori status of the evidence counter as
a proportion of ; init  absolute value of the starting position;   average rate of information uptake;  
time used for everything except making a decision.
Four Main Parameters of the Wiener Diffusion Model, With
Their Substantive Interpretations
Interpretation
Boundary separation
Speed–accuracy trade-off (high 
means high accuracy)
Initial bias
Bias for either response ( means
bias toward Response A)
Drift rate
Speed of information processing
(close to 0 means ambiguous
information)
Nondecision time
Motor response time, encoding time
(high means slow encoding,
execution)
VANDEKERCKHOVE, TUERLINCKX, AND LEE
Uses of the Hierarchical Diffusion Model
In a hierarchical model , it is assumed
that participants are a randomly drawn sample from some partly
speciﬁed population. Individual participants each have their own
set of parameters, and because these participants are typically
randomly selected from some larger population, the differences in
parameter values between participants can be seen as a random
effect in the statistical sense. A random effect occurs when experimental units are randomly drawn, interchangeable samples from a
larger population. This may apply not only to participants but also
to items, trials, blocks, and other units, as long as they are interchangeable samples. If the selected units comprise the entirety of
the relevant population (about which one wants to make inferences), then a ﬁxed effect is appropriate. In this way, individual
differences can be explicitly permitted in a hierarchical model.
However, not only the person-speciﬁc parameters are important
but the unknown characteristics of their population distributions
are as well, characteristics such as the means, variances, and
covariances, the latter two of which are indications of the magnitude (i.e., importance) of individual differences.2 In a hierarchical
framework, it is relatively easy to construct models in which some
parameters are constrained to be equal across participants, whereas
others may vary from individual to individual. Hierarchical models
are ideally suited to handle data sets with few trials per participant
(discussed earlier), even in the case in which single individuals do
not provide enough information to estimate all model parameters
and in which the number of data points per participant (or per cell
of the design) seems absurdly small. Hierarchically extending the
diffusion model leads to what we call the hierarchical diffusion
model (HDM).3
Hierarchical models have proven useful in many areas of research. Some selected domains include psychological measurement when item response models have been used , educational measurement and school
effectiveness studies , and longitudinal data analysis in psychology and
biostatistics .
In this article, we rely particularly on the framework proposed
by De Boeck and Wilson for item response models. In their
book, De Boeck and Wilson sharply distinguish between describing and explaining individual differences. Describing individual
differences refers to the possibility of assuming population distributions for certain parameters and estimating some characteristics
of these distributions. In such an approach, one merely acknowledges that differences between persons exist, and one quantiﬁes
the variability in the population (through the variances of the
population distributions). However, in any scientiﬁc enterprise, the
ultimate goal is not to simply observe differences but to attempt to
explain why they occur. Individual differences can be explained by
relating the person-speciﬁc parameters to predictors (see later). In
doing so, we consider the variability in the population as to-beexplained, and by including a predictor in the model, we explicitly
intend to decrease this unexplained variability.
It is important to emphasize that, although the previous discussion was centered on differences between persons, an HDM can
equally well be applied to populations of items, trials, or indeed
any experimental unit (e.g., subgroups within populations, items
nested in conditions). Variability across these other experimental
units can be captured in exactly the same way as is variability
across persons. The sample applications make extensive use of this
ability of HDMs.
The main difference between the approach of De Boeck and
Wilson and our framework is that De Boeck and Wilson
worked within a context of item response models: The data they
considered are binary (or polytomous) responses of persons to a set of
items. These item response models are logistic regression models or
extensions and generalizations thereof that relate the responses (or
more correctly: the probability of a certain response) to an underlying
latent trait (i.e., the individual difference variable). There, the logistic
regression model can be considered as the measurement model. In our
case, the data are bivariate (choice response and RT) and the measurement level is the Wiener diffusion model, which is considerably
more complex (both computationally, because the probability density
function is mathematically somewhat intricate, and conceptually, because of having a process interpretation).
In the remainder of this section, we further elaborate on and
apply the framework of De Boeck and Wilson to the
diffusion model. This will be done by deﬁning several basic
building blocks that may be combined with the diffusion model in
order to arrive at an HDM capable of describing and explaining
interindividual differences. As it turns out, not only interindividual
differences but other sources of variation may be tackled in such a
way. Before doing so, however, we deﬁne some notation.
Suppose a person p (with p  1, . . ., P) is observed in condition
i (with i  1, . . ., I) on trial j (with j  1, . . ., J) and the person’s
choice responses (corresponding to the absorbing boundaries) and
response times are recorded, denoted by the random variables X(pij)
and T(pij), respectively (realizations of these random variables are
x(pij) and t(pij)). Also, Y(pij) and y(pij) refer to the random vector
(X(pij),T(pij)) and the vector of realizations (x(pij), t(pij)), respectively. Then Y(pij) would be distributed according to a Wiener
distribution as follows:
Ypij  Wienerpij ,pij ,pij ,pij .
We use Wiener distribution as shorthand for the joint density
function of hitting the boundary X(pij) at time T(pij). The distribu-
2 Although it may seem that such an approach leads to even more
parameters than when no population assumptions are made, invoking the
population assumption actually reduces the number of effective parameters
because it acts as a constraint on the person-speciﬁc parameters (this effect
is in some cases also called shrinkage to the mean). A limiting case is when
the variance of the population distribution is zero such that there are no
individual differences and all person-speciﬁc parameters are exactly equal
to the mean. Moreover, shrinkage is stronger for parameters of individuals
who provide less information. For more information on hierarchical modeling and shrinkage we refer to Gelman and Hill .
3 There is some ambiguity here about the word model. In one sense, the
diffusion model is a process model and the hierarchical extension is a
statistical modeling tool. It is the combination of these two aspects,
however, that makes the HDM a powerful framework.
HIERARCHICAL DIFFUSION MODELS
tion is characterized by four basic parameters (explained earlier in
The Diffusion Model section) that here carry a triple index, which
means that, in principle, they can differ across persons, conditions,
and trials. In some of the examples, we add additional indices to
allow more nuanced differences. To avoid confusion with other
subscripts, running indices will always be put between parentheses; for example, (i) indicates the parameter  that belongs to
condition i, but
descriptor are distinct, singular parameters.
Finally, it should be noted that we often recycle symbols for new
models or new examples, so that a symbol used in one model may
be redeﬁned in another model to refer to something else.
Model Building Blocks
On the basis of the framework of De Boeck and Wilson ,
we discern three types of useful model building blocks: levels of
random variation, manifest predictors, and latent predictors. In
order to render the discussion of these three aspects more concrete,
we illustrate the theoretical concepts with the drift rate parameter
of the diffusion model. We choose to limit the illustrations to a
single parameter for reasons of clarity, but a similar story can be
told for the other parameters, as will become obvious when we
move to the applications later in the article.
Levels of random variation.
The data may contain different
levels of hierarchy. We have already implicitly referred to the most
basic case when talking about individual differences: Imagine a
situation in which a sample of individuals is measured repeatedly.
In such a case, the data consist of two levels: At the higher level
are the individuals, and at the lower level are the measurements
within the persons.
As an example, consider drift rate (pij). Assume that a set of
persons p are presented with a series of stimuli j in a single
condition (such that the index i for condition may be dropped). The
drift rate (pj) can then be written as follows:
p  εpj ,
where ε(pj)
2), with ε(pj) and
independent. Here, the variance
2 represents trial-to-trial variability in drift rate within a person. This example is akin to the
assumption of trial-to-trial variability made by Ratcliff . The
is the population average of individual drift rates,
2 is the variance of individual drift rates in the population.
The importance of individual differences can be judged by comparing 
2 is much larger than
2, this means that
there are sizable individual differences, which is not the case if 
is much smaller than
2. Other methods of comparing the amounts
of variability at different levels of hierarchy are intraclass correlation coefﬁcients .
There exist several alternative ways of writing the model in
Equation 1. For instance, one could include the population average
directly into the linear decomposition (i.e., (pj) 
εε(pj)) and assume a mean of zero and unit variance for
all random effects distributions.
Equation 1 can be extended readily to include ﬁxed condition
effects as follows:
pij  i 
where (i) is a ﬁxed condition effect. Hence, the mean drift rate in
condition i for a person p depends on a ﬁxed condition effect (i)
and a random person effect
(p). A related model has been proposed earlier by Ratcliff and Tuerlinckx and De Boeck
Because individual differences are the main motivation for
developing an HDM, we have thus far restricted the hierarchical
structure to trials nested within persons (conditions are viewed as
ﬁxed effects). However, there is no reason to stop there if there is
a sound reason for more complex forms of levels of random
variation. For example, persons may be nested in groups and those
groups nested in larger groups. In such a case, there are more than
the traditional two levels in the data.
In addition, there is no reason to allow random effects only on
the person side. On the condition or item side, it can make sense
to allow for condition or random effects . In the types of applications we envision for
the HDM, the stimulus material often consists of words or pictures
 . In psycholinguistics, for example, there
has been some controversy over the modeling of word effects. In
a seminal article, Clark strongly argued that stimulus words
should be considered as randomly sampled from a population
distribution as well. In such cases, the parameter (i) in Equation 2
can also be assumed to follow a normal distribution with mean
and variance 
2. This would yield a crossed random effects design
 . Similarly, conditions or items could be nested in
categories that are in turn nested in larger categories.
Manifest predictors.
By identifying and including levels of
variation in the analyses, we describe individual differences or, if
there are random item effects, differences between stimuli. We call
this type of analysis descriptive because we are merely observing
how the variability in the data is distributed among several sources.
However, in a next step we want to explain the variability in
parameters by using predictors (continuous or discrete or both).
More broadly, interindividual, interstimulus, or less intuitively,
intertrial variability (represented in random effects and their population variances) might be explained by regressing basic parameters on known predictors or covariates.
As an example of explaining interindividual variability, assume
that the drift rate is person-speciﬁc and that there is a person
covariate such as age available (with A(p) being the age of person
p). We could then adopt the following model for the drift rate:
pij  i  0  1Ap 
p  εpij ,
where 0 and 1 are the regression coefﬁcients of the univariate
linear regression of (pij) on A(p) and
(p) is a person-speciﬁc error
term with distribution
2). The other parameters are
deﬁned as in Equation 2.
Alternatively, we may try to use covariates in order to explain
some of the variability between items. For example, differences in
recognizability between words may be related to their frequency of
use .
In sum, working with manifest predictors in the HDM means
building a regression model for a random effect with known
VANDEKERCKHOVE, TUERLINCKX, AND LEE
predictors but unknown regression coefﬁcients. Explaining variability in parameters through covariates will be an important theme
in the examples in this article.
Latent predictors.
De Boeck and Wilson showed that
predictors do not necessarily need to be manifest; they may also be
latent. That is, they may be unobserved but inferred from the data.
For example, one might suspect that there exist two subgroups in
the participant population, each with its own particular qualities
(e.g., different people might use different strategies to respond to
the stimuli). A mixture model for the diffusion model parameters
may then be used to detect hidden groups or subpopulations in the
Latent predictors may even be continuous. Suppose that in a
given experiment two (or more) dimensions of information processing are required and some conditions rely more on one dimension and other conditions more on the other dimension. Such a
model for drift rate can be expressed as (pij)  (i)  (i)1
(p)2  ε(pij), where (i)1 and (i)2 are the loadings of the
underlying dimensions in condition i and
(p)2 are the
positions of person p on the two dimensions. Such a model can be
called a factor analysis diffusion model.
We do not discuss latent predictor models further because they
rapidly become complex and suffer from model identiﬁcation
issues and estimating their parameters is computationally intensive
(at least, using current standard computational approaches for
Bayesian inference).
Statistical Inference for HDMs
In the practical application of the HDM framework, statistical
inference is performed using Bayesian statistical methods . In this section, we provide some background on Bayesian
methods that is required for interpreting the results of our analyses,
as well as for the application of our software. We believe this
background to be important because, although the philosophy
behind Bayesian statistics is fairly straightforward and easy to
explain, the computational techniques involved are not (further
details about the computational challenges involved are given in
the Appendix and Supplementary Materials).
Several reasons motivate our choice to use Bayesian inference.
The Bayesian framework has many inherent advantages, such as
the principled, consistent, and intuitive treatment of uncertainty
concerning the parameters of the model. However, there are several advantages speciﬁc to the topic of the present article. Bayesian
methods are most suited for ﬂexible implementation of hierarchical models in particular .
The diffusion model in itself, without any hierarchical extension, is already a computationally difﬁcult model . These difﬁculties are
exacerbated by even small increases in the hierarchical structure of
the model . Models
with more extensive hierarchical structures (as discussed here) are
often more interesting, but they rapidly become computationally
intractable in the classical statistical framework where parameters
have to be estimated using maximum likelihood methods. Take for
example a crossed random effects model for drift rate (a random
effect of person and of item), and assume for simplicity that the
other parameters are kept constant across persons and items.
When applying such a relatively unembellished model to a data
set of P persons and I items, one is confronted with a likelihood
function that contains an integral of dimension P  I. Having P
and I values both around 100 results in a 200-dimensional integral,
the approximation of which is computationally prohibitive with
standard numerical integration techniques (e.g., if one were to
approximate each integral with a sum of 10 terms, then the total
number of sums in each evaluation of the likelihood would be a
disheartening 10200). However, integrating over many distributions simultaneously is a basic modus operandi in Bayesian statistics, so that the addition of hierarchical structures such as random effects poses little additional burden.
Bayesian Basics
Bayesian methods depend on the computation of the posterior
distribution of model parameters. That is, the probability distribution of the parameters, given the data. The posterior quantiﬁes
one’s uncertainty about the model’s parameters posterior to having
observed the data. It is computed by updating one’s prior knowledge about a parameter through observing the data (represented by
the likelihood function). The posterior distribution can be obtained
through Bayes’ rule
pD   p
where D and  are generic notations referring to all the data and all
the parameters, respectively; p(D  ) is the likelihood; p() is the
prior distribution of the parameters; and p(D) is the marginal
probability of the data.
The posterior distribution p(D  ) is a very intuitive measure,
because it represents the uncertainty regarding the parameters after
having observed the data. Its mean, the expected a posteriori
(EAP) measure, is typically used as a point estimate of the parameter , with the standard deviation as a measure of uncertainty.
Often, a hypothesis can be tested by mere examination of the
(marginal) posterior of one well-chosen parameter, such as a
regression weight, a difference between two means, or a more
complex function of one or more parameters. “Examining” a
posterior distribution in this sense implies computing or estimating
the probability mass of the parameter relative to some critical
value. One might, for example, investigate the (signed) difference
between two parameters, which should be, say, larger than zero.
We then examine p(D  ) by estimating p(  0  D).
Another popular method is to compute a parameter’s .90 or .95
highest posterior density region (the smallest region of the posterior that contains the speciﬁed proportion of its mass) or its .90 or
.95 central credibility interval (the central, contiguous region of
the posterior that contains the speciﬁed proportion of posterior
mass) and then evaluate whether 0 is contained within this region
or not .
As straightforward as the main principle behind Bayesian statistics is, the practical task of estimating these probabilities can be
somewhat daunting. For the case of the HDM, we have produced
ﬂexible software to facilitate putting it into practice. This software,
HIERARCHICAL DIFFUSION MODELS
along with further information regarding the computational methods, is described in the Supplementary Materials.
Graphical Models
Advanced algorithms for sampling from a high-dimensional
posterior distribution are implemented in the freely available statistical software package WinBUGS . WinBUGS can also be used easily to apply
an HDM (for details, see the Supplementary Materials). In order to
use WinBUGS, however, it is necessary to translate the hierarchical model into a directed acyclical graph, or graphical model.
Graphical models are a convenient formalism for describing the probabilistic relationships between parameters and data. In
a graphical model, variables of interest are represented by nodes in
a directed graph, with children depending on their parents. Circular
nodes represent continuous variables, square nodes discrete variables, shaded nodes observed variables, and unshaded nodes unobserved variables. In addition, plates enclose parts of a graph to
denote independent replications. An example of a basic graphical
model is given in Figure 2. In this Figure, the data y(ij) are
generated by a process that has parameters , , , and (ij). (ij) in
turn is generated from a distribution with parameters
The graphical model is equivalent to the following set of assumptions (omitting for conciseness the prior assumptions):
yij  Wiener, , , ij
As soon as a model has been translated into a graphical model,
it can be implemented in WinBUGS. In fact, WinBUGS code is
only marginally more complex (because it includes prior information as well) than the enumeration of assumptions given earlier. In
WinBUGS, it is straightforward to perform full Bayesian inference
computationally, using standard Markov chain Monte Carlo methods to sample from the posterior distribution.
Evaluating Model Performance in the
Bayesian Framework
After the posterior distributions of all parameters have been
found, two aspects of model performance can be ascertained. To
determine relative model ﬁt across a series of models, the deviance
information criterion (DIC) measure can be
computed. This statistic can be considered as a Bayesian alternative to Akaike’s information criterion . Like
the AIC, the DIC also expresses a balance between the model ﬁt
and its complexity. Smaller DICs indicate better ﬁtting models.
Depending on context, we may choose the AIC over the DIC
 . In general, if the objective of the analysis is
to generalize to other populations (and hence the person- or itemspeciﬁc parameters cannot be considered as given), then the AIC
is more appropriate. In contrast, typically the concern is to quantify
model ﬁt for the particular data set at hand (and hence not to
generalize to a larger population of persons or items), so the DIC
is preferred.
In order to determine absolute model ﬁt, however, we might
apply posterior predictive checks . The
most basic type of PPC involves deﬁning an interesting test statistic G() on the data and computing those statistics for the
observed data (i.e., G(yobs)). Then the same statistic G() can be
computed on a large number of data sets (say, 1,000) that are generated from the model, leading to a set G(yrep 1), . . ., G(yrep 1000).
Finally, the position of G(yobs) in the distribution of G(yrep 1), . . .,
G(yrep 1000) then indicates the viability of the model with regard to the
In general, PPCs can be applied to identify (graphically or
numerically) misﬁt of a model to data. It can also be used to ﬁnd
speciﬁc loci of misﬁt of the model .5 The method is somewhat controversial ,
but it is very practical, easy to carry out, and highly ﬂexible.
Application Examples
To illustrate the usefulness of the HDM framework, we now
apply it to two data sets with very different designs, both of which
seem usefully dealt with using HDMs. In the ﬁrst application, we
apply a series of HDMs to a benchmark data set concerning
4 A more complex type of PPC can be deﬁned as well (such that the test
statistic is not pivotal but also depends on the parameters). However, this
type of test statistic requires a (very time-consuming) reestimation of the
model parameters for each replicated data set, which is why we do not
5 Philip Smith (acting as reviewer) captured the importance of this
practice particularly well, and we quote him here: “Conventional statistics
treats the ﬁts to the individual distributions as ‘error’ and then tends to
ignore them—a practice that seems to carry over into the Bayesian setting.
However, in the cognitive model the quality of these ﬁts is absolutely
central to the adequacy of the model, and any evidence of misﬁt wholly
vitiates the enterprise. It follows that ‘cognitive psychometrics’ will require
us to adopt different habits of data analysis and that the practice of what,
in conventional statistics, is known as the ‘analysis of residuals’ will have
to become the primary focus of modeling. This is why graphical contact
between model and data is so important.”
A sample graphical model. The shaded node y(ij) indicates the
(bivariate) data. Nodes , , , and (ij) are parameters of the distribution
of y(ij). In turn,
are parameters of the distribution of (ij).
VANDEKERCKHOVE, TUERLINCKX, AND LEE
contrast perception and apply mainly regression-type analyses, as
well as trial-to-trial variability in drift rate, initial bias, and nondecision time. We also include a basic hierarchical structure,
namely, the addition of random variability over conditions. Note
that in this example, we do not consider individual differences or
complex hierarchical structures. We use the ﬁrst application
mainly to demonstrate the basic features of the diffusion model,
the Bayesian modeling approach, the principles of Bayesian model
selection, and the relative ease with which these otherwise involved analyses can be performed.
In the second application, the data set is different because it has
more participants (P  9), and we construct an HDM that permits
the simultaneous analysis of data from different individuals.
Thanks in part to the Bayesian framework, we are able to deﬁne a
statistic that directly quantiﬁes the effect under consideration and
estimate the distribution of its size in the population.
In both of the examples, we make a large number of assumptions regarding the structure in the data. We, sometimes somewhat
arbitrarily, select whether certain parameters are allowed to change
between experimental units, whether effects are ﬁxed or random,
and which parametric forms are taken by population distributions
or regression functions. These assumptions are often debatable, but
the central point to be made here is that a wide variety of assumptions can be made explicit in the HDM framework with relative
ease. For the purposes of illustrating this, it is not of crucial
importance exactly which assumptions are made. In real-world
applications the tools that are developed allow for the checking of
assumptions.
Example 1: Fixed Effects and Nonlinear Regression
Introduction.
The ﬁrst application example involves a data
set in a contrast discrimination task that has become something of
a benchmark for RT model ﬁtting . An
important reason for this is that these data clearly show the
standard RT phenomena for which any model of choice RT should
be able to account (see the earlier section entitled The Diffusion
Model). In the experiment, three participants saw 10 blocks of
trials (after two practice blocks). Each trial consisted of a grid with
75% gray pixels and the remaining 25% either black or white.
There were 33 different proportions of black versus white pixels
(evenly spaced, so that the middle level was 50% black and 50%
white), and the task was to determine whether this proportion was
a draw from a “bright” or from a “dark” distribution. Additionally,
in half of the blocks, the participants were asked to respond as
accurately as possible (accuracy condition [AC]), and in the other
half, to be as fast as possible (speed condition [SC]).
The research goal in this study was to study the relationship
between stimulus brightness and drift rate. A link was clearly
conﬁrmed, and it was found that this link was nonlinear in nature.
Here, we go two steps further. First, we formalize the nonlinear
relation using a cumulative Weibull link function, which is a
nonlinear function that is common in the vision literature . Then we investigate the effect of the
instruction (AC vs. SC) on the relation between stimulus brightness and drift rate—because it could be hypothesized that a task
instruction affects the rate of information processing in the decision-making system, which would show itself in a different shape
of the link function. We focus on a single participant’s data.
As an introductory example, we apply a basic HDM
to these data. However, the features added to the Wiener diffusion
are not limited to the trial-to-trial variance used by Ratcliff and
Rouder : We also implement a nonlinear regression and
allow a difference between the instruction conditions. Speciﬁcally,
let C(s)  s/32 (s  0, . . ., 32) be a measure of intensity (i.e.,
brightness) and i (i  1 for AC; i  2 for SC) be the instruction
condition. The index j (j  1, . . ., J(si)) separates different trials for
a given intensity-by-instruction combination. Considering only
one participant (so that we can drop the index p), we have the
following model for the observed response vector Y(sij):
Ysij  Wienersij ,sij ,sij ,sij .
We assume that the parameter  was subject to only a ﬁxed effect
of instruction, as seen in
sij  i
(because it captures the speed–accuracy trade-off, which is exactly
what the instructions were), whereas  (initial bias),  (nondecision time), and  (rate of information uptake) are made subject to
random effects of trial, as seen in
sij  Ui
sij  N, 2 , and
sij  N
The mean of the trial-to-trial distribution of  is additionally
subject to a random condition effect, as seen in
which introduces a key ability of the HDM. Here it becomes most
clear why these models are called hierarchical, because “layers” of
randomness are added incrementally (in this case, one at the
condition level and one at the trial level). (Note that this means that
we predict
(si) to be identical if the same combination of stimulus
and instruction is presented twice. However, that does not imply
that the drift rate (sij) will be identical as well, because it is still a
draw from N(
2).) In this context,
(si) is the more interesting
parameter, because it pertains directly to the quality of the stimulus
but is not confounded by random trial-to-trial ﬂuctuations . The model with this set of assumptions will be
called BM1 (see Figure 3 for a graphical model illustration). Note
that model BM1, although acknowledging the possibility of a
difference between the 66 drift rates, contains no information to
quantify the differences between the conditions (i.e., nothing links
it to brightness): We are ignoring the available covariate information and are assuming that all 66 drift rates are drawn from the
same pool (with mean
and variance ε
We model the experimental manipulation of speed-versus-accuracy instruction as a ﬁxed effect because these levels constitute an
exhaustive list of the possibilities—they are not a random selection
from a larger pool of possible instructions. By contrast, the differ-
HIERARCHICAL DIFFUSION MODELS
ence between trials is modeled as a random effect, because we are
not (currently) interested in the effects of particular trials—we
assume those to be a selection from a larger pool of possible trials.
In the ﬁrst model, we pretend to have no knowledge of the nature
of the differences in stimulus intensity but assume them to be a
random draw from the large set of possible stimulus intensities.
To continue, we can deﬁne multiple competing models. Ratcliff
and Rouder’s model did not restrict the across-condition
drift rate distributions.6 In contrast, we now deﬁne a second model
in which we formalize the connection between stimulus intensity
and drift rate with a Weibull regression. Formally, we redeﬁne
low 1  exp[(C(s)/
shape]}ε(si),
with upper and lower asymptotes
low, shape parameter
shape, scale
scale, and an error term ε(si)
2). Note that
although ε
2 in BM1 indicated the across-condition variability in
(si), here it refers to the residual variability after accounting for the
nonlinear effect of the brightness condition. Importantly, the ability to quantify residual variability after controlling for the effect of
the brightness condition allowed us to investigate the magnitude of
interstimulus variability that is not due to an experimental manipulation (but rather due to other manipulations or due to random,
uncontrolled differences between stimuli). The choice of the
Weibull function as a regression function is somewhat arbitrary
and can be adapted at will. For our purposes, the Weibull is a
nonlinear function with two horizontal asymptotes, a scale, and a
shape. The second model, now completely speciﬁed, is called
Model BM2.
However, we had originally set out to investigate the effect of
the experimental instruction on the drift rates. We therefore constructed a third model in which we allowed a difference in the drift
rate distributions as a function of the instruction condition, using
the link function
low 1  exp[(C(s)/
shape]}ε(si).
Note that we added subscripts i to the Weibull’s parameters to
indicate their dependence on the instruction condition. This model
is called BM3. The three models are displayed as graphical models
in Figure 3.
The means and standard deviations of the marginal
posteriors for some of the parameters in each model are given in
Table 2. Several results are immediately obvious. First, the parameters AC and SC are very different: The boundary separation in
the SC is much lower than in the AC, in all models. This is
consistent with the interpretation of that parameter. Second, the
posterior standard deviations are generally small compared with
the posterior means (EAPs), indicating narrow distributions and
therefore reliable estimates. Third, the mean nondecision time  is
about 270 ms, and its trial-to-trial standard deviation  is about 41
ms, which is normal in this type of application. Fourth, estimates
of the parameters that do not pertain to the Weibull regression
remain more or less constant between different models, indicating
that the models’ restrictions on the drift rate parameters do not lead
to trade-off effects for other parameters. Finally, the parameter ε,
which indicates the amount of unexplained variability in drift rates,
strongly differs between models—apparently the added covariates
do explain a fair amount of variance. We therefore used the
difference in unexplained stimulus variance as a quality measure
of the Weibull regression, using a statistic akin to the familiar
where in this case total was ε in BM1 and res was ε in the
model with which we wanted to compare. Given a series of
samples from each of these parameters, we computed a posterior
mean for the proportion of variance that was explained by the
addition of the nonlinear regressions.7 In BM2, the proportion of
variance explained was 96.50%, whereas in BM3 it was as high as
In Table 3, the parameters of the Weibull regression are shown
for BM2 and BM3. It is clear from the posterior means and
standard deviations that the Weibull regression function is quite
different between the two instruction conditions. In particular, the
upper and lower asymptotes are more extreme in the SC, and the
function is somewhat steeper in that condition as well. In fact,
according to the analysis, P(
shape  D)  .9590. Figure 4
shows the Weibull regression lines for each instruction condition
and the individually estimated drift rates. To compare the performance of the three models, we computed DIC values for each
model and found that BM3 performed best (DIC was 3,373.40,
2,087.60, and 642.63, for BM1, BM2, and BM3, respectively).
Finally, we provide a graphical illustration of the model ﬁt using
posterior predictive checks. We generated, on the basis of 1,000
samples from the posterior, 1,000 posterior predictive response
probabilities (i.e., expected probability of a “bright” response) and
posterior predictive mean RTs for each of the 66 conditions. Figure
5 shows the generated probabilities (as gray dots) overlaid with the
observed data (black line). As can be seen, the changes in response
probability and RT over brightness conditions are well captured by
the model (even the somewhat capricious behavior near the extremes is well within the posterior uncertainty of the ﬁtted model).8
Conclusion.
Although the model we have applied to these
data is quite different from the one used by Ratcliff and Rouder
 , our conclusions generally echo theirs, with one signiﬁcant
difference: We ﬁnd an effect of instruction on drift rate. The
Weibull link functions are manifestly different between the instruction conditions—evidently the rate of information accumulation is not entirely independent of the participants’ motivations
6 Ratcliff and Rouder do mention that they could (in principle)
further simplify the model by implementing a regression of mean drift rate as
a linear function of the probability that the stimulus was a draw from the
“bright” distribution, that is,
(ps)  (p)0  (p)1P(s), with P(s)  N(s  1, )/
[N(s  1, )  N(s  2, )] and 1  5/8, 2  3/8, and   3/16. However,
they did not actually apply this regression. Similar nonlinear regression models
of drift rate (cast in a classical statistical framework) have been investigated by
Smith, Ratcliff, and Wolfgang and Palmer, Huk, and Shadlen .
7 Because we were not dealing with a linear model and were in fact
comparing across models with strongly different assumptions, the R2
statistic used here is not exactly the same as the familiar statistic. However,
for the purpose of comparing model ﬁts, we believe it is a succinct
summary measure.
8 Further posterior predictives for this data set and a similar model are
reported in Vandekerckhove, Tuerlinckx, and Lee .
VANDEKERCKHOVE, TUERLINCKX, AND LEE
 .
Although we do not describe them here, the results were analogous
for the other two participants.
In addition to the relative ease with which it was applied (only
30 or so lines of highly redundant WinBUGS code; see the
Supplementary Materials), the model just described contains two
properties that are fundamentally novel in the domain. Trial-totrial variance and constraints on parameters have already been
applied , but the
application of Bayesian inference and in particular the addition of
random effects on the condition (stimulus) level are new. Random
effects are an important modeling construct that has not previously
been considered in this context. In the next example, we focus
more closely on the addition of random effects.
Example 2: ANOVA and Random
Person–Domain Effects
Introduction.
In the previous application, we focused on
single participants (mainly because the data set contained only
three participants in total). However, one of the more signiﬁcant
advantages of the hierarchical setting is that it allows for the
simultaneous analysis of many participants’ choice response time
A graphical model representation of each of the models for the ﬁrst application. (a) In Model BM1,
, , and  are indexed for instruction conditions i, stimuli s, and trials j, indicating that we allowed these
parameters to be different for each condition-by-stimulus-by-trial combination. , on the other hand, is indexed
by only i, for condition. The population parameters for , called high and low, also depend on only the
instruction condition.
may differ between stimuli and instruction conditions, but its population distribution has
invariant parameters
and ε. Finally,
, , and  do not vary between conditions or stimuli either. (b) In Model
BM2, the mean of
is determined not by one but by four parameters (the parameters of the Weibull link). (c)
In Model BM3, those Weibull parameters are moved inside the plate over i, so that they may differ between
instruction conditions.
HIERARCHICAL DIFFUSION MODELS
data. For example, diffusion parameters could be kept constant
across items for each participant, but individual participants’ parameters would be considered random draws from a population
distribution. This would be a very typical hierarchical model; van
der Linden would call this a population model. Analyzing
data from different participants simultaneously results in greater
stability for the statistical inferences. In particular, it becomes
possible to ﬁt the model even with relatively few data points per
participant.
However, because it remains unreasonable to assume that all
parameters stay exactly constant across trials, we combined mixing
over trials with mixing over persons. This yielded a multilevel
random effects design wherein the parameters of individual participants’ mixing distributions were themselves draws from a
population-level distribution. A graphical representation of this
multilevel diffusion model is given in Figure 6. The data set to
which we applied this model was taken from a change detection
study . For a detailed description of the research questions, the reader is referred to Vandekerckhove et al. . For
the purposes of our demonstration, it sufﬁces to know that the
difﬁculty of a visual detection task was manipulated in a 2  2
factorial design and that there were nine participants. The independent variables of interest are called Q, for quality, and T, for
type. Because the manipulations were all intended to affect higher
order properties of the stimulus, we expected changes in drift rate
but not in any other variable. The main research question was
whether there is an effect of T on detection performance and
whether this effect is independent of Q. It is hence a straightforward ANOVA-type design, and we were interested in the main
effect of T and the T  Q interaction. The factorial design is given
in the second and third columns of Table 4.
We deﬁne only one model, which includes a hierarchical structure that simultaneously incorporates different participants’ data. The assumptions of this population-hierarchical model
(PHM) are as follows.
The core of the PHM is still the Wiener diffusion model, so that
each individual data point y(pij) follows a Wiener distribution with
four parameters, as seen in
ypij  Wienerpij ,pij ,pij , pij ,
with indices p for participants (p  1, . . ., 9), i for conditions (i 
1, . . ., 4), and j for trials (j  1, . . ., 80). We assume an unbiased
diffusion process: (pij)  .5.
The hierarchical structure now contains two levels of random
variation: the trial level and the participant level. At the trial level,
the nondecision time  and drift rate  are assumed to vary between
trials, as seen in (pij)
N((p), (p)
2 ) and (pij)
contrast, we treated the boundary separation as constant across
trials (for a given participant), as in (pij)  (p).
At the participant level, although the boundary separation is
assumed constant across trials, at a higher level of heterogeneity,
interindividual differences arise. We treated all interindividual
differences as random effects (because we knew that participants
were a random sample from a larger population), as seen in (p)
Further interindividual differences are allowed; that is, the parameters of the intertrial mixing distributions (those for  and ,
described earlier) depend on participant p and may depend on
condition i (in the case of drift rate), as seen in (p)
Note that the ﬁxed effect of condition i remains present in the
dependence of
(i) on i, but now it exists on the population level.
It is not necessary to deﬁne the factorial structure of the conditions
in the experiment at this stage; because the parameters in a linear
model that quantify main effects and interactions are mere linear
combinations of the data (i.e., the mean in each condition), we
could compute posterior distributions for each conditional mean
ﬁrst and derive the posterior distributions of the ANOVA parameters later9 (see the next section).
Finally, although it was not the primary focus of the present
analysis, the trial-to-trial variability parameters were also given
population distributions, as seen in (p)
2). We did this primarily to formalize our knowledge that
participants were a random selection from a pool yet did exhibit
interindividual differences.
9 Parameters that are not directly estimated themselves but are obtained
from transformations or combinations of other parameters are sometimes
called derived parameters or structural parameters .
Some Parameter Estimates for the First Application
Posterior SD ( 100)
Parameter interpretation
Caution (AC)
Caution (SC)
Mean nondecision time
Intertrial SD of nondecision time
Intertrial SD of drift rate
Lower limit of initial bias (AC)
Upper limit of initial bias (AC)
Lower limit of initial bias (SC)
Upper limit of initial bias (SC)
Residual interstimulus SD of drift rate
BM1–BM3 refer to the model names. EAP  expected a posteriori; AC  accuracy condition; SC  speed condition.
VANDEKERCKHOVE, TUERLINCKX, AND LEE
In all cases, population distributions are truncated to a reasonable interval (for numerical stability; see the Supplementary Materials for the intervals).
We were interested in two different aspects of the
results. For the experimenter, it is important to know whether a
main effect of T and a T  Q interaction appear on the mean drift
(i). From a general-interest perspective, we were additionally interested in the population-level variability of the different
parameters.
Summary statistics of the obtained drift rate population distributions (per condition) are given in Table 1. It can be seen that the
distributions differ strongly between conditions. In order to more
precisely investigate our hypotheses, we transformed the drift rate
distributions into ANOVA contrast parameters that exactly quantiﬁed the effects in which we were interested. First, the main effect
of T is given by the contrast T  (
for which the posterior distribution is shown in Figure 7. It is clear
from that ﬁgure that there is strong evidence for a main effect of
T, averaged over levels of Q. Indeed, P(T  0  D)  0. Similarly,
in the second panel in Figure 7, we can conﬁrm that there is a main
effect of Q, because for Q  (
P(Q  0  D)  .994. To investigate the interaction, we computed
the interaction contrast I  (
it turned out, P(I  0  D)  .886, providing only marginally
convincing evidence for an interaction. The interaction pattern is
shown in Figure 8.
The population variability in the parameters is directly quanti-
ﬁed by their variance parameters. Although not the focus of the
present experiment, each of these parameters has a unique interpretation that may be relevant in other contexts (here their main
purpose was to account for extraneous variability in the data). For
example, the EAP of the interindividual standard deviation of
boundary separation  estimates to 0.0541 and that of the interindividual standard deviation of mean nondecision time  
The Weibull regression on drift rate. Both panels, one for each instruction condition, contain
individually estimated drift rates for each level of stimulus intensity (
(si) from BM1), with error bars extending
one posterior standard deviation in both directions. Overlaid are the Weibull regression lines (from BM3), based
on the posterior means of
scale, and
shape from BM3. The Weibull function captures the effect of
stimulus intensity well.
Parameter Estimates of the Weibull Regression in the First Application
Posterior SD ( 100)
Parameter interpretation
Upper asymptote (AC)
Upper asymptote (SC)
Lower asymptote (AC)
Lower asymptote (SC)
Location (AC)
Location (SC)
Steepness (AC)
Steepness (SC)
BM2 and BM3 refer to the model names, and BM2 does not allow for differences between AC and SC. EAP  expected a posteriori; AC 
accuracy condition; SC  speed condition.
HIERARCHICAL DIFFUSION MODELS
0.0663. By themselves, these numbers mean little, but given these
estimated population distribution parameters and their remaining
uncertainty (i.e., the posterior variance of these parameters), we
could now depict the distribution of the model parameters in the
population by computing posterior predictive distributions. Take,
for illustration, the population distribution of . Given a single
(s) from the posterior distribution of
, and a single
(s) from the posterior distribution of , one can generate
a single sample (s). Repeating this procedure many times yields a
vector of  values that are sampled from the population distribution. Thus, a sufﬁciently high number of samples obtained this way
represents the expected population distribution of . Figure 9
shows these predicted population distributions for the  and 
parameters. The parameter estimates for the nine participants in the
experiment are shown as circles under the distribution curve.
Figure 9 invites several insights with substantive implications
regarding the hierarchical diffusion model. First, it can be seen that
population variability in  is quite large—it spans almost the entire
range of —whereas it is comparatively small for .10 Also,
although the  parameters seem to follow a bell-shaped distribution,  parameters are more spread out and even appear to occur in
The large interindividual differences in boundary separation
support a general argument of choice RT modeling as an improvement over models for accuracy or RT only: If there are large
differences in how cautiously participants respond to stimuli, pure
accuracy or pure RT data may paint a deceptive picture. The
diffusion model allows one to quantify differences in participant
caution, and the HDM framework can be used to model boundary
separation (or indeed any other diffusion model parameter) at the
population level.
To conclude the discussion of the parameter estimates, it can be
interesting to compare the size of the variance on the trial level
with that on the population level. In the present model this is
possible for the nondecision time and for the drift rate in each
condition. For the nondecision time, the interindividual standard
deviation of , , is estimated at 0.0659, and the average intertrial
standard deviation,
, is of a similar magnitude: 0.0807. For the
drift rates, however, the average intertrial standard deviation,
0.3500, clearly exceeds the interindividual standard deviations,
(i): 0.0858, 0.1496, 0.1388, and 0.1169.
Finally, in order to evaluate absolute model ﬁt, we generated
posterior predictives by simulating 1,000 new data sets from the
joint posterior distribution of the parameters (i.e., we generated
data as predicted by the ﬁtted model). Then we pooled these data
sets in each person-by-condition cell of the design and constructed
a histogram for each of these pooled data sets. Figure 10 shows this
histogram for each condition for three participants. The black line
is a (smoothed) histogram of the simulated data, whereas the gray
bars indicate the real data. The RTs for error responses were given
a negative sign, so that the inverted distribution on the left side of
the vertical axis indicates the error response distribution. Each cell
contains 80 responses. The ﬁgure does not seem to betray any large
systematic misﬁt of the model.
10 See Matzke and Wagenmakers for plausible ranges of diffusion model parameters, as found in published studies.
accuracy instruction
mean reaction time
speed instruction
p(’bright’)
brightness
brightness
A comparison of posterior predictive mean reaction times (top
panels) and the probability of “bright” responses (bottom panels) with the
empirically observed quantities. The gray crosses indicate the model predictions (which have some variability due to posterior uncertainty), and the
black lines indicate the data. Note the different scales in the two reaction
time graphs.
The graphical model representation of the population-hierarchical model. The shaded node indicates the observed data, and its direct
contributors are (clockwise starting from the left) the drift rate , boundary
separation , and nondecision time . Drift rate and nondecision time have
across-trial means (
and , respectively) and standard deviations (
respectively). The rest of the parameters belong to prior distributions.
Subscripts (and plates) indicate repetitions of the parameter across participants p, conditions i, and trials j. See text for more details.
Posterior Distributions of the Mean Drift Rate in the
Population, per Condition
Condition i
Where T  0, the population distribution of the drift rate has much
mass around 0. T  type; Q  quality;
(i)  posterior mean; 
posterior standard deviation.
VANDEKERCKHOVE, TUERLINCKX, AND LEE
Conclusion.
In this advanced application, we applied a population-hierarchical model to choice response time data. We computed ANOVA-style contrasts for the 2  2 factorial design and
found two main effects of the independent variables on the drift
rate parameter. The population-hierarchical diffusion model is
especially noteworthy because it combines information from different participants (and conditions) in a single model, allowing for
more reliable parameter estimates and hypothesis tests at the
population level.
Software Implementation
We believe the hierarchical extension of the Wiener diffusion
process has much potential for the ﬁeld of cognitive science.
However, applying this model in practice is difﬁcult and may be
prohibitively onerous for many researchers. For this reason, it is
important also to publish computer software to aid in the application of the HDM.
Our software implementation is not a full software package but
rather a plug-in to an existing software package, WinBUGS. A
guide in the Supplementary Materials details how the software and
plug-ins need to be installed and also presents some examples of
usage. Note that the software is dependent on Microsoft Windows
and cannot currently be made to run optimally on other systems.
Discussion
We have introduced a hierarchical extension of the Wiener
diffusion model for two-choice response times (hierarchical diffusion model, or HDM). With two examples, we have demonstrated
the feasibility of the HDM. In strictly formal terms, the HDM is
just another nonlinear mixed model, but it is special because it has
a bivariate random variable at the measurement level. As a new
data analysis approach, it is characterized by great ﬂexibility
compared with existing treatments of choice response time data.
Additionally, by using the Wiener diffusion process as the measurement level, the hierarchical analysis can be performed on
parameters that have well-deﬁned substantive interpretations.
The substantively interesting process interpretation of the diffusion model parameters is important for several reasons. First, it
is particularly appealing in the context of Borsboom’s view
that the fact that measurement models lack substantive foundation
is the main reason psychometrics has had a limited impact. That is,
although the analysis of choice response time data in a hierarchical
framework has already been addressed in the psychometric literature , our use of a popular process model such as the diffusion
model is novel. At the same time, of course, the process interpretation limits the applicability of the present approach, because one
is now required to assume that the process assumptions are not
violated. As a result, certain types of data (e.g., long response
times that would belie the assumption that a single informationgathering process without regulatory processes is generating the
data) cannot be dealt with by the HDM.
Second, thanks to the substantively interesting process interpretation of the diffusion model parameters, the HDM framework is
an instance of cognitive psychometrics, a relatively young subdiscipline of psychology. In this subdiscipline, models of cognition
are extended to encompass individual differences (i.e., participants
are no longer considered as mere replications of one another;
posterior density
posterior density
Posterior distributions of the main effect contrast parameters of
the analysis of variance in Example 2. In the left panel, T has all its mass
above zero, indicating strong evidence for a main effect of type (T),
averaged over levels of quality (Q). In the right panel, a negative main
effect of quality is clear, because more than 99% of the mass of the
posterior distribution of Q is below zero. See text for more details.
population mean drift rate µν
Drift rate parameters for the second application. Error bars
extend one posterior standard deviation in either direction. The main effect
of type is clearly seen. There is evidence of a superadditive interaction.
population density
population density
Posterior predictive population distributions of two parameters
of the population-hierarchical model (person-speciﬁc mean nondecision
time  and person-speciﬁc boundary separation ). The population distribution of  is bell-shaped and narrow. The population distribution of  is
HIERARCHICAL DIFFUSION MODELS
Batchelder, 1998) in order to allow for population-level inferences.
This strategy has gained some momentum recently, with several
articles applying hierarchical models to pool data over participants
 , but the approach is
presently far from mainstream. We believe that cognitive psychometrics—the result of trading between subdisciplines as advocated
by Cronbach —has many possible applications in a wide
variety of domains where psychological measurement is used.
We have elected to implement the HDM using Bayesian statistical methods. This choice was inﬂuenced by many factors, both
practical and principled. An important corollary of the Bayesian
framework is that results from such an analysis have direct and
often intuitive interpretations. In one of the examples, we derived
posterior distributions of ANOVA contrasts, from which we could
directly draw (probabilistic) inferences regarding the hypotheses at
In order to facilitate the dissemination of hierarchical models
(i.e., cognitive psychometrics) into mainstream cognitive science,
we have provided software with which a hierarchical model for
two-choice response time data can be put into practice. Although
this software has some limitations (in particular, somewhat inef-
ﬁcient sampling due to WinBUGS’s use of general-purpose rather
than tailor-made sampling algorithms), we believe it may be useful
for a wide audience.