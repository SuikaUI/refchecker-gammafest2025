Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 383–391
Copenhagen, Denmark, September 7–11, 2017. c⃝2017 Association for Computational Linguistics
Unsupervised Pretraining for Sequence to Sequence Learning
Prajit Ramachandran and Peter J. Liu and Quoc V. Le
Google Brain
{prajit, peterjliu, qvl}@google.com
This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq)
In our method, the weights of
the encoder and decoder of a seq2seq
model are initialized with the pretrained
weights of two language models and then
ﬁne-tuned with labeled data.
We apply this method to challenging benchmarks in machine translation and abstractive summarization and ﬁnd that it signiﬁcantly improves the subsequent supervised models.
Our main result is that
pretraining improves the generalization of
seq2seq models. We achieve state-of-theart results on the WMT English→German
task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our
method achieves a signiﬁcant improvement of 1.3 BLEU from the previous best
models on both WMT’14 and WMT’15
English→German. We also conduct human evaluations on abstractive summarization and ﬁnd that our method outperforms a purely supervised learning baseline in a statistically signiﬁcant manner.
Introduction
 are extremely effective
on a variety of tasks that require a mapping
between a variable-length input sequence to
a variable-length output sequence.
weakness of sequence to sequence models, and
deep networks in general, lies in the fact that they
can easily overﬁt when the amount of supervised
training data is small.
In this work, we propose a simple and effective technique for using unsupervised pretraining
to improve seq2seq models. Our proposal is to
initialize both encoder and decoder networks with
pretrained weights of two language models. These
pretrained weights are then ﬁne-tuned with the labeled corpus. During the ﬁne-tuning phase, we
jointly train the seq2seq objective with the language modeling objectives to prevent overﬁtting.
We benchmark this method on machine translation for English→German and abstractive summarization on CNN and Daily Mail articles. Our
main result is that a seq2seq model, with pretraining, exceeds the strongest possible baseline
in both neural machine translation and phrasebased machine translation.
Our model obtains
an improvement of 1.3 BLEU from the previous best models on both WMT’14 and WMT’15
English→German. On human evaluations for abstractive summarization, we ﬁnd that our model
outperforms a purely supervised baseline, both in
terms of correctness and in avoiding unwanted
repetition.
We also perform ablation studies to understand
the behaviors of the pretraining method. Our study
conﬁrms that among many other possible choices
of using a language model in seq2seq with attention, the above proposal works best. Our study
also shows that, for translation, the main gains
come from the improved generalization due to the
pretrained features. For summarization, pretraining the encoder gives large improvements, suggesting that the gains come from the improved optimization of the encoder that has been unrolled
for hundreds of timesteps. On both tasks, our proposed method always improves generalization on
the test sets.
First RNN Layer
Second RNN Layer
Figure 1: Pretrained sequence to sequence model. The red parameters are the encoder and the blue
parameters are the decoder. All parameters in a shaded box are pretrained, either from the source side
(light red) or target side (light blue) language model. Otherwise, they are randomly initialized.
In the following section, we will describe our basic
unsupervised pretraining procedure for sequence
to sequence learning and how to modify sequence
to sequence learning to effectively make use of the
pretrained weights. We then show several extensions to improve the basic model.
Basic Procedure
Given an input sequence x1, x2, ..., xm and an
output sequence yn, yn−1, ..., y1, the objective of
sequence to sequence learning is to maximize the
likelihood
p(yn, yn−1, ..., y1|x1, x2, ..., xm).
p(yn, yn−1, ..., y1|x1, x2, ..., xm)
t=1 p(yt|yt−1, ..., y1; x1, x2, ..., xm).
In sequence to sequence learning, an RNN encoder is used to represent x1, ..., xm as a hidden
vector, which is given to an RNN decoder to produce the output sequence. Our method is based
on the observation that without the encoder, the
decoder essentially acts like a language model on
y’s. Similarly, the encoder with an additional output layer also acts like a language model. Thus it
is natural to use trained languages models to initialize the encoder and decoder.
Therefore, the basic procedure of our approach
is to pretrain both the seq2seq encoder and decoder networks with language models, which can
be trained on large amounts of unlabeled text data.
This can be seen in Figure 1, where the parameters in the shaded boxes are pretrained. In the following we will describe the method in detail using
machine translation as an example application.
First, two monolingual datasets are collected,
one for the source side language, and one for the
target side language. A language model (LM) is
trained on each dataset independently, giving an
LM trained on the source side corpus and an LM
trained on the target side corpus.
After two language models are trained, a multilayer seq2seq model M is constructed. The embedding and ﬁrst LSTM layers of the encoder and
decoder are initialized with the pretrained weights.
To be even more efﬁcient, the softmax of the decoder is initialized with the softmax of the pretrained target side LM.
Monolingual language modeling losses
After the seq2seq model M is initialized with the
two LMs, it is ﬁne-tuned with a labeled dataset.
However, this procedure may lead to catastrophic
forgetting, where the model’s performance on the
language modeling tasks falls dramatically after
ﬁne-tuning . This may
hamper the model’s ability to generalize, especially when trained on small labeled datasets.
To ensure that the model does not overﬁt the labeled data, we regularize the parameters that were
pretrained by continuing to train with the monolingual language modeling losses. The seq2seq and
language modeling losses are weighted equally.
In our ablation study, we ﬁnd that this technique
is complementary to pretraining and is important
in achieving high performance.
Other improvements to the model
Pretraining and the monolingual language modeling losses provide the vast majority of improvements to the model. However in early experimentation, we found minor but consistent improvements with two additional techniques: a) residual
connections and b) multi-layer attention (see Figure 2).
Figure 2: Two small improvements to the baseline
model: (a) residual connection, and (b) multi-layer
attention.
Residual connections:
As described, the input
vector to the decoder softmax layer is a random
vector because the high level (non-ﬁrst) layers of
the LSTM are randomly initialized. This introduces random gradients to the pretrained parameters. To avoid this, we use a residual connection
from the output of the ﬁrst LSTM layer directly to
the input of the softmax (see Figure 2-a).
Multi-layer attention:
In all our models, we use
an attention mechanism ,
where the model attends over both top and ﬁrst
layer (see Figure 2-b). More concretely, given a
query vector qt from the decoder, encoder states
from the ﬁrst layer h1
1, . . . , h1
T , and encoder states
from the last layer hL
1 , . . . , hL
T , we compute the attention context vector ct as follows:
exp . We
used the WMT 14 training dataset, which is
slightly smaller than the WMT 15 dataset. Because the dataset has some noisy examples, we
used a language detection system to ﬁlter the
training examples.
Sentences pairs where either the source was not English or the target
was not German were thrown away.
This resulted in around 4 million training examples.
Following Sennrich et al. , we use subword units with 89500
merge operations, giving a vocabulary size around
90000. The validation set is the concatenated newstest2012 and newstest2013, and our test sets are
newstest2014 and newstest2015.
Evaluation on
the validation set was with case-sensitive BLEU
 on tokenized text using
multi-bleu.perl.
Evaluation on the test
sets was with case-sensitive BLEU on detokenized
text using mteval-v13a.pl. The monolingual
training datasets are the News Crawl English and
German corpora, each of which has more than a
billion tokens.
Experimental settings:
The language models
were trained in the same fashion as We used a 1 layer 4096 dimensional
LSTM with the hidden state projected down to
1024 units and trained for one
week on 32 Tesla K40 GPUs. Our seq2seq model
was a 3 layer model, where the second and third
layers each have 1000 hidden units. The monolingual objectives, residual connection, and the modiﬁed attention were all used. We used the Adam
optimizer and train with
asynchronous SGD on 16 GPUs for speed. We
used a learning rate of 5e-5 which is multiplied
by 0.8 every 50K steps after an initial 400K steps,
gradient clipping with norm 5.0 , and dropout of 0.2 on non-recurrent connections . We used early
stopping on validation set perplexity. A beam size
of 10 was used for decoding. Our ensemble is con-
newstest2014
newstest2015
Phrase Based MT 
Supervised NMT 
Edit Distance Transducer NMT 
Edit Distance Transducer NMT 
ensemble 8
Backtranslation 
Backtranslation 
ensemble 4
Backtranslation 
ensemble 12
No pretraining
Pretrained seq2seq
Pretrained seq2seq
ensemble 5
Table 1: English→German performance on WMT test sets. Our pretrained model outperforms all other
models. Note that the model without pretraining uses the LM objective.
Difference in BLEU
Pretrain on parallel corpus
No pretraining
Only pretrain embeddings
No LM objective
Only pretrain encoder
Only pretrain embeddings & LSTM
Only pretrain decoder
Pretrain on Wikipedia
Figure 3: English→German ablation study measuring the difference in validation BLEU between various
ablations and the full model. More negative is worse. The full model uses LMs trained with monolingual
data to initialize the encoder and decoder, plus the language modeling objective.
structed with the 5 best performing models on the
validation set, which are trained with different hyperparameters.
Table 1 shows the results of our
method in comparison with other baselines. Our
method achieves a new state-of-the-art for single model performance on both newstest2014
and newstest2015, signiﬁcantly outperforming the
competitive semi-supervised backtranslation technique . Equally impressive
is the fact that our best single model outperforms
the previous state of the art ensemble of 4 models.
Our ensemble of 5 models matches or exceeds the
previous best ensemble of 12 models.
Ablation study:
In order to better understand
the effects of pretraining, we conducted an ablation study by modifying the pretraining scheme.
We were primarily interested in varying the pretraining scheme and the monolingual language
modeling objectives because these two techniques
produce the largest gains in the model.
3 shows the drop in validation BLEU of various
ablations compared with the full model. The full
model uses LMs trained with monolingual data to
initialize the encoder and decoder, in addition to
the language modeling objective. In the follow-
ing, we interpret the ﬁndings of the study. Note
that some ﬁndings are speciﬁc to the translation
Given the results from the ablation study, we
can make the following observations:
• Only pretraining the decoder is better than
only pretraining the encoder: Only pretraining the encoder leads to a 1.6 BLEU point
drop while only pretraining the decoder leads
to a 1.0 BLEU point drop.
• Pretrain as much as possible because the beneﬁts compound: given the drops of no pretraining at all (−2.0) and only pretraining the
encoder (−1.6), the additive estimate of the
drop of only pretraining the decoder side is
−2.0 −(−1.6) = −0.4; however the actual
drop is −1.0 which is a much larger drop than
the additive estimate.
• Pretraining the softmax is important: Pretraining only the embeddings and ﬁrst LSTM
layer gives a large drop of 1.6 BLEU points.
• The language modeling objective is a strong
regularizer: The drop in BLEU points of pretraining the entire model and not using the
LM objective is as bad as using the LM objective without pretraining.
• Pretraining on a lot of unlabeled data is essential for learning to extract powerful features: If the model is initialized with LMs
that are pretrained on the source part and
target part of the parallel corpus, the drop
in performance is as large as not pretraining at all.
However, performance remains
strong when pretrained on the large, nonnews Wikipedia corpus.
To understand the contributions of unsupervised pretraining vs. supervised training, we track
the performance of pretraining as a function of
dataset size. For this, we trained a a model with
and without pretraining on random subsets of the
English→German corpus. Both models use the
additional LM objective. The results are summarized in Figure 4. When a 100% of the labeled
data is used, the gap between the pretrained and
no pretrain model is 2.0 BLEU points. However,
that gap grows when less data is available. When
trained on 20% of the labeled data, the gap becomes 3.8 BLEU points. This demonstrates that
the pretrained models degrade less as the labeled
dataset becomes smaller.
Percent of entire labeled dataset used for training
No pretrain
Figure 4: Validation performance of pretraining
vs. no pretraining when trained on a subset of the
entire labeled dataset for English→German translation.
Abstractive Summarization
Dataset and Evaluation:
For a low-resource
abstractive
summarization
CNN/Daily Mail corpus from . Following Nallapati et al. , we modify the data collection scripts to restore the bullet
point summaries. The task is to predict the bullet
point summaries from a news article. The dataset
has fewer than 300K document-summary pairs. To
compare against Nallapati et al. , we used
the anonymized corpus. However, for our ablation study, we used the non-anonymized corpus.1
We evaluate our system using full length ROUGE
 . For the anonymized corpus in particular, we considered each highlight as a separate sentence following Nallapati et al. . In
this setting, we used the English Gigaword corpus as our larger, unlabeled
“monolingual” corpus, although all data used in
this task is in English.
Experimental settings:
We use subword units
 with 31500 merges, resulting in a vocabulary size of about 32000. We
use up to the ﬁrst 600 tokens of the document and
1We encourage future researchers to use the nonanonymized version because it is a more realistic summarization setting with a larger vocabulary. Our numbers on
the non-anonymized test set are 35.56 ROUGE-1, 14.60
ROUGE-2, and 25.08 ROUGE-L. We did not consider highlights as separate sentences.
Seq2seq + pretrained embeddings 
+ temporal attention 
Pretrained seq2seq
Table 2: Results on the anonymized CNN/Daily Mail dataset.
Difference in ROUGE
No pretraining
Only pretrain decoder
No LM objective
Only pretrain embeddings
Only pretrain embeddings & LSTM
Only pretrain encoder
Pretrain on parallel corpus
Figure 5: Summarization ablation study measuring the difference in validation ROUGE between various
ablations and the full model. More negative is worse. The full model uses LMs trained with unlabeled
data to initialize the encoder and decoder, plus the language modeling objective.
predict the entire summary. Only one language
model is trained and it is used to initialize both
the encoder and decoder, since the source and target languages are the same. However, the encoder
and decoder are not tied. The LM is a one-layer
LSTM of size 1024 trained in a similar fashion to
Jozefowicz et al. . For the seq2seq model,
we use the same settings as the machine translation experiments. The only differences are that
we use a 2 layer model with the second layer having 1024 hidden units, and that the learning rate is
multiplied by 0.8 every 30K steps after an initial
100K steps.
Table 2 summarizes our results on the
anonymized version of the corpus. Our pretrained
model is only able to match the previous baseline seq2seq of Nallapati et al. .
Interestingly, they use pretrained word2vec vectors to initialize their word embeddings. As we show in our ablation study, just
pretraining the embeddings itself gives a large improvement. Furthermore, our model is a unidirectional LSTM while they use a bidirectional LSTM.
They also use a longer context of 800 tokens,
whereas we used a context of 600 tokens due to
GPU memory issues.
Ablation study:
We performed an ablation
study similar to the one performed on the machine translation model.
The results are reported in Figure 5. Here we report the drops on
ROUGE-1, ROUGE-2, and ROUGE-L on the nonanonymized validation set.
Given the results from our ablation study, we
can make the following observations:
• Pretraining appears to improve optimization: in contrast with the machine translation
model, it is more beneﬁcial to only pretrain
the encoder than only the decoder of the sum-
marization model. One interpretation is that
pretraining enables the gradient to ﬂow much
further back in time than randomly initialized
weights. This may also explain why pretraining on the parallel corpus is no worse than
pretraining on a larger monolingual corpus.
• The language modeling objective is a strong
regularizer: A model without the LM objective has a signiﬁcant drop in ROUGE scores.
Human evaluation:
As ROUGE may not be
able to capture the quality of summarization, we
also performed a small qualitative study to understand the human impression of the summaries
produced by different models. We took 200 random documents and compared the performance of
a pretrained and non-pretrained system. The document, gold summary, and the two system outputs were presented to a human evaluator who was
asked to rate each system output on a scale of 1-5
with 5 being the best score. The system outputs
were presented in random order and the evaluator
did not know the identity of either output. The
evaluator noted if there were repetitive phrases or
sentences in either system outputs. Unwanted repetition was also noticed by Nallapati et al. .
Table 3 and 4 show the results of the study. In
both cases, the pretrained system outperforms the
system without pretraining in a statistically significant manner. The better optimization enabled by
pretraining improves the generated summaries and
decreases unwanted repetition in the output.
Table 3: The count of how often the no pretrain
system (NP) achieves a higher, equal, and lower
score than the pretrained system (P) in the side-byside study where the human evaluator gave each
system a score from 1-5. The sign statistical test
gives a p-value of < 0.0001 for rejecting the null
hypothesis that there is no difference in the score
obtained by either system.
Related Work
Unsupervised pretraining has been intensively
studied in the past years, most notably is the work
by Dahl et al. who found that pretraining
with deep belief networks improved feedforward
No pretrain
No repeats
No repeats
Table 4: The count of how often the pretrain and
no pretrain systems contain repeated phrases or
sentences in their outputs in the side-by-side study.
McNemar’s test gives a p-value of < 0.0001 for
rejecting the null hypothesis that the two systems
repeat the same proportion of times.
The pretrained system clearly repeats less than the system
without pretraining.
acoustic models.
More recent acoustic models
have found pretraining unnecessary , probably because the reconstruction objective of deep
belief networks is too easy. In contrast, we ﬁnd
that pretraining language models by next step prediction signiﬁcantly improves seq2seq on challenging real world datasets.
Despite its appeal, unsupervised learning has
not been widely used to improve supervised training. Dai and Le ; Radford et al. are
amongst the rare studies which showed the beneﬁts of pretraining in a semi-supervised learning
setting. Their methods are similar to ours except
that they did not have a decoder network and thus
could not apply to seq2seq learning.
Similarly,
Zhang and Zong found it useful to add an
additional task of sentence reordering of sourceside monolingual data for neural machine translation. Various forms of transfer or multitask learning with seq2seq framework also have the ﬂavors
of our algorithm .
Perhaps most closely related to our method is
the work by Gulcehre et al. , who combined
a language model with an already trained seq2seq
model by ﬁne-tuning additional deep output layers. Empirically, their method produces small improvements over the supervised baseline. We suspect that their method does not produce signiﬁcant
gains because (i) the models are trained independently of each other and are not ﬁne-tuned (ii) the
LM is combined with the seq2seq model after the
last layer, wasting the beneﬁt of the low level LM
features, and (iii) only using the LM on the decoder side. Venugopalan et al. addressed (i)
but still experienced minor improvements. Using
pretrained GloVe embedding vectors had more impact.
Related to our approach in principle is the work
by Chen et al. who proposed a two-term,
theoretically motivated unsupervised objective for
unpaired input-output samples. Though they did
not apply their method to seq2seq learning, their
framework can be modiﬁed to do so. In that case,
the ﬁrst term pushes the output to be highly probable under some scoring model, and the second
term ensures that the output depends on the input.
In the seq2seq setting, we interpret the ﬁrst term
as a pretrained language model scoring the output sequence. In our work, we fold the pretrained
language model into the decoder. We believe that
using the pretrained language model only for scoring is less efﬁcient that using all the pretrained
weights. Our use of labeled examples satisﬁes the
second term. These connections provide a theoretical grounding for our work.
In our experiments, we benchmark our method
on machine translation, where other unsupervised
methods are shown to give promising results . In backtranslation , the trained
model is used to decode unlabeled data to yield
extra labeled data. One can argue that this method
may not have a natural analogue to other tasks
such as summarization. We note that their technique is complementary to ours, and may lead
to additional gains in machine translation.
method of using autoencoders in Cheng et al.
 is promising, though it can be argued that
autoencoding is an easy objective and language
modeling may force the unsupervised models to
learn better features.
Conclusion
We presented a novel unsupervised pretraining
method to improve sequence to sequence learning.
The method can aid in both generalization and optimization. Our scheme involves pretraining two
language models in the source and target domain,
and initializing the embeddings, ﬁrst LSTM layers, and softmax of a sequence to sequence model
with the weights of the language models. Using
our method, we achieved state-of-the-art machine
translation results on both WMT’14 and WMT’15
English to German. A key advantage of this technique is that it is ﬂexible and can be applied to a
large variety of tasks.