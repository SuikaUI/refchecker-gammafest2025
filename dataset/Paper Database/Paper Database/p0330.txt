UC Riverside
UC Riverside Previously Published Works
The geometry of weighted low-rank approximations
 
IEEE Transactions on Signal Processing, 51(2)
Manton, J H
Publication Date
2003-02-01
Peer reviewed
eScholarship.org
Powered by the California Digital Library
University of California
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 51, NO. 2, FEBRUARY 2003
The Geometry of Weighted
Low-Rank Approximations
Jonathan H. Manton, Member, IEEE, Robert Mahony, and Yingbo Hua, Fellow, IEEE
Abstract—The low-rank approximation problem is to approximate optimally, with respect to some norm, a matrix by one of
the same dimension but smaller rank. It is known that under
the Frobenius norm, the best low-rank approximation can be
found by using the singular value decomposition (SVD). Although
this is no longer true under weighted norms in general, it is
demonstrated here that the weighted low-rank approximation
problem can be solved by finding the subspace that minimizes a
particular cost function. A number of advantages of this parameterization over the traditional parameterization are elucidated.
Finding the minimizing subspace is equivalent to minimizing a
cost function on the Grassmann manifold. A general framework
for constructing optimization algorithms on manifolds is presented and it is shown that existing algorithms in the literature
are special cases of this framework. Within this framework, two
novel algorithms (a steepest descent algorithm and a Newton-like
algorithm) are derived for solving the weighted low-rank approximation problem. They are compared with other algorithms
for low-rank approximation as well as with other algorithms
for minimizing a cost function on a Grassmann manifold.
Index Terms—Grassman manifold, low-rank approximations,
optimization on manifolds, reduced rank signal processing.
I. INTRODUCTION
HE weighted low-rank approximation problem is to compute
for a given data matrix
and positive definite
symmetric weighting matrix
. Here, vec
denotes the vec operator , and it is important to note that
is more general than the usual weighted norm
, where tr
is the trace operator. The
minimizing
in (1) is the best rank
approximation of
under the norm
is the identity matrix, denoted
Manuscript received August 31, 2000; revised October 3, 2002. This work
was supported by the Australian Research Council. The first author is associated with the ARC Special Research Centre for Ultra-Broadband Information
Networks (CUBIN). The associate editor coordinating the review of this paper
and approving it for publication was Prof. Jian Li.
J. H. Manton is with the Department of Electrical and Electronic Engineering, The University of Melbourne, Parkville, Victoria, Australia (e-mail:
 ).
R. Mahony is with the Department of Engineering, Australian National University, Canberra, Australia (e-mail: ).
Y. Hua is with the Department of Electrical Engineering, University of California, Riverside, CA, 92521 USA (e-mail: ).
Digital Object Identifier 10.1109/TSP.2002.807002
, then (1) reduces to the well-studied unweighted
low-rank approximation problem. This paper analyzes the
geometry of the low-rank approximation problem, drawing
connections between the weighted and unweighted cases. It
then uses this analysis to construct efficient algorithms for
locally minimizing (1).
The weighted low-rank approximation problem has received
less attention in the literature than the unweighted low-rank
approximation problem . Presumably, this is because a
closed-form solution does not exist for the weighted low-rank
approximation problem in general. Furthermore, existing
algorithms for the weighted case only converge to a local
minimum of (1) in general. Despite this though, the following
applications illustrate that it is still beneficial to consider the
weighted low-rank approximation problem.
A. Applications
One application that benefits from the use of a weighted low
rank matrix approximation is the two-dimensional (2-D) filter
design problem. The approach in and to the 2-D filter
design problem is to start with a matrix
whose elements
correspond to samples of the desired frequency response and
then decompose the 2-D design task into a set of simpler
one-dimensional design tasks by applying the singular value
decomposition (SVD) to
. A disadvantage of using the SVD
to decompose the desired frequency response
is that it treats
all entries of
equally, which in some cases leads to degraded
designs. In order to discriminate between the important and
unimportant elements of
, the idea of replacing the SVD
with a weighted low-rank approximation was proposed in ,
 . (See for a design example.)
Although finding the global minimum of (1) is ideal, it may
still be the case that a filter design resulting from finding a local
minimum of (1) outperforms the unweighted filter design. Furthermore, since the performance of the resulting filter is readily
measurable, if it so happens that the weighted design is worse
than the unweighted design, the weighted low-rank approximation can be recomputed with a different initial condition in the
hopes that a better local minimum of (1) will be found.
A second application requiring the solution of a weighted
approximation
convolutive
reduced-rank Wiener filtering problem . Motivated by the
same idea of using a double minimization (2) to reformulate
the original optimization problem (1), it was shown in that
the convolutive reduced-rank Wiener filter can be found by
solving a weighted low-rank approximation problem. As in the
2-D filter design problem, because the mean-square error of the
resulting convolutive reduced-rank Wiener filter can be readily
1053-587X/03$17.00 © 2003 IEEE
MANTON et al.: GEOMETRY OF WEIGHTED LOW-RANK APPROXIMATIONS
calculated and compared with that of the standard (nonconvolutive) reduced-rank Wiener filter , , even a suboptimal
solution of (1) can lead to a convolutive reduced-rank Wiener
filter whose performance is verifiably better than that of the
standard reduced-rank Wiener filter.
B. Known Properties and Related Work
In (1), if
is the Frobenius norm. The
low-rank approximation problem with respect to the Frobenius
norm was first studied by Eckart and Young . They proved
is the compact SVD of
the best rank
approximation of
is obtained from
by setting all but the first
values to zero. This result is commonly referred to as the
Eckart–Young–Mirsky Theorem (Mirsky proved the result
also holds under the 2-norm).
The best unweighted rank
approximation
is also readily
computed from the eigenvector decomposition (EVD) of
contains the normalized (so that
eigenvectors associated with the
largest eigenvalues of
. This follows almost immediately from the
Eckart–Young–Mirsky Theorem and the fact that the eigenvectors of
are the right singular vectors of
. In a sense to be
made precise later, this result is generalized in the present paper
to the weighted case.
An alternative to performing an SVD or EVD and one that
immediately extends to the weighted case, is to first over-parameterize the problem to remove the rank constraint and then
apply an alternating projection algorithm . Specifically, the
algorithm proposed in works as follows. Replace
with the matrix product
Fix a value for
and minimize over
, then fix
, minimize
, and repeat until the product
converges. It can be
shown that, in general,
converges to a local minimum
Remark : If
in (1), then it is known that (1)
has no local minimum other than the global minimum. It does,
however, have saddle points.
Copious works deal with the unweighted low-rank approximation problem and applications thereof. This is because
appropriate usage of reduced-rank approximations can result
in increased computational efficiency and robustness against
noise and model errors. Fundamental results on optimal reduced-rank estimators and filters can be found in , , ,
 , – , , and . Other algorithms for solving
the (adaptive) unweighted low-rank approximation problem
include , . However, the only algorithm the authors are
aware of for solving the weighted low-rank approximation
problem is the alternating projection algorithm presented in
A variant of the low-rank approximation problem appears in
 and references therein. Specifically, uses a modified inverse power method to solve (1) under the extra constraints
is restricted to have some affine structure.
is diagonal. This variant is discussed further in Section II.
C. Contributions
The main contributions of this paper can be summarized as
• We introduce a novel reformulation of the weighted
low-rank approximation problem, which is more natural
reformulation traditionally used in
rank-reduced problems , .
• We determine conditions on the weighting matrix
closed-form solution of (1) to exist.
• We derive efficient numerical algorithms that converge to
a local minimum of (1).
• We compare the existing alternating projection algorithm
with the novel algorithms proposed here for solving (1).
The other contributions, arising from the reformulation of the
weighted low-rank approximation problem as a constrained
optimization problem on the Grassmann manifold, are the
following.
• We derive a general framework for minimizing a cost function on a Grassmann manifold.
• We prove that the algorithms in are a special case of
this framework.
• We discuss the advantages this framework has over the
narrower Riemannian framework in and, in particular,
why it is misleading to interpret the algorithms proposed
here as approximations of the Riemannian-based algorithms in .
These contributions are now discussed in relation to existing
results in the literature.
As already mentioned, the traditional approach to reduced-rank problems is to write the rank
of two matrices, where
columns, and
rows. The potential disadvantage of this approach is
that the decomposition
is not unique, or equivalently,
too many parameters are used to represent rank
The novel idea in this paper is to use a parameterization that
is one-to-one, thus reducing the number of parameters and accordingly reducing the dimension of the optimization problem.
This is achieved by reformulating (1) as an unconstrained optimization problem on a Grassmann manifold. (A Grassmann
manifold is the collection of all subspaces of a given dimension
 , .) The authors believe this reformulation to be more
natural than the
reformulation not only because the
parameterization is one-to-one but because conditions for (1) to
have a closed-form solution become readily apparent as well.
The reformulated problem of minimizing a cost function on
a Grassmann manifold can be solved numerically using the recent algorithms in , and indeed, it is candidly stated that such
an approach leads to algorithms that perform comparably with
the proposed algorithms in this paper. Nevertheless, for reasons
given in Section III, the approach taken here is to derive first
a more general framework for optimizing a cost function on a
manifold and then specialize it to the weighted low-rank approximation problem. Although the resulting algorithms might be
interpreted by some as “flat space approximations” of the algorithms in , Section III explains why this interpretation is
misleading; the algorithms in can just as well be interpreted
as approximations of the algorithms proposed here.
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 51, NO. 2, FEBRUARY 2003
Remark : If
, then the resulting cost function on the
Grassmann manifold takes a special form (specifically, it is a
generalized Rayleigh quotient cost function) for which dedicated minimization algorithms have been proposed in and
 . This specific case has also been studied in detail in .
The algorithms proposed for solving (1), which are a modified steepest descent method and a modified Newton method,
are shown to have the following advantages over the alternating
projection algorithm in . The alternating projection algorithm asymptotically has a linear rate of convergence,1 meaning
that often, a significant number of iterations are required to
achieve an acceptable accuracy. The modified Newton method
presented here overcomes this problem since, asymptotically, it
has a quadratic rate of convergence in general and a cubic rate
of convergence if
. Furthermore, simulations show that
closely spaced eigenvalues of
adversely affect the convergence rate of the alternating projection algorithm, whereas
they do not seem to affect the algorithms in this paper. [This observation is mathematically substantiated in for the special
in (1).] Yet another
advantage is that here the optimization algorithms work over an
dimensional space (assuming
), whereas the
alternating projection algorithm works over an
-dimensional space [see (1) for the definitions of
much larger than
is small, the reduction in dimension is
very significant. For instance, if
D. Organization of Paper
The rest of this paper is organized as follows. Section II shows
how the low-rank approximation problem can be solved by first
computing the minimizing subspace of a certain cost function.
It also derives conditions for (1) to have a closed form solution.
Section III derives a general framework for finding a minimizing
subspace of a cost function. It highlights the advantages of this
more general framework over the Riemannian-based framework
presented recently in . This framework is used to derive novel
steepest descent algorithms in Section IV and Newton methods
in Section V for solving the weighted low-rank approximation
problem. These algorithms are not standard steepest descent and
Newton algorithms; the cost function changes at each iteration.
A numerical study in Section VI demonstrates that the algorithms are superior to the classical alternating projection algorithm, which is the only other algorithm the authors’ are aware
of for solving the weighted low-rank approximation problem.
All proofs are relegated to Appendix A.
II. WEIGHTED LOW-RANK APPROXIMATION
This section derives a novel reformulation of the low-rank
approximation problem (1). This reformulation is used in this
1A linear rate of convergence means that the logarithm of the error decreases
linearly or, equivalently, that the number of correct digits in the answer increases
by approximately a fixed amount per iteration. Similarly, a quadratic rate of convergence means that the logarithm of the error decreases quadratically, implying
that the number of correct digits approximately doubles each iteration. It is a
standard result that steepest descent methods asymptotically have a linear rate
of convergence, whereas Newton methods asymptotically have a quadratic rate
of convergence .
section to derive conditions for (1) to have a closed-form solution, and it is used in subsequent sections to derive efficient
algorithms for converging to a local minimum of (1). Connections with the Riemannian SVD are also discussed.
Without loss of generality, it is assumed throughout that
are the number of rows and columns of the
data matrix
, simply replace
and adjust
in (1) accordingly.]
The underlying idea in this paper is to reformulate (1) as the
double-minimization
Close inspection shows that if
are the minimizing arguments of the two minimizations in (2), then
is the solution of the low-rank approximation problem (1); the restriction
enforces the constraint rank
since every
must belong to the null space of
. Theorem 1
below shows that the inner minimization has a closed-form solution. Moreover, because the inner minimization depends only
on the span of the columns of
and not on the individual elements of
, it will be shown in subsequent sections that the
outer minimization reduces to one of dimension
Theorem 1: For any given data matrix
and positive definite symmetric weighting matrix
is defined in (1). Then, the
minimizing
is given by
is Kronecker’s product . Furthermore,
and depends only on the range space of
; for any invertible
By considering the unweighted case
, it will be seen
that (2) is the generalization of the EVD approach, which is
defined in Section I-B, to the weighted case.
Corollary 2: Define
as in (3). If
in (3), then (5)
, then (4) and (5) become
MANTON et al.: GEOMETRY OF WEIGHTED LOW-RANK APPROXIMATIONS
The cost function (6) is called the generalized Rayleigh quotient since it is a generalization of the usual Rayleigh quotient . Furthermore, it is known that the minimum of (6), or
of (8) subject to
, occurs when the columns of
correspond to the
smallest eigenvectors of
. That is,
(2) is precisely the EVD approach; if the columns of
largest eigenvectors of
and if the columns of
smallest eigenvectors, then
therefore, (7) becomes
The traditional approach , to reduced-rank problems
is to enforce the rank
constraint on
by replacing
columns and
rows. The interpretation is
that matrices with rank at most
are being parameterised by the
many-to-one map
. Reasons for believing the
reformulation (2) to be more natural than the traditional reformulation are now given.
1) The implicit mapping in (2) from a null space represented
to a matrix
is a one-to-one mapping.2
2) As shown above, if
, then (2) is equivalent to the
EVD approach for computing a low-rank approximation.
3) The fact that the SVD or EVD can be used to solve (1)
for certain weighting matrices
is not apparent from the
reformulation. However, as is shown below,
conditions on
for (2) to have a solution in terms of an
SVD or EVD are easily found.
is chosen so that (2) is equivalent to the minimization
of a generalized Rayleigh quotient, then (1) has a closed-form
solution in terms of an SVD or EVD. Such a
must make (5)
a quadratic function when
is appropriately restricted, cf., (8).
For (5) to be quadratic, it is necessary to “remove” the
term. This can be done whenever
is of the form
Note that if
Theorem 3: In (1), if
are both positive definite and symmetric, then
the solution
of (1) is given by the following closed-form expression. Let
be the compact SVD 
is the unique positive definite symmetric matrix such that
and similarly for
is obtained from
by setting all but the first
singular values to zero.
For completeness, connections with the Riemannian SVD 
are discussed briefly. The Riemannian SVD can be used to solve
(1) only in the special case of a rank one reduction (
(It also requires
to be diagonal.) If
, then the algorithm in for computing the Riemannian SVD reduces to the
standard inverse power method, whereas the steepest descent
algorithm in Section IV-B specializes to the algorithm in .
As shown in , the steepest descent algorithm is preferable
to the inverse power method since it is not sensitive to closely
spaced eigenvalues. Furthermore, the Newton method in Section V asymptotically has a cubic rate of convergence when
, whereas the inverse power method only has a linear
rate of convergence asymptotically.
2Specifically, it induces a one-to-one mapping from points on the Grassmann
manifold to matrices with rank at most r. Since the mapping is not onto, it uses
even fewer parameters than the traditional R = AB parameterization.
More detailed comparisons with the Riemannian SVD have
not been made because the Riemannian SVD is not used in practice to solve (1). Instead, it is used to solve (1) subject to the
extra constraint that the rank-reduced matrix
has a particular
affine structure . Although not pursued here, it may be possible to incorporate this structural constraint into (2), leading to
modified steepest descent and Newton methods having superior
performance to the modified Inverse Power method in .
III. OPTIMIZATION ALGORITHMS ON GRASSMANN MANIFOLDS
The previous section showed that (1) can be solved by first
finding the matrix
, which minimizes the cost function (5).
Directly minimizing
dimensional optimization problem because
. However,
only depends on the range space of
and not on the individual
elements of
. As recognized in , this symmetry can be exploited to reduce the dimension of the optimization problem to
parameters (see Section III-A for an elementary proof).
Moreover, the algorithms in can be used to minimize
(once the necessary derivatives have been calculated), thus resulting in efficient algorithms for solving the weighted low-rank
approximation problem.
For reasons given later though, this paper prefers to use the algorithm in Section III-A for minimizing
. The motivation
for considering alternatives to the algorithms in is that 
introduces an artificial structure, namely, a Riemannian structure, into the optimization problem that, depending on the actual function to be minimized, may or may not be detrimental.
Specifically, unless
possesses properties that make it natural or desirable to introduce a Riemannian structure, there is
no compelling reason to do so; see Section III-B. Therefore,
the algorithm in Section III-A takes the liberty of introducing
a different artificial structure into the constraint surface that
serendipitously appears to be better suited to the specific cost
function (5). It is candidly stated, however, that the improvement over the algorithms in for the specific cost function (5)
appears to be relatively minor and has not been rigorously established; the reasons then for presenting this alternative approach
are that this approach is more accessible to readers since it does
not require knowledge of differential geometry, and moreover,
the secondary aim of this paper is to correct the possible misconception that only geodesic-based optimization algorithms are
“natural” or “correct” algorithms.
A. Elementary Optimization Algorithm
This section derives an algorithm for the constrained minimization of a function
subject to
the assumption that the value of
at any point
depends only
on the range space of
. The algorithm itself is not new but its
interpretation is; previously, the algorithm was thought to be a
“flat space approximation” of the geodesic-based algorithms in
 , whereas Section III-B shows that it is just as valid as the
algorithms in .
Henceforth,
is used to denote the orthogonal complement
, that is,
is any full column rank matrix satisfying
is not uniquely defined, implicit
in any statement involving
is that the statement holds for
any fixed choice of
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 51, NO. 2, FEBRUARY 2003
Take an arbitrary matrix
satisfying
, and consider a perturbation matrix
. For certain
, the range space of
is the same as the range space
(written range
), and in this case,
. It is therefore not necessary to consider all
search directions when trying to minimize
, a perturbation
decomposes as
. Since range
it suffices to consider only search directions
is necessary to consider these directions because range
elements, minimizing
-dimensional problem.
The above suggests the following iterative minimization
Algorithm 4
be a function that only depends
on the range of
. A locally minimizing
, subject to
, can be found as
1) Choose a starting position
satisfying
Use the local parameterization from
defined by
to form the local cost function
3) By applying a standard optimization
technique (such as steepest descent or
Newton’s method) to
at the point
, compute a descent step
to any matrix such that
(Gram–Schmidt orthogonalization or the QR
algorithm can be used to compute such
5. Repeat steps 2–4 until convergence.
The descent step
typically is a function of the first and
second derivatives of
. The following proposition gives
formulae for the first and second differentials of (10). (For the
definition of differentials, see . See also Example 7 in Section III-B.)
Proposition 5: Fix
, and define
as in (10). If
are the first and second differentials
about the point
, then the first and second differentials of
are given by
Remark: A consequence of Proposition 5 is that
never appear on its own but always in the form
Algorithm 4. This fact can be exploited for a more efficient
implementation (cf., ) but is not done here for clarity of
presentation and because computing the derivatives of
in (5) and not computing
is the most complex computation
per iteration.
B. Discussion
This section first states a general framework for deriving optimization on manifold algorithms and then shows that the Riemannian framework in is a special case of this more general
framework. This general framework is used to explain the similarities and differences between Algorithm 4 and the algorithms
in . Readers only interested in the low-rank approximation
problem are advised to skip this section.
Minimizing a function
whose value only depends on
the range of
can be posed as an optimization problem on a
Grassmann manifold . There is no unique way of generalizing Newton’s method in Euclidean space to a Newton method
on a manifold. One way is to continue to use Newton’s formula
by treating the first and second derivatives in Newton’s formula
as the gradient and Hessian of the cost function on the manifold; this necessitates endowing the manifold with a Riemannian
structure and is the approach taken in . Another way is to use
the manifold structure to form a local cost function at each iteration and apply Newton’s method to this local cost function; this
is the approach taken here and is discussed in detail below. Yet
another way is to generalize the property that a Newton method
approximates the cost function by a quadratic function at each
iteration and then moves to the minimum of this quadratic approximation; this generalization is different from the above two
generalizations and is a topic for future research.
The general framework (but not the only possible framework)
proposed here for minimizing a function
dimensional
is the following. (For this section only, the symbols
have a new meaning.) For every point
on the manifold
, choose a particular local parameterization3
centred on
, that is,
is a diffeomorphism, and
Different choices of local parameterizations lead to different optimization algorithms in general. Given the current iterate
the next iterate
is obtained as follows. Define the local
cost function
. Apply to
a single iteration of an ordinary optimization algorithm (such
as Newton’s method in Euclidean space) at the origin (recall
) to obtain a
. Finally, set
For brevity, any algorithm which can be written in the above
form (with a Newton method used to find
) is called a true
Newton method. Clearly, Algorithm 4 (with Step 3 a Newton
step) is a true Newton method.
An interesting and nontrivial observation is that the Newton
algorithm in is also a true Newton method. Specifically,
the Newton algorithm in is obtained from Algorithm 4 by
3In more general cases, the domain of
can be chosen to be an open subset
rather than the whole of
MANTON et al.: GEOMETRY OF WEIGHTED LOW-RANK APPROXIMATIONS
making Step 3 a Newton step and by replacing the local parameterization (9) with the alternative local parameterization
are obtained from the compact SVD
is diagonal).
A proof of this follows from the facts stated in the proof of
Proposition 6.
Under the local parameterization (13), the local cost function
(10) becomes
Before arguing that Algorithm 4 is just as valid an algorithm as
those in , the derivatives of (14) are calculated.
Proposition 6: Let
be a cost function such that
and define
, as in (14). If
are the first and second differentials of
about the point
, then the first and second differentials of
are given by
The following example clarifies the notation in Proposition 6.
Example 7: The first and second differentials of
be computed when
is as defined in (3) and
Since Proposition 6 assumes
and, furthermore, only
to hold for orthogonal matrices
and not for invertible matrices
, (8) can be used instead of
(6). Thus, define
. Its first differential
. Its second differential is
. Applying Proposition 6 shows that the first and second differentials of (14) about
Theorem 8 proves that the first and second derivatives of (10)
and (14) about
are the same.
Theorem 8: Let
be a cost function such that
have full column rank. Choose
. Then, the first and second differentials about
defined in (10) are identical to
the first and second differentials about
Theorem 8 shows that the step
will be the same for both
Algorithm 4, with Step 3 a Newton step and the Newton algorithm in . The only difference between the two algorithms
is that Algorithm 4 moves along the straight line
rather than along the geodesic
. Therefore, it is possible to derive Algorithm 4 by starting with the algorithm in and approximating
geodesics by straight lines and moreover; this makes Algorithm
4 appear to be a “flat space approximation” of the algorithms in
However, the algorithms in can equally well be thought
of as approximations of Algorithm 4; replace the straight line
parameterization (10) by the geodesic approximation (14). The
key point though is that thinking of either algorithm as an
approximation of the other is misleading because the term
“approximation” has the connotation of inferiority, yet both
algorithms are true Newton methods and neither can claim
superiority in general; for some cost functions, the algorithms in
 may converge more quickly,4 whereas for others, Algorithm
4 may be faster. The following simplified example in Euclidean
space is used to explain this phenomenon.
Consider the two cost functions
. Newton’s method
applied to
finds the exact solution after a single iteration.
However, it requires an infinite number of iterations to converge
to the exact solution if it is applied to
. This is because
the standard Newton method assumes that the cost function is
approximately quadratic in Cartesian coordinates. Conversely, a
Newton method in polar coordinates converges in one iteration
when applied to
. Clearly, the Newton algorithm in Cartesian
coordinates and the Newton algorithm in polar coordinates
are equally valid Newton algorithms, and neither can claim
superiority.
The difference between Algorithm 4 and the algorithms in 
is analogous to the above example; they merely use different coordinate systems (cf., (10) and (14)). Which algorithm is better
depends on the particular cost function to minimize. (Roughly
speaking, for a given cost function
, if the local cost function (10) centred at the minimum of
more closely resembles
a quadratic function than (14) does, then (10) should be used
instead of (14).)
Last, to refute any claim that the algorithms in are superior
because they correctly exploit the geometry of the Grassmann
manifold, it is emphasised that the “geometry” in is an artificial geometry. In the original constrained optimization problem,
only the constraint set
is given. Making
into a manifold is already adding an artificial structure (a
topology and an atlas), yet there is a clear choice here; making
a Stiefel or Grassmann manifold means that if
is smooth as
a function in Euclidean space, then it remains smooth as a function on the Stiefel or Grassmann manifold. However, if nothing
else is known about
, then there is no compelling reason to go
further and endow the constraint set
with a metric, making
it a Riemannian manifold. In other words, using the artificial
Riemannian structure is conceptually no better or no worse than
using the artificial local parameterization (10).
4Note that the asymptotic rate of convergence will be the same for
both algorithms (e.g., quadratic for Newton methods) but the constant of
proportionality will in general be different; one algorithm may require twice
as many iterations as the other to achieve the same level of accuracy,
for instance.
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 51, NO. 2, FEBRUARY 2003
Remark: Note that (10) is a canonical parameterization of
the Grassmann manifold known as homogeneous coordinates
in the literature; if the cost function is not specified in advance,
then an arbitrary choice must be made, and the choice (10) is a
natural one, as is (14).
IV. FIRST-ORDER DESCENT METHODS
This section presents algorithms for solving the weighted
low-rank approximation problem (1). The classical alternating
projection algorithm is derived in Section IV-A, whereas novel
steepest descent algorithms are proposed in Section IV-B. It
is important to note that these steepest descent algorithms are
not standard descent algorithms; the cost function changes at
each iteration (see Algorithm 4). The performance of these
algorithms is discussed in Section VI.
The computational complexity of each algorithm is calculated for an arbitrary weighting matrix, a diagonal weighting
matrix, and the identity weighting matrix (unweighted case). It
is expected that in many applications the weighting matrix
will be diagonal. Indeed, taking
to be diagonal corresponds
to considering the weighted norm in .
A. Alternating Projections
An alternating projection algorithm was proposed in for
finding the weighted low-rank approximation of a matrix. Since
 used a different5 (and less general) weighting function,
their notation was somewhat cumbersome. Proposition 9 derives
a compact form of the alternating projection algorithm.
Proposition 9: Let
be an arbitrary matrix. Then
for a fixed
which minimizes
Similarly, for a fixed
, which minimizes
is given by
Based on Proposition 9, the alternating projection algorithm
is as follows. Initialize
randomly. Use (20) to compute
Use (21) to compute a new
. Repeat until convergence. The
(locally) best rank
approximation of
, (20) and (21) reduce to
, respectively.
Complexity: One iteration of the alternating projection algorithm requires
is diagonal, only
flops are required. (These flop counts are obtained
by exploiting the block structure introduced by the Kronecker
product.) If
, there are
flops per iteration.
B. Steepest Descent
This section first derives an expression for the steepest descent direction of the local cost function
in Algorithm 4.
5The weighted norm used in takes the form kX   Rk
for some weighting matrix W . This is equivalent to
restricting Q in (1) to be diagonal.
It then uses this expression to derive steepest descent algorithms
for solving the low-rank approximation problem (1).
Theorem 10 (Steepest Descent): Define
as in (5) and,
having fixed
in (10). Then, the gradient of
are the unique matrices
that satisfy
is instead defined as in (14), then under the extra condition that
, the gradient of
is also given by (22).
Note that if
, then grad
, agreeing with (17).
Complexity: Computing grad
is diagonal, this reduces to
is precomputed, then
flops are required. If
, then the
flop counts for these three cases are
, respectively.
Remark: Evaluating
flops for arbitrary
is diagonal. If
it will be seen later that minimizing
is equivalent to
maximizing
, and the latter requires only
to be evaluated, provided
is precomputed.
Theorem 10 combined with Algorithm 4 leads to four different steepest descent algorithms, depending on which local
parameterization [(9) or (13)] is used and on whether or not
. The two algorithms based on (9) are presented below.
Their counterparts, based on (13), are presented in Appendix B.
They all use Armijo’s step-size rule [22, Sec. 1.2.3], and they
are all tailored for the case when
Notation: The norm
appearing in the algorithms is the
Frobenius norm. The “Q-Factor” operator qf
is defined to
be the orthogonal part qf
decomposition
Algorithm 11 (Steepest Descent Along
Straight Lines)
. Set step size
2) Evaluate
3) Compute descent direction
are defined in (23).
4) Evaluate
, then set
, and repeat Step 4.
5) Evaluate
, then set
repeat Step 5.
. Renormalize
. Go to Step 2.
MANTON et al.: GEOMETRY OF WEIGHTED LOW-RANK APPROXIMATIONS
Complexity: Each
flops in general and
is diagonal. If
then these flop counts approach
respectively.
1) In any given iteration of Algorithm 11, if Step 4 is repeated
at least once, then the test in step 5 becomes redundant. This
holds for all the steepest descent algorithms.
2) In practice, the algorithms must include a test for convergence. One possibility is to test to see if the magnitude of
the gradient
is sufficiently close to zero . Once the
algorithm is terminated, the low-rank approximation
found by evaluating (4).
3) Renormalizing
in Step 6 serves the purpose of computing an
orthogonal to
The disadvantage of Algorithm 11 is its computational complexity; many flops are required to evaluate the cost function.
The following algorithm overcomes this in the unweighted case
by optimizing over
rather than over
. Specifically,
Thus, performing steepest descent on
is identical to
performing steepest ascent on
is computationally more efficient to maximize
than minimize
Algorithm 12 (Steepest Descent along
Straight Lines, Unweighted Case)
Set step size
. Precompute
2) Evaluate
3) Compute ascent direction
4) Evaluate
, then set
, and repeat Step 4.
5) Evaluate
, then set
, and repeat
. Renormalize
Complexity: Each
1) Algorithm 11 with
and Algorithm 12 are equivalent
in that they both produce the same sequence of points
However, Algorithm 12 requires fewer flops per iteration.
2) It is not necessary to renormalize
in Step 6 at every
iteration. However,
can become ill-conditioned if it is
not renormalized regularly.
3) The low-rank approximation
is given by
, then the optimal step size rule can
be used instead of Armijo’s rule .
In Algorithm 12, modest computational savings can be made
by first reducing
to tridiagonal form. Specifically, if
is an orthonormal matrix such that
is tridiagonal and if
maximizes tr
, and thus, the best rank
approximation of
V. SECOND-ORDER DESCENT METHODS
This section presents quadratically (and, in the unweighted
case, cubically) convergent algorithms for solving the low-rank
approximation problem (1). At each iteration, the algorithms
perform a Newton step in local coordinates.
The following theorem derives an expression for the Hessian
in Algorithm 4. Its statement requires the commutation
matrix 
, which is the unique matrix
holds for all
Theorem 13 (Quadratic Approximation): Define
(5) and, having fixed
as in (10). Then, the second-order Taylor series approximation of
where grad
is defined in (22), and
the symmetric matrix in (27), shown at the bottom of the page.
The commutation matrix
in (27) is defined in (25).
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 51, NO. 2, FEBRUARY 2003
is instead defined as in (14), then under the extra
condition that
, the second-order Taylor
series approximation of
is also given by
, then the Hessian (27)
simplifies to
Complexity: Computing
flops in general and
is diagonal. If
flops are required if
is precomputed. If
then these flop counts are
respectively.
It is now straightforward to derive the Newton step
the Newton step
is obtained by solving the linear equation
It requires
flops to solve (29), which is fewer
than it takes to compute the Hessian (27) in the weighted case.
A Newton step is not guaranteed to decrease the cost function.
It is standard [22, Sec. 1.4.4] to include a test to ensure that
the Newton step significantly decreases the cost function. If the
test fails, an alternative descent step, such as one iteration of
Algorithm 11, should be used.
Algorithm 14 (Newton Step)
2) Compute the negative of the gradient
are defined in (23).
3) Compute the Hessian
as defined in
4) Solve the linear equation
for the matrix
5) Evaluate
6) Evaluate
, then abort
Newton step.
. Renormalize
. Go to Step 2.
Complexity: One
flops in general and
is diagonal. If
, then these
flop counts are
, respectively.
1) It is straightforward to modify Algorithm 14 to move along
geodesics rather than straight lines; refer to Algorithm 16 to
see how. Such a modification does not alter the order of the
computational complexity of the algorithm. The same goes
for Algorithm 15 as well.
2) The constant 1/4 in Step 6 can be replaced by any constant
strictly between 0 and 1/2; see .
3) Analogous to the algorithms in Section IV-B, Algorithms
14 and 15 are deemed to have converged (and hence should
be terminated) if
is sufficiently small.
In the unweighted case, (29) can be written in the form
Thus, the step size
is found by solving the Sylvester equation
(30); efficient algorithms to do so appear in and . They
Algorithm 15 (Newton Step, Unweighted
. Precompute
2) Compute one half times the negative of
the gradient
3) Compute
and solve the Sylvester equation
4) Evaluate
5) Evaluate
; then, abort
Newton step.
. Renormalize
. Go to Step 2.
Complexity: One iteration of Algorithm 15 requires
1) Algorithm 15 is equivalent to Algorithm 14 with
in that they both produce the same sequence of points
2) In practice,
in Step 5 of Algorithm
, renormalizing
, and then
should be saved for subsequent use in Step
3) Since it is faster to compute qf
rather than qf
small computational saving will be made by maximizing
rather than minimizing
; refer to Algorithm
12 to see how.
4) Dedicated algorithms for minimizing
in the unweighted case appear in , . They have similar numerical behavior to Algorithm 15 but require fewer flops
per iteration.
The rate of convergence of Algorithm 15 is cubic because,
about the minimum of
, the local cost function
defined in either (10) or (14) is symmetrical (
. This means that the Taylor series expansion
has no cubic term, and thus, the approximation (26) is
correct up to degree three .
MANTON et al.: GEOMETRY OF WEIGHTED LOW-RANK APPROXIMATIONS
Graphs illustrating the poor performance of the alternating projection
method yet good performance of the steepest descent method when there is very
little separation in the singular values of the data matrix.
VI. NUMERICAL STUDY
This section compares the performance of the following algorithms in a limited number of situations.
AP: The Alternating Projection algorithm described in
Section IV-A.
SD: The Steepest Descent algorithm (Algorithm 11) and,
in the unweighted case, its equivalent version (Algorithm
SD (geod): The Steepest Descent along Geodesics algorithm (Algorithm 16) and, in the unweighted case, its
equivalent version (Algorithm 17).
NS: The Newton Step algorithm (Algorithm 14) and, in the
unweighted case, its equivalent version (Algorithm 15).
NS (geod): The Newton Step algorithm (either Algorithm
14 or Algorithm 15) appropriately modified to move along
geodesics rather than straight lines.
Figs. 1–6 show the performance of the various algorithms in
six different situations. Each figure contains two graphs, corresponding to initializing the algorithms at two different randomly
chosen points. Within each graph, all algorithms were initialized
identically. The error, which is defined as the current cost
(defined in (3)) minus the minimum cost, is graphed against the
number of iterations taken by each algorithm. Only Fig. 4 used a
weighting matrix; the other five figures studied the unweighted
. Notice that the
eigenvalues of
(equivalently, the singular values of
are closely spaced. Each algorithm was required to find the
approximation of
. As Fig. 1 shows, the
AP algorithm performs extremely poorly. The SD method,
however, exhibits rapid convergence. In fact, the SD method
converges more quickly than it does in Fig. 2, showing that an
ill-conditioned problem for the AP algorithm is a well-conditioned problem for the SD algorithm. Fig. 1 also shows that
only two iterations of the NS algorithm (run after the second
iteration of the SD algorithm) are required for convergence.
Graphs illustrating comparable performance of the steepest descent
method and alternating projection method when the singular values of the data
matrix are well separated.
Graphs illustrating better performance of the steepest descent method
over the alternating projection method on a randomly chosen matrix. In addition,
notice that the Newton method will converge to the closest critical point rather
than continue downhill.
In Fig. 2, the data matrix
chosen to have well-separated singular values. Each algorithm
sought to find the best rank
approximation. Fig. 2 shows
that both AP and SD perform comparably in this situation. The
NS algorithm exhibits cubic convergence. However, since the
NS algorithm converges to the nearest critical point, it is just as
likely to attempt to move uphill rather than downhill. (The test
in Step 6 of Algorithm 14 will detect this, however.) It is thus
necessary to start the NS algorithm after the fourth iteration of
SD in the graph on the left of Fig. 2 and after the sixth iteration
of SD in the graph on the right.
In Fig. 3, the data matrix
was a randomly chosen 120
matrix. The algorithms attempted to find the best rank
approximation. In both cases, the NS algorithm was run after
ten iterations of the SD algorithm. In the first case, the NS converged to a local minimum, whereas in the second case, it converged to the global minimum. It is interesting to see how the
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 51, NO. 2, FEBRUARY 2003
Graphs illustrating more robust performance of the steepest descent
method over the alternating projection method on a randomly chosen data and
weighting matrix.
local minimum affects the performance of the AP and SD algorithms. The SD algorithm escapes from the local minimum on
its 23rd iteration. The AP algorithm is still significantly hampered by the local minimum after 39 iterations.
In Fig. 4, the data matrix
was a randomly chosen 10
matrix. The weighting matrix
was chosen at random with singular values between 0.2857 and 1. Each algorithm was required
to find the best rank
approximation of
. For the NS algorithm to converge, it was necessary to run it after the 13th
iteration of SD in the left-hand graph and after the fifth iteration of SD in the right-hand graph. The left-hand graph shows
the AP algorithm converging to a local minimum, whereas the
SD algorithm escapes the local minimum. The right-hand graph
shows the AP algorithm performing slightly better than the SD
algorithm.
The final two figures compare the straight line algorithms
with the geodesic algorithms. Fig. 5 uses the same data as in
Fig. 1, and Fig. 6 uses the same data as in Fig. 2. Figs. 5 and 6
show that the straight line and geodesic SD algorithms perform
comparably, whereas the straight line NS algorithm is superior
to the geodesic NS algorithm.
One important factor that the above results have neglected
to show is the number of flops required per iteration. The AP
algorithm generally requires the least number of flops per iteration. However, as Fig. 1 illustrates, the AP algorithm can suffer
from exceptionally slow convergence. Moreover, the SD algorithm empirically appears to be more robust than the AP algorithm; Figs. 3 and 4 show the SD algorithm escaping from local
minima. In certain circumstances, the quadratic (or, in the unweighted case, cubic) convergence of the NS algorithm more
than compensates for its computational complexity.
A small number of simulations were done to compare the
number of flops (as calculated by Matlab’s flops command) required for the straight line and geodesic versions of SD and NS
algorithms. It was found that SD and SD (geod) perform comparably; sometimes SD requires fewer flops per iteration, and
other times, SD (geod) does. (The step selection rule in the SD
Graphs illustrating better performance of straight line Newton method
over geodesic Newton method and comparable performance of straight line
steepest descent and geodesic steepest descent.
Graphs illustrating better performance of straight line Newton method
over geodesic Newton method and comparable performance of straight line
steepest descent and geodesic steepest descent.
algorithms makes the actual number of flops per iteration unpredictable in advance.) It was also found that the NS algorithm
requires fewer flops per iteration than NS (geod) does.
VII. CONCLUSION
This paper studied the weighted low-rank approximation
problem (1). It generalized the EVD method for the unweighted
case to the weighted case by showing that the best low-rank
approximation can be found by first computing the minimizing
subspace of a certain cost function (Theorem 1). This novel
approach led to the derivation of closed-form solutions of
(1) for certain weighting matrices (Theorem 3). A general
framework for numerically finding the minimizing subspace
of a cost function was given in Section III. The advantage of
this framework is that it considerably reduces the dimension
of the optimization problem. This framework was then applied
MANTON et al.: GEOMETRY OF WEIGHTED LOW-RANK APPROXIMATIONS
in Sections IV and V to derive modified steepest descent and
Newton algorithms for the low-rank approximation problem.
These algorithms are not standard optimization algorithms
because the cost function changes at each iteration. The
numerical study in Section VI demonstrated the advantages
of these algorithms over the traditional alternating projection
algorithm. For practical applications of weighted low-rank
approximations, see and .
APPENDIX A
Equalities (31)–(33), shown at the bottom of the page, are
used in the following proofs.
A. Proof of Theorem 1
The method of Lagrange multipliers, as elucidated in , is
applied to (3). Define
Minimizing
subject to
results in the Lagrangian
is the Lagrange multiplier. Its differential
where the last line is obtained by using (31). This shows that
if and only if
Writing both this condition and the condition
matrix form yields
where use has been made of (32). Using the fact that
denotes unimportant elements, (34) is readily solved
, yielding (4). Substituting this solution into the
cost function
immediately gives (5). Finally, the reason
for any invertible matrix
and only if
, that is, the constraint set
in (3) equals the constraint set
B. Proof of Corollary 2
, then (5) becomes
where the second last line is obtained by using (32) and the last
line by (31). Equation (7) is obtained similarly.
C. Proof of Theorem 3
Substituting
into (5) yields
generalized Rayleigh quotient in
, cf., (6), its minimum occurs
when the columns of
span the same space as do the
smallest eigenvectors of
. The solution of (1) is found by
have the same dimensions
is the number of rows of
is the number of columns of
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 51, NO. 2, FEBRUARY 2003
substituting this value of
into (4). Substituting
into (4) yields
If the columns of
span the same space as the
smallest eigenvectors of
D. Proof of Proposition 5
is affine in
, (11) clearly holds. Similarly,
(12) follows from the chain rule for second differentials [18, Ch.
6, Th. 11].
E. Proof of Proposition 6
Under the Levi-Civita connection, the gradient and Hessian of
a function on the Grassmann manifold are equivalent to the first
and second derivatives of the function expressed in normal coordinates around the point at which the derivatives are taken. Since
is the exponential map (that is, it traces
out geodesics) [8, Th. 2.3],
in (14) is precisely
expressed in normal coordinates. Thus, the first and second derivatives (and hence the differentials) of
are readily obtained
from the formulae for the gradient and Hessian of a function on
a Grassmann manifold, as given in [8, Sec. 2.5.3 and 2.5.4].
F. Proof of Theorem 8
Comparing (15) and (16) with (11) and (15) shows that they
will be the same if
. Under the
hypothesis of the theorem,
sufficiently small because range
provided (
) is invertible. Therefore,
, implying
G. Proof of Proposition 9
is fixed, the cost function
quadratic. By differentiating it and setting the result to zero, the
proposition readily follows.
H. Proof of Theorem 10
The gradient grad
is defined to be the unique matrix
is thus necessary to first compute
. From Proposition 5,
. The differential
is now determined.
For convenience, define the symmetric bilinear function
(5) becomes
is defined in (23). Defining
as in (23) shows that
can be compactly written as
Therefore,
verifying (22).
I. Proof of Theorem 13
Taylor’s theorem implies that
is the unique symmetric matrix, which satisfies
In order to first calculate
as in (35), and note that
. Differentiating (36) yields
Differentiating (23) gives
After replacing
(see Proposition 5), it follows
This is equivalent to (27).
MANTON et al.: GEOMETRY OF WEIGHTED LOW-RANK APPROXIMATIONS
APPENDIX B
GEODESIC-BASED ALGORITHMS
This section presents the geodesic based counterparts to the
algorithms in Section IV-B.
Algorithm 16 (Steepest Descent Along
Geodesics)
. Set Step size
2) Evaluate
3) Compute descent direction
are defined in (23).
4 Determine the compact SVD of
that is, compute
is square and diagonal,
5 Evaluate
, and repeat Step 5.
6) Evaluate
, and repeat Step 6.
by setting
. Go to Step 2.
Complexity: Each
flops in general and
is diagonal. If
then these flop counts approach
respectively.
Algorithm 17 (Steepest Descent Along
Geodesics, Unweighted Case)
Set Step Size
. Precompute
2) Evaluate
3) Compute ascent direction
4) Determine the compact SVD of
is, compute
is square and diagonal, and
5) Evaluate
, then set
and repeat Step 5.
6 ) Evaluate
, then set
, and repeat Step 6.
. Renormalize
. Go to Step 2.
Complexity: Each
1) Algorithm 16 with
and Algorithm 17 are equivalent
in that they both produce the same sequence of points
although Algorithm 17 requires fewer flops per iteration.
is not renormalized in the last step of Algorithm 17,
round off error can cause the Armijo rule in Step 6 to repeat
indefinitely.
ACKNOWLEDGMENT
The authors would like to thank P. Stoica for initial discussions on the weighted low-rank approximation problem and an
anonymous reviewer for valuable comments.