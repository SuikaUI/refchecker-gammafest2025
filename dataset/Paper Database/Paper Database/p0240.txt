Dynamic Few-Shot Visual Learning without Forgetting
Spyros Gidaris
University Paris-Est, LIGM
Ecole des Ponts ParisTech
 
Nikos Komodakis
University Paris-Est, LIGM
Ecole des Ponts ParisTech
 
The human visual system has the remarkably ability to
be able to effortlessly learn novel concepts from only a few
examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging
research problem with many practical advantages on real
world vision applications. In this context, the goal of our
work is to devise a few-shot visual learning system that
during test time it will be able to efﬁciently learn novel categories from only a few training data while at the same
time it will not forget the initial categories on which it was
trained (here called base categories). To achieve that goal
we propose (a) to extend an object recognition system with
an attention based few-shot classiﬁcation weight generator,
and (b) to redesign the classiﬁer of a ConvNet model as the
cosine similarity function between feature representations
and classiﬁcation weight vectors. The latter, apart from
unifying the recognition of both novel and base categories,
it also leads to feature representations that generalize better on “unseen” categories. We extensively evaluate our
approach on Mini-ImageNet where we manage to improve
the prior state-of-the-art on few-shot recognition (i.e., we
achieve 56.20% and 73.00% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacriﬁce
any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our
approach on the recently introduced few-shot benchmark
of Bharath and Girshick where we also achieve stateof-the-art results. The code and models of our paper will
be published on: 
FewShotWithoutForgetting.
1. Introduction
Over the last few years, deep convolutional neural networks (ConvNets) have achieved impressive
This work was supported by the ANR SEMAPOLIS project, an INTEL
gift, and hardware donation by NVIDIA.
results on image classiﬁcation tasks, such as object recognition or scene classiﬁcation . In order for a ConvNet
to successfully learn to recognize a set of visual categories
(e.g., object categories or scene types), it requires to manually collect and label thousands of training examples per
target category and to apply on them an iterative gradient
based optimization routine that is extremely computationally expensive, e.g., it can consume hundreds or even
thousands of GPU hours. Moreover, the set of categories that
the ConvNet model can recognize remains ﬁxed after training. In case we would like to expand the set of categories
that the ConvNet can recognize, then we need to collect training data for the novel categories (i.e., those that they were
not in the initial training set) and restart the aforementioned
computationally costly training procedure this time on the
enhanced training set such that we will avoid catastrophic
interference. Even more, it is of crucial importance to have
enough training data for the novel categories (e.g., thousands
of examples per category) otherwise we risk overﬁtting on
In contrast, the human visual system exhibits the remarkably ability to be able to effortlessly learn novel concepts
from only one or a few examples and reliably recognize them
later on. It is assumed that the reason the human visual
system is so efﬁcient when learning novel concepts is that it
exploits its past experiences about the (visual) world. For example, a child, having accumulated enough knowledge about
mammal animals and in general the visual world, can easily
learn and generalize the visual concept of “rhinoceros” from
only a single image. Mimicking that behavior on artiﬁcial vision systems is an interesting and very challenging research
problem with many practical advantages, such as developing
real-time interactive vision applications for portable devices
(e.g., cell-phones).
Research on this subject is usually termed few-shot learning. However, most prior methods neglect to fulﬁll two very
important requirements for a good few-shot learning system:
(a) the learning of the novel categories needs to be fast, and
(b) to not sacriﬁce any recognition accuracy on the initial
categories that the ConvNet was trained on, i.e., to not “forarXiv:1804.09458v1 [cs.CV] 25 Apr 2018
get” (from now on we will refer to those initial categories by
calling them base categories). Motivated by this observation,
in this work we propose to tackle the problem of few-shot
learning under a more realistic setting, where a large set
of training data is assumed to exist for a set of base categories and, using these data as the sole input, we want to
develop an object recognition learning system that, not only
is able to recognize these base categories, but also learns
to dynamically recognize novel categories from only a few
training examples (provided only at test time) while also not
forgetting the base ones or requiring to be re-trained on them
(dynamic few-shot learning without forgetting). Compared
to prior approaches, we believe that this setting more closely
resembles the human visual system behavior (w.r.t. how it
learns novel concepts). In order to achieve our goal, we
propose two technical novelties.
Few-shot classiﬁcation-weight generator based on attention. A typical ConvNet based recognition model, in
order to classify an image, ﬁrst extracts a high level feature
representation from it and then computes per category classiﬁcation scores by applying a set of classiﬁcation weight
vectors (one per category) to the feature. Therefore, in order
to be able to recognize novel categories we must be able
to generate classiﬁcation weight vectors for them. In this
context, the ﬁrst technical novelty of our work is that we
enhance a typical object recognition system with an extra
component, called few-shot classiﬁcation weight generator
that accepts as input a few training examples of a novel
category (e.g., no more than ﬁve examples) and, based on
them, generates a classiﬁcation weight vector for that novel
category. Its key characteristic is that in order to compose
novel classiﬁcation weight vectors, it explicitly exploits the
acquired past knowledge about the visual world by incorporating an attention mechanism over the classiﬁcation weight
vectors of the base categories. This attention mechanism
offers a signiﬁcant boost on the recognition performance
of novel categories, especially when there is only a single
training example available for learning them.
Cosine-similarity based ConvNet recognition model.
In order for the few-shot classiﬁcation weight generator to
be successfully incorporated into the rest of the recognition
system, it is essential the ConvNet model to be able to simultaneously handle the classiﬁcation weight vectors of both
base and novel categories. However, as we will explain in the
methodology, this is not feasible with the typical dot-product
based classiﬁer (i.e., the last linear layer of a classiﬁcation
neural network). Therefore, in order to overcome this serious issue, our second technical novelty is to implement the
classiﬁer as a cosine similarity function between the feature
representations and the classiﬁcation weight vectors. Apart
from unifying the recognition of both base and novel categories, features learned with the cosine-similarity based
classiﬁer turn out to generalize signiﬁcantly better on novel
categories than those learned with a dot-product based classiﬁer. Moreover, we demonstrate in the experimental section
that, by simply training a cosine-similarity based ConvNet
recognition model, we are able to learn feature extractors
that when used for image matching they surpass prior stateof-the-art approaches on the few-shot recognition task.
To sum up, our contributions are: (1) We propose a fewshot object recognition system that is capable of dynamically
learning novel categories from only a few training data while
at the same time does not forget the base categories on which
it was trained. (2) In order to achieve that we introduced two
technical novelties, an attention based few-shot classiﬁcation
weight generator, and to implement the classiﬁer of a ConvNet model as a cosine similarity function between feature
representations and classiﬁcation vectors. (3) We extensively
evaluate our object recognition system on Mini-ImageNet,
both w.r.t. its few-shot object recognition performance and
its ability to not forget the base categories, and we report
state-of-the-art results that surpass prior approaches by a
very signiﬁcant margin. (4) Finally, we apply our approach
on the recently introduced fews-shot benchmark of Bharath
and Girshick where we achieve state-of-the-art results.
In the following sections, we provide related work in §2,
we describe our few-shot object learning methodology in §3,
we provide experimental results in §4, and ﬁnally we conclude in §5.
2. Related work
Recently, there is resurgence of interest on the few-shot
learning problem. In the following we brieﬂy discuss the
most relevant approaches to our work.
Meta-learning based approaches. Meta-learning approaches typical involve a meta-learner model that given a
few training examples of a new task it tries to quickly learn a
learner model that “solves” this new task .
Speciﬁcally, Ravi and Larochelle propose a LSTM 
based meta-learner that is trained given as input a few training examples of a new classiﬁcation task to sequentially
generate parameter updates that will optimize the classiﬁcation performance of a learner model on that task. Their
LSTM also learns the parameter initialization of the learner
model. Finn et al. simpliﬁed the above meta-learner
model and only learn the initial learner parameters such that
only a few gradient descent steps w.r.t. those initial parameters will achieve the maximal possible performance on the
new task. Mishra et al. instead propose a generic temporal convolutional network that given as input a sequence
of a few labeled training examples and then an unlabeled
test example, it predicts the label of that test example. Our
system also includes a meta-learner network component, the
few-shot classiﬁcation weight generator.
Metric-learning based approaches. In general, metric
learning approaches attempt to learn feature representations
that preserve the class neighborhood structure (i.e., features
of the same object are closer than features of different objects). Speciﬁcally, Koch et al. formulated the one-shot
object recognition task as image matching and train Siamese
neural networks to compute the similarity between a training
example of a novel category and a test example. Vinyals et
al. proposed Matching Networks that in order to classify
a test example it employs a differentiable nearest neighbor
classiﬁer implemented with an attention mechanism over
the learned representations of the training examples. Prototypical Networks learn to classify test examples by
computing distances to prototype feature vectors of the novel
categories. They propose to learn the prototype feature vector of a novel category as the average of the feature vectors
extracted by the training examples of that category. A similar
approach was proposed before by Mensink et al. and
Prototypical Networks can be viewed as an adaption of that
work for ConvNets. Despite their simplicity, Prototypical
Networks demonstrated state-of-the-art performance. Our
few-shot classiﬁcation weight generator also includes a feature averaging mechanism. However, more than that, it also
explicitly exploits past knowledge about the visual world
with an attention based mechanism and the overall framework allows to perform uniﬁed recognition of both base and
novel categories without altering the way base categories are
learnt and recognized.
In a different line of work, Bharath and Girshick propose to use during training a l2 regularization loss on the
feature representations that forces them to better generalize
on “unseen” categories. In our case, the cosine-similarity
based classiﬁer, apart from unifying the recognition of both
base and novel categories, it also leads to feature representations that are able to better generalize on “unseen” categories. Also, their framework is able to recognize both base
and novel categories as ours. However, to achieve that goal
they re-train the classiﬁer on both the base categories (with
a large set of training data) and the novel categories (with
few training data), which is in general slow and requires
constantly maintaining in disc a large set of training data.
3. Methodology
As an input to our object recognition learning system we
assume that there exists a dataset of Kbase base categories:
where Nb is the number of training examples of the b-th category and xb,i is its i-th training example. Using this as the
only input, the goal of our work is to be able to both learn to
accurately recognize base categories and to learn to perform
few-shot learning of novel categories in a dynamic manner
and without forgetting the base ones. An overview of our
framework is provided in Figure 1. It consists of two main
components, a ConvNet-based recognition model that is able
to recognize both base and novel categories and a few-shot
classiﬁcation weight generator that dynamically generates
classiﬁcation weight vectors for the novel categories at test
ConvNet-based recognition model. It consists of (a) a
feature extractor F(.|θ) (with learnable parameters θ) that
extracts a d-dimensional feature vector z = F(x|θ) ∈Rd
from an input image x, and (b) a classiﬁer C(.|W ∗), where
k=1 are a set of K∗classiﬁcation weight
vectors - one per object category, that takes as input the
feature representation z and returns a K∗-dimensional vector
with the probability classiﬁcation scores p = C(z|W ∗) of
the K∗categories. Note that in a typical convolutional neural
network the feature extractor is the part of the network that
starts from the ﬁrst layer and ends at the last hidden layer
while the classiﬁer is the last classiﬁcation layer. During
the single training phase of our algorithm, we learn the
θ parameters and the classiﬁcation weight vectors of the
base categories Wbase = {wk}Kbase
such that by setting
W ∗= Wbase the ConvNet model will be able to recognize
the base object categories.
Few-shot classiﬁcation weight generator. This comprises a meta-learning mechanism that, during test time,
takes as input a set of Knovel novel categories with few
training examples per category
n is the number of training examples of the n-th
novel category and x′
n,i is its i-th training example, and is
able to dynamically assimilate the novel categories on the
repertoire of the above ConvNet model. More speciﬁcally,
for each novel category n ∈[1, Nnovel], the few-shot classi-
ﬁcation weight generator G(., .|φ) gets as input the feature
vectors Z′
i=1 of its N ′
n training examples, where
n,i = F(x′
n,i|θ), and the classiﬁcation weight vectors of the
base categories Wbase and generates a classiﬁcation weight
n, Wbase|φ) for that novel category. Note
that φ are the learnable parameters of the few-shot weight
generator, which are learned during the single training phase
of our framework. Therefore, if Wnovel = {w′
are the classiﬁcation weight vectors of the novel categories
inferred by the few-shot weight generator, then by setting
W ∗= Wbase ∪Wnovel on the classiﬁer C(.|W ∗) we enable the ConvNet model to recognize both base and novel
categories.
A key characteristic of our framework is that it is able to
effortlessly (i.e., quickly during test time) learn novel categories and at the same time recognize both base and novel
categories in a uniﬁed manner. In the following subsections,
Feature Extractor
Dynamic Few-Shot Learning without Forgetting
Classifier
Classification
weight vectors
classification weight
Test image
Training data for
base categories
Few training data
of novel category
Probability
base & novel
categories
Training procedure
Figure 1: Overview of our system. It consists of: (a) a ConvNet based recognition model (that includes a feature extractor and a classiﬁer)
and (b) a few-shot classiﬁcation weight generator. Both are trained on a set of base categories for which we have available a large set of
training data. During test time, the weight generator gets as input a few training data of a novel category and the classiﬁcation weight
vectors of base categories (green rectangle inside the classiﬁer box) and generates a classiﬁcation weight vector for this novel category (blue
rectangle inside the classiﬁer box). This allows the ConvNet to recognize both base and novel categories.
we will describe in more detail the ConvNet-based recognition model in §3.1 and the few-shot weight generator in §3.2.
Finally, we will explain the training procedure in §3.3.
3.1. Cosine-similarity based recognition model
A crucial difference of our ConvNet based recognition
model compared to a standard one is that it should be able
to dynamically incorporate at test time a variable number of
novel categories (through the few-shot weight generator).
The standard setting for classiﬁcation neural networks
is, after having extracted the feature vector z, to estimate
the classiﬁcation probability vector p = C(z|W ∗) by ﬁrst
computing the raw classiﬁcation score sk of each category
k ∈[1, K∗] using the dot-product operator:
where wk is the k-th classiﬁcation weight vector in W ∗,
and then applying the softmax operator across all the K∗
classiﬁcation scores, i.e., pk = softmax(sj), where pk
is the k-th classiﬁcation probability of p. In our case the
classiﬁcation weight vectors w∗
k could come both from the
base categories, i.e., w∗
k ∈Wbase, and the novel categories,
k ∈Wnovel. However, the mechanisms involved during learning those classiﬁcation weights are very different.
The base classiﬁcation weights, starting from their initial
state, are slowly modiﬁed (i.e., slowly learned) with small
SGD steps and thus their magnitude changes slowly over
the course of their training. In contrast, the novel classiﬁcation weights are dynamically predicted (i.e., quickly learned)
by the weight generator based on the input training feature
vectors and thus their magnitude depends on those input
features. Due to those differences, the weight values in those
two cases (i.e., base and novel classiﬁcation weights) can be
completely different, and so the same applies to the raw classiﬁcation scores computed with the dot-product operation,
which can thus have totally different magnitudes depending
on whether they come from the base or the novel categories.
This can severely impede the training process and, in general,
does not allow to have a uniﬁed recognition of both type of
categories. In order to overcome this critical issue, we propose to modify the classiﬁer C(.|W ∗) and compute the raw
classiﬁcation scores using the cosine similarity operator:
sk = τ · cos(z, w∗
k) = τ · z⊺w∗
k∥are the l2-normalized vectors (from now on we will use the overline symbol z to
indicate that a vector z is l2-normalized), and τ is a learnable scalar value1. Since the cosine similarity can be implemented by ﬁrst l2-normalizing the feature vector z and
the classiﬁcation weight vector w∗
k and then applying the
dot-product operator, the absolute magnitudes of the classiﬁcation weight vectors can no longer affect the value of the
raw classiﬁcation score (as a result of the l2 normalization
that took place).
In addition to the above modiﬁcation, we also choose to
remove the ReLU non-linearity after the last hidden
1 The scalar parameter τ is introduced in order to control the peakiness
of the probability distribution generated by the softmax operator since the
range of the cosine similarity is ﬁxed to [−1, 1]. In all of our experiments
τ is initialized to 10.
(a) Cosine-similarity based features of novel categories
(b) Dot-product based features of novel categories
Figure 2: Here we visualize the t-SNE scatter plots of the feature representations learned with (a) the cosine-similarity based ConvNet
recognition model, and (b) the dot-product based ConvNet recognition model. Note that in the case of the cosine-similarity based ConvNet
recognition model, we visualize the l2-normalized features. The visualized feature data points are from the “unseen” during training
validation categories of Mini-ImageNet. Each data point in the t-SNE scatter plots is colored according to its category.
layer of the feature extractor, which allows the feature vector
z to take both positive and negative values, similar to the
classiﬁcation weight vectors. Note that the removal of the
ReLU non-linearity does not make the composition of the
last hidden layer with the classiﬁcation layer a linear operation, since we l2-normalize the feature vectors, which is
a non-linear operation. In our initial experiments with the
cosine similarity based classiﬁer we found that such a modiﬁcation can signiﬁcantly improve the recognition performance
of novel categories.
We note that, although cosine similarity is already well
established as an effective similarity function for classifying
a test feature by comparing it with the available training
features vectors , in this work we use it for a
different purpose, i.e., to replace the dot-product operation
of the last linear layer of classiﬁcation ConvNets used for
applying the learnable weights of that layer to the test feature
vectors. The proposed modiﬁcation in the architecture of
a classiﬁcation ConvNet allows to unify the recognition of
base and novel categories without signiﬁcantly altering the
classiﬁcation pipeline for the recognition of base categories
(in contrast to ). To the best of our knowledge,
employing the cosine similarity operation in such a way
is novel in the context of few shot learning. Interestingly,
concurrently to us, Qi et al. also propose to use the
cosine similarity function in a similar way for the few-shot
learning task. In a different line of work, very recently
Chunjie et al. also explored cosine similarity for the
typical supervised classiﬁcation task.
Advantages of cosine-similarity based classiﬁer. Apart
from making possible the uniﬁed recognition of both base
and novel categories, the cosine-similarity based classiﬁer
leads the feature extractor to learn features that generalize
signiﬁcantly better on novel categories than features learned
with the dot-product based classiﬁer. A possible explanation
for this is that, in order to minimize the classiﬁcation loss of
a cosine-similarity based ConvNet model, the l2-normalized
feature vector of an image must be very closely matched
with the l2-normalized classiﬁcation weight vector of its
ground truth category. As a consequence, the feature extractor is forced to (a) learn to encode on its feature activations
exactly those discriminative visual cues that also the classi-
ﬁcation weight vectors of the ground truth categories learn
to look for, and (b) learn to generate l2-normalized feature
vectors with low intraclass variance, since all the feature vectors that belong to the same category must be very closely
matched with the single classiﬁcation weight vector of that
category. This is visually illustrated in Figure 2, where we
visualize t-SNE scatter plots of cosine-similarity-based and
dot-product-based features related to categories “unseen”
during training. As can be clearly observed, the features
generated from the cosine-similarity-based ConvNet form
more compact and distinctive category-speciﬁc clusters (i.e.,
they provide more discriminative features). Moreover, our
cosine-similarity based classiﬁcation objective resembles
the training objectives typically used by metric learning approaches . In fact, it turns out that our feature extractor
trained solely on cosine-similarity based classiﬁcation of
base categories, when used for image matching, it manages
to surpass all prior state-of-the-art approaches on the fewshot object recognition task.
3.2. Few-shot classiﬁcation weight generator
The few-shot classiﬁcation weight generator G(., .|φ)
gets as input the feature vectors Z′ = {z′
i=1 of the N ′
training examples of a novel category (typically N ′ ≤5)
and (optionally) the classiﬁcation weight vectors of the base
categories Wbase. Based on them, it infers a classiﬁcation
weight vector w′ = G(Z′, Wbase|φ) for that novel category. Here we explain how the above few-shot classiﬁcation
weight generator is constructed.
Feature averaging based weight inference. Since, as
we explained in section § 3.1, the cosine similarity based
classiﬁer of the ConvNet model forces the feature extractor
to learn feature vectors that form compact category-wise
clusters and the classiﬁcation weight vectors to learn to be
representative feature vectors of those clusters, an obvious
choice is to infer the classiﬁcation weight vector w′ by averaging the feature vectors of the training examples (after they
have been l2-normalized):
The ﬁnal classiﬁcation weight vector in case we only use the
feature averaging mechanism is:
w′ = φavg ⊙w′
where ⊙is the Hadamard product, and φavg ∈Rd is a
learnable weight vector. Similar strategy has been previously
proposed by Snell et al. and has demonstrated very good
results. However, it does not fully exploit the knowledge
about the visual world that the ConvNet model acquires
during its training phase. Furthermore, in case there is only a
single training example for the novel category, the averaging
cannot infer an accurate classiﬁcation weight vector.
Attention-based weight inference.
We enhance the
above feature averaging mechanism with an attention based
mechanism that composes novel classiﬁcation weight vectors
by “looking” at a memory that contains the base classiﬁcation weight vectors Wbase = {wb}Kbase
b=1 . More speciﬁcally,
an extra attention-based classiﬁcation weight vector w′
computed as:
i, kb) · wb ,
where φq ∈Rd×d is a learnable weight matrix that transforms the feature vector z′
i to query vector used for querying
the memory, {kb ∈Rd}Kbase
is a set of Kbase learnable
keys (one per base category) used for indexing the memory,
and Att(., .) is an attention kernel implemented as a cosine
similarity function2 followed by a softmax operation over the
Kbase base categories. The ﬁnal classiﬁcation weight vector
is computed as a weighted sum of the average based classiﬁcation vector w′
avg and the attention based classiﬁcation
w′ = φavg ⊙w′
avg + φatt ⊙w′
where ⊙is the Hadamard product, and φavg, φatt ∈Rd are
learnable weight vectors.
Why using an attention-based weight composition?
Thanks to the cosine-similarity based classiﬁer, the base
classiﬁcation weight vectors learn to be representative feature vectors of their categories. Thus, the base classiﬁcation
weight vectors also encode visual similarity, e.g., the classi-
ﬁcation vector of a mammal animal should be closer to the
classiﬁcation vector of another mammal animal rather than
the classiﬁcation vector of a vehicle. Therefore, the classiﬁcation weight vector of a novel category can be composed
as a linear combination of those base classiﬁcation weight
vectors that are most similar to the few training examples
of that category. This allows our few-shot weight generator
to explicitly exploit the acquired knowledge about the visual word (here represented by the base classiﬁcation weight
vectors) in order to improve the few-shot recognition performance. This improvement is very signiﬁcant especially
in the one-shot recognition setting where averaging cannot
provide an accurate classiﬁcation weight vector.
3.3. Training procedure
In order to learn the ConvNet-based recognition model
the feature extractor F(.|θ) as well as the classi-
ﬁer C(.|W ∗)) and the few-shot classiﬁcation weight generator G(., .|φ), we use as the sole input a training set
Dtrain = SKbase
b=1 {xb,i}Nb
i=1 of Kbase base categories. We
split the training procedure into 2 stages and at each stage
we minimize a different cross-entropy loss of the following
loss(xb,i, b),
2The cosine similarity scores are also scaled by a learnable scalar parameter γ in order to increase the peakiness of the softmax distribution.
where loss(x, y) is the negative log-probability −log(py)
of the y-th category in the probability vector p
C(F(x|θ)|W ∗). The meaning of W ∗is different on each of
the training stages, as we explain below.
1st training stage: During this stage we only learn the
ConvNet recognition model without the few-shot classiﬁcation weight generator. Speciﬁcally, at this stage we learn the
parameters θ of the feature extractor F(.|θ) and the base classiﬁcation weight vectors Wbase = {wb}Kbase
b=1 . This is done
in exactly the same way as for any other standard recognition
model. In this case W ∗is equal to the base classiﬁcation
weight vectors Wbase.
2nd training stage: During this stage we train the learnable parameters φ of the few-shot classiﬁcation weight generator while we continue training the base classiﬁcation weight
vectors Wbase (in our experiments during that training stage
we freezed the feature extractor). In order to train the fewshow classiﬁcation weight generator, in each batch we randomly pick Knovel “fake” novel categories from the base categories and we treat them in the same way as we will treat the
actual novel categories after training. Speciﬁcally, instead
of using the classiﬁcation weight vectors in Wbase for those
“fake” novel categories, we sample N ′ training examples
(typically N ′ ≤5) for each of them, compute their feature
vectors Z′ = {z′
i=1, and give those feature vectors to the
few-shot classiﬁcation weight generator G(., .|φ) in order to
compute novel classiﬁcation weight generators. The inferred
classiﬁcation weight vectors are used for recognizing the
“fake” novel categories. Everything is trained end-to-end.
Note that we take care to exclude from the base classiﬁcation weight vectors that are given as a second argument to
the few-shot weight generator G(., .|φ) those classiﬁcation
vectors that correspond to the “fake” novel categories. In
this case W ∗is the union of the “fake” novel classiﬁcation
weight vectors generated by G(., .|φ) and the classiﬁcation
weight vectors of the remaining base categories. More implementation details of this training stage are provided in
appendix A.
4. Experimental results
We extensively evaluate the proposed few-shot recognition system w.r.t. both its few-shot recognition performance
of novel categories and its ability to not “forget” the base
categories on which it was trained.
4.1. Mini-ImageNet experiments
Evaluation setting for recognition of novel categories.
We evaluate our few-shot object recognition system on the
Mini-ImageNet dataset that includes 100 different categories with 600 images per category, each of size 84×84. For
our experiments we used the splits by Ravi and Laroche 
that include 64 categories for training, 16 categories for validation, and 20 categories for testing. The typical evaluation
setting on this dataset is ﬁrst to train a few-shot model on the
training categories and then during test time to use the validation (or the test) categories in order to form few-shot tasks
on which the trained model is evaluated. Those few-shot
tasks are formed by ﬁrst sampling Knovel categories and
one or ﬁve training example per category (1-shot and 5-shot
settings respectively), which the trained model uses for metalearning those categories, and then evaluating it on some test
examples that come from the same novel categories but do
not overlap with the training examples.
Evaluation setting for the recognition of the base categories. When we evaluate our model w.r.t. few-shot recognition task on the validation / test categories, we consider
as base categories the 64 training categories on which we
trained the model. Since the proposed few-shot object recognition system has the ability to not forget the base categories,
we would like to also evaluate the recognition performance
of our model on those base categories. Therefore, we sampled 300 extra images for each training category that we
use as validation image set for the evaluation of the recognition performance of the base categories and also another
300 extra images that are used for the same reason as test
image set. Therefore, when we evaluate our model w.r.t.
the few-shot learning task on the validation / test categories
we also evaluate w.r.t. recognition performance of the base
categories on the validation / test image set of the training
categories.
Ablation study
In Table 1 we provide an ablation study of the proposed
object recognition framework on the validation set of mini-
ImageNet. We also compare with two prior state-of-theart approaches, Prototypical Networks and Matching
Nets , that we re-implemented ourselves in order to
ensure a fair comparison. The feature extractor used in all
cases is a ConvNet model that has 4 convolutional modules,
with 3 × 3 convolutions, followed by batch normalization,
ReLU nonlinearity3, and 2 × 2 max-pooling. Given as input
images of size 84 × 84 it yields feature maps with spatial
size 5 × 5. The ﬁrst two convolutional layers have 64 feature
channels and the latter two have 128 feature channels.
Cosine-similarity based ConvNet model. First we examine the performance of the cosine-similarity based ConvNet recognition model (entry Cosine Classiﬁer) without
training the few-shot classiﬁcation weight generator (i.e.,
we only perform the 1st training stage as was described in
section 3.3). In order to test its performance on the novel
categories, during test time we estimate classiﬁcation weight
vectors using feature averaging. We want to stress out that
3Unless otherwise stated, our cosine-similarity based models as well as
the re-implementation of Matching-Nets do not have a ReLU nonlinearity
after the last convolutional layer, since in both cases this modiﬁcation
improved the recognition performance on the few-shot recognition task
5-Shot learning – Knovel=5
1-Shot learning – Knovel=5
Matching-Nets 
68.87 ± 0.38%
55.53 ± 0.48%
Prototypical-Nets 
72.67 ± 0.37%
54.44 ± 0.48%
Cosine Classiﬁer
72.83 ± 0.35%
54.55 ± 0.44%
Cosine Classiﬁer & Avg. Weight Gen
74.66 ± 0.35%
55.33 ± 0.46%
Cosine Classiﬁer & Att. Weight Gen
74.92 ± 0.36%
58.55 ± 0.50%
Dot Product
64.58 ± 0.38%
46.09 ± 0.40%
Dot Product & Avg. Weight Gen
60.30 ± 0.39%
44.31 ± 0.40%
Dot Product & Att. Weight Gen
67.81 ± 0.37%
53.88 ± 0.48%
Cosine w/ ReLU.
71.04 ± 0.36%
52.91 ± 0.45%
Cosine w/ ReLU. & Avg. Weight Gen
71.30 ± 0.38%
53.19 ± 0.45%
Cosine w/ ReLU. & Att. Weight Gen
73.03 ± 0.38%
56.09 ± 0.54%
Table 1: Average classiﬁcation accuracies on the validation set of Mini-ImageNet. The Novel columns report the average 5-way and 1-shot
or 5-shot classiﬁcation accuracies of novel categories (with 95% conﬁdence intervals), the Base and Both columns report the classiﬁcation
accuracies of base categories and of both type of categories respectively. In order to report those results we sampled 2000 tasks each with
15 × 5 test examples of novel categories and 15 × 5 test examples of base categories.
5-Shot learning – Knovel=5
1-Shot learning – Knovel=5
Matching-Nets 
Ravi and Laroche 
60.20 ± 0.71%
43.40 ± 0.77%
Finn et al. 
63.10 ± 0.92%
48.70 ± 1.84%
Prototypical-Nets 
68.20 ± 0.66%
49.42 ± 0.78%
Mishra et al. 
68.88 ± 0.92%
55.71 ± 0.99%
70.27 ± 0.64%
54.33 ± 0.81%
72.81 ± 0.62%
56.20 ± 0.86%
73.00 ± 0.64%
55.95 ± 0.84%
70.13 ± 0.68%
55.45 ± 0.89%
Table 2: Average classiﬁcation accuracies on the test set of Mini-ImageNet. In order to report those results we sampled 600 tasks in a
similar fashion as for the validation set of Mini-ImageNet.
in this case there are no learnable parameters involved in
the generation of the novel classiﬁcation weight vectors and
also the ConvNet model it was never trained on the fewshot recognition task. Despite that, the features learned by
the cosine-similarity based ConvNet model matches or even
surpasses the performance of the Matching-Nets and Prototypical Networks, which are explicitly trained on the few-shot
object recognition task. By comparing the cosine-similarity
based ConvNet models (Cosine Classiﬁer entries) with the
dot-product based models (Dot Product entries) we observe
that the former drastically improve the few-shot object recognition performance, which means that the feature extractor
that is learned with the cosine-similarity classiﬁer generalizes signiﬁcantly better on “unseen” categories than the
feature extractor learned with the dot-product classiﬁer. Notably, the cosine-similarity classiﬁer signiﬁcantly improves
also the recognition performance on the base categories.
Removing the last ReLU unit. In our work we propose
to remove the last ReLU non-linearity from the feature extractor when using a cosine classiﬁer. Instead, keeping the
ReLU units (Cosine w/ ReLU entries) decreases the accuracy
on novel categories while increasing it on base categories.
Few-shot classiﬁcation weight generator. Here we examine the performance of our system when we also incorporate on it the proposed few-shot classiﬁcation weight generator. In Table 1 we provide two solutions for the few-shot
weight generator: the entry Cosine Classiﬁer & Avg. Weight
Gen that uses only the feature averaging mechanism described in section 3.2 and the entry Cosine Classiﬁer & Att.
Weight Gen that uses both the feature averaging and the
attention based mechanism. Both types of few-shot weight
generators are trained during the 2nd training stage that is
described in section 3.3. We observe that both of them offer
a very signiﬁcant boost on the few-shot recognition performance of the cosine similarity based model (entry Cosine
Classiﬁer). Among the two, the attention based solution
exhibits better few-shot recognition behavior, especially in
the 1-shot setting where it has more than 3 percentage points
higher performance. Also, it is easy to see that the few-shot
classiﬁcation weight generator does not affect the recognition performance of the base categories, which is around
70.50% in all the cosine-similarity based models. Moreover,
by introducing the few-shot weight generator, the recognition performance in both type of categories (columns Both)
increases signiﬁcantly, which means that the ConvNet model
achieves better behavior w.r.t. our goal of uniﬁed recognition
of both base and novel categories. The few-shot recognition performance of our full system, which is the one that
includes the attention based few-shot weight generator (entry
Cosine classiﬁer & Att. Weight Gen), offers a very signiﬁcant improvement w.r.t. the prior state-of-the-art approaches
on the few-shot object recognition task, i.e., from 72.67% to
74.92% in the 5-shot setting and from 55.53% to 58.55% in
the 1-shot setting. Also, our system achieves signiﬁcantly
higher performance on the recognition of base categories
compared to Prototypical Networks4.
Comparison with state-of-the-art
Here we compare the proposed few-shot object recognition
system with other state-of-the-art approaches on the Mini-
ImageNet test set.
Explored feature extractor architectures.
prior approaches use several different network architectures
for implementing the feature extractor of the ConvNet model,
we evaluate our model with each of those architectures.
Speciﬁcally the architectures that we evaluated are: C32F
is a 4 module ConvNet network (which was described in
§ 4.1.1) with 32 feature channels on each convolutional layer,
C64F has 64 feature channels on each layer, and in C128F
the ﬁrst two layers have 64 channels and the latter two have
128 channels (exactly the same as the model that was used
in § 4.1.1). With RESNET we refer to the ResNet like
network that was used from Mishra et al. (for more
details we refer to ).
In Table 2 we provide the experimental results. In all
cases, our models (that include the cosine-similarity based
ConvNet model and the attention-based few-shot weight
generator) achieve better few-shot object recognition performance than prior approaches. Moreover, it is very important
to note that our approach is capable to achieve such excellent accuracy on the novel categories while at the same time
4In order to recognize base categories with Prototypical Networks, the
prototypes for the base categories are computed by averaging all the available training features vectors
it does not sacriﬁce the recognition performance of the base
categories, which is an ability that prior methods lack.
Qualitative evaluation with t-SNE scatter plots
Here we compare qualitatively the feature representations
learned by the proposed cosine-similarity based ConvNet
recognition model with those learned by the typical dotproduct based ConvNet recognition model. For that purpose
in Figure 2 we provide the t-SNE scatter plots that
visualize the local-structures of the feature representations
learned in those two cases. Note that the visualized features
are from the validation categories of the Mini-ImageNet
dataset that are “unseen” during training. Also, in the case of
the cosine-similarity based ConvNet recognition model, we
visualize the l2-normalized features, which are the features
that are actually learned by the feature extractor.
We observe that the feature extractor learned with the
cosine-similarity based ConvNet recognition model, when
applied on the images of “unseen” categories (in this case
the validation categories of Mini-ImageNet), it generates
features that form more compact and distinctive categoryspeciﬁc clusters (i.e., more discriminative features). Due to
that, as it was argued in section §3.1, the features learned
with the proposed cosine-similarity based recognition model
generalize better on the “unseen” categories than the features learned with the typical dot-product based recognition
4.2. Few-shot benchmark of Bharath & Girshick 
Here we evaluate our approach on the ImageNet based
few-shot benchmark proposed by Bharath and Girshick 
using the improved evaluation metrics proposed by Wang
et al. . Brieﬂy, this benchmark splits the ImageNet categories into 389 base categories and 611 novel categories;
193 of the base categories and 300 of the novel categories
are used for cross validation and the remaining 196 base
categories and 311 novel categories are used for the ﬁnal
evaluation (for more details we refer to ). We use the
same categories split as they did. However, because it was
not possible to use the same training images that they did for
the novel categories5, we sample ourselves N ′ training images per novel category and, similar to them, evaluate using
the images in the validation set of ImageNet. We repeat the
above experiment 100 times (sampling each time a different
set of training images for the novel categories) and report in
Table 3 the mean accuracies and the 95% conﬁdence intervals for the recognition accuracy metrics proposed in .
Comparison to prior and concurrent work. We compare our full system (Cosine Classiﬁer & Att. Weight Gen
entry) against prior work, such as Prototypical-Nets ,
5It was not possible to establish a correspondence between the index
ﬁles that they provide and the ImageNet images
All with prior
Prior work
Prototypical-Nets (from )
Matching Networks (from )
Logistic regression (from )
Logistic regression w/ H (from )
SGM w/ H 
Batch SGM 
Concurrent work
Prototype Matching Nets w/ H 
Prototype Matching Nets 
Cosine Classiﬁer & Avg. Weight Gen
45.23 56.90 68.68 74.36 77.69
57.65 64.69 72.35 76.18 78.46
56.43 63.41 70.95 74.75 77.00
± .25 ± .16 ± .09 ± .06 ± .06
± .15 ± .10 ± .06 ± .04 ± .04
± .15 ± .10 ± .06 ± .04 ± .03
Cosine Classiﬁer & Att. Weight Gen
46.02 57.51 69.16 74.83 78.11
58.16 65.21 72.72 76.50 78.74
56.76 63.80 72.72 75.02 77.25
± .25 ± .15 ± .09 ± .06 ± .05
± .15 ± .09 ± .06 ± .04 ± .03
± .15 ± .10 ± .06 ± .04 ± .04
Table 3: Top-5 accuracy on the novel categories and on all categories (with and without priors) fot the ImageNet based few-shot benchmark
proposed in (for more details about the evaluation metrics we refer to ). For each novel category we use N ′ = 1, 2, 5, 10 or 20
training examples. Methods with “w/ H” use mechanisms that hallucinate extra training examples for the novel categories. The second rows
in our entries report the 95% conﬁdence intervals.
Matching Networks , and the work of Bharath and Girshick . We also compare against the work of Wang et
al. , which is concurrent to ours. We observe that in
all cases our approach achieves superior performance than
prior approaches and even exceeds (in all but one cases)
the Prototype Matching Net based approaches that are
concurrent to our work.
Feature extractor:
The feature extractor of all approaches is implemented with a ResNet-10 network
architecture6 that gets as input images of 224 × 224 size.
Also, when training the attention based few-shot classiﬁcation weight generator component of our model (2nd training
stage) we found helpful to apply dropout with 0.5 probability
on the feature vectors generated by the feature extractor.
5. Conclusions
In our work we propose a dynamic few-shot object recognition system that is able to quickly learn novel categories
without forgetting the base categories on which it was trained,
a property that most prior approaches on the few-shot learning task neglect to fulﬁll. To achieve that goal we propose a
novel attention based few-shot classiﬁcation weight generator as well as a cosine-similarity based ConvNet classiﬁer.
This allows to recognize in a uniﬁed way both novel and
base categories and also leads to learn feature representations with better generalization capabilities. We evaluate our
framework on Mini-ImageNet and the recently introduced
fews-shot benchmark of Bharath and Girshick where
we demonstrate that our approach is capable of both maintaining high recognition accuracy on base categories and to
6Similar to what it is already explained, our model does not include the
last ReLU non-linearity of the ResNet-10 feature extractor
achieve excellent few-shot recognition accuracy on novel
categories that surpasses prior state-of-the-art approaches by
a signiﬁcant margin.