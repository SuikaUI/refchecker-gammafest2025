Bayesian inference for inverse problems
Ali Mohammad-Djafari
Laboratoire des Signaux et Systèmes,
Supélec, Plateau de Moulon, 91192 Gif-sur-Yvette, France
Abstract. Traditionally, the MaxEnt workshops start by a tutorial day. This paper summarizes my
talk during 2001’th workshop at John Hopkins University. The main idea in this talk is to show how
the Bayesian inference can naturally give us all the necessary tools we need to solve real inverse
problems: starting by simple inversion where we assume to know exactly the forward model and
all the input model parameters up to more realistic advanced problems of myopic or blind inversion
where we may be uncertain about the forward model and we may have noisy data.
Starting by an introduction to inverse problems through a few examples and explaining their
ill posedness nature, I brieﬂy presented the main classical deterministic methods such as data
matching and classical regularization methods to show their limitations. I then presented the main
classical probabilistic methods based on likelihood, information theory and maximum entropy and
the Bayesian inference framework for such problems. I show that the Bayesian framework, not
only generalizes all these methods, but also gives us natural tools, for example, for inferring the
uncertainty of the computed solutions, for the estimation of the hyperparameters or for handling
myopic or blind inversion problems. Finally, through a deconvolution problem example, I presented
a few state of the art methods based on Bayesian inference particularly designed for some of the
mass spectrometry data processing problems.
INTRODUCTION
Forward and inverse problems
In experimental science, it is hard to ﬁnd an example where we can measure directly
a desired quantity. Describing mathematical models to relate the measured quantities to
the unknown quantity of interest is called forward modeling problem. The main object
of a forward modeling is to be able to generate data which are as likely as possible to the
observed data if the unknown quantity was known. But, almost always, we want to use
this model and the observed data to make inference on the unknown quantity of interest:
This is the inversion problem. To be more explicit, let take an example that we will use
all along this paper to illustrate the different aspects of inverse problems. The example
is taken from the mass spectrometry where the ideal physical quantity of interest is the
components mass distribution of the material under the test. There are many techniques
used in mass spectrometry. The Time-of-Flight (TOF) technique is one of them. In this
technique, one measures the electrical current generated on the surface of a detector by
the charged ions generated by the material under the test. Finding a very ﬁne physical
model to relate the time variation of this current to the distribution of the arrival times
of the charged ions, which is itself related to the components mass distribution of the
material under the test, is not an easy task. However, in a ﬁrst approximation, assuming
that the instrument is linear and its characteristics do not change during the acquisition
time of the experiment, a very simple convolution model relates the raw data g(t) to the
unknown quantity of interest f(t):
f(t)h(τ −t) dt,
where h(t) is the point spread function (psf) of the instrument. Figure 1 shows an
example of data observed (signal in b) for a theoretical mass distribution (signal in a).
Blurring effect in TOF mass spectrometry data:
a) desired or theoretical spectrum,
observed data.
In this example, the forward problem consists in computing g given f and h which is
given by a simple convolution operation. The inverse problem of inferring f given g and
h is called deconvolution, the inverse problem of inferring h given g and f is called psf
identiﬁcation and the inverse problem of inferring h and f given only g is called blind
deconvolution.
In my talk, I have given many more examples such as image restoration
g(x′,y′) =
f(x,y)h(x′ −x,y′ −y) dx dy,
or Fourier synthesis inversion
f(ω) exp{−jωτ} dω
as well as a few non linear inverse problems. I am not going to detail them here, but I
try to give a uniﬁed method to deal with all these problems. For this purpose, ﬁrst we
note that, in all these problems, we have always limited the number of data, for example
yi = g(τi), i = 1,... ,m. We also note that, to be able to do numerical computation, we
need to model the unknown function f by a ﬁnite number of parameters x = [x1,... ,xn].
As an example, we may assume that
where bj(t) are known basis functions. With this assumption the raw data
y = [y1,... ,ym] are related to the unknown parameters x by
yi = g(τi) =
with Hi,j =
bj(t)h(t−τi) dt
which can be written in the simple matrix form y = Hx. The inversion problem can
then be simpliﬁed to the estimation of x given H and y. Two approaches are then in
competition:
i) the dimensional control approach which consists in an appropriate choice of the basis
functions bj(r) and n ≤m in such a way that the equation y = Ax be well conditioned;
ii) the more general regularization approach where a classical sampling basis for bj(r)
with desired resolution is chosen no matter if n > m or if A is ill conditioned. In
the following, we follow the second approach which is more ﬂexible for adding more
general prior information on x.
We must also remark that, in general, it is very difﬁcult to give a very ﬁne mathematical model to take account for all the different quantities affecting the measurement
process. However, we can almost always come up with a more general relation such as
yi = hθ(x)+ǫi,
i = 1,... ,m
where θ represents the unknown parameters of the forward model (for example the
amplitude and the width of a Gaussian shape psf in a deconvolution problem) and
ǫ = [ǫ1,... ,ǫm] represents all the errors (measurement noise, discretization errors and
all the other uncertainties of the model). For the case of linear models we have
y = Hθx+ǫ.
In this paper we focus on this general problem. We ﬁrst consider the case where the
model is assumed to be perfectly known. This is the simple inversion problem. Then we
consider the more general case where we have also to infer on θ. This is the myopic or
blind inversion problem.
Even in the simplest case of perfectly known linear system and exact data:
i) the operator H may not be invertible (H−1 does not exist);
ii) it may admit more than one inverse (∃G1 and G2|G1(H) = G2(H) = I where I is
the identity operator); or
iii) it may be very ill-posed or ill-conditioned (meaning that there exists x and x+αδx
H−1(x)−H−1(x+αδx)
never vanishes even if α 7→0 .
These are the three necessary conditions of existence, uniqueness and stability of
Hadamard for the well-posedness of an inversion problem. This explains the fact that,
in general, even in this simple case, many naïve methods based on generalized inversion
or on least squares may not give satisfactory results. The following ﬁgure shows, in a
simple way, the ill-posedness of a deconvolution problem. On this ﬁgure, we see that
three different input signals can result three outputs which are practically indistinguishable from each other. This means that, data matching alone can not distinguish between
any of these inputs.
Ill-posedness of a deconvolution problem: Inputs on the left give practically indistinguishable outputs.
As a conclusion, we see that, apart from the data, we need extra information. The art of
inversion in a particular inverse problem is how to include just enough prior information
to obtain a satisfactory result. In the following, ﬁrst we summarize the classical deterministic approaches of data matching and regularization. Then, we focus on probabilistic
approaches where errors and uncertainties are taken into account through the probability
laws. Here, we distinguish, three classes of methods: those which only account for the
data errors (error probability distribution matching and likelihood based methods), those
which only account for uncertainties of unknown parameters (entropy based methods)
and those which account for both of them (Bayesian inference approach).
DATA MATCHING AND REGULARIZATION METHODS
Exact data matching
Let consider the discretized equation yi = hi(x) + ǫi, i = 1,... ,m; and assume ﬁrst
that the model and data are exact (ǫi = 0). We can then write y = h(x).
Assume now the system of equations is under determined, i.e., there is more than one
solution satisfying it (for example when the number of data is less than the number of
unknowns). Then, one way to obtain a unique solution is to deﬁne an a priori criterion,
for example ∆(x,m) to choose that unique solution by
bx = argmin
where m is an a priori solution and ∆a distance measure.
In the linear inverse problems case, the solution to this constrained optimization
can be obtained via Lagrangian techniques which consists in deﬁning the Lagrangian
L(x,λ) = ∆(x,m)+λt(y −Hx) and searching for (bλ, bx) through
argminλ {D(λ) = infx L(x,λ)}
Noting that ∇xL = ∇x∆(x,m)−Htλ and ∇λL = y −Hx and deﬁning G(s,m) =
supx{xts−∆(x,m)} the algorithm to ﬁnd the solution bx becomes:
– Determine G(s,m) = supx{xts−∆(x,m)};
– Find bλ = argminλ
D(λ) = λty −G(Htλ,m)
– Determine bx = ∇sG(Htbλ).
As an example, when ∆(x,m) = 1
2 ∥x−m∥2 then G(s,m) = mts+ 1
2 ∥s∥2, ∇sG =
m + s and D(λ) = λty −mtHtλ + 1
2 which results to bλ = (HHt)−1(y −
Hm) and the solution is given by
bx = m+Ht(HHt)−1(y −Hm).
One can remark that, when m = 0 we have bx = Ht(HHt)−1y and this is the classical
minimum norm generalized inverse solution.
Another example is the classical Maximum Entropy method case where ∆(x,m) =
KL(x,m) is the Kullback-Leibler distance or cross entropy between x and the a priori
solution m:
Here, the solution is given by
bxj = mj exp
with bλ = argmin
D(λ) = λty −G(Atλ,m)
where G(s,m) = P
j mj (1−exp[−sj]). But, unfortunately here D(λ) is not a quadratic
function of λ and thus there is not an analytic expression for bλ. However, it can
be computed numerically and many algorithms have been proposed for its efﬁcient
computation. See for example and the cited references for more discussions on the
computational issues and algorithm implementation.
The main issue here is that, this approach gives a satisfactory solution to the uniqueness of the inverse problem, but in general, the performances obtained by the resulting
algorithms stay sensitive to error on the data.
Least squares data matching and regularization
When the discretized equation y = h(x) is over-determined, i.e., there is no solution
satisfying it exactly (for example when the number of data is greater than the number of
unknowns or when the data are not exact), one can try to estimate them by:
bx = argmin
{∆(y,h(x))},
where ∆(y,h(x)) is a distance measure in the data space. The case where ∆(y,h(x)) =
∥y −h(x)∥2 is the classical Least Squares (LS) criterion.
For a linear inversion problem y = Hx, it is easy to see that any bx which satisﬁes
the normal equation HtHbx = Hty is a LS solution. If HtH is invertible and wellconditioned then bx = (HtH)−1Hty is again the unique generalized inverse solution.
But, in general, this is not the case: HtH is rank deﬁcient and we need to constrain the
space of the admissible solutions. The constraint LS is then deﬁned as
bx = argmin
where C is a convex set. The choice of the set C is primordial to satisfy the three
conditions of a well-posed solution. An example is the positivity constraint: C = {x :
∀j, xj > 0}. Another example is C = {x : ∥x∥< α} where the solution can be computed
via the optimization of
J(x) = ∥y −H(x)∥2 +λ∥x∥.
The main technical difﬁculty is the relation between α and λ. The minimum norm LS
solution can also be computed using the singular value decomposition . The main
issue here is that, even if this approach has been well understood and commonly used,
it assumes implicitly that the noise and the x are Gaussian. This may not be suitable in
some applications, and more speciﬁcally in mass spectrometry data processing where
the unknowns are spiky spectra.
A more general regularization procedure is to deﬁne the solution to the inversion
problem y = H(x)+ǫ as the optimizer of a compound criterion J(x) = ∥y −Hx∥2 +
λφ(x) or the more general criterion
J(x) = ∆1(y,Hx)+λ∆2(x,m).
where ∆1 and ∆2 are two distances or discrepancy measures, λ a regularization parameter and m an a priori solution . The main questions here are: i) how to choose ∆1
and ∆2 and ii) how to determine λ and m.
For the ﬁrst question, many choices exist:
– Quadratic or L2 distance:
∆(x,z) = ∥x−z∥2 = P
j(xj −zj)2;
– Lp distance:
∆(x,z) = ∥x−z∥p = P
j |xj −zj|p;
– Kullback distance:
∆(x,z) = P
j xj ln(xj/zj)−(xj −zj);
– Roughness distance:
∆(x,z) any of the previous distances with zj = xj−1 or zj =
(xj−1 + xj+1)/2 or any linear function zj = ψ(xk,k ∈N(j)) where N(j) stands for
the neighborhood of j. (One can see the link between this last case and the Gibbsian
energies in the Markovian modeling of signals and images.)
The second difﬁculty in this deterministic approach is the determination of the regularization parameter λ. Even if there are some techniques based on cross validation
 , there is not natural tools for their extension to other hyperparameters in a natural way.
As a simple example, we consider the case where both ∆1 and ∆2 are quadratic:
J(x) = ∥y −Hx∥2
W + λ∥x−m∥2
Q. The optimization problem, in this case, has an
analytic solution:
 HtW H +λQ
−1 HtW y −Qm)
which can also be written
bx = m+Q−1Ht  HQ−1Ht +λ−1W −1−1(y −Hm)
which is a linear function of the a priori solution m and the data y. Note also
that when m = 0, Q = I and W = I we have bx =
−1 Hty or bx =
Ht  HHt +λ−1I
−1y and when λ = 0 we obtain the generalized inverse solutions
−1Hty or bx = Ht  HHt−1y.
As we mentioned before, the main practical difﬁculties in this approach are the choice
of ∆1 and ∆2 and the determination of the hyperparameters λ and the inverse covariance
matrices W and Q.
As a main conclusion on these deterministic inversion methods, we can say that,
even if, in practice, they are used and give satisfaction, they lack tools to handle with
uncertainties and to account for more precise a priori knowledge of statistical properties
of errors and unknown parameters. The probabilistic methods can exactly handle more
easily these problems as we will see in the following.
PROBABILISTIC METHODS
Probability distribution matching and maximum likelihood
The main idea here is to account for data and model uncertainty through the assignment of a theoretical distribution pY |X(y|x) to the data. In probability distribution
matching method, the main idea is to determine the unknown parameters x by minimizing a distance measure ∆(ρ,p) between the empirical histogram ρ of the data deﬁned
and the theoretical distribution of the data pY |X(z|x).
When ∆(p,ρ) is choosed to be the Kullback-Leibler mismatch measure
pY |X(z|x) dz
ρ(z)lnpY |X(z|x) dz +
ρ(z)lnρ(z) dz
bx = argmin
{KL[ρ,p]} = argmin
ρ(z)lnpY |X(z|x) dz
It is then easy to see that, for the i.i.d. data, this estimate becomes equivalent to the
maximum likelihood (ML) estimate
bx = argmin
−lnpY |X(z|x)|z=y
pY |X(y|x)
In the case of a linear model and Gaussian noise, it is easy to show that the ML estimate
becomes equivalent to the LS one, which in general, does not give satisfactory results as
we have discussed it in the previous section.
The important point to note here is that, in this approach, only the data uncertainty
is considered and modeled through the probabilty law pY |X(y|x). We will see in the
following that, in contrary to this approach, in information theory and maximum entropy
methods, the data and model are assumed to be exact and only the uncertainty of
x is modeled through an a priori reference measure µ(x) which is updated to an a
posteriori probabilty law p(x) by optimizing the KL mismatch KL(p,µ) subject to the
data constraints.
Maximum entropy in the mean
The main idea in this approach is to consider x as the mean value of a quantity X ∈C,
where C is a compact set on which we want to deﬁne a probability law P: x = EP {X}
and the data y as exact equality constraints on it:
y = Hx = HEP {X} =
Then, assuming that we can translate our prior information on the unknowns through a
prior law (a reference measure) dµ(x), we can determine the distribution P by:
dµ(x) dP(x)
y = Hx = HEP {X}.
The solution is obtained via the Lagrangian:
dµ(x) −λt(y −Hx)
and is given by:
dP(x,λ) = exp
λt[Hx]−lnZ(λ)
dµ(x). The Lagrange parameters are obtained by searching
the unique solution (if exists) of the following system of non linear equations:
i = 1,··· ,M.
Then, naturally, the solution to the inverse problem is deﬁned as the expected value of
this distribution:
bx(λ) = EP {X} =
x dP(x,λ). The interesting point here is that,
the solution bx(bλ) can be computed without actually computing P in two ways:
– Via optimization of a dual criterion: The solution bx is expressed as a function of the
dual variable bs = Htbλ by bx(bs) = ∇sG(bs,m) where
G(s,m) = lnZ(s,m) = ln
m = Eµ {X} =
bλ = argmax
D(λ) = λty −G(Htλ)
– Via optimization of a primal or direct criterion:
bx = argmin
{H(x,m)} s.t. y = Hx where H(x,m) = sup
{stx−G(s,m)}.
Another interesting point is the link between these two options:
i) Functions G and H depend on the reference measure µ(x);
ii) The dual criterion D(λ) depends on the data and the function G;
iii) The primal criterion H(x,m) is a distance measure between x and m which means:
H(x,m) ≥0 and H(x,m) = 0
H(x,m) is differentiable and convex
on C and H(x,m) = ∞
iv) If the reference measure is separable: µ(x) = QN
j=1µj(xj) then P is too:
dP(x,λ) = QN
j=1 dPj(xj,λ) and we have
gj (sj,mj),
hj(xj,mj),
where gj is the log Laplace transform (Cramer transform) of µj:
gj(s) = ln
exp[sx] dµj(x);
and hj is the convex conjugate of gj:
hj(x) = max
s {sx−gj(s)}.
The following table gives three examples of choices for µj and the resulting expressions for gj and hj:
−(1/2)(x−m)2
(1/2)(s−m)2
(1/2)(x−m)2
(mx/x!)exp[−m]
−xln(x/m)+m−x
xα−1 exp[−(x/m)]
−ln(x/m)+(x/m)−1
We may remark that the two famous expressions of the Burg lnx and Shannon −xlnx
entropies are obtained as special cases.
As a conclusion, we see that the Maximum entropy in mean extends in some way the
classical ME approach by giving other expressions for the criterion to optimize. Indeed,
it can be shown that when we optimize a convex criterion subject to the data constraints
we are optimizing the entropy of some quantity related to the unknowns and vise versa.
However, as we have mentioned, basically, in this approach the data and the model are
assumed to be exact even if some extensions to the approach gives the possibility to
account for the errors . In the next section, we see how the Bayesian approach can
naturally account for both uncertainties on the data and on the unknown parameters x.
BAYESIAN INFERENCE APPROACH
In Bayesian approach, the main idea is to translate our prior knowledge on the errors
ǫ and on the unknowns x to prior probability laws p(ǫ) and p(x). The next step is to
use the forward model and p(ǫ) to deduce p(y|x). The Bayes rule can then be used to
determine the posterior law of the unknowns p(x|y) from which we can deduce any
information about the unknowns x. The posterior p(x|y) is thus the ﬁnal product of the
Bayesian approach. However, very often, we need a last step which is to take out the
necessary information about x from this posterior. The tools for this last step are the
decision and estimation theories.
To illustrate this, let consider the case of linear inverse problems y = Hx + ǫ. The
ﬁrst step is to write down explicitly our hypothesis: starting by the hypothesis that ǫ is
zero-mean (no systematic error), white (no time correlation for the errors) and assuming
that we may only have some idea about its energy σ2
ǫ = 1/(2φ1), and using either
the intuition or the Maximum Entropy Principle (MEP) lead to a Gaussian prior law:
ǫ ∼N (0,1/(2φ1)I). Then, using the forward model with this assumption leads to:
p(y|x,φ1) ∝exp
−φ1∥y −Hx∥2
The next step is to assign a prior law to the unknowns x. This step is more difﬁcult and
needs more caution. In inverse problems, as we presented, x represents the samples of a
signal or the pixel values of an aerian image. Very often then we have ensemblist prior
knowledge about the signals or images concerned by the application and we can model
them. The art of the engineer is then to choose the appropriate model and to translate
this information to a probability law to reﬂect it.
Again here, let illustrate this step, ﬁrst through a few general examples and then more
speciﬁcally the case of mass spectrometry deconvolution problem.
In the ﬁrst example, we assume that, a priori we do not have (or we do not want
or we are not able to account for) any knowledge about the correlation between the
components of x. This leads us to
Now, we have to assign pj(xj). For this, we may assume to know the mean values mj
and some idea about the dispersions about these mean values. This again leads us to
Gaussian laws N(mj,σ2
xj), and if we assume the same dispersions σ2
xj = 1/(2φ2),∀j we
−φ2 ∥x−m∥2
With these assumptions, using the Bayes rule, we obtain
p(x|y) ∝exp
−φ1 ∥y −Hx∥2 −φ2 ∥x−m∥2
This posterior law contains all the information we can have on x (combination of our
prior knowledge and data). If x was a scalar or a vector of only two components, we
could plot the probability distribution and look at it. But, in practical applications, x may
be a vector with huge number of components. Then, even if we can obtain an expression
for this posterior, we may need to summarize its information content. In general then, we
may choose, equivalently, between summarizing it by its mode, mean, marginal modes,
etc ... , or use the decision and estimation theory to deﬁne point estimators to be used
to compute (best representing values). For example, we can choose the value bx which
corresponds to the mode of p(x|y)– the Maximum a posteriori (MAP) estimate, or the
value bx which corresponds to the mean of this posterior– the Posterior mean (PM)
estimate, or when interested to the component xj, to choose bxj corresponding to the
mode of the posterior marginal p(xj|y).
We can also generate samples from this posterior and just look at them as a movie
or use them to compute the PM estimate. We can also use it to compute the posterior
covariance matrix (P = E{(x−bx)(x−bx)t} where bx is the posterior mean), from
which we can infer on the uncertainty of the proposed solutions.
In the Gaussian priors case we just presented, it is easy to see that, the posterior law is
also Gaussian and all these estimates are the same and can be computed by minimizing
J(x) = −lnp(x|y) = ∥y −Hx∥2 +λ∥x−m∥2
with λ = φ2
We may note here the analogy with the quadratic regularization criterion (16) with the
emphasis that the choice ∆1(y,Hx) = ∥y −Hx∥2 and ∆2(x,m) = ∥x−m∥2 are
the direct consequences of Gaussian choices for prior laws of the noise p(ǫ) and the
unknowns p(x).
The Gaussian choice for pj(xj) is not always a pertinent one. For example, we may
a priori know that the distribution of xj around their means mj are more concentrated
but great deviations from them are also more likely than a Gaussian distribution. This
knowledge can be translated by choosing a Generalized Gaussian law:
p(xj) ∝exp
In some cases we may know more, for example we may know that xj are positive
values. Then a Gamma prior law
p(xj) = G(α,mj) ∝(xj/mj)−α exp[−xj/mj]
would be a better choice.
In some other cases we may know that xj are discrete positive values. Then a Poisson
xj! exp[−mj]
is a better choice.
In all these cases, the expression of the posterior is p(x|y) ∝exp[−J(x)] with J(x) =
∥y −Hx∥2 + λφ(x) where φ(x) = −lnp(x). It is interesting to note the different
expressions of φ(x) for the prior laws discussed and remark that they contain different
entropy expressions for the x.
The last general example is the case where a priori we know that xj are not independent, for example when they represents the pixels of an aerian image. We may then use
a Markovian modeling
p(xj|xk,k ∈S) = p(xj|xk,k ∈N(j)),
where S = {1,... ,N} stands for the whole set of pixels and N (j) = {k : |k −j| ≤r}
stands for r-th order neighborhood of j.
With some assumptions on the border limits, such models again result to the optimization of the same criterion with
φ(x) = ∆2(x,z) =
φ(xj,zj) where zj = ψ(xk,k ∈N(j))
with different potential functions φ(xj,zj).
A simple example is the case where zj = xj−1 and φ(xj,zj) any function in between
the following:
|xj −zj|α,
See ( ) for some more discussion and properties of these potential functions.
As one of the main conclusions here, we see that, as it concerns the MAP estimation,
the Bayesian approach is equivalent to the general regularization. However, here the
choice of the distance measure ∆1(y,Hx) depends on the forward model and the
hypothesis on the noise and the choice of the distance measure ∆2(x,m) depends on
the prior law chosen for x.
One more extra feature here is that, we have access to the whole posterior p(x|y) from
which, not only we can deﬁne an estimate but also, we can quantify its corresponding
remained uncertainty. We can also compare posterior and prior laws of the unknowns to
measure the amount of information contained in the observed data. Finally, as we will
see in the following, we have ﬁner tools to model unknown signals or images and to
estimate the hyperparameters.
OPEN PROBLEMS AND ADVANCED METHODS
As we have remarked in previous sections, in general, the solution of an inverse problem
depends on our prior hypothesis on errors ǫ and on x. Before applying the Bayes rule,
we have to assign the prior laws to them. From the forward model and assumptions
on ǫ we assign p(y|x,φ1) and from the assumptions on x we assign p(x|φ2). This
step is one of the most crucial part of the applicability of the Bayesian framework for
inverse problems. Modeling a signal and ﬁnding the corresponding expression for the
prior law p(x|φ2) is not an easy task. This choice may have many consequences: the
complexity of the computation of the posterior and consequently the computation of
any point estimators such as MAP (which needs optimization) or PM (which needs
integration either analytically or by Monte Carlo methods). This modeling depends also
on the application. We discuss this point through the particular deconvolution problem
in mass spectrometry.
Appropriate modeling of input signal
We actually had started this discussion in previous section and we saw that, at least
for linear inverse problems with a white Gaussian assumption of the noise, the posterior
has for expression: p(x|y) ∝exp[−J(x)] with
J(x) = ∥y −Hx∥2 +λφ(x)
with φ(x) = −lnp(x). Thus the expression and properties of J(x), and consequently
those of the posterior p(x|y) depend on the prior p(x). For example if p(x) is Gaussian
φ(x) = −lnp(x) =
is a quadratic function of x. Then the MAP or PM estimates have the same values and
their computation needs the optimization of a quadratic criterion which can be done
either analytically or by using any simple gradient based algorithm. But the Gaussian
modeling is not always an appropriate one. Let take our example of deconvolution of
mass spectrometry data. We know a priori that the input signal must be positive. Then a
truncated Gaussian will be a better choice:
else φ(x) = ∞.
But, we know still more about the input signal: it has pulse shapes, meaning that, if we
look at the histogram of the samples of a typical signal, we see that great number of
samples are near to zero but great deviations from this background are not rare. Thus, a
generalized Gaussian
|xj|p with 1 ≤p ≤2;
else φ(x) = ∞.
or a Gamma prior law
lnxj +xj if xj ≥0;else φ(x) = ∞.
would be better choices.
We can also go further in details and want to account for the fact that we are looking
for atomic pulses. Then we can imagine a binary valued random vector z with p(zj =
1) = α and p(zj = 0) = 1−α, and describe the distribution of x hierarchically:
p(xj|zj) = zj p0(xj)
with p0(xj) being either a Gaussian p(xj) = N (m,σ2) or a Gamma law p(xj) = G(a,b).
The second choice is more appropriate while the ﬁrst results on simpler estimation
algorithms. The inference can then be done through the joint posterior
p(x,z|y) ∝p(y|x)p(x|z)p(z)
The estimation of z is then called Detection and that of x Estimation. The case where
we assume p(z) = Q
j p(zj) = αn1(1 −α)(n−n1) with n1 the number of ones and n the
length of the vector z, is called Bernoulli process and this modelization for x is called
Bernoulli-Gaussian or Bernoulli-Gamma as a function of the choice for p0(xj).
The difﬁcult step in this modeling is the detection step which needs the computation
p(z|y) ∝p(z)
p(y|x)p(x|z) dx
and then its optimization over {0,1}n where n is the length of the vector z. The cost of
the computation of the exact solution is huge (a combinatorial problem).
Many approximations to this optimization have been proposed which result to different algorithms for this detection-estimation problem . Many Monte Carlo techniques
have also been proposed for generating samples of z and x from the posterior and thus
compute the PM estimates of x. Giving more details on this modeling and details of
corresponding algorithms is out of the scope of this paper.
The results on the following ﬁgure illustrate this discussion. Here, we used the data
in ﬁgure 1 and computed x by optimizing the MAP criterion (36), with different prior
laws p(x) ∝exp[−λφ(x)] in between the following choices:
a) Gaussian: φ(x) = Px2
b) Gaussian truncated on positive axis: φ(x) = Px2
j, xj > 0,
c) Generalized Gaussian truncated on positive axis: φ(x) = P|xj|p with p = 1.1, xj > 0.
d) Entropic prior φ(x) = Pxj lnxj −xj, xj > 0,
e) Gamma prior: φ(x) = Plnxj +xj, xj > 0.
Deconvolution results with different priors:
a) Gaussian b) Gaussian truncated to positive
axis c) Generalized Gaussian. d) −xlnx entropic prior e) lnx entropic or Gamma prior.
As it can be seen from these results 1 , for this application, the Gaussian prior does not
give satisfactory result, but in almost all the other cases the results are more satisfactory,
because the corresponding priors are more in agreement with the nature of the unknown
input signal.
x ln x − x
Plots of the different prior laws p(x) ∝exp[−λφ(x)]: a) Truncated Gaussian
x2,λ = 3 b) Truncated generalized Gaussian
φ(x) = xp, p = 1.1, λ = 4; c) Entropic φ(x) = xlnx −
x,λ = 10 d) Entropic φ(x) = lnx+ x,λ = 0.1.
The Gaussian prior (a) is not at all appropriate, Gaussian truncated to positive axis (b)
is a better choice. The generalized Gaussian truncated to positive axis (c) and the −xlnx
entropic priors (d) give also almost the same results than the truncated Gaussian case.
The Gamma prior (e) seems to give slightly better result (less missing and less artifacts)
than all the others. This can be explained if we compare the shape of all these priors
shown in ﬁgure (4). The Gamma prior is sharper near to zero and has longer tail than
other priors. It thus favorites signals with greater number of samples near to zero and still
leaves the possibility to have very high amplitude pulses. However, we must be careful
on this interpretation, because all these results depend also on the hyperparameter λ
whose value may be critical for this conclusion. In these experiments we used the same
value for all cases. This brings us to the next open problem which is the determination
of the hyperparameters.
1 Remark that the results are presented on a logarithmic scale for the amplitudes to show in more detail
the low amplitude pulses. We used log(1 + a) scale in place of y scale which has the advantage of being
equal to zero for a=0.
Hyperparameter estimation
The Bayesian approach can be exactly applied when the direct (prior) probability laws
p(y|x,φ1) and p(x|φ2) are assigned. Even, when we have chosen appropriate laws, still
we have to determine their parameters φ = [φ1,φ2]. This problem has been addressed
by many authors and the subject is an active area in statistics. See ,
 and also .
The Bayesian approach gives natural tools to handle this problem by considering
φ = (φ1,φ2) as extra unknown parameters to infer on. We may then assign a prior law
p(φ) to them too. However, the way to do this is also still an open problem. We do not
discuss it more in this paper. The readers are invited to see for some extended
discussions and references. When this step is done, we can again use the Bayesian
approach and compute the joint posterior p(x,φ|y) from which we can follow three
main directions:
– Joint MAP optimization: In this approach one tries to estimate both the hyperparameters and the unknown variables x directly from the data by deﬁning:
(bx, bφ) = argmax
{p(x,φ|y)} where p(x,φ|y) ∝p(y|x,φ)p(x|φ)p(φ)
and where p(φ) is an appropriate prior law for φ. Many authors used the non informative
prior law for them.
– Marginalization: The main idea in this approach is to distinguish between the two sets
of unknowns: a high dimensional vector x representing in general a physical quantity
and a low dimensional vector φ representing the parameters of its prior probability laws.
This argument leads to estimate ﬁrst the hyperparameters by marginalizing over the
unknown variables x:
p(φ|y) ∝p(φ)
p(y|x,φ)p(x|φ) dx
and then, using them in the estimation of the unknown variables x:
bφ = argmax
{p(φ|y)} −→bx = argmax
p(x|y, bφ)
Note also that when p(φ) is choosed to be uniform, then p(φ|y) ∝p(y|φ) which is the
likelihood of the hyperparameters φ and the corresponding maximum likelihood (ML)
estimate has all the good asymptotic properties which may not be the case for the joint
MAP estimation. However, for practical applications with ﬁnite data we may not care
too much about the asymptotic properties of these estimates.
– Nuisance parameters: In this approach the hyperparameters are considered as the
nuisance parameters, so integrated out of p(x,φ|y) to obtain p(x|y) and x is estimated
bx = argmax
{p(x|y)} where p(x|y) =
p(y,x,φ) dφ
– Joint Posterior Mean: Here, x and φ are estimated as the posterior means:
bx = E{x|y} =
xp(x|y) dx and bφ = E{φ|y} =
φp(φ|y) dφ.
The main issue here is that, excepted the ﬁrst approach, all the others need integrations
for which, in general, there is not analytical expressions and their numerical computation
cost may be very high. At the other hand, unfortunately, the estimation by the joint
maximization has not the good asymptotic properties (when number of data goes to
inﬁnity) of the estimators obtained through the marginalization or expectation. However,
in ﬁnite number of data, a comparison of their relative properties is still to be done. To
see some more discussions and different possible implementations of these approaches
see . We have also to mention that, we can always use the Markov Chain Monte
Carlo (MCMC) techniques to generate samples from the joint posterior p(x,φ|y) and
then compute the joint posterior means and corresponding variances. It seems that these
techniques are growing up. However, I see two main limitations for their application on
real data: their huge computational cost and the need for some discussions on the tools
to control their convergences.
Myopic or blind inversion problems
Consider the deconvolution problems (1) or (2) and assume now that the psf h(t) or
h(x,y) are partially known. For example, we know they have Gaussian shape, but the
amplitude a and the width σ of the Gaussian are unknown. Noting by θ = (a,σ) the
problem then becomes the estimation of both x and θ from y = Hθx + ǫ. The case
where we know only the support of the psf but not its shape can also be casted in the
same way with θ = [h(0),... ,h(p)]
Before going more in details, we must note that, in general, the blind inversion problems are much harder than the simple inversion. Taking the deconvolution problem, we
have seen in introduction that, the problem even when the psf is given is ill-posed. The
blind deconvolution then is still more ill-posed, because here there are more fundamental
under-determinations. For example, it is easy to see that, we can ﬁnd an inﬁnite number of pairs (h,x) which result to the same convolution product h∗x. This means that,
to ﬁnd satisfactory methods and algorithms for these problems need much more prior
knowledge both on x and on h, and in general, the inputs must have more structures (be
rich in information content) to be able to obtain satisfactory results.
Conceptually however, the problem is identical to the estimation of hyperparameters
in previous section and any of the four approaches presented there can be used. One
may wish however to distinguish between these parameters of the system θ = (a,σ) and
those hyperparameters of the prior law model descriptions φ = (σ2
x,...). In that case,
one can try to write down p(x,θ,φ|y) and use one of the following:
– Joint MAP estimation of x, θ and φ: (bx,bθ, bφ) = argmax
{p(x,θ,φ|y)}.
– Marginalize over x and estimate θ and φ using: (bθ, bφ) = argmax
{p(θ,φ|y)} and
then, estimate x using: bx = argmaxx
p(x|y,bθ, bφ)
– Marginalize over x and θ and estimate φ using:
bφ = argmax
{p(φ|y)},,
then estimate θ using: bθ = argmaxθ
p(θ|y, bφ)
and ﬁnally, estimate x using:
bx = argmaxx
p(x|y,bθ, bφ)
– Joint Posterior Mean: Here, x, θ and φ are estimated through their respective posterior
means: bx = E{x|y}, bθ = E{θ|y} and bφ = E{φ|y}.
Here again, the joint optimization stays the simpler but we must be careful on interpretation of the results. For others, one can either use the Expectation-Maximization
(EM) algorithms and/or MCMC sampling tools to approximately compute the necessary
integration or expectation computations and overcome the computational cost issues.
CONCLUSIONS
In this paper I presented a synthetic overview of methods for inversion problems starting
by deterministic data matching and regularization methods followed by a general presentation of the probabilistic methods such as error probability law matching and likelihood based and the information theory and maximum entropy based methods. Then,
I focused on the Bayesian inference. I show that, as it concerns the maximum a posteriori estimation method, one can see easily the link with regularization methods. We
discussed however the superiority of the Bayesian framework which gives naturally the
necessary tools for inferring the uncertainty of the computed solution, for the estimation
of the hyperparameters or for handling myopic and blind inversion problems. We saw
also that probabilistic modeling of signal and images is more ﬂexible for introduction of
practical prior knowledge about them. Finally, we illustrated some of these discussions
through a deconvolution example in mass spectrometry data processing.