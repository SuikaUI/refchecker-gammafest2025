UNETR: Transformers for 3D Medical Image Segmentation
Ali Hatamizadeh
Yucheng Tang
Vanderbilt University
Vishwesh Nath
Andriy Myronenko
Bennett Landman
Vanderbilt University
Holger R. Roth
Daguang Xu
Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the
majority of medical image segmentation applications since
the past decade. In FCNNs, the encoder plays an integral
role by learning both global and local features and contextual
representations which can be utilized for semantic output
prediction by the decoder. Despite their success, the locality
of convolutional layers in FCNNs, limits the capability of
learning long-range spatial dependencies. Inspired by the
recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate
the task of volumetric (3D) medical image segmentation as
a sequence-to-sequence prediction problem. We introduce a
novel architecture, dubbed as UNEt TRansformers (UNETR),
that utilizes a transformer as the encoder to learn sequence
representations of the input volume and effectively capture
the global multi-scale information, while also following the
successful ‚ÄúU-shaped‚Äù network design for the encoder and
decoder. The transformer encoder is directly connected to
a decoder via skip connections at different resolutions to
compute the Ô¨Ånal semantic segmentation output. We have
validated the performance of our method on the Multi Atlas
Labeling Beyond The Cranial Vault (BTCV) dataset for multiorgan segmentation and the Medical Segmentation Decathlon
(MSD) dataset for brain tumor and spleen segmentation tasks.
Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.
Code: 
1. Introduction
Image segmentation plays an integral role in quantitative
medical image analysis as it is often the Ô¨Årst step for analysis
of anatomical structures . Since the advent of deep learning, FCNNs and in particular ‚ÄúU-shaped‚Äú encoder-decoder ar-
Transformer Encoder
Linear Projection of Flattened Patches
ùêª√ó ùëä√ó ùê∑√ó ùê∂
Segmentation
3D Patches
Figure 1. Overview of UNETR. Our proposed model consists
of a transformer encoder that directly utilizes 3D patches and is
connected to a CNN-based decoder via skip connection.
chitectures have achieved state-of-the-art results
in various medical semantic segmentation tasks . In
a typical U-Net architecture, the encoder is responsible
for learning global contextual representations by gradually
downsampling the extracted features, while the decoder upsamples the extracted representations to the input resolution
for pixel/voxel-wise semantic prediction. In addition, skip
connections merge the output of the encoder with decoder
at different resolutions, hence allowing for recovering spatial
information that is lost during downsampling.
Although such FCNN-based approaches have powerful
representation learning capabilities, their performance in
learning long-range dependencies is limited to their localized
receptive Ô¨Åelds .
As a result, such a deÔ¨Åciency
in capturing multi-scale information leads to sub-optimal
segmentation of structures with variable shapes and scales
(e.g. brain lesions with different sizes). Several efforts have
used atrous convolutional layers to enlarge the
 
receptive Ô¨Åelds. However, locality of the receptive Ô¨Åelds in
convolutional layers still limits their learning capabilities to
relatively small regions. Combining self-attention modules
with convolutional layers has been proposed to
improve the non-local modeling capability.
In Natural Language Processing (NLP), transformer-based
models achieve state-of-the-art benchmarks in
various tasks. The self-attention mechanism of transformers
allows to dynamically highlight the important features of
word sequences. Additionally, in computer vision, using
transformers as a backbone encoder is beneÔ¨Åcial due to their
great capability of modeling long-range dependencies and
capturing global context . SpeciÔ¨Åcally, unlike the local
formulation of convolutions, transformers encode images as
a sequence of 1D patch embeddings and utilize self-attention
modules to learn a weighted sum of values that are calculated
from hidden layers. As a result, this Ô¨Çexible formulation
allows to effectively learn the long-range information.
Furthermore, Vision Transformer (ViT) and its variants
have shown excellent capabilities in learning pre-text tasks
that can be transferred to down-stream applications .
In this work, we propose to leverage the power of
transformers for volumetric medical image segmentation and
introduce a novel architecture dubbed as UNEt TRansformers
(UNETR). In particular, we reformulate the task of 3D segmentation as a 1D sequence-to-sequence prediction problem
and use a transformer as the encoder to learn contextual
information from the embedded input patches. The extracted
representations from the transformer encoder are merged
with the CNN-based decoder via skip connections at multiple
resolutions to predict the segmentation outputs. Instead of
using transformers in the decoder, our proposed framework
uses a CNN-based decoder. This is due to the fact that transformers are unable to properly capture localized information,
despite their great capability of learning global information.
We validate the effectiveness of our method on 3D CT
and MRI segmentation tasks using Beyond the Cranial
Vault (BTCV) and Medical Segmentation Decathlon
(MSD) datasets. In BTCV dataset, UNETR achieves
new state-of-the-art performance on both Standard and
Free Competition sections on its leaderboard.
outperforms the state-of-the-art methodologies on both brain
tumor and spleen segmentation tasks in MSD dataset.
our main contributions of this work are as follows::
‚Ä¢ We propose a novel transformer-based model for
volumetric medical image segmentation.
‚Ä¢ To this end, we propose a novel architecture in which (1)
a transformer encoder directly utilizes the embedded 3D
volumes to effectively capture long-range dependencies;
(2) a skip-connected decoder combines the extracted
representations at different resolutions and predicts the
segmentation output.
‚Ä¢ We validate the effectiveness of our proposed model for
different volumetric segmentation tasks on two public
datasets: BTCV and MSD . UNETR achieves
new state-of-the-art performance on leaderboard of
BTCV dataset and outperforms competing approaches
on the MSD dataset.
2. Related Work
CNN-based Segmentation Networks
: Since the introduction of the seminal U-Net , CNN-based networks
have achieved state-of-the-art results on various 2D and 3D
various medical image segmentation tasks .
For volume-wise segmentation, tri-planar architectures
are sometimes used to combine three-view slices for each
voxel, also known for 2.5D methods
 .
contrast, 3D approaches directly utilize the full volumetric
image represented by a sequence of 2D slices or modalities.
The intuition of employing varying sizes was followed
by multi-scan, multi-path models to capture
downsampled features of the image. In addition, to exploit
3D context and to cope with limitation of computational
resource, researchers investigated hierarchical frameworks.
Some efforts proposed to extract features at multiple scales
or assembled frameworks . Roth et al. proposed
a multi-scale framework to obtain varying resolution
information in pancreas segmentation.
These methods
provide pioneer studies of 3D medical image segmentation
at multiple levels, which reduces problems in spatial context
and low-resolution condition.
Despite their success, a
limitation of these networks is their poor performance in
learning global context and long-range spatial dependencies,
which can severely impact the segmentation performance for
challenging tasks.
Vision Transformers
: Vision transformers have recently
gained traction for computer vision tasks.
Dosovitskiy
et al. demonstrated state-of-the-art performance on
image classiÔ¨Åcation datasets by large-scale pre-training and
Ô¨Åne-tuning of a pure transformer. In object detection, endto-end transformer-based models have shown prominence
on several benchmarks . Recently, hierarchical vision
transformers with varying resolutions and spatial embeddings have been proposed. These methodologies gradually decrease the resolution of features in the
transformer layers and utilize sub-sampled attention modules.
Unlike these approaches, the size of representation in UNETR
encoder remains Ô¨Åxed in all transformer layers. However, as
described in Sec. 3, deconvolutional and convolutional operations are used to change the resolution of extracted features.
Recently, multiple methods were proposed that explore the
possibility of using transformer-based models for the task of
2D image segmentation . Zheng et al. introduced the SETR model in which a pre-trained transformer
encoder with different variations of CNN-based decoders
were proposed for the task of semantic segmentation. Chen et
al. proposed a methodology for multi-organ segmentation
by employing a transformer as an additional layer in the bottleneck of a U-Net architecture. Zhang et al. proposed to use
CNNs and transformers in separate streams and fuse their outputs. Valanarasu et al. proposed a transformer-based axial attention mechanism for 2D medical image segmentation.
There are key differences between our model and these efforts:
(1) UNETR is tailored for 3D segmentation and directly utilizes volumetric data; (2) UNETR employs the transformer as
the main encoder of a segmentation network and directly connects it to the decoder via skip connections, as opposed to using it as an attention layer within the segmentation network (3)
UNETR does not rely on a backbone CNN for generating the
input sequences and directly utilizes the tokenized patches.
For 3D medical image segmentation, Xie et al. 
proposed a framework that utilizes a backbone CNN for
feature extraction, a transformer to process the encoded
representation and a CNN decoder for predicting the
segmentation outputs. Similarly, Wang et al. proposed to
use a transformer in the bottleneck of a 3D encoder-decoder
CNN for the task of semantic brain tumor segmentation. In
contrast to these approaches, our method directly connects
the encoded representation from the transformer to decoder
by using skip connections.
3. Methodology
3.1. Architecture
We have presented an overview of the proposed model
in Fig. 2.
UNETR utilizes a contracting-expanding pattern consisting of a stack of transformers as the encoder
which is connected to a decoder via skip connections. As
commonly used in NLP, the transformers operate on 1D
sequence of input embeddings. Similarly, we create a 1D
sequence of a 3D input volume x ‚ààRH√óW √óD√óC with
resolution (H,W,D) and C input channels by dividing it into
Ô¨Çattened uniform non-overlapping patches xv ‚ààRN√ó(P 3.C)
where (P,P,P) denotes the resolution of each patch and
N =(H√óW √óD)/P 3 is the length of the sequence.
Subsequently, we use a linear layer to project the patches
into a K dimensional embedding space, which remains
constant throughout the transformer layers.
In order to
preserve the spatial information of the extracted patches, we
add a 1D learnable positional embedding Epos ‚ààRN√óK to
the projected patch embedding E‚ààR(P 3.C)√óK according to
v E]+Epos,
Note that the learnable [class] token is not added to
the sequence of embeddings since our transformer backbone
is designed for semantic segmentation. After the embedding
layer, we utilize a stack of transformer blocks comprising of multi-head self-attention (MSA) and multilayer
perceptron (MLP) sublayers according to
i =MSA(Norm(zi‚àí1))+zi‚àí1,
zi =MLP(Norm(z‚Ä≤
where Norm() denotes layer normalization , MLP comprises of two linear layers with GELU activation functions,
i is the intermediate block identiÔ¨Åer, and L is the number of
transformer layers.
A MSA sublayer comprises of n parallel self-attention
(SA) heads. SpeciÔ¨Åcally, the SA block, is a parameterized
function that learns the mapping between a query (q) and
the corresponding key (k) and value (v) representations
in a sequence z ‚ààRN√óK. The attention weights (A) are
computed by measuring the similarity between two elements
in z and their key-value pairs according to
A=Softmax( qk‚ä§
where Kh = K/n is a scaling factor for maintaining the
number of parameters to a constant value with different
values of the key k. Using the computed attention weights,
the output of SA for values v in the sequence z is computed as
Here, v denotes the values in the input sequence and
Kh = K/n is a scaling factor. Furthermore, the output of
MSA is deÔ¨Åned as
MSA(z)=[SA1(z);SA2(z);...;SAn(z)]Wmsa,
where Wmsa ‚ààRn.Kh√óK represents the multi-headed
trainable parameter weights.
Inspired by architectures that are similar to U-Net ,
where features from multiple resolutions of the encoder are
merged with the decoder, we extract a sequence representation zi (i ‚àà{3,6,9,12}), with size H√óW √óD
√óK, from the
transformer and reshape them into a H
P √óK tensor.
A representation in our deÔ¨Ånition is in the embedding space
after it has been reshaped as an output of the transformer
with feature size of K (i.e. transformer‚Äôs embedding size).
Furthermore, as shown in Fig. 2, at each resolution we project
the reshaped tensors from the embedding space into the input
space by utilizing consecutive 3√ó3√ó3 convolutional layers
that are followed by normalization layers.
At the bottleneck of our encoder (i.e. output of transformer‚Äôs last layer), we apply a deconvolutional layer to the
transformed feature map to increase its resolution by a factor
of 2. We then concatenate the resized feature map with the
Multi-Head
Projection
Conv 3 √ó 3 √ó 3, BN, ReLU
Deconv 2 √ó 2 √ó 2, Conv 3 √ó 3 √ó 3, BN, ReLU
Deconv 2 √ó 2 √ó 2
Conv 1 √ó 1 √ó 1
ùêª√ó ùëä√ó ùê∑√ó 4
ùêª√ó ùëä√ó ùê∑√ó 64
ùêª√ó ùëä√ó ùê∑√ó 64
ùêª√ó ùëä√ó ùê∑√ó 3
Figure 2. Overview of UNETR architecture. A 3D input volume (e.g. C =4 channels for MRI images), is divided into a sequence of uniform
non-overlapping patches and projected into an embedding space using a linear layer. The sequence is added with a position embedding and
used as an input to a transformer model. The encoded representations of different layers in the transformer are extracted and merged with a
decoder via skip connections to predict the Ô¨Ånal segmentation. Output sizes are given for patch resolution P =16 and embedding size K =768.
feature map of the previous transformer output (e.g. z9), and
feed them into consecutive 3 √ó 3 √ó 3 convolutional layers
and upsample the output using a deconvolutional layer. This
process is repeated for all the other subsequent layers up
to the original input resolution where the Ô¨Ånal output is fed
into a 1√ó1√ó1 convolutional layer with a softmax activation
function to generate voxel-wise semantic predictions.
3.2. Loss Function
Our loss function is a combination of soft dice loss 
and cross-entropy loss, and it can be computed in a voxel-wise
manner according to
L(G,Y )=1‚àí2
i=1Gi,jYi,j
Gi,jlogYi,j.
where I is the number of voxels; J is the number of classes;
Yi,j and Gi,j denote the probability output and one-hot
encoded ground truth for class j at voxel i, respectively.
4. Experiments
4.1. Datasets
To validate the effectiveness of our method, we utilize
BTCV and MSD datasets for three different
segmentation tasks in CT and MRI imaging modalities.
BTCV (CT): The BTCV dataset consists of 30 subjects with abdominal CT scans where 13 organs were annotated by interpreters under supervision of clinical radiologists
at Vanderbilt University Medical Center. Each CT scan was
acquired with contrast enhancement in portal venous phase
and consists of 80 to 225 slices with 512√ó512 pixels and slice
thickness ranging from 1 to 6 mm. Each volume has been
pre-processed independently by normalizing the intensities
in the range of [-1000,1000] HU to . All images are
resampled into the isotropic voxel spacing of 1.0 mm during
pre-processing. The multi-organ segmentation problem is formulated as a 13 class segmentation task with 1-channel input.
MSD (MRI/CT): For the brain tumor segmentation task,
the entire training set of 484 multi-modal multi-site MRI
data (FLAIR, T1w, T1gd, T2w) with ground truth labels
of gliomas segmentation necrotic/active tumor and oedema
is utilized for model training. The voxel spacing of MRI
images in this tasks is 1.0 √ó 1.0 √ó 1.0 mm3. The voxel
intensities are pre-processed with z-score normalization. The
SETR NUP 
SETR PUP 
SETR MLA 
nnUNet 
TransUNet 
CoTr w/o CNN encoder 
CoTr* 
RandomPatch 
nnUNet-v2 
nnUNet-dys3 
Table 1. Quantitative comparisons of segmentation performance in BTCV test set. Top and bottom sections represent the benchmarks of
Standard and Free Competitions respectively. Our method is compared against current state-of-the-art models. All SETR baselines use ViT-
B-16 backbone. Note: Spl: spleen, RKid: right kidney, LKid: left kidney, Gall: gallbladder, Eso: esophagus, Liv: liver, Sto: stomach, Aor:
aorta IVC: inferior vena cava, Veins: portal and splenic veins, Pan: pancreas, AG: adrenal gland. All results obtained from BTCV leaderboard.
Task/Modality
Spleen Segmentation (CT)
Brain tumor Segmentation (MRI)
AttUNet 
SETR NUP 
SETR PUP 
SETR MLA 
TransUNet 
TransBTS 
CoTr w/o CNN encoder 
Table 2. Quantitative comparisons of the segmentation performance in brain tumor and spleen segmentation tasks of the MSD dataset. WT,
ET and TC denote Whole Tumor, Enhancing tumor and Tumor Core sub-regions respectively.
problem of brain tumor segmentation is formulated as a 3
class segmentation task with 4-channel input.
For the spleen segmentation task, 41 CT volumes with
spleen body annotation are used. The resolution/spacing of
volumes in task 9 ranges from 0.613√ó0.613√ó1.50 mm3 to
0.977√ó0.977√ó8.0 mm3. All volumes are re-sampled into
the isotropic voxel spacing of 1.0 mm during pre-processing.
The voxel intensities of the images are normalized to the
range according to 5th and 95th percentile of overall foreground intensities. Spleen segmentation is formulated as a binary segmentation task with 1-channel input. For multi-organ
and spleen segmentation tasks, we randomly sample the input
images with volume sizes of . For brain segmentation task, we randomly sample the input images with volume
sizes of . For all experiments, the random
patches of foreground/background are sampled at ratio 1:1.
4.2. Evaluation Metrics
We use Dice score and 95% Hausdorff Distance (HD) to
evaluate the accuracy of segmentation in our experiments.
For a given semantic class, let Gi and Pi denote the ground
truth and prediction values for voxel i and G‚Ä≤ and P ‚Ä≤ denote
ground truth and prediction surface point sets respectively.
The Dice score and HD metrics are deÔ¨Åned as
Dice(G,P)=
HD(G‚Ä≤,P ‚Ä≤)=max{max
p‚Ä≤‚ààP ‚Ä≤‚à•g‚Ä≤‚àíp‚Ä≤‚à•,
p‚Ä≤‚ààP ‚Ä≤ min
g‚Ä≤‚ààG‚Ä≤‚à•p‚Ä≤‚àíg‚Ä≤‚à•}.
The 95% HD uses the 95th percentile of the distances
between ground truth and prediction surface point sets. As
a result, the impact of a very small subset of outliers is
minimized when calculating HD.
4.3. Implementation Details
We implement UNETR in PyTorch1 and MONAI2. The
modelwastrainedusingaNVIDIADGX-1server. Allmodels
were trained with the batch size of 6, using the AdamW optimizer with initial learning rate of 0.0001 for 20,000 iterations. For the speciÔ¨Åed batch size, the average training time
1 
2 
Right kidney
Left Kidney
Gallbladder
Adrenal glands
Figure 3. Qualitative comparison of different baselines in BTCV cross-validation. The Ô¨Årst row shows a complete representative CT slice.
We exhibit four zoomed-in subjects (row 2 to 5), where our method shows visual improvement on segmentation of kidney and spleen (row 2),
pancreas and adrenal gland (row 3), gallbladder (row 4) and portal vein (row 5). The subject-wise average Dice score is shown on each sample.
was 10 hours for 20,000 iterations. Our transformer-based
encoder follows the ViT-B16 architecture with L = 12
layers, an embedding size of K =768. We used a patch resolution of 16√ó16√ó16. For inference, we used a sliding window
approach with an overlap portion of 0.5 between the neighboring patches and the same resolution as speciÔ¨Åed in Sec. 4.1.
We did not use any pre-trained weights for our transformer
backbone (e.g. ViT on ImageNet) since it did not demonstrate
any performance improvements. For BTCV dataset, we have
evaluated our model and other baselines in the Standard and
Free Competitions of its leaderboard. Additional data from
the same cohort was used for the Free Competition increasing
the number of training cases to 80 volumes. For all experiments, we employed Ô¨Åve-fold cross validation with a ratio of
95:5. In addition, we used data augmentation strategies such
as random rotation of 90, 180 and 270 degrees, random Ô¨Çip in
axial, sagittal and coronal views and random scale and shift
intensity. We used ensembling to fuse the outputs of models
from four different Ô¨Åve-fold cross-validations. For brain and
spleen segmentation tasks in MSD dataset, we split the data
into training, validation and test with a ratio of 80:15:5.
4.4. Quantitative Evaluations
outperforms
state-of-the-art
for both Standard and Free Competitions on the BTCV
leaderboard. As shown in Table 1, in the Free Competition,
UNETR achieves an overall average Dice score of 0.899
and outperforms the second, third and fourth top-ranked
methodologies by 1.238%, 1.696% and 5.269% respectively.
In the Standard Competition, we compared the performance of UNETR against CNN and transformer-based
baselines. UNETR achieves a new state-of-the-art performance with an average Dice score of 85.3% on all organs.
SpeciÔ¨Åcally, on large organs, such as spleen, liver and
stomach, our method outperforms the second best baselines
by 1.043%, 0.830% and 2.125% respectively,in terms of
Dice score. Furthermore, in segmentation of small organs,
our method signiÔ¨Åcantly outperforms the second best
Ground Truth
Figure4.UNETReffectivelycapturestheÔ¨Åne-graineddetailsinsegmentationoutputs. TheWholeTumor(WT)encompassesaunionofred, blue
and green regions. The Tumor Core (TC) includes the union of red and blue regions. The Enhancing Tumor core (ET) denotes the green regions.
baselines by 6.382% and 6.772% on gallbladder and adrenal
glands respectively, in terms of Dice score.
In Table 2, we compare the performance of UNETR
against CNN and transformer-based methodologies for brain
tumor and spleen segmentation tasks on MSD dataset. For
brain segmentation, UNETR outperforms the closest baseline
by 1.5% on average over all semantic classes. In particular,
UNETR performs considerably better in segmenting tumor
core (TC) subregion. Similarly for spleen segmentation,
UNETR outperforms the best competing methodology by
least 1.0% in terms of Dice score.
4.5. Qualitative Results
Qualitative multi-organ segmentation comparisons are presented in Fig. 3. UNETR shows improved segmentation performance for abdomen organs. Our model‚Äôs capability of
learning long-range dependencies is evident in row 3 (from
the top), in which nnUNet confuses liver with stomach tissues,
while UNETR successfully delineates the boundaries of these
organs. In Fig. 3, rows 2 and 4 demonstrate a clear detection of
kidney and adrenal glands against surrounding tissues, which
indicate that UNETR captures better spatial context. In comparison to 2D transformer-based models, UNETR exhibits
higher boundary segmentation accuracy as it accurately identi-
Ô¨Åes the boundaries between kidney and spleen. This is evident
for gallbladder in row 2, liver and stomach in row 3, and portal
vein against liver in row 5. In Fig. 4, we present qualitative
segmentation comparisons for brain tumor segmentation on
the MSD dataset. SpeciÔ¨Åcally, our model demonstrates better
performance in capturing the Ô¨Åne-grained details of tumors.
5. Discussion
Our experiments in all datasets demonstrate superior performance of UNETR over both CNN and transformer-based
segmentation models. SpeciÔ¨Åcally, UNETR achieves better
segmentation accuracy by capturing both global and local
dependencies. In qualitative comparisons, this is illustrated
in various cases in which UNETR effectively captures
long-range dependencies (e.g. accurate segmentation of the
pancreas tail in Fig. 3).
Moreover, the segmentation performance of UNETR on
the BTCV leaderboard demonstrates new state-of-the-art
benchmarks and validates its effectiveness.
SpeciÔ¨Åcally
for small anatomies, UNETR outperforms both CNN and
transformer-based models. Although 3D models already
demonstrate high segmentation accuracy for small organs
such as gallbladder, adrenal glands, UNETR can still
outperform the best competing model by a signiÔ¨Åcant margin
(See Table 1). This is also observed in Fig. 3, in which
Table 3. Effect of the decoder architecture on segmentation
performance. NUP, PUP and MLA denote Naive UpSampling,
Progressive UpSampling and Multi-scale Aggregation.
UNETR has a signiÔ¨Åcantly better segmentation accuracy for
left and right adrenal glands, and UNETR is the only model
to correctly detect branches of the adrenal glands. For more
challenging tissues, such as gallbladder in row 4 and portal
vein in row 5, which have low contrast with the surrounding
liver tissue, UNETR is still capable of segmenting clear
connected boundaries.
6. Ablation
Decoder Choice
In Table 3, we evaluate the effectiveness
of our decoder by comparing the performance of UNETR
with other decoder architectures on two representative
segmentation tasks from MRI and CT modalities. In these experiments, we employ the encoder of UNETR but replaced the
decoder with 3D counterparts of Naive UPsampling (NUP),
Progressive UPsampling (PUP) and MuLti-scale Aggregation
(MLA) . We observe that these decoder architectures
yield sub-optimal performance, despite MLA marginally
outperforming both NUP and PUP. For brain tumor segmentation, UNETR outperforms its variants with MLA, PUP
and NUP decoders by 2.7%, 4.3% and 7.5% on average
Dice score. Similarly, for spleen segmentation, UNETR
outerforms MLA, PUP and NUP by 1.4%, 2.3% and 3.2%.
Patch Resolution
A lower input patch resolution leads
to a higher sequence length, and therefore higher memory
consumption, since it is inversely correlated to the cube of the
resolution. As shown in Table 4, our experiments demonstrate
that decreasing the resolution leads to consistently improved
performance. SpeciÔ¨Åcally, decreasing the patch resolution
from 32 to 16 improves the performance by 1.1% and
0.8% in terms of average Dice score in spleen and brain
segmentation tasks respectively. We did not experiment with
lower resolutions due to memory constraints.
Model and Computational Complexity
In Table 5,
we present number of FLOPs, parameters and averaged
inference time of the models in BTCV benchmarks. Number
of FLOPs and inference time are calculated based on an input
size of 96 √ó 96 √ó 96 and using a sliding window approach.
According to our benchmarks, UNETR is a moderate-sized
Resolution
Table 4. Effect of patch resolution on segmentation performance.
#Params (M)
Inference Time (s)
nnUNet 
TransUNet 
Table 5. Comparison of number of parameters, FLOPs and averaged
inference time for various models in BTCV experiments.
model with 92.58M parameters and 41.19G FLOPs. For comparison, other transformer-based methods such as CoTr ,
TransUNet and SETR have 46.51M, 96.07M and
86.03M parameters and 399.21G, 48.24G and 43.49G FLOPs,
respectively. UNETR shows comparable model complexity
while outperforming these models by a large margin in
BTCV benchmarks. CNN-based segmentation models of
nnUNet and ASPP have 19.07M and 47.92M
parameters and 412.65G and 44.87G FLOPs, respectively.
Similarly, UNETR outperforms these CNN-based models
while having a moderate model complexity. In addition,
UNETR has the second lowest averaged inference time after
nnUNet and is signiÔ¨Åcantly faster than transformer-based
models such as SETR , TransUNet and CoTr .
7. Conclusion
This paper introduces a novel transformer-based architecture, dubbed as UNETR, for semantic segmentation of
volumetric medical images by reformulating this task as a 1D
sequence-to-sequence prediction problem. We proposed to
use a transformer encoder to increase the model‚Äôs capability
for learning long-range dependencies and effectively
capturing global contextual representation at multiple scales.
We validated the effectiveness of UNETR on different
volumetric segmentation tasks in CT and MRI modalities.
UNETR achieves new state-of-the-art performance in both
Standard and Free Competitions on the BTCV leaderboard
for the multi-organ segmentation and outperforms competing
approaches for brain tumor and spleen segmentation on the
MSD dataset. In conclusion, UNETR has shown the potential
to effectively learn the critical anatomical relationships
represented in medical images. The proposed method could
be the foundation for a new class of transformer-based
segmentation models in medical images analysis.