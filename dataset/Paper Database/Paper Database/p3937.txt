Received 24 Jun 2016 | Accepted 9 Nov 2016 | Published 9 Jan 2017
Quantum-chemical insights from deep
tensor neural networks
Kristof T. Schu¨tt1, Farhad Arbabzadah1, Stefan Chmiela1, Klaus R. Mu¨ller1,2 & Alexandre Tkatchenko3,4
Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text
and image search, speech recognition, as well as bioinformatics. Can machine learning enable
similar breakthroughs in understanding quantum many-body systems? Here we develop an
efﬁcient deep learning approach that enables spatially and chemically resolved insights into
quantum-mechanical observables of molecular systems. We unify concepts from many-body
Hamiltonians with purpose-designed deep tensor neural networks, which leads to sizeextensive and uniformly accurate (1 kcal mol  1) predictions in compositional and conﬁgurational chemical space for molecules of intermediate size. As an example of chemical
relevance, the model reveals a classiﬁcation of aromatic rings with respect to their stability.
Further applications of our model for predicting atomic energies and local chemical potentials
in molecules, reliable isomer energies, and molecules with peculiar electronic structure
demonstrate the potential of machine learning for revealing insights into complex quantumchemical systems.
DOI: 10.1038/ncomms13890
1 Machine Learning Group, Technische Universita¨t Berlin, Marchstr. 23, 10587 Berlin, Germany. 2 Department of Brain and Cognitive Engineering, Korea
University, Anam-dong, Seongbuk-gu, Seoul 136-713, Republic of Korea. 3 Theory Department, Fritz-Haber-Institut der Max-Planck-Gesellschaft, Faradayweg
4-6, D-14195 Berlin, Germany. 4 Physics and Materials Science Research Unit, University of Luxembourg, Luxembourg,, L-1511 Luxembourg. Correspondence
and requests for materials should be addressed to K.R.M. (email: ) or to A.T. (email: ).
NATURE COMMUNICATIONS | 8:13890 | DOI: 10.1038/ncomms13890 | www.nature.com/naturecommunications
hemistry permeates all aspects of our life, from the
development of new drugs to the food that we consume
and materials we use on a daily basis. Chemists rely
on empirical observations based on creative and painstaking
experimentation that leads to eventual discoveries of molecules
and materials with desired properties and mechanisms to
synthesize them. Many discoveries in chemistry can be guided
by searching large databases of experimental or computational
molecular structures and properties by using concepts based
on chemical similarity. Because the structure and properties
of molecules are determined by the laws of quantum mechanics,
ultimately chemical discovery must be based on fundamental
quantum principles. Indeed, electronic structure calculations
and intelligent data analysis (machine learning) have recently
been combined aiming towards the goal of accelerated discovery
of chemicals with desired properties1–8. However, so far the
majority of these pioneering efforts have focused on the
construction of reduced models trained on large data sets of
density-functional theory calculations.
In this work, we develop an efﬁcient deep learning approach
chemically
quantum-mechanical
properties
Obviously, computational models are not predictive if they lack
accuracy. In addition to being interpretable, size-extensive
and efﬁcient, our deep tensor neural network (DTNN) approach
is uniformly accurate (1 kcal mol  1) throughout compositional
and conﬁgurational chemical space. On the more fundamental
side, the mathematical construction of the DTNN model provides
statistically rigorous partitioning of extensive molecular properties
contributions—a
long-standing
for quantum-mechanical calculations of molecules.
Molecular deep tensor neural networks. It is common to use a
carefully chosen representation of the problem at hand as a basis
for machine learning9–11. For example, molecules can be
represented as Coulomb matrices7,12,13, scattering transforms14,
bags of bonds15, smooth overlap of atomic positions16,17
or generalized symmetry functions18,19. Kernel-based learning
properties
transforms
representations
non-linearly by virtue of kernel functions. In contrast, deep
neural networks20 are able to infer the underlying regularities and
learn an efﬁcient representation in a layer-wise fashion21.
Molecular properties are governed by the laws of quantum
mechanics, which yield the remarkable ﬂexibility of chemical
systems, but also impose constraints on the behaviour of bonding
in molecules. The approach presented here utilizes the manybody Hamiltonian concept for the construction of the DTNN
architecture (Fig. 1), embracing the principles of quantum
chemistry, while maintaining the full ﬂexibility of a complex
data-driven learning machine.
DTNN receives molecular structures through a vector of
nuclear charges Z and a matrix of atomic distances D ensuring
rotational and translational invariance by construction (Fig. 1a).
The distances are expanded in a Gaussian basis, yielding a feature
vector ^dij 2 RG, which accounts for the different nature of
interactions at various distance regimes. Similar approaches have
been applied to the entries of the Coulomb matrix for the
prediction of molecular properties before12.
The total energy EM for the molecule M composed of N atoms
is written as a sum over N atomic energy contributions Ei, thus
satisfying permutational invariance with respect to atom indexing. Each atom i is represented by a coefﬁcient vector c 2 RB,
where B is the number of basis functions, or features. Motivated
by quantum-chemical atomic basis set expansions, we assign
an atom type-speciﬁc descriptor vector cZi to these coefﬁcients
i . Subsequently, this atomic expansion is repeatedly reﬁned by
pairwise interactions with the surrounding atoms
where the interaction term vij reﬂects the inﬂuence of atom j at a
distance Dij on atom i. Note that this reﬁnement step is seamlessly
integrated into the architecture of the molecular DTNN, and
throughout
Supplementary Discussion, we show the relation to convolutional
neural networks that have been applied to images, speech and text
with great success because of their ability to capture local
structure22–27. Considering a molecule as a graph, T reﬁnements
of the coefﬁcient vectors are comprised of all walks of length
T through the molecule ending at the corresponding atom28,29.
From the point of view of many-body interatomic interactions,
subsequent reﬁnement steps t correlate atomic neighbourhoods
with increasing complexity.
While the initial atomic representations only consider isolated
atoms, the interaction terms characterize how the basis functions
of two atoms overlap with each other at a certain distance. Each
reﬁnement step is supposed to reduce these overlaps, thereby
embedding the atoms of the molecule into their chemical
environment. Following this procedure, the DTNN implicitly
learns an atom-centered basis that is unique and efﬁcient with
respect to the property to be predicted.
Non-linear coupling between the atomic vector features and
the interatomic distances is achieved by a tensor layer30–32, such
that the coefﬁcient k of the reﬁnement is given by
vijk¼ tanh c tð Þ
j Vk^dij þ Wcc tð Þ
k þ Wd^dij
where bk is the bias of feature k and Wc and Wd are the weights of
atom representation and distance, respectively. The slice Vk of the
V 2 RBBG
multiplicatively. Since V incorporates many parameters, using
this kind of layer is both computationally expensive as well as
prone to overﬁtting. Therefore, we employ a low-rank tensor
factorization, as described in (ref. 33), such that
vij¼ tanh Wfc
Wcfcj þ bf1
Wdf^dij þ bf 2
where ‘’ represents element-wise multiplication, while Wcf, bf1,
Wdf, bf 2 and Wfc are the weight matrices and corresponding
biases of atom representations, distances and resulting factors,
respectively.
dimensionality
corresponds to the number of factors, choosing only a few
drastically decreases the number of parameters, thus solving both
issues of the tensor layer at once.
Arriving at the ﬁnal embedding after a given number of
interaction reﬁnements, two fully-connected layers predict an
energy contribution from each atomic coefﬁcient vector, such that
their sum corresponds to the total molecular energy EM.
Therefore, the DTNN architecture scales with the number of
atoms in a molecule, fully capturing the extensive nature of the
energy. All weights, biases, as well as the atom type-speciﬁc
descriptors were initialized randomly and trained using stochastic
gradient descent.
Learning molecular energies. To demonstrate the versatility of
the proposed DTNN, we train models with up to three interaction
passes T ¼ 3 for both compositional and conﬁgurational degrees
of freedom in molecular systems. The DTNN accuracy saturates
at T ¼ 3, and leads to a strong correlation between atoms in
NATURE COMMUNICATIONS | DOI: 10.1038/ncomms13890
NATURE COMMUNICATIONS | 8:13890 | DOI: 10.1038/ncomms13890 | www.nature.com/naturecommunications
molecules, as can be visualized by the complexity of the potential
learned by the network (Fig. 1e). For training, we employ
chemically diverse data sets of equilibrium molecular structures,
as well as molecular dynamics (MD) trajectories for small
molecules. We employ two subsets of the GDB-13 database34,35
referred to as GDB-7, including 47,000 molecules with up to
seven heavy (C, N, O, F) atoms, and GDB-9, consisting of 133,885
molecules with up to nine heavy atoms36. In both cases, the
learning task is to predict the molecular total energy calculated
with density-functional theory (DFT). All GDB molecules are
stable and synthetically accessible according to organic chemistry
functional
signatures include single, double and triple bonds; (hetero-)
cycles, carboxy, cyanide, amide, amine, alcohol, epoxy, sulphide,
ether, ester, chloride, aliphatic and aromatic groups. For each of
the many possible stoichiometries, many constitutional isomers
are considered, each being represented only by a low-energy
conformational isomer.
As Supplementary Table 1 demonstrates, DTNN achieves a
mean absolute error of 1.0 kcal mol  1 on both GDB data sets,
training on 5.8 k GDB-7 (80%) and 25 k (20%) GDB-9 reference
calculations, respectively. Figure 1c shows the performance
on GDB-9 depending on the size of the molecule. We observe
that larger molecules have lower errors because of their
Mean abs. error (kcal mol−1)
Mean abs. error
Total energy (kcal mol−1)
Feedback loop
Interaction module
Gaussian expansion
Hyperbolic tangent
Element-wise product
Element-wise sum
Molecules with ≥ 20 atoms
# add. calcs. ≤ 15 atoms
W dfdˆij + bf2
W cfcj + bf1
Figure 1 | Prediction and explanation of molecular energies with a deep tensor neural network. (a) Molecules are encoded as input for the neural
network by a vector of nuclear charges and an inter-atomic distance matrix. This description is complete and invariant to rotation and translation.
(b) Illustration of the network architecture. Each atom type corresponds to a vector of coefﬁcients c 0
, which is repeatedly reﬁned by interactions vij.
The interactions depend on the current representation c tð Þ
j , as well as the distance Dij to an atom j. After T iterations, an energy contribution Ei is predicted
for the ﬁnal coefﬁcient vector c T
i . The molecular energy E is the sum over these atomic contributions. (c) Mean absolute errors of predictions for the
GDB-9 dataset of 133,885 molecules as a function of the number of atoms. The employed neural network uses two interaction passes (T ¼ 2) and 50,000
reference calculation during training. The inset shows the error of an equivalent network trained on 5,000 GDB-9 molecules with 20 or more atoms, as
small molecules with 15 or less atoms are added to the training set. (d) Extract from the calculated (black) and predicted (orange) molecular dynamics
trajectory of toluene. The curve on the right shows the agreement of the predicted and calculated energy distributions. (e) Energy contribution Eprobe
(or local chemical potential OM
H rð Þ, see text) of a hydrogen test charge on a P
k  2 isosurface for various molecules from the GDB-9 dataset for a
DTNN model with T ¼ 2.
NATURE COMMUNICATIONS | DOI: 10.1038/ncomms13890
NATURE COMMUNICATIONS | 8:13890 | DOI: 10.1038/ncomms13890 | www.nature.com/naturecommunications
abundance in the training data. However, when predicting larger
molecules than present in the training set, the errors increase.
This is because the molecules in the GDB-9 set are quite small,
so we considered all atoms to be in each other’s chemical
environment. Imposing a distance cutoff to interatomic interactions of 3 Å leads to a 0.1 kcal mol  1 increase in the error.
However, this distance cutoff restricts only the direct interactions
considered in the reﬁnement steps. With multiple reﬁnements,
the effective cutoff increases by a factor of T because of indirect
interactions over multiple atoms. Given large enough molecules,
so that a reasonable distance cutoff can be chosen, scaling
to larger molecules will require only to have well-represented
local environments. For now, we observe that at least a few larger
molecules are needed to achieve a good prediction accuracy.
Following this train of thought, we trained the network on a
restricted subset of 5 k molecules with 420 atoms. By adding
smaller molecules to the training set, we are able to reduce the
test error from 2.1 kcal mol  1 to o1.5 kcal mol  1 (see inset in
Fig. 1c). This result demonstrates that our model is able to
transfer knowledge learned from small molecules to larger
molecules with diverse functional groups.
While only encompassing conformations of a single molecule,
reproducing MD simulation trajectories poses a radically different
challenge to predicting energies of purely equilibrium structures.
We learned potential energies for MD trajectories of benzene,
toluene, malonaldehyde and salicylic acid, carried out at a rather
high temperature of 500 K to achieve exhaustive exploration of
the potential-energy surface of such small molecules. The neural
network yields mean absolute errors of 0.05, 0.18, 0.17 and
0.39 kcal mol  1 for these molecules, respectively (Supplementary
Table 1). Figure 1d shows the excellent agreement between
the DFT and DTNN MD trajectory of toluene, as well as the
corresponding energy distributions. The DTNN errors are much
smaller than the energy of thermal ﬂuctuations at room
temperature (B0.6 kcal mol  1), meaning that DTNN potentialenergy surfaces can be utilized to calculate accurate molecular
thermodynamic properties by virtue of Monte Carlo simulations.
Supplementary Figs 1 and 2 illustrate how the performance
of DTNN depends on the number of employed reference
calculations and reﬁnement steps (Supplementary Discussion).
The ability of DTNN to accurately describe equilibrium structures
within the GDB-9 database and MD trajectories of selected
molecules of chemical relevance demonstrates the feasibility of
developing
architecture
can capture compositional as well as conﬁgurational degrees of
freedom in the vast chemical space. While the employed
architecture of the DTNN is universal, the learned coefﬁcients
are different for GDB-9 and MD trajectories of single molecules.
potential.
predicting
energies, the true power of DTNN lies in its ability to provide
novel quantum-chemical insights. In the context of DTNN, we
deﬁne a local chemical potential OM
A rð Þ as an energy of a certain
atom type A, located at a position r in the molecule M. While the
DTNN models the interatomic interactions, we only allow the
atoms of the molecule act on the probe atom, while the probe
does not inﬂuence the molecule. The spatial and chemical
sensitivity provided by our DTNN approach is shown in Fig. 1e
for a variety of fundamental molecular building blocks. In this
case, we employed hydrogen as a test charge, while the results for
C;N;O rð Þ are shown in Fig. 2. Despite being trained only on total
energies of molecules, the DTNN approach clearly grasps
fundamental chemical concepts such as bond saturation and
different degrees of aromaticity. For example, the DTNN model
predicts the C6O3H6 molecule to be ‘more aromatic’ than benzene
or toluene (Fig. 1e). Remarkably, it turns out that C6O3H6 does
have higher ring stability than both benzene and toluene and
DTNN predicts it to be the molecule with the most stable
aromatic carbon ring among all molecules in the GDB-9 database
(Fig. 3). Further chemical effects learned by the DTNN model are
shown in Fig. 2 that demonstrates the differences in the chemical
potential distribution of H, C, N and O atoms in benzene,
toluene, salicylic acid and malonaldehyde. For example, the
chemical potentials of different atoms over an aromatic ring are
qualitatively different for H, C, N and O atoms—an evident fact
for a trained chemist. However, the subtle chemical differences
described by DTNN are accompanied by chemically accurate
predictions—a challenging task for humans.
Because DTNN provides atomic energies by construction, it
allows us to classify molecules by the stability of different building
blocks, for example aromatic rings or methyl groups. An example
of such classiﬁcation is shown in Fig. 3, where we plot the
molecules with most stable and least stable carbon aromatic rings
in GDB-9. The distribution of atomic energies is shown in
Supplementary Fig. 3, while Supplementary Fig. 4 lists the full
stability ranking. The DTNN classiﬁcation leads to interesting
stability trends, notwithstanding the intrinsic non-uniqueness of
atomic energy partitioning. However, unlike atomic projections
electronic-structure
calculations,
approach has a ﬁrm foundation in statistical learning theory.
In quantum-chemical calculations, every molecule would correspond to a different partitioning depending on its self-consistent
electron density. In contrast, the DTNN approach learns the
partitioning on a large molecular dataset, generating a transferable and global ‘dressed atom’ representation of molecules in
chemical space. Recalling that DTNN exhibits errors below
1 kcal mol  1, the classiﬁcation shown in Fig. 3 can provide useful
guidance for the chemical discovery of molecules with desired
properties. Analytical gradients of the DTNN model with respect
to chemical composition or OM
A rð Þ could also aid in the
exploration of chemical compound space37.
Energy predictions for isomers. The quantitative accuracy
achieved by DTNN and its size extensivity paves the way to
the calculation of conﬁgurational and conformational energy
differences—a long-standing challenge for machine learning
approaches7,12,13,38. The reliability of DTNN for isomer energy
predictions is demonstrated by the energy distribution in Fig. 4
for molecular isomers with C7O2H10 chemical formula (a total of
6,095 isomers in the GDB-9 data set).
Training a common model for chemical as well as conformational freedoms requires a more complex model. Furthermore,
it comes with technical challenges like sampling and multiscale
issues since the MD trajectories form clusters of small variation
within the chemical compound space. As a proof of principle, we
trained the DTNN to predict various MD trajectories of the
C7O2H10 isomers. To this end, we calculated short MD
trajectories of 5,000 steps each for 113 randomly picked isomers
as well as consistent total energies for all equilbrium structures.
The training set is composed of all isomers in equilibrium as well
as 50% of each MD trajectory. The remaining MD calculations
are used for validation and testing. Despite the added complexity,
our model achieves a mean absolute error of 1.7 kcal mol  1.
Discussion
DTNNs provide an efﬁcient way to represent chemical environments allowing for chemically accurate predictions. To this end,
an implicit, atom-centered basis is learned from reference
calculations.
Employing this
representation,
atoms can be
embedded in their chemical environment within a few reﬁnement
NATURE COMMUNICATIONS | DOI: 10.1038/ncomms13890
NATURE COMMUNICATIONS | 8:13890 | DOI: 10.1038/ncomms13890 | www.nature.com/naturecommunications
steps. Furthermore,
DTNNs have the advantage that
embedding is built recursively from pairwise distances. Therefore,
all necessary invariances (translation, rotation, permutation) are
guaranteed to be exploited by the model. In addition, the learned
embedding can be used to generate alchemical reaction paths
(Supplementary Fig. 5).
approaches,
potential-energy
constructed
expansions
networks39–41. However, these methods require a separate NN
for each non-equivalent many-body term in the expansion. Since
DTNN learns a common basis in which the atom interact, higherorder interactions can obtained more efﬁciently without separate
Approaches like smooth overlap of atomic positions16,17
or manually crafted atom-centered symmetry functions18,19,42
are, like DTNN, based on representing chemical environments.
All these approaches have in common that size-extensivity
regarding the number of atoms is achieved by predicting atomic
energy contributions using a non-linear regression method
(for example, neural networks or kernel ridge regression).
However, the previous approaches have a ﬁxed set of basis
functions describing the atomic environments. In contrast,
DTNNs are able to adapt to the problem at hand in a
A (r) in kcal mol–1
Figure 2 | Chemical potentials OM
A rð Þ for A ¼ {C, N, O, H} atoms. The isosurface was generated for P
k  2 ¼ 3.8 Å  2 (the index i is used to sum
over all atoms of the corresponding molecule). The molecules shown are (in order from top to bottom of the ﬁgure): benzene, toluene, salicylic acid and
malondehyde. Atom colouring: carbon ¼ black, hydrogen ¼ white, oxygen ¼ red.
# 281 – 290
Ering in kcal mol–1
Ering in kcal mol–1
Figure 3 | Classiﬁcation of molecular carbon ring stability. Shown are
20 molecules (10 most stable and 10 least stable) with respect to the
energy of the carbon ring predicted by the DTNN model. Atom colouring:
carbon ¼ black; hydrogen ¼ white; oxygen ¼ red; nitrogen ¼ blue;
ﬂuorine ¼ yellow.
Kendall rank correlation
coefficient = 0.969
–1.900 –1,850 –1,800 –1,750
Atomization energy (DFT)
Atomization energy (NN)
Atomization energy (kcal mol–1)
Figure 4 | Isomer energies with chemical formula C7O2H10. DTNN
trained on the GDB-9 database is able to acurately discriminate between
6,095 different isomers of C7O2H10, which exhibit a non-trivial spectrum of
relative energies.
NATURE COMMUNICATIONS | DOI: 10.1038/ncomms13890
NATURE COMMUNICATIONS | 8:13890 | DOI: 10.1038/ncomms13890 | www.nature.com/naturecommunications
data-driven fashion. Beyond the obvious advantage of not having
to manually select symmetry functions and carefully tune hyperparameters of the representation, this property of the DTNN
quantum-chemical
by analysing the learned representation.
Obviously, more work is required to extend this predictive
power for larger molecules, where the DTNN model will have
to be combined with a reliable model for long-range interatomic
(van der Waals) interactions. The intrinsic interpolation smoothness achieved by the DTNN model can also be used to identify
molecules with peculiar electronic structure. Supplementary Fig. 6
shows a list of molecules with the largest DTNN errors compared
with reference DFT calculations. It is noteworthy that most
molecules in this ﬁgure are characterized by unconventional
bonding and the electronic structure of these molecules has
potential multi-reference character. The large prediction errors
sufﬁciently
represented by the training data. On the other hand, DTNN
predictions might turn out to be closer to the correct answer
because of its smooth interpolation in chemical space. Higherlevel quantum-chemical calculations would be required to
investigate this interesting hypothesis in the future.
We have proposed and developed a deep tensor neural network
that enables understanding of quantum-chemical many-body
systems beyond properties contained in the training dataset. The
DTNN model is scalable with molecular size, efﬁcient, and
achieves uniform accuracy of 1 kcal mol  1 throughout compositional and conﬁguration space for molecules of intermediate size.
The DTNN model leads to novel insights into chemical systems, a
fact that we illustrated on the example of relative aromatic ring
stability, local molecular chemical potentials, relative isomer
energies and the identiﬁcation of molecules with peculiar
electronic structure.
Many avenues remain for improving the DTNN model on
multiple fronts. Among these we mention the extension of
increasingly
molecules,
predicting
atomic forces and frequencies, and non-extensive electronic and
optical properties. We propose the DTNN model as a versatile
framework for understanding complex quantum-mechanical
high-throughput
electronic
calculations.
Reference data sets. We employ two subsets of the GDB database34, referred to
in this paper as GDB-7 and GDB-9. GDB-7 contains 7,211 molecules with up to
seven heavy atoms out of the elements C, N, O, S and Cl, saturated with
hydrogen12. Similarly, GDB-9 includes 133,885 molecules with up to 9 heavy atoms
out of C, O, N, F (ref. 36). Both data sets include calculations of atomization
energies employing density-functional theory43 with the PBE0 (ref. 44) and B3LYP
(ref. 45–49) exchange-correlation potential, respectively.
The molecular dynamics trajectories are calculated at a temperature of 500 K
and resolution of 0.5 fs using density-functional theory with the PBE exchangecorrelation potential50. The data sets for benzene, toluene, malonaldehyde and
salicylic acid consist of 627, 442, 993 and 320 k time steps, respectively. In the
presented experiments, we predict the potential energy of the MD geometries.
Details on the deep tensor neural network model. The molecular energies of the
various data sets are predicted using a deep tensor neural network. The core idea is
to represent atoms in the molecule as vectors depending on their type and to
subsequently reﬁne the representation by embedding the atoms in their neighbourhood. This is done in a sequence of interaction passes, where the atom
representations inﬂuence each other in a pair-wise fashion. While each of these
reﬁnements depends only on the pair-wise atomic distances, multiple passes enable
the architecture to also take angular information into account. Because of this
decomposition of atomic interactions, an efﬁcient representation of embedded
atoms is learned following quantum-chemical principles.
In the following, we describe the deep tensor neural network step-by-step,
including hyper-parameters used in our experiments.
1. Assign initial atomic descriptors
We assign an initial coefﬁcient vector to each atom i of the molecule according
to its nuclear charge Zi:
i ¼cZi 2 RB;
where B is the number of basis functions. All presented models use atomic
descriptors with 30 coefﬁcients. We initialize each coefﬁcient randomly following
cz  N 0; 1=
2. Gaussian feature expansion of the inter-atomic distances
The inter-atomic distances Dij are spread across many dimensions by a uniform
grid of Gaussians
 Dij  mmin þ kDm
0kmmax=Dm
with Dm being the gap between two Gaussians of width s.
In our experiments, we set both to 0.2 Å. The centre of the ﬁrst Gaussian mmin
was set to  1, while mmax was chosen depending on the range of distances in the
data (10 Å for GDB-7 and benzene, 15 Å for toluene, malonaldehyde and salicylic
acid and 20 Å for GDB-9).
3. Perform T interaction passes
Each coefﬁcient vector c tð Þ
i , corresponding to atom i after t passes, is corrected
by the interactions with the other atoms of the molecule:
Here, we model the interaction v as follows:
vij¼ tanh Wfc
Wcfcj þ bf1
Wdf ^dij þ bf2
where the circle () represents the element-wise matrix product. The factor
representation in the presented models employs 60 neurons.
4. Predict energy contributions
Finally, we predict the energy contributions Ei from each atom i. Employing two
fully-connected layers, for each atom a scaled energy contribution ^Ei is predicted:
oi¼ tanh Wout1c T
^Ei¼Wout2oi þ bout2
In our experiments, the hidden layer oi possesses 15 neurons. To obtain the ﬁnal
contributions, ^Ei is shifted to the mean Em and scaled by the s.d. Es of the energy
per atom estimated on the training set.
Ei¼Es^Ei þ Em
This procedure ensures a good starting point for the training.
5. Obtain the molecular energy E ¼ P
The bias parameters as well as Wout2 are initially set to zero. All other weight
matrices are initialized drawing from a uniform distribution according to (ref. 51).
Neural network code is available.
The deep tensor neural networks have been trained for 3,000 epochs
minimizing the squared error, using stochastic gradient descent with 0.9
momentum and a constant learning rate52. The ﬁnal results are taken from the
models with the best validation error in early stopping.
All DTNN models were trained and executed on an NVIDIA Tesla K40 GPU.
The computational cost of the employed models depends on the number of
reference calculations, the number of interaction passes as well as the number of
atoms per molecule. The training times for all models and data sets are shown in
Supplementary Table 2, ranging from 6 h for 5.768 reference calculations of GDB-7
with one interaction pass, to 162 h for 100,000 reference calculations of the GDB-9
data set with three interaction passes.
On the other hand, the prediction is instantaneous: all models predict examples
from the employed data sets in o1 ms. Supplementary Fig. 7 shows the scaling of
the prediction time with the number of atoms and interaction layers. Even for a
molecule with 100 atoms, a DTNN with three interaction layers requires o5 ms for
a prediction.
The prediction as well as the training steps scale linearly with the number of
interaction passes and quadratically with the number of atoms, since the pairwise
atomic distances are required for the interactions. For large molecules it is
reasonable to introduce a distance cutoff. In that case, the DTNN will also scale
linearly with the number of atoms.
Computing and visualizing the local potentials of the DTNN. Given a trained
neural network as described in the previous section, one can extract the coefﬁcients
vectors c tð Þ
for each atom i and each interaction pass t for a molecule of interest.
From each ﬁnal representation c T
, the energy contribution Ei of the corresponding atom to the molecular energy can be obtained. Instead, we let the
molecule act on a probe atom, described by its charge z and the pairwise distances
NATURE COMMUNICATIONS | DOI: 10.1038/ncomms13890
NATURE COMMUNICATIONS | 8:13890 | DOI: 10.1038/ncomms13890 | www.nature.com/naturecommunications
d1,y, dn to the atoms of the molecule:
probe ¼c tð Þ
with vj¼ tanhðWfcððWcfcj þ bf1Þ  ðWdf ^dj þ bf2ÞÞÞ. While this is equivalent to
how the coefﬁcient vectors of the molecule are corrected, here, the molecule does
not get to be inﬂuenced by the probe. Now, the energy of the probe atom is
predicted as usual from the ﬁnal representation c T
Interpreting this as a local potential OM
A rð Þ generated by the molecule, we can
use the neural network to visualize the learned interactions as illustrated in
Supplementary Fig. 8. The presented energy surfaces show the potential for
different probe atoms plotted on an isosurface of Pn
. We used Mayavi53 for
the visualization of the surfaces.
Data availability. The GDB-9 data set is available under the DOI 10.6084/
m9.ﬁgshare.978904. All data sets used in this work are available at