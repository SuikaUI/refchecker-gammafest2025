Time-Series Representation Learning via Temporal and Contextual Contrasting
Emadeldeen Eldele1 , Mohamed Ragab1 , Zhenghua Chen2∗, Min Wu2∗, Chee Keong
Kwoh1 , Xiaoli Li2 and Cuntai Guan1
1School of Computer Science and Engineering, Nanyang Technological University, Singapore
2Institute for Infocomm Research, A*STAR, Singapore
 , {mohamedr002,chen0832}@e.ntu.edu.sg, ,
 , , 
Learning decent representations from unlabeled
time-series data with temporal dynamics is a
very challenging task.
In this paper, we propose an unsupervised Time-Series representation
learning framework via Temporal and Contextual
Contrasting (TS-TCC), to learn time-series representation from unlabeled data. First, the raw timeseries data are transformed into two different yet
correlated views by using weak and strong augmentations. Second, we propose a novel temporal contrasting module to learn robust temporal representations by designing a tough cross-view prediction
task. Last, to further learn discriminative representations, we propose a contextual contrasting module built upon the contexts from the temporal contrasting module. It attempts to maximize the similarity among different contexts of the same sample while minimizing similarity among contexts of
different samples. Experiments have been carried
out on three real-world time-series datasets. The results manifest that training a linear classiﬁer on top
of the features learned by our proposed TS-TCC
performs comparably with the supervised training.
Additionally, our proposed TS-TCC shows high
efﬁciency in few-labeled data and transfer learning scenarios.
The code is publicly available at
 
Introduction
Time-series data are being incrementally collected on daily
basis from IoT and wearable devices for various applications
in healthcare, manufacturing, etc. However, they generally do
not have human recognizable patterns and require specialists
for annotation/labeling. Therefore, it is much harder to label
time-series data than images, and little time-series data have
been labeled in real-world applications [Ching et al., 2018].
Given that deep learning methods usually require a massive
amount of labeled data for training, it is thus very challenging
to apply them on time-series data with these labeling limitations.
∗Corresponding Author
Self-supervised learning gained more attention recently
to extract effective representations from unlabeled data for
downstream tasks. Compared with models trained on full
labeled data (i.e., supervised models), self-supervised pretrained models can achieve comparable performance with
limited labeled data [Chen et al., 2020].
Various selfsupervised approaches relied on different pretext tasks to train
the models and learn representations from unlabeled data,
such as solving puzzles [Noroozi and Favaro, 2016] and predicting image rotation [Gidaris et al., 2018]. However, the
pretext tasks can limit the generality of the learned representations. For example, classifying the different rotation angles
of an image may deviate the model from learning features
about the color or orientation of objects [Oord et al., 2018].
Contrastive learning has recently shown its strong ability
for self-supervised representation learning in computer vision
domain because of its ability to learn invariant representation
from augmented data [Hjelm et al., 2019; He et al., 2020;
Chen et al., 2020].
It explores different views of the input images by ﬁrst applying data augmentation techniques
and then learns the representations by maximizing the similarity of different views from the same sample and minimizing the similarity with the views from different samples.
However, these image-based contrastive learning methods are
not able to work well on time-series data for the following
reasons. First, they may not be able to address the temporal dependencies of data, which are key characteristics of
time-series [Franceschi et al., 2019].
Second, some augmentation techniques used for images such as color distortion, generally cannot ﬁt well with time-series data. So far,
few works on contrastive learning have been proposed for
time-series data.
For example, [Mohsenvand et al., 2020;
Cheng et al., 2020] developed contrastive learning methods
for bio-signals such as EEG and ECG. However, the above
two methods are proposed for speciﬁc applications and they
are not generalizable to other time-series data.
To address the above issues, we propose a Time-Series representation learning framework via Temporal and Contextual
Contrasting (TS-TCC). Our framework employs simple yet
efﬁcient data augmentations that can ﬁt any time-series data
to create two different, but correlated views of the input data.
Next, we propose a novel temporal contrasting module to
learn robust representations by designing a tough cross-view
prediction task, which for a certain timestep, it utilizes the
Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)
past latent features of one augmentation to predict the future of another augmentation. This novel operation will force
the model to learn robust representation by a harder prediction task against any perturbations introduced by different
timesteps and augmentations.
Furthermore, we propose a
contextual contrasting module in TS-TCC to further learn discriminative representations upon the robust representations
learned by the temporal contrasting module. In this contextual contrasting module, we aim to maximize the similarity
among different contexts of the same sample while minimizing similarity among contexts of different samples.
In summary, the main contributions of this work are as follows.
• A novel contrastive learning framework is proposed for
unsupervised time-series representation learning.
• Simple yet efﬁcient augmentations are designed for
time-series data in the contrastive learning framework.
• We propose a novel temporal contrasting module to
learn robust representations from time series data by designing a tough cross-view prediction task. In addition,
we propose a contextual contrasting module to further
learn discriminative representations upon the robust representations.
• We perform extensive experiments on our proposed TS-
TCC framework using three datasets. Experimental results show that the learned representations are effective
for downstream tasks under supervised learning, semisupervised learning and transfer learning settings.
Related Works
Self-supervised Learning
The recent advances in self-supervised learning started with
applying pretext tasks on images to learn useful representations, such as solving jigsaw puzzles [Noroozi and Favaro,
2016], image colorization [Zhang et al., 2016] and predicting image rotation [Gidaris et al., 2018]. Despite the good
results achieved by these pretext tasks, they relied on heuristics that might limit the generality of the learned representations. On the other hand, contrastive methods started to shine
via learning invariant representations from augmented data.
For instance, MoCo [He et al., 2020] utilized a momentum
encoder to learn representations of negative pairs obtained
from a memory bank. SimCLR [Chen et al., 2020] replaced
the momentum encoder by using a larger batch of negative
pairs. Also, BYOL [Grill et al., 2020] learned representations
by bootstrapping representations even without using negative
samples. Last, SimSiam [Chen and He, 2020] supported the
idea of neglecting the negative samples, and relied only on
a Siamese network and stop-gradient operation to achieve the
state-of-the-art performance. While all these approaches have
successfully improved representation learning for visual data,
they may not work well on time series data that have different
properties, such as temporal dependency.
Self-supervised Learning for Time-Series
Representation learning for time series is becoming increasingly popular. Some approaches employed pretext tasks for
Contextual Contrasting
Similarity
Transformer
Transformer
Non-linear
Projection Head
Non-linear
Projection Head
Strong Augmentation
Weak Augmentation
Temporal Contrasting
Figure 1: Overall architecture of proposed TS-TCC model.
time series data.
For example, [Saeed et al., 2019] designed a binary classiﬁcation pretext task for human activity recognition by applying several transformations on the
data, and trained the model to classify between the original
and the transformed versions. Similarly, SSL-ECG approach
[P. Sarkar, 2020] learned ECG representations by applying
six transformations to the dataset, and assigned pseudo labels according to the transformation type. Additionally, [Aggarwal et al., 2019] learned subject-invariant representations
by modeling local and global activity patterns. Inspired by
the success of contrastive learning, few works have recently
leveraged contrastive learning for time series data. For example, CPC [Oord et al., 2018] learned representations by
predicting the future in the latent space and showed great advances in various speech recognition tasks. Also, [Mohsenvand et al., 2020] designed EEG related augmentations and
extended SimCLR model [Chen et al., 2020] to EEG data.
Existing approaches used either temporal or global features.
Differently, we ﬁrst construct different views for input data
by designing time-series speciﬁc augmentations. Additionally, we propose a novel cross-view temporal and contextual
contrasting modules to improve the learned representations
for time-series data.
This section describes our proposed TS-TCC in details. As
shown in Figure 1, we ﬁrst generate two different yet correlated views of the input data based on strong and weak augmentations. Then, a temporal contrasting module is proposed
to explore the temporal features of the data with an autoregressive model. These models perform a tough cross-view
prediction task by predicting the future of one view using the
past of the other. We further maximize the agreement be-
Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)
tween the contexts of the autoregressive models by a contextual contrasting module. Next, we will introduce each component in the following subsections.
Time-Series Data Augmentation
Data augmentation is a key part in the success of the contrastive learning methods [Chen et al., 2020; Grill et al.,
2020]. Contrastive methods try to maximize the similarity
among different views of the same sample, while minimizing
its similarity with other samples. It is thus important to design proper data augmentations for contrastive learning [Chen
et al., 2020; Mohsenvand et al., 2020]. Usually, contrastive
learning methods use two (random) variants of the same augmentation. Given a sample x, they produce two views x1
and x2 sampled from the same augmentations family T , i.e.,
x1 ∼T and x2 ∼T . However, we argue that using different augmentations can improve the robustness of the learned
representations. Consequently, we propose applying two separate augmentations, such that one augmentation is weak and
the other is strong. In this paper, weak augmentation is a
jitter-and-scale strategy. Speciﬁcally, we add random variations to the signal and scale up its magnitude. For strong augmentation, we apply permutation-and-jitter strategy, where
permutation includes splitting the signal into a random number of segments with a maximum of M and randomly shuf-
ﬂing them. Next, a random jittering is added to the permuted
signal. Notably, the augmentation hyperparameters should
be chosen carefully according to the nature of the time-series
data. For example, the value of M in a time-series data with
longer sequences should be greater than its value in those with
shorter sequences when applying permutation. Similarly, the
jittering ratio for normalized time-series data should be much
less than the ratio for unnormalized data.
For each input sample x, we denote its strongly augmented
view as xs, and its weakly augmented view as xw, where
xs ∼Ts and xw ∼Tw. These views are then passed to the encoder to extract their high dimensional latent representations.
In particular, the encoder has a 3-block convolutional architecture as proposed in [Wang et al., 2017]. For an input x, the
encoder maps x into a high-dimensional latent representation
z = fenc(x). We deﬁne z = [z1, z2, . . . zT ], where T is the
total timesteps, zi ∈Rd, where d is the feature length. Thus,
we get zs for the strong augmented views, and zw for the
weak augmented views, which are then fed into the temporal
contrasting module.
Temporal Contrasting
The Temporal Contrasting module deploys a contrastive loss
to extract temporal features in the latent space with an autoregressive model. Given the latent representations z, the
autoregressive model far summarizes all z≤t into a context
vector ct = far(z≤t), ct ∈Rh, where h is the hidden dimension of far. The context vector ct is then used to predict
the timesteps from zt+1 until zt+k (1 < k ≤K). To predict
future timesteps, we use log-bilinear model that would preserve the mutual information between the input xt+k and ct,
such that fk(xt+k, ct) = exp((Wk(ct))T zt+k), where Wk is
a linear function that maps ct back into the same dimension
as z, i.e. Wk : Rh→d.
Multi-Head
Projection
Figure 2: Architecture of Transformer model used in Temporal Contrasting module. The token c in the output is sent next to the Contextual Contrasting module.
In our approach, the strong augmentation generates cs
the weak augmentation generates cw
t . We propose a tough
cross-view prediction task by using the context of the strong
augmentation cs
t to predict the future timesteps of the weak
augmentation zw
t+k and vice versa. The contrastive loss tries
to minimize the dot product between the predicted representation and the true one of the same sample, while maximizing
the dot product with the other samples Nt,k within the minibatch. Accordingly, we calculate the two losses Ls
T C as follows:
exp((Wk(cs
n∈Nt,k exp((Wk(cs
exp((Wk(cw
n∈Nt,k exp((Wk(cw
t ))T zsn)
We use Transformer as the autoregressive model because
of its efﬁciency and speed [Vaswani et al., 2017]. The architecture of the Transformer model is shown in Figure 2. It
mainly consists of successive blocks of multi-headed attention (MHA) followed by an MLP block. The MLP block is
composed of two fully-connected layers with a non-linearity
ReLU function and dropout in between. Pre-norm residual
connections, which can produce more stable gradients [Wang
et al., 2019], are adopted in our Transformer. We stack L
identical layers to generate the ﬁnal features.
Inspired by
BERT model [Devlin et al., 2019], we add a token c ∈Rh
to the input whose state acts as a representative context vector in the output. The operation of the Transformer starts
by applying the features z≤t to a linear projection WT ran
layer that maps the features into the hidden dimension, i.e.
WT ran : Rd→h. The output of this linear projection is then
sent to the Transformer i.e. ˜z = WT ran(z≤t),
Next, we attach the context vector into the features vector ˜z
such that the input features become ψ0 = [c; ˜z], where the
subscript 0 denotes being the input to the ﬁrst layer. Next,
we pass ψ0 through Transformer layers as in the following
equations:
˜ψℓ= MHA(Norm(ψℓ−1)) + ψℓ−1,
ψℓ= MLP(Norm( ˜ψℓ)) + ˜ψℓ,
Finally, we re-attach the context vector from the ﬁnal output
such that ct = ψ0
L. This context vector will be the input of
the following contextual contrasting module.
Contextual Contrasting
We further propose a contextual contrasting module that aims
to learn more discriminative representations. It starts with
Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)
applying a non-linear transformation to the contexts using a
non-linear projection head as in [Chen et al., 2020]. The projection head maps the contexts into the space of where the
contextual contrasting is applied.
Given a batch of N input samples, we will have two contexts for each sample from its two augmented views, and thus
have 2N contexts. For a context ci
t, we denote ci+
positive sample of ci
t that comes from the other augmented
view of the same input, and hence, (ci
t ) are considered
to be a positive pair. Meanwhile, the remaining (2N −2)
contexts from other inputs within the same batch are considered as the negative samples of ci
t, i.e., ci
t can form (2N −2)
negative pairs with its negative samples. Therefore, we can
derive a contextual contrasting loss to maximize the similarity between the positive pair and minimizing the similarity
between negative pairs. As such, the ﬁnal representations can
be discriminative.
Eq. 5 deﬁnes the contextual contrasting loss function LCC.
Given a context ci
t, we divide its similarity with its positive
sample ci+
by its similarity with all the other (2N −1) samples, including the positive pair and (2N −2) negative pairs,
to normalize the loss.
m=1 1[m̸=i] exp
where sim(u, v) = uT v/∥u∥∥v∥denotes the dot product between ℓ2 normalized u and v (i.e., cosine similarity),
1[m̸=i] ∈{0, 1} is an indicator function, evaluating to 1 iff
m ̸= i, and τ is a temperature parameter.
The overall self-supervised loss is the combination of the
two temporal contrasting losses and the contextual contrasting loss as follows.
L = λ1 · (Ls
T C) + λ2 · LCC,
where λ1 and λ2 are ﬁxed scalar hyperparameters denoting
the relative weight of each loss.
Experimental Setup
To evaluate our model, we adopted three publicly available
datasets for human activity recognition, sleep stage classiﬁcation and epileptic seizure prediction, respectively. Additionally, we investigated the transferability of our learned features
on a fault diagnosis dataset.
Human Activity Recognition (HAR)
We use UCI HAR dataset [Anguita et al., 2013] which contains sensor readings for 30 subjects performing 6 activities
(i.e. walking, walking upstairs, downstairs, standing, sitting,
and lying down). They collected the data using a mounted
Samsung Galaxy S2 device on their waist, with a sampling
rate of 50 Hz.
Table 1: Description of datasets used in our experiments. The details
of FD is the same for all the 4 working conditions.
Sleep Stage Classiﬁcation
In this problem, we aim to classify the input EEG signal into
one of ﬁve classes: Wake (W), Non-rapid eye movement (N1,
N2, N3) and Rapid Eye Movement (REM). We downloaded
Sleep-EDF dataset from the PhysioBank [Goldberger et al.,
2000]. Sleep-EDF includes whole-night PSG sleep recordings, where we used a single EEG channel (i.e., Fpz-Cz) with
a sampling rate of 100 Hz, following previous studies [Eldele
et al., 2021].
Epilepsy Seizure Prediction
The Epileptic Seizure Recognition dataset [Andrzejak et al.,
2001] consists of EEG recordings from 500 subjects, where
the brain activity was recorded for each subject for 23.6
seconds. Note that the original dataset is labeled with ﬁve
classes. As four of them do not include epileptic seizure, so
we merged them into one class and treat it as a binary classi-
ﬁcation problem.
Fault Diagnosis (FD)
We conducted the transferability experiment on a real-world
fault diagnosis dataset [Lessmeier et al., 2016]. This dataset
was collected under four different working conditions. Each
working condition can be considered as a separate domain as
it has different characteristics from the other working conditions [Ragab et al., 2020]. Each domain has three classes,
namely, two fault classes (i.e., inner fault and outer fault) and
one healthy class.
Table 1 summarizes the details of each dataset, e.g., the
number of training samples (# Train) and testing samples (#
Test), the length of the sample, the number of sensor channels
(# Channel) and the number of classes (# Class).
Implementation Details
We split the data into 60%, 20%, 20% for training, validation and testing, with considering subject-wise split for
Sleep-EDF dataset to avoid overﬁtting. Experiments were repeated for 5 times with 5 different seeds, and we reported
the mean and standard deviation. The pretraining and downstream tasks were done for 40 epochs, as we noticed that
the performance does not improve with further training. We
applied a batch size of 128 (which was reduced to 32 in
few-labeled data experiments as data size may be less than
128). We used Adam optimizer with a learning rate of 3e-4,
weight decay of 3e-4, β1 = 0.9, and β2 = 0.99. For the
strong augmentation, we set MHAR = 10, MEp = 12 and
MEDF = 20, while for the weak augmentation, we set the
scaling ratio to 2 for all the datasets. We set λ1 = 1, while
we achieved good performance when λ2 ≈1. Particularly,
we set it as 0.7 in our experiments on the four datasets. In
Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)
Random Initialization
57.89±5.13
55.45±5.49
35.61±6.96
23.80±7.96
90.26±1.77
81.12±4.22
Supervised
90.14±2.49
90.31±2.24
83.41±1.44
74.78±0.86
96.66±0.24
94.52±0.43
SSL-ECG [P. Sarkar, 2020]
65.34±1.63
63.75±1.37
74.58±0.60
65.44±0.97
93.72±0.45
89.15±0.93
CPC [Oord et al., 2018]
83.85±1.51
83.27±1.66
82.82±1.68
73.94±1.75
96.61±0.43
94.44±0.69
SimCLR [Chen et al., 2020]
80.97±2.46
80.19±2.64
78.91±3.11
68.60±2.71
96.05±0.34
93.53±0.63
TS-TCC (ours)
90.37±0.34
90.38±0.39
83.00±0.71
73.57±0.74
97.23±0.10
95.54±0.08
Table 2: Comparison between our proposed TS-TCC model against baselines using linear classiﬁer evaluation experiment.
Supervised
TS-TCC (FT)
Table 3: Cross-domains transfer learning experiment applied on Fault Diagnosis dataset in terms of accuracy. (FT stands for ﬁne-tuning)
the Transformer, we set the L = 4, and the number of heads
as 4. We tuned h ∈{32, 50, 64, 100, 128, 200, 256} and set
hHAR,Ep = 100, hEDF = 64. We also set its dropout to 0.1.
In contextual contrasting, we set τ = 0.2. Lastly, we built our
model using PyTorch 1.7 and trained it on a NVIDIA GeForce
RTX 2080 Ti GPU.
To show the efﬁcacy of our proposed TS-TCC, we test it on
three different training settings, including linear evaluation,
semi-supervised training and transfer learning. We evaluate
the performance using two metrics namely the accuracy and
the macro-averaged F1-score (MF1) to better evaluate the imbalanced datasets.
Comparison with Baseline Approaches
We compare our proposed approach against the following
baselines. (1) Random Initialization: training a linear classiﬁer on top of randomly initialized encoder; (2) Supervised:
supervised training of both encoder and classiﬁer model; (3)
SSL-ECG [P. Sarkar, 2020]; (4) CPC [Oord et al., 2018]; (5)
SimCLR [Chen et al., 2020]. It is worth noting that, we use
time-series speciﬁc augmentations to adapt SimCLR to our
application as it was originally designed for images.
To evaluate the performance of our TS-TCC model,
we follow the standard linear benchmarking evaluation
scheme [Oord et al., 2018; Chen et al., 2020]. Particularly,
we train a linear classiﬁer (single MLP layer) on top of a
frozen self-supervised pretrained encoder model.
shows the linear evaluation results of our approach against
the baseline methods. Overall, our proposed TS-TCC outperforms all the three state-of-the-art methods. Furthermore,
TS-TCC, with only linear classiﬁer, performs best on two out
of three datasets while achieving comparable performance to
the supervised approach on the third dataset. This demonstrates the powerful representation learning capability of our
TS-TCC model. Notably, contrastive methods (e.g., CPC,
SimCLR and our TS-TCC) generally achieve better results
than the pretext-based method (i.e., SSL-ECG), which re-
ﬂects the power of invariant features learned by contrastive
methods. Additionally, CPC method shows better results than
SimCLR, indicating that temporal features are more important than general features in time-series data.
Semi-supervised Training
We investigate the effectiveness of our TS-TCC under the
semi-supervised settings, by training the model with 1%, 5%,
10%, 50%, and 75% of randomly selected instances of the
training data.
Figure 3 shows the results of our TS-TCC
along with the supervised training under the aforementioned
settings. In particular, TS-TCC ﬁne-tuning (i.e., red curves
in Figure 3) means that we ﬁne-tuned the pretrained encoder
with few labeled samples.
We observe that supervised training performs poorly with
limited labeled data, while our TS-TCC ﬁne-tuning achieves
signiﬁcantly better performance than supervised training with
only 1% of labeled data. For example, TS-TCC ﬁne-tuning
can still achieve around 70% and 90% for HAR and Epilepsy
datasets respectively. Furthermore, our TS-TCC ﬁne-tuning
with only 10% of labeled data can achieve comparable performance with the supervised training with 100% of labeled
data in the three datasets, demonstrating the effectiveness of
our TS-TCC method under the semi-supervised setting.
Transfer Learning Experiment
We further examine the transferability of the learned features
by designing a transfer learning experiment. We use Fault
Diagnosis (FD) dataset introduced in Table 1 for the evaluation under the transfer learning setting.
Here, we train
the model on one condition (i.e., source domain) and test it
on another condition (i.e., target domain). In particular, we
adopt two training schemes on the source domain, namely,
(1) supervised training and (2) TS-TCC ﬁne-tuning where we
ﬁne-tuned our pretrained encoder using the labeled data in the
source domain.
Table 3 shows the performance of the two training schemes
under 12 cross-domain scenarios. Clearly, our pretrained TS-
TCC model with ﬁne-tuning (FT) consistently outperforms
Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)
82.76±1.50
82.17±1.64
80.55±0.39
70.99±0.86
94.39±1.19
90.93±1.41
TC + X-Aug
87.86±1.33
87.91±1.09
81.58±1.70
71.88±1.71
95.56±0.24
92.57±0.29
TS-TCC (TC + X-Aug + CC)
90.37±0.34
90.38±0.39
83.00±0.71
73.57±0.74
97.23±0.10
95.54±0.08
TS-TCC (Weak only)
76.55±3.59
75.14±4.66
80.90±1.87
72.51±1.74
97.18±0.17
95.47±0.31
TS-TCC (Strong only)
60.23±3.31
56.15±4.14
78.55±2.94
68.05±1.87
97.14±0.23
95.39±0.29
Table 4: Ablation study of each component in TS-TCC model performed with linear classiﬁer evaluation experiment.
Figure 3: Comparison between supervised training vs.
ﬁne-tuning for different few-labeled data scenarios in terms of MF1.
the supervised pretraining in 8 out of 12 cross-domain scenarios. TS-TCC model can achieve at least 7% improvement
in 7 out of 8 winning scenarios (except for D→B scenario).
Overall, our proposed approach can improve the transferability of learned representations over the supervised training by
about 4% in terms of accuracy.
Ablation Study
We study the effectiveness of each component in our proposed TS-TCC model.
Speciﬁcally, we derive different
model variants for comparison as follows. First, we train the
Temporal Contrasting module (TC) without the cross-view
prediction task, where each branch predicts the future timesteps of the same augmented view. This variant is denoted
as ‘TC only’. Second, we train the TC module with adding
the cross-view prediction task, which is denoted as ‘TC + X-
Aug’. Third, we train the whole proposed TS-TCC model,
which is denoted as ‘TC + X-Aug + CC’. We also study the
effect of using a single augmentation in TS-TCC. In particular, for an input x, we generate two different views x1 and x2
from the same augmentation type, i.e., x1 ∼Tw and x2 ∼Tw
when using the weak augmentation.
Table 4 shows this ablation study on the three datasets.
Clearly, the proposed cross-view prediction task generates robust features and thus improves the performance by more than
5% on HAR datasets, and ∼1% on Sleep-EDF and Epilepsy
datasets. Additionally, the contextual contrasting module further improves the performance, as it helps the features to be
more discriminative. Studying the augmentations effect, we
ﬁnd that generating different views from the same augmentation type is not helpful with HAR and Sleep-EDF datasets.
On the other hand, Epilepsy dataset can achieve comparable performance with only one augmentation. Overall, our
proposed TS-TCC method using both types of augmentations
achieves the best performance.
Sensitivity Analysis
We perform sensitivity analysis on HAR dataset to study three
parameters namely, the number of predicted future timesteps
K in the temporal contrasting module, besides λ1 and λ2 in
Figure 4a shows the effect of K on the overall performance, where x-axis is the percentage K/d, d is the length
of the features. Clearly, increasing the percentage of the predicted future timesteps improves the performance. However,
larger percentages can harm the performance as it reduces
the amount of past data used for training the autoregressive
model. We observe that predicting 40% of the total feature
length performs the best, and thus we set K as d×40% in our
experiments. Figures 4b and 4c show the results of varying
λ1 and λ2 in a range between 0.001 and 1000 respectively.
We ﬁx λ1 = 1 and change the values of λ2 in Figure 4c.
We observe that our model achieves good performance when
λ2 ≈1, where the model performs best with λ2 = 0.7. Consequently, we ﬁx λ2 = 0.7 and tune the value of λ1 as in
Figure 4b, where we ﬁnd that our model achieves the best
performance when λ1 = 1. We also ﬁnd that as λ1 < 10, our
model is less sensitive to its value, while it is more sensitive
to different values of λ2.
Conclusions
We propose a novel framework called TS-TCC for unsupervised representation learning from time-series data. The proposed TS-TCC framework ﬁrst creates two views for each
sample by applying strong and weak augmentations. Then the
temporal contrasting module learns robust temporal features
by applying a tough cross-view prediction task. We further
propose a contextual contrasting module to learn discriminative features upon the learned robust representations. The
experiments show that a linear classiﬁer trained on top the
features learned by our TS-TCC performs comparably with
supervised training. In addition, our proposed TS-TCC shows
high efﬁciency on few-labeled data and transfer learning scenarios, e.g., our TS-TCC by using only 10% of the labeled
data can achieve close performance to the supervised training
with full labeled data.
Proceedings of the Thirtieth International Joint Conference on Artiﬁcial Intelligence (IJCAI-21)
Figure 4: Three sensitivity analysis experiments on HAR dataset.
Acknowledgements
This research is supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funds (Grant No. A20H6b0151) and Career Development Award (Grant No. C210112046).