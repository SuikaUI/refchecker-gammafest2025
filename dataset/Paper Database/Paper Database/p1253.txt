An Empirical Study of Spatial Attention Mechanisms in Deep Networks
Xizhou Zhu1,3†∗Dazhi Cheng2,3†∗
Zheng Zhang3∗
Stephen Lin3
Jifeng Dai3
1University of Science and Technology of China
2Beijing Institute of Technology
3Microsoft Research Asia
 
{v-dachen,zhez,stevelin,jifdai}@microsoft.com
Attention mechanisms have become a popular component in deep neural networks, yet there has been little examination of how different inﬂuencing factors and methods for computing attention from these factors affect performance. Toward a better general understanding of attention mechanisms, we present an empirical study that ablates various spatial attention elements within a generalized
attention formulation, encompassing the dominant Transformer attention as well as the prevalent deformable convolution and dynamic convolution modules. Conducted on a
variety of applications, the study yields signiﬁcant ﬁndings
about spatial attention in deep networks, some of which run
counter to conventional understanding. For example, we
ﬁnd that the comparison of query and key content in Transformer attention is negligible for self-attention, but vital for
encoder-decoder attention. On the other hand, a proper
combination of deformable convolution with key content
saliency achieves the best accuracy-efﬁciency tradeoff in
self-attention. Our results suggest that there exists much
room for improvement in the design of attention mechanisms.
1. Introduction
Attention mechanisms enable a neural network to focus more on relevant elements of the input than on irrelevant parts.
They were ﬁrst studied in natural language
processing (NLP), where encoder-decoder attention modules were developed to facilitate neural machine translation . In computing the output for a given query
element (e.g., a target word in the output sentence), certain key elements (e.g., source words in the input sentence)
∗Equal contribution. †This work is done when Xizhou Zhu and Dazhi
Cheng are interns at Microsoft Research Asia.
are prioritized according to the query. Later, self-attention
modules were presented for modeling intra-sentence relations , where both the key and query
are from the same set of elements.
In a milestone paper , the Transformer attention module is presented,
superseding past works and substantially surpassing their
performance. The success of attention modeling in NLP
has led to its adoption in computer vision, where different variants of Transformer attention are applied to recognition tasks such as object detection and semantic segmentation , where the query and key are
visual elements such as image pixels or regions of interest.
In determining the attention weight assigned to a certain
key for a given query, there exist just a few properties of the
input that are commonly considered. One is the content of
the query. For the case of self-attention, the query content
may be the features at the query pixel in an image, or of a
word in a sentence. Another is the content of the key, where
a key may be a pixel within a local neighborhood of the
query, or another word within the sentence. The third is the
relative position of the query and key.
Based on these input properties, there are four possible attention factors from which the attention weight for
a key with respect to a query is determined, as these factors must account for information about the key. Speciﬁcally, these factors are (1) the query and key content, (2)
the query content and relative position, (3) the key content only, and (4) the relative position only. In the latest
version of Transformer attention , attention weights are
expressed as a sum of four terms (E1, E2, E3, E4), one for
each of these attention factors as illustrated in Fig. 1. The
nature of the dependencies involved with these terms vary.
For example, the ﬁrst two (E1, E2) are sensitive to the query
content. While, the latter two (E3, E4) do not account for
query content, but rather they mainly capture salient key
elements and exploit global positional biases, respectively.
Although attention weights can be decomposed into terms
 
(a) query and key content
(b) query content and relative position
(c) key content only
(d) relative position only
relative position
relative position query
relative position
relative position
Figure 1. Illustration of different attention terms. The color bar above a sampling point denotes its content feature. The existence of content
features and/or relative position indicates that the term uses them for attention weight calculation.
based on these factors, their relative signiﬁcance in various inference problems has not been closely examined in
the literature. Moreover, prevalent modules like deformable
convolution and dynamic convolution , though
seemingly orthogonal to Transformer attention, also employ
mechanisms that focus on certain parts of an input. Whether
these modules can all be viewed from a uniﬁed perspective
and how their operational mechanisms differ also have not
been explored.
In this work, we perceive Transformer attention, deformable convolution, and dynamic convolution modules as
various instantiations of spatial attention, involving different subsets of the attention factors and accounting for these
factors in different ways. Towards disentangling the effects
of different attention factors and mechanisms, we present
an empirical study of spatial attention, in which various elements of attention mechanisms are ablated within a generalized attention formulation. This investigation is conducted on a variety of applications, namely neural machine
translation, semantic segmentation, and object detection.
From this study, we ﬁnd that: 1) In the Transformer attention module, the query-sensitive terms, especially the query
and key content term, play a minor role in self-attention.
But in encoder-decoder attention, the query and key content term is vital. 2) Though deformable convolution utilizes an attention mechanism based only on the query content and relative position term, it operates more effectively
and efﬁciently on image recognition than the counterpart in
Transformer attention. 3) In self-attention, the factors of
query content & relative position and key content only are
the most important. A proper combination of deformable
convolution and the key content only term in Transformer
attention delivers higher accuracy than that of the Transformer attention module, with much lower computational
overhead on image recognition tasks.
The observations made in this paper challenge the conventional understanding of current spatial attention mechanisms. For example, it is widely believed that their success
can mainly be attributed to query-sensitive attention, especially the query and key content term. This understanding
perhaps originates from the initial success of the encoderdecoder attention module in neural machine translation.
Thus, in some recent variants , like the
non-local block and criss-cross attention module ,
only the query and key content term is kept, with all the
other terms removed.
These modules still function well
in self-attention applications, which strengthen this perception. However, our study suggests that this understanding is
incorrect. We ﬁnd that these attention modules with only
query-sensitive terms actually perform on par with those
with only query-irrelevant terms. Our study further suggests that this degeneration is likely due to the design of
the attention modules, rather than an inherent characteristic of self-attention, since deformable convolution is found
to exploit query content & relative position effectively and
efﬁciently in image recognition tasks.
This empirical analysis suggests that there is much room
for improvement in the design of spatial attention mechanisms in deep networks. Its ﬁndings are used in this paper to make some initial headway in this direction, and it is
hoped that this study will spur further investigation into the
operational mechanisms used in modeling spatial attention.
2. Related Work
Development and application of attention-based modules. The ﬁeld of NLP has witnessed steady development of
attention mechanisms in recent years .
Starting from the introduction of an attention module in
neural machine translation , various attention factors and
weight assignment functions based on these factors have
been utilized. In , the inner product of vectors encoding query and key contents is recommended for computing
attention weights, and absolute spatial positions are incorporated as an attention factor. In , the weight assignment additionally accounts for the inner product of spatial
positions encoded in high-dimensional vectors. The landmark work of Transformer set a new standard, and its
latest variants use relative positions instead of absolute positions for better generalization ability . In this paper,
we conduct the empirical study on the latest instantiation of
Transformer attention from this family of works.
Motivated by their success in NLP tasks , attention mechanisms have also been employed
in computer vision applications such as relational reasoning
among objects , image captioning , image generation , image recognition ,
and video recognition .
In vision, the key and
query refer to visual elements, but aside from that, most
of these works use a formulation similar to Transformer attention. Since the effects of different attention module elements may vary with the target application, we conduct the
empirical study on three different tasks that have been in-
ﬂuenced greatly by attention modeling, namely neural machine translation in NLP, and object detection and semantic
segmentation in computer vision.
Aside from Transformer attention, there are variants of
convolution, such as deformable convolution and
dynamic convolution , that also can be viewed as types
of attention mechanisms which operate on a subset of the
attention factors using different attention weight functions.
They also are included in the study for examination.
It is worth mentioning a dual form of spatial attention,
called channel-wise feature attention . As
different feature channels encode different semantic concepts, these works seek to capture the correlations among
these concepts through activation/deactivation of certain
channels. Meanwhile, in the spatial domain, relationships
among elements at different spatial positions are modeled,
with the same attention weights on feature channels assigned to related spatial positions.
The development of
channel-wise feature attention has been focused on certain image recognition tasks, like semantic segmentation
and image classiﬁcation.
In this paper, our empirical
study speciﬁcally examines spatial attention mechanisms
designed for broad application.
Analysis of spatial attention mechanisms.
There exists relatively little analysis of spatial attention mechanisms despite their prevalence in deep networks. This research has largely been conducted by visualizing or analyzing the learned attention weights of a whole attention module on only NLP tasks . Many
works suggest that attention weight assignment
in encoder-decoder attention plays a role similar to word
alignment in traditional approaches . The implicit underlying assumption in these works is that the input
elements accorded high attention weights are responsible
for the model outputs. However, recent research casts doubt
on this assumption , ﬁnding that attention weights do
not correlate well with feature importance measures, and
that counterfactual attention weight conﬁgurations do not
yield corresponding changes in prediction.
In this paper, we conduct the ﬁrst comprehensive empirical study on the elements of spatial attention modules over
both NLP and computer vision tasks. Different attention
factors and weight assignment functions are carefully disentangled, with their effects directly measured by the ﬁnal
performance on these tasks.
3. Study of Spatial Attention Mechanisms
To facilitate our study, we develop a generalized attention formulation that is able to represent various module
designs. We then show how the dominant attention mechanisms can be represented within this formulation, and how
ablations can be conducted using this formulation with respect to different attention module elements.
Generalized attention formulation
Given a query element and a set of key elements, an attention function adaptively aggregates the key contents according to attention weights that measure the compatibility
of query-key pairs. To allow the model to attend to key contents from different representation subspaces and different
positions, the outputs of multiple attention functions (heads)
are linearly aggregated with learnable weights. Let q index
a query element with content zq, and k index a key element
with content xk. Then the multi-head attention feature yq is
computed as
Am(q, k, zq, xk) ⊙W ′
where m indexes the attention head, Ωq speciﬁes the
supporting key region for the query, Am(q, k, zq, xk)
denotes the attention weights in the m-th attention
head, and Wm and W ′
m are learnable weights.
Usually, the attention weights are normalized within Ωq, as
k∈Ωq Am(q, k, zq, xk) = 1.
In encoder-decoder attention, the key and the query are
from two different sets of elements, where in most applications the two sets of elements need to be properly aligned.
For example, in the encoder-decoder attention of neural machine translation, the key and the query elements correspond to the words in the input and the output sentences,
respectively, where proper alignment is necessary for correct translation. Meanwhile, in self-attention, the key and
the query are from the same set of elements. For example, both the key and the query are of words in the input or
output sentence. In such scenarios, the self-attention mechanism is expected to capture intra-relationships among the
elements, and usually the query and the key contents are
modeled by the same set of features, i.e., x = z.
Transformer attention
In the most recent instantiation of the Transformer attention module , the attention weight of each query-key
pair is computed as the sum of four terms {Ej}4
j=1 that are
based on different attention factors, as
(q, k, zq, xk) ∝exp
normalized by P
k∈Ωq ATrans
(q, k, zq, xk) = 1 where the
supporting key region Ωq spans the key elements (e.g., the
attention mechanism
spatial properties query content key content relative position
complexity
Transformer attention
dense, global
s C + NsC2)
dense, global
s C + NsC2)
dense, global
dense, global
s C + NsC2)
Regular convolution
sparse, local
Deformable convolution
sparse, global
Dynamic convolution
sparse, local
O(NsCNgNk + NsC2)
Table 1. Comparison of different attention mechanisms. Ns denotes number of spatial elements, i.e. width by height for images, and
number of tokens for text; C denotes representation dimension; Nk denotes kernel size of convolution (Nk = 3 × 3 for images and
Nk = 3 for text, by default); Ng denotes number of feature groups in dynamic convolution.
whole input sentence). By default, 8 attentional heads are
utilized in this paper.
The E1 and E2 terms are sensitive to the query content.
The E1 term measures the compatibility of the query and
key content, as E1 = z⊤
mxk, where Um, V C
learnable embedding matrices for the query and key content, respectively.
It enables the network to focus more
on the keys compatible with the query in terms of content.
A possible outcome is the correspondence between similar
query and key elements, as illustrated in Fig. 1 (a). For
the E2 term, it is based on the query content and relative
position, as E2 = z⊤
mRk−q, where Rk−q encodes the
relative position k−q by projecting it to a high-dimensional
representation through computing sine and cosine functions
of different wavelengths1 . V R
m is a learnable embedding matrix for the encoded relative position Rk−q. This
term allows the network to adaptively determine where to
assign high attention weights based on the query content. It
may help to disentangle appearance from spatial transformations in image recognition, as illustrated in Fig. 1 (b).
The E3 and E4 terms are irrelevant to the query content.
The E3 term involves key content only, as E3 = u⊤
where um is a learnable vector. It captures salient key content which should be focused on for the task, and is irrelevant to the query. An illustration is shown in Fig. 1 (c).
As for the E4 term, it involves relative position only, as
mRk−q, where vm is a learnable vector. It captures global positional bias between the key and query elements, as illustrated in Fig. 1 (d).
It is widely believed that query-sensitive prioritization,
especially the query and key content compatibility term E1,
is the key to the success of Transformer attention. Thus, in
some recent variants , only E1 is kept, while
the other terms are all removed.
In Transformer attention, both Wm and W ′
m in Eq. (1)
are learnable. W ′
m projects the features of xk to a relatively
low dimension for reducing computational overhead, and
Wm projects the aggregated features back to the same dimension as yq.
1For 2-d image data, we separately encode the x-axis relative position
k−q and y-axis relative position RY
k−q, and concatenate them to be the
ﬁnal encoding Rk−q = [RX
Regular and deformable convolution
Regular and deformable convolution can be deemed
as special instantiations of spatial attention mechanisms,
where subsets of the attention factors are involved.
In regular convolution, given a query element, a ﬁxed
number of key elements (e.g., 3 × 3) are sampled, according to predetermined positional offsets with respect to the
query. From the perspective of Eq. (1), the attention weight
of regular convolution can be expressed as
if k = q + pm
where each sampled key element is of a separate attention
head (e.g., 3×3 regular convolution corresponds to 9 attention heads), and pm denotes the offset for the m-th sampling
position. In addition, the weight W ′
m in Eq. (1) is ﬁxed as
identity, leaving Wm as learnable. In regular convolution,
only relative position is involved, without learnable parameters for adapting attention to content. The supporting key region Ωq is restricted to a local window centered at the query
position and determined by the convolution kernel size.
In deformable convolution , learnable offsets are
added to adjust the sampling positions of the key elements,
so as to capture spatial transformations. The learnable offsets are predicted based on the query content, and are thus
dynamic to the input. The key and the query elements are
from the same set. It can also be incorporated into the generalized attention formulation as a special instantiation of
self-attention, where the attention weight is
(q, k, xq) = G(k, q + pm + w⊤
where pm also denotes a predetermined offset, and w⊤
projects the query content xq to a deformation offset according to a learnable vector wm2.
G(a, b) is the bilinear interpolation kernel in N-d space, which can be decomposed into 1-d bilinear interpolations as G(a, b) =
n=1 g(an, bn), where an and bn denote the n-th dimension of a and b respectively, and g(an, bn) = max(0, 1 −
2Following , the learning rate of wm is set to 0.1 times that of other
parameters to stabilize training.
|an −bn|). Similar to regular convolution, the weight W ′
in Eq. (1) is ﬁxed as identity.
In deformable convolution, the attention factors are
query content and relative position.
The supporting key
region Ωq can span over all the input elements due to the
introduced learnable offsets, while non-zero weights are assigned to a sparse set of key elements where bilinear interpolation is performed.
Dynamic convolution
Dynamic convolution is recently proposed to replace the Transformer attention module in self-attention,
and is claimed to be simpler and more efﬁcient. It is built
upon depth-wise separable convolution with shared dynamic kernel weights, which are predicted based on the
query content. In depth-wise separable convolution, a standard convolution is factorized into a depth-wise convolution
and a 1×1 convolution called a point-wise convolution, for
reducing computation and model size. In depth-wise convolution, a single ﬁlter is applied to each input channel, which
is ﬁxed for all positions. In dynamic convolution, the kernel
weights for the depth-wise convolution are dynamically predicted from the input features, followed by a Softmax normalization. For computational savings, the input channels
are divided into several groups, where each group shares the
same dynamic kernel weights. In the system of , an orthogonal module called the gated linear unit (GLU) is
applied before the dynamic convolution module to improve
accuracy. We include the GLU to respect the original design.
Dynamic convolution can also be incorporated into the
general attention formulation in Eq. (1) with minor modi-
ﬁcations, where each input feature channel is of a separate
attention head. It can be expressed as
(q, k, xq) · xk,c
where c enumerates the channels of the input features (Cin
channels in total), xk,c denotes the feature value at the c-th
channel of xk, and Wc is of the 1 × 1 point-wise convolution. Adynamic
(q, k, xq) is the attention weight speciﬁed by
the dynamic kernel in depth-wise convolution, written as
(q, k, xq) =
if k = q + pj
where pj denotes the j-th sampling position in the dynamic
kernel, and Kj,c is the corresponding kernel weight. Zero
attention weight is assigned to keys outside of the kernel.
The kernel weight Kj,c is predicted from the input features,
and is shared among channels in the same group, as
Kj,c = Kshare
The input features are divided into Ng groups (Ng = 16 by
default). Kshare
denotes the dynamic kernel weight for the
g-th group, and dj,g is the corresponding learnable weight
vector. Kshare
is normalized by PNk
j=1 Kshare
= 1, where
Nk denotes the number of elements in the dynamic kernel.
In dynamic convolution, attention assignment is based
on the query content and relative position factor. The supporting key region Ωq is restricted to a local window around
the query position covered by the dynamic kernel.
Comparing attention mechanisms
Tab. 1 compares the three attention mechanisms discussed above. Transformer attention exploits comprehensive content and position information from both query and
key. The E1, E2 and E4 terms require computation proportional to the product of the query and key element numbers, because they involve a traversal of each query-key
pair. The E3 term captures key content only, and thus involves computation linear to the key element number. In
neural machine translation, the key and query elements are
commonly dozens of words in a sentence, so the computational overheads of E1, E2 and E4 are comparable to E3. In
image recognition, the key and query elements consist of
numerous pixels in an image. The computational overheads
of E1, E2 and E4 are thus much heavier than E3. Note that
when the four terms are put together, some computational
overhead can be shared among them.
Similar to the E2 term, deformable convolution also is
based on query content and relative position.
But deformable convolution samples just a sparse set of key elements for each query, and the complexity is linear to the
query element number.
Deformable convolution is thus
much faster to compute than E2 for image recognition, and
is comparable in speed to E2 for machine translation.
Dynamic convolution also relies on query content and
relative position. The attention weights of key elements are
assigned by the dynamic convolution kernel, based on the
query content. Non-zero attention weights only exist in a
local range covered by the dynamic kernel. The computational overhead is proportional to the product of the kernel
size and query element number. Compared to the E2 term,
the computational overhead can be considerably lower if the
kernel size is much smaller than the key element number.
We seek to further disentangle the effects of different attention factors, and to facilitate comparison to other instantiations of spatial attention that use a subset of the factors.
Thus, manual switches are introduced into the Transformer
attention module, which enable us to manually activate / deactivate particular terms. This is expressed as
(q, k, zq, xk) ∝exp
where {βTrans
} takes values in {0, 1} to control the activa-
tion of corresponding terms, and ˆATrans
(q, k, zq, xk) is normalized by P
k∈Ωq ˆATrans
(q, k, zq, xk) = 1.
Incorporating attention modules into deep networks
We incorporate various attention mechanisms into deep
networks to study their effects. There are different design
choices in inserting the modules, e.g., whether to connect
them in series or in parallel, and where to place the modules in the backbone network. We empirically observed the
results to be quite similar for different well-considered designs. In this paper, we select the design choices in Fig. 2.
For the object detection and semantic segmentation
tasks, ResNet-50 is chosen as the backbone and just
the self-attention mechanism is involved. The Transformer
attention module is incorporated by applying it on the 3 × 3
convolution output in the residual block. For insertion into
a pre-trained model without breaking the initial behavior,
the Transformer attention module includes a residual connection, and its output is multiplied by a learnable scalar
initialized to zero, as in . The manner of incorporating
dynamic convolution is the same. To exploit deformable
convolution, the 3 × 3 regular convolution in the residual
block is replaced by its deformable counterpart. The resulting architecture is called “Attended Residual Block”, shown
in Fig. 2 (a).
In the neuron machine translation (NMT) task, the network architecture follows the Transformer base model ,
where both self-attention and encoder-decoder attention
mechanisms are involved. Different from the original paper,
we update the absolute position embedding in the Transformer attention module by the latest relative position version as in Eq. 2. Because both deformable convolution
and dynamic convolution capture self-attention, they are
added to only the blocks capturing self-attention in Transformer. For dynamic convolution, we replace the Transformer attention module by dynamic convolution directly,
as in . The architecture is shown in Fig. 2 (b). For
its deformable convolution counterpart, because the Transformer model does not utilize any spatial convolution (with
kernel size larger than 1), we insert the deformable convolution unit (with kernel size of 3) prior to the input of the
Transformer attention module. The resulting architecture is
called “Transformer + Deformable”, shown in Fig. 2 (c).
4. Experiments and Analysis
4.1. Experimental settings
Image Object Detection
Models are trained on the 118k images of the COCO
2017 train set. Evaluation is done on the 5k images
of the COCO 2017 validation set. Accuracy is measured by
the standard mean AP scores at different box IoUs (mAP).
Faster R-CNN with Feature Pyramid Networks
(FPN) is chosen as the baseline system. ImageNet 
pre-trained ResNet-50 is utilized as the backbone.
attended residual blocks in Fig. 2 (a) are applied in the
last two stages (conv4 and conv5 stages) of ResNet-50.
In Transformer attention, the relative position encoding is
of the same dimension as the content feature embedding,
speciﬁcally 256-d and 512-d in the conv4 and conv5 stages,
respectively.
Experiments are implemented based on the open source
mmdetection code base. The hyper-parameter setting
strictly follows FPN . Anchors of 5 scales and 3 aspect ratios are utilized. 2k and 1k region proposals are generated at a non-maximum suppression threshold of 0.7 at
training and inference respectively. In SGD training, 256
anchor boxes (of positive-negative ratio 1:1) and 512 region
proposals (of positive-negative ratio 1:3) are sampled for
backpropagating their gradients. In our experiments, the
networks are trained on 8 GPUs with 2 images per GPU for
12 epochs. The learning rate is initialized to 0.02 and is divided by 10 at the 8-th and the 11-th epochs. The weight
decay and the momentum parameters are set to 10−4 and
0.9, respectively.
Image Semantic Segmentation
Models are trained on the 5,000 ﬁnely annotated images
of the Cityscapes train set. Evaluation is done on the 500
images of the validation set. The standard mean IoU score
(mIoU) is used to measure semantic segmentation accuracy.
The CCNet for semantic segmentation is utilized,
with ImageNet pre-trained ResNet-50 and without the crisscross attention module proposed in , which is a variant
of Transformer attention. As done for object detection, the
attended residual blocks in Fig. 2 (a) are applied in the last
two stages. An additional Transformer attention / dynamic
convolution module is placed after the ResNet-50 output
following the practice in for improving performance.
The hyper-parameter setting strictly follows that in the
CCNet paper . In SGD training, the training images are
augmented by randomly scaling (from 0.7 to 2.0), randomly
cropping (size of 769 × 769 pixels) and random ﬂipping
horizontally. In our experiments, the networks are trained
on 8 GPUs with 1 image per GPU for 60k iterations. The
“poly” learning rate policy is employed, where the initial
learning rate is set as 0.005 and multiplied by
Synchronized Batch Normalization is placed after every newly added layer with learnable weights. The weight
decay and the momentum parameters are set as 10−4 and
0.9, respectively.
Neural Machine Translation (NMT)
Model training is conducted on the standard WMT 2014
English-German dataset, consisting of about 4.5 million
sentence pairs. Sentences are encoded using byte-pair encoding , with a shared source-target vocabulary of about
37k tokens. Evaluation is on the English-to-German new-
(a) Attended Residual Block
(Deformable)
Convolution
Transformer attention
/ Dynamic convolution
Convolution
Convolution
Transformer attention
/ Dynamic convolution
Attention module
Transformer attention
/ Dynamic convolution
(b) Transformer / Dynamic convolution
Deformable
convolution
Transformer
(c) Transformer + Deformable
Transformer
Deformable
convolution
Attention module
Figure 2. Illustration of attention module conﬁgurations for empirical study. The modules in blue color are newly added to existing blocks.
stest2014 set. Accuracy is measured by the standard bilingual evaluation understudy (BLEU) scores .
The Transformer base model with relative position
encoding is utilized as the backbone. Experiments are
implemented based on the open source fairseq code
The hyper-parameters follows the original setting
in . We used the Adam optimizer with β1 = 0.9,
β2 = 0.98 and ϵ = 10−9. In our experiments, the networks
are trained on 8 GPUs for 100k iterations. Each training
batch contained a set of sentence pairs containing approximately 30k source tokens and 30k target tokens. The initial
learning rate is set as 10−7 and linearly increased to 0.001
after iterwarmup = 4000 iterations, and then multiplied by
iterwarmup
−0.5. No weight decay is adopted. During training,
label smoothing of value 0.1 is employed.
4.2. Effects of various attention-based modules
Disentanglement in Transformer attention
We ﬁrst seek to disentangle the effects of the four terms
in the Transformer attention module. This is achieved by
manually setting the {βTrans
j=1 values in Eq. (8) to control
the activation / deactivation of individual terms. The network is trained and tested for all 16 possible conﬁgurations
of {βTrans
j=1. In this set of experiments, no other attention
mechanisms are involved. Thus, for the object detection and
semantic segmentation tasks, the 3×3 convolution is of regular convolution in the network of Fig. 2 (a). For the NMT
task, the network architecture in Fig. 2 (b) is utilized. Transformer attention is used in the choices of “Transformer attention / Dynamic convolution” in Fig. 2 (a) and (b). Note
that for the NMT task, Transformer attention modules are
utilized for both self-attention and encoder-decoder attention. To reduce experimental complexity, the Transformer
attention modules in encoder-decoder attention are kept as
their full version (βTrans
= 1, j = 1, . . . , 4, abbreviated as
conﬁguration “1111” here) when we study self-attention.
Fig. 3 plots the accuracy-efﬁciency tradeoffs of different
j=1 conﬁgurations, where the accuracy-efﬁciency
envelopes are indicated by connected line segments. Note
that only the computational overheads from the Transformer
attention modules under study are counted here, without the
overheads from other parts of the network. From the plot,
we draw the following conclusions:
(1) In self-attention, the query-sensitive terms play a minor role compared to the query-irrelevant terms. Especially,
the query and key content term have a negligible effect
on accuracy, while being computationally heavy in image
recognition tasks. Overall, the accuracy gain brought by
the Transformer attention module is large (from the con-
ﬁguration where the Transformer attention module is removed (“w/o”) to that where the full version of Transformer attention is utilized (“1111”)). It can be seen that
the gain brought by the query-irrelevant terms (from con-
ﬁguration “w/o” to “0011”) is much larger than that brought
by the query-sensitive terms (from conﬁguration “0011” to
“1111”). Particularly, the performance gain brought by the
query and key content term (controlled by βTrans
) is negligible. Removing it (from conﬁguration “1111” to “0111”) incurs only a tiny drop in accuracy, while considerably reducing the computational overhead in image recognition tasks.
(2) In encoder-decoder attention, the query and key content term is vital. Deactivation of it (controlled by βTrans
) incurs a noticeable drop in accuracy, while only utilizing the
query and key content term (conﬁguration “1000”) delivers
accuracy almost the same as the full version (conﬁguration
“1111”). This is because the key step in NMT is to align the
words in the source and the target sentences. A traversal of
the query and key content is essential for such alignment.
(3) In self-attention, the attention factors of query content & relative position and the key content only are most
important.
The corresponding conﬁguration “0110” delivers accuracy very close to the full version (conﬁguration “1111”), while saving a considerable amount of computational overhead in image recognition tasks. It is also
worth noting that the key content only term, which captures saliency information, can effectively improve the per-
0010 + deformable
(a) Image Object detection on COCO
0010 + deformable
(b) Image Semantic Segmentation on Cityscapes
+ deformable
(c) Translation on newstest2014 (self-attention)
(d) Translation on newstest2014 (encoder-decoder attention)
Figure 3. Accuracy-efﬁciency tradeoffs of the four terms in Transformer attention (E1 for query and key content, E2 for query content
and relative position, E3 for key content only, and E4 for relative position only). The activation and deactivation of particular terms is
set by conﬁguration {βTrans
j=1 (e.g., “0011” denotes the activation of E3 and E4). Because the encoder-decoder attention mechanism is
indispensable for NMT, there is no “w/o” setting in (d). The results of some conﬁgurations overlap in the plots because they are of the
same accuracy and computational overhead. The key conﬁgurations under study are highlighted in red. The recommended conﬁguration
of “0010 + deformable” for self-attention in Tab. 2 is also plotted here.
1,2,3,4 →βTrans
1,2,3,4 + deformable
Object Detection (self-attention)
Semantic Segmentation (self-attention)
Neural Machine Translation (self-attention)
w/o →1111 + deformable
36.4 →41.0
213.7 →281.4
71.9 →77.8
449.5 →1112.1
20.9 →28.0
1111 →1011 + deformable
38.8 →41.0
281.4 →281.4
76.7 →77.8
1112.1 →1112.1
27.7 →28.0
1110 →1010 + deformable
38.8 →40.9
281.4 →281.2
76.7 →77.7
1112.1 →1111.2
27.7 →28.0
1101 →1001 + deformable
38.8 →41.0
281.4 →281.4
76.7 →77.8
1112.1 →1112.1
27.7 →28.0
1100 →1000 + deformable
38.8 →40.9
281.4 →281.2
76.7 →77.7
1112.1 →1111.2
27.7 →28.0
0111 →0011 + deformable
38.8 →41.0
253.6 →250.1
76.6 →77.5
814.0 →794.4
27.6 →27.7
0110 →0010 + deformable
38.8 →40.8
253.6 →221.1
76.6 →77.3
814.0 →489.5
27.6 →27.7
0101 →0001 + deformable
38.6 →40.7
251.1 →247.6
76.3 →77.3
800.7 →781.1
27.4 →27.6
0100 →w/o + deformable
38.6 →39.9
251.1 →213.7
76.3 →77.2
800.7 →449.5
27.4 →27.3
Table 2. Deformable convolution vs. E2 in Transformer attention, where both exploit query content and relative position information. The
underlined conﬁguration of “0010 + deformable” is recommended for an optimal accuracy-efﬁciency tradeoff.
formance with little additional overhead.
Our ﬁndings contradict the widespread belief that querysensitive terms, especially the query and key content term,
are crucial for the success of Transformer attention. The
experimental results suggest that this is only true for the
encoder-decoder attention scenario. In self-attention scenarios, the query and key content term is even removable.
Deformable convolution vs. E2 in Transformer attention
Here, we compare deformable convolution and the E2
term from Transformer attention in Eq. (2).
deformable convolution is designed for capturing selfattention, we restrict the experiments to self-attention scenarios only. Note that when deformable convolution is utilized in the NMT task, the network architecture is of “Transformer + Deformable” in Fig. 2 (c).
Tab. 2 compares deformable convolution and the E2 term
in a variety of settings. We ﬁnd that:
(1) For object detection and semantic segmentation, de-
1,2,3,4 →dynamic
Object Detection (self-attention)
Semantic Segmentation (self-attention)
Neural Machine Translation (self-attention)
0100 (nk = 31) →dynamic (nk = 31) 38.6 →37.9
229.4 →352.9
75.5 →74.2
523.3 →1029.0
27.4 →27.6
0100 (nk = 25) →dynamic (nk = 25) 38.6 →37.8
226.6 →306.8
75.5 →74.2
511.8 →840.4
27.4 →27.6
0100 (nk = 19) →dynamic (nk = 19) 38.6 →37.6
224.4 →270.6
75.4 →73.7
502.6 →692.1
27.4 →27.5
0100 (nk = 13) →dynamic (nk = 13) 38.5 →37.5
222.7 →244.3
74.4 →71.9
495.9 →584.3
27.3 →27.4
Table 3. Dynamic convolution vs. E2 in Transformer attention, where both exploit query content and relative position information. The
kernel size of dynamic convolution Nk is n2
k for image recognition and nk for NMT. The spatial range of Transformer attention is also
constrained to be the kernel size of dynamic convolution for ablation.
formable convolution considerably surpasses the E2 term in
both accuracy and efﬁciency. While for NMT, deformable
convolution is on par with the E2 term in both accuracy and
efﬁciency. In terms of efﬁciency, deformable convolution
does not need to traverse all the key elements. This advantage is obvious on images, where numerous pixels are
involved. In terms of accuracy, the bilinear sampling in deformable convolution is based on the hypothesis of local
linearity of feature maps. This hypothesis holds better on
images where local image content changes gradually, than
on languages where words change abruptly.
(2) The combination of deformable convolution and the
key content only term (“0010 + deformable”) delivers the
best accuracy-efﬁciency tradeoff. The accuracy is on par
with using deformable convolution and the whole attention module (“1111 + deformable”), while the overhead
is slightly higher than that of deformable convolution only
(“w/o + deformable”). This ﬁnding is in line with ﬁnding
(3) of “Disentanglement in Transformer attention”. It further suggests the importance of the query content & relative
position and key content only factors in self-attention. The
conﬁguration “0010 + deformable” is also plotted in Fig. 3.
Dynamic convolution vs. E2 in Transformer attention
We compare these two instantiations in self-attention
scenarios. The network architectures are of Fig. 2 (a) for
image recognition tasks, and of Fig. 2 (b) for NMT, where
either the Transformer attention with E2 only (conﬁguration
“0100”) or dynamic convolution is utilized.
Tab. 3 presents the results. We can ﬁnd that for NMT,
dynamic convolution achieves accuracy on par with the E2
term at reduced computational cost.
However, dynamic
convolution is not effective for object detection and semantic segmentation, delivering considerably lower accuracy.
To further study the inﬂuence of kernel size in dynamic convolution, we also constrain the spatial range of the E2 term
to be the same as that in dynamic convolution. The accuracy
drops as the spatial range shrinks for both dynamic convolution and the E2 term. But it is worth noting that the E2
term still surpasses dynamic convolution at the same spatial
range in image recognition tasks, with even smaller computational overhead. The inferior accuracy of dynamic convolution in image recognition tasks might be because dynamic
convolution is originally designed for NMT, and some design choices may not be suitable for image recognition.