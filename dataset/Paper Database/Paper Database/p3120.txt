Few-shot Learning via Saliency-guided Hallucination of Samples
Hongguang Zhang1,2
Jing Zhang1,2
Piotr Koniusz2,1
1Australian National University,
2Data61/CSIRO
ﬁrstname.lastname@{anu.edu.au1, data61.csiro.au2}
Learning new concepts from a few of samples is a standard challenge in computer vision. The main directions to
improve the learning ability of few-shot training models include (i) a robust similarity learning and (ii) generating or
hallucinating additional data from the limited existing samples. In this paper, we follow the latter direction and present
a novel data hallucination model. Currently, most datapoint generators contain a specialized network (i.e., GAN)
tasked with hallucinating new datapoints, thus requiring
large numbers of annotated data for their training in the
ﬁrst place. In this paper, we propose a novel less-costly
hallucination method for few-shot learning which utilizes
saliency maps. To this end, we employ a saliency network to
obtain the foregrounds and backgrounds of available image
samples and feed the resulting maps into a two-stream network to hallucinate datapoints directly in the feature space
from viable foreground-background combinations. To the
best of our knowledge, we are the ﬁrst to leverage saliency
maps for such a task and we demonstrate their usefulness
in hallucinating additional datapoints for few-shot learning. Our proposed network achieves the state of the art on
publicly available datasets.
1. Introduction
Convolutional Neural Networks (CNN) have demonstrated their usefulness in numerous computer vision tasks
e.g., image classiﬁcation and scene recognition. However,
training CNNs on these tasks requires large numbers of labeled data. In contrast to CNNs, human ability to learn
novel concepts from a few of samples remains unrivalled.
Inspired by this observation, researchers proposed the
one- and few-shot learning tasks with the goal of training
algorithms with low numbers of datapoints.
Recently, the concept of learning relations with deep
learning has been explored in several papers 
which can be viewed as a variant of metric learning adapted to the few-shot learning scenario. In these
works, a neural network extracts convolutional descriptors,
and another learning mechanism (e.g., a relation network)
Figure 1: Illustration of saliency-based data generation for oneshot case. The foreground objects are combined with different
backgrounds in attempt to reﬁne the classiﬁcation boundaries.
captures relationship between descriptors. Most papers in
this category propose improvements to relationship modeling for the purpose of similarity learning. In contrast, 
employs a separate Multilayer Perceptron (MLP) to hallucinate additional image descriptors by modeling foregroundbackground relationships in feature space to obtain implicitly augmented new samples. To train the feature generator, MLP uses manually labelled features clustered into 100
clusters, which highlights the need for extra labelling. Another approach generates data in a meta-learning scenario, which means the network has to be pre-trained on
several datasets, thus increasing the cost of training.
In this paper, we adopt the data hallucination strategy
and propose a saliency-guided data hallucination network
dubbed as Salient Network (SalNet). Figure 1 shows a simple motivation for our work. Compared with previous feature hallucinating approaches, we employ a readily available saliency network pre-trained on MSRA Salient
Object Database (MSRA-B) to segment foregrounds
and backgrounds from given images, followed by a twostream network which mixes foregrounds with backgrounds
(we call it the Mixing Network) in the feature space of an encoder (c.f. image space). As we obtain spatial feature maps
from this process, we embed mixed feature vectors into a
second-order representation which aggregates over the spatial dimension of feature maps. Then, we capture the similarity between ﬁnal co-occurrence descriptors of a so-called
training query sample and hallucinated support matrices via
 
a similarity-learning network. Moreover, we regularize our
mixing network to promote hallucination of realistically
blended foreground-background representations.
end, whenever a foreground-background pair is extracted
from the same image (c.f. two separate images), we constrain the resulting blended representation via the ℓ2-norm
to be close to a representation from a supervising network
which, by its design, is trained only on real foregroundbackground pairs (c.f. infeasible combinations). We refer to
this strategy as Real Representation Regularization (TriR).
Lastly, we propose the similarity-based strategies regarding
how to choose backgrounds for mixing with a given foreground. To this end, we perform either (i) intra-class mixing
(foregrounds/backgrounds of the same class) or (ii) interclass mixing (for any given foreground, we take its corresponding background, retrieve its nearest-neighbour backgrounds from various classes, and use the retrieval distance
to express the likelihood how valid the mixed pair is). Below, we list our contributions:
I. We propose a novel saliency-guided data hallucination
network for few-shot learning.
II. We investigate various hallucination strategies.
propose a simple but effective regularization and two
strategies to prevent substandard hallucinated samples.
III. We investigate the effects of different saliency map generators on the few-shot learning performance.
To the best of our knowledge, we are the ﬁrst to employ saliency maps for datapoints hallucination for few-shot
learning. Our experiments achieve the state of the art on two
challenging publicly available few-shot learning datasets.
2. Related Work
In what follows, we describe popular zero-, one- and
few-shot learning algorithms followed by the saliency detection methods and a discussion on second-order statistics.
2.1. Learning From Few Samples
For deep learning algorithms, the ability of “learning
quickly from only a few examples is deﬁnitely the desired
characteristic to emulate in any brain-like system” .
Learning from scarce data poses a challenge to typical
CNN-based classiﬁcation systems which have to learn
millions of parameters. Current trends in computer vision
highlight the need for “an ability of a system to recognize
and apply knowledge and skills learned in previous tasks to
novel tasks or new domains, which share some commonality”. This problem was introduced in 1901 under a notion
of “transfer of particle” and is closely related to zeroshot learning which can be deﬁned as an ability to generalize to unseen class categories from categories
seen during training. For one- and few-shot learning, some
“transfer of particle” is also a desired mechanism as generalizing from one or few datapoints to account for intra-class
variability of thousands images is a formidable task.
One- and Few-shot Learning has been studied widely in
computer vision in both shallow and
deep learning scenarios .
Early works propose generative models with an
iterative inference for transfer. In contrast, a recent Siamese
Network uses a two-stream convolutional neural network which performs simple metric learning.
Network introduces the concept of support set and Nway W-shot learning protocols. It captures the similarity
between one query and several support images, and also
implicitly performs metric learning. Prototypical Networks
 learn a model that computes distances between a datapoint and prototype representations of each class. Model-
Agnostic Meta-Learning (MAML) is a meta-learning
model which can be seen a form of transfer learning. Relation Net is similar to Matching Network , but uses
an additional network to learn similarity between images.
Second-order Similarity Network (SoSN) leverages
second-order descriptors and power normalization which
help infer rich relation statistics. SoSN descriptors are more
effective than the ﬁrst-order Relation Net .
Hallucination-based approaches and use descriptors manually assigned into 100 clusters to generate
plausible combinations of datapoints. Mixup network 
applies a convex combination of pairs of datapoints and labels. In contrast, we decompose images into foreground and
background representations via saliency maps and we propose several strategies for mixing foreground-background
pairs to hallucinate meaningful auxiliary training samples.
Zero-shot Learning can be implemented within few-shot
learning frameworks . Attribute Label Embedding (ALE) , Zero-shot Kernel Learning (ZSKL) 
all use so-called compatibility mapping (linear/non-linear)
and some form of regularization to associate feature vectors with attributes (class descriptors).
Recent methods
such as Feature Generating Networks and Model Selection Network hallucinate the training data for unseen classes via Generative Adversarial Networks (GAN).
2.2. Saliency Detection
A saliency detector highlights image regions containing
foreground objects which correlate with human visual attention, thus producing a dense likelihood saliency map which
assigns some relevance score in range to each pixel.
Conventional saliency detectors underperform on complex
scenes due to computations based on human-deﬁned priors
 . In contrast, deep saliency models outperform
conventional saliency detectors but they require laborious
pixel-wise labels. In this paper, we use saliency maps as
a guiding signal, thus we adopt a highly-efﬁcient weakly-
Saliency Net
(a) Saliency Net
Similarity Net
(b) Foreground-background Encoding and Mixing Net
Query Image
Support Set
(c) Similarity Net
Figure 2: Our pipeline consists of three units: (a) pre-trained Saliency Net, (b) Foreground-background Encoding and Mixing Net (FEMN),
and (c) Similarity Net. The FEMN block consists of two streams which take foreground/background images as inputs, respectively, and a
Mixing Net which combines foreground-background pairs via ⊕and reﬁnes them via a single-stream network prior to aggregation of the
resulting feature maps via the Second-order Encoder.
supervised deep convolutional saliency detector MNL .
We compare the performance of MNL with (i) RFCN ,
a fully-supervised deep model, and (ii) a cheap non-CNN
Robust Background Detector (RBD) , one of the best
unsupervised saliency detectors according to evaluation .
2.3. Second-order Statistics
Below we discuss brieﬂy the role of second-order statistics and related shallow and CNN-based approaches.
Second-order statistics have been studied in the context
of texture recognition via so-called Region Covariance Descriptors (RCD), often applied to semantic segmentation and object category recognition .
Second-order statistics have to deal with the so-called
burstiness which is “the property that a given visual element appears more times in an image than a statistically
independent model would predict” . Power Normalization , used with Bag-of-Words , was
shown to limit such a burstiness. A survey showed that
so-called MaxExp feat. pooling is in fact a detector of
“at least one particular visual word being present in an image”. MaxExp on second-order matrices was shown in 
to be in fact the Sigmoid function. Such a pooling also performed well in few-shot learning . Thus, we employ
second-order pooling with Sigmoid.
3. Approach
Our pipeline builds on the generic few-shot Relation Net
pipeline which learns implicitly a metric for so-called
query and support images. To this end, images are encoded
into feature vectors by an encoding network.
Then, socalled episodes with query and support images are formed.
Each query-support pair is forwarded to a so-called relation
network and a loss function to learn if a query-support pair
is of the same class (1) or not (0). However, such methods
suffer from scarce training data which we address below.
3.1. Network
Figure 2 presents a foreground-background two-stream
network which leverages saliency maps to isolate foreground and background image representations in order to
hallucinate additional training data to improve the few-shot
learning performance. The network consists of (i) Saliency
Net (SalNet) whose role is to generate foreground hypotheses, (ii) Foreground-background Encoding and Mixing Net
(FEMN) whose role is to combine foreground-background
image pairs into episodes, and the Similarity Net (SimNet)
which learns the similarity between query-support pairs.
To illustrate how our network works, consider an image I
which is passed through some saliency network h to extract
the corresponding saliency map h(I), the foreground F and
the background B of I, respectively:
FI = h(I) ⊙I,
BI = (1 −h(I)) ⊙I,
where ⊙is the Hadamart product. The feature encoding
network consists of two parts, f and g. For images I ∈
R3×M×M and J∈R3×M×M (I=J or I̸=J), we proceed by
encoding their foreground FI ∈R3×M×M and background
BJ ∈R3×M×M via feature encoder f : R3×M×M→RK×Z2,
where M×M denotes the spatial size of an image, K is the
feature size and Z2 refers to the vectorized spatial dimension of map of f of size Z×Z. Then, the encoded foreground
and background are mixed via summation and reﬁned in encoder g : RK×Z2→RK′×Z′2, where K′ is the feature size
and Z′2 corresponds to the vectorized spatial dimension of
map of g of size Z′×Z′. As in the SoSN approach , we
apply the outer-product on g(·) to obtain an auto-correlation
of features and we perform pooling via Sigmoid ψ to tackle
the burstiness in our representation. Thus, we have:
ΦIJ = g(f(FI) + f(BJ)),
RIJ = ψ(ΦIJΦT
where ψ is a zero-centered Sigmoid function with σ as the
parameter that controls the slope of its curve:
ψ(X, σ) = (1−e−σX)/(1+e−σX) = tanh(2σX).
Descriptors RII ∈RK′×K′ represent a given image I
while RIJ ∈RK′×K′ represent a combined foregroundbackground pair of images I and J. Subsequently, we form
the query-support pairs (e.g., we concatenate their representations) and we pass episodes to the similarity network. We
use the Mean Square Error (MSE) loss to train our network:
(r(Rsnw, Rq) −δ(lsnw −lq))2,
where snw chooses support images from I =I∗+I′, I∗and
I′are original and hallucinated images, q chooses the query
image, r is the similarity network, l is the label of an image,
N is the number of classes in an episode, W is the shot
number per support class, δ(0)=1 (0 elsewhere). Note that
Eq. (6) does not form foreground-background hallucinated
pairs per se. We describe this process in Section 3.3.
3.2. Saliency Map Generation
For brevity, we consider three approaches: deep supervised saliency approaches and an unsupervised
shallow method . In this paper, we use saliency maps as
a prior to generate foreground and background hypotheses.
In our main experiemnts, we use the deep weaklysupervised slaiency detector MNL due to its superior performance. Moreover, we investigate the deep supervised RFCN approach pre-trained on THUS10K
dataset , which has no intersection with our few-shot
learning datasets. We also investigate the cheap RBD model
 which performed best among unsupervised models .
Figure 3 shows saliency maps generated by the above
methods. In the top row, the foreground and background
have distinct textures. Thus, both conventional and deep
models isolate the foreground well. However, for the scenes
whose foreground/background share color and texture composition (bottom row), the unsupervised method fails to detect the correct foreground. As our dataset contains both
Figure 3: Saliency maps generated by different methods. For a
simple scene (top row), the all three methods are able to detect
the foreground. However, for a complex scene, the unsupervised
method fails to detect the salient object.
simple and complex scenes, the performance of our method
is somewhat dependent on the saliency detector e.g., results
based on RBD are expected to be worse in comparison
to RFCN and MNL . The performance of few-shot
learning combined with different saliency detectors will be
presented in Section 4.3. Firstly, we detail our strategies for
hallucinating additional training data for few-shot learning.
3.3. Data Hallucination
The additional datapoints are hallucinated by the summation of foreground and background feature vector pairs
obtained from the feature encoder f and reﬁned by the encoder g. Taking the N-way W-shot problem as example
(see Relation Net or SoSN for the detailed deﬁnition of such a protocol), we will randomly sample W images from each of N training classes. Let snw be the index
selecting the w-th image from the n-th class of an episode
and q be the index selecting the query image. Where required, assume the foreground and background descriptors
for images are extracted. Then, the following strategies for
the hallucination of auxiliary datapoints can be formulated.
Strategy I: Intra-class hallucination. For this strategy,
given an image index snw, a corresponding foreground is
only mixed with backgrounds of images from the same class
n. Thus, we can generate W −1 datapoints for every image. Figure 5 shows that the intra-class hallucination produces plausible new datapoints. Note that the image class
n typically correlates with foreground objects, and such objects appear on backgrounds which, statistically speaking,
if swapped, will produce plausible object-background combinations. However, the above strategy cannot work in oneshot setting as only one support image per class is given.
Although our intra-class hallucination presents a promising direction, our results will show that sometimes the performance may lie below baseline few-shot learning due to a
very simple mixing foreground-background strategy which
includes the foreground-background feature vector summa-
Foreground-background Encoding and Mixing Net
Similarity Net
Convolutional Layer
Max Pooling
Fully-connected Layer
Figure 4: The detailed architecture of Foreground-Background Encoding and Mixing Net and the Similarity Net. Best viewed in color.
Foreground 1
Foreground 2
Background 1
Background 2
Airplane A
Airplane B
Airplane A is taking oﬀ from the land
Airplane A is ﬂying in the sky
Airplane B is taking oﬀ from the land
Airplane B is ﬂying in the sky
Figure 5: The intra-class datapoint hallucination strategy: the majority of datapoints generated in this way are statistically plausible.
tion followed by the reﬁning encoder g. Such a strategy incurs possible noises from (i) the substandard saliency maps
and/or (ii) mixing incopatible foreground-background pairs.
Therefore, in order to further reﬁne the hallucinated datapoints, we propose to exploit foreground-background mixed
pairs Fsnw and Bsnw which come from the same image
(e.g., their mixing should produce the original image) and
enforce their feature vectors to be close in the ℓ2-norm sense
to some baseline teacher network which does not perform
hallucination. Speciﬁcally, we take Φ = g(Fsnw, Bsnw)
and encourage its proximity to some teacher representation
Φ∗=g∗({Fsnw, Bsnw}) where Fsnw+Bsnw=Isnw∈I∗:
g(f(Fsnw) + f(Bsnw)) −g∗({Fsnw, Bsnw})
s.t. Fsnw+Bsnw=Isnw∈I∗
L′ = L + βΩ,
where I∗is a set of orig. train. images, β adjusts the impact
of Ω, L′is the combined loss, and net. g∗is already trained.
We investigate g∗that encodes (i) the original images
only i.e., g∗(f(Inw)) or (ii) foreground-background pairs
from original images i.e., g∗(f(Fsnw)+f(Bsnw). We call Ω
as Real Representation Regularization (TriR). Our experiments will demonstrate that TriR improves the ﬁnal results.
Strategy II: Inter-class hallucination. For this strategy,
we allow mixing the foregrounds of support images with all
available backgrounds (between-class mixing is allowed) in
the support set. Compared to the intra-class generator, the
inter-class hallucination can generate W−1+W(N−1) new
datapoints. However, many foreground-background pairs
Foreground 2
Background 2
Giraﬀe is walking on the grassland
Giraﬀe is ﬂying in the sky
Airplane is taking oﬀ from grassland
Airplane is ﬂying in the sky
Background 1
Foreground 1
Figure 6: The inter-class datapoint hallucination may generate
impossible instances e.g., ‘giraffe in the sky’ is an unlikely concept
(except for a giraffe falling off a helicopter during transportation?).
will be statistically implausible, as shown in Figure 6, which
would cause the degradation of the classiﬁcation accuracy.
To eliminate the implausible foreground-background
pairs from the inter-class hallucination process, we design a
similarity prior which assigns probabilities to backgrounds
in terms of their compatibility with a given foreground.
Numerous similarity priors can be proposed e.g., one can
use the label information to specify some similarity between
two given classes. Intuitively, backgrounds between images
containing dogs and cats should be more correlated than
backgrounds of images of dogs and radios. However, modeling such relations explicitly may be cumbersome and it
has its shortcomings e.g., backgrounds of images containing
cars may also be suitable for rendering animals on the road
or sidewalk, despite of an apparent lack of correlation between say cat and car classes. Thus, we ignore class labels
and perform a background retrieval instead. Speciﬁcally,
once all backgrounds of support images are extracted, we
measure the distance between the background of a chosen
image of index snw and all other backgrounds to assign a
probability score of how similar two backgrounds are, thus:
d(Bsnw, Bsn′w′) =
f(Bsnw) −f(Bsn′w′)
p(Bsn′w′| Bsnw) =
2e−αd(Bsnw , Bsn′w′)
1 + e−αd(Bsnw , Bsn′w′) ,
where α is a hyper-parameter to control our probability pro-
ﬁle function p(d) shown in Figure 7: a Sigmoid reﬂected
along its y axis. We apply the proﬁle p to hallucinated outputs of g to obtain g′. We show this strategy in Figure 8 and
Figure 7: The probability proﬁle p w.r.t. the dist. d and various α.
Background 2
Giraﬀe is walking on the grassland
Giraﬀe is ﬂying in the sky
Giraﬀe is walking on the grassland
Airplane is sliding on the sea
Background 1
Foreground 1
Background 3
Probability:
Background 4
Probability:
Probability:
Probability:
Figure 8: The inter-class hallucination strategy with the similarity
prior. We assign likelihoods to generated datapoints based on the
similarity of a background of a given image to other backgrounds.
we call it as Soft Similarity Prior (SSP):
g′(Fsnw, Bsn′w′)=p(Bsn′w′| Bsnw) g(f(Fsnw), f(Bsn′w′)).
Also, we propose a Hard Similarity Prior (HSP) according to which we combine a given foreground with the most
relevant retrieved backgrounds whose p is above certain τ:
g′(Fsnw, Bsn′w′) =
if p(Bsn′w′|Bsnw) ≤τ,
g(f(Fsnw), f(Bsn′w′)),
otherwise.
We will show in our experiments that the use of priors
signiﬁcantly enhances the performance of the inter-class
hallucination, especially for the 1-shot protocol, to which
the intra-class hallucination is not applicable. We will show
in Section 4 that both HSP and SSP improve the performance of few-shot learning; SSP being a consistent performer on all protocols. Firstly, we detail datasets and then
we show the usefulness of our approach experimentally.
4. Experiments
Our network is evaluated in the few-shot learning scenario on the miniImagenet dataset and a recently proposed Open MIC dataset which was used for few-shot
learning by the SoSN approach . Our implementation
is based on PyTorch and models are trained on a Titan Xp
Table 1: Evaluations on the miniImagenet dataset. See 
for details of baselines. Note that intra-class hallucination has no
effect on one-shot learning, so the scores of without (w/o Hal.) and
with intra-class hallucination (Intra-class Hal.) on 1-shot are the
same. The astersik (*) denotes the ‘sanity check’ results on our
proposed pipeline given disabled both saliency segmentation and
hallucination (see the supp. material for details).
5-way Acc.
Matching Nets 
43.56 ± 0.84
55.31 ± 0.73
Meta Nets 
49.21 ± 0.96
Meta-Learn Nets 
43.44 ± 0.77
60.60 ± 0.71
Prototypical Net 
49.42 ± 0.78
68.20 ± 0.66
48.70 ± 1.84
63.11 ± 0.92
Relation Net 
51.36 ± 0.86
65.63 ± 0.72
52.96 ± 0.83
68.63 ± 0.68
SalNet w/o Sal. Seg. (*)
53.15 ± 0.87
68.87 ± 0.67
SalNet w/o Hal.
55.57 ± 0.86
70.35 ± 0.66
SalNet Intra. Hal.
71.78 ± 0.69
SalNet Inter. Hal.
57.45 ± 0.88
72.01 ± 0.67
GPU via the Adam solver. The architecture of our saliencyguided hallucination network is shown in Fig. 2 and 4. The
results are compared with several state-of-the-art methods
for one- and few-shot learning.
4.1. Datasets
Below, we describe our setup, datasets and evaluations.
miniImagenet consists of 60000 RGB images from
100 classes. We follow the standard protocol and use
80 classes for training (including 16 classes for validation)
and 20 classes for testing. All images are resized to 84×84
pixels for fair comparison with other methods. We also investigate larger sizes, e.g. 224×224, as our SalNet model
can use richer spatial information from larger images to obtain high-rank auto-correlation matrices without a need to
modify the similarity network to larger feature maps.
Open MIC is a recently proposed Open Museum Identiﬁcation Challenge (Open MIC) dataset which contains photos of various exhibits, e.g. paintings, timepieces,
sculptures, glassware, relics, science exhibits, natural history pieces, ceramics, pottery, tools and indigenous crafts,
captured from 10 museum exhibition spaces according to
which it is divided into 10 subproblems. In total, Open MIC
has 866 diverse classes and 1–20 images per class. The
within-class images undergo various geometric and photometric distortions as the data was captured with wearable
cameras. This makes Open MIC a perfect candidate for
testing one-shot learning algorithms. Following the setup
in SoSN , we combine (shn+hon+clv), (clk+gls+scl),
(sci+nat) and (shx+rlc) splits into subproblems p1, ..., p4.
We randomly select 4 out of 12 possible pairs in which subproblem x is used for training and y for testing (x→y).
Relation Net and SoSN are employed as baselines against which we compare our SalNet approach.
Table 2: Evaluations on the Open MIC dataset. p1: shn+hon+clv,
p2: clk+gls+scl, p3: sci+nat, p4: shx+rlc. Notation x→y means
training on exhibition x and testing on exhibition y.
N-way W-shot p1→p2 p2→p3 p3→p4 p4→p1
Relation Net 
Intra.-Hal.
Inter.-Hal.
Relation Net 
Intra.-Hal.
Inter.-Hal.
Relation Net 
Intra.-Hal.
Inter.-Hal.
Relation Net 
Intra.-Hal.
Inter.-Hal.
Relation Net 
Intra.-Hal.
Inter.-Hal.
Relation Net 
Intra.-Hal.
Inter.-Hal.
4.2. Experimental setup
For the miniImagenet dataset, we perform 1- to 10-shot
experiments in 5-way scenario to demonstrate the improvements obtained with our SalNet on different number of Wshot images. For every training and testing episode, we randomly select 5 and 3 query samples per class. We average
the ﬁnal results over 600 episodes. The initial learning rate
is set to 1e−3. We train the model with 200000 episodes.
For the Open MIC dataset, we select 4 out of 12 possible subproblems, that is p1→p2, p2→p3, p3→p4, and
p4→p1. Firstly, we apply the mean extraction on patch images (Open MIC provides three large crops per image) and
resize them to 84×84 pixels. As some classes of Open MIC
contain less than 3 images, we apply 5-way 1-shot to 3-shot
learning protocol. During training, to form an episode, we
randomly select 1–3 patch images for the support set and
another 2 patch images for the query set for each class. During testing, we use the same number of support and query
samples in every episode and we average the accuracy over
1000 episodes for the ﬁnal score. The initial learning rate is
set to 1e−4. The models are trained with 50000 episodes.
4.3. Results
For miniImagenet dataset, Table 6 shows that our proposed SalNet outperforms all other state-of-the-art methods
on standard 5-way 1- and 5-shot protocols. Compared with
current state-of-the-art methods, our SalNet Inter-class Hal.
model achieves ∼4.4% and ∼3.3% higher top-1 accuracy
than SoSN on 1- and 5-shot protocols, respectively, while
our SalNet Intra-class Hal. yields improvements of ∼2.5%
and ∼3.1% accuracy over SoSN.
Top-1 Accuracy
Shot Number
Relation Net
Intra-class Hal.
Inter-class Hal.
Way Number
Relation Net
Inter-class Hal.
Figure 9: The accuracy as a function of (left) W-shot (5-way) and
(right) N-way (5-shot) numbers on miniImagenet given different
methods. Our models improve results over all baselines.
Top-1 Accuracy
Figure 10: The accuracy on miniImagenet as a function of (a) β
of TriR from Eq. (7) (5-shot 5-way) and (b) α of SSP from Eq.
(10) (1-shot 5-way).
Table 2 presents results on Open MIC. The improvements of SalNet Inter-class Hal. and SalNet Intra-class Hal.
on this dataset are consistent with miniImagenet. However,
the improvements on some splits are small (i.e., ∼1.1%)
due to the difﬁculty of these splits e.g., jewellery, fossils,
complex non-local engine installations or semi-transparent
exhibits captured with wearable cameras cannot be easily
segmented out by saliency detectors.
Ablation study. The network proposed in our paper builds
on the baseline framework . However, we have added
several non-trivial units/sub-networks to accomplish our
goal of the datapoint hallucination in the feature space.
Thus, we perform additional experiments to show that the
achieved accuracy gains stem from our contributions. We
also break down the accuracy w.r.t. various components.
Firstly, Table 6 shows that if the saliency segmentation
and data hallucination are disabled in our pipeline (SalNet
w/o Sal. Seg.), the performance on all protocols drops down
to the baseline level of SoSN.
Moreover, we observe that SalNet outperforms SoSN
even if we segment images into foregrounds and backgrounds and pass them via our network without the use of
hallucinated datapoints (SalNet w/o Hal.). We assert that
such improvements stem from the ability of the saliency detector to localize main objects in images. This is a form of
spatial knowledge transfer which helps our network capture
the similarity between query and support images better.
Figure 9 (a) shows the accuracy of our (SalNet Intraclass Hal.) model on miniImagenet for 5-shot 5-way case
Intra-class Hal.
Inter-class Hal.
Figure 11: The results on RBD , RFCN and MNL 
saliency methods for miniImagenet.
as a function of the parameter β of our regularization loss
TriR. We observe that for β = 0.01 we gain ∼1% accuracy
over β = 0 (TriR disabled). Importantly, the gain remains
stable over a large range 0.005 ≤β ≤0.5. Table 3 veriﬁes
further the usefulness of our TriR regularization in combination with the intra- and inter-class hallucination SalNet
(Intra.-Hal.+TriR) and (Inter.-Hal.+TriR) with gains up to
1.6% and 1.5% accuracy on miniImagenet. We conclude
that TriR helps our end-to-end training by forcing encoder
g to mimic teacher g∗for real foreground-background pairs
(g∗is trained on such pairs only to act as a reliable superv.).
Figure 9 (b) shows the accuracy of our (SalNet Interclass Hal.) model on miniImagenet for 1-shot 5-way as
a function of the Soft Similarity Prior (SSP). The maximum observed gain in accuracy is ∼3.3%. Table 3 further compares the hard and soft priors (SalNet Inter-class
Hal.+HSP) and (SalNet Inter-class Hal.+SSP) with SSP
outperforming HSP by up to ∼2.2%.
Lastly, Figure 11 compares several saliency methods in
terms of few-shot learning accuracy. The complex saliency
methods perform equally well.
However, the use of the
RBD approach results in a signiﬁcant performance loss
due to its numerous failures e.g., see Figure 3.
Saliency Map Dilation. As backgrounds extracted via a
saliency detector contain ‘cut out’ silhouettes, they unintentionally carry some foreground information. Figure 12
suggests that if we apply the Gaussian blur and a threshold
over the masks to eliminate the silhouette shapes, we can
prevent mixing the primary foreground with a foreground
corresponding to silhouettes. Table 4 shows that pairing
each foreground with background images whose silhouettes
Figure 12: Gradual dilation of the foreground mask.
5-way 1-shot 5-way 5-shot
Intra-class Hal.
55.57 ± 0.86 71.78 ± 0.69
Intra-class Hal.+Dilation 56.67 ± 0.85 72.15 ± 0.68
Table 4: Results for dilating contours of silhouettes.
were removed by dilating according to two different radii
(Dilation) leads to further improvements due to doubling of
possible within-class combinations for (Intra-class Hal.).
5. Conclusions
In this paper, we have presented two novel light-weight
data hallucination strategies for few-shot learning. in contrast to other costly hallucination methods based on GANs,
we have leveraged the readily available saliency network
to obtain foreground-background pairs on which we trained
our SalNet network in end-to-end manner. To cope with
noises of saliency maps, we have proposed a Real Representation Regularization (TriR) which regularizes our network with viable solutions for real foreground-background
pairs. To alleviate performance loss caused by implausible foreground-background hypotheses, we have proposed
a similarity-based priors effectively reduced the inﬂuence of
incorrect hypotheses. For future work, we will investigate
a self-supervised attention module for similarity perception
and study relaxations of saliency segmentation methods.
Acknowledgements.
This research is supported by
Scholarship
201603170283).
Computing, NVIDIA (GPU grant) and National University
of Defense Technology for their support.
Table 3: 5-way evaluations on the miniImagenet dataset for different N-shot numbers. Refer to for details of baselines.
5-way Accuracy
Relation Net 
51.4 ±0.7 56.7 ±0.8 60.6 ±0.8 63.3 ±0.7 65.6 ±0.7 66.9 ±0.7 67.7 ±0.7 68.6 ±0.6 69.1 ±0.6 69.3 ±0.6
53.0 ±0.8 60.8 ±0.8 64.5 ±0.8 67.1 ±0.7 68.6 ±0.7 70.3 ±0.7 71.5 ±0.6 72.0 ±0.6 72.3 ±0.6 73.4 ±0.6
w/o Sal. Seg.
53.1 ±0.9 60.9 ±0.8 64.7 ±0.8 67.3 ±0.7 68.9 ±0.7 70.6 ±0.7 71.7 ±0.6 72.1 ±0.6 72.6 ±0.6 73.6 ±0.6
55.6 ±0.9 63.5 ±0.8 66.2 ±0.8 68.2 ±0.7 70.4 ±0.7 71.2 ±0.7 72.2 ±0.7 73.2 ±0.6 74.0 ±0.6 74.6 ±0.6
Intra.-Hal.
55.6 ±0.9 63.1 ±0.8 65.9 ±0.7 68.7 ±0.7 70.8 ±0.7 71.8 ±0.7 73.6 ±0.6 73.8 ±0.6 74.1 ±0.6 75.2 ±0.6
Intra.-Hal.+TriR
55.6 ±0.9 64.5 ±0.8 67.5 ±0.7 70.3 ±0.7 71.8 ±0.7 72.8 ±0.7 74.1 ±0.6 74.4 ±0.6 74.7 ±0.6 75.7 ±0.6
Inter.-Hal.
53.7 ±0.9 58.9 ±0.8 62.4 ±0.8 65.2 ±0.7 67.7 ±0.7 68.5 ±0.7 69.6 ±0.7 69.9 ±0.6 70.6 ±0.6 71.1 ±0.6
Inter.-Hal.+TriR
54.1 ±0.9 60.1 ±0.8 63.4 ±0.7 65.8 ±0.7 67.9 ±0.7 69.6 ±0.7 70.5 ±0.6 71.0 ±0.7 72.1 ±0.6 72.5 ±0.7
Inter.-Hal.+TriR+HSP 56.4 ±0.9 63.0 ±0.8 67.3 ±0.8 69.2 ±0.7 71.0 ±0.6 71.8 ±0.7 72.1 ±0.6 73.0 ±0.6 74.2 ±0.6 75.4 ±0.6
Inter.-Hal.+TriR+SSP 57.5 ±0.9 64.8 ±0.8 67.9 ±0.8 70.5 ±0.7 72.0 ±0.7 73.2 ±0.7 74.3 ±0.6 74.6 ±0.6 75.2 ±0.6 76.1 ±0.6
Appendices
A. Saliency Maps on the Open MIC dataset
In Figure 13, we present saliency maps for some exhibit
instances from the Open MIC dataset. Many exhibits can be
ﬁltered out reliably. However, saliency maps for composite scenes containing numerous exhibits are the ones most
likely to fail. In the future, we will investigate how to improve the use of such unreliable saliency maps for such exhibits. Note that our results on exhibitions containing such
composite scences still beneﬁt from our approach–our mixing network can reduce the noise from saliency maps.
Figure 13: Examples of saliency maps on the Open MIC dataset.
The MNL detector was used.
B. Evaluations for 224×224 pixel images
We employ 84 × 84 image in our experiments for fair
comparison with other state-of-the-art methods presented in
our paper. However, it is easy to use large size images in
our network without its modiﬁcations due to the ability of
second-order representations to aggregate variable number
of feature vectors into a ﬁxed-size matrix (our relationship
descriptors are stacked matrices). Here we apply 224 × 224
image to demonstrate the beneﬁts from larger image size.
C. Network Architecture of Our Baseline Models and Additional Experiments for TriR
Below we present the diagrams of two baseline networks
used in our paper. The baseline 1 in Figure 14 is the original
pipeline ’w/o Sal. Seg.’, which is trained without saliency
Table 5: Accuracy on the miniImagenet dataset given different
size of images. See for details of baselines. The astersik (*) denotes the ‘sanity check’ results on our proposed pipeline
given disabled both saliency segmentation and hallucination.
5-way Acc.
Matching Nets 
43.56 ± 0.84
55.31 ± 0.73
Meta Nets 
49.21 ± 0.96
Meta-Learn Nets 
43.44 ± 0.77
60.60 ± 0.71
Prototypical Net 
49.42 ± 0.78
68.20 ± 0.66
48.70 ± 1.84
63.11 ± 0.92
Relation Net 
51.36 ± 0.86
65.63 ± 0.72
52.96 ± 0.83
68.63 ± 0.68
SalNet w/o Sal. Seg. (*)
53.15 ± 0.87
68.87 ± 0.67
SalNet w/o Hal.
55.57 ± 0.86
70.35 ± 0.66
SalNet Intra-class Hal.
71.78 ± 0.69
SalNet Inter-class Hal.
57.45 ± 0.88
72.01 ± 0.67
59.22 ± 0.91
73.24 ± 0.69
SalNet w/o Sal. Seg. (*)
60.36 ± 0.86
74.34 ± 0.67
SalNet w/o Hal.
62.22 ± 0.87
76.86 ± 0.65
SalNet Intra-class Hal.
77.95 ± 0.65
SalNet Inter-class Hal.
63.88 ± 0.86
78.34 ± 0.63
Table 6: Evaluations on the miniImagenet dataset given different
teacher networks for the TriR regularization.
5-way Acc.
baseline 1 (opt. (i)) as a teacher network in TriR
SalNet Intra-class Hal.
56.11 ± 0.88 71.56 ± 0.67
SalNet Inter-class Hal.
57.24 ± 0.94 72.49 ± 0.65
baseline 2 (opt. (ii)) as a teacher network in TriR
SalNet Intra-class Hal.
55.57 ± 0.86 71.78 ± 0.69
SalNet Inter-class Hal.
57.45 ± 0.88 72.01 ± 0.67
segmentation or data hallucination – it is very similar to the
SoSN pipeline . Figure 15 demonstrates the baseline 2
’w/o Hal.’, which employs saliency network to segment the
foregrounds and backgrounds but does not hallucinate the
data (no mixing of a foreground with numerous different
backgrounds is allowed).
In our paper, the reported results are obtained by using
baseline 2 ’w/o Hal.’ pipeline as teacher in TriR regularization (option (ii) in line 513 of our main submission). However, for completeness, we also investigate baseline 1 ’w/o
Hal.’ pipeline as teacher in TriR regularization (option (i) in
line 512 of our main submission). Table 6 shows that both
TriR teachers perform similarly to each other.