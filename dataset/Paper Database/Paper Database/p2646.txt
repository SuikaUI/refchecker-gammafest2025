Contents lists available at ScienceDirect
Earth-Science Reviews
journal homepage: www.elsevier.com/locate/earscirev
Trend analysis of climate time series: A review of methods
Manfred Mudelsee
Alfred Wegener Institute Helmholtz Centre for Polar and Marine Research, Bussestrasse 24, 27570 Bremerhaven, Germany
Climate Risk Analysis, Kreuzstrasse 27, Heckenbeck, 37581 Bad Gandersheim, Germany
A R T I C L E I N F O
Bootstrap resampling
Global surface temperature
Instrumental period
Linear regression
Nonparametric regression
Statistical change-point model
A B S T R A C T
The increasing trend curve of global surface temperature against time since the 19th century is the icon for the
considerable influence humans have on the climate since the industrialization. The discourse about the curve has
spread from climate science to the public and political arenas in the 1990s and may be characterized by terms
such as “hockey stick” or “global warming hiatus”. Despite its discussion in the public and the searches for the
impact of the warming in climate science, it is statistical science that puts numbers to the warming. Statistics has
developed methods to quantify the warming trend and detect change points. Statistics serves to place error bars
and other measures of uncertainty to the estimated trend parameters. Uncertainties are ubiquitous in all natural
and life sciences, and error bars are an indispensable guide for the interpretation of any estimated curve—to
assess, for example, whether global temperature really made a pause after the year 1998.
Statistical trend estimation methods are well developed and include not only linear curves, but also changepoints, accelerated increases, other nonlinear behavior, and nonparametric descriptions. State-of-the-art, computing-intensive simulation algorithms take into account the peculiar aspects of climate data, namely non-
Gaussian distributional shape and autocorrelation. The reliability of such computer age statistical methods has
been testified by Monte Carlo simulation methods using artificial data.
The application of the state-of-the-art statistical methods to the GISTEMP time series of global surface temperature reveals an accelerated warming since the year 1974. It shows that a relative peak in warming for the
years around World War II may not be a real feature but a product of inferior data quality for that time interval.
Statistics also reveals that there is no basis to infer a global warming hiatus after the year 1998. The post-1998
hiatus only seems to exist, hidden behind large error bars, when considering data up to the year 2013. If the fit
interval is extended to the year 2017, there is no significant hiatus. The researcher has the power to select the fit
interval, which allows her or him to suppress certain fit solutions and favor other solutions. Power necessitates
responsibility. The recommendation therefore is that interval selection should be objective and oriented on
general principles. The application of statistical methods to data has also a moral aspect.
1. Introduction
A univariate time series is a sample of data values in dependence on
time, where for each element of a set of time points, t(i), there exists one
corresponding data point, x(i). The time values are assumed to increase
strictly monotonically with the counter, i, that means, t(1) < t(2) < t
(3), and so forth. The data size or sample size, n, is the number of
time–data pairs. The compact notation for a time series sample is {t(i),x
n. The spacing of a time series is defined as d(i) = t(i) −t(i −1).
An evenly spaced time series has a constant spacing. The methods
presented in this review can be applied to evenly and unevenly spaced
time series. The average spacing of a time series, which is given by
1), is also called time resolution. Although the
methods explained in this review are presented for univariate series, it
is principally possible to extend them to multivariate time series, where
several data points, x(i), y(i), z(i), and so forth, are available at time t(i).
The GISTEMP time series (Fig. 1) is a reconstruction of global surface temperature based on land and ocean data. The x-values are the
temperature anomalies relative to the 1951–1980 mean in units of
degrees Celsius. The t-values are the years from 1880 to 2017. This is an
evenly spaced series of size n = 138, and the time resolution is 1 year.
The method of calculation of GISTEMP from a number of records from
globally distributed measurement stations is described by Hansen et al.
The data generating system considered in this article is the climate.
It is the climate system that is in the center of interest of climate re 
Received 19 June 2018; Received in revised form 17 October 2018; Accepted 6 December 2018
E-mail address: .
Earth-Science Reviews 190 310–322
Available online 11 December 2018
0012-8252/ © 2018 The Author. Published by Elsevier B.V. This is an open access article under the CC BY license
( 
searchers, not a particular time series. The time series serves only to
learn about the climate by means of statistical analysis of the time series
data. The target of the learning procedure considered in this article is
the trend, which is, loosely speaking, the long-term systematic change
of the mean value over time. This concept is specified in Section 2. The
act of learning is called estimation. It is part of applied statistical science. Since the x-values are affected by uncertainties from the measurements, and since the data size is less than infinity, an estimation is
never exact. Therefore, an uncertainty measure has to accompany an
estimation result to allow the assessment of the accuracy of the estimation result. Estimates without error bars are useless. Statistical science has developed methods for trend estimation and uncertainty determination, which support climate science. This review explains those
statistical methods in detail and at a level that is accessible to nonexperts. It is based on the book by the author . It gives
a brief account of the historical development of the methods. As an
illustrative example, we study temperature trends for the instrumental
period (Fig. 1). The explained methods can evidently be applied to any
type of climate time series, also from the deep past.
2. Climate
Climate is a paradigm of a complex system, which comprises the
atmosphere, the hydrosphere, the cryosphere, the lithosphere, the
biosphere, and the interactions among these compartments . For the analysis of climate data, it is useful to employ
conceptualizations—mathematical equations—which reduce the complexity. This goes via the introduction of statistical random variables, X,
which are representing a climate variable (e.g., temperature) with not
exactly known value. The random variables are concatenated over time
to form a stochastic process, X(i). Note the distinction between the
process, written capitalized, and the non-capitalized numerical value, x
(i). This distinction stems from statistical science Brockwell and Davis,
1991). One simple climate equation is
Eq. (1) decomposes climate into a trend and a noise component. The
noise component has mean zero; E[Xnoise(i)] = 0 for i = 1, …, n, where
E is the expectation operator. In other words, the center of location of
the distribution of Xnoise(i) is zero. The center of location for the climate
variable, X(i), is, hence, described by the time-dependent component
Xtrend(i), the trend.
[Xnoise(i)] = 1 for i = 1, …, n, where STD is the standard deviation
operator. In other words, the spread of the distribution of Xnoise(i) is
unity. The spread for the climate variable, X(i), around the trend,
Xtrend(i), is, hence, described by the time-dependent scaling function S
(i), the variability. Whereas X(i) and Xtrend(i) have physical units (e.g.,
degrees Celsius), Xnoise(i) has not. The units are being brought into the
noise component via the function S(i).
Eq. (1) is a mathematical representation of the definition of climate
in terms of mean and variability. This definition was developed around
the end of the 19th century by Austrian, German and Russian–German
researchers . Today, at the
beginning of the 21st century, it seems appropriate to extend the definition to a full statistical description of the climate system —to one that includes not only the first statistical moment
(expectation) or the second (standard deviation), but also higher orders
and extremes. A climate equation that includes also an extreme component was presented by Mudelsee .
The noise component, Xnoise(i), has a distribution with mean zero
and standard deviation unity, but the full shape of the distribution is
not prescribed. Climate variables often show noise distributions that
differ in shape from a normal or Gaussian, bell-shaped form. The noise
component for climate variables often exhibits autocorrelation: if
Xnoise(i) is positive, then Xnoise(i + 1) is likely also positive. This
“memorizing ability” of climate may act on many timescales. It is also
called persistence or serial dependence. These two climate noise ingredients, non-Gaussian shape and autocorrelation, have to be taken
into account in the estimation and uncertainty determination. This can
be done by suitable statistical techniques (Section 3). If they are are
ignored, then there is a risk of bias and overstatements stemming from
too small uncertainties.
Eq. (1) has the noise term added. It may be an interesting option to
describe climate by means of multiplicative noise, but this seems not
yet to have been seriously tried.
3. Regression
Regression is a statistical method to estimate the trend component,
Xtrend(i), from the climate Eq. (1). It can also be applied to a climate
equation with an extreme component . Required is a
statistical regression model of the trend. The input to the regression
method is the time series, {t(i),x(i)}i=1
n. If the statistical model employs
parameters, then the output is in the form of estimated parameter values with uncertainty measure. This review article considers the simple
linear model (Section 3.1) and more complex nonlinear models (Section
3.2). It is also possible to not specify a parametric model and instead
estimate Xtrend(i) by means of smoothing techniques (Section 3.3).
3.1. Linear regression
The linear regression describes Xtrend(i) by means of two parameters,
namely the intercept, β0, and the slope, β1. The model is “on the process
level” given by
T(i) is the time variable assigned to X(i).
The ordinary least-squares (OLS) estimation minimizes the sum of
squares of differences between data and the linear fit. It is “on the
sample level” given by
The estimates, denoted with a “hat” as
1, can be calculated
Temperature Anomaly (°C)
Fig. 1. Global surface temperature time series. Shown against time is the
temperature anomaly relative to the 1951–1980 mean. The series is called
GISTEMP after its producer, the NASA Goddard Institute for Space Studies, New
York, NY, United States of America. 
M. Mudelsee
Earth-Science Reviews 190 310–322
by setting the first derivatives of SSQ(β0,β1) with respect to β0 and β1
equal to zero. This yields two linear equations—called normal equations—for
analytically
“Analytical” means that the solution consists of two simple formulas,
It can be shown that the second derivatives of SSQ(β0,β1) at the
solution point (
1 are positive, which means that the extremum is a
There exist analytical, “classical” formulas for the uncertainty
measures for
1. However, these formulas are based on the assumptions
distributional
autocorrelation—a quite unrealistic situation for climate data (Section 2). A
superior method is moving block bootstrap (MBB) resampling Künsch,
1989). A description of the MBB for the application to linear OLS regression is given in Appendix A.
Fig. 2a shows the linear regression model fitted by means of OLS to
the GISTEMP time series. The intercept estimate with MBB standard
The slope estimate with MBB standard error is
0.0072 C/a
0.0008 C/a.
Fig. 2b shows the linear regression residuals (Eq. A.1) against time.
It appears obvious to the eye that there is still structure in the residuals;
this structure consists of (1) a long-term trend and (2) systematic
shorter-term deviations, such as the positive residuals at around the
year 1940.
In other words, the linear model is not well suited to describe the
trend for the GISTEMP time series. The intercept and slope estimates
should therefore be interpreted with caution (see Section 4).
Fig. 3a shows a histogram of the linear regression residuals for the
GISTEMP time series. It appears that there exists a mild right-skewness,
a deviation from the unskewed Gaussian shape.
Fig. 3b shows a lag-1 scatterplot of the linear regression residuals. It
appears obvious that there is autocorrelation because the cloud of
points is oriented along the 1:1 line. The estimated persistence time
 on the residuals is
Fig. 3 demonstrates on the GISTEMP time series that the assumptions of Gaussian shape and absent autocorrelation are violated.
Therefore, it is important to employ the MBB (or other bootstrap variants) for the calculation of uncertainty measures (see Section 4).
Efron collected and synthesized earlier works by
others and presented ordinary (i.e., pointwise or block length unity)
bootstrap resampling. He showed theoretically on simple estimation
problems—including Gaussianity and absent autocorrelation—the correctness of the bootstrap. In other words, he showed that resampling
yields the same values for the uncertainty measures as the classical
approach, which is based on assumptions such as the Gaussian. The
advantage of the ordinary bootstrap is that it yields reliable uncertainty
measures also for situations where the distributional shape is unknown
and may deviate from a Gaussian. The bootstrap “lets the data speak for
themselves.” The reason of the bootstrap's success is that resampling
from the residuals preserves the distributional shape of the noise
component. Singh soon pointed out that the ordinary bootstrap
fails for autocorrelated noise components because it does not preserve
the autocorrelation. Künsch presented the MBB (Fig. 4) as a
method to preserve autocorrelation (over the length of a block). Efron
and Tibshirani and Hall pursued the construction of
confidence intervals from bootstrap replications. Efron and Tibshirani
 wrote a textbook on the bootstrap. Efron and Hastie 
showed how the bootstrap is embedded into the modern computer age
statistical methodology.
The validity of the MBB can be tested by means of Monte Carlo
simulation experiments, which generate articifical data from stochastic
processes with known properties. One such test is about the coverage of
confidence intervals. For example, what coverage does a 95% confidence interval (for a parameter estimate, say,
1) achieve. In other
words, what is the fraction of simulations for which the confidence
interval does contain the prescribed, and hence known, value (for β1).
There exist other autocorrelation-preserving bootstrap methods than
mentioned, and there exists a variety of confidence interval construction methods. Mudelsee gives an overview of those methodical
aspects, presents own Monte Carlo tests, and shows applications to
climate time series.
The previous paragraphs have addressed the computational aspect
of linear regression. Another, key aspect is the suitability of the linear
model (Eq. 2). For climatological applications, this means the question
whether a linear increase (or decrease) is not too simple for describing
the trend component. Model suitability can be evaluated graphically via
various types of plots of the regression residuals (Eq. A.1). These realizations of the noise process should nominally not exhibit more
structure than the assumed AR(1) autocorrelation model (Montgomery
Linear Fit Residuals, r(i) (°C)
Temperature Anomaly (°C)
Fig. 2. GISTEMP, linear OLS regression. (a) Time series and linear fit (solid
line); (b) residuals against time.
M. Mudelsee
Earth-Science Reviews 190 310–322
and Peck, 1992) (see Section 4). If the linear model is found too simple,
then nonlinear models may lead to further insights (Section 3.2).
There are more estimation procedures for the linear model than
minimizing the OLS sum (Eq. 3). Weighted least-squares (WLS) employs
a weighting and minimizes the sum
( )] / ( ) .
The idea is that points, i, with smaller variability, S(i), contribute
heavier to the calculation of the regression parameter estimates. This
may lead to smaller classical standard errors than from using OLS . The challenge with WLS is that S(i) is usually
unknown and has to be estimated. For example, Mudelsee and Raymo
 fitted ramp models to S(i) per eye for data documenting the
glaciation of the Northern Hemisphere in the Pliocene. Generalized
least-squares (GLS) employs another sum of squares, which includes
Linear Fit Residuals, r(i) (°C)
Linear Fit Residuals, r(i
Linear Fit Residuals, r(i) (°C)
Fig. 3. GISTEMP, linear OLS regression, residuals. (a) Histogram; the number of classes follows the rule by Scott . (b) Lag-1 scatterplot (filled circles) with 1:1
line (solid line).
M. Mudelsee
Earth-Science Reviews 190 310–322
also the autocorrelation structure . Also
GLS may lead to smaller classical standard errors than OLS or WLS . In his book, Mudelsee calculated
standard-error ratios (GLS over OLS) to quantify this effect in dependence on the strength of the (equivalent) autocorrelation coefficient,
finding modest improvements of GLS over OLS. The challenge with GLS
is that not only S(i), but also the autocorrelation parameter is usually
unknown and has to be estimated. The WLS and GLS challenges can be
met by means of iterative procedures . These
are based on making initial guesses of variability (and autocorrelation),
performing a first estimation, updating the variability (and autocorrelation) estimates via usage of the regression residuals, and repeating the estimation. This procedure is iterated until parameter estimates and least-squares sum do not change significantly (measured by
means of the accuracy of the representation of real numbers in the
computer).
There are more autocorrelation models than AR(1), although it is a
matter of debate how useful they are in climatological practice. An
argument for usage of the simple AR(1) model is that the complexity
can be put into the other terms of the climate equations, such as nonlinear trend components (Section 3.2) or an extreme component. Another argument is that for usage in MBB resampling, only rough
knowledge about the AR(1) autocorrelation parameter is required for
selecting the block length (Eq. A.5). That means, the AR(1) parameter
should in many situations capture a good portion of the autocorrelation
structure and deliver reliable uncertainty measures from MBB resampling. The AR(1) model has an exponentially (i.e., fast) decaying autocorrelation function . It
is hence called a short-memory model. In case of long-memory autocorrelation , where
the autocorrelation function decays hyperbolically (i.e., slowly), the
MBB may have to be replaced by a subsampling procedure to yield
reliable uncertainty measures . Subsampling means that
the resample has fewer points than the original sample and that only
one random block is drawn . The difficulty here is to
adequately set the subsampling length . The book by
Mudelsee presents a Monte Carlo experiment on
subsampling block length selection in linear OLS regression. To summarize, the simple AR(1) autocorrelation model has a good empirical
and theoretical justification in climatology, and uncertainty measures
from MBB resampling based on the AR(1) model (Eq. A.5) should be
reliable. There may be situations, however, where long-memory models
should be considered as alternatives, but usage of those has then to be
justified on basis of a climatological theory. One example is river
runoff, for which a hydrological explanation of long memory (i.e., the
Hurst phenomenon) via spatial aggregation of AR(1) components in a
river network was given by Mudelsee .
Paleoclimate time series, {t(i),x(i)}i=1
n, are mostly obtained via the
measurement of proxy variables for the climate on natural archives,
such as marine sediment cores. (Another archive of paleoclimate is
formed by historical documents.) The timescale construction is based
on absolute dating of fixpoints in the archive and a statistical regression
model for the accumulation of the archive. A certainly not exhaustive
list of early works on timescale construction is the following: Bennett
 ; Agrinier et al. ; Bennett and Fuller ; Buck and
Millard ; Drysdale et al. ; Blaauw and Christen ;
Heegaard et al. ; Spötl et al. ; Haslett and Parnell ;
Blaauw ; Klauenberg et al. ; Scholz and Hoffmann ;
Blaauw and Heegaard ; Fohlmeister ; Hendy et al. ;
Hercman and Pawlak . Absolute dating inevitably shows measurement errors, and the accumulation model as well is subject to
systematic and random errors. These error influences can be captured
by means of a statistical accumulation model, which is able to generate
simulated timescales, t∗(i). The simulated timescales, in turn, are fused
into the resample, {t∗(i),x∗(i)}i=1
n, which is used for generating the
bootstrap replications (Step 8). The topic of uncertain timescales has
not been a focus of statisticians in the past, and the methodology of
quantitatively
uncertainties
 is, at the time of writing , not very far
3.2. Nonlinear regression
Climate is a complex system, and usually the trend component is
better described by a nonlinear than a linear regression function. Fig. 5
shows a selection of useful nonlinear functions.
The parabolic model (Fig. 5e),
is, strictly speaking, not a nonlinear model because T(i)2 can be seen as
a second variable. This allows usage of multivariate linear regression
estimation , which is straightforward.
“Real” nonlinear regression models, such as the saturation function
(Fig. 5f),
are therefore nonlinear in the parameters.
Estimation of nonlinear trend models is usually more complex than
of linear models because numerical procedures have to be employed.
Although there are several types of procedures, the general estimation
Resampled Linear Fit Residuals, r*(i) (°C)
Linear Fit Residuals, r(i) (°C)
Fig. 4. GISTEMP, MBB resampling. (a) Residual time series; note broken time
axis. The series is segmented into blocks of length l = 4. (b) MBB resample; note
broken time axis. The resample is obtained by randomly concatenating blocks.
Only two points (i.e., half a block) are required for the last two points. It is
possible to resample a block several times. The shown selection (4,1,20,4,
…,57,1) is random; other calls of the MBB procedure may generate different
selections.
M. Mudelsee
Earth-Science Reviews 190 310–322
concept may be outlined as follows. The parameters of the model span a
hyperspace. The estimation consists in minimizing a cost function (e.g.,
least-squares). The best estimate is a point in the hyperspace for which
the cost function is minimal. The problem consists in finding that point.
This is also called an optimization problem. The general concept and
mathematical foundations of nonlinear regression are treated in the
books by Gallant and Seber and Wild . Methods to solve
optimization problems are treated by Michalewicz and Fogel .
Particularly useful for climatological trend estimations are nonlinear change-point functions (Fig. 5) because they serve to quantify
climate transitions. In particular, the break and the ramp models have
been used by climate researchers.
The break regression model (Fig. 5a) is described by four parameters, t2, x2, β1, and β2. The break model can be fitted to a time
series, {t(i),x(i)}i=1
n, that means, the parameters can be estimated, by
means of WLS. Since the break function is not differentiable with respect to time at t2, the WLS estimate for t2 cannot be obtained by taking
the derivative. Mudelsee presented a brute-force estimation—for all trial t2 points from the set {t(i)}i=i1
i2, the optimal x2, β1,
and β2 values are calculated (via the derivatives)—which allows to
calculate the WLS sum. Finally, that value for t2 is taken, for which the
WLS sum is minimal. This is a global optimization technique for estimating t2, which is associated with high computing costs. However, for
typical data sizes in climatology (say, up to a few tens of thousands of
points), it is a feasible procedure on modern computers. By tailoring the
search range (i.e., selecting i1 and i2), it is possible to reduce the
computing costs. Since the t2 estimate is taken from the original time
values, there may be an estimate superior to the brute-force estimate
somewhere in between the original time values. However, this issue of a
superior fine search would only be relevant if the standard error for the
brute-force determined value of t2 is smaller than the spacing at around
t2. On the contrary, the practical observation is that
usually the spacing is smaller and is, hence, not the accuracy-limiting
There are no classical standard errors for the break model parameters. However, the MBB resampling, presented in Section 3.1, can
also be applied to the residuals of the break model fit. Mudelsee 
presents Monte Carlo tests of confidence intervals for the break model
parameter estimates based on MBB resampling. The software for fitting
a break trend model to data, BREAKFIT , includes also
graphical residual analysis.
Fig. 6 shows the break regression model fitted by means of OLS to
the GISTEMP time series. The estimate of the change-point time with
MBB standard error is
The estimate of the change-point level is
The estimate of the left slope is
0.0037 C/a
0.0007 C/a.
The estimate of the right slope is
0.0185 C/a
0.0026 C/a.
It appears obvious to the eye that the break fit is superior to the
linear fit (Fig. 2a) to describe the trend in the GISTEMP time series.
There is still some structure left in the form of shorter-term deviations,
such as the positive excursions at around the year 1940. Owing to the
better fit quality, the estimates may be helpful for a climatological interpretation (Section 4).
The estimated change-point time (Eq. 12) is rather stable with respect to the selection of the fit interval, as a sensitivity study reveals.
Replacing the lower interval bound of 1880 by 1890, 1900, 1910, 1920,
or 1930, had no effect (within error bars) on t2. Also replacing the
upper interval bound of 2017 by 2010 had no effect (within error bars)
on t2. Only further shrinking the fit interval has some effect. For example, for the fit interval 1940–2000, the result of the break fit is
The procedure for the ramp regression model (Fig. 5b) is rather
similar to the procedure for the break model. The ramp is described by
four parameters, t1, x1, t2, and x2. The ramp model can be fitted to a
time series by means of WLS. Since the ramp function is not differentiable with respect to time at t1 and t2, the WLS estimates for t1 and
t2 cannot be obtained by taking the derivatives. Mudelsee 
Fig. 5. Nonlinear regression models. The change points in the continuous
change-point models (a, b, c, and d) are indicated by filled circles. The change
points in discontinuous, abrupt models (h, i, and j) are indicated by open/filled
circles. The parameterization of the models is explained for two cases. First, the
break model (a) is defined by four parameters: t2, change-point time; x2,
change-point level; β1, left slope; and β2, right slope. Second, the ramp model
(b) is also defined by four parameters: t1, left change-point time; x1, left
change-point level; t2, right change-point time; and x2, right change-point
Temperature Anomaly (°C)
Fig. 6. GISTEMP time series and break regression curve (solid line) fitted by
means of OLS. Also shown is a histogram of the 2000 bootstrap replications of
the change-point time.
M. Mudelsee
Earth-Science Reviews 190 310–322
presented a brute-force estimation—for all trial t1–t2 combinations
from the set {t(i)}i=i1
i2, with the constraint t1 < t2, the optimal x1 and
x2 values are calculated (via the derivatives)—which allows to calculate the WLS sum. Finally, that pair of values for t1–t2 is taken, for
which the WLS sum is minimal. This is a global optimization technique
for estimating the t1–t2 combination for the ramp fitting, which is associated with even higher computing costs (which are roughly proportional to n2/2) than fitting the break (roughly proportional to n for
i1 = 1 and i2 = n). Even here, it is a feasible procedure on modern
computers. By tailoring the search ranges for t1 and t2, it is possible to
reduce the computing costs. Also in case of the ramp, the issue of a
superior fine search would only be relevant if the standard errors for the
brute-force determined values of t1 and t2 are smaller than the spacing
respective
estimates.
observation
 , that usually the spacing is not the accuracy-limiting
factor, applies also to the ramp.
There are no classical standard errors for the ramp model. However,
the MBB resampling can also be applied to the residuals of the ramp
model fit. The original ramp paper employed the
stationary bootstrap (SB) instead of MBB. The SB is a bootstrap variant
with geometrically distributed (i.e., non-constant) block lengths aimed at ensuring the stationarity of the resample
generating process. Olatayo adapted the SB by invoking a
truncated geometric distribution of block lengths. Lahiri presents theoretical comparisons between the MBB, the SB, the autoregressive boostrap (ARB), and other bootstrap variants. The ARB
 is a semi-parametric
bootstrap variant, which employs an AR(1) model for the autocorrelation but keeps the bootstrap's idea to resample from the data to preserve
the distributional shape. The practical conclusion is
that the deviations among the methods appear minor and that exclusive
usage of the MBB for estimation problems cannot be condemned.
Mudelsee presents Monte Carlo tests of confidence intervals for
the ramp model parameter estimates based on ARB resampling. The
software for fitting a ramp trend model to data, RAMPFIT , includes graphical residual analysis. Also given is a version with
MBB resampling (RAMPFITc). RAMPFITc can also be used for linear
WLS and OLS regression (Section 3.1) via fixing t1 = t(1) and t2 = t(n).
Fig. 7 shows the ramp regression model fitted by means of OLS to a
selected time interval of the GISTEMP time series. The
estimate of the left change-point time is
The estimate of the left change-point level is
The estimate of the right change-point time is
The estimate of the right change-point level is
It appears obvious to the eye—and it can be tested by means of the
supplied links to data and software—that the ramp fit over the full
interval would not be superior to the break fit (Fig. 6). Therefore only
the earlier interval, before the change-point time for the break model
(Eq. 12), is investigated by means of the ramp model. The ramp fit finds
a transition in the form of a warming from the 1920s to the 1940s (see
Section 4).
3.3. Nonparametric regression
Instead of identifying the trend component, Xtrend(i), with a linear or
certain nonlinear function with parameters to be estimated, the
smoothing method estimates the trend at a time point, T′, by, loosely
speaking, averaging the data points, X(i), within a neighbourhood
around T′. A simple example is the running mean, where the points
inside a window are averaged and the window runs along the time axis.
Statistical science recommends to replace the non-smooth weighting
window (points inside receive constant, positive weight and points
outside zero weight) by a smooth kernel function, K. The kernel trend
estimator after Gasser and Müller is given by
The kernel function used by the software (KERNEL) for making the
illustration of the method (Fig. 8), is the Epanechnikov kernel, K
(y) = 0.75 × (1 −y2). The bandwidth parameter, h, is crucial because
Temperature Anomaly (°C)
Fig. 7. GISTEMP time series and ramp regression curve (solid line) fitted by
means of OLS. The fit interval is [1880;1974] (i.e., n = 95). Also shown are
histograms of the 2000 bootstrap replications of the change-point times t1
(blue) and t2 (red), respectively. (For interpretation of the references to colour
in this figure legend, the reader is referred to the web version of this article.)
Fig. 8. Nonparametric regression using the smoothing after Gasser and Müller
 with Epanechnikov kernel. The GISTEMP time series (black
curve) for the interval from 1950 (i = 71) to 1960 (i = 81) is used for illustration. The sequence {s(i)} is given as the time midpoints (time axis, upper tick
marks). The time for the shown trend estimation is T = 1953.75. The kernel
function (blue curve) has a bandwidth of h = 2 years. The area under the kernel
is divided into several subareas (dark and light blue shading). The subareas
correspond (blue numbers) to the integrals in Eq. (20). (For example, x(73) is
multiplied by the size of subarea 73.) The trend estimate, X
, is obtained
by subarea-weighting the affected x-values (Eq. 20). The values X
are marked red. The kernel is moved along the time axis (red arrow) to
estimate the trend at other time values, T . 310–322
it determines the uncertainty measures for the trend estimate. On the
other hand, the choice of K (Epanechnikov, Gaussian, and so forth), is
more of “cosmetic” interest. KERNEL places the integration bound, s(i), in the middle between two time points (Fig. 8).
KERNEL further sets
Note that the selection of the sequence s(i) in particular, and the
Gasser–Müller smoothing procedure in general, can be performed on
unevenly spaced time series. The trend can be estimated for all points,
T′, within the observation interval, [t(1);t(n)]. The kernel functions are
modified near the interval boundaries ,
so that the trend can be estimated also there.
An uncertainty measure for the estimated trend curve, X
essential for assessing the significance of the ups and downs in the
estimate, whether these variations constitute real features or are
generated by the noise. A pointwise standard-error band can be constructed from the standard-error intervals for the trend estimate as
The standard-error interval for the time value T′ is given by
. The band is obtained by concatenating the upper bounds,
, for the full time interval, t(1) ≤T′ ≤t(n), and by concatenating the lower bounds,
. A stricter test of the significance of the ups and
downs than from using 1 × se(T′) is obtained by using 2 × se(T′).
The KERNEL software has implemented MBB resampling to calculate the bootstrap standard errors. Resampling is performed on the residuals of the nonparametric regression (i.e., data minus fit). Also the
other methodological steps (autocorrelation estimation, block length
selection, and so forth) are analogous to the MBB procedure for the
linear model (Section 3.1). This brings the twofold advantage of the
MBB to the nonparametric regression: (1) distributional robustness and
(2) consideration of autocorrelation. There exist bootstrap adaptations
Davison and Hinkley, 1997; Härdle, 1990) aimed at further increasing
resampling-derived
uncertainty
Autocorrelation may constitute a major hurdle for applying guidelines for
bandwidth selection and for setting the confidence level in confidence
interval construction; see Mudelsee 
and references cited therein for more details. The general advice is to
consider theoretical knowledge about the data generating system (i.e.,
the climate), to try many settings, to “play” with the bandwidth, h, and
study the sensitivity of the resulting trend estimations.
Fig. 9 shows the trend estimated by means of nonparametric regression for the GISTEMP time series. The bandwidth of 5 years was
predefined to inspect mid- and shorter-term (decadal-scale) variations,
such as the warming in the years around 1940, and to smooth away
faster variations. The nonparametric trend estimate has a relative
maximum at the year 1942 with a peak value of 0.083 °C. This warming
was statistically significant in the sense that on the flanks of the peak,
temperature
uncertainty
, that are below the peak value (Fig. 9). “Relative
maximum” means that the 1942-peak is not the largest value; from the
year 1979 onward, the trend estimate is always greater than the upper
uncertainty bound of 0.142 °C for the 1942-peak. For an interpretation,
see Section 4.2.
4. Discussion
The previous Section 3 presented the statistical concepts and technical details of trend estimation with uncertainty-measure determination. The GISTEMP time series of global surface temperature for the
interval from 1880 to 2017 (Fig. 1) served to illustrate the methods.
This Section 4 pursues the interpretation of the statistical results. It
highlights topics that are relevant not only with regard to the GISTEMP
record, but to climate time series in general. The first topic is model
suitability (Section 4.1), to which related are (1) fit-interval selection
and (2) the interplay between data availability and the complexity of
the questions (i.e., statistical models) that can be addressed by means of
analysis of the data.
The second topic of discussion (Section 4.2) puts a caveat on the
1942-peak in temperature due to data-quality problems caused by
changes in the measurement praxis of sea-surface temperature (SST)
during World War II.
Finally, Section 4.3 is devoted to the analysis of global temperature
for the years after and before the warm year 1998, which led some
people interested in climate—mainly amateurs from the blogosphere,
but also a few professionals—to suspect that a hiatus occurred.
4.1. Model suitability
philosophical
parsimony—Ockham's
razor—applied to the context of trend estimation, posits that simple regression
models are preferable to complex ones. But not too simple. The linear
model, with just two parameters, was assessed by eye as too simple for
describing the trend for the GISTEMP time series (Fig. 2). On the other
hand, the break model appeared as clearly better suited (Fig. 6).
The reduced sum of squares, SSQν, penalizes for usage of a high
number, p, of parameters by means of dividing SSQ at the estimation
point by the degrees of freedom,
See Bevington and Robinson for a practical view of the reduced
sum of squares and the degrees of freedom. The concept of degrees of
freedom was introduced by Fisher .
The linear fit to the GISTEMP time series has a reduced sum of
squares of
0.027 ( C) .
The break fit to the same data set has a reduced sum of squares of
( 2, 2, 1, 2)/(
0.013 ( C) .
This result confirms quantitatively the superiority of the break over
the linear model.
The ramp fit to the GISTEMP time series over the full interval from
1880 to 2017 with n = 138 (not shown) would yield a rather large
Fig. 9. GISTEMP, trend estimation by means of nonparametric regression.
Gasser–Müller smoothing (Eq. 20) with an Epanechnikov kernel and a bandwidth of h = 5 a is applied to the GISTEMP time series (black) to obtain the
trend (red solid line) with a 2-se(T′) band (red shaded). (For interpretation of
the references to colour in this figure legend, the reader is referred to the web
version of this article.)
M. Mudelsee
Earth-Science Reviews 190 310–322
reduced sum of squares,
( 1, 1, 2, 2)/(
0.018 ( C) .
This result confirms quantitatively the superiority of the break over
the ramp model for this data set. If the full interval is to be analyzed,
then the four parameters are better invested into the break than the
However, the ramp may be useful to quantify the onset of the 1942warmth (Fig. 7). For the selected time interval from 1880 to 1974, with
n = 95, the fit measure is
( 1, 1, 2, 2)/(
0.011 ( C) ,
which is comparable to the quality of the break fit.
There are other quantitative measures of the fit than the reduced
sum of squares, such as Akaike's information criterion (AIC) or a corrected version of it (AICC), which takes into account the number of fit
parameters . However, climate researchers should not
slavishly follow the values of the fit measures. It is important to keep in
mind (1) background knowledge about the data generating system and
(2) what the concrete research questions are. For example, if the interest is in quantifying a climate transition, then the questions may
consist in: When did the transition start, when did it end, what was the
amplitude? These questions directly invoke the parametric ramp regression model, and the estimated parameters provide the answers
(with error bars).
The example of fitting a ramp to quantify the onset of the 1942warmth illustrates the importance of the selection of the fit interval. A
kind of an analogue to Ockham's razor would suggest that having many
data points (a large fit interval) is preferable to having only few. But not
too many points. This allows to extract the fit interval that is still
compatible with a relatively simple model. In case of the 1942-warmth
(Fig. 7), the upper bound of the fit interval of 1974 was determined via
the estimated change-point time in the break fit (Fig. 6). In other words,
the interval from 1974 to the present was excluded from the ramp fit to
keep the compatibility.
If the interest is less in quantifying a climate transition or determining change points but more in describing the trend over the full
time interval, then the nonparametric regression model may be a good
choice (Fig. 9).
To summarize, there is no golden rule what particular model to
employ. The advice is rather to “play” with the models and parameter
setting, to observe the sensitivity of the results, and to acquire an intuition for what is real and what is noise. The final task, however, is to
honestly communicate those findings.
4.2. World War II bias
The statistical finding of a warming in the years around 1940 and a
relative maximum at the year 1942 with a kernel-determined peak
value of 0.083 °C (Fig. 9) assumes a rather homogeneous quality of the
data (Fig. 1) over time. However, the consultation of the metadata
 shows that the method to measure the temperature of the upper meters of the sea changed during World War II.
On British ships, SST was measured differently compared to American
ships. The relative contributions to the combined average SST was not
constant over time during that period, and they are not exactly known.
Furthermore, measuring temperature was not the prime objective on
the ships traveling through the seas at that time, which means a reduced data quality. Since the oceans cover about two thirds of the
surface of the Earth, the derived global surface temperature curve may
be considerably distorted during the interval around World War II.
Statistical science calls such a systematic distortion bias. It is not
straightforward to quantify the bias , and climate
researchers are trying to improve the metadata situation . This problem is not unique to GISTEMP (Fig. 1). The two other
“competing” series, published by the Climatic Research Unit of the
University of East Anglia in the United Kingdom and the National
Oceanic and Atmospheric Administration of the United States of
America, respectively, also rely on the SST measurements.
Due to the World War II bias, the results of the OLS fit of a ramp to
the interval from 1880 to 1974 (Fig. 7), that means, the quantification
of the onset of the warming, could be questionable. One remedy may be
to employ WLS, with lower weights put to the data for the World War II
period . Another reason for using WLS is that the number
of measurement stations, and hence the reliability of the temperature
values, grew gradually from the 19th to the 21st century . The repetition of the fit of the ramp with WLS uses the following
function, S(i), for weighting. For the year 1880, S(i) is set equal to 0.2 °C
and for the year 2017 equal to 0.05 °C . For the years in between those end points, S(i) is set as the
linear connection between those points—with the exception of the interval 1939–1945. For this period, S(i) is set equal to 0.3 °C, which
corresponds to the rough bias estimate of SST made by Folland et al.
The WLS results are as follows. The estimate of the left change-point
The estimate of the left change-point level is
The estimate of the right change-point time is
The estimate of the right change-point level is
To summarize the results, the warming in the years around 1940
becomes less significant when taking into account the World War II bias
by means of WLS. However, bias is a systematic distortion, and there
may be ways to improve the metadata situation . This
would allow to quantify the bias better with the help of a statistical
model with a structure that is based on the physics of the SST recording.
This bias model may then lead to more accurate data and reduce the
uncertainties that affect the interpretation of global temperatures in the
4.3. Suspected global warming hiatus
The story of the suspected hiatus came up in the months before the
15th session of the Conference of the Parties to the United Nations
Framework Convention on Climate Change (COP 15), which was held
in Copenhagen in December 2009. It was kept alive for a couple of years
after the conference. The story is as follows. Global temperature, on the
rise since several decades (Figs. 2a, 6, and 9), reached a peak in 1998
(the GISTEMP temperature anomaly for that year is 0.62 °C) and then
stopped in its rise.
The story can be translated into two quantitative hypotheses. First,
there was a break point at the year 1998. Second, the slope of the
temperature trend after 1998 was zero. The hypotheses can be tested by
means of parametric change-point regressions with the break model. To
simulate what was known in the immediate years after COP 15, the
analysis “plays” and varies the upper bound of the fit interval.
The results of this exercise are as follows (Fig. 10, Table 1). As regards the first hypothesis, change-point time estimates of 1998 are
found for two selected fit intervals .
However, the standard errors for that value are considerable (3 years).
If the fit interval is extended to 1992–2017, then the 1998 estimate
disappears and a new, “cheaper” (in terms of SSQ) estimate of 2013
emerges. This fit solution shows then an opposite behavior—an
M. Mudelsee
Earth-Science Reviews 190 310–322
accelerated warming in the years after 2013. The standard error for the
year-2013 estimate is even larger (6 years) than for the year-1998 estimates .
As regards the second hypothesis, the right slopes indicate a
warming in the years after 1998 (Table 1). However, the standard errors for the slopes render them statistically insignificant. If the fit interval is extended to 1992–2017, then the right slope becomes significant —and the left slope
becomes insignificant.
The second hypothesis, of zero slope of the temperature trend after
1998, can also be tested by means of nonparametric regression. Instead
of the trend, its first derivative with respect to time, becomes the target
of the kernel estimation after Gasser and Müller . The
analysis (Fig. 11) employs the same kernel function (Epanechnikov)
and bandwidth (h = 5 a) as used for trend estimation (Fig. 9).
The zoom on the recent time interval (Fig. 11b) reveals that for the
years from, roughly, 2005 to 2010, the slope is within standard-error
band indistinguishable from zero. This result agrees with the findings
obtained from fitting the parametric break model (Fig. 10).
After 2010, the slope increased and has been since then significantly
positive. In earlier periods since 1880, the slope is mostly positive as
well. There have been only occasionally cooling periods, such as in the
years after 1900 (Fig. 11a).
To summarize, it seems impossible to show with a decent level of
statistical significance that a hiatus in surface temperature occurred in
the years after 1998. The number of data points is too small, and the
statistical uncertainties are too large. This effect is exacerbated by the
presence of serial dependence (Fig. 3b), which reduces the number of
independent data points Mudelsee .
The resulting estimates for the change-point time and the slopes
before and after depend on the selected fit interval (Table 1). For example, if the fit interval is extended from 1992–2013 to 1992–2017,
then the estimated change-point time of 1998 disappears in the break fit
(Fig. 10).
This observation points toward a general issue in trend estimation
on time series. The researcher has the power to select the fit interval,
which allows her or him to suppress certain fit solutions and favor other
solutions. The interval selection should therefore be objective and oriented on general principles, such as to have a long interval that still
yields results compatible with a simple trend model.
The power (to select a fit interval) has therefore to be accompanied
by responsibility: to employ objective criteria and also reveal the sensitivity of results to the fit-interval selection. Ultimately, however, the
Temperature Anomaly (°C)
Fig. 10. GISTEMP and the suspected global warming hiatus. Break trend
models are fitted by means of OLS to various intervals . Also
shown are histograms of the 2000 bootstrap replications of the change-point
time for the intervals 1992–2017 (red) and 1992–2013 (purple), respectively;
the histogram for 1992–2011 (not shown for legibility) looks very similar to
that for 1992–2013. See Table 1 for parameter estimates. (For interpretation of
the references to colour in this figure legend, the reader is referred to the web
version of this article.)
GISTEMP and the suspected global warming hiatus, results of break fits
(Fig. 10).
Fit Interval
0.048 ± 0.040
0.011 ± 0.025
0.049 ± 0.035
0.010 ± 0.019
0.018 ± 0.031
0.066 ± 0.043
Fig. 11. GISTEMP and the suspected global warming hiatus. Gasser–Müller
smoothing (Eq. 20) with an Epanechnikov kernel and a bandwidth of h = 5 a is
applied to the GISTEMP time series (Fig. 1) to obtain the first derivative of the
trend (red solid line) with a 2-se(T′) band (red shaded). (a) Full time interval,
1880–2017; (b) zoomed time interval, 1992–2017. (For interpretation of the
references to colour in this figure legend, the reader is referred to the web
version of this article.)
M. Mudelsee
Earth-Science Reviews 190 310–322
misuse of statistics is a social, cultural and ethical problem, for which
technical fixes are doomed .
5. Conclusions
At the time of writing , the statistical methodology of
trend estimation is well elaborated. Some development may come in
the form of GLS estimation techniques for nonlinear regression functions (Fig. 5), such as the break or the ramp models. Another direction
is the development of estimation routines for the piecewise linear
model (Fig. 5c). Furthermore, the fitting of multiple change-point
models (i.e., more than two change points) is of genuine interest. This is
technically challenging and likely necessitates the implementation of
optimization
techniques,
algorithms
 . The reward of such a technology may
consist in a reduction of the problem of fit-interval selection. An interesting example in that regard is the analysis of regional temperatures
in the Alpine region , which demonstrated an accelerated warming in several phases. As regards nonparametric regression, it appears that the potential of that method
(standard-error band and derivative estimation) for climatology has
only occasionally been appreciated. One example is the search for 14C
plateaus (i.e., zero slope) in marine sedimentary records from the Holocene .
The statistical methodology of uncertainty determination for climate time series is well elaborated, as far as uncertainties stemming
from measurement or proxy errors in the climate variable, X, are concerned. Bootstrap methods take into account deviations from Gaussian
shape. Blocking variants of the bootstrap, such as the MBB, take into
account autocorrelation. This means that the two major peculiarities of
climate time series—non-Gaussian shape and autocorrelation—can be
successfully dealt with in the statistical analysis. As a result, it is
possible to avoid unrealistically small error bars from ignored autocorrelation, which could lead to overstatements. On the other hand, as
regards timescale errors in the variable T, this is a topic where further
research will be quite relevant for paleoclimatology. The book by
Mudelsee contains some algorithms, simulation tests, and references on that emerging field. Also the Bayesian view of probability
can be adopted for research .
As time proceeds, new data sources become available and existing
time series are updated, which leads to new insights. This is a part of
normal science , and the climate community should not
expect too many big surprises. As an example, the suspected global
warming hiatus in the years after 1998 disappears when considering
not only the interval up to 2013 for the fit, but rather the interval up to
2017. This example illustrates that the selection of the fit interval for
trend estimation—has also a moral aspect. Climate researchers should
be aware of this.
Acknowledgments
I thank the referee for a helpful and constructive review. I thank
James Hansen (The Earth Institute, Columbia University, New York,
NY, United States of America), Philip Jones (Climatic Research Unit,
University of East Anglia, Norwich, United Kingdom), Gerrit Lohmann
(Alfred Wegener Institute Helmholtz Centre for Polar and Marine
Bremerhaven,
Mathematics and Statistics, University College Dublin, Dublin, Ireland),
and Gavin Schmidt (NASA Goddard Institute for Space Studies and
Center for Climate Systems Research, Columbia University, New York,
NY, United States of America) for comments on a previous manuscript
version. The used software packages (BREAKFIT, KERNEL, RAMPFIT,
and RAMPFITc) are
freely available at .
Appendix A. Appendix
We describe the MBB for the application to linear regression in the following steps. It is detailed on an algorithmic level, ready to be implemented
in various forms of software.
Step 1: OLS estimation
Use the time series, {t(i),x(i)}i=1
n, and calculate the OLS estimates of intercept and slope (Eqs. 4 and 5). See, for example, the GISTEMP time
series (Fig. 2a).
Step 2: Residuals
Calculate the residuals as
See, for example, the GISTEMP time series (Figs. 2b and 3).
Step 3: Autocorrelation estimation
Use the residuals to calculate the autocorrelation coefficient estimate. For even spacing, this is given by
The underlying mathematical autocorrelation model is the first-order autoregressive (AR(1)) process, where a noise value “remembers” only its
own immediate past . For climate variables, the AR(1) model has been found useful empirically and theoretically for describing the autocorrelation structure. The AR(1) model for even spacing has one parameter, −1 < a < 1,
which measures the strength of the “memory”. The noise component for climate data usually has a ≥0 .
For uneven spacing, the procedure is more complex. First, the persistence time estimate,
, has to be numerically calculated (i.e., there is no
analytical formula) as the minimizer of the least-squares sum,
exp{ [ ( )
The idea behind that formula is that the memory is the larger, the smaller the time distance from a time point, t(i), to the
preceding time point, t(i −1), is. The persistence time, τ ≥0, is the parameter of the AR(1) model for uneven spacing 310–322
Step 4: Block length selection
The MBB resamples blocks of a certain length of residuals (see Step 6). An optimal block length selector is
where NINT is the nearest integer function. (If a approaches zero, then set lopt = 1; if a approaches unity, then set lopt = n −1.) Eq. (A.5) is for even
spacing; in case of uneven spacing, use Eq. (5) with aequ instead of a. “Optimality” should not be seen too strictly. The key point is that the block
length should be long enough to capture a sufficient amount of the autocorrelation. There exist other block length selectors . There exist also bias-corrected versions of a and aequ , but this is less relevant here.
Step 5: Counter
Set the resampling counter, b, equal to 1.
Step 6: Start (MBB resampling)
Use lopt for the MBB (Fig. 4) to generate a random set of residuals, {r∗(i)}i=1
Step 7: Resample
Calculate the resample, {t(i),x∗(i)}i=1
The residuals are employed as realizations of the noise component, S(i) × Xnoise(i).
Step 8: Replications
Use the resample, {t(i),x∗(i)}i=1
n, and calculate the OLS estimates of intercept and slope (Eqs. 4 and 5). These quantities,
1 , are called
replications; they capture the noise influence on the estimation uncertainty.
Step 9: End (MBB resampling)
Increase b by 1 and go back to Step 6 until b = B = 2000 replications exist for both intercept and slope.
Step 10: Uncertainty measure
A simple uncertainty measure is the bootstrap standard error, which is the sample standard deviation over the replications:
where the sample means are given by
Other uncertainty measures, such as confidence intervals, can also be calculated from the replications .