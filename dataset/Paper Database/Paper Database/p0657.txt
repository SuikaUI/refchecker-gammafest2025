HAL Id: hal-03048735
 
Submitted on 9 Dec 2020
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Deep Model Compression and Architecture
Optimization for Embedded Systems: A Survey
Anthony Berthelier, Thierry Chateau, Stefan Duffner, Christophe Garcia,
Christophe Blanc
To cite this version:
Anthony Berthelier, Thierry Chateau, Stefan Duffner, Christophe Garcia, Christophe Blanc. Deep
Model Compression and Architecture Optimization for Embedded Systems: A Survey. Journal of
Signal Processing Systems, 2020, ￿10.1007/s11265-020-01596-1￿. ￿hal-03048735￿
Noname manuscript No.
(will be inserted by the editor)
Deep Model Compression and Architecture
Optimization for Embedded Systems: A Survey
Anthony Berthelier · Thierry Chateau ·
Stefan Duﬀner · Christophe Garcia ·
Christophe Blanc
Received: date / Accepted: date
Abstract Over the past, deep neural networks have proved to be an essential
element for developing intelligent solutions. They have achieved remarkable
performances at a cost of deeper layers and millions of parameters. Therefore
utilising these networks on limited resource platforms for smart cameras is a
challenging task. In this context, models need to be (i) accelerated and (ii)
memory eﬃcient without signiﬁcantly compromising on performance. Numerous works have been done to obtain smaller, faster and accurate models. This
paper presents a survey of methods suitable for porting deep neural networks
on resource-limited devices, especially for smart cameras. These methods can
be roughly divided in two main sections. In the ﬁrst part, we present compression techniques. These techniques are categorized into: knowledge distillation,
pruning, quantization, hashing, reduction of numerical precision and binarization. In the second part, we focus on architecture optimization. We introduce
the methods to enhance networks structures as well as neural architecture
search techniques. In each of their parts, we describe diﬀerent methods, and
analyse them. Finally, we conclude this paper with a discussion on these methods.
Keywords Deep learning · Compression · Neural networks · Architecture
A. Berthelier
Institut Pascal - 4 Avenue Blaise Pascal, 63178 Aubiere, France
Tel.: +33630899676
E-mail: 
T. Chateau
Institut Pascal - 4 Avenue Blaise Pascal, 63178 Aubiere, France
LIRIS - 20, Avenue Albert Einstein, 69621 Villeurbanne Cedex, France
LIRIS - 20, Avenue Albert Einstein, 69621 Villeurbanne Cedex, France
Institut Pascal - 4 Avenue Blaise Pascal, 63178 Aubiere, France
Anthony Berthelier et al.
Data Precision Reduction
Redundancy Reduction
Transfer Learning
Deep Model Compression and Optimization
Compression Techniques
Architecture Optimization
Knowledge Distillation
Quantization
Numerical Precision
Binarization
Architecture Overview
Neural Architecture
Fig. 1: Roadmap of our paper.
1 Introduction
Since the advent of deep neural network architectures and their massively parallelized implementations , deep learning based methods have achieved
state-of-the-art performance in many applications such as face recognition,
semantic segmentation, object detection, etc. In order to achieve these performances, a high computation capability is needed as these models have usually
millions of parameters. Moreover, the implementation of these methods on
resource-limited devices for smart cameras is diﬃcult due to high memory consumption and strict size constraints. For example, AlexNet , is over 200MB
and all the milestone models that followed such as VGG , GoogleNet and
ResNet are not necessarily time or memory eﬃcient. Thus ﬁnding solutions
to implement deep models on resource-limited platforms such as mobile phones
or smart cameras is essential. Each device has a diﬀerent computational capacity. Therefore, to run these applications on embedded devices the deep models
need to be less-parametrized in size and time eﬃcient.
Few works has been done focusing on dedicated hardware or FPGA with
a ﬁxed speciﬁc architecture. Having a speciﬁc hardware is helpful to optimize
a given application. However, it is diﬃcult to generalise. The CPU architectures of the smartphones are diﬀerent from each other. Thus, it is important
to develop generic methods to help optimize neural networks. This paper aims
to describe general compression methods for deep models that can be implemented on a large range of hardware architectures, especially on various
generic-purpose CPU architectures. Moreover, we are speciﬁcally interested in
multilayer perceptron (MLP) and Convolutional Neural Networks (CNNs) because these types of state-of-the-art models have a large number of parameters.
Title Suppressed Due to Excessive Length
However, some methods could be applied to recurrent neural networks such as
LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) .
Few surveys exist on deep neural compression . However these works
are mainly focused on compression and acceleration algorithms of existing
models. In this paper, we present not only the methods to compress or accelerate deep model, but also the recent research concerning optimized architectures search and design. The article is organized as follows. The ﬁrst part
addresses the compression techniques (Section 2) which reduce the size and
computation requirements of a model by using diﬀerent algorithms. Knowledge
distillation methods are explained to tackle the problem of transfer learning
(Section 2.1). Followed are the hashing (Section 2.2), pruning (Section 2.3)
and quantization (Section 2.4) methods which explore the redundancy of the
networks. Numerical precision (Section 2.5) and binarization (Section 2.6) are
presented by introducing the use of data with lower precision. The second part
of this paper describes architecture optimization (Section 3). We begin with
the description of implementations to optimize network architecture and the
way diﬀerent modules are designed to interact with each other (Section 3.1).
Then we explain methods to automatically search optimized architecture (Section 3.2). In each of these parts, we present existing methods, their strengths,
weaknesses and in which context they may be applied. The structure of the
article is detailed in Figure 1.
2 Compression techniques
2.1 Knowledge distillation
To design a neural network, it is important to evaluate how deep the network
needs to be. A neural network is composed of an input, an output and intermediate layers. A shallow neural network is a network with a lower number
of intermediate layers as opposed to a deep neural network. A deeper network
has more parameters and can potentially learn more complex functions e.g.
hierarchical representations . The theoretical work from revealed the
diﬃculty involved to train a shallow neural network with the same accuracy
as a deep network. However, an attempt was made to train a shallow network
on SIFT features in order to classify the Imagenet dataset . The authors
concluded that it was a challenging task to train highly accurate shallow models .
In spite of that, Ba et al. reported that neural networks with a shallower architecture are able to learn the same function as deep networks, with a
better accuracy and sometimes with a similar number of parameters (see Figure 2). Inspired by , their model compression consists in training a compact
model to approximate, to mimic, the function learned by a complex model.
This is what knowledge distillation is about : transfer the knwoledge learned
by a model to another one. The preliminary step is to train a deep network
(the teacher network) to generate automatically labelled data by sending univ
Anthony Berthelier et al.
labelled data through this deep network. Next, this "synthetic" dataset is then
used to train a smaller mimic model (the student network), which assimilates
the function that was learned by the larger model. It is expected that the
mimic model should produce same predictions and mistakes as the deep network. Thus, similar accuracy can be achieved between an ensemble of neural
networks and its mimic model with 1000 times fewer parameters. In , the
authors demonstrated this assertion on the CIFAR-10 dataset. An ensemble of
deep CNN models was used to label some unlabeled data of the dataset. Next,
the new data were used to train a shallow model with a single convolution and
maxpooling layer followed by a fully connected layer with 30k non-linear units.
In the end, the shallow model and the ensemble of CNN acquired the same
level of accuracy. Further improvements have been made on student-teacher
techniques, especially with the work of Hinton et al. . Their framework utilizes the output from the teacher’s network to penalize the student network.
Additionally it is also capable of retrieving an ensemble of teacher networks
to compress their knowledge into a student network of similar depth.
Fig. 2: Accuracy of diﬀerent deep neural networks, shallow neural networks
and shallow mimic neural networks against their number of parameters on
TIMIT speech database Dev (left) and Test (right) sets. Results and ﬁgures
are from .
In recent years, other compression methods that are described in this paper
are preferred. However, some works are coupling transfer learning techniques
with their own methods to achieve strong improvements. For example, the
works of Chen et al. and Huang et al. follow this approach employing
additional pruning techniques (see section 2.3). The former uses a deep metric
learning model, whereas the latter handles the student-teacher problem as a
distribution matching problem by trying to match neuron selectivity patterns
between them to increase the performance. Aguilar et al. propose to distill
the internal representations of a teacher models into a simpliﬁed version of it to
improve the learning and the performance of the student model. Lee et al. 
use a self-supervised learning algorithm to improve transfer learning methods.
These methods are eﬃcient. However their performances can vary largely according to the application. Classiﬁcation tasks are easy to learn for a shallow
Title Suppressed Due to Excessive Length
model, but tasks like segmentation or tracking are diﬃcult to apprehend even
with a deep model. Furthermore, Muller et al. recently showed with label
smoothing experiments that teacher and student networks are sensitive to the
format of the data. Thus, improving knowledge distillation methods is also a
diﬃcult mission.
2.2 Hashing
Hashing is employed to regroup data in a neural network to avoid redundancy
and access the data faster. Through empirical studies, hashing methods have
proven themselves to be an eﬀective strategy for dimensionality reduction .
HashedNets is a hashing methods utilized and developed by Nvidia. In
this model, a hash function is used to uniformly and randomly group network
connections into hash buckets. As a result, every connection that is in the ith
hash bucket has the same weight value wi. This technique is especially eﬃcient
on fully connected feed forward neural networks. Moreover, It can also be used
in conjunction with other neural network compression methods.
Several other hashing methods have been developed in the past few years.
Spring et al. proposed an approach where adaptive dropout (i.e. choosing nodes with a probability proportional to some monotonic functions of their
activations) and hash tables based on locality-sensitive hashing (LSH) 
are utilized. These techniques once combined allowed the authors to construct
a smart structure for maximum inner product search . This technique exhibits better results, reducing computational costs for both training and testing. Furthermore, this kind of structure leads to sparse gradient updates and
thus a massively asynchronous model. Thereby, models can be easily parallelized as the data dispersion could be wider. However, wider data dispersion
can result in a slow down of the model. A trade-oﬀbetween these criteria is
necessary.
2.3 Pruning
The compression of neural networks by using pruning techniques has been
widely studied. These techniques enable to remove parameters of a network
that are not necessary for a good inference. The early work in this domain
was aiming to reduce the complexity and the over-ﬁtting in networks .
In these papers, the authors used pruning techniques based on the Hessian
of the loss function to reduce the number of connections inside the network.
The method ﬁnds a set of parameters whose deletion would cause the least
increase of the objective function by measuring the saliency of these parameters. The authors use numerous approximations to ﬁnd these parameters. For
instance, the objective function is approximated by a Taylor series. Finding
parameters whose deletion does not increase this function is a diﬃcult problem
that involves, for example, the computation of huge matrices as well as second
Anthony Berthelier et al.
derivatives. Also, these methods suggest that reducing the number of weights
by using the Hessian of the loss function is more accurate than magnitudebased pruning like weight decay. Additionnally, it reduces the network over-
ﬁtting and complexity. However, the second-order derivatives introduce some
computational overhead.
Signorini et al. utilized an intuitive and eﬃcient method to remove
parameters. The ﬁrst step is to learn the connectivity of the network via a
conventional training of the network i.e. to learn which parameters (or connections) are more important than the other. The next step consists in pruning
those connections with weights below a threshold i.e. converting a dense network into a sparse one. Further, the important step of this method is to retrain
(ﬁne-tune) the network to learn the weights of the remaining sparse connections. If the pruned network is not retrained, then the resulting accuracy is
considerably lower. The general steps for pruning a network are presented on
Training the network
Evaluate the neurons
Remove the unimportant
Fine-tuning
Continue pruning ?
Pruning ends
Fig. 3: Basic steps for pruning a deep network. Figure inspired by .
Anwar et al. used a similar method. However, they state that pruning
has the drawback of constructing a network that has "irregular" connections,
which is ineﬃcient for parallel computing. To avoid this problem, the authors
introduced a structured sparsity at diﬀerent scales for CNN. Thus, pruning
is performed at : the feature map, the kernel and the intra-kernel levels. The
idea is to force some weights to zero but also to use sparsity at well deﬁned
activation locations in the network. The technique consists in constraining each
outgoing convolution connection for a source feature map to have similar stride
and oﬀset. This results in a signiﬁcant reduction of both feature and kernel
matrices. Usually, sparsity has been studied in numerous works in order to
penalize non-essential parameters .
Similar pruning approach is seen in Molchanov et al. . However diﬀerent pruning criteria and technical considerations are deﬁned to remove features
maps and kernel weights, e.g. the minimum weight criteria . They assume
that if an activation value (an output feature map) is small, then the feature detector is not important in the application. Another criteria involves
the mutual information which measures how much information is present in
Title Suppressed Due to Excessive Length
a variable about another one. Further, the Taylor expansion is used similar
to LeCun , to minimise the computational cost between the pruned and
the non-pruned network. In this case, pruning is treated as an optimization
A recent pruning method consists in removing ﬁlters that are proven
to have a small impact on the ﬁnal accuracy of the network. This results
in automatically removing the ﬁlter’s corresponding feature map and related
kernels in the next layer. The relative importance of a ﬁlter in each layer
is measured by calculating the sum of its absolute weights, which gives an
expectation of the magnitude of the output feature map. At each iteration,
the ﬁlters with the smallest values are pruned. Recently, Jian-Hao et al. 
developed a pruning network called ThiNet which, instead of using information
of the current layer to prune unimportant ﬁlters of that layer, uses information
and statistics of the subsequent layer to prune ﬁlters from a given layer. Not
only weights and ﬁlters but also channels can be pruned using complex
thresholding methods.
Fig. 4: Comparison of the speed of AlexNet and VGG before and after pruning
on CPU, GPU and TK1. Figure from .
These past few years, numerous networks compression algorithms using
pruning methods and achieving state-of-the-art results have emerged. Yu et al.
 proposed a neurons importance score propagation (NISP) method based
on the response of the ﬁnal layers to evaluate the pruning impact of the prior
layers. Zhuang et al. developed discrimination-aware losses in order to determine the most useful channels in intermediate layers. Some methods such as
Filter Pruning Via Geometric Median (FPGM) are not focused on pruning
ﬁlters with less importance but only by evaluating their redundancy. Similarly,
Lin et al. tackled the problem of redundant structures by proposing a generative adversarial learning method (GAL) (not only to remove ﬁlters, but also
branches and blocks).
Factorization methods are also use such as matrix or tensor decomposition . However decomposition operations are computationally expensive and factorization methods are also time-consuming as the model needs to
be retrained numerous times. As a result, we will not go into detail on these
methods in this paper. However, an overview of these techniques can be found
Anthony Berthelier et al.
Numerous pruning methods exist and each of them has strength and weaknesses. The main disadvantage of these methods is that it takes a long time to
prune networks due to the constant retraining that they demand. Recent techniques like try to bypass some steps by pruning neural networks during
their training by using recurrent neural networks. However, all of them result
in considerable reduction of parameters. Pruning methods allow to eliminate
10 to 30 percent of the network’s weights. Regardless of the method, the size
of a network can be decreased with pruning without change or signiﬁcant drop
in accuracy. The inference with the resulting models will also be faster (see
Figure 4) but the actual speed depends on which method has been utilized
and the sparsity of the network after pruning.
2.4 Quantization
Network quantization is similar to pruning as this is a common technique in
the deep learning community. It aims to reduce the number of bits required to
represent every weight. In other words, it decreases the number of parameters
by exploiting redundancy. Quantization reduces the storage size with minimal
loss in performance. In a neural network, it means that parameters will be
stacked into clusters and share the same value with the parameters within the
same cluster.
Gong et al. performed a study on a series of vector quantization methods and found that performing scalar quantization on parameter values using
a simple k-means is suﬃcient to compress them 8 to 16 times without a huge
loss in accuracy. Few years later, Han et al. utilized a trivial quantization method using k-means clustering. They performed a pruning step before
and a Huﬀman coding step after the quantization in order to perform a larger
compression of the network. In their experiments, the authors were able to
reduce network storage by 35 to 49 times across diﬀerent networks. Pruning
and quantization are methods that are often used together to achieve a solid
compression rate. For example, for a LeNet5-like network , pruning and
quantization compressed the model 32 times and with huﬀman coding even 40
It is possible to apply several quantization methods on neural networks.
Choi Y. et al. deﬁned a Hessian-weighted distortion measure as an objective function in order to decrease the quantization loss locally. Further,
a Hessian-weighted k-means clustering is used for quantization purposes to
minimize the performance loss. Recent neural network optimizers can provide
alternatives to the Hessian and thus reduce the overall computation cost, like
Adam , AdaGrad , Adadelta or RMSProp . However one of the
advantages of using the Hessian-weighted method is that the parameters of all
layers in a neural network can be quantized together at once compared to the
layer-by-layer quantization used previously .
Quantization techniques are eﬃcient as they achieve an impressive compression rate and can be coupled with other methods to compress the models
Title Suppressed Due to Excessive Length
further. Their eﬃciency is integrated in some frameworks and tools to directly
quantify a network and port it on mobile devices .
2.5 Reducing numerical precision
Although the number of weights can be considerably reduced using pruning
or quantization methods, the overall number of parameters and costly matrix
multiplications might still be enormous. A solution is to reduce the computational complexity by limiting the numerical precision of the data. Deep neural
networks are usually trained using 32-bit ﬂoating-point precision for parameters and activations. The aim is to decrease the number of bits used (16, 8 or
even less) and to change from ﬂoating-point to a ﬁxed-point representation.
Selecting the precision of data has always been a fundamental choice when it
comes to embedded systems. When committed to a speciﬁc system, the models and algorithms can be optimized for the speciﬁc computing and memory
architecture of the device .
However, applying quantization for deep neural networks is a challenging
task. Quantization errors might be propagated and ampliﬁed throughout the
model and thus have a large impact on the overall performance. Since the beginning of the 90’s, experiments have been made in order to limit the precision
of the data in a neural network, especially during backpropagation. Iwata et
al. created a backpropagation algorithm with 24-bit ﬂoating-point processing units. Hammerstrom presented an architecture for on-chip learning using 8-16 bits ﬁxed-point arithmetic. Furthermore, Holt and Hwang 
showed empirically that only 8-16 bits are enough for backpropagation learning. Nonetheless, even if all these works are helping to understand the impact
of limited numerical precision on neural networks, they are done on rather
small models such as multilayers perceptron with only a single hidden layer
and very few units. More sophisticated algorithms are required for more complex deep models.
In 2015, Gupta et al. trained deep CNN using 16-bit ﬁxed-point instead
of 32-bit ﬂoating-point precision. It constrained neural networks parameters
such as bias, weights and other variables used during the backpropagation
such as activations, backpropagated error, weight updates and bias updates.
Diﬀerent experimentations have been made with this 16-bit ﬁxed-point word
length, e.g. varying the number of bits that encode the fractional (integer)
part between 8 (8), 10 (6) and 14 (2), respectively. In other terms, the number
of integer bits IL added to the number of fractional bit FL is always equal to
16. Tested on the MNIST and CIFAR-10 datasets with a fully connected and
a convolutional network, the results were nearly the same as the ﬂoating-point
baseline when decreasing the fractional part to 12-bit precision.
The crucial part in this method is the conversion of a ﬂoating point number
(or higher precision format) into a lower precision representation. To achieve
this, describe two rounding schemes. The ﬁrst one is the round-to-nearest
method. It consists of deﬁning ⌊x⌋as the largest integer multiple of ϵ = 2−F L
Anthony Berthelier et al.
less than or equal to x. So given a number x and the target representation
(IL,FL), the rounding is done as follows:
if⌊x⌋≤x ≤⌊x⌋+ ϵ
⌊x⌋+ ϵ if⌊x⌋+ ϵ
2 ≤x ≤⌊x⌋+ ϵ .
The second rounding scheme is stochastic rounding. It is a statistic and unbiased rounding where the probability of x to be rounded to ⌊x⌋is proportional
to its proximity to ⌊x⌋:
⌊x⌋+ ϵ w.p.
Courbariaux et al. investigated the impact of numerical precision, especially to reduce the computational cost of multiplications. Their experiments
were performed with three formats: ﬂoating point, ﬁxed point and dynamic ﬁxed point (which is a compromise of the ﬁrst two). Instead of
having a single scaling factor with a ﬁxed number for the integer part and
another ﬁxed number for the fractional part, several scaling factors are shared
between grouped variables and are updated from time to time. The authors
achieved similar conclusions as : a low precision is suﬃcient to run and
train a deep neural network. However, limited precision can be eﬃcient when
it is paired and optimized with a speciﬁc hardware. Gupta et al. achieved
good results when they paired the ﬁxed point format with FPGA-based hardware but the hardware optimization of dynamic ﬁxed point representations
is not as simple. Neural networks with limited-precision parameters and their
optimized integration on hardware have already been studied in the past. For
example, Mamalet et al. and Roux et al. developed optimized CNNs
to detect faces and facial features in videos on embedded platforms in realtime. They used a ﬁxed-point parameter representation but also optimized
the inference algorithms for speciﬁc platforms. This allowed them to exploit
parallel computing and memory locality.
To conclude, a limited numerical precision is suﬃcient to train deep models.
It is helpful to save memory storage and computation time, even more if a
dedicated hardware is used. However, not every step can be done with low
precision in a neural network. For instance, a higher precision must be used
to update the parameters during training.
2.6 Binarization
In recent works, limited numerical precision was extended to binary operations.
In a binary network, the weights and the activations at least are constrained
to either +1 or −1. Following the same idea as previously with limited numerical precision , the same authors decided to apply two rounding schemes
Title Suppressed Due to Excessive Length
to binarize a variable: deterministic and stochastic rounding . The most
common rounding method is to maintain the sign of the variable. So for a
variable x, its binary value xb will be the sign of x (+1 if x ≥0, −1 otherwise). The second binarization scheme is a stochastic rounding. Thus xb = +1
with probability p = σ(x) and xb = −1 with probability 1 −p where σ is the
hard sigmoid function . The stochastic method is diﬃcult to implement
as it requires randomly generating bits from the hardware. As a result, the
deterministic method is commonly used. However, recent works like are
focusing on alternative methods to approximate the weight values in order to
obtain a more accurate network. For example, weights can be approximated
using a linear combination of multiple binary weight bases.
Nevertheless, just like limited numerical precision, a higher precision is required at some point and real-valued weights are required during the backpropagation phase. Adding noise to weights and activations (such as dropout ) is beneﬁcial to generalization when the gradient of the parameters is computed. Binarization can also be seen as a regularization method .
With all these observations, Courbariaux et al. developed a method
called BinaryConnect to train deep neural networks using binary weights during the forward and backward propagation, while storing the true precision
of the weights in order to compute the gradients. Firstly the forward propagation: layer-by-layer, the weights are binarized and the computation of the
neuron’s activation is faster because multiplications are becoming additions.
Secondly the backward propagation: the training objective’s gradient is computed in function of each layer’s activation (from the top layer and going
down layer-by-layer until the ﬁrst hidden layer). Lastly the parameter update:
the parameters are updated using their previous values and their computed
gradients. During this ﬁnal step more precision is needed.
As a consequence the real values are used (the weights are binarized only
during the ﬁrst two steps). Tests on datasets like MNIST, CIFAR-10 and
SVNH can achieve state-of-the-art results with two-thirds less multiplications,
training time accelerated by a factor of 3 and a memory requirement decreased
by at least 16.
In a binary weight network, only weight values are approximated with
binary values. This also works on CNNs where the models are signiﬁcantly
smaller (up to 32 times). Then, the operation of convolution can be simpliﬁed
as follows:
I ∗W ≈(I ⊕B)α ,
where, I is the input, W the real-value weight ﬁlter, B the binary ﬁlter
(sign(W)), α a scaling factor such that W ≈αB and ⊕indicates a convolution without multiplications. Further improvements have been done with
the XNOR-Net proposed by Rastegari et al. where both the weights and
the input to the convolutional and fully connected layers are binarized. In this
case, all the operands of the convolutions are binary, and thus the convolution
can be performed by only XNOR and bitcounting operations:
I ∗W ≈(sign(I) ⊕sign(W)) ⊙Kα ,
Anthony Berthelier et al.
where, I is the input, W is the real-value weight ﬁlter and K is composed of
the scaling factors for all sub-tensors in the input I.
The resulting network in is as accurate as a single-precision network. It
also runs faster (58 times on GPU) and is smaller (AlexNet is reduced to 7MB).
Many existing models (like the hourglass model ) have been enhanced with
the XNOR-Net method to achieve state-of-the-art results . Recently, the
XNOR-Net method has been studied to be transformed from a binarization
task to a ternarization task . Values are constrained in a ternary space -1,
0, +1. It allows to remove the need for full-precision values during the training
by using a discretization method.
3 Architecture optimization
Compression methods are widely studied. In present times, some of them are
part of popular deep learning frameworks. Tensorﬂow Lite has tools to
quantify models, allowing to transfer models to mobile devices easier. Core
ML , the Apple framework for deep learning, is also able to apply some
of these methods on the devices of the brand. Thus, on the one hand, a few
compression techniques are already integrated in useful tools for developers
but on the other hand, we are still quite far from understanding the intricacies
of deep neural models.
However, these methods are usually applied on already constructed models as they aim to reduce their complexity. Thereby, recent research focuses
directly on the architectures of these deep models, i.e. creating optimized architectures from the ground-up instead of ﬁnding methods to optimize them
afterwards. This section of the survey is adressing these approaches. Firstly,
a review of optimized architectures and modules to obtain eﬃcient models is
performed. Secondly, we present neural architecture search (NAS) methods to
construct models "from scratch".
3.1 Architecture overview
To begin with, convolution operations are responsible for an important fraction of the computation time in a network. In early works of LeCun et al. ,
5x5 and 3x3 ﬁlters are used. Although, some common deep models use larger
kernels (e.g. Alexnet ), recent works recommend the use of 3x3 ﬁlters (e.g.
VGG ). An architecture like GoogleNet even use 1x1 ﬁlters. GoogleNet
introduced the idea of modules, followed by ResNet and DenseNet .
Modules are blocks composed of multiple convolution layers with diﬀerent sizes
and with a speciﬁc organization.
Title Suppressed Due to Excessive Length
Fig. 5: Architecture of the SqueezeNet Fire module. Figure from .
Using the concept of modules, Iandola et al. designed an architecture
called SqueezeNet, which relies on a particular organisation of its layers. The
ﬁre module (see Figure 5) allowed to decrease the number of parameters of
the network which helped to reduce the model size. The design strategy of this
module is based on three main choices:
– the use of 1x1 ﬁlters to replace most of the 3x3 ﬁlters that are usually
present in CNNs,
– decreasing the number of input channels with 3x3 ﬁlters,
– downsampling later in the network in order to have convolution layers with
larger activation maps.
The ﬁrst two choices are aiming to reduce the global number of parameters.
The third point improves the classiﬁcation accuracy due to the large activation
maps induced by the 1x1 ﬁlters and the delay of the downsampling step .
Thereby, the ﬁre module is composed of a squeeze convolution layer with only
1x1 ﬁlters followed by an expand layer incorporating a mix of 1x1 and 3x3
ﬁlters. The ﬁnal SqueezeNet model is 50 times smaller than AlexNet while
maintaining the same accuracy.
This architecture has been taken one step further by Nanafack et al. to
create the Squeeze-SegNet architecture, a deep fully convolutional neural network for pixel-wise semantic segmentation. This model is an encoder-decoder
style network. The encoder part is similar to the SqueezeNet architecture while
the decoder part is composed of inverted ﬁre and convolutional layers proposed
by the authors and inspired by the SqueezeNet architecture. Thus, the inverted
ﬁre module is called a DFire module, which is a series of alternating expand
and squeeze modules. Both of these modules are retrieved from SqueezeNet.
The downsampling stages are replaced by upsampling steps as the model needs
to produce dense activation maps. Inspired by SegNet , the Squeeze-SegNet
Anthony Berthelier et al.
model is able to get the same level of accuracy as SegNet on a dataset like
CamVid with 10 times fewer parameters.
In 2012, Mamalet et al. introduced the simpliﬁcation of convolutional
ﬁlters by using separable convolution layers. This work was improved by
Howard et al. in the MobileNet model. Inspired by the work of Chollet , the core layers of their architecture is based on depthwise separable
features . Stated diﬀerently, the convolution step is factorized into two
separate steps to decrease the computation time taken by multiplication operations. Firstly, a depth-wise convolution applies a single ﬁlter to each input
channel. Secondly, a point-wise convolution applies a 1x1 convolution in order
to combine the outputs of the depth-wise convolution. This factorisation introduced in drastically reduces the computational cost of the convolutions.
Convolution
Convolution
Deconvolution
Deconvolution
Fig. 6: Simpliﬁed architecture of the Squeeze-SegNet network. Figure inspired
Separable convolution layers have become an eﬀective solution to accelerate convolution operations. Zhang et al. also investigated this path with
a neural network called ShuﬄeNet by adding to the depth-wise separable features a ShuﬄeNet unit. This unit allows the model to shuﬄe channels for
group convolutions. Usually, each output channel is only related to a group of
input channels. Here, we suppose a convolution layer with g ∗n channels and
g groups. The output channel dimension is ﬁrst reshaped into (g, n) and then
transposed and ﬂattened as the input of the next layer. Compared to other
models, the complexity is widely reduced. Compared to MobileNet , the
eﬃciency and accuracy are slightly improved.
In section 2.6, we presented methods to binarize deep models. Inspired by
this approach, Bulat et al. developed a binary version of an architecture
called the stacked hourglass network , a state of the art model in human
pose estimation. The main contribution of the binarized model is to improve
a bottleneck layer by limiting 1x1 ﬁlters and augmenting skip layers to limit
the loss of binary information. On 3D face alignment, this model outperforms
the current best performing methods up to 35%. However on human pose
estimation tasks, the binary model is far behind the real-valued version. Thus
there is still room for improvements on binary networks. It is important to note
that an architecture can be changed and improved in order to use parameters
Title Suppressed Due to Excessive Length
with limited numerical precision. Compression and architecture design are
deeply intertwined.
3.2 Neural architecture search
These past few years, the understanding of deep networks has grown due to
the development of new modules and architectures. However, knowing which
model to use for a speciﬁc idea is still a diﬃcult task. Tasks have become
more challenging and to overcome them, the key is to ﬁnd an architecture to
ﬁt them perfectly. But the more challenging is the task, the more diﬃcult it
is to design a network "by hand". Since constructing a proper architecture
can be time-consuming, work has been done to study the possibility of letting
networks automatically grow, adapt or even construct their own architectures.
It is interesting to note that the ﬁrst works in this ﬁeld were oriented around
physics and biology. Rosenblatt Kohonen or Willshaw and al. were
associating the organisation of the brain structure to the neural networks in
order to ﬁnd theoretical self-organising processes. Since then, numerous works
on the subject have been done and they could be regrouped under the name
of neural architecture search (NAS).
We give an overview of diﬀerent methods in this ﬁeld. We begin by introducing NAS with the early works in the domain regarding neural gas, followed by the neuroevolution methods, inpired by genetical algorithms, and
the network morphism methods which aim to transform trained architectures.
In these methods, the designed architectures are mostly optimized to obtain
the best performance for a resulting task. However the size or memory consumption of these structures may not be optimized. Thus, in a last section we
describes supergraph methods capable of ﬁnding structure optimized on these
3.2.1 Neural gas
Followed by these initial works, the neural gas methods, introduced by Martinetz and Schulten were among the ﬁrst approaches to push forward the
idea of self-organized structures. they aimed to ﬁnd an optimal data representation based on features vectors. In the beginning of the 90’s, the works of
Fritzke B. studied the basis of self-organizing and incremental neural
networks by enhancing the neural gas methods into growing structures. The
authors mainly explored two ideas:
The ﬁrst one, described in , was to develop an unsupervized learning
approach for data visualisation, clustering and vector quantization to ﬁnd a
suitable architecture automatically. In this work, a neural network could be
seen as a graph where a controlled growth process is applied. Furthermore, a
supervized learning approach was also developed adding a radial basis function. This addition permitted, for the ﬁrst time, to add new units and to
supervise the training of the parameters at the same time. Moreover, the new
Anthony Berthelier et al.
units were not added randomly anymore, leading to small networks that were
able to generalise better. Their method was tested on vowel recognition problems and had better results than the nearest neighbour algorithm (the former
state-of-the-art technique).
The second idea described by Fritzke B. in is an extension of the ﬁrst
method based on a Hebb-like learning rule. As opposed to , the model has
no parameters which change overtime but is still able of continuous learning
until a performance criterion is met. From a theoretical point of view, these
research works helped to better understand how the information is passed and
transmitted inside a network.
3.2.2 Neuroevolution
Genetic algorithms are a well-known technique to ﬁnd solutions of complex optimization problems. Adapted to deep networks, these methods were used to
design evolutionary architectures and named neuroevolution. The basic statement of the evolutionary methods is as follows: an evolving topology along with
weights should provide an advantage over evolving weights on a ﬁxed topology.
For decades, neuroevolution methods were successfully applied on sequential decision tasks. Part of this success comes from the fact that sequential
decision tasks are optimizing the weights of the neural networks instead of the
gradient descent. Stanley et al. went further ahead with a method called
NeuroEvolution of Augmenting Topologies (NEAT). This technique minimises
the dimensionality of the search space of connection weights, resulting in an
eﬃcient and rapid search of new topologies.
Crossover Points
OFFSPRINGS
Fig. 7: Example of a crossover step. The two parent structures (left) are randomly decomposed at certain points and reconstructed to build oﬀspring structures (right).
However, an important issue in neuroevolution is the permutation problem . In evolutionary methods, there is more than one way to express a
solution. In neural networks, it means that there is more than one architecture
Title Suppressed Due to Excessive Length
to express the same weight optimization problem. The basis of evolutionary
algorithms is to automatically design topologies that will compete with each
others. The best topologies (or solutions) are mixed together to obtain an even
better topology. This step is called a crossover (see Figure 7). These processes
are repeated until the solution can not be improved anymore. During, these
processes, the permutation problem is occurring when structures representing
the same solution do not have the same encoding. Indeed, if two topologies with
the same architecture but diﬀerent encoding are mixed with each other, the
resulting structure may lead to damaged structures and missing information
(see Figure 8). Thereby, these algorithms are following conventions for ﬁxed or
constrained topologies such as non-redundant genetic encoding . Stated
diﬀerently, it becomes impossible to obtain two similar structures. Nonetheless,
on neural networks where both weights and topologies are constantly evolving, these conventions may not be respected. Thus, using neural networks, the
permutation problem is diﬃcult to avoid. The work of Stanley et al. has
found one solution by keeping tracks of the history of the networks in order to
deﬁne which parts must be shuﬄed without losing information. However, this
method is memory-consuming due to the constant tracking of the networks.
As a consequence NEAT model is only successful with small networks.
Fig. 8: The two networks compute the same solution, even if their units are
appearing in a diﬀerent order. This is making crossover impossible or one of
the main unit will disappear. Figure from .
The NEAT model is enhanced in a method called CoDeepNEAT .
This improvement consists of a coevolution of components, topologies and hyperparameters. Moreover, the evolution and optimization are based on the
gradient, as opposed to previous methods where the optimization is based
on the weights. On the CIFAR-10 image recognition dataset, CoDeepNEAT
is able to automatically discover structures that have performances comparable to the state-of-the-art. Furthermore, on image captionning problem the
Anthony Berthelier et al.
approach is able to ﬁnd better structures than human design with enough
computation time. These methods are able to ﬁnd automatically adapted and
eﬃcient structure for a speciﬁc task. However the computational cost is expensive.
3.2.3 Network morphism
NAS is not only limited to neuroevolution methods. Network morphism is also an important part of the domain. This approach aims to modify
(to morph) a trained neural network into a new architecture. Thus, morphism
operations are applied on the network e.g inserting a layer or adding a skipconnection. As a consequence, the main diﬃculty is to determine which operation should be applied. In , the authors use Bayesian optimization
and select the most promising operations each time in the search space. The
upside of this technique is that it does not need an important additional number of epochs to be operational. However, as the morphism operations are
limited to a layer level, the topologies of the networks are constrained to be
chain-structured. As most of the state-of-the-art networks are in multi-path
structures, this is an noticeable limitation. Nonetheless, Cai et al. were
able to expand the search space to multi-path structures by allowing weight
reusing and tree-structured architectures. As a result, expensive computational
resources to achieve these searches are needed.
3.2.4 Supergraphs
Nowadays, an important number of NAS techniques and numerous other methods can be found in the literature. One of the principal issues that these methods are confronting with is the enormous size of the search space of possible
solutions. Theoretically, this search space is inﬁnite. Thereby, it is necessary to
limit this space. In the methods described in the previous section, this limitation is mainly done by limiting and controlling the number of operations that
could be done during the evolution of the networks. However an alternative
approach would be not to limit the operations but the search space where they
are operating. This is the idea behind supergraphs.
A supergraph is a large computational graph, where each subgraph is a neural
network (see Figure 9 for an example). Thus, the supergraph is becoming the search space and the solution to the task at hand will be one of its
subgraphs. The beneﬁt of this method is the reduced computational capacity
needed as the solution is searched in a smaller space than in other methods.
In recent times the work of Pham et al. have decreased the computation
resources required to produce networks with strong performance. Their idea
is as follows: all the subgraph (child models) produced by the supergraph are
sharing parameters to avoid a constant retraining from scratch. We could argue that sharing parameters between diﬀerent models could lead to numerous
mistakes. However, transfer learning methods have shown that parameters
learned for a speciﬁc task can be used by other models on other tasks [13,
Title Suppressed Due to Excessive Length
111]. Thus, in principle the fact that child models are using their parameters
for diﬀerent purposes is not a problem.
Fig. 9: Example of subgraph found via the convolutional neural fabrics
method . All edges are convolutionnal layers. All edges are oriented to
the right. Feature map size of each layers are given by height. Thus, red and
green are seven-layer convolutional layers and blue is a ten-layer convolutionaldeconvolutional network. Figure from .
A diﬀerent approach was developed by Veniat et al. in their method
called Budgeted Super Networks (BSN). Here, the supergraph is deﬁning a
set of possible architectures but a maximum authorized cost must also be de-
ﬁned. This cost is directly related to the performances, computation capability
and memory consumption, allowing the authors to chose which criteria the algorithm must privilege during the search of the solution. To achieve this, a
stochastic model is proposed and is optimized using policy-gradient-inspired
methods. Moreover, the authors have also demonstrated that the solution of
their stochastic model corresponds to the optimal constrained architecture of
the supergraph. Tested on the CIFAR image classiﬁcation and segmentation
datasets, their solution was able to converge to design architectures similar to
the ResNet model . Furthermore, the designed models have the same computational cost or memory consumption than the original ResNet but with a
better accuracy.
Several conclusion can be drawn from these experiments. Firstly, constraining the search of optimized architectures leads to a reduction of the computational capability needed for this search. Secondly, it also permits to obtain
networks that have a limited computation and memory cost with negligible
compromise on the accuracy.
Tan et al. have pushed their research in this direction. The authors
proposed an automated NAS speciﬁcally for mobile devices called MnasNet.
In their work the accuracy and the inference latency of the model on a mobile
device are both taken into account for the optimization of the model. Thus,
the model is trying to ﬁnd the best trade-oﬀbetween these two criterias. In
Anthony Berthelier et al.
order to reach this balance, two diﬀerent hierarchical search spaces are used:
one to factorise a deep network into a sequence of blocks and another one to
determine the layer architecture for each block. Thereby, diﬀerent layers are
able to use diﬀerent operations but all the layers in one block are sharing the
same structure. The adapted block is chosen at diﬀerent depth of the network
to reduce the overall latency. These improvements allow to reduce the search
space while ﬁnding architecture that are performing better and faster than
MobileNets .
distillation
Using a deep CNN
to train a smaller
comparable
performances.
Diﬃcult for the tasks
classiﬁcation.
into a hash table.
parallelization;
Better data dispersion;
Less computation time.
Considerably slower if
the model is too sparse.
performance.
Signiﬁcant
reduction;
Compression
10x to 15x (up to 30x).
time-consuming;
interesting
sparse model.
Quantization Reducing the number of distinct neurons by gathering
them into clusters.
High compression rate :
10x to 15x; Can be coupled with pruning.
Considerably slower if
the model is too sparse.
Decreasing the numerical precision of
the neurons.
High compression rate
and speed up.
parameters
Could require speciﬁc
hardwares.
Binarization
Decreasing the numerical precision of
the data to 2 bits.
Very high compression
rate (30x) and speed
up (50x to 60x).
parameters update.
Table 1: Summary of diﬀerent compression methods.
4 Discussion and Conclusion
In Table 1, we summarized and compared various deep-learning model compression methods from the literature discussed in this paper. These methods
aim to reduce the size, computation time or the memory employed by deep
models. However, a sparse model may not always be computationally eﬃcient.
Pruning and quantization can be utilized to achieve impressive performances
on trained models. However, they can easily lead to sparse model (same problem for hashing methods). In this case, binarization or reducing the numerical
Title Suppressed Due to Excessive Length
precision method can be one of the solutions. The speed gained for limiting
the numerical precision is important, especially if the structures is well designed. Nevertheless, higher precision is needed in some steps and accuracy
could vary signiﬁcantly. In the end, compressing a deep model will always lead
to a trade-oﬀbetween accuracy and computational eﬃciency.
Faster models provide a great beneﬁt for resource-limited devices and further work needs to be done in this direction if we want to leverage all of their
power on mobile devices. However, ﬁnding new methods to compress deep
models is not the only solution. We can focus on how the models are constructed beforehand. For example a simple architecture like SqueezeNet 
is able to reach the same accuracy as a deep model like AlexNet , but is 50
times smaller.
Compared to the size of the model, computational eﬃciency is crucial for
running such algorithms on mobile platforms. Despite the eﬀort on hardware
optimization, algorithmic optimizations like and recent works such as
Mobile-Net and Shuﬄe-Net have shown that it is promising to not only
compress models but also to construct them intelligently. Thus a well-designed
architecture is the ﬁrst key to optimized networks.
The works on NAS design an optimized architecture (performance-wise and
computational eﬃciency-wise) for a speciﬁc tasks. Though this is a challenging
exercise, some research works have already shown promising results through
new algorithms and theories like the lottery ticket hypothesis . All these
works are pushing forward the understanding of the mechanics behind deep
models and towards building optimized models capable of solving challenging
applications at a lower cost.
Acknowledgements: This work has been sponsored by the Auvergne Regional Council and the European funds of regional development (FEDER).