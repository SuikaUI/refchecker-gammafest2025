HAL Id: hal-00022528
 
Submitted on 5 Jul 2007
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Generalization error bounds in semi-supervised
classification under the cluster assumption
Philippe Rigollet
To cite this version:
Philippe Rigollet.
Generalization error bounds in semi-supervised classification under the cluster
assumption. Journal of Machine Learning Research, 2007, 8 (Jul), pp.1369–1392. ￿hal-00022528v4￿
Journal of Machine Learning Research X (200X) X-X
Submitted XX/XX; Published XX/XX
Generalization error bounds in semi-supervised classiﬁcation
under the cluster assumption
Philippe Rigollet
 
School of Mathematics
Georgia Institute of Technology
Atlanta, GA 30332-0160, U.S.A
Editor: G´abor Lugosi
We consider semi-supervised classiﬁcation when part of the available data is unlabeled.
These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger proposed the well-known cluster assumption as a reasonable one. We
propose a mathematical formulation of this assumption and a method based on density
level sets estimation that takes advantage of it to achieve fast rates of convergence both in
the number of unlabeled examples and the number of labeled examples.
Semi-supervised learning, statistical learning theory, classiﬁcation, cluster
assumption, generalization bounds.
1. Introduction
Semi-supervised classiﬁcation has been of growing interest over the past few years and many
methods have been proposed. The methods try to give an answer to the question: “How
to improve classiﬁcation accuracy using unlabeled data together with the labeled data?”.
Unlabeled data can be used in diﬀerent ways depending on the assumptions on the model.
There are mainly two approaches to solve this problem. The ﬁrst one consists in using the
unlabeled data to reduce the complexity of the problem in a broad sense. For instance,
assume that we have a set of potential classiﬁers and we want to aggregate them. In that
case, unlabeled data is used to measure the compatibility between the classiﬁers and reduces
the complexity of the set of candidate classiﬁers . Unlabeled data can also be used to reduce the dimension of the problem,
which is another way to reduce complexity. For example, in Belkin and Niyogi , it is
assumed that the data actually live on a submanifold of low dimension.
The second approach is the one that we use here. It assumes that the data contains
clusters that have homogeneous labels and the unlabeled observations are used to identify
these clusters. This is the so-called cluster assumption. This idea can be put in practice in
several ways giving rise to various methods. The simplest is the one presented here: estimate the clusters, then label each cluster uniformly. Most of these methods use Hartigan’s
 deﬁnition of clusters, namely the connected components of the density
level sets. However, they use a parametric–usually mixture–model to estimate the underc⃝200X Philippe Rigollet.
lying density which can be far from reality. Moreover, no generalization error bounds are
available for such methods. In the same spirit, Tipping and Rattray propose
methods that learn a distance using unlabeled data in order to have intra-cluster distances
smaller than inter-clusters distances. The whole family of graph-based methods aims also
at using unlabeled data to learn the distances between points. The edges of the graphs
reﬂect the proximity between points. For a detailed survey on graph methods we refer to
Zhu . Finally, we mention kernel methods, where unlabeled data are used to build
the kernel. Recalling that the kernel measures proximity between points, such methods
can also be viewed as learning a distance using unlabeled data .
The cluster assumption can be interpreted in another way, i.e., as the requirement that
the decision boundary has to lie in low density regions. This interpretation has been widely
used in learning since it can be used in the design of standard algorithms such as Boosting or SVM , which are closely related to kernel methods mentioned above. In these
algorithms, a greater penalization is given to decision boundaries that cross a cluster. For
more details, see, e.g., Seeger ; Zhu ; Chapelle et al. . Although most
methods make, sometimes implicitly, the cluster assumption, no formulation in probabilistic
terms has been provided so far. The formulation that we propose in this paper remains
very close to its original text formulation and allows to derive generalization error bounds.
We also discuss what can and cannot be done using unlabeled data. One of the conclusions is that considering the whole excess-risk is too ambitious and we need to concentrate
on a smaller part of it to observe the improvement of semi-supervised classiﬁcation over
supervised classiﬁcation.
Outline of the paper. After describing the model, we formulate the cluster assumption and
discuss why and how it can improve classiﬁcation performance in Section 2. The main result
of this section is Proposition 2.1 which essentially states that the eﬀect of unlabeled data on
the rates of convergence cannot be observed on the whole excess-risk. We therefore introduce
the cluster excess-risk which corresponds to a part of the excess-risk that is interesting for
this problem. In Section 3, we study the population case where the clusters are perfectly
known, to get an idea of our target. Indeed, such a population case corresponds in some
way to the case where the amount of unlabeled data is inﬁnite. Section 4 contains the
main result: after having deﬁned the clusters in terms of density level sets, we propose an
algorithm for which we derive rates of convergence for the cluster excess-risk as a measure
of performance. An example of consistent density level set estimators is given in Section 5.
Section 6 is devoted to a discussion on the choice of λ as well as possible implementations
and improvements. Proofs of the results are gathered in Section 7.
Notation. Throughout the paper, we denote positive constants by cj. We write Γc for the
complement of the set Γ. For two sequences (up)p and (vp)p (in that paper, p will be m
or n), we write up = O(vp) if there exists a constant C > 0 such that up ≤Cvp and we
write up = eO(vp) if up ≤C(log p)αvp for some constants α > 0, C > 0. Moreover, we write
up = o(vp), if there exists a non negative sequence (εp)p that tends to 0 when p tends to
inﬁnity and such that |up| ≤εp|vp|. Thus, if up = eO(vp), we have up = o(vppβ), for any
Generalization error bounds in semi-supervised classification
2. The model
Let (X, Y ) be a random couple with joint distribution P, where X ∈X ⊂IRd is a vector
of d features and Y ∈{0, 1} is a label indicating the class to which X belongs.
distribution P of the random couple (X, Y ) is completely determined by the pair (PX, η)
where PX is the marginal distribution of X and η is the regression function of Y on X,
i.e., η(x) ≜P(Y = 1|X = x). The goal of classiﬁcation is to predict the label Y given the
value of X, i.e., to construct a measurable function g : X →{0, 1} called a classiﬁer. The
performance of g is measured by the average classiﬁcation error
R(g) ≜P (g(X) ̸= Y ) .
A minimizer of the risk R(g) over all classiﬁers is given by the Bayes classiﬁer g⋆(x) =
1I{η(x)≥1/2}, where 1I{·} denotes the indicator function. Assume that we have a sample of
n observations (X1, Y1), . . . , (Xn, Yn) that are independent copies of (X, Y ). An empirical
classiﬁer is a random function ˆgn : X →{0, 1} constructed on the basis of the sample
(X1, Y1), . . . , (Xn, Yn). Since g⋆is the best possible classiﬁer, we measure the performance
of an empirical classiﬁer ˆgn by its excess-risk
E(ˆgn) = IEnR(ˆgn) −R(g⋆) ,
where IEn denotes the expectation with respect to the joint distribution of the sample
(X1, Y1), . . . , (Xn, Yn). We denote hereafter by IPn the corresponding probability.
In many applications, a large amount of unlabeled data is available together with a small
set of labeled data (X1, Y1), . . . , (Xn, Yn) and the goal of semi-supervised classiﬁcation is
to use unlabeled data to improve the performance of classiﬁers. Thus, we observe two independent samples Xl = {(X1, Y1), . . . , (Xn, Yn)} and Xu = {Xn+1, . . . , Xn+m}, where n is
rather small and typically m ≫n. Most existing theoretical studies of supervised classi-
ﬁcation use empirical processes theory to obtain rates of convergence for the excess-risk that are
polynomial in n. Typically these rates are of the order O(1/√n) and can be as small as
eO(1/n) under some low noise assumptions . However, simulations indicate that much faster rates should be attainable
when unlabeled data is used to identify homogeneous clusters. Of course, it is well known
that in order to make use of the additional unlabeled observations, we have to make an
assumption on the dependence between the marginal distribution of X and the joint distribution of (X, Y ) . Seeger formulated the rather
intuitive cluster assumption as follows1
Two points x, x′ ∈X should have the same label y if there is a path between
them which passes only through regions of relatively high PX.
This assumption, in its raw formulation cannot be exploited in the probabilistic model
since (i) the labels are random variables Y, Y ′ so that the expression “should have the same
label” is meaningless unless η takes values in {0, 1} and (ii) it is not clear what “regions of
relatively high PX” are. To match the probabilistic framework, we propose the following
modiﬁcations.
1. The notation is adapted to the present framework.
(i) Assume P[Y = Y ′|X, X′ ∈C] ≥P[Y ̸= Y ′|X, X′ ∈C], where C is a cluster.
(ii) Deﬁne “regions of relatively high PX” in terms of density level sets.
Assume for the moment that we know what the clusters are, so that we do not have to
deﬁne them in terms of density level sets. This will be done in Section 4. Let T1, T2, . . . , be
a countable family of subsets of X. We now make the assumption that the Tj’s are clusters
of homogeneous data.
Cluster Assumption (CA) Let T1, T2, . . . , be a collection of measurable sets (clusters)
such that Tj ⊂X, j = 1, 2, . . . Then the function x ∈X 7→1I{η(x) ≥1/2} takes a
constant value on each of the Tj, j = 1, 2, . . ..
It is not hard to see that the cluster assumption (CA) is equivalent to the following
assumption.
Let Tj, j = 1, 2, . . . , be a collection of measurable sets such that Tj ⊂X, j =
1, 2, . . . Then, for any j = 1, 2, . . ., we have
P[Y = Y ′|X, X′ ∈Tj] ≥P[Y ̸= Y ′|X, X′ ∈Tj] .
A question remains: what happens outside of the clusters? Deﬁne the union of the
and assume that we are in the problematic case, PX(Cc) > 0 such that the question makes
sense. Since the cluster assumption (CA) says nothing about what happens outside of the
set C, we can only perform supervised classiﬁcation on Cc. Consider a classiﬁer ˆgn,m built
from labeled and unlabeled samples (Xl, Xu) pooled together. The excess-risk of ˆgn,m can
be written ,
E(ˆgn,m) = IEn,m
|2η(x) −1|1I{ˆgn,m(x)̸=g⋆(x)}dPX(x) ,
where IEn,m denotes the expectation with respect to the pooled sample (Xl, Xu). We denote
hereafter by IPn,m the corresponding probability. Since, the unlabeled sample is of no help
to classify points in Cc, any reasonable classiﬁer should be based on the sample Xl so that
ˆgn,m(x) = ˆgn(x), ∀x ∈Cc, and we have
E(ˆgn,m) ≥IEn
Cc |2η(x) −1|1I{ˆgn(x)̸=g⋆(x)}dPX(x) .
Since we assumed PX(Cc) ̸= 0, the RHS of (2) is bounded from below by the optimal rates
of convergence that appear in supervised classiﬁcation.
The previous heuristics can be stated more formally as follows. Recall that the distribution P of the random couple (X, Y ) is completely characterized by the couple (PX, η)
where PX is the marginal distribution of X and η is the regression function of Y on X. In
the following proposition, we are interested in a class of distributions with cylinder form,
i.e. a class D that can be decomposed as D = M × Ξ where M is a ﬁxed class of marginal
distributions on X and Ξ is a ﬁxed class of regression functions on X with values in .
Generalization error bounds in semi-supervised classification
Proposition 2.1 Fix n, m ≥1 and let C be a measurable subset of X. Let M be a class of
marginal distributions on X and let Ξ be a class of regression functions. Deﬁne the class of
distributions D as D = M × Ξ. Then, for any marginal distribution P 0
X ∈M, we have
Cc |2η −1|1I{Tn̸=g⋆}dP 0
Cc |2η −1|1I{Tn,m̸=g⋆}dPX ,
where infTn,m denotes the inﬁmum over all classiﬁers based on the pooled sample (Xl, Xu)
and infTn denotes the inﬁmum over all classiﬁers based only on the labeled sample Xl.
The main consequence of Proposition 2.1 is that even when the cluster assumption (CA)
is valid the unlabeled data are useless to improve the rates of convergence. If the class
M is reasonably large and satisﬁes P 0
X(Cc) > 0, the left hand side in (3) can be bounded
from below by the minimax rate of convergence with respect to n, over the class D. Indeed
a careful check of the proofs of minimax lower bounds reveals that they are constructed
using a single marginal P 0
X that is well chosen.
These rates are typically of the order
n−α, 0 < α ≤1 ; Tsybakov ; Audibert and
Tsybakov and Boucheron et al. for a comprehensive survey).
Thus, unlabeled data do not improve the rate of convergence of this part of the excessrisk. To observe the eﬀect of unlabeled data on the rates of convergence, we have to consider
the cluster excess-risk of a classiﬁer ˆgn,m deﬁned by
EC(ˆgn,m) ≜IEn,m
|2η(x) −1|1I{ˆgn,m(x)̸=g⋆(x)}dPX(x) .
We will therefore focus on this measure of performance. The cluster excess-risk can also
be expressed in terms of an excess-risk. To observe it, deﬁne the set GC of all classiﬁers
restricted to C:
g : C →{0, 1}, g measurable
The performance of a classiﬁer g ∈GC is measured by the average classiﬁcation error on C
 g(X) ̸= Y
 g(X) ̸= Y, X ∈C
A minimizer of R(·) over GC is given g⋆
|C(x) = 1I{η(x)≥1/2}, x ∈C, i.e., the restriction of the
Bayes classiﬁer to C. Now it can be easily shown that for any classiﬁer g ∈GC we have,
R(g) −R(g⋆
|2η(x) −1|1I{g(x)̸=g⋆
|C(x)}dPX(x) .
Taking expectations on both sides of (5) with g = ˆgn,m, it follows that
IEn,mR(ˆgn,m) −R(g⋆
|C) = EC(ˆgn,m) .
Therefore, cluster excess-risk equals the excess-risk of classiﬁers in GC. In the sequel, we
only consider classiﬁers ˆgn,m ∈GC, i.e., classiﬁers that are deﬁned on C.
We now propose a method to obtain good upper bounds on the cluster excess-risk, taking
advantage of the cluster assumption (CA). The idea is to estimate the regions where the
sign of (η −1/2) is constant and make a majority vote on each region.
3. Results for known clusters
Consider the ideal situation where the family T1, T2, . . ., is known and we observe only the
labeled sample Xl = {(X1, Y1), . . . , (Xn, Yn)}. Deﬁne
Under the cluster assumption (CA), the function x 7→η(x)−1/2 has constant sign on each
Tj. Thus a simple and intuitive method for classiﬁcation is to perform a majority vote on
For any j ≥1, deﬁne δj ≥0, δj ≤1 by
|2η(x) −1|PX(dx) .
We now deﬁne our classiﬁer based on the sample Xl . For any j ≥1, deﬁne the random
(2Yi −1) 1I{Xi∈Tj} ,
and denote by ˆgj
n the function ˆgj
n(x) = 1I{Zj
n>0} , for all x ∈Tj . Consider the classiﬁer
deﬁned on C by
n(x)1I{x∈Tj} ,
The following theorem gives rates of convergence for the cluster excess-risk of the classiﬁer
ˆgn under (CA) that can be exponential in n under a mild additional assumption.
Theorem 3.1 Let Tj, j ≥1 be a family of measurable sets that satisfy Assumption (CA).
Then, the classiﬁer ˆgn deﬁned above satisﬁes
EC(ˆgn) ≤2
Moreover, if there exists δ > 0 such that δ = infj{δj : δj > 0}, we obtain an exponential
rate of convergence:
EC(ˆgn) ≤2e−nδ2/2 .
In a diﬀerent framework, Castelli and Cover have proved that exponential
rates of convergence were attainable for semi-supervised classiﬁcation. A rapid overview of
the proof shows that the rate of convergence e−nδ2/2 cannot be improved without further
assumption.
It will be our target in semi-supervised classiﬁcation.
However, we need
estimators of the clusters Tj, j = 1, 2, . . .. In the next section we provide the main result on
semi-supervised learning, that is when the clusters are unknown but we can estimate them
using the unlabeled sample Xu.
Generalization error bounds in semi-supervised classification
4. Main result
We now deal with a more realistic case where the clusters T1, T2, . . . , are unknown and we
have to estimate them using the unlabeled sample Xu = {X1, . . . , Xm}. We begin by giving
a deﬁnition of the clusters in terms of density level sets. In this section, we assume that X
has ﬁnite Lebesgue measure.
4.1 Deﬁnition of the clusters
Following Hartigan , we propose a deﬁnition of clusters that is also compatible with
the expression “regions of relatively high PX” proposed by Seeger .
Assume that PX admits a density p with respect to the Lebesgue measure on IRd denoted
hereafter by Lebd. For a ﬁxed λ > 0, the λ-level set of the density p is deﬁned by
Γ(λ) = {x ∈X : p(x) ≥λ} .
On these sets, the density is relatively high. The cluster assumption involves also a notion of
connectedness of a set. For any C ⊂X, deﬁne the binary relation R on any set C as follows:
two points x, y ∈C satisfy xRy if and only if there exists a continuous map f : →C,
such that f(0) = x and f(1) = y. If xRy, we say that x and y are pathwise connected.
It can be easily show that R is an equivalence relation and its classes of equivalence are
called connected components of C. At this point, in view of the formulation of the cluster
assumption, it is very tempting to deﬁne the clusters as the connected components of C.
However, this deﬁnition suﬀers from two major ﬂaws:
1. a connected set cannot be deﬁned up to a set of null Lebesgue measure.
consider for example the case d = 1 and C = . This set is obviously connected
(take the map f equal to the identity on ) but the set eC = C \ {1/2} is not
connected anymore even though C and eC only diﬀer by a set of null Lebesgue measure.
In our setup we want to impose connectedness on certain subsets of the λ-level set of
the density p which is actually deﬁned up to a set of null Lebesgue measure. Figure 1
(left) is an illustration of a set with one connected component whereas it is desirable
to have two clusters.
2. There is no scale consideration in this deﬁnition of clusters. When two clusters are
too close to each other in a certain sense, we wish identify them as a single cluster.
In Figure 1 (right), the displayed set has two connected components whereas we wish
to identify only one cluster.
To ﬁx the ﬁrst ﬂaw, we introduce the following notions. Let B(z, r) be the d-dimensional
closed ball of center z ∈IRd and radius r > 0, deﬁned by
x ∈IRd : ∥z −x∥≤r
where ∥· ∥denotes the Euclidean norm in IRd.
Deﬁnition 4.1 Fix r0 ≥0 and let ¯d be an integer such that ¯d ≥d. We say that a measurable set C ⊂X is r0-standard if for any z ∈C and any 0 ≤r ≤r0, we have
 B(z, r) ∩C
We now comment upon this deﬁnition.
Remark 4.1 The deﬁnition of a standard set has been introduced by Cuevas and Fraiman
 . This deﬁnition ensures that the set C has no “ﬂat” parts which allows to exclude
pathological cases such as the one presented on the left hand side of Figure 1.
Remark 4.2 The constant c0 may depend on r0 and this avoids large-scale shape considerations. Indeed, if the set C is bounded, then for any z ∈C, Lebd
 B(z, r) ∩C
for r ≥r0 where r0 is the diameter of C. Thus for C to be r0-standard, we have to impose
at least that c0 ≤Lebd
Remark 4.3 The case ¯d > d allows us to include a wide variety of shapes in this deﬁnition.
Consider the following example where d = 2:
(x, y) : −1 ≤x ≤1, 0 ≤y ≤|x|δ
2 and consider the point z = (0, 0). It holds
 B(z, r) ∩Cδ
−r′ min(|x|δ, r′)dx ,
where r′ =
For any |x| ≤r′ ≤1, we have |x|δ ≥|x|(δ∨1) and |x|(δ∨1) ≤|r′|(δ∨1) ≤r′. Thus
 B(z, r) ∩Cδ
−r′ |x|(δ∨1)dx = 2(r′)(δ∨1)+1 .
We conclude that (9) is satisﬁed at z = (0, 0) for ¯d = (δ ∨1) + 1. However, notice that
 B(z, r) ∩Cδ
|x|δdx = 2r(δ+1) .
Thus (9) is not satisﬁed at z = (0, 0) when ¯d = d, if δ > 1.
To overcome the scale problem described in the second ﬂaw, we introduce the notion of
s0-separated sets.
Deﬁne the pseudo-distance distance d∞, between two sets C1 and C2 by
d∞(C1, C2) = inf
We say that two sets C1, C2, are s0-separated if d∞(C1, C2) > s0, for some s0 ≥0. More
generally, we say that the sets C1, C2, . . . are mutually s0-separated if for any j ̸= j′, Cj and
Cj′ are s0-separated. On the right hand side of Figure 1, we show an example of two sets
that are not s0-separated for a reasonable s0. In that particular example, if s0 is suﬃciently
small, we would like to identify a single cluster.
We now deﬁne s0-connectedness which is a weaker version of connectedness in the form
of a binary relation
Generalization error bounds in semi-supervised classification
Deﬁnition 4.2 Fix s > 0 and let
be the binary relation deﬁned on C ⊂X as follows:
two points x, y ∈C satisfy x
y if and only if there exists a piecewise constant map
f : →C such that f(0) = x and f(1) = y and such that f has a ﬁnite number of
jumps that satisfy ∥f(t+) −f(t−)∥≤s for any t ∈ , where
f(t+) = lim
f(t−) = lim
y, we say that x and y are s-connected.
Note that x and y are s-connected if and only if there exists z1, . . . , zn ∈C such that
∥x −z1∥≤s, ∥y −zn∥≤s and ∥zi −zi+1∥≤s for any j = 1, . . . , n −1. In other words,
there exists a ﬁnite sequence of points in C that links x to y and such that two consecutive
points in this sequence have distance smaller that s.
Lemma 4.1 Fix s > 0, then the binary relation
is an equivalence relation and C can
be partitioned into its classes of equivalence. The classes of equivalence of
are called
s-connected components of C.
In the next proposition we prove that given a certain scale s > 0, it is possible split a
r0-standard and closed set C into a unique partition that is a coarser than the partition
deﬁned by the connected components of C and that this partition is ﬁnite for such sets.
Proposition 4.1 Fix r0 > 0, s > 0 and assume that C is a r0-standard and closed set.
Then there exists a unique partition C1, . . . CJ, J ≥1, of C such that
• for any j = 1, . . . , J and any x, y ∈Cj, we have x
• the sets C1, . . . , CJ are mutually s-separated.
Remark 4.4 In what follows we assume that the scale s = s0 is ﬁxed by the statistician. It
should be ﬁxed depending on a priori considerations about the scale of the problem. Actually,
in the proof of Proposition 4.3, we could even assume that s0 = 1/(3 log m), which means
that we can have the scale depend on the number of observations. This is consistent with
the fact that the ﬁnite number of unlabeled observations allows us to have only a blurred
vision of the clusters. In this case, we are not able to diﬀerentiate between two clusters that
are too close to each other but our vision becomes clearer and clearer as m tends to inﬁnity.
We now formulate the cluster assumption when the clusters are deﬁned in terms of
density level sets. In the rest of the section, ﬁx λ > 0 and let Γ denote the λ-level set of the
density p. We also assume in what follows that Γ is closed which is the case if the density
p is continuous for example.
Strong Cluster Assumption (SCA) Fix s0 > 0 and r0 > 0 and assume that Γ admits
a version that is r0-standard and closed. Denote by T1, . . . , TJ the s0-connected components of this version of Γ. Then the function x ∈X 7→1I{η(x)≥1/2} takes a constant
value on each of the Tj, j = 1, . . . J.
Figure 1: A set that is not r0-standard for any r0 (left). A set that has two connected
components but only one s0-connected components (right).
4.2 Estimation of the clusters
Assume that p is uniformly bounded by a constant L(p) and that X is bounded. Denote
by IPm and IEm respectively the probability and the expectation w.r.t the sample Xu of size
m. Assume that we use the sample Xu to construct an estimator ˆGm of Γ satisfying
Lebd( ˆGm △Γ)
where △is the sign for the symmetric diﬀerence. We call such estimators consistent estimators of Γ. Recall that we are interested in identifying the s0-connected components
T1, . . . , TJ of Γ. That is, we seek a partition of ˆGm, denoted here by ˆH1, . . . , ˆHJ′ such that
for any j = 1, . . . , J, ˆHj is a consistent estimator of Tj and IEm
Lebd( ˆHj)
→0 for j > J.
From Proposition 4.1, we know that for any 1 ≤j, j′ ≤J, j ̸= j′, we have d∞(Tj, Tj′) > s0.
Let ¯s > s0 be deﬁned by
j̸=j′ d∞(Tj, Tj′) .
To deﬁne the partition ˆH1, . . . , ˆHJ′, it is therefore natural to use a suitable reordering of the
(s0 + um)-connected components of ˆGm, where um is a positive sequence that tends to 0 as
m tends to inﬁnity. Since the measure of performance IEm
Lebd( ˆGm △Γ)
is deﬁned up to
a set of null Lebesgue measure it may be the case that even an estimator ˆGm that satisﬁes
Lebd( ˆGm △Γ)
= 0 has only one (s0+um)-connected components whereas Γ has several
s0-connected components. This happens for example in the case where ˆGm = Γ∪R where R
is a set of thin ribbons with null Lebesgue measure that link the s0-connected components
of Γ to each other (see Figure 1, left). If ˆGm were r0-standard, such conﬁgurations would
Generalization error bounds in semi-supervised classification
not occur. To have ˆGm more “standard”, we apply the following clipping transformation:
deﬁne the set
Clip( ˆGm) =
x ∈ˆGm : Lebd
  ˆGm ∩B(x, (log m)−1)
≤(log m)−d
In the sequel, we will only consider the clipped version of ˆGm deﬁned by ˜Gm = ˆGm \
Clip( ˆGm). For any x ∈˜Gm, we have
  ˆGm ∩B(x, (log m)−1)
> (log m)−d
However, this is not enough to ensure that the union of several s0-connected components of
Γ is not estimated by a single (s0 + um)-connected component of ˜Gm due to the magnitude
of random ﬂuctuations of ˜Gm around Γ.
To ensure componentwise consistency, we make assumptions on the estimator ˆGm. Note
that the performance of a density level set estimator ˆGm is measured by the quantity
Lebd( ˆGm △Γ)
Lebd( ˆGm ∩Γc)
For some estimators, such as the oﬀset plug-in density level sets estimators presented in
Section 5, we can prove that the dominant term in the RHS of (12) is IEm
yields that the probability of having Γ included in the consistent estimator ˆGm is negligible.
We now give a precise deﬁnition of such estimators.
Deﬁnition 4.3 Let ˆGm be an estimator of Γ and ﬁx α > 0. We say that the estimator ˆGm
is consistent from inside at rate m−α if it satisﬁes
Lebd( ˆGm △Γ)
= eO(m−α) ,
Lebd( ˆGm ∩Γc)
= eO(m−2α) .
The following proposition ensures that the clipped version of an estimator that is consistent from inside is also consistent from inside at the same rate.
Proposition 4.2 Fix α > 0, s0 > 0 and let (um) be a positive sequence. Assume that X is
bounded and let ˆGm be an estimator of Γ that is consistent from inside at rate m−α. Then,
the clipped estimator ˜Gm = ˆGm\Clip( ˆGm) is also consistent from inside a rate m−α and has
a ﬁnite number ˜Km ≤Lebd(X)mα of (s0 + um)-connected components that have Lebesgue
measure greater than or equal to m−α. Moreover, the (s0 + um)-connected components of
˜Gm are mutually (s0 + θum)-separated for any θ ∈(0, 1).
We are now in position to deﬁne the estimators of the s0-connected components of Γ. Deﬁne
sm = s0+(3 log m)−1 and denote by ˜H1, . . . , ˜H ˜
Km the sm-connected components of ˜Gm that
have Lebesgue measure greater than or equal to m−α. The number ˜Km depends on Xu and
is therefore random but bounded from above by the deterministic quantity Lebd(X)mα.
Let J be a subset of {1, . . . , J}. Deﬁne κ(j) = {k = 1, . . . , ˜Km : ˜Hk ∩Tj ̸= ∅} and let
D(J ) be the event on which the sets κ(j), j ∈J are reduced to singletons {k(j)} that are
disjoint, i.e.,
κ(j) = {k(j)}, k(j) ̸= k(j′), ∀j, j′ ∈J , j ̸= j′o
κ(j) = {k(j)}, (Tj ∪˜Hk(j)) ∩(Tj′ ∪˜Hk(j′)) = ∅, ∀j, j′ ∈J , j ̸= j′o
In other words, on the event D(J ), there is a one-to-one correspondence between the
collection {Tj}j∈J and the collection
{ ˜Hk}k∈κ(j)
j∈J . Componentwise convergence of ˜Gm
to Γ, is ensured when D({1, . . . , J}) has asymptotically overwhelming probability.
following proposition ensures that D(J ) has large enough probability.
Proposition 4.3 Fix r0 > 0 and s0 ≥(3 log m)−1. Assume that there exists a version of Γ
that is r0-standard and closed. Then, denoting by J the number of s0-connected components
if Γ, for any J ⊂{1, . . . , J}, we have
 Dc(J )) = eO
where D(J ) is deﬁned in (13).
4.3 Labeling the clusters
From the strong cluster assumption (SCA) the clusters are homogeneous regions.
estimate the clusters, we apply the method described above that consists in estimating
the sm-connected components of the clipped estimator ˜Gm and keep only those that have
Lebesgue measure greater than or equal to m−α. Then we make a majority vote on each
homogeneous region. It yields the following procedure.
Three-step procedure
1. Use the unlabeled data Xu to construct an estimator ˆGm of Γ that is consistent from inside at rate m−α.
2. Deﬁne homogeneous regions as the sm-connected components of ˜Gm = ˆGm \
Clip( ˆGm) (clipping step) that have Lebesgue measure greater than or equal
3. Assign a single label to each estimated homogeneous region by a majority
vote on labeled data.
This method translates into two distinct error terms, one term in m and another term in
n. We apply our three-step procedure to build a classiﬁer ˜gn,m based on the pooled sample
(Xl, Xu). Fix α > 0 and let ˆGm be an estimator of the density level set Γ, that is consistent
from inside at rate m−α. For any 1 ≤k ≤˜Km, deﬁne the random variable
(2Yi −1) 1I{Xi∈˜
Generalization error bounds in semi-supervised classification
where ˜Hk is obtained by Step 2 of the three-step procedure. Denote by ˜gk
n,m the function
n,m(x) = 1I{Zkn,m>0} for all x ∈˜Hk and consider the classiﬁer deﬁned on X by
˜gn,m(x) =
n,m(x)1I{x∈˜
Note that the classiﬁer ˜gn,m assigns label 0 to any x outside of ˜Gm. This is a notational
convention and we can assign any value to x on this set since we are only interested in
the cluster excess-risk. Nevertheless, it is more appropriate to assign a label referring to a
rejection, e.g., the values “2”or “R” (or any other value diﬀerent from {0, 1}). The rejection
meaning that this point should be classiﬁed using labeled data only. However, when the
amount of labeled data is too small, it might be more reasonable not to classify this point
This modiﬁcation is of particular interest in the context of classiﬁcation with a
rejection option when the cost of rejection is smaller than the cost of misclassiﬁcation .
Remark that when there is only a ﬁnite number of
clusters, there exists δ > 0 such that
δj : δj > 0
Theorem 4.1 Fix α > 0 and assume that (SCA) holds. Consider an estimator ˆGm of Γ,
based on Xu that is consistent from inside at rate m−α. Then, the classiﬁer ˜gn,m deﬁned in
(14) satisﬁes
EΓ (˜gn,m) ≤eO
δje−n(θδj)2/2 ≤eO
+ e−n(θδ)2/2 ,
for any 0 < θ < 1 and where δ > 0 is deﬁned in (15).
Note that, since we often have m ≫n, the ﬁrst term in the RHS of (16) can be considered
negligible so that we achieve an exponential rate of convergence in n which is almost the
same (up to the constant θ in the exponent) as in the case where the clusters are completely
known. The constant θ seems to be natural since it balances the two terms.
5. Plug-in rules for density level sets estimation
Fix λ > 0 and recall that our goal is to use the unlabeled sample Xu of size m to construct
an estimator ˆGm of Γ = Γ(λ) = {x ∈X : p(x) ≥λ}, that is consistent from inside at rate
m−α for some α > 0 that should be as large as possible. A simple and intuitive way to
achieve this goal is to use plug-in estimators of Γ deﬁned by
ˆΓ = ˆΓ(λ) = {x ∈X : ˆpm(x) ≥λ} ,
where ˆpm is some estimator of p. A straightforward generalization are the oﬀset plug-in
estimators of Γ(λ), deﬁned by
˜Γℓ= ˜Γℓ(λ) = {x ∈X : ˆpm(x) ≥λ + ℓ} ,
where ℓ> 0 is an oﬀset. Clearly, we have ˜Γℓ⊂ˆΓ. Keeping in mind that we want estimators
that are consistent from inside we are going to consider suﬃciently large oﬀset ℓ= ℓ(m).
Plug-in rules is not the only choice for density level set estimation. Direct methods such
as empirical excess mass maximization are also popular. One advantage of plug-in rules over direct methods is that
once we have an estimator ˆpm, we can compute the whole collection {˜Γℓ(λ), λ > 0}, which
might be of interest for the user who wants to try several values of λ. Note also that a
wide range of density estimators is available in usual software. A density estimator can
be parametric, typically based on a mixture model, or nonparametric such as histograms
or kernel density estimators. In Section 6, we brieﬂy describe a possible implementation
based on existing software that makes use of kernel or nearest neighbors density estimators.
To conclude this discussion, remark that the greater ﬂexibility of plug-in rules may result
in a poorer learning performance and even though we do not discuss any implementation
based on direct methods, it may well be the case that the latter perform better in practice.
However, it is not our intent to propose here the best clustering algorithm or the best
density level set estimator and we present a simple proof of convergence for oﬀset plug-in
rules only for the sake of completeness.
The next assumption has been introduced in Polonik .
It is an analog of the
margin assumption formulated in Mammen and Tsybakov and Tsybakov but
for arbitrary level λ in place of 1/2.
Deﬁnition 5.1 For any λ, γ ≥0, a function f : X →IR is said to have γ-exponent at
level λ if there exists a constant c⋆> 0 such that, for all ε > 0,
Lebd {x ∈X : |f(x) −λ| ≤ε} ≤c⋆εγ .
When γ > 0 it ensures that the function f has no ﬂat part at level λ.
The next theorem gives fast rates of convergence for oﬀset plug-in rules when ˆpm satisﬁes
an exponential inequality and p has γ-exponent at level λ. Moreover, it ensures that when
the oﬀset ℓis suitably chosen, the plug-in estimator is consistent from inside.
Theorem 5.1 Fix λ > 0, γ > 0 and ∆> 0. Let ˆpm be an estimator of the density p based
on the sample Xu of size m ≥1 and let P be a class of densities on X. Assume that there
exist positive constants c1, c2 and a ≤1, such that for PX-almost all x ∈X, we have
IPm (|ˆpm(x) −p(x)| ≥δ) ≤c1e−c2maδ2 , m−a/2 < δ < ∆.
Assume further that p has γ-exponent at level λ for any p ∈P and that the oﬀset ℓis chosen
ℓ= ℓ(m) = m−a
Then the plug-in estimator ˜Γℓis consistent from inside at rate m−γa
2 for any p ∈P.
Consider a kernel density estimator ˆpK
m based on the sample Xu deﬁned by
Generalization error bounds in semi-supervised classification
where h > 0 is the bandwidth parameter and K : IRd →IR is a kernel. If p is assumed
to have H¨older smoothness parameter β > 0 and if K and h are suitably chosen, it is a
standard exercise to prove inequality of type (17) with a = 2β/(2β +d). In that case, it can
be shown that the rate m−γa
2 is optimal in a minimax sense .
6. Discussion
We proposed a formulation of the cluster assumption in probabilistic terms. This formulation relies on Hartigan’s deﬁnition of clusters but it can be modiﬁed to
match other deﬁnitions of clusters.
We also proved that there is no hope to improve the classiﬁcation performance outside
of these clusters.
Based on these remarks, we deﬁned the cluster excess-risk on which
we observe the eﬀect of unlabeled data. Finally we proved that when we have consistent
estimators of the clusters, it is possible to achieve exponential rates of convergence for the
cluster excess-risk. The theory developed here can be extended to any deﬁnition of clusters
as long as they can be consistently estimated.
Note that our deﬁnition of clusters is parametrized by λ which is left to the user, depending on his trust in the cluster assumption. Indeed, density level sets have the monotonicity
property: λ ≥λ′, implies Γ(λ) ⊂Γ(λ′). In terms of the cluster assumption, it means that
when λ decreases to 0, the assumption (SCA) concerns bigger and bigger sets Γ(λ) and
in that sense, it becomes more and more restrictive. As a result, the parameter λ can be
considered as a level of conﬁdence characterizing to which extent the cluster assumption is
valid for the distribution P and its choice is left to the user.
The choice of λ can be made by ﬁxing PX(C), where C is deﬁned in (1), the probability
of the rejection region. We refer to Cuevas et al. for more details. Note that datadriven choices of λ could be easily derived if we impose a condition on the purity of the
clusters, i.e., if we are given the δ in (15). Such a choice could be made by decreasing λ
until the level of purity is attained. However, any data-driven choice of λ has to be made
using the labeled data. It would therefore yield much worse bounds when n ≪m.
A possible implementation of the ideas presented in this paper can be designed using
existing clustering software such as dbscan ) or runt pruning . These
three algorithms implement clustering using a deﬁnition of clusters that involves density
level sets and a certain notion of connectedness. The idea is to use these algorithms on the
pooled sample of instances (X1, . . . , Xn+m, X), where X is the new instance to be classiﬁed.
As a result every instance will be aﬀected to a cluster by the chosen algorithm. The label
for X is then predicted using a majority vote on the labeled instances that are aﬀected to
the same cluster as X. Observe that unlike the method described in the paper, the clusters
depend on the labeled instances (X1, . . . , Xn). Proceeding so allows us to use directly existing clustering algorithms without any modiﬁcation. Since all three algorithms are distance
based, we could run them only on unlabeled instance and then aﬀect each labeled instance
and the new instance to the same cluster as its nearest neighbor. However, if we assume that
m ≫n, incorporating labeled instances will not signiﬁcantly aﬀect the resulting clusters.
We now describe more precisely why these algorithms produce estimated clusters that
are related to the sm-connected components of a plug-in estimator of the density level
set. Each algorithm has instances (X1, . . . , Xm) and several parameters described below as
inputs. Note that these clustering algorithms will aﬀect every instance to a cluster. This
can be transformed into our framework by removing clusters that contain only one instance.
• dbscan has two input parameters: a real number ε > 0 and and integer M ≥1.
The basic version of this algorithm proceeds as follows.
For a given instance Xi,
let Jε(i) ⊂{1, . . . , m} be the set of indexes j ̸= i such that ∥Xj −Xi∥≤ε.
card(Jε(i)) ≥M then all instances Xj, j ∈Jε(i) are aﬀected to the same cluster as
Xi and the procedure is repeated with each Xj, j ∈Jε(i). Otherwise a new cluster is
deﬁned and the procedure is repeated with another instance.
Observe ﬁrst that the instances Xj that satisfy ∥Xj −Xi∥≤ε are ε-connected to Xi.
Also, deﬁne the kernel density estimator ˆpm by:
where K : IRd →IR is deﬁned by K(x) = 1I{∥x∥≤1} for any x ∈IRd. Then card(Jε(i)) ≥
M is equivalent to ˆpm(Xi) ≥
Thus, if we chose s0 = ε −(3 log m)−1 and
λ + ℓ(m) = M+1
mεd , we see that dbscan implements our method. Conversely, for given
λ and s0, we can derive the parameters ε and M such that dbscan implements our
• optics is a modiﬁcation of dbscan that allows the user to compute in an eﬃcient
fashion all cluster partitions for diﬀerent ε ≤ε0 for some user speciﬁed ε0 > 0. The
user still has to input the chosen value for ε so that from our point of view, the two
algorithms are the same.
• Both of the previous algorithms suﬀer from a major drawback that is inherent to
our deﬁnition of cluster based on a global level when determining the density level
sets. Indeed, in many real data sets, some clusters can only be identiﬁed using several
density levels. Stuetzle recently described an algorithm called runt pruning
that is free from this drawback. Since, it does not implement our method, we do not
describe the algorithm in detail but mention it because it implements a more suitable
deﬁnition of clusters that is also based on connectedness and density level sets. In
particular it resolves the problem of choosing λ. It uses a nearest neighbor density
estimator as a running horse and uses a single input parameter that corresponds to
the scale s0.
This paper is an attempt to give a proper mathematical framework for the cluster
assumption proposed in Seeger . As mentioned above, the deﬁnition of clusters we
use here is one among several available and it could be interesting to modify the formulation
of the cluster assumption to match other deﬁnitions of cluster. In particular, the deﬁnition
of cluster as s0-connected components of the λ-level set of the density leaves the problem
of choosing λ correctly.
Acknowledgments.
The author is most indebted to anonymous referees that contributed to a signiﬁcant improvement of the paper through their questions and comments.
Generalization error bounds in semi-supervised classification
This section contains proofs of the results presented in the paper.
7.1 Proof of Proposition 2.1
Since the distribution of the unlabeled sample Xu does not depend on η, we have for any
marginal distribution PX,
Cc |2η −1|1I{Tn,m̸=g⋆}dPX = sup
Cc |2η −1|1I{Tn,m̸=g⋆}dPX
Cc |2η −1|1I{Tn,m̸=g⋆}dPX
Cc |2η −1|1I{Tn̸=g⋆}dPX ,
where in the last inequality, we used the fact that conditionally on Xu, the classiﬁer Tn,m
only depends on Xl and can therefore be written Tn.
7.2 Proof of Theorem 3.1
We can decompose EC(ˆgn) into
EC(ˆgn) = IEn
|2η(x) −1|1I{ˆgj
n(x)̸=g⋆(x)}p(x)dx .
Fix j ∈{1, 2, . . .} and assume w.l.o.g. that η ≥1/2 on Tj. It yields g⋆(x) = 1, ∀x ∈Tj,
and since ˆgn is also constant on Tj, we get
|2η(x) −1|1I{ˆgj
n(x)̸=g⋆(x)}p(x)dx = 1I{Zj
(2η(x) −1)p(x)dx
Taking expectation IEn on both sides of (20) we get
|2η(x) −1|1I{ˆgj
n(x)̸=g⋆(x)}p(x)dx ≤δjIPn
where we used Hoeﬀding’s inequality to get the last bound. Summing now over j yields the
7.3 Proof of Lemma 4.1
The binary relation
is an equivalence relation if it satisﬁes reﬂexivity, symmetry and
transitivity.
To prove, reﬂexivity, consider the trivial constant path f(t) = x for all t ∈ . We
immediately obtain that x
To prove symmetry, ﬁx x, y ∈C such that x
y and denote by f1 the piecewise
constant map with n1 jumps that satisﬁes f1(0) = x, f1(1) = y and ∥f1(t+) −f1(t−)∥≤s.
It is not diﬃcult to see that the map ˜f1 deﬁned by ˜f1(t) = f1(1 −t) for any t ∈ is
piecewise constant with n1 jumps, satisﬁes ˜f1(0) = y, ˜f1(1) = x and ∥˜f1(t+) −˜f1(t−)∥≤s
for any t ∈ , so that y
To prove transitivity, let z ∈C be such that y
z and let f2 be a piecewise constant
map with n2 jumps that satisﬁes f2(0) = y, f2(1) = z and ∥f2(t+), f2(t−)∥< s for any
t ∈ . Let now f : →X be the map deﬁned by:
if t ∈[0, 1/2]
if t ∈[1/2, 1] .
This map is obviously piecewise constant with n1+n2 jumps and satisﬁes f(0) = x, f(1) = z.
Moreover, for any t ∈ , f satisﬁes ∥˜f(t+), ˜f(t−)∥≤s.
is an equivalence relation and C can be partitioned into its classes of equivalence.
7.4 Proof of Proposition 4.1
From Lemma 4.1, we know that
is an equivalence relation and C can be partitioned
into its classes of equivalence denoted by C1, C2, . . .. The classes of equivalences C1, C2, . . .
obviously satisfy the ﬁrst point of Proposition 4.1 from the very deﬁnition of a class of
equivalence.
To check the second point, remark ﬁrst that since C is a closed set, each Cj, j ≥1 is also
a closed set. Indeed, ﬁx some j ≥1 and let (xn, n ≥1) be a sequence of points in Cj that
converges to ¯x. Since C is closed, we have ¯x ∈C so there exists j′ ≥1 such that ¯x ∈Cj′.
If j ̸= j′, then ∥xn −¯x∥> s for any n ≥1 which contradicts the fact that xn converges to
¯x. Therefore, ¯x ∈Cj and Cj is closed. Then let Cj and Cj′, be two classes of equivalence
such that d∞(Cj, Cj′) ≤s. Using the fact that Cj and Cj′ are closed sets, we conclude that
there exist x ∈Cj and x′ ∈Cj′ such that ∥x −x′∥≤s and hence that x
Cj = Cj′ and we conclude that for any Cj, Cj′, j ̸= j′, we have d∞(Cj, Cj′) > s and the Cj
are mutually s-separated.
We now prove that the decomposition is ﬁnite. Since the Cj are mutually s-separated,
for any 1 ≤j ≤k, for any xj ∈Cj, the Euclidean balls B(xj, s/3) are disjoint. Using the
facts that X is bounded and that C is r0-standard we obtain,
∞> Lebd(X) ≥
B(xj, s/3) ∩X
B(xj, s/3) ∩C
for a positive constant c. Thus we proved the existence of a ﬁnite partition
Generalization error bounds in semi-supervised classification
It remains to prove that this partition is unique.
To this end, we make use of the
fundamental theorem of equivalence relations which states that any partition of C corresponds to the classes of equivalences of
a unique equivalence relation. Let P′ = {C′
1, . . . , C′
J′} be a partition of C that satisﬁes the
two points of Proposition 4.1 and denote by R′ the corresponding equivalence relation. We
now prove that
C ≡R′. From the ﬁrst point of Proposition 4.1, we easily conclude that
if xR′y then x
y. Now if we choose x, y ∈C such that xR′y does not hold, then there
exist j ̸= j′ such that x ∈C′
j and y ∈C′
j′. If we had x
y, it would hold d∞(C′
which contradicts the second point of Proposition 4.1 so x
y does not hold. As a
consequence we have proved that for any x, y ∈C, xRy if and only if x
y and the two
relations are the same so as their classes of equivalence. This allows us to conclude that
7.5 Proof of Proposition 4.2
Consider a regular grid G on IRd with step size 1/ log(m) and observe that the Euclidean
balls of centers in ˜G = G ∩Clip( ˆGm) and radius
d/ log(m) cover the set Clip( ˆGm). Since
X is bounded, there exists a constant c1 > 0 such that card{ ˜G} = c1(log m)d. Therefore
Lebd(Clip( ˆGm)) ≤
d/ log(m)) ∩ˆGm
≤c2(log m)d−¯d
for some positive constant c2. Therefore, the rate of convergence ˜Gm is the same as that of
ˆGm. Observe also that ˜Gm ⊂ˆGm, so that ˜Gm is also consistent from inside.
Assume that ˜Gm can be decomposed in at least a number k of (s0 + um)-connected
components, ˜H1, . . . , ˜Hk with Lebesgue measure greater than or equal to m−α. It holds
∞> Lebd(X) ≥
Lebd( ˜Tj) ≥km−α ,
Therefore, the number of (s0 + um)-connected components of ˜Gm with Lebesgue measure
greater than or equal to m−α is at most Lebd(X)mα.
To prove that the (s0 + um)-connected components of ˜Gm are mutually s0-separated,
let ˜T1 ̸= ˜T2 be two (s0 + um)-connected components of ˜Gm and ﬁx x1 ∈˜T1, x2 ∈˜T2. We
have ∥x1 −x2∥> s0 + um, otherwise ˜T1 = ˜T2. Thus d∞( ˜T1, ˜T2) ≥s0 + um > s0 + θum for
any um > 0, θ ∈(0, 1). Thus two (s0 + um)-connected components of ˜Gm are (s0 + θum)separated for any θ ∈(0, 1).
7.6 Proof of Proposition 4.3
Deﬁne m0 = e
3(r0∧s0) and denote D(J ) by D. Remark that
A1(j) ∪A2(j) ∪A3(j) ,
A1(j) = {card[κ(j)] = 0}
A2(j) = {card[κ(j)] ≥2}
{κ(j) ∩κ(j′) ̸= ∅} .
In words, A1(j) is the event on which Tj is estimated by none of the ( ˜Hk)k, A2(j) is the
event on which Tj is estimated by at least two diﬀerent elements of the collection ( ˜Hk)k
and A3(j) is the event on which Tj is estimated by an element of the collection ( ˜Hk)k that
also estimates another Tj′ from the collection (Tj)j.
For any j = 1, . . . , J, we have
A1(j) = {card[κ(j)] = 0} ⊂{Tj ⊂˜Gm △Γ} ⊂{B(x, r) ∩Tj ⊂˜Gm △Γ} ,
for any x ∈Tj and r > 0. Remark that from Proposition 4.1, the Tj are mutually s0separated so we have B(x, r) ∩Tj = B(x, r) ∩Γ for any r ≤s0. Thus, for any m ≥m0, it
holds (3 log m)−1 ≤s0 ∧r0 and
A1(j) ⊂{Lebd[B(x, (3 log m)−1) ∩Tj] ≤Lebd[ ˜Gm △Γ]} ⊂{Lebd[ ˜Gm △Γ] ≥c0(3 log m)−¯d} ,
where in the last inclusion we used the fact that Γ is r0-standard.
We now treat A2(j). Assume without loss of generality that {1, 2} ⊂κ(j). On A2(j),
there exist x1 ∈Tj ∩˜H1, xn ∈Tj ∩˜H2 and a sequence x2, . . . , xn−1 ∈Tj such that
∥xj −xj+1∥≤s0. Observe now that from Proposition 4.2, we have ∥x1 −xn∥> s0 ≥
(3 log m)−1 for m ≥m0. Therefore the integer
j : 2 ≤j ≤n, ∃z ∈˜H1 s.t. ∥xj −z∥> (3 log m)−1
is well deﬁned. Moreover, there exists z0 ∈˜H1 such that ∥xj⋆−1 −z0∥≤(3 log m)−1. Now,
if there exists z ∈˜Hk, for some k ∈{2, . . . , ˜Km}, such that ∥xj⋆−z∥≤(3 log m)−1, then
d∞( ˜H1, ˜Hk) ≤∥z0 −xj⋆−1∥+ ∥xj⋆−1 −xj⋆∥+ ∥xj⋆−z∥≤s0 + 2(3 log m)−1 .
This contradicts the conclusion of Proposition 4.2 which states that d∞( ˜H1, ˜Hk) > s0 +
θ(log m)−1 for any k = 2, . . . , ˜Km in particular when θ = 2/3. Therefore we obtain that on
A2(j) there exists xj⋆∈Tj such that
B(xj⋆, (3 log m)−1) ∩˜Gm = ∅.
B(xj⋆, (3 log m)−1) ∩Tj ⊂˜Gm △Γ
> c0(3 log m)−¯d
where in the second inclusion used the fact that B(xj⋆, r)∩Tj = B(xj⋆, r)∩Γ for any r ≤s0
and that Γ is r0-standard.
Generalization error bounds in semi-supervised classification
We now consider the event A3(j). Assume without loss of generality that j = 1 and let
k be such that k ∈κ(1)∩κ(j′) for some j′ ∈{2, . . . , J}. On A3(1), there exist y1 ∈T1 ∩˜Hk,
yn ∈Tj′ ∩˜Hk and a sequence y2, . . . , yn−1 ∈˜Hk such that ∥yj −yj+1∥≤sm.
Observe now that from Proposition 4.1, we have ∥y1 −yn∥> s0 ≥(3 log m)−1 for
m ≥m0. Therefore the integer
j : 2 ≤j ≤n, ∃z ∈T1 s.t. ∥yj −z∥> (3 log m)−1
is well deﬁned. Moreover, there exists z1 ∈T1 such that ∥yj♯−1 −z1∥≤(3 log m)−1. Now,
if there exists z ∈Tj′ for some j′ ∈{2, . . . , J} such that ∥yj♯−z∥≤(3 log m)−1, then
d∞(T1, Tj′) ≤∥yj♯−1 −z1∥+ ∥yj♯−1 −yj♯∥+ (3 log m)−1 ≤s0 + (log m)−1 < ¯s ,
for suﬃciently large m and where ¯s is deﬁned in (11). This contradicts the deﬁnition of
¯s which implies that d∞(T1, Tj′) ≥¯s for any j ∈{2, . . . , J}. Therefore we obtain that on
A3(1) there exists yj♯∈˜Hk such that B(yj♯, (3 log m)−1) ⊂Γc. It yields
Lebd( ˜Gm ∩Γc) ≥Lebd( ˜Gm ∩B(yj♯, (3 log m)−1))
Since yj♯∈˜Gm ⊂ˆGm, we have Lebd( ˆGm ∩B(yj♯, (3 log m)−1)) ≥m−α(3 log m)−d. On the
other hand, we have
Lebd( ˜Gm ∩B(yj♯, (3 log m)−1)) = Lebd( ˆGm ∩B(yj♯, (3 log m)−1))
−Lebd(Clip( ˆGm) ∩B(yj♯, (3 log m)−1))
≥m−α(3 log m)−d −Lebd( ˆGm ∩Γc)
≥m−α(3 log m)−d −c3m−1.1α
≥c4m−α(log m)−d ,
where we used the fact that ˆGm is consistent from inside at rate m−α . Hence,
{κ(j) ∩κ(j′) ̸= ∅} ⊂
Lebd( ˜Gm ∩Γc) ≥c5m−α(log m)−d
Combining the results for A1(j), A2(j) and A3(j), we have
IPm(Dc) ≤IPm
> c0(3 log m)−¯d
Lebd( ˜Gm ∩Γc) ≥c5m−α(log m)−d
Using the Markov inequality for both terms we obtain
> c0(log m)−¯d
Lebd( ˜Gm ∩Γc) ≥c5m−α(log m)−d
where we used the fact that ˜Gm is consistent from inside with rate m−α. It yields the
statement of the proposition.
7.7 Proof of Theorem 4.1
The cluster excess-risk EΓ(˜gn,m) can be decomposed w.r.t the event D and its complement.
EΓ(˜gn,m) ≤IEm
|2η(x) −1|1I{˜gn,m(x)̸=g⋆(x)}p(x)dx
+ IPm (Dc) .
We now treat the ﬁrst term of the RHS of the above inequality, i.e., on the event D. Fix
j ∈{1, . . . , J} and assume w.l.o.g. that η ≥1/2 on Tj. Simply write Zk for Zk
deﬁnition of D, there is a one-to-one correspondence between the collection {Tj}j and the
collection { ˜Hk}k. We denote by ˜Hj the unique element of { ˜Hk}k such that ˜Hj ∩Tj ̸= ∅.
On D, for any j = 1, . . . , J, we have,
|2η(x) −1|1I{˜gj
n,m(x)̸=g⋆(x)}p(x)dx
(2η −1)dPX + IEn
(2η −1)dPX
≤L(p)Lebd(Tj \ ˜Gm) + δjIPn
 Zj ≤0|Xu) .
On the event D, for any 0 < θ < 1, it holds
 Zj ≤0|Xu) = IPn
(2η −1)dPX −Zj ≥δj|Xu
(2η −1)dPX
Using Hoeﬀding’s inequality to control the ﬁrst term, we get
 Zj ≤0|Xu) ≤2e−n(θδj)2/2 + 1In
Taking expectations, and summing over j, the cluster excess-risk is upper bounded by
EΓ(˜gn,m) ≤2L(p)
Lebd(Γ △˜Gm)
δje−n(θδj)2/2 + IPm (Dc) ,
where we used the fact that on D,
From Proposition 4.3, we have IPm (Dc) = eO (m−α) and IEm
Lebd(Γ △˜Gm)
= eO (m−α)
and the theorem is proved.
Generalization error bounds in semi-supervised classification
7.8 Proof of Theorem 5.1
Recall that
We begin by the ﬁrst term. We have
x ∈X : ˆpm(x) ≥λ + ℓ, p(x) < λ
x ∈X : |ˆpm(x) −p(x)| ≥ℓ
The Fubini theorem yields
Lebd(˜Γℓ∩Γc)
≤Lebd(X) sup
IPm [|ˆpm(x) −p(x)| ≥ℓ] ≤c6e−c2maℓ2 ,
where the last inequality is obtained using (17) and c6 = c1Lebd(X) > 0. Taking ℓas in
(18) yields for m ≥exp(γa/c2),
Lebd(˜Γℓ∩Γc)
We now prove that IEm
Lebd(˜Γℓ∩Γc)
. Consider the following decomposition
where we drop the dependence in x for notational convenience,
ℓ∩Γ = B1 ∪B2,
ˆpm < λ + ℓ, p ≥λ + 2ℓ
|ˆpm −p| ≥ℓ
ˆpm < λ + ℓ, λ ≤p(x) < λ + 2ℓ
Using (17) and (18) in the same fashion as above we get IEm
term corresponding to B2 is controlled using the γ-exponent of density p at level λ. Indeed,
Lebd(B2) ≤c⋆ℓγ = c⋆(log m)γm−γa
The previous upper bounds for Lebd(B1) and Lebd(B2) together with (22) yield the consistency from inside.