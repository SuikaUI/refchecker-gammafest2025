Noname manuscript No.
(will be inserted by the editor)
Premise Selection for Mathematics by
Corpus Analysis and Kernel Methods
Jesse Alama∗· Tom Heskes∗∗· Daniel
K¨uhlwein∗∗· Evgeni Tsivtsivadze∗∗·
Josef Urban∗∗
the date of receipt and acceptance should be inserted later
Abstract Smart premise selection is essential when using automated reasoning as a tool for large-theory formal proof development. A good method for
premise selection in complex mathematical libraries is the application of machine learning to large corpora of proofs.
This work develops learning-based premise selection in two ways. First,
a newly available minimal dependency analysis of existing high-level formal
mathematical proofs is used to build a large knowledge base of proof dependencies, providing precise data for ATP-based re-veriﬁcation and for training premise selection algorithms. Second, a new machine learning algorithm
for premise selection based on kernel methods is proposed and implemented.
To evaluate the impact of both techniques, a benchmark consisting of 2078
large-theory mathematical problems is constructed, extending the older MPTP
Challenge benchmark. The combined eﬀect of the techniques results in a 50%
improvement on the benchmark over the Vampire/SInE state-of-the-art system for automated reasoning in large theories.
1 Introduction
In this paper we signiﬁcantly improve theorem proving in large formal mathematical libraries by using a two-phase approach combining precise proof analysis with machine learning of premise selection.
The ﬁrst phase makes the ﬁrst practical use of the newly available minimal dependency analysis of the proofs in the large Mizar Mathematical Li-
∗Center for Artiﬁcial Intelligence, New University of Lisbon. Funded by the FCT project
“Dialogical Foundations of Semantics” (DiFoS) in the ESF EuroCoRes programme Log-
ICCC . Research for this paper was partially done while a visiting fellow at the Isaac Newton Institute for the Mathematical Sciences in the programme
‘Semantics & Syntax’.
∗∗Intelligent Systems, Institute for Computing and Information Sciences, Radboud University Nijmegen. Funded by the NWO projects “Learning2Reason” and “MathWiki”.
 
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
brary (mml)1. This analysis allows us to construct precise problems for ATPbased re-veriﬁcation of the Mizar proofs. More importantly, the precise dependency data can be used as a large repository of previous problem-solving knowledge from which premise selection can be eﬃciently automatically learned by
machine learning algorithms.
In the second phase, a complementary improvement is achieved by using
a new kernel-based machine learning algorithm, which outperforms existing
methods for premise selection. This means that based on the large number
of previously solved mathematical problems, we can more accurately estimate
which premises will be useful for proving a new conjecture.
Such learned knowledge considerably helps automated proving of new formally expressed mathematical problems by recommending the most relevant
previous theorems and deﬁnitions from the very large existing libraries, and
thus shielding the existing ATP methods from considering thousands of irrelevant axioms. The better such symbiosis of formal mathematics and learningassisted automated reasoning gets, the better for both parties: improved automated reasoning increases the eﬃciency of formal mathematicians, and lowers
the cost of producing formal mathematics. This in turn leads to larger corpora
of previously solved nontrivial problems from which the learning-assisted ATP
can extract additional problem-solving knowledge covering larger and larger
parts of mathematics.
The rest of the paper is organized as follows. Section 2 describes recent
developments in large-theory automated reasoning and motivates our problem. Section 3 summarizes the recent implementation of precise dependency
analysis over the large mml, and its use for ATP-based cross-veriﬁcation and
training premise selection. Section 4 describes the general machine learning approach to premise selection and an eﬃcient kernel-based multi-output ranking
algorithm for premise selection. In Section 5 a new large-theory benchmark of
2078 related mml problems is deﬁned, extending the older and smaller MPTP
Challenge benchmark, and our techniques are evaluated on this benchmark in
Section 6. Section 7 concludes and discusses future work and extensions.
2 Automated Reasoning in Large Theories (ARLT)
In recent years, large formal libraries of re-usable knowledge expressed in rich
formalisms have been built with interactive proof assistants, such as Mizar ,
Isabelle , Coq , HOL (light) , and others. Formal approaches are also
being used increasingly in non-mathematical ﬁelds such as software and hardware veriﬁcation and common-sense reasoning about real-world knowledge.
Such trends lead to growth of formal knowledge bases in these ﬁelds.
One important development is that a number of these formal knowledge
bases and core logics have been translated to ﬁrst-order formats suitable for
ATPs , and ﬁrst-order ATP is today routinely used for proof assistance in systems like Isabelle , Mizar , and HOL . These
1 
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
ﬁrst-order translations give rise to large, semantically rich corpora that present
signiﬁcant new challenges for the ﬁeld of automated reasoning. The techniques
developed so far for ATP in large theories can be broadly divided into two categories:
1. Heuristic symbolic analysis of the formulas appearing in problems, and
2. Analysis of previous proofs.
In the ﬁrst category, the SInE preprocessor by K. Hoder has so far been
the most successful. SInE is particularly strong in domains with many hierarchical deﬁnitions such as those in common-sense ontologies. In the second category, machine learning of premise selection, as done e.g. by the MaLARea 
system, is an eﬀective method in hard mathematical domains, where the knowledge bases contain proportionally many more nontrivial lemmas and theorems
than simple deﬁnitions, and previous veriﬁed proofs can be used for learning
proof guidance.
Automated reasoning in large mathematical corpora is an interesting new
ﬁeld in several respects. Large theories permit data-driven approaches 
to constructing ATP algorithms; indeed, the sheer size of such libraries actually necessitates such methods. It turns out that purely deductive, bruteforce search methods can be improved signiﬁcantly by heuristic and inductive2 methods, thus allowing experimental research into combinations of
inductive and deductive methods. Large-theory benchmarks like the MPTP
Challenge3, and its extended version developed here in Section 5, can serve
for rigorous evaluation of such novel Artiﬁcial Intelligence (AI) methods over
thousands of real-world mathematical problems.4 Apart from the novel AI aspect, and the obvious proof assistance aspect, automated reasoning over large
formal mathematical corpora can also become a new tool in the established
ﬁeld of reverse mathematics . This line of research has been already started,
for example by Solovay’s analysis of the connection between Tarski’s axiom and the axiom of choice, and by Alama’s analysis of the Euler’s
polyhedron formula , both conducted over the mml.
3 Computing Minimal Dependencies in Mizar
In the world of automated theorem proving, proofs contain essentially all logical steps, even very small ones (such as the steps taken in a resolution proof).
In the world of interactive theorem proving, one of the goals is to allow the
users to express themselves with minimal verbosity. Towards that end, interactive theorem proving (ITP) systems often come with mechanisms for
suppressing some steps of an argument. By design, an ITP can suppress logical and mathematical steps that might be necessary for a complete analysis of
2 The word inductive denotes here inductive reasoning, as opposed to deductive reasoning.
3 
4 We do not evaluate on the CASC LTB datasets, because they are too small to allow
machine learning techniques. Our goal is to help mathematicians who work with and re-use
large amounts of previously established complex proofs and theorems.
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
what a particular proof depends upon. In this section we summarize a recently
developed solution to this problem for the mml. The basis of the solution is
refactoring of the articles of the mml into one-item micro-articles, and computing their minimal dependencies by a brute-force minimization algorithm. For
a more detailed discussion of Mizar, see ; for a more detailed discussion
of refactoring and minimization algorithms, see .
As an example of how inferences in ITP-assisted formal mathematical
proofs can be suppressed, consider a theorem of the form
∀x : τ[ψ (g(x))],
where “x : τ” means that the variable x has type τ, and g is a unary function
symbol that accepts arguments of type τ ′. Suppose further that, prior to the
assertion of this theorem, it is proved that τ is a subtype of τ ′. The wellformedness of the theorem depends on this subtyping relationship. Moreover,
the proof of the theorem may not mention this fact; the subtyping relationship
between τ and τ ′ may very well not be an outright theorem. In such a situation,
∀x(x : τ →x : τ ′)
is suppressed. We can see that by not requiring the author of a formal proof
to supply such subtyping relationships, we permit him to focus more on the
heart of the matter of his proof, rather than repeating the obvious. But if we
are interested in giving a complete answer to the question of what a formalized
proof depends upon, we must expose suppressed facts and inferences. Having
the complete answer is important for a number of applications, see for
examples. The particular importance for the work described here is that when
eﬃcient ﬁrst-order ATPs are used to assist high-level formal proof assistants
like Mizar, the diﬀerence between the implicitly used facts and the explicitly
used facts disappears. The ATPs need to explicitly know all the facts that are
necessary for ﬁnding the proofs. (If we were to omit the subtyping axiom, for
example, an ATP might ﬁnd that the problem is countersatisﬁable.)
The ﬁrst step in the computation of ﬁne-grained dependencies in Mizar is to
break up each article in the mml into a sequence of Mizar texts, each consisting
of a single top-level item (e.g., theorem, deﬁnition). Each of these texts can—
with suitable preprocessing—be regarded as a complete, valid Mizar article in
its own right. The decomposition of a whole article from the mml into such
smaller articles typically requires a number of nontrivial refactoring steps,
comparable, e.g., to automated splitting and re-factoring of large programs
written in programming languages with complicated syntactic mechanisms.
In Mizar, every article begins with a so-called environment specifying the
background knowledge (theorems, notations, etc.) that is used to verify the
article. The actual Mizar content that is imported, given an environment, is,
in general, a rather conservative overestimate of the items that the article
actually needs. That is why we apply a greedy minimization process to the
environment to compute a minimal set of items that are suﬃcient to verify
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
each “micro-article”. This produces a minimal set of dependencies5 for each
Mizar item, both syntactic (e.g., notational macros), and semantic (e.g., theorems, typings, etc.). The drawback of this minimization process is that the
greedy approach to minimization6 of certain kinds of dependencies can be time
consuming.7 The advantage is that (unlike in any other proof assistant) the
computed set of dependencies is truly minimal (with respect to the power of the
proof checker), and does not include redundant dependencies which are typically drawn in by overly powerful proof checking algorithms (like congruence
closure over sets of all available equalities, etc.) when the dependency tracking
is implemented internally inside a proof assistant. The dependency minimization is particularly important for the ATP and premise-selection applications
that are explained in this paper: a day more of routine computation of the
minimal dependencies is a very good time investment if it can provide better
guidance for the fast-growing search space explored by ATPs. Another advantage of this approach is that it also provides syntactic dependencies, which are
needed for real-world recompilation of the particular item as written in the
article. This functionality is important for fast ﬁne-grained recompilation in
formal wikis , however for semantic applications like ATP we are only considering the truly semantic dependencies, i.e., those dependencies that result
in a formula when translated by the MPTP system to ﬁrst-order logic.
Table 1 provides a summary of the ﬁne-grained dependency data for the
set of 33 Mizar articles coming from the MPTP2078 benchmark developed in
Section 5, and used for the experiments in Section 6. For each theorem in the
sequence of the 33 Mizar articles (ordered from ﬁrst to last by their order in
the mml) we show how many explicit dependencies are involved (on average)
in their proofs and how many implicit dependencies (on average) it contains.
The table also shows how much of an improvement the exact dependency
calculation is, compared to a simple safe ﬁxed-point MPTP construction of an
over-approximation of what is truly used in the mml proof.
4 Premise Selection in Large Theories by Machine Learning
When reasoning over a large theory (like the mml), thousands of premises are
available. In the presence of such large numbers of premises, the performance
of most ATP systems degrades considerably . Yet typically only a fraction
of the available premises are actually needed to construct a proof. Estimating
which premises are likely to be useful for constructing a proof is our research
5 Precisely, the minimality means that removing any dependence will cause the veriﬁcation
6 The basic greedy minimization proceeds by checking if an article still compiles after
removing increasingly larger parts of the environment.
7 This can be improved by heuristics for guessing the needed dependencies, analogous to
those used for ATP premise selection.
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
Table 1 Eﬀectiveness of ﬁne-grained dependencies on the 33 MPTP2078 articles ordered
from top to bottom by their order in the mml.
Expl. Refs.
Uniq. Expl. Refs.
Fine Deps.
MPTP Deps.
Article: Mizar Article relevant to the MPTP2078 benchmark.
Theorems: Total number of theorems in the article.
Expl. Refs.: Average number of (non-unique) explicit references (to theorems, deﬁnitional
theorems, and schemes) per theorem in the article.
Uniq. Expl. Refs.: Average number of unique explicit references per theorem.
Fine Deps.: Average number of all (both explicitly and implicitly) used items (explicitly
referred to theorems, together with implicitly used items) per theorem as computed by
dependency analysis.
MPTP Deps.: Average number of items per theorem as approximated by the MPTP ﬁxpoint
algorithm.
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
Deﬁnition 1 (Premise selection problem)
Given a large number of premises P and a new conjecture c, predict those
premises from P that are likely to be useful for automatically constructing a
proof of c.
Knowledge of previous proofs and problem-solving techniques is used by
mathematicians to guide their thinking about new problems. The detailed
mml proof analysis described above provides a large computer-understandable
corpus of dependencies of mathematical proofs. In this section we present
the machine learning setting and algorithms that are used to train premise
selection on such corpora. Our goal is to begin emulating the training of human
mathematicians.
When the translation from Mizar to ATP formats is applied, the Mizar
theorems and their proof dependencies (deﬁnitions, other theorems, etc.) translate to ﬁrst-order formulas, used in the corresponding ATP problems as conjectures and their premises (axioms). For further presentation here we identify
each mml formula with its ﬁrst-order translation.8 We will work in the following setting, which is tailored to the mml, but can easily be translated to other
large datasets. Let Γ be the set of ﬁrst order formulas that appear in the mml.
Deﬁnition 2 (Proof matrix) Using the ﬁne-grained Mizar proof analysis which says for each pair of formulas p, c ∈Γ whether p is used in the mml
proof of c - deﬁne the function µ : Γ × Γ →{0, 1} by
µ(c, p) :=
if p is used to prove c,
otherwise.
In other words, µ is the adjacency matrix of the graph of the direct mml
proof dependencies. This proof matrix, together with suitably chosen formula
features, will be used for training machine learning algorithms.
Note that in the mml, there is always exactly one (typically a textbook)
proof of a particular theorem c, and hence exactly one set of premises
usedPremises(c) := {p | µ(c, p) = 1} used in the proof of c. This corresponds
to the mathematical textbook practice where typically only one proof is given
for a particular theorem. It is however obvious that for example any expansion
of the proof dependencies can lead to an alternative proof.
In general, given a mathematical theory, there can be a variety of more or
less related alternative proofs of a particular theorem. This variety however
typically is not the explicit textbook data on which mathematicians study.
Such variety is only formed (in diﬀerent measure) in their minds, after studying
(training on) the textbook proofs, which typically are chosen for some nice
properties (simplicity, beauty, clarity, educational value, etc.). Hence it is also
plausible to use the set of mml proofs for training algorithms that attempt to
emulate human proof learning. Below we will often refer to the set of premises
8 Mizar is “nearly ﬁrst-order”, so the correspondence is “nearly” one-to-one, and in particular it is possible to construct the set of ATP premises from the exact Mizar dependencies
for each Mizar theorem.
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
used in the (unique) mml proof of a theorem c as the set of premises of c.
This concept is not to be read as the only set of premises of c, but rather as
the particular set of premises that is used in human training, and therefore is
also likely to be useful in training computers. It would not be diﬃcult to relax
this approach, if the corpora from which we learn contained a number of good
alternative proofs. This is however so far not the case with the current mml,
on which we conduct these experiments.
Also note that although our training set consists of formal proofs, these
proofs have been authored by humans, and not found fully automatically by
ATPs. But the evaluation conducted here (Section 6) is done by running ATPs
on the recommended premises. It could be the case (depending on the ATP
implementation) that a fully automatically found proof would provide a better
training example than the human proof from the mml. A major obstacle for
such training is however the relative weakness of existing ATPs in ﬁnding more
involved proofs of mml theorems (see ), and thus their failure to provide
the training examples for a large part of mml. Still, a comparison of the power
of training on mml and ATP proofs could be interesting future work.
Deﬁnition 3 (Feature matrix) We characterize mml formulas by the symbols and (sub)terms appearing in them. We use de Bruijn indices for variables,
and term equality is then just string equality. Let T := {t1, . . . , tm} be a ﬁxed
enumeration of the set of all symbols and (sub)terms that appear in all formulas from Γ. We deﬁne Φ : Γ × {1, . . . , m} →{0, 1} by
Φ(c, i) :=
if ti appears in c,
otherwise.
This matrix gives rise to the feature function ϕ : Γ →{0, 1}m which for c ∈Γ
is the vector ϕc with entries in {0, 1} satisfying
i = 1 ⇐⇒Φ(c, i) = 1.
The expressed features of a formula are denoted by the value of the function
e : Γ →P(T) that maps c to {ti | Φ(c, i) = 1}.
Note that our choice of feature characterization is quite arbitrary. We could
try to use only symbols, or only (sub)terms, or some totally diﬀerent features.
The better the features correspond to the concepts that are relevant when
choosing theorems for solving a particular problem, the more successful the
machine learning of premise selection can be. As with the case of using alternative proofs for training, we just note that ﬁnding suitable feature characterizations is a very interesting problem in this area, and that our current
choice seems to perform already quite reasonably in the experiments. For the
particular heuristic justiﬁcation of using formula (sub)terms, see .
The premise selection problem can be treated as a ranking problem, or
as a classiﬁcation problem. In the ranking approach, we for a given a conjecture c rank the available premises by their predicted usefulness for an automated proof of c, and use some number n of premises with the highest ranking (denoted here as advisedPremises(c, n)). In the classiﬁcation approach,
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
we are looking for each premise p ∈Γ for a real-valued classiﬁer function
Cp(·) : Γ →R which, given a conjecture c, estimates how useful p is for proving c. In standard classiﬁcation, a premise p would then be used if Cp(c) is
above certain threshold. A common approach to ranking is to use classiﬁcation,
and to combine the real-valued classiﬁers : the premises for a conjecture c
are ranked by the values of Cp(c), and we choose a certain number of the best
ones. This is the approach that we use in this paper.
Given a training corpus, machine learning algorithms can automatically
learn classiﬁer functions. The main diﬀerence between learning algorithms is
the function space in which they search for the classiﬁers and the measure they
use to evaluate how good a classiﬁer is. In our prior work on the applications of
machine learning techniques to the premise selection problem we used the
SNoW implementation of a multiclass naive Bayes learning method because
of its eﬃciency. In this work, we experiment with state-of-the-art kernel-based
learning methods for premise selection. We present both methods and show
the beneﬁts of using kernels.
4.1 A Naive Bayes Classiﬁer
Naive Bayes is a statistical learning method based on Bayes’s theorem about
conditional probabilities9 with a strong (read: naive) independence assumptions. In the naive Bayes setting, the value Cp(c) of the classiﬁer function of
a premise p at a conjecture c is the probability that µ(c, p) = 1 given the
expressed features e(c).
To understand the diﬀerence between the naive Bayes and the kernel-based
learning algorithm we need to take a closer look at the naive Bayes classiﬁer.
Let θ denote the statement that µ(c, p) = 1 and for each feature ti ∈T let
¯ti denote that Φ(c, i) = 1. Furthermore, let e(c) = {s1, . . . , sl} ⊆T be the
expressed features of c (with corresponding ¯s1, . . . , ¯sl). We have
9 In its simplest form, Bayes’s theorem asserts for a probability function P and random
variables X and Y that
P(X|Y ) = P(Y |X)P(X)
where P(X|Y ) is understood as the conditional probability of X given Y .
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
P(θ | ¯s1, . . . , ¯sl) ∝ln P(θ | ¯s1, . . . , ¯sl)
P(¬θ | ¯s1, . . . , ¯sl)
P(¯s1, . . . , ¯sl | θ)P(θ)
P(¯s1, . . . , ¯sl | ¬θ)P(¬θ)
= ln P(¯s1, . . . , ¯sl | θ)
P(¯s1, . . . , ¯sl | ¬θ) + ln P(θ)
P(¯si | θ)
P(¯si | ¬θ) + ln P(θ)
P(¬θ) by independence
 P(¯ti | θ)
P(¯ti | ¬θ)
= wT ϕc + ln P(θ)
 P(¯ti | θ)
P(¯ti) | ¬θ
Line (6) shows that the naive-Bayes classiﬁer is “essentially” (after the monotonic transformation) a linear function of the features of the conjecture. The
feature weights w are here computed using formula (7).
4.2 Kernel-based Learning
We saw that the naive Bayes algorithm gives rise to a linear classiﬁer. This
leads to several questions: ‘Are there better parameters?’ and ‘Can one get
better performance with non-linear functions?’. Kernel-based learning provides
a framework for investigating such questions. In this subsection we give a
simpliﬁed, brief description of kernel-based learning that is tailored to our
present problem; for further information, see .
4.2.1 Are there better parameters?
To answer this question we must ﬁrst deﬁne what ‘better’ means. Using the
number of problems solved as measure is not feasible because we cannot practically run an ATP for every possible parameter combination. Instead, we
measure how good a classiﬁer approximates our training data. We would like
to have that
∀x ∈Γ : Cp(x) = µ(x, p).
However, this will almost never be the case. To compare how well a classiﬁer
approximates the data, we use loss functions and the notion of expected loss
that they provide, which we now deﬁne.
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
Deﬁnition 4 (Loss function and Expected Loss) A loss function is any
function l : R × R →R+.
Given a loss function l we can then deﬁne the expected loss E(·) of a
classiﬁer Cp as
l(Cp(x), µ(x, p))
One might add additional properties such as l(x, x) = 0, but this is not necessary. Typical examples of a loss function l(x, y) are the square loss (y −x)2
or the 0-1 loss deﬁned by I(x = y).
We can compare two diﬀerent classiﬁers via their expected loss. If the
expected loss of classiﬁer Cp is less than the expected loss of a classiﬁer Cq
then Cp is the better classiﬁer. It should be noted that a lower expected loss
on a particular training set (like the mml proofs) need not necessarily lead
to more solved problems by an ATP. One could imagine that the training
set contains proofs that are very diﬀerent from the way a particular ATP
would proceed most easily. Also, what happens if the classiﬁer is not able to
predict all mml premises, but just a large part of them? These are questions
about alternative proofs, and about the robustness of the ATP and prediction
methods. An experimental answer is provided in Section 6.3.
4.2.2 Nonlinear Classiﬁers
It seems straightforward that more complex functions would lead to a lower
expected loss and are hence desirable. However, parameter optimization becomes tedious once we leave the linear case. Kernels provide a way to use the
machinery of linear optimization on non-linear functions.
Deﬁnition 5 (Kernel) A kernel is is a function k : Γ × Γ →R satisfying
k(x, y) = ⟨φ(x), φ(y)⟩
where φ : Γ →F is a mapping from Γ to an inner product space F with inner
product ⟨·, ·⟩. A kernel can be understood as a similarity measure between two
Example 1 A simple kernel for our setting is the linear kernel:
klin(x, y) := ⟨ϕx, ϕy⟩
with ⟨·, ·⟩being the normal dot product in Rm. Here, ϕf denotes the features
of a formula f (see deﬁnition 3), and the inner product space F is Rm. A
nontrivial example is the Gaussian kernel with parameter σ:
kgauss(x, y) := exp
−⟨ϕx, ϕx⟩−2⟨ϕx, ϕy⟩+ ⟨ϕy, ϕy⟩
We can now deﬁne our kernel function space in which we will search for
classiﬁcation functions.
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
Deﬁnition 6 (Kernel Function Space) Given a kernel k, we deﬁne
f ∈RΓ | f(x) =
αvk(x, v), αv ∈R, ∥f∥< ∞
as our kernel function space, where
αuαvk(u, v)
Essentially, every function in Fk compares the input x with formulas in Γ using
the kernel, and the weights α determine how important each comparison is10.
The kernel function space Fk naturally depends on the kernel k. It can
be shown that when we use klin, Fklin consists of linear functions of the mml
features T. In contrast, the Gaussian kernel kgauss gives rise to a very nonlinear
kernel function space.
4.2.3 Putting it all together
Having deﬁned loss functions, kernels and kernel function spaces we can now
deﬁne how kernel-based learning algorithms learn classiﬁer functions. Given a
kernel k and a loss function l, recall that we measure how good a classiﬁer Cp
is with the expected loss E(Cp). With all our deﬁnitions it seems reasonable
to deﬁne Cp as
Cp := arg min
However, this is not what a kernel based learning algorithm does. There are
two reasons for this. First, the minimum might not exist. Second, in particular when using complex kernel functions, such an approach might lead to
overﬁtting: Cp might perform very well on our training data, but bad on data
that was not seen before. To handle both problems, a regularization parameter λ > 0 is introduced to penalize complex functions (assuming that high
complexity implies a high norm). This regularization parameter allows us to
place a bound on possible solution which together with the fact that Fk is a
Hilbert space ensures the existence of Cp. Hence we deﬁne
Cp = arg min
E(f) + λ∥f∥2
Recall from the deﬁnition of Fk that Cp has the form
αvk(x, v),
with αv ∈R. Hence, for any ﬁxed λ, we only need to compute the weights
αv for all v ∈Γ in order to ﬁnd Cp. In section 4.3 we show how to solve this
optimization problem in our setting.
10 A more general approach to kernel spaces is available; see .
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
4.2.4 Naive Bayes vs Kernel-based Learning
Kernel-based methods typically outperform the naive Bayes algorithm. There
are several reasons for this. Firstly and most importantly, while naive Bayes is
essentially a linear classiﬁer, kernel based methods can learn non-linear dependencies when an appropriate non-linear (e.g. Gaussian) kernel function is used.
This advantage in expressiveness usually leads to signiﬁcantly better generalization11 performance of the algorithm given properly estimated hyperparameters (e.g., the kernel width for Gaussian functions). Secondly, kernel-based
methods are formulated within the regularization framework that provides
mechanism to control the errors on the training set and the complexity (”expressiveness”) of the prediction function. Such setting prevents overﬁtting of
the algorithm and leads to notably better results compared to unregularized
methods. Thirdly, some of the kernel-based methods (depending on the loss
function) can use very eﬃcient procedures for hyperparameter estimation (e.g.
fast leave-one-out cross-validation ) and therefore result in a close to optimal model for the classiﬁcation/regression task. For such reasons kernel-based
methods are among the most successful algorithms applied to various problems
from bioinformatics to information retrieval to computer vision . A general
advantage of naive Bayes over kernel-based algorithms is the computational
eﬃciency, particularly when taking into account the fact that computing the
kernel matrix is generally quadratic in the number of training data points.
However, recent advances in large scale learning have led to extensions of
various kernel-based methods such as SVMs, with sublinear complexity, provably fast convergence rate, and the generalization performance that cannot be
matched by most of the methods in the ﬁeld .
4.3 MOR Experimental Setup
For our experiments, we will now deﬁne a kernel-based multi-output ranking
(MOR) algorithm that is a relatively straightforward extension of our preference learning algorithm presented in . MOR is also based on the regularized
least-squares algorithm presented in .
Let Γ = {x1, . . . , xn}. Then formula (10) becomes
αik(x, xi)
Using this and the square-loss l(x, y) = (x −y)2 function, solving equation (9)
is equivalent to ﬁnding weights αi that minimize
αjk(xi, xj) −µ(xi, p)
αiαjk(xi, xj)
11 Generalization is the ability of a machine learning algorithm to perform accurately on
new, unseen examples after training on a ﬁnite data set.
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
Recall that Cp is the classiﬁer for a single premise. Since we eventually
want to rank all premises, we need to train a classiﬁer for each premise. So
we need to ﬁnd weights αi,p for each premise p. This does seem to complicate
the problem quite a bit. However, we can use the fact that for each premise
p, Cp depends on the values of k(xi, xj), where 1 ≤i, j ≤n, to speed up the
computation. Instead of learning the classiﬁers Cp for each premise separately,
we learn all the weights αp,i simultaneously.
To do this, we ﬁrst need some deﬁnitions. Let
A = (αi,p)i,p
(1 ≤i ≤n, p ∈Γ).
A is the matrix where each column contains the parameters of one premise
classiﬁer. Deﬁne the kernel matrix K and the label matrix Y as
K := (k(xi, xj))i,j
(1 ≤i, j ≤n)
Y := (µ(xi, p))i,p
(1 ≤i ≤n, p ∈Γ).
We can now rewrite (11) in matrix notation to state the problem for all
 (Y −KA)T(Y −KA) + λATKA
where tr(A) denotes the trace of the matrix A. Taking the derivative with
respect to A leads to:
 (Y −KA)T(Y −KA) + λATKA
−2K(Y −KA) + 2λKA
−2KY + (2KK + 2λK)A
To ﬁnd the minimum, we set the derivative to zero and solve with respect to
A. This leads to:
A = (KK + λK)−1KY
= (K + λI)−1Y
In the experiments, we use the Gaussian kernel kgauss we deﬁned in Example 1. Ergo, if we ﬁx the regularization parameter λ and the kernel parameter
σ we can ﬁnd the optimal weights through simple matrix computations. Thus,
to fully determine the classiﬁers, it remains to ﬁnd good values for the parameters λ and σ. This is done, as is common with such parameter optimization
for kernel methods, by simple (logarithmically scaled) grid search and crossvalidation on the training data using a 70/30 split.
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
5 Data: The MPTP2078 Benchmark
The eﬀects of using the minimized dependency data (both for direct re-proving
and for training premise selection), and the eﬀect of using our kernel-based
MOR algorithm are evaluated on a newly created large-theory benchmark12
of 2078 related mml problems, which extends the older and smaller MPTP
Challenge benchmark.
The original MPTP Challenge benchmark was created in 2006, with the
purpose of supporting the development of ARLT (automated reasoning for
large theories) techniques. It contains 252 related problems, leading to the
Mizar proof of one implication of the Bolzano-Weierstrass theorem. The challenge has two divisions: chainy (harder) and bushy (easier). The motivation
behind them is given below when we describe their analogs in the MPTP2078
benchmark.
Both the ARLT techniques and the computing power (particularly multicore technology) have developed since 2006. Appropriately, we deﬁne a larger
benchmark with a larger numbers of problems and premises, and making use
of the more precise dependency knowledge. The larger number of problems
together with their dependencies more faithfully mirror the setting that mathematicians are facing: typically, they know a number of related theorems and
their proofs when solving a new problem.
The new MPTP2078 benchmark is created as follows: The 33 Mizar articles
from which problems were previously selected for constructing the MPTP
Challenge are used. We however use a new version of Mizar and mml allowing
the precise dependency analysis, and use all problems from these articles. This
yields 2078 problems. As with the MPTP Challenge benchmark, we create two
groups (divisions) of problems.
Chainy: Versions of the 2078 problems containing all previous mml contents as
premises. This means that the conjecture is attacked with “all existing
knowledge”, without any premise selection. This is a common use case for
proving new conjectures fully automatically, see also Section 6.2. In the
MPTP Challenge, the name chainy has been introduced for this division,
because the problems and dependencies are ordered into a chronological
chain, emulating the growth of the library.
Bushy: Versions of the 2078 problems with premises pruned using the new ﬁnegrained dependency information. This use-case has been introduced in
proof assistants by Harrison’s MESON TACTIC , which takes an explicit
list of premises from the large library selected by a knowledgeable user,
and attempts to prove the conjecture just from these premises. We are
interested in how powerful ATPs can get on mml with such precise advice.
To evaluate the beneﬁt of having precise minimal dependencies, we additionally also produce in this work versions of the 2078 problems with premises
pruned by the old heuristic dependency-pruning method used for constructing re-proving problems by the MPTP system. The MPTP heuristic proceeds
12 
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
by taking all explicit premises contained in the original human-written Mizar
proof. To get all the premises used by Mizar implicitly, the heuristic watches
the problem’s set of symbols, and adds the implicitly used formulas (typically typing formulas about the problem’s symbols) in a ﬁxpoint manner. The
heuristic attempts hard to guarantee completeness, however, minimality is not
achievable with such simple approach.
All three datasets contain the same conjectures. They only diﬀer in the
number of redundant axioms. Note that the problems in the second and third
dataset are considerably smaller than the unpruned problems. The average
number of premises is 1976.5 for the unpruned (chainy) problems, 74 for the
heuristically-pruned problems (bushy-old) and 31.5 for the problems pruned
using ﬁne-grained dependencies (bushy). Table 2 summarizes the datasets.
Table 2 Average Number of Premises in the three Datasets
Premises used
Avg. number of premises
All previous
Heuristic dependencies
Minimized dependencies
6 Experiments and Results
We use Vampire 0.6 as the ATP system for all experiments conducted here.
Adding other ATP systems is useful (see, e.g., for recent evaluation), and
there are metasystems like MaLARea which attempt to exploit the joint power
of diﬀerent systems in an organized way. However, the focus of this work is on
premise selection, which has been shown to have similar eﬀect across the main
state-of-the-art ATP systems. Another reason for using the recent Vampire is
that in , Vampire with the SInE preprocessor was suﬃciently tested and
tuned on the mml data, providing a good baseline for comparing learningbased premise-selection methods with robust state-of-the-art methods that can
run on any isolated large problem without any learning. All measurements are
done on an Intel Xeon E5520 2.27GHz server with 8GB RAM and 8MB CPU
cache. Each problem is always assigned one CPU.
In Section 6.1 we evaluate the ATP performance when ﬁne-grained dependencies (bushy problems) are used by comparing it to the ATP performance on
the old MPTP heuristic pruning (bushy-old problems), and to the ATP performance on the large (chainy) versions of the MPTP2078 problems. These results
show that there is a lot to gain by constructing good algorithms for premise
selection. In Section 6.2 SNoW’s naive Bayes and the MOR machine learning algorithms are incrementally trained on the ﬁne-grained mml dependency data,
and their precision in predicting the mml premises on new problems are compared. This standard machine-learning comparison is then in Section 6.3 com-
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
pleted by running Vampire on the premises predicted by the MOR and SNoW
algorithms. This provides information about the overall theorem-proving performance of the whole dependency-minimization/learning/ATP stack. This
performance is compared to the performance of Vampire/SInE.
6.1 Using the Fine-Grained Dependency Analysis for Re-proving
The ﬁrst experiment evaluates the eﬀect of ﬁne-grained dependencies on reproving Mizar theorems automatically. The results of Vampire/SInE run with
10s time limit13 on the datasets deﬁned above are shown in Table 3.
Table 3 Performance of Vampire (10s time limit) on 2078 MPTP2078 benchmark with
diﬀerent axiom pruning.
Solved problems
Solved as percentage
Chainy (Vampire -d1)
Vampire (run in the unmodiﬁed automated CASC mode) solves 548 of
the unpruned problems. If we use the -d1 parameter14, Vampire solves 556
problems. Things change a lot with external premise pruning. Vampire solves
1023 of the 2078 problems when the old MPTP heuristic pruning (bushy-old)
is applied. Using the pruning based on the new ﬁne-grained analysis Vampire
solves 1105 problems, which is an 8% improvement over the heuristic pruning
in the number of problems solved. Since the heuristic pruning becomes more
and more inaccurate as the mml grows (the ratio of MPTP Deps. to Fine Deps.
in Table 1 has a growing trend from top to bottom), we can conjecture that this
improvement will be even more signiﬁcant when considering the whole mml.
Also note that these numbers point to the signiﬁcant improvement potential
that can be gained by good premise selection: the performance on the pruned
dataset is doubled in comparison to the unpruned dataset. Again, this ratio
grows as mml grows, and the number of premises approaches 100.000.15
13 There are several reasons why we use low time limits. First, Vampire performs reasonably
with them in . Second, low time limits are useful when conducting large-scale experiments
and combining diﬀerent strategies. Third, in typical ITP proof-advice scenarios , the
preferable query response time is in (tens of) seconds. Fourth, 10 seconds in 2011 is much
more than it was ﬁfteen years ago, when the CASC competition started.
14 The -d parameter limits the depth of recursion for the SInE algorithm. In running
Vampire with the -d1 pruning parameter resulted in signiﬁcant performance improvement
on large Mizar problems.
15 In the evaluation done in on the whole mml with Vampire/SInE, this ratio is 39%
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
6.2 Combining Fine-Grained Dependencies with Learning
For the next experiment, we emulate the growth of the library , by considering all previous theorems and deﬁnitions when a
new conjecture is attempted. This is a natural “ATP advice over the whole
library” scenario, in which the ATP problems however become very large,
containing thousands of the previously proved formulas. Premise selection can
therefore help signiﬁcantly.
We use the ﬁne-grained mml dependencies extracted from previous proofs16
to train the premise-selection algorithms, use their advice on the new problems, and compare the recall (and also the ATP performance in the next
subsection). For each problem, the learning algorithms are allowed to learn on
the dependencies of all previous problems, which corresponds to the situation
in general mathematics when mathematicians not only know many previous
theorems, but also re-use previous problem solving knowledge. This approach
requires us to do 2078 training steps as the problems and their proofs are
added to the library and the dataset grows. We compare the MOR algorithm
with SNoW’s naive Bayes.
Figure 1 shows the average recall of SNoW and MOR on this dataset.
The rankings obtained from the algorithms are compared with the actual
premises used in the mml proof, by computing the size (ratio) of the overlap
for the increasing top segments of the ranked predicted premises (the size of
the segment is the x axis in Figure 1). Formally, the recall recall(c, n) for one
conjecture c when n premises are advised is deﬁned as:
recall(c, n) = |usedPremises(c) ∩advisedPremises(c, n)|
|usedPremises(c)|
It can be seen that the MOR algorithm performs considerably better than
SNoW. E.g., on average 88% of the used premises are within the 50 highest MOR-ranked premises, whereas when we consider the SNoW ranking only
around 80% of the used premises are with the 50 highest ranked premises.
Note that this kind of comparison is the standard endpoint in machine
learning applications like keyword-based document retrieval, consumer choice
prediction, etc. However, in a semantic domain like ours, we can go further,
and see how this improved prediction performance helps the theorem proving
process. This is also interesting to see, because having for example only 90%
coverage of the original mml premises could be insuﬃcient for constructing an
ATP proof, unless the ATP can invent alternative (sub-)proofs.17 This ﬁnal
evaluation is done in the next section.
16 We do not evaluate the performance of learning on the approximate bushy-old dependencies here and in the next subsection. Table 1 and Section 6.1 already suﬃciently show
that these data are less precise than the ﬁne-grained mml dependencies.
17 See for an initial exploration of the phenomenon of alternative ATP proofs for mml
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
Fig. 1 Comparison of the SNoW and MOR average recall of premises used in the Mizar
proofs. The x-axis shows the number of premises asked from SNoW and MOR, and the y-axis
shows their relative overlap with the premises used in the original Mizar proof.
6.3 Combining It All: ATP Supported by Learning from Fine Dependencies
In the last experiment, we ﬁnally chain the whole ITP/Learning/ATP stack
together, and evaluate how the inﬂuence of the improved premise selection
reﬂects on performance of automated theorem proving on new large-theory
conjectures. Both the naive Bayes (SNoW) and the new MOR learning algorithms are evaluated.
Figure 2 shows the numbers of problems solved by Vampire using diﬀerent
numbers of the top premises predicted by SNoW and MOR, and a 5 seconds
time limit. The maximum number of problems solved with MOR is 729 with the
top 60 advised premises. SNoW’s maximum is 652 with the top 70 premises.
The corresponding numbers for a 10 second time limit are 795 solved problems
for MOR-60, and 722 for SNoW-70. Table 4 compares these data with the
overall performance of Vampire with a 10 second time limit run on problems
with pruning done by SInE. The SNoW-60 resp. MOR-70 runs give a 32% resp.
45% improvement over the 548 problems Vampire solves in auto-mode, and a
30% resp. 43% improvement over the 556 problems solved by Vampire using
the -d1 option.
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
Fig. 2 Comparison of the number of solved problems by SNoW and MOR. The x-axis shows
the number of premises given to the Vampire, and the y-axis shows the number of problems
solved within 5 seconds. The number of problems solved by Vampire/SInE in 10 seconds is
given as a baseline.
Table 4 Comparison of Vampire (10s time limit) performance on MPTP2078 with diﬀerent
premise selections.
Solved Problems
Gain over Vampire
Vampire/SInE
Vampire/SInE (-d1)
Table 5 additionally compares the performance of Vampire/SInE with the
performance of SNoW and MOR when computed for each of them as a union
of the two 5s runs with the largest joint coverage. Those are obtained by using
the top 40 advised premises and the top 180 advised premises for SNoW, and
the top 40 and top 100 advised premises for MOR. These SNoW resp. MOR
combined runs give a 44% resp. 50% improvement over the 548 problems
Vampire solves in auto-mode, and a 42% resp. 48% improvement over the 556
problems solved by Vampire using the -d1 option.
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
Note that Vampire/SInE does strategy scheduling internally, and with different SInE parameters. Thus combining two diﬀerent premise selection strategies by us is perfectly comparable to the way Vampire’s automated mode is
constructed and used. Also note that combining the two diﬀerent ways in
which unadvised Vampire/SInE was run is not productive: the union of both
unadvised runs is just 559 problems, which is only 3 more solved problems
(generally in 20s) than with running Vampire/SInE with -d1 for ten seconds.
Table 5 10s performance of the two strategies with the largest joint coverage for SNoW
Solved Problems
Gain over Vampire
Vampire/SInE
Vampire/SInE (-d1)
SNoW-40/180
MOR-40/100
Finally, Figure 3 and Figure 4 compare the cumulative and average performance of the algorithms (combined with ATPs) at diﬀerent points of the
MPTP2078 benchmark, using the chronological ordering of the MPTP2078
problems. The average available number of premises for the theorems ordered
chronologically grows linearly (the earlier theorems and deﬁnitions become eligible premises for the later ones), making the later problems harder on average.
Figure 3 shows the performance computed on initial segments of problems using step value of 50. The last value corresponds to the performance
of the algorithms on the whole MPTP2078 (0.38 for MOR-60), while for example the value for 1000 (0.60 for MOR-60) shows the performance of the
algorithms on the ﬁrst 1000 MPTP2078 problems. Figure 4 compares the average performance of the algorithms when the problems are divided into four
successive segments of equal size. Note that even with the precise use of the
mml premises the problems do not have uniform diﬃculty across the benchmark, and on average, even the bushy versions of the later problems get harder.
To visualize this, we also add the values for Vampire-bushy to the comparison.
Except from small deviations, the ratio of solved problems decreases for all
the algorithms. Vampire/MOR-60 is able to keep up with Vampire-bushy in the
range of the initial 800 problems, and after that the human selection increasingly outperforms all the algorithms. Making this gap as small as possible is
an obvious challenge on the path to strong automated reasoning in general
mathematics.
7 Conclusion and Future Work
The performance of automated theorem proving over real-world mathematics
has been signiﬁcantly improved by using detailed minimized formally-assisted
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
"Vampire-bushy"
Fig. 3 Performance of the algorithms on initial segments of MPTP2078.
Fig. 4 Average performance of the algorithms on four successive equally sized segments of
Premise Selection for Mathematics by Corpus Analysis and Kernel Methods
analysis of a large corpus of theorems and proofs, and by using improved prediction algorithms. In particular, it was demonstrated that premise selection
based on learning from exact previous proof dependencies improves the ATP
performance in large mathematical theories by about 44% when using oﬀ-theshelf learning methods like naive Bayes in comparison with state-of-the-art
general premise-selection heuristics like SInE. It was shown that this can be
further improved to about 50% when employing state-of-the-art kernel-based
learning methods.
Automated reasoning in large mathematical libraries is becoming a complex AI ﬁeld, allowing interplay of very diﬀerent AI techniques. Manual tuning
of strategies and heuristics does not scale to large complicated domains, and
data-driven approaches are becoming very useful in handling such domains.
At the same time, existing strong learning methods are typically developed on
imprecise domains, where feedback loops between prediction and automated
veriﬁed conﬁrmation as done for example in MaLARea are not possible. The
stronger such AI systems become, the closer we get to formally assisted mathematics, both in its “forward” and “reverse” form. And this is obviously another
positive feedback loop that we explore here: the larger the body of formally
expressed and veriﬁed ideas, the smarter the AI systems that learn from them.
The work started here can be improved in many possible ways. While we
have achieved 50% ATP improvement on large problems by better premise selection resulting in 824 problems proved within 10 seconds, we know (from 6.1)
that with a better premise selection it is possible to prove at least 1105 problems. Thus, there is still a great opportunity for improved premise selection
algorithms. Our dependency analysis can be ﬁner and faster, and combined
with ATP and machine learning systems, can be the basis for a research tool
for experimental formal (reverse) mathematics. An interesting AI problem that
is becoming more and more relevant as the ATP methods for mathematics are
getting stronger, is translation of the (typically resolution-based) ATP proofs
into human-understandable formats used by mathematicians. We believe that machine learning from large human-proof corpora like mml is likely
to be useful for this task, in a similar way to how it is useful for ﬁnding relevant
The MOR algorithm has a number of parameterizations that we have ﬁxed
for the experiments done here. Further experiments with diﬀerent loss functions could yield better results. One of the most interesting parameterizations
is the right choice of features for the formal mathematical domain. So far, we
have been using only the symbols and terms occurring in formulas as their feature characterizations, but other features are possible, and very likely used by
mathematicians. In particular, for ad hoc problem collections like the TPTP library, where symbols are used inconsistently across diﬀerent problems, formula
features that abstract from particular symbols will likely be needed. Also, the
output of the learning algorithms does not have to be limited to the ranking of
premises. In general, all kinds of relevant problem-solving parameterizations
can be learned, and an attractive candidate for such treatment is the large set
of ATP strategies and options parameterizing the proof search. With such ex-
Alama, Heskes, K¨uhlwein, Tsivtsivadze, and Urban
periments, a large number of alternative ATP proofs are likely to be obtained,
and an interesting task is to productively learn from such a combination of
alternative (both human and machine) proofs. Premise selection is only one
instance of the ubiquitous proof guidance problem, and recent prototypes like
the MaLeCoP system indicate that guidance obtained by machine learning
can considerably help also inside automated theorem provers.
Finally, we hope that this work and the performance numbers obtained
will provide a valuable feedback to the CADE competition organizers: Previous proofs and theory developments in general are an important part of
real-world mathematics and theorem proving. At present, the LTB division of
CASC does not recognize proofs in the way that we are recognizing them here.
Organizing large-theory competitions that separate theorems from their proofs
is like organizing web search competitions that separate web pages from their
link structure . We believe that re-introducing a large-theory competition
that does provide both a large number of theorems and a large number of
proofs will cover this important research direction, and most of all, properly
evaluate techniques that signiﬁcantly improve the ATP end-user experience.
8 Acknowledgment
We would like to thank the anonymous JAR referees for a number of questions,
comments, and insights that helped to signiﬁcantly improve the ﬁnal version
of this paper.