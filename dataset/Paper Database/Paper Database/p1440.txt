Pedestrian Detection via Classification
on Riemannian Manifolds
Oncel Tuzel, Student Member, IEEE, Fatih Porikli, Senior Member, IEEE, and
Peter Meer, Senior Member, IEEE
Abstract—We present a new algorithm to detect pedestrians in still images utilizing covariance matrices as object descriptors. Since
the descriptors do not form a vector space, well-known machine learning techniques are not well suited to learn the classifiers. The
space of d-dimensional nonsingular covariance matrices can be represented as a connected Riemannian manifold. The main
contribution of the paper is a novel approach for classifying points lying on a connected Riemannian manifold using the geometry of the
space. The algorithm is tested on the INRIA and DaimlerChrysler pedestrian data sets where superior detection rates are observed
over the previous approaches.
Index Terms—Pedestrian detection, classification, Riemannian manifolds, symmetric positive definite matrices, boosting, object
descriptors.
INTRODUCTION
ETECTING different categories of objects in image and
video content is one of the fundamental tasks in
computer vision research. The success of many applications
such as visual surveillance, image retrieval, robotics,
autonomous vehicles, and smart cameras are conditioned
on the accuracy of the detection process.
Two main processing steps can be distinguished in a
typical object detection algorithm. The first task is feature
extraction, in which the most informative object descriptors
regarding the detection process are obtained from the visual
content. The second task is detection, in which the obtained
object descriptors are utilized in a classification framework
to detect the objects of interest.
The feature extraction methods can be further categorized into two groups based on the representation. The first
group of methods is the sparse representations, where a set of
representative local regions is obtained as the result of an
interest point detection algorithm. Reliable interest points
should encapsulate valuable information about the local
image content and remain stable under changes, such as in
viewpoint and/or illumination. There exists an extensive
literature on interest point detectors, and , , ,
 , and are only a few of the most commonly used
methods that satisfy consistency over a large range of
operating conditions.
Earlier approaches for part descriptors utilized intensitybased features. However, histogram-based representations
of image gradients and edges in spatial context, such as
scale-invariant feature transform (SIFT) descriptors or
shape contexts , were shown to yield more robust and
distinctive descriptors. Several object detection algorithms
were proposed by assembling the detected parts according
to spatial relationships in probabilistic frameworks ,
 , by discriminative approaches , , or via matching
shapes , .
The second group of feature extraction methods is the
dense representations, where object descriptors are obtained
inside a detection window. The entire image is scanned
densely (possibly each pixel), and a learned classifier of the
object model is evaluated. Earlier studies utilized image
intensities such as intensity templates , or principal
component analysis (PCA) coefficients , to represent the object model. More recently, Haar-wavelet-based
descriptors, which are a set of basis functions encoding
intensity differences between two regions, became increasingly popular due to efficient computation and superiority
to encode visual patterns. In , an overcomplete dictionary of basis functions were computed from overlapping
regions utilizing horizontal, vertical, and diagonal features
inside the detection window. Instead of sampling among an
overcomplete dictionary of features, in , a small set of
important features were selected via a greedy selection
method using AdaBoost.
Most of the leading approaches in object detection are
discriminative methods such as neural networks (NNs) ,
support vector machines (SVMs) , and boosting .
These methods became increasingly popular since they can
cope with high-dimensional state spaces and/or are able to
select relevant descriptors among a large set. In and
 , NNs and, in , SVMs were utilized as a single strong
classifier for detection of various categories of objects.
The NNs and SVMs were also utilized for intermediate
representations , for final object detectors. In ,
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
OCTOBER 2008
. O. Tuzel is with Rutgers University, Department of Computer Science,
110 Frelinghuysen Road, Piscataway, NJ 08854.
E-mail: .
. F. Porikli is with Mitsubishi Electric Research Laboratories, 201 Broadway,
Cambridge, MA 02139. E-mail: .
. P. Meer is with Rutgers University, Department of Electrical and
Computer Engineering, 94 Brett Road, Piscataway, NJ 08854.
E-mail: .
Manuscript received 12 Oct. 2007; revised 12 Mar. 2008; accepted 13 Mar.
2008; published online 20 Mar. 2008.
Recommended for acceptance by S. Baker, J. Matas, and R. Zabih.
For information on obtaining reprints of this article, please send e-mail to:
 , and reference IEEECS Log Number TPAMISI-2007-
Digital Object Identifier no. 10.1109/TPAMI.2008.75.
0162-8828/08/$25.00  2008 IEEE
Published by the IEEE Computer Society
multiple weak classifiers trained using AdaBoost were
combined to form a rejection cascade such that if any
classifier rejects a hypothesis, then it is considered a
negative example. The approach provides an efficient
algorithm due to sparse feature selection; besides, only a
few classifiers are evaluated at most of the regions due to
the cascade structure. Variants of the algorithm were also
proposed to share features among different object categories, thereby providing a more efficient solution for
multiple-class object detection . The other approaches
include the probabilistic methods , , where the
conditional densities of object and nonobject classes are
modeled explicitly.
Pedestrian detection in still images is considered among
the hardest examples of object detection problems. The
articulated structure and variable appearance of the human
body, combined with illumination and pose variations,
contribute to the complexity of the problem.
Sparse representations for pedestrian detection include
models for detecting body parts , or common
shapes , where these local features were assembled
according to geometric constraints to form the final
pedestrian model. In , parts were represented by cooccurrences of local orientation features, and separate
detectors were trained for each part using AdaBoost. Target
location was determined by maximizing the joint likelihood
of part occurrences combined according to the geometric
relations. A pedestrian detection system for crowded scenes
was described in . The approach combined local
appearance features and their geometric relations with
global cues by top-down segmentation based on per-pixel
likelihoods. Other approaches include using silhouette
information either in matching or in a classification
framework .
Dense representations for pedestrian detection include
 , where a cost function is defined based on the part
likelihoods and their joint configuration. The minimizer of
the function with respect to all the possible part locations
in the image plane is efficiently found using dynamic
programming. In , a polynomial SVM was learned using
Haar wavelets as pedestrian descriptors. Later, the work
was extended to multiple classifiers trained to detect
human parts, and the responses inside the detection
window were combined to give the final decision .
Similar to still images, in , a real-time moving pedestrian
detection algorithm was described also using Haar wavelet
descriptors but extracted from space-time differences in
video. Using AdaBoost, the most discriminative features
were selected, and multiple classifiers were combined to
form a rejection cascade. In , an excellent pedestrian
detector was described by training an SVM classifier using a
densely sampled histogram of oriented gradients (HOG,
similar to SIFT descriptors) inside the detection window.
The performance of the proposed descriptors was shown on
the INRIA human data set. In a similar approach , nearreal-time detection performances were achieved by training
a cascade model using HOG features. Recently, in , a
two-stage AdaBoost classifier was learned using shapelet
features, and superior performances were reported on the
INRIA human data set over all the existing methods. The
initial stage learns a set of classifiers that are a combination
of oriented-gradient responses, whereas the second stage
combines the classifier responses to form the final detector.
We refer to for a detailed survey on object and
pedestrian detection methods.
In this paper, we present a dense model where
covariance features are utilized as pedestrian descriptors
inside a detection window. Covariance features were first
introduced in for matching and texture classification
problems and were later extended to tracking . A region
was represented by the covariance matrix of image features
such as spatial location, intensity, higher order derivatives,
etc. Similarly, we represent a pedestrian with several
covariance descriptors of overlapping regions, where the
best descriptors are determined with a greedy feature
selection algorithm combined with boosting.
It is not trivial to build a classifier where the domain is
the space of covariance matrices. Covariance matrices do
not form a vector space; therefore, it is not adequate to use
classical machine learning techniques to learn the classifiers. The space of nonsingular covariance matrices (symmetric positive definite matrices) can be formulated as a
connected Riemannian manifold. The main contribution of
this paper is a novel approach for classifying points lying on
a Riemannian manifold by incorporating the a priori
knowledge of the geometry of the space. Although there
have been previous approaches for clustering data points
lying on differentiable manifolds , , , to our
knowledge, this paper is one of the first studies aiming at
the classification problem.
The paper is organized as follows: In Section 2, we
describe the covariance descriptors. In Section 3, we present
an introduction to Riemannian geometry, focusing on the
space of symmetric positive definite matrices. In Sections 4
and 5, we describe our algorithm for classification on
Riemannian manifolds and its application to pedestrian
detection. The experiments are presented in Section 6.
COVARIANCE DESCRIPTORS
Let I be a one-dimensional intensity or three-dimensional
color image and F be the W  H  d dimensional feature
image extracted from I,
Fðx; yÞ ¼ ðI; x; yÞ;
where the function  can be any mapping such as intensity,
color, gradients, filter responses, etc. For a given rectangular
region R  F, let fzigi¼1::S be the d-dimensional feature
points inside R. The region R is represented with the
d  d covariance matrix of the feature points
ðzi  Þðzi  ÞT;
where  is the mean of the points. In Fig. 1, we delineate the
construction of covariance descriptors.
The diagonal entries of the covariance matrix represent
the variance of each feature, and the nondiagonal entries are
their respective correlations. There are several advantages
of using covariance matrices as region descriptors. The
representation proposes a natural way of fusing multiple
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
OCTOBER 2008
features that might be correlated. A single covariance
matrix extracted from a region is usually enough to match
the region in different views and poses. The noisecorrupting individual samples are largely filtered out with
the average filter during covariance computation. The
descriptors are low-dimensional, and due to symmetry,
CR has only ðd2 þ dÞ=2 different values. Given a region R,
its covariance CR does not have any information regarding
the ordering and the number of points. This implies a
certain scale and rotation invariance over the regions in
different images. Nevertheless, if information regarding the
orientation of the points is represented, such as the gradient
with respect to x and y, the covariance descriptor is no
longer rotationally invariant. The same argument is also
correct for illumination.
Integral Images for Fast Covariance
Computation
Integral images are intermediate image representations used
for the fast calculation of region sums , . Each pixel
of the integral image is the sum of all the pixels inside the
rectangle bounded by the upper left corner of the image and
the pixel of interest. For an intensity image I, its integral
image is defined as
Integral Image ðx0; y0Þ ¼
Using this representation, any rectangular region sum can
be computed in constant time. In , the integral images
were extended to higher dimensions for the fast calculation of region histograms. Here, we follow a similar idea
for the fast calculation of region covariances.
We can write the ði; jÞth element of the covariance matrix
defined in (2) as
CRði; jÞ ¼
zkðiÞ  ðiÞ
Þ zkðjÞ  ðjÞ
Expanding the mean and rearranging the terms, we can
CRði; jÞ ¼
zkðiÞzkðjÞ  1
To find the covariance in a given rectangular region R, we
have to compute the sum of each feature dimension,
zðiÞi¼1...d, as well as the sum of the multiplication of any
two feature dimensions, zðiÞzðjÞi;j¼1...d. We construct d þ d2
integral images for each feature dimension zðiÞ and multiplication of any two feature dimensions zðiÞzðjÞ.
Let P be the W  H  d tensor of the integral images
Pðx0; y0; iÞ ¼
Fðx; y; iÞ
i ¼ 1 . . . d
and Q be the W  H  d  d tensor of the second-order
integral images
Qðx0; y0; i; jÞ ¼
Fðx; y; iÞFðx; y; jÞ
i; j ¼ 1 . . . d: ð7Þ
In , it is shown that the integral image can be computed
in one pass over the image. In our notation, px;y is the
d-dimensional vector and Qx;y is the d  d dimensional
px;y ¼ Pðx; y; 1Þ . . . Pðx; y; dÞ
Qðx; y; 1; 1Þ
Qðx; y; 1; dÞ
Qðx; y; d; 1Þ
Qðx; y; d; dÞ
Note that Qx;y is a symmetric matrix, and d þ ðd2 þ dÞ=2
passes over the image are enough to compute both P and Q.
The computational complexity of constructing the integral
images is Oðd2WHÞ.
Let Rðx0; y0; x00; y00Þ be the rectangular region, where
ðx0; y0Þ is the upper left coordinate and ðx00; y00Þ is the lower
right coordinate, as shown in Fig. 2. The covariance of the
region bounded by (1, 1) and ðx0; y0Þ is
CRð1;1;x0;y0Þ ¼
S  1 Qx0;y0  1
S px0;y0pT
TUZEL ET AL.: PEDESTRIAN DETECTION VIA CLASSIFICATION ON RIEMANNIAN MANIFOLDS
Fig. 1. Covariance descriptor. The d-dimensional feature image F is constructed from input image I through mapping . The region R is represented
with the covariance matrix CR of the features fzigi¼1::S.
Fig. 2. Integral image. The rectangle Rðx0; y0; x00; y00Þ is defined by its
upper left ðx0; y0Þ and lower right ðx00; y00Þ corners in the image, and each
point is a d dimensional vector.
where S ¼ x0  y0. Similarly, after a few rearrangements, the
covariance of the region Rðx0; y0; x00; y00Þ can be computed as
CRðx0;y0;x00;y00Þ ¼
Qx00;y00 þ Qx01;y01  Qx00;y01  Qx01;y00
S px00;y00 þ px01;y01  px01;y00  px00;y01
px00;y00 þ px01;y01  px01;y00  px00;y01
where S ¼ ðx00  x0 þ 1Þ  ðy00  y0 þ 1Þ. Therefore, after constructing integral images, the covariance of any rectangular
region can be computed in Oðd2Þ time.
Covariance Descriptors for Pedestrian
For the pedestrian detection problem, we define the
mapping ðI; x; yÞ as
x y jIxj jIyj
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
jIxxj jIyyj arctan jIxj
where x and y are the pixel location, Ix; Ixx; :: are intensity
derivatives, and the last term is the edge orientation. With
the defined mapping, the input image is mapped to a
d ¼ 8-dimensional feature image. The covariance descriptor
of a region is an 8  8 matrix, and due to symmetry,
only the upper triangular part is stored, which has only
36 different values. The descriptor encodes information of
the variances of the defined features inside the region,
their correlations with each other, and the spatial layout.
Given an arbitrary-sized detection window R, there are a
very large number of covariance descriptors that can be
computed from subwindows r1;2;..., as shown in Fig. 3. We
perform sampling and consider all the subwindows r,
starting with a minimum size of 1/10 of the width and
height of the detection window R, at all possible locations.
The size of r is incremented in steps of 1/10 along the
horizontal, vertical, or both, until r ¼ R. Although the
approach might be considered redundant due to overlaps,
there is significant evidence that the overlapping regions
are an important factor in detection performances , .
The greedy feature selection mechanism, which will be
described later, allows us to search for the best regions
during learning classifiers.
Although it has been mentioned that the covariance
descriptors are robust toward illumination changes, we
would like to enhance the robustness to also include local
illumination variations in an image. Let r be a possible
feature subwindow inside the detection window R. We
compute the covariance of the detection window CR and
subwindow Cr using an integral representation. The
normalized covariance descriptor of region r, denoted as
^Cr, is computed by dividing the columns and rows of Cr
with the square root of the respective diagonal entries of
^Cr ¼ diagðCRÞ1
2CrdiagðCRÞ1
where diagðCRÞ is equal to CR at the diagonal entries and
the rest is truncated to zero. The method described is
equivalent to first normalizing the feature vectors inside the
region R to have zero mean and unit standard deviation
and, after that, computing the covariance descriptor of
subwindow r. Notice that, under the transformation, ^CR is
equal to the correlation matrix of the features inside the
region R. The process only requires d2 extra division
operations.
RIEMANNIAN GEOMETRY
We present a brief introduction to Riemannian geometry,
focusing on the space of symmetric positive definite
matrices. See for a more detailed description. We refer
to points in a vector space with small bold letters, x 2 IRm,
and to points on the manifold with capital bold letters,
X 2 M. Unless explicitly specified by a subscript, all the
matrix norms are computed by the Frobenius norm
kXk2 ¼ traceðXXTÞ, and the vector norms are computed
by the L2 norm.
Riemannian Manifolds
A manifold is a topological space that is locally similar to a
euclidean space. Every point on the manifold has a
neighborhood for which there exists a homeomorphism
(one-to-one, onto, and continuous mapping in both
directions) mapping the neighborhood to IRm. For differentiable manifolds, it is possible to define the derivatives
of the curves on the manifold. The derivatives at a point X
on the manifold lie in a vector space TX, which is the
tangent space at that point. A Riemannian manifold M is a
differentiable manifold in which each tangent space has an
inner product <; >X2M , which varies smoothly from point
to point. The inner product induces a norm for the tangent
vectors in the tangent space such that kyk2
X ¼ < y; y >X .
The minimum length curve connecting two points on the
manifold is called the geodesic, and the distance between
the points dðX; YÞ is given by the length of this curve. Let
y 2 TX and X 2 M. From X, there exists a unique geodesic
starting with the tangent vector y. The exponential map
expX : TX 7! M maps the vector y to the point reached by
this geodesic, and the distance of the geodesic is given by
dðX; expXðyÞÞ ¼ kykX.
In general, the exponential map expX is onto but only
one-to-one in a neighborhood of X. Therefore, the inverse
mapping logX : M 7! TX is uniquely defined only around a
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
OCTOBER 2008
Fig. 3. The detection window is R, and r1 and r2 are two possible
descriptor subwindows.
small neighborhood of the point X. If for any Y 2 M, there
exist several y 2 TX such that Y ¼ expXðyÞ, then logXðYÞ is
given by the tangent vector with the smallest norm. Notice
that both operators are point dependent, where the
dependence is made explicit with the subscript.
Space of Symmetric Positive Definite Matrices
The d  d dimensional symmetric positive definite matrices
(nonsingular covariance matrices) Symþ
d can be formulated
as a connected Riemannian manifold, and an invariant
Riemannian metric on the tangent space of Symþ
d is given
< y; z >X¼ trace X1
The exponential map associated to the Riemannian metric
expXðyÞ ¼ X
is a global diffeomorphism (one-to-one, onto, and continuously differentiable mapping in both directions). Therefore,
the logarithm is uniquely defined at all the points on the
logXðYÞ ¼ X
The exp and log are the ordinary matrix exponential and
logarithm operators. Not to be confused, expX and logX are
manifold specific operators, which are also point dependent, X 2 Symþ
d . The tangent space of Symþ
d is the space of
d  d symmetric matrices, and both the manifold and the
tangent spaces are m ¼ dðd þ 1Þ=2 dimensional.
For symmetric matrices, the ordinary matrix exponential
and logarithm operators can be computed easily. Let  ¼
UDUT be the eigenvalue decomposition of a symmetric
matrix. The exponential series is
k! ¼ U expðDÞUT;
where expðDÞ is the diagonal matrix of the eigenvalue
exponentials. Similarly, the logarithm is given by
ð  IÞk ¼ U logðDÞUT:
The exponential operator is always defined, whereas the
logarithms only exist for symmetric matrices with positive
eigenvalues, Symþ
From the definition of the geodesic given in the previous
section, the distance between two points on Symþ
measured by substituting (15) into (13)
d2ðX; YÞ ¼ < logXðYÞ; logXðYÞ >X
¼ trace log2 X1
We note that an equivalent form of the affine-invariant
distance metric was first given in in terms of joint
eigenvalues of X and Y.
Minimal Representation on the Tangent Space
For classification, we need a minimal representation of the
points in the tangent space. Since the tangent space is the
space of symmetric matrices, there are only dðd þ 1Þ=2
independent coefficients, which are the upper triangular or
lower triangular part of the matrix. The off-diagonal entries
are counted twice during norm computation. We define an
orthonormal coordinate system for the tangent space with
the vector operation. The orthonormal coordinates of a
tangent vector y in the tangent space at point X is given by
the vector operator
vecXðyÞ ¼ vecI X1
where I is the identity matrix, and the vector operator at
identity is defined as
vecIðyÞ ¼ y1;1
y1;3 . . . y2;2
y2;3 . . . yd;d
Notice that the tangent vector y is a symmetric matrix, and
with the vector operator vecXðyÞ, we get the orthonormal
coordinates of y, which is in IRm.
The vector operator relates the Riemannian metric (13)
on the tangent space to the canonical metric defined in IRm
< y; y >X¼ vecXðyÞ
Mean of the Points on Riemannian Manifolds
Let fXigi¼1...N be the set of points on a Riemannian
manifold M. Similar to euclidean spaces, the Karcher mean
 of points on the Riemannian manifold is the point on
M that minimizes the sum of squared Riemannian
 ¼ arg min
d2ðXi; XÞ;
where, in our case, d2 is the distance metric (18). In general,
the Riemannian mean for a set of points is not necessarily
unique. This can be easily verified by considering two
points at antipodal positions on a sphere, where the error
function is minimal for any point lying on the equator.
However, it is shown in several studies that the mean is
unique, and the gradient descent algorithm is convergent
d , , .
Differentiating the error function with respect to X, we
see that the mean is the solution to the nonlinear matrix
logXðXiÞ ¼ 0;
which gives the following gradient descent procedure :
tþ1 ¼ expt
logtðXiÞ
The method iterates by computing first-order approximations to the mean on the tangent space. The weighted mean
computation is similar to (24). We replace, inside of the
TUZEL ET AL.: PEDESTRIAN DETECTION VIA CLASSIFICATION ON RIEMANNIAN MANIFOLDS
exponential, the mean of the tangent vectors with the
weighted mean
i¼1 wi logtðXiÞ.
CLASSIFICATION ON RIEMANNIAN MANIFOLDS
Let fðXi; yiÞgi¼1...N be the training set with respect to class
labels, where Xi 2 M, yi 2 f0; 1g, and M is a Riemannian
manifold. We want to find a function FðXÞ : M 7! f0; 1g,
which divides the manifold into two based on the training
set of labeled items.
A function that divides the manifold is a rather
complicated notion compared with the euclidean space.
For example, consider the simplest form of a linear classifier
in IR2. A point and a direction vector in IR2 define a line that
separates IR2 into two. Equivalently, on a two-dimensional
differentiable manifold, we can consider a point on the
manifold and a tangent vector in the tangent space of the
point, which together define a curve on the manifold via an
exponential map. For example, if we consider the image of
the lines on the 2-torus, the curves never divide the
manifold into two.
A straightforward approach for classification would be
to map the manifold to a higher dimensional euclidean
space, which can be considered as flattening the manifold.
However, in a general case, there is no such mapping that
globally preserves the distances between the points on the
manifold. Therefore, a classifier trained on the flattened
space does not reflect the global structure of the points.
Local Maps and Boosting
We propose an incremental approach by training several
weak classifiers on the tangent spaces and combining them
through boosting. We start by defining mappings from
neighborhoods on the manifold to the euclidean space,
similar to coordinate charts. Our maps are the logarithm
maps logX that map the neighborhood of points X 2 M to
the tangent spaces TX. Since this mapping is a homeomorphism around the neighborhood of the point, the
structure of the manifold is locally preserved. The tangent
space is a vector space, and we use standard machine
learning techniques to learn the classifiers on this space.
For the classification task, the approximations to the
Riemannian distances computed on the ambient space
should be as close to the true distances as possible. Since we
approximate the distances (13) on the tangent space TX,
d2ðY; ZÞ  vecX logXðZÞ
Þ  vecX logXðYÞ
it is a first-order approximation. The approximation error
can be expressed in terms of the pairwise distances
computed on the manifold and the tangent space
 vecX logXðXiÞ
Þ  vecX logXðXjÞ
which is equal to
for the space of symmetric positive definite matrices, using
(15) and (21).
The classifiers can be learned on the tangent space at
any point X on the manifold. The best approximation,
which preserves the pairwise distances, is achieved at
the minimum of Symþ
d . The error can be minimized with
respect to X, which gives the best tangent space to learn
the classifier.
Since the mean of the points (22) is the minimizer of the
sum of squared distances from the points in the set and the
mapping preserves the structure of the manifold locally, it
is also a good candidate for the minimizer of the error
function (27). However, to our knowledge, a theoretical
proof does not exist. For some special cases, it can be easily
verified that the mean is the minimizer. Such a case arises
when all the points lie on a geodesic curve, where the
approximation error is zero for any point lying on the curve.
Since the mean also lies on the geodesic curve, the
approximation is perfect. Nevertheless, for a general set of
points, we only have empirical validation based on
simulations. We generated random points on Symþ
times with varying d. The approximation errors were
measured on the tangent spaces at any of the points
TXi¼1...N and at the mean TX. In our simulations, the errors
computed on the tangent spaces at the means were
significantly lower than any other choice, and counterexamples were not observed. The simulations were also
repeated for weighted sets of points, where the minimizers
of the weighted approximation errors were achieved at the
weighted means of the points.
Therefore, the weak learners are learned on the tangent
space at the mean of the points. At each iteration, we
compute the weighted mean of the points through (24),
where the weights are adjusted through boosting. Then, we
map the points to the tangent space at the weighted mean
and learn a weak classifier on this vector space. Since the
weights of the samples that are misclassified during the
earlier stages of boosting increase, the weighted mean
moves toward these points, and more accurate classifiers
are learned for these points. The process is illustrated in
Fig. 4. To evaluate a test example, the sample is projected to
the tangent spaces at the computed weighted means and the
weak learners are evaluated (Fig. 5). The approximation
error is minimized by averaging over several weak learners.
LogitBoost on Riemannian Manifolds
We start with a brief description of the LogitBoost algorithm
 on vector spaces. We consider the binary classification
problem yi 2 f0; 1g. The probability of x being in class 1 is
represented by
eFðxÞ þ eFðxÞ ;
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
OCTOBER 2008
The LogitBoost algorithm learns the set of regression
functions fflðxÞgl¼1...L (weak learners) by minimizing the
negative binomial log likelihood of the data lðy; pðxÞÞ;
yilog pðxiÞ
Þ þ ð1  yiÞlog 1  pðxiÞ
through Newton iterations. At the core of the algorithm,
LogitBoost fits a weighted least squares regression flðxÞ of
training points xi 2 IRm to response values zi 2 IR with
weights wi, where
yi  pðxiÞ
pðxiÞ 1  pðxiÞ
wi ¼ pðxiÞ 1  pðxiÞ
The LogitBoost algorithm on Riemannian manifolds
is similar to the original LogitBoost, except for a few
differences at the level of weak learners. In our case,
the domain of the weak learners are in M such that
flðXÞ : M 7! IR. Following the discussion of the previous
section, we learn the regression functions on the tangent
space at the weighted mean of the points. We define the
weak learners as
flðXÞ ¼ glðveclðloglðXÞÞÞ
and learn the functions glðxÞ : IRm 7! IR and the weighted
mean of the points l 2 M. Notice that the mapping vecl
(19) gives the orthonormal coordinates of the tangent
vectors in Tl.
The algorithm is presented in Fig. 6. The steps marked
with ð Þ are the differences from the original LogitBoost
algorithm. For functions fglgl¼1...L, it is possible to use any
TUZEL ET AL.: PEDESTRIAN DETECTION VIA CLASSIFICATION ON RIEMANNIAN MANIFOLDS
Fig. 4. Two iterations of boosting on a Riemannian manifold. The manifold is depicted with the surface of the sphere, and the plane is the tangent
space at the mean. The samples are projected to tangent spaces at means via logl . The weak learners gl are learned on the tangent spaces Tl.
(a) Sample X3 is misclassified; therefore its weight increases. (b) In the second iteration of boosting, the weighted mean moves toward X3.
Fig. 5. Classification on a Riemannian manifold. The sample X is
projected to the tangent spaces Tl
and the weak learners are
evaluated.
Fig. 6. LogitBoost on Riemannian manifolds.
form of weighted least squares regression, such as linear
functions, regression stumps, etc., since the domains of the
functions are in IRm.
PEDESTRIAN DETECTION
In this section, we describe the utilization of the LogitBoost
algorithm on Riemannian manifolds for the pedestrian
detection problem. The domain of the classifier is the space
of eight-dimensional symmetric positive definite matrices
8 . We combine K ¼ 30 LogitBoost classifiers on Symþ
with rejection cascade, as shown in Fig. 7. Weak learners
gk;l are linear regression functions learned on the tangent
space of Symþ
8 . The tangent space is a m ¼ 36-dimensional
vector space.
To avoid confusion with the previous section, we refer
to the training set by fðRi; yiÞgi¼1...N, where Ri are the
image windows containing the background and pedestrians, and yi 2 f0; 1g are the labels. A very large number of
covariance descriptors can be computed from a single
detection window R. Therefore, we do not have a single set
of positive and negative features but several sets corresponding to each of the possible subwindows. Each weak
learner is associated with a single subwindow of the
detection window. Let rk;l be the subwindow associated
with lth weak learner of cascade level k. The normalized
covariance descriptor of the ith training sample associated
with region rk;l is referred by Xi;k;l ¼ ^Ci;rk;l. For simplicity,
we use the shorthand notation
fk;lðRiÞ ¼ fk;lðXi;k;lÞ
for weak learners.
i refer to the Np positive and Nn negative
samples in the training set, where N ¼ Np þ Nn. While
training the kth cascade level, we classify all the negative
examples fR
i gi¼1...Nn with the cascade of the previous ðk  1Þ
LogitBoost classifiers. The samples that are correctly classified (samples classified as negative) are removed from the
training set. Any window sampled from a negative image is a
negative example; therefore, the cardinality of the negative
set Nn is very large. During the training of each cascade level,
we sample 10,000 negative examples.
The learning algorithm is slightly customized for the
pedestrian detection task. We do not have a fixed number of
weak learners L for each LogitBoost classifier k but a variable
number Lk. Each cascade level is optimized to correctly
detect at least 99.8 percent of the positive examples, while
rejecting at least 35 percent of the negative examples. In
addition, we enforce a margin constraint between the
positive samples and the decision boundary. Let pkðRÞ be
the learned probability function of a sample being positive at
cascade level k, evaluated through (28). Let Rp be the positive
example that has the ð0:998NpÞth largest probability among
all the positive examples. Let Rn be the negative example
that has the ð0:35NnÞth smallest probability among all the
negative examples. We continue to add weak classifiers to
cascade level k until pkðRpÞ  pkðRnÞ > margin, where we set
margin ¼ 0:2. When the constraint is satisfied, the threshold
(decision boundary) for cascade level k is stored as
thrdk ¼ FkðRnÞ.
A test sample is classified as positive by cascade level k
if FkðRÞ > thrdk or, equivalently, pkðRÞ > pkðRnÞ. With the
proposed method, any of the positive training samples in
the top 99.8 percentile have at least margin more probability than the points on the decision boundary. The
process continues with the training of the ðk þ 1Þth cascade
level until k ¼ K.
We incorporate a greedy feature selection method to
produce a sparse set of classifiers focusing on important
subwindows. At each boosting iteration l of the kth
LogitBoost level, we sample 200 subwindows among all
the possible subwindows and construct normalized covariance descriptors. We learn the weak classifiers representing each subwindow and add the best classifier that
minimizes the negative binomial log likelihood (29) to the
cascade level k. The procedure iterates with the training of
the ðl þ 1Þth weak learner until the specified detection rates
are satisfied.
The negative sample set is not well characterized for
detection tasks. Therefore, while projecting the points to the
tangent space, we compute the weighted mean of only the
positive samples. Although it rarely happens, if some of the
features are fully correlated, there will be singularities in the
covariance descriptor. We ignore those cases by adding a
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
OCTOBER 2008
Fig. 7. Cascade of LogitBoost classifiers. The kth LogitBoost classifier selects normalized covariance descriptors of subwindows rk;i.
very small identity matrix to the covariance. The pedestrian
detection with cascade of LogitBoost classifiers on Symþ
given in Fig. 8.
The learning algorithm produces a set of K LogitBoost
classifiers, which are composed of Lk triplets,
ðrk;l; k;l; gk;lÞ
l¼1...Lk and thrdk;
where rk;l is the selected subwindow, k;l is the mean, and
gk;l is the learned regression function of the lth weak
learner of the kth cascade. To evaluate a test region R with
kth classifier, the normalized covariance descriptors constructed from regions rk;l are projected to tangent spaces
Tk;l, and the features are evaluated with gk;l,
sign FkðRÞ  thrdk
gk;l veck;l logk;l
The initial levels of the cascade are learned on relatively
easy examples; thus, there are very few weak classifiers in
these levels. Due to the cascade structure, only a few are
evaluated for most of the test samples, which produce a
very efficient solution.
EXPERIMENTS
We conduct experiments on two challenging data sets,
INRIA and DaimlerChrysler. We compare the performance
of our method with the best results published on the given
data sets. In addition, we present several detection
examples for crowded scenes.
Experiments on the INRIA Data Set
The INRIA pedestrian data set contains 1,774 pedestrian annotations (3,548 with reflections) and 1,671 personfree images. The pedestrian annotations were scaled into a
fixed size of 64  128 windows, which include a margin
of 16 pixels around the pedestrians. The data set was
divided into two, where 2,416 pedestrian annotations and
1,218 person-free images were selected as the training set,
and 1,132 pedestrian annotations and 453 person-free
images were selected as the test set. Detection on the INRIA
pedestrian data set is challenging since it includes subjects
with a wide range of variations in pose, clothing, illumination, background, and partial occlusions.
In the first experiment, we compare our results with 
and . Although it has been noted that kernel SVM is
computationally expensive, we consider both the linear and
kernel SVM method in . In , a cascade of AdaBoost
classifiers was trained using HOG features, and two
different results were reported based on the normalization
of the descriptors. Here, we consider only the best
performing result, the L2 norm.
In Fig. 9, we plot the detection error trade-off curves on a
log-log scale. The y-axis corresponds to the miss rate
FalseNegþTruePos , and the x-axis corresponds to false positives
per window (FPPW)
TrueNegþFalsePos . The curve for our
method is generated by adding one cascade level at a time.
For example, in our case, the rightmost marker at 7:5 103
FPPW corresponds to detection using only the first 11 levels
of cascade, whereas the marker positioned at 4 105 FPPW
corresponds to the cascade of all 30 levels. The markers
between the two extremes correspond to a cascade of
between 11 and 30 levels.
TUZEL ET AL.: PEDESTRIAN DETECTION VIA CLASSIFICATION ON RIEMANNIAN MANIFOLDS
Fig. 8. Pedestrian detection with cascade of LogitBoost classifiers on
Fig. 9. Comparison with the methods of Dalal and Triggs and Zhu
et al. on the INRIA data set. The curves for other approaches are
generated from the respective papers. See text for details.
To generate the result at 105 FPPW (leftmost marker),
we shifted the decision boundaries of all the cascade levels
thrdk to produce fewer false positives at the cost of higher
miss rates. We see that at almost all the false-positive rates,
our miss rates are significantly lower than the other
approaches. The closest result to our method is the kernel
SVM classifier in , which requires kernel evaluation at
1,024-dimensional space to classify a single detection
window. If we consider 104 as an acceptable FPPW, our
miss rate is 6.8 percent, where the second best result is
9.3 percent.
Since the method removes samples that were rejected by
the previous levels of cascade, during the training of the last
levels, only a very small number of negative samples, on the
order of 102, remained. At these levels, the training error
did not generalize well such that the same detection rates
are not achieved on the test set. This can be seen by the
dense markers around FPPW < 7 105. We believe that
better detection rates can be achieved at low false-positive
rates with the introduction of more negative images. In our
method, 25 percent of false positives originated from a
single image, which contained a flower texture, where the
training set did not include a similar example. We note that,
recently, in , a pedestrian detection system utilizing
shapelet features was described, which had 20-40 percent
lower miss rates at equal FPPWs on the INRIA data set,
compared to our approach. The drawback of the method is
the significantly higher computational requirement.
In the second experiment, we consider an empirical
validation of the presented classification algorithm on
Riemannian manifolds. In Fig. 10, we present the detection
error trade-off curves for the following four different
approaches:
We apply the original method, which maps the
points to the tangent spaces at the weighted means.
The mean computation step is removed from the
original algorithm, and points are always mapped to
the tangent space at the identity.
We ignore the geometry of Symþ
and stack the
upper triangular part of the covariance matrix into
a vector such that learning is performed on the
vector space.
We replace the covariance descriptors with HOG
descriptors and perform original (vector space)
LogitBoost classification.
The original method outperforms all the other approaches
significantly. The second best result is achieved by mapping
points to the tangent space at the identity matrix followed
by the vector space approaches. Notice that our LogitBoost
implementation utilizing HOG descriptors has 3 percent
more miss rate at 104 FPPW than , which trains an
AdaBoost classifier. The performance is significantly degraded beyond this point.
In the third experiment, we examine the sensitivity of the
covariance and HOG descriptors to translation and scaling
of the target windows relative to the original position. The
performance of the HOG descriptors is tested with our
implementation. The false-positive rates of both classifiers
are fixed at 104 FPPW. The translation sensitivity is
presented in Fig. 11, where we observe that the covariance
descriptors are less sensitive to small translations of the
target windows. The detection rate is almost constant for
6 pixel translation, which approximately corresponds to
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
OCTOBER 2008
Fig. 10. Detection rates of different approaches for our method on the
INRIA data set. See text for details.
Fig. 11. Translation sensitivity. (a) Covariance descriptors. (b) HOG descriptors. Covariance descriptors have a larger region of support.
10 percent translation in the x-axis and 5 percent translation
in the y-axis of the target window (64  128).
The scale sensitivity of both methods are presented in
Fig. 12, where we again observe that the covariance
descriptors are less sensitive to small scalings of the target
windows. For covariance descriptors, the detection rates are
almost constant for
20 percent scalings and gradually
decrease beyond.
For detection applications, there is a trade-off between
the low and high sensitivity of the detector with respect to
small transformations. While detecting objects in novel
scenes, objects are searched through the transformation
space. A detector with invariance to small transformations
has the advantage of reducing the size of the search space,
whereas the space should be searched more densely with a
high-sensitivity detector. Besides, a less sensitive detector is
more desirable when the target objects have high variability
and training data is not perfectly aligned. On the other
hand, a highly sensitive detector can better localize the
targets. In Figs. 11 and 12, we see that the detection rates for
our approach are smooth and symmetric functions with
respect to both translation and scale, having a peak at the
original location of the target. Therefore, with a simple
maxima search, we can accurately localize the targets.
Please see Section 6.3 for more details.
In Fig. 13, we plot the number of weak classifiers at each
cascade level and the accumulated rejection rate over the
cascade levels. There are very few classifiers on early levels
of cascade, and the first five levels reject 90 percent of the
negative examples. On the average, our method requires the
evaluation of 8.45 covariance descriptors per negative
detection window, whereas on the average, 15.62 HOG
evaluations were required in .
Experiments on the DaimlerChrysler Data Set
The DaimlerChrysler data set contains 4,000 pedestrian
(24,000 with reflections and small shifts) and 25,000
nonpedestrian annotations. As opposed to the INRIA data
set, nonpedestrian annotations were selected by a preprocessing step from the negative samples, which match a
pedestrian shape template based on the average Chamfer
distance score. Both annotations were scaled into a fixed
size of 18  36 windows, and pedestrian annotations
include a margin of 2 pixels around. The data set was
organized into three training and two test sets, each of them
having 4,800 positive and 5,000 negative examples. The
small size of the windows, combined with a carefully
arranged negative set, makes detection on the Daimler-
Chrysler data set extremely challenging. In addition,
3,600 person-free images with varying sizes between
360  288 and 640  480 were also supplied.
In , an experimental study was described, comparing
three different feature descriptors and various classification
techniques. The compared feature descriptors were the PCA
coefficients, Haar wavelets, and local receptive fields
(LRFs), which are the output of the hidden layer of a
specially designed feed-forward NN. The connections of the
neurons in the hidden layer of the NN were restricted to
local regions of the image, and the hidden layers were
divided into branches, with all the neurons sharing the
same set of weights. Although several other classification
methods were also considered, the best detection performances among all the different features were achieved
utilizing SVMs.
In the first experiment, we compare our method with the
best results for each descriptor in . The same training
configuration is prepared by selecting two out of three
training sets. Since the number of nonpedestrian annotations was very limited for the training of our method, we
adapted the training parameters. A cascade of K ¼ 15
LogitBoost classifiers on Symþ
8 is learned, where each level
is optimized to detect at least 99.75 percent of the positive
TUZEL ET AL.: PEDESTRIAN DETECTION VIA CLASSIFICATION ON RIEMANNIAN MANIFOLDS
Fig. 12. Scale sensitivity. The x-axis is plotted at log scale. Covariance
descriptors are less sensitive to small scale changes.
Fig. 13. The number of weak classifiers at each cascade level and the accumulated rejection rate over the cascade levels. See text for details.
examples, while rejecting at least 25 percent negative
In Fig. 14, we plot the detection error trade-off curves.
The cascade of 15 LogitBoost classifiers produced an FPPW
rate of 0.05. The detection rates with lower FPPW are
generated by shifting the decision boundaries of all the
cascade levels gradually, until FPPW ¼ 0:01. We see that
our approach has significantly lower miss rates at all the
false-positive rates. This experiment should not be confused
with the experiments on the INRIA data set, where much
lower FPPW rates were observed. Here, the negative set
consists of hard examples selected by a preprocessing step.
In the second experiment, we set up a different test
configuration on the DaimlerChrysler data set. The 3,600
person-free images are divided into two, where 2,400 images
are selected as the negative training set, and 1,200 images are
selected for the negative test set. For both the covariance
descriptors and the HOG descriptors, we trained a cascade
of K ¼ 25 classifiers. We observed that the object sizes were
too small for HOG descriptors to separate among positive
and negative examples at the later levels of cascade. The
classifiers trained utilizing HOG descriptors failed to
achieve the specified detection (99.8 percent) and rejection
rates (35.0 percent). We stopped adding weak learners to a
cascade level after reaching Lk ¼ 100. The detection error
trade-off curves are given in Fig. 15, where we see that
the covariance descriptors significantly outperform HOG
descriptors.
Detection Examples
Since the sizes of the pedestrians in novel scenes are not
known a priori, the images are searched at multiple scales.
There are two searching strategies. The first strategy is to
scale the detection window and apply the classifier at
multiple scales. The second strategy is to scale the image
and apply the classifier at the original scale. In covariance
representation, we utilized gradient-based features, which
are scale dependent. Therefore, evaluating the classifier at
the original scale (second strategy) produces the optimal
result. However, in practice, up to scales of 2x, we observed
that the detection rates were almost the same, whereas in
more extreme scale changes, the performance of the first
strategy degraded. The drawback of the second strategy is a
slightly increased search time, since the method requires
computation of the filters and the integral representation at
multiple scales.
Utilizing the classifier trained on the INRIA data set, we
generated several detection examples for crowded scenes
with pedestrians having variable illumination, appearance,
pose, and partial occlusion. The results are shown in Fig. 16.
The images are searched at five scales using the first
strategy, starting with the original window size of 64  128
and two smaller and two larger scales of ratio 1.2. The white
dots are all the detection results, and we filtered them with
adaptive bandwidth mean shift filtering with bandwidth
1/10th of the window width and height. Black dots show
the modes, and ellipses are generated by averaging the
detection window sizes converging to the modes.
Computational Complexity
The training of the classifiers took two days on a Pentium D
2.80-Ghz processor with 2.00 Gbytes of RAM with a C++
implementation, which is a reasonable time to train a
cascade model. The computation of the tensor of integral
images requires Oðd2WHÞ arithmetic operations, which
approximately takes 0.1 second for a 320  240 image. The
computation of the normalized covariance descriptor of an
arbitrary region requires Oðd2Þ operations using the integral
structures and is invariant of the region size.
The most computationally expensive operation during
the classification is the eigenvalue decomposition to compute the logarithm of a symmetric matrix, which requires
Oðd3Þ arithmetic operations. Given a test image, on the
average, the method can search around 3,000 detection
windows per second, which approximately corresponds to
3 seconds for a dense scan of a 320  240 image, 3 pixel
jumps vertically and horizontally.
CONCLUSION
We presented a new approach for the pedestrian detection
problem utilizing covariance matrices as object descriptors
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
OCTOBER 2008
Fig. 14. Comparison with on the DaimlerChrysler data set. The
curves for other approaches are generated from the original paper. See
text for details.
Fig. 15. Comparison of covariance and HOG descriptors on the
DaimlerChrysler data set. See text for details.
and a novel learning algorithm on the Riemannian manifolds. The superior performance of the proposed approach
is shown on the INRIA and DaimlerChrysler human data
sets with detailed comparisons to the existing methods.
The proposed learning algorithm is not specific to Symþ
and can be used to train classifiers for points lying on any
connected Riemannian manifold. In addition, the approach
can be combined with any boosting method. During our
experiments, we implemented LogitBoost, GentleBoost, and
AdaBoost classifiers on Riemannian manifolds using LDA,
decision stumps, and linear SVMs as weak learners. The
results of the methods were comparative. Due to its
simplicity (training time and ease of implementation) and
slightly better performance, we presented the LogitBoost
algorithm.
In the future, we are planning to extend our research in
two directions. First, the method will be generalized for the
multiclass object detection task, where instead of a binary
TUZEL ET AL.: PEDESTRIAN DETECTION VIA CLASSIFICATION ON RIEMANNIAN MANIFOLDS
Fig. 16. Detection examples. The classifier is trained on the INRIA data set. White dots show all the detection results. Black dots are the modes
generated by mean shift smoothing, and the ellipses are average detection window sizes. There are extremely few false positives and negatives.
classifier, a multiclass learning scheme will be investigated
on Riemannian manifolds. Second, we are planning to test
our algorithm on different Riemannian manifolds that
commonly occur in computer vision problems, such as
rigid motion estimation on a special euclidean group SEðnÞ
or shape space descriptors on the Grassmann manifold Gn;k.
ACKNOWLEDGMENTS
The authors would like to thank Mitsubishi Electric
Research Labs, Cambridge, Massachusetts, for supporting
this study and Navneet Dalal, Bill Triggs, Qiang Zhu, Shai
Avidan, Stefan Munder, and Dariu Gavrila for providing
the data sets and the results of their experiments.