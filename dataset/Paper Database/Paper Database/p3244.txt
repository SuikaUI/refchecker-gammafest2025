Published as a conference paper at ICLR 2017
TRAINED TERNARY QUANTIZATION
Chenzhuo Zhu∗
Tsinghua University
 
Stanford University
 
Stanford University
 
William J. Dally
Stanford University
 
Deep neural networks are widely used in machine learning applications. However,
the deployment of large neural networks models can be difﬁcult to deploy on mobile
devices with limited power budgets. To solve this problem, we propose Trained
Ternary Quantization (TTQ), a method that can reduce the precision of weights in
neural networks to ternary values. This method has very little accuracy degradation
and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on
CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from
scratch, which means it’s as easy as to train normal full precision model. We
highlight our trained quantization method that can learn both ternary values and
ternary assignment. During inference, only ternary values (2-bit weights) and
scaling factors are needed, therefore our models are nearly 16× smaller than fullprecision models. Our ternary models can also be viewed as sparse binary weight
networks, which can potentially be accelerated with custom circuit. Experiments on
CIFAR-10 show that the ternary models obtained by trained quantization method
outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%,
respectively. On ImageNet, our model outperforms full-precision AlexNet model
by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.
INTRODUCTION
Deep neural networks are becoming the preferred approach for many machine learning applications.
However, as networks get deeper, deploying a network with a large number of parameters on a small
device becomes increasingly difﬁcult. Much work has been done to reduce the size of networks. Halfprecision networks cut sizes of neural networks in half. XNOR-Net , DoReFa-Net and network binarization use aggressively quantized weights, activations and gradients to further reduce
computation during training. While weight binarization beneﬁts from 32× smaller model size, the
extreme compression rate comes with a loss of accuracy. Hubara et al. and Li & Liu 
propose ternary weight networks to trade off between model size and accuracy.
In this paper, we propose Trained Ternary Quantization which uses two full-precision scaling
coefﬁcients W p
l for each layer l, and quantize the weights to {−W n
l , 0, +W p
l } instead of
traditional {-1, 0, +1} or {-E, 0, +E} where E is the mean of the absolute weight value, which is
not learned. Our positive and negative weights have different absolute values W p
l that are
trainable parameters. We also maintain latent full-precision weights at training time, and discard
them at test time. We back propagate the gradient to both W p
l and to the latent full-precision
weights. This makes it possible to adjust the ternary assignment (i.e. which of the three values a
weight is assigned).
Our quantization method, achieves higher accuracy on the CIFAR-10 and ImageNet datasets. For
AlexNet on ImageNet dataset, our method outperforms previously state-of-art ternary network by 3.0% of Top-1 accuracy and the full-precision model by 1.6%. By converting most of
the parameters to 2-bit values, we also compress the network by about 16x. Moreover, the advantage
of few multiplications still remains, because W p
l are ﬁxed for each layer during inference.
On custom hardware, multiplications can be pre-computed on activations, so only two multiplications
per activation are required.
MOTIVATIONS
The potential of deep neural networks, once deployed to mobile devices, has the advantage of lower
latency, no reliance on the network, and better user privacy. However, energy efﬁciency becomes the
bottleneck for deploying deep neural networks on mobile devices because mobile devices are battery
constrained. Current deep neural network models consist of hundreds of millions of parameters.
Reducing the size of a DNN model makes the deployment on edge devices easier.
First, a smaller model means less overhead when exporting models to clients. Take autonomous
driving for example; Tesla periodically copies new models from their servers to customers’ cars.
Smaller models require less communication in such over-the-air updates, making frequent updates
more feasible. Another example is on Apple Store; apps above 100 MB will not download until you
connect to Wi-Fi. It’s infeasible to put a large DNN model in an app. The second issue is energy
consumption. Deep learning is energy consuming, which is problematic for battery-constrained
mobile devices. As a result, iOS 10 requires iPhone to be plugged with charger while performing
photo analysis. Fetching DNN models from memory takes more than two orders of magnitude more
energy than arithmetic operations. Smaller neural networks require less memory bandwidth to fetch
the model, saving the energy and extending battery life. The third issue is area cost. When deploying
DNNs on Application-Speciﬁc Integrated Circuits (ASICs), a sufﬁciently small model can be stored
directly on-chip, and smaller models enable a smaller ASIC die.
Several previous works aimed to improve energy and spatial efﬁciency of deep networks. One
common strategy proven useful is to quantize 32-bit weights to one or two bits, which greatly reduces
model size and saves memory reference. However, experimental results show that compressed
weights usually come with degraded performance, which is a great loss for some performancesensitive applications. The contradiction between compression and performance motivates us to work
on trained ternary quantization, minimizing performance degradation of deep neural networks while
saving as much energy and space as possible.
RELATED WORK
BINARY NEURAL NETWORK (BNN)
Lin et al. proposed binary and ternary connections to compress neural networks and speed up
computation during inference. They used similar probabilistic methods to convert 32-bit weights into
binary values or ternary values, deﬁned as:
wb ∼Bernoulli( ˜w + 1
wt ∼Bernoulli(| ˜w|) × sign( ˜w)
Here wb and wt denote binary and ternary weights after quantization. ˜w denotes the latent full
precision weight.
During back-propagation, as the above quantization equations are not differentiable, derivatives of
expectations of the Bernoulli distribution are computed instead, yielding the identity function:
Here L is the loss to optimize.
For BNN with binary connections, only quantized binary values are needed for inference. Therefore
a 32× smaller model can be deployed into applications.
Published as a conference paper at ICLR 2017
DOREFA-NET
Zhou et al. proposed DoReFa-Net which quantizes weights, activations and gradients of neural
networks using different widths of bits. Therefore with speciﬁcally designed low-bit multiplication
algorithm or hardware, both training and inference stages can be accelerated.
They also introduced a much simpler method to quantize 32-bit weights to binary values, deﬁned as:
wb = E(| ˜w|) × sign( ˜w)
Here E(| ˜w|) calculates the mean of absolute values of full precision weights ˜w as layer-wise scaling
factors. During back-propagation, Equation 2 still applies.
TERNARY WEIGHT NETWORKS
Li & Liu proposed TWN (Ternary weight networks), which reduce accuracy loss of binary
networks by introducing zero as a third quantized value. They use two symmetric thresholds ±∆l
and a scaling factor Wl for each layer l to quantize weighs into {−Wl, 0, +Wl}:
Wl : ˜wl > ∆l
0 : | ˜wl| ≤∆l
−Wl : ˜wl < −∆l
They then solve an optimization problem of minimizing L2 distance between full precision and
ternary weights to obtain layer-wise values of Wl and ∆l:
∆l = 0.7 × E(| ˜wl|)
wl(i)|>∆}(| ˜wl(i)|)
And again Equation 2 is used to calculate gradients. While an additional bit is required for ternary
weights, TWN achieves a validation accuracy that is very close to full precision networks according
to their paper.
DEEP COMPRESSION
Han et al. proposed deep compression to prune away trivial connections and reduce precision
of weights. Unlike above models using zero or symmetric thresholds to quantize high precision
weights, Deep Compression used clusters to categorize weights into groups. In Deep Compression,
low precision weights are ﬁne-tuned from a pre-trained full precision network, and the assignment of
each weight is established at the beginning and stay unchanged, while representative value of each
cluster is updated throughout ﬁne-tuning.
Our method is illustrated in Figure 1. First, we normalize the full-precision weights to the range
[-1, +1] by dividing each weight by the maximum weight. Next, we quantize the intermediate
full-resolution weights to {-1, 0, +1} by thresholding. The threshold factor t is a hyper-parameter
that is the same across all the layers in order to reduce the search space. Finally, we perform trained
quantization by back propagating two gradients, as shown in the dashed lines in Figure 1. We
back-propagate gradient1 to the full-resolution weights and gradient2 to the scaling coefﬁcients.
The former enables learning the ternary assignments, and the latter enables learning the ternary
At inference time, we throw away the full-resolution weights and only use ternary weights.
LEARNING BOTH TERNARY VALUES AND TERNARY ASSIGNMENTS
During gradient descent we learn both the quantized ternary weights (the codebook), and choose
which of these values is assigned to each weight (choosing the codebook index).
Published as a conference paper at ICLR 2017
Feed Forward
Back Propagate
Inference Time
Quantization
Full Precision Weight
Normalized
Full Precision Weight
Final Ternary Weight
Intermediate Ternary Weight
Figure 1: Overview of the trained ternary quantization procedure.
To learn the ternary value (codebook), we introduce two quantization factors W p
l for positive
and negative weights in each layer l. During feed-forward, quantized ternary weights wt
l are calculated
l : ˜wl > ∆l
0 : | ˜wl| ≤∆l
l : ˜wl < −∆l
Unlike previous work where quantized weights are calculated from 32-bit weights, the scaling coefﬁcients W p
l are two independent parameters and are trained together with other parameters.
Following the rule of gradient descent, derivatives of W p
l are calculated as:
l = {i| ˜wl(i) > ∆l} and In
l = {i|(i) ˜wl < −∆l}. Furthermore, because of the existence
of two scaling factors, gradients of latent full precision weights can no longer be calculated by
Equation 2. We use scaled gradients for 32-bit weights:
: ˜wl > ∆l
: | ˜wl| ≤∆l
: ˜wl < −∆l
Note we use scalar number 1 as factor of gradients of zero weights. The overall quantization process
is illustrated as Figure 1. The evolution of the ternary weights from different layers during training is
shown in Figure 2. We observe that as training proceeds, different layers behave differently: for the
ﬁrst quantized conv layer, the absolute values of W p
l get smaller and sparsity gets lower,
while for the last conv layer and fully connected layer, the absolute values of W p
l get larger
and sparsity gets higher.
We learn the ternary assignments (index to the codebook) by updating the latent full-resolution
weights during training. This may cause the assignments to change between iterations. Note that
the thresholds are not constants as the maximal absolute values change over time. Once an updated
weight crosses the threshold, the ternary assignment is changed.
The beneﬁts of using trained quantization factors are: i) The asymmetry of W p
neural networks to have more model capacity. ii) Quantized weights play the role of "learning rate
multipliers" during back propagation.
QUANTIZATION HEURISTIC
In previous work on ternary weight networks, Li & Liu proposed Ternary Weight Networks
(TWN) using ±∆l as thresholds to reduce 32-bit weights to ternary values, where ±∆l is deﬁned
as Equation 5. They optimized value of ±∆l by minimizing expectation of L2 distance between
full precision weights and ternary weights. Instead of using a strictly optimized threshold, we adopt
Published as a conference paper at ICLR 2017
Ternary Weight Value
res1.0/conv1/Wn
res1.0/conv1/Wp
res3.2/conv2/Wn
res3.2/conv2/Wp
Ternary Weight
Percentage
Figure 2: Ternary weights value (above) and distribution (below) with iterations for different layers
of ResNet-20 on CIFAR-10.
different heuristics: 1) use the maximum absolute value of the weights as a reference to the layer’s
threshold and maintain a constant factor t for all layers:
∆l = t × max(| ˜w|)
and 2) maintain a constant sparsity r for all layers throughout training. By adjusting the hyperparameter r we are able to obtain ternary weight networks with various sparsities. We use the ﬁrst
method and set t to 0.05 in experiments on CIFAR-10 and ImageNet dataset and use the second one
to explore a wider range of sparsities in section 5.1.1.
We perform our experiments on CIFAR-10 and ImageNet . Our network is implemented on both TensorFlow and Caffe frameworks.
EXPERIMENTS
CIFAR-10 is an image classiﬁcation benchmark containing images of size 32×32RGB pixels in
a training set of 50000 and a test set of 10000. ResNet structure is used for our
experiments.
We use parameters pre-trained from a full precision ResNet to initialize our model. Learning rate is
set to 0.1 at beginning and scaled by 0.1 at epoch 80, 120 and 300. A L2-normalized weight decay
Validation error
Full precision
Binary weight (DoReFa-Net)
Ternary weight (Ours)
Figure 3: ResNet-20 on CIFAR-10 with different weight precision.
Published as a conference paper at ICLR 2017
of 0.0002 is used as regularizer. Most of our models converge after 160 epochs. We take a moving
average on errors of all epochs to ﬁlter off ﬂuctuations when reporting error rate.
We compare our model with the full-precision model and a binary-weight model. We train a a full
precision ResNet on CIFAR-10 as the baseline (blue line in Figure 3). We ﬁne-tune
the trained baseline network as a 1-32-32 DoReFa-Net where weights are 1 bit and both activations
and gradients are 32 bits giving a signiﬁcant loss of accuracy (green line) . Finally, we ﬁne-tuning the
baseline with trained ternary weights (red line). Our model has substantial accuracy improvement
over the binary weight model, and our loss of accuracy over the full precision model is small. We
also compare our model to Tenary Weight Network (TWN) on ResNet-20. Result shows our model
improves the accuracy by ∼0.25% on CIFAR-10.
We expand our experiments to ternarize ResNet with 32, 44 and 56 layers. All ternary models are
ﬁne-tuned from full precision models. Our results show that we improve the accuracy of ResNet-32,
ResNet-44 and ResNet-56 by 0.04%, 0.16% and 0.36% . The deeper the model, the larger the
improvement. We conjecture that this is due to ternary weights providing the right model capacity
and preventing overﬁtting for deeper networks.
Full resolution
Ternary (Ours)
Improvement
Table 1: Error rates of full-precision and ternary ResNets on Cifar-10
We further train and evaluate our model on ILSVRC12 ). ILSVRC12 is a
1000-category dataset with over 1.2 million images in training set and 50 thousand images in validation
set. Images from ILSVRC12 also have various resolutions. We used a variant of AlexNet ) structure by removing dropout layers and add batch normalization for all models in our experiments. The same variant is also used in experiments described in
the paper of DoReFa-Net.
Our ternary model of AlexNet uses full precision weights for the ﬁrst convolution layer and the last
fully-connected layer. Other layer parameters are all quantized to ternary values. We train our model
on ImageNet from scratch using an Adam optimizer ). Minibatch size is set to
128. Learning rate starts at 10−4 and is scaled by 0.2 at epoch 56 and 64. A L2-normalized weight
decay of 5 × 10−6 is used as a regularizer. Images are ﬁrst resized to 256 × 256 then randomly
cropped to 224 × 224 before input. We report both top 1 and top 5 error rate on validation set.
We compare our model to a full precision baseline, 1-32-32 DoReFa-Net and TWN. After around
64 epochs, validation error of our model dropped signiﬁcantly compared to other low-bit networks
as well as the full precision baseline. Finally our model reaches top 1 error rate of 42.5%, while
DoReFa-Net gets 46.1% and TWN gets 45.5%. Furthermore, our model still outperforms full
precision AlexNet (the batch normalization version, 44.1% according to paper of DoReFa-Net) by
1.6%, and is even better than the best AlexNet results reported (42.8%1). The complete results are
listed in Table 2.
Full precision
Table 2: Top1 and Top5 error rate of AlexNet on ImageNet
1 
Published as a conference paper at ICLR 2017
DoReFa-Net
Validation
Full precision (with Dropout)
Figure 4: Training and validation accuracy of AlexNet on ImageNet
We draw the process of training in Figure 4, the baseline results of AlexNet are marked with dashed
lines. Our ternary model effectively reduces the gap between training and validation performance,
which appears to be quite great for DoReFa-Net and TWN. This indicates that adopting trainable W p
l helps prevent models from overﬁtting to the training set.
We also report the results of our methods on ResNet-18B in Table 3. The full-precision error rates are
obtained from Facebook’s implementation. Here we cite Binarized Weight Network(BWN)Rastegari
et al. results with all layers quantized and TWN ﬁnetuned based on a full precision network,
while we train our TTQ model from scratch. Compared with BWN and TWN, our method obtains a
substantial improvement.
Full precision
Table 3: Top1 and Top5 error rate of ResNet-18 on ImageNet
DISCUSSION
In this section we analyze performance of our model with regard to weight compression and inference
speeding up. These two goals are achieved through reducing bit precision and introducing sparsity.
We also visualize convolution kernels in quantized convolution layers to ﬁnd that basic patterns of
edge/corner detectors are also well learned from scratch even precision is low.
SPATIAL AND ENERGY EFFICIENCY
We save storage for models by 16× by using ternary weights. Although switching from a binaryweight network to a ternary-weight network increases bits per weight, it brings sparsity to the weights,
which gives potential to skip the computation on zero weights and achieve higher energy efﬁciency.
TRADE-OFF BETWEEN SPARSITY AND ACCURACY
Figure 5 shows the relationship between sparsity and accuracy. As the sparsity of weights grows
from 0 (a pure binary-weight network) to 0.5 (a ternary network with 50% zeros), both the training
and validation error decrease. Increasing sparsity beyond 50% reduces the model capacity too far,
increasing error. Minimum error occurs with sparsity between 30% and 50%.
We introduce only one hyper-parameter to reduce search space. This hyper-parameter can be
either sparsity, or the threshold t w.r.t the max value in Equation 6. We ﬁnd that using threshold
produces better results. This is because ﬁxing the threshold allows the sparsity of each layer to vary
(Figure refﬁg:weights).
Published as a conference paper at ICLR 2017
Error Rate
Sparsity: percentage of zero weights
w/o pruning 10%
Full Precision
Validation Error
Train Error
Figure 5: Accuracy v.s. Sparsity on ResNet-20
SPARSITY AND EFFICIENCY OF ALEXNET
We further analyze parameters from our AlexNet model. We calculate layer-wise density (complement
of sparsity) as shown in Table 4. Despite we use different W p
l for each layer, ternary weights
can be pre-computed when fetched from memory, thus multiplications during convolution and inner
product process are still saved. Compared to Deep Compression, we accelerate inference speed using
ternary values and more importantly, we reduce energy consumption of inference by saving memory
references and multiplications, while achieving higher accuracy.
We notice that without all quantized layers sharing the same t for Equation 9, our model achieves
considerable sparsity in convolution layers where the majority of computations takes place. Therefore
we are able to squeeze forward time to less than 30% of full precision networks.
As for spatial compression, by substituting 32-bit weights with 2-bit ternary weights, our model is
approximately 16× smaller than original 32-bit AlexNet.
KERNEL VISUALIZATION
We visualize quantized convolution kernels in Figure 6. The left matrix is kernels from the second
convolution layer (5 × 5) and the right one is from the third (3 × 3). We pick ﬁrst 10 input channels
and ﬁrst 10 output channels to display for each layer. Grey, black and white color represent zero,
negative and positive weights respectively.
We observe similar ﬁlter patterns as full precision AlexNet. Edge and corner detectors of various
directions can be found among listed kernels. While these patterns are important for convolution
neural networks, the precision of each weight is not. Ternary value ﬁlters are capable enough
extracting key features after a full precision ﬁrst convolution layer while saving unnecessary storage.
Furthermore, we ﬁnd that there are a number of empty ﬁlters (all zeros) or ﬁlters with single non-zero
value in convolution layers. More aggressive pruning can be applied to prune away these redundant
kernels to further compress and speed up our model.
Full precision
Pruning (NIPS’15)
conv total
Table 4: Alexnet layer-wise sparsity
Published as a conference paper at ICLR 2017
Figure 6: Visualization of kernels from Ternary AlexNet trained from Imagenet.
CONCLUSION
We introduce a novel neural network quantization method that compresses network weights to ternary
values. We introduce two trained scaling coefﬁcients W l
n for each layer and train these
coefﬁcients using back-propagation. During training, the gradients are back-propagated both to the
latent full-resolution weights and to the scaling coefﬁcients. We use layer-wise thresholds that are
proportional to the maximum absolute values to quantize the weights. When deploying the ternary
network, only the ternary weights and scaling coefﬁcients are needed, which reducing parameter size
by at least 16×. Experiments show that our model reaches or even surpasses the accuracy of full
precision models on both CIFAR-10 and ImageNet dataset. On ImageNet we exceed the accuracy of
prior ternary networks (TWN) by 3%.
Published as a conference paper at ICLR 2017