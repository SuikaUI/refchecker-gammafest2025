SHINE: Signed Heterogeneous Information Network Embedding
for Sentiment Link Prediction
Hongwei Wang∗
Shanghai Jiao Tong University
Shanghai, China
 
Fuzheng Zhang
Microsoft Research Asia
Beijing, China
 
University of Science and Technology
of China, Hefei, Anhui, China
 
Microsoft Research Asia
Beijing, China
 
Minyi Guo†
Shanghai Jiao Tong University
Shanghai, China
 
University of Science and Technology
of China, Hefei, Anhui, China
 
In online social networks people often express attitudes towards
others, which forms massive sentiment links among users. Predicting the sign of sentiment links is a fundamental task in many areas
such as personal advertising and public opinion analysis. Previous
works mainly focus on textual sentiment classification, however,
text information can only disclose the “tip of the iceberg” about
users’ true opinions, of which the most are unobserved but implied
by other sources of information such as social relation and users’
profile. To address this problem, in this paper we investigate how
to predict possibly existing sentiment links in the presence of heterogeneous information. First, due to the lack of explicit sentiment
links in mainstream social networks, we establish a labeled heterogeneous sentiment dataset which consists of users’ sentiment
relation, social relation and profile knowledge by entity-level sentiment extraction method. Then we propose a novel and flexible
end-to-end Signed Heterogeneous Information Network Embedding
(SHINE) framework to extract users’ latent representations from
heterogeneous networks and predict the sign of unobserved sentiment links. SHINE utilizes multiple deep autoencoders to map
each user into a low-dimension feature space while preserving the
network structure. We demonstrate the superiority of SHINE over
state-of-the-art baselines on link prediction and node recommendation in two real-world datasets. The experimental results also
prove the efficacy of SHINE in cold start scenario.
ACM Reference Format:
Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, and Qi
Liu. 2018. SHINE: Signed Heterogeneous Information Network Embedding
for Sentiment Link Prediction. In Proceedings of the 11th ACM International
Conference on Web Search and Data Mining (WSDM’18). ACM, New York,
NY, USA, 9 pages. 
INTRODUCTION
The past decade has witnessed the proliferation of online social
networks such as Facebook, Twitter and Weibo. In these social
network sites, people often share feelings and express attitudes
towards others, e.g., friends, movie stars or politicians, which forms
∗This work is done while H. Wang and M. Hou are visiting Microsoft Research Asia.
†M. Guo is the corresponding author.
WSDM’18, February 5–9, 2018, Marina Del Rey, CA, USA
2018. ACM ISBN 978-1-4503-5581-0/18/02...$15.00
 
sentiment links among these users. Different from explicit social
links indicating friend or follow relationship, sentiment links are
implied by the semantic content posted by users, and involve different types: positive sentiment links express like, trust or support
attitudes, while negative sentiment links signify dislike or disapproval of others. For example, a tweet saying “Vote Trump!” shows
a positive sentiment link from the poster to Donald Trump, and
“Trump is mad...” indicates the opposite case.
For a given sentiment link, we define its sign to be positive
or negative depending on whether its related content expresses
a positive or negative attitude from the generator of the link to
the recipient , and all such sentiment links form a new network topology called sentiment network. Previous work 
mainly focuses on sentiment classification based on the concrete
content posted by users. However, they cannot detect the existence
of sentiment links without any prior content information, which
greatly limits the number of possible sentiment links that could be
found. For example, if a user does not post any word concerning
Trump, it is impossible for traditional sentiment classifiers to extract the user’s attitude towards him because “one cannot make
bricks without straw”. Therefore, a fundamental question is, can
we predict the sign of a given sentiment link without observing its
related content? The solution to this problem will benefit a great
many online services such as personalized advertising, new friends
recommendation, public opinion analysis, opinion polls, etc.
Despite the great importance, there is little prior work concerning predicting the sign of sentiment links among users in social
networks. The challenges are two-fold. On the one hand, lack of
explicit sentiment labels makes it difficult to determine the polarity
of existing and potential sentiment links. On the other hand, the
complexity of sentiment generation and the sparsity of sentiment
links make it hard for algorithms to achieve desirable performance.
Recently, several studies propose methods to solve
the problem of predicting signed links. However, they rely heavily
on manually designed features and cannot work well in real-world
scenarios. Another promising approach called network embedding
 , which automatically learns features of users in network, seems plausible to solve the task. However, they can only
apply to networks with positive-weighted (i.e., unsigned) and singletype (i.e., homogeneous) edges, which limits their power in the task
of practical sentiment link prediction.
 
California
Donald Trump
Politician
Emma Watson
Albert Einstein
positive sentiment link
negative sentiment link
social link
Fig. 1: Illustration of a snippet of heterogeneous networks
with sentiment, social relationship and user profile.
Based on the above facts, in this paper we investigate the problem of predicting sentiment links in absence of sentiment related
content in online social networks. Our work is two-step. First, considering the lack of labeled data, we establish a labeled sentiment
dataset from Weibo, one of the most popular social network sites in
China. We leverage state-of-the-art entity-level sentiment extraction method to calculate the sentiment of the poster towards the
celebrity in each tweet. Besides, to handle the sparsity problem, we
collect two additional types of side information: social relationship
among users and profile knowledge of users and celebrities. Our
choices are enlightened by and , respectively, in which
 demonstrates that the structural information of social networks can greatly affect users’ preference towards online items,
and proves that information from knowledge base could boost
the performance of recommendation. The heterogeneous information networks are illustrated in Fig. 1.
To explore more possible sentiment links from the network, in
the second step, we propose a novel end-to-end framework termed
as Signed Heterogeneous Information Network Embedding (SHINE).
Greatly different from existing network embedding approaches,
SHINE is able to learn user representation and predict sentiment
from signed heterogeneous networks. Specifically, SHINE adopts
multiple deep autoencoders , a type of deep-learning-based
embedding technique, to extract users’ highly nonlinear representations from the sentiment network, social network and profile
network, respectively. The learned three types of user representations are subsequently fused together by specific aggregation
function for further sentiment prediction. In addition to the adaptability to signed heterogeneous networks, the superiority of SHINE
also lies in its end-to-end prediction technology and high flexibility of adding or removing modules of side information (i.e., social
relationship and profile knowledge), which is discuss in Section 5.
We conduct extensive experiments on two real-world datasets.
The results show that SHINE achieves substantial gains compared
with baselines. Specifically, SHINE outperforms other strong baselines by 8.8% to 16.8% in the task of link prediction on Accuracy,
and by 17.2% to 219.4% in the task of node recommendation on
Recall@100 for positive nodes. The results also prove that SHINE
is able to utilize the side information efficiently, and maintains a
decent performance in cold start scenario.
RELATED WORK
Signed Link Prediction
Our problem of predicting positive and negative sentiment links
connects to a large body of work on signed social networks, including trust propagation , spectral analysis , and social media mining . For the link prediction problem in signed graphs,
Leskovec. et al. adopt signed triads as features for prediction
based on structural balance theory. Ye et al. utilize transfer
learning to leverage edge sign information from source network
and improve prediction accuracy in target network. Tang et al. design NeLP framework which exploits positive links in social
media to predict negative links. The difference between the above
work and ours is that we construct a labeled dataset by entity-level
sentiment extraction method, as there is no explicit signed links in
mainstream online social networks. Besides, we use state-of-the-art
deep learning approach to learn the representation of links.
Network Embedding
There is a long history of work on network embedding. Earlier
works such as IsoMap and Laplacian Eigenmap first construct the affinity graph of data using the feature vectors and then
embed the affinity graph into a low-dimension space. Recently,
DeepWalk deploys random walk to learning representations
of social network. LINE proposes objective functions that preserve both local and global network structures for network embedding. Node2vec designs a biased random walk procedure to learn
a mapping of nodes that maximizes the likelihood of preserving
network neighborhoods of nodes. SDNE uses autoencoder to
capture first-order and second-order network structures and learn
user representation. However, these methods can only address unsigned and homogeneous networks. Additionally, several studies
focus on representation learning in the scenario of heterogeneous
network , attributed network , or signed network .
However, these methods are specialized in only one particular type
of networks, which is not applicable to the problem of sentiment
prediction in real-world signed and heterogeneous networks.
DATASET ESTABLISHMENT
In this section we introduce the process of collecting data from
online social networks, and discuss the details of how to extract
sentiment towards celebrities from tweets.
Data Collection
Weibo Tweets. We select Weibo1 as the online social networks studied in this work. Weibo is one of the most popular social
network sites in China which is akin to a hybrid of Facebook and
Twitter. We collected 2.99 billion tweets on Weibo from August 14,
2009 to May 23, 2014 as raw dataset. To filter out useful data which
contains sentiment towards celebrities, we first apply Jieba2, the
most popular Chinese text segmentation tool, to tag the part of
speech (POS) of each word for each tweet. Then we select those
tweets containing words with POS tagging as “person name” which
exist in our established celebrity database (detailed in Section 3.1.4).
1 
2 
Table 1: Statistics of Weibo sentiment datasets. “celebrities
v.” means the celebrities owning verified accounts on Weibo.
# social links
# celebrities
# celebrities v.
# pos. tweets
# ordinary users
# neg. tweets
After getting the set of candidate tweets, for each tweet we calculate
its sentiment value (-1 to +1) towards the mentioned celebrities,
and select those tweets with high absolute sentiment values. The
final dataset consists of a set of triples (a,b,s), where a is the user
who posts the tweet, b is the certain celebrity mentioned in the
tweet, and s ∈{+1, −1} is the sentiment polarity of user a towards
user b. The method of calculating sentiment values is detailed in
Section 3.2.
Social Relation. In addition to the sentiment dataset, we
also collect the social relation among users from Weibo. The dataset
of social relation consists of tuples (a,b), where a is the follower
and b is the followee.
Profile of Ordinary Users. The profile of ordinary users
are collected from Weibo. For each ordinary user, we extract two of
his attributes, gender and location, as his profile information. The
attribute values are represented as one-hot vectors.
Profile of Celebrities. We use Microsoft Satori3 knowledge
base to extract profile of celebrities. First, we traverse the knowledge
base and select terms with object type as “person”. Then we filter
out popular celebrities with high edit frequency in knowledge base
and high appearance frequency in Weibo tweets. For each of these
“hot” celebrities, we extract 9 attributes as his profile information:
place of birth, date of birth, ethnicity, nationality, specialization, gender, height, weight, and astrological sign. Values of these attributes
are discretized so that every celebrity’s attribute values can be expressed as one-hot vectors. Furthermore, we remove celebrities
with ambiguous names as well as other noises.
Sentiment Extraction
To extract users’ sentiment towards celebrities in tweets, we first
generate a sentiment lexicon consisting of words and their sentiment
orientation (SO) scores. To achieve this, we manually construct a
emoticon-sentiment mapping file and map each tweet to positive
or negative class according to the label of emoticon appeared in
the tweet. For example, “I love Kobe! [kiss]” is mapped to positive
class if the key-value pair ([kiss], positive) exists in the emoticonsentiment mapping file. Note that the class of emoticon cannot
be directly regarded as the sentiment towards celebrities since we
found a large number of mismatch cases, e.g., “Miss you Taylor Swift
[cry][cry]”. Afterwards, for each word (segmented by Jieba) with
occurrence frequency from 2,000 to 10,000,000 in the raw tweets
datasets, similar to , we calculate its SO score as
SO(word) = PMI(word,pos) −PMI(word,neд),
where PMI is the point-wise mutual information defined as
PMI(x,y) = log
p(x)p(y), pos and neд are the tweets of positive and
3 
(a) Sentiment network
(b) Social network
nationality
specialization
(c) Profile network
Fig. 2: Illustration of the three studied networks.
negative class, respectively. SO scores are subsequently normalized
to [−1, 1].
After getting the lexicon, we use SentiCircle to calculate
sentiment towards celebrities in each tweet. Given a piece of tweet
as well as the mentioned celebrity, we represent the contextual
semantics of the celebrity as a polar coordinate space, where the
celebrity is situated in the origin and other terms in the tweet are
scattered around. Specifically, for celebrity term c, the coordinate
of term ti is (ri,θi), where ri is the inverse of distance between
c and ti in syntax dependence graph generated by LTP , and
θi = SO(ti) · π. The overall sentiment towards the celebrity c is,
therefore, approximated as the geometric center of all terms ci.
We take the projection of the geometric center on y-axis as final
sentiment value towards the celebrity.
To validate the effectiveness of sentiment extraction, we randomly select 1,000 tweets (500 positive and 500 negative tagged
by our method) in Weibo sentiment dataset, and manually label
each one of them. The result shows that the precision is 95.2% for
positive class and 91.0% for negative class, which we believe is
accurate enough for subsequent experiments. The basic statistics
of Weibo sentiment datasets is presented in Table 1.
PROBLEM FORMULATION
In this section we formulate the problem of predicting sentiment
links in heterogeneous information networks. For better illustration,
we split the original heterogeneous network into the following three
single-type networks:
Sentiment network. The directed sentiment network is denoted
as Gs = (V,S), where V = {1, ..., |V |} represents the set of users
(either ordinary users or celebrities) and S = {sij | i ∈V, j ∈V }
represents sentiment links among users. Each sij can take the value
of +1, −1 or 0, representing that user i holds a positive, negative,
or unobserved sentiment towards user j, respectively.
Social network. The directed social network is denoted as Gr =
(V,R), where R = {rij | i ∈V, j ∈V } represents social links among
users. Each rij can take the value of 1 or 0, representing that user i
follows user j or not in the social network.
Profile network. We denote A = {A1, ...,A|A |} the set of
user’s attributes, and akl ∈Ak the l-th possible value of attribute
Ak. We take the union of all possible values of attributes and renumber them as U = ÐAk = {aj | j = 1, ..., Í
k |Ak |}. Then the undirected bipartite profile network can be denoted as Gp = (V,U , P),
where P = {pij | i ∈V,aj ∈U } represents profile links between
users and attribute values. Each pij can take the value of 1 or 0,
representing that user i possesses attribute value j or not.
“Vote Trump!”
sentiment embedding
sentiment autoencoder
sentiment embedding
social embedding
social autoencoder
profile embedding
profile autoencoder
heterogeneous
heterogeneous
aggregation
aggregation
social embedding
profile embedding
extraction
sentiment link
sentiment network
profile network
social network
Fig. 3: Framework of the end-to-end SHINE model. To clearly demonstrate the model, we only show the encoder part of all
the three autoencoders and leave out the decoder part in this figure.
The three networks are illustrated in Fig. 2.
Sentiment links prediction. We define the problem of predicting sentiment links in heterogeneous information networks as
follows: Given the sentiment network Gs, social network Gr and
profile network Gp, we aim to predict the sentiment of unobserved
links between users in Gs.
SIGNED HETEROGENEOUS INFORMATION
NETWORK EMBEDDING
In this section we introduce the proposed SHINE model. We first
show the whole framework of SHINE. Then we present the details
of the SHINE model, including how to extract user representation
jointly from the three networks as well as the learning algorithm.
At last we give some discussions on the model.
In this paper we propose an end-to-end SHINE model to predict
sentiment links. The framework of SHINE is shown in Fig. 3. In
general, the whole framework consists of three major components:
sentiment extraction and heterogeneous networks construction
(the left part), user representation extraction (the middle part), as
well as representation aggregation and sentiment prediction (the
right part). For each tweet mentioning a specific celebrity, we first
calculate the associated sentiment (discussed in Section 3.1), and
represent the user and the celebrity in this sentiment link by using their neighborhood information from the three constructed
networks (introduced in Section 4). We then design three distinct
autoencoders to extract short and dense embeddings from original sparse neighborhood-based representation respectively, and
aggregate these three kinds of embeddings into final heterogeneous
embedding. The predicted sentiment can thus be calculated by applying specific similarity measurement function (e.g., inner product
or logistic regression) to the two heterogeneous embeddings, and
the whole model can be trained based on the predicted sentiment
and the target (i.e., the ground truth obtained in sentiment extraction step). In the following subsections we will introduce SHINE
model in detail.
Sentiment Network Embedding
Given the sentiment graph Gs = (V,S), for each user i ∈V , we
define its sentiment adjacency vector xi = {sij | j ∈V } ∪{sji | j ∈
V }. Note that xi fully contains the global incoming and outgoing
sentiment information of user i. However, it is impractical to take xi
directly as the sentiment representation of user i, as the adjacency
vector is too long and sparse for further processing. Recently, a lot of
network embedding models are proposed, which aim
to learn low-dimension representations of vertices while preserving
the network structure. Among those models, deep autoencoder is
proved to be one of state-of-the-art solutions, as it is able to capture
highly nonlinear network structure by using deep models .
In general, autoencoder is an unsupervised neural network
model of codings aiming to learn a representation of a set of data.
Autoencoder consists of two parts, the encoder and the decoder,
which contains multiple nonlinear functions (layers) for mapping
the input data to representation space and reconstructing original
input from representation, respectively. In our SHINE model, we
propose to use autoencoders for efficiently user representation
Fig. 4 illustrates the autoencoder for sentiment network embedding. As shown in Fig. 4, the sentiment autoencoder maps each
user to a low-dimension latent representation space and recover
original information from latent representation by using multiple
fully-connected layers. Given the input xi, the hidden representations for each layer are
, k = 1, 2, ...,Ks,
where Wks and bks are weight and bias parameters of layer k in the
sentiment autoencoder, respectively, σ(·) is the nonlinear activation
function, Ks is the number of layers of sentiment autoencoder, and
i = xi. For simplicity, we denote x′
the reconstruction of
The basic goal of the autoencoder is to minimize the reconstruction loss between input and output representations. Similar to ,
in SHINE model the reconstruction loss term of sentiment autoencoder is defined as
hidden layers
hidden layers
reconstructed
adjacency vector
Fig. 4: Illustration of a 6-layer autoencoder for sentiment
network embedding.
where ⊙denotes the Hadamard product, and li = (li,1,li,2, ...,li,2|V |)
is the sentiment reconstruction weight vector in which
if sij = ±1;
if sij = 0.
The meaning of the above loss term lies in that we impose more
penalty to the reconstruction error of the non-zero elements than
that of zero elements in input xi, as a non-zero sij carries more
explicit sentiment information than an implicit zero sij. Note that
the sentiment embedding of user i can be obtained from the layer
Ks/2 in the sentiment autoencoder, and we denote bxi = xKs/2
sentiment embedding of user i for simplicity.
Social Network Embedding
Similar to previous sentiment network embedding, we apply autoencoder to extract user representation from the social network.
Given the social network Gr = (V,R), for each user i ∈V , we
define its social adjacency vector yi = {rij | j ∈V } ∪{rji | j ∈V },
which fully contains the structural information of user i in the
social network. The hidden representations of each layer in the
social autoencoder are
, k = 1, 2, ...,Kr ,
where the meaning of notations are similar to those in Eq. (2).
We also denote y′
the reconstruction of yi. Similarly, the
reconstruction loss term of social autoencoder is
where mi = (mi,1,mi,2, ...,mi,2|V |) is the social reconstruction
weight vector in which if rij = 1, mi,j = α > 1, else mi,j = 1. The
social embedding of user i is denoted as byi = yKr /2
Profile Network Embedding
The profile network Gp = (V,U, P) is an undirected bipartite graph
which consists of two disjoint sets of users and attribute values.
For each user i ∈V , its profile adjacency vector is defined as
zi = {pij | j ∈U }. User i’s hidden representations of each layer in
the profile autoencoder are
, k = 1, 2, ...,Kp,
where the meaning of notations are similar to those in Eq. (2). We
also use the notation z′
i to denote the reconstruction of zi. Therefore,
the reconstruction loss term of profile autoencoder is
where ni is the profile reconstruction weight vector defined similarly to mi in the previous subsection. The profile embedding of
user i is denoted asbzi = zKp/2
Representation Aggregation and Sentiment
Prediction
Once we obtain the sentiment embedding bxi, social embedding byi,
and profile embeddingbzi of user i, we can aggregate these embeddings into final heterogeneous embedding ei by specific aggregation
functionд(·, ·, ·). We list some of the available aggregation functions
as follows:
• Summation , i.e., ei = bxi + byi +bzi;
• Max pooling , i.e., ei = element-wise-max(bxi,byi,bzi);
• Concatenation , i.e., ei = ⟨bxi,byi,bzi⟩.
Finally, given two users i and j as well as their heterogeneous
embedding ei and ej, the predicted sentiment ¯sij can be calculated
as ¯sij = f (i, j), where f (·, ·) is specific similarity measurement
function. For example:
• Inner product , i.e., ¯sij = eT
i ej + b, where b is a trainable
bias parameter;
• Euclidean distance , i.e., ¯sij = −∥ei −ej ∥2 +b, where b is a
trainable bias parameter;
• Logistic regression , i.e., ¯sij = WT⟨ei, ej⟩+b, where W and
b are trainable weights and bias parameters.
We will study the choices of f and д in the experimental part.
Optimization
The complete objective function of SHINE model is as follows:
 f (ei, ej) −sij
where λ1, λ2, λ3 and λ4 are balancing parameters. The first three
terms in Eq. (9) are the reconstruction loss terms of sentiment autoencoder, social autoencoder, and profile autoencoder, respectively.
The fourth term in Eq. (9) is the supervised loss term for penalizing the divergence between predicted sentiment and ground truth.
The last term in Eq. (9) is the regularization term that prevents
over-fitting, i.e.,
where Wks , Wkr , Wkp are the weight parameters of layer k in the
sentiment autoencoder, social autoencoder, and profile autoencoder,
respectively, and ∥f ∥2
2 is the regularization penalty for similarity
measurement function f (·, ·) (if appropriate).
We employ the AdaGrad algorithm to minimize the objective
functions in Eq. (9). In each iteration, we randomly select a batch
of sentiment links from training dataset and compute the gradient
of the objective function with respect to each trainable parameter
respectively. Then we update each trainable parameter according
to the AdaGrad algorithm till convergence.
Discussions
Asymmetry. Many real-world networks are directed, which
implies that for two nodes i and j in the network, edges (i, j) and
(j,i) may coexist and their values are not necessarily identical. A
few recent studies have focused on this asymmetry issue . In
this work, whether the basic SHINE model can characterize asymmetry depends on the choice of similarity measurement function f .
Specifically, SHINE is capable of dealing with the direction of a link
if and only if f (i, j) , f (j,i) (e.g., logistic regression). However (and
fortunately), even if we choose a symmetric function (e.g., inner
product or Euclidean distance) as f , we can still easily extend the
basic SHINE model to asymmetry-aware version by setting two
distinct sets of autoencoders to extract representation of source
node and target node respectively. From this point of view, in basic
SHINE model the parameters of autoencoders are actually shared
for source node and target node to alleviate over-fitting, and we
can choose to explicitly distinguish the two sets of autoencoders
for asymmetry reasons.
Cold start problem. A practical issue for network embedding is how to learn representations for newly arrived node, which
is the cold start problem. Almost all existing models cannot work
well in cold start scenario because they only use the information
from the target network (e.g., sentiment network in this paper),
which is not applicable for the newly arrived node who has little interaction with the existing target network. However, SHINE is free
of the cold start problem, as it makes full use of side information
and incorporate it naturally into the target network when learning
user representations. We will further study the performance of
SHINE in cold start scenario in the experiment part.
Flexibility. It is worth noticing that SHINE is also a framework with high flexibility. For any other new available side information of users (e.g., users’ browsing history), we can easily design
a new parallel processing component and “plug” it in the original
SHINE framework to assist learning representation. Contrarily, we
can also “pull out” social autoencoder or profile autoencoder from
SHINE framework if such side information is unavailable. Besides,
the flexibility of SHINE also lies in that one can choose different
aggregation functions д and similarity measurement functions f ,
as discussed in Section 5.5.
EXPERIMENTS
In this section, we evaluate the performance of our proposed SHINE
on real-world datasets. We first introduce the datasets, baselines,
and parameter settings for experiments, then present the experimental results of SHINE and baselines.
To comprehensively demonstrate the effectiveness of SHINE framework, we use the following two datasets for experiments:
• Weibo-STC: Our proposed Weibo Sentiment Towards Celebrities dataset consists of three heterogeneous networks with
12,814 users, 126,380 tweets, 71,268 social links and 37,689
profile values, of which the detail is presented in Section 3.
• Wiki-RfA: Wikipedia Requests for Adminship is a signed
network with 10,835 nodes and 159,388 edges, corresponding
to votes cast by Wikipedia uses in election for promoting
individuals to the role of administrator. A signed link indicates
a positive or negative vote by one user on the promotion
of another. Note that Wiki-RfA does not contain any side
information of nodes, therefore, this dataset is used to validate
the efficacy of the basic sentiment autoencoder in SHINE.
We use the following five methods as baselines, in which the first
three are network embedding methods, FxG is a signed link prediction approach, and LIBFM is a generic classification model. Note
that the first three methods are not directly applicable to signed
heterogeneous networks, so we use them to learn user representations from positive and negative part of each network respectively,
and concatenate them to form the final embeddings. For FxG on
Weibo-STC dataset, we only use the sentiment network as input
because the FxG model cannot utilize the side information of nodes.
• LINE: Large-scale Information Network Embedding defines loss functions to preserve the first-order and secondorder proximity and learns representations of vertices.
• Node2vec: Node2vec designs a biased random walk procedure to learn a mapping of nodes that maximizes the likelihood
of preserving network neighborhoods of nodes.
• SDNE: Structural Deep Network Embedding is a semisupervised network embedding model using autoencoder to
capture local and global structure of target networks.
• FxG: Fairness and Goodness predicts the weights of edges
in weighted signed networks by introducing two measures of
node behavior: goodness (i.e., how much the node is liked by
other nodes) and fairness (i.e., how fair the node is in rating
other nodes’ likeability).
• LIBFM: LIBFM is a state-of-the-art feature based factorization model. In this paper, we use the concatenated one-hot
vectors of users in three networks as input to feed LIBFM.
Parameter Setttings
We design a 4-layer autoencoder in SHINE for each network, in
which the hidden layer is with 1,000 units and the embedding layer
is with 100 units. Deeper architectures cannot further improve the
performance but incur heavier computational overhead according
to our experimental results. We choose concatenation as the aggregation function д and inner product as the similarity measurement
function f . Besides, we set the reconstruction weight of non-zero
elements α = 10, the balancing parameters λ1 = 1, λ2 = 1, λ3 = 20,
and λ4 = 0.01 for SHINE. We will study the sensitivity of these
parameters in Section 6.6. For LINE, we concatenate the first-order
and second-order representations to form the final 100-dimension
embeddings for each node, and the total number of samples is 100
million. For node2vec, the number of embedding dimension is set
as 100. For SDNE, the reconstruction weight of non-zero elements
is 10 and the weight of first-order term is 0.05. For LIBFM, the
dimensionality of the factorization machine is set as {1, 1, 0} and
Percentage of training set
(a) Accuracy on Weibo-STC
Percentage of training set
(b) Micro-F1 on Weibo-STC
Percentage of training set
(c) Accuracy on Wiki-RfA
Percentage of training set
(d) Micro-F1 on Wiki-RfA
Fig. 5: Accuracy and micro-F1 on Weibo-STC and Wiki-RfA for link prediction.
we use SGD method for training with learning rate of 0.5 and 200
iterations. Other parameters in these baselines are set as default.
In the following subsections, we conduct experiments on two
tasks: link prediction and node recommendation.
Link Prediction
In link prediction setting, our task is to predict the sign of an unobserved link between two given nodes. As the existing links in the
original network are known and can serve as the ground truth, we
randomly hide 20% of links in the sentiment network and select a
balanced test set (i.e., the number of positive links is the same as
negative links) out of them, while use the remaining network to
train SHINE as well as all baselines. We use Accuracy and Micro-F1
as the evaluation metrics in link prediction task. For a more finegrained analysis, we compare the performance while varying the
percentage of training set from 10% to 100%. The result is presented
in Fig. 5, from which we have the following observations:
• Fig. 5 shows that our methods SHINE achieves significant improvements in Accuracy and Micro-F1 over the baselines in
both datasets. Specifically, in Weibo-STC, SHINE outperforms
LINE, node2vec, and SDNE by 13.8%, 16.2%, and 8.78% respectively on Accuracy, and achieves 15.5%, 17.6%, 9.71% gains
respectively on Micro-F1.
• Among the three state-of-the-art network embedding methods,
SDNE performs best while LINE and node2vec show relatively
poor performance. Note that SDNE also uses autoencoder to
learning the embedding of nodes, which proves the superiority
of autoencoder in extracting highly nonlinear representations
of networks from a side.
• FxG performs much better in Wiki-RfA than in Weibo-STC.
This is probably due to the following two reasons: 1) Unlike
other methods, FxG cannot utilize the side information in
Weibo-STC dataset. 2) Weibo-STC is sparser than Wiki-RfA,
which is unfavorable to the computing of goodness and fairness of nodes in FxG model.
• Although LIBFM is not specially designed for network-structured
data, it still achieves fine performance compared with other
network embedding methods. However, during experiments
we find that LIBFM is unstable and prone to parameters tuning.
This can also be validated by the fluctuating curves of LIBFM
in Fig. 5c and Fig. 5d.
To compare the performance of SHINE and baselines in cold
start scenario, we construct a test set of newly arrived users for
Table 2: Comparison of models in terms of Accuracy and
Micro-F1 on Weibo-STC in cold start scenario.
Weibo-STC, in which the associated ordinary user of each sentiment link dose not appear in the training set. We report Accuracy
and Micro-F1 for all users and new users in Table 2. From the results
in Table 2 it is evident that SHINE can still maintain a decent performance in the cold start scenario, as it fully exploits the information
from social network and profile network to compensate for the
lack of sentiment links. By comparison, the performance of other
baselines degrades significantly in cold start scenario. Specifically,
the Accuracy decreases by 2.46% for SHINE and by 11.58%, 11.28%,
15.14%, 17.90%, 14.57% respectively for LINE, node2vec, SDNE, FxG
and LIBFM, which proves that SHINE are more capable of effectively transferring knowledge among heterogeneous information
networks, especially in cold start scenario.
Node Recommendation
In addition to link prediction, we also conduct experiments on node
recommendation, in which for each user we aim to recommend a set
of users who have not been explicitly expressed attitude to but may
be liked by the user. The performance of node recommendation can
reveal the quality of learned representations as well. Specifically, for
each user, we calculate his sentiment score toward all other users,
and select K users with largest sentiment score for recommendation.
For completeness, we recommend not only the nodes that a user
may like but also the nodes that he may dislike. Therefore, we use
positive and negative Precision@K and Recall@K respectively for
evaluation in corresponding experimental scenarios. The results
are shown in Fig. 6, which provides us the following observations:
• The curve of SHINE is almost consistently above the curves
of baselines, which proves that SHINE can better learn the
representations of heterogeneous networks and perform recommendation than baselines.
pos. Precision@K
(a) pos. Precision@K on Weibo-STC
neg. Precision@K
(b) neg. Precision@K on Weibo-STC
pos. Recall@K
(c) pos. Recall@K on Weibo-STC
neg. Recall@K
(d) neg. Recall@K on Weibo-STC
pos. Precision@K
(e) pos. Precision@K on Wiki-RfA
neg. Precision@K
(f) neg. Precision@K on Wiki-RfA
pos. Recall@K
(g) pos. Recall@K on Wiki-RfA
neg. Recall@K
(h) neg. Recall@K on Wiki-RfA
Fig. 6: Positive and negative Precision@K and Recall@K on Weibo-STC and Wiki-RfA for node recommendation.
Table 3: Accuracy on Weibo-STC w.r.t. the combinations of
similarity measurement function and aggregate function.
Max pooling
Concatenation
Inner product
Euclidean distance
Logistic regression
• Negative precision is low than positive precision while negative recall is higher than positive recall for most methods. This
is because negative links are far fewer than positive links in
both datasets, which makes it easier to cover more negative
links in the recommendation set.
• In general, the results of precision and recall on Weibo-STC is
better than Wiki-RfA, which is in accordance with the results
in link prediction. The reason lies in that Weibo-STC provides
more side information which can greatly improve the quality
of learned user representations.
Parameters Sensitivity
SHINE involves a number of hyper-parameters. In this subsection
we examine how the different choices of parameters affect the
Accuracy of SHINE on Weibo-STC dataset. Except for the parameter
being tested, all other parameters are set as default.
Similarity measurement function f and aggregation function д. We first investigate how the similarity measurement function f and aggregation function д affect the performance by testing
on all combinations of f and д, and present the results in Table 3.
It is clear that the combination of inner product and concatenation
achieves the best Accuracy, while max pooling performs worst,
which is probably due to the reason that concatenation preserves
more information out of the three types of embeddings than summation and max pooling during embedding aggregation. It should
also be noted that there is no absolute advantage of all the three f
functions according to the results in Table 3.
Dimension of embedding layer and reconstruction weight
of non-zero elements α. We also show how the dimension of
embedding layer in the three autoencoders of SHINE and the hyperparameterα affect the performance in Fig. 7a. We have the following
two observations: 1) The performance is initially improved with
the increase of dimension, because more bits in embedding layer
can encode more useful information. However, the performance
drops when the dimension further increases, as too large number
of dimensions may introduce noises which mislead the subsequent
prediction. 2) α controls the reconstruction weight of non-zero
elements in autoencoders. When α is too small (e.g., α = 1), SHINE
will reconstruct the zero and non-zero elements without much
discrimination, which deteriorates the performance because nonzero elements are more informative than zero ones. However, the
performance will decrease if α gets too large (e.g., α = 30), because
large α will lead SHINE to totally ignore the dissimilarity (i.e., zero
elements) among users.
Balancing parameters λ1, λ2, and λ3. λ1, λ2, and λ3 balance
the loss terms of the objective function in Eq. (9). We treat λ1 and
λ2 as binary parameters and vary the value of λ3 to study the performance of SHINE. Note that whether λ1 or λ2 equals 1 indicates
that whether we use the additional social information or profile
information in link prediction. Therefore, the study of λ1 and λ2
can also be seen as to validate the effectiveness of social network
embedding module and profile network embedding module. The
result is presented in Fig. 7b, from which we can conclude that:
1) The curve of λ1 = 1, λ2 = 0 and λ1 = 0, λ2 = 1 are both above
the curve of λ1 = 0, λ2 = 0, which demonstrates the significant
gain by incorporating the social information and profile information (especially the latter) into the sentiment network. Moreover,
combining both additional information can further improve the
performance. 2) Increasing the value of λ3 can greatly boost the
accuracy, as SHINE will concentrate more on the prediction error rather than the reconstruction error. However, similar to other
hyper-parameters, too large λ3 is not satisfactory since it breaks
the trade-off among loss terms in objective function.
Dimension of embedding layer
(a) dim. of embedding layer and α
λ1=0, λ2=0
λ1=1, λ2=0
λ1=0, λ2=1
λ1=1, λ2=1
(b) λ1, λ2, and λ3
Fig. 7: Parameter sensitivity w.r.t. the dimension of embedding layers, α, λ1, λ2, and λ3.
CONCLUSIONS
In this paper we study the problem of predicting sentiment links in
absence of sentiment related content in online social networks. We
first establish a labeled, heterogeneous, and entity-level sentiment
dataset from Weibo due to the lack of explicit sentiment links. To
efficiently learn from these heterogeneous networks, we propose
Signed Heterogeneous Information Network Embedding (SHINE),
a deep-learning-based network embedding framework to extract
users’ highly nonlinear representations while preserving the structure of original networks. We conduct extensive experiments to
evaluate the performance of SHINE. Experimental results prove
the competitiveness of SHINE against several strong baselines and
demonstrate the effectiveness of usage of social relation and profile
information, especially in cold start scenario.
ACKNOWLEDGMENTS
We thank our anonymous reviewers for their feedback and suggestions. This work was partially sponsored by the National Basic
Research 973 Program of China under Grant 2015CB352403.