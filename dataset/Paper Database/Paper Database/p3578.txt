IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 4, NO.2, MARCH 1993
Recursive Dynamic Node Creation
in Multilayer Neural Networks
Mahmood R. Azimi-Sadjadi, Senior Member, IEEE, Sassan Sheedvash, and Frank O. Trujillo
configurations are tried, and if they do not yield an acceptable
solution, they are discarded. Another topology is then defined
and the whole training process is repeated. As a result, the
possible benefits of training the original network architecture
are lost and the computational costs of retraining become
prohibitive. Another approach involves using a larger than
needed topology and training it until a convergent solution
is found. After that, the weights of the network are pruned
off, if their values are negligible and have no influence on the
performance of the network . Since the pruning approach
starts with a large network, the training time is larger than
necessary and the method is computationally inefficient. It
may also get trapped
in one of the intermediately sized
solutions because of the shape of the error surface and hence
never finds the smallest network solution. Additionally, the
relative importance of the nodes and weights depend on the
particular mapping problem which the network is attempting
to approximate and the pruning method makes it difficult to
come up with a general cost function that would yield small
networks for an arbitrary mapping. In the procedure suggested
in , the error curve is monitored during the training process
and a node is created when the ratio of the drop in the mean
squared error (MSE) over a fixed number of trials falls below
a priori chosen threshold slope. This procedure then uses the
conventional, LMS-type, back-propagation algorithm to train
the new architecture.
In this paper a new recursive procedure for node creation
in multilayer back-propagation neural networks is introduced.
The derivations of the methodology are based upon the application of the Orthogonal Projection Theorem . Simulation
results on various examples are presented which indicate the
effectiveness of the node creation scheme developed in this
paper when used in conjunction with the RLS based learning
TRAINING PROCESS OF MULTILAYER NEURAL NETWORK
In this section the problem of weight updating in multilayer
neural networks is formulated in the context of the geometric
orthogonal projection , . The sum of the squared error
is viewed as the squared length (or norm) of an error vector
which is minimized using the geometric approach. It will be
shown that the solution of the time updating leads to the RLS
adaptation , , and the solution to the order updating
allows us to recursively add nodes to the hidden layers during
the training process.
Consider an M-layer network as shown in Fig. 1. This
network has No inputs, N[ hidden layer nodes in layer l, l E
1045-9227/93$03.00 © 1993 IEEE
Abstract-This paper presents the derivations of a novel approach for simultaneous recursive weight adaptation and node
creation in multilayer back-propagation neural networks. The
method uses time and order update formulations in the orthogonal projection method to derive a recursive weight updating
procedure for the training process of the neural network and
a recursive node creation algorithm for weight adjustment of a
layer with added nodes during the training process. The proposed
approach allows optimal dynamic node creation in the sense that
the mean-squared error is minimized for each new topology.
The effectiveness of the algorithm is demonstrated on several
benchmark problems, namely, the multiplexer and the decoder
problems as well as a real world application for detection and
classification of buried dielectric anomalies using a microwave
INTRODUCTION
N a supervised neural network such as the back-propagation
network the choices of the training algorithm, the network
architecture, the input signal representation, and the training
set, play dominant roles for optimal training and generalization
capability of the network.
The choice of the training algorithm determines the rate of
convergence to a solution and the optimality of the training.
The training process of the back-propagation network is based
upon the least-squares criterion. If enough training samples
and internal parameters are used, the input-output mapping
may be approximated to within an arbitrary accuracy . In
this case, the performance of the network can approach to that
of Bayes estimator which is optimal . Fast adaptation or
learning is one of the main issues in these neural networks. In
 , , a new algorithm for expediting the learning process
of multilayer neural networks is introduced using a recursive
least squares (RLS) based method.
The choice of the network architecture is another important
consideration for optimal training and generalization characteristics. It is proved that a three-layer neural network
with Sigmoidal type nonlinearity at nodes can approximate any
arbitrary nonlinear function and generate any complex decision
region needed for classification and recognition tasks. Neural
network architectures with hidden layers bottlenecks have been
shown to generalize better than networks which contain larger
hidden layer nodes than their previous layer .
However, the selection of an architecture with appropriate
size has been mainly empirical. For the most part, different
Manuscript received December 23, 1991; revised July 3, 1992.
M. R. Azimi-Sadjadi and F. O. Trujillo are with the Department of Electrical
Engineering, Colorado State University, Fort Collins, CO.
S. Sheedvash is with IBM Corp., Austin, TX 78712.
IEEE Log Number 9203736.
AZIMI-SADJADI et al.: NODE CREATION IN MULTILAYER NEURAL NETWORKS
1(1) Nodej
IJ 11•1(t)
A model for neurons (nodes) in the network.
A multilayer neural network.
where l/al determines the slope of the ramp region. Note that
similar input/output relationship can be used for the piecewise
linearized sigmoidal function. In this case 1/tu (t) should be
determined at every iteration. However, in this paper for
simplicity in derivations and without loss of generality we have
considered the threshold logic nonlinear activation function.
Now, the aim of weight updating is to find an appropriate
set of weight vectors W;/)(t) for Vl
Vj E [0, NI
- 1], so that the global weighted sum of the
squared errors between the actual output, i.e., z;M)(t) and the
desired output (externally specified),
d~M) (t), is minimized
over the entire training set, t E [1, n], and all output nodes,
i E[O,NM -IJ. This index of performance is given by
J(n) = (1/2) L L An - t [c~M\t)]
where A is a positive number less than but close to one which
is referred to as "forgetting factor"; and the error e~M)(t) for
the ith node in the output of layer M at training time t is
defined as
c~M\t) := d~M)(t)_z;M\t),
for i E [0, NM - 1]. (4b)
The index of performance in (4) can be minimized for WP) (n)
by taking the partial derivative of J(n) with respect to WP) (n)
and setting it equal to zero. It is shown that the problem of
weight adjustment using the RLS approach leads to a normal
equation for each layer. For example, for layer l we get the
following normal equation:
= °=> ~ An-t .!.. i(l-I)(t)c(l)(t) = 0,
Vlc[l, M],
Vje[O,NI - IJ
[1,M - 1] and N M output nodes. At training sample t, we
denote the inputs to the first layer by x;(t)'s, i E [0,No - 1],
the inputs to other layers, say layer l, by YJI)(t),j E [0,NI-IJ
and the outputs of this layer by ZJ/)(t). Then the total input to
node j in layer l can be expressed as
yy)(t) = L
wtJ(t) z;l-l)(t)
Vj E [0,NI - 1]' and vl E [1,M]
w;~J(t) represents the weight connecting node i,i E
[0,NI - 1 -IJ in layer l-1 to node i.i E [O,NI -IJ in layer
l, l E [1,MJ. Alternatively, in vector form we can write
yy)(t) = i(l-I)T (t) W;/)(t),
vi E [0,NI - 1]' and Vl E [1,MJ
t "",zN/-l t
where W;/) (t) is the weight vector associated with node j
in layer land Z(I -1) (t) represents the input vector to layer l
(coming from layer l- 1 ). The output of node j, zJ/)(t), has
a real value that is normally a nonlinear function of the total
input, y;n (t). If the standard threshold nonlinear activation
function, as in Fig. 2 is used, then this output is given by
c(l)(t) := d(l)(t) - i CI- 1)T(t) W.
Here "1\" represents the estimate of the relevant quantity,
and d;/) (t) is the desired output for node j in the output of
layer l which is determined through the error back-propagation
procedure as described in .
dy)(t) := zJ/)(t) + cy)(t)
where the error e;I)(t) is determined by
YJ/)(t) :S 0,
°< YJl)(t) :S al'
YJ/)(t) > al
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 4, NO.2, MARCH 1993
where E~I+1)(t) represents the error at node k in the output of
layer (l + 1) which is back-propagated through the weights of
this layer, i.e., w(lt1)(n), to generate E)(I) (t), the error at node
j in the output of previous layer l,
Note that when the total input to node j is off the ramp region of the threshold logic nonlinearity function, the derivative
in (5a) is always zero since (8zy)(t))/(8Wp)(n)) = 0, (see
[4, eqs. (15) and (16)]). This implies that the normal equation
in (5a) will only be solved when the input to the relevant node
lies within the ramp region and in this case the error is given
Since each layer can be treated independently we can, for
simplicity in notation, drop the layer index, l, and assume that
the slope of the nonlinear activation function, i.e., al = 1, for
Vi E [1, M]. The latter assumption is equivalent to normalizing
the input vector Z(l-l)(t) by dividing its elements by ai. The
normal (5a) can then be rewritten in a more compact form as
and the normal equation in (9) can be represented as
From definitions Ey(n) and fly(n) in (12) and (14), it can
readily be verified that the following properties hold for these
Let us also introduce another matrix 2y (n) as
which is the left generalized inverse of the input data matrix
J;;:N(n). Then the transpose of the optimum weight vector
Wj (n) in (10) can be represented as
forVj E [O,N -1]
where Ej (n) is the n-dimensional error vector given by
Ej(n) := 111/2(n)[Ej(1), Ej(2), ... ,Ej(n)f
1::N(n) is the input matrix defined by
1::N(n) := II
(n) Z(I), Z(2), ... , Z(n)
where IN is an identity matrix of size N x N.
TIME AND ORDER UPDATE PROCESSES
Let 1::, z...,!l, and 1:. be arbitrary matrices of the same column length, then the following orthogonal updating equations
Ey:z = E y + E~z...(z...TE~z...) -1z...TE~
Now using (5b), (7a) can alternatively be written as
1::~(n)(jjj(n) - 1::N(n)H1j(n)) = 0,
for Vj E [0, N - IJ.
This normal equation can be solved for Wj (n) to give the
standard least squares (LS) solution as
(1::~(n)1::N(n)) -11::~(n)jjj(n)
111/2(n):= Diag[.\(n-1)/2,.\(n-2)/2, ... ,.\1/2,1].
Let us also define the desired output vector jjj (n) as
which is the optimum weight vector for node j in a particular
layer. This vector yields the minimum error vector,
Ej(n) = [1- 1::N(n)(1::~(n)1::N(n))
-11::~(n)]jjj(n).
Let us introduce the following projection operators , 
Ey(n) := 1::N(n)(1::~(n)1::N(n)r11::~(n),
and E~(n) := 1 - Ey(n)
where Ey (n) is the projection matrix on the column space of
1::N(n) and E~(n) is its orthogonal complement. Thus, using
the definitions in (12) the minimum error vector in (11) can
be represented as
fly:z = [fly
0] + E~z...(z...TE~z...) -1 [_z...Tfly
where Ey :z represents the projection operator for the combined space spanned by 1:: and z...; and Y : Z denotes the
appending of vector Z to column space of 1::. Pre- and postmultiplying (I7b) by !IT and 1:., respectively, yields
!ITE~:z1:.=!ITE~1:._(!ITE~z...)(z...TE~z...r1(z...TE~1:.).
AZIMI-SADJADI et al.: NODE CREATION IN MULTILAYER NEURAL NETWORKS
Also from (17c) we have
!1?2y:z = [rLT2 y
0] + (rLTE~~)(~TE~~rl
In the next section we shall derive the "time" and "order"
update equations for the optimum weight vector in (15) using
the projection update formula.
is the data matrix with its current data vector (last row)
removed and replaced by a zero row. Since the column space
of YN(n - 1) is orthogonal to iJ(n), we have
(n)=P- (n-l)+P
= [EYN(n-l)
+ [~ n= [EYN (~ -
and obviously
From (24) and the property in (16e), one can easily deduce
where "*" denotes a don't care block. Using (25) in (19b) we
A. Time Updating and Weight Adaptation
In this section, the identities in (17) are used to derive the
time update relationships. To achieve time updating and hence
weight adaptation let Z =
iJ(n) := [0,... , I]T and l:: =
l::N(n). Note that the dimension of vector iJ(n) has to be equal
to the dimension (length) of the columns of the data matrix
l::N(n), i.e., the current time index n. Then substituting for
Z = iJ(n) and l:: = l::N(n) into (17a) and (17c), respectively,
EyN:a(n) =EyN(n) + E~N(n)iJ(n)(iJT(n)E~N(n)iJ(n»)-1
. iJT(n)E~N (n)
[E~N(n - 1)
Now using (23) and the property in (16d) we obtain
2 yN)n) = [2y)n)
0] + E~N(n)iJ(n)
. (iJT(n)E~N (n)iJ(n»)-1
. [-iJT(n)2y)n)
However, invoking the property EyN:a(n) = Ea:yN(n) and
using (17a) we can write
EyN:a(n) =Ea:yN(n) = E; + E~l::N(n)
. (l::~(n)E~l::N(n») -1l::~(n)E~
where the projection operator Ea is
>. - 1/22 yN(n - l l ] =Q
(n)+pi- (n)iJ(n)
. (iJT(n)E~N (n)iJ(n)r
[-iJ(nf2y)n)] .
Now, in order to arrive at the weight updating equations we
premultiply both sides of the "time update" equation (26) by
Dj (n), i.e.,
DT(n)[A-l/22~(n-l)] =
DT(n)2y)n) -
DT(n)E~N(n)iJ(n)
. (iJT(n)E~N (n)iJ(n») -1 (iJ(nf2YN(n»). (27)
But using (8) and (15), we can write
Mr ·(n) = Mr(n - 1) + Ej(n) CN(n)
pi- = I _ P
The interesting property of the vector iJ(n) which is called the
pinning vector or the time annihilator is that the projection
of l::N(n) onto the space spanned by iJ(n), i.e., Eal::N(n) is
i.e., the current data vector. Thus
Ej(n) : = iJT(n)E~N(n)Dj(n) = iJT(n)Ej(n)
= dj(n) - ZT(n)Mrj(n)
CN(n) :=2~)n)iJ(n) =
(l::~(n)l::N(n)rl
. l::~(n)iJ(n) = E'i/(n)Z(n)
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 4, NO.2, MARCH 1993
"fN(n): = O'T(n)E~N(n)if(n) = O'T(n)[I - EyN(n)]O'(n)
= 1- O'T(n)EyN(n)O'(n) = 1- ZT(n)g;/(n)Z(n).
)...-lgi/(n - I)Z(n)
N(n) = -N (n)
(n) = 1+ )...-IZT(n)E"i/(n _ I)Z(n)'
(3N(n) : = f&(n)E~N(n).z3j(n)
= zN(n)O' (n)EYN(n)Dj(n) = zN(n)Cj(n)
Note that according to (15) the term on the left side of (33) is
the new weight vector associated with node j after a node is
added to its input (previous) layer. Thus we can write
()N(n) : = f&(n)E~N(n)YN(n)
= z;'(n)O'T(n)E~N(n)if(n) = z;'(nhN(n)
[-9.~N (~)YN(n)] = [ -ZN(n)9.[N (n)O'(n)]
= [ -ZN(niCN(n)]
~j(N + 1, n) = [~j(~, n)] +::i:; BN(n)
where BN(n), (3N(n) and ()N(n) can be expressed, using
the definitions in (29a)-(29d), in terms of time update (RLS)
parameters as
A new node, N, is added to a hidden layer in the network.
. (f&(n)E~N (n)YN(n)) -1 [-f&(n)9.YN (n) 1].
where Cj (n) is the a posteriori error at the ouWut of node j at
iteration 1"!: as was defined in (29a). Note that Wj(N, n) is the
same as Wj (n) used before, i.e., the weight vector associated
with node i, which receives N inputs prior to node addition
and hence Wj (N + 1, n) is the weight vector associated with
the same node after an additional node is added to its input
layer. The incoming weights to the added node are updated
using the standard RLS training algorithm.
This gives
._ Cj(n) _
cj(n) .- -(-) - dj(n) - Z (n)Wj(n - 1)
In most of the signal processing applications, one can exploit the shifting or the serial property of the data sequence
and obtain recursive relationships for computing CN (n) and
'YN (n). This would lead to the fast transversal filters (FfF)
formulations which require only O(N) operations as
opposed to O(N2 ) operations for the standard RLS approach.
For neural networks, however, the seriality property can not
be assumed for the input and output sequences in each layer.
Thus it is inevitable to use a RLS-based scheme , for the
weight adaptation procedure. It can be shown that the updating
equation (28) is equivalent to that of the standard RLS by
expressing the terms in Cj (n)h N(n) and CN(n) using the
RLS equations ; namely
E"i/(n) =)...-lgi/(n - 1)
)...-2E"i/(n -1)Z(n)ZT(n)g;/(n -1)
1+ )...-IZT(n)gi/(n - I)Z(n)
Equations (28), (31a), and (31b) represent the RLS algorithm
developed in , and CN(n) is the relevant gain matrix.
c~(n) is the a priori error (i.e., before updating)
Cj (n) represents the a posteriori error (i.e., after
updating).
B. Order Updating and Recursive Node Creation
The order update equation and the node addition formulation
can be obtained by appending a column vector, YN(n), to the
column space ofY N (n). When a new node N is added, assuming that the weights coming to this node are randomly selected,
the corresponding vector is YN(n) =
[O,O, ... ,zN(n)]T =
zN(n)O'(n) where zN(n) is the output of the added node.
Fig. 3 demonstrates this node addition process. The expanded Y(n) matrix is then given by
YN+l(n) = [YN(n)
YN(n)] = [YN(n)
zN(n)O'(n)].
Now using (17c) for Y = YN(n) and Z = YN(n) and
pre-multiplying the result by .z3T(n) gives
.z3T(n)9.YN+l (n) = [.z3T(n)9.y)n)
+ Dj (n)EYN(n)YN(n)
AZIMI-SADJADI et 01.: NODE CREATION IN MULTILAYER NEURAL NETWORKS
C. Time---Order Update Interface
To proceed with the training using the RLS equations in
Section III-A for the layer with added node (at its input), the
relationship between the order-update (34) and the time-update
(28) must be derived.
Comparing (34) and (28), and considering definitions in
(29a)-(29d), indicate that we only need to establish a relationship between EN~l(n) and EN1(n), in terms of the
parameters CN(n) and 'YN(n). This would take into account
for the effects of added dimension in the RLS equations as a
result of node addition. Let us first partition matrix EN+1(n)
as follows
where ZN(n) = [O,O, ... ,zN(n)]
= zN(n)5(n), so that
(n) = [Y~(n)YN(n)
ZN(n)Y~(n)5(n)]
5T(n)YN(n)zN(n)
= [ _ EN(n)
ZN(n)ZN(n)]
Z'J:(n)zN(n)
Now using the inverse of partitioned matrices we obtain
R- 1(n) + CN(n)C,&(n)
zN(n)l,N(n).
zN(n),N(n)
z~(n)'N(n)
This time-order update interface equation allows continuous
weigh adaptation after a node is added to the network.
Summary of the Algorithm
1. Initial Architecture:
Construct a neural network architecture with a small number of hidden layer nodes.
2. Time Update - Weight Adaptation:
Present the training data and iterate the standard RLS weight adaptation equations in conjunction with the analog of backpropagation method using (28)-(31). Monitor the Average Mean Squared Error (AMSE) at the output.
3. Order Update - Node Creation:
If the rate of change of
AMSE is not acceptable, increase the number of hidden
layer nodes by one and update the outgoing weights!
using the order update (34) and (35).
4. Order - Time Updates Interface:
To proceed with subsequent weight updating after node creation use (37) to
generate the required EN~1 (n) and then switch back to
the time updating process.
SIMULATION RESULTS
This section presents the simulation results for the multiplexer, decoder, and a target detection and classification
1The incoming weights to the added node are updated using the standard
problem. The objectives are: to compare the convergence rates
as well as the detection, classification, and false-alarm rates for
the target detection problem for both the fixed and the variable
topologies and to study the effects of adding nodes to the first
and second hidden layers on the learning behavior. For all the
examples and all the cases, the networks are considered to
have converged when the actual outputs are within 10% of the
desired outputs for all the training data.
A. Multiplexer Network
The architecture of the multiplexer network problem consisted of three-layers with six inputs and a single output node.
The numbers of first and second hidden layer nodes were
varied. The two address lines activate one of the four data
lines using a binary code. The desired output was the input
to the activated data line. The index of performance used
to determine the training of the network was the averaged
mean squared error (AMSE) at the output averaged over all
the output nodes. The AMSE was monitored every 25 and
40 iterations when the nodes were added to the first and the
second layers, respectively, and the threshold value for the
rate of change of AMSE was set to 0.001. Table I shows
the convergence results for both the fixed and the variable
topologies. Figs. (4a)-(4d) show the learning behavior for
cases 1 and 2. Figs. (5a)-(5d) represent the corresponding
learning curves for cases 3 and 4, and Figs. (6a)-(6e) show
the corresponding learning curves for cases 5 through 7.
In Table I, comparing the convergence rates for the fixed
and the corresponding variable architectures in cases 1 through
4 indicates a significant improvement when the nodes were
added to the first hidden layer. This improvement is more
obvious in cases 1 and 2 with a small initial fixed architecture
than those of cases 3 and 4 with a larger initial topology. As
seen from the learning curves, the addition of nodes during
the early stages of the training process caused a significant
reduction in the AMSE value and a sharp decline of the error.
On the other hand, addition of nodes at final stages of training
process caused only slight changes in the AMSE. This trend in
the learning behavior seems to be consistent for all the cases
1 through 4. When the nodes were added to the second hidden
layer, as in cases 5 and 6 in Table I, the improvement in the
convergence rate was not so obvious. In case 5, for the fixed
network architecture, the AMSE was "stuck" to a value near
0.5 and never converged to a solution. Although the variation
of the topology caused the AMSE to drop to about 0.25
during the first 620 iterations, the network did not converge
after adding 15 nodes during the allowed 3500 iterations.
Nonetheless, the dynamic architecture showed substantially
better learning behavior than its fixed counterpart. Case 6
represents an interesting situation; the AMSE for the fixed
architecture was oscillating around 0.25 during the first 450
iterations and started to decline at a reasonable rate to converge
in 1256 iterations. In the corresponding variable architecture,
the AMSE increased to a value around 0.32 during the first 660
iterations and then started to drop slowly after 16 nodes were
added to the second layer. The error continued to decrease to
a value below 0.25 after 2500 iterations, but then it started to
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 4, NO.2, MARCH 1993
CONVERGENCE RESULTS FOR THE MULTIPLEXER PROBLEM USING FIXED AND VARIABLE TOPOLOGIES
Fixed Topology
Variable Topology
Iterations
Iterations
The AMSE was monitored every 25 iterations for cases 1 through 4.
The AMSE was monitored every 40 iterations for cases 5 and 6.
Did not Converge
Did not Converge
The AMSE was monitored every 200 iterations for case 7.
I.earniDgCurve for the Dynamic
6-3-5-1 Initial Architecture (Layer 1)
I.earniDg Curve for the Dynamic
6-1·5-1 InitialArchitecture (Layer 1)
Number of lleratlona
01 It.r.tlons
LearoiogCurve for the
6-1-5-1 Fixed Architecture
It.r.tlon.
LeemhIg curve for the
6-3-5-1 Fixed Architecture
01 It.r.tlons
The AMSE for the multiplexer problem, cases 1 and 2.
AZIMI-SADJADI et al.: NODE CREATION IN MULTILAYER NEURAL NETWORKS
0.5.,------------------
0.5,...-----------------,
Ita,atlona
LeamiogCurve for the Dyoamic
6-5-5-1 InitialArchitecture (Layer 1)
O.o+--.....--r-........---,----....--,....----/
LearniDgCurve for the
6-6-5-1FIxed Architecture
Ita,atlona
LearniDg Curve for the
6-7-5-1 Fixed Architecture
Leamiog Curve for the Dyoamic
6-7-5-1 InitialArchitecture (Layer 1)
0.5 ......---------------...,
Ita,atlona
o.o+-.....___...._~-.......---,....---.-.......-~
Ita,atlona
The AMSE for the multiplexer problem, cases 3 and 4.
increase and consequently the network never converged. This
implies that the addition of nodes to the upper (closer to the
outputs) hidden layer affects the AMSE at the output layer
more quickly than when the nodes are added to the lower
(closer to the inputs) hidden layer. This behavior is expected
since the nodes in the upper hidden layer have a filtering effect
for the nodes added to the lower hidden layers, whereas there
is no buffering layer for the case when the nodes are added
to the upper hidden layer. To overcome this condition, it is
useful to increase the iteration span for which the AMSE
is monitored in order to reduce the total number of nodes
created and to give the network enough time to "relax" into a
more stable state before a new node is added. Otherwise, the
network may end up with more added neurons than needed.
Fig. (6e) shows the learning curve for case 7 which is the same
dynamic architecture as in case 6 with the exception that the
iteration span is chosen to be 200 instead of 40. As can be
seen the network converged in only 903 iterations without
exhibiting the same difficulty as in case 6. It appears that
a good rule of thumb for determining a reasonable iteration
span is to multiply the number of training samples by three.
Another approach would be to alternate the addition of nodes
amongst different hidden layers or to perform node deletion
and addition at different layers.
B. Decoder Network
The decoder network was a two-layer neural network with
three inputs and eight output nodes. The number of hidden
layer nodes was varied. The three input lines activate one
of the eight output lines using a binary code. The desired
output is the binary decoded value of the three input lines.
Again, the index of performance used was the AMSE at the
output averaged over all the output nodes and the threshold
value for the rate of change of AMSE was set to 0.001.
Table II presents the results for the fixed and the variable
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 4, NO.2, MARCH 1993
I A·u."nillJ.! (~urvt' f••r th .."
(,-!l-t-t Fix...1An-hi ....-turv
Learrung' Curve for the Dynamic
&-9-1- I Initial Architecture (Layer 2)
05 ...------------------,
0.0 +----"""T--~-...-~-___,--~-_i
iterations
Learning Curve for the
6-9-3-1 Fixed Architecture
I.eaming Curve for the Dynamic
6-9-3-1 InitialArchitecture (Layer 2)
0.0 +-~-~-r_~-~__r-~-~~-~~--i
0.5.,....------------------,
Iterations
Iterations
Learning Curve for the 6-9-3-1Dynamic
Architecture (200 Iteration Span)
05.,....-------------------,
00 +--~-"""T--~-.,._-~-"""T--~__l
Number of Iterations
The AMSE for the multiplexer problem, cases 5 through 7.
AZIMI-SADJADI et al.: NODE CREATION IN MULTILAYER NEURAL NETWORKS
LeeruiogCurve Iortbe
3-1-8 FIxedArchitecture
0.20..,..-----------------,
II.r.llon.
lAlaroingCurve for the Dynamic 3-1-8
IDidalArchitecture (20 Iterations Sp&n)
II.r.llon.
LearoiDgCurve lbr the Dyoamic 3-1-8
IDitiai Architecture (215 iteradoD spen)
II.r.llon.
The AMSE for the decoder problem, cases 1 and 2.
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 4, NO.2, MARCH 1993
01 Iterltlons
Leaming Curve for the I>yDamic 3-lS-8
IDitialArchitecture (15 Iterations Span)
0.20..------------------,
01 lI.rltlonl
Le&rniDgCurve for the
3-lS-8 Fb:ed Architecture
01 Iterations
0.00 +--......- ......--r--......-
......--r--......-
LeamingCurve for the I>yDamic 3-lS-8
IDitial Architecture (211 Iterations Span)
Le&rniDgCurve for the
3-9-8 Fb:ed Architecture
01 Itarltlons
The AMSE for the decoder problem, cases 3 through 5.
CONVERGENCE RESULTS FOR THE DECODER PROBLEM
FIXED AND VARIABLE TOPOLOGIES
Fixed Topology
Variable Topology
Iterations
Iterations
Iterations
Iterations
Iterations
Iterations
AZIMI-SADJADI et al.: NODE CREATION IN MULTILAYER NEURAL NETWORKS
LeamiDg ClIne for the
49-1·10-3 FWld Architecture
LeamiDg Curve for the Dynamic
49-1·10-3lDitial.Architecture (Layer 1)
0.4.,...------------------,
It...llon.
It.r.llon.
LeamiDgCllne for the
Learoi.og Cun>efor the Dyuamic
49-15-10-3 Fixed Architecture
49-15-10-3lDitialArchitecture (Layer 1)
It.r.llon.
It.ration.
LeamiDgCurve for the
LeamiDg Cune for the Dynamic
49-25-10-3 Fixed Architecture
49-25-10-3lDitialArchitecture (Layer 1)
Iteratlone
It.r.llon.
The AMSE for target detection and classification problem, cases 1 through 3.
IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 4, NO.2, MARCH 1993
LearniDIrCurve for the
49-15-1-8Flzed Architecture
0.4T""----------------..,
LeamiDg Curve for the Dyuamic
49-15-1-8laitial Architecture (layer2)
0.4,-----------------,
II.rallona
UlarniDgCurve for the Dynamic
49-15-10-3laitialArchitecture (Layer2)
II.r.lIona
II.r.lIona
II.r.lIona
LeamiDgCurve for the
49-15-10-3Flzed Architecture
0.1+---.---r--...---,r---.---T"""----!
0.1+--..-T"""--r--.---,r---....,.-......-r-...--!
0.4-r-----------------.,
The AMSE for target detection and classification problem, cases 4 and 5.
topologies. The corresponding learning curves for cases 1 and
2 are presented in Figs. (7a}-(7c) and those of cases 3 through
5 are presented in Figs. (8a}-(8d).
Comparing the convergence rates for the fixed and the corresponding variable architecture in cases 1 and 2 in Table II,
indicates a drastic improvement when the nodes are dynamically added to the hidden layer. However, the results in cases
3 and 4 indicate only a minor improvement when the dynamic
node creation method is employed. As discussed previously,
this is due to the fact that in cases 3 and 4, the training process
starts with a larger initial topology than the previous two cases
and possibly closer to an "optimal" topology already, hence
fewer additional nodes are needed. Comparing the results of
the fixed topology in case 5 to the variable topology in case 4,
which had the same final topology as the fixed architecture in
case 5, indicates an interesting situation. That is, although the
training process starts with a small network, the varied network
topology finds a solution faster than the fixed network with
identical final topology. This indicates the impact of changing
the dimensionality and the form of the error space on both the
training and the convergence rate of the network.
C. Target Detection and Classification
In this section we present the results of applying the
proposed method for detection and classification of two types
of targets, namely of Nylon and Wood compositions, from
microwave data (15]. These targets were buried in dry loamy
soil. The networks were trained for six targets and six backgrounds from each type of data. The input signal consisted
of the amplitude of sensor data in windows of size 15x 15.
The method of principal components or Karhunen-Loeve (KL)
transform (16] was used to reduce the amount of data without
losing the accuracy in the representation. This was achieved
by evaluating the two-sided covariances of the 2-D amplitude
data in 7x 7 grids within each 15x 15 window. The covariances
were used to form the doubly block Toeplitz covariance matrix
AZIMI-SADJADI et al.: NODE CREATION IN MULTILAYER NEURAL NETWORKS
CONVERGENCE RESULTS FOR THE TARGET DETECTION AND CLASSIFICATION PROBLEM USING FIXED AND VARIABLE TOPOLOGIES
Fixed Topology
Variable Topology
Iterations
Performance
Iterations
Performance
In cases 1 through 3 the nodes were added to the first hidden layer
49-15-10-3
49-19-10-3
100/100/35
49-25-10-3
49-28-10-3
100/100/13
In cases 4 and 5 the nodes were added to the second hidden layer
49-15-10-3
49-15-13-3
All entries under the performance are of the form:
Detection Rate (%) / Classification Rate (%)
/ False-Alarm Rate (%)
of the process. This matrix was then diagonalized using a
unitary transformation to yield the eigenvalues associated with
the data within each window. This resulted in 49 eigenvalues
representing the most significant energy components of the
data in an orthonormal signal space. These are used for both
training and testing procedures of the neural networks.
Once the network is trained, the generalization of the
network is tested by using the training data and the testing data
(the data from other parts of the lanes which the network had
not seen before). The testing data at each frequency consisted
of 15 targets and 9 background windows for nylon data and 12
targets and 8 background windows for wood data. The initial
architecture used was a 49-25-10-3 network architecture, i.e.,
49 inputs, 25 first layer nodes, 10 second layer nodes, and
the 3 output nodes which was determined empirically. The
desired output sequences for nylon targets, wood targets and
backgrounds were (1,0,0), (0,1,0), and (0,0,1), respectively.
The index of performance used to determine the trainability
of the network was the AMSE at the output nodes. Table
III summarizes the convergence results for the fixed and the
variable topologies. The initial fixed network architectures
were heuristically chosen based on our empirical studies.
The architecture was held fixed and RLS-based algorithm in
Section III-A was applied to train the network. In the variable
topology cases, the network architecture was dynamically
changed during the training process. The criterion for adding
a node to a given layer was based upon monitoring the slope
of the AMSE at the output layer after each 6 to 10 iterations.
If the slope was below a selected threshold of 0.001, a new
node was created; otherwise the RLS updating of the weights
continued without interruption.
Figs. 9(a)-9(f) represent the learning curves for cases 1
through 3 and Figs. 10(a)-10(d) show the corresponding
learning curves for cases 4 and 5 in Table III. As evident from
these results, the dynamic node creation algorithm shows a
drastic improvement in the convergence rate. It can be seen
from these graphs that while the AMSE is oscillating around
a fiat portion of the error surface, upon addition of nodes, the
error drastically decreases to its final value for convergence
in a few iterations. Note that when the nodes were added
to the first hidden layer only, no prominent overshoot was
observed in the monitored AMSE at the output. However,
when the nodes are added to the second hidden layer, as
in cases 4 and 5, the overshoot is more observable. This is
mainly attributed to the filtering operation performed at the
upper layer as described before.
In addition, the performance of the network, in terms of the
detection and classification rates and generalization properties,
was relatively unchanged compared to the heuristically chosen
fixed architectures. In particular, the performance in terms of
detection and classification rates, was improved in cases 2 and
3 and was unchanged in case 4 when the variable topology
method was employed. In case 1 a slight degradation in the
classification rate was observed while the false-alarm rate was
substantially reduced; and in case 5 a degredation in detection
rate was seen while both classification and false-alarm rates
were improved. This indicates the fact that the AMSE criterion
alone may not necessarily lead to an optimum final architecture
of the network as far as the detection, classification, and
false-alarm rates are concerned. More research will be needed
to address these issues. The proposed algorithm exhibits
much better convergence rate than the standard LMS learning
method .
CONCLUSION
The problem of simultaneous weight adaptation and node
creation is considered in this paper. The projection updating
method is utilized to arrive at recursive equations for both
the weight updating and dynamic node creation during the
training process. The vector-space interpretation for the RLS
type algorithm allows the variation of the number of hidden
layer nodes, and provides an optimal weight vector solution
without requiring the prohibitive cost of retraining process.
In this approach, it is possible to gain more insight into
the internal behavior of the learning and the convergence
characteristics, since the error is monitored at each stage of the
algorithm. The effectiveness of the algorithm is demonstrated
on a real world application for detecting and classifying buried
dielectric anomalies as well as the standard multiplexer and
the decoder problems. Comparison of the simulation results
indicates a significant improvement when compared to the
LMS-based approach suggested in .
Also, the proposed scheme provides an optimal or near
optimal topology in the least squares sense. It is also plausible that changing the shape of the error surface during
the training process, will in effect, reduce the possibility of
getting stuck in a local minima which is a prominent problem
with all the LMS-based training algorithms. The simulation
results demonstrate how the node creation method impacts the
convergence rate especially when the nodes are added to the
lower layers as the upper hidden layer has a filtering effect and
suppresses the large transients which may cause instability in
the learning process. The addition of nodes to the upper (closer
to the outputs) impacts the convergence rate more quickly,
however, this may cause oscillation and divergence. This can
be overcome by increasing the iteration span and allowing
the network to relax into a stable mode. The improvement
in the convergence rate is more obvious when the training
process starts with a small network topology, and the method is
particularly effective for problems requiring complex decision
regions (hard learning). There are a few open issues that need
to be mentioned at this time. First, it seems that the rate of
AMSE alone is not enough to determine exactly when and
where to add the new nodes in order to obtain an optimal
network topology while preserving its generalization property.
Second, one can not be certain that the variable architecture
method reaches the same solution as its fixed architecture
counterpart.