Knowledge-aware Graph Neural Networks with Label
Smoothness Regularization for Recommender Systems
Hongwei Wang
Stanford University
 
Fuzheng Zhang
Meituan-Dianping Group
 
Mengdi Zhang
Meituan-Dianping Group
 
Jure Leskovec
Stanford University
 
Miao Zhao, Wenjie Li
Hong Kong Polytechnic University
{csmiaozhao,cswjli}@comp.polyu.edu.hk
Zhongyuan Wang
Meituan-Dianping Group
 
Knowledge graphs capture structured information and relations
between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve
recommender systems. However, existing approaches in this domain rely on manual feature engineering and do not allow for an
end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to
provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable
function that identifies important knowledge graph relationships
for a given user. This way we transform the knowledge graph
into a user-specific weighted graph and then apply a graph neural
network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which
posits that adjacent items in the knowledge graph are likely to have
similar user relevance labels/scores. Label smoothness provides
regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an
efficient implementation that shows strong scalability with respect
to the knowledge graph size. Experiments on four datasets show
that our method outperforms state of the art baselines. KGNN-LS
also achieves strong performance in cold-start scenarios where
user-item interactions are sparse.
Knowledge-aware recommendation; graph neural networks; label
propagation
ACM Reference Format:
Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao,
Wenjie Li, and Zhongyuan Wang. 2019. Knowledge-aware Graph Neural
Networks with Label Smoothness Regularization for Recommender Systems.
In The 25th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD ’19), August 4–8, 2019, Anchorage, AK, USA. ACM, New York,
NY, USA, 10 pages. 
INTRODUCTION
Recommender systems are widely used in Internet applications to
meet user’s personalized interests and alleviate the information
overload . Traditional recommender systems that are
KDD ’19, August 4–8, 2019, Anchorage, AK, USA
2019. ACM ISBN 978-1-4503-6201-6/19/08...$15.00
 
based on collaborative filtering usually suffer from the coldstart problem and have trouble recommending brand new items that
have not yet been heavily explored by the users. The sparsity issue
can be addressed by introducing additional sources of information
such as user/item profiles or social networks .
Knowledge graphs (KGs) capture structured information and
relations between a set of entities . KGs
are heterogeneous graphs in which nodes correspond to entities (e.g.,
items or products, as well as their properties and characteristics) and
edges correspond to relations. KGs provide connectivity information
between items via different types of relations and thus capture
semantic relatedness between the items.
The core challenge in utilizing KGs in recommender systems
is to learn how to capture user-specific item-item relatedness captured by the KG. Existing KG-aware recommender systems can be
classified into path-based methods , embedding-based
methods , and hybrid methods . However,
these approaches rely on manual feature engineering, are unable to
perform end-to-end training, and have poor scalability. Graph Neural Networks (GNNs), which aggregate node feature information
from node’s local network neighborhood using neural networks,
represent a promising advancement in graph-based representation
learning . Recently, several works developed GNNs
architecture for recommender systems , but these
approaches are mostly designed for homogeneous bipartite useritem interaction graphs or user-/item-similarity graphs. It remains
an open question how to extend GNNs architecture to heterogeneous
knowledge graphs.
In this paper, we develop Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) that extends
GNNs architecture to knowledge graphs to simultaneously capture
semantic relationships between the items as well as personalized
user preferences and interests. To account for the relational heterogeneity in KGs, similar to , we use a trainable and personalized
relation scoring function that transforms the KG into a user-specific
weighted graph, which characterizes both the semantic information
of the KG as well as user’s personalized interests. For example, in
the movie recommendation setting the relation scoring function
could learn that a given user really cares about “director” relation
between movies and persons, while somebody else may care more
about the “lead actor” relation. Using this personalized weighted
graph, we then apply a graph neural network that for every item
node computes its embedding by aggregating node feature information over the local network neighborhood of the item node. This
 
way the embedding of each item captures it’s local KG structure in
a user-personalized way.
A significant difference between our approach and traditional
GNNs is that the edge weights in the graph are not given as input. We set them using user-specific relation scoring function that
is trained in a supervised fashion. However, the added flexibility
of edge weights makes the learning process prone to overfitting,
since the only source of supervised signal for the relation scoring
function is coming from user-item interactions (which are sparse
in general). To remedy this problem, we develop a technique for
regularization of edge weights during the learning process, which
leads to better generalization. We develop an approach based on
label smoothness , which assumes that adjacent entities in
the KG are likely to have similar user relevancy labels/scores. In
our context this assumption means that users tend to have similar
preferences to items that are nearby in the KG. We prove that label
smoothness regularization is equivalent to label propagation and
we design a leave-one-out loss function for label propagation to
provide extra supervised signal for learning the edge scoring function. We show that the knowledge-aware graph neural networks
and label smoothness regularization can be unified under the same
framework, where label smoothness can be seen as a natural choice
of regularization on knowledge-aware graph neural networks.
We apply the proposed method to four real-world datasets of
movie, book, music, and restaurant recommendations, in which the
first three datasets are public datasets and the last is from Meituan-
Dianping Group. Experiments show that our method achieves significant gains over state-of-the-art methods in recommendation
accuracy. We also show that our method maintains strong recommendation performance in the cold-start scenarios where user-item
interactions are sparse.
RELATED WORK
Graph Neural Networks
Graph Neural Networks (or Graph Convolutional Neural Networks,
GCNs) aim to generalize convolutional neural networks to non-
Euclidean domains (such as graphs) for robust feature learning.
Bruna et al. define the convolution in Fourier domain and calculate the eigendecomposition of the graph Laplacian, Defferrard et al.
 approximate the convolutional filters by Chebyshev expansion
of the graph Laplacian, and Kipf et al. propose a convolutional
architecture via a first-order approximation. In contrast to these
spectral GCNs, non-spectral GCNs operate on the graph directly
and apply “convolution” (i.e., weighted average) to local neighbors
of a node .
Recently, researchers also deployed GNNs in recommender systems: PinSage applies GNNs to the pin-board bipartite graph in
Pinterest. Monti et al. and Berg et al. model recommender
systems as matrix completion and design GNNs for representation
learning on user-item bipartite graphs. Wu et al. use GNNs
on user/item structure graphs to learn user/item representations.
The difference between these works and ours is that they are all
designed for homogeneous bipartite graphs or user/item-similarity
graphs where GNNs can be used directly, while here we investigate
GNNs for heterogeneous KGs. Wang et al. use GCNs in KGs
for recommendation, but simply applying GCNs to KGs without
proper regularization is prone to overfitting and leads to performance degradation as we will show later. Schlichtkrull et al. also
propose using GNNs to model KGs , but not for the purpose of
recommendations.
Semi-supervised Learning on Graphs
The goal of graph-based semi-supervised learning is to correctly
label all nodes in a graph given that only a few nodes are labeled.
Prior work often makes assumptions on the distribution of labels
over the graph, and one common assumption is smooth variation of
labels of nodes across the graph. Based on different settings of edge
weights in the input graph, these methods are classified as: (1) Edge
weights are assumed to be given as input and therefore fixed ; (2) Edge weights are parameterized and therefore learnable
 . Inspired by these methods, we design a module of
label smoothness regularization in our proposed model. The major
distinction of our work is that the label smoothness constraint is
not used for semi-supervised learning on graphs, but serves as
regularization to assist the learning of edge weights and achieves
better generalization for recommender systems.
Recommendations with Knowledge Graphs
In general, existing KG-aware recommender systems can be classified into three categories: (1) Embedding-based methods pre-process a KG with knowledge graph embedding (KGE)
 algorithms, then incorporate learned entity embeddings into
recommendation. Embedding-based methods are highly flexible in
utilizing KGs to assist recommender systems, but the KGE algorithms focus more on modeling rigorous semantic relatedness (e.g.,
TransE assumes head +relation = tail), which are more suitable
for graph applications such as link prediction rather than recommendations. In addition, embedding-based methods usually lack
an end-to-end way of training. (2) Path-based methods 
explore various patterns of connections among items in a KG (a.k.a
meta-path or meta-graph) to provide additional guidance for recommendations. Path-based methods make use of KGs in a more
intuitive way, but they rely heavily on manually designed metapaths/meta-graphs, which are hard to tune in practice. (3) Hybrid
methods combine the above two categories and learn
user/item embeddings by exploiting the structure of KGs. Our proposed model can be seen as an instance of hybrid methods.
PROBLEM FORMULATION
We begin by describing the KG-aware recommendations problem
and introducing notation. In a typical recommendation scenario, we
have a set of users U and a set of items V. The user-item interaction
matrix Y is defined according to users’ implicit feedback, where
yuv = 1 indicates that user u has engaged with item v, such as
clicking, watching, or purchasing. We also have a knowledge graph
G = {(h,r,t)} available, in which h ∈E, r ∈R, and t ∈E denote
the head, relation, and tail of a knowledge triple, E and R are the set
of entities and relations in the knowledge graph, respectively. For
example, the triple (The Silence of the Lambs, film.film.star, Anthony
Hopkins) states the fact that Anthony Hopkins is the leading actor in
film “The Silence of the Lambs”. In many recommendation scenarios,
an item v ∈V corresponds to an entity e ∈E (e.g., item “The
Original knowledge
𝒢with real-valued weights
𝐀𝒖for given user 𝑢
Entity (item)
features 𝐄
Label smoothness regularization on KG
weights: 𝑅(𝐀&) (label propagation)
Userengagement
Knowledge-aware graph neural networks:
𝐯& = 𝐾𝐺𝑁𝑁(𝐄, 𝐀&) (feature propagation)
ℒ= 𝐽A𝑦&C, 𝑦&C + 𝜆𝑅𝐀&
Predicting function A𝑦&C = 𝑓(𝐮, 𝐯&)
Figure 1: Overview of our proposed KGNN-LS model. The original KG is first transformed into a user-specific weighted graph,
on which we then perform feature propagation using a graph neural network with the label smoothness regularization. The
two modules constitute the complete loss function L.
Silence of the Lambs” in MovieLens also appears in the knowledge
graph as an entity). The set of entities E is composed from items
V (V ⊆E) as well as non-items E\V (e.g. nodes corresponding
to item/product properties). Given user-item interaction matrix Y
and knowledge graph G, our task is to predict whether user u has
potential interest in item v with which he/she has not engaged
before. Specifically, we aim to learn a prediction function ˆyuv =
F (u,v|Θ, Y, G), where ˆyuv denotes the probability that user u will
engage with item v, and Θ are model parameters of function F .
We list the key symbols used in this paper in Table 1.
OUR APPROACH
In this section, we first introduce knowledge-aware graph neural
networks and label smoothness regularization, respectively, then
we present the unified model.
U = {u1, · · · }
Set of users
V = {v1, · · · }
Set of items
User-item interaction matrix
G = (E, R)
Knowledge graph
E = {e1, · · · }
Set of entities
R = {r1, · · · }
Set of relations
Set of non-item entities
User-specific relation scoring function
Adjacency matrix of G w.r.t. user u
Diagonal degree matrix of Au
Raw entity feature
H(l), l = 0, ..., L −1
Entity representation in the l-th layer
W(l), l = 0, ..., L −1
Transformation matrix in the l-th layer
lu(e), e ∈E
Item relevancy labeling function
l∗u(e), e ∈E
Minimum-energy labeling function
ˆlu(v), v ∈V
Predicted relevancy label for item v
Label smoothness regularization on Au
Table 1: List of key symbols.
Preliminaries: Knowledge-aware Graph
Neural Networks
The first step of our approach is to transform a heterogeneous KG
into a user-personalized weighted graph, which characterizes user’s
preferences. To this end, similar to , we use a user-specific relation scoring function su(r) that provides the importance of relation
r for user u: su(r) = д(u, r), where u and r are feature vectors of
user u and relation type r, respectively, and д is a differentiable
function such as inner product. Intuitively, su(r) characterizes the
importance of relation r to user u. For example, a user may be more
interested in movies that have the same director with the movies
he/she watched before, but another user may care more about the
leading actor of movies.
Given user-specific relation scoring function su(·) of user u,
knowledge graph G can therefore be transformed into a userspecific adjacency matrix Au ∈R|E |×|E |, in which the (i, j)-entry
u = su(rei,ej ), and rei,ej is the relation between entities ei and
ej in G.1 Aij
u = 0 if there is no relation between ei and ej. See the
left two subfigures in Figure 1 for illustration. We also denote the
raw feature matrix of entities as E ∈R|E |×d0, where d0 is the dimension of raw entity features. Then we use multiple feed forward
layers to update the entity representation matrix by aggregating
representations of neighboring entities. Specifically, the layer-wise
forward propagation can be expressed as
, l = 0, 1, · · · , L −1.
In Eq. (1), Hl is the matrix of hidden representations of entities in
layer l, and H0 = E. Au is to aggregate representation vectors of
neighboring entities. In this paper, we set Au ←Au + I, i.e., adding
self-connection to each entity, to ensure that old representation
vector of the entity itself is taken into consideration when updating
entity representations. Du is a diagonal degree matrix with entries
1In this work we treat G an undirected graph, so Au is a symmetric matrix. If both
triples (h, r1, t) and (t, r2, h) exist, we only consider one of r1 and r2. This is due to
the fact that: (1) r1 and r2 are the inverse of each other and semantically related; (2)
Treating Au symmetric will greatly increase the matrix density.
u , therefore, D−1/2
is used to normalize Au and keep
the entity representation matrix Hl stable. Wl ∈Rdl ×dl+1 is the
layer-specific trainable weight matrix, σ is a non-linear activation
function, and L is the number of layers.
A single GNN layer computes the representation of an entity via a
transformed mixture of itself and its immediate neighbors in the KG.
We can therefore naturally extend the model to multiple layers to
explore users’ potential interests in a broader and deeper way. The
final output is HL ∈R|E |×dL, which is the entity representations
that mix the initial features of themselves and their neighbors up
to L hops away. Finally, the predicted engagement probability of
user u with item v is calculated by ˆyuv = f (u, vu), where vu (i.e.,
the v-th row of HL) is the final representation vector of item v,
and f is a differentiable prediction function, for example, inner
product or a multilayer perceptron. Note that vu is user-specific
since the adjacency matrix Au is user-specific. Furthermore, note
that the system is end-to-end trainable where the gradients flow
from f (·) via GNN (parameter matrix W) to д(·) and eventually to
representations of users u and items v.
Label Smoothness Regularization
It is worth noticing a significant difference between our model and
GNNs: In traditional GNNs, edge weights of the input graph are
fixed; but in our model, edge weights D−1/2
in Eq. (1) are
learnable (including possible parameters of function д and feature
vectors of users and relations) and also requires supervised training
like W. Though enhancing the fitting ability of the model, this will
inevitably make the optimization process prone to overfitting, since
the only source of supervised signal is from user-item interactions
outside GNN layers. Moreover, edge weights do play an essential
role in representation learning on graphs, as highlighted by a large
amount of prior works . Therefore, more regularization on edge weights is needed to assist the learning of entity
representations and to help generalize to unobserved interactions
more efficiently.
Let’s see how an ideal set of edge weights should be like. Consider
a real-valued label function lu : E →R on G, which is constrained
to take a specific value lu(v) = yuv at node v ∈V ⊆E. In our
context,lu(v) = 1 if useru finds the itemv relevant and has engaged
with it, otherwise lu(v) = 0. Intuitively, we hope that adjacent
entities in the KG are likely to have similar relevancy labels, which
is known as label smoothness assumption. This motivates our choice
of energy function E:
E(lu, Au) = 1
ei ∈E,ej ∈E
 lu(ei) −lu(ej)2 .
We show that the minimum-energy label function is harmonic by
the following theorem:
Theorem 1. The minimum-energy label function
lu:lu(v)=yuv,∀v ∈V
w.r.t. Eq. (2) is harmonic, i.e., l∗u satisfies
u(ej), ∀ei ∈E\V.
Proof. Taking the derivative of the following equation
E(lu, Au) = 1
 lu(ei) −lu(ej)2
with respect to lu(ei) where ei ∈E\V, we have
∂E(lu, Au)
 lu(ei) −lu(ej) .
The minimum-energy label function l∗u should satisfy that
∂E(lu, Au)
Therefore, we have
u(ej), ∀ei ∈E\V.
The harmonic property indicates that the value of l∗u at each
non-item entity ei ∈E\V is the average of its neighboring entities,
which leads to the following label propagation scheme :
Theorem 2. Repeating the following two steps:
(1) Propagate labels for all entities: lu(E) ←D−1
u Aulu(E), where
lu(E) is the vector of labels for all entities;
(2) Reset labels of all items to initial labels: lu(V) ←Y[u, V]⊤,
where lu(V) is the vector of labels for all items and Y[u, V] =
[yuv1,yuv2, · · · ] are initial labels;
will lead to lu →l∗u.
Proof. Let lu(E) =
. Since lu(V) is fixed on Y[u, V],
we are only interested in lu(E\V). We denote P = D−1
subscript u is omitted from P for ease of notation), and partition
matrix P into sub-matrices according to the partition of lu:
Then the label propagation scheme is equivalent to
lu(E\V) ←PEV Y[u, V]⊤+ PEElu(E\V).
Repeat the above procedure, we have
lu(E\V) = lim
n→∞(PEE)nl(0)
PEV Y[u, V]⊤,
where l(0)
u (E\V) is the initial value for lu(E\V). Now we show
that limn→∞(PEE)nl(0)
u (E\V) = 0. Since P is row-normalized and
PEE is a sub-matrix of P, we have
PEE[i, j] ≤ϵ,
positive items
Unobserved
positive items
Upward force for
observed items
Downward force for
unobserved items
Rubber bands
(relations in the KG)
Unobserved
negative items
(a) Without the KG
(d) L = 1 for another user
(e) LS regularization
Figure 2: (a) Analogy of a physical equilibrium model for recommender systems; (b)-(d) Illustration of the effect of the KG; (e)
Illustration of the effect of label smoothness regularization.
for all possible row index i. Therefore,
(PEE)n[i, j] =
(PEE)(n−1)PEE
(PEE)(n−1)[i,k] PEE[k, j]
(PEE)(n−1)[i,k]
(PEE)(n−1)[i,k] ϵ
≤· · · ≤ϵn.
As n goes infinity, the row sum of (PEE)n converges to zero, which
implies that (PEE)nl(0)
u (E\V) →0. It’s clear that the choice of
initial value l(0)
u (E\V) does not affect the convergence.
Since limn→∞(PEE)nl(0)
u (E\V) = 0, Eq. (6) becomes
lu(E\V) = lim
PEV Y[u, V]⊤.
(PEE)i−1 =
and we have
(PEE)i−1 −
(PEE)i = I.
Therefore, we derive that
T = (I −PEE)−1,
lu(E\V) = (I −PEE)−1PEV Y[u, V]⊤.
This is the unique fixed point and therefore the unique solution to
Eq. (5). Repeating the steps in Theorem 2 leads to
(I −PEE)−1PEV Y[u, V]⊤
Theorem 2 provides a way for reaching the minimum-energy
of relevancy label function E. However, l∗u does not provide any
signal for updating the edge weights matrix Au, since the labeled
part of l∗u, i.e., l∗u(V), equals their true relevancy labels Y[u, V];
Moreover, we do not know true relevancy labels for the unlabeled
nodes l∗u(E\V).
To solve the issue, we propose minimizing the leave-one-out loss
 . Suppose we hold out a single item v and treat it unlabeled.
Then we predict its label by using the rest of (labeled) items and
(unlabeled) non-item entities. The prediction process is identical to
label propagation in Theorem 2, except that the label of item v is
hidden and needs to be calculated. This way, the difference between
the true relevancy label of v (i.e., yuv) and the predicted label ˆlu(v)
serves as a supervised signal for regularizing edge weights:
J  yuv, ˆlu(v),
where J is the cross-entropy loss function. Given the regularization
in Eq. (7), an ideal edge weight matrix A should reproduce the
true relevancy label of each held-out item while also satisfying the
smoothness of relevancy labels.
The Unified Loss Function
Combining knowledge-aware graph neural networks and LS regularization, we reach the following complete loss function:
W,A L = min
J(yuv, ˆyuv) + λR(A) + γ ∥F ∥2
where ∥F ∥2
2 is the L2-regularizer, λ and γ are balancing hyperparameters. In Eq. (8), the first term corresponds to the part of
GNN that learns the transformation matrix W and edge weights A
simultaneously, while the second term R(·) corresponds to the part
of label smoothness that can be seen as adding constraint on edge
weights A. Therefore, R(·) serves as regularization on A to assist
GNN in learning edge weights.
It is also worth noticing that the first term can be seen as feature
propagation on the KG while the second term R(·) can be seen as
label propagation on the KG. A recommender for a specific user u
is actually a mapping from item features to user-item interaction
labels, i.e., Fu : Ev →yuv where Ev is the feature vector of item
v. Therefore, Eq. (8) utilizes the structural information of the KG
on both the feature side and the label side of Fu to capture users’
higher-order preferences.
Discussion
How can the knowledge graph help find users’ interests? To intuitively understand the role of the KG, we make an analogy with a
Restaurant
# interactions
13,501,622
23,416,418
# entities
# relations
# KG triples
Table 2: Statistics of the four datasets: MovieLens-20M
Book-Crossing
Dianping-Food (restaurant).
physical equilibrium model as shown in Figure 2. Each entity/item
is seen as a particle, while the supervised positive user-relevancy
signal acts as the force pulling the observed positive items up from
the decision boundary and the negative items signal acts as the
force pushing the unobserved items down. Without the KG (Figure
2a), these items are only loosely connected with each other through
the collaborative filtering effect (which is not drawn here for clarity). In contrast, edges in the KG serve as the rubber bands that
impose explicit constraints on connected entities. When number of
layers is L = 1 (Figure 2b), representation of each entity is a mixture
of itself and its immediate neighbors, therefore, optimizing on the
positive items will simultaneously pull their immediate neighbors
up together. The upward force goes deeper in the KG with the
increase of L (Figure 2c), which helps explore users’ long-distance
interests and pull up more positive items. It is also interesting to
note that the proximity constraint exerted by the KG is personalized
since the strength of the rubber band (i.e., su(r)) is user-specific and
relation-specific: One user may prefer relation r1 (Figure 2b) while
another user (with same observed items but different unobserved
items) may prefer relation r2 (Figure 2d).
Despite the force exerted by edges in the KG, edge weights may
be set inappropriately, for example, too small to pull up the unobserved items (i.e., rubber bands are too weak). Next, we show by
Figure 2e that how the label smoothness assumption helps regularizing the learning of edge weights. Suppose we hold out the positive
sample in the upper left and we intend to reproduce its label by the
rest of items. Since the true relevancy label of the held-out sample
is 1 and the upper right sample has the largest label value, the LS
regularization term R(A) would enforce the edges with arrows to
be large so that the label can “flow” from the blue one to the striped
one as much as possible. As a result, this will tighten the rubber
bands (denoted by arrows) and encourage the model to pull up the
two upper pink items to a greater extent.
EXPERIMENTS
In this section, we evaluate the proposed KGNN-LS model, and
present its performance on four real-world scenarios: movie, book,
music, and restaurant recommendations.
We utilize the following four datasets in our experiments for movie,
book, music, and restaurant recommendations, respectively, in
which the first three are public datasets and the last one is from
shortest distance
probability
without common rater
with common rater(s)
(a) MovieLens-20M
9 10 11 12 >12
shortest distance
probability
without common rater
with common rater(s)
(b) Last.FM
Figure 3: Probability distribution of the shortest path distance between two randomly sampled items in the KG under the circumstance that (1) they have no common user in
the dataset; (2) they have common user(s) in the dataset.
Meituan-Dianping Group. We use Satori2, a commercial KG built
by Microsoft, to construct sub-KGs for MovieLens-20M, Book-
Crossing, and Last.FM datasets. The KG for Dianping-Food dataset
is constructed by the internal toolkit of Meituan-Dianping Group.
Further details of datasets are provided in Appendix A.
• MovieLens-20M3 is a widely used benchmark dataset in movie
recommendations, which consists of approximately 20 million
explicit ratings (ranging from 1 to 5) on the MovieLens website.
The corresponding KG contains 102,569 entities, 499,474 edges
and 32 relation-types.
• Book-Crossing4 contains 1 million ratings (ranging from 0
to 10) of books in the Book-Crossing community. The corresponding KG contains 25,787 entities, 60,787 edges and 18
relation-types.
• Last.FM5 contains musician listening information from a set
of 2 thousand users from Last.fm online music system. The
corresponding KG contains 9,366 entities, 15,518 edges and 60
relation-types.
• Dianping-Food is provided by Dianping.com6, which contains over 10 million interactions (including clicking, buying,
and adding to favorites) between approximately 2 million users
and 1 thousand restaurants. The corresponding KG contains
28,115 entities, 160,519 edges and 7 relation-types.
The statistics of the four datasets are shown in Table 2.
We compare the proposed KGNN-LS model with the following
baselines for recommender systems, in which the first two baselines
are KG-free while the rest are all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.
• SVD is a classic CF-based model using inner product to
model user-item interactions. We use the unbiased version (i.e.,
the predicted engaging probability is modeled as yuv = u⊤v).
The dimension and learning rate for the four datasets are set
as: d = 8, η = 0.5 for MovieLens-20M, Book-Crossing; d = 8,
η = 0.1 for Last.FM; d = 32, η = 0.1 for Dianping-Food.
2 
3 
4 
5 
6 
MovieLens-20M
Book-Crossing
Dianping-Food
R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100
0.036 0.124 0.277
0.027 0.046 0.077
0.029 0.098 0.240
0.039 0.152 0.329
0.039 0.121 0.271
0.033 0.062 0.092
0.030 0.103 0.263
0.043 0.156 0.332
LibFM + TransE 0.041 0.125 0.280
0.037 0.064 0.097
0.032 0.102 0.259
0.044 0.161 0.343 0.455
0.022 0.077 0.160
0.022 0.041 0.064
0.014 0.052 0.116
0.023 0.102 0.256
0.034 0.107 0.244
0.028 0.051 0.079
0.023 0.070 0.180
0.034 0.138 0.305
0.045 0.130 0.278
0.036 0.074 0.107
0.032 0.101 0.242
0.040 0.155 0.328
0.043 0.155 0.321 0.458 0.045 0.082 0.117 0.149 0.044 0.122 0.277 0.370 0.047 0.170 0.340 0.487
Table 3: The results of Recall@K in top-K recommendation.
Restaurant
LibFM + TransE
Table 4: The results of AUC in CTR prediction.
• LibFM is a widely used feature-based factorization model
for CTR prediction. We concatenate user ID and item ID as input for LibFM. The dimension is set as {1, 1, 8} and the number
of training epochs is 50 for all datasets.
• LibFM + TransE extends LibFM by attaching an entity representation learned by TransE to each user-item pair. The
dimension of TransE is 32 for all datasets.
• PER is a representative of path-based methods, which
treats the KG as heterogeneous information networks and extracts meta-path based features to represent the connectivity
between users and items. We use manually designed “useritem-attribute-item” as meta-paths, i.e., “user-movie-directormovie”, “user-movie-genre-movie”, and “user-movie-star-movie”
for MovieLens-20M; “user-book-author-book” and “user-bookgenre-book” for Book-Crossing, “user-musician-date_of_birthmusician” (date of birth is discretized), “user-musician-countrymusician”, and “user-musician-genre-musician” for Last.FM;
“user-restaurant-dish-restaurant”, “user-restaurant-business_arearestaurant”, “user-restaurant-tag-restaurant” for Dianping-Food.
The settings of dimension and learning rate are the same as
• CKE is a representative of embedding-based methods,
which combines CF with structural, textual, and visual knowledge in a unified framework. We implement CKE as CF plus
a structural knowledge module in this paper. The dimension
of embedding for the four datasets are 64, 128, 64, 64. The
training weight for KG part is 0.1 for all datasets. The learning
rate are the same as in SVD.
• RippleNet is a representative of hybrid methods, which
is a memory-network-like approach that propagates users’
preferences on the KG for recommendation. For RippleNet,
d = 8, H = 2, λ1 = 10−6, λ2 = 0.01, η = 0.01 for MovieLens-
20M; d = 16, H = 3, λ1 = 10−5, λ2 = 0.02, η = 0.005 for
Last.FM; d = 32, H = 2, λ1 = 10−7, λ2 = 0.02, η = 0.01 for
Dianping-Food.
Validating the Connection between G and Y
To validate the connection between the knowledge graph G and
user-item interaction Y, we conduct an empirical study where we
investigate the correlation between the shortest path distance of two
randomly sampled items in the KG and whether they have common
user(s) in the dataset, that is there exist user(s) that interacted with
both items. For MovieLens-20M and Last.FM, we randomly sample
ten thousand item pairs that have no common users and have at
least one common user, respectively, then count the distribution of
their shortest path distances in the KG. The results are presented in
Figure 3, which clearly show that if two items have common user(s)
in the dataset, they are likely to be more close in the KG. For example,
if two movies have common user(s) in MovieLens-20M, there is a
probability of 0.92 that they will be within 2 hops in the KG, while
the probability is 0.80 if they have no common user. This finding
empirically demonstrates that exploiting the proximity structure
of the KG can assist making recommendations. This also justifies
our motivation to use label smoothness regularization to help learn
entity representations.
Comparison with Baselines. We evaluate our method in two
experiment scenarios: (1) In top-K recommendation, we use the
trained model to select K items with highest predicted click probability for each user in the test set, and choose Recall@K to evaluate
the recommended sets. (2) In click-through rate (CTR) prediction,
we apply the trained model to predict each piece of user-item pair in
the test set (including positive items and randomly selected negative
items). We use AUC as the evaluation metric in CTR prediction.
The results of top-K recommendation and CTR prediction are
presented in Tables 3 and 4, respectively, which show that KGNN-
LS outperforms baselines by a significant margin. For example,
the AUC of KGNN-LS surpasses baselines by 5.1%, 6.9%, 8.3%, and
4.3% on average in MovieLens-20M, Book-Crossing, Last.FM, and
Dianping-Food datasets, respectively.
We also show daily performance of KGNN-LS and baselines on
Dianping-Food to investigate performance stability. Figure 4 shows
their AUC score from September 1, 2018 to September 30, 2018. We
notice that the curve of KGNN-LS is consistently above baselines
over the test period; Moreover, the performance of KGNN-LS is also
LibFM+TransE
Figure 4: Daily AUC of all methods on
Dianping-Food in September 2018.
Figure 5: Effectiveness of LS regularization on Last.FM.
# KG triples
running time (s)
LibFM+TransE
Figure 6: Running time of all methods
w.r.t. KG size on MovieLens-20M.
LibFM+TransE
Table 5: AUC of all methods w.r.t. the ratio of training set r.
with low variance, which suggests that KGNN-LS is also robust and
stable in practice.
Effectiveness of LS Regularization. Is the proposed LS regularization helpful in improving the performance of GNN? To study the
effectiveness of LS regularization, we fix the dimension of hidden
layers as 4, 8, and 16, then vary λ from 0 to 5 to see how performance changes. The results of R@10 in Last.FM dataset are plotted
in Figure 5. It is clear that the performance of KGNN-LS with a
non-zero λ is better than λ = 0 (the case of Wang et al. ), which
justifies our claim that LS regularization can assist learning the
edge weights in a KG and achieve better generalization in recommender systems. But note that a too large λ is less favorable, since it
overwhelms the overall loss and misleads the direction of gradients.
According to the experiment results, we find that a λ between 0.1
and 1.0 is preferable in most cases.
Results in cold-start scenarios. One major goal of using KGs
in recommender systems is to alleviate the sparsity issue. To investigate the performance of KGNN-LS in cold-start scenarios, we
vary the size of training set of MovieLens-20M from r = 100% to
r = 20% (while the validation and test set are kept fixed), and report
the results of AUC in Table 5. When r = 20%, AUC decreases by
8.4%, 5.9%, 5.4%, 3.6%, 2.8%, and 4.1% for the six baselines compared to the model trained on full training data (r = 100%), but the
performance decrease of KGNN-LS is only 1.8%. This demonstrates
that KGNN-LS still maintains predictive performance even when
user-item interactions are sparse.
Hyper-parameters Sensitivity. We first analyze the sensitivity
of KGNN-LS to the number of GNN layers L. We vary L from 1 to 4
while keeping other hyper-parameters fixed. The results are shown
in Table 6. We find that the model performs poorly when L = 4,
which is because a larger L will mix too many entity embeddings
MovieLens-20M
Book-Crossing
Dianping-Food
Table 6: R@10 w.r.t. the number of layers L.
MovieLens-20M
Book-Crossing
Dianping-Food
Table 7: R@10 w.r.t. the dimension of hidden layers d.
in a given entity, which over-smoothes the representation learning
on KGs. KGNN-LS achieves the best performance when L = 1 or 2
in the four datasets.
We also examine the impact of the dimension of hidden layers d
on the performance of KGNN-LS. The result in shown in Table 7.
We observe that the performance is boosted with the increase ofd at
the beginning, because more bits in hidden layers can improve the
model capacity. However, the performance drops when d further
increases, since a too large dimension may overfit datasets. The
best performance is achieved when d = 8 ∼64.
Running Time Analysis
We also investigate the running time of our method with respect to
the size of KG. We run experiments on a Microsoft Azure virtual machine with 1 NVIDIA Tesla M60 GPU, 12 Intel Xeon CPUs (E5-2690
v3 @2.60GHz), and 128GB of RAM. The size of the KG is increased
by up to five times the original one by extracting more triples from
Satori, and the running times of all methods on MovieLens-20M are
reported in Figure 6. Note that the trend of a curve matters more
than the real values, since the values are largely dependent on the
minibatch size and the number of epochs (yet we did try to align
the configurations of all methods). The result show that KGNN-LS
exhibits strong scalability even when the KG is large.
CONCLUSION AND FUTURE WORK
In this paper, we propose knowledge-aware graph neural networks
with label smoothness regularization for recommendation. KGNN-
LS applies GNN architecture to KGs by using user-specific relation
scoring functions and aggregating neighborhood information with
different weights. In addition, the proposed label smoothness constraint and leave-one-out loss provide strong regularization for
learning the edge weights in KGs. We also discuss how KGs benefit recommender systems and how label smoothness can assist
learning the edge weights. Experiment results show that KGNN-
LS outperforms state-of-the-art baselines in four recommendation
scenarios and achieves desirable scalability with respect to KG size.
In this paper, LS regularization is proposed for recommendation
task with KGs. It is interesting to examine the LS assumption on
other graph tasks such as link prediction and node classification.
Investigating the theoretical relationship between feature propagation and label propagation is also a promising direction.
Acknowledgements. This research has been supported in part
by NSF OAC-1835598, DARPA MCS, ARO MURI, Boeing, Docomo,
Hitachi, Huawei, JD, Siemens, and Stanford Data Science Initiative.