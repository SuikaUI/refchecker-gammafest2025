Published as a conference paper at ICLR 2019
DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH
Hanxiao Liu∗
 
Karen Simonyan
 
Yiming Yang
 
This paper addresses the scalability challenge of architecture search by formulating
the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space,
our method is based on the continuous relaxation of the architecture representation,
allowing efﬁcient search of the architecture using gradient descent. Extensive
experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that
our algorithm excels in discovering high-performance convolutional architectures
for image classiﬁcation and recurrent architectures for language modeling, while
being orders of magnitude faster than state-of-the-art non-differentiable techniques.
Our implementation has been made publicly available to facilitate further research
on efﬁcient architecture search algorithms.
INTRODUCTION
Discovering state-of-the-art neural network architectures requires substantial effort of human experts.
Recently, there has been a growing interest in developing algorithmic solutions to automate the
manual process of architecture design. The automatically searched architectures have achieved highly
competitive performance in tasks such as image classiﬁcation and object detection .
The best existing architecture search algorithms are computationally demanding despite their remarkable performance. For example, obtaining a state-of-the-art architecture for CIFAR-10 and ImageNet
required 2000 GPU days of reinforcement learning (RL) or 3150 GPU days of
evolution . Several approaches for speeding up have been proposed, such as imposing a particular structure of the search space , weights or performance prediction
for each individual architecture and weight sharing/inheritance
across multiple architectures , but the fundamental challenge of scalability remains. An inherent cause of inefﬁciency for the
dominant approaches, e.g. based on RL, evolution, MCTS , SMBO or Bayesian optimization , is the fact that architecture search
is treated as a black-box optimization problem over a discrete domain, which leads to a large number
of architecture evaluations required.
In this work, we approach the problem from a different angle, and propose a method for efﬁcient
architecture search called DARTS (Differentiable ARchiTecture Search). Instead of searching over
a discrete set of candidate architectures, we relax the search space to be continuous, so that the
architecture can be optimized with respect to its validation set performance by gradient descent. The
data efﬁciency of gradient-based optimization, as opposed to inefﬁcient black-box search, allows
DARTS to achieve competitive performance with the state of the art using orders of magnitude
less computation resources. It also outperforms another recent efﬁcient architecture search method,
ENAS . Notably, DARTS is simpler than many existing approaches as it does
not involve controllers , hypernetworks or performance predictors ,
yet it is generic enough handle both convolutional and recurrent architectures.
The idea of searching architectures within a continuous domain is not new , but there are several major
∗Current afﬁliation: Google Brain.
 
Published as a conference paper at ICLR 2019
distinctions. While prior works seek to ﬁne-tune a speciﬁc aspect of an architecture, such as ﬁlter
shapes or branching patterns in a convolutional network, DARTS is able to learn high-performance
architecture building blocks with complex graph topologies within a rich search space. Moreover,
DARTS is not restricted to any speciﬁc architecture family, and is applicable to both convolutional
and recurrent networks.
In our experiments (Sect. 3) we show that DARTS is able to design a convolutional cell that achieves
2.76 ± 0.09% test error on CIFAR-10 for image classiﬁcation using 3.3M parameters, which is
competitive with the state-of-the-art result by regularized evolution obtained using
three orders of magnitude more computation resources. The same convolutional cell also achieves
26.7% top-1 error when transferred to ImageNet (mobile setting), which is comparable to the best RL
method . On the language modeling task, DARTS efﬁciently discovers a recurrent
cell that achieves 55.7 test perplexity on Penn Treebank (PTB), outperforming both extensively tuned
LSTM and all the existing automatically searched cells based on NAS and ENAS .
Our contributions can be summarized as follows:
• We introduce a novel algorithm for differentiable network architecture search based on
bilevel optimization, which is applicable to both convolutional and recurrent architectures.
• Through extensive experiments on image classiﬁcation and language modeling tasks we show
that gradient-based architecture search achieves highly competitive results on CIFAR-10
and outperforms the state of the art on PTB. This is a very interesting result, considering
that so far the best architecture search methods used non-differentiable search techniques,
e.g. based on RL or evolution .
• We achieve remarkable efﬁciency improvement (reducing the cost of architecture discovery
to a few GPU days), which we attribute to the use of gradient-based optimization as opposed
to non-differentiable search techniques.
• We show that the architectures learned by DARTS on CIFAR-10 and PTB are transferable
to ImageNet and WikiText-2, respectively.
The implementation of DARTS is available at 
DIFFERENTIABLE ARCHITECTURE SEARCH
We describe our search space in general form in Sect. 2.1, where the computation procedure for an
architecture (or a cell in it) is represented as a directed acyclic graph. We then introduce a simple
continuous relaxation scheme for our search space which leads to a differentiable learning objective
for the joint optimization of the architecture and its weights (Sect. 2.2). Finally, we propose an
approximation technique to make the algorithm computationally feasible and efﬁcient (Sect. 2.3).
SEARCH SPACE
Following Zoph et al. ; Real et al. ; Liu et al. , we search for a computation
cell as the building block of the ﬁnal architecture. The learned cell could either be stacked to form a
convolutional network or recursively connected to form a recurrent network.
A cell is a directed acyclic graph consisting of an ordered sequence of N nodes. Each node x(i) is
a latent representation (e.g. a feature map in convolutional networks) and each directed edge (i, j)
is associated with some operation o(i,j) that transforms x(i). We assume the cell to have two input
nodes and a single output node. For convolutional cells, the input nodes are deﬁned as the cell outputs
in the previous two layers . For recurrent cells, these are deﬁned as the input at
the current step and the state carried from the previous step. The output of the cell is obtained by
applying a reduction operation (e.g. concatenation) to all the intermediate nodes.
Each intermediate node is computed based on all of its predecessors:
o(i,j)(x(i))
Published as a conference paper at ICLR 2019
Figure 1: An overview of DARTS: (a) Operations on the edges are initially unknown. (b) Continuous
relaxation of the search space by placing a mixture of candidate operations on each edge. (c) Joint
optimization of the mixing probabilities and the network weights by solving a bilevel optimization
problem. (d) Inducing the ﬁnal architecture from the learned mixing probabilities.
A special zero operation is also included to indicate a lack of connection between two nodes. The
task of learning the cell therefore reduces to learning the operations on its edges.
CONTINUOUS RELAXATION AND OPTIMIZATION
Let O be a set of candidate operations (e.g., convolution, max pooling, zero) where each operation
represents some function o(·) to be applied to x(i). To make the search space continuous, we relax
the categorical choice of a particular operation to a softmax over all possible operations:
¯o(i,j)(x) =
exp(α(i,j)
o′∈O exp(α(i,j)
where the operation mixing weights for a pair of nodes (i, j) are parameterized by a vector α(i,j) of
dimension |O|. The task of architecture search then reduces to learning a set of continuous variables
, as illustrated in Fig. 1. At the end of search, a discrete architecture can be obtained by
replacing each mixed operation ¯o(i,j) with the most likely operation, i.e., o(i,j) = argmaxo∈O α(i,j)
In the following, we refer to α as the (encoding of the) architecture.
After relaxation, our goal is to jointly learn the architecture α and the weights w within all the mixed
operations (e.g. weights of the convolution ﬁlters). Analogous to architecture search using RL or evolution 
where the validation set performance is treated as the reward or ﬁtness, DARTS aims to optimize the
validation loss, but using gradient descent.
Denote by Ltrain and Lval the training and the validation loss, respectively. Both losses are determined not only by the architecture α, but also the weights w in the network. The goal for architecture
search is to ﬁnd α∗that minimizes the validation loss Lval(w∗, α∗), where the weights w∗associated
with the architecture are obtained by minimizing the training loss w∗= argminw Ltrain(w, α∗).
This implies a bilevel optimization problem with
α as the upper-level variable and w as the lower-level variable:
Lval(w∗(α), α)
w∗(α) = argminw Ltrain(w, α)
The nested formulation also arises in gradient-based hyperparameter optimization , which is related in a sense that the architecture α
could be viewed as a special type of hyperparameter, although its dimension is substantially higher
than scalar-valued hyperparameters such as the learning rate, and it is harder to optimize.
Published as a conference paper at ICLR 2019
Algorithm 1: DARTS – Differentiable Architecture Search
Create a mixed operation ¯o(i,j) parametrized by α(i,j) for each edge (i, j)
while not converged do
1. Update architecture α by descending ∇αLval(w −ξ∇wLtrain(w, α), α)
(ξ = 0 if using ﬁrst-order approximation)
2. Update weights w by descending ∇wLtrain(w, α)
Derive the ﬁnal architecture based on the learned α.
APPROXIMATE ARCHITECTURE GRADIENT
Evaluating the architecture gradient exactly can be prohibitive due to the expensive inner optimization.
We therefore propose a simple approximation scheme as follows:
∇αLval(w∗(α), α)
≈∇αLval(w −ξ∇wLtrain(w, α), α)
where w denotes the current weights maintained by the algorithm, and ξ is the learning rate for a step
of inner optimization. The idea is to approximate w∗(α) by adapting w using only a single training
step, without solving the inner optimization (equation 4) completely by training until convergence.
Related techniques have been used in meta-learning for model transfer , gradientbased hyperparameter tuning and unrolled generative adversarial networks
 . Note equation 6 will reduce to ∇αLval(w, α) if w is already a local optimum for
the inner optimization and thus ∇wLtrain(w, α) = 0.
The iterative procedure is outlined in Alg. 1. While we are not currently aware of the convergence
guarantees for our optimization algorithm, in practice it is able to reach a ﬁxed point with a suitable
choice of ξ1. We also note that when momentum is enabled for weight optimisation, the one-step
unrolled learning objective in equation 6 is modiﬁed accordingly and all of our analysis still applies.
Applying chain rule to the approximate architecture gradient (equation 6) yields
∇αLval(w′, α) −ξ∇2
α,wLtrain(w, α)∇w′Lval(w′, α)
where w′ = w−ξ∇wLtrain(w, α) denotes the weights for a one-step forward model. The expression
above contains an expensive matrix-vector product in its second term. Fortunately, the complexity
can be substantially reduced using the ﬁnite difference approximation. Let ϵ be a small scalar2 and
w± = w ± ϵ∇w′Lval(w′, α). Then:
α,wLtrain(w, α)∇w′Lval(w′, α) ≈∇αLtrain(w+, α) −∇αLtrain(w−, α)
Evaluating the ﬁnite difference requires only two forward passes for the weights and two backward
passes for α, and the complexity is reduced from O(|α||w|) to O(|α| + |w|).
First-order Approximation
When ξ = 0, the second-order derivative in equation 7 will disappear.
In this case, the architecture gradient is given by ∇αLval(w, α), corresponding to the simple heuristic
of optimizing the validation loss by assuming the current w is the same as w∗(α). This leads to some
speed-up but empirically worse performance, according to our experimental results in Table 1 and
Table 2. In the following, we refer to the case of ξ = 0 as the ﬁrst-order approximation, and refer to
the gradient formulation with ξ > 0 as the second-order approximation.
DERIVING DISCRETE ARCHITECTURES
To form each node in the discrete architecture, we retain the top-k strongest operations (from distinct
nodes) among all non-zero candidate operations collected from all the previous nodes. The strength
of an operation is deﬁned as
exp(α(i,j)
o′∈O exp(α(i,j)
). To make our derived architecture comparable with
1A simple working strategy is to set ξ equal to the learning rate for w’s optimizer.
2We found ϵ = 0.01/∥∇w′Lval(w′, α)∥2 to be sufﬁciently accurate in all of our experiments.
Published as a conference paper at ICLR 2019
Architecture ( )
Weights (w)
Figure 2: Learning dynamics of our iterative algorithm
when Lval(w, α) = αw −2α + 1 and Ltrain(w, α) =
w2−2αw+α2, starting from (α(0), w(0)) = (2, −2). The
analytical solution for the corresponding bilevel optimization problem is (α∗, w∗) = (1, 1), which is highlighted
in the red circle. The dashed red line indicates the feasible set where constraint equation 4 is satisﬁed exactly
(namely, weights in w are optimal for the given architecture α). The example shows that a suitable choice of ξ
helps to converge to a better local optimum.
those in the existing works, we use k = 2 for convolutional cells and k = 1 for recurrent cells .
The zero operations are excluded in the above for two reasons. First, we need exactly k non-zero
incoming edges per node for fair comparison with the existing models. Second, the strength of the
zero operations is underdetermined, as increasing the logits of zero operations only affects the scale
of the resulting node representations, and does not affect the ﬁnal classiﬁcation outcome due to the
presence of batch normalization .
EXPERIMENTS AND RESULTS
Our experiments on CIFAR-10 and PTB consist of two stages, architecture search (Sect. 3.1) and
architecture evaluation (Sect. 3.2). In the ﬁrst stage, we search for the cell architectures using DARTS,
and determine the best cells based on their validation performance. In the second stage, we use these
cells to construct larger architectures, which we train from scratch and report their performance on
the test set. We also investigate the transferability of the best cells learned on CIFAR-10 and PTB by
evaluating them on ImageNet and WikiText-2 (WT2) respectively.
ARCHITECTURE SEARCH
SEARCHING FOR CONVOLUTIONAL CELLS ON CIFAR-10
We include the following operations in O: 3 × 3 and 5 × 5 separable convolutions, 3 × 3 and 5 × 5
dilated separable convolutions, 3 × 3 max pooling, 3 × 3 average pooling, identity, and zero. All
operations are of stride one (if applicable) and the convolved feature maps are padded to preserve
their spatial resolution. We use the ReLU-Conv-BN order for convolutional operations, and each
separable convolution is always applied twice .
Our convolutional cell consists of N = 7 nodes, among which the output node is deﬁned as the
depthwise concatenation of all the intermediate nodes (input nodes excluded). The rest of the setup
follows Zoph et al. ; Liu et al. ; Real et al. , where a network is then formed by
stacking multiple cells together. The ﬁrst and second nodes of cell k are set equal to the outputs of cell
k−2 and cell k−1, respectively, and 1×1 convolutions are inserted as necessary. Cells located at the
1/3 and 2/3 of the total depth of the network are reduction cells, in which all the operations adjacent
to the input nodes are of stride two. The architecture encoding therefore is (αnormal, αreduce), where
αnormal is shared by all the normal cells and αreduce is shared by all the reduction cells.
Detailed experimental setup for this section can be found in Sect. A.1.1.
SEARCHING FOR RECURRENT CELLS ON PENN TREEBANK
Our set of available operations includes linear transformations followed by one of tanh, relu, sigmoid
activations, as well as the identity mapping and the zero operation. The choice of these candidate
operations follows Zoph & Le ; Pham et al. .
Our recurrent cell consists of N = 12 nodes. The very ﬁrst intermediate node is obtained by linearly
transforming the two input nodes, adding up the results and then passing through a tanh activation
Published as a conference paper at ICLR 2019
Best Valid Error So Far (%)
DARTS (run 1)
DARTS (run 2)
DARTS (run 3)
DARTS (run 4)
AmoebaNet-A
Valid Error (%)
AmoebaNet-A
Best Valid Perplexity So Far
Penn Treebank
DARTS (run 1)
DARTS (run 2)
DARTS (run 3)
DARTS (run 4)
Valid Perplexity
Penn Treebank
Figure 3: Search progress of DARTS for convolutional cells on CIFAR-10 and recurrent cells on
Penn Treebank. We keep track of the most recent architectures over time. Each architecture snapshot
is re-trained from scratch using the training set (for 100 epochs on CIFAR-10 and for 300 epochs on
PTB) and then evaluated on the validation set. For each task, we repeat the experiments for 4 times
with different random seeds, and report the median and the best (per run) validation performance of
the architectures over time. As references, we also report the results (under the same evaluation setup;
with comparable number of parameters) of the best existing cells discovered using RL or evolution,
including NASNet-A , AmoebaNet-A (3150 GPU days) and ENAS (0.5 GPU day) .
function, as done in the ENAS cell . The rest of the cell is learned. Other settings
are similar to ENAS, where each operation is enhanced with a highway bypass and
the cell output is deﬁned as the average of all the intermediate nodes. As in ENAS, we enable batch
normalization in each node to prevent gradient explosion during architecture search, and disable it
during architecture evaluation. Our recurrent network consists of only a single cell, i.e. we do not
assume any repetitive patterns within the recurrent architecture.
Detailed experimental setup for this section can be found in Sect. A.1.2.
sep_conv_3x3
sep_conv_3x3
skip_connect
skip_connect
sep_conv_3x3
sep_conv_3x3
sep_conv_3x3
dil_conv_3x3
Figure 4: Normal cell learned on CIFAR-10.
max_pool_3x3
max_pool_3x3
max_pool_3x3
max_pool_3x3
max_pool_3x3
skip_connect
skip_connect
skip_connect
Figure 5: Reduction cell learned on CIFAR-10.
Figure 6: Recurrent cell learned on PTB.
ARCHITECTURE EVALUATION
To determine the architecture for ﬁnal evaluation, we run DARTS four times with different random
seeds and pick the best cell based on its validation performance obtained by training from scratch for
Published as a conference paper at ICLR 2019
a short period (100 epochs on CIFAR-10 and 300 epochs on PTB). This is particularly important for
recurrent cells, as the optimization outcomes can be initialization-sensitive (Fig. 3).
To evaluate the selected architecture, we randomly initialize its weights (weights learned during the
search process are discarded), train it from scratch, and report its performance on the test set. We
note the test set is never used for architecture search or architecture selection.
Detailed experimental setup for architecture evaluation on CIFAR-10 and PTB can be found in
Sect. A.2.1 and Sect. A.2.2, respectively. Besides CIFAR-10 and PTB, we further investigated the
transferability of our best convolutional cell (searched on CIFAR-10) and recurrent cell (searched on
PTB) by evaluating them on ImageNet (mobile setting) and WikiText-2, respectively. More details of
the transfer learning experiments can be found in Sect. A.2.3 and Sect. A.2.4.
Table 1: Comparison with state-of-the-art image classiﬁers on CIFAR-10 (lower error rate is better).
Note the search cost for DARTS does not include the selection cost (1 GPU day) or the ﬁnal evaluation
cost by training the selected architecture from scratch (1.5 GPU days).
Architecture
Test Error
Search Cost
(GPU days)
DenseNet-BC 
NASNet-A + cutout 
NASNet-A + cutout †
BlockQNN 
AmoebaNet-A 
3.34 ± 0.06
AmoebaNet-A + cutout †
AmoebaNet-B + cutout 
2.55 ± 0.05
Hierarchical evolution 
3.75 ± 0.12
PNAS 
3.41 ± 0.09
ENAS + cutout 
ENAS + cutout *
Random search baseline‡ + cutout
3.29 ± 0.15
DARTS (ﬁrst order) + cutout
3.00 ± 0.14
gradient-based
DARTS (second order) + cutout
2.76 ± 0.09
gradient-based
* Obtained by repeating ENAS for 8 times using the code publicly released by the authors. The cell for ﬁnal
evaluation is chosen according to the same selection protocol as for DARTS.
† Obtained by training the corresponding architectures using our setup.
‡ Best architecture among 24 samples according to the validation error after 100 training epochs.
Table 2: Comparison with state-of-the-art language models on PTB (lower perplexity is better). Note
the search cost for DARTS does not include the selection cost (1 GPU day) or the ﬁnal evaluation
cost by training the selected architecture from scratch (3 GPU days).
Architecture
Perplexity
Search Cost
(GPU days)
Variational RHN 
LSTM 
LSTM + skip connections 
LSTM + 15 softmax experts 
NAS 
1e4 CPU days
ENAS *
ENAS †
Random search baseline‡
DARTS (ﬁrst order)
gradient-based
DARTS (second order)
gradient-based
* Obtained using the code publicly released by the authors.
† Obtained by training the corresponding architecture using our setup.
‡ Best architecture among 8 samples according to the validation perplexity after 300 training epochs.
Published as a conference paper at ICLR 2019
Table 3: Comparison with state-of-the-art image classiﬁers on ImageNet in the mobile setting.
Architecture
Test Error (%)
Search Cost
(GPU days)
Inception-v1 
MobileNet 
ShufﬂeNet 2× (g = 3) 
NASNet-A 
NASNet-B 
NASNet-C 
AmoebaNet-A 
AmoebaNet-B 
AmoebaNet-C 
PNAS 
DARTS (searched on CIFAR-10)
gradient-based
RESULTS ANALYSIS
The CIFAR-10 results for convolutional architectures are presented in Table 1. Notably, DARTS
achieved comparable results with the state of the art while using
three orders of magnitude less computation resources . Moreover, with slightly longer search time, DARTS
outperformed ENAS by discovering cells with comparable error rates but less
parameters. The longer search time is due to the fact that we have repeated the search process four
times for cell selection. This practice is less important for convolutional cells however, because the
performance of discovered architectures does not strongly depend on initialization (Fig. 3).
Alternative Optimization Strategies
To better understand the necessity of bilevel optimization,
we investigated a simplistic search strategy, where α and w are jointly optimized over the union of
the training and validation sets using coordinate descent. The resulting best convolutional cell (out of
4 runs) yielded 4.16 ± 0.16% test error using 3.1M parameters, which is worse than random search.
In the second experiment, we optimized α simultaneously with w (without alteration) using SGD,
again over all the data available (training + validation). The resulting best cell yielded 3.56 ± 0.10%
test error using 3.0M parameters. We hypothesize that these heuristics would cause α (analogous
to hyperparameters) to overﬁt the training data, leading to poor generalization. Note that α is not
directly optimized on the training set in DARTS.
Table 2 presents the results for recurrent architectures on PTB, where a cell discovered by DARTS
achieved the test perplexity of 55.7. This is on par with the state-of-the-art model enhanced by a
mixture of softmaxes , and better than all the rest of the architectures that are either
manually or automatically discovered. Note that our automatically searched cell outperforms the
extensively tuned LSTM , demonstrating the importance of architecture search in
addition to hyperparameter search. In terms of efﬁciency, the overall cost (4 runs in total) is within 1
GPU day, which is comparable to ENAS and signiﬁcantly faster than NAS .
It is also interesting to note that random search is competitive for both convolutional and recurrent
models, which reﬂects the importance of the search space design. Nevertheless, with comparable or
less search cost, DARTS is able to signiﬁcantly improve upon random search in both cases (2.76 ±
0.09 vs 3.29 ± 0.15 on CIFAR-10; 55.7 vs 59.4 on PTB).
Results in Table 3 show that the cell learned on CIFAR-10 is indeed transferable to ImageNet. It is
worth noticing that DARTS achieves competitive performance with the state-of-the-art RL method
 while using three orders of magnitude less computation resources.
Table 4 shows that the cell identiﬁed by DARTS transfers to WT2 better than ENAS, although the
overall results are less strong than those presented in Table 2 for PTB. The weaker transferability
between PTB and WT2 (as compared to that between CIFAR-10 and ImageNet) could be explained by
the relatively small size of the source dataset (PTB) for architecture search. The issue of transferability
could potentially be circumvented by directly optimizing the architecture on the task of interest.
Published as a conference paper at ICLR 2019
Table 4: Comparison with state-of-the-art language models on WT2.
Architecture
Perplexity
Search Cost
(GPU days)
LSTM + augmented loss 
LSTM + continuous cache pointer 
LSTM 
LSTM + skip connections 
LSTM + 15 softmax experts 
ENAS † (searched on PTB)
DARTS (searched on PTB)
gradient-based
† Obtained by training the corresponding architecture using our setup.
CONCLUSION
We presented DARTS, a simple yet efﬁcient architecture search algorithm for both convolutional and
recurrent networks. By searching in a continuous space, DARTS is able to match or outperform the
state-of-the-art non-differentiable architecture search methods on image classiﬁcation and language
modeling tasks with remarkable efﬁciency improvement by several orders of magnitude.
There are many interesting directions to improve DARTS further. For example, the current method
may suffer from discrepancies between the continuous architecture encoding and the derived discrete
architecture. This could be alleviated, e.g., by annealing the softmax temperature (with a suitable
schedule) to enforce one-hot selection. It would also be interesting to investigate performance-aware
architecture derivation schemes based on the shared parameters learned during the search process.
ACKNOWLEDGEMENTS
The authors would like to thank Zihang Dai, Hieu Pham and Zico Kolter for useful discussions.