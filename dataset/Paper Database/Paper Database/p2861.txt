Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762–4779
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
: Commonsense Transformers
for Automatic Knowledge Graph Construction
Antoine Bosselut ♦♠Hannah Rashkin ♦♠Maarten Sap ♦♠Chaitanya Malaviya ♦
Asli Celikyilmaz ♣Yejin Choi ♦♠
♦Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA
♠Paul G. Allen School of Computer Science & Engineering, Seattle, WA, USA
♣Microsoft Research, Redmond, WA, USA
We present the ﬁrst comprehensive study
on automatic knowledge base construction
for two prevalent commonsense knowledge
graphs: ATOMIC and ConceptNet .
Contrary to
many conventional KBs that store knowledge
with canonical templates, commonsense KBs
only store loosely structured open-text descriptions of knowledge.
We posit that an
important step toward automatic commonsense completion is the development of generative models of commonsense knowledge,
and propose COMmonsEnse Transformers
) that learn to generate rich and
diverse commonsense descriptions in natural
Despite the challenges of commonsense modeling, our investigation reveals
promising results when implicit knowledge
from deep pre-trained language models is
transferred to generate explicit knowledge in
commonsense knowledge graphs. Empirical
results demonstrate that COMET is able to
generate novel knowledge that humans rate as
high quality, with up to 77.5% (ATOMIC) and
91.7% (ConceptNet) precision at top 1, which
approaches human performance for these resources. Our ﬁndings suggest that using generative commonsense models for automatic
commonsense KB completion could soon be
a plausible alternative to extractive methods.
Introduction
When reading text, humans make commonsense
inferences that frame their understanding of the
narrative being presented. For machines to achieve
this capability, they must be able to acquire relevant and correct commonsense for an unbounded
set of situations. In this work, we cast commonsense acquisition as knowledge base construction
and investigate whether large-scale language models can effectively learn to generate the knowledge
puts their
arms around
goes to the
Commonsense Knowledge Bases
(seen events)
Automatic KB
Completion
Unseen Events
HasSubevent
HasSubevent
ConceptNet
Figure 1: COMET
learns from an existing knowledge
base (solid lines) to be able to generate novel nodes and
edges (dashed lines).
necessary to automatically construct a commonsense knowledge base (KB).
Automatic KB construction is a long-standing
goal of artiﬁcial intelligence research due to the
difﬁculty of achieving high concept coverage in
high-precision curated KBs . Previous work has developed models capable of reading and extracting semi-structured text
 and unstructured text into relational schemas that can be queried for downstream applications. A common thread of these
approaches, however, is the focus on encyclopedic knowledge, which lends itself to a well-deﬁned
space of entities and relations that can be modeled.
Commonsense knowledge, however, does not
cleanly ﬁt into a schema comparing two entities
with a known relation, leading current approaches
Commonsense Transformer (COMeT)
Multi-headed Attention
Transformer Block
Multi-headed Attention
Layer Normalization
Layer Normalization
Feedforward Network
e0 p0 e1 p1
e |s| p |s|
PersonX sails … <xNeed>
[MASK][MASK]
Concatenation
Linear Projection
Figure 2: Model diagram. (a) In the multi-headed attention module, the key, value, and query all pass through a
head-speciﬁc projection before a scaled dot-product attention is computed between them. The outputs of the heads
are concatenated and projected. (b) Inside the transformer block, the outputs of all the previous layer blocks from
earlier time steps are input to the multi-headed attention with the preceding block for the current time step as the
query. (c) Each token is an input to a ﬁrst-layer block along with all preceding tokens. Dotted lines indicate outputs
to all future blocks in the next layer and inputs from all preceding blocks in the previous layer.
to model “entities" as natural language phrases
and relations as any concept that can link them
 .
OpenIE approaches display this property of open text entities and relations , but being extractive, they only capture knowledge that is explicitly mentioned in text, limiting their applicability
for capturing commonsense knowledge, which is
often implicit .
Meanwhile, recent progress in training deep
contextualized language models 
provides an opportunity to explore beyond extractive methods as an avenue for commonsense KB
construction. These large-scale language models
display impressive performance when their underlying representations are tuned to solve end tasks,
achieving state-of-the-art results on a variety of
complex problems. In this work, we deﬁne the
COMmonsEnse Transformer (COMET
constructs commonsense KBs by using existing
tuples as a seed set of knowledge on which to
train. Using this seed set, a pre-trained language
model learns to adapt its learned representations to
knowledge generation, and produces novel tuples
that are high quality.
We summarize our contributions in this work as
follows. First, we develop a generative approach
to knowledge base construction. A model must
learn to produce new nodes and identify edges between existing nodes by generating phrases that
coherently complete an existing seed phrase and
relation type1. Second, we develop a framework
for using large-scale transformer language models
to learn to produce commonsense knowledge tuples2. Finally, we perform an empirical study on
the quality, novelty, and diversity of the commonsense knowledge produced by our approach for
two domains, ATOMIC and ConceptNet, as well as
an efﬁciency study on the number of seed tuples
needed to learn an effective knowledge model.
The results indicate that COMET is able to produce high quality tuples as human judges ﬁnd that
77.5% of generated tuples for ATOMIC events and
91.7% of generated tuples for ConceptNet relations are correct.
Learning to Generate Commonsense
COMET is an adaptation framework for constructing commonsense knowledge bases from language
models by training the language model on a seed
set of knowledge tuples.
These tuples provide
COMET with the KB structure and relations that
must be learned, and COMET learns to adapt the
language model representations learned from pretraining to add novel nodes and edges to the seed
knowledge graph.
1Demo is available at 
allenai.org/
 
atcbosselut/comet-commonsense
More speciﬁcally, the problem assumes COMET is
given a training knowledge base of natural language tuples in {s, r, o} format, where s is the
phrase subject of the tuple, r is the relation of the
tuple, and o is the phrase object of the tuple. For
example, a ConceptNet tuple relating to “taking
a nap" would be: (s=“take a nap", r=Causes,
o=“have energy"). The task is to generate o given
s and r as inputs.
We deﬁne Xs = {xs
0, ..., xs
|s|} as the
tokens that make up the subject of the relation,
0, ..., xr
|r|} as the tokens that make up
the relation of the tuple, and Xo = {xo
0, ..., xo
as the tokens that make up the object of the tuple.
The embedding for any word x is denoted as e.
Transformer Language Model
While COMET is agnostic to the language model
with which it is initialized, in this work, we use
the transformer language model architecture introduced in Radford et al. (GPT), which
uses multiple transformer blocks of multi-headed
scaled dot product attention and fully connected
layers to encode input text .
Figure 2 depicts different components of the GPT
architecture and we deﬁne each component in
more depth below.
Transformer Block
As shown in Figure 2(b),
each transformer layer l contains an architecturally
identical transformer block (though with unique
trainable parameters) that applies the following
transformations to the input to the block:
˜gl = MULTIATTN(hl−1)
gl = LAYERNORM(˜gl + hl−1)
˜hl = FFN(gl)
hl = LAYERNORM(˜hl + gl)
multi-headed
selfattention mechanism (deﬁned below), FFN is
a two-layer feed-forward network, and LAYER-
NORM represents a layer normalization operation that is applied to the output of
the self-attention and the feedforward network.
Note that the inputs to the LAYERNORM operations contain a residual connection that sums the
output of and input to the previous operation.
Multi-headed Attention
The multi-headed attention module of each transformer block, shown
in Figure 2(a), is identical to the one originally de-
ﬁned by Vaswani et al. . The attention function receives three inputs, a query Q, key K, and
value V . The attention is made of multiple heads
that each compute a unique scaled dot product attention distribution over V using Q and K:
ATTENTION(Q, K, V ) = softmax
where dk is the dimensionality of the input vectors
representing the query, key and value. For each
of the heads, Q, K, and V are uniquely projected
prior to the attention being computed:
Hi = ATTENTION(QW Q
where Hi is the output of a single attention head
i , and W V
are head-speciﬁc projections for Q, K, and V , respectively. The outputs
of the attention heads Hi are then concatenated:
MULTIH(Q, K, V) = [H1; ...; Hb]W O
where W O is an output projection of the concatenated outputs of the attention heads. As shown in
Figure 2(c), we follow Radford et al. and
use the output of the previous layer’s transformer
block as the query input for the multi-headed attention of the next block. The keys and values are
outputs of the previous layer’s block for all preceding time steps:
MULTIATTN(hl−1
) = MULTIH(hl−1
where hl−1
= {hl−1}<t is the set of previous
layer transformer block outputs for time steps preceding t.
Input Encoder
As input to the model, we represent a knowledge tuple {s, r, o} as a concatenated
sequence of the words of each item of the tuple:
X = {Xs, Xr, Xo}
Since the transformer (a self-attention model) has
no concept of ordering of tokens, a position embedding pt is initialized for each absolute position
in the sequence . For any
input word xt ∈X, our encoding of the input is
mask tokens
mask tokens
mask tokens
ATOMIC Input Template and ConceptNet Relation-only Input Template
ConceptNet Relation to Language Input Template
PersonX goes to the mall [MASK]
to buy clothes
go to mall [MASK] [MASK] has prerequisite [MASK] have money
Figure 3: Input token setup for training conﬁgurations.
For the ATOMIC dataset, the tokens of the subject, Xs
(e.g., PersonX goes to the mall) are followed by masking tokens, which is followed by a single relation token
Xr (e.g., xIntent), and then the object tokens Xo
(e.g., to buy clothes). The model receives the same input for ConceptNet, except that a second set of masking tokens separate Xr and Xo because Xr can have a
variable number of tokens for ConceptNet (§5.2)
the sum of its word embedding, et with a position
embedding encoding its absolute position in the
sequence X:
t = et + pt
where pt is the position embedding for time step t,
and h0 is the input to the ﬁrst transformer layer.
Training COMET
COMET is trained to learn to produce the phrase
object o of a knowledge tuple given the tuple’s
phrase subject s and relation r. More speciﬁcally,
given the concatenation of the tokens of s and r:
[Xs, Xr] as input, the model must learn to generate the tokens of o: Xo (See §2.1 for deﬁnitions of
these variables).
Loss Function
To achieve this goal, COMET is
trained to maximize the conditional loglikelihood
of predicting the phrase object tokens, Xo:
|s|+|r|+|o|
log P(xt|x<t)
where |s|, |r|, and |o| are the number of tokens
in the subject phrase, relation, and object phrase,
respectively. Figure 3 outlines how the tokens in s,
r, and o are organized for different training tasks.
COMET relies on a seed set of knowledge tuples from an existing KB to learn to produce commonsense knowledge.
In this work,
we use ATOMIC and ConceptNet as knowledge
seed sets, but other commonsense knowledge resources could have been used as well as COMET is
domain-agnostic.
Initialization
Parameters are initialized to the ﬁnal language model weights from Radford et al.
 . Additional special tokens that are added
to the vocabulary for ﬁne tuning (e.g., relation embeddings such as oReact for ATOMIC and IsA
for ConceptNet) are initialized by sampling from
the standard normal distribution.
Hyperparameters
Following Radford et al.
 ’s design of the GPT model, we initialize
COMET with 12 layers, 768-dimensional hidden
states, and 12 attention heads. We use a dropout
rate of 0.1 and use GeLU units as activation functions. During training, our batch size is 64. Other dataset-speciﬁc
hyperparameters are provided in Appendix A.1.
ATOMIC Experiments
The ATOMIC dataset3, released by Sap et al.
 , contains 877K tuples covering a variety
of social commonsense knowledge around speciﬁc
event prompts (e.g., “X goes to the store”). Specifically, ATOMIC distills its commonsense in nine
dimensions, covering the event’s causes (e.g., “X
needs to drive there”), its effects on the agent (e.g.,
“to get food”) and its effect on other direct (or
implied) participants (e.g., “Others will be fed”).
More details about ATOMIC can be found in Appendix D. For our experiments, ATOMIC events
(e.g., “X goes to the store”) are phrase subjects, s,
the dimension (e.g., xIntent) is the phrase relation, r, and the causes/effects (e.g., “to get food”)
are phrase objects, o. We use the training splits
from Sap et al. , resulting in 710k training,
80k development, and 87k test tuples respectively.
Following Sap et al. , we evaluate our method using BLEU-2 as an automatic
evaluation metric. We also report the perplexity
of the model on its gold generations. The remaining automatic metrics in Table 1 measure the proportion of generated tuples and generated objects
which are not in the training set. We report the
proportion of all generated tuples that are novel
(% N/T sro) and that have a novel object (% N/T
o)4. To show that these novel objects are diverse
(i.e., the same novel object is not the only one being generated), we also report the number of novel
3 
~msap/atomic/
4a new o represents a new node in the knowledge graph
9ENC9DEC 
NearestNeighbor 
Event2(IN)VOLUN 
Event2PERSONX/Y 
Event2PRE/POST 
COMET (- pretrain)
Table 1: Automatic evaluations of quality and novelty for generations of ATOMIC commonsense. No novelty
scores are reported for the NearestNeighbor baseline because all retrieved sequences are in the training set.
9Enc9Dec 
Event2(In)voluntary 
Event2PersonX/Y 
Event2Pre/Post 
COMET (- pretrain)
Table 2: Human score of generations of ATOMIC commonsense. We present comparisons to the baselines from
Sap et al. . Underlined results are those where COMET is not signiﬁcantly better at p < 0.05
objects as a function of the set of unique objects
produced for all test set events (% N/U o).
Finally, we perform a human evaluation using
workers from Amazon Mechanical Turk (AMT).
Workers are asked to identify whether a model
generation of ATOMIC commonsense adequately
completes a plausible tuple of phrase subject, relation, and phrase object. Following the setup of Sap
et al. , we evaluate 100 randomly selected
events from the test set. For each event and relation type, 10 candidates are generated using beam
search and the full beam is evaluated by ﬁve different workers. Overall, n=5000 ratings are produced
per relation (100 events × 5 workers × 10 candidates). The reported Avg in Table 2 is an average of these scores, yielding n=45000 total ratings
for each model. We use Pitman’s test with 100k permutations to test for statistical signiﬁcance. Because 50 different hypotheses are tested (9 relations + the total), the Holm-
Bonferroni method is used to correct
signiﬁcance thresholds. Example events from the
development set and their generated phrase objects
are available in Table 5.
We report the performance of our
method against the models trained in Sap et al.
 that use LSTM sequence-to-sequence models to encode the input subject and relation and produce an output object.
To evaluate how pre-training on a
large corpus helps the model learn to produce
knowledge, we train a version of COMET that is
not initialized with pre-trained weights (COMET (pretrain)). We also evaluate the data efﬁciency of
our method by training models on different proportions of the training data.
Finally, because
the ultimate goal of our method is to be able
to perform high-quality, diverse knowledge base
construction, we explore how various decoding
schemes affect the quality of candidate knowledge
tuples. We present the effect of the following generation strategies: argmax greedy decoding, beam
search with beam sizes, b=2, 5, 10, and top-k sampling with k = 5, 10. For each decoding method,
we conduct the human evaluation on the number
of ﬁnal candidates produced by each method.
Overall performance
The BLEU-2 results in
Table 1 indicate that COMET exceeds the performance of all baselines, achieving a 51% relative
improvement over the top performing model of
Sap et al. . More interesting, however, is the
result of the human evaluation, where COMET reported a statistically signiﬁcant relative Avg performance increase of 18% over the top baseline,
5Sap et al. ’s models were trained with a different
vocabulary so a direct perplexity comparison is not possible.
6All test set s do not appear in the training set so all full
tuples must be novel.
COMET Decoding method
Top-5 random sampling (n=2500 per relation)
Top-10 random sampling (n=5000 per relation)
Beam search - 2 beams (n=1000 per relation)
Beam search - 5 beams (n=2500 per relation)
Beam search - 10 beams (n=5000 per relation)
Greedy decoding (n=500 per relation)
Human validation of gold ATOMIC
Table 3: Human evaluation testing effect of different decoding schemes on candidate tuple quality. The number of
ratings made per relation for each decoding method is provided in the ﬁrst column.
% train data
FULL (- pretrain)
FULL train
Table 4: Effect of amount of training data on automatic
evaluation of commonsense generations
Event2IN(VOLUN). This performance increase is
consistent, as well, with an improvement being
observed across every relation type. In addition
to the quality improvements, Table 1 shows that
COMET produces more novel tuple objects than
the baselines, as well.
Learning knowledge from language
Signiﬁcant differences were also observed between the
performance of the model whose weights were initialized with the pre-trained parameters from the
GPT model of Radford et al. and a model
with the same architecture that was trained from
random initialization. This 14% relative improvement in overall human performance conﬁrms that
the language representations learned by the GPT
model are transferable to generating natural language commonsense knowledge.
Effect of decoding algorithm
In Table 3, we
show the effect of different generation policies on
knowledge quality.
The most interesting result
is that using greedy decoding to produce knowledge tuples only results in a 10% relative performance gap compared to a human evaluation of
the ATOMIC test set, showing that the knowledge
produced by the model approaches human performance.
While producing more total candidates
does lower overall performance, quality assess-
Seed Concept
X holds out X’s hand to Y
X meets Y eyes
X watches Y every ___
X eats red meat
X makes crafts
gets dirty
X turns X’s phone
gets a text
X pours ___ over Y’s head
X takes Y’s head off
X pisses on Y’s bonﬁre
gets burned
X spoils somebody rotten
to be mean
X gives Y some pills
X provides for Y’s needs
to be helpful
X explains Y’s reasons
X fulﬁls X’s needs
to have a plan
X gives Y everything
to buy something
X eats pancakes
X makes ___ at work
X moves house
X gives birth to the Y
X gives Y’s friend ___
X goes ___ with friends
X gets all the supplies
to make a list
X murders Y’s wife
to hide the body
X starts shopping
to go home
X develops Y theory
to thank X
X offer Y a position
to accept the job
X takes ___ out for dinner
Table 5: Generations that were randomly selected
from a subset of novel generations from the ATOMIC
development set. A novel generation is a sro tuple not
found in the training set. Manual evaluation of each tuple indicates whether the tuple is considered plausible
by a human annotator.
ments still hover around 55%7 for a beam size of
10. This result suggests that COMET could be effective with human evaluators in the loop to con-
ﬁrm the correctness of generated tuples.
Efﬁciency of learning from seed tuples
Because not all domains will have large available
commonsense KBs on which to train, we explore
how varying the amount of training data available for learning affects the quality and novelty
of the knowledge that is produced. Our results in
Table 4 indicate that even with only 10% of the
available training data, the model is still able to
7This number is partially low due to the many “none" references in the oEffect, oReact, oWant categories. In
any set of 10 candidates, “none" can only be predicted once,
which causes most candidates in the beam to be incorrect if
“none" is the appropriate answer.
produce generations that are coherent, adequate,
and novel.
Using only 1% of the training data
clearly diminishes the quality of the produced generations, with signiﬁcantly lower observed results
across both quality and novelty metrics. Interestingly, we note that training the model without pretrained weights performs comparably to training
with 10% of the seed tuples, quantifying the impact of using pre-trained language representations.
ConceptNet Experiments
The ConceptNet dataset8, provided by Li et al.
 , consists of tuples obtained from the Open
Mind Common Sense (OMCS) entries in Concept-
Net 5 . Tuples are in the standard sro form – (e.g., take a nap, Causes, have
The most conﬁdent 1200 tuples were
used to create the test set, while the next 1200
tuples were used to create two development sets,
which we combine in this work. The 100k version
of the training set was used to train models, which
contains 34 relation types.
We evaluate our models that generate
ConceptNet relations using the following metrics.
First, we report the perplexity of the gold relations
in the test set (PPL). To evaluate the quality of generated knowledge, we also report the number of
generated positive examples in the test set that are
scored as correct by the pre-trained Bilinear AVG
model developed by Li et al. .9 For a given
sro tuple, this model produces a probability for
whether the tuple is correct. We threshold scores
at 50% probability to identify positive predictions.
On the completion task originally proposed in Li
et al. , this model achieved 92.5% accuracy
on the test set, indicating that it is a strong proxy
for automatically evaluating whether a generated
tuple is correct. Finally, we report the same novelty metrics as for ATOMIC: N/T sro and N/T o.
re-implement
the BiLSTM model proposed by Saito et al.
 with minor modiﬁcations outlined in Appendix A.2. This model is trained to learn to encode knowledge in both directions: sr →o and
8 
commonsense.html
9 A pre-trained model can be found at https:
//ttic.uchicago.edu/~kgimpel/comsense_
resources/ckbc-demo.tar.gz
CKBG 
COMET (- pretrain)
COMET - RELTOK
Table 6: ConceptNet generation Results
or →s to help augment a knowledge base completion model. It is only evaluated on the sr →o
tuple generation task, however. For posterity, we
also include the result from a LSTM model that is
only trained on the sr →o task (LSTM - s).
We include the following ablations
of our full model.
First, we evaluate how pretraining on a large-scale corpus helps performance by training a comparison
model from scratch, denoted COMET (- pretrain)
in Table 6. Second, in our main model, we map
relation names to natural language (e.g., IsA →
“is a”; HasSubevent →“has subevent”) so the
model can learn to represent these concepts with
language, as opposed to learning a special embedding from scratch for each relation (Levy et al.,
As an ablation, we train a model without converting relation tokens to natural language
(e.g., IsA ̸→“is a”), which we denote COMET -
Our results indicate that high-quality
knowledge can be generated by the model: the low
perplexity scores in Table 6 indicate high model
conﬁdence in its predictions, while the high classiﬁer score (95.25%) indicates that the KB completion model of Li et al. scores the generated tuples as correct in most of the cases. While
adversarial generations could be responsible for
this high score, a human evaluation (following
the same design as for ATOMIC) scores 91.7% of
greedily decoded tuples as correct. Randomly selected examples provided in Table 7 also point to
the quality of knowledge produced by the model.
In addition to being high quality, the
generated tuples from COMET are also novel, with
59.25% of the tuples not being present in the training set, showing that the model is capable of generating new edges between nodes, and even creating new nodes – 3.75% of o nodes are novel –
to extend the size of the knowledge graph. One
shortcoming, however, is that novel generations
Classiﬁer Accuracy
% of tuples with edit distance >= X
Edit Distance
% of novel tuples
Figure 4: The percentage of novel ConceptNet development set tuples per minimum edit distance from
training tuples. In green: classiﬁer-scored accuracy of
each subset.
are sometimes simpliﬁed forms of tuples from the
training set. In Table 7, for example, the tuple
“doctor CapableOf save life” is not present in
the training set, but “doctor CapableOf save
person life” is. Many tuples, however, are completely novel, such as “bird bone HasProperty
fragile” and “driftwood AtLocation beach”,
which have no related tuples in the training set.
To explore further, we investigate by how much
novel tuples from the development set differ from
training set phrase objects for the same s, r using
minimum edit distance of phrase objects. We measure the edit distance of phrase object odev in the
tuple (s, r, odev) to the otrn from the nearest training tuple (s, r, otrn). Edit distance is measured using word tokens (excluding stop words) and normalized by the maximum number of words in odev
or otrn. The maximum edit distance is one (i.e.,
entirely different word sequences) and the minimum edit distance is zero (i.e., the same sequence
excluding stopwords). Figure 4 shows the percentage of novel development set tuples that have an
edit distance from the closest training set tuple of
at least the value on the x-axis. Over 75% of the
novel tuples have objects that are a normalized edit
distance of >= 0.5 from the training phrase objects, indicating that most of the novel phrase objects have signiﬁcantly different word sequences
from their closest analogues in the training set.
Learning knowledge from language
Similarly to ATOMIC, we explore how pre-training
on a large language corpus affects its
ability to generalize commonsense.
This effect
is apparent in Table 6, with a clear improvement on automatic and human evaluations by the
pretrained COMET over the randomly initialized
Completion
oldsmobile
AtLocation
AtLocation
AtLocation
AtLocation
dental chair
AtLocation
AtLocation
your ﬁnger
you feel good
post ofﬁce
receive letter
HasProperty
HasProperty
many plant
HasPrerequisite
print on printer
HasPrerequisite
get printer
HasPrerequisite
HasLastSubevent
HasSubevent
MotivatedByGoal
you be tire
ReceivesAction
Table 7: Randomly selected and novel generations
from the ConceptNet development set. Novel generations are sro tuples not found in the training set. Manual evaluation of each tuple indicates whether the tuple
is considered plausible by a human annotator
model. Qualitatively, we observe this effect in Table 7 with the generated example tuple “mango
IsA fruit", which is not present in the training set.
The only tuple containing the “mango" entity in
the training set is “mango UsedFor salsa", which
is not informative enough. As conﬁrmation, we
observe that the output from COMET (- pretrain) is
“mango IsA spice”, which could be a reasonable
inference given the information about “mango" in
the seed set of knowledge.
Representing relations with language
the automatic metrics point to insigniﬁcant differences when comparing models with symbol relations and those with natural language relations
(Table 6), examples can provide qualitative insights into the beneﬁts of representing relations as
language. While the only non-ornithological reference to a “dove" in the ConceptNet training set
is “dove CapableOf ﬂy”, our model learns to
generalize to produce the tuple “dove SymbolOf
purity”. The model that uses symbol relation embeddings only manages to produce the relation
“dove SymbolOf submarine”, which seems to
relate “submarine" to a more nautical (and unrelated) word sense of “dove".
Related Work
Knowledge base construction
Previous work
has looked at constructing knowledge bases as relational schemas using expert knowledge , semistructured text extraction and unstructured text extraction
 . In our work, we focus on construction of commonsense knowledge
bases which require the use of open-text events
rather than a well-deﬁned relational schema structure.
Other work in information extraction can
also be applied to knowledge base construction
with open-text entities , but these
methods typically extract explicitly stated text relations. Conversely, our approach generates new
knowledge that is often unstated in text, as commonsense information typically is .
Commonsense
completion
Existing work on generation of novel commonsense knowledge has also used ConceptNet and
ATOMIC as underlying KBs. Speciﬁcally, Li et al.
 proposed a set of neural network models
for scoring tuples in ConceptNet. Our work differs
from this approach as their models evaluate full tuples rather than learning to generate the phrases to
make new nodes in the knowledge graph. Saito
et al. builds upon this work by proposing a
joint model for completion and generation of commonsense tuples. Their work, however, focuses on
using tuple generation to augment their KB completion model, rather than to increase coverage in
commonsense KB construction. Finally, Sap et al.
 use LSTM encoder-decoder models to generate commonsense knowledge about social situations. We use transformers and investigate the effect of using pre-trained language representations
 to initialize them.
Transformers and pre-training
Finally, our
work builds on previous work on adapting pretrained language models for various sequence labeling, classiﬁcation, and NLI end tasks . Our research investigates how pre-trained
language models can be used for large-scale commonsense KB construction by generating new
graph nodes and edges between nodes.
Conclusion
COMmonsense
Transformers
(COMET) for automatic construction of commonsense knowledge bases. COMET is a framework
for adapting the weights of language models to
learn to produce novel and diverse commonsense knowledge tuples.
Empirical results on
two commonsense knowledge bases, ATOMIC
and ConceptNet, show that COMET
frequently
produces novel commonsense knowledge that
human evaluators deem to be correct.
positive results point to future work in extending the approach to a variety of other types of
knowledge bases, as well as investigating whether
OpenIE-style
knowledge tuples for arbitrary knowledge seeds.
Acknowledgments
We thank Thomas Wolf, Ari Holtzman, Chandra
Bhagavatula, Peter Clark, Rob Dalton, Ronan Le
Bras, Rowan Zellers and Scott Yih for helpful discussions over the course of this project, as well as
the anonymous reviewers for their insightful comments.
This research was supported in part by
NSF (IIS-1524371, IIS-1714566, NRI-1525251),
DARPA under the CwC program through the ARO
(W911NF-15-1-0543), and Samsung Research.
This material is based, in part, upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No.
DGE-1256082.