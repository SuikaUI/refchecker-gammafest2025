Hide-and-Seek: Forcing a Network to be Meticulous for
Weakly-supervised Object and Action Localization
Krishna Kumar Singh and Yong Jae Lee
University of California, Davis
‘Hide-and-Seek’,
weakly-supervised
framework that aims to improve object localization in images and action localization in videos.
Most existing
weakly-supervised methods localize only the most discriminative parts of an object rather than all relevant parts,
which leads to suboptimal performance. Our key idea is
to hide patches in a training image randomly, forcing the
network to seek other relevant parts when the most discriminative part is hidden. Our approach only needs to
modify the input image and can work with any network designed for object localization. During testing, we do not
need to hide any patches. Our Hide-and-Seek approach obtains superior performance compared to previous methods
for weakly-supervised object localization on the ILSVRC
dataset. We also demonstrate that our framework can be
easily extended to weakly-supervised action localization.
1. Introduction
Weakly-supervised approaches have been proposed for
various visual classiﬁcation and localization tasks including object detection ,
semantic segmentation and visual attribute localization . The main advantage of weaklysupervised learning is that it requires less detailed annotations compared to the fully-supervised setting, and therefore
has the potential to use the vast weakly-annotated visual
data available on the Web. For example, weakly-supervised
object detectors can be trained using only image-level labels
(‘dog’ or ‘no dog’) without any object location annotations.
Existing weakly-supervised methods identify discriminative patterns in the training data that frequently appear in
one class and rarely in the remaining classes. This is done
either explicitly by mining discriminative image regions or
features or implicitly by analyzing the higher-layer activation maps produced by a deep
network trained for image classiﬁcation . However, due to intra-category variations or relying only on a
classiﬁcation objective, these methods often fail to identify
the entire extent of the object and instead localize only the
Full image
Randomly hidden patches
Figure 1. Main idea. (Top row) A network tends to focus on the
most discriminative parts of an image (e.g., face of the dog) for
classiﬁcation. (Bottom row) By hiding images patches randomly,
we can force the network to focus on other relevant object parts in
order to correctly classify the image as ’dog’.
most discriminative part.
Recent work tries to address this issue of identifying only
the most discriminative part. Song et al. combine multiple co-occurring discriminative regions to cover a larger
extent of the object. While multiple selections ensure larger
coverage, it does not guarantee selection of less discriminative patches of the object in the presence of many highly
discriminative ones. Singh et al. use motion cues and
transfer tracked object boxes from weakly-labeled videos
to the images. However, this approach requires additional
weakly-labeled videos, which may not always be available.
Finally, Zhou et al. replace max pooling with global average pooling after the ﬁnal convolution layer of an image
classiﬁcation network. Since average pooling aggregates
activations across an entire feature map, it encourages the
network to look beyond the most discriminative part (which
would sufﬁce for max pooling). However, the network can
still avoid ﬁnding less discriminative parts if identifying a
few highly-discriminative parts can lead to accurate classi-
ﬁcation performance, as shown in Figure 1(top row).
Main Idea.
In this paper, we take a radically different
approach to this problem. Instead of making algorithmic
 
changes or relying on external data , we make
changes to the input image. The key idea is to hide patches
from an image during training so that the model needs to
seek the relevant object parts from what remains. We thus
name our approach ‘Hide-and-Seek’. Figure 1 (bottom row)
demonstrates the intuition: if we randomly remove some
patches from the image then there is a possibility that the
dog’s face, which is the most discriminative, will not be
visible to the model. In this case, the model must seek other
relevant parts like the tail and legs in order to do well on the
classiﬁcation task. By randomly hiding different patches in
each training epoch, the model sees different parts of the
image and is forced to focus on multiple relevant parts of
the object beyond just the most discriminative one. Importantly, we only apply this random hiding of patches during
training and not during testing. Since the full image is observed during testing, the data distribution will be different
to that seen during training. We show that setting the hidden
pixels’ value to be the data mean can allow the two distributions to match, and provide a theoretical justiﬁcation.
Since Hide-and-Seek only alters the input image, it
can easily be generalized to different neural networks and
In this work, we demonstrate its applicability on
AlexNet and GoogLeNet , and apply the idea
to weakly-supervised object localization in images and
weakly-supervised action localization in videos.
temporal action localization task (in which the start and
end times of an action need to be found), random frame
sequences are hidden while training a network on action
classiﬁcation, which forces the network to learn the relevant
frames corresponding to an action.
Contributions.
Our work has three main contributions:
1) We introduce the idea of Hide-and-Seek for weaklysupervised localization and produce state-of-the-art object
localization results on the ILSVRC dataset ; 2) We
demonstrate the generalizability of the approach on different networks and layers; 3) We extend the idea to the relatively unexplored task of weakly-supervised temporal action localization.
2. Related Work
Weakly-supervised
localization.
Fullysupervised convolutional networks (CNNs) have demonstrated great performance on object detection ,
segmentation and attribute localization ,
but require expensive human annotations for training
bounding box for object localization).
To alleviate
annotation
weakly-supervised
approaches learn using cheaper labels,
for example,
image-level
predicting
location .
Most weakly-supervised object localization approaches
mine discriminative features or patches in the data that frequently appear in one class and rarely in other classes . However, these approaches tend to
focus only on the most discriminative parts, and thus fail to
cover the entire spatial extent of an object. In our approach,
we hide image patches (randomly) during training, which
forces our model to focus on multiple parts of an object and
not just the most discriminative ones. Other methods use
additional motion cues from weakly-labeled videos to improve object localization . While promising, such
videos are not always readily available and can be challenging to obtain especially for static objects. In contrast, our
method does not require any additional data or annotations.
Recent work modify CNN architectures designed for image classiﬁcation so that the convolutional layers learn to
localize objects while performing image classiﬁcation . Other network architectures have been designed for
weakly-supervised object detection . Although
these methods have signiﬁcantly improved the state-of-theart, they still essentially rely on a classiﬁcation objective
and thus can fail to capture the full extent of an object if
the less discriminative parts do not help improve classiﬁcation performance. We also rely on a classiﬁcation objective.
However, rather than modifying the CNN architecture, we
instead modify the input image by hiding random patches
from it. We demonstrate that this enforces the network to
give attention to the less discriminative parts and ultimately
localize a larger extent of the object.
Masking pixels or activations.
Masking image patches
has been applied for object localization , self-supervised
feature learning , semantic segmentation , generating hard occlusion training examples for object detection , and to visualize and understand what a CNN has
learned . In particular, for object localization, 
train a CNN for image classiﬁcation and then localize the
regions whose masking leads to a large drop in classiﬁcation
performance. Since these approaches mask out the image
regions only during testing and not during training, the localized regions are limited to the highly-discriminative object parts. In our approach, image regions are masked during training, which enables the model to learn to focus on
even the less discriminative object parts. Finally, our work
is closely related to the adversarial erasing method of ,
which iteratively trains a sequence of models for weaklysupervised semantic segmentation. Each model identiﬁes
the relevant object parts conditioned on the previous iteration model’s output. In contrast, we only train a single
model once—and is thus less expensive—and do not rely on
saliency detection to reﬁne the localizations as done in .
Dropout and its variants are also related.
There are two main differences: (1) these methods are designed to prevent overﬁtting while our work is designed to
improve localization; and (2) in dropout, units in a layer are
Training image
Training phase
Tes,ng phase
Trained CNN
Test image
(no hidden patches)
Class Ac=va=on Map (CAM)
Predicted label: ‘dog’
Figure 2. Approach overview. Left: For each training image, we divide it into a grid of S × S patches. Each patch is then randomly
hidden with probability phide and given as input to a CNN to learn image classiﬁcation. The hidden patches change randomly across
different epochs. Right: During testing, the full image without any hidden patches is given as input to the trained network.
dropped randomly, while in our work, contiguous image regions or video frames are dropped. We demonstrate in the
experiments that our approach produces signiﬁcantly better
localizations compared to dropout.
Action localization.
Action localization is a well studied problem . Recent CNN-based approaches have shown superior performance compared to previous hand-crafted approaches.
These fullysupervised methods require the start and end time of an action in the video during the training to be annotated, which
can be expensive to obtain. Weakly-supervised approaches
learn from movie scripts or an ordered list of actions . Sun et al. combine weakly-labeled videos
with web images for action localization. In contrast to these
approaches, our approach only uses a single video-level action label for temporal action localization. also only
use video-level action labels for action localization with the
focus on ﬁnding the key event frames of an action. We instead focus on localizing the full extent of an action.
3. Approach
In this section, we ﬁrst describe our Hide-and-Seek algorithm for object localization in images followed by action
localization in videos.
3.1. Weakly-supervised object localization
For weakly-supervised object localization, we are given
a set of images Iset = {I1, I2, ....., IN} in which each image I is labeled only with its category label. Our goal is to
learn an object localizer that can predict both the category
label as well as the bounding box for the object-of-interest
in a new test image Itest. In order to learn the object localizer, we train a CNN which simultaneously learns to localize the object while performing the image classiﬁcation
task. While numerous approaches have been proposed to
solve this problem, existing methods (e.g., )
are prone to localizing only the most discriminative object
parts, since those parts are sufﬁcient for optimizing the classiﬁcation task.
To enforce the network to learn all of the relevant parts
of an object, our key idea is to randomly hide patches of
each input image I during training, as we explain next.
Hiding random image patches.
The purpose of hiding
patches is to show different parts of an object to the network while training it for the classiﬁcation task. By hiding
patches randomly, we can ensure that the most discriminative parts of an object are not always visible to the network,
and thus force it to also focus on other relevant parts of the
object. In this way, we can overcome the limitation of existing weakly-supervised methods that focus only on the most
discriminative parts of an object.
Concretely, given a training image I of size W × H × 3,
we ﬁrst divide it into a grid with a ﬁxed patch size of S×S×
3. This results in a total of (W × H)/(S × S) patches. We
then hide each patch with phide probability. For example,
in Fig. 2 left, the image is of size 224 × 224 × 3, and it is
divided into 16 patches of size 56 × 56 × 3. Each patch
is hidden with phide = 0.5 probability. We take the new
image I′ with the hidden patches, and feed it as a training
input to a CNN for classiﬁcation.
Importantly, for each image, we randomly hide a different set of patches. Also, for the same image, we randomly
hide a different set of patches in each training epoch. This
property allows the network to learn multiple relevant object parts for each image. For example, in Fig. 2 left, the
network sees a different I′ in each epoch due to the randomness in hiding of the patches. In the ﬁrst epoch, the dog’s
face is hidden while its legs and tail are clearly visible. In
Inside visible patch
Inside hidden patch
Partially in hidden patch
Figure 3. There are three types of convolutional ﬁlter activations
after hiding patches: a convolution ﬁlter can be completely within
a visible region (blue box), completely within a hidden region (red
box), or partially within a visible/hidden region (green box).
contrast, in the second epoch, the face is visible while the
legs and tail are hidden. Thus, the network is forced to learn
all of the relevant parts of the dog rather than only the highly
discriminative part (i.e., the face) in order to perform well
in classifying the image as a ‘dog’.
We hide patches only during training. During testing,
the full image—without any patches hidden—is given as
input to the network; Fig. 2 right. Since the network has
learned to focus on multiple relevant parts during training,
it is not necessary to hide any patches during testing. This
is in direct contrast to , which hides patches during testing but not during training. For , since the network has
already learned to focus on the most discimirinative parts
during training, it is essentially too late, and hiding patches
during testing has no signiﬁcant effect on localization performance.
Setting the hidden pixel values.
There is an important
detail that we must be careful about. Due to the discrepancy
of hiding patches during training while not hiding patches
during testing, the ﬁrst convolutional layer activations during training versus testing will have different distributions.
For a trained network to generalize well to new test data,
the activation distributions should be roughly equal. That is,
for any unit in a neural network that is connected to x units
with w outgoing weights, the distribution of w⊤x should be
roughly the same during training and testing. However, in
our setting, this will not necessarily be the case since some
patches in each training image will be hidden while none of
the patches in each test image will ever be hidden.
Speciﬁcally, in our setting, suppose that we have a
convolution ﬁlter F with kernel size K × K and threedimensional weights W = {w1, w2, ...., wk×k}, which is
applied to an RGB patch X = {x1, x2, ...., xk×k} in image
I′. Denote v as the vector representing the RGB value of
every hidden pixel. There are three types of activations:
1. F is completely within a visible patch (Fig. 3, blue
box). The corresponding output will be Pk×k
2. F is completely within a hidden patch (Fig. 3, red
box). The corresponding output will be Pk×k
is partially within a hidden patch (Fig. 3,
green box).
The corresponding output will be
m∈visible w⊤
n∈hidden w⊤
During testing, F will always be completely within a visible patch, and thus its output will be Pk×k
i xi. This
matches the expected output during training in only the ﬁrst
case. For the remaining two cases, when F is completely or
partially within a hidden patch, the activations will have a
distribution that is different to those seen during testing.
We resolve this issue by setting the RGB value v of a
hidden pixel to be equal to the mean RGB vector of the
images over the entire dataset: v = µ =
where j indexes all pixels in the entire training dataset and
Npixels is the total number of pixels in the dataset. Why
would this work? Essentially, we are assuming that in expectation, the output of a patch will be equal to that of an
average-valued patch: E[Pk×k
i xi] = Pk×k
replacing v with µ, the outputs of both the second and third
cases will be Pk×k
i µ, and thus will match the expected
output during testing (i.e., of a fully-visible patch).1
This process is related to the scaling procedure in
dropout , in which the outputs are scaled proportional
to the drop rate during testing to match the expected output during training. In dropout, the outputs are dropped
uniformly across the entire feature map, independently of
spatial location. If we view our hiding of the patches as
equivalent to “dropping” units, then in our case, we cannot have a global scale factor since the output of a patch
depends on whether there are any hidden pixels. Thus, we
instead set the hidden values to be the expected pixel value
of the training data as described above, and do not scale the
corresponding output. Empirically, we ﬁnd that setting the
hidden pixel in this way is crucial for the network to behave
similarly during training and testing.
Object localization network architecture.
Our approach of hiding patches is independent of the network architecture and can be used with any CNN designed for object localization. For our experiments, we choose to use
the network of Zhou et al. , which performs global average pooling (GAP) over the convolution feature maps to
generate a class activation map (CAM) for the input image that represents the discriminative regions for a given
This approach has shown state-of-the-art performance for the ILSVRC localization challenge in the
weakly-supervised setting, and existing CNN architectures
like AlexNet and GoogLeNet can easily be modi-
ﬁed to generate a CAM.
1For the third case: P
m∈visible w⊤
n∈hidden w⊤
m∈visible w⊤
n∈hidden w⊤
n µ = Pk×k
To generate a CAM for an image, global average pooling is performed after the last convolutional layer and the
result is given to a classiﬁcation layer to predict the image’s
class probabilities. The weights associated with a class in
the classiﬁcation layer represent the importance of the last
convolutional layer’s feature maps for that class. More formally, denote F = {F1, F2, .., FM} to be the M feature
maps of the last convolutional layer and W as the N × M
weight matrix of the classiﬁcation layer, where N is number of classes. Then, the CAM for class c for image I is:
CAM(c, I) =
W(c, i) · Fi(I).
Given the CAM for an image, we generate a bounding box using the method proposed in .
we ﬁrst threshold the CAM to produce a binary foreground/background map, and then ﬁnd connected components among the foreground pixels. Finally, we ﬁt a tight
bounding box to the largest connected component. We refer
the reader to for more details.
3.2. Weakly-supervised action localization
{V1, V2, ..., VN} and video class labels, our goal here
is to learn an action localizer that can predict the label of an
action as well as its start and end time for a test video Vtest.
Again the key issue is that for any video, a network will
focus mostly on the highly-discriminative frames in order
to optimize classiﬁcation accuracy instead of identifying all
relevant frames. Inspired by our idea of hiding the patches
in images, we propose to hide frames in videos to improve
action localization.
Speciﬁcally, during training, we uniformly sample video
Ftotal frames from each videos. We then divide the Ftotal
frames into continuous segments of ﬁxed size Fsegment;
i.e., we have Ftotal/Fsegemnt segments. Just like with image patches, we hide each segment with probability phide
before feeding it into a deep action localizer network. We
generate class activation maps (CAM) using the procedure
described in the previous section. In this case, our CAM
is a one-dimensional map representing the discriminative
frames for the action class. We apply thresholding on this
map to obtain the start and end times for the action class.
4. Experiments
We perform quantitative and qualitative evaluations of
Hide-and-Seek for object localization in images and action
localization in videos. We also perform ablative studies to
compare the different design choices of our algorithm.
Datasets and evaluation metrics.
We use ILSVRC
2016 to evaluate object localization accuracy.
training, we use 1.2 million images with their class labels
(1000 categories). We compare our approach with the baselines on the validation data. We use three evaluation metrics to measure performance: 1) Top-1 localization accuracy (Top-1 Loc): fraction of images for which the predicted
class with the highest probability is the same as the groundtruth class and the predicted bounding box for that class has
more than 50% IoU with the ground-truth box. 2) Localization accuracy with known ground-truth class (GT-known
Loc): fraction of images for which the predicted bounding
box for the ground-truth class has more than 50% IoU with
the ground-truth box. As our approach is primarily designed
to improve localization accuracy, we use this criterion to
measure localization accuracy independent of classiﬁcation
performance. 3) We also use classiﬁcation accuracy (Top-
1 Clas) to measure the impact of Hide-and-Seek on image
classiﬁcation performance.
For action localization, we use THUMOS 2014 validation data , which consists of 1010 untrimmed videos belonging to 101 action classes. We train over all untrimmed
videos for the classiﬁcation task and then evaluate localization on the 20 classes that have temporal annotations.
Each video can contain multiple instances of a class. For
evaluation we compute mean average precision (mAP), and
consider a prediction to be correct if it has IoU > θ with
ground-truth. We vary θ to be 0.1, 0.2, 0.3, 0.4, and 0.5. As
we are focusing on localization ability of the network, we
assume we know the ground-truth class label of the video.
Implementation details.
To learn the object localizer, we
use the same modiﬁed AlexNet and GoogLeNet networks
introduced in (AlexNet-GAP and GoogLeNet-GAP).
AlexNet-GAP is identical to AlexNet until pool5 (with
stride 1) after which two new conv layers are added. Similarly for GoogLeNet-GAP, layers after inception-4e are removed and a single conv layer is added. For both AlexNet-
GAP and GoogLeNet-GAP, the output of the last conv layer
goes to a global average pooling (GAP) layer, followed by a
softmax layer for classiﬁcation. Each added conv layer has
512 and 1024 kernels of size 3 × 3, stride 1, and pad 1 for
AlexNet-GAP and GoogLeNet-GAP, respectively.
We train the networks from scratch for 55 and 40 epochs
for AlexNet-GAP and GoogLeNet-GAP, respectively, with
a batch size of 128 and initial learning rate of 0.01. We gradually decrease the learning rate to 0.0001. We add batch
normalization after every conv layer to help convergence of GoogLeNet-GAP. For simplicity, unlike the original AlexNet architecture , we do not group the conv
ﬁlters together (it produces statistically the same Top-1 Loc
accuracy as the grouped version for both AlexNet-GAP but
has better image classiﬁcation performance). The network
remains exactly the same with (during training) and without
(during testing) hidden image patches. To obtain the binary
fg/bg map, 20% and 30% of the max value of the CAM is
chosen as the threshold for AlexNet-GAP and GoogLeNet-
GT-known Loc
Top-1 Clas
AlexNet-GAP 
AlexNet-HaS-16
AlexNet-HaS-32
AlexNet-HaS-44
AlexNet-HaS-56
AlexNet-HaS-Mixed
GoogLeNet-GAP 
GoogLeNet-HaS-16
GoogLeNet-HaS-32
GoogLeNet-HaS-44
GoogLeNet-HaS-56
Table 1. Localization accuracy on ILSVRC validation data with
different patch sizes for hiding. Our Hide-and-Seek always performs better than AlexNet-GAP , which sees the full image.
GAP, respectively; the thresholds were chosen by observing
a few qualitative results on training data. During testing,
we average 10 crops (4 corners plus center, and same with
horizontal ﬂip) to obtain class probabilities and localization maps. We ﬁnd similar localization/classiﬁcation performance when ﬁne-tuning pre-trained networks.
For action localization, we compute C3D fc7 features using a model pre-trained on Sports 1 million .
We compute 10 feats/sec (each feature is computed over
16 frames) and uniformly sample 2000 features from the
video. We then divide the video into 20 equal-length segments each consisting of Fsegment = 100 features. During
training, we hide each segment with phide = 0.5. For action
classiﬁcation, we feed C3D features as input to a CNN with
two conv layers followed by a global max pooling and softmax classiﬁcation layer. Each conv layer has 500 kernels
of size 1 × 1, stride 1. For any hidden frame, we assign it
the dataset mean C3D feature. For thresholding, 50% of the
max value of the CAM is chosen. All continuous segments
after thresholding are considered predictions.
4.1. Object localization quantitative results
We ﬁrst analyze object localization accuracy on the
ILSVRC validation data.
Table 1 shows the results using the Top-1 Loc and GT-known Loc evaluation metrics.
AlexNet-GAP is our baseline in which the network
has seen the full image during training without any hidden
patches. Alex-HaS-N is our approach, in which patches of
size N × N are hidden with 0.5 probability during training.
Which patch size N should we choose?
We explored
four different patch sizes N = {16, 32, 44, 56}, and each
performs signiﬁcantly better than AlexNet-GAP for both
GT-known Loc and Top-1 Loc.
Our GoogLeNet-HaS-N
models also outperfors GoogLeNet-GAP for all patch sizes.
These results clearly show that hiding patches during training leads to better localization. Although our approach can
lose some classiﬁcation accuracy (Top-1 Clas) since it has
2 does not provide GT-known loc, so we compute on our own GAP
implementations, which achieve similar Top-1 Loc accuracy.
GT-known Loc
Backprop on AlexNet 
AlexNet-GAP 
AlexNet-GAP-ensemble
Ours-ensemble
Backprop on GoogLeNet 
GoogLeNet-GAP 
Table 2. Localization accuracy on ILSVRC val data compared to
state-of-the-art. Our method outperforms all previous methods.
never seen a complete image and thus may not have learned
to relate certain parts, the huge boost in localization performance (which can be seen by comparing the GT-known Loc
accuracies) makes up for any potential loss in classiﬁcation.
We also train a network (AlexNet-HaS-Mixed) with
mixed patch sizes. During training, for each image in every
epoch, the patch size N to hide is chosen randomly from 16,
32, 44 and 56 as well as no hiding (full image). Since different sized patches are hidden, the network can learn complementary information about different parts of an object (e.g.
small/large patches are more suitable to hide smaller/larger
parts). Indeed, we achieve the best results for Top-1 Loc
using AlexNet-HaS-Mixed.
Comparison to state-of-the-art.
Next, we choose our
best model for AlexNet and GoogLeNet, and compare it
with state-of-the-art methods on ILSVRC validation data;
see Table 2. Our method performs 3.78% and 1.40% points
better than AlexNet-GAP on GT-known Loc and Top-1
Loc, respectively. For GoogLeNet, our model gets a boost
of 1.88% and 1.61% points compared to GoogLeNet-GAP
for GT-known Loc and Top-1 Loc accuracy, respectively.
Importantly, these gains are obtained simply by changing
the input image without changing the network architecture.
Ensemble model.
Since each patch size provides complementary information (as seen in the previous section),
we also create an ensemble model of different patch sizes
(Ours-ensemble). To produce the ﬁnal localization for an
image, we average the CAMs obtained using AlexNet-HaS-
16, 32, 44, and 56, while for classiﬁcation, we average
the classiﬁcation probabilities of all four models as well as
the probability obtained using AlexNet-GAP. This ensemble model gives a boost of 5.24 % and 4.15% over AlexNet-
GAP for GT-known Loc and Top-1 Loc, respectively. For a
more fair comparison, we also combine the results of ﬁve
independent AlexNet-GAPs to create an ensemble baseline.
Ours-ensemble outperforms this strong baseline (AlexNet-
GAP-ensemble) by 3.23% and 1.82% for GT-known Loc
and Top-1 Loc, respectively.
4.2. Object localization qualitative results
In Fig. 4, we visualize the class activation map (CAM)
and bounding box obtained by our AlexNet-HaS approach
Bounding Box
(AlexNet-GAP)
(AlexNet-GAP)
Bounding Box
Bounding Box
(AlexNet-GAP)
(AlexNet-GAP)
Bounding Box
Figure 4. Qualitative object localization results. We compare our approach with AlexNet-GAP on the ILVRC validation data. For
each image, we show the bounding box and CAM obtained by AlexNet-GAP (left) and our method (right). Our Hide-and-Seek approach
localizes multiple relevant parts of an object whereas AlexNet-GAP mainly focuses only on the most discriminative parts.
versus those obtained with AlexNet-GAP. In each image
pair, the ﬁrst image shows the predicted (green) and groundtruth (red) bounding box.
The second image shows the
CAM, i.e., where the network is focusing for that class. Our
approach localizes more relevant parts of an object compared to AlexNet-GAP and is not conﬁned to only the most
discriminative parts. For example, in the ﬁrst, second, and
ﬁfth rows AlexNet-GAP only focuses on the face of the animals, whereas our method also localizes parts of the body.
Similarly, in the third and last rows AlexNet-GAP misses
the tail for the snake and squirrel while ours gets the tail.
4.3. Further Analysis of Hide-and-Seek
Comparison with dropout.
Dropout has been extensively used to reduce overﬁtting in deep network. Although it is not designed to improve localization, the dropping of units is related to our hiding of patches. We therefore conduct an experiment in which 50% dropout is applied at the image layer. We noticed that the due to the
large dropout rate at the pixel-level, the learned ﬁlters de-
GT-known Loc
AlexNet-dropout-trainonly
AlexNet-dropout-traintest
Table 3. Our approach outperforms Dropout for localization.
GT-known Loc
AlexNet-GAP
AlexNet-Avg-HaS
AlexNet-GMP
AlexNet-Max-HaS
Table 4. Global average pooling (GAP) vs. global max pooling
(GMP). Unlike , for Hide-and-Seek GMP still performs well
for localization. For this experiment, we use patch size 56.
GT-known Loc
AlexNet-GAP
AlexNet-HaS-conv1-5
AlexNet-HaS-conv1-11
Table 5. Applying Hide-and-Seek to the ﬁrst conv layer. The improvement over shows the generality of the idea.
velop a bias toward a dropped-out version of the images and
produces signiﬁcantly inferior classiﬁcation and localization performance (AlexNet-dropout-trainonly). If we also
do dropout during testing (AlexNet-dropout-traintest) then
performance improves but is still much lower compared to
our approach (Table 3). Since dropout drops pixels (and
RGB channels) randomly, information from the most relevant parts of an object will still be seen by the network with
high probability, which makes it likely to focus on only the
most discriminative parts.
Do we need global average pooling?
 showed that
GAP is better than global max pooling (GMP) for object
localization, since average pooling encourages the network
to focus on all the discriminative parts. For max pooling,
only the most discriminative parts need to contribute. But
is global max pooling hopeless for localization?
With our Hide-and-Seek, even with max pooling, the
network is forced to focus on a different discriminative
parts. In Table 4, we see that max pooling (AlexNet-GMP)
is inferior to average poling (AlexNet-GAP) for the baselines.
But with Hide-and-Seek, max pooling (AlexNet-
Max-HaS) localization accuracy increases by a big margin
and even slightly outperforms average pooling (AlexNet-
Avg-HaS). The slight improvement is likely due to max
pooling being more robust to noise.
Hide-and-Seek in convolutional layers.
We next apply
our idea to convolutional layers. We divide the convolutional feature maps into a grid and hide each patch (and all
of its corresponding channels) with 0.5 probability. We hide
patches of size 5 (AlexNet-HaS-conv1-5) and 11 (AlexNet-
HaS-conv1-11) in the conv1 feature map (which has size
55×55×96). Table 5 shows that this leads to a big boost in
performance compared to the baseline AlexNet-GAP. This
GT-known Loc
AlexNet-HaS-25%
AlexNet-HaS-33%
AlexNet-HaS-50%
AlexNet-HaS-66%
AlexNet-HaS-75%
Table 6. Varying the hiding probability.
Higher probabilities
lead to decrease in Top-1 Loc whereas lower probability leads to
smaller GT-known Loc. For this experiment, we use patch size 56.
IOU thresh = 0.1
Video-full
Table 7. Action localization accuracy on THUMOS validation
data. Across all 5 IoU thresholds, our Video-HaS outperforms the
full video baseline (Video-full).
shows that our idea of randomly hiding patches can be generalized to the convolutional layers.
Probability of hiding.
In all of the previous experiments,
we hid patches with 50% probability. In Table 6, we measure the GT-known Loc and Top-1 Loc when we use different hiding probabilities. If we increase the probability
then GT-known Loc remains almost the same while Top-1
Loc decreases a lot. This happens because the network sees
fewer pixels when the hiding probability is high; as a result,
classiﬁcation accuracy reduces and Top-1 Loc drops. If we
decrease the probability then GT-known Loc decreases but
our Top-1 Loc improves. In this case, the network sees more
pixels so its classiﬁcation improves but since less parts are
hidden, it will focus more on only the discriminative parts
decreasing its localization ability.
4.4. Action localization results
Finally, we evaluate action localization accuracy.
compare our approach (Video-HaS), which randomly hides
frame segments while learning action classiﬁcation, with a
baseline that sees the full video (Video-full). Table 7 shows
the result on THUMOS validation data. Video-HaS consistently outperforms Video-full for action localization task,
which shows that hiding frames forces our network to focus
on more relevant frames, which ultimately leads to better
action localization. We show qualitative results in the supp.
5. Conclusion
‘Hide-and-Seek’,
weaklysupervised framework to improve object localization in images and temporal action localization in videos. By randomly hiding patches/frames in a training image/video, we
force the network to learn to focus on multiple relevant parts
of an object/action. Our extensive experiments showed improved localization accuracy over state-of-the-art methods.
Acknowledgements.
This work was supported in part by
Intel Corp, Amazon Web Services Cloud Credits for Research, and GPUs donated by NVIDIA.