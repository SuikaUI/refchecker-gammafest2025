Received May 28, 2019, accepted June 17, 2019, date of publication June 21, 2019, date of current version July 9, 2019.
Digital Object Identifier 10.1109/ACCESS.2019.2924353
Feature Engineering for Mid-Price
Prediction With Deep Learning
ADAMANTIOS NTAKARIS
1, GIORGIO MIRONE2, JUHO KANNIAINEN
MONCEF GABBOUJ
1, (Fellow, IEEE), AND
ALEXANDROS IOSIFIDIS3, (Senior Member, IEEE)
1Faculty of Information Technology and Communication Sciences, Tampere University of Technology, FI-33720 Tampere, Finland
2Danmarks Nationalbank, 1093 Copenhagen, Denmark
3Department of Electrical and Computer Engineering, Aarhus University, 8000 Aarhus, Denmark
Corresponding authors: Adamantios Ntakaris ( ) and Giorgio Mirone ( )
This work was supported by the H2020 Project BigDataFinance MSCA-ITN-ETN under Grant 675044.
ABSTRACT Mid-price movement prediction based on the limit order book data is a challenging task due
to the complexity and dynamics of the limit order book. So far, there have been very limited attempts for
extracting relevant features based on the limit order book data. In this paper, we address this problem by
designing a new set of handcrafted features and performing an extensive experimental evaluation on both
liquid and illiquid stocks. More speciﬁcally, we present an extensive set of econometric features that capture
the statistical properties of the underlying securities for the task of mid-price prediction. The experimental
evaluation consists of a head-to-head comparison with other handcrafted features from the literature and
with features extracted from a long short-term memory autoencoder by means of a fully automated process.
Moreover, we develop a new experimental protocol for online learning that treats the task above as a multiobjective optimization problem and predicts: 1) the direction of the next price movement and; 2) the number
of order book events that occur until the change takes place. In order to predict the mid-price movement,
features are fed into nine different deep learning models based on multi-layer perceptrons, convolutional
neural networks, and long short-term memory neural networks. The performance of the proposed method is
then evaluated on liquid and illiquid stocks (i.e., TotalView-ITCH US and Nordic stocks). For some stocks,
results suggest that the correct choice of a feature set and a model can lead to the successful prediction of
how long it takes to have a stock price movement.
INDEX TERMS
Deep learning, econometrics, high-frequency trading, limit order book, mid-price,
U.S. data.
I. INTRODUCTION
The automation of ﬁnancial markets has increased the complexity of information analysis. This complexity can be effectively managed by the use of ordered trading universes like
the limit order book (LOB). LOB is a formation that translates
the daily unexecuted trading activity in price levels according
to the type of orders (i.e., bid and ask side). The daily trading
activity is a big data problem, since millions of trading events
take place inside a trading session. Information extraction and
digital signal (i.e., time series) analysis from every trading
session provide the machine learning (ML) trader with useful instructions for orders, executions, and cancellations of
The associate editor coordinating the review of this manuscript and
approving it for publication was Bora Onat.
Traditional time series analysis methods have failed to
capture the complexity of the contemporary trading markets
adequately. For instance, the work in and suggest
that classical machine learning and deep learning methods
for ﬁnancial metric predictions achieve better results compared to ARIMA and GARCH models. On the contrary,
machine and deep learning methods have proved to be very
effective mechanisms for time series analysis and prediction
(e.g., , , ). The main advantage of these methods is
their ability to capture non-linearities of the input data and
ﬁlter them consecutively by creating new weighted features
more relevant to the suggested problem.
Despite their efﬁcacy to predict time series, machine and
deep learning methods are developed mainly through empirical testing. The majority of the literature (e.g., , , )
that focuses on deep learning frameworks solely relies either
This work is licensed under a Creative Commons Attribution 3.0 License. For more information, see 
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
on raw data or a limited number of features. So far, very
little attention has been paid to the information a neural network should analyze for the mid-price prediction task. In this
paper, we shed light on the information that the ML trader
should consider utilizing in mid-price movement prediction.
To this end, we employ an extensive list of econometric
features1 for mid-price prediction and make a head-to-head
comparison with indicators derived from: i) technical and
quantitative analysis (i.e., ), ii) time-sensitive and timeinsensitive features (i.e., and ), and iii) features
extracted through a fully automated process. This fully automated feature extraction process is conducted by a long shortterm memory (LSTM) autoencoder (AE).
We choose econometrics as motivation for our handcrafted
features since it is the ﬁeld of ﬁnancial engineering that
captures the empirical evidence of microstructure noise and
causality of the data. Our data comes with variations in prices,
known in the ﬁnancial literature as volatility – a measure
that we incorporate into our handcrafted features. Despite the
general perception in academic literature that volatility itself
is not a factor that affects stock returns, ample evidence exists
to support the opposite. For instance, in the author ﬁnds
that volatility together with other proxies that are not directly
observable in the data, like liquidity premium, affect stock
returns. In the same direction, Lettau and Ludvigson 
provide evidence that consumption-to-wealth ratio offers
information for excess stock market returns, with volatility
explaining a signiﬁcant portion of these returns. Another
example is the work by Chung and Chuwonganant ,
where authors ﬁnd strong evidence that market volatility
affects individual stock returns. Under this light, we believe
that these are reliable indicators in considering econometrics
as features for the task of mid-price movement prediction.
We perform our analysis based on deep learning models
which have recently been proposed for ﬁnancial time series
analysis. These models vary from multi-layer perceptrons
(MLP) to convolutional neural networks (CNN) and recurrent neural networks (RNN) like LSTM. For our experiments, we use two TotalView-ITCH datasets from the US and
the Nordic stock markets. We formulate these experiments
based on two protocols: the ﬁrst one (i.e., ‘‘Protocol I’’
in our experiments) is introduced here for the ﬁrst time,
and is based on online learning. The prediction of the midprice movement takes place every next event and is treated
as a multi-objective optimization problem, since it predicts
when and in which direction the mid-price movement will
happen. The second protocol (i.e., ‘‘Protocol II’’ in our
experiments) is an existing protocol based on the work of
Tsantekidis et al. , ,according to which the mid-price
movement prediction is treated as a three-class classiﬁcation
problem (i.e., up, down or stationary mid-price states) for
every next 10th event.
1Econometrics features were used in the past for tasks such as identiﬁcation of big changes in exchange rate volatility (i.e., ), or bankruptcy
prediction in .
The main contribution of our work lies on three pillars.
The ﬁrst pillar refers to the utilization of an extensive econometric features list as input to deep learning models for midprice movement prediction. The second pillar is related to an
extensive evaluation of the newly introduced features with
two other handcrafted feature sets and a feature set based
on a fully automated process. We conduct a fair evaluation
of these feature sets by using the same nine deep learning
models for liquid and illiquid stocks, as well as unbalanced
and balanced feature sets. Next, we test them not only on the
newly introduced experimental protocol but also on a protocol suggested in the literature for the Nordic dataset (also
utilized here). Our ﬁndings indicate that handcrafted features,
which overperformed the fully automated feature extraction
process (i.e., based on LSTM AE), transform the forecasting universe of high-frequency trading. More speciﬁcally,
the present evaluation facilitates traders’ task of selecting
suitable features according to data, stock, and model availability. The third pillar, ﬁnally, refers to the development of a
new experimental protocol that takes into consideration every
trading event and is unaffected by time irregularities in highfrequency data. Our work suggests that feature extraction
should be customized according to stock and model selection;
similar ﬁndings can be in seen in . The present research
opens avenues for several other applications. For instance,
the same sets of features can be tested for time series such
as exchange rates or bitcoin price predictions. Furthermore,
the newly introduced protocol can be the basis of every time
series problem since it is event-driven and unaffected by
time irregularities. Ultimately, there is no need for any type
of data sampling, even for high-frequencies time resolution
environments where datasets are massive.
The remainder of the paper is organized as follows. We provide a comprehensive literature review in Section II. The
problem statement is provided in Section III. The list of
handcrafted features follows in Section IV. In Section V,
we describe the various deep learning models adopted in
our analysis, while in Section VI we describe details of the
datasets and the experimental protocol. In Section VII we
provide the empirical results and Section VIII concludes the
paper. A detailed description of the econometric features used
in our experiments are provided in Appendix together with
results for Protocol II.
II. LITERATURE REVIEW
High-frequency LOB data analysis has captured the interest
of the machine learning community. The complex and chaotic
behavior of the data inﬂow gave space to the use of nonlinear methods like the ones that we see in the machine and
deep learning. For instance, Zhang et al. utilize neural
networks for the prediction of Baltic Dry index and provide
a head-to-head comparison with econometric models. The
author in develops a new type of deep neural network
that captures the local behavior of a LOB for spatial distribution modeling. Dixon applies RNN on S&P500 E-mini
futures data for a metric prediction like price change
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
forecasting. Minh et al. also propose RNN architecture for short-term stock predictions by utilizing successfully
ﬁnancial news and sentiment dictionary. In , authors
apply a combined neural network model based on CNN and
RNN for mid-price prediction.
Metrics prediction, like mid-price, can be facilitated by
the use of handcrafted features. Handcrafted features reveal
hidden information as they are capable of translating timeseries signals to meaningful trading instructions for the
ML trader. Several authors worked towards this direction,
like , , , , , and . These works
present a limited set of features which varies from raw
LOB data to change of price densities and imbalance volume
metrics. Another work that provides a wider range of features
is presented by Ntakaris et al. . The authors there extract
handcrafted features based on the majority of the technical indicators and develop a new quantitative feature based
on logistic regression, which outperformed the suggested
feature list.
Handcrafted features represent only one part of the
experimental protocol in the quest for mid-price movement
prediction. Classiﬁcation, via deep learning methods, is the
continuation of a machine learning protocol. Many authors
have used deep learning in ﬁnancial literature for several
problems. For example, Alberg and Lipton use MLPs
and RNNs for companies’ future fundamentals forecasting.
Qian utilizes machine and deep learning methods, like
support vector machines (SVM), MLPs, denoising autoencoder (DAE), and an assembled DAE-SVM model in order
to predict future trends of stock’s index prices. These machine
and deep learning models outperformed traditional time
series models like ARIMA and generalized autoregressive
conditional heteroskedasticity (GARCH). Sezer et al. 
use MLPs and the three most commonly used technical indicators as inputs for stock price movement predictions.
Many authors utilize LOB data as input to their models.
For instance, Nousi et al. examine the performance of
several machine learning methods, like autoencoders (AE),
bag-of-features algorithm, single hidden layer feedforward
neural networks (SLFN), and MLPs for mid-price prediction.
Han et al. apply decision trees on LOB data and outperform support vector machines (SVM) for the problem of
mid-price prediction. In the same direction, authors in 
apply similar methods on market order book data for market movement predictions. Doering et al. utilize event
ﬂow and limit order datasets for price-trend and pricevolatility predictions based on a deep learning architecture.
Mäkinen et al. predict price jumps with the use of LSTM,
where the input data is based on LOB data. A similar work,
in terms of the neural model, is conducted in in order to
forecast LOB’s mid-price.
To the best of our knowledge, this is the ﬁrst time that
an extensive list of econometric features based on highfrequency LOB data is proposed as input to several neural
networks for mid-price prediction. We conduct a head-tohead comparison with state-of-the-art handcrafted features is
TABLE 1. Message list example.
TABLE 2. Order book example.
conducted together with features based on a fully automated
process; Finally, we report results extracted from two highfrequency datasets with two US and ﬁve Nordic stocks for
both balanced and unbalanced sets.
III. PROBLEM STATEMENT
The problem under consideration is the mid-price movement prediction based on high-frequency LOB data. More
speciﬁcally, we use message and limit order books as input
for the suggested features. Message book (MB), as seen in
Table 1, contains the ﬂow of information which takes place at
every event occurrence. The information displayed by every
incoming event includes the timestamp of the order, execution or cancellation, the id of the trade, the price, the volume,
the type of the event (i.e., order, execution or cancellation),
and the side of the event (i.e., ask or bid).
LOB (Table 2) works under speciﬁc rules based on the
operation of the trading system-exchange. The main advantage of an order book is that it accepts orders under limits
(i.e., limit orders) and market orders. In the former case,
the trader/broker is willing to sell or buy a ﬁnancial instrument under a speciﬁc price. In the latter case, the action of
buying or selling a stock at the current price takes place.
LOBs accept orders by the liquidity providers who submit
limit orders and the liquidity takers who submit market
orders. These limit orders, which represent the unexecuted
trading activity until a market order arrives or cancellation
takes place, construct the LOB that is divided into levels. The
best level consists of the highest bid and the lowest ask price
orders, and their average price deﬁnes the so-called midprice, whose movement we try to predict.
We treat the mid-price movement prediction as a multiobjective optimization problem with two outputs – one is
related to classiﬁcation and the other one to regression. The
ﬁrst part of our objective is to classify whether the mid-price
will go up or down and the second part – the regression part is
to predict in how many events in the future this movement will
happen. To further explain this, let us consider the following
example: in order to extract the intraday labels, we measure
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
starting from time tk, in how many events the mid-price will
change and in which direction (i.e., up or down). For instance,
the mid-price will change in 10 events from now, and will go
up. This means that our label at time k is going to be {1,10},
where 1 is the direction of mid-price and 10 is the number of
events that need to pass in order to see that movement taking
We depart from this labeling system to answer the critical
question of whether handcrafted features derived from econometrics can boost deep learning classiﬁcation and regression performance. We conduct extensive experiments based
on nine neural topologies (i.e., ﬁve MLPs, two CNNs, and
two LSTMs) and two TotalView-ITCH datasets, and compare the performance of econometric features to three other
feature sets. The ﬁrst set is based on time-sensitive and timeinsensitive features as presented in and , the second
feature set is based on technical and quantitative analysis,
introduced in , and the third one is based on feature
representations extracted automatically for the train of an
LSTM AE with a description provided in Section V-D .
IV. HANDCRAFTED FEATURE POOL
In this section we provide the nominal list (see Table 3)
of the extensive econometric feature list together with the
two other state-of-the-art handcrafted feature sets from the
literature that are based on technical and quantitative analysis
and time-insensitive and time-sensitive indicators. Description of the econometric features is seen in Appendix while the
description of technical and quantitative feature set and timesensitive and time-insensitive set extracted from the LOB can
be found in .
We extract our econometric features from both MB and
LOB and divide them into four main categories: Statistical features, volatility measures, noise measures, and price
discovery features. The ﬁrst category encompasses basic
statistical features that are widely used in the literature
(e.g., , ). The logic behind the choice of the volatility
measure features is the intimate relation between the volatility
of the price process and the price movement itself. As such,
we regard the volatility measures included in the present article to retain information useful to real-time price prediction.
This is particularly true when the predicted objective is the
next price movement. Additionally, the econometric literature
widely evidences the signiﬁcant detrimental impact of the
so-called microstructure noise in the measurement of fundamental quantities when working at the highest frequencies.
Furthermore, the noise process directly affects the underlying
price process itself and as such contributes to the observed
price movements. For these reasons we implement a number
of estimates of the characteristics of the noise process, which
we identify as the noise measures features set.2 The last
2Most of the presented measures have been developed and are consistent
estimators under broad assumptions on the underlying price process and
contaminating noise process; we will not discuss these assumptions into
details here as outside the scope of the article. Interested readers are referred
to and references within for an exhaustive review of the literature
group of features includes all those features related to the
price discovery process; i.e., those that take into account the
interaction of the two sides of the LOB. Several articles in
the literature (e.g., , ) have focused and demonstrated
the importance of accounting for the differences between the
ask and bid side in order to improve the mid-price forecasting
Each of the features in Table 3 operates under a different time duration. Time duration of the features plays
an important role in capturing information about underline
behavior of time series. More speciﬁcally, the feature extraction process consists of low frequency (e.g., technical indicators based on interpolation) and high-frequency features
(e.g., adaptive logistic regression), which complement each
other. Low frequency features identify long-term trends and
structural data components, while high-frequency features
capture discontinuities and rapid metric changes. This combination of features facilitates improves neural network perfromance (e.g., and ).
V. DEEP LEARNING
The goal of this paper is to forecast the movement of
the mid-price. The predicted output has dual information: the
direction of the mid-price movement and the prediction of the
number of events taking the mid-price to move up or down.
An efﬁcient way to do that is by using deep learning architectures. We consider three different neural networks types
(i.e., MLPs, CNNs, and LSTMs) and run them seperately.
We, then, examine their validity with respect to our optimization problem.
A. MLP FOR CLASSIFICATION AND REGRESSION
MLP (i.e., ) is a type of neural network that shows a
high degree of connectivity among its components/neurons
(see Fig. 1). The strength of this connectivity is determined
by the synaptic weights of the neural network. These synaptic
weights are determined by a differentiable nonlinear activation function. These basic characteristics of the neural network complicate the analysis of MLPs’ behavior. As a result,
several MLP architectures have to be examined in order to
see whether input data (i.e., handcrafted features) affect the
outcome/prediction. The way that an MLP can be trained is
based on a sequential data feeding process called batch learning. Batch learning is a process according to which the neural
network adjusts the synaptic weights after the presentation of
all the samples J = {x(i), d(i)}N
i=1 in the training process,
where x(i) is the input multi-dimensional vector and d(i) the
response vector of the supervised problem at instance i, and
the error function at instance i is:
e(i) = d(i) −y(i)
where d(i) is the ith element of the d(i) and y(i) is the produced
output term at instance i. The error function that we use for
our experiments is bespoke to our supervised problem and
its components are based on the binary cross entropy (for
the classiﬁcation task) and the mean squared error , as follows:
Lall = arg min
{λL1 + (1 −λ)L2}
where L1 = −t log ˆy(i) −(1 −t) log(1 −ˆy(i)), t ∈{0, 1} and
(y(i) −ˆy(i))2 with a free parameter λ, where y(i)
and ˆy(i) be the ground truth and the predicted values of the
ith training sample which belongs to RN, respectively. This
customized function is part of the backpropagation algorithm
that helps the neural network (e.g. MLP) to correct the synaptic weights in order to optimize Eq. 2. Backpropagation in our
case follows the automatic differentiation (AD) reverse mode
(i.e., ). Reverse AD facilitates the process of correcting
the synaptic weights and it can be done as follows: Initially
we deﬁne the input variables as vi−n = xi, i = 1, ..., n, all
the intermediate variables of the neural network as vi, i
1, ..., k and ym−i = vk−i, i = m −1, ..., 0 be the output
variables. Derivatives calculation is a two-step process. During the ﬁrst phase the intermediate variables vi are populated
and create the graph trace, whereas during the second phase
derivatives are calculated based on the propagation of the
adjoints ¯vi = ∂yl
∂vi . In general, the reverse AD performs the
calculations from the output to the input starting from the
output as seed:
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
FIGURE 1. Example of an MLP neural network with two hidden layers
and 4 units output.
and moves to the inputs via the intermediate states based on
the calculation:
where Pa(j) denotes the parent formation of node j and gj
the intermediate functions of the graph. The next part of the
MLP training is the learning process, which is deﬁned as
the method through which the loss function will reach the
optimal solution via proper parameter updates. For this reason
we choose the Nesterov accelerated gradient (NAG) method
incorporated into the adaptive moment estimation (Adam)
method named as Nadam by Dozat . Nadam applies the
momentum step only once and takes into consideration the
current momentum – rather than the previous momentum –
vectors. This gives us the Nadam update parameters rule:
θt+1 := θt −
(β1 ˆmt + (1 −β1)∇θtLall(θt)
where the ﬁrst (i.e., mean) and second (i.e., variance) moment
for the current momentum vector are, respectively:
with mt = β1mt−1 +(1−β1)∇θtLall(θt), vt = β2vt−1 +(1−
θtLall(θt) and learning rate η = 0.002.
B. CNN FOR CLASSIFICATION AND REGRESSION
CNN, as described in , is a type of neural network
that handles time series of multidimensional data for metric prediction. The main motivation for choosing this type
of neural network is its capability for sparse connectivity
between neural layers, for sharing the so-called tied weights
and equivariant representation properties. More speciﬁcally,
sparse connectivity can be achieved by using a kernel smaller
than the sample input. This action reduces the amount of
memory that is required for the training process. The second
FIGURE 2. Two CNN examples that demonstrate their operation
mechanisms. These two CNNs (i.e., CNN_1 and CNN_2) will later on
utilized in the experiments.
FIGURE 3. Visual representation of LSTM’s internal cell calculations.
advantage of a CNN is the use of tied weights. Tied weights
are shared among the inputs since the same amount of weights
is applied to the inputs.
CNN has three main parts: the convolution layer, the pooling layer, and the fully connected layer. The convolution layer
extracts features from the input multi-dimensional signal
expressed usually as a tensor or matrix. This process creates
linear activations that run via a non-linear activation function
such as the rectiﬁed linear activation function (ReLU) and
the Leaky ReLU. Then the pooling layer will convert the
local output based on a summary statistic related to the local
outputs (e.g. max-pooling). The last step of the process is
the connection to the fully connected layers (see examples
in Fig. 2) that will perform the classiﬁcation and regression
tasks. These tasks are based on discrete time series events
that formulate the (forward) convolution layer calculation as
yil+1,jl+1,d =
fi,j,d × xl
il+1+i,jl+1+j,d
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
TABLE 4. List of the nine deep learning models that are used for the two experimental protocols. Output, in the neural networks above, means that for
Protocol I the output is a dense layer with 1 unit and linear activation function for the regression task and a dense layer with two units and softmax
activation function. For Protocol II, the output is a dense layer with three units and softmax activation function.
where H, D,and D are the row, columns and depth dimension of the input tensor x ∈RHl×W l×Dl respectively, f ∈
RHl×W l×Dl is the ﬁlter bank, and the indexing (il+1+i, jl+1+
j, d) refers to the iterative local convolution of the ﬁlter
bank on the suggested input for the l-layer. Pooling is performed right after convolution; to conduct our experiments,
we choose the formation of max pooling. The ﬁnal step is
the use of fully connected layers. The structure of these fully
connected layers is the same as in Sec. V-A. The process that
we follow in order to train our CNN parameters (i.e., ﬁlter
banks and synaptic/tied weights) is based on batch learning
combined with reverse AD (i.e., backpropagation) as we did
for the MLP case.
C. LSTM FOR CLASSIFICATION AND REGRESSION
The ML trader has to consider the temporal behavior of
time series. The events that we have to deal with in the
LOB universe are likewise formed in a sequential manner. Sequential systems, like RNNs, are based on computational graphs and are, thus, ideal for time series analysis.
RNNs provide much ﬂexibility in terms of architecture formation, which is described in Eq. 6:
ht = f (ht−1, xt; θ)
where h and x are the state and the input at time t and
θ are the shared parameters for a transition function f at
time t. Since we use RNN for empirical calculations we
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
choose to forecast mid-price by using gated RNNs (named
LSTM) as presented in . Motivation for choosing this
type of gated RNN is its ability to create connections through
time and account for the problem of vanishing (or exploding)
gradients. Instead of applying just element-wise nonlinear
input transformations, LSTM units (see LSTM’s internal cell
calculations in Fig. 3), contain processes which that take
into consideration the sequential nature of time series. More
speciﬁcally, an LSTM cell is equipped with gates that ﬁlter
the information ﬂow by applying weights internally. The ﬁrst
pass is the forget gate vector f t
where x(t)
are the current input and hidden state
vectors of cell i at time t, respectively. The attached weight
matrices to these vectors are W f and Uf for the forget gate
vector with bf the bias term. The next pass is related to the
information to be saved to the so-called ‘‘cell state’’. The cell
state can be divided in two parts - the input vector and a tanh
layer as follows:
where g(t) is the input gate:
The last remaining part is the ﬁltered output. More specifically, the LSTM output/hidden state will be formulated by
the output gate vector o(t)
which is calculated as follows:
and the ﬁnal output h(t)
is equal to:
∗tanh(C(t)
The formation above refers to the case of a typical LSTM
neural network, which we implement in Section VI. We also
apply an attention mechanism to the LSTM architecture
in order to weight/measure the signiﬁcance of the input
sequence. We follow the implementation in and 
where the sequential LSTM outputs (i.e., hidden states H(t),
t ∈{1, ..., T}) are ﬁltered via the following steps for every
K-dimensional vector w:
M = tanh(H(t))
r = H(t) ∗α
FIGURE 4. Two LSTM examples with one main LSTM block (orange
colored box) with several hidden cell units (orange cycles). These two
LSTMs (i.e., LSTM_1 and LSTM_2) will later on utilized in the experiments.
and the ﬁnal LSTM with attention output is:
h∗= tanh(r).
Here we use the same backpropagation mechanism as we did
for MLPs. Examples of LSTM neural networks can be seen
D. FULLY AUTOMATED FEATURE EXTRACTION BASED
ON AUTOENCODERS
Autoencoders (AE) (i.e., , ) are neural networks
which operate on a self-feedback loop fashion. They do not
require any labeling system since they depend on this semisupervised protocol. This type of neural network is divided
in three main parts; the encoder, the latent representation,
and the decoder (i.e. encoder and decoder). An example of
AE can be seen in Fig. 5.
The basic structure of AE is deﬁned as a mapping from
encoder to decoder, the main objective being the following
minimization problem:
f , g = arg min
||X −(f ◦g)X||2
where f : X →F and g : F →X with X be the input raw
LOB data in the present work.
The fully automated feature extraction process is based
on the latent representation. This latent representation in
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
FIGURE 5. AE Example.
the present work plays the role of the vector representation,
which will, later on act as input to each of the suggested
nine deep neural networks. In order to use this latent space
as feature set we train an LSTM AE.3 LSTM AE has exactly
the same structure as a simple AE with the difference that
the ﬁltering is based on LSTM layers for the encoding and
decoding part. We choose LSTM AE since they take into
consideration the temporal behavior of our time series.
VI. DATA DESCRIPTION AND EXPERIMENTAL
Our objective is to provide informative handcrafted features to ML traders and market makers for the task of
mid-price movement prediction. Prediction of this movement requires in-depth analysis in terms of data selection
(e.g., liquid or illiquid stocks) and experimental protocol
development. For these reasons, our analysis consists of two
TotalView-ITCH based on two US and ﬁve Nordic stocks
and two experimental protocols. The ﬁrst protocol, named
Protocol I, is based on online prediction for every 10-block
rolling events, and we introduce it here for the ﬁrst time.
The second protocol, named Protocol II, is derived from the
literature (i.e., ) and is based on mid-price movement
prediction with 10-event lag. Both protocols are event driven,
which means that there are no-missing values. However,
Protocol II is based on independent 10-block events, which
creates a lag of 10 events. Some of the suggested features can
partially overcome this problem by ﬁnding averages or other
types of transformations inside these blocks, but, still some
information will be parsed. A possible solution to this problem comes from Protocol I where every single trading event is
taken into consideration and, as a result, there are no missing
values. We should also mention that LOB data is exposed
to bid-ask4 bounce effect which may inject bias. We leave
this topic for future research, where we plan to increase the
3Details of the training are provided in Section VII.
4Bid-ask bounce is the rapid stock’s price bounce between bid and ask
rolling event block size in Protocol I since a wider block will,
potentially, improve stability.
We utilize two TotalView-ITCH datasets based on two US
and ﬁve Nordic stocks. The time resolution of the datasets
is in milliseconds. For the US datasets, we use two stocks,
Amazon and Google, whereas for the Nordic dataset we use
Kesko Oyj, Outokumpu Oyj, Sampo Oyj, Rautaruukki, and
Wartsila Oyj. We use ten business days for both datasets
covering the periods: from 22.09.15 to 05.10.15 for the US
dataset and from 01.06.10 to 14.06.10 for the Nordic dataset,
respectively. The trading activity for these ten business days
is 13,000,000 events for the US dataset and 4,000,000 events
for the Nordic dataset. We use MBs in order to create relevant LOBs. We utilize super clustering computational power
based on HP Apollo 6000 XL230a/SL230s supercluster to
convert MBs to LOBs (i.e., LOBs are of depth 10 for both
sides). We follow several pre-processing steps before we start
training the deep learning models. A general description of
the pre-processing process can be seen in Fig. 6
B. PROTOCOL I
Both TotalView-ITCH datasets convey asynchronous information varying from events taking place at the same millisecond to events several minutes apart from each other. In order
to address this issue, we develop Protocol I, which utilizes
all the given events in an online manner. More speciﬁcally,
our protocol extracts feature representation every ten events
with an overlap of nine events for every next feature representation. We decided to use a 10-window block for our
experiments due to the frequency 5 of the stationarity present
in both datasets. In order to identify whether our time series
have unit roots, we perform an Engle-Granger cointegration
test,6 with focus on the augmented Dickey-Fuller test, on the
pair Ask −Bid prices from LOBs level I. The hypothesis
test shows that there is a constant alternation between its
states (i.e. 1 for non-stationarity and 0 for stationarity of the
suggested time series), which occurs several times during the
day. This is indicative for both datasets as seen in Figure 7.
These stationarity breaks supports the initial idea, as this
presented by many authors (e.g., , , ), that neural
networks are capable of identifying underlying processes of a
non-stationary time series. Neural networks are nonlinear and
non-parametric adaptive-learning ﬁlters which operate with
fewer assumptions compare to more traditional time series
models like ARIMA and GARCH.
A visual description of our protocol can be seen in plot (a)
in Fig. 8. The problem under consideration in Protocol I is
to predict the movement of mid-price (i.e., classiﬁcation:
up or down) together with the number of events it takes
for that movement to occur in the future to CSC superclusters
and obtain the LOBs. The next step is to apply statistical filtering (description can be found in Section VI-D). What follows is the
process of feature extraction for the four different feature sets for Protocols I & II. An HDF5 conversion takes place right
afterwards, and a MinMax normalization follows for every feature set case for both protocols. Next, each of the nine neural
networks is trained independently for the four different feature lists based on unbalanced and balanced sets. The training
process is based on python scripts, which are sent to CSC superclaster in order to obtain results for Protocol I & II.
number of events until next mid-price’s movement change).
More speciﬁcally, in order to testing performance evaluation,
we utilize f1 score for the classiﬁcation task and RMSE
(i.e., Root Mean Square Error) for the regression task.
F1 score is deﬁned as:
f 1 = 2 × Recall × Precision
Recall + Precision
Precision =
where TP, FN, and FP are the True-Positives, False-
Negatives, and False-Positives, respectively, and RMSE is
deﬁned as:
i=1(Pi −Oi)2
where Pi and Oi are the predicted and observed values of n
samples, respectively.
We have a labeling system that requires classiﬁcation
and regression. The ﬁrst part of the dual labeling format
contains the binary information 1 and −1 for the up and
down mid-price movement, respectively. The second part
of the labeling format represents the discretization of the
numeric data expressed as the steps until the next midprice change. A pictorial example of the above labeling
system is in Fig. 9. The label extraction is described as
1) d(i) = 1MP(i)−MP(i−1)>0 OR −1MP(i)−MP(i−1)<0, where
i ∈RN−1, with N be the number of the mid-prices
(MP) samples,
2) L(p) ≤d(i) < L(p + 1), 1 ≤p < Q, where L(p) is
a vector that contains the bin limits in a monotonically
increasing order and Q is the number of bins equal to
the total number of the non-zero elements in the vector
of mid-price differences.
C. PROTOCOL II
Protocol II is based on independent 10-event blocks for the
creation of the feature representations as this can be seen in
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
FIGURE 7. Hypothesis test for stationarity check, where constant
transition from state 0 to state 1 is present.
the plot (b) in Fig. 8. More speciﬁcally, feature representations are based on the information that can be extracted from
10 events each time with these 10-event blocks independent
from each other. Protocol II treats the problem of mid-price
movement prediction as a three-class classiﬁcation problem,
with three states: up, down, and stationary condition for the
mid-price movement. These changes in the mid-price are
deﬁned by means of the following calculations:
MP(t) > 1 + α
MP(t) < 1 −α
where MP(t) is the mid-price at time t, ma(t)
i=1 MP(t+1) is the average of the future mid-price events
with window size r = 10, and α determines the signiﬁcance
of the mid-price movement which is equal to 2 × 10−5.
D. DATA NORMALIZATION AND FILTERING
The next step of the pre-processing step is data normalization.
We perform a ﬁltering and a normalization method during
the feature extraction process and training. The ﬁrst one
is a statistical ﬁltering method , while the second one is
based on MinMax. More speciﬁcally, we perform the ﬁltering
methodology ﬁrst and apply it directly on the raw MB data.
The main idea of the methodology is to identify and eliminate
any observation that does not reﬂect market activity. In the
FIGURE 8. Feature extraction based on the two protocols for the task of
mid-price movement prediction. For given time series (t1, t2, ..., tN ) there
N −10 + 1 feature representations (FR) for Protocol I and N
Protocol II.
FIGURE 9. Labeling sample for the dual prediction problem of our
classification and regression objective. The left part represents the
direction (i.e., up or down) of the mid-price (MP) movement while the
right part represents the remaining steps until the next change in MP will
take place.
ﬁnancial econometrics literature this is often referred to as
data cleaning and its importance has been widely discussed
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
in the literature (e.g., , , and ).7 In more detail,
to ﬁlter the raw data for outliers we follow a two-step procedure. We initially remove all transactions recorded outside
ofﬁcial trading time and clearly misrecorded transactions.8
We then proceed by implementing a more elaborate ﬁltering
algorithm, which takes into account the statistical properties
of the series to assess the validity of each observation according to its likelihood of being an outlier.9 More speciﬁcally, for
a k size window, we identify a set of (centered) neighboring
observations for each data point. To avoid including prices too
distant in time, the window size k should be chosen according
to the trading intensity of the series. We then compute the
trimmed mean of the neighboring set and mark as an outlier
the considered observation if it falls more than α+γ standard
deviations away from the neighbors’ mean. Where γ is a
granularity parameter, which should be chosen as a multiple
of the tick size. The idea behind γ is to create a lower positive
bound for the price variation. This is particularly important
for the cleaning procedure as it is not uncommon to observe
a sequence of equal mid prices in the LOB, which would
lead to a zero variance and a consequent rejection of every
price different from the mean value. Technically, be Xi the
ith element of a time series of observations X, we check:
(|Xi −¯Xi(k)| < α ∗si(k) + γ )
where si(k) and ¯Xi(k) are respectively the sample standard
deviation and the trimmed mean computed over a neighborhood of k observations around Xi. Hence, we identify and
remove observation Xi if (21) is true and keep it otherwise.
The normalization procedure is based on MinMax for the
handcrafted features, as follows:
MM = X(i) −Xmin
Xmax −Xmin
where N is the total sample size for every feature vector X
and X(i) is the ith element of X.
VII. RESULTS & DISCUSSION
In this section, we provide results of the experiments we
conducted, based on two massive LOB datasets from the
US (i.e., two stocks: Amazon and Google) and Nordic
(i.e., ﬁve stocks: Kesko Oyj, Outokumpu Oyj, Sampo Oyj,
Rautaruukki, Wartsila Oyj) stock markets. We also discuss
the perfromance of the handcrafted feature extraction universe for mid-price movement prediction and test its efﬁcacy
against a fully automated process. What is more, we make
a head-to-head comparison of the three handcrafted feature
sets, namely: i) ‘‘Limit Order Book (LOB):L’’, based on the
works of and , ii) ‘‘Tech-Quant:T-Q’’, based on ,
and iii) ‘‘Econ:E’’, which uses econometric features. Finally,
7While the advancement of technology has drastically reduced the number of outliers and misrecorded observations, their effect on the statistical
analysis is still signiﬁcant and the implementation of a cleaning procedure
is, to this day, required to avoid biased results.
8These can be, for example, observations with a price equal to zero.
9The methodology follows closely 
FIGURE 10. Plots (a) and (b) show the process for predicting the
mid-price movement based on Protocol I and Protocol II, respectively.
In both protocols, the first step is the choice of dataset. The ML trader has
to choose the US or Nordic stock(s) (e.g., there is the option of choosing a
stock or the ’Joint’ case where all the stocks from the US or Nordic
markets used for training). The second step is to choose the feature set.
The ML trader has to choose one of the four suggested feature sets,
which are: The newly introduced econometric set, the one that is based
on technical and quantitative indicators, another one based on
time-sensitive and time-insensitive LOB features, and the last one based
on fully automated features. The third step is whether the prediction
should be based on a balanced or unbalanced set. The fourth step is the
choice of one of the suggested nine deep learning models. The final step
is the one that differs in Protocol I and Protocol II. The difference lies in
the fact that Protocol I is a combined classification and regression
optimization problem with zero event lag and Protocol II is a three-class
classification problem based on a 10-event lag.
we compare these three sets of handcrafted features with
features extracted based on an LSTM autoencoder.
Latent representations are extracted after training an
LSTM AE. This training employs an extensive grid search,
in which the best perfromance is reported. The grid search is
based on symmetrical, assymetrical, shallow, deep, overcomplete, and undercomplete LSTM AE. The provided options
vary from: i) the encoder with maximum depth up to four
hidden LSTM layers with different numbers of ﬁlters varying
according to the list {128, 64, 18, 9}, ii) the decoder with
maximum depth up to four hidden LSTM layers with different
numbers of ﬁlters varying according to the list {128, 64, 18,
9}, and iii) the latent representation with different options
varying according to the list {5, 10, 20, 50, and 130}. The
best performance reported is based on a symmetrical and
undercomplete LSTM AE of four hidden LSTM layers with
128, 64, 18, and 9 ﬁlters respectively, and 10 for the latent
representation vector size. The list of the suggested grid
search is limited; however, we believe it provides a wide
range of combinations in order to make a fair comparison of
a fully automated feature extraction process against advanced
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
TABLE 5. Protocol I: f1 scores for the US stocks. Note: Highlighted text
shows the best f1 performance for: 1) Joint/Unbalanced, 2)
Joint/Balanced, 3) Stock-Specific/Unbalanced, and 4)
Stock-Specific/Balanced cases.
handcrafted features. We should also mention that, despite the
extensive grid search on the LSTM AE, we limited our search
to up to four hidden units for the encoding and decoding parts
with four different ﬁlter options. Further analysis on the topic
is required.
In order to scrutinize the efﬁcacy of the handcrafted and
fully automated features, we use two experimental protocols
and nine deep learning models, and present results based on
unbalanced and balanced inputs. In particular, we test the four
feature sets according to two protocols: the newly introduced
experimental protocol (i.e., Protocol I) for online learning,
as we explain in Section VI, and Protocol II, that follows .
Protocol I is suitable for online learning, whose main objective is to predict when a change in the mid-price will happen
(i.e., regression problem) and in which direction, for instance,
up or down (i.e., two-class classiﬁcation problem). Protocol II
predicts the mid-price movement direction for every next
10th event, where feature representations are based on independent 10-event blocks. Authors in used a joint training
set of the ﬁve Nordic stocks for seven trading days and the
next three days as testing for mid-price movement prediction
(i.e., up, down, and stationary movement). We incorporate
the same idea here, under the name ‘‘Joint’’, and we also use
the same 7-3 training and testing proportion for each stock
individually for both US and Nordic datasets. A general idea
for both protocols can be seen in Fig. 10.
Protocol I and Protocol II use three types of deep neural
networks as classiﬁers and regressors. In particular, we utilize
TABLE 6. Protocol I: RMSE scores for the US stocks. Note: Highlighted
text shows the best RMSE performance for: 1) Joint/Unbalanced, 2)
Joint/Balanced, 3) Stock-Specific/Unbalanced, and 4)
Stock-Specific/Balanced cases.
ﬁve different MLPs, two CNNs, and two LSTMs. Motivation for choosing MLPs is the fact that such a simple
neural network can perform extremely well when descriptive
handcrafted features are used as input. The next type of
neural network that we use is CNN. The ﬁrst CNN, named
‘‘CNN_1’’ is based on , whereas the second one, named
‘‘CNN_2’’ is based on the grid search that we describe below.
The last type of neural network that we utilize is LSTM.
We use two different architectures: the ﬁrst one, named
‘‘LSTM_1’’, is based on , and the second one, named
‘‘LSTM_2’’ is based on LSTM with attention mechanism.
In total, we train independently nine deep neural networks
for each of the two experimental protocols separately. Details
of these nine topologies can be found in Table 4.
We report results for nine different neural networks, two of
which are based on existing works as shown above. For the
remaining seven neural networks we conduct the following
grid search:
• For MLPs we set a limit up to three hidden layers,
where for the number of nodes we set the options {
4, 9, 18, 64, 128, 256, and 512} nodes per layer and
for dropout 20% and 50%. We report results based on
ﬁve MLPs since these neural networks achieved good
results for several cases (see Section VII-A for results
discussion).
• For CNN we conduct an extensive grid search limited
to up to three convolutional layers and RMSE (right column plots) scores for the nine deep learning models based on the US data.
types) with 8, 16, and 32 ﬁlters and kernels size options
{4×10, 4×20, 4×30, and 4×40 for the 2-dimensional
case and 3 and 4 for the 1-dimensional case}. Dropout
options are restricted to 20% and 50%. We report only
one CNN since we noticed that shallower CNN architectures had very poor performance and no signiﬁcant
difference for the deeper ones.
• For LSTM we follow the same approach with up to three
hidden layers and ﬁve options for hidden LSTM units
{9, 18, 32, 64, 128} and the option of attention layer.
We report only one LSTM performance since all other
topologies performed worse for our task.
The training of these nine neural networks takes place at
CSC super-cluster where we use Pascal P100 and K80 GPUs.
We use multi-GPUs, under Keras (i.e., ) framework,
in order to reduce the training time. The models, apart from
CNN_1 and LSTM_1, use the Nesterov-Adam optimizer with
a learning rate of 0.002, with mean squared error and binary
cross-entropy for the dual output of Protocol I where this
dual output is weighted by 0.01 and 0.99, respectively, and
categorical cross-entropy as loss function for Protocol II.
Additionally, we use 250 epochs to train our models with
data shufﬂing and validation ratio of 0.2. Finally, in order
to control overﬁtting we utilize Dropout to the majority of
the suggested neural networks. By dropping out some nodes
(i.e., a dropped out node have a zero output) from neural
network topologies we control node dependencies and we
achieve more robust results.
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
FIGURE 12. F1 (left column plots) and RMSE (right column plots) scores for the nine deep learning models based on the Nordic data.
A. RESULTS
We present our results in separate tables for Protocol I (see
Table 5 - Table 10) and Protocol II (see Appendix VIII-B).
For each protocol, we split the results (i.e., f1 score and
RMSE for Protocol I and f1 scores for Protocol II) for both
US and Nordic datasets. We would like to mention that
results derived from the LSTM AE, for both f1 and RMSE
scores, are presented in separate tables (see Tables 9 & 10 for
Protocol I and Tables A.2 & A.4 for Protocol II). Since handcrafted feature results overperformed the fully automated
feature set we emphasize more on their perfromance by
providing tables together with bar plots (see Fig. 11 & 12).
Each of the tables contains the full head-to-head comparison for the three handcrafted features sets for each of the
nine different deep learning models separately. For instance,
Table 7 contains f1 scores for the Nordic stocks based on Protocol I. The table has ﬁve main columns (i.e., Model, Stock,
Econ, Tech-Quant, and LOB) and six subcolumns divided
into three pairs (i.e., UnBal. and Bal.). The ﬁrst main column
contains the nine deep neural networks; the second main
column contains the ﬁve independent and different Nordic
stocks, in which the sixth row for every model is the joint
training set based on these ﬁve stocks; and the third, fourth
and ﬁfth main columns represent the three handcrafted feature sets. Moreover, for every feature set, we present results
for unbalanced and balanced cases, whereas for the balanced
cases we use random undersampling for the majority class.
Even though balanced datasets do not project a realistic
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
TABLE 7. Protocol I: f1 scores for the Nordic stocks. Note: Highlighted
text shows the best f1 performance for: 1) Joint/Unbalanced,
2) Joint/Balanced, 3) Stock-Specific/Unbalanced, and 4)
Stock-Specific/Balanced cases.
trading scenario (i.e., trading fees are not applicable), it is
important to give an equal opportunity to the minority class,
which can be an ML trader’s trading position. More specifically, for Protocol I and the classiﬁcation task, the Nordic
dataset has 45% for the downward movement and 55% for
the upward, while for the US dataset is 47% for the downward movement and 53% for the upward. The undersampling
offers an 85% data reduction for the Nordic set and 90%
for the US set. For better interpretation of Protocol I we
provide bar plots which show the reaction of every deep
learning model and dataset for the unbalanced and balanced
cases (see Fig. 11 and Fig. 12). Protocol II and the Nordic
dataset exhibits a 75% for the stationary condition, with the
TABLE 8. Protocol I: RMSE scores based on Nordic stocks for the
handcrafted features. Note: Highlighted text shows the best RMSE
performance for: 1) Joint/Unbalanced, 2) Joint/Balanced,
3) Stock-Specific/Unbalanced, and 4) Stock-Specific/Balanced cases.
remaining 25% being equally divided to the upward and
downward mid-price movement before undersampling. For
the US dataset 73% belongs to the stationary condition,
20% to the upward movement and the remaining 7% to the
downward movement. The undersampling offers a 30% data
reduction for the Nordic dataset and 10% data reduction for
the US dataset.
B. DISCUSSION
The conducted experiments reveal some interesting results
for both experimental protocols and datasets selection. Both
protocols forecast the mid-price movement, with Protocol I
forecasting the mid-price movement every next event and
Protocol I with a lag of 10 events. Protocol I provides
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
TABLE 9. Protocol I: f1 and RMSE scores based on US stocks for the
fully-automated features. Note: Highlighted text shows the best
f1 performance for: 1) Joint/Unbalanced, 2) Joint/Balanced,
3) Stock-Specific/Unbalanced, and 4) Stock-Specific/Balanced cases.
more information regarding the high-frequency activity since
it takes into consideration every trading event. We cannot
directly compare the two protocols since both tackle the
problem of mid-price forecasting from a different angle.
We also observe that in general, larger data samples increase
deep learning models’ performance. However, by focusing
on each protocol seperately, we can see that: for Protocol I,
the best classiﬁcation score comes from US dataset and best
regression score from Nordic dataset, while, for Protocol II,
the best classiﬁcation score comes again from the US dataset.
Each one of the nine neural networks has to perform a dual
task, regression and classiﬁcation simultaneously. To begin
with, the Joint (i.e., the full range of stocks is used for training) reports for the Nordic dataset the best f1 performance
that comes from MLP_3, for both unbalanced and balanced
datasets under the Econ feature set with 53% and 56% for
the Tech-Quant set. This MLP did not perform well for the
regression task where the RMSE was above 165.29. For the
stock speciﬁc case: we achieve the best classiﬁcation performance of 53% f1 score for Outokumpu Oyj under MLP_4 and
the Econ feature set with RMSE of 98.44. Thisvstock-speciﬁc
performance of the MLP_4 is the best trade-off between classiﬁcation and regression for the Nordic dataset. If we want to
focus on the regression task only, we can choose the more
advanced model, LSTM_2, with RMSE of approximately 24
for both unbalanced and balanced Tech-Quant feature sets for
Kesko Oyj.
TABLE 10. Protocol I: f1 and RMSE scores based on Nordic stocks for the
fully-automated features. Note: Highlighted text shows the best RMSE
performance for: 1) Joint/Unbalanced, 2) Joint/Balanced,
3) Stock-Specific/Unbalanced, and 4) Stock-Specific/Balanced cases.
For the US dataset, the new protocol presents more interesting results. For the Joint case, where both Amazon and
Google used for training, the LSTM_2 achieves 59% f1 score
and RMSE of 89.69, whereas, for the stock speciﬁc case,
LSTM_1 under the Tech-Quant feature set achieves 58%
f1 score and high RMSE of 123.36 for Google and the unbalanced case. If we focus only on the regression part, we can
choose the entire MLP universe and the Econ feature set
for Amazon and the Joint case. The newly introduced Econ
feature set performed very well for the regression task also
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
for LSTM_2 across the entire protocol for the unbalanced
dataset. One more interesting observation is that the Econ
feature set together with the shallower MLP_1 and the balanced set reports very low RMSE for Amazon, Google, and
the Joint cases, respectively. That means that the Econ feature
set, for the Amazon and Joint case, were able to predict
that the mid-price will change its direction in a millisecond
duration. Here, it is vital to report that the daily trading
activity, for the US and Nordic stocks, contains several trades
with the same timestamp/millisecond. Approximately 30% of
the trades, in the US dataset, occur in a millisecond, whereas
this percentage for the Nordic dataset is 36%.
For Protocol II and the Joint case we achieve the best forecasting performance of 51% f1 for the Nordic dataset based
on MLP_4 (which is one of our deeper MLP architectures)
under the Tech-Quant feature set and the unbalanced case. For
the Joint case in the US dataset, we achieve the best f1 performance of 65% based on MLP_4 under the Tech-Quant
feature set and the balanced case. In terms of individual stock
performance for the Nordic case we achieve 63% f1 score
for Kesko Oyj, and our shallower MLP (i.e., MLP_1) under
the Tech-Quant set, while for the US dataset we achieve an
f1 performance of 65% for Google based on MLP_4 for the
balanced case. We can see that MLPs for Protocol II were
able to retain the information that the Tech-Quant feature set
carries. The majority of the Tech-Quant features was derived
from technical analysis, a type of analysis which is based on
geometrical pattern identiﬁcation of agglutinated times series
like ours. What is more, the data size affected the performance
of models and feature sets. For instance, Kesko Oyj, which
scored the highest f1 score, is the stock with the least daily
trading activity compared to the rest of the Nordic stocks
and of course compared to the massive US dataset. Finally,
we would like to point out that we limited the experiments
to two US and ﬁve Nordic stocks; we leave the extension
of the present evaluation on wider LOB datasets for future
research that will help us to identify similarities among stock
categories and time periods.
VIII. CONCLUSION
In this paper, we extracted handcrafted features based on the
econometric literature for mid-price prediction using deep
learning techniques. Our work is the ﬁrst of its kind since
we do not only utilize an extensive feature set list, based on
econometrics for the mid-price prediction task, but we also
provide a fair comparison with two other existing state-ofthe-art handcrafted and fully automated feature sets . Our
extensive experimental setup, based on liquid and illiquid
stocks (i.e., two US and ﬁve Nordic stocks) showed superiority of the suggested handcrafted feature sets against the
fully automated process derived from an LSTM AE. What
is more, our research sheds light on the area of deep learning and feature engineering by providing information based
on online mid-price predictions. Our ﬁndings suggest that
extensive analysis of the input signal leads to high forecasting
performance even with simpler neural network architects like
shallow MLPs, particularly when advanced features capture
the relevant information edge. More speciﬁcally, econometric
features and deep learning predicted that the mid-price would
change direction in a millisecond duration for Amazon and
the Joint (i.e., training on both Amazon and Google) cases.
Although these results are promising, our study here also
suggests that selection of features and models should be
differentiated for liquid and illiquid stocks
A. FEATURE POOL
See Figure 11–12 and Tables 7–10.
1) STATISTICAL FEATURES
• Mid price is deﬁned as:
MP = Askbest + Bidbest
• Financial duration is deﬁned as:
FD = Tt −Tt−1,
where T denotes the time instance at time t.
• Average mid-price ﬁnancial duration is deﬁned as:
T1, T2, . . . , TN
P1, P2, . . . , PN
k=1 are the partial cumulative
sums of time and price differences for every LOB level
for N samples.
• Mid price deeper levels are equal to:
DMP = Askl + Bidl
, l = 2 : 10
where l denotes the depth of the LOB.
• Log returns are deﬁned as:
r(X)i = Xi −Xi−1;
where Xi is the logarithmic price
2) VOLATILITY MEASURES
The features in this category aim to estimate, either the integrated variance (IV), that is the process
or, more generally, the quadratic variation (QV)
Here X is the logarithmic price of some given asset.
We assume that Xt follows an Itô semimartingale; that is,
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
where b is locally bounded, σ is cádlág and predictable, and
W is a standard Weiner process, ζ is a thin (i.e., ﬁnite) process
mapping the jump size, and N is the counting process associated to the jump times of X. We deﬁne 1n the time elapsed
between two adjacent observations; speciﬁcally, if we assume
the observations are equidistant in time we have 1n = ⌊t
As we do not work in calendar time we will have 1n = 1
• Realized variance
The realized variance is the most natural estimator
of the quadratic variation process and is equal to:
• Realized kernel
Realized kernels are used to obtain a noise robust
estimate of QV as follows:
RKt = γ0(X1n) +
{γh(X1n) + γ−h(X1n)},
with H the kernel bandwidth, γh(X1n) the autocovariation process, k is the kernel function of choice. In particular we use a non-ﬂat-top Parzen and our implementation follows closely .
• Realized pre-averaged variance
The pre-averaged realized variance is akin to the
realized kernel estimator (in fact they are asymptotically
equivalent). As for the realized kernel, the pre-averaged
realized variance is used to retrieve a noise-free measurement of the quadratic variation of our price process
and it is calculated as follows:
i )2 −ψ11n
As before we have H the kernel bandwidth and θ the preaveraging horizon. Further, given a nonzero real-valued
function g : →R with g(0) = g(1) = 0 and
which is further continuous and piecewise continuously
differentiable such that its derivative g′ is piecewise
Lipschitz. Then, we deﬁne:
(g′(s))2ds,
(g(s))2ds.
In our application we follow and set H = θ√n and
θ = 1, g(x) = x ∧(1 −x). Hence we will have ψ1 = 1
and, ψ2 = 1
• Realized semi-variance (+, −)
semivariances measure upside and downside risk respectively, as follows:
RSV +(X)t =
i 1(r(X)i>0)
RSV −(X)t =
i 1(r(X)i<0)
where 1 is a simple indicator function.
• Realized bipower variation
The realized bipower variation measures the diffusive component of the price process, isolating it from the
variation caused by the jump components and it is equal
BV(X)t := π
|r(X)i||r(X)i−1|
• Realized bipower variation (lag 2)
BV(X)t := π
|r(X)i||r(X)i−2|
• Realized bipower semivariance (+, −)
Realized bipower semivariances are used to measure the upside and downside risk of the diffusive component:
BV +(X)t : = π
|r(X)i||r(X)i−1|1(r(X)i>0)
BV −(X)t : = π
|r(X)i||r(X)i−1|1(r(X)i<0). (A.15)
• Jump variation
We use a modiﬁed version of the jump variation estimator which is both non-negative and consistent.
As hinted by the name, the jump variation estimator
provides a measures of the discontinuous variability
component:
JV(X)t := max(RV(X)t −BV(X)t, 0).
• Spot volatility
We only compute the spot volatility (i.e., and )
estimates on the block. The spot volatility measures
the instantaneous volatility. The deﬁnition is consistent
with the terminology commonly used in the literature
on parametric stochastic volatility models in continuoustime:
SV(X)t := lim
h→0{E[([X, X]t+h −[X, X]t)/h]|Ft}.
with h →0 being the time interval upon which the
measure is computed.
• Average spot volatility
The average spot volatility provides an historical average of the estimated spot volatilities:
SV(X)t := 1
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
TABLE 11. Protocol II: f1 scores based on US stocks for the handcrafted
features. Note: Highlighted text shows the best f1 performance for:
1) Joint/Unbalanced, 2) Joint/Balanced, 3) Stock-Specific/Unbalanced,
and 4) Stock-Specific/Balanced cases.
3) NOISE AND UNCERTAINTY MEASURES
In this category, we incorporate two kinds of measures which
are intimately linked to each other. We provide three different
estimates for the integrated quarticity and two different estimates for the variance of the contaminating noise process.
The integrated quarticity measures the degree of estimation
error in the realized variance and can be consistently estimated through the realized quarticity estimators presented
below for a ﬁxed window size of 2000 events. The noise
variance estimates provide a measure of the intensity of the
noise process affecting the underlying price, as follows:
with the noise variance estimates providing a measure of the
contaminating:
• Realized quarticity :
(Xi −Xi−1)4
• Realized quarticity Tripower
The tri-power quarticity is a generalization of the
realized bipower variation and is a consistent estimator
for the integrated quarticity in the presence of jumps:
RQt = nµ−3
|r(X)i|4/3|r(X)i−1|4/3|r(X)i−2|4/3
TABLE 12. Protocol II: f1 scores based on US stocks for the fully
automated features. Note: Highlighted text shows the best
f1 performance for: 1) Joint/Unbalanced, 2) Joint/Balanced,
3) Stock-Specific/Unbalanced, and 4) Stock-Specific/Balanced cases.
with µp = E (|Z|p),where Z denotes a standard normally distributed random variable.
• Realized quarticity Quadpower
A generalization of multipower variation measures led
to the realized quadpower quarticity estimator proposed
by and it is equal to:
RQt = nµ−4
|r(X)i||r(X)i−1||r(X)i−2||r(X)i−3|
• Noise variance :
(r(X)ir(X)i−1).
• Noise variance :
(Xi −Xi−1)2.
4) PRICE DISCOVERY FEATURES
• Mid price weighted by order imbalance:
MidPricet = Ask ∗VAsk + Bid ∗VBid
VAsk + VBid
• Volume imbalance:
VolImbalance =
VAsk + VBid
VOLUME 7, 2019
A. Ntakaris et al.: Feature Engineering for Mid-Price Prediction With Deep Learning
TABLE 13. Protocol II: f1 scores based on Nordic stocks for the
handcrafted features. Note: Highlighted text shows the best
f1 performance for: 1) Joint/Unbalanced, 2) Joint/Balanced,
3) Stock-Specific/Unbalanced, and 4) Stock-Specific/Balanced cases.
• Bid-ask spread:
BAspread = Ask −Bid.
• Normalized bid-ask spread
The normalized bid-ask spread expresses the spread as
the number of ticks between the bid and the ask price:
BAspread = Ask −Bid
TickSize .
B. PROTOCOL II RESULTS
See Tabels 11–14.
TABLE 14. Protocol II: f1 scores based on Nordic stocks for the fully
automated features. Note: Highlighted text shows the best
f1 performance for: 1) Joint/Unbalanced, 2) Joint/Balanced,
3) Stock-Specific/Unbalanced, and 4) Stock-Specific/Balanced cases.
ACKNOWLEDGMENT
The authors would like to thank CSC-IT Center for Science,
Finland, for generous computational resources. The views
and conclusions expressed in this paper are solely those of the
authors and do not necessarily reﬂect the views of Danmarks
Nationalbank.