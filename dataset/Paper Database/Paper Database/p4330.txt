The Unreasonable Eﬀectiveness of Noisy Data
for Fine-Grained Recognition
Jonathan Krause1(B), Benjamin Sapp2, Andrew Howard2, Howard Zhou2,
Alexander Toshev2, Tom Duerig2, James Philbin2, and Li Fei-Fei1
1 Stanford University, Stanford, USA
{jkrause,feifeili}@cs.stanford.edu
2 Google, Mountain View, USA
 , , ,
 , , 
Abstract. Current approaches for ﬁne-grained recognition do the
following: First, recruit experts to annotate a dataset of images, optionally also collecting more structured data in the form of part annotations
and bounding boxes. Second, train a model utilizing this data. Toward
the goal of solving ﬁne-grained recognition, we introduce an alternative
approach, leveraging free, noisy data from the web and simple, generic
methods of recognition. This approach has beneﬁts in both performance
and scalability. We demonstrate its eﬃcacy on four ﬁne-grained datasets,
greatly exceeding existing state of the art without the manual collection of even a single label, and furthermore show ﬁrst results at scaling
to more than 10,000 ﬁne-grained categories. Quantitatively, we achieve
top-1 accuracies of 92.3 % on CUB-200-2011, 85.4 % on Birdsnap, 93.4 %
on FGVC-Aircraft, and 80.8 % on Stanford Dogs without using their
annotated training sets. We compare our approach to an active learning
approach for expanding ﬁne-grained datasets.
Introduction
Fine-grained recognition refers to the task of distinguishing very similar categories, such as breeds of dogs , species of birds , or models of
cars . Since its inception, great progress has been made, with accuracies on
the popular CUB-200-2011 bird dataset steadily increasing from 10.3 % 
to 84.6 % .
The predominant approach in ﬁne-grained recognition today consists of two
steps. First, a dataset is collected. Since ﬁne-grained recognition is a task inherently diﬃcult for humans, this typically requires either recruiting a team of
experts or extensive crowd-sourcing pipelines . Second, a method
Work done while J. Krause was interning at Google.
Electronic supplementary material The online version of this chapter (doi:10.
1007/978-3-319-46487-9 19) contains supplementary material, which is available to
authorized users.
⃝Springer International Publishing AG 2016
B. Leibe et al. (Eds.): ECCV 2016, Part III, LNCS 9907, pp. 301–320, 2016.
DOI: 10.1007/978-3-319-46487-9 19
J. Krause et al.
Fig. 1. There are more than 14,000 species of birds in the world. In this work we
show that using noisy data from publicly-available online sources can not only improve
recognition of categories in today’s datasets, but also scale to very large numbers of
ﬁne-grained categories, which is extremely expensive with the traditional approach of
manually collecting labels for ﬁne-grained datasets. Here we show 4,225 of the 10,982
categories recognized in this work.
for recognition is trained using these expert-annotated labels, possibly also
requiring additional annotations in the form of parts, attributes, or relationships . While methods following this approach have shown some success , their performance and scalability is constrained by the paucity
of data available due to these limitations. With this traditional approach it is
prohibitive to scale up to all 14,000 species of birds in the world (Fig. 1), 278,000
species of butterﬂies and moths, or 941,000 species of insects .
In this paper, we show that it is possible to train eﬀective models of ﬁnegrained recognition using noisy data from the web and simple, generic methods of recognition . We demonstrate recognition abilities greatly exceeding current state of the art methods, achieving top-1 accuracies of 92.3 % on
CUB-200-2011 , 85.4 % on Birdsnap , 93.4 % on FGVC-Aircraft , and
80.8 % on Stanford Dogs without using a single manually-annotated training
label from the respective datasets. On CUB, this is nearly at the level of human
experts . Building upon this, we scale up the number of ﬁne-grained classes
recognized, reporting ﬁrst results on over 10,000 species of birds and 14,000
species of butterﬂies and moths.
The rest of this paper proceeds as follows: After an overview of related work
in Sect. 2, we provide an analysis of publicly-available noisy data for ﬁne-grained
recognition in Sect. 3, analyzing its quantity and quality. We describe a more
traditional active learning approach for obtaining larger quantities of ﬁne-grained
data in Sect. 4, which serves as a comparison to purely using noisy data. We
present extensive experiments in Sect. 5, and conclude with discussion in Sect. 6.
The Unreasonable Eﬀectiveness of Noisy Data for Fine-Grained Recognition
Related Work
Fine-Grained Recognition. The majority of research in ﬁne-grained recognition has focused on developing improved models for classiﬁcation . While these works
have made great progress in modeling ﬁne-grained categories given the limited
data available, very few works have considered the impact of that data .
Xu et al. augment datasets annotated with category labels and parts with web
images in a multiple instance learning framework, and Xie et al. do multitask
training, where one task uses a ground truth ﬁne-grained dataset and the other
does not require ﬁne-grained labels. While both of these methods have shown
that augmenting ﬁne-grained datasets with additional data can help, in our work
we present results which completely forgo the use of any curated ground truth
dataset. In one experiment hinting at the use of noisy data, Van Horn et al. 
show the possibility of learning 40 bird classes from Flickr images. Our work validates and extends this idea, using similar intuition to signiﬁcantly improve performance on existing ﬁne-grained datasets and scale ﬁne-grained recognition to over
ten thousand categories, which we believe is necessary in order to fully explore the
research direction.
Considerable work has also gone into the challenging task of curating ﬁnegrained datasets and developing interactive methods
for recognition with a human in the loop . While these works have
demonstrated eﬀective strategies for collecting images of ﬁne-grained categories,
their scalability is ultimately limited by the requirement of manual annotation.
Our work provides an alternative to these approaches.
Learning from Noisy Data. Our work is also inspired by methods that
propose to learn from web data or reason about label
noise . Works that use web data typically focus on detection and
classiﬁcation of a set of coarse-grained categories, but have not yet examined the
ﬁne-grained setting. Methods that reason about label noise have been divided in
their results: some have shown that reasoning about label noise can have a substantial eﬀect on recognition performance , while others demonstrate little
change from reducing the noise level or having a noise-aware model .
In our work, we demonstrate that noisy data can be surprisingly eﬀective for
ﬁne-grained recognition, providing evidence in support of the latter hypothesis.
Noisy Fine-Grained Data
In this section we provide an analysis of the imagery publicly available for ﬁnegrained recognition, which we collect via web search.1 We describe its quantity,
distribution, and levels of noise, reporting each on multiple ﬁne-grained domains.
1 Google image search: 
J. Krause et al.
Fig. 2. Distributions of the number of images per category available via image search
for the categories in CUB, Birdsnap, and L-Bird (far left), FGVC and L-Aircraft (middle left), and L-Butterﬂy (middle right). At far right we aggregate and plot the average
number of images per category in each dataset in addition to the training sets of each
curated dataset we consider, denoted CUB-GT, Birdsnap-GT, and FGVC-GT.
Categories
We consider four domains of ﬁne-grained categories: birds, aircraft, Lepidoptera
(a taxonomic order including butterﬂies and moths), and dogs. For birds and
Lepidoptera, we obtained lists of ﬁne-grained categories from Wikipedia, resulting in 10,982 species of birds and 14,553 species of Lepidoptera, denoted L-Bird
(“Large Bird”) and L-Butterﬂy. For aircraft, we assembled a list of 409 types of
aircraft by hand (including aircraft in the FGVC-Aircraft dataset, abbreviated FGVC). For dogs, we combine the 120 dog breeds in Stanford Dogs 
with 395 other categories to obtain the 515-category L-Dog. We evaluate on
two other ﬁne-grained datasets in addition to FGVC and Stanford Dogs: CUB-
200-2011 and Birdsnap , for a total of four evaluation datasets. CUB
and Birdsnap include 200 and 500 species of common birds, respectively, FGVC
has 100 aircraft variants, and Stanford Dogs contains 120 breeds of dogs. In
this section we focus our analysis on the categories in L-Bird, L-Butterﬂy, and
L-Aircraft in addition to the categories in their evaluation datasets.
Images from the Web
We obtain imagery via Google image search results, using all returned images
as images for a given category. For L-Bird and L-Butterﬂy, queries are for the
scientiﬁc name of the category, and for L-Aircraft and L-Dog queries are simply
for the category name (e.g. “Boeing 737-200” or “Pembroke Welsh Corgi”).
Quantifying the Data. How much ﬁne-grained data is available? In Fig. 2 we
plot distributions of the number of images retrieved for each category and report
aggregates across each set of categories. We note several trends: Categories in
existing datasets, which are typically common within their ﬁne-grained domain,
have more images per category than the long-tail of categories present in the
larger L-Bird, L-Aircraft, or L-Butterﬂy, with the eﬀect most pronounced in
L-Bird and L-Butterﬂy. Further, domains of ﬁne-grained categories have substantially diﬀerent distributions, i.e. L-Bird and L-Aircraft have more images
The Unreasonable Eﬀectiveness of Noisy Data for Fine-Grained Recognition
Fig. 3. Examples of cross-domain noise for birds, butterﬂies, airplanes, and dogs.
Images are generally of related categories that are outside the domain of interest,
e.g. a map of a bird’s typical habitat or a t-shirt containing the silhouette of a dog.
per category than L-Butterﬂy. This makes sense – ﬁne-grained categories and
domains of categories that are more common and have a larger enthusiast base
will have more imagery since more photos are taken of them. We also note that
results tend to be limited to roughly 800 images per category, even for the most
common categories, which is likely a restriction placed on public search results.
Most striking is the large diﬀerence between the number of images available
via web search and in existing ﬁne-grained datasets: even Birdsnap, which has
an average of 94.8 images per category, contains only 13 % as many images as
can be obtained with a simple image search. Though their labels are noisy, web
searches unveil an order of magnitude more data which can be used to learn
ﬁne-grained categories.
In total, for all four datasets, we obtained 9.8 million images for 26,458 categories, requiring 151.8 GB of disk space. All urls will be released.
Noise. Though large amounts of imagery are freely available for ﬁne-grained
categories, focusing only on scale ignores a key issue: noise. We consider two
types of label noise, which we call cross-domain noise and cross-category noise.
We deﬁne cross-domain noise to be the portion of images that are not of any
category in the same ﬁne-grained domain, i.e. for birds, it is the fraction of
images that do not contain a bird (examples in Fig. 3). In contrast, cross-category
noise is the portion of images that have the wrong label within a ﬁne-grained
domain, i.e. an image of a bird with the wrong species label.
To quantify levels of cross-domain noise, we manually label a 1,000 image
sample from each set of search results, with results in Fig. 4. Although levels of
noise are not too high for any set of categories (max. 34.2 % for L-Butterﬂy), we
notice an interesting correlation: cross-domain noise decreases moderately as the
number of images per category (Fig. 2) increases. We hypothesize that categories
with many search results have a corresponding large pool of images to draw
results from, and thus actual search results will tend to be higher-precision.
In contrast to cross-domain noise, cross-category noise is much harder to
quantify, since doing so eﬀectively requires ground truth ﬁne-grained labels of
query results. To examine cross-category noise from at least one vantage point,
J. Krause et al.
cross-domain
search results for each domain.
percentage
retained after ﬁltering.
we show the confusion matrix of given versus predicted labels on 30 categories
in the CUB test set and their web images in Fig. 6, left and right, which
we generate via a classiﬁer trained on the CUB training set, acting as a noisy
proxy for ground truth labels. In these confusion matrices, cross-category noise
is reﬂected as a strong oﬀ-diagonal pattern, while cross-domain noise would
manifest as a diﬀuse pattern of noise, since images not of the same domain are
an equally bad ﬁt to all categories. Based on this interpretation, the web images
show a moderate amount more cross-category noise than the clean CUB test set,
though the general confusion pattern is similar.
We propose a simple, yet eﬀective strategy to reduce the eﬀects of crosscategory noise: exclude images that appear in search results for more than one
category. This approach, which we refer to as ﬁltering, speciﬁcally targets images
for which there is explicit ambiguity in the category label (examples in Fig. 7).
As we demonstrate experimentally, ﬁltering can improve results while reducing
training time via the use of a more compact training set – we show the portion
of images kept after ﬁltering in Fig. 5. Agreeing with intuition, ﬁltering removes
more images when there are more categories. Anecdotally, we have also tried a
few techniques to combat cross-domain noise, but initial experiments did not
see any improvement in recognition so we do not expand upon them here. While
reducing cross-domain noise should be beneﬁcial, we believe that it is not as
important as cross-category noise in ﬁne-grained recognition due to the absence
of out-of-domain classes during testing.
Data via Active Learning
In this section we brieﬂy describe an active learning-based approach for collecting
large quantities of ﬁne-grained data. Active learning and other human-in-theloop systems have previously been used to create datasets in a more cost-eﬃcient
way than manual annotation , and our goal is to compare this more
traditional approach with simply using noisy data, particularly when considering
the application of ﬁne-grained recognition. In this paper, we apply active learning
to the 120 dog breeds in the Stanford Dogs dataset.
The Unreasonable Eﬀectiveness of Noisy Data for Fine-Grained Recognition
Fig. 6. Confusion matrices of the predicted label (column) given the provided label (row) for 30 CUB categories
on the CUB test set (left) and search
results for CUB categories (right). For
visualization purposes we remove the
Greater Necklaced Laughingthrush
Spot-Breasted Laughingthrush
Black-Hooded Laughingthrush
Bare-Headed Laughingthrush
Keel-Billed Toucan
Chestnut-Mandibled Toucan
Cuban Emerald
Cuban Tody
Cuban Vireo
Key West Quail-Dove
Black-Headed Saltator
Red-Billed Pigeon
Northern Potoo
Bushy-Crested Jay
Fig. 7. Examples of images removed
via ltering and the categories whose
results they appeared in. Some share
similar names (left examples), while
others share similar locations (right
examples).
Our system for active learning begins by training a classiﬁer on a seed set
of input images and labels (i.e. the Stanford Dogs training set), then proceeds
by iteratively picking a set of images to annotate, obtaining labels with human
annotators, and re-training the classiﬁer. We use a convolutional neural network for the classiﬁer, and now describe the key steps of sample
selection and human annotation in more detail.
Sample Selection. There are many possible criterion for sample selection .
We employ conﬁdence-based sampling: For each category c, we select the b ˆP(c)
images with the top class scores fc(x) as determined by our current model, where
ˆP(c) is a desired prior distribution over classes, b is a budget on the number of
images to annotate, and fc(x) is the output of the classiﬁer. The intuition is as
follows: even when fc(x) is large, false positives still occur quite frequently – in
Fig. 8 left, observe that the false positive rate is about 20 % at the highest conﬁdence range, which might have a large impact on the model. This contrasts with
approaches that focus sampling in uncertain regions . We ﬁnd that
images sampled with uncertainty criteria are typically ambiguous and diﬃcult
or even impossible for both models and humans to annotate correctly, as demonstrated in Fig. 8 bottom row: unconﬁdent samples are often heavily occluded, at
unusual viewpoints, or of mixed, ambiguous breeds, making it unlikely that they
can be annotated eﬀectively. This strategy is similar to the “expected model
change” sampling criteria , but done for each class independently.
Human Annotation. Our interface for human annotation of the selected
images is shown in Fig. 9. Careful construction of the interface, including the
addition of both positive and negative examples, as well as hidden “gold standard” images for immediate feedback, improves annotation accuracy considerably (see Supplementary Material for quantitative results). Final category decisions are made via majority vote of three annotators.
J. Krause et al.
Experiments
Implementation Details
The base classiﬁer we use in all noisy data experiments is the Inception-v3 convolutional neural network architecture , which is among the state of the art
methods for generic object recognition . Learning rate schedules are
determined by performance on a holdout subset of the training data, which
is 10 % of the training data for control experiments training on ground truth
datasets, or 1 % when training on the larger noisy web data. Unless otherwise
noted, all recognition results use as input a single crop in the center of the image.
Our active learning comparison uses the Yahoo Flickr Creative Commons
100M dataset as its pool of unlabeled images, which we ﬁrst pre-ﬁlter with
a binary dog classiﬁer and localizer , resulting in 1.71 million candidate dogs.
We perform up to two rounds of active learning, with a sampling budget B of
10× the original dataset size per round2. For experiments on Stanford Dogs, we
use the CNN of , which is pre-trained on a version of ILSVRC with
dog data removed, since Stanford Dogs is a subset of ILSVRC training data.
most conf dent:
unconf dent:
Fig. 8. Left: Classiﬁer conﬁdence versus false positive rate on 100,000 randomly sampled from Flickr images (YFCC100M ) with dog detections. Even the most conﬁdent
images have a 20 % false positive rate. Right: Samples from Flickr. Rectangles below
images denote correct (green), incorrect (red), or ambiguous (yellow). Top row: Samples with high conﬁdence for class “Pug” from YFCC100M. Bottom row: Samples
with low conﬁdence score for class “Pug”. (Color ﬁgure online)
Fig. 9. Our tool for binary annotation of ﬁne-grained categories. Instructional positive
images are provided in the upper left and negatives are provided in the lower left.
2 To be released.
The Unreasonable Eﬀectiveness of Noisy Data for Fine-Grained Recognition
Table 1. Comparison of data source used during training with recognition performance,
given in terms of Top-1 accuracy. “CUB-GT” indicates training only on the ground
truth CUB training set, “Web (raw)” trains on all search results for CUB categories,
and “Web (ﬁltered)” applies ﬁltering between categories within a domain (birds). L-
Bird denotes training ﬁrst on L-Bird, then ﬁne-tuning on the subset of categories under
evaluation (i.e. the ﬁltered web images), and L-Bird + CUB-GT indicates training on
L-Bird, then ﬁne-tuning on Web (ﬁltered), and ﬁnally ﬁne-tuning again on CUB-GT.
Similar notation is used for the other datasets. “(MC)” indicates using multiple crops
at test time (see text for details). We note that only the rows with “-GT” make use of
the ground truth training set; all other rows rely solely on noisy web imagery.
Training data
Training data
Web (ﬁltered)
Web (ﬁltered)
L-Aircraft
L-Bird(MC)
L-Aircraft(MC)
L-Bird + CUB-GT
L-Aircraft + FGVC-GT
L-Bird + CUB-GT(MC)
L-Aircraft + FGVC-GT(MC)
Birdsnap-GT
Birdsnap 
Stanford-GT
Web (ﬁltered)
Web (ﬁltered)
L-Bird(MC)
L-Bird + Birdsnap-GT
L-Dog + Stanford-GT
L-Bird + Birdsnap-GT(MC)
L-Dog + Stanford-GT(MC)
Removing Ground Truth from Web Images
One subtle point to be cautious about when using web images is the risk of
inadvertently including images from ground truth test sets in the web training data. To deal with this concern, we performed an aggressive deduplication
procedure with all ground truth test sets and their corresponding web images.
This process follows Wang et al. , which is a state of the art method for
learning a similarity metric between images. We tuned this procedure for high
near-duplicate recall, manually verifying its quality. More details are included in
the Supplementary Material.
Main Results
We present our main recognition results in Table 1, where we compare performance when the training set consists of either the ground truth training set, raw
web images of the categories in the corresponding evaluation dataset, web images
after applying our ﬁltering strategy, all web images of a particular domain, or
all images including even the ground truth training set.
On CUB-200-2011 , the smallest dataset we consider, even using raw
search results as training data results in a better model than the annotated
training set, with ﬁltering further improving results by 1.3 %. For Birdsnap ,
J. Krause et al.
the largest of the ground truth datasets we evaluate on, raw data mildly underperforms using the ground truth training set, though ﬁltering improves results
to be on par. On both CUB and Birdsnap, training ﬁrst on the very large set of
categories in L-Bird results in dramatic improvements, improving performance
on CUB further by 2.9 % and on Birdsnap by 4.6 %. This is an important point:
even if the end task consists of classifying only a small number of categories,
training with more ﬁne-grained categories yields signiﬁcantly more eﬀective networks. This can also be thought of as a form of transfer learning within the
same ﬁne-grained domain, allowing features learned on a related task to be useful for the ﬁnal classiﬁcation problem. When permitted access to the annotated
ground truth training sets for additional ﬁne-tuning and domain transfer, results
increase by another 0.3 % on CUB and 1.1 % on Birdsnap.
For the aircraft categories in FGVC, results are largely similar but weaker
in magnitude. Training on raw web data results in a signiﬁcant gain of 2.6 %
compared to using the curated training set, and ﬁltering, which did not aﬀect the
size of the training set much (Fig. 5), changes results only slightly in a positive
direction. Counterintuitively, pre-training on a larger set of aircraft does not
improve results on FGVC. Our hypothesis for the diﬀerence between birds and
aircraft in this regard is this: since there are many more species of birds in L-Bird
than there are aircraft in L-Aircraft (10,982 vs. 409), not only is the training
size of L-Bird larger, but each training example provides stronger information
because it distinguishes between a larger set of mutually-exclusive categories.
Nonetheless, when access to the curated training set is available for ﬁne-tuning,
performance dramatically increases to 94.5 %. On Stanford Dogs we see results
similar to FGVC, though for dogs we happen to see a mild loss when comparing
to the ground truth training set, not much diﬀerence with ﬁltering or using
L-Dog, and a large boost from adding in the ground truth training set.
An additional factor that can inﬂuence performance of web models is domain
shift – if images in the ground truth test set have very diﬀerent visual properties
compared to web images, performance will naturally diﬀer. Similarly, if category
names or deﬁnitions within a dataset are even mildly oﬀ, web-based methods will
be at a disadvantage without access to the ground truth training set. Adding the
ground truth training data ﬁxes this domain shift, making web-trained models
quickly recover, with a particularly large gain if the network has already learned
a good representation, matching the pattern of results for Stanford Dogs.
Limits of Web-Trained Models. To push our models to their limits, we
additionally evaluate using 144 image crops at test time, averaging predictions across each crop, denoted “(MC)” in Table 1. This brings results up to
92.3 %/92.8 % on CUB (without/with CUB training data), 85.4 %/85.4 % on
Birdsnap, 93.4 %/95.9 % on FGVC, and 80.8 %/85.9 % on Stanford Dogs. We
note that this is close to human expert performance on CUB, which is estimated
to be between 93 % and 95.6 % .
The Unreasonable Eﬀectiveness of Noisy Data for Fine-Grained Recognition
Table 2. Comparison with prior work on CUB-200-2011 . We only include methods which use no annotations at test time. Here “GT” refers to using Ground Truth
category labels in the training set of CUB, “BBox” indicates using bounding boxes,
and “Parts” additionally uses part annotations.
Training annotations
Alignments 
GT + BB + Parts
PB R-CNN 
GT + BB + Parts
Weak Sup. 
PN-DCN 
GT + BB + Parts
Two-Level 
Consensus 
GT + BB + Parts
FG-Without 
Bilinear 
Augmenting 
GT + BB + Parts + Web 84.6
Noisy Data + CNN Web
Comparison with Prior Work. We compare our results to prior work on
CUB, the most competitive ﬁne-grained dataset, in Table 2. While even our
baseline model using only ground truth data from Table 1 was at state of the
art levels, by forgoing the CUB training set and only training using noisy data
from the web, our models greatly outperform all prior work. On FGVC, which
is more recent and fewer works have evaluated on, the best prior performing
method we are aware of is the Bilinear CNN model of Lin et al. , which
has accuracy 84.1 % (ours is 93.4 % without FGVC training data, 95.9 % with),
and on Birdsnap, which is even more recent, the best performing method we are
aware of that uses no extra annotations during test time is the original 66.6 % by
Berg et al. (ours is 85.4 %). On Stanford Dogs, the most competitive related
work is , which uses an attention-based recurrent neural network to achieve
76.8 % (ours is 80.8 % without ground truth training data, 85.9 % with).
We identify two key reasons for these large improvements: The ﬁrst is the
use of a strong generic classiﬁer . A number of prior works have identiﬁed the
importance of having well-trained CNNs as components in their systems for ﬁnegrained recognition , which our work provides strong evidence
for. On all four evaluation datasets, our CNN of choice , trained on the
ground truth training set alone and without any architectural modiﬁcations,
performs at levels at or above the previous state-of-the-art. The second reason
for improvement is the large utility of noisy web data for ﬁne-grained recognition,
which is the focus of this work.
J. Krause et al.
We ﬁnally remind the reader that our work focuses on the application-level
problem of recognizing a given set of ﬁne-grained categories, which might not
come with their own expert-annotated training images. The use of existing test
sets serves to provide an accurate measure of performance and put our work in a
larger context, but results may not be strictly comparable with prior work that
operates within a single given dataset.
Comparison with Active Learning. We compare using noisy web data with a
more traditional active learning-based approach (Sect. 4) under several diﬀerent
settings in Table 3. We ﬁrst verify the eﬃcacy of active learning itself: when
training the network from scratch (i.e. no ﬁne-tuning), active learning improves
performance by up to 15.6 %, and when ﬁne-tuning, results still improve by
How does active learning compare to using web data? Purely using ﬁltered
web data compares favorably to non-ﬁne-tuned active learning methods (4.4 %
better), though lags behind the ﬁne-tuned models somewhat. To better compare
the active learning and noisy web data, we factor out the diﬀerence in scale
by performing an experiment with subsampled active learning data, setting it
to be the same size as the ﬁltered web data. Surprisingly, performance is very
similar, with only a 0.4 % advantage for the cleaner, annotated active learning
data, highlighting the eﬀectiveness of noisy web data despite the lack of manual
annotation. If we furthermore augment the ﬁltered web images with the Stanford
Dogs training set, which the active learning method notably used both as training
data and its seed set of images, performance improves to even be slightly better
than the manually-annotated active learning data (0.5 % improvement).
Table 3. Active learning-based results on Stanford Dogs , presented in terms of
top-1 accuracy. Methods with “(scratch)” indicate training from scratch and “(ft)”
indicates ﬁne-tuning from a network pre-trained on ILSVRC, with web models also
ﬁne-tuned. “subsample” refers to downsampling the active learning data to be the
same size as the ﬁltered web images. Note that Stanford-GT is a subset of active
learning data, which is denoted “A.L.” .
Training Procedure
Stanford-GT (scratch)
A.L., one round (scratch)
A.L., two rounds (scratch)
Stanford-GT (ft)
A.L., one round (ft)
A.L., one round (ft, subsample) 78.8
A.L., two rounds (ft)
Web (ﬁltered)
Web (ﬁltered) + Stanford-GT
The Unreasonable Eﬀectiveness of Noisy Data for Fine-Grained Recognition
These experiments indicate that, while more traditional active learning-based
approaches towards expanding datasets are eﬀective ways to improve recognition
performance given a suitable budget, simply using noisy images retrieved from
the web can be nearly as good, if not better. As web images require no manual
annotation and are openly available, we believe this is strong evidence for their
use in solving ﬁne-grained recognition.
Very Large-Scale Fine-Grained Recognition. A key advantage of using
noisy data is the ability to scale to large numbers of ﬁne-grained classes. However,
this poses a challenge for evaluation – it is infeasible to manually annotate images
with one of the 10,982 categories in L-Bird, 14,553 categories in L-Butterﬂy, and
would even be very time-consuming to annotate images with the 409 categories in
L-Aircraft. Therefore, we turn to an approximate evaluation, establishing a rough
estimate on true performance. Speciﬁcally, we query Flickr for up to 25 images of
each category, keeping only those images whose title strictly contains the name
of each category, and aggressively deduplicate these images with our training set
in order to ensure a fair evaluation. Although this is not a perfect evaluation
set, and is thus an area where annotation of ﬁne-grained datasets is particularly
valuable , we ﬁnd that it is remarkably clean on the surface: based on a
1,000-image estimate, we measure the cross-domain noise of L-Bird at only 1 %,
L-Butterﬂy at 2.3 %, and L-Aircraft at 4.5 %. An independent evaluation 
further measures all sources of noise combined to be only 16 % when searching
for bird species. In total, this yields 42,115 testing images for L-Bird, 42,046 for
L-Butterﬂy, and 3,131 for L-Aircraft.
Fig. 10. Classiﬁcation results on very large-scale ﬁne-grained recognition. From top
to bottom, depicted are examples of categories in L-Bird, L-Butterﬂy, and L-Aircraft,
along with their category name. The ﬁrst examples in each row are correctly predicted
by our models, while the last two examples in each row are errors, with our prediction
in grey and correct category (according to Flickr metadata) printed below.
J. Krause et al.
Given the diﬃculty and noise, performance is surprisingly high: On L-Bird
top-1 accuracy is 73.1 %/75.8 % (1/144 crops), for L-Butterﬂy it is 65.9 %/68.1 %,
and for L-Aircraft it is 72.7 %/77.5 %. Corresponding mAP numbers, which are
better suited for handling class imbalance, are 61.9, 54.8, and 70.5, reported for
the single crop setting. We show qualitative results in Fig. 10. These categories
span multiple continents in space (birds, butterﬂies) and decades in time (aircraft), demonstrating the breadth of categories in the world that can be recognized using only public sources of noisy ﬁne-grained data. To the best of our
knowledge, these results represent the largest number of ﬁne-grained categories
distinguished by any single system to date.
How Much Data is Really Necessary? In order to better understand the
utility of noisy web data for ﬁne-grained recognition, we perform a control experiment on the web data for CUB. Using the ﬁltered web images as a base, we
train models using progressively larger subsets of the results as training data,
taking the top ranked images across categories for each experiment. Performance
versus the amount of training data is shown in Fig. 11. Surprisingly, relatively
few web images are required to do as well as training on the CUB training set,
and adding more noisy web images always helps, even when at the limit of search
results. Based on this analysis, we estimate that one noisy web image for CUB
categories is “worth” 0.507 ground truth training images .
Error Analysis. Given the high performance of these models, what room is left
for improvement? In Fig. 12 we show the taxonomic distribution of the remaining
errors on L-Bird. The vast majority of errors (74.3 %) are made between very
similar classes at the genus level, indicating that most of the remaining errors
are indeed between extremely similar categories, and only very few errors (7.4 %)
Fig. 11. Number of web images used for training vs. performance on CUB-200-
2011 . We vary the amount of web training data in multiples of the CUB training
set size (5,994 images). Also shown is performance when training on the ground truth
CUB training set (CUB-GT).
The Unreasonable Eﬀectiveness of Noisy Data for Fine-Grained Recognition
Fig. 12. The errors on L-Bird that fall in each taxonomic rank, represented as a portion
of all errors made. For each error made, we calculate the taxonomic rank of the least
common ancestor of the predicted and test category.
are made between dissimilar classes, whose least common ancestor is the “Aves”
(i.e. Bird) taxonomic class. This suggests that most errors still made by the
models are fairly reasonable, corroborating the qualitative results of Fig. 10.
Discussion
In this work we have demonstrated the utility of noisy data toward solving
the problem of ﬁne-grained recognition. We found that the combination of a
generic classiﬁcation model and web data, ﬁltered with a simple strategy, was
surprisingly eﬀective at discriminating ﬁne-grained categories. This approach
performs favorably when compared to a more traditional active learning method
for expanding datasets, but is even more scalable, which we demonstrated experimentally on up to 14,553 ﬁne-grained categories. One potential limitation of the
approach is the availability of imagery for categories either not found or not
described in the public domain, for which an alternative method such as active
learning may be better suited. Another limitation is the current focus on classiﬁcation, which may be problematic if applications arise where multiple objects are
present or localization is otherwise required. Nonetheless, with these insights on
the unreasonable eﬀectiveness of noisy data, we are optimistic for applications
of ﬁne-grained recognition in the near future.
Acknowledgments. We thank Gal Chechik, Chuck Rosenberg, Zhen Li, Timnit
Gebru, Vignesh Ramanathan, Oliver Groth, and the anonymous reviewers for valuable feedback.