Instance-sensitive Fully Convolutional Networks
Jifeng Dai1, Kaiming He1, Yi Li2⋆, Shaoqing Ren3⋆, Jian Sun1
1Microsoft Research, 2Tsinghua University,
3University of Science and Technology of China
Abstract Fully convolutional networks (FCNs) have been proven very
successful for semantic segmentation, but the FCN outputs are unaware
of object instances. In this paper, we develop FCNs that are capable of
proposing instance-level segment candidates. In contrast to the previous
FCN that generates one score map, our FCN is designed to compute a
small set of instance-sensitive score maps, each of which is the outcome
of a pixel-wise classiﬁer of a relative position to instances. On top of
these instance-sensitive score maps, a simple assembling module is able
to output instance candidate at each position. In contrast to the recent
DeepMask method for segmenting instances, our method does not have
any high-dimensional layer related to the mask resolution, but instead
exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and
Introduction
Fully convolutional networks (FCN) have been proven an eﬀective end-to-end
solution to semantic image segmentation. An FCN produces a score map of a
size proportional to the input image, where every pixel represents a classiﬁer
of objects. Despite good accuracy and ease of usage, FCNs are not directly
applicable for producing instance segments (Fig. 1 (top)). Previous instance
semantic segmentation methods (e.g., ) in general resorted to oﬀ-theshelf segment proposal methods (e.g., ).
In this paper, we develop an end-to-end fully convolutional network that is
capable of segmenting candidate instances. Like the FCN in , in our method
every pixel still represents a classiﬁer; but unlike an FCN that generates one
score map (for one object category), our method computes a set of instancesensitive score maps, where each pixel is a classiﬁer of relative positions to an
object instance (Fig. 1 (bottom)). For example, with a 3×3 regular grid depicting
relative positions, we produce a set of 9 score maps in which, e.g., the map #6
in Fig. 1 has high scores on the “right side” of object instances. With this set of
score maps, we are able to generate an object instance segment in each sliding
window by assembling the output from the score maps. This procedure enables
a fully convolutional way of producing segment instances.
⋆This work was done when Yi Li and Shaoqing Ren were interns at Microsoft Research.
 
FCN for semantic segmentation
instance-sensitive score maps
InstanceFCN for instance segment proposal
assembling
Figure 1. Methodological comparisons between: (top) FCN for semantic segmentation; (bottom) our InstanceFCN for instance segment proposal.
Most related to our method, DeepMask is an instance segment proposal
method driven by convolutional networks. DeepMask learns a function that maps
an image sliding window to an m2-d vector representing an m×m-resolution
mask (e.g., m = 56). This is computed by an m2-d fully-connected (fc) layer.
See Fig. 2. Even though DeepMask can be implemented in a fully convolutional
way (as at inference time in ) by recasting this fc layer into a convolutional
layer with m2-d outputs, it fundamentally diﬀers from the FCNs in where
each output pixel is a low-dimensional classiﬁer. Unlike DeepMask, our method
has no layer whose size is related to the mask size m2, and each pixel in our
method is a low-dimensional classiﬁer. This is made possible by exploiting local
coherence of natural images for generating per-window pixel-wise predictions.
We will discuss and compare with DeepMask in depth.
On the PASCAL VOC and MS COCO benchmarks, our method
yields compelling instance segment proposal results, comparing favorably with
a series of proposal methods . Thanks to the small size of the layer for
predicting masks, our model trained on the small PASCAL VOC dataset exhibits good accuracy with less risk of overﬁtting. In addition, our system also
shows competitive results for instance semantic segmentation when used with
downstream classiﬁers. Our method, dubbed InstanceFCN, shows that segmenting instances can still be addressed by the FCN fashion in , ﬁlling a missing
piece among the broad applications of FCNs.
Figure 2. Methodological comparisons between DeepMask and InstanceFCN for
instance segment proposal. DeepMask uses a high-dimensional m2-d fc layer to generate
an instance, e.g., m = 56 and m2 = 3136. Our network has no any m2-d layer.
Related Work
The general concept of fully convolutional models dates back to at least two
decades ago . For convolutional neural networks (CNNs) , a sliding
window (or referred to as a patch or crop) is not necessarily run on the image domain but instead is run on a feature map, which can be recast into convolutional
ﬁlters on that feature map. These fully convolutional models are naturally applicable for image restoration problems, such as denoising , super-resolution
 , and others , where each output pixel is a real-number regressor of intensity values.
Recently FCNs have shown compelling quality and eﬃciency for semantic segmentation. In , each output pixel is a classiﬁer corresponding to the
receptive ﬁeld of the network. The networks can thus be trained end-to-end,
pixel-to-pixel, given the category-wise semantic segmentation annotation. But
this method can not distinguish object instances (Fig. 1).
Operated fully convolutionally, the Region Proposal Network (RPN) in Faster
R-CNN is developed for proposing box-level instances. In an RPN, each
pixel of the output map represents a bounding box regressor and an objectness
classiﬁer. The RPN does not generate mask-level proposals. In , the RPN
boxes are used for regressing segmentation masks, conducted by an fc layer on
Region-of-Interest (RoI) pooling features .
Instance-sensitive FCNs for Segment Proposal
From FCN to InstanceFCN
Although the original FCN for semantic segmentation produces no explicit
instance, we can still think of some special cases in which such an FCN can do a
good job generating an instance. Let’s consider an image that contains only one
object instance. In this case, the original FCN can produce a good mask about
this object category, and because there is only one instance, this is also a good
mask about this object instance. In this procedure, the FCN does not have any
pre-deﬁne ﬁlters that are dependent on the mask resolution/size (say, m×m).
Next let’s consider an image that contains two object instances that are close
to each other (Fig. 1(top)). Although now the FCN output (Fig. 1(top)) does
not distinguish the two instances, we notice that the output is indeed reusable
for most pixels, except for those where one object is conjunct the other — e.g.,
when the “right side” of the left instance is conjunct the “left side” of the right
instance (Fig. 1). If we can discriminate “right side” from “left side”, we can
still rely on FCN-like score maps to generate instances.
Instance-sensitive score maps
The above analysis motivates us to introduce the concept of relative positions
into FCNs. Ideally, relative positions are with respect to object instances, such
as the “right side” of an object or the “left side” of an object. In contrast to the
original FCN where each output pixel is a classiﬁer of an object category, we
propose an FCN where each output pixel is a classiﬁer of relative positions of
instances. For example, for the #4 score map in Fig. 1 (bottom), each pixel is a
classiﬁer of being or not being “left side” of an instance.
In our practice, we deﬁne the relative positions using a k×k (e.g., k = 3)
regular grid on a square sliding window (Fig. 1 (bottom)). This leads to a set
of k2 (e.g., 9) score maps which are our FCN outputs. We call them instancesensitive score maps. The network architecture for producing these score maps
can be trained end-to-end, with the help of the following module.
Instance assembling module
The instance-sensitive score maps have not yet produced object instances.
But we can simply assemble instances from these maps. We slide a window of
resolution m×m on the set of instance-sensitive score maps (Fig. 1 (bottom)).
In this sliding window, each m
k sub-window directly copies values from the
same sub-window in the corresponding score map. The k2 sub-windows are then
put together (according to their relative positions) to assemble a new window of
resolution m×m. This is the instance assembled from this sliding window.
Figure 3. Our method can exploit image local coherence. For a window shifted by one
small step (from blue to red), our method can reuse the same prediction from the same
score map at that pixel. This is not the case if the masks are produced by a sliding
m2-dimensional fc layer (for illustration m = 14 in this ﬁgure).
This instance assembling module is adopted for both training and inference.
During training, this model generates instances from sparsely sampled sliding
windows, which are compared to the ground truth. During inference, we densely
slide a window on the feature maps to predict an instance segment at each
position. More details are in the algorithm section.
We remark that the assembling module is the only component in our architecture that involves the mask resolution m×m. Nevertheless, the assembling
module has no network parameter to be learned. It is inexpensive because it
only has copy-and-paste operations. This module impacts training as it is used
for computing the loss function.
Local Coherence
Next we analyze our method from the perspective of local coherence . By local
coherence we mean that for a pixel in a natural image, its prediction is most
likely the same when evaluated in two neighboring windows. One does not need
to completely re-compute the predictions when a window is shifted by a small
The local coherence property has been exploited by our method. For a window
that slides by one stride (Fig. 3 (bottom)), the same pixel in the image coordinate
system will have the same prediction because it is copied from the same score
map (except for a few pixels near the partitioning of relative positions). This
allows us to conserve a large number of parameters when the mask resolution
m2 is high.
This is in contrast to DeepMask’s mechanism which is based on a “sliding
fc layer” (Fig. 3 (top)). In DeepMask, when the window is shifted by one stride,
the same pixel in the image coordinate system is predicted by two diﬀerent
channels of the fc layer, as shown in Fig. 3 (top). So the prediction of this pixel
is in general not the same when evaluated in two neighboring windows.
By exploiting local coherence, our network layers’ sizes and dimensions are
all independent of the mask resolution m×m, in contrast to DeepMask. This
not only reduces the computational cost of the mask prediction layers, but more
importantly, reduces the number of parameters required for mask regression,
leading to less risk of overﬁtting on small datasets such as PASCAL VOC. In
the experiment section we show that our mask prediction layer can have hundreds
times fewer parameters than DeepMask.
Algorithm and Implementation
Next we describe the network architecture, training algorithm, and inference
algorithm of our method.
Network architecture. As common practice, we use the VGG-16 network 
pre-trained on ImageNet as the feature extractor. The 13 convolutional layers
in VGG-16 are applied fully convolutionally on an input image of arbitrary size.
We follow the practice in to reduce the network stride and increase feature
map resolution: the max pooling layer pool4 (between conv4 3 and conv5 1) is
modiﬁed to have a stride of 1 instead of 2, and accordingly the ﬁlters in conv5 1
to conv5 3 are adjusted by the “hole algorithm” . Using this modiﬁed VGG
network, the eﬀective stride of the conv5 3 feature map is s = 8 pixels w.r.t. the
input image. We note that this reduced stride directly determines the resolutions
of our score maps from which our masks are copied and assembled.
On top of the feature map, there are two fully convolutional branches, one
for estimating segment instances and the other for scoring the instances. For
the ﬁrst branch, we adopt a 1×1 512-d convolutional layer (with ReLU ) to
transform the features, and then use a 3×3 convolutional layer to generate a set
of instance-sensitive score maps. With a k×k regular grid for describing relative
positions, this last convolutional layer has k2 output channels corresponding to
the set of k2 instance-sensitive score maps. See the top branch in Fig. 4. On top
of these score maps, an assembling module is used to generate object instances
in a sliding window of a resolution m×m. We use m = 21 pixels (on the feature
map with a stride of 8).
For the second branch of scoring instances (bottom in Fig. 4), we use a
3×3 512-d convolutional layer (with ReLU) followed by a 1×1 convolutional
layer. This 1×1 layer is a per-pixel logistic regression for classifying instance/notinstance of the sliding window centered at this pixel. The output of this branch is

Figure 4. Details of the InstanceFCN architecture. On the top is a fully convolutional
branch for generating k2 instance-sensitive score maps, followed by the assembling
module that outputs instances. On the bottom is a fully convolutional branch for
predicting the objectness score of each window. The highly scored output instances are
on the right. In this ﬁgure, the objectness map and the “all instances” map have been
sub-sampled for the purpose of illustration.
thus an objectness score map (Fig. 4 (bottom)), in which one score corresponds
to one sliding window that generates one instance.
Training. Our network is trained end-to-end. We adopt the image-centric strategy in . The forward pass computes the set of instance-sensitive score maps
and the objectness score map. After that, a set of 256 sliding windows are randomly sampled , and the instances are only assembled from these 256
windows for computing the loss function. The loss function is deﬁned as:
L(Si,j, S∗
Here i is the index of a sampled window, pi is the predicted objectness score of
the instance in this window, and p∗
i is 1 if this window is a positive sample and
0 if a negative sample. Si is the assembled segment instance in this window, S∗
is the ground truth segment instance, and j is the pixel index in the window. L
is the logistic regression loss. We use the deﬁnition of positive/negative samples
in , and the 256 sampled windows have a positive/negative sampling ratio of
Our model accepts images of arbitrary size as input. We follow the scale
jittering in for training: a training image is resized such that its shorter side
is randomly sampled from 600×1.5{−4,−3,−2,−1,0,1} pixels. We use Stochastic
Gradient Descent (SGD) as the solver. A total of 40k iterations are performed,
with a learning rate of 0.001 for the ﬁrst 32k and 0.0001 for the last 8k. We
perform training with an 8-GPU implementation, where each GPU holds 1 image
Table 1. Ablation experiments on the numbers of instance-sensitive score maps (i.e.,
# of relative positions, k2), evaluated on the PASCAL VOC 2012 validation set.
AR@100 (%)
AR@1000 (%)
with 256 sampled windows (so the eﬀective mini-batch size is 8 images). The
weight decay is 0.0005 and the momentum is 0.9. The ﬁrst thirteen convolutional
layers are initialized by the ImageNet pre-trained VGG-16 , and the extra
convolutional layers are randomly initialized from a Gaussian distribution with
zero mean and standard derivation of 0.01.
Inference. A forward pass of the network is run on the input image, generating
the instance-sensitive score maps and the objectness score map. The assembling
module then applies densely sliding windows on these maps to produce a segment
instance at each position. Each instance is associated with a score from the
objectness score map. To handle multiple scales, we resize the shorter side of
images to 600×1.5{−4,−3,−2,−1,0,1} pixels, and compute all instances at each
scale. It takes totally 1.5 seconds evaluating an images on a K40 GPU.
For each output segment, we truncate the values to form a binary mask.
Then we adopt non-maximum suppression (NMS) to generate the ﬁnal set of
segment proposals. The NMS is based on the objectness scores and the box-level
IoU given by the tight bounding boxes of the binary masks. We use a threshold
of 0.8 for the NMS. After NMS, the top-N ranked segment proposals are used
as the output.
Experiments
Experiments on PASCAL VOC 2012
We ﬁrst conduct experiments on PASCAL VOC 2012 . Following , we use
the segmentation annotations from , and train the models on the training set
and evaluate on the validation set. All segment proposal methods are evaluated
by the mask-level intersection-over-union (IoU) between the predicted instances
and the ground-truth instances. Following , we measure the Average Recall
(AR) (between IoU thresholds of 0.5 to 1.0) at a ﬁxed number N of proposals,
denoted as “AR@N”. In , the AR metrics have been shown to be more
correlated to the detection accuracy (when used with downstream classiﬁers
 ) than traditional metrics for evaluating proposals.
Ablations on the number of relative positions k2
Table 1 shows our results using diﬀerent values of k2. Our method is not
sensitive to k2, and can perform well even when k = 3. Fig. 5 shows some
examples of the instance-sensitive maps and assembled instances for k = 3.
instance-sensitive score maps
Figure 5. Examples of instance-sensitive maps and assembled instances on the PAS-
CAL VOC validation set. For simplicity we only show the cases of k = 3 (9 instancesensitive score maps) in this ﬁgure.
Table 1 also shows that our results of k = 5 and k = 7 are comparable, and
are slightly better than the k = 3 baseline. Our method enjoys a small gain
with a ﬁner division of relative position, but gets saturated around k = 5. In the
following experiments we use k = 5.
Ablation comparisons with the DeepMask scheme
For fair comparisons, we implement a DeepMask baseline on PASCAL VOC.
Speciﬁcally, the network structure is VGG-16 followed by an extra 512-d 1×1
convolutional layer , generating a 14×14 feature map as in from a 224×224
image crop. Then a 512-d fc layer is applied to this feature map, followed by
a 562-d fc for generating a 56×56-resolution mask. The two fc layers under
Table 2. Ablation comparisons between ∼DeepMask and our method on the PASCAL
VOC 2012 validation set. “∼DeepMask” is our implementation based on controlled
settings (see more descriptions in the main text).
AR@100 (%)
AR@1000 (%)
crop 224×224
sliding fc
crop 224×224
fully conv.
fully conv.
fully conv.
Table 3. Comparisons with state-of-the-art segment proposal methods on the PASCAL
VOC 2012 validation set. The results of SS and MCG are from the publicly
available code, and the results of MNC is provided by the authors of .
AR@100 (%)
AR@1000 (%)
this setting have 53M parameters1. The objectness scoring branch is constructed
as in . All other settings are the same as ours for fair comparisons. We refer
to this model as ∼DeepMask which means our implementation of DeepMask.
This baseline’s results are in Table 2.
Table 2 shows the ablation comparisons. As the ﬁrst variant, we train our
model on 224×224 crops as is done in DeepMask. Under this ablative training,
our method still outperforms ∼DeepMask by healthy margins. When trained on
full-size images (Table 2), our result is further improved. The gain from training
on full-size images further demonstrates the beneﬁts of our fully convolutional
It is noteworthy that our method has considerably fewer parameters. Our
last k2-d convolutional layer has only 0.1M parameters2 (all other layers being
the same as the DeepMask counterpart). This mask generation layer has only
1/500 of parameters comparing with DeepMask’s fc layers. Regressing highdimensional m×m masks is possible for our method as it exploits local coherence.
We also expect fewer parameters to have less risk of overﬁtting.
Comparisons with state-of-the-art segment proposal methods
In Table 3 and Fig. 6 we compare with state-of-the-art segment proposal
methods: Selective Search (SS) , Multiscale Combinatorial Grouping (MCG)
 , ∼DeepMask, and Multi-task Network Cascade (MNC) . MNC is a joint
multi-stage cascade method that proposes box-level regions, regresses masks
1 512 × 14 × 14 × 512 + 512 × 562 = 53M
2 512 × 3 × 3 × 25 = 0.1M
10 proposals
100 proposals
1000 proposals
Figure 6. Recall vs. IoU curves of diﬀerent segment proposals on the PASCAL VOC
2012 validation set. AR is the area under the curves.
Table 4. Semantic instance segmentation on the PASCAL VOC 2012 validation set.
All methods are based on VGG-16 except SDS based on AlexNet .
downstream classiﬁer
 (%)
 (%)
Hypercolumn 
from these regions, and classiﬁes these mask. With a trained MNC, we treat
the mask regression outputs as the segment proposals.
Table 3 and Fig. 6 show that the CNN-based methods (∼DeepMask, MNC,
ours) perform better than the bottom-up segmentation methods of SS and MCG.
In addition, our method has AR@100 and AR@1000 similar to MNC, but has
5.5% higher AR@10. The mask regression of MNC is done by high-dimensional
fc layers, in contrast to our fully convolutional fashion.
Comparisons on Instance Semantic Segmentation
Next we evaluate the instance semantic segmentation performance when used
with downstream category-aware classiﬁers. Following , we evaluate mean
Average Precision (mAP) using mask-level IoU at threshold of 0.5 and 0.7. In
Table 4 we compare with: SDS , Hypercolumn , CFM , and MNC .
We use MNC’s stage 3 as our classiﬁer structure, which is similar to Fast R-CNN
 except that its RoI pooling layer is replaced with an RoI masking layer that
generates features from the segment proposals. We adopt a two-step training:
ﬁrst train our model for proposing segments and then train the classiﬁer with
the given proposals. Our method uses N = 300 proposals in this comparison.
Table 4 shows that among all the competitors our method has the highest
 score of 43.0%, which is 1.5% better than the closest competitor. Our
method has the second best , lower than MNC. We note that MNC is
Table 5. Comparisons of instance segment proposals on the ﬁrst 5k images from
the MS COCO validation set. DeepMask’s results are from .
segment proposals
AR@100 (%)
AR@1000 (%)
Rigor 
DeepMask 
DeepMaskZoom 
10 proposals
DeepMaskZoom
100 proposals
DeepMaskZoom
1000 proposals
DeepMaskZoom
Figure 7. Recall vs. IoU curves on the ﬁrst 5k images on the MS COCO validation
set. DeepMask’s curves are from .
a joint training algorithm which simultaneously learns proposals and category
classiﬁers. Our result (61.5%) is based on two-step training, and is better than
MNC’s step-by-step training counterpart (60.2% ).
Experiments on MS COCO
Finally we evaluate instance segment proposals on the MS COCO benchmark
 . Following , we train our network on the 80k training images and evaluate
on the ﬁrst 5k validation images. The results are in Table 5 (DeepMask’s results
are reported from ). For fair comparisons, we use the same multiple scales
used in for training and testing on COCO. Our method has higher AR scores
than DeepMask and a DeepMaskZoom variant . Fig. 7 shows the recall vs.
IoU curves on COCO.
Conclusion
We have presented InstanceFCN, a fully convolutional scheme for proposing segment instances. It is driven by classifying pixels based on their relative positions,
which leads to a set of instance-sensitive score maps. A simple assembling module
InstanceFCN
Figure 8. Comparisons with DeepMask on the MS COCO validation set. Left:
DeepMask, taken from the paper of . Proposals with highest IoU to the ground
truth are displayed. The missed ground-truth objects (no proposals with IoU > 0.5) are
marked by red outlines ﬁlled with white. Right: Our results displayed in the same way.
is then able to generate segment instances from these score maps. Our network
architecture handles instance segmentation without using any high-dimensional
layers that depend on the mask resolution. We expect our novel design of fully
convolutional models will further extend the family of FCNs.