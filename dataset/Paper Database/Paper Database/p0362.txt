Deep High-Resolution Representation Learning for Human Pose Estimation
Ke Sun1,2∗†
Bin Xiao2∗
Jingdong Wang2
1University of Science and Technology of China
2Microsoft Research Asia
{sunk,dongeliu}@ustc.edu.cn, {Bin.Xiao,jingdw}@microsoft.com
In this paper, we are interested in the human pose estimation problem with a focus on learning reliable highresolution representations. Most existing methods recover
high-resolution representations from low-resolution representations produced by a high-to-low resolution network.
Instead, our proposed network maintains high-resolution
representations through the whole process.
We start from a high-resolution subnetwork as the ﬁrst
stage, gradually add high-to-low resolution subnetworks
one by one to form more stages, and connect the mutliresolution subnetworks in parallel.
We conduct repeated
multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich highresolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially
more precise. We empirically demonstrate the effectiveness
of our network through the superior pose estimation results
over two benchmark datasets: the COCO keypoint detection
dataset and the MPII Human Pose dataset. In addition, we
show the superiority of our network in pose tracking on the
PoseTrack dataset. The code and models have been publicly
available at 
deep-high-resolution-net.pytorch.
1. Introduction
2D human pose estimation has been a fundamental yet
challenging problem in computer vision. The goal is to localize human anatomical keypoints (e.g., elbow, wrist, etc.)
or parts. It has many applications, including human action
recognition, human-computer interaction, animation, etc.
This paper is interested in single-person pose estimation,
which is the basis of other related problems, such as multiperson pose estimation ,
video pose estimation and tracking , etc.
∗Equal contribution.
†This work is done when Ke Sun was an intern at Microsoft Research,
Beijing, P.R. China
Figure 1. Illustrating the architecture of the proposed HRNet. It
consists of parallel high-to-low resolution subnetworks with repeated information exchange across multi-resolution subnetworks
(multi-scale fusion). The horizontal and vertical directions correspond to the depth of the network and the scale of the feature
maps, respectively.
The recent developments show that deep convolutional
neural networks have achieved the state-of-the-art performance.
Most existing methods pass the input through a
network, typically consisting of high-to-low resolution subnetworks that are connected in series, and then raise the
resolution. For instance, Hourglass recovers the high
resolution through a symmetric low-to-high process. SimpleBaseline adopts a few transposed convolution layers
for generating high-resolution representations. In addition,
dilated convolutions are also used to blow up the later layers of a high-to-low resolution network (e.g., VGGNet or
ResNet) .
architecture,
Resolution Net (HRNet), which is able to maintain highresolution representations through the whole process. We
start from a high-resolution subnetwork as the ﬁrst stage,
gradually add high-to-low resolution subnetworks one by
one to form more stages, and connect the multi-resolution
subnetworks in parallel. We conduct repeated multi-scale
fusions by exchanging the information across the parallel multi-resolution subnetworks over and over through the
whole process. We estimate the keypoints over the highresolution representations output by our network. The resulting network is illustrated in Figure 1.
Our network has two beneﬁts in comparison to exist-
 
Figure 2. Illustration of representative pose estimation networks that rely on the high-to-low and low-to-high framework. (a) Hourglass .
(b) Cascaded pyramid networks . (c) SimpleBaseline : transposed convolutions for low-to-high processing. (d) Combination with
dilated convolutions . Bottom-right legend: reg. = regular convolution, dilated = dilated convolution, trans. = transposed convolution,
strided = strided convolution, concat. = concatenation. In (a), the high-to-low and low-to-high processes are symmetric. In (b), (c) and
(d), the high-to-low process, a part of a classiﬁcation network (ResNet or VGGNet), is heavy, and the low-to-high process is light. In (a)
and (b), the skip-connections (dashed lines) between the same-resolution layers of the high-to-low and low-to-high processes mainly aim
to fuse low-level and high-level features. In (b), the right part, reﬁnenet, combines the low-level and high-level features that are processed
through convolutions.
ing widely-used networks for pose estimation. (i) Our approach connects high-to-low resolution subnetworks in parallel rather than in series as done in most
existing solutions.
Thus, our approach is able to maintain the high resolution instead of recovering the resolution through a low-to-high process, and accordingly the predicted heatmap is potentially spatially more precise. (ii)
Most existing fusion schemes aggregate low-level and highlevel representations. Instead, we perform repeated multiscale fusions to boost the high-resolution representations
with the help of the low-resolution representations of the
same depth and similar level, and vice versa, resulting in
that high-resolution representations are also rich for pose
estimation. Consequently, our predicted heatmap is potentially more accurate.
We empirically demonstrate the superior keypoint detection performance over two benchmark datasets: the COCO
keypoint detection dataset and the MPII Human Pose
dataset . In addition, we show the superiority of our network in video pose tracking on the PoseTrack dataset .
2. Related Work
Most traditional solutions to single-person pose estimation adopt the probabilistic graphical model or the pictorial structure model , which is recently improved by
exploiting deep learning for better modeling the unary and
pair-wise energies or imitating the iterative inference process . Nowadays, deep convolutional neural
network provides dominant solutions . There are two mainstream methods: regressing
the position of keypoints , and estimating keypoint
heatmaps followed by choosing the locations
with the highest heat values as the keypoints.
convolutional
heatmap estimation consist of a stem subnetwork similar to
the classiﬁcation network, which decreases the resolution,
a main body producing the representations with the same
resolution as its input, followed by a regressor estimating
the heatmaps where the keypoint positions are estimated
and then transformed in the full resolution. The main body
mainly adopts the high-to-low and low-to-high framework,
possibly augmented with multi-scale fusion and intermediate (deep) supervision.
High-to-low and low-to-high.
The high-to-low process
aims to generate low-resolution and high-level representations, and the low-to-high process aims to produce highresolution representations . Both the
two processes are possibly repeated several times for boosting the performance .
Representative network design patterns include: (i) Symmetric high-to-low and low-to-high processes. Hourglass
and its follow-ups design the low-to-high
process as a mirror of the high-to-low process. (ii) Heavy
high-to-low and light low-to-high.
The high-to-low process is based on the ImageNet classiﬁcation network, e.g.,
ResNet adopted in , and the low-to-high process is
simply a few bilinear-upsampling or transpose convolution layers. (iii) Combination with dilated convolutions. In , dilated convolutions are adopted in
the last two stages in the ResNet or VGGNet to eliminate
the spatial resolution loss, which is followed by a light lowto-high process to further increase the resolution, avoiding
expensive computation cost for only using dilated convolutions . Figure 2 depicts four representative pose
estimation networks.
Multi-scale fusion.
The straightforward way is to feed
multi-resolution images separately into multiple networks
and aggregate the output response maps .
Hourglass and its extensions combine low-level
features in the high-to-low process into the same-resolution
high-level features in the low-to-high process progressively through skip connections. In cascaded pyramid network , a globalnet combines low-to-high level features
in the high-to-low process progressively into the low-tohigh process, and then a reﬁnenet combines the low-to-high
level features that are processed through convolutions. Our
approach repeats multi-scale fusion, which is partially inspired by deep fusion and its extensions .
Intermediate supervision.
Intermediate supervision or
deep supervision, early developed for image classiﬁcation , is also adopted for helping deep networks
training and improving the heatmap estimation quality,
e.g., . The hourglass approach and
the convolutional pose machine approach process the
intermediate heatmaps as the input or a part of the input of
the remaining subnetwork.
Our approach.
Our network connects high-to-low subnetworks in parallel.
It maintains high-resolution representations through the whole process for spatially precise
heatmap estimation. It generates reliable high-resolution
representations through repeatedly fusing the representations produced by the high-to-low subnetworks. Our approach is different from most existing works, which need
a separate low-to-high upsampling process and aggregate
low-level and high-level representations.
Our approach,
without using intermediate heatmap supervision, is superior
in keypoint detection accuracy and efﬁcient in computation
complexity and parameters.
There are related multi-scale networks for classiﬁcation
and segmentation .
Our work is partially inspired by some of
them , and there are clear differences making
them not applicable to our problem. Convolutional neural
fabrics and interlinked CNN fail to produce highquality segmentation results because of a lack of proper design on each subnetwork (depth, batch normalization) and
multi-scale fusion. The grid network , a combination
of many weight-shared U-Nets, consists of two separate fusion processes across multi-resolution representations: on
the ﬁrst stage, information is only sent from high resolution
to low resolution; on the second stage, information is only
sent from low resolution to high resolution, and thus less
competitive. Multi-scale densenets does not target and
cannot generate reliable high-resolution representations.
3. Approach
Human pose estimation, a.k.a. keypoint detection, aims
to detect the locations of K keypoints or parts (e.g., elbow,
wrist, etc) from an image I of size W × H × 3. The stateof-the-art methods transform this problem to estimating K
heatmaps of size W
′, {H1, H2, . . . , HK}, where each
heatmap Hk indicates the location conﬁdence of the kth
We follow the widely-adopted pipeline to
predict human keypoints using a convolutional network,
which is composed of a stem consisting of two strided convolutions decreasing the resolution, a main body outputting
the feature maps with the same resolution as its input feature maps, and a regressor estimating the heatmaps where
the keypoint positions are chosen and transformed to the
full resolution. We focus on the design of the main body
and introduce our High-Resolution Net (HRNet) that is depicted in Figure 1.
Sequential multi-resolution subnetworks. Existing networks for pose estimation are built by connecting high-tolow resolution subnetworks in series, where each subnetwork, forming a stage, is composed of a sequence of convolutions and there is a down-sample layer across adjacent
subnetworks to halve the resolution.
Let Nsr be the subnetwork in the sth stage and r be the
resolution index (Its resolution is
2r−1 of the resolution of
the ﬁrst subnetwork). The high-to-low network with S (e.g.,
4) stages can be denoted as:
Parallel multi-resolution subnetworks. We start from a
high-resolution subnetwork as the ﬁrst stage, gradually add
high-to-low resolution subnetworks one by one, forming
new stages, and connect the multi-resolution subnetworks
in parallel. As a result, the resolutions for the parallel subnetworks of a later stage consists of the resolutions from the
previous stage, and an extra lower one.
An example network structure, containing 4 parallel subnetworks, is given as follows,
Figure 3. Illustrating how the exchange unit aggregates the information for high, medium and low resolutions from the left to the
right, respectively. Right legend: strided 3×3 = strided 3×3 convolution, up samp. 1×1 = nearest neighbor up-sampling following
a 1 × 1 convolution.
Repeated multi-scale fusion. We introduce exchange units
across parallel subnetworks such that each subnetwork repeatedly receives the information from other parallel subnetworks. Here is an example showing the scheme of exchanging information. We divided the third stage into several (e.g., 3) exchange blocks, and each block is composed
of 3 parallel convolution units with an exchange unit across
the parallel units, which is given as follows,
sr represents the convolution unit in the rth resolution of the bth block in the sth stage, and Eb
s is the corresponding exchange unit.
We illustrate the exchange unit in Figure 3 and present
the formulation in the following. We drop the subscript s
and the superscript b for discussion convenience. The inputs are s response maps: {X1, X2, . . . , Xs}. The outputs
are s response maps: {Y1, Y2, . . . , Ys}, whose resolutions
and widths are the same to the input. Each output is an aggregation of the input maps, Yk = Ps
i=1 a(Xi, k). The
exchange unit across stages has an extra output map Ys+1:
Ys+1 = a(Ys, s + 1).
The function a(Xi, k) consists of upsampling or downsampling Xi from resolution i to resolution k. We adopt
strided 3 × 3 convolutions for downsampling. For instance,
one strided 3×3 convolution with the stride 2 for 2× downsampling, and two consecutive strided 3 × 3 convolutions
with the stride 2 for 4× downsampling. For upsampling,
we adopt the simple nearest neighbor sampling following a
1 × 1 convolution for aligning the number of channels. If
i = k, a(·, ·) is just an identify connection: a(Xi, k) = Xi.
Heatmap estimation.
We regress the heatmaps simply
from the high-resolution representations output by the last
exchange unit, which empirically works well.
function, deﬁned as the mean squared error, is applied
for comparing the predicted heatmaps and the groundtruth
heatmaps. The groundtruth heatmpas are generated by applying 2D Gaussian with standard deviation of 1 pixel centered on the grouptruth location of each keypoint.
Network instantiation. We instantiate the network for keypoint heatmap estimation by following the design rule of
ResNet to distribute the depth to each stage and the number
of channels to each resolution.
The main body, i.e., our HRNet, contains four stages
with four parallel subnetworks, whose the resolution is
gradually decreased to a half and accordingly the width (the
number of channels) is increased to the double. The ﬁrst
stage contains 4 residual units where each unit, the same to
the ResNet-50, is formed by a bottleneck with the width 64,
and is followed by one 3×3 convolution reducing the width
of feature maps to C. The 2nd, 3rd, 4th stages contain 1, 4,
3 exchange blocks, respectively. One exchange block contains 4 residual units where each unit contains two 3 × 3
convolutions in each resolution and an exchange unit across
resolutions. In summary, there are totally 8 exchange units,
i.e., 8 multi-scale fusions are conducted.
In our experiments, we study one small net and one big
net: HRNet-W32 and HRNet-W48, where 32 and 48 represent the widths (C) of the high-resolution subnetworks in
last three stages, respectively. The widths of other three
parallel subnetworks are 64, 128, 256 for HRNet-W32, and
96, 192, 384 for HRNet-W48.
4. Experiments
4.1. COCO Keypoint Detection
Dataset. The COCO dataset contains over 200, 000
images and 250, 000 person instances labeled with 17 keypoints. We train our model on COCO train2017 dataset, including 57K images and 150K person instances. We evaluate our approach on the val2017 set and test-dev2017 set,
containing 5000 images and 20K images, respectively.
Evaluation metric.
The standard evaluation metric is
based on Object Keypoint Similarity (OKS): OKS
i )δ(vi>0)
. Here di is the Euclidean distance between the detected keypoint and the corresponding ground truth, vi is the visibility ﬂag of the ground
truth, s is the object scale, and ki is a per-keypoint
constant that controls falloff.
We report standard average precision and recall scores1: AP50 (AP at OKS =
0.50) AP75, AP (the mean of AP scores at 10 positions, OKS = 0.50, 0.55, . . . , 0.90, 0.95; APM for medium
objects, APL for large objects, and AR at OKS
0.50, 0.55, . . . , 0.90, 0.955.
Training. We extend the human detection box in height
or width to a ﬁxed aspect ratio: height : width = 4 : 3,
and then crop the box from the image, which is resized to
a ﬁxed size, 256 × 192 or 384 × 288. The data augmentation includes random rotation ([−45◦, 45◦]), random scale
1 
Table 1. Comparisons on the COCO validation set. Pretrain = pretrain the backbone on the ImageNet classiﬁcation task. OHKM = online
hard keypoints mining .
Input size
8-stage Hourglass 
8-stage Hourglass
CPN + OHKM 
SimpleBaseline 
SimpleBaseline 
ResNet-101
SimpleBaseline 
ResNet-152
SimpleBaseline 
ResNet-152
Table 2. Comparisons on the COCO test-dev set. #Params and FLOPs are calculated for the pose estimation network, and those for human
detection and keypoint grouping are not included.
Input size
Bottom-up: keypoint detection and grouping
OpenPose 
Associative Embedding 
PersonLab 
MultiPoseNet 
Top-down: human detection and single-person keypoint detection
Mask-RCNN 
ResNet-50-FPN
G-RMI 
ResNet-101
Integral Pose Regression 
ResNet-101
G-RMI + extra data 
ResNet-101
ResNet-Inception
PyraNet 
CPN (ensemble) 
ResNet-Inception
SimpleBaseline 
ResNet-152
HRNet-W48 + extra data
([0.65, 1.35]), and ﬂipping. Following , half body data
augmentation is also involved.
We use the Adam optimizer . The learning schedule follows the setting . The base learning rate is set as
1e−3, and is dropped to 1e−4 and 1e−5 at the 170th and
200th epochs, respectively. The training process is terminated within 210 epochs.
Testing. The two-stage top-down paradigm similar as is used: detect the person instance using a person
detector, and then predict detection keypoints.
We use the same person detectors provided by Simple-
Baseline2 for both validation set and test-dev set. Fol-
2 
lowing the common practice , we compute the
heatmap by averaging the headmaps of the original and
ﬂipped images. Each keypoint location is predicted by adjusting the highest heatvalue location with a quarter offset in
the direction from the highest response to the second highest response.
Results on the validation set. We report the results of our
method and other state-of–the-art methods in Table 1. Our
small network - HRNet-W32, trained from scratch with the
input size 256 × 192, achieves an 73.4 AP score, outperforming other methods with the same input size. (i) Compared to Hourglass , our small network improves AP
human-pose-estimation.pytorch
by 6.5 points, and the GFLOPs of our network is much
lower and less than half, while the number of parameters
are similar and ours is slightly larger.
(ii) Compared to
CPN w/o and w/ OHKM, our network, with slightly
larger model size and slightly higher complexity, achieves
4.8 and 4.0 points gain, respectively. (iii) Compared to the
previous best-performed SimpleBaseline , our small net
HRNet-W32 obtains signiﬁcant improvements: 3.0 points
gain for the backbone ResNet-50 with a similar model size
and GFLOPs, and 1.4 points gain for the backbone ResNet-
152 whose model size (#Params) and GLOPs are twice as
many as ours.
Our nets can beneﬁt from (i) training from the model pretrained for the ImageNet classiﬁcation problem: The gain
is 1.0 points for HRNet-W32; (ii) increasing the capacity
by increasing the width: Our big net HRNet-W48 gets 0.7
and 0.5 improvements for the input sizes 256 × 192 and
384 × 288, respectively.
Considering the input size 384 × 288, our HRNet-W32
and HRNet-W48, get the 75.8 and 76.3 AP, which have 1.4
and 1.2 improvements compared to the input size 256 ×
192. In comparison to the SimpleBaseline that uses
ResNet-152 as the backbone, our HRNet-W32 and HRNet-
W48 attain 1.5 and 2.0 points gain in terms of AP at 45%
and 92.4% computational cost, respectively.
Results on the test-dev set. Table 2 reports the pose estimation performances of our approach and the existing state-ofthe-art approaches. Our approach is signiﬁcantly better than
bottom-up approaches. On the other hand, our small network, HRNet-W32, achieves an AP of 74.9. It outperforms
all the other top-down approaches, and is more efﬁcient in
terms of model size (#Params) and computation complexity
(GFLOPs). Our big model, HRNet-W48, achieves the highest 75.5 AP. Compared to the SimpleBaseline with the
same input size, our small and big networks receive 1.2 and
1.8 improvements, respectively. With additional data from
AI Challenger for training, our single big network can
obtain an AP of 77.0.
4.2. MPII Human Pose Estimation
Dataset. The MPII Human Pose dataset consists of images taken from a wide-range of real-world activities with
full-body pose annotations. There are around 25K images
with 40K subjects, where there are 12K subjects for testing and the remaining subjects for the training set. The data
augmentation and the training strategy are the same to MS
COCO, except that the input size is cropped to 256 × 256
for fair comparison with other methods.
Testing. The testing procedure is almost the same to that
in COCO except that we adopt the standard testing strategy
to use the provided person boxes instead of detected person
boxes. Following , a six-scale pyramid testing
procedure is performed.
Performance
comparisons
MPII test set
( ).
Insafutdinov et al. 96.8 95.2 89.3 84.4 88.4 83.4 78.0 88.5
Wei et al. 
97.8 95.0 88.7 84.0 88.4 82.8 79.4 88.5
Bulat et al. 
97.9 95.1 89.9 85.3 89.4 85.7 81.7 89.7
Newell et al. 
98.2 96.3 91.2 87.1 90.1 87.4 83.6 90.9
Sun et al. 
98.1 96.2 91.2 87.2 89.8 87.4 84.1 91.0
Tang et al. 
97.4 96.4 92.1 87.7 90.2 87.7 84.3 91.2
Ning et al. 
98.1 96.3 92.2 87.8 90.6 87.6 82.7 91.2
Luvizon et al. 
98.1 96.6 92.0 87.5 90.6 88.0 82.7 91.2
Chu et al. 
98.5 96.3 91.9 88.1 90.6 88.0 85.0 91.5
Chou et al. 
98.2 96.8 92.2 88.0 91.3 89.1 84.9 91.8
Chen et al. 
98.1 96.5 92.5 88.5 90.2 89.6 86.0 91.9
Yang et al. 
98.5 96.7 92.5 88.7 91.1 88.6 86.0 92.0
Ke et al. 
98.5 96.8 92.7 88.4 90.6 89.3 86.3
Tang et al. 
98.4 96.9 92.6 88.7 91.8 89.4 86.2
SimpleBaseline 
98.5 96.6 91.9 87.6 91.1 88.1 84.1 91.5
98.6 96.9 92.8 89.0 91.5 89.0 85.7
Evaluation metric.
The standard metric , the PCKh
(head-normalized probability of correct keypoint) score, is
used. A joint is correct if it falls within αl pixels of the
groundtruth position, where α is a constant and l is the head
size that corresponds to 60% of the diagonal length of the
ground-truth head bounding box. The (α = 0.5)
score is reported.
Results on the test set.
Tables 3 and 4 show the
 results, the model size and the GFLOPs of the
top-performed methods. We reimplement the SimpleBaseline by using ResNet-152 as the backbone with the
input size 256 × 256. Our HRNet-W32 achieves a 92.3
 score, and outperforms the stacked hourglass
approach and its extensions . Our
result is the same as the best one among the previouslypublished results on the leaderboard of Nov. 16th, 20183.
We would like to point out that the approach , complementary to our approach, exploits the compositional model
to learn the conﬁguration of human bodies and adopts multilevel intermediate supervision, from which our approach
can also beneﬁt. We also tested our big network - HRNet-
W48 and obtained the same result 92.3. The reason might
be that the performance in this datatset tends to be saturate.
4.3. Application to Pose Tracking
Dataset. PoseTrack is a large-scale benchmark for human pose estimation and articulated tracking in video. The
dataset, based on the raw videos provided by the popular
MPII Human Pose dataset, contains 550 video sequences
with 66, 374 frames. The video sequences are split into
3 
Table 4. #Params and GFLOPs of some top-performed methods
reported in Table 3. The GFLOPs is computed with the input size
256 × 256.
Insafutdinov et al. 
Newell et al. 
Yang et al. 
Tang et al. 
SimpleBaseline 
292, 50, 208 videos for training, validation, and testing, respectively. The length of the training videos ranges between
41−151 frames, and 30 frames from the center of the video
are densely annotated. The number of frames in the validation/testing videos ranges between 65 −298 frames. The
30 frames around the keyframe from the MPII Pose dataset
are densely annotated, and afterwards every fourth frame is
annotated. In total, this constitutes roughly 23, 000 labeled
frames and 153, 615 pose annotations.
Evaluation metric. We evaluate the results from two aspects: frame-wise multi-person pose estimation, and multiperson pose tracking. Pose estimation is evaluated by the
mean Average Precision (mAP) as done in . Multiperson pose tracking is evaluated by the multi-object tracking accuracy (MOTA) . Details are given in .
Training. We train our HRNet-W48 for single person pose
estimation on the PoseTrack2017 training set, where the
network is initialized by the model pre-trained on COCO
dataset. We extract the person box, as the input of our network, from the annotated keypoints in the training frames
by extending the bounding box of all the keypoints (for one
single person) by 15% in length. The training setup, including data augmentation, is almost the same as that for COCO
except that the learning schedule is different (as now it is
for ﬁne-tuning): the learning rate starts from 1e−4, drops
to 1e−5 at the 10th epoch, and to 1e−6 at the 15th epoch;
the iteration ends within 20 epochs.
Testing. We follow to track poses across frames. It
consists of three steps: person box detection and propagation, human pose estimation, and pose association cross
nearby frames. We use the same person box detector as used
in SimpleBaseline , and propagate the detected box into
nearby frames by propagating the predicted keypoints according to the optical ﬂows computed by FlowNet 2.0 4,
followed by non-maximum suppression for box removing.
The pose association scheme is based on the object keypoint similarity between the keypoints in one frame and the
keypoints propagated from the nearby frame according to
the optical ﬂows. The greedy matching algorithm is then
used to compute the correspondence between keypoints in
4 
Table 5. Results of pose tracking on the PoseTrack2017 test set.
Additional training Data
ML-LAB 
COCO+MPII-Pose
SOPT-PT 
COCO+MPII-Pose
BUTD2 
COCO+MPII-Pose
PoseFlow 
COCO+MPII-Pose
ProTracker 
COCO+MPII-Pose
JointFlow 
COCO+MPII-Pose
MIPAL 
FlowTrack 
Table 6. Ablation study of exchange units that are used in repeated multi-scale fusion. Int. exchange across = intermediate
exchange across stages, Int. exchange within = intermediate exchange within stages.
Final exchange
Int. exchange across
Int. exchange within
nearby frames. More details are given in .
Results on the PoseTrack2017 test set. Table 5 reports
the results. Our big network - HRNet-W48 achieves the superior result, a 74.9 mAP score and a 57.9 MOTA score.
Compared with the second best approach, the FlowTrack
in SimpleBaseline , that uses ResNet-152 as the backbone, our approach gets 0.3 and 0.1 points gain in terms
of mAP and MOTA, respectively. The superiority over the
FlowTrack is consistent to that on the COCO keypoint
detection and MPII human pose estimation datasets. This
further implies the effectiveness of our pose estimation network.
4.4. Ablation Study
We study the effect of each component in our approach
on the COCO keypoint detection dataset. All results are
obtained over the input size of 256 × 192 except the study
about the effect of the input size.
Repeated multi-scale fusion. We empirically analyze the
effect of the repeated multi-scale fusion. We study three
variants of our network.
(a) W/o intermediate exchange
units (1 fusion):
There is no exchange between multiresolution subnetworks except the last exchange unit. (b)
W/ across-stage exchange units only (3 fusions): There
is no exchange between parallel subnetworks within each
stage. (c) W/ both across-stage and within-stage exchange
units (totally 8 fusion): This is our proposed method. All
the networks are trained from scratch. The results on the
Figure 4. Qualitative results of some example images in the MPII (top) and COCO (bottom) datasets: containing viewpoint and appearance
change, occlusion, multiple persons, and common imaging artifacts.
Figure 5. Ablation study of high and low representations. 1×, 2×,
4× correspond to the representations of the high, medium, low
resolutions, respectively.
COCO validation set given in Table 6 show that the multiscale fusion is helpful and more fusions lead to better performance.
Resolution maintenance. We study the performance of a
variant of the HRNet: all the four high-to-low resolution
subnetworks are added at the beginning and the depth are
the same; the fusion schemes are the same to ours. Both
our HRNet-W32 and the variant (with similar #Params and
GFLOPs) are trained from scratch and tested on the COCO
validation set. The variant achieves an AP of 72.5, which
is lower than the 73.4 AP of our small net, HRNet-W32.
We believe that the reason is that the low-level features extracted from the early stages over the low-resolution subnetworks are less helpful.
In addition, the simple highresolution network of similar parameter and computation
complexities without low-resolution parallel subnetworks
shows much lower performance .
Representation resolution. We study how the representation resolution affects the pose estimation performance
from two aspects: check the quality of the heatmap estimated from the feature maps of each resolution from high
SimpleBaseline
Figure 6. Illustrating how the performances of our HRNet and
SimpleBaseline are affected by the input size.
to low, and study how the input size affects the quality.
We train our small and big networks initialized by the
model pretrained for the ImageNet classiﬁcation. Our network outputs four response maps from high-to-low solutions. The quality of heatmap prediction over the lowestresolution response map is too low and the AP score is below 10 points. The AP scores over the other three maps
are reported in Figure 5. The comparison implies that the
resolution does impact the keypoint prediction quality.
Figure 6 shows how the input image size affects the
performance in comparison with SimpleBaseline (ResNet-
50) . We can ﬁnd that the improvement for the smaller
input size is more signiﬁcant than the larger input size, e.g.,
the improvement is 4.0 points for 256 × 192 and 6.3 points
for 128 × 96. The reason is that we maintain the high resolution through the whole process. This implies that our approach is more advantageous in the real applications where
the computation cost is also an important factor. On the
other hand, our approach with the input size 256 × 192 outperforms the SimpleBaseline with the large input size
of 384 × 288.
5. Conclusion and Future Works
In this paper, we present a high-resolution network for
human pose estimation, yielding accurate and spatiallyprecise keypoint heatmaps. The success stems from two
aspects: (i) maintain the high resolution through the whole
process without the need of recovering the high resolution; and (ii) fuse multi-resolution representations repeatedly, rendering reliable high-resolution representations.
The future works include the applications to other
prediction
segmentation,
detection,
alignment,
translation,
investigation
aggregating
multiresolution representations in a less light way.
them are available at 
github.io/Projects/HRNet/index.html.
Results on the MPII Validation Set
We provide the results on the MPII validation set .
Our models are trained on a subset of MPII training set and
evaluate on a heldout validation set of 2975 images. The
training procedure is the same to that for training on the
whole MPII training set. The heatmap is computed as the
average of the heatmaps of the original and ﬂipped images
for testing. Following , we also perform six-scale
pyramid testing procedure (multi-scale testing). The results
are shown in Table 7.
More Results on the PoseTrack Dataset
We provide the results for all the keypoints on the Pose-
Track dataset . Table 8 shows the multi-person pose estimation performance on the PoseTrack2017 dataset. Our
Table 7. Performance comparisons on the MPII validation set
( ).
Single-scale testing
Newell et al. 
Yang et al. 
Tang et al. 
SimpleBaseline 
Multi-scale testing
Newell et al. 
Yang et al. 
Tang et al. 
SimpleBaseline 
Table 8. Multi-person pose estimation performance (MAP) on
the PoseTrack2017 dataset.
“*” means models trained on thr
train+valid set.
Knee Ank. Total
PoseTrack validation set
Girdhar et al. 
70.2 62.0 51.7 60.7
Xiu et al. 
73.3 68.3 61.1 67.5
Bin et al. 
83.4 80.0 72.4 75.3
83.6 80.4 73.3 75.5
PoseTrack test set
Girdhar et al.* 
Xiu et al. 
67.5 65.0 59.0 62.5
Bin et al.* 
80.2 76.9 71.5 72.5
HRNet-W48*
80.2 76.9 72.0 73.4
Table 9. Multi-person pose tracking performance (MOTA) on
the PoseTrack2017 test set.“*” means models trained on the
train+validation set.
Knee Ank. Total
Girdhar et al.* 
Xiu et al. 
57.4 52.8 46.6 51.0
Xiao et al.* 
68.5 52.3 49.3 56.8
HRNet-W48*
68.9 52.2 49.6 57.7
HRNet-W48 achieves 77.3 and 74.9 points mAP on the validation and test setss, and outperforms previous state-ofthe-art method by 0.6 points and 0.3 points respectively. We provide more detailed results of multi-person
pose tracking performance on the PoseTrack2017 test set
as a supplement of the results reported in the paper, shown
in Table 9.
Results on the ImageNet Validation Set
We apply our networks to image classiﬁcation task. The
models are trained and evaluated on the ImageNet 2013
classiﬁcation dataset .
We train our models for 100
epochs with a batch size of 256. The initial learning rate
is set to 0.1 and is reduced by 10 times at epoch 30, 60
and 90. Our models can achieve comparable performance
as those networks speciﬁcally designed for image classiﬁcation, such as ResNet . Our HRNet-W32 has a singlemodel top-5 validation error of 6.5% and has a single-model
top-1 validation error of 22.7% with the single-crop testing.
Our HRNet-W48 gets better performance: 6.1% top-5 errors and 22.1% top-1 error. We use the models trained on
the ImageNet dataset to initialize the parameters of our pose
estimation networks.
Acknowledgements. The authors thank Dianqi Li and Lei
Zhang for helpful discussions.