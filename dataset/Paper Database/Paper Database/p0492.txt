HAL Id: inria-00633013
 
Submitted on 17 Oct 2011
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Aggregating local image descriptors into compact codes
Hervé Jégou, Florent Perronnin, Matthijs Douze, Jorge Sánchez, Patrick
Pérez, Cordelia Schmid
To cite this version:
Hervé Jégou, Florent Perronnin, Matthijs Douze, Jorge Sánchez, Patrick Pérez, et al.. Aggregating
local image descriptors into compact codes. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2012, 34 (9), pp.1704-1716. ￿10.1109/TPAMI.2011.235￿. ￿inria-00633013￿
Aggregating local image descriptors into compact
Herv´e J´egou, Florent Perronnin, Matthijs Douze, Jorge S´anchez, Patrick P´erez, Cordelia Schmid
Abstract— This paper addresses the problem of large-scale
image search. Three constraints have to be taken into account:
search accuracy, efﬁciency, and memory usage. We ﬁrst present
and evaluate different ways of aggregating local image descriptors
into a vector and show that the Fisher kernel achieves better
performance than the reference bag-of-visual words approach for
any given vector dimension. We then jointly optimize dimensionality reduction and indexing in order to obtain a precise vector
comparison as well as a compact representation. The evaluation
shows that the image representation can be reduced to a few
dozen bytes while preserving high accuracy. Searching a 100
million image dataset takes about 250 ms on one processor core.
Index Terms— image search, image retrieval, indexing
I. INTRODUCTION
This paper proposes an image indexing scheme for very
large databases. Image search consists here in ﬁnding in a
large set of images the ones that represent the same object
or location possibly viewed from different viewpoints and in
the presence of occlusions and clutter. Typical applications
include ﬁnding locations and particular objects , as
well as detecting partial image duplicates on the web 
and deformed copies . We do not consider speciﬁc visual
recognition tasks, such as face detection and recognition,
which require the use of speciﬁc descriptors and detectors.
We also do not consider semantic queries such as “retrieve
all images containing cars”, which are usually addressed by
supervised classiﬁcation techniques, such as those evaluated
in the PASCAL visual object recognition competition .
The bag-of-words (BOW) representation is the most widely
adopted method for this purpose . There are three reasons
for the success of the BOW representation. First, this representation beneﬁts from powerful local descriptors, such as
the SIFT descriptor or more recent ones , , .
Second, the BOW vector representation can be compared
with standard distances. Third, the BOW vector can be high
dimensional , , in which case the vectors are sparse and
inverted lists can be used to implement efﬁcient search .
However, two factors limit the number of images that can
be indexed in practice: the search time itself, which becomes
prohibitive when considering more than 10 million images,
and the memory usage per image.
In this paper, we address the problem of searching for the
most similar images to a given query in a very large image
database (a hundred million images). We put an emphasis
on the joint optimization of three constraints: the search
 
accuracy, its efﬁciency and the memory usage. The last two
are related , as search efﬁciency can be approximated
by the amount of memory visited during the search. Most
similar to our work is the approach of , which proposes
an approximate nearest neighbor search for BOW features.
However, this method is limited to small vocabularies and,
thus, obtains a relatively low search accuracy. Moreover, an
image needs to be encoded by more than a hundred bytes to
approximate the search accuracy of the initial representation.
The efﬁciency problem was partially addressed by the
min-Hash approach , and the pre-ﬁltering method
of . However, these techniques still require a signiﬁcant
amount of memory per image, and are more relevant in the
context of near-duplicate image detection, as their search
accuracy is signiﬁcantly lower than BOW. Some authors
address the efﬁciency and memory constraints by using GIST
descriptors , and by converting them to compact binary
vectors , , . These approaches suffer from the
limited robustness of the global GIST descriptor to geometric
transformations , and none fulﬁlls jointly the three aforementioned constraints: , require an exhaustive search
while is memory consuming due to the redundancy of
the LSH algorithm. In contrast, our approach obtains a higher
accuracy with a very compact representation, i.e., codes of 20
bytes. This performance is achieved by optimizing:
1) the representation, i.e., how to aggregate local image
descriptors into a vector representation;
2) the dimensionality reduction of these vectors;
3) the indexing algorithm.
These steps are closely dependent: a high-dimensional image representation usually provides better exhaustive search results than a low-dimensional one. However, high dimensional
vectors are more difﬁcult to index efﬁciently. A low dimensional vector can be indexed efﬁciently, but its discriminative
power may be too low.
Our ﬁrst contribution is an image representation of moderate
dimensionality that provides excellent search accuracy. Our
representation is based on Fisher kernels , a powerful
tool to transform an incoming variable-size set of independent
samples into a ﬁxed-size vector representation. We assume that
the samples are distributed according to a parametric generative model, in this case a Gaussian Mixture Model (GMM)
estimated on a training set. Perronnin et al. applied Fisher
kernels in the context of image classiﬁcation and large scale
image search . Experimental results demonstrate that this
choice signiﬁcantly outperforms BOW for the same size on
most datasets. Moreover, it is cheaper to compute because
fewer visual words are required. We also evaluate a simpliﬁed
version of Fishers kernels, the Vector of Locally Aggregated
Descriptors (VLAD) .
The description vectors are compared using the L2 distance.
Their dimensionality is reduced to a few hundreds components
by principal component analysis (PCA). To index the resulting
vectors, we show the advantage of jointly optimizing the tradeoff between the dimensionality reduction and the indexing
algorithm. We use the recent approximate nearest neighbor
search method of , which performs similarity search with
a technique derived from source coding, i.e., the encoded
vectors can be approximately reconstructed. As a result, the
error induced by PCA can be directly compared with the error
resulting from the vector compression.
The paper is organized as follows. Section II introduces
the datasets used to evaluate the components of our approach.
The different strategies used to aggregate the descriptors,
the BOW, the Fisher Kernel and the VLAD representations,
are presented in Section III and compared in Section IV.
We introduce the subsequent indexing stage in Section V,
where a joint optimization of dimensionality reduction and
indexing is proposed. Experimental results demonstrate the
performance of our approach in section VI: the performance
of BOW is obtained with an image representation of about
20 bytes. In terms of memory usage, search accuracy and
efﬁciency, this is a signiﬁcant improvement over the state
of the art , as well as over our two prior conference
publications , . This paper is built upon these two
prior publications and includes the following additions: (a) an
analysis of the dimensionality reduction for local descriptors as
well as for the ﬁnal aggregated representation; (b) a theoretical
and experimental comparison of the Fisher Kernel and the
VLAD representation , which allows improving our image
search system; and (c) additional experiments, in particular on
a dataset with 100 million images.
II. DATASETS AND EVALUATION PROTOCOL
To extract local features, we have used the experimental
setup of and the feature extraction software available
on-line1. The regions of interest are extracted using the
Hessian rotation- and afﬁne-invariant region detector 
and described by the SIFT descriptor . We have used
an independent image set for all the learning stages. The
evaluation is performed on the datasets described below, and
conducted by submitting each query image of the dataset to
the evaluated system, which returns a list of images sorted by
decreasing relevance.
The INRIA Holidays dataset is a collection of 1491
holiday images, 500 of them being used as queries. The
accuracy is measured by the mean Average Precision (mAP),
as deﬁned in .
The University
Recognition
(UKB ) contains images of 2550 objects, each of which is
represented by 4 images. The most common evaluation metric
for this dataset counts the average number of relevant images
1 
(including the query itself) that are ranked in the ﬁrst four
positions when searching the 10 200 images. This corresponds
to 4 times the recall@4 measure, i.e., the best performance is
The Oxford5K building dataset contains 5062 photos
gathered from Flickr with tags related to buildings in Oxford.
Each of the 55 queries is deﬁned by a rectangular region
delimiting a building on an image. The correct results for a
query are images of this building. The accuracy is measured
The Flickr10M dataset allows to evaluate the performance
of our method on a large scale with high quality pictures.
It was obtained by downloading 10 million random images
from Flickr. This image set is merged with the other datasets
to evaluate the accuracy and efﬁciency on a large scale .
The Exalead100M dataset is used for large scale evaluation
on up to 100M images and used in conjunction with the
Copydays dataset described below. The images have been
extracted from crawled web pages by Exalead, and were
downscaled uniformly to 150 pixels in their larger dimension
and compressed with a JPEG quality of 75.
INRIA Copydays was designed to evaluate near-duplicate
detection . The dataset contains 157 original images. To
obtain query images relevant in a copy detection scenario, each
image of the dataset has been transformed with three different
types of transformation:
• Image resizing (by a factor of 4 in dimension = 16
in surface), followed by JPEG compression ranging
from JPEG3 (very low quality) to JPEG75 (typical web
quality). For each compression factor, 157 images are
generated (one per database image).
• Cropping ranging from 5% to 80% of the image surface.
Here, we use only the queries with the cropping parameter ﬁxed to 50% (one per database image, i.e., 157 query
images in total).
• Strong transformations: print and scan, occlusion, change
in contrast, perspective effect, blur, very strong cropping,
etc. There is in total 229 transformed images, each of
which has only a single matching image in the database.
The accuracy is measured by mAP. We merge the database
with 10,000 distractor images from Flickr10M for most experiments. It is merged with images from Exalead100M for
our very large scale evaluation. In this case, in order to obtain
an image quality consistent with that of Exalead100M, the
same down-scaling factor and JPEG compression is applied
to the Copydays images.
For experiments on large image sets, we will use the
recall@R metric, i.e., the rate of relevant images that are
ranked in the top R positions. This measure is relevant for two
reasons. First, evaluating recall@R with respect to R indicates
the empirical distribution of the rank of the true positives.
Second, if the rank R is ﬁxed to a small value (e.g., R = 100),
this indicates if a method provides a relevant shortlist for a
subsequent precise (and more costly) direct image comparison
based on geometric veriﬁcation , .
III. IMAGE VECTOR REPRESENTATION
In this section, we introduce the different vector aggregation methods compared in this paper, namely the BOW
representation, the Fisher Vector (FV) representation, and the
VLAD vectors, which are shown to be an extremal case of
FV. All these methods aim at aggregating d-dimensional local
descriptors into a single vector representation.
A. Bag-of-features
The BOW representation groups together local descriptors.
It requires the deﬁnition of a codebook of K centroids (“visual
words”) usually obtained by k-means clustering. Each local
descriptor of dimension d from an image is assigned to the
closest centroid. The BOW representation is the histogram
of the number of image descriptors assigned to each visual
word. Therefore, it produces a K-dimensional vector, which is
subsequently normalized. There are several variations on how
to normalize the histogram. When seen as an empirical distribution, the BOW vector is normalized using the Manhattan
distance. Another common choice consists in using Euclidean
normalization. The vector components are then weighted by idf
(inverse document frequency) terms , . In the following,
we perform L2 normalization of histograms and use the idf
calculation of . Several variations have been proposed to
improve the quality of this representation. One among the
most popular , consists in using soft quantization
techniques instead of a k-means.
B. Fisher vector
By counting the number of occurrences of visual words,
BOW encodes the 0-order statistics of the distribution of
descriptors. The Fisher vector extends the BOW by encoding
high-order statistics (ﬁrst and, optionally, second order). This
description vector is the gradient of the sample’s likelihood
with respect to the parameters of this distribution, scaled by
the inverse square root of the Fisher information matrix. As
a result, it gives the direction, in parameter space, into which
the learned distribution should be modiﬁed to better ﬁt the
observed data. In other terms, FV describes how the set of descriptors deviates from an average distribution of descriptors,
modeled by a parametric generative model. It has been shown
 that discriminative classiﬁers can be learned in this new
representation space. In our image search framework, the FV
is seen as a method to capture the information conveyed by a
set of descriptors into a ﬁxed-length signature.
The Fisher kernel framework. Let X = {xt, t = 1 . . . T}
denote the set of T local descriptors extracted from an image.
We assume for now that the generation process of X can be
modeled by an image-independent probability density function
uλ with parameters λ 2. Jaakkola and Haussler proposed
to describe X by the vector :
T ∇λ log uλ(X).
2We abuse the notation to simplify the presentation: λ denotes both the set
of parameters of u as well as the estimate of these parameters.
The gradient of the log-likelihood describes the contribution
of the parameters to the generation process. Its dimensionality
only depends on the number of parameters in λ. A natural
kernel on these gradients is the “Fisher kernel” :
K(X, Y ) = GX
where Fλ is the Fisher information matrix of uλ:
Fλ = Ex∼uλ
∇λ log uλ(x)∇λ log uλ(x)⊤
is symmetric and positive deﬁnite, it has a Cholesky
decomposition F −1
λ Lλ. Therefore the kernel K(X, Y )
is advantageously rewritten as a dot-product between normalized vectors Gλ, obtained as
We refer to GX
λ as the Fisher vector (FV) of X.
The FV image representation. We follow Perronnin and
Dance and choose uλ to be a GMM: uλ(x)
i=1 wiui(x). We denote λ = {wi, µi, σi, i = 1 . . . K}
where wi, µi and σi are respectively the mixture weight, mean
vector and variance matrix (assumed diagonal) of Gaussian
ui. The model uλ can be understood as a probabilistic visual
vocabulary. It is trained ofﬂine on a large number of images
using Maximum Likelihood (ML) estimation. In this work, we
focus on the gradient with respect to the mean. We use the
diagonal closed-form approximation of the Fisher information
matrix of , in which case the normalization of the gradient
by Lλ = F −1/2
is simply a whitening of the dimensions.
Let γt(i) be the soft assignment of descriptor xt to the ith
j=1 wjuj(xt)
be the d-dimensional gradient with respect to the
mean µi of Gaussian i. Assuming that the xt’s were generated
independently by uλ, we obtain after standard mathematical
derivations:
The ﬁnal vector GX
λ is the concatenation of the GX
vectors for
i = 1 . . . K and is therefore Kd-dimensional3. In comparison
with the BOW, d times fewer visual words are required to
obtain a signature of the same length. Experimental results
show that excellent results can be obtained even with a
relatively small number of visual words K: we typically
consider values ranging from K=16 to K=256.
FV Normalization. The FV undergoes two normalization
steps. First, a power normalization is performed by applying
the following operator independently on each component:
f(z) = sign(z)|z|α,
3Taking the gradient with respect to the mean and variance leads to vectors
of size 2Kd. We can obtain vectors of the same size by taking the gradient
with respect to the mean only on a GMM with 2K Gaussians. In a set of
preliminary experiments, we observed that for a ﬁxed FV size the two options
led to very similar results.
with 0 ≤α ≤1. We empirically observe that this step
consistently improves the quality of the representation. Several
complementary interpretations justify this transform. First, it
reduces the inﬂuence of bursty visual elements, which were
shown to corrupt the image similarity in . Second, assuming the compound Poisson distribution as a good generative
model of FVs, the power normalization can be interpreted as a
variance stabilizing transform, which corrects the dependence
between the variance and the mean. We provide an analysis of
this interpretation in the Appendix provided as supplemental
material. Variance stabilizing transforms have previously been
applied to BOW histograms, e.g. in for GMM modeling
and in , for better separability of linear classiﬁers.
We found experimentally that the optimal value for α
depends on the number of Gaussians K. However, setting
α = 0.5 consistently leads to near-optimal results on the range
of K values we consider.
The power normalized FV is then L2-normalized, which
amounts to using the cosine similarity between FVs. It guarantees that if we query a database with one of its images, the
ﬁrst result will be the image itself if the inner product is used
as a similarity measure – a desirable property.
Properties of the FV. We now assume that the descriptors
X = {xt, t = 1 . . . T} of a given image are iid and follow a
distribution p which departs from the generic distribution uλ.
According to the law of large numbers (convergence of the
sample average to the expected value when T increases), we
∇λ log uλ(xt)
∇λEx∼p log uλ(x)
p(x) log uλ(x)dx.
Now let us assume that we can decompose p into a mixture of
two parts: a background image-independent part which follows
uλ and an image-speciﬁc part which follows an image-speciﬁc
distribution q. Let 0 ≤ω ≤1 be the proportion of imagespeciﬁc information contained in the image:
p(x) = ωq(x) + (1 −ω)uλ(x).
We can rewrite:
q(x) log uλ(x)dx
uλ(x) log uλ(x)dx.
If the values of the parameters λ were estimated with a ML
process – i.e., to maximize (at least locally and approximately)
Ex∼uλ log uλ(x) – then:
uλ(x) log uλ(x)dx = ∇λEx∼uλ log uλ(x) ≈0. (13)
Consequently, we have:
q(x) log uλ(x)dx = ω∇λEx∼q log uλ(x).
This shows that the image-independent information is approximately discarded from the FV signature, and that an image
is described by what makes it different from other images (on
average). Such a decomposition of images into background
and image-speciﬁc information has also been employed in
BOW approach by Zhang et al. . However, while the
decomposition is explicit in , it is implicit in the FV case.
Relationship with BOW. We introduce
t=1 γt(i)xt
is the proportion of descriptors of X soft-assigned
to visual word i, i.e., this is the soft-BOW representation. The
i is the average of the descriptors of X weighted by
their probability of being assigned to Gaussian i (i.e., loosely
speaking, the average of the descriptors of X assigned to
Gaussian i). We can rewrite the gradient (6) as follows:
This clearly shows that the FV extends the BOW representation. Not only does the FV count the number of descriptors
assigned to each region in the space, it also encodes the approximate location of the descriptors in each region (relatively
to the mean µi and the variance σi). The division by √wi in
(17) can be interpreted as a BOW idf term: the descriptors
which occur frequently are automatically discounted.
C. VLAD: non probabilistic Fisher Kernel
In , J´egou et al. proposed the VLAD representation. As
for the BOW, a codebook {µ1, ...µK} is ﬁrst learned using
k-means. Each local descriptor xt is associated to its nearest
visual word NN(xt) in the codebook. For each codeword µi,
the differences xt −µi of the vectors xt assigned to µi are
accumulated:
xt:NN(xt)=i
The VLAD is the concatenation of the d-dimensional vectors vi and is therefore Kd dimensional. Algorithm 1 sums
up the resulting algorithm. As for FV, the VLAD can then be
power- and L2-normalized.
We now show that the VLAD is a simpliﬁed nonprobabilistic version of the FV: the VLAD is to the FV what
k-means is to GMM clustering. The k-means clustering can be
viewed as a non-probabilistic limit case of GMM clustering
a) the mixture weights are equal, i.e., wi = 1/K, ∀i,
b) covariance matrices are isotropic, i.e., σi = ǫI with ǫ ≥0,
c) the value ǫ converges to zero.
If we enforce hypotheses a) and b), Equation 6 simpliﬁes to:
γt(i) (xt −µi) ,
Images and corresponding VLAD descriptors, for K=16 centroids. The components of the descriptor are represented like SIFT, with negative
components in red.
Algorithm 1 Computation of the VLAD descriptor V from a
set of descriptors x1, . . . , xT . The set µ1, . . . , µK of centroids
is learned on a training set using k-means.
For i = 1, . . . , K
% accumulate descriptor
For t = 1, . . . , T
i = arg minj ||xt −µj||
vi := vi + xt −µi
1 . . . v⊤
% apply power normalization
For u = 1, . . . , Kd
Vu := sign(Vu) |Vu|α
% apply L2 Normalization
up to a factor independent of ǫ after normalization.
As ǫ →0, the Gaussians converge to Dirac distributions and
hypothesis c) makes the assignments γt(i) binary: γt(i) = 1
if NN(xt) = i and γt(i) = 0 otherwise. Therefore, as ǫ →0
Equation 19 becomes
xt:NN(xt)=i
where we recognize the VLAD representation.
Figure 1 depicts the VLAD representations associated with a
few images, when aggregating 128-dimensional SIFT descriptors. The components of our descriptor map to components of
SIFT descriptors. Therefore we adopt the usual 4 × 4 spatial
grid with oriented gradients for each vi,, i = 1...K, with
K = 16. In contrast to SIFT descriptors, a component may be
positive or negative, due to the subtraction in Equation 18.
One can observe that the descriptors are relatively sparse
(few values have a signiﬁcant energy) and very structured:
most high descriptor values are located in the same cluster, and
the geometrical structure of SIFT descriptors is observable.
Intuitively and as shown later, a principal component analysis
is likely to capture this structure. For resembling images, the
similarity of the descriptors is obvious.
D. Dimensionality reduction on local descriptors
Principal component analysis (PCA) is a standard tool 
for dimensionality reduction: the eigenvectors associated with
the most energetic eigenvalues of the empirical vector covariance matrix are used to deﬁne a matrix M mapping a vector
x ∈R128 to a vector x′ = Mx in a lower-dimensional space.
We will show in next section that applying the Fisher Kernel
framework directly on local descriptors leads to suboptimal
results. Therefore, we apply a PCA on the SIFT descriptors
to reduce them from 128 to d = 64 dimensions. Two reasons
may explain the positive impact of this PCA:
1. Decorrelated data can be ﬁtted more accurately by a
GMM with diagonal covariance matrices;
2. The GMM estimation is noisy for the less energetic
components.
To conﬁrm that these hypotheses are reasonable, we have
performed two experiments on descriptor transformations.
First, we applied a random rotation matrix after the PCA,
that cancels its decorrelating effect. This signiﬁcantly degrades
the search quality. The second transformation is the PCA
rotation without dimensionality reduction. This also degrades
the retrieval performance, which means that the least energetic
components are detrimental.
IV. EVALUATION OF THE AGGREGATION METHODS
In this section, we evaluate and compare the different local
aggregation methods described in Section III, BOW, FV, and
VLAD, as well as the global GIST descriptor . We analyze
the impact of the number K of centroids/mixture components
and study the impact of dimensionality reduction at two stages
of the algorithms:
• the local SIFT descriptors are reduced from 128 to 64
components using PCA (see Section III-D);
• the ﬁnal VLAD, Fisher and BOW vectors are reduced
from D = Kd to D′ components using PCA.
In both cases, the PCA rotation matrices are learned on
an independent image dataset. Note that in this section the
evaluation is performed without the subsequent indexing stage.
Dimensionality reduction of local descriptors. Figure 2
compares the FV and VLAD representations on the Holidays
512 1024 2048 4096
number of centroids/Gaussians
Fisher, Full
VLAD, Full
Fisher, PCA
Fig. 2. Comparison of the Fisher and VLAD representations with and without
reduction of the local descriptors with PCA (mAP on Holidays).
dataset as a function of K. The respective results are similar
if these representations are learned and computed on the plain
SIFT descriptors, as observed in . However, applying PCA
to local descriptors (reduced to 64 dimensions) consistently
improves the results of the FV, see Section III-D for possible
reasons. This dimensionality reduction does not improve the
results for VLAD, probably because it does not beneﬁt from
the decorrelating effect of the PCA. As a result, FV+PCA
outperforms VLAD by a few points of mAP.
In the rest of the paper, the local descriptors are reduced to
64 dimensions using PCA in all experiments.
Impact of the codebook size. Figure 2 clearly shows that
the larger the number of centroids, the better the performance.
For K=4096 we obtain a mAP=68.9%, which outperforms
any result reported for standard BOW on this dataset ( 
reports mAP=57.2% with a 200k vocabulary using the same
descriptors). Interestingly, the slope of the curve does not
exhibit a saturation for the values of K considered. However,
for high values of K the dimensionality of the vector becomes
very large and is not compatible with the dimensionality
reduction strategy proposed in this paper.
Comparison of BOW/VLAD/FV. The results are presented
in Tables I and II. Overall, one can observe that Fisher and
VLAD provide competitive results. On Holidays and UKB,
both Fisher and VLAD with only K=64 outperform the BOW
representation. However, BOW is comparatively better when
the variability of the objects/images is limited, as for the
Oxford Building dataset and the Copydays dataset. In this
case, very large vocabularies are necessary and Philbin et al.
 used up to 1 million visual words, thereby obtaining a
mAP=49.3% on Oxford5K when learning the 1M vocabulary
on the Paris dataset4 and using soft quantization. FV outperforms these results by increasing K: using the setup of 
(same descriptors and same learning set), we obtained the
following mAP values:
4Results obtained when learning the vocabulary on the Oxford Building set
itself do not correspond to a realistic setup, as in that case the vocabulary is
very speciﬁc and not likely to be re-used for other types of images or larger
Dimensionality reduction on BOW/VLAD/FV. Our objective
is to obtain a very compact representation. We, therefore, evaluate the performance of each representation when reduced to
D′=128 dimensions. Tables I and II show that the conclusions
for the plain representations remain valid. The main difference
appears for the Oxford dataset where the PCA has a strong
negative impact on the performance, especially in the case of
Dimension reduction does not necessarily reduce the accuracy, as already observed in
 . Indeed, Table I shows
that a limited reduction tends to improve the accuracy for
BOW, Fisher and VLAD. It is also worth noticing that higher
dimensional representations, which usually provide better accuracy, suffer more from the dimensionality reduction. This
is especially true for Fisher kernels and VLAD: for D′=64,
128, 512, using only 64 centroids/mixtures is better than using
larger values of K. Using only D′=128 dimensions, i.e., the
dimensionality of a single SIFT descriptor, Fisher attains the
excellent accuracy of mAP=56.5% on Holidays, which is
comparable to the result obtained with BOW when using a
vocabulary of 200k visual words.
Comparison with GIST. The global GIST descriptor is very
popular in some recognition systems . It was compared
with BOW in , where it was shown to provide lower
results than BOW for image search. However, its performance
is competitive in a near duplicate detection context. Table II
conﬁrms these observations. It appears that the full 960dimensional GIST descriptor is signiﬁcantly outperformed by
all other representations except for the JPEG5 transformation,
which does not change the image geometry. This is true even
when BOW/Fisher/VLAD are reduced to a small number of
dimensions. Overall, this global descriptor results in a poor
performance for image search.
Conclusion. In this section, we have evaluated the performance of VLAD and FV and measured the impact of the
different dimensionality reduction steps. In particular, our
study explains most of the difference between the results
reported for FV in and : the better performance of
FV in is mainly due to the power normalization, which
consistently improves the results, and to the dimensionality
reduction of the local descriptors by PCA, which is highly
beneﬁcial for FV. In this setup, the results of FV are better
than those of VLAD on most databases, see Table II. In the
following, we adopt the choices that are, on average, the best:
• SIFT descriptors reduced to 64 dimensions by PCA;
• FV with power normalization of the components (α =
0.5) followed by a L2 normalization.
V. FROM VECTORS TO CODES
This section addresses the general problem of coding an
image descriptor. Given a D-dimensional input vector, we
want to produce a code of B bits encoding the image representation, such that the nearest neighbors of a (non-encoded)
query vector can be efﬁciently searched in a set of n encoded
database vectors.
Descriptor
Holidays (mAP)
Fisher (µ)
COMPARISON OF BOW, FISHER AND VLAD REPRESENTATIONS, BEFORE AND AFTER DIMENSION REDUCTION: THE PERFORMANCE IS GIVEN FOR THE
FULL D-DIMENSIONAL DESCRIPTOR AND AFTER A REDUCTION TO D′ COMPONENTS WITH PCA. THE NUMBER K STANDS FOR THE NUMBER OF
CENTROIDS FOR BOW AND VLAD, AND FOR THE NUMBER OF GAUSSIANS FOR FISHER.
Descriptor
PCA to D′ =
UKB (4 × R@4)
Oxford (learned on Paris)
Copydays JPEG 5 (+10k images)
Copydays Crop 50% (+10k images)
Copydays Strong (+10k images)
PERFORMANCE OF THE RAW DESCRIPTORS AS WELL AS DESCRIPTORS COMPRESSED TO D′=128 ON SEVERAL DATASETS, MEASURED BY MAP FOR ALL
DATASETS EXCEPT FOR UKB.
We handle this problem in two steps, that must be optimized jointly: 1) a projection that reduces the dimensionality
of the vector (see previous section) and 2) a quantization
used to index the resulting vectors. We consider the recent
approximate nearest neighbor search method of , which
is brieﬂy described in the next subsection. We will show the
importance of the joint optimization by measuring the mean
squared Euclidean error generated by each step.
A. Approximate nearest neighbor
Approximate nearest neighbors search methods , ,
 , , are required to handle large databases in computer vision applications . One of the most popular techniques is Euclidean Locality-Sensitive Hashing , which
has been extended in to arbitrary metrics. However, these
approaches and the one of are memory consuming, as
multiple hash tables or trees are required. The method of ,
which embeds the vector into a binary space, better satisﬁes the
memory constraint. It is, however, signiﬁcantly outperformed
in terms of the trade-off between memory and accuracy by
the product quantization-based approximate search method
of . In the following, we use this method, as it offers
better accuracy and because the search algorithm provides
an explicit approximation of the indexed vectors. This allows
us to compare the vector approximations introduced by the
dimensionality reduction and the quantization, respectively.
We use the asymmetric distance computation (ADC) variant of
this approach, which only encodes the vectors of the database,
but not the query vector. This method is summarized in the
following.
a) ADC approach.: Let x ∈RD be a query vector and
Y = {y1, . . . , yn} a set of vectors in which we want to
ﬁnd the nearest neighbor NN(x) of x. The ADC approach
consists in encoding each vector yi by a quantized version
ci = q(yi) ∈RD. For a quantizer q(.) with k centroids, the
vector is encoded by B=log2(k) bits, k being a power of 2.
Finding the a nearest neighbors NNa(x) of x simply consists
in computing
NNa(x) = a- arg min
||x −q(yi)||2.
Note that, in contrast with the embedding method of , the
query x is not converted to a code: there is no approximation
error on the query side.
To get a good vector approximation, k should be large (k =
264 for a 64 bit code). For such large values of k, learning a
k-means codebook as well as assigning to the centroids is not
tractable. Our solution is to use a product quantization method
which deﬁnes the quantizer without explicitly enumerating its
centroids. A vector x is ﬁrst split into m subvectors x1, ... xm
of equal length D/m. A product quantizer is then deﬁned as
a function
 q1(x1), ..., qm(xm)
which maps the input vector x to a tuple of indices by
separately quantizing the subvectors. Each individual quantizer
qj(.) has ks reproduction values learned by k-means. To limit
the assignment complexity O(m × ks), ks is a small value
(e.g., ks=256). However, the set k of centroids induced by the
product quantizer q(.) is large: k = (ks)m.
The square distances in Equation 21 are computed using the
decomposition
||x −q(yi)||2 =
||xj −qj(yj
i is the jth subvector of yi. The square distances in
this summation are read from look-up tables computed, prior
to the search, between each subvector xj and the ks centroids
associated with the corresponding quantizer qj. The generation
of the tables is of complexity O(D × ks). When ks ≪n, this
complexity is negligible compared with the summation cost of
O(D × n) in Equation 21.
This quantization method offers an explicit vector approximation: a database vector yi can be decomposed as
yi = q(yi) + εq(yi),
where q(yi) is the centroid associated with yi and εq(yi) the
error vector generated by the quantizer.
Notation: ADC m × bs refers to the method when using m
subvectors and bs bits to encode each subvector (bs = log2 ks).
The total number of bits B used to encode a vector is then
given by B = m bs.
B. Indexation-aware dimensionality reduction
Dimensionality reduction is an important step in approximate nearest neighbor search, as it impacts the subsequent
indexing. In this section, for the ADC approach, we express
the tradeoff between this operation and the indexing scheme
using a single quality measure: the approximation error. For
the sake of presentation, we assume that the mean of each
vector component is 0. By construction, this is approximately
the case for Fisher5 and VLAD vectors.
The D′ ×D PCA matrix M maps descriptor x ∈RD to the
transformed descriptor x′ = Mx ∈RD′. It is the upper part of
an orthogonal matrix. This dimensionality reduction can also
be interpreted in the initial space as a projection. In that case,
x is approximated by
xp = x −εp(x)
where the error vector εp(x) lies in the null space of M. The
vector xp is related to x′ by the pseudo-inverse of M, which
is the transpose of M in this case. Therefore, the projection
is xp = M ⊤Mx. For the purpose of indexing, the vector x′
is subsequently encoded as q(x′) using the ADC approach,
which can also be interpreted in the original D-dimensional
space as the approximation6
q(xp) = x −εp(x) −εq(xp)
Ex∼uλ∇λ log uλ(x)
∇λEx∼uλ log uλ(x) ≈0 if uλ is estimated with MLE.
6For the sake of conciseness, the quantities MT q(x′) and MT εq(x′) are
simpliﬁed to q(xp) and εq(xp) respectively.
where εp(x) ∈Null(M) and εq(xp) ∈Null(M)⊥(because
the ADC quantizer is learned in the principal subspace) are
orthogonal. At this point, we make two observations:
1) Due to the PCA, the variance of the different components of x′ is not balanced. Therefore the ADC structure,
which allocates a ﬁxed number of bits per subvector,
quantizes the ﬁrst principal components more coarsely
in comparison with the last components, leading to
a high quantization error on the ﬁrst components. In
order to address this problem, it is possible to balance
the components’ variance by applying an orthogonal
transformation after the PCA. In , two strategies
are compared. First a Householder matrix is learned to
perfectly balance the energy on the components. The
second strategy simply consists in applying a random
rotation matrix after the PCA. Both approaches improve
the results, and the random matrix is shown to be as
effective as the learned one. We therefore adopt this
choice in the following.
2) There is a trade-off on the number of dimensions D′ to
be retained by the PCA. If D′ is large, the projection
error vector εp(x) is of limited magnitude, but a large
quantization error εq(xp) is introduced. On the other
hand, keeping a small number of components leads to a
high projection error and a low quantization error.
Joint optimization of reduction/indexing. Let us now consider the second problem, i.e., optimizing the dimension D′,
having ﬁxed a constraint on the number of bits B used to
represent the D-dimensional vector x, for instance B=128
(16 bytes). The squared Euclidean distance between the reproduction value and x is the sum of the errors ||εp(x)||2
and ||εq(xp)||2, both of which depend on the selected D′.
The mean square error e(D′) is empirically measured on a
learning vector set L as
e(D′) = ep(D′) + eq(D′)
||εp(x)||2 + ||εq(xp)||2.
This gives us an objective criterion to optimize directly the
dimensionality, which is obtained by ﬁnding on the learning
set the value of D′ minimizing this criterion.
• The selection of D′ using this mean square error minimization is not optimized with respect to an image search
criterion. Note however minimizing this error appears to
be a reasonable choice, as empirically observed in .
• In the ADC indexing scheme, D′ must be a multiple of
m. For instance, by keeping D′=64 eigenvalues, the valid
set of values for m is {1,2,4,8,16,32,64}.
The impact of dimensionality reduction and indexation
based on ADC is illustrated by the VLAD pictorial representation introduced in Section III. We can present the
projected and quantized VLAD in this form, as both the PCA
projection and the quantization provide a way of reconstructing
the projected/quantized vector. Figure 3 illustrates how each
Effect of the encoding steps on the descriptor. Top: VLAD vector x for K=16 . Middle: vector xp altered by the projection onto the PCA
subspace (D′=128). Bottom: vector q(xp) after indexing by ADC 16 × 8 (16-bytes code).
Fisher, K=16, ADC 16x8
Fisher, K=64, ADC 16x8
Fisher, K=256, ADC 16x8
Fisher, K=16, ADC 256x10
Fisher, K=64, ADC 256x10
Fisher, K=256, ADC 256x10
Search on Holidays with ADC 16×8 and ADC 256×10 representations, for reduction to varying dimensions D′. Experiments are averaged over
5 learning runs (error bars = standard deviations over the runs).
of these operations impacts our representation. One can see
that the vector is only slightly altered, even for a compact
representation of B=16 bytes.
VI. EXPERIMENTS
In this section, we evaluate the performance of the
Fisher vector when used with the joint dimensionality reduction/indexing approach of Section V. This evaluation uses
the improved FV representation of in conjunction with
the indexing scheme of . Our comparison focuses on
the UKB and Holidays datasets. Large scale experiments on
Holidays+Flickr10M were used to measure search accuracy
and efﬁciency on a large scale of 10 million images. As we
focus on the intrinsic quality of the large scale system, we do
not apply the re-ranking stage which is usually performed on
a shortlist to ﬁlter out geometrically inconsistent results ,
A. Dimensionality reduction and indexation
Given an image representation with a vector of length D
and a ﬁxed number B of bits to encode this vector, Figure 4
conﬁrms the analysis of Section V: there is an important
variation with respect to D′. A method to ﬁx D′ consists in
minimizing the total square error introduced by the projection
and the quantization steps, as suggested in V-B. This choice
may not be optimal with respect to image search quality
measured by, e.g., mAP. However, as the performance is stable
BOW, K=20,000
BOW, K=200,000
miniBOF 
FV K=64, spectral hashing 128 bits
VLAD, K=16, ADC 16×8 
VLAD, K=64, ADC 32×10 
FV K=8, binarized 
FV K=64, binarized 
FV K=64, ADC 16×8 (D′=96)
FV K=256, ADC 256×10 320
COMPARISON OF OUR APPROACH WITH THE STATE OF THE ART ON UKB
(SCORE/4) AND HOLIDAYS (MAP).
around the optimal value of D′ (see Figure 4), in practice
the proposed optimization procedure provides close-to-optimal
B. Comparison with the state of the art
Table III and Figure 5 compare the performance obtained by
our indexing scheme to the state of the art on the Holidays and
UKB datasets. Compared to the miniBOF approach of ,
the proposed approach is signiﬁcantly more precise at all
operating points. Compared to BOW, our approach obtains a
comparable search quality with about two orders of magnitude
less memory. With respect to the approaches and , a
signiﬁcant improvement is obtained by using the improved FV
of jointly with the indexing method of .
Figure 5 also illustrates the trade-off between search quality
and memory usage. Interestingly, the best choice for the
number K of Gaussians depends on the number of bits B
chosen to represent the image. Compared to BOW, which gives
mAP=54% for a 200k vocabulary, a competitive accuracy
of mAP=55.2% is obtained with only 32 bytes. Note that
small (resp. large) values of K should be associated with
small (resp. large) values of B, as they are more impacted
by dimensionality reduction. On this ﬁgure, the variant K=16
is never selected as the best, which explains why only K=64
and K=256 appear in the plot.
Table III also compares our indexing scheme to the spectral
hashing coding scheme. For a memory usage of 16 bytes,
ADC outperforms spectral hashing by more than 10 points of
mAP on Holidays. Similarly, it is possible to use signiﬁcantly
number of bytes
ADC parameters
Fisher K=64
Fisher K=256
miniBOF 
miniBOF 16
miniBOF 32
mAP for a search on Holidays with varying ADC quantization
parameters (number of bytes). Results of are reported with circles for
reference.
less memory to attain the same performance: using 5 bytes per
image in ADC achieves the same mAP (≈40%) as SH with
16 bytes, see Figure 5.
C. Large-scale experiments
1) Experiments on Holidays and Flickr10M:
shows the behavior of our approach on a large scale. We have
constructed a dataset by combining the images from Holidays
with a subset of Flickr10M of increasing size. Recall that, in
such a large dataset, the number of outliers is very important.
The challenge is to rank the relevant images well enough to
ensure that geometrical veriﬁcation will validate them in a
re-ranking stage, which can typically be performed for a few
hundred images only.
For this experiment, we have used the non exhaustive search
variant of ADC, called IVFADC. IVFADC combines ADC
with an inverted ﬁle to restrict the search to a subset of
vectors: in our case, only 64 lists of coded descriptors are
visited out of 8192 lists in total. Consequently, it stores the
image identiﬁers explicitly, which requires an extra 4 bytes of
memory per image (see for details). Compared to ADC,
on large datasets IVFADC is one or two orders of magnitude
faster and gives slightly better results.
The mAP performance is displayed as a function of the
dataset size. We report the results for
• the plain Fisher vector (K=64, D=4096);
• the same Fisher vector reduced to D′=96 dimensions by
• these PCA-reduced FV indexed by IVFADC with 16×8
codes, i.e., 16+4=20 bytes per indexed image. We also
present a more expensive operating point, for which
K=256, D′=2048, and 256×10 codes have been used,
leading to a representation of 324 bytes in memory.
The results are signiﬁcantly better than those reported
in , where a mAP of 6.6% is reported for 1 million
images and a 20-bytes representation, against mAP=27.9%
for the same memory usage with the proposed method, and
mAP=37.0% when using 324 bytes. Note that this is also an
Database size
BOW, K=200k
Fisher K=64, D=4096
Fisher K=64, PCA D’=96
Fisher K=64, IVFADC 64/8192, 16x8
Fisher K=256, IVFADC 64/8192, 256x10
Search accuracy as a function of the database size.
recall@R (%)
BOW K=200k
Fisher K=64, PCA 96
Fisher K=64, IVFADC 64/8192, 16x8
Fisher K=256, IVFADC 64/8192, 256x10
Quality of the shortlist (of varying size R): recall@R when searching
in 1 million images.
improvement over , where we obtained mAP=24.1% using
the VLAD vector. Interestingly, the 96-dimensional vector
obtained by PCA offers results comparable to those of BOW
with a large vocabulary.
In order to measure how our system would beneﬁt from
being used in conjunction with a post-veriﬁcation scheme,
Figure 7 gives the recall@R as a function of the number R.
It can be interpreted as the rate of relevant images that will
be geometrically veriﬁed if we consider that a veriﬁcation is
applied on a short-list of R images (typically, R is limited
to 100). The experiment is limited to Holidays+Flickr1M
because BOW does not scale to 10 million images. With a
representation of 20 bytes, the proposed approach is almost
as accurate as BOW, and becomes signiﬁcantly better when
increasing the size of the codes.
2) Experiments on Copydays and Exalead100M: Given the
compactness of our image descriptor encoding, our method
can scale up to one billion images with a reasonable amount
of RAM (for instance, 20GB when using only 20 bytes per
image). Only a few systems are able to work on such a scale,
an example is the GISTIS approach of . This approach
database size
Crop 50% of image surface
Fisher+IVFPQ
100 1000 10k
database size
Strong transformations
Fisher+IVFPQ
Copydays retrieval results (mAP), for two types of transformations
and a varying number of distractors.
combines the GIST descriptor with an indexing method
derived from . Results are reported on 110 million tiny
images(32 × 32 pixels). In the following, we report a comparison of our approach with the GISTIS method for Copydays
merged with the Exalead100M dataset (See Section II for
a description). Since the images are smaller in this setup
(150 pixels in their larger dimension), the local descriptor’s
threshold (on cornerness) is reduced to increase the number
of descriptors input to the Fisher computation.
Figure 8 shows the results for two subsets of Copydays:
crop 50% (157 transformed queries, 1 per database image)
and strong transformations (229 query images, each of which
has only 1 matching image in the database). The number of
distracting images from Exalead100M is increased up to the
full dataset (100M images). We can observe that the uncompressed Fisher descriptor (K = 64, D = 4096) is clearly more
discriminant than the uncompressed color GIST descriptor
(D = 960). The IVFADC conﬁguration (IVF 64/8192, 64×8)
was selected to allocate the same size per descriptor as the
GISTIS indexing method: 68 bytes per image. The results after
indexing are better than for GISTIS. Moreover, the slope of the
curves shows that our approach is less sensitive to distractors:
the comparative performance is much better for our scheme
as the database grows.
Timings: All timing experiments have been performed on a
single processor core. On average, searching our 100 million
dataset with the IVFADC indexing structure (64×8 codes, 64
lists visited out of 8192) takes 245 ms. This efﬁciency is at
least two orders of magnitude above the BOW: reports
a query time of 620 ms on a quad-core processor to search
in 1 million images given a vocabulary of K=200k visual
words. The time search is 50% higher than the one of GISTIS
(160ms), but for a signiﬁcantly better search quality.
VII. CONCLUSION
Many state-of-the-art large-scale image search systems follow the same paradigm: statistics computed from local invariant features are aggregated into an image-level vector
signature which is subsequently compressed and indexed for
computational and memory efﬁciency. The BOW histogram
has become a standard for the aggregation part. For the
compression part, most approaches use binary embeddings.
This article departs from this paradigm in two ways.
We ﬁrst propose to use the Fisher kernel framework for the
local feature aggregation. This representation is shown to yield
high performance and its accuracy remains competitive even
after a signiﬁcant dimensionality reduction to 128 dimensions,
i.e., of the same size as a single SIFT vector.
Secondly, we employ an asymmetric product quantization
scheme for the vector compression part, and jointly optimize
the dimensionality reduction and compression. Impressive
search results are achieved with a tiny binary code, e.g., a
mAP of 50.6% on Holidays with 128 bits signatures. With
such a small memory footprint, one billion images ﬁt in the
RAM of a 20GB server. We achieve a response time of 250 ms
on a 100 million image dataset on a single processor core.
ACKNOWLEDGEMENTS
We thank the company Exalead for providing the corpus of
100M images. This work was partially funded by the European
project PINVIEW, by the QUAERO project (supported by
OSEO, French State agency for innovation), by the European
integrated project AXES, and by the ANR project GAIA.