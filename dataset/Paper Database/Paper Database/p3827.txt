Adaptive Search with Stochastic Acceptance Probabilities for Global
Optimization
Archis Ghatea∗and Robert L. Smith b
aIndustrial Engineering, University of Washington, Box 352650, Seattle, Washington, 98195, USA,
 .
bIndustrial and Operations Engineering, University of Michigan, Ann Arbor, Michigan, 48109, USA,
 .
We present an extension of continuous domain Simulated Annealing. Our algorithm employs a globally reaching
candidate generator, adaptive stochastic acceptance probabilities, and converges in probability to the optimal
value. An application to simulation-optimization problems with asymptotically diminishing errors is presented.
Numerical results on a noisy protein-folding problem are included.
Keywords: Global Optimization, Simulated Annealing, Markov Chain Monte Carlo, Simulation-Optimization
1. Introduction
We present a Markov chain method to solve the
min f(x) s.t. x ∈S ⊂Rd,
where f is a continuous function and S is a compact set. In particular, let R(., .) be a Markov kernel called the candidate kernel on (S, B), where
B denotes the Borel sigma ﬁeld on S. We focus
on Algorithm 1.1.
Algorithm 1.1. Markov Chain Algorithm
1. Start with X0 ∈S, k = 0.
2. Generate
3. Let 0 ≤Pk ≤1 be a random variable and
with probability Pk,
with probability 1 −Pk.
4. Go to step 2.
The distribution of Pk may depend on the entire sequence of iterates X0, Y1, X1, . . . , Yk+1 thus
∗Corresponding author.
allowing for adaptive tuning of algorithmic parameters.
We provide conditions under which
the sequence {f(Xk)} converges in probability
to the global minimum function value f∗of f
over S (value convergence) in the second section
and present the proof in the third. This result
is applied to variants of (1) with noisy objective
functions in the fourth section.
Numerical experiments on a noisy protein folding problem are
presented in the ﬁfth.
Note that Algorithm 1.1 includes Simulated
Annealing (SA) and several of its variants as
special cases.
SA was originally proposed as a
stochastic search algorithm for discrete optimization using a Metropolis acceptance ﬁlter parameterized by the so-called ‘temperature’.
convergence properties for a logarithmic temperature schedule are well-known .
Recent generalizations of SA in the discrete domain include
 . SA has also been extended to continuous
problems .
Convergence analyses of
continuous SA similar to this paper include .
An elegant proof for value convergence of SA in
probability was presented in where a globally
reaching candidate kernel was used to allow for
adaptive temperature schedules. This proof technique was adapted in where locally reaching
kernels were allowed but a deterministic temperature schedule was employed.
Ghate and Smith
Convergence of discrete SA for noisy objective
functions has been analyzed frequently in the literature (see and references therein).
Recent work on continuous domain stochastic
search methods for noisy problems includes .
Continuity of the objective function was exploited
in to develop a novel estimation procedure.
However, the technique was analyzed only for the
special case of Pure Random Search (PRS) rendering it impractical. The convergence analysis in
 was based on the restrictive assumptions that
the objective function values have independent,
heteroskedastic Normal distributions, the candidate point distribution does not depend on the
current iterate, and tuning of algorithmic parameters is not adaptive.
Thus, to the best of our knowledge, Algorithm
1.1 is the ﬁrst rigorously analyzed continuous domain Markov Chain Monte Carlo framework that
simultaneously allows for generalized, stochastic
acceptance probabilities, adaptive tuning of algorithmic parameters, and as we will show, noisy
objective functions without restrictive assumptions on their distribution.
2. Suﬃcient Conditions for Convergence
We state ﬁve suﬃcient conditions for value convergence in probability. The ﬁrst three are from
2.1. The feasible region S is a
bounded and closed subset of Rd. f is continuous on S. Hence, there exists an x∗∈S where f
achieves its minimum f∗. Assume moreover that
for every ϵ > 0, the set {x ∈S : |x −x∗| < ϵ} has
positive Lebesgue measure.
Condition 2.2. For every open subset G in S,
the candidate Markov kernel R(x, G) is continuous in x.
Condition 2.3. The candidate Markov kernel R
is absolutely continuous in its second argument
and its density is uniformly bounded away from
0. That is, R is of the form
r(x, y)dy with
r(x, y) > 0.
The next two conditions are on the acceptance
probability ﬁlter Pk. For each β > 0, we deﬁne a
subset Uβ of S × S as Uβ = {(x, y) : x ∈S, y ∈
S, f(y) ≥f(x) + β}. Let Uk(β) be the event that
(Xk, Yk+1) ∈Uβ.
Condition 2.4. For any initial iterate X0, ϵ > 0,
β > 0, and δ > 0, there exists an integer N such
P[Pk > ϵ, Uk(β)] < δ ∀k ≥N.
We also deﬁne a subset V of S×S as V = {(x, y) :
x ∈S, y ∈S, f(y) ≤f(x)}. Vk is deﬁned as the
event that (Xk, Yk+1) ∈V .
Condition 2.5. For any initial iterate X0, ϵ > 0,
β > 0, and δ > 0, there exists an integer N such
P[1 −Pk > ϵ, Vk] < δ ∀k ≥N.
The ﬁrst two conditions are regularity conditions.
Condition 2.1 ensures in the presence of Condition 2.3 that neighborhoods of a global minimizer
are visited inﬁnitely often. Note that the second
part of Condition 2.1 is satisﬁed when S is ‘locally full dimensional’ at x∗, meaning that there
exists a δ > 0 such that the open ball of radius δ centered at x∗is contained in S. Equation (3) in Condition 2.3 states that the density
function of the Markov kernel is globally reaching,
which helps ensure value convergence under weak
conditions on the acceptance probability ﬁlter.
The Hit-and-Run candidate generator proposed
by one of us is perhaps the most eﬀective (see
 and references therein) globally reaching candidate generator Markov chain in the literature.
Hit-and-Run also satisﬁes Condition 2.2 .
The intuition behind the last two conditions
is as follows. Optimization algorithms based on
Markov chain candidate generators typically compute the diﬀerence between the function values at
the current iterate and the candidate point. Intuitively, the probability of accepting a candidate
point whose objective function value is ‘strictly
worse’ than the current iterate should asymptotically vanish. This is characterized by the deﬁnition of set Uβ and Condition 2.4. On the other
hand, if the function value at the candidate point
Adaptive Search with Stochastic Acceptance Probabilities
is at least as good as the current iterate, there is
probably no harm in accepting it. In fact, as we
will show later, it suﬃces to let the probability
of accepting points that are ‘no worse’ asymptotically approach 1. This is characterized by the
deﬁnition of set V and Condition 2.5. In short,
Condition 2.4 requires the acceptance probability ﬁlter to employ asymptotically diminishing
exploration, while Condition 2.5 ensures asymptotically increasing exploitation.
As an example, we illustrate that the acceptance probability in SA satisﬁes these two conditions. In that case, given (Xk = x, Yk+1 = y), Pk
is a deterministic function pk(x, y) given by
pk(x, y) =
exp[−f(y)−f(x)
if f(y) > f(x),
if f(y) ≤f(x),
where {tk} is a deterministic sequence of numbers
that converges to zero. Condition 2.5 is trivially
satisﬁed for all k. To see that Condition 2.4 is also
satisﬁed ﬁx β > 0 and ϵ > 0. Since tk converges to
0, there exists an integer M such that tk <
all k ≥M. For all such values of k, pk(x, y) < ϵ
whenever (x, y) ∈Uβ. Similarly, it is easy to con-
ﬁrm that Conditions 2.4 and 2.5 are met in essentially all variants of continuous SA, for example,
SA with an adaptive temperature schedule [?, ?],
Barker acceptance criterion with a deterministic temperature schedule, generalized acceptance
probability proposed by Tsallis with a deterministic temperature schedule, and Improving
Hit-and-Run . The main result in this paper
Theorem 2.6. For every initial iterate X0 ∈
S, the sequence of function values {f(Xk), k =
0, 1, 2, . . .} generated by Algorithm 1.1 converges
in probability to f∗whenever Conditions 2.1-2.5
are satisﬁed.
3. Proof of Convergence
The proof technique is similar to Belisle .
For any ϵ > 0, let the set Sϵ = {x ∈S : f(x) ≤
f∗+ϵ}. For positive integers l, m, n we deﬁne the
following four events.
A = A(m, n) = the event that none of the states
Xn, Xn+1, ...Xn+m is in Sϵ.
B = B(l, m, n) = the event that at least one of
the transitions Xn+(k−1) →Xn+k, k = 1, 2...m
is a move from Sϵ to {x ∈S : f∗+ ϵ < f(x) <
f∗+ ϵ + 1/l}.
C = C(l, m, n) = the event that at least one of
the transitions Xn+(k−1) →Xn+k, k = 1, 2...m is
a move from Sϵ to {x ∈S : f(x) ≥f∗+ ϵ + 1/l}.
D = the event that Xn+m /∈Sϵ.
To prove theorem 2.6, we need to prove that for
every x0 ∈S, ϵ > 0, and δ > 0, there exists an
integer N such that
P[Xn /∈Sϵ|(X0 = x0)] < δ ∀n ≥N.
We begin by ﬁxing x0 ∈S, ϵ > 0 and δ > 0. For
simplicity of notation, henceforth, we may not
write the conditioning on (X0 = x0). It is to be
understood implicitly. Since D ⊂A S B S C,
P[Xn+m /∈Sϵ] = P[D] ≤P[A]+P[B]+P[C].
Also, the symbol φ will be used to denote the
Lebesgue measure.
The proof requires several
Lemmas which are now presented one by one.
Note that there exists an integer m0 such that
(1 −γφ(Sϵ))m0 < δ/6. Fix this m0. For integers
n, m and r, we deﬁne Gr ≡G(n, m, r) as the
event that the candidate point Yn+r is in Sϵ, for
Lemma 3.1. There exists an integer n0 such that
for any integer r, 1 ≤r ≤m0, P[A(m0, n), Gr] ≤
6m0 ∀n ≥n0.
Proof. Fix an integer r such that 1 ≤r ≤m0.
Then P[A(m0, n), Gr] is bounded above by
P[Xn+r−1 /∈Sϵ, Xn+r /∈Sϵ, Yn+r ∈Sϵ]
= P[Xn+r−1 /∈Sϵ, Yn+r ∈Sϵ, Yn+r not accepted].
Let Hn+r be the event that candidate Yn+r is
not accepted by the acceptance probability ﬁlter.
Moreover, let In+r(δ) be the event that
1 −Pn+r−1 >
The above probability is
then bounded above by
P[Xn+r−1 /∈Sϵ, Yn+r ∈Sϵ, Hn+r, In+r(δ)]
+P[Xn+r−1 /∈Sϵ, Yn+r ∈Sϵ, Hn+r, Ic
≤P[Vn+r−1, In+r(δ)] +
∀n ≥nr for some nr,
Ghate and Smith
where the last step follows from Condition
Thus, there exists an integer n0 such
that for any integer r between 1 and m0,
P[A(m0, n), G(n, m0, r)] ≤
6m0 for all n ≥n0.
This completes the proof.
Lemma 3.2. Let m0 and n0 be as above. Then
P[A(m0, n)] < δ/3 for all integers n ≥n0.
Proof. We deﬁne F ≡F(n, m) as the event that
none of the candidate points Yn+1, ...Yn+m is in
We have A ⊂(A T F) S(Sm
r=1(A T Gr)).
Therefore, P[A(m, n)] is bounded above by
P[A(m, n), F] +
P[A(m, n), G(m, n, r)]
≤P[Xn /∈Sϵ, F] +
P[A(m, n), G(m, n, r)]
≤(1 −γφ(Sϵ))m +
where the last inequality follows since the probability that the candidate point will be in Sϵ is
at least γφ(Sϵ) regardless of the past at each
iteration.
Then, from the deﬁnition of m0,
P[A(m0, n)] is bounded above by
P[A(m0, n), G(m0, n, r)] < δ/3 (5)
for all n ≥n0 by Lemma 3.1.
Lemma 3.3. Let m0 be as in Lemma 3.2. There
exists an integer l0 such that
P[B(l0, m0, n)] < δ/3 ∀n ≥0.
Proof. The proof of this lemma depends only on
Conditions 2.2 and 2.3 on the Markov kernel and
compactness of the feasible region S. In particular, it does not depend on the form of the acceptance probability. Since our conditions on the
Markov kernel are the same as those of Belisle ,
and our feasible region S is compact, his proof
carries over to our case.
Lemma 3.4. Let m0 and l0 be as in Lemmas 3.2
and 3.3. There exists an integer n1 such that
P[C(l0, m0, n)] < δ/3 ∀n ≥n1.
Proof. For k = 1, 2, . . . , m0, let Ck(l0, m0, n)
be the event that the transition Xn+k−1 to
f∗+ ϵ + 1/l0}.
C(l0, m0, n) = Sm0
k=1 Ck(l0, m0, n).
Therefore,
P[C(l0, m0, n)] ≤Pm0
k=1 P[Ck(l0, m0, n)].
probability P[Ck(l0, m0, n)] is equal to
P[Ck(l0, m0, n), Pn+k−1 >
+P[Ck(l0, m0, n), Pn+k−1 ≤
≤P[Un+k−1, Pn+k−1 >
for some integer n1 by Condition 2.4.
Let n2 = max(n0, n1). Lemmas 3.2, 3.3, and 3.4
imply that equation (4) holds with N = n2 + m0.
The proof of Theorem 2.6 is now complete.
4. Application to Stochastic Optimization
In this section, we apply Theorem 2.6 to the important special case of global optimization problems with noisy objective functions. Consider a
speciﬁc version of problem (1) given by
i.e., f(x) in (1) is now the well-deﬁned expected
value of a stochastic performance measure G(x).
We assume that analytical computation of the
expected value is hard, forcing us to estimate
f(x) subject to random errors. We use ˜f to denote function estimates. In particular, if the acceptance probability ﬁlter in Algorithm 1.1 used
function values for solving problem (1), it must
now use estimates ˜f instead while solving the
noisy problem (6). We use ϵk+1(x) to denote the
estimation error at point x ∈S in iteration k +1,
i.e., ϵk+1(x) = ˜fk+1(x) −f(x).
Condition 4.1. |ϵk+1(Xk)| →0 in probability
and |ϵk+1(Yk+1)| →0 in probability, i.e., for any
ϵ > 0 and δ > 0, there exists an integer N such
that P[|ϵk+1(Xk)| > ϵ] < δ for all k ≥N, and
similarly for Yk+1.
Adaptive Search with Stochastic Acceptance Probabilities
Note that when the estimates ˜f are unbiased,
it suﬃces for the sequence of error variances
k+1(Xk) and σ2
k+1(Yk+1) to converge to zero as
k →∞in order for Condition 4.1 to be satisﬁed
owing to Chebyshev’s inequality.
Corollary 4.2. Suppose Conditions 2.1-2.5 and
Then for every initial state X0, the
sequence of objective function value estimates
( ˜fk(Xk), k = 0, 1, 2...) converges in probability to
Proof. Based on Theorem 2.6 and the fact that
˜fk(Xk) →f(Xk) in probability.
Corollary 4.2 can be used to prove convergence
of several simulation-optimization algorithms. As
an example, we consider SA with an adaptive
temperature schedule Tk for problems with noisy
function evaluations.
Intuitively, the following
condition means that the errors converge to zero
faster than the temperature.
{ |ϵk+1(Xk)|
} converges to 0 in probability. Similarly, sequence of ratios { |ϵk+1(Yk+1)|
} converges
to 0 in probability.
The next condition is typical when the temperature schedule is adaptive .
Condition 4.4. The sequence of temperatures
{Tk} converges to 0 in probability.
Corollary 4.5. Suppose Conditions 2.1-2.3 hold
for problem (6) to which we apply SA such
that Conditions 4.3-4.4 are satisﬁed.
sequence of objective function value estimates
( ˜fk(Xk), k = 0, 1, 2...) converges in probability to
Proof. It suﬃces to show that Conditions 2.4, 2.5
and 4.1 hold. Conditions 4.3 and 4.4 clearly imply Condition 4.1.
For brevity, let ∆k denote
f(Yk+1) −f(Xk) and Ξk denote ϵk+1(Yk+1) −
ϵk+1(Xk). Then note that for SA with noisy objective function evaluations, the acceptance probability Pk is given by
exp{−[∆k + Ξk]+
Proof for Condition 2.4.
Let δ > 0 and ϵ > 0 be given.
First notice
that for any three real numbers a, b, and c ≥a,
[a + b]+ ≤[c + b]+.
Hence, exp(−[a + b]+) ≥
exp(−[c + b]+). We will use this with a =
Tk , and c =
Observe that the event
Uk(β) implies that ∆k ≥β.
Hence, it implies
that Pk ≤exp( −[β+Ξk]+
). Thus, Pk > ϵ implies
exp( −[β+Ξk]+
) > ϵ. Therefore, we can write
P[Pk > ϵ, Uk(β)] ≤P[exp(−[β + Ξk]+
ϵ , Ek(β/2)] + P[Ec
where Ek(β/2) is the event that |Ξk| < β/2. The
second term in the inequality can be made smaller
than δ/2 for all k ≥n0 due to Condition 4.1.
Therefore, the above inequality reduces to
P[Pk > ϵ, Uk(β)] < P[ β
The ﬁrst term can be made smaller than δ/2
for all k ≥n1 due to Condition 4.4.
Therefore, we have P[Pk
> ϵ, Uk(β)] < δ for all
k ≥max{n0, n1}.
Proof for Condition 2.5.
Let δ > 0 and ϵ > 0 be given.
First notice that for any two real numbers a and b,
[a+b]+ ≤[a]++[b]+. Therefore, exp(−[a+b]+) ≥
exp(−([a]+ + [b]+)). Using this with a = ∆k
Tk , we get Pk ≥exp( −[∆k]+
) exp( −[Ξk]+
Thus, the event 1 −Pk > ϵ implies the event
1 −exp( −[∆k]+
) exp( −[Ξk]+
) > ϵ. Observe that
Vk implies [∆k]+ = 0. Therefore, the probability
P[1 −Pk > ϵ, Vk] is bounded above by
P[1 −exp(−[Ξk]+
) > ϵ] ≤P[log
1 −ϵ < [|Ξk|]
< δ ∀k ≥n0 by Condition 4.3.
This completes the proof.
As an example, suppose Tk is a deterministic sequence of temperatures that converges to 0 as
k →∞and estimates ˜f are unbiased.
Ghate and Smith
Chebyshev’s inequality implies that Condition 4.3
is satisﬁed when the ratios σ2
k+1(Xk)/T 2
k+1(Yk+1)/T 2
k converge to zero as k →∞.
5. Numerical Experiments: Protein Folding with Noise
A challenging test problem commonly used for
testing global optimization software is the following energy minimization problem from the protein folding literature . Let x1, x2, . . . , xn
be the 3-dimensional coordinates of n molecules.
Our goal is to place these n molecules in the box
[−1, 1]3 in R3 such that the potential energy of
interaction ψ(·) between these molecules is minimized. This can be formulated as follows.
x1,x2,...,xn∈R3ψ(x1, . . . , xn) :=
r(||xi −xj||2)
xi ∈[−1, 1]3, i = 1, 2, . . . , n,
where ||.||2 denotes the Euclidean distance, and
the function r(s) := s−12 −2s−6 is the Lennard-
Jones potential between two molecules. The number of local minima in the above objective function is exponential in n.
additional
complication
whence the potential energy of interaction among
n molecules at x = (x1, x2, . . . , xn) is computed
by a ‘stochastic black-box’, which receives x as
input and produces a random number as output with distribution Θ(x) whose expectation is
ψ(x) and variance is σ2(x).
Since our goal is
to minimize the expected value function ψ(·),
we use estimates ˜ψk(x) of the expected value
ψ(x) in iteration k of Algorithm 1.1.
For simplicity, we generate these estimates by repeatedly querying the black-box.
In particular, let
Θ1(x), Θ2(x), . . . , ΘNk(x) be Nk independent and
identically distributed outputs produced by the
stochastic black-box on receiving x as input and
˜ψk(x) be their sample average. Note that ˜ψk(x)
is unbiased and converges almost surely to ψ(x)
as k →∞by the Strong Law of Large Numbers.
Moreover, the variance of the estimation error is
We employed the Hit-and-Run candidate generator from and SA with deterministic temperature schedules so that the ratio of variance to
temperature square converges to zero with iterations. Numerical results for the 3 molecule case,
which is a 9-dimensional problem with optimum
objective function value −3 , are presented
in Table 1. The ﬁrst column lists an abbreviation
for the distribution Θ(x) whose expectation is our
objective function. For each Θ(x), the temperature schedule Tk and sample size Nk were parameterized by real numbers α and β as Tk = k−α
and Nk = kβ. It suﬃces to choose any α > 0
to satisfy Condition 4.4, and any β > 2α so that
the variance of our objective function estimate decreases faster than the square of the temperature.
We used 0.5 < α < 1 so that the corresponding
β value is between 1 and 2 ensuring a moderate
increase in sample size with iterations. For every
combination of Θ(x), Tk, Nk, we performed 30
independent runs of 1000 iterations each with an
independently generated random starting point in
[−1, 1]9. The fourth column lists the mean of the
estimated objective function value reached at termination with standard deviation in the ﬁfth column. Figure 1 shows progress of the mean of the
estimated objective function value for two such
combinations. The quality of these results is evident since the Adaptive SA algorithm in is
not able to decrease the objective function value
below -2.5 in 12000 iterations on the noise free
version of our example.
Normal(ψ(x), 0.1),
Normal(ψ(x), 1),
Uniform(ψ(x) −1, ψ(x) + 1),
Uniform(ψ(x) −0.1, ψ(x) + 0.1).
Adaptive Search with Stochastic Acceptance Probabilities
mean objective value
mean objective value Vs iteration for N1(0.8,1.65)
mean objective value
mean objective value Vs iteration for U2(0.85,1.8)
Figure 1. Evolution of the mean of the estimated objective function value with iterations for two example
rows in Table 1. The ﬁrst fourteen iterations are not shown as their objective function values were too
big to ﬁt in the plot. Notation such as N1(0.8, 1.65) in the title of a plot indicates that the distribution
Θ(x) is N1 given in Table 1, α = 0.8 and β = 1.65 respectively.
Ghate and Smith
Acknowledgements
Research supported in part by the NSF under
grant DMI-0244291. We thank the referees and
the Area Editor for their suggestions.