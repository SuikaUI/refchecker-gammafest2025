An Introductory Survey on Attention Mechanisms in NLP Problems
College of Computing, Georgia Institute of Technology
801 Atlantic Dr NW
Atlanta, Georgia 30332
First derived from human intuition, later adapted to machine
translation for automatic token alignment, attention mechanism, a simple method that can be used for encoding sequence data based on the importance score each element is
assigned, has been widely applied to and attained signiﬁcant
improvement in various tasks in natural language processing, including sentiment classiﬁcation, text summarization,
question answering, dependency parsing, etc. In this paper,
we survey through recent works and conduct an introductory
summary of the attention mechanism in different NLP problems, aiming to provide our readers with basic knowledge
on this widely used method, discuss its different variants for
different tasks, explore its association with other techniques
in machine learning, and examine methods for evaluating its
performance.
Introduction
We introduce our main topic via a concrete example of neural machine translation. Traditional methods are formulated by an
encoder-decoder architecture, both of which are recurrent
neural networks. An input sequence of source tokens is ﬁrst
fed into the encoder, of which the last hidden representation is extracted and used to initialize the hidden state of
the decoder, and then target tokens are generated one after
another. Despite achieving higher performance compared to
purely statistical methods, the RNN-based architecture suffers from two serious drawbacks. First, RNN is forgetful,
meaning that old information gets washed out after being
propagated over multiple time steps. Second, there is no explicit word alignment during decoding and therefore focus
is scattered across the entire sequence. Aiming to resolve
the issues above, attention mechanism was ﬁrst introduced
into neural machine translation . They maintain the same RNN encoder, for each
step j during decoding they compute an attention score αji
for hidden representation hin
of each input token to obtain
a context vector cj (see Figure 1):
eji = a(hin
Here cj, a weighted average of elements in the input sequence, is the encoded sentence representation with respect
to the current element hout
, and a(hin
) is the alignment function that measures similarity between two tokens
and will be discussed in detail later. Then cj is combined
with the current hidden state hj and the last target token
yj−1 to generate the current token yj:
yj = fy(hout
, yj−1, cj)
j+1 = fh(hout
Here fy and fh stands for the output layer and hidden layer
in recurrent networks. This procedure is repeated for each
token yj until the end of the output sequence. By introducing this additional encoding step, problems mentioned earlier can be tackled: the bad memory of RNN is no longer
an issue, since the computation of attention score is performed on each element in the input sequence and therefore
computation of the encoded representation cj is unaffected
by the sequence length; on the other hand, soft alignment
across the input sequence can be achieved since each element is either highlighted or down-weighted based on its
attention score and focus is paid only to the important parts
in the sequence, discarding useless or irrelevant parts. As a
result, the attention-based translation model achieved great
success in machine translation and then attention mechanism
got widely applied to other NLP tasks. Moreover, different
variants of the basic form of attention have been proposed to
handle more complex tasks. As an overview of the following
sections, we will:
1. Explain the basic form of attention in detail. (Formulation)
2. Discuss different variants based on the special task they
are dealing with. (Variation)
3. Explore how attention is associated with other concepts or
techniques in machine learning, such as pre-training and
ensemble. (Application)
 
Figure 1: A comparison between the traditional encoderdecoder architecture (left) and the attention-based architecture (right). During decoding, an additional attention score
αji is computed for each source token hin
with respect to
the target token hout
, and the scores are then used to obtain
the contextual encoding cj.
4. Examine methods for evaluating the performance of attention. (Evaluation)
Formulation
Previously, we discussed the application of attention in neural machine translation. To formally generalize it into the
basic form of attention, we deﬁne V = {vi} ∈Rn×dv as a
sequence of vector elements and rewrite the previous steps
ei = a(u, vi)
(compute attention scores)
(normalize)
In the ﬁrst step, u ∈Rdu is a task-speciﬁc pattern vector
to match with each element in the sequence {vi} based on
the alignment function a(u, v) that outputs a scalar score
ei ∈R to indicate quality of match. In most cases we assume du = dv = d. Common choices are given by:
Multiplicative:
2 tanh(W1[u; v])
2 tanh(W1[u; v] + b1) + b2)
all of which measures pairwise similarity in different representations. The ﬁnal score αi ∈R is the normalized weight
for each element vi and then used to encode the entire sequence into a context vector c ∈Rdu, which is later incorporated into a downstream task as an additional contextual
feature. Intuitively, vi that closely matches the pattern u receives a large weight and therefore dominates the ﬁnal encoding c. In machine translation, the attention score can be
naturally interpreted as the alignment measure of a target token to each source token. Ideally, when generating a target
token yj, we expect the gold alignment source token(s) to
receive high attention score and therefore the encoded representation cj can provide closely relevant information for
To avoid confusion, we explain our use of notation and abbreviation before our further discussion. We use lower-case
symbols for scalars, lower-case boldface symbols for vectors, upper-case symbols for matrices, and upper-case boldface symbols for tensors. When we refer to attention scores,
e stands for original scores and α stands for normalized
scores. W, b are by default weights and biases to be learned
during training, and vi is by default a general element that
can refer to either one of the following depend on the scenario: an embedded token within a sentence, a hidden representation of the token, an embedded sentence within a document, etc. For simplicity, we occasionally use softmax to
represent the normalization step that involves exponentiation, summation and division.
Previously, we discussed basic form of the attention mechanism. Because of its simplicity and interpretability, it
is widely used in various NLP tasks. Nevertheless, such
attention mechanism is in general not powerful enough
for more complicated tasks. Here is a simple example
 :
Sam walks into the kitchen. (1)
Sam picks up an apple. (2)
Sam walks into the bedroom. (3)
Sam drops the apple. (4)
Q: Where is the apple?
A: Bedroom
The underlying difﬁculty is that there is no direct relationship between Q and (3), and therefore we need to
design a more sophisticated mechanism to guide attention
to the correct location using latent clues within the context
(temporal reasoning in this case). As different variants of
attention have been proposed in recent years, we summarize
them into several most representative categories (see Table
1): basic attention, multi-dimensional attention, hierarchical
attention, self-attention, memory-based attention and taskspeciﬁc attention. From left to right, the corresponding task
increases in complexity, and the mechanism becomes more
task speciﬁc. We will discuss each category in detail in the
following sections.
Multi-dimensional Attention
The basic form of attention mechanism computes a scalar
score αi for each term in a sequence V = {vi}. This could
be named as 1D attention or vector attention because the
Type of Attention
Purpose in brief
Basic Attention
Extracting important elements from a sequence
Multi-dimensional Attention
Capturing multiple types of interaction between terms
Hierarchical Attention
Extracting globally and locally important information
Self-Attention
Capturing deep contextual information within a sentence
Memory-based Attention
Discovering latent dependencies in sophisticated NLP tasks
Task-speciﬁc Attention
Capturing important information speciﬁed by the task
Table 1: An overview of each type of attention and its corresponding purpose
1D Attention
2D Attention
uT Wv (W ∈Rd×d)
uT W v (W ∈Rk×d×d)
2 tanh(W1[u; v]) (w2 ∈Re)
2 tanh(W1[u; v]) (W2 ∈Re×k)
2 tanh(W1[u; v] + b1) + b2)
2 tanh(W1[u; v] + b1) + b2)
Table 2: Some examples of extending from 1D attention to
2D, assuming u, v ∈Rd, W1 ∈Re×2d
concatenated output scores α = {αi} is a vector α ∈Rn.
The motivation for multi-dimensional attention is simply to
capture multiple interaction between terms in different representation space, which can be easily constructed by directly stacking together multiple single dimensional representations.
As an example of 2D attention in aspect and opinion terms
extraction , given a sequence of hidden
representation of tokens V = {vi}, an aspect prototype vector u, and a 3D tensor W = {Wk} ∈RK×d×d where each
slice Wk ∈Rd×d is a matrix that captures one type of composition between each token and the prototype, the vector
attention score ei (not normalized) for each element vi is
ei = tanh(uT W vi) = concat(tanh(uT Wkvi))
Then the 2D attention representation E for the entire sequence is obtained by concatenation: E = {ei}. As a concrete example, consider the sentence ”Fish burger is the best
dish”, here Fish burger is an aspect term and therefore will
receive high attention score with respect to the aspect prototype u, and the learned W should be able to highlight
both Fish and burger after they are projected to different
spaces. A drawback of this multi-dimension approach is that
a strongly indicative element can capture multiple types of
attention at the same time and therefore reduces its representation power. To compensate for this, we can impose a
regularization penalty in Frobenius norm on the 2D attention matrix A :
||AAT −I||2
to constrain each attention column to focus on different parts
in the sequence.
Hierarchical Attention
discussion
hierarchical
attention by looking at an example in document classiﬁcation. Given a short document :
How do I get rid of all the old web searches I have on my
web browser? I want to clean up my web browser go to tools
→options Then click ”delete history” and ”clean up temporary internet ﬁles.”, the task is to classify the document into
one of several categories, which in this case is Computer
and Internet. Intuitively, we can identify words that provide
clues for classiﬁcation, since the term web browser can appear frequently in computer-related documents. Moreover,
typical sentences can be potentially informative for classiﬁcation as well, as non-professionals using a computer software for the ﬁrst time tend to seek instruction on how to
get rid of ... If we think of the nested structure of textual
data: character ∈word ∈sentence ∈document, a hierarchical attention can be constructed accordingly, either
bottom-up (i.e, word-level to sentence-level) or top-down
(word-level to character-level) to identify clues or extract
important information both globally and locally.
Bottom-up construction has been used in document classiﬁcation . Two BiGRUs are applied to generate a word-level and a sentence-level contextual representation, respectively. Then a pair of hierarchical attention layers
are applied to obtain a word-level and sentence-level encoding:
= BiGRU(v(t)
softmax(uT
i ) · h(t)
hi = BiGRU(vi)
softmax(uT
s hi) · hi
softmax is equivalent to the normlization step we previously discussed. h(t)
and hi stand for hidden representation for words and sentences. uT
s are word-level and
sentence-level pattern vectors to be learned during training.
The ﬁnal sentence-level representation c is then fed into a
logistic regression layer to predict the category.
Another type of hierarchical attention takes a top-down approach, an example of which is for grammatical error correction . Consider a corrupted sentence: I have
no enough previleges. The idea is to ﬁrst bulid an encoderdecoder architecture similar to the one for machine translation, then apply a word-level attention for global grammar and ﬂuency error correction (I have no enough →I
don’t have enough), and optionally a character-level attention for local spelling error correction (previleges →privileges). Top-down techniques are also used in album summarization , where a photo-level
attention is used to select appropriate photos from an album and a word-level attention is integrated with a sequence
model for text generation.
Self Attention
Let us revisit the steps for constructing the basic form of
attention. Given a sequence of elements V = {vi} and a
pattern vector u, for each element vi we can compute the
attention score αi = a(u, vi). This can also be termed as
external attention, since attention is computed by matching an external pattern u with each element vi, and each
score ei indicates quality of match. On the contrary, in selfattention, the external pattern u is replaced by parts of the
sequence itself, and therefore is also termed as internal attention. To illustrate this with an example: Volleyball match is
in progress between ladies, here match is the sentence head
on which all other tokens depend, and ideally we want to
use self-attention to capture such intrinsic dependency automatically. Alternatively, we can interpret self-attention as
matching with each element vi an internal pattern v′ within
ei = a(v′, vi)
A typical choice for v′ is simply another element vj, so as to
compute a pairwise attention score, but in order to fully capture complex interaction between terms within a sequence,
we can further extend this to compute attention between every pair of terms within a sequence, i.e., to set v′ as each
element vj in a sequence and compute a score for each pair
of terms. Therefore we modify the previous equations to:
eij = a(vi, vj)
αij = softmax(eij)
aiming to capture complex interaction and dependency between terms within a sequence. Then the choice of the alignment function a is literally the same as the basic attention,
such as a single layer neural network:
αij = softmax(tanh(wT [vi; vj] + b))
In this way, each token maintains a distributed relation representation with respect to all other tokens and the complex
pairwise relationship can be easily interpreted from its assigned scores. And the model can further be enriched with
multi-dimensional attention as we mentioned earlier.
Another motivation for self-attention is related to the word
embedding. To be speciﬁc, we want to utilize self-attention
models to learn complex contextual token representation in
a self-adaptive manner. We can illustrate this point by an example of word sense disambiguation:
I arrived at the bank after crossing the street.
I arrived at the bank after crossing the river.
The word bank has different meanings under different contexts, and we want our model to learn contextual token embeddings that can capture semantic information from their
surrounding contexts. Transformer 
is an exemplar novel attention-based architecture for machine translation. It is a hybrid neural network with sequential blocks of feed forward layers and self-attention layers.
Similar to the previous self-attention mode, the novel selfattentive encoding can be expressed as:
A = softmax[(V W1)(V W2)T
C = AT (V W3)
Here V = {vi} ∈Rn×din represents an input sequence
and W1, W2, W3 ∈Rdin×dout are matrices to be learned
for transforming V to its query, key and value representation. C = {ci} ∈Rn×dout therefore forms a sequence
of self-attentive token encoding. We can expect each input
token to learn a deep context-aware conﬁguration via adjusting its relation with its surroundings during end-to-end
training. We should also be aware that the architecture excludes all recurrent and convolution layers, as computation
within a self-attention layer is parallel (therefore outweighs
RNN) and parameter-efﬁcient (compared with CNN). Various techniques have been proposed to further enhance its
representation power. Positional encoding is introduced to provide the model with additional positional information of each token, an example of which can
be constructed as follows:
PE(pos, 2i) = sin(pos/100002i/d)
PE(pos, 2i + 1) = cos(pos/100002i/d)
and later incorporated into the sentence encoding as additional features. To avoid receving attention from undesired
direction, a directional mask M , a triangular matrix with -∞entries on the disabled position and 0s
otherwise, is added to the score representation before normalizing:
αij = softmax(eij + Mij)
for backward disabling. This can be useful while training a
left-to-right language model since the future context should
not provide any clue for generating the next token. Other
techniques include relative position encoding that aims for incorporating pairwise
distance information into the contextual token representation ci as following:
αij(vj + wij)
where each weight wij corresponds to a directed edge from
vertex vi to vj, if V is considered as a fully connected graph.
These weights are initialized and learned during training.
Besides the pair-wise score computation, Lin et al. proposed
an alternative method to obtain self-attention scores based
on a fully connected neural network:
A = softmax(W2tanh(W1(V T ))
where each row in A represents a single type of attention.
Then the entire sequence encoding C can be obtained by:
In this case, the attention that an element receives is determined by its relevance to all elements in the entire sequence
via full connection. Such a technique can be used for obtaining a ﬁx-length encoding from a variable-length sequence
since the dimension of C is independent of the input sequence length.
Memory-based Attention
To introduce a new type of attention, we ﬁrst reconstruct the
old attention in an alternative way. Given a list of key value
pairs {(ki, vi)} stored in memory and a query vector q, we
redeﬁne the three steps as:
1. ei = a(q, ki) (address memory)
i exp(ei) (normalize)
i αivi (read contents)
Here we re-interpret computing attention score as soft memory addressing using query q, and encoding as reading contents from memory based on attention scores {αi}, which
constitutes the very basic form of memory-based attention.
In fact, in quite a few literatures, ”memory” is simply a synonym for the input sequence. Also note that if every pair
of ki and vi are equal, this reduces to the basic attention
mechanism. However, the alternative (memory-based) attention mechanism can become much more powerful as we incorporate additional functionalities to enable reusability and
increase ﬂexibility, both of which we will later discuss in
Reusability
A fundamental difﬁculty in some question answering tasks is that the answer is indirectly related to the
question and therefore can not be easily solved via basic
attention techniques (demonstrated at the beginning of this
section). However, this can be achieved if we can simulate
a temporal reasoning procedure by making iterative memory updates (also called multi-hop) to navigate attention to
the correct location of the answer step-by-step , an outline of which is illustrated in Figure 2.
Intuitively, in each iteration, the query is updated with new
contents, and updated query is used for retrieving relevant
contents. A pseudo run on an early example is given in Figure 3, where the query is initialized as the original question
and is later updated by simply summing up the current query
and content :
q(t+1) = q(t) + c(t)
More sophisticated update methods include constructing a
recurrent network across query and content of multiple time
steps , or inducing the output based on
both content and location information . Results show that when complex temporal
reasoning tasks are given (similar to Figure 3), the memorybased attention model can successfully locate the answer after several hops.
Flexibility
Since keys and values are distinctly represented, we have freedom to incorporate prior knowledge
in designing separate key and value embeddings to allow
them to better capture relevant information respectively. To
be speciﬁc, key embeddings can be manually designed to
match the question and value embeddings to match the response. In key-value memory network , a
window level representation is proposed such that keys are
constructed as windows centered around entity tokens and
values are those corresponding entities, aiming for more efﬁcient and accurate matching. Then for the example in Figure
3, entities such as apple and bedroom are value embeddings,
and their surrounding tokens are key embeddings.
More sophisticated architectures include the Dynamic Memory Network where the overall architecture is ﬁne-split into four parts: question module, input module, episodic memory module and answer module,
1. initialize q = question
2. ei = a(q, φk(ki)) (address the memory)
i exp(ei) (normalize)
i αiφv(vi) (retrieve contents)
5. q = update query(q, c) (update query)
6. goto 2 (multi-hop)
Figure 2: An enhanced version of memory-based attention
with multi-hop and alternative key value embeddings
Figure 3: A pseudo illustration of memory-based attention
updates on an early question answering example.
each of which is a complex neural micro-architecture itself. Such modularized design enables piecewise domain
knowledge injection, efﬁcient communication among modules, and generalization to a wider range of tasks beyond
traditional question answering. A similar architecture is proposed to handle both textual and visual question answering
tasks , where visual inputs are fed into a deep convolutional network and high-level
features are extracted and processed into an input sequence
for the attention network. If we further extend memory and
query representation to ﬁelds beyond question answering,
memory-based attention techniques are also used in aspect
and opinion term mining where query is
represented as aspect prototypes, in recommender systems
 where users become the memory component and items become queries, in topic modelings where latent topic representation extracted from
a deep network constitutes the memory, etc.
Task-speciﬁc Attention
In this section we include alternative usage of attention that
are intricately designed for a speciﬁc task. Although not as
generalizable as methods introduced earlier, they are wellmotivated and ﬁt properly into their own task, therefore
worth mentioning. Tan et al. proposed an alternative for
computing attention scores for the task of abstractive document summarization . The formulation of new graph-based attention is similar to PageRank
algorithm . Given a document V = {vi}
where each vi represents a sentence, the stationary attention
distribution α = {αi} satisﬁes:
α = λWD−1α + (1 −λ)y
Here W is a square matrix in which each entry Wij encodes a multiplicative composition between vi and vj, and
D is a diagonal normalizing matrix to ensure each column
of WD−1 sums up to 1. λ and y are the damping factor and
an auxiliary uniform distribution, respectively. Then α can
be solved analytically as:
α = (1 −λ)(I −λWD−1)−1y
The underlying motivation is that a sentence is important in
a document if it is heavily linked with many important sentences.
Kim et al. proposed a structured attention network which
integrates attention mechanism with probabilistic graphical
model by introducing a sequence of discrete latent variables
Z = {zi} as soft selectors into the input sequence . An attention distribution p(Z = z|V, q) is generated
from a conditional random ﬁeld and then used to encode the
context vector as an expectation over this probability distribution:
c = Ez∼p(z|V,q)[f(V, z)]
where f(V, z) is the annotated function that models relationship between latent variables and the given inputs. If Z
is a single random variable and given f(V, Z = z) = Vz
(i.e., selecting the zth element from V ), then this is equivalent to soft selection as in the basic attention. Without this
restriction, they demonstrate its adaptability to multiple input selection tasks such as syntactic tree selection and subsequence selection under a general case.
In machine translation, a local attention model is proposed to handle long sequence translation where computation of global attention
(i.e. attending to every element) is expensive. During decoding, a pivot position pt ∈[0, length(V )] , which speciﬁes
the center of attention, is ﬁrst predicted by a small neural
network, and then Gaussian Smoothing is applied around the
center to produce soft alignment.
Application
In the previous section, we have showed that attention along
with its variants have been widely applied to various NLP
tasks. Here we will further explore the connection of attention to other abstract concepts in machine learning. As we
have discussed previously, attention can be used for encoding a sequence by extracting important terms based on its
match with a given pattern; attention can also be used for iterative memory addressing and reading given a query. Here
we present three more applications of attention: emsemble,
gating, and pre-training.
Attention for Ensemble
If we interpret each element vi in a sequence as an individual model, and normalized scores αi as their weighted
votes, applying the attention mechanism can then be analogous to model ensemble. This is explored in Kieta et al.
where they ensemble a set of word embeddings to construct
a meta-embedding with more representative power and ﬂexibility . Speciﬁcally, attention
score α[j]
for the embedding v[j]
(the ith embedding for the
jth word) is given via self-attention:
= softmax receives larger weights than
FastText embeddings for concrete
Attention for Gating
Another application of attention is to integrate this mechanisms with memory updates in recurrent network. In traditional GRU , hidden state updates are
˜hi = tanh(Wvi + ri ◦(Uhi−1) + b(h))
hi = ui ◦˜
hi + (1 −ui) ◦hi−1
where ui and ri are update and reset gates learned during training. While in an alternative attention-based GRU
 , ui is replaced by a scalar
attention score αi received by the ith elelment when updating its hidden state. Then the last update step can be replaced
hi = αi ◦˜hi + (1 −αi) ◦hi−1
The attention scores are computed in an external module.
Such an attention-based gating allows context-aware updates based on global knowledge of previous memory, and
easier interpretability of importance of each element.
Similarly in text comprehension, memory-based attention
gate ˜qi is constructed based on the
interaction between the query Q and each token vi in the
document and iteratively update each token embeddings:
αi = softmax(QT vi)
˜vi = vi ◦˜qi
vi, Q = GRUv( ˜vi), GRUQ(Q)
aiming to build up deep query-speciﬁc token representation.
Attention for Pre-training
Pre-trained word embeddings are crucial to many NLP tasks.
Traditional methods such as Skipgram, Cbow, and Glove
 take use of large text corpora
to train an upsupervised prediction model based on contexts
and learn a high dimensional distributed representation of
each token. On the contrary, recently proposed pre-training
methods integrate attention-based techniques with deep neural architectures, aiming to learn higher quality token representation that incorporates syntactic and semantic information from the surrounding contexts. and then the model is
ﬁne-tuned to adapt to a downstream supervised task. BERT
 is a bi-directional pre-training model
backboned by the Transformer Encoder , a deep hybrid neural network with feed forward layers and self-attention layers which we have brieﬂy discussed
in section 3.3. During pre-training, one task is to learn a bidirectional masked language model, meaning that a small
percent of tokens in a sentence are masked and the goal is to
predict these tokens based on their context. The other task is
binary next sentence prediction, where two spans of texts are
sampled from the corpora and the model is trained to predict whether they are contiguous. As discussed in Section
3.3, each token redistributes attention across the sequence
and reconstruct its interaction with other tokens in a selfadaptive manner as the training proceeds, aiming to learn
its contextual representation based on the entire sequence.
When the pre-trained model is integrated with a supervised
task, an additional layer is added on top of the model and
ﬁne-tuned to adapt to its supervised downstream task. The
new model has achieved ground-breaking results on various
NLP tasks, by focusing on pre-training the deep hybrid architecture on large text corpora and then sparing minimal efforts on ﬁne-tuning. Other attention-based pre-training models include OpenAI GPT, which instead uses a Transformer
Decoder (with backward disabling mask) to pre-train a deep
left-to-right language model based on a different set of tasks.
Evaluation
Compared to the universal usage of attention, only a few attempts are made either to give a rigorous mathematical justiﬁcation of why it works in various scenarios. Nevertheless,
there are several works that have attempted to set up standards on evaluating its performance, either qualitatively or
quantitatively, task-speciﬁc or general, and here we give a
short summarization of these approaches.
Quantitative
Quantitative evaluation on attention can be further divided
into intrinsic or extrinsic based on whether the contribution
of attention is assessed on itself or along within a downstream supervised task.
Intrinsic evaluation methods are typically proposed in machine translation , where
attention is analogous to word alignment, performance of attention could be directly measured by comparing the attention distribution with the gold alignment data, and quantiﬁed
using alignment error rate (AER). Similarly, Liu et al. proposed a method to manually construct ”gold attention vectors” by ﬁrst identifying labelled key words
within a sentence and then conducting post-processing procedures such as smoothing and normalization, given abundant well-annotated data. For example, for the sentence Mohamad ﬁred Anwar, his former protege, in 1998, the four
tokens in boldface are labelled as argument words and receive an attention score of 0.25 each (0.25 × 4 = 1), and
Figure 4: An illustration of extrinsic evaluation of different
attention mechanisms under a downstream machine translation task. The image is modiﬁed from 
Figure 5: An illustration of qualitative methods based on
heatmaps. Higher color intensity indicates higher attention
score. The image is modiﬁed from 
then smoothing is optionally applied around each attended
token. Though intrinsic evaluation methods produce precise
measurements on performance, they tend to be restricted to
their speciﬁc tasks and rely heavily on plentitude of labelled
On the other hand, extrinsic evaluation methods (Figure 4)
are more general and widely used. This can be easily formulated by comparing the overall performance across different models under the downstream task. However, the result
could be misleading since whether or not the improvements
should be attributed to the attention component can not be
determined.
Qualitative
Qualitative evaluation for attention is currently the most
widely used evaluation technique, due to its simplicity and
convenience for visualization (Figure 5). To be speciﬁc, a
heat-map is constructed across the entire sentence where the
intensity is proportional to the normalized attention score
each element receives. Intuitively, attention is expected to
be focused on key words for the corresponding task. However, such an approach turns out to be better for visualization
than for analysis.
Conclusion and Prospects
In this paper, we have surveyed through recent works on the
attention mechanism and conducted an introductory summary based on its formulation, variation, application and
evaluation. Compared to its wide usage in various NLP
tasks, attempts to explore its mathematical justiﬁcation still
remain scarce. Recent works that explore its application
in embedding pre-training have attained great success and
might be a prospective area of future research.