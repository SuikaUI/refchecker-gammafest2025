© World Scientiﬁc Publishing Company
DOI: 10.1142/S0129065721300011
An Experimental Review on Deep Learning Architectures
for Time Series Forecasting*
Pedro Lara-Ben´ıtez†
Manuel Carranza-Garc´ıa
Jos´e C. Riquelme
Division of Computer Science, University of Sevilla,
ES-41012 Seville, Spain
E-mail: 
In recent years, deep learning techniques have outperformed traditional models in many machine learning
tasks. Deep neural networks have successfully been applied to address time series forecasting problems,
which is a very important topic in data mining. They have proved to be an eﬀective solution given their
capacity to automatically learn the temporal dependencies present in time series. However, selecting the
most convenient type of deep neural network and its parametrization is a complex task that requires
considerable expertise. Therefore, there is a need for deeper studies on the suitability of all existing
architectures for diﬀerent forecasting tasks. In this work, we face two main challenges: a comprehensive
review of the latest works using deep learning for time series forecasting; and an experimental study
comparing the performance of the most popular architectures. The comparison involves a thorough
analysis of seven types of deep learning models in terms of accuracy and eﬃciency. We evaluate the
rankings and distribution of results obtained with the proposed models under many diﬀerent architecture
conﬁgurations and training hyperparameters. The datasets used comprise more than 50000 time series
divided into 12 diﬀerent forecasting problems. By training more than 38000 models on these data, we
provide the most extensive deep learning study for time series forecasting. Among all studied models,
the results show that long short-term memory (LSTM) and convolutional networks (CNN) are the best
alternatives, with LSTMs obtaining the most accurate forecasts. CNNs achieve comparable performance
with less variability of results under diﬀerent parameter conﬁgurations, while also being more eﬃcient.
Keywords: deep learning; forecasting; time series; review.
Introduction
Time series forecasting (TSF) plays a key role in a
wide range of real-life problems that have a temporal component. Predicting the future through TSF
is an essential research topic in many ﬁelds such
as the weather,1 energy consumption,2 ﬁnancial indices,3 retail sales,4 medical monitoring,5 anomaly
detection,6 traﬃc prediction,7 etc. The unique characteristics of time series data, in which observations
have a chronological order, often make their analysis
a challenging task. Given its complexity, TSF is an
area of paramount importance in data mining. TSF
models need to take into account several issues such
as trend and seasonal variations of the series and the
∗When referring to this paper, please cite the published version: P. Lara-Ben´ıtez, M. Carranza-Garc´ıa and J. C. Riquelme,
An experimental review on deep learning architectures for time series forecasting. International Journal of Neural Systems
31(03), 2130001 . 
†Corresponding author
 
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
correlation between observed values that are close
in time. Therefore, over the last decades, researchers
have placed their eﬀorts on developing specialized
models that can capture the underlying patterns of
time series, so that they can be extrapolated to the
future eﬀectively.
In recent times, the use of deep learning (DL)
techniques has become the most popular approach
for many machine learning problems, including
TSF.8 Unlike classical statistical-based models that
can only model linear relationships in data, deep
neural networks have shown a great potential to
map complex non-linear feature interactions.9 Modern neural systems base their success in their deep
structure, stacking several layers and densely connecting a large number of neurons. The increase in
computing capacity during the last years has allowed
creating deeper models, which has signiﬁcantly improved their learning capacity compared to shallow
networks. Therefore, deep learning techniques can be
understood as a large-scale optimization task: similar to an easy problem in terms of formulation but
complex due to its size. Moreover, their capacity to
adapt directly to the data without any prior assumptions provides signiﬁcant advantages when dealing
with little information about the time series.10 With
the increasing availability of data, more sophisticated deep learning architectures have been proposed
with substantial improvements in forecasting performances.11 However, there is the need more than ever
for works that provide a comprehensive analysis of
the TSF literature, in order to better understand the
scientiﬁc advances in the ﬁeld.
In this work, a thorough review of existing deep
learning techniques for TSF is provided. Existing reviews have either focused just on a speciﬁc type of
deep learning architecture12 or on a particular data
scenario.13 Therefore, in this study, we aim to ﬁll
this gap by providing a more complete analysis of
successful applications of DL for TSF. The revision
of the literature includes studies from diﬀerent TSF
domains considering all the most popular DL architectures (multi-layer perceptron, recurrent, and
convolutional). Furthermore, we also provide a thorough experimental comparison between these architectures. We study the performance of seven types of
DL models: multi-layer perceptron, Elman recurrent,
long short-term memory, echo state, gated recurrent
unit, convolutional, and temporal convolutional networks. For evaluating these models, we have used 12
publicly available datasets from diﬀerent ﬁelds such
as ﬁnance, energy, traﬃc, or tourism. We compare
these models in terms of accuracy and eﬃciency, analyzing the distribution of results obtained with diﬀerent hyperparameter conﬁgurations. A total of 6432
models have been tested for each dataset, covering
a large range of possible architectures and training
parameters.
Since novel DL approaches in the literature are
often compared to classical models instead of other
DL techniques, this experimental study aims to provide a general and reliable benchmark that can be
used for comparison in future studies. To the best of
our knowledge, this work is the ﬁrst to assess the performance of all the most relevant types of deep neural networks over a large number of TSF problems
of diﬀerent domains. Our objective in this work is to
evaluate standard DL networks that can be directly
applicable to general forecasting tasks, without considering reﬁned architectures designed for a speciﬁc
problem. The proposed architectures used for comparison are domain-independent, intending to provide general guidelines for researchers about how to
approach forecasting problems.
In summary, the main contributions of this paper are the following:
• An updated exhaustive review on the most
relevant DL techniques for TSF according to
recent studies.
• A comparative analysis that evaluates the
performance of several DL architectures on
a large number of datasets of diﬀerent nature.
• An open-source deep learning framework for
TSF that implements the proposed models.
The rest of the paper is structured as follows:
Section 2 provides a comprehensive review of the existing literature on deep learning for TSF; in Section
3, the materials used and the methods proposed for
the experimental study are described; Section 4 reports and discusses the results obtained; Section 5
presents the conclusions and potential future work.
Deep learning architectures for time
series forecasting
A time series is a sequence of observations in chronological order that are recorded over ﬁxed intervals
Experimental Review on Deep Learning for Time Series Forecasting
of time. The forecasting problem refers to ﬁtting
a model to predict future values of the series considering the past values (which is known as lag).
Let X = {x1, x2, ..., xT } be the historical data of
a time series and H the desired forecasting horizon, the task is to predict the next values of the
series {xT +1, ..., xT +H}. Being ˆX = {ˆx1, ˆx2, ..., ˆxT }
the vector of predicted values, the goal is to minimize the prediction error as follows:.
|xT +i −ˆxT +i|
Time series can be divided into univariate or
multivariate depending on the number of variables
at each timestep. In this paper, we deal only with
univariate time series analysis, with a single observation recorded sequentially over time. Over the last
decades, artiﬁcial intelligence (AI) techniques have
increased their popularity for approaching TSF problems, with traditional statistical methods being regarded as baselines for performance comparison in
novel studies.
Before the rise of data mining techniques, the
traditional methods used for TSF were mainly based
on statistical models, such as exponential smoothing
(ETS)14 and Box-Jenkins methods like ARIMA.15
These models rely on building linear functions from
recent past observations to provide future predictions, and have been extensively used for forecasting
tasks over the last decades.16 However, these statistical methods often fail when they are applied directly, without considering the theoretical requirements of stationarity and ergodicity as well as the
preprocessing required. For instance, in an ARIMA
model, the time series has to be transformed into
stationary (without trends or seasonality) through
several diﬀerencing transformations. Another issue
is that the applicability of these models is limited
to situations when suﬃcient historical data, with an
explainable structure, is available. When the data
availability for a single series is scarce, these models
often fail to extract eﬀectively the underlying features and patterns. Furthermore, since these methods create a model for each individual time series, it
is not possible to share the learning across similar instances. Therefore, it is not feasible to use these techniques for dealing with massive amounts of data as it
would be computationally prohibitive. Hence, artiﬁcial intelligence (AI) techniques, that allow building
global models to forecast on multiple related series,
began earning popularity. Meanwhile, linear methods
started to be regarded as baselines for performance
comparison in novel studies.
Ref. 17 provides a survey on early data mining
techniques for TSF and its comparison with classical
approaches. Among all proposed data mining methods in the literature, the models that have attracted
more attention have been those based on artiﬁcial
neural networks (ANNs). They have shown better
performance than statistical methods in many situations, especially due to their capacity and ﬂexibility to map non-linear relationships from data given
their deep structure.18 Another advantage of ANNs
is that they can extract temporal patterns automatically without any theoretical assumptions on
the data distribution, reducing preprocessing eﬀorts.
Furthermore, the capacity of generalization of ANNs
allows exploiting cross-series information. When using ANNs, a single global model that learns from
multiple related time series can be built, which can
greatly improve the forecasting performance.
However, researchers have struggled to develop
optimal network topologies and learning algorithms,
due to the inﬁnite possibilities of architecture con-
ﬁgurations that ANNs allow. Starting from very basic Multi-Layer Perceptron (MLP) proposals, a large
number of studies have proposed increasingly sophisticated architectures, such as recurrent or convolutional networks, that have enhanced the performance
for TSF. In this work, we review the most relevant
types of DL networks according to the existing literature, which can be divided into three categories:
• Fully connected neural networks.
– MLP: Multi-layer perceptron.
• Recurrent neural networks.
– ERNN: Elman recurrent neural network.
– LSTM: Long short-term memory network.
– ESN: Echo state network.
– GRU: Gated recurrent units network.
• Convolutional networks.
– CNN: Convolutional neural network.
– TCN: Temporal convolutional network.
The architectural variants and the ﬁelds in
which these DL networks have been successfully applied will be discussed in the following sections.
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
Multi-Layer Perceptron
The concept of ANNs was ﬁrst introduced in Ref. 27,
and was inspired in the functioning of the brain with
neurons acting in parallel for processing data. Multi-
Layer Perceptron (MLP) is the most basic type of
feed-forward artiﬁcial neural network. Their architecture is composed of a three-block structure: an input layer, hidden layers, and an output layer. There
can be one or multiple hidden layers, which determines the depth of the network. It is the increase
of the depth, the inclusion of more hidden layers,
which makes an MLP network a deep learning model.
Each layer contains a deﬁned set of neurons that
have connections associated with trainable parameters. The neural network learning algorithm updates
the weights of these connections in order to map the
input/output relationship. MLP networks only have
forward connections between neurons, unlike other
architectures that have feedback loops. The most relevant studies using MLPs are presented below.
In the early 90s, researchers started to pose
ANNs as a promising alternative compared to traditional statistical models. These initial studies proposed the use of MLP networks with one or a few
hidden layers. At that time, due to the lack of systematic methodologies to build ANNs and their dif-
ﬁcult interpretation, researchers were still skeptical
about their applicability to TSF.
At the end of the decade, several works reviewed
the existing literature with the aim of clarifying the
strengths and shortcomings of ANNs.28 These reviews agreed about the potential of ANNs for forecasting due to their unique characteristics as universal function approximators and their ﬂexibility
to adapt to data without prior assumptions. However, they all claimed that more rigorous validation
procedures were needed to conclude generally that
ANNs improve the performance of classical alternatives. Furthermore, another general reasoning was
that determining the optimal structure and hyperparameters of the ANNs was a diﬃcult process that
had a key inﬂuence on performance. From that time
on, many references proposing ANNs as a powerful
tool for forecasting can be found in the literature.
Table 1 presents a collection of studies using
MLPs for diﬀerent time series forecasting problems.
Most studies have focused on the design of the network, concluding that the past-history parameter has
a greater inﬂuence on the accuracy than the number
of hidden layers. Furthermore, these works proved
the importance of the preprocessing steps, since simple models with carefully selected input variables
tend to achieve better performance. However, the
most recent studies propose the use of ensembles of
MLP networks to achieve higher accuracy.
Although feed-forward neural networks such as
Relevant studies on time series forecasting using MLP networks.
Outperforms
Application
Description/ﬁndings
ARIMA and MLP
individually
Exchange rates and
environmental data
The ARIMA component models the linear correlation
structures while MLP works on the nonlinear part.
Statistical methods
Tourism expenditure
Pre-processing steps such as detrending and deseasonalization were essential. MLPs performed better
as the forecast horizon increases.
48 MLPs with
diﬀerent inputs
No comparison
Quarterly time series
M3 competition
Data preparation was more important than optimizing the number of nodes. Carefully selected the input
variables and designed simple models.
Six datasets of
diﬀerent domains
Comparison between iterative and direct forecasting
methods. Best performance with direct approach.
Competition winner
NN3 competition
Automatic scheme based on generalized regression
neural networks (GRNNs).
Tourism data
ARIMA models were better for short-horizon prediction while ANNs were better for longer forecasts.
Ensemble MLP
Exponential smoothing
and naive forecast
Retail sales
Evaluates diﬀerent ensemble operators (mean, median, and mode). The mode operator was the best.
Ensemble MLP
Statistical methods
NN3 and NN5
competitions
Two-layers ensemble. The ﬁrst layer ﬁnds an appropriate lag, and the second layer employs the obtained
lag for forecasting.
Parallelized
Linear Regression,
Gradient-Boosted Trees,
Random Forest
Electricity demand
Split the problem into several forecasting subproblems to predict many samples simultaneously.
Experimental Review on Deep Learning for Time Series Forecasting
MLP have been applied eﬀectively in many circumstances, they are unable to capture the temporal
order of a time series, since they treat each input
independently. Ignoring the temporal order of input windows restrains performance in TSF, especially when dealing with instances of diﬀerent lengths
that change dynamically. Therefore, more specialized
models, such as recurrent neural networks (RNNs)
or convolutional neural networks (CNNs), started
to raise interest. With these networks, the temporal problem is transformed into a spatial architecture
that can encode the time dimension, thus capturing
more eﬀectively the underlying dynamical patterns
of time series.33
Recurrent Neural Networks
Recurrent Neural Networks (RNN) were introduced
as a variant of ANN for time-dependent data. While
MLPs ignores the time relationships within the input data, RNNs connects each time step with the
previous ones to model the temporal dependency of
the data, providing RNN native support for sequence
data.34 The network sees one observation at a time
and can learn information about the previous observations and how relevant the observation is to
forecasting. Through this process, the network not
only learns patterns between input and output but
also learns internal patterns between observations of
the sequence. This characteristic makes RNNs ones
of the most common neural networks used for timeseries data. They have been successfully implemented
for forecasting applications in diﬀerent ﬁelds such as
stock market price forecasting,35 wind speed forecasting,36 or solar radiation forecasting.37 Furthermore, RNNs have achieved top results at forecasting
competitions, like the recent M4 competition.38
In the late 80s, several studies worked on diﬀerent approaches to provide memory to a neural network.39 This memory would help the model to learn
from time series data. One of the most promising
proposals was the Jordan RNN.40 It was the main inspiration to create the Elman RNN, which is known
as the base of modern RNN.34
Elman Recurrent Neural Networks
The ERNN aimed to tackle the problem of dealing
with time patterns in data. ERNN changed the feedforward hidden layer to a recurrent layer, which connects the output of the hidden layer to the input of
the hidden layer for the next time step. This connection allows the network to learn a compact representation of the input sequence. In order to implement the optimization algorithm, the networks are
usually unfolded and the backpropagation method
is modiﬁed, resulting in the Truncated Backpropagation Through Time (TBPTT) algorithm. Table 2
presents some studies where ERNNs have been successfully applied for TSF in diﬀerent ﬁelds. In general, the studies demonstrate the improvement of recurrent networks compared to MLPs and linear models such as ARIMA.
Despite the success of Elman’s approximation,
the application of TBTT faces two main problems:
the weights may start to oscillate (exploding gradient problem), or an excessive computational time to
learn long-term patterns (vanishing gradient problem)41
Long Short-Term Memory Networks
In 1997, Long Short-Term Memory (LSTM) networks56 were introduced as a solution for ERNN’s
problems. LSTMs are able to model temporal dependencies in larger horizons without forgetting the
Relevant studies on time series forecasting using ERNNs.
Outperforms
Application
Description/ﬁndings
MLP, RBF, ARIMA
Solar radiation
Despite the good results, ERNNs were unable to
model the discontinuities of time series. ERNNs had
slower convergence rate than non-recurrent models.
RBFNN, ARMA-ANN,
Chaotic time series
Compares diﬀerent training algorithms, concluding
that evolutionary algorithms take signiﬁcantly more
time to converge than gradient-based methods.
ARIMA, SVR, BPNN,
Electricity data
Improved performance with pre-processing steps such
as signal decomposition and feature selection.
Energy consumption
Reported an important improvement with respect to
nonlinear autoregressive networks.
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
short-term patterns. LSTM networks diﬀer from
ERNN in the hidden layer, also known as LSTM
memory cell. LSTM cells use a multiplicative input
gate to control the memory units, preventing them
from being modiﬁed by irrelevant perturbations.
Similarly, a multiplicative output protects other cells
from perturbations stored in the current memory.
Later, another work added a forget gate to the LSTM
memory cell.57 This gate allows LSTM to learn to reset memory contents when they become irrelevant.
A list of relevant studies that address TSF problems with LSTM networks can be found in Table
3. Overall, these studies prove the advantages of
LSTM over traditional MLP and ERNN for extracting meaningful information from time series. Furthermore, some recent studies proposed more innovative solutions such as hybrid models, genetic algorithms to optimize the network architecture, or
adding a clustering step to train LSTM networks on
multiple related time series.
Echo State Networks
Echo State Networks (ESNs) were introduced by Ref.
62. They are based on Reservoir Computing (RC),
which simpliﬁes the training procedure of traditional
RNNS. Previous RNNs, such as ERNN or LSTM,
have to ﬁnd the best values for all neurons of the
network. In contrast, the ESN tunes just the weights
from the output neurons, which makes the training
problem a simple linear regression task.63
Relevant studies on time series forecasting using LSTM networks.
Outperforms
Application
Description/ﬁndings
MLP, NARX, SVM
Traﬃc speed
LSTM proved eﬀective for short-term prediction
without prior information of time lag.
MLP, Autoencoders
LSTM determines the optimal time lags dynamically,
showing higher generalization ability.
Logistic regression,
S&P 500 index
Disentangle the LSTM black-box to ﬁnd common patterns of stocks in noisy ﬁnancial data.
Linear regression, kNN,
Electric load
Feature selection and genetic algorithm to ﬁnd optimal time lags and number of layers for LSTM model.
ARIMA, ERNN, GRU
production
Deep LSTM using genetic algorithms to optimally
conﬁgure the architecture.
LSTM + Attention
Solar generation
Temporal attention mechanism to improve performance over standard LSTM, also using partial autocorrelation to determine the input lag.
Hybrid ETS-LSTM
Competition winner
M4 competition
ETS captures the main components such as seasonality, while the LSTM networks allow non-linear trends
and cross-learning from multiple related series.
Clustering + LSTM
LSTM, ARIMA, ETS
CIF2016 and NN5
competitions
Building a model for each cluster of related time series
can improve the forecast accuracy, while also reducing
training time.
Relevant studies on time series forecasting using ESNs.
Outperforms
Application
Description/ﬁndings
MLP, RBFNN
Stock market price
Applying PCA to ﬁlter noise improved the ESN performance over some indexes.
Other types of reservoirs
7 time-series from
diﬀerent ﬁelds
An ESN with a simple cycle reservoir topology
achieved high performance for TSF problems.
Laplace function
Support Vector, Gaussian,
and Bayesian ESN
Simulated datasets
Applied ESN in a Bayesian learning framework. The
Laplace function proved to be more robust to outliers
than the Gaussian distribution.
No comparison
Electricity load
forecasting
The ESN proved its capacity to learn complex dynamics of electric load, obtaining high accuracy results without additional inputs.
GA-optimized
ARIMA, Generalized
Regression NN
Wind speed
ESN to predict multiple wind speeds simultaneously.
PCA and spectral clustering to preprocess data and
genetic algorithm to search optimal parameters.
Bagged ESN
BPNN, RNN, LSTM
Energy consumption
Combines ESN, bagging, and diﬀerential evolution algorithm to reduce forecasting error and improve generalization capacity.
Experimental Review on Deep Learning for Time Series Forecasting
An ESN is a neural network with a random RNN
called the reservoir as the hidden layer. The reservoir has usually a high number of neurons and sparse
interconnectivity (around 1%) between the hidden
neurons. These characteristics make the reservoir a
set of subsystems that work as echo functions, being
able to reproduce speciﬁc temporal patterns.64
In the literature, many ESN applications for
TSF can be found. Especially, ESN networks have
proved to outperform MLPs and statistical methods when modeling chaotic time series data. Furthermore, it is worth mentioning that the non-trainable
reservoir neurons make this network a very timeeﬃcient model compared to other RNNs. Table 4
presents several studies using ESNs and their most
interesting ﬁndings.
Gated Recurrent Units
Gated Recurrent Units (GRU) were introduced by
Ref. 65 as another solution to the exploding gradient
and vanishing gradient problem of ERNNs. However,
it can also be seen as a simpliﬁcation of LSTM units.
In a GRU unit, the forget and input are combined
into a single update gate. This modiﬁcation reduces
the trainable parameters providing GRU networks
with a better performance in terms of computational
time while achieving similar results.66
The GRU cell uses an update gate and a reset gate that will learn to decide which information
should be kept without vanishing it through time and
which information is irrelevant for the problem. The
update gate is in charge of deciding how much of the
past information should be passed along to the future, while the reset gate decides how much of the
past information to forget.
Table 5 presents several studies that use GRU
networks for TSF problems. These works often propose modiﬁcations to the standard GRU model in
order to improve performance over other recurrent
networks. However, the number of existing studies
proposing GRU models is much lower than those using LSTM networks.
Convolutional Neural Networks
CNNs are a family of deep architectures that were
originally designed for computer vision tasks. They
are considered state-of-the-art for many classiﬁcation
tasks such as object recognition,78 speech recognition79 and natural language processing.80 CNNs can
automatically extract features from high dimensional
raw data with a grid topology, such as the pixels of
an image, without the need of any feature engineering. The model learns to extract meaningful features
from the raw data using the convolutional operation,
which is a sliding ﬁlter that creates feature maps and
aims to capture repeated patterns at diﬀerent regions
of the data. This feature extracting process provides
CNNs with an important characteristic so-called distortion invariance, which means that the features are
extracted regardless of where they are in the data.
These characteristics make CNNs suitable for dealing
with one-dimensional data such as time series. Sequence data can be seen as a one-dimensional image
from where the convolutional operation can extract
A CNN architecture is usually composed of convolution layers, pooling layers (to reduce the spatial
dimension of feature maps), and fully connected layers (to combine local features into global features).
These networks are based on three principles: local
connectivity, shared weights, and translation equivariance. Unlike standard MLP networks, each node
Relevant studies on time series forecasting using GRU networks.
Outperforms
Application
Description/ﬁndings
Music and speech
signal modeling
The results proved the advantages of GRU and LSTM
networks over ERNN but did not show a signiﬁcant
diﬀerence between both gated units.
Electricity load
Proposed scaled exponential linear units to overcome
vanishing gradients, showing a signiﬁcant improvement over LSTM and standard GRU models.
LSTM, SVM,
Photovoltaic power
Pearson coeﬃcient is used to extract the main features and K-means to cluster similar groups that train
the GRU model.
MLP, CNN, LSTM
Electricity price
GRU performed better and trained faster than LSTM
networks. Using a larger number of lagged values improved the results.
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
Relevant studies on time series forecasting using CNNs.
Outperforms
Application
Description/ﬁndings
Stock price
The data was preprocessed using temporally-aware
normalization. The model obtained better performance in short-term predictions.
Energy load
The model used convolution and pooling to predict
the load for the next three days. It proved useful for
reducing expenses in future smart grids.
Solar power and
electricity
The CNN was signiﬁcantly faster than the recurrent
approach with similar accuracy, hence more suitable
for practical applications.
Hybrid CNN-LSTM
RNN and LSTM
individually
Electricity
The LSTM is able to extract the long-term dependencies and the CNN captures local trend patterns.
ARIMA, ERNN,
Wind speed
The data is decomposed using the wavelet packet,
with a CNN predicting the high-frequency data and
a CNN-LSTM for the low-frequency data.
RNN, ARIMAX
Building load
The CNN model with a direct approach provided the
best results, improving signiﬁcantly the forecasting
accuracy of the seasonal ARIMAX model.
Hybrid CNN-LSTM
Financial data
The LSTM learns features and reduces dimensionality, and the dilated CNN learns diﬀerent time intervals.
is connected only to a region of the input, which is
known as the receptive ﬁeld. Moreover, the neurons
in the same layers share the same weight matrix for
the convolution, which is a ﬁlter with a deﬁned kernel size. These special properties allow CNNs to have
a much smaller number of trainable parameters compared to a RNN, hence the learning process is more
time eﬃcient.74 Another key aspect of the success of
CNNs is the possibility of stacking diﬀerent convolutional layers so that the deep learning model can
transform the raw data into an eﬀective representation of the time series at diﬀerent scales.81
CNN-based models have not been extensively
used in the TSF literature since RNNs have been
given far more importance. Nevertheless, several
works have proposed CNNs as feature extractors
alone or together with recurrent blocks to provide
forecasts. The most relevant works involving these
proposals are detailed in Table 6.
Temporal Convolutional Network
Recent studies have proposed a more specialized
CNN architecture known as Temporal Convolutional
Network (TCN). This architecture is inspired by
the Wavenet autoregressive model, which was designed for audio generation problems.82 The term
TCN was ﬁrst presented in Ref. 83 to refer to a
type of CNN with special characteristics: the convolutions are causal to prevent information loss, and
the architecture can process a sequence of any length
and map it to an output of the same length. To enable the network to learn the long-term dependencies
present in time series, the TCN architecture makes
use of dilated causal convolutions. This convolution
increases the receptive ﬁeld of the network (neurons
that are convolved with the ﬁlter) without losing resolution since pooling operation is not needed.84 Furthermore, TCN employs residual connections to allow to increase the depth of the network, so that
Relevant studies on time series forecasting using TCNs.
Outperforms
Application
Description/ﬁndings
Financial data
TCN can eﬀectively learn dependencies in and between series without the need for long historical data.
Encoder-decoder
Retail sales
Combined with representation learning, TCN can
learn complex patterns such as seasonality. TCNs are
eﬃcient due to the paralellization of convolutions.
LSTM, ConvLSTM
Meteorology
The TCN presented higher eﬃciency and capacity of
generalization, performing better at longer forecasts.
Energy demand
TCN models showed a greater capacity to deal with
longer input sequences. TCNs were less sensitive to
the parametrization than LSTM networks.
Experimental Review on Deep Learning for Time Series Forecasting
it can deal eﬀectively with a large history size. In
Ref. 83 the authors present an experimental comparison between generics RNNs (LSTM and GRU)
and TCNs over several sequence modeling tasks. This
work emphasizes the advantages of TCNs such as:
their low memory requirements for training due to
the shared convolutional ﬁlters; long input sequences
can be processed with parallel convolutions instead of
sequentially as in RNNs; and a more stable training
scheme, hence avoiding vanishing gradient problems.
TCNs are acquiring increasing popularity in recent years due to their suitability for dealing with
temporal data. Table 7 presents some studies where
TCNs have successfully applied for time series forecasting.
Materials and methods
In this section, we present the datasets used for
the experimental study and the details of the architectures and hyperparameter conﬁgurations studied.
The source code of the experiments can be found
at Ref. 85. This repository provides a deep learning
framework based on TensorFlow for TSF, allowing
full reproducibility of the experiments.
Datasets used for the experimental study.
Columns N, FH, M and m refer to number of time series,
forecast horizon, maximum length and minimum length
respectively.
CIF2016o12
ExchangeRate
SolarEnergy
Traﬃc-metr-la
Traﬃc-perms-bay
WikiWebTraﬃc
In this study, we have selected 12 publicly available
datasets of diﬀerent nature, each of them with multiple related time series. This collection represents a
wide variety of time series forecasting problems, covering diﬀerent sizes, domains, time-series length, and
forecasting horizon. Table 8 presents the characteristics of each dataset in detail. In total, the experimental study involves more than 50000 time series
among all the selected datasets. Furthermore, Figure
1 illustrates some examples instances of each dataset.
As can be seen, the selected datasets present a wide
diversity of characteristics in terms of scale and seasonality. Most of these datasets have been used in
forecasting competitions and other TSF reviews such
as Ref. 12.
The CIF 2016 competition dataset86 contains a
total of 72 monthly time series, from which 12 of
them have a 6-month forecasting horizon while the
remaining 57 series have a 12-month forecasting horizon. Some of these time series are real bank risk analysis indicators while others are artiﬁcially generated.
The ExchangeRate dataset87 records the daily
exchange rates of 8 countries including Australia,
British, Canada, Switzerland, China, Japan, New
Zealand, and Singapore from 1990 to 2016 (a total of
7588 days for each country). The goal of this dataset
is to predict values for the next 6 days.
Both M3 and M4 competitions88,89 datasets
include time series of diﬀerent domains and observation frequencies. However, due to computational
time constraints, we have limited the experimentation to the monthly time series as they are longer
than the yearly, quarterly, weekly and daily time series. Both competitions ask for an 18 months prediction and contain time series of diﬀerent categories
such as industry, ﬁnance, or demography. The number of instances belonging to each category are evenly
distributed according to their presence in the real
world, leading novel studies to representative conclusions. Considering only the monthly time series,
the M4 dataset has 48000 time series, while M3 is
considerably smaller with 1428 time series.
The NN5 competition dataset90 is formed by
111 time series with a length of 735 values. This
dataset represents 2 years of daily cash withdrawals
at automatic teller machines (ATMs) from England.
The competition established a forecasting horizon of
56 days ahead.
The Tourism dataset91 is composed of 336
monthly time series of diﬀerent length, requiring a
24 months prediction. The data was provided by
tourism bodies from Australia, Hong Kong, and New
Zealand, representing total tourism numbers at a
country level.
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
Two examples of time series instances of each data set. The colours diﬀerentiate the two instances. The y-axis
represents the value that the time series takes for each timestep along the x-axis.
The SolarEnergy dataset92 contains the solar
power production records in the year of 2006, which
is sampled every 10 minutes from 137 solar photovoltaic power plants in Alabama State. The forecasting horizon has been established to one hour, which
comprises 6 observations.
Traﬃc-metr-la and Traﬃc-perms-bay are two
public traﬃc network datasets.93 Traﬃc-metr-la contains traﬃc speed information of 207 sensors on the
highways of Los Angeles for four months. Similarly,
Traﬃc-perms-bay records six months of statistics on
traﬃc speed from 325 sensors in the Bay area. For
both datasets, the readings from the sensors are aggregated into ﬁve-minute windows.
The Traﬃc dataset is a collection of hourly time
series from the California Department of Transportation collected during 2015 and 2016. The data describes the road occupancy rates (between 0 and 1)
measured by diﬀerent sensors on San Francisco Bay
area free-ways.
The WikiWebTraﬃc dataset belongs to a Kaggle competition94 with the goal of predicting future
web traﬃc of a given set of Wikipedia pages. The
traﬃc is measured in the daily number of hits and
the forecasting horizon is 59 days.
Experimental setup
This subsection presents the comparative study carried out to evaluate the performance of seven stateof-the-art deep-learning architectures for time series
forecasting. We explain the architectures and hyperparameter conﬁgurations that we have explored, together with the details of the evaluation procedure.
The experimental study is based on a statistical analysis with the results obtained with 6432 diﬀerent architecture conﬁgurations over 12 datasets, resulting
in more than 38000 tested models.
Deep learning architectures
Seven diﬀerent types of deep learning models for
TSF are compared in this experimental study: multilayer perceptron, Elman recurrent, long short-term
memory, echo state, gated recurrent unit, convolutional, and temporal convolutional networks. For all
these models, the number of hyperparameters that
have to be conﬁgured is high compared to traditional
machine-learning techniques. Therefore, the proper
tuning of these parameters is a complex task that
requires considerable expertise and is usually driven
by intuition. In this work, we have performed an exhaustive grid search on the conﬁguration of each type
Experimental Review on Deep Learning for Time Series Forecasting
of architecture in order to ﬁnd the most suitable values. Table 9 presents the search carried out over the
main parameters of the deep learning models such as
the number of layers or units. The grid of possibilities
has been decided based on typical values found in the
literature. For instance, it is a common practice to
select powers of two for the number of neurons, units,
or ﬁlters. We have tried to establish a fair comparison between architectures, maintaining the values
consistent across the models as long as possible. For
example, in all cases, we explore single-layer models
up to deeper networks with four stacked layers.
Parameter grid of the architecture conﬁgurations for the seven types of deep learning models.
Parameters
Hidden Layers
 , , , ,
 , ,
 , , ,
 , ,
 
32, 64, 128
Return sequence
True, False
32, 64, 128
Return sequence
True, False
32, 64, 128
Return sequence
True, False
32, 64, 128
Return sequence
True, False
16, 32, 64
 , 
Kernel size
Return sequence
True, False
Firstly, we experiment with the simplest neural network architecture, the multi-layer perceptron.
The MLP models can be used as a baseline for morecomplex architectures that obtain a better performance. In particular, we have implemented 12 MLP
models, which diﬀer in the number of hidden layers and the number of units in each layer. The selected conﬁgurations can be seen in Table 9, where
the hidden-layers parameters are deﬁned as a list
[v1, v2, ..., vi, ..., vn]. A value vi from the list represents the number of units in the i-th hidden layer.
The number of neurons in each layer ranges from
8 to 128, which aims to suit shorter and longer
past-history input sequences. Furthermore, the design considers both encoder and decoder-like structures, with more neurons in initial layers and less at
the end, and vice versa.
Concerning recurrent architectures, four diﬀerent types of models have been implemented in this
study: Elman recurrent neural network (ERNN),
long-short term memory (LSTM), gated recurrent
unit (GRU), and echo state network (ESN). A grid
search on the three main parameters is performed,
resulting in 18 models for each architecture. These
parameters are the number of stacked recurrent layers, the number of recurrent units in each layer, and
whether the last layer returns the state sequence or
just the ﬁnal state. If return sequence is False, the
last layer returns one value for each recurrent unit.
In contrast, if return sequence is True, the recurrent
layer returns the state of each unit for each timestep,
resulting in a matrix of shape (input timesteps ×
number of units). Finally, as we are working with
a multi-step-ahead forecasting problem, the output
of the recurrent block is connected to a dense layer
with one neuron for each prediction. Table 9 shows
all the possible values for each parameter. The number of units ranges from 32 to 128, aiming to explore
the convenience of having more or less learning cells
in parallel.
Furthermore, 18 convolutional models have been
implemented (CNN). These models are formed by
stacked convolutional blocks, which are composed of
a one-dimensional convolutional layer followed by a
max-pooling layer. In Table 9, we present the value
search for each parameter. The convolutional blocks
are implemented with decreasing kernel sizes, as it is
common in the literature. Single-layer models have a
kernel of size 3, two-layers models have kernel sizes
of 5 and 3, and four-layers models have kernels of
size 7-5-3, and 3. Since the input sequences in the
studied datasets are not excessively long (ranging approximately from 20 to 300 timesteps), we have only
considered a pooling factor of 2.
Finally, we have also experimented with the
temporal convolutional network (TCN) architecture.
TCN models are mainly deﬁned by ﬁve parameters:
number of TCN layers, number of convolutional ﬁlters, dilation factors, convolutional kernel size, and
whether to return the last output or the full sequence. A grid search on these parameters has been
done with the values speciﬁed in Table 9, resulting
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
in 32 diﬀerent TCN architectures. The number of
dilations and the kernel sizes have been selected according to the receptive ﬁeld of the TCN, which follows the formula (number of layers×kernel size×
last dilation). With the selected grid we cover receptive ﬁelds ranging from 24 to 288, which suits the
variable length of the input sequences of the studied
Evaluation procedure
evaluation
datasets into training and test sets. We have followed a ﬁxed origin testing scheme, which is the same
splitting procedure as in recent works dealing with
datasets containing multiple related time series.12
The test set consists of the last part of each individual time series within the dataset, hence the length
is equal to the forecast horizon. Consequently, the
remaining part of the time series is used as training
data. This division has been applied equally to all
datasets, obtaining a total of more than one million
timesteps for testing and more than 210 million for
training. In this study, we use thousands of time series with a wide variety of forecasting horizons over
more than 38000 models. Therefore, this ﬁxed origin validation procedure can lead to representative
results and ﬁndings.
Once we have split the time series into training
and test, we perform a series of preprocessing steps.
Firstly, a normalization method is used to scale each
time series of training data between 0 and 1, which
helps to improve the convergence of deep neural networks. Then, we transform the time series into training instances to be fed to the models. There are several techniques available for this purpose:95 the recursive strategy, which performs one-step predictions
and feeds the result as the last input for the next prediction; the direct strategy, which builds one model
for each time step; and the multi-output approach,
which outputs the complete forecasting horizon vector using just one model. In this study, we have selected the Multi-Input Multi-Output (MIMO) strategy. Recent studies show that MIMO outperforms
single-output approaches because, unlike the recursive strategy, it does not accumulate the error over
the predictions. It is also more eﬃcient in terms of
computation time than the direct strategy since it
uses one single model for every prediction.95
Following this approach, we use a movingwindow scheme to transform the time series into
training instances that can feed the models. This
process slides a window of a ﬁxed size, resulting in
an input–output instance at each position. The deep
learning models receive a window of ﬁxed-length
(past history) as input and have a dense layer as
output with as many neurons as the forecasting horizon deﬁned by the problem. The length of the input
depends on the past-history parameter that should
be decided. For the experimentation, we study three
diﬀerent values of past history depending on the forecast horizon (1.25, 2, or 3 times the forecast horizon).
Figure 2 illustrates an example of this technique with
7 observations as past history and a forecasting horizon of 3 values.
Example of moving window procedure that
obtains the input–output instances to train the models.
In this example, the models receive an instance with pasthistory window of length 7 as input and output 3 values
as forecasting horizon.
Along with the past-history parameter, we have
also conducted a grid search over the optimal training conﬁguration of the deep learning models. We
have experimented with several possibilities for the
batch size, the learning rate, and the normalization
method to be used. We have selected commonly used
values for the batch size (32 and 64) and learning
rate (0.001 and 0.01). The Adam optimizer has been
chosen, which implements an adaptive stochastic optimization method that has been proven to be robust
Experimental Review on Deep Learning for Time Series Forecasting
and well-suited for a wide range of machine learning
problems.96 Furthermore, the two most common normalization functions in the literature have been selected to preprocess the data : min-max scaler (Eq. 2)
and mean normalization, also known as z-score (Eq.
3). The complete grid of training parameters can be
found in Table 10.
min-max(x) =
max(x) −min(x)
z-score(x) = x −average(x)
max(x) −min(x)
Grid of training parameters for the deep
learning models.
Parameters
Past History
(1.25, 2.0, 3.0) × Forecast horizon
Batch size
No. of epochs
Learning rate
0.001, 0.01
Normalization
minmax, zscore
Evaluation metrics
The performance of the proposed models is evaluated in terms of accuracy and eﬃciency. We analyze the best results obtained with each type of architecture, as well as the distribution of results obtained with the diﬀerent hyperparameter conﬁgurations. For evaluating the predictive performance of
all models we use the weighted absolute percentage
error (WAPE). This metric is a variation of the mean
absolute percentage error (MAPE), which is one of
the most widely used measures of forecast accuracy
due to its advantages of scale-independency and interpretability.97 WAPE is a more suitable alternative
for intermittent and low-volume data. It rescales the
error dividing by the mean to make it comparable
across time series of varying scales.
WAPE can be deﬁned as follows:
WAPE(y, o) = MAE(y, o)
= mean(|y −o|)
where y and o are two vectors with the real and predicted values, respectively, that have a length equal
to the forecasting horizon
Furthermore, developing eﬃcient models is essential in TSF since many applications require realtime responses. Therefore, we also study the average
training and inference times of each model.
Statistical analysis
Once the error values for each method over each
dataset have been recorded, a statistical analysis
will be carried out. This analysis allows us to compare correctly the performance of the diﬀerent deeplearning architectures. Since we are studying multiple models over multiple datasets, the Friedman test
is the recommended method.98 This non-parametric
test allows detecting global diﬀerences and provides a ranking of the diﬀerent methods. In the
case of obtaining a p-value below the signiﬁcance
level (0.05), the null hypothesis (all algorithms are
equivalent) can be rejected, and we can proceed
with the post-hoc analysis. For this purpose, we use
Holm-Bonferroni’s procedure, which performs pairwise comparisons between the models. With this
n×n procedure, we can detect signiﬁcant diﬀerences
between each pair of models, which allows establishing a statistical ranking.
In this study, we perform the statistical analysis over diﬀerent performance metrics to evaluate
the seven types of deep learning models from all
perspectives. We rank the models according to the
best results in terms of forecasting accuracy (WAPE)
and eﬃciency (training and inference times). Furthermore, we evaluate the statistical diﬀerences of
the worst performance and the mean and standard
deviation of the results obtained with each type of
architecture. Finally, we carry out a ranking of average rankings to see which models are better overall.
An additional statistical analysis is performed
using a paired Wilcoxon signed-rank test, in order to
study the statistical diﬀerence among the studied architecture conﬁgurations of each type of model. Note
that in this case, we compare models within the same
type of deep learning network, so we will discover
what conﬁgurations are optimal for each of them.
The null hypothesis is that models with two diﬀerent
values for a speciﬁc conﬁguration are not statistically
diﬀerent at the signiﬁcance level of α = 0.05. A pvalue ≤0.05 rejects the null hypothesis, indicating a
signiﬁcant diﬀerence between the models.99
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
Results and Discussion
This section reports and discusses the results obtained from the experiments carried out. It provides
an analysis of the results in terms of forecasting accuracy and computational time. The experiments have
taken around four months, using ﬁve NVIDIA TI-
TAN Xp 12GB GPU in computers with an Intel i7-
8700 CPU, and an additional GPU in the Amazon
cloud service.
Forecasting accuracy
The ﬁrst part of the analysis is focused on the forecasting accuracy obtained for each model, using the
WAPE metric. In Figure 3, we present a comparison
between the distribution of results obtained with all
the diﬀerent model architectures for each dataset. In
some plots, the ERNN distribution has been cut to
allow better visualization of the rest of the models. It
can be seen at ﬁrst glance, that some architectures,
such as the ERNN or TCN, are more sensitive to
Boxplots showing the distribution of WAPE accuracy results for each type of model over each dataset. The
red dot indicates the mean WAPE, the box shows the quartiles of the results and the whiskers extend to show the rest of
the distribution.
Experimental Review on Deep Learning for Time Series Forecasting
Best WAPE results obtained with each type of architecture for all datasets.
the parametrization as they present a wider WAPE
distribution compared to others like CNN or MLP.
In general, except for the MLP which is not speciﬁcally designed to deal with time series, the rest of the
architectures can obtain forecasting accuracy similar
to the best model in almost all cases. This can be
seen by observing the minimum WAPE values of the
models, which are close to each other. However, in
this plot, it is more important to analyze how hard it
is to achieve such performance. Wider distributions
indicate a higher diﬃculty to ﬁnd the optimal hyperparameter conﬁguration. In this sense, as will be
seen later in the statistical analysis, CNN and LSTM
are the most suitable alternatives for a fast design of
models with good performance.
Furthermore, Table 11 presents a more detailed
view of the best results obtained for each architecture
on each dataset. As expected, MLP models perform
the worst overall. MLP networks are simple models that can serve as a useful comparison baseline
with the rest of the architectures. We can notice that
LSTM models achieve the best results in four out of
twelve datasets and it is in the top three architectures for the remaining datasets, except Tourism for
which LSTM is in sixth place only over MLP. Furthermore, we can also point out that GRU seems to
be a very consistent technique as it obtains optimal
predictions for most of the datasets, being within the
ﬁrst three architectures for ten out of twelve datasets.
TCN and ERNN models obtain the best results in
two datasets, similarly to GRU. However, these two
architectures are much more unstable, observing very
diﬀerent results depending on the dataset. The CNN
presents a behavior slightly worse than the TCN in
terms of best results. Nevertheless, as it was seen
in the boxplot, it outperforms TCN models when
comparing the average WAPE for all hyperparameter conﬁgurations. Finally, the ESN is one of the
worst models, ranking sixth in most datasets, only
outperforming MLP.
Due to space constraints, the complete report
of results is provided in an online appendix that
can be found at Ref. 85. This appendix contains
the results for each architecture conﬁguration in several spreadsheets. Furthermore, it has a summary
ﬁle with the best, mean, standard deviation, and
worst results grouped by type of model. It is worth
mentioning that CNN outperforms the rest of the
models in terms of the mean and standard deviation of WAPE on many datasets. This indicates that
it is easier to ﬁnd a high performing set of parameters for convolutional models than for recurrent ones.
However, TCNs have a higher standard deviation of
WAPE, given that they are more complex architectures with more parameters to take into account.
LSTMs also achieve good performance in terms of
average WAPE, which further supports that it is
the best alternative among the studied recurrent networks. GRU and ERNN are the models amongst all
that suﬀer a higher standard deviation of results,
which demonstrates the diﬃculty of their hyperparameter tuning. The ESN models also have higher
variability of accuracy and do not perform well on
Computation time
The second aspect in which the deep learning architectures are evaluated is computational eﬃciency.
Figure 4 represents the distribution of training and
inference time for each architecture. In general, we
can see that MLP is the fastest model, closely followed by CNN. The diﬀerence between CNN and
the rest of the models is highly signiﬁcant. Within
the recurrent neural networks, LSTM and GRU perform similarly while ERNN is the slowest architecture overall. In general, the analysis for all architectures is analogous when comparing training and inference times. However, the ESN models do not meet
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
Distribution of training and inference time results for all architectures.
this standard as most of the weights of the network
are non-trainable, which results in eﬃcient training
and slow inference time. Finally, the TCN architecture presents comparable results in terms of average train and inference time with GRU and LSTM
models. However, it can be seen that TCN models
have less variability in the distribution of computation time. This indicates that a deeper architecture
with more number of layers has a smaller eﬀect in
convolutional models than in recurrent architectures.
Therefore, designing very deep recurrent models may
not be suitable under hard time constraints.
Normalized accuracy versus normalized training time of the best models of each deep learning architecture for each dataset. The lines represent a logistic
regression for each model.
Figure 5 aims to illustrate the speed/accuracy
experimental
presents accuracy against computation time for each
deep learning model over each dataset, which con-
ﬁrms the ﬁndings discussed previously. The ﬁgure
shows that MLP networks are the fastest model but
provide the worst accuracy results. It can also be
seen that the ERNN obtains accurate forecasts but
requires high computing resources. Among the recurrent networks, LSTM is the best network with very
high predictive performance and adequate computation time. CNN strikes the best time-accuracy balance among all models, with TCN being signiﬁcantly
Statistical analysis
We perform the statistical analysis, grouping the results obtained with each type of architecture, with respect to eleven diﬀerent rankings: best WAPE, mean
WAPE, WAPE standard deviation, worst WAPE,
training and inference time of best model, training and inference mean time, standard deviation of
training and inference time, and a global ranking of
the average rankings of all these comparisons. In all
cases, the p-value obtained in the Friedman test indicated that the global diﬀerences in rankings between architectures were signiﬁcant. With these results, we can proceed with the Holm-Bonferroni’s
post-hoc analysis to perform a pairwise comparison
between the models. Figure 6 presents a compact visualization of the results of the statistical analysis
carried out. It displays the ranking of models from
left to right for all the considered metrics. Furthermore, it also allows visualizing the signiﬁcance of the
observed paired diﬀerences with the plotted horizontal lines. In this critical diﬀerences (CD) diagram,
models are linked when the null hypothesis of their
equivalence is not rejected by the test. The plots in
which there are several over-lapped groups indicate
Experimental Review on Deep Learning for Time Series Forecasting
Rankings and critical diﬀerences diagram (using the Holm-Bonferroni’s post-hoc procedure) of the diﬀerent
deep learning architectures according to several performance metrics
that it is hard to detect diﬀerences between models
with similar performance.
In terms of the best WAPE forecasting accuracy, the recurrent networks lead the ranking. LSTM
ranks ﬁrst, providing the best precision for most of
the datasets, closely followed by GRU. The convolutional models, TCN and CNN, obtain rank four
and ﬁve respectively. However, the CD diagram tells
that the statistical diﬀerences among the best models
obtained for each type of architecture are not significant, except for the MLP. This fact indicates that,
when the optimal hyperparameter conﬁguration is
found, all these architectures can achieve similar performance for TSF.
When analyzing the mean WAPE ranking, the
results are completely diﬀerent. In this case, CNN
ranks ﬁrst, with LSTM in second place. This indicates that these two models are less sensitive to the
parametrization, hence being the best alternatives
to reduce the high cost of performing an extensive
grid search. This fact is further supported by the
WAPE standard deviation diagram, in which CNN
and LSTM also occupy the ﬁrst positions. The MLP
models logically have a low standard deviation given
their simplicity. The diﬀerence in ranking between
CNN and LSTM is greater in the standard deviation,
conﬁrming that CNN performs well under a wider
range of conﬁgurations. We also notice that ERNN
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
provides the worst performance in terms of the distribution of results, even below the MLP. This suggests that the parametrization of ERNNs is complex,
hence being the less recommended method amongst
all the studied.
With regard to the eﬃciency, the conclusions
obtained analyzing training and inference times were
similar in both cases. Therefore, to simplify the visualization, we only display the rankings referring
to the training time. Despite its poorer forecasting
accuracy, the MLP ranks ﬁrst in computational ef-
ﬁciency. MLP networks are the fastest models and
also present a very low training-time standard deviation. CNN models appear as the most eﬃcient DL
model in all rankings, with ESN in third place. It can
also be seen that there are diﬀerences between the
ﬁrst time diagram (training time of the model with
best performing hyperparameter conﬁguration) and
the second (mean training time among all conﬁgurations). LSTM has a much lower rank in terms of average computation time and standard deviation, which
indicates that designing very deep LSTM models is
very costly and not convenient for real-time applications. Furthermore, it is worth mentioning that CNN
seems a better option than TCN for univariate TSF.
Although TCNs are better designed for time series,
CNN models have shown to provide similar performance in terms of best WAPE results. Furthermore,
CNNs have shown to be more computationally eﬃcient and simpler in terms of ﬁnding a good hyperparameter conﬁguration.
With all these analyses in mind, it can be concluded that CNN and LSTM are the most suitable
alternatives to approach these TSF problems, as it
is displayed in the last diagram (ranking of average
rankings). While LSTM provides the best forecasting accuracy, CNNs are more eﬃcient and suﬀer less
variability of results. This accuracy versus eﬃciency
trade-oﬀimplies that the most convenient model will
be dependent on the problem requirements, time
constraints, and the objective of the researcher.
Model architecture conﬁguration
Designing the most appropriate architecture for the
studied deep learning models is a very complex task
that requires considerable expertise. Therefore, for
each architecture, we analyze the results obtained
with the diﬀerent parameters in terms of the number of layers and neurons, the characteristics of convolutional and recurrent units, etc. We present the
accuracy results tables with the architecture conﬁgurations ordered by WAPE average rankings. Note
that, in this analysis, the rankings are independent
for each of the seven types of networks. Each model
conﬁguration is trained several times with diﬀerent
training hyper-parameters. Given the large number
of possibilities, for simplicity, we only report the best
WAPE results for each conﬁguration. Among these
best results, we display in the tables the four topranked and the three worst conﬁgurations (among
the best WAPE) for each type of model. The complete results tables are provided in the online appendix.85
Multi-Layer Perceptron (MLP)
Table 12 presents the best results for each conﬁguration of the MLP networks. The top-ranked MLP
model is composed of 5 hidden layers, with a total of
80 neurons. The results in the table conﬁrm the ﬁndings that can be read in the literature, given that the
design of MLP is a well-studied ﬁeld. They show that
the increase of hidden neurons does not imply better performance. In fact, the most complex model,
an MLP with 320 neurons distributed in 5 layers, is
positioned penultimate, obtaining worse predictions
than a simple network of one layer of 8 neurons.
WAPE results of the best
MLP architecture conﬁgurations.
Mean ranks
Hidden layers
 
 
 
Recurrent Neural Networks
Among the recurrent architectures, the best performance was obtained with LSTM models. Table 13
presents the WAPE metrics obtained for the diﬀerent
LSTM model conﬁgurations. The best results were
obtained with two stacked layers of 32 units which returns the complete sequence before the output dense
Since all recurrent networks have the same parameters, we present a global comparison in Figure
Experimental Review on Deep Learning for Time Series Forecasting
Distribution of WAPE ranks obtained for each recurrent architecture with the diﬀerent conﬁgurations studied.
The red dot represents the mean average.
7. This plot shows the results of the diﬀerent RNN
architectures depending on the three main parameters: number of layers, units, and whether sequences
are returned. On average, we can observe that all
recurrent models tend to obtain better predictions
with a small number of units. When analyzing the
number of stacked layers parameter, we can observe
that LSTM, ERNN, and GRU have similar behavior,
obtaining worse average results when it is increased.
However, ESN models present the opposite pattern,
with the best results obtained with 4 layers. With regard to returning the complete sequence or just the
last output, it can clearly be seen that ESN models achieve better performance when returning the
whole sequence.
WAPE results of the best LSTM architecture conﬁgurations.
Mean ranks
Num. Layers
Return sequence
Convolutional Neural Networks
Table 14 reports the ranking of the best CNN model
conﬁgurations. It can be noticed that the prediction
accuracy is proportional to the number of convolutional layers. The best results have been obtained
with models with four layers while the single-layer
models are at the bottom of the ranking. However,
regarding the number of ﬁlters, there is no signiﬁcant diﬀerence at ﬁrst sight. It is also worth mentioning that the best predictions have been obtained
from models without max-pooling, suggesting that
this popular image-processing operation is not suitable for time series forecasting.
WAPE results of the best CNN architecture conﬁgurations.
Mean ranks
N. Filters
Pool factor
WAPE results of the best TCN architecture
conﬁgurations.
Mean ranks
N. Filters
Return seq.
 
 
 
 
 
 
Table 15 shows results obtained with the diﬀerent conﬁgurations of TCN models. The best network
overall is designed with one single TCN layer with 4
dilated convolutional layers, a kernel of length 6, and
64 ﬁlters that do not return the complete sequence
before the output layer. These results indicate a clear
pattern for designing TCN models. Firstly, it should
be noted that single-layer models with few dilated
layers outperform more-complex models. Concerning
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
Example of how LSTM results are gathered to compare the number of layers parameter with
the Wilcoxon signed-ranked test.
Parameters
Fixed parameter
Return Seq.
the kernel size, the models with a larger kernel size
provide the best predictions. Furthermore, according to the results, it is revealed that returning the
complete sequence before the last dense layer may
increases the complexity of the model too much, resulting in a performance downturn.
Statistical comparison of
architecture conﬁgurations
Using the results presented in the previous subsection, we present a statistical analysis comparing the
studied architecture conﬁgurations for each type of
model. This study aims to conﬁrm the ﬁndings that
were discussed previously regarding the best parameter choices. The method used for this comparison
is the paired Wilcoxon signed-rank test. Table 16 illustrates an example of how results are gathered for
the comparison. Given a certain parameter, we compare the results between each pair of possible values, keeping ﬁxed the rest of the parameters. It is
worth mentioning that, given the large sample distribution of possible architecture conﬁgurations, the
results presented here are both reliable and signiﬁcant. For instance, in the RNN case, a total of 108
samples are compared for parameters with 2 possible
values, while 72 samples are compared for parameters with 3 values.
Table 17 presents the ﬁndings obtained from
the Wilcoxon test, providing the best conﬁgurations
found for each type of architecture. The numbers
with ** indicate that it is the best value with a
signiﬁcant statistical diﬀerence compared to the rest
(p < 0.05). We indicate with * that there is a certain
tendency suggesting that it is better to choose that
parameter (p < 0.2), and with = when there are no
signiﬁcant diﬀerences between choosing any of the
possible parameters.
Best architecture conﬁguration for each
type of model. Values with ** are signiﬁcantly better (p-value ≤0.05), * suggests it is a better choice
although not signiﬁcant (p-value ≤0.2), and = means
that no diﬀerences were found among the possible values.
Return Sequence
Return Sequence
32**, 64**
Return sequence
Return Sequence
Pooling factor
Return Sequence
For the recurrent networks, it can be seen that a
lower number of stacked layers (one or two) provides
better performance. The only exception is the ESN,
for which the optimal value is four layers. With regard to the number of units, the values 32 or 64 are
better than 128 for ERNN, GRU, and ESN, while
Experimental Review on Deep Learning for Time Series Forecasting
Best training hyperparameter values for each architecture. Values with ** are signiﬁcantly better
(p-value ≤0.05), * suggests it is a better choice although not statistically signiﬁcant (p-value ≤0.2), and =
means that no diﬀerences were found among the possible values.
Architecture
Batch size
Past History factor
Learning Rate
Normalization Method
no diﬀerences are found for the LSTM. Furthermore,
the statistical test conﬁrms that ESN achieves significantly better performance when returning the whole
sequence. It also suggests that ERNN works better
without returning the sequences, while the performance of LSTM and GRU is not aﬀected by this parameter. In the case of LSTM models, it can be seen
that the parameter choice has a minimal impact on
the accuracy results. This ﬁnding further supports
the fact that LSTMs are the most robust type of recurrent network. As was seen in Section 4.3, LSTM
models were the best in terms of mean WAPE, showing less variability of results. Since no important difference in performance was found among the parameters, ﬁnding a high performing architecture design
is easier.
For CNN models, the Wilcoxon test conﬁrms
that stacking four layers is signiﬁcantly better than
using just one and two. The number of ﬁlters does
not aﬀect performance, while it is clear that maxpooling is not recommended for these TSF problems.
In the case of TCNs, the test indicates that singlelayer models outperform more-complex models. Additionally, it is also conﬁrmed that the models with a
larger kernel size provide signiﬁcantly better predictions. Concerning the number of ﬁlters and dilated
layers, no statistical diﬀerence was found. Furthermore, the results suggest that returning the complete
sequence before the last dense layer may increase the
complexity of the model excessively, resulting in performance degradation.
Analysis of the training
parameters
The grid search performed in the experimental study
also involved several training hyperparameters. We
use the paired Wilcoxon signed-rank test to be able
to compare the results. Table 18 summarizes the
best training parameter values for each deep learning architecture. First of all, a clear pattern that
is common to all architectures can be noted. The
best results are achieved with a low learning rate
and a small past history factor. Regarding the normalization method, we can observe that the minmax normalization method performs better for GRU
and LSTM while z-score signiﬁcantly beats the minmax method in the other architectures. Furthermore,
results also show that most architectures (ERNN,
GRU, CNN, and TCN) perform better with larger
batch sizes. Only for LSTM, the preferred value is
32, while no diﬀerence was found for MLP and ESN.
Conclusions
In this paper, we carried out an experimental review
on the use of deep learning models for time series
forecasting. We reviewed the most successful applications of deep neural networks in recent years, observing that recurrent networks have been the most
extended approach in the literature. However, convolutional networks are increasingly gaining popularity
due to their eﬃciency, especially with the development of temporal convolutional networks.
Furthermore, we conducted an extensive experimental study using seven popular deep learning
architectures: multilayer perceptron (MLP), Elman
recurrent neural network ERNN, long-short term
memory (LSTM), gated recurrent unit (GRU), echo
state network (ESN), convolutional neural network
(CNN) and temporal convolutional network (TCN).
We evaluated the performance of these models, in
terms of accuracy and eﬃciency, over 12 diﬀerent
forecasting problems with more than 50000 time series in total. We carried out an exhaustive search
of architecture conﬁguration and training hyperparameters, building more than 38000 diﬀerent models.
Moreover, we performed a thorough statistical analysis over several metrics to assess the diﬀerences in
the performance of the models.
The conclusions obtained from this experimental study are summarized below:
Pedro Lara-Ben´ıtez, Manuel Carranza-Garc´ıa and Jos´e C. Riquelme
• Except MLP, all the studied models obtain
accurate predictions when parametrized correctly. However, the distribution of results of
the models presented signiﬁcant diﬀerences.
This illustrates the importance of ﬁnding an
optimal architecture conﬁguration.
• Regardless of the depth of their hidden
blocks, MLP networks are unable to model
the temporal order of the time series data,
providing poor predictive performance.
• LSTM obtained the best WAPE results followed by GRU. However, CNN outperforms
them in the mean and standard deviation
of WAPE. This indicated that convolutional
architectures are easier to parameterize than
the recurrent models.
• CNNs strike the best speed/accuracy tradeoﬀ, which makes them more suitable for realtime applications than recurrent approaches.
• LSTM networks obtain better results with a
lower number of stacked layers, contrary to
the GRU architecture. Within recurrent networks, the number of units was not important and returning the complete sequences
only proved useful in the ESN models.
• CNNs require more stacked layers to improve accuracy, but without using maxpooling operations. For TCNs, a single-block
with a larger kernel size is recommended.
• With regard to the training hyperparameters, we discovered that lower values of
past history and learning rates are a better
choice for these deep learning models. CNNs
perform better with z-score normalization,
while min-max is suggested for LSTM.
In future works, we aim to study the application of these deep learning techniques for time series
forecasting in streaming. In this real-time scenario, a
more in-depth analysis of the speed versus accuracy
trade-oﬀwill be necessary. Another relevant work
would be to study the best deep learning models depending on the nature of the time-series data and
other characteristics such as the length or the forecasting horizon. It would also be important to focus
future experiments on tuning the training hyperparameters, once that the most suitable model architectures have been found. Other possible extensions
of this study are the analysis of regularization techniques in deep networks and performing the experimental study over multivariate time series. Furthermore, future research should address current trends
in the literature such as ensemble models to enhance
accuracy or transfer learning to reduce the burden
of high training times. Future eﬀorts should also be
focused on building a larger high-quality forecasting
database that could serve as a general benchmark for
validating novel proposals.
This research has been funded by FEDER/Ministerio
de Ciencia, Innovaci´on y Universidades – Agencia Estatal de Investigaci´on/Proyecto TIN2017-88209-C2
and by the Andalusian Regional Government under
the projects: BIDASGRI: Big Data technologies for
Smart Grids (US-1263341), Adaptive hybrid models
to predict solar and wind renewable energy production (P18-RT-2778).
Acknowledgments
We are grateful to NVIDIA for their GPU Grant Program that has provided us high-quality GPU devices
for carrying out the study.