One Class Classiﬁcation for Anomaly Detection:
Support Vector Data Description Revisited
Eric J. Pauwels and Onkar Ambekar
Centrum Wiskunde & Informatica CWI,
Science Park 123, 1098 XG Amsterdam, The Netherlands
 
 
Abstract. The Support Vector Data Description (SVDD) has been introduced to address the problem of anomaly (or outlier) detection. It
essentially ﬁts the smallest possible sphere around the given data points,
allowing some points to be excluded as outliers. Whether or not a point
is excluded, is governed by a slack variable. Mathematically, the values
for the slack variables are obtained by minimizing a cost function that
balances the size of the sphere against the penalty associated with outliers. In this paper we argue that the SVDD slack variables lack a clear
geometric meaning, and we therefore re-analyze the cost function to get
a better insight into the characteristics of the solution. We also introduce
and analyze two new deﬁnitions of slack variables and show that one of
the proposed methods behaves more robustly with respect to outliers,
thus providing tighter bounds compared to SVDD.
Key words: One class classiﬁcation, outlier detection, anomaly detection, support vector data description, minimal sphere ﬁtting
Introduction
In a conventional classiﬁcation problem, the aim is to ﬁnd a classiﬁer that optimally separates two (or more) classes. The input to the problem is a labelled
training set comprising a roughly comparable number of exemplars from each
class. Howerever, there are types of problems in which this assumption of (approximate) equi-distribution of exemplars no longer holds. The prototypical example that springs to mind is anomaly detection. By its very deﬁnition, an
anomaly is a rare event and training data will more often than not contain very
few or even no anomalous exemplars. Furthermore, anomalies can often only be
exposed when looked at in context, i.e. when compared to the majority of regular
points. Anomaly detection therefore provides an example of so-called one-class
classiﬁcation, the gist of which amounts to the following: Given data points that
all originated from a single class but are possibly contaminated with a small
number of outliers, ﬁnd the class boundary.
In this paper we will focus on an optimization approach championed by Tax
 and Sch¨olkopf et.al. . The starting point is a classical problem in quadratic
Support Vector Data Description Revisited
programming: given a set of n points x1, . . . , xn in a p-dimensional space, ﬁnd
the most tightly ﬁtting (hyper)sphere that encompasses all. Denoting the centre
of this sphere by a and its radius by R, this problem boils down to a constrained
minimization problem:
subject to
∥xi −a∥2 ≤R2,
∀i = 1, . . . , n.
However, if the possibility exist that the dataset has been contaminated with
a small number of anomalies, it might prove beneﬁcial to exclude suspicious
points from the sphere and label them as outliers. This then allows one to shrink
the sphere and obtain a better optimum for the criterion in eq.(1). Obviously,
in order to keep the problem non-trivial, one needs to introduce some sort of
penalty for the excluded points. In and the authors take their cue from
standard support vector machines (SVM) and propose the use of non-negative
slack variables meant to relax the inclusion criterion in eq.(1). More precisely,
for each point they introduce a variable ξi ≥0 such that
∥xi −a∥2 ≤R2 + ξi.
This relaxation of the constraints is then oﬀset by adding a penalty term to the
cost function:
ζ(R, a, ξ) := R2 + C
The constant C is a (pre-deﬁned) unit cost that governs the trade-oﬀbetween
the size of the sphere and the number of outliers. After these modiﬁcations the
authors in arrive at the following constrained optimization problem: given
n data points x1, . . . , xn and a pre-deﬁned unit cost C, ﬁnd
a,R,ξ {R2 + C
∀i = 1, . . . , n : ∥xi −a∥2 ≤R2 + ξi,
ξi ≥0. (3)
The resulting data summarization segregates “regular” points on the inside from
“outliers” on the outside of the sphere and is called support vector data description (SVDD).
Aim of this paper The starting point for this paper is the observation that
the slack variables in eq.(3) lack a straightforward geometrical interpretation.
Indeed, denoting di = ∥xi −a∥, it transpires that the slack variables can be
represented explicitly as:
i −R2 if di > R,
However, except in the case where the dimension of the ambient space (p) equals
two or three, these slack variables don’t have an obvious geometric interpretation.
Support Vector Data Description Revisited
It would therefore be more natural to set the slack variable equal to ϕi = (di −
R)+ upon which the relaxed constraints can be expressed as:
∥xi −a∥≤R + ϕi,
The corresponding penalized function would then take the form:
ζ2(a, R) := R2 + C
(Notice that we can drop ϕ from the list of arguments as it can be computed as
soon as a and R are speciﬁed). For lack of space we will not be able to study this
alternative in detail. Suﬃce it to say that the solution includes non-acceptable,
trivial conﬁgurations. However, there is no obvious reason why the variables in
the cost function should appear as squares. This suggests that we also should
look at a second — completely linear — alternative:
ζ1(a, R) := R + C
The goal of this paper is therefore twofold. Firstly, we want to re-analyze the
original optimization problem (3) as introduced in and . However, in contradistinction to these authors, we will refrain from casting it in its dual form,
but focus on the primal problem instead. This will furnish us with additional
insights into the geometry and behaviour of the solutions. Secondly, we will then
extend this analysis to the alternative ζ1 (see eq. 7) mentioned above and conclude that, in some respects, it is preferable to the original. In fact the diﬀerence
between these two solutions is not unlike the diﬀerence in behaviour between
the mean and median (for a quick preview of this result, we suggest to have a
peek at Fig. 2).
Related work
Although lack of space precludes a comprehensive revision of
all related work, it is fair to say that after the seminal papers most activity focussed on applications, in particular clustering, see e.g. . In particular,
a lot of research has gone into the appropriate choice of the Gaussian kernel
size when using the kernelized version of this technique , as well as eﬃcient
methods for cluster labeling. In a diﬀerent direction of generalization is pursued: rather than mapping the data into a high-dimensional feature space, the
spherical constraints are relaxed into ellipsoidal ones in the original data space,
thereby side-stepping the vexing question of kernel-choice.
Support Vector Data Description Revisited
In this section we will re-analyze the cost function (3) which lies at the heart of
the SVDD classiﬁer. However, rather than recasting the problem in its dual form
(as is done in and ), we will focus directly on the primal problem. This
allows us to gain additional insight in the qualitative behaviour of the solutions
(cf. section 2.2) as well as sharpen the bounds on the unit cost C (see item 3 of
Support Vector Data Description Revisited
Outlier Detection as an Optimization Problem
Recall from (3) that the anomaly (a.k.a. outlier) detection problem has been
recast into the following constrained optimization problem. As input we accept
n points x1, . . . , xn in p-dimensional space, and some ﬁxed pre-deﬁned unit cost
C. In addition, we introduce a vector ξ = (ξ1, . . . , ξn) of n slack variables in
terms of which we can deﬁne the cost function
ζ(a, R, ξ) := R2 + C
The SVDD outlier detection (as introduced in and ) now amounts to ﬁnding
the solution to the following constrained minimization problem:
a,R,ξ ζ(a, R, ξ)
∀i = 1, . . . , n : ∥xi −a∥2 ≤R2 + ξi,
If we denote the distance of each point xi to the centre a as di = ∥xi −a∥
then it’s straightforward to see that the slack variables can be expliciﬁed as
i −R2)+, where the ramp function x+ is deﬁned by:
x if x ≥0,
0 if x < 0,
This allows us to rewrite the cost function in a more concise form:
ζ(a, R) = R2 + C
Notice that the cost function is now a function of a and R only, with all other
constraints absorbed in the ramp function x+. From this representation it immediately transpires that ζ is continuous in its arguments, albeit not everywhere
diﬀerentiable.
Properties of the solution
Proposition 1 The solution of the (unconstrained) optimization problem
(a∗, R∗) := arg min
a,R ζ(a, R)
ζ(a, R) = R2 + C
has the following qualitative properties:
1. Behaviour of the marginal functions:
(a) Keeping R ﬁxed, ζ is a convex function of the centre a.
(b) Keeping a ﬁxed, ζ is piecewise quadratic in R.
Support Vector Data Description Revisited
2. Location of the optimal centre a∗:
The centre of the optimal sphere
can be speciﬁed as a weighted mean of the data points
if di > R∗
0 ≤θi ≤1 if di = R∗
if di < R∗.
3. Dependency on penalty cost C:
The value of the unit cost C determines the qualitative behaviour of the
solution. More precisely:
– If C < 1/n then the optimal radius R∗will be zero, i.e. all points will
reside outside of the sphere.
– If C ≥1/2 all points will be enclosed, and the sphere will be the minimum
volume enclosing sphere.
– For values 1/n ≤C ≤1/2, the qualitative shape of the solution changes
whenever C = 1/k for k = 2, 3, . . . n.
1. Behaviour of the marginal functions
– 1.a: Keeping R ﬁxed, ζ is a convex function of the centre a.
Assuming that in eq. (12) the radius R and cost C are ﬁxed, the dependency
of the cost functional is completely captured by second term:
i −R2, 0}.
Convexity of ζ as a function of a is now immediate as each d2
∥xi −a∥2 is convex and both the operations of maximization and summing
are convexity-preserving.
– 1.b: Keeping a ﬁxed, ζ is piecewise quadratic in R.
Introducing
the auxiliary binary variables:
1 if di > R,
0 if di ≤R,
for a ﬁxed,
allows us to rewrite (d2
i −R2)+ ≡bi(R)(d2
i −R2), from which
Support Vector Data Description Revisited
ζ(R) = β(R) R2 + Cγ(R).
β(R) := 1 −C
As it is clear that the coeﬃcients β and γ are piecewise constant, producing a
jump whenever R grows beyond one of the distances di, it follows that ζ(R)
is (continuous) piecewise quadratic. More precisely, if we assume that the
points xi have been (re-)labeled such that d1 ≡∥x1 −a∥≤d2 ≡∥x2 −a∥≤
. . . ≤dn ≡∥xn −a∥, then for 0 ≤R < d1, all bi(R) = 1 and hence
β(R) = 1 −nC. On the interval d1 ≤R < d2 we ﬁnd that b1 = 0 while
b2 = b3 = . . . bn = 1 implying that β(R) = 1 −(n −1)C, and so on. So
we conclude that β is a piecewise constant function, making an upward
jump of size C whenever R passes a di. This is illustrated in Fig. 1 where
the bottom ﬁgure plots the piecewise constant coeﬃcient β for two diﬀerent
values of C, while the corresponding ζ functions are plotted in the top graph.
Clearly, every β-plateau gives rise to a diﬀerent quadratic part of ζ. More
importantly, as long as β(R) < 0 the resulting quadratic part in ζ is strictly
decreasing. Hence we conclude that the minimum of ζ occurs at the point
R∗= arg min ζ(R) where β jumps above zero. Indeed, at that point, the
corresponding quadratic becomes strictly increasing, forcing the minimum
to be located at the jump between the two segments.
From the above we can also conclude that the optimal radius R∗= arg min ζ(R)
is unique except when C = 1/k for some integer 1 ≤k ≤n. In those instances
there will be an R-segment on which P bi = k, forcing the corresponding
β coeﬃcient to vanish. This then gives rise to a ﬂat, horizontal plateau of
minimal values for the ζ function. In such cases we will pick (arbitrarily) the
maximal possible value for R, i.e.: R∗:= sup{R : ζ(R) is minimal}. Finally,
we want to draw attention to the fact that the optimal sphere always passes
through at least one data point, as the optimal radius R∗coincides with at
least one di.
2. Location of the optimal centre Earlier we pointed out that the ζ(a, R) is
continuous but not everywhere diﬀerentiable. This means that we cannot simply
insist on vanishing gradients to determine the optimum, as the gradient might
not exist. However, we can take advantage of a more general concept that is
similar in spirit: subgradients. Recall that for a diﬀerentiable convex function f,
the graph of the function lies above every tangent. Mathematically this can be
reformulated by saying that at any x:
f(y) ≥∇f(x) · (y −x),
If f is not necessarily diﬀerentiable at x then we will say that any vector gx is a
subgradient at x if:
f(y) ≥gx · (y −x),
Support Vector Data Description Revisited
The collection of all subgradients at a point x is called the subdiﬀerential of f at
x and denoted by ∂f(x). Notice that the subdiﬀerential is a set-valued function!
It is now easy to prove that the classical condition for x∗to be the minimum of
a convex function f (i.e. ∇f(x∗) = 0) can be generalized to non-diﬀerentiable
functions as:
0 ∈∂f(x∗).
To apply the above the problem at hand, we ﬁrst note that the subdiﬀerential
of the ramp function x+ is given by:
 if x = 0
(i.e. set-valued)
as at x = 0 any straight line with slope between 0 and 1 will be located under
the graph of the ramp function. To streamline notation, we introduce (a version
of) the Heaviside stepfunction
0 ≤h ≤1 if x = 0
(i.e. set-valued)
To forestall confusion we point out that, unlike when used as a distribution, this
deﬁnition of the Heaviside function insists its value at the origin is between zero
and one. Using this convention, we have the convenient shorthand notation:
∂x+ = H(x).
Computing the subgradients (for convenience we will drop the notational distinction between standard- and sub-gradients) we obtain:
∂R = 2R −2RC
i −R2) (xi −a)
where we used the well-known fact:
i ) = ∇a||xi −a||2 = ∇a(xi · xi −2xi · a + a · a) = −2(xi −a).
Insisting that zero is indeed a subgradient means that we need to pick values
hi := H(d2
i −R2) such that:
0 ∈∂ζ/∂R ⇒
hi(xi −a) = 0
Support Vector Data Description Revisited
The above characterization allows us to draw a number of straightforward
conclusions (for notational convenience we will drop the asterisk to indicate
optimality, and simply write a∗= a and R∗= R):
1. Combining eqs.(28) and (29) it immediately transpires that
or again, and more suggestively,
Furthermore, the sums in the RHS can be split into three parts depending
on whether a point lies inside (di < R), on (di = R) or outside (di > R)
the sphere, e.g.:
where 0 ≤θi ≡H(di −R = 0) ≤1. Hence:
This representation highlights the fact that the centre a is a weighted mean
of the points on or outside the sphere (the so-called support vectors (SV),
 ), while the points inside the sphere exert no inﬂuence on its position.
Notice that the points outside of the sphere are assigned maximal weight.
2. If we denote the number of points inside, on and outside the sphere by
nin, non and nout respectively, then by deﬁnition #SV = non+nout. Invoking
eq. (28) and combining this with the fact that 0 ≤θi ≤1 it follows that
Hence, since 0 ≤θi ≤1 it can be concluded that
non + nout = #SV
Put diﬀerently:
(a) 1/C is a lower bound on the number of support vectors (#SV ).
(b) 1/C is an upper bound on the number of outliers (nout).
The same result was obtained by Sch¨olkopf , who introduced the parameter
ν = 1/nC as a bound on the fraction of support vectors (#SV/n) and
outliers (nout/n).
Support Vector Data Description Revisited
3. Dependency on unit-cost C
In this section we try to gain further
insight into how the cost function determines the behaviour of the optimum. Let
us assume that we have already minimized the cost function (11) and identiﬁed
the optimal centre a∗and corresponding radius R∗. For convenience’s sake, we
again assume that we have relabeled the data points in such a way that the
distances di = ∥xi −a∗∥are ordered in ascending order: 0 ≤d1 ≤d2 ≤. . . ≤dn.
We now investigate how the total cost ζ depends on the unit cost C in the
neighbourhood of this optimum.
Figure 1 nicely illustrate the inﬂuence of the unit cost C on the qualitative behaviour of the optimal radius R∗. Indeed, increasing C slightly has the
following eﬀects on the β-function:
– The values of the coeﬃcients hi will change (cf. eq. 28) which in turn will
result in a shift of the optimal centre (through eq. 31). As a consequence
the distances di to the data points xi will slightly change, resulting in slight
shifts of the step locations of the β-function. Since the position of the optimal
radius R∗coincides with one of these step locations (viz. the jump from a
negative to a positive β-segment), increasing C slightly will typically induces
small changes in R∗. However, from time to time, one will witness a jump-like
change in R∗as explained below.
– Since the size of a β-step equals the unit cost, slightly increasing C will
push the each β-segment slightly downwards as the maximum of β remains
ﬁxed at one (i.e. limR→∞β(R) = 1). As a consequence, β-segments that are
originally positive, will at some point dip below the X-axis. As this happens,
the corresponding quadratic segment will make the transition from convex
and increasing to concave and decreasing forcing the minimum R∗to make
This now allows us to draw a number of straightforward conclusions about the
constraints on the unit cost C.
– The ﬁrst segment of the β function occurs for 0 ≤R < d1. On this segment
bi = 1 for all i = 1, . . . , n and hence β(R) = 1−C P
i bi = 1−nC. If C < 1/n,
then β > 0 on this ﬁrst segment and hence on all the subsequent ones. In
that case, ζ(R) is strictly increasing and has a single trivial minimum at
R∗= 0. Put diﬀerently, in order to have a non-trivial optimization problem,
we need to insist on C ≥1/n (cf. item 3 in proposition 1 ). icicic
– If, on the other hand, we want to make sure that there are no outliers, then
the optimum R∗has to coincide with the last jump, i.e. R∗= dn. This implies
that the quadratic segment on the interval [dn−1, dn] has to be decreasing (or
ﬂat), and consequently β(R) = 1 −C P
i bi ≤0. Since on this last segment
we have that all bi vanish except for bn, it follows that β(R) = 1 −C ≤0
(and vice versa). We therefore conclude that for values C ≥1 there will be
no outliers.
This result was also obtained in but we can now further tighten the
above bound by observing that when the optimal sphere encloses all points,
Support Vector Data Description Revisited
it has to pass through at least two points (irrespective of the ambient dimension). This implies that dn−1 = dn and the ﬁrst non-trivial interval preceding
dn is in fact [dn−2, dn−1]. Rerunning the above analysis, we can conclude that
C ≥1/2 implies that all data points are enclosed.
– Using the same logic, if we insist that at most k out of n are outside the circle,
we need to make sure that the quadratic on [dn−k, dn−k+1] is convex and
increasing. On that interval we know that P
i bi = k. Hence we conclude that
on this interval β(R) = 1 −kC > 0 or again: C < 1/k. Hence, ν = 1/nC >
k/n is an upper bound on the fraction of points outside the descriptor (cf.
– In fact, by incorporating some straightforward geometric constraints into the
set-up we can further narrow down the diﬀerent possible conﬁguration. As a
simple example, consider the case of a generic 2-dimensional data set. The
sphere then reduces to a circle and we can conclude that – since we assume
the data set to be generic – the number of points on the optimal circle (i.e.
non) either equals 1 (as the optimal circle passes through at least one point),
2 or 3. Indeed, there is a vanishing probability that a generic data set will
have 4 (or more) co-circular points (points on the same circle). In this case
we can rewrite the Sch¨olkopf inequality (35) as:
nout ≤1/C ≤nout + 3
For values C < 1/3 it then follows that
3 < 1/C ≤nout + 3
So we arrive at the somewhat surprising conclusion that if the unit cost
is less than 1/3, we are guaranteed to have at least one outlier, no matter
what the data set looks like (as long as it is generic). This is somewhat
counter-intuitive as far as the usual concept of an outlier is concerned!
This concludes the proof.
Linear Slacks and Linear Loss
Basic analysis
As announced earlier, this section busies itself with minimizing the linear function
ζ1(a, R) := R + C
subject to
∀i : di ≡∥xi −a∥≤R + ϕi,
Again, we absorb the constraints into the function by introducing the ramp
ζ1(a, R) = R + C
Support Vector Data Description Revisited
Taking subgradients with respect to a and R yields:
H(di −R) (xi −a)
since it is straightforward to check that:
∇a(di) = ∇a
(∥xi −a∥2) = −(xi −a)
Equating the gradient to zero and re-introducing the notation hi = H(di −R)
we ﬁnd that the optimum is characterized by:
∥xi −a∥= 0
Notice how eq. (38) is identical to eq. (28) whereas eq. (39) is similar but subtly
diﬀerent from eq.(29). In more detail:
1. Once again we can make the distinction between the nin points that reside
inside the sphere, the non points that lie on the sphere and the nout points
that are outside the sphere. The latter two categories constitute the support
vectors: #SV = non + nout. Hence,
θi + nout.
So also in this case we get (cf. eq. (35)):
2. Comparing eqs. (39) and (29) we conclude that we can expect the solution
corresponding to linear loss function (36) to be more robust with respect to
outliers. Indeed, in Section 2 we’ve already argued that eq. (29) implies that
the sphere’s centre is the (weighted) mean of the support vectors. Noticing
that in eq. (39) the vectors have been substituted by the corresponding unit
vectors reveals that in the case of a linear loss function the centre can be
thought of as the weighted median of the support vectors. Indeed, for a set of
1-dimensional points x1, . . . , xn the median m is deﬁned by the fact that it
separates the data set into two equal parts. Noticing that (xi−m)/|xi−m| =
Support Vector Data Description Revisited
sgn(xi −m) equals −1, 0 or 1 depending on whether xi < m, xi = m or
xi > m respectively, we see that the median can indeed be deﬁned implicitly
∥xi −m∥= 0.
This characterization of the median has the obvious advantage that the
generalization to higher dimensions is straightforward . The improved robustness of the solution of the linear cost function (36) with respect to the
original one (7) is nicely illustrated in Fig. 2.
Further properties
To gain further insight in the behaviour of solutions we once again assume that
the centre of the sphere has already been located, so that the cost function
depends solely on R. We also assume that the points have been labeled to produce
an increasing sequence of distances di = ∥xi −a∥. Hence:
ζ1(R) = R+C
ϱ(di−R) = R+C
(di−R)H(di−R) = R+C
where we have once again re-introduced the binary auxiliary variables bi deﬁned
in eq.(16) Rearranging the terms we arrive at:
which elucidates that the function is piecewise linear, with a piecewise constant
slope equal to 1 −C P bi. For notational convenience, we deﬁne
β(R) = 1 −C
resulting in ζ1(R) = β(R) R+Cδ(R). Furthermore, β(0) = 1−nC and increases
by jumps of size (multiples of) C to reach 1 when R = dn. Hence the minimum
R∗is located at the distance di for which β jumps above zero.
These considerations allow us to mirror the conclusions we obtained for the
original cost function:
1. The optimal value of R∗coincides with one of the distances di which means
that the optimal circle passes through at least one of the data points.
2. The optimal value R∗changes discontinuously whenever the unit cost takes
on a value C = 1/k (for k = 2, . . . , n).
3. Non-trivial solutions exist only within the range:
For other values of C either all or no points are outliers.
4. The Sch¨olkopf bounds (35) (and the ensuing conclusions) prevail.
Support Vector Data Description Revisited
Conclusions
In this paper we re-examined the support vector data descriptor (SVDD) (introduced by and ) for one-class classiﬁcation. Our investigation was prompted
by the observation that the deﬁnition of slack variables as speciﬁed in the SVDD
approach, lacks a clear geometric interpretation. We therefore re-analyzed the
SVDD constrained optimization problem, focussing on the primal formulation,
as this allowed us to gain further insight into the behaviour of the solutions. We
applied the same analysis to two natural alternatives for the SVDD function.
The ﬁrst one turned out to suﬀer from unacceptable limitations, but the second
one produces results that are very similar to the original formulation, but enjoys
enhanced robustness with respect to outliers. We therefore think it could serve
as an alternative for the original.
Acknowledgement
This research is partially supported by the Speciﬁc Targeted Research Project (STReP) FireSense Fire Detection and Management
through a Multi-Sensor Network for the Protection of Cultural Heritage Areas from the Risk of Fire and Extreme Weather Conditions of the European Union’s 7th Framework Programme Environment (including Climate Change).