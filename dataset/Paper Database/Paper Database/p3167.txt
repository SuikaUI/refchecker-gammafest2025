JMLR: Workshop and Conference Proceedings 1–14, 2012 European Workshop on Reinforcement Learning (EWRL)
Path Integral Policy Improvement
with Covariance Matrix Adaptation
Freek Stulp
 
Cognitive Robotics, ENSTA-ParisTech, Paris, France
FLOWERS Research Team, INRIA Bordeaux Sud-Ouest, Talence, France
Olivier Sigaud
 
Institut des Syst`emes Intelligents et de Robotique, Universit´e Pierre Marie Curie CNRS UMR 7222,
Paris, France
There has been a recent focus in reinforcement learning on addressing continuous state
and action problems by optimizing parameterized policies. PI2 is a recent example of this
approach. It combines a derivation from ﬁrst principles of stochastic optimal control with
tools from statistical estimation theory. In this paper, we consider PI2 as a member of
the wider family of methods which share the concept of probability-weighted averaging
to iteratively update parameters to optimize a cost function. We compare PI2 to other
members of the same family – Cross-Entropy Methods and CMAES – at the conceptual
level and in terms of performance.
The comparison suggests the derivation of a novel
algorithm which we call PI2-CMA for “Path Integral Policy Improvement with Covariance
Matrix Adaptation”. PI2-CMA’s main advantage is that it determines the magnitude of
the exploration noise automatically.
This is a double submission with ICML2012 paper 171
1. Introduction
Scaling reinforcement learning (RL) methods to continuous state-action problems, such as
humanoid robotics tasks, has been the focus of numerous recent studies . Most of the progress in the domain
comes from direct policy search methods based on trajectory roll-outs. These direct policy
improvement methods aim at determining the parameters of a policy that minimizes a cost
function. The ‘direct’ label refers to not requiring a model of the system to do so. The
recently proposed direct ‘Policy Improvement with Path Integrals’ algorithm (PI2) is derived
from ﬁrst principles of stochastic optimal control, and is able to outperform gradient-based
RL algorithms such as REINFORCE and Natural Actor-Critic by an order of magnitude in terms of convergence speed and quality of the
ﬁnal solution .
Direct policy search approaches have frequently been used in combination with Dynamic
Movement Primitives (DMPs), where the policy is represented as a dynamical system with
an attractor point g and a forcing term consisting of a set of basis functions multiplied with a
parameter vector θ which determines the shape of the motion. A variety of robotic tasks has
c⃝2012 F. Stulp & O. Sigaud.
Stulp Sigaud
been learned with direct policy search and DMPs, such as baseball batting , dart throwing and table tennis , pouring water , and various manipulation tasks .
What sets PI2 apart from other direct policy improvement algorithms is its use of
probability-weighted averaging to perform a parameter update, rather than basing this update on an estimate of the gradient. Interestingly enough, the CMAES evolutionary optimization algorithm, which is considered to be state-of-the-art in black-box optimization, is
also based on probability-weighted averaging. The similar Cross-Entropy Method (CEM)
is also based on this principle. It is striking that these algorithms, despite having been
derived from very diﬀerent principles, have converged to almost identical parameter update
rules. To the best of our knowledge, this paper is the ﬁrst to make this relationship between the three algorithms explicit. This hinges on 1) re-interpreting CEM as performing
probability-weighted averaging; 2) demonstrating that CEM is a special case of CMAES,
by setting certain CMAES parameters to extreme values.
A further contribution of this paper is that we conceptually and empirically investigate the diﬀerences and similarities between PI2, CEM, and CMAES. These comparisons
suggest a new algorithm, PI2-CMA, which has the algorithm structure of PI2, but uses
covariance matrix adaptation as found in CEM and CMAES. A practical contribution of
this paper is that we demonstrate how PI2-CMA automatically determines the appropriate
exploration magnitude, the only parameter which is not straightforward to tune in PI2.
The rest of this paper is structured as follows: in the next section, we present the CEM,
CMAES and PI2 algorithms, and discuss further related work. In Section 3, we compare
these three algorithms with respect to their method of generating exploration noise, their
deﬁnition of eliteness, and their covariance matrix update rules. Each of these comparisons
is backed up by empirical evaluations on a 10-DOF simulated arm. From these comparisons,
we derive a new algorithm called PI2-CMA in Section 3.4. We conclude with Section 4.
2. Background and Related Work
In this section, we describe the Cross-Entropy Method (CEM), Covariance Matrix Adaptation – Evolutionary Strategy (CMAES) and Policy Improvement with Path Integrals (PI2).
All these algorithms aim at optimizing a parameter vector θ with respect to a cost function
2.1. Cross-Entropy Method (CEM)
Given a n-dimensional parameter vector θ and a cost function J : Rn 7→R, the Cross-
Entropy Method (CEM) for optimization searches for the global minimum with the following steps: Sample – Take K samples θk=1...K from a distribution. Sort – Sort the samples
in ascending order with respect to the evaluation of the cost function J(θk). Update –
Recompute the distribution parameters, based only on the ﬁrst Ke ‘elite’ samples in the
sorted list. Iterate – return to the ﬁrst step with the new distribution, until costs converge,
or up to a certain number of iterations.
Path Integral Policy Improvement with Covariance Matrix Adaptation
Cross-Entropy Method (one iteration)
θk=1...K ∼N(θ, Σ)
Jk = J(θk)
θk=1...K ←sort θk=1...K w.r.t Jk=1...K
(θk −θ)(θk −θ)⊺
A commonly used distribution is a multi-variate Gaussian distribution N(θ, Σ) with
parameters θ (mean) and Σ (covariance matrix), such that these three steps are implemented
as in (1)-(5). An example of one iteration of CEM is visualized in Figure 1, with a multivariate Gaussian distribution in a 2D search space. Note that in (5), the unbiased estimate
of the covariance is acquired by multiplying with
Ke , rather than
Ke−1, because we know
the true mean of the distribution to be θ.
Throughout this paper, it will be useful to think of CEM as performing probabilityweighted averaging, where the elite samples have probability 1/Ke, and the non-elite have
probability 0. With these values of Pk, (1)-(5) can be rewritten as in the left algorithm
in Table 1. Here we use QKe/K to denote the Keth quantile of the distribution Jk=1...K.
This notation is chosen for brevity; it simply means that in the sorted array of ascending
Jk, Pk is 1/Ke if K ≤Ke, and 0 otherwise, as in (4). The resulting parameter updates are
equivalent to those in (4) and (5), but this representation makes the relation to PI2 more
CEM for Policy Improvement.
Because CEM is a very general algorithm, it is used
in many diﬀerent contexts in robot planning and control. CEM for policy optimization
was introduced by Mannor et al. . Although their focus is on solving ﬁnite small
Markov Decision Processes (MDPs), they also propose to use CEM with parameterized
policies to solve MDPs with large state spaces. Busoniu et al. extend this work, and
use CEM to learn a mapping from continuous states to discrete actions, where the centers
and widths of the basis functions are automatically adapted. The main diﬀerence with our
work is that we use continuous action spaces of higher dimensionality, and compare CEM
to PI2 and CMAES. CEM has also been used in combination with sampling-based motion
planning . An interesting aspect of this work is that it uses a mixture
of Gaussians rather than a single distribution to avoid premature convergence to a local
2.2. Covariance Matrix Adaptation - Evolution Strategy
The Covariance Matrix Adaptation - Evolution Strategy algorithm is very similar to CEM, but uses a more sophisticated method to update the
Stulp Sigaud
Figure 1: Visualization of an update with CEM. The upper right graph shows the 2D parameter space. The cost of a sample is its distance to the origin in Cartesian
The original multivariate Gaussian distribution N([ 8
8 ] , [ 9 0
0 9 ]) is represented by the dark dashed circle (68% conﬁdence interval). K = 10 samples θk
are taken from this distribution. The Ke = 5 elite samples are used to compute
the new Gaussian distribution, which in this case is N([ 7.2
6.4 ] , [ 3.2 1.4
1.4 8.0 ]). As can
be seen, the mean of the distribution has moved closer to the origin, and the
main axis of the covariance matrix points more towards the origin. The lower
left graph shows the mapping from cost to probability, as computed with (12).
Note that when Ke = K (red dot-dashed graphs), we are simply estimating the
original distribution.
covariance matrix, as listed in Table 2. There are three diﬀerences to CEM: • The probabilities in CMAES do not have to be Pk = 1/Ke as for CEM, but can be chosen by the
user, as long as the constraints PKe
k=1 Pk = 1 and P1 ≥· · · ≥PKe are met. Here, we use
the default suggested by Hansen & Ostermeier , i.e. Pk = ln (0.5(K + 1)) −ln(k).
• Sampling is done from a distribution N(θ, σ2Σ), i.e. the covariance matrix of the normal
distribution is multiplied with a scalar step-size σ. These components govern the magnitude
(σ) and shape (Σ) of the exploration, and are updated separately. • For both step-size and
covariance matrix an ‘evolution path’ is maintained (pσ and pΣ respectively), which stores
information about previous updates to θ. Using the information in the evolution path leads
to signiﬁcant improvements in terms of convergence speed, because it enables the algorithm
to exploit correlations between consecutive steps. For a full explanation of the algorithm
we refer to Hansen & Ostermeier .
Reducing CMAES to CEM.
This is done by setting certain parameters to extreme
values: 1) set the time horizon cσ = 0. This makes (21) collapse to σnew = σ × exp(0),
which means the step-size stays equal over time. Initially setting the step-size σinit = 1
Path Integral Policy Improvement with Covariance Matrix Adaptation
Policy Improvement
Cross-Entropy Method
Description
with Path Integrals (PI2)
Exploration Phase
for k = 1 . . . K do
←loop over trials →
for k = 1 . . . K do
θk ∼N (θ, Σ)
θk,i=1...N ∼N (θ, Σ)
execute policy
τk,i=1...N = executepolicy(θk,i=1...N )
Parameter Update Phase
loop over time steps
for i = 1 . . . N do
for k = 1 . . . K do
←loop over trials →
for k = 1 . . . K do
Jk = J(θk)
←evaluate →
Sk,i ≡S(τ k,i) = PN
j=i J(τ j,k)
if Jk < QKe/K
if Jk > QKe/K
←probability →
←parameter update →
k=1 Pk,iθk
k=1 Pk(θk −θ)(θk −θ)⊺
←covar. matrix adap. →
k=1 Pk(θk,i −θ)(θk,i −θ)⊺
temporal avg.
i=0(N−i)θnew
temporal avg.
i=0(N−i)Σnew
Table 1: Comparison of the CEM and PI2. This pseudo-code represents one iteration of
the algorithm, consisting of an exploration phase and a parameter update phase.
Both algorithms iterate these two phases until costs have converged, or up to a
certain number of iterations. The green equations – (17) and (19) – are only used
in PI2-CMA (to be explained in Section 3.4), and not part of ‘standard’ PI2.
means σ will always be 1, thus having no eﬀect during sampling. 2) For the covariance
matrix update, we set c1 = 0 and cµ = 1. The ﬁrst two terms of (23) then drop, and what
remains is PKe
k=1 Pk(θk −θ)(θk −θ)⊺, which is equivalent to (16) in CEM, if Pk is chosen
as in (12).
CMAES for Policy Improvement.
Heidrich-Meisner and Igel use CMAES
to directly learn a policy for a double pole-balancing task. R¨uckstiess et al. use
Natural Evolution Strategies (NES), which has comparble results with CMAES, to directly
learn policies for pole balancing, robust standing, and ball catching. The results above are
compared with various gradient-based methods, such as REINFORCE 
and NAC . To the best of our knowledge, our paper is the ﬁrst to
directly compare CMAES with CEM and PI2. Also, we use Dynamic Movement Primitives
as the underlying policy representation, which 1) enables us to scale to higher-dimensional
problems, as demonstrated by ; 2) requires us to perform temporal
averaging, cf. (18) and (19).
2.3. Policy Improvement with Path Integrals
A recent trend in reinforcement learning is to use parameterized policies in combination
with probability-weighted averaging; the PI2 algorithm is a recent example of this approach.
Using parameterized policies avoids the curse of dimensionality associated with (discrete)
Stulp Sigaud
Covariance Matrix Adaptation of CMAES
pσ ←(1 −cσ) pσ +
cσ(2 −cσ)µP Σ −1 θnew −θ
σnew = σ × exp
E∥N(0, I)∥−1
pΣ ←(1 −cΣ) pΣ + hσ
cΣ(2 −cΣ)µP
Σnew = (1 −c1 −cµ) Σ + c1(pΣpT
Σ + δ(hσ)Σ)
Pk(θk −θ)(θk −θ)⊺
Table 2: The step-size (21) and covariance matrix adaptation (23) update rule of CMAES,
which make use of the evolution paths (20) and (22). µP is the variance eﬀective
selection mass, with µP = 1/ PKe
k . The entire CMAES algorithm is acquired
by replacing (16) of CEM in Table 1 with these four equations, and multiplying
Σ with σ2 in (6).
state-action spaces, and using probability-weighted averaging avoids having to estimate a
gradient, which can be diﬃcult for noisy and discontinuous cost functions.
PI2 is derived from ﬁrst principles of optimal control, and gets its name from the application of the Feynman-Kac lemma to transform the Hamilton-Jacobi-Bellman equations into a
so-called path integral, which can be approximated with Monte Carlo methods . The PI2 algorithm is listed to the right in Table 1. As in CEM, K samples
θk=1...K are taken from a Gaussian distribution. In PI2, the vector θ represents the parameters of a policy, which, when executed, yields a trajectory τ i=1...N with N time steps.
This multi-dimensional trajectory may represent the joint angles of a n-DOF arm, or the
3-D position of an end-eﬀector.
So far, PI2 has mainly been applied to policies represented as Dynamic Movement
Primitives (DMPs) , where θ determines the shape of the movement.
Although PI2 searches in the space of θ, the costs are deﬁned in terms of the trajectory
τ generated by the DMP when it is integrated over time.
The cost of a trajectory is
determined by evaluating J for every time step i, where the cost-to-go of a trajectory at
time step i is deﬁned as the sum over all future costs S(τ i,k) = PN
j=i J(τ j,k), as in (11)1.
Analogously, the parameter update is applied to every time step i with respect to the
cost-to-go S(τ i). The probability of a trajectory at i is computed by exponentiating the
cost, as in (13). This assigns high probability to low-cost trials, and vice versa. In practice,
λSi,k is implemented with optimal baselining as
−h(Si,k−min(Si,k))
max(Si,k)−min(Si,k) cf. (Theodorou et al.,
1. For convenience, we abbreviate S(τ i,k) with Si,k.
Path Integral Policy Improvement with Covariance Matrix Adaptation
As can be seen in (15), a diﬀerent parameter update θnew
is computed for each time
step i. To acquire the single parameter update θnew, the ﬁnal step is therefore to average
over all time steps (18).
This average is weighted such that earlier parameter updates
in the trajectory contribute more than later updates, i.e.
the weight at time step i is
Ti = (N −1)/ PN
j=1(N −1).
The intuition is that earlier updates aﬀect a larger time
horizon and have more inﬂuence on the trajectory cost.
PoWeR is another recent policy improvement algorithm that uses probability-weighted
averaging . In PoWeR, the immediate costs must behave like an
improper probability, i.e. sum to a constant number and always be positive. This can
make the design of cost functions diﬃcult in practice; (24) for instance cannot be used with
PoWeR. PI2 places no such constraint on the cost function, which may be discontinuous.
When a cost function is compatible with both PoWeR and PI2, they perform essentially
identical .
An excellent discussion of the relationship between evolution strategies and reinforcement learning is given by R¨uckstiess et al. , where extensive empirical comparisons
between several gradient-based methods in both ﬁelds are made. The focus in our paper
is on methods based on probability-weighted averaging (e.g. PoWeR, PI2) rather than
gradients (e.g. REINFORCE, NAC), as these have proven to be superior in the context of
direct reinforcement learning for robotics problems .
3. Comparison of PI2, CEM and CMAES
When comparing the equations for CEM, CMAES and PI2, there are some interesting
similarities and diﬀerences. All sample from a Gaussian to explore parameter space – (6)
and (7) are identical – and both use probability-weighted averaging to update the parameters
– (14) and (15). It is striking that these algorithms, which have been derived within very
diﬀerent frameworks, have converged towards the same principle of probability-weighted
averaging.
We would like to emphasize that PI2’s properties follow directly from ﬁrst principles of
stochastic optimal control. For instance, the eliteness mapping follows from the application
of the Feymann-Kac lemma to the (linearized) Hamilton Jacobi Bellmann equations, as does
the concept of probability-weighted averaging. Whereas in other works the motivation for
using CEM/CMAES for policy improvement is based on its empirical performance (e.g. it is shown
to outperform a particular gradient-based method), the PI2 derivation demonstrates that there is a theoretically sound motivation for using methods based
on probability-weighted averaging, as this principle follows directly from ﬁrst principles of
stochastic optimal control.
Whereas Section 2 has mainly highlighted the similarities between the algorithms, this
section focuses on the diﬀerences. Note that any diﬀerences between PI2 and CEM/CMAESin
general also apply to the speciﬁc application of CEM/CMAES to policy improvement, as
done for instance by Busoniu et al. or . Before
comparing the algorithms, we ﬁrst present the evaluation task used in the paper.
Stulp Sigaud
3.1. Evaluation Task
For evaluation purposes, we use a viapoint task with a 10-DOF arm. The task is visualized
and described in Figure 2. This viapoint task is taken from , where
it is used to compare PI2 with PoWeR , NAC , and REINFORCE .
Figure 2: The evaluation task.
The gray line represents a 10-DOF arm of 1m length,
consisting of 10 0.1m links. At t = 0 the arm is stretched horizontally. Before
learning (left ﬁgure), each of the joints makes a minimum-jerk movement of 0.5s
towards the end position where the end-eﬀector is just ‘touching’ the y-axis.
The end-eﬀector path is shown (thick black line), as well as snapshots of the
arm posture at 50Hz (thin gray lines).
The goal of this task is for the endeﬀector to pass through the viapoint (0.5,0.5) at t = 0.3s, whilst minimizing joint
accelerations. The right ﬁgure depicts an example of a learned movement.
The goal of this task is expressed with the cost function in (24), where a represents
the joint angles, x and y the coordinates of the end-eﬀector, and D = 10 the number of
DOF. The weighting term (D + 1 −d) penalizes DOFs closer to the origin, the underlying
motivation being that wrist movements are less costly than shoulder movements for humans,
cf. .
J(τ ti) = δ(t −0.3) · ((xt −0.5)2 + (yt −0.5)2)
d=1(D + 1 −d)(¨at)2
d=1(D + 1 −d)
The 10 joint angles trajectories are generated by a 10-dimensional DMP, where each
dimension has B = 5 basis functions. The parameter vectors θ (one 1×5 vector for each of
the 10 dimensions), are initialized by training the DMP with a minimum-jerk movement.
During learning, we run 10 trials per update K = 10, where the ﬁrst of these 10 trials is
a noise-free trial used for evaluation purposes. For PI2, the eliteness parameter is h = 10,
and for CEM and CMAES it is Ke = K/2 = 5. The initial exploration noise is set to
Σ = 104IB=5 for each dimension of the DMP.
Path Integral Policy Improvement with Covariance Matrix Adaptation
3.2. Exploration Noise
A ﬁrst diﬀerence between CEM/CMAES and PI2 is the way exploration noise is generated.
In CEM and CMAES, time does not play a role, so only one exploration vector θk is
generated per trial. In stochastic optimal control, from which PI2 is derived, θi represents
a motor command at time i, and the stochasticity θi + ϵi is caused by executing command
in the environment.
When applying PI2 to DMPs, this stochasticity rather represents
controlled noise to foster exploration, which the algorithm samples from θi ∼N(θ, Σ). We
call this time-varying exploration noise. Since this exploration noise is under our control,
we need not vary it at every time step. In the work by Theodorou et al. for instance,
only one exploration vector θk is generated at the beginning of a trial, and exploration
is only applied to the DMP basis function that has the highest activation. We call this
per-basis exploration noise. In the most simple version, called constant exploration noise,
we sample θk,i=0 once at the beginning for i = 0, and leave it unchanged throughout the
execution of the movement, i.e. θk,i = θk,i=0.
The learning curves for these diﬀerent variants are depicted in Figure 3. We conclude
that time-varying exploration convergences substantially slower. Because constant exploration gives the fastest convergence, we use it throughout the rest of the paper.
Figure 3: Learning curves for time-varying, per-basis and constant exploration.
3.3. Deﬁnition of Eliteness
In each of the algorithms, the mapping from costs to probabilities is diﬀerent.
implements a cut-oﬀvalue for ‘eliteness’: you are either elite (Pk = 1/Ke) or not (Pk = 0).
PI2 rather considers eliteness to be a continuous value that is inversely proportional to the
cost of a trajectory. CMAES uses a hybrid eliteness measure where samples have zero
probability if they are not elite, and a continuous value which is inverse proportional to
the cost if they are elite. The probabilities in CMAES do not have to be Pk = 1/Ke as
for CEM, but can be chosen by the user, as long as the constraints PKe
k=1 Pk = 1 and
P1 ≥· · · ≥PKe are met. Here, we use the defaults suggested by Hansen & Ostermeier
 , i.e. Pk = ln (0.5(K + 1)) −ln(k).
These diﬀerent mappings are visualized in Figure 4. An interesting similarity between
the algorithms is that they each have a parameter – Ke in CEM/CMAES, and h in PI2 –
Stulp Sigaud
that determines how ‘elitist’ the mapping from cost to probability is. Typical values are
h = 10 and Ke = K/2. These and other values of h and Ke are depicted in Figure 4.
Figure 4: Lower left graph: Comparison of the mapping from costs Jk to probabilities Pk
for PI2 (with h = {10, 5}) and CEM/CMAES (with Ke = {3, 5}). Upper right
graph: The updated distributions are very similar with CEM (Ke = 3), CMAES
(Ke = 5) and PI2 (h = 5).
The learning curves for the diﬀerent weighting schemes with diﬀerent settings of their
eliteness parameter are depicted in Figure 5. The average learning curves in Figure 5 are
all very similar except for CEM with Ke = 5/7. This veriﬁes the conclusion by Hansen &
Ostermeier that choosing these weights is “relatively uncritical and can be chosen in a
wide range without disturbing the adaptation procedure.” and choosing the optimal weights
for a particular problem “only achieves speed-up factors of less than two” when compared
with CEM-style weighting where all the weights are Pk = 1/Ke. Because choosing the
weights is uncritical, we use the PI2 weighting scheme with h = 10, the default suggested
by Theodorou et al. , throughout the rest of this paper.
3.4. Covariance Matrix Adaptation
We now turn to the most interesting and relevant diﬀerence between the algorithms. In
CEM/CMAES, both the mean and covariance of the distribution are updated, whereas
PI2 only updates the mean. This is because in PI2 the shape of the covariance matrix is
constrained by the relation Σ = λR−1, where R is the (ﬁxed) command cost matrix, and
λ is a parameter inversely proportional to the parameter h. This constraint is necessary
to perform the derivation of PI2 ; the underlying intuition is that
there should be less exploration in directions where command costs are high.
In this paper, we choose to ignore the constraint Σ = λR−1, and apply covariance matrix
updating to PI2. Because a covariance matrix update is computed for each time step i (17),
Path Integral Policy Improvement with Covariance Matrix Adaptation
Figure 5: Average learning curves for diﬀerent weighting schemes, averaged over 3 learning
sessions. Conﬁdence intervals have been left out for clarity, but are similar in
magnitude to those in Figure 3. The inset highlights the similarity with CEM
(Ke = 3), CMAES (Ke = 5) and PI2 (h = 5).
we need to perform temporal averaging for the covariance matrix (19), just as we do for
the mean θ. Temporal averaging over covariance matrices is possible, because 1) every
positive-semideﬁnite matrix is a covariance matrix and vice versa 2) a weighted averaging
over positive-semideﬁnite matrices yields a positive-semideﬁnite matrix .
Thus, rather than having a ﬁxed covariance matrix, PI2 now adapts Σ based on the
observed costs for the trials, as depicted in Figure 4. This novel algorithm, which we call
PI2-CMA, for “Path Integral Policy Improvement with Covariance Matrix Adaptation”,
is listed in Table 1 (excluding the red indices i = 1 . . . N in (7), and including the green
equations (17) and (19)). A second algorithm, PI2-CMAES, is readily acquired by using
the more sophisticated covariance matrix updating rule of CMAES. Our next evaluation
highlights the main advantage of these algorithms, and compares their performance.
In Figure 6, we compare PI2 (where the covariance matrix is constant2) with PI2-
CMA (CEM-style covariance matrix updating) and PI2-CMAES (covariance matrix updating with CMAES). Initially, the covariance matrix for each of the 10 DOFs is set to
Σinit = λinitI5, where 5 is the number of basis functions, and λinit = {102, 104, 106} determines the initial exploration magnitude. All experiments are run for 200 updates, with
K = 20 trials per update. We chose a higher K because we are now not only computing
an update of the mean of the parameters (a 1 × 5 vector for each DOFs), but also its covariance matrix (a 5 × 5 matrix), and thus more information is needed per trial to get a
robust update . After each update, a small amount of base
level exploration noise is added to the covariance matrix (Σnew ←Σnew + 102I5) to avoid
premature convergence, as suggested by Kobilarov .
When the covariance matrices are not updated, the exploration magnitude remains the
same during learning, i.e. λ = λinit (labels
A in Figure 6), and the convergence behavior
2. Please note the diﬀerence between 1) constant exploration as in Section 3.2, where a sampled parameter
vector θk is not varied during the movement made in one trial; 2) constant covariance matrix, where Σ
is not updated and thus constant during an entire learning session.
Stulp Sigaud
Figure 6: Top:
Average learning curves with and without covariance matrix updating
for diﬀerent initial exploration magnitudes, averaged over 5 learning sessions.
Bottom: The magnitude of the exploration λ as learning progresses. Initially
Σinit = λinitI5 for each DOF.
is diﬀerent for the diﬀerent exploration magnitudes λinit = {102, 104, 106}. For λinit = 104
we have nice convergence behavior
B , which is not a coincidence – this value has been
speciﬁcally tuned for this task, and it is the default we have used so far. However, when we
set the exploration magnitude very low (λinit = 102) convergence is much slower C . When
the exploration magnitude is set very high λinit = 106, we get quick convergence
due to the high stochasticity in sampling, we still have a lot of stochasticity in the cost after
convergence in comparison to lower λinit. This can be seen in the inset, where the y-axis
has been scaled ×20 for detail E .
Path Integral Policy Improvement with Covariance Matrix Adaptation
For PI2-CMA, i.e. with covariance matrix updating, we see that the exploration magnitude λ changes over time (bottom graph), whereby λ is computed as the mean of the
eigenvalues of the covariance matrix. For λinit = 102, λ rapidly increases
F until a maximum value is reached, after which it decreases and converges to a value of 102.8
same holds for λinit = 104, but the initial increase is not so rapid
H . For λinit = 106, λ
only decreases
I , but converges to 102.8 as the others.
From these results we derive three conclusions: 1) with PI2-CMA, the convergence speed
does not depend as much on the initial exploration magnitude λinit, i.e. after 500 updates
the µ ± σ cost for PI2-CMA over all λinit is 105 · (8 ± 7), whereas for PI2 without covariance
matrix updating it is 105 · (35 ± 43)
J . 2) PI2-CMA automatically increases λ if more
exploration leads to quicker convergence
F H . 3) PI2-CMA automatically decreases λ once
the task has been learned
G K . Note that 2) and 3) are emergent properties of covariance
matrix updating, and has not been explicitly encoded in the algorithm. In summary, PI2-
CMA is able to ﬁnd a good exploration/exploitation trade-oﬀ, independent of the initial
exploration magnitude.
This is an important property, because setting the exploration magnitude by hand is not
straightforward, because it is highly task-dependent, and might require several evaluations
to tune. One of the main contributions of this paper is that we demonstrate how using
probability-weighted averaging to update the covariance matrix (as is done in CEM) allows
PI2 to autonomously tune the exploration magnitude – the user thus no longer needs to tune
this parameter. The only remaining parameters of PI2 are K (number of trials per update)
and h (eliteness parameter), but choosing them is not critical as noted independently by
diﬀerent research groups . Although an
initial Σ must be given, Figure 6 shows that with an initial exploration magnitude two orders
of magnitude higher/lower than a tuned value, PI2-CMA still converges to the same cost
and exploration magnitude, with only slight diﬀerences in the initial speed of convergence.
When comparing PI2-CMA and PI2-CMAES, we only see a very small diﬀerence in terms
of convergence when the initial exploration is low λinit = 102
L . This is because the covariance update rule of CMAES is damped, (21) and (23), and it makes more conservative
updates than CEM, cf.
N . In our experiments, PI2-CMAES uses the default parameters suggested by Hansen & Ostermeier . We have tried diﬀerent parameters for
PI2-CMAES, the conclusion being that the best parameters are those that reduce CMAES
to CEM, cf. Section 2.2. In general, we do not claim that PI2-CMAES outperforms PI2,
and Hansen & Ostermeier also conclude that there are tasks where CMAES has
identical performance to simpler algorithms. Our results on comparing PI2-CMAES and
PI2-CMA are therefore not conclusive. An interesting question is whether typical cost functions found in robotics problems have properties that do not allow CMAES to leverage the
advantages it has on benchmark problems used in optimization.
4. Conclusion
In this paper, we have scrutinized the recent state-of-the-art direct policy improvement algorithm PI2 from the speciﬁc perspective of belonging to a family of methods based on the
concept of probability-weighted averaging. We have discussed similarities and diﬀerences
between three algorithms in this family, being PI2, CMAES and CEM. In particular, we
Stulp Sigaud
have demonstrated that using probability-weighted averaging to update the covariance matrix, as is done in CEM and CMAES, allows PI2 to autonomously tune the exploration
magnitude. The resulting algorithm PI2-CMA shows more consistent convergence under
varying initial conditions, and alleviates the user from having to tune the exploration magnitude parameter by hand. We are currently applying PI2-CMA to challenging tasks on
a physical humanoid robot. Given the ability of PI2 to learn complex, high-dimensional
tasks on real robots , we are conﬁdent that
PI2-CMA can also successfully be applied to such tasks.