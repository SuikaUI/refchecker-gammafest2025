Improving Unsupervised Defect Segmentation
by Applying Structural Similarity To Autoencoders
Paul Bergmann1, Sindy L¨owe1,2, Michael Fauser1, David Sattlegger1, and Carsten Steger1
1MVTec Software GmbH
www.mvtec.com
{bergmannp,fauser,sattlegger,steger}@mvtec.com
2University of Amsterdam
 
Abstract—Convolutional autoencoders have emerged as popular methods for unsupervised defect segmentation on image
data. Most commonly, this task is performed by thresholding
a per-pixel reconstruction error based on an ℓp-distance.
This procedure, however, leads to large residuals whenever
the reconstruction includes slight localization inaccuracies
around edges. It also fails to reveal defective regions that
have been visually altered when intensity values stay roughly
consistent. We show that these problems prevent these approaches from being applied to complex real-world scenarios and that they cannot be easily avoided by employing
more elaborate architectures such as variational or feature
matching autoencoders. We propose to use a perceptual
loss function based on structural similarity that examines
inter-dependencies between local image regions, taking into
account luminance, contrast, and structural information,
instead of simply comparing single pixel values. It achieves
signiﬁcant performance gains on a challenging real-world
dataset of nanoﬁbrous materials and a novel dataset of two
woven fabrics over state-of-the-art approaches for unsupervised defect segmentation that use per-pixel reconstruction
error metrics.
1. INTRODUCTION
Visual inspection is essential in industrial manufacturing
to ensure high production quality and high cost efﬁciency
by quickly discarding defective parts. Since manual inspection by humans is slow, expensive, and error-prone,
the use of fully automated computer vision systems is becoming increasingly popular. Supervised methods, where
the system learns how to segment defective regions by
training on both defective and non-defective samples, are
commonly used. However, they involve a large effort
to annotate data and all possible defect types need to
be known beforehand. Furthermore, in some production
processes, the scrap rate might be too small to produce
a sufﬁcient number of defective samples for training,
especially for data-hungry deep learning models.
In this work, we focus on unsupervised defect segmentation for visual inspection. The goal is to segment defective regions in images after having trained exclusively on
non-defective samples. It has been shown that architectures based on convolutional neural networks (CNNs) such
as autoencoders or generative
adversarial networks can
be used for this task. We provide a brief overview of
such methods in Section 2. These models try to reconstruct their inputs in the presence of certain constraints
such as a bottleneck and thereby manage to capture the
essence of high-dimensional data (e.g., images) in a lowerdimensional space. It is assumed that anomalies in the test
data deviate from the training data manifold and the model
is unable to reproduce them. As a result, large reconstruction errors indicate defects. Typically, the error measure
that is employed is a per-pixel ℓp-distance, which is an
ad-hoc choice made for the sake of simplicity and speed.
However, these measures yield high residuals in locations
where the reconstruction is only slightly inaccurate, e.g.,
due to small localization imprecisions of edges. They also
fail to detect structural differences between the input and
reconstructed images when the respective pixels’ color
values are roughly consistent. We show that this limits the
usefulness of such methods when employed in complex
real-world scenarios.
To alleviate the aforementioned problems, we propose
to measure reconstruction accuracy using the structural
similarity (SSIM) metric . SSIM is
a distance measure designed to capture perceptual similarity that is less sensitive to edge alignment and gives
importance to salient differences between input and reconstruction. It captures inter-dependencies between local pixel regions that are disregarded by the current
state-of-the-art unsupervised defect segmentation methods
based on autoencoders with per-pixel losses. We evaluate
the performance gains obtained by employing SSIM as
a loss function on two real-world industrial inspection
datasets and demonstrate signiﬁcant performance gains
over per-pixel approaches. Figure 1 demonstrates the advantage of perceptual loss functions over a per-pixel ℓ2loss on the NanoTWICE dataset of nanoﬁbrous materials . While both autoencoders alter
the reconstruction in defective regions, only the residual
map of the SSIM autoencoder allows a segmentation of
these areas. By changing the loss function and otherwise
keeping the same autoencoding architecture, we reach
a performance that is on par with other state-of-the-art
 
Figure 1: A defective image of nanoﬁbrous materials is reconstructed by an autoencoder optimizing either the commonly used
pixel-wise ℓ2-distance or a perceptual similarity metric based on structural similiarity (SSIM). Even though an ℓ2-autoencoder fails
to properly reconstruct the defects, a per-pixel comparison of the original input and reconstruction does not yield signiﬁcant residuals
that would allow for defect segmentation. The residual map using SSIM puts more importance on the visually salient changes made
by the autoencoder, enabling for an accurate segmentation of the defects.
unsupervised defect segmentation approaches that rely on
additional model priors such as handcrafted features or
pretrained networks.
2. RELATED WORK
Detecting anomalies that deviate from the training data
has been a long-standing problem in machine learning.
Pimentel et al. give a comprehensive overview of
the ﬁeld. In computer vision, one needs to distinguish
between two variants of this task. First, there is the
classiﬁcation scenario, where novel samples appear as
entirely different object classes that should be predicted
as outliers. Second, there is a scenario where anomalies
manifest themselves in subtle deviations from otherwise
known structures and a segmentation of these deviations
is desired. For the classiﬁcation problem, a number of
approaches have been proposed . Here, we limit ourselves to an
overview of methods that attempt to tackle the latter
Napoletano et al. extract features from a CNN
that has been pretrained on a classiﬁcation task. The
features are clustered in a dictionary during training and
anomalous structures are identiﬁed when the extracted
features strongly deviate from the learned cluster centers.
General applicability of this approach is not guaranteed
since the pretrained network might not extract useful
features for the new task at hand and it is unclear which
features of the network should be selected for clustering.
The results achieved with this method are the current stateof-the-art on the NanoTWICE dataset, which we also use
in our experiments. They improve upon previous results
by Carrera et al. , who build a dictionary that yields
a sparse representation of the normal data. Similar approaches using sparse representations for novelty detection
are .
Schlegl et al. train a GAN on optical coherence
tomography images of the retina and detect anomalies
such as retinal ﬂuid by searching for a latent sample that
minimizes the per-pixel ℓ2-reconstruction error as well as
a discriminator loss. The large number of optimization
steps that must be performed to ﬁnd a suitable latent
sample makes this approach very slow. Therefore, it is
only useful in applications that are not time-critical. Recently, Zenati et al. proposed to use bidirectional
GANs to add the missing encoder
network for faster inference. However, GANs are prone
to run into mode collapse, i.e., there is no guarantee
that all modes of the distribution of non-defective images
are captured by the model. Furthermore, they are more
difﬁcult to train than autoencoders since the loss function
of the adversarial training typically cannot be trained
to convergence . Instead, the
training results must be judged manually after regular
optimization intervals.
Baur et al. propose a framework for defect
segmentation using autoencoding architectures and a perpixel error metric based on the ℓ1-distance. To prevent
the disadvantages of their loss function, they improve the
reconstruction quality by requiring aligned input data and
adding an adversarial loss to enhance the visual quality of
the reconstructed images. However, for many applications
that work on unstructured data, prior alignment is impossible. Furthermore, optimizing for an additional adversarial
loss during training but simply segmenting defects based
on per-pixel comparisons during evaluation might lead
to worse results since it is unclear how the adversarial
training inﬂuences the reconstruction.
Other approaches take into account the structure
of the latent space of variational autoencoders in order to deﬁne measures
for outlier detection. An and Cho deﬁne a reconstruction probability for every image pixel by drawing
multiple samples from the estimated encoding distribution
and measuring the variability of the decoded outputs.
Soukup and Pinetz disregard the decoder output
entirely and instead compute the KL divergence as a novelty measure between the prior and the encoder distribution. This is based on the assumption that defective inputs
will manifest themselves in mean and variance values
that are very different from those of the prior. Similarly,
Vasilev et al. deﬁne multiple novelty measures,
either by purely considering latent space behavior or by
combining these measures with per-pixel reconstruction
losses. They obtain a single scalar value that indicates
an anomaly, which can quickly become a performance
bottleneck in a segmentation scenario where a separate
forward pass would be required for each image pixel
to obtain an accurate segmentation result. We show that
per-pixel reconstruction probabilities obtained from VAEs
suffer from the same problems as per-pixel deterministic
losses (cf. Section 4).
All the aforementioned works that use autoencoders
for unsupervised defect segmentation have shown that
autoencoders reliably reconstruct non-defective images
while visually altering defective regions to keep the reconstruction close to the learned manifold of the training data.
However, they rely on per-pixel loss functions that make
the unrealistic assumption that neighboring pixel values
are mutually independent. We show that this prevents
these approaches from segmenting anomalies that differ
predominantly in structure rather than pixel intensity. Instead, we propose to use SSIM as
the loss function and measure of anomaly by comparing
input and reconstruction. SSIM takes interdependencies
of local patch regions into account and evaluates their
ﬁrst and second order moments to model differences in
luminance, contrast, and structure. Ridgeway et al. 
show that SSIM and the closely related multi-scale version
MS-SSIM can be used as differentiable loss functions to generate more realistic images
in deep architectures for tasks such as superresolution,
but do not examine its usefulness for defect segmentation
in an autoencoding framework. In all our experiments,
switching from per-pixel to perceptual losses yields signiﬁcant gains in performance, sometimes enhancing the
method from a complete failure to a satisfactory defect
segmentation result.
3. METHODOLOGY
3.1. Autoencoders for Unsupervised Defect Segmentation
Autoencoders attempt to reconstruct an input image
x ∈Rk×h×w through a bottleneck, effectively projecting
the input image into a lower-dimensional space, called
latent space. An autoencoder consists of an encoder
function E : Rk×h×w →Rd and a decoder function
D : Rd →Rk×h×w, where d denotes the dimensionality
of the latent space and k, h, w denote the number of channels, height, and width of the input image, respectively.
Choosing d ≪k × h × w prevents the architecture from
simply copying its input and forces the encoder to extract
meaningful features from the input patches that facilitate
accurate reconstruction by the decoder. The overall process can be summarized as
ˆx = D(E(x)) = D(z) ,
where z is the latent vector and ˆx the reconstruction of
the input. In our experiments, the functions E and D are
parameterized by CNNs. Strided convolutions are used
to down-sample the input feature maps in the encoder
and to up-sample them in the decoder. Autoencoders can
be employed for unsupervised defect segmentation by
Figure 2: Different responsibilities of the three similarity functions employed by SSIM. Example patches p and q differ in either luminance, contrast, or structure. SSIM is able to distinguish
between these three cases, assigning close to minimum similarity
values to one of the comparison functions l(p, q), c(p, q), or
s(p, q), respectively. An ℓ2-comparison of these patches would
yield a constant per-pixel residual value of 0.25 for each of the
three cases.
training them purely on defect-free image data. During
testing, the autoencoder will fail to reconstruct defects that
have not been observed during training, which can thus be
segmented by comparing the original input to the reconstruction and computing a residual map R(x, ˆx) ∈Rw×h.
3.1.1. ℓ2-Autoencoder. To force the autoencoder to reconstruct its input, a loss function must be deﬁned that guides
it towards this behavior. For simplicity and computational
speed, one often chooses a per-pixel error measure, such
as the L2 loss
L2(x, ˆx) =
(x(r, c) −ˆx(r, c))2 ,
where x(r, c) denotes the intensity value of image x at
the pixel (r, c). To obtain a residual map Rℓ2(x, ˆx) during
evaluation, the per-pixel ℓ2-distance of x and ˆx is computed.
3.1.2. Variational Autoencoder. Various extensions to
the deterministic autoencoder framework exist. VAEs
 impose constraints on the
latent variables to follow a certain distribution z ∼P(z).
For simplicity, the distribution is typically chosen to be
a unit-variance Gaussian. This turns the entire framework
into a probabilistic model that enables efﬁcient posterior
inference and allows to generate new data from the training manifold by sampling from the latent distribution.
The approximate posterior distribution Q(z|x) obtained
by encoding an input image can be used to deﬁne further
anomaly measures. One option is to compute a distance
between the two distributions, such as the KL-divergence
KL(Q(z|x)||P(z)), and indicate defects for large deviations from the prior P(z) .
However, to use this approach for the pixel-accurate
segmentation of anomalies, a separate forward pass for
each pixel of the input image would have to be performed. A second approach for utilizing the posterior
Figure 3: A toy example illustrating the advantages of SSIM over ℓ2 for the segmentation of defects. (a) 128 × 128 checkerboard
pattern with gray strokes and dots that simulate defects. (b) Output reconstruction ˆx of the input image x by an ℓ2-autoencoder
trained on defect-free checkerboard patterns. The defects have been removed by the autoencoder. (c) ℓ2-residual map. Brighter colors
indicate larger dissimilarity between input and reconstruction. (d) Residuals for luminance l, contrast c, structure s, and their pointwise
product that yields the ﬁnal SSIM residual map. In contrast to the ℓ2-error map, SSIM gives more importance to the visually more
salient disturbances than to the slight inaccuracies around reconstructed edges.
Q(z|x) that yields a spatial residual map is to decode N
latent samples z1, z2, . . . , zN drawn from Q(z|x) and to
evaluate the per-pixel reconstruction probability RV AE =
P(x|z1, z2, . . . , zN) as described by An and Cho .
3.1.3. Feature
Autoencoder.
Another extension
autoencoders
Dosovitskiy and Brox . It increases the quality of
the produced reconstructions by extracting features from
both the input image x and its reconstruction ˆx and enforcing them to be equal. Consider F : Rk×h×w →Rf to be
a feature extractor that obtains an f-dimensional feature
vector from an input image. Then, a regularizer can be
added to the loss function of the autoencoder, yielding
the feature matching autoencoder (FM-AE) loss
LFM(x, ˆx) = L2(x, ˆx) + λ∥F(x) −F(ˆx)∥2
where λ > 0 denotes the weighting factor between the two
loss terms. F can be parameterized using the ﬁrst layers of
a CNN pretrained on an image classiﬁcation task. During
evaluation, a residual map RFM is obtained by comparing
the per-pixel ℓ2-distance of x and ˆx. The hope is that
sharper, more realistic reconstructions will lead to better
residual maps compared to a standard ℓ2-autoencoder.
3.1.4. SSIM Autoencoder. We show that employing more
elaborate architectures such as VAEs or FM-AEs does
not yield satisfactory improvements of the residial maps
over deterministic ℓ2-autoencoders in the unsupervised
defect segmentation task. They are all based on per-pixel
evaluation metrics that assume an unrealistic independence between neighboring pixels. Therefore, they fail to
detect structural differences between the inputs and their
reconstructions. By adapting the loss and evaluation functions to capture local inter-dependencies between image
regions, we are able to drastically improve upon all the
aforementioned architectures. In Section 3.2, we speciﬁcally motivate the use of the strucutural similarity metric
SSIM(x, ˆx) as both the loss function and the evaluation
metric for autoencoders to obtain a residual map RSSIM.
3.2. Structural Similarity
The SSIM index deﬁnes a distance
measure between two K × K image patches p and q,
taking into account their similarity in luminance l(p, q),
contrast c(p, q), and structure s(p, q):
SSIM(p, q) = l(p, q)αc(p, q)βs(p, q)γ ,
where α, β, γ ∈R are user-deﬁned constants to weight the
three terms. The luminance measure l(p, q) is estimated
by comparing the patches’ mean intensities µp and µq.
The contrast measure c(p, q) is a function of the patch
variances σ2
q. The structure measure s(p, q) takes
into account the covariance σpq of the two patches. The
three measures are deﬁned as:
2µpµq + c1
µ2p + µ2q + c1
2σpσq + c2
σ2p + σ2q + c2
2σpσq + c2
The constants c1 and c2 ensure numerical stability and are
typically set to c1 = 0.01 and c2 = 0.03. By substituting
(5)-(7) into (4), the SSIM is given by
SSIM(p, q) =
(2µpµq + c1)(2σpq + c2)
(µ2p + µ2q + c1)(σ2p + σ2q + c2) .
It holds that SSIM(p, q)
[−1, 1]. In particular,
SSIM(p, q) = 1 if and only if p and q are identical
 . Figure 2 shows the different perceptions of the three similarity functions that form the SSIM
index. Each of the patch pairs p and q has a constant ℓ2residual of 0.25 per pixel and hence assigns low defect
scores to each of the three cases. SSIM on the other hand
is sensitive to variations in the patches’ mean, variance,
and covariance in its respective residual map and assigns
low similarity to each of the patch pairs in one of the
comparison functions.
To compute the structural similarity between an entire
image x and its reconstruction ˆx, one slides a K × K
window across the image and computes a SSIM value at
each pixel location. Since (8) is differentiable, it can be
employed as a loss function in deep learning architectures
that are optimized using gradient descent.
Figure 3 indicates the advantages SSIM has over perpixel error functions such as ℓ2 for segmenting defects.
After training an ℓ2-autoencoder on defect-free checkerboard patterns of various scales and orientations, we apply
it to an image (Figure 3(a)) that contains gray strokes
and dots that simulate defects. Figure 3(b) shows the corresponding reconstruction produced by the autoencoder,
which removes the defects from the input image. The
two remaining subﬁgures display the residual maps when
Output Size
Parameters
Table 1: General outline of our autoencoder architecture. The
depicted values correspond to the structure of the encoder. The
decoder is built as a reversed version of this. Leaky rectiﬁed
linear units (ReLUs) with slope 0.2 are applied as activation
functions after each layer except for the output layers of both
the encoder and the decoder, in which linear activation functions
evaluating the reconstruction error with a per-pixel ℓ2comparison or SSIM. For the latter, the luminance, contrast, and structure maps are also shown. For the ℓ2distance, both the defects and the inaccuracies in the
reconstruction of the edges are weighted equally in the
error map, which makes them indistinguishable. Since
SSIM computes three different statistical features for image comparison and operates on local patch regions, it
is less sensitive to small localization inaccuracies in the
reconstruction. In addition, it detects defects that manifest
themselves in a change of structure rather than large
differences in pixel intensity. For the defects added in
this particular toy example, the contrast function yields
the largest residuals.
4. EXPERIMENTS
4.1. Datasets
Due to the lack of datasets for unsupervised defect
segmentation in industrial scenarios, we contribute a novel
dataset of two woven fabric textures, which is available
to the public1. We provide 100 defect-free images per
texture for training and validation and 50 images that
contain various defects such as cuts, roughened areas, and
contaminations on the fabric. Pixel-accurate ground truth
annotations for all defects are also provided. All images
are of size 512 × 512 pixels and were acquired as singlechannel gray-scale images. Examples of defective and
defect-free textures can be seen in Figure 4. We further
evaluate our method on a dataset of nanoﬁbrous materials , which contains ﬁve defectfree gray-scale images of size 1024 × 700 for training
and validation and 40 defective images for evaluation. A
sample image of this dataset is shown in Figure 1.
4.2. Training and Evaluation Procedure
For all datasets, we train the autoencoders with their
respective losses and evaluation metrics, as described in
Section 3.1. Each architecture is trained on 10 000 defectfree patches of size 128×128, randomly cropped from the
given training images. In order to capture a more global
context of the textures, we down-scaled the images to
 
company/research/publications
Figure 4: Example images from the contributed texture dataset
of two woven fabrics. (a) and (b) show examples of nondefective textures that can be used for training. (c) and (d) show
exemplary defects for both datasets. See the text for details.
size 256 × 256 before cropping. Each network is trained
for 200 epochs using the ADAM 
optimizer with an initial learning rate of 2 × 10−4 and
a weight decay set to 10−5. The exact parametrization
of the autoencoder network shared by all tested architectures is given in Table 1. The latent space dimension
for our experiments is set to d = 100 on the texture
images and to d = 500 for the nanoﬁbres due to their
higher structural complexity. For the VAE, we decode
N = 6 latent samples from the approximate posterior
distribution Q(z|x) to evaluate the reconstruction probability for each pixel. The feature matching autoencoder
is regularized with the ﬁrst three convolutional layers
of an AlexNet pretrained on
ImageNet and a weight factor
of λ = 1. For SSIM, the window size is set to K = 11
unless mentioned otherwise and its three residual maps
are equally weighted by setting α = β = γ = 1.
The evaluation is performed by striding over the test
images and reconstructing image patches of size 128×128
using the trained autoencoder and computing its respective
residual map R. In principle, it would be possible to set the
horizontal and vertical stride to 128. However, at different
spatial locations, the autoencoder produces slightly different reconstructions of the same data, which leads to some
striding artifacts. Therefore, we decreased the stride to 30
pixels and averaged the reconstructed pixel values. The
resulting residual maps are thresholded to obtain candidate
regions where a defect might be present. An opening with
a circular structuring element of diameter 4 is applied as
a morphological post-processing to delete outlier regions
that are only a few pixels wide . We
compute the receiver operating characteristic (ROC) as
the evaluation metric. The true positive rate is deﬁned as
the ratio of pixels correctly classiﬁed as defect across the
entire dataset. The false positive rate is the ratio of pixels
misclassiﬁed as defect.
Figure 5: Qualitative comparison between reconstructions, residual maps, and segmentation results of an ℓ2-autoencoder and an
SSIM autoencoder on two datasets of woven fabric textures. The ground truth regions containing defects are outlined in red while
green areas mark the segmentation result of the respective method.
Figure 6: Resulting ROC curves of the proposed SSIM autoencoder (red line) on the evaluated datasets of nanoﬁbrous materials (a)
and the two texture datasets (b), (c) in comparison with other autoencoding architectures that use per-pixel loss functions (green,
orange, and blue lines). Corresponding AUC values are given in the legend.
4.3. Results
Figure 5 shows a qualitative comparison between the
performance of the ℓ2-autoencoder and the SSIM autoencoder on images of the two texture datasets. Although both
architectures remove the defect in the reconstruction, only
the SSIM residual map reveals the defects and provides an
accurate segmentation result. The same can be observed
for the NanoTWICE dataset, as shown in Figure 1.
We conﬁrm this qualitative behavior by numerical
results. Figure 6 compares the ROC curves and their
respective AUC values of our approach using SSIM to
the per-pixel architectures. The performance of the latter
is often only marginally better than classifying each pixel
randomly. For the VAE, we found that the reconstructions
obtained by different latent samples from the posterior
does not vary greatly. Thus, it could not improve on
the deterministic framework. Employing feature matching
only improved the segmentation result for the dataset of
nanoﬁbrous materials, while not yielding a beneﬁt for
the two texture datasets. Using SSIM as the loss and
evaluation metric outperforms all other tested architectures
signiﬁcantly. By merely changing the loss function, the
achieved AUC improves from 0.688 to 0.966 on the
dataset of nanoﬁbrous materials, which is comparable
to the state-of-the-art by Napoletano et al. , where
values of up to 0.974 are reported. In contrast to this
method, autoencoders do not rely on any model priors
such as handcrafted features or pretrained networks. For
the two texture datasets, similar leaps in performance are
Since the dataset of nanoﬁbrous materials contains
defects of various sizes and smaller sized defects contribute less to the overall true positive rate when weighting all pixel equally, we further evaluated the overlap of each detected anomaly region with the ground
truth for this dataset and report the p-quantiles for p ∈
{25%, 50%, 75%} in Figure 7. For false positive rates
as low as 5%, more than 50% of the defects have an
overlap with the ground truth that is larger than 91%.
This outperforms the results achieved by Napoletano et al.
 , who report a minimal overlap of 85% in this
We further tested the sensitivity of the SSIM autoencoder to different hyperparameter settings. We varied
the latent space dimension d, SSIM window size k, and
the size of the patches that the autoencoder was trained
on. Table 2 shows that SSIM is insensitive to different
hyperparameter settings once the latent space dimension
is chosen to be sufﬁciently large. Using the optimal setup
of d = 500, k = 11, and patch size 128 × 128, a
forward pass through our architecture takes 2.23 ms on a
Tesla V100 GPU. Patch-by-patch evaluation of an entire
image of the NanoTWICE dataset takes 3.61 s on average,
which is signiﬁcantly faster than the runtimes reported by
Napoletano et al. . Their approach requires between
False Positive Rate
25-quantile
50-quantile
75-quantile
Figure 7: Per-region overlap for individual defects between our
segmentation and the ground truth for different false positive
rates using an SSIM autoencoder on the dataset of nanoﬁbrous
materials.
15 s and 55 s to process a single input image.
Figure 8 depicts qualitative advantages that employing
a perceptual error metric has over per-pixel distances such
as ℓ2. It displays two defective images from one of the
texture datasets, where the top image contains a highcontrast defect of metal pins which contaminate the fabric.
The bottom image shows a low-contrast structural defect
where the fabric was cut open. While the ℓ2-norm has
problems to detect the low-constrast defect, it easily segments the metal pins due to their large absolute distance
in gray values with respect to the background. However,
misalignments in edge regions still lead to large residuals
in non-defective regions as well, which would make these
thin defects hard to segment in practice. SSIM robustly
segments both defect types due to its simultaneous focus
on luminance, contrast, and structural information and
insensitivity to edge alignment due to its patch-by-patch
comparisons.
5. CONCLUSION
We demonstrate the advantage of perceptual loss functions over commonly used per-pixel residuals in autoencoding architectures when used for unsupervised defect
segmentation tasks. Per-pixel losses fail to capture interdependencies between local image regions and therefore
are of limited use when defects manifest themselves in
structural alterations of the defect-free material where
pixel intensity values stay roughly consistent. We further
show that employing probabilistic per-pixel error metrics
obtained by VAEs or sharpening reconstructions by feature matching regularization techniques do not improve
the segmentation result since they do not address the
problems that arise from treating pixels as mutually independent.
SSIM, on the other hand, is less sensitive to small inaccuracies of edge locations due to its comparison of local
patch regions and takes into account three different statistical measures: luminance, contrast, and structure. We
demonstrate that switching from per-pixel loss functions
to an error metric based on structural similarity yields
signiﬁcant improvements by evaluating on a challenging
real-world dataset of nanoﬁbrous materials and a contributed dataset of two woven fabric materials which we
make publicly available. Employing SSIM often achieves
an enhancement from almost unusable segmentations to
results that are on par with other state of the art approaches
window size
Patch size
Table 2: Area under the ROC curve (AUC) on NanoTWICE for
varying hyperparameters in the SSIM autoencoder architecture.
Different settings do not signiﬁcantly alter defect segmentation
performance.
Figure 8: In the ﬁrst row, the metal pins have a large difference
in gray values in comparison to the defect-free background
material. Therefore, they can be detected by both the ℓ2 and
the SSIM error metric. The defect shown in the second row,
however, differs from the texture more in terms of structure than
in absolute gray values. As a consequence, a per-pixel distance
metric fails to segment the defect while SSIM yields a good
segmentation result.
for unsupervised defect segmentation which additionally
rely on image priors such as pre-trained networks.