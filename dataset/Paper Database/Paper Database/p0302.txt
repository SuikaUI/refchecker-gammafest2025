Data Augmentation Using Pre-trained Transformer Models
Varun Kumar
 
Ashutosh Choudhary
 
 
Language model based pre-trained models
such as BERT have provided signiﬁcant gains
across different NLP tasks. In this paper, we
study different types of transformer based pretrained models such as auto-regressive models
(GPT-2), auto-encoder models (BERT), and
seq2seq models (BART) for conditional data
augmentation. We show that prepending the
class labels to text sequences provides a simple
yet effective way to condition the pre-trained
models for data augmentation. Additionally,
on three classiﬁcation benchmarks, pre-trained
Seq2Seq model outperforms other data augmentation methods in a low-resource setting.
Further, we explore how different data augmentation methods using pre-trained model
differ in-terms of data diversity, and how well
such methods preserve the class-label information.
Introduction
Data augmentation (DA) is a widely used technique
to increase the size of the training data. Increasing training data size is often essential to reduce
overﬁtting and enhance the robustness of machine
learning models in low-data regime tasks.
In natural language processing (NLP), several
word replacement based methods have been explored for data augmentation. In particular, Wei
and Zou showed that simple word replacement using knowledge bases like WordNet improves classiﬁcation performance. Further,
Kobayashi utilized language models (LM)
to augment training data. However, such methods
struggle with preserving class labels. For example,
non-conditional DA for an input sentence of sentiment classiﬁcation task “a small impact with a big
movie” leads to “a small movie with a big impact”.
Using such augmented data for training, with the
original input sentence’s label (i.e. negative sentiment in this example) would negatively impact the
performance of the resulting model.
To alleviate this issue, Wu et al. proposed conditional BERT (CBERT) model which
extends BERT masked language modeling (MLM) task, by considering class
labels to predict the masked tokens. Since their
method relies on modifying BERT model’s segment embedding, it cannot be generalized to other
pre-trained LMs without segment embeddings.
Similarly,
Anaby-Tavor et al. used
GPT2 for DA where examples are generated for a given class by providing
class as input to a ﬁne-tuned model. In their work,
GPT2 is used to generate 10 times the number of examples required for augmentation and then the generated sentences are selected based on the model
conﬁdence score. As data selection is applied only
to GPT2 but not to the other models, the augmentation methods can not be fairly compared. Due
to such discrepancies, it is not straightforward to
comprehend how the generated data using different
pre-trained models varies from each other and their
impact on downstream model performance.
This paper proposes a uniﬁed approach to use
any pre-trained transformer 
based models for data augmentation. In particular, we explore three different pre-trained model
types for DA, including 1) an autoencoder (AE)
LM: BERT, 2) an auto-regressive (AR) LM: GPT2,
and 3) a pre-trained seq2seq model: BART . We apply the data generation for three
different NLP tasks: sentiment classiﬁcation, intent
classiﬁcation, and question classiﬁcation.
In order to understand the signiﬁcance of DA,
we simulate a low-resource data scenario, where
we utilize only 10 training examples per class in a
classiﬁcation task. Section 3.2 provides details of
the task and corpora.
We show that all three types of pre-trained modarXiv:2003.02245v2 [cs.CL] 31 Jan 2021
els can be effectively used for DA, and using the
generated data leads to improvement in classiﬁcation performance in the low-data regime setting.
Among three types of methods, pre-trained seq2seq
model provides the best performance. Our code is
available at 1.
Our contribution is three-fold: (1) implementation of a seq2seq pre-trained model based data augmentation, (2) experimental comparison of different data augmentation methods using conditional
pre-trained model, (3) a uniﬁed data augmentation
approach with practical guidelines for using different types of pre-trained models.
DA using Pre-trained Models
LM pre-training has been studied extensively . During pre-training, such models are either
trained in an AE setting or in an AR setting. In
the AE setting, certain tokens are masked in the
sentence and the model predicts those tokens. In
an AR setting, the model predicts the next word
given a context. Recently, pre-training for seq2seq
model has been explored where a seq2seq model is
trained for denoising AE tasks . Here, we explore how these
models can be used for DA to potentially improve
text classiﬁcation accuracy.
Algorithm 1:
Data Augmentation approach
Input :Training Dataset Dtrain
Pretrained model G∈{AE,AR,Seq2Seq}
1 Fine-tune G using Dtrain to obtain Gtuned
2 Dsynthetic←{}
3 foreach {xi,yi}∈Dtrain do
Synthesize s examples { ˆxi, ˆyi}1
Dsynthetic←Dsynthetic∪{ ˆxi, ˆyi}1
DA Problem formulation: Given a training
dataset Dtrain = {xi, yi}1
n, where xi = {wj}1
is a sequence of m words, yi is the associated label,
and a pre-trained model G, we want to generate
a dataset of Dsynthetic. Algorithm 1 describes the
data generation process. For all augmentation methods, we generate s = 1 synthetic example for every
1 
TransformersDataAugmentation
example in Dtrain. Thus, the augmented data is
same size as the size of the original data.
Conditional DA using Pre-trained LM
For conditional DA, a model G incorporates label
information during ﬁne-tuning for data generation.
Wu et al. proposed CBERT model where
they utilized BERT’s segment embeddings to condition model on the labels. Similarly, models can
be conditioned on labels by prepending labels yi to
xi .
Due to segment embedding reuse, CBERT conditioning is very speciﬁc to BERT architecture thus
cannot be applied directly to other pre-trained LMs.
Thus, we compare two generic ways to condition a
pre-trained model on class label:
• prepend : prepending label yi to each sequence xi in the training data without adding
yi to model vocabulary
• expand : prepending label yi to each sequence xi in the training data and adding yi
to model vocabulary.
Note that in prepend, the model may split yi
into multiple subword units , expand treats a label as a single token.
Here, we discuss the ﬁne-tuning and the data
generation process for both AE and AR LMs. For
transformer based LM implementation, we use Pytorch based transformer package .
For all pre-trained models, during ﬁne-tuning, we
further train the learnable parameters of G using
its default task and loss function.
Fine-tuning and generation using AE
We choose BERT as a representative of AE models.
For ﬁne-tuning, we use the default masking parameters and MLM objective which randomly masks
some of the tokens from the raw sequence, and
the objective is to predict the original token of the
masked words using the context. Both BERTprepend
and BERTexpand models are ﬁne-tuned using the
same objective.
Fine-tuning and generation using AR
For AR LM experiments, we choose GPT2 as a generator model and follow the method proposed by
Anaby-Tavor et al. to ﬁne-tune and generate
data. For ﬁne-tuning GPT2, we create a training
dataset by concatenating all sequences in Dtrain
as follows: y1SEPx1EOSy2...ynSEPxnEOS.
SEP denotes a separation token between label and
sentence, and EOS denotes the end of a sentence.
For generating data, we provide yiSEP as a
prompt to G, and we keep generating until the
model produces EOS token. We use GPT2 to refer to this model. We found that such generation
struggles in preserving the label information, and
a simple way to improve the generated data label
quality is to provide an additional context to G. Formally, we provide yiSEPw1..wk as prompt where
w1..wk are the ﬁrst k words of a sequence xi. In
this work, we use k = 3. We call this method
GPT2context.
Conditional DA using Pre-trained
Seq2Seq model
Like pre-trained LM models, pre-training seq2seq
models such as T5 and
BART have shown to improve
performance across NLP tasks. For DA experiments, we choose BART as a pre-trained seq2seq
model representative for its relatively lower computational cost.
Fine-tuning and generation using
Seq2Seq BART
Similar to pre-trained LMs, we condition BART
by prepending class labels to all examples of a
given class. While BART can be trained with different denoising tasks including insertion, deletion,
and masking, preliminary experiments showed that
masking performs better than others. Note that
masking can be applied at either word or subword
level. We explored both ways of masking and
found subword masking to be consistently inferior to the word level masking. Finally, we applied
word level masking in two ways:
• BARTword : Replace a word wi with a mask
token < mask >
• BARTspan: Replace a continuous chunk of
k words wi, wi+1..wi+k with a single mask
token < mask >.
Masking was applied to 40% of the words. We
ﬁne-tune BART with a denoising objective where
the goal is to decode the original sequence given a
masked sequence.
Pre-trained Model Implementation
BERT based DA models
For AutoEncoder (AE) experiments, we use “bertbase-uncased” model with the default parameters
provided in huggingface’s transformer package. In
prepend setting we train model for 10 epochs
and select the best performing model on dev data
partition keeping initial learning rate at 4e−5. For
expand setting, training requires 150 epochs to
Moreover, a higher learning rate of
1.5e−4 was used for all three datasets. The initial
learning rate was adjusted for faster convergence.
This is needed for expand setting as embeddings
for labels are randomly initialized.
GPT2 model implementation
For GPT2 experiments, we use GPT2-Small model
provides in huggingface’s transformer package. We
use default training parameters to ﬁne-tune the
GPT2 model. For all experiments, we use SEP
as a separate token and <| endoftext |> as EOS
token. For text generation, we use the default nucleus sampling parameters
including top k = 0, and top p = 0.9.
BART model implementation
For BART model implementation, we use fairseq
toolkit implementation of BART.
Additionally, we used bart large model weights2.
Since BART model already contains < mask >
token, we use it to replace mask words. For BART
model ﬁne-tuning, we use denoising reconstruction task where 40% words are masked and the
goal of the decoder is to reconstruct the original sequence. Note that the label yi is prepended to each
sequence xi, and the decoder also produces the label yi as any other token in xi. We use fairseq’s label smoothed cross entropy criterion with a labelsmoothing of 0.1. We use 1e−5 as learning rate.
For generation, beam search with a beam size of 5
Base classiﬁer implementation
For the text classiﬁer, we use “bert-base-uncased”
model. The BERT model has 12 layers, 768 hidden
states, and 12 heads. We use the pooled representation of the hidden state of the ﬁrst special token
([CLS]) as the sentence representation. A dropout
probability of 0.1 is applied to the sentence representation before passing it to the Softmax layer.
2 
fairseq/models/bart.large.tar.gz
Adam is used for optimization with an initial learning rate of 4e−5. We use
100 warmup steps for BERT classiﬁer. We train the
model for 8 epochs and select the best performing
model on the dev data.
All experiments were conducted using a single GPU instance of Nvidia Tesla v100 type. For
BART model, we use f16 precision. For all data
augmentation models, validation set performance
was used to select the best model.
Experimental Setup
Baseline Approaches for DA
In this work, we consider three data augmentation
methods as our baselines.
(1) EDA is a simple wordreplacement based augmentation method, which
has been shown to improve text classiﬁcation performance in the low-data regime.
(2) Backtranslation 
augmentation
method .
For backtranslation, we use a
pre-trained EN-DE3, and DE-EN4 translation
models .
(3) CBERT language model
which, to the best of our knowledge, is the latest
model-based augmentation that outperforms other
word-replacement based methods.
We use three text classiﬁcation data sets.
(1) SST-2 : (Stanford Sentiment Treebank) is a dataset for sentiment classiﬁcation on movie reviews, which are annotated with
two labels (Positive and Negative).
(2) SNIPS dataset contains
7 intents which are collected from the Snips personal voice assistant.
(3) TREC is a ﬁne-grained
question classiﬁcation dataset sourced from TREC.
It contains six question types (whether the question
is about person, location, etc.).
For SST-2 and TREC, we use the dataset versions provided by
 5, and for
3 
fairseq/models/wmt19.en-de.joined-dict.
single_model.tar.gz
4 
fairseq/models/wmt19.de-en.joined-dict.
single_model.tar.gz
5 
SNIPS dataset, we use 6. We replace numeric class
labels with their text versions. For our experiments,
we used the labels provided in Table 1. Note that
pre-trained methods rely on different byte pair encodings that might split labels into multiple tokens.
For all experiments, we use the lowercase version
of the class labels.
Low-resourced data scenario
Following previous works to simulate the low-data
regime setting for text classiﬁcation , we subsample a small training set on each
task by randomly selecting an equal number of
examples for each class.
In our preliminary experiments, we evaluated
classiﬁcation performance with various degrees of
low-data regime settings, including 10, 50, 100 examples per class. We observed that state-of-the-art
classiﬁers, such as the pre-trained BERT classiﬁer,
performs relatively well for these data sets in a
moderate low-data regime setting. For example,
using 100 training examples per class for SNIPS
dataset, BERT classiﬁer achieves 94% accuracy,
without any data augmentation. In order to simulate a realistic low-resourced data setting where we
often observe poor performance, we focus on experiments with 10 and 50 examples per class. Note
that using a very small dev set leads the model to
achieve 100% accuracy in the ﬁrst epoch which
prevents a fair model selection based on the dev set
performance. To avoid this and to have a reliable
development set, we select ten validation examples
per class.
Evaluation
To evaluate DA, we perform both intrinsic and extrinsic evaluation. For extrinsic evaluation, we
add the generated examples into low-data regime
training data for each task and evaluate the performance on the full test set. All experiments are
repeated 15 times to account for stochasticity. For
each experiment, we randomly subsample both
training and dev set to simulate a low-data regime.
For intrinsic evaluation, we consider two aspects of the generated text. The ﬁrst one is semantic
ﬁdelity, where we measure how well the generated
text retains the meaning and the class information
of the input sentence. In order to measure this, we
train a classiﬁer on each task by ﬁne-tuning a pretrained English BERT-base uncased model. Section
6 
SlotGated-SLU/tree/master/data/snips
Label Names
Positive, Negative
Description, Entity, Abbreviation, Human, Location, Numeric
PlayMusic, GetWeather, RateBook, SearchScreeningEvent, SearchCreativeWork, AddTo-
Playlist, BookRestaurant
Table 1: Label Names used for ﬁne-tuning pre-trained models. Label names are lower-cased for all experiments.
Table 2: Data statistics for three corpora, without any
sub-sampling. This setup is used to train a classiﬁer for
intrinsic evaluation, as described in Section 3.3. When
simulating low-data regime, we sample 10 or 50 training examples from each category. For testing, we use
the full test data.
3.3.1 describes corpus and classiﬁer performance
Another aspect we consider is text diversity. To
compare different models’ ability to generate diverse output, we measured type token ratio . Type token ratio is calculated
by dividing the number of unique n-grams by the
number of all n-grams in the generated text.
Classiﬁers for intrinsic evaluation
In this work, we measure semantic ﬁdelity by evaluating how well the generated text retains the meaning and the label information of the input sentence.
To measure this, we ﬁne-tune the base classiﬁer
described in Section 2.4.
To take full advantage of the labeled data and
to make our classiﬁer more accurate, we combine
100% of training and test partitions of the corresponding dataset, and use the combined data for
training. Then, the best classiﬁer is selected based
on the performance on the dev partition. Classiﬁcation accuracy of the best classiﬁer on dev partition
for each corpus is provided in Table 3.
Classiﬁer performance on dev set for each
corpus. Classiﬁers are used for intrinsic evaluation.
Results and Discussion
Generation by Conditioning on Labels
As described in Section 2.1, we choose BERT as
a pre-trained model and explored different ways
of conditioning BERT on labels: BERTprepend,
BERTexpand and CBERT.
Table 4 shows BERTprepend, BERTexpand and
CBERT have similar performance on three datasets.
Note that BERT is pre-trained on a very huge
corpus, but ﬁne-tuning is applied on a limited
data. This makes it difﬁcult for the model to learn
new, meaningful label representations from scratch
as in case the BERTexpand. While CBERT and
BERTprepend both converge in less than 8 epochs,
BERTexpand requires more than 100 epochs to converge.
Further, the class conditioning technique used
in CBERT is speciﬁc to BERT architecture which
relies on modifying BERT’s segment embedding
and hence cannot be applied to other model architectures. Since the labels in most of the datasets are
well-associated with the meaning of the class (e.g.
SearchCreativeWork), prepending tokens allows
the model to leverage label information for conditional word replacement. Given these insights, we
recommend prepend as a preferred technique for
pre-trained model based data augmentation.
Pre-trained Model Comparison
Classiﬁcation Performance
Table 4 shows that
seq2seq pre-training based BART outperforms
other DA approaches on all data sets. We also
observe that back translation (shown as BackTrans.
in table) is a very strong baseline as it consistently
outperforms several pre-trained data augmentation
techniques including CBERT baseline.
Generated Data Fidelity
As described in Section 3.3.1, we train a classiﬁer for each dataset and
use the trained classiﬁer to predict the label of the
generated text.
Table 5 shows that AE based methods outperform AR models like GPT2, and Seq2seq based
52.93 (5.01)
79.38 (3.20)
48.56 (11.53)
53.82 (4.44)
85.78 (2.96)
52.57 (10.49)
BackTrans.
57.45 (5.56)
86.45 (2.40)
66.16 (8.52)
57.36 (6.72)
85.79 (3.46)
64.33 (10.90)
BERTexpand
56.34 (6.48)
86.11 (2.70)
65.33 (6.05)
BERTprepend
56.11 (6.33)
86.77 (1.61)
64.74 (9.61)
GPT2context
55.40 (6.71)
86.59 (2.73)
54.29 (10.12)
57.97 (6.80)
86.78 (2.59)
63.73 (9.84)
57.68 (7.06)
87.24 (1.39)
67.30 (6.13)
Table 4: DA extrinsic evaluation in low-data regime. Results are reported as Mean (STD) accuracy on the full test
set. Experiments are repeated 15 times on randomly sampled training and dev data. For data augmentation model
ﬁne-tuning, we use 10 examples per class for training.
BackTrans.
BERTexpand
BERTprepend
GPT2context
Semantic ﬁdelity of generated output. We
trained a classiﬁer using all labelled data in order to
perform accuracy test on the generated data. Higher
accuracy score means that the model retains the class
label of the input sentence more accurately. For data
augmentation model ﬁne-tuning, we use 10 examples
per class for training.
model like BART, in terms of semantic ﬁdelity of
the generated data. On two datasets, back translation approach outperforms all other methods in
terms of ﬁdelity which underlines the effectiveness
of the state of the art translation systems in terms
of preserving the semantics of the language.
Generated Data Diversity
To further analyze
the generated data, we explore type token ratio
as described in Section 3.3. Table 6 shows that
EDA generates the most diverse tri-grams and back
translation approach produces the most diverse unigrams. Since EDA method modiﬁes tokens at random, it leads to more diverse n-grams, not necessarily preserving the semantic of the input sentence.
Also, unlike AE and Seq2seq methods that rely on
word or span replacements, back translation is an
open-ended system that often introduces unseen
BERTexpand
BERTprepend
GPT2context
Table 6: Type token ratio for generated text using each
model. For data augmentation model ﬁne-tuning, we
use 10 examples per class for training.
Guidelines For Using Different Types Of
Pre-trained Models For DA
: We found that simply prepending
the label to raw sequences provides competitive
performance than modifying the model architecture. As expected, more complex AE models such
as RoBERTaprepend outperforms
BERTprepend (66.12 vs 64.74 mean acc on TREC).
: While AR based model such as
GPT2 produces very coherent text, it does not preserve the label well. In our experiments, we found
that providing a few starting words along with the
label as in GPT2context is crucial to generate meaningful data.
Seq2Seq models
: Seq2Seq models provide an
opportunity to experiment with various kinds of
denoising autoencoder tasks including masking at
subword, word or span level, random word insertion or deletion. We observe that word or span
masking performs better than other denoising objectives, and should be preferred for DA.
Overall, we found that while AE models are constrained to produce similar length sequences and
are good at preserving labels, AR models excel at
unconstrained generation but might not retain label
information. Seq2Seq models lie between AE and
AR by providing a good balance between diversity
and semantic ﬁdelity. Further, in Seq2Seq models,
diversity of the generated data can be controlled by
varying the masking ratio.
Limitations
Our paper shows that a pre-trained model can be
used for conditional data augmentation by ﬁnetuning it on the training data where the class labels
are prepended to the training examples. Such a
uniﬁed approach allows utilizing different kinds of
pre-trained models for text data augmentation to
improve performance in low-resourced tasks. However, as shown in Table 7, improving pre-trained
classiﬁer’s model performance in rich-resource setting is still challenging.
Our results also show that a particular pre-trained
model based augmentation may do well on one task
or dataset, but may not work well for other scenarios. In our experiments, we use the same set of
hyperparameters such as masking rate, learning
rate and warmup schedule for all three datasets
which might not lead to the best performance for
all considered tasks. While our primary goal in
this work is to propose a uniﬁed data augmentation
technique, we believe that further studies on optimizing performance for a given task or model will
be beneﬁcial.
Conclusion And Future Work
We show that AE, AR, and Seq2Seq pre-trained
models can be conditioned on labels by prepending
label information and provide an effective way to
augment training data. These DA methods can be
easily combined with other advances in text content
manipulation such as co-training the data generator
and the classiﬁer . Further, the
proposed data augmentation techniques can also be
combined with latent space augmentation . We hope that unifying different DA
methods would inspire new approaches for universal NLP data augmentation.