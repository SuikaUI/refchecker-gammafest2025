Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 606–615,
Austin, Texas, November 1-5, 2016. c⃝2016 Association for Computational Linguistics
Attention-based LSTM for Aspect-level Sentiment Classiﬁcation
Yequan Wang and Minlie Huang and Li Zhao* and Xiaoyan Zhu
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
*Microsoft Research Asia
 , 
 , 
Aspect-level sentiment classiﬁcation is a ﬁnegrained task in sentiment analysis. Since it
provides more complete and in-depth results,
aspect-level sentiment analysis has received
much attention these years. In this paper, we
reveal that the sentiment polarity of a sentence
is not only determined by the content but is
also highly related to the concerned aspect.
For instance, “The appetizers are ok, but the
service is slow.”, for aspect taste, the polarity is positive while for service, the polarity
is negative. Therefore, it is worthwhile to explore the connection between an aspect and
the content of a sentence. To this end, we
propose an Attention-based Long Short-Term
Memory Network for aspect-level sentiment
classiﬁcation. The attention mechanism can
concentrate on different parts of a sentence
when different aspects are taken as input. We
experiment on the SemEval 2014 dataset and
results show that our model achieves state-ofthe-art performance on aspect-level sentiment
classiﬁcation.
Introduction
Sentiment analysis , also
known as opinion mining , is a key
NLP task that receives much attention these years.
Aspect-level sentiment analysis is a ﬁne-grained
task that can provide complete and in-depth results.
In this paper, we deal with aspect-level sentiment
classiﬁcation and we ﬁnd that the sentiment polarity of a sentence is highly dependent on both content and aspect. For example, the sentiment polarity
of “Staffs are not that friendly, but the taste covers
all.” will be positive if the aspect is food but negative when considering the aspect service. Polarity
could be opposite when different aspects are considered.
Neural networks have achieved state-of-the-art
performance in a variety of NLP tasks such as machine translation , paraphrase
identiﬁcation , question answering and text summarization .
However, neural network models are still in infancy to deal with aspectlevel sentiment classiﬁcation. In some works, target dependent sentiment classiﬁcation can be beneﬁted from taking into account target information,
such as in Target-Dependent LSTM (TD-LSTM)
and Target-Connection LSTM (TC-LSTM) . However, those models can only take
into consideration the target but not aspect information which is proved to be crucial for aspect-level
classiﬁcation.
Attention has become an effective mechanism to
obtain superior results, as demonstrated in image
recognition , machine translation , reasoning about entailment and sentence summarization . Even more, neural
attention can improve the ability to read comprehension . In this paper, we propose an attention mechanism to enforce the model
to attend to the important part of a sentence, in response to a speciﬁc aspect. We design an aspect-tosentence attention mechanism that can concentrate
on the key part of a sentence given the aspect.
We explore the potential correlation of aspect and
sentiment polarity in aspect-level sentiment classiﬁcation. In order to capture important information in
response to a given aspect, we design an attentionbased LSTM. We evaluate our approach on a benchmark dataset , which contains
restaurants and laptops data.
The main contributions of our work can be summarized as follows:
• We propose attention-based Long Short-Term
memory for aspect-level sentiment classiﬁcation.
The models are able to attend different parts of a sentence when different aspects
are concerned. Results show that the attention
mechanism is effective.
• Since aspect plays a key role in this task, we
propose two ways to take into account aspect
information during attention: one way is to
concatenate the aspect vector into the sentence
hidden representations for computing attention
weights, and another way is to additionally append the aspect vector into the input word vectors.
• Experimental results indicate that our approach can improve the performance compared
with several baselines, and further examples
demonstrate the attention mechanism works
well for aspect-level sentiment classiﬁcation.
The rest of our paper is structured as follows:
Section 2 discusses related works, Section 3 gives a
detailed description of our attention-based proposals, Section 4 presents extensive experiments to justify the effectiveness of our proposals, and Section 5
summarizes this work and the future direction.
Related Work
In this section, we will review related works on
aspect-level sentiment classiﬁcation and neural networks for sentiment classiﬁcation brieﬂy.
Sentiment Classiﬁcation at Aspect-level
Aspect-level sentiment classiﬁcation is typically
considered as a classiﬁcation problem in the literature. As we mentioned before, aspect-level sentiment classiﬁcation is a ﬁne-grained classiﬁcation
task. The majority of current approaches attempt to
detecting the polarity of the entire sentence, regardless of the entities mentioned or aspects. Traditional
approaches to solve those problems are to manually design a set of features. With the abundance of
sentiment lexicons , the lexicon-based features were built for sentiment analysis . Most of
these studies focus on building sentiment classiﬁers
with features, which include bag-of-words and sentiment lexicons, using SVM . However, the results highly depend on the
quality of features. In addition, feature engineering
is labor intensive.
Sentiment Classiﬁcation with Neural
Since a simple and effective approach to learn distributed representations was proposed , neural networks advance sentiment analysis substantially. Classical models including Recursive Neural Network , Recursive Neural Tensor Network , Recurrent Neural Network , LSTM and Tree-LSTMs were applied into sentiment analysis currently. By utilizing
syntax structures of sentences, tree-based LSTMs
have been proved to be quite effective for many NLP
tasks. However, such methods may suffer from syntax parsing errors which are common in resourcelacking languages.
LSTM has achieved a great success in various
NLP tasks.
TD-LSTM and TC-LSTM , which took target information into consideration, achieved state-of-the-art performance
in target-dependent sentiment classiﬁcation.
LSTM obtained a target vector by averaging the
vectors of words that the target phrase contains.
However, simply averaging the word embeddings of
a target phrase is not sufﬁcient to represent the semantics of the target phrase, resulting a suboptimal
performance.
Despite the effectiveness of those methods, it is
still challenging to discriminate different sentiment
polarities at a ﬁne-grained aspect level. Therefore,
we are motivated to design a powerful neural network which can fully employ aspect information for
sentiment classiﬁcation.
Attention-based LSTM with Aspect
Long Short-term Memory (LSTM)
Recurrent Neural Network(RNN) is an extension of
conventional feed-forward neural network. However, standard RNN has the gradient vanishing
or exploding problems.
In order to overcome
the issues,
Long Short-term Memory network
(LSTM) was developed and achieved superior performance . In
the LSTM architecture, there are three gates and a
cell memory state. Figure 1 illustrates the architecture of a standard LSTM.
architecture
{w1, w2, . . . , wN} represent the word vector in a sentence whose length is N.
{h1, h2, . . . , hN} is the hidden
More formally, each cell in LSTM can be computed as follows:
ft = σ(Wf · X + bf)
it = σ(Wi · X + bi)
ot = σ(Wo · X + bo)
ct = ft ⊙ct−1 + it ⊙tanh(Wc · X + bc)
ht = ot ⊙tanh(ct)
where Wi, Wf, Wo ∈Rd×2d are the weighted matrices and bi, bf, bo ∈Rd are biases of LSTM to be
learned during training, parameterizing the transformations of the input, forget and output gates respectively. σ is the sigmoid function and ⊙stands for
element-wise multiplication. xt includes the inputs
of LSTM cell unit, representing the word embedding vectors wt in Figure 1. The vector of hidden
layer is ht.
We regard the last hidden vector hN as the representation of sentence and put hN into a softmax
layer after linearizing it into a vector whose length is
equal to the number of class labels. In our work, the
set of class labels is {positive, negative, neutral}.
LSTM with Aspect Embedding
Aspect information is vital when classifying the polarity of one sentence given aspect. We may get opposite polarities if different aspects are considered.
To make the best use of aspect information, we propose to learn an embedding vector for each aspect.
Vector vai ∈Rda is represented for the embedding of aspect i, where da is the dimension of aspect
embedding. A ∈Rda×|A| is made up of all aspect
embeddings. To the best of our knowledge, it is the
ﬁrst time to propose aspect embedding.
Attention-based LSTM (AT-LSTM)
The standard LSTM cannot detect which is the important part for aspect-level sentiment classiﬁcation.
In order to address this issue, we propose to design an attention mechanism that can capture the
key part of sentence in response to a given aspect.
Figure 2 represents the architecture of an Attentionbased LSTM (AT-LSTM).
Let H ∈Rd×N be a matrix consisting of hidden vectors [h1, . . . , hN] that the LSTM produced,
where d is the size of hidden layers and N is the
length of the given sentence. Furthermore, va represents the embedding of aspect and eN ∈RN is a
vector of 1s. The attention mechanism will produce
an attention weight vector α and a weighted hidden
Word Representation
Aspect Embedding
Figure 2: The Architecture of Attention-based LSTM. The aspect embeddings have been used to decide the attention weights
along with the sentence representations. {w1, w2, . . . , wN} represent the word vector in a sentence whose length is N. va
represents the aspect embedding. α is the attention weight. {h1, h2, . . . , hN} is the hidden vector.
representation r.
α = softmax(wT M)
∈R(d+da)×N, α ∈RN, r ∈Rd.
Wh ∈Rd×d, Wv ∈Rda×da and w ∈Rd+da are
projection parameters. α is a vector consisting of
attention weights and r is a weighted representation
of sentence with given aspect. The operator in 7 (a
circle with a multiplication sign inside, OP for short
here) means: va⊗eN = [v; v; . . . ; v], that is, the operator repeatedly concatenates v for N times, where
eN is a column vector with N 1s. Wvva ⊗eN is
repeating the linearly transformed va as many times
as there are words in sentence.
The ﬁnal sentence representation is given by:
h∗= tanh(Wpr + WxhN)
where, h∗∈Rd, Wp and Wx are projection parameters to be learned during training. We ﬁnd that this
works practically better if we add WxhN into the ﬁnal representation of the sentence, which is inspired
by .
The attention mechanism allows the model to
capture the most important part of a sentence when
different aspects are considered.
h∗is considered as the feature representation of
a sentence given an input aspect. We add a linear
layer to convert sentence vector to e, which is a realvalued vector with the length equal to class number
|C|. Then, a softmax layer is followed to transform e to conditional probability distribution.
y = softmax(Wsh∗+ bs)
where Ws and bs are the parameters for softmax
Attention-based LSTM with Aspect
Embedding (ATAE-LSTM)
The way of using aspect information in AE-LSTM
is letting aspect embedding play a role in computing the attention weight.
In order to better
take advantage of aspect information, we append
the input aspect embedding into each word input
The structure of this model is illustrated
3. In this way, the output hidden representations (h1, h2, ..., hN) can have the information from
the input aspect (va). Therefore, in the following
step that compute the attention weights, the inter-
Word Representation
Aspect Embedding
Aspect Embedding
Figure 3: The Architecture of Attention-based LSTM with Aspect Embedding. The aspect embeddings have been take as input
along with the word embeddings. {w1, w2, . . . , wN} represent the word vector in a sentence whose length is N. va represents the
aspect embedding. α is the attention weight. {h1, h2, . . . , hN} is the hidden vector.
dependence between words and the input aspect can
be modeled.
Model Training
The model can be trained in an end-to-end way by
backpropagation, where the objective function (loss
function) is the cross-entropy loss. Let y be the target distribution for sentence, ˆy be the predicted sentiment distribution. The goal of training is to minimize the cross-entropy error between y and ˆy for all
sentences.
i + λ||θ||2
where i is the index of sentence, j is the index of
class. Our classiﬁcation is three way. λ is the L2 regularization term. θ is the parameter set.
Similar to standard LSTM, the parameter set
is {Wi, bi, Wf, bf, Wo, bo, Wc, bc, Ws, bs}.
Furthermore, word embeddings are the parameters
too. Note that the dimension of Wi, Wf, Wo, Wc
changes along with different models. If the aspect
embeddings are added into the input of the LSTM
cell unit, the dimension of Wi, Wf, Wo, Wc will be
enlarged correspondingly.
Additional parameters
are listed as follows:
AT-LSTM: The aspect embedding A is added
into the set of parameters naturally.
In addition,
Wh, Wv, Wp, Wx, w are the parameters of attention. Therefore, the additional parameter set of AT-
LSTM is {A, Wh, Wv, Wp, Wx, w}.
The parameters include the aspect embedding A.
Besides, the dimension of
Wi, Wf, Wo, Wc will be expanded since the aspect
vector is concatenated. Therefore, the additional parameter set consists of {A}.
ATAE-LSTM:
The parameter set consists of
{A, Wh, Wv, Wp, Wx, w}. Additionally, the dimension of Wi, Wf, Wo, Wc will be expanded with the
concatenation of aspect embedding.
The word embedding and aspect embedding are
optimized during training. The percentage of outof-vocabulary words is about 5%, and they are randomly initialized from U(−ϵ, ϵ), where ϵ = 0.01.
In our experiments, we use AdaGrad as our optimization method, which has
improved the robustness of SGD on large scale
learning task remarkably in a distributed environment . AdaGrad adapts the learning rate to the parameters, performing larger updates
for infrequent parameters and smaller updates for
frequent parameters.
Experiment
We apply the proposed model to aspect-level sentiment classiﬁcation. In our experiments, all word
vectors are initialized by Glove1 . The word embedding vectors are pre-trained
on an unlabeled corpus whose size is about 840 billion. The other parameters are initialized by sampling from a uniform distribution U(−ϵ, ϵ).
dimension of word vectors, aspect embeddings and
the size of hidden layer are 300. The length of attention weights is the same as the length of sentence.
Theano is used for implementing our neural network models. We trained all models with a batch size of 25 examples, and a momentum of 0.9, L2-regularization weight of 0.001 and
initial learning rate of 0.01 for AdaGrad.
We experiment on the dataset of SemEval 2014 Task
42 . The dataset consists of
customers reviews. Each review contains a list of
aspects and corresponding polarities. Our aim is to
identify the aspect polarity of a sentence with the
corresponding aspect. The statistics is presented in
Task Deﬁnition
Aspect-level Classiﬁcation
Given a set of preidentiﬁed aspects, this task is to determine the
polarity of each aspect.
For example, given a
sentence, “The restaurant was too expensive.”,
there is an aspect price whose polarity is negative.
The set of aspects is {food, price, service, ambience, anecdotes/miscellaneous}. In the dataset of
SemEval 2014 Task 4, there is only restaurants
data that has aspect-speciﬁc polarities.
1Pre-trained word vectors of Glove can be obtained from
 
2The introduction about SemEval 2014 can be obtained
from 
Table 1: Aspects distribution per sentiment class. {Fo., Pr.,
Se, Am., An.} refer to {food, price, service, ambience, anecdotes/miscellaneous}. “Asp.” refers to aspect.
Table 2: Accuracy on aspect level polarity classiﬁcation about
restaurants. Three-way stands for 3-class prediction. Pos./Neg.
indicates binary prediction where ignoring all neutral instances.
Best scores are in bold.
illustrates the comparative results.
Aspect-Term-level Classiﬁcation For a given set
of aspects term within a sentence, this task is to determine whether the polarity of each aspect term is
positive, negative or neutral. We conduct experiments on the dataset of SemEval 2014 Task 4. In
the sentences of both restaurant and laptop datasets,
there are the location and sentiment polarity for
each occurrence of an aspect term. For example,
there is an aspect term fajitas whose polarity is negative in sentence “I loved their fajitas.”.
Experiments results are shown in Table 3 and Table 4.
Similar to the experiment on aspect-level
classiﬁcation, our models achieve state-of-the-art
performance.
Comparison with baseline methods
We compare our model with several baselines, including LSTM, TD-LSTM, and TC-LSTM.
LSTM: Standard LSTM cannot capture any aspect information in sentence, so it must get the same
(a) the aspect of this sentence: service
(b) the aspect of this sentence: food
Figure 4: Attention Visualizations. The aspects of (a) and (b) are service and food respectively. The color depth expresses the
importance degree of the weight in attention vector α. From (a), attention can detect the important words from the whole sentence
dynamically even though multi-semantic phrase such as “fastest delivery times” which can be used in other areas. From (b),
attention can know multi-keypoints if more than one keypoint existing.
Table 3: Accuracy on aspect term polarity classiﬁcation about
restaurants. Three-way stands for 3-class prediction. Pos./Neg.
indicates binary prediction where ignoring all neutral instances.
Best scores are in bold.
Table 4: Accuracy on aspect term polarity classiﬁcation about
laptops. Three-way stands for 3-class prediction. Pos./Neg. indicates binary prediction where ignoring all neutral instances.
Best scores are in bold.
sentiment polarity although given different aspects.
Since it cannot take advantage of the aspect information, not surprisingly the model has worst performance.
TD-LSTM: TD-LSTM can improve the performance of sentiment classiﬁer by treating an aspect
as a target. Since there is no attention mechanism in
TD-LSTM, it cannot “know” which words are important for a given aspect.
TC-LSTM: TC-LSTM extended TD-LSTM by
incorporating a target into the representation of a
It is worth noting that TC-LSTM performs worse than LSTM and TD-LSTM in Table 2.
TC-LSTM added target representations, which was
obtained from word vectors, into the input of the
LSTM cell unit.
In our models, we embed aspects into another
vector space. The embedding vector of aspects can
be learned well in the process of training. ATAE-
LSTM not only addresses the shortcoming of the
unconformity between word vectors and aspect embeddings, but also can capture the most important
information in response to a given aspect. In addition, ATAE-LSTM can capture the important and
different parts of a sentence when given different
Qualitative Analysis
It is enlightening to analyze which words decide the
sentiment polarity of the sentence given an aspect.
We can obtain the attention weight α in Equation 8
and visualize the attention weights accordingly.
Figure 4 shows the representation of how attention focuses on words with the inﬂuence of a given
We use a visualization tool Heml (Deng
The appetizers are ok, but the service is slow.
I highly recommend it for not just its superb cuisine, but also for its friendly owners and staff.
The service, however, is a peg or two below the quality of food (horrible bartenders), and
the clientele, for the most part, are rowdy, loud-mouthed commuters (this could explain the
bad attitudes from the staff) getting loaded for an AC/DC concert or a Knicks game.
aspect: service; polarity: negative
aspect: food; polarity: neutral
aspect: food; polarity: positive
aspect: food; polarity: positive
aspect: food; polarity: positive
aspect: service; polarity: positive
aspect: ambience; polarity: negative
Figure 5: Examples of classiﬁcation. (a) is an instance with different aspects. (b) represents that our model can focus on where
the keypoints are and not disturbed by the privative word not. (c) stands for long and complicated sentences. Our model can obtain
correct sentiment polarity.
et al., 2014) to visualize the sentences. The color
depth indicates the importance degree of the weight
in attention vector α, the darker the more important.
The sentences in Figure 4 are “I have to say they
have one of the fastest delivery times in the city .”
and “The fajita we tried was tasteless and burned
and the mole sauce was way too sweet.”. The corresponding aspects are service and food respectively.
Obviously attention can get the important parts from
the whole sentence dynamically. In Figure 4 (a),
“fastest delivery times” is a multi-word phrase, but
our attention-based model can detect such phrases
if service can is the input aspect. Besides, the attention can detect multiple keywords if more than one
keyword is existing. In Figure 4 (b), tastless and too
sweet are both detected.
Case Study
As we demonstrated, our models obtain the state-ofthe-art performance. In this section, we will further
show the advantages of our proposals through some
typical examples.
In Figure 5, we list some examples from the test
set which have typical characteristics and cannot be
inferred by LSTM. In sentence (a), “The appetizers are ok, but the service is slow.”, there are two
aspects food and service. Our model can discriminate different sentiment polarities with different aspects. In sentence (b), “I highly recommend it for
not just its superb cuisine, but also for its friendly
owners and staff.”, there is a negation word not. Our
model can obtain correct polarity, not affected by
the negation word who doesn’t represent negation
here. In the last instance (c), “The service, however,
is a peg or two below the quality of food (horrible bartenders), and the clientele, for the most part,
are rowdy, loud-mouthed commuters (this could explain the bad attitudes from the staff) getting loaded
for an AC/DC concert or a Knicks game.”, the sentence has a long and complicated structure so that
existing parser may hardly obtain correct parsing
Hence, tree-based neural network models
are difﬁcult to predict polarity correctly. While our
attention-based LSTM can work well in those sentences with the help of attention mechanism and aspect embedding.
Conclusion and Future Work
In this paper, we have proposed attention-based
LSTMs for aspect-level sentiment classiﬁcation.
The key idea of these proposals are to learn aspect
embeddings and let aspects participate in computing
attention weights. Our proposed models can concentrate on different parts of a sentence when different aspects are given so that they are more competitive for aspect-level classiﬁcation. Experiments
show that our proposed models, AE-LSTM and
ATAE-LSTM, obtain superior performance over the
baseline models.
Though the proposals have shown potentials for
aspect-level sentiment analysis, different aspects are
input separately.
As future work, an interesting
and possible direction would be to model more than
one aspect simultaneously with the attention mechanism.
Acknowledgments
This work was partly supported by the National
Basic Research Program (973 Program) under
grant No.2012CB316301/2013CB329403, the National Science Foundation of China under grant
No.61272227/61332007, and the Beijing Higher
Education Young Elite Teacher Project. The work
was also supported by Tsinghua University Beijing
Samsung Telecom R&D Center Joint Laboratory for
Intelligent Media Computing.