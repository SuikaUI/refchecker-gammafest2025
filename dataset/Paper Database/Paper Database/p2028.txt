Zero-Shot Text-Guided Object Generation with Dream Fields
Ajay Jain1,2∗
Ben Mildenhall2
Jonathan T. Barron2
Pieter Abbeel1
Ben Poole2
Caption: “Washing blueberries”
Transmittance
Transmittance loss
encouraging sparsity
Neural Radiance Field
Differentiabl
Figure 1. Given a caption, we learn a Dream Field, a continuous volumetric
representation of an object’s geometry and appearance learned with guidance
from a pre-trained model. We optimize the Dream Field by rendering images
of the object from random camera poses that are scored with frozen pretrained image and text encoders trained on web images and alt-text. 2D
views share the same underlying radiance ﬁeld for consistent geometry.
Figure 2. Example Dream Fields rendered from four
perspectives. On the right, we show transmittance from
the ﬁnal perspective. We create diverse outputs using
the compositionality of language; these captions from
MSCOCO describe three ﬂower arrangements with different properties like context and color.
We combine neural rendering with multi-modal image and
text representations to synthesize diverse 3D objects solely
from natural language descriptions. Our method, Dream
Fields, can generate the geometry and color of a wide range
of objects without 3D supervision. Due to the scarcity of
diverse, captioned 3D data, prior methods only generate objects from a handful of categories, such as ShapeNet. Instead,
we guide generation with image-text models pre-trained on
large datasets of captioned images from the web. Our method
optimizes a Neural Radiance Field from many camera views
so that rendered images score highly with a target caption according to a pre-trained CLIP model. To improve ﬁdelity and
visual quality, we introduce simple geometric priors, including sparsity-inducing transmittance regularization, scene
bounds, and new MLP architectures. In experiments, Dream
Fields produce realistic, multi-view consistent object geometry and color from a variety of natural language captions.
1UC Berkeley, 2Google Research.
∗Work done at Google.
Correspondence to .
Project website and code: 
1. Introduction
Detailed 3D object models bring multimedia experiences
to life. Games, virtual reality applications and ﬁlms are each
populated with thousands of object models, each designed
and textured by hand with digital software. While expert
artists can author high-ﬁdelity assets, the process is painstakingly slow and expensive. Prior work leverages 3D datasets
to synthesize shapes in the form of point clouds, voxel grids,
triangle meshes, and implicit functions using generative models like GANs . These approaches only support a few object categories due to small labeled 3D shape
datasets. But multimedia applications require a wide variety
of content, and need both 3D geometry and texture.
In this work, we propose Dream Fields, a method to
automatically generate open-set 3D models from natural
language prompts. Unlike prior work, our method does
not require any 3D training data, and uses natural language
prompts that are easy to author with an expressive interface
for specifying desired object properties. We demonstrate that
the compositionality of language allows for ﬂexible creative
control over shapes, colors and styles.
A Dream Field is a Neural Radiance Field (NeRF) trained
to maximize a deep perceptual metric with respect to both
 
the geometry and color of a scene. NeRF and other neural 3D
representations have recently been successfully applied to
novel view synthesis tasks where ground-truth RGB photos
are available. NeRF is trained to reconstruct images from
multiple viewpoints. As the learned radiance ﬁeld is shared
across viewpoints, NeRF can interpolate between viewpoints
smoothly and consistently. Due to its neural representation,
NeRF can be sampled at high spatial resolutions unlike voxel
representations and point clouds, and are easy to optimize
unlike explicit geometric representations like meshes as it is
topology-free.
However, existing photographs are not available when
creating novel objects from descriptions alone. Instead of
learning to reconstruct known input photos, we learn a radiance ﬁeld such that its renderings have high semantic similarity with a given text prompt. We extract these semantics with
pre-trained neural image-text retrieval models like CLIP ,
learned from hundreds of millions of captioned images. As
NeRF’s volumetric rendering and CLIP’s image-text representations are differentiable, we can optimize Dream Fields
end-to-end for each prompt. Figure 1 illustrates our method.
In experiments, Dream Fields learn signiﬁcant artifacts
if we naively optimize the NeRF scene representation with
textual supervision without adding additional geometric constraints (Figure 3). We propose general-purpose priors and
demonstrate that they greatly improve the realism of results.
Finally, we quantitatively evaluate open-set generation performance using a dataset of diverse object-centric prompts.
Our contributions include:
• Using aligned image and text models to optimize NeRF
without 3D shape or multi-view data,
• Dream Fields, a simple, constrained 3D representation
with neural guidance that supports diverse 3D object
generation from captions in zero-shot, and
• Simple geometric priors including transmittance regularization, scene bounds, and an MLP architecture that
together improve ﬁdelity.
2. Related Work
Our work is primarily inspired by DeepDream and
other methods for visualizing the preferred inputs and features of neural networks by optimizing in image space
 . These methods enable the generation of interesting images from a pre-trained neural network without
the additional training of a generative model. Closest to our
work is , which studies differentiable image parameterizations in the context of style transfer. Our work replaces the
style and content-based losses from that era with an imagetext loss enabled by progress in contrastive representation
learning on image-text datasets . The use of
image-text models enables easy and ﬂexible control over
the style and content of generated imagery through textual
prompt design. We optimize both geometry and color using
“a large blue bird standing
next to a painting of ﬂowers.”
(c) Dream Fields
(b) Neural Radiance Fields
conditioning information
(latent, pose)
(a) Category-speciﬁc generator
Figure 3. Challenges of text-to-3D synthesis: (a) Poor generalization from limited 3D datasets: Most 3D generative models are
learned on datasets of speciﬁc object categories like ShapeNet ,
and won’t generalize to novel concepts zero-shot. (b) Neural Radiance Fields are too ﬂexible without multi-view supervision:
NeRF learns to represent geometry and texture from scene-speciﬁc
multi-view data, so it does not require a diverse dataset of objects.
Yet, when only a source caption is available instead of multi-view
images, NeRF produces signiﬁcant artifacts (e.g., near ﬁeld occlusions). (c) Dream Fields: We introduce general geometric priors
that retain much of NeRF’s ﬂexibility while improving realism.
the differentiable volumetric rendering and scene representation provided by NeRF, whereas was restricted to ﬁxed
geometry and only optimized texture. Together these advances enable a fundamentally new capability: open-ended
text-guided generation of object geometry and texture.
Concurrently to Dream Fields, a few early works have
used CLIP to synthesize or manipulate 3D object representations. CLIP-Forge generates multiple object
geometries from text prompts using a CLIP embeddingconditioned normalizing ﬂow model and geometry-only decoder trained on ShapeNet categories. Still, CLIP-Forge generalizes poorly outside of ShapeNet categories and requires
ground-truth multi-view images and voxel data. Text2Shape
 learns a text-conditional Wasserstein GAN to
synthesize novel voxelized objects, but only supports ﬁnite resolution generation of individual ShapeNet categories.
In , object geometry is optimized evolutionarily for
high CLIP score from a single view then manually colored. ClipMatrix edits the vertices and textures of
human SMPL models to create stylized, deformable humanoid meshes. creates an interactive interface to edit
signed-distance ﬁelds in localized regions, though they do
not optimize texture or synthesize new shapes. Text-based
manipulation of existing objects is complementary to us.
For images, there has been an explosion of work that
leverages CLIP to guide image generation. Digital artist
Ryan Murdock (@advadnoun) used CLIP to guide learning
of the weights of a SIREN network , similar to NeRF
but without volume rendering and focused on image generation. Katherine Crowson (@rivershavewings) combined
CLIP with optimization of VQ-GAN codes and used
diffusion models as an image prior . Recent work from
Mario Klingemann (@quasimondo) and have shown how
CLIP can be used to guide GAN models like StyleGAN .
Some works have optimized parameters of vector graphics, suggesting CLIP guidance is highly general .
These methods highlighted the surprising capacity of what
image-text models have learned and their utility for guiding
2D generative processes. Direct text to image synthesis with
generative models has also improved tremendously in recent
years , but requires training large generative models
on large-scale datasets, making such methods challenging to
directly apply to text to 3D where no such datasets exist.
There is also growing progress on generative models with NeRF-based generators trained solely from 2D
However, these models are category-speciﬁc
and trained on large datasets of mostly forward-facing
scenes , lacking the ﬂexibility of open-set
text-conditional models. Shape-agnostic priors have been
used for 3D reconstruction .
3. Background
Our method combines Neural Radiance Fields (NeRF)
 with an image-text loss from . We begin by discussing these existing methods, and then detail our improved
approach and methodology that enables high quality text to
object generation.
3.1. Neural Radiance Fields
NeRF parameterizes a scene’s density and color using a multi-layer perceptron (MLP) with parameters θ trained
with a photometric loss relying on multi-view photographs
of a scene. In our simpliﬁed model, the NeRF network takes
in a 3D position x and outputs parameters for an emissionabsorption volume rendering model: density σθ(x) and color
cθ(x). Images can be rendered from desired viewpoints by
integrating color along an appropriate ray, r(t), for each
pixel according to the volume rendering equation:
T(r, t)σθ(r(t))cθ(r(t))dt,
where T(r, θ, t) = exp
σθ(r(s))ds
The integral T(r, θ, t) is known as “transmittance” and describes the probability that light along the ray will not be
absorbed when traveling from tn (the near scene bound) to
t. In practice , these two integrals are approximated by
breaking up the ray into smaller segments [ti−1, ti) within
which σ and c are assumed to be roughly constant:
Ti(1 −exp(−σθ(r(ti))δi))cθ(r(ti))
j<i σθ(r(tj))δj
δi = ti −ti−1 . (4)
For a given setting of MLP parameters θ and pose p, we determine the appropriate ray for each pixel, compute rendered
colors C(r, θ) and transmittances, and gather the results to
form the rendered image, I(θ, p) and transmittance T(θ, p).
In order for the MLP to learn high frequency details more
quickly , the input x is preprocessed by a sinusoidal
positional encoding γ before being passed into the network:
cos(2lx), sin(2lx)
where L is referred to as the number of “levels” of positional
encoding. In our implementation, we speciﬁcally apply the
integrated positional encoding (IPE) proposed in mip-NeRF
to combat aliasing artifacts combined with a random
Fourier positional encoding basis with frequency components sampled according to
where u ∼U[0, L], d ∼U(S2) .
3.2. Image-text models
Large-scale datasets of images paired with associated text
have enabled training large-scale models that can accurately
score whether an image and an associated caption are likely
to correspond . These models consist of an image
encoder g, and text encoder h, that map images and text
into a shared embedding space. Given a sentence y and an
image I, these image-text models produce a scalar score:
g(I)Th(y) that is high when the text is a good description
of the image, and low when the the image and text are mismatched. Note that the embeddings g(I) and h(y) are often
normalized, i.e. ∥g(I)∥= ∥h(y)∥= 1. Training is typically performed with a symmetric version of the InfoNCE
loss that aims to maximize a variational lower bound
on the mutual information between images and text. Prior
work has shown that once trained, the image and text encoders are useful for a number of downstream tasks .
In , the image and text encoders are used to score the
correspondence of outputs of a generative image model to a
target caption . We build on this work by optimizing a
volume to produce a high-scoring image, not just reranking.
In this section, we develop Dream Fields: a zero-shot object synthesis method given only a natural language caption.
4.1. Object representation
Building on the NeRF scene representation (Section 3.1),
a Dream Field optimizes an MLP with parameters θ that produces outputs σθ(x) and cθ(x) representing the differential
volume density and color of a scene at every 3D point x.
This ﬁeld expresses object geometry via the density network.
Our object representation is only dependent on 3D coordinates and not the camera’s viewing direction, as we did not
ﬁnd it beneﬁcial. Given a camera pose p, we can render an
image I(θ, p) and compute the transmittance T(θ, p) using
N segments via (4). Segments are spaced at roughly equal
intervals with random jittering along the ray. The number
of segments, N, determines the ﬁdelity of the rendering. In
practice, we ﬁx it to 192 during optimization.
4.2. Objective
How can we train a Dream Field to represent a given caption? If we assume that an object can be described similarly
when observed from any perspective, we can randomly sample poses and try to enforce that the rendered image matches
the caption at all poses. We can implement this idea by using
a CLIP network to measure the match between a caption and
image given parameters θ and pose p:
LCLIP(θ, pose p, caption y) = −g(I(θ, p))Th(y)
where g(·) and h(·) are aligned representations of image and
text semantics, and I(θ, p) is a rendered image of the scene
from camera pose p. Each iteration of training, we sample
a pose p from a prior distribution, render I, and minimize
LCLIP with respect to the parameters of the Dream Field
MLP, θ. Equation (7) measures the similarity of an image
and the provided caption in feature space.
We primarily use image and text encoders from CLIP ,
which has a Vision Transformer image encoder g(·) 
and masked transformer text encoder h(·) trained contrastively on a large dataset of 400M captioned 2242 images.
We also use a baseline Locked Image-Text Tuning (LiT)
ViT B/32 model from trained via the same procedure
as CLIP on a larger dataset of billions of higher-resolution
(2882) captioned images. The LiT training set was collected
following a simpliﬁed version of the ALIGN web alt-text
dataset collection process and includes noisy captions.
Figure 1 shows a high-level overview of our method.
DietNeRF proposed a related semantic consistency regularizer for NeRF based on the idea that “a bulldozer is a
bulldozer from any perspective”. The method computed the
similarity of a rendered and a real image. In contrast, (7)
compares rendered images and a caption, allowing it to be
used in zero-shot settings when there are no object photos.
4.3. Challenges with CLIP guidance
Due to their ﬂexibility, Neural Radiance Fields are capable of high-ﬁdelity novel view synthesis on a tremendous
diversity of real-world scenes when supervised with multiview consistent images. Their reconstruction loss will typically learn to remove artifacts like spurious density when
sufﬁciently many input images are available. However, we
4K iterations
8K iterations
24K iterations
100K (ﬁnal)
Figure 4. To encourage coherent foreground objects, Dream Fields
train with 3 types of background augmentations: blurred Gaussian
noise, textures and checkerboards. At test time, we render with a
white background. Prompt: “A sculpture of a rooster.”
ﬁnd that the NeRF scene representation is too unconstrained
when trained solely with LCLIP (7) alone from a discrete set
of viewpoints, resulting in severe artifacts that satisfy LCLIP
but are not visually compatible according to humans (see
Figure 3b). NeRF learns high-frequency and near-ﬁeld 
artifacts like partially-transparent “ﬂoating“ regions of density. It also ﬁlls the entire camera viewport rather than generating individual objects. Geometry is unrealistic, though
textures reﬂect the caption, reminiscent of the artifacts in
Deep Dream feature visualizations .
4.4. Pose sampling
Image data augmentations such as random crops are commonly used to improve and regularize image generation in
DeepDream and related work. Image augmentations can
only use in-plane 2D transformations. Dream Fields support
3D data augmentations by sampling different camera pose
extrinsics at each training iteration. We uniformly sample
camera azimuth in 360◦around the scene, so each training
iteration sees a different orientation of the object. As the
underlying scene representation is shared, this improves the
realism of object geometry. For example, sampling azimuth
in a narrow interval tended to create ﬂat, billboard geometry.
The camera elevation, focal length and distance from
the subject can also be augmented, but we did not ﬁnd this
necessary. Instead, we use a ﬁxed camera focal length during
optimization that is scaled by mfocal = 1.2 to enlarge the
object 20%. Rendering cost is constant in the focal length.
4.5. Encouraging coherent objects through sparsity
To remove near-ﬁeld artifacts and spurious density, we
regularize the opacity of Dream Field renderings. Our best
results maximize the average transmittance of rays passing
through the volume up to a target constant. Transmittance
is the probability that light along ray r is not absorbed by
participating media when passing between point t along the
ray and the near plane at tn (2). We approximate the total
transmittance along the ray as the joint probability of light
passing through N discrete segments of the ray according to
Eq. (4). Then, we deﬁne the following transmittance loss:
LT = −min(τ, mean(T(θ, p)))
Ltotal = LCLIP + λLT
This encourages a Dream Field to increase average transmittance up to a target transparency τ. We use τ = 88% in
experiments. τ is annealed in from τ = 40% over 500 iterations to smoothly introduce transparency, which improves
scene geometry and is essential to prevent completely transparent scenes. Scaling 1−τ ∝f 2/d2 preserves object cross
sectional area for different focal and object distances.
When the rendering is alpha-composited with a simple
white or black background during training, we ﬁnd that the
average transmittance approaches τ, but the scene is diffuse
as the optimization populates the background. Augmenting
the scene with random background images leads to coherent
objects. Dream Fields use Gaussian noise, checkerboard
patterns and the random Fourier textures from as backgrounds. These are smoothed with a Gaussian blur with
randomly sampled standard deviation. Background augmentations and a rendering during training are shown in Figure 4.
We qualitatively compare (9) to baseline sparsity regularizers in Figure 5. Our loss is inspired by the multiplicative
opacity gating used by . However, the gated loss has
optimization challenges in practice due in part to its nonconvexity. The simpliﬁed additive loss is more stable, and
both are signiﬁcantly sharper than prior approaches for sparsifying Neural Radiance Fields.
4.6. Localizing objects and bounding scene
When Neural Radiance Fields are trained to reconstruct
images, scene contents will align with observations in a consistent fashion, such as the center of the scene in NeRF’s
Realistic Synthetic dataset . Dream Fields can place density away from the center of the scene while still satisfying
the CLIP loss as natural images in CLIP’s training data will
not always be centered. During training, we maintain an
estimate of the 3D object’s origin and shift rays accordingly.
The origin is tracked via an exponential moving average of
the center of mass of rendered density. To prevent objects
from drifting too far, we bound the scene inside a cube by
masking the density σθ.
4.7. Neural scene representation architecture
The NeRF network architecture proposed in parameterizes scene density with a simple 8-layer MLP of constant
width, and radiance with an additional two layers. We use
a residual MLP architecture instead that introduces residual
connections around every two dense layers. Within a residual
block, we ﬁnd it beneﬁcial to introduce Layer Normalization at the beginning and increase the feature dimension in a
bottleneck fashion. Layer Normalization improves optimization on challenging prompts. To mitigate vanishing gradient
backgrounds
regularizer
Perturb density
softplus(f (x + ))
backgrounds
Figure 5. Our transmittance losses and background augmentations
are complementary. Top: Without background augmentations,
priors on transmittance (right three columns) do not remove lowdensity structures. NeRF’s density perturbations improve coherence, but cloudy artifacts remain. Bottom: When the object is
alpha composited with random backgrounds during training, CLIP
ﬁlls the scene with opaque material to conceal the background.
However, gated and our simpliﬁed additive transmittance regularizers both limit the opacity of the volume successfully and lead to
a sharper object. Inset panels depict transmittance. Prompt: “an
illustration of a pumpkin on the vine.”
issues in highly transparent scenes, we replace ReLU activations with Swish and rectify the predicted density σθ
with a softplus function. Our MLP architecture uses 280K
parameters per scene, while NeRF uses 494K parameters.
5. Evaluation
We evaluate the consistency of generated objects with
their captions and the importance of scene representation,
then show qualitative results and test whether Dream Fields
can generalize compositionally. Ablations analyze regularizers, CLIP and camera poses. Finally, supplementary
materials have further examples and videos.
5.1. Experimental setup
3D reconstruction methods are evaluated by comparing
the learned geometry with a ground-truth reference model,
e.g. with Chamfer Distance. Novel view synthesis techniques like LLFF and NeRF do not have ground truth
models, but compare renderings to pixel-aligned ground
truth images from held-out poses with PSNR or LPIPS, a
deep perceptual metric .
As we do not have access to diverse captioned 3D models
or captioned multi-view data, Dream Fields are challenging
to evaluate with geometric and image reference-based metrics. Instead, we use the CLIP R-Precision metric from
the text-to-image generation literature to measure how well
rendered images align with the true caption. In the context of
text-to-image synthesis, R-Precision measures the fraction of
generated images that a retrieval model associates with the
caption used to generate it. We use a different CLIP model
for learning the Dream Field and computing the evaluation
metric. As with NeRF evaluation, the image is rendered from
a held-out pose. Dream Fields are optimized with cameras
at a 30◦angle of elevation and evaluated at 45◦elevation.
For quantitative metrics, we render at resolution 1682 during
training as in . For ﬁgures, we train with a 50% higher
resolution of 2522.
We collect an object-centric caption dataset with 153
captions as a subset of the Common Objects in Context
(COCO) dataset (see supplement for details). Object
centric examples are those that have a single bounding box
annotation and are ﬁltered to exclude those captioned with
certain phrases like “extreme close up”. COCO includes
5 captions per image, but only one is used for generation.
Hyperparameters were manually tuned for perceptual quality
on a set of 20-74 distinct captions from the evaluation set,
and are shared across all other scenes. Additional dataset
details and hyperparameters are included in the supplement.
5.2. Analyzing retrieval metrics
In the absence of 3D training data, Dream Fields use
geometric priors to constrain generation. To evaluate each
proposed technique, we start from a simpliﬁed baseline Neural Radiance Field largely following and introduce the
priors one-by-one. We generate two objects per COCO
caption using different seeds, for a total of 306 objects. Objects are synthesized with 10K iterations of CLIP ViT B/16
guided optimization of 168×168 rendered images, bilinearly upsampled to the contrastive model’s input resolution
for computational efﬁciency. R-Precision is computed with
CLIP ViT B/32 and LiTuu B/32 to measure the
alignment of generations with the source caption.
Table 1 reports results. The most signiﬁcant improvements come from sparsity, scene bounds and architecture. As an oracle, the ground truth images associated with
object-centric COCO captions have high R-Precision. The
NeRF representation converges poorly and introduces aliasing and banding artifacts, in part from its use of axis-aligned
positional encodings.
We instead combine mip-NeRF’s integrated positional
encodings with random Fourier features, which improves
qualitative results and removes a bias toward axis-aligned
structures. However, the effect on precision is neutral or
negative. The transmittance loss LT in combination with
background augmentations signiﬁcantly improves retrieval
precision +18% and +15.6%, while the transmittance loss
is not sufﬁcient on its own. This is qualitatively shown
in Figure 5. Our MLP architecture with residual connections, normalization, bottleneck-style feature dimensions
and smooth nonlinearities further improves the R-Precision
+8% and +2%. Bounding the scene to a cube improves
retrieval +13% and +11%. The additional bounds explicitly
mask density σ and concentrate samples along each ray.
We also scale up Dream Fields by optimizing with an
image-text model trained on a larger captioned dataset of
R-Precision ↑
LiTuu B/32
Baseline COCO GT images
Simpliﬁed NeRF
Positional
+ mip-NeRF IPE
+ Higher freq.
Fourier features
+ random crops
+ transmittance loss
+ background aug.
+ MLP architecture
+ scene bounds
+ track origin
+ LiTuu ViT B/32
+ 20K iterations,
2522 renders
Table 1. When used together, geometric priors improve caption
retrieval precision. We start with a simpliﬁed version of the NeRF
scene representation and add in one prior at a time until all are used
in conjunction. Captions are retrieved from rendered images of the
generated objects at held-out camera poses using CLIP’s ViT B/32.
Objects are generated with LCLIP guidance from the pre-trained
CLIP ViT B/16 except in scaling experiments where we experiment
with the higher-resolution LiTuu B/32 model.
3.6B images from . We use a ViT B/32 model with image
and text encoders trained from scratch. This corresponds to
the uu conﬁguration from , following the CLIP training
procedure to learn both encoders contrastively. The LiTuu
ViT encoder used in our experiments takes higher resolution
2882 images while CLIP is trained with 2242 inputs. Still,
LiTuu B/32 is more compute-efﬁcient than CLIP B/16 due
to the larger patch size in the ﬁrst layer.
LiTuu does not signiﬁcantly help R-Precision when optimizing Dream Fields with low resolution renderings, perhaps
because the CLIP B/32 model used for evaluation is trained
on the same dataset as the CLIP B/16 model in earlier rows.
Optimizing for longer with higher resolution 2522 renderings
closes the gap. LiTuu improves visual quality and sharpness
(Appendix A), suggesting that improvements in multimodal
image-text models transfer to 3D generation.
5.3. Compositional generation
In Figure 6, we show non-cherrypicked generations that
test the compositional generalization of Dream Fields to
ﬁne-grained variations in captions taken from the website
of . We independently vary the object generated and
stylistic descriptors like shape and materials. DALL-E 
also had a remarkable ability to combine concepts in prompts
out of distribution, but was limited to 2D image synthesis.
brain coral
lotus root
Figure 6. Compositional object generation. Dream Fields allow users to express speciﬁc artistic styles via detailed captions. Top two rows:
Similar to text-to-image experiments in , we generate objects with the caption “armchair in the shape of an avocado. armchair imitating
avocado.” Bottom: Generations vary the texture of a single snail. Captions follow the template “a snail made of baguette. a snail with the
texture of baguette” Results are not cherry-picked.
Loss or parameterization
No regularizer
Perturb σ 
σ = softplus(fθ(x) + ϵ)
Beta prior 
Gated T 
−mean(T(θ, p)) · LCLIP
Clipped gated T
−LT · LCLIP (11)
Clipped additive T
LCLIP + λLT (9)
Table 2. Ablating sparsity regularizers. Optimization is done for
10K iterations at 1682 resolution with LiTuu ViT B/32 and background augmentation, and retrieval uses CLIP ViT B/32. For the
purposes of ablation, we run one seed per caption (153 runs).
Dream Fields produces compositions of concepts in 3D, and
supports ﬁne-grained variations in prompts across several
categories of objects. Some geometric details are not realistic, however. For example, generated snails have eye stalks
attached to their shell rather than body, and the generated
green vase is blurry.
5.4. Model ablations
Ablating sparsity regularizers
While we regularize the
mean transmittance, other sparsity losses are possible. We
compare unregularized Dream Fields, perturbations to the
density σ , regularization with a beta prior on transmittance , multiplicative gating versions of LT and our
additive LT regularizer in Figure 5. On real-world scenes,
NeRF added Gaussian noise to network predictions of the
density prior to rectiﬁcation as a regularizer. This can encourage sharper boundary deﬁnitions as small densities will
often be zeroed by the perturbation. The beta prior from
Figure 7. The target transmittance τ affects the size of generated
objects. Inset panels depict transmittance. Prompt from Object
Centric COCO: “A cake toped [sic] with white frosting ﬂowers
with chocolate centers.”
Neural Volumes encourages rays to either pass through
the volume or be completely occluded:
total = LCLIP+λ·mean(log T(θ, p) + log(1 −T(θ, p)))
The multiplicative loss is inspired by the opacity scaling
of for feature visualization. We scale the CLIP loss by
a clipped mean transmittance:
Ltotal = min(τ, mean(T(θ, p))) · LCLIP
Table 2 compares the regularizers, showing that density perturbations and the beta prior improve R-Precision +12.4%
and +15%, respectively. Scenes with clipped mean transmittance regularization best align with their captions, +26.8%
over the baseline. The beta prior can ﬁll scenes with opaque
material even without background augmentations as it encourages both high and low transmittance. Multiplicative
gating works well when clipped to a target and with background augmentations, but is also non-convex and sensitive
to hyperparameters. Figure 7 shows the effect of varying the
target transmittance τ with an additive loss.
Retrieval model R-Precision
Optimized model
LiTuu B/32
CLIP B/32 
(86.6±2.0)
CLIP B/16 
(93.5±1.4)
LiTuu B/32
(88.9±1.8)
Table 3. The aligned image-text representation used to optimize
Dream Fields inﬂuences their quantitative validation R-Precision
according to a held-out retrieval model. All contrastive models
produce high retrieval precision, though qualitatively CLIP B/32
produced overly smooth and simpliﬁed objects. We optimize for
10K iterations at 1682 resolution. (Italicized) metrics use the optimized model at a held-out pose and indicate Dream Fields overﬁt.
Varying the image-text model
We compare different image and text representations h(·), g(·) used in LCLIP (7) and
for retrieval metrics. Table 3 shows the results. CLIP B/32,
B/16 and LiTuu B/32 all have high retrieval precision, indicating they can synthesize objects generally aligned with
the provided captions. CLIP B/32 performs the best, outperforming the more compute intensive CLIP B/16 model. The
architectures differ in the number of pixels encoded in each
token supplied to the Transformer backbone, i.e. the ViT
patch size. A larger patch size may be sufﬁcient due to the
low resolution of renders: 1682 cropped to 1542, then upsampled to CLIP’s input size of 2242. Qualitatively, training
with LiTuu B/32 produced the most detailed geometry and
textures, suggesting that open-set evaluation is challenging.
Varying optimized camera poses
Each training iteration, Dream Fields samples a camera pose p to render the
scene. In experiments, we used a full 360◦sampling range
for the camera’s azimuth, and ﬁxed the elevation. Figure 8
shows multiple views of a bird when optimizing with smaller
azimuth ranges. In the left-most column, a view from the central azimuth (frontal) is shown, and is realistic for all training
conﬁgurations. Views from more extreme angles (right, left,
rear view columns) have artifacts when the Dream Field
is optimized with narrow azimuth ranges. Training with
diverse cameras is important for viewpoint generalization.
6. Discussion and limitations
There are a number of limitations in Dream Fields. Generation requires iterative optimization, which can be expensive.
2K-20K iterations are sufﬁcient for most objects, but more
detail emerges when optimizing longer. Meta-learning 
or amortization could speed up synthesis.
We use the same prompt at all perspectives. This can
lead to repeated patterns on multiple sides of an object. The
target caption could be varied across different camera poses.
Many of the prompts we tested involve multiple subjects,
Frontal view
Right view
Figure 8. Training with diversely sampled camera poses improves
generalization across views. In the top row, we sample camera
azimuth from a single viewpoint. The rendered view from the same
perspective (left column) is realistic, but the object structure is poor
as seen from other angles. Qualitative results improve with larger
sampling intervals, with the best results from 360◦sampling.
but we do not target complex scene generation 
partly because CLIP poorly encodes spatial relations .
Scene layout could be handled in a post-processing step.
The image-text models we use to score renderings are not
perfect even on ground truth training images, so improvements in image-text models may transfer to 3D generation.
Our reliance on pre-trained models inherits their harmful
biases. Identifying methods that can detect and remove these
biases is an important direction if these methods are to be
useful for larger-scale asset generation.
7. Conclusion
Our work has begun to tackle the difﬁcult problem of
object generation from text. By combining scalable multimodal image-text models and multi-view consistent differentiable neural rendering with simple object priors, we are able
to synthesize both geometry and color of 3D objects across
a large variety of real-world text prompts. The language
interface allows users to control the style and shape of the
results, including materials and categories of objects, with
easy-to-author prompts. We hope these methods will enable
rapid asset creation for artists and multimedia applications.
Acknowledgements
We thank Xiaohua Zhai, Lucas Beyer and Andreas Steiner
for providing pre-trained models on the LiT 3.6B dataset,
Paras Jain, Kevin Murphy, Matthew Tancik and Alireza Fathi
for useful discussions and feedback on our work, and many
colleagues at Google for building and supporting key infrastructure. Ajay Jain is supported in part by the NSF GRFP
under Grant Number DGE 1752814.