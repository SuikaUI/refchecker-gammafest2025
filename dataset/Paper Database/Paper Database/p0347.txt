Journal of the Physical Society of Japan
Deep Learning the Quantum Phase Transitions in Random
Two-Dimensional Electron Systems
Tomoki Ohtsuki1 ∗and Tomi Ohtsuki2 †
1 NTT DATA Mathematical Systems Inc, Shinjuku-ku, Tokyo 160-0016, Japan
2Physics Division, Sophia University, Chiyoda-ku, Tokyo 102-8554, Japan
Random electron systems show rich phases such as Anderson insulator, diﬀusive metal, quantum Hall and quantum anomalous Hall insulators, Weyl semimetal, as well as strong/weak
topological insulators. Eigenfunctions of each matter phase have speciﬁc features, but owing to the random nature of systems, determining the matter phase from eigenfunctions is
diﬃcult. Here, we propose the deep learning algorithm to capture the features of eigenfunctions. Localization-delocalization transition, as well as disordered Chern insulator-Anderson
insulator transition, is discussed.
Introduction– More than half a century has passed since the discovery of Anderson localization,1) and the random electron systems continue to attract theoretical as well as experimental interest. Symmetry classiﬁcation of topological insulators2–5) based on the universality
classes of random noninteracting electron systems6,7) gives rise to a fundamental question:
can we distinguish the random topological insulator from Anderson insulators? Note that
topological numbers are usually deﬁned in the randomness free systems via the integration of
the Berry curvature of Bloch function over the Brillouin zone, although topological numbers
in random systems have recently been proposed.8–10)
Determining the phase diagram and the critical exponents requires large-scale numerical
simulation combined with detailed ﬁnite size scaling analyses.11–14) This is because, owing
to large ﬂuctuations of wavefunction amplitudes, it is almost impossible to judge whether
the eigenfunction obtained by diagonalizing small systems is localized or delocalized, or
whether the eigenfunction is a chiral/helical edge state of a topological insulator. In fact, it
often happens that eigenfunctions in the localized phase seem less localized than those in the
delocalized phase [see Figs. 1(b) and 1(c) for example].
∗ootsuki 
† 
J. Phys. Soc. Jpn.
Recently, there has been great progress on image recognition algorithms15) based on deep
machine learning.16,17) Machine learning has recently been applied to several problems of
condensed matter physics such as Ising and spin ice models18,19) and strongly correlated systems.20–25)
In this Letter, we test the image recognition algorithm to determine whether
the eigenfunctions for relatively small systems are localized/delocalized, and topological/nontopological. As examples, we test two types of two-dimensional (2D) quantum phase
transitions: Anderson-type localization-delocalization transition in symplectic systems, and
disordered Chern insulator to Anderson insulator transition in unitary systems.
Distinguishing Localized States from Delocalized Ones– We start with a 2D symplectic
system, which is realized in the presence of spin-orbit scattering. We use the SU(2) Hamiltonian26) that describes the 2D electron on a square lattice with nearest-neighbor hopping,
⟨i, j⟩,σ,σ′
R(i, j)σ,σ′c†
i,σcj,σ′ ,
i,σ (ci,σ) denotes the creation (annihilation) operator of an electron at site i = (x, y)
with spin σ, and ǫi denotes the random potential at site i. We assume a box distribution with
each ǫi uniformly and independently distributed on the interval [−W/2, W/2]. The modulus of
the transfer energy is taken to be the energy unit. R(i, j) is an SU(2) matrix,
eiαi,j cos βi, j
eiγi,j sin βi, j
−e−iγi,j sin βi, j
e−iαi,j cos βi, j
,
with α and γ uniformly distributed in the range [0, 2π). The probability density P(β) is
0 ≤β ≤π/2 ,
otherwise .
Examples of the eigenfunctions in delocalized [Figs. 1(a) and 1(b)] and localized phases
[Figs. 1(c) and 1(d)] are shown in Fig. 1.
For E = 0 (band center), from the ﬁnite size scaling analyses of the quasi-1D localization
length,26,27) it is known that the states are delocalized when W < WSU2
(≈6.20), while they are
localized when W > WSU2
. We impose periodic boundary conditions in x- and y-directions,
and diagonalize systems of 40×40. From the resulting 3200 eigenstates with Kramers degeneracy, we pick up the 1600th eigenstate (i.e., a state close to the band center). For simplicity,
the maximum modulus of the eigenfunction is shifted to the center of the system. Changing
W and the seed of the random number stream (Intel MKL MT2023), we prepare 2000 samples of states, i.e., 1000 for W < WSU2
and 1000 for W > WSU2
. We then teach the machine
J. Phys. Soc. Jpn.
Examples of eigenfunction modulus squared |ψ(x, y)|2 for (a) W = 0.124 ≈WSU2
/50, (b) W = 5.58 ≈
, (c) W = 6.82 ≈1.1WSU2
, and (d) W = 12.4 ≈2WSU2
. Peak positions are shifted to the center of the
whether the states belong to the localized (delocalized) phase.
For our network architecture, we consider two types of simple convolutional neural network (CNN), which output two real numbers, i.e., probabilities for each phase, given 40 × 40
input eigenfunction. The ﬁrst one is a very simple network with two weight layers, which ﬁrst
convolves the input with a 5 × 5 ﬁlter with stride 1 to 10 channels, then applies max pooling
with a kernel size of 2 × 2 and stride 2, and ﬁnally performs fully connected linear transformation to output the learned probabilities. The loss function can then be deﬁned by the cross
entropy of probabilities and the localized/delocalized labels. The second, rather deep one with
four weight layers is a variant of LeNet28) included in Caﬀe29) (with the input size changed to
40 × 40), which utilizes rectiﬁed linear unit (ReLU) as its activation function. See Fig. 2 for
illustration and detailed parameters. The network weight parameters (to be trained) are sampled from gaussian distribution, the scale of which is determined by the number of input and
output dimensions,30) except for the ﬁrst convolution layer connected to the raw input: since
we are dealing with eigenfunctions, whose typical values at each lattice site are much smaller
than those of gray-scale images, we have manually chosen the weight initialization scale to be
100, which worked better in practice for the two networks. As the stochastic gradient descent
solver, we have used the RMSProp solver31) with the parameters in the Caﬀe MNIST example
(which is contained as examples/mnist/lenet_solver_rmsprop.prototxt in the Caﬀe
source). Before the training, we always partition the training data into 90% and 10%, and use
the latter as the validation set during the training. The solver performs enough iterations so
that the validation error becomes stationary. We have used a workstation: Intel Xeon E5-1620
v4, single CPU with 4 cores with GPU Quadro K420 and GPGPU TESLA K40 running on
Linux CentOS 6.8.
We then test 5 sets of ensemble, each consisting of 100 eigenstates, and let the machine
J. Phys. Soc. Jpn.
conv1 (Convolution) kernel size: 5 stride: 1 pad: 0
data (MemoryData)
ip1 (InnerProduct)
prob (SoftmaxWithLoss)
pool1 (MAX Pooling) kernel size: 2 stride: 2 pad: 0
conv1 (Convolution) kernel size: 5 stride: 1 pad: 0
conv2 (Convolution) kernel size: 5 stride: 1 pad: 0
loss (SoftmaxWithLoss)
data (MemoryData)
ip1 (InnerProduct)
ip2 (InnerProduct)
pool1 (MAX Pooling) kernel size: 2 stride: 2 pad: 0
pool2 (MAX Pooling) kernel size: 2 stride: 2 pad: 0
relu1 (ReLU)
(Color online) Network architectures used in this work. (a) A simple two-weight-layer CNN, which
consists of convolution and max pooling, followed by dense linear transformation. (b) LeNet-like architecture
with ReLU activation.
judge whether the states are localized or not. The resulting probability for eigenfunction to
be delocalized, P, is shown in Fig. 3(a).
We then apply the results of the learning around E = 0 to judge whether the states around
E = 1.0, 2.0, and 3.0 are delocalized. Results are shown in Fig. 3(b), in which we observe that,
J. Phys. Soc. Jpn.
Disorder, W
(Color online) Probability of eigenfunction to be judged delocalized as a function of disorder W.
Averages over 5 samples are taken. (a) Band Center E = 0. Critical disorder WSU2
≈6.20, as well as 50%
probability, is indicated as the dashed lines. The dotted line is for two-weight-layer network [Fig. 2(a)], while
the solid one is for four-weight-layer network [Fig. 2(b)]. (b) For E = 1.0, 2.0, and 3.0. The red line is for twoweight-layer network, while the black line is for four-weight-layernetwork. The values of WSU2
for E = 1.0, 2.0,
and 3.0 estimated via the ﬁnite size scaling of the localization length27) are 5.953, 5.165, and 3.394, respectively,
which are indicated by the vertical dashed lines.
with increasing E, that is, as we move from band center to band edge, the electron begins to be
localized with a smaller strength of the disorder W, qualitatively consistent with the ﬁnite size
scaling analysis.27) There seems to be, however, a systematic deviation of the 50% criterion of
localization-delocalization transition and the actual critical point with increasing E. This may
be due to the appearance of bound states near the band edge, which is absent in the machine
learning around E = 0. We have further applied the results of SU(2) model machine learning
for the Ando model,32) and veriﬁed that once the machine learns the eigenfunction features
in certain systems, it can be applied to other systems belonging to the same class of quantum
phase transition (see Supplemental material for detail33)).
Distinguishing Topological Edge States from Non-topological Ones– We next study the
topological Chern insulator to nontopological Anderson insulator transition.34–36) We use a
spinless two-orbital tight-binding model on a square lattice, which consists of s-orbital and
p ≡px + ipy orbital,37)
(ǫs + vs(x))c†
x,scx,s + (ǫp + vp(x))c†
x+eµ,scx,s −tpc†
x+eµ,pcx,p)
x+ex,p −c†
x−ex,p)cx,s −itsp(c†
x+ey,p −c†
x−ey,p)cx,s + h.c. ,
J. Phys. Soc. Jpn.
where ǫs, vs(x), ǫp, and vp(x) denote atomic energy and disorder potential for the s- and porbitals, respectively. Both vs(x) and vp(x) are uniformly distributed within [−W/2, W/2] with
identical and independent probability distribution. ts, tp, and tsp are transfer integrals between
neighboring s-orbitals, p-orbitals, and that between s- and p-orbitals, respectively.
In the absence of disorder, the system is a Chern insulator when the band inversion condition is satisﬁed: 0 < |ǫs −ǫp| < 4(ts + tp). We set ǫs −ǫp = −2(ts + tp), ǫs = −ǫp < 0, and
ts = tp > 0 so that this condition is satisﬁed, and set tsp = 4ts/3. The energy unit is set to 4ts.
A bulk band gap appears in |E| < Eg = 0.5 where chiral edge states exist.
Eigenfunction modulus squared |ψ(x, y)|2 for (a) W = 0.064 ≈WCI
c /50, (b) W = 2.88 ≈0.9WCI
W = 3.52 ≈1.1WCI
c , and (d) W = 6.4 ≈2WCI
For E = 0, the system remains as a Chern insulator for W < WCI
≈3.2,35) while it is an
Anderson insulator for W > WCI
c . (Unfortunately, the estimate of WCI
c is less precise than the
SU(2) model.) We impose ﬁxed boundary conditions in the x- and y-directions, so that the
edge states appear if the system is a topological insulator.
We diagonalize square systems of 40 × 40 sites, and from the resulting 3200 eigenstates, we pick up the 1600th eigenstate. Examples of the eigenfunctions in topological Chern
[Figs. 4(a) and 4(b)] and nontopological Anderson insulators [Figs. 4(c) and 4(d)] are shown
in Fig. 4. As shown in Fig. 4, it is diﬃcult to judge whether the state is an edge state or not
when W is close to WCI
c : see, for example, W = 0.9WCI
[Fig. 4(b), Chern insulator phase]
and W = 1.1WCI
[Fig. 4(c), Anderson insulator phase]. In fact, learning 1000 samples for
each phase gives 93% validation accuracy for four-weight-layer network compared with 98%
or more as in the SU(2) model. The diﬃculty may be due to the ﬁxed boundary condition
where shifting the locus of the maximum of the eigenfunction amplitude is not allowed. Another reason for diﬃculty is that the bulk of the systems are localized in both topological and
nontopological regions. To overcome these diﬃculties, we increased the number of samples:
27000 samples belonging to the topological phase, and 27000 to the nontopological phase.
J. Phys. Soc. Jpn.
eigenenergy
(a) Probability of eigenfunction around E = 0 to be judged topological edge states as a function of
disorder W. Averages over 5 samples are taken. 50% probability is indicated as the horizontal dashed line. Since
the critical disorder is less accurate, WCI
c = 3.25 ± 0.1 is shown as a shaded region. The dotted line is for a twoweight-layer network, while the solid one is for a four-weight-layer one. (b) Same quantity but as a function of
eigenenergy E inside the bulk band gap region |E| < Eg = 0.5. Results for W = 1 < WCI
(×, solid line) and
W = 6 > WCI
c (+, dotted line) are shown.
We have also increased the number of hidden units to be 32 for the ﬁrst convolution layer
(“conv1” in Fig. 2), 128 for the second (“conv2”), and 512 for the hidden dense connection
layer (“ip1”).
In Fig. 5(a), we plot the probability of the eigenfunction to be judged topological. A new
ensemble of eigenfunctions with diﬀerent random number sequences has been prepared to test
this method. As in the case of delocalization-localization transition, the probability ﬂuctuates
near the critical point and vanishes in the nontopological region. The validation accuracy is
90% for the case of two layers of network (dotted line), and 97% for four layers of network
(solid line), which demonstrates clearly that a deeper network exhibits better performance.
We next apply the result of the deep learning around E = 0 to judge the states in the bulk
band gap region at zero disorder, |E| < Eg = 0.5. We diagonalize a system for W = 1 < WCI
and W = 6 > WCI
c , take all the eigenstates with |E| < Eg, and let the machine judge them.
Figure 5(b) shows that topological edge states other than E = 0 are also well distinguished
from nontopological ones based on the learning around E = 0.
Concluding Remarks– In this paper, we focused on 2D random electron systems. We have
demonstrated the validity of deep learning for distinguishing various random electron states
in quantum phase transitions. For strong enough and weak enough randomness, the precision of judgement is 0.99999· · ·, while in the critical regime, the judgement becomes less
accurate. This region is related to the critical region where the characteristic length scale ξ is
J. Phys. Soc. Jpn.
comparable to or longer than the system size L. That is, the probability P for the eigenfunction to be judged delocalized/topological obeys the scaling law, P(W, L) = f [(W −Wc)L1/ν],
although determining the exponent ν is beyond the scope of this Letter. Since all we need to
calculate are eigenfunctions with relatively small systems, the method will work for systems
where the transfer matrix method is not applicable (localization problems on random38–41)
and fractal lattices,42) for example).
We have used the known values of critical disorder to teach the machine. After learning the feature of eigenfunctions near the band center, the machine could capture localized/delocalized and topological/nontopological features away from the band center. We have
also veriﬁed that the results of the SU(2) model learning can be applied to the Ando model.33)
In the cases of Anderson transition near the band edge in the SU(2) model [Fig. 3(b)]
and that at the band center in the Ando model, the machine tends to predict the transition
for a slightly smaller disorder than the estimate of ﬁnite size scaling analyses.32,43) We have
extracted the features in the middle layers to explain this tendency,33) but could not clarify
how the machine judges phases. The details of judgement should be clariﬁed in the future.
We have focused on the amplitude of eigenfunction in 2D. In higher dimensions, the
same algorithm will be applicable via dimensional reduction: integration of |ψ2| over certain
directions, reducing the image to two dimensions. The dimensional reduction will also work
for disordered 3D strong and weak topological insulators.44) Other interesting quantities for
machine learning are phase and spin texture of eigenfunctions in random electron systems.
Classical waves (photon, phonon) in random media45–47) as well as disordered magnon48) are
also worth machine learning.
Acknowledgments
The authors would like to thank Keith Slevin, Koji Kobayashi, and Ken-Ichiro Imura
for useful discussions. This work was partly supported by JSPS KAKENHI Grant No.
JP15H03700.
J. Phys. Soc. Jpn.