Give to AgEcon Search
The World’s Largest Open Access Agricultural & Applied Economics Digital Library
This document is discoverable and free to researchers across the
globe due to the work of AgEcon Search.
Help ensure our sustainability.
AgEcon Search
 
 
Papers downloaded from AgEcon Search may be used for non-commercial purposes and personal study only.
No other use, including posting to another Internet site, is permitted without permission from the copyright
owner (not AgEcon Search), or as allowed under the provisions of Fair Use, U.S. Copyright Act, Title 17 U.S.C.
No endorsement of AgEcon Search or its fundraising activities by the author(s) of the following work or their
employer(s) is intended or implied.
The Stata Journal
H. Joseph Newton
Department of Statistics
Texas A & M University
College Station, Texas 77843
979-845-8817; FAX 979-845-6077
 
Nicholas J. Cox
Department of Geography
Durham University
South Road
Durham City DH1 3LE UK
 
Associate Editors
Christopher F. Baum
Boston College
Rino Bellocco
Karolinska Institutet, Sweden and
Univ. degli Studi di Milano-Bicocca, Italy
A. Colin Cameron
University of California–Davis
David Clayton
Cambridge Inst. for Medical Research
Mario A. Cleves
Univ. of Arkansas for Medical Sciences
William D. Dupont
Vanderbilt University
Charles Franklin
University of Wisconsin–Madison
Allan Gregory
Queen’s University
James Hardin
University of South Carolina
ETH Z¨urich, Switzerland
Stephen Jenkins
University of Essex
Ulrich Kohler
WZB, Berlin
Jens Lauritsen
Odense University Hospital
Stanley Lemeshow
Ohio State University
J. Scott Long
Indiana University
Thomas Lumley
University of Washington–Seattle
Roger Newson
Imperial College, London
Marcello Pagano
Harvard School of Public Health
Sophia Rabe-Hesketh
University of California–Berkeley
J. Patrick Royston
MRC Clinical Trials Unit, London
Philip Ryan
University of Adelaide
Mark E. Schaﬀer
Heriot-Watt University, Edinburgh
Jeroen Weesie
Utrecht University
Nicholas J. G. Winter
University of Virginia
Jeﬀrey Wooldridge
Michigan State University
Stata Press Production Manager
Stata Press Copy Editor
Lisa Gilmore
Deirdre Patterson
Copyright Statement: The Stata Journal and the contents of the supporting ﬁles (programs, datasets, and
help ﬁles) are copyright c
⃝by StataCorp LP. The contents of the supporting ﬁles (programs, datasets, and
help ﬁles) may be copied or reproduced by any means whatsoever, in whole or in part, as long as any copy
or reproduction includes attribution to both (1) the author and (2) the Stata Journal.
The articles appearing in the Stata Journal may be copied or reproduced as printed copies, in whole or in part,
as long as any copy or reproduction includes attribution to both (1) the author and (2) the Stata Journal.
Written permission must be obtained from StataCorp if you wish to make electronic copies of the insertions.
This precludes placing electronic copies of the Stata Journal, in whole or in part, on publicly accessible web
sites, ﬁleservers, or other locations where the copy may be accessed by anyone other than the subscriber.
Users of any of the software, ideas, data, or other materials published in the Stata Journal or the supporting
ﬁles understand that such use is made without warranty of any kind, by either the Stata Journal, the author,
or StataCorp. In particular, there is no warranty of ﬁtness of purpose or merchantability, nor for special,
incidental, or consequential damages such as loss of proﬁts. The purpose of the Stata Journal is to promote
free communication among Stata users.
The Stata Journal, electronic version (ISSN 1536-8734) is a publication of Stata Press. Stata and Mata are
registered trademarks of StataCorp LP.
The Stata Journal 
7, Number 4, pp. 507–541
Causal inference with observational data
Austin Nichols
Urban Institute
Washington, DC
 
Problems with inferring causal relationships from nonexperimental
data are brieﬂy reviewed, and four broad classes of methods designed to allow
estimation of and inference about causal parameters are described: panel regression, matching or reweighting, instrumental variables, and regression discontinuity. Practical examples are oﬀered, and discussion focuses on checking required
assumptions to the extent possible.
Keywords: st0136, xtreg, psmatch2, nnmatch, ivreg, ivreg2, ivregress, rd, lpoly,
xtoverid, ranktest, causal inference, match, matching, reweighting, propensity
score, panel, instrumental variables, excluded instrument, weak identiﬁcation, regression, discontinuity, local polynomial
Introduction
Identifying the causal impact of some variables, XT , on y is diﬃcult in the best of circumstances, but faces seemingly insurmountable problems in observational data, where
XT is not manipulable by the researcher and cannot be randomly assigned. Nevertheless, estimating such an impact or “treatment eﬀect” is the goal of much research,
even much research that carefully states all ﬁndings in terms of associations rather than
causal eﬀects. I will call the variables XT the “treatment” or treatment variables, and
the term simply denotes variables of interest—they need not be binary (0/1) nor have
any medical or agricultural application.
Experimental research designs oﬀer the most plausibly unbiased estimates, but experiments are frequently infeasible due to cost or moral objections—no one proposes
to randomly assign smoking to individuals to assess health risks or to randomly assign marital status to parents so as to measure the impacts on their children. Four
types of quasiexperimental research designs oﬀering approaches to causal inference using observational data are discussed below in rough order of increasing internal validity
 :
• Ordinary regression and panel methods
• Matching and reweighting estimators
• Instrumental variables (IV) and related methods
• Regression discontinuity (RD) designs
c⃝2007 StataCorp LP
Causal inference with observational data
Each has strengths and weaknesses discussed below. In practice, the data often dictate
the method, but it is incumbent upon the researcher to discuss and check (insofar as
possible) the assumptions that allow causal inference with these models, and to qualify
conclusions appropriately. Checking those assumptions is the focus of this paper.
A short summary of these methods and their properties is in order before we proceed. To eliminate bias, the regression and panel methods typically require confounding
variables either to be measured directly or to be invariant along at least one dimension
in the data, e.g., invariant over time. The matching and reweighting estimators require
that selection of treatment XT depend only on observable variables, both a stronger
and weaker condition. IV methods require extra variables that aﬀect XT but not outcomes directly and throw away some information in XT to get less eﬃcient and biased
estimates that are, however, consistent (i.e., approximately unbiased in suﬃciently large
RD methods require that treatment XT exhibit a discontinuous jump at a
particular value (the “cutoﬀ”) of an observed assignment variable and provide estimates
of the eﬀect of XT for individuals with exactly that value of the assignment variable.
To get plausibly unbiased estimates, one must either give up some eﬃciency or generalizability (or both, especially for IV and RD) or make strong assumptions about the
process determining XT .
Identifying a causal eﬀect
Consider an example to ﬁx ideas. Suppose that for people suﬀering from depression,
the impact of mental health treatment on work is positive. However, those who seek
mental health treatment (or seek more of it) are less likely to work, even conditional on
all other observable characteristics, because their depression is more severe (in ways not
measured by any data we can see). As a result, we estimate the impact of treatment on
work, incorrectly, as being negative.
A classic example of an identiﬁcation problem is the eﬀect of college on earnings
 . College is surely nonrandomly assigned, and there are various important unobserved factors, including the alternatives available to individuals, their
time preferences, the prices and quality of college options, academic achievement (often
“ability” in economics parlance), and access to credit. Suppose that college graduates
earn 60 and others earn 40 on average. One simple (implausible but instructive) story
might be that college has no real eﬀect on productivity or earnings, but those who pass
a test S that grants entry to college have productivity of 60 on average and go to college.
Even in the absence of college, they would earn 60 if they could signal 
productivity to employers by another means (e.g., by merely reporting the result of test
S). Here extending college to a few people who failed test S would not improve their
productivity at all and might not aﬀect their earnings (if employers observed the result
of test S).
If we could see the outcome for each case when treated and not treated (assuming
a single binary treatment XT ) or an outcome y for each possible level of XT , we could
calculate the treatment eﬀect for each individual i and compute an average. Of course,
A. Nichols
this is not possible as each gets some level of XT or some history of XT in a panel
Thus we must compare individuals i and j with diﬀerent XT to estimate
an average treatment eﬀect (ATE). When XT is nonrandomly assigned, we have no
guarantee that individuals i and j are comparable in their response to treatment or
what their outcome would have been given another XT , even on average. The notion
of “potential outcomes” is known as the Rubin causal model. Holland
 provided the classic exposition of this now dominant theoretical framework for
causal inference, and Rubin clariﬁed the debt that the Rubin causal model owes
to Neyman and Fisher .
In all the models discussed in this paper, we assume that the eﬀect of treatment
is on individual observations and does not spill over onto other units. This is called
the stable-unit-treatment-value assumption by Rubin . Often, this may be only
approximately true, e.g., the eﬀect of a college education is not only on the earnings of
the recipient, since each worker participates in a labor market with other graduates and
nongraduates.
What is the most common concern about observational data? If XT is correlated
with some other variable XU that also has a causal impact on y, but we do not measure
XU, we might assess the impact of XT as negative even though its true impact is
positive. Sign reversal is an extreme case, sometimes called Simpson’s paradox, though
it is not a paradox and Simpson pointed out the possibility long after Yule .
More generally, the estimate of the impact of XT may be biased and inconsistent when
XT is nonrandomly assigned. That is, even if the sign of the estimated impact is not
the opposite of the true impact, our estimate need not be near the true causal impact on
average, nor approach it asymptotically. This central problem is usually called omittedvariable bias or selection bias (here selection refers to the nonrandom selection of XT ,
not selection on the dependent variable as in heckman and related models).
Sources of bias and inconsistency
The selection bias (or omitted-variable bias) in an ordinary regression arises from endogeneity (a regressor is said to be endogenous if it is correlated with the error), a
condition that also occurs if the explanatory variable is measured with error or in a
system of “simultaneous equations” (e.g., suppose that work also has a causal impact
on mental health or higher earnings cause increases in education; in this case, it is not
clear what impact, if any, our single-equation regressions identify).
Often a suspected type of endogeneity can be reformulated as a case of omitted
variables, perhaps with an unobservable (as opposed to merely unobserved) omitted
variable, about which we can nonetheless make some predictions from theory to sign
the likely bias.
The formula for omitted-variable bias in linear regression is instructive. With a true
y = β0 + XT βT + XUβU + ε
Causal inference with observational data
where we regress y on XT but leave out XU (for example, because we cannot observe
it), the estimate of βT has bias
E(βT ) −βT = δβU
where δ is the coeﬃcient of an auxiliary regression of XU on XT (or the matrix of
coeﬃcients of stacked regressions when XU is a matrix containing multiple variables)
so the bias is proportional to the correlation of XU and XT and to the eﬀect of XU
(the omitted variables) on y.
In nonlinear models, such as a probit or logit regression, the estimate will be
biased and inconsistent even when XT and XU are uncorrelated, though Wooldridge
 demonstrates that some quantities of interest may still be identiﬁed under
additional assumptions.
Sensitivity testing
Manski demonstrates how a causal eﬀect can be bounded under very unrestrictive assumptions and then how the bounds can be narrowed under more restrictive
parametric assumptions. Given how sensitive the quasiexperimental methods are to assumptions (selection on observables, exclusion restrictions, exchangeability, etc.), some
kind of sensitivity testing is in order no matter what method is used.
 provides a comprehensive treatment of formal sensitivity testing under various
parametric assumptions.
Lee advocates another useful method of bounding treatment eﬀects, which
was used in Leibbrandt, Levinsohn, and McCrary .
Systems of equations
Some of the techniques discussed here to address selection bias are also used in the
simultaneous-equations setting. The literature on structural equations models is extensive, and a system of equations may encode a complicated conceptual causal model,
with many “causal arrows” drawn to and from many variables. The present exercise of
identifying the causal impact of some limited set of variables XT on a single outcome
y can be seen as restricting our attention in such a complicated system to just one
equation, and identifying just some subset of causal eﬀects.
For example, in a simpliﬁed supply-and-demand system:
lnQsupply = eslnP + aTransportCost + εs
lnQdemand = edlnP + bIncome + εd
where price (lnP) is endogenously determined by a market-clearing condition lnQsupply =
lnQdemand, our present enterprise limits us to identifying only the demand elasticity ed
using factors that shift supply to identify exogenous shifts in price faced by consumers
A. Nichols
(exogenous relative to the second equation’s error εd), or identifying only the supply
elasticity es using factors that shift demand to identify exogenous shifts in price faced
by ﬁrms (exogenous relative to the ﬁrst equation’s error εs).
See [R] reg3 for alternative approaches that can simultaneously identify parameters
in multiple equations, and Heckman and Vytlacil and Goldberger and Duncan
 for more detail.
In an experimental setting, typically the only two quantities to be estimated are the
sample ATE or the population ATE—both estimated with a diﬀerence in averages across
treatment groups (equal in expectation to the mean of individual treatment eﬀects over
the full sample).
In a quasiexperimental setting, several other ATEs are commonly
estimated: the ATE on the treated, the ATE on the untreated or control group, and
a variety of local ATEs (LATE)—local to some range of values or some subpopulation.
One can imagine constructing at least 2N diﬀerent ATE estimates in a sample of N
observations, restricting attention to two possible weights for each observation. Allowing
a variety of weights and speciﬁcations leads to inﬁnitely many LATE estimators, not all
of which would be sensible.
For many decision problems, a highly relevant eﬀect estimate is the marginal treatment eﬀect (MTE), either the ATE for the marginal treated case—the expected treatment
eﬀect for the case that would get treatment with a small expansion of the availability of
treatment—or the average eﬀect of a small increase in a continuous treatment variable.
Measures of comparable MTEs for several options can be used to decide where a marginal
dollar (or metaphorical marginal dollar, including any opportunity costs and currency
translations) should be spent. In other words, with ﬁnite resources, we care more about
budget-neutral improvements in eﬀectiveness than the eﬀect of a unit increase in treatment, so we can choose among treatment options with equal cost. Quasiexperimental
methods, especially IV and RD, often estimate such MTEs directly.
If the eﬀect of a treatment XT varies across individuals (i.e., it is not the case
that βi = β for all i), the ATE for diﬀerent subpopulations will diﬀer.
expect diﬀerent consistent estimators to converge to diﬀerent quantities. This problem
is larger than the selection-bias issue.
Even in the absence of endogenous selection
of XT (but possibly with some correlation between XT
and βi, itself now properly
regarded as a random variable) in a linear model, ordinary least squares (OLS) will not,
in general, be consistent for the average over all i of individual eﬀects βi. Only with
strong distributional assumptions can we proceed; e.g., if we assume βi is normally
distributed then the ATE may be consistently estimated by xtmixed or xtrc, or if we
assume XT is normally distributed then the ATE may be consistently estimated by OLS.
Causal inference with observational data
Regression and panel methods
If an omitted variable can be measured or proxied by another variable, an ordinary
regression may yield an unbiased estimate. The most eﬃcient estimates (ignoring issues
around weights or nonindependent errors) are produced by OLS when it is unbiased.
The measurement error entailed in a proxy for an unobservable, however, could actually exacerbate bias, rather than reduce it. One is usually concerned that cases with
diﬀering XT may also diﬀer in other ways, even conditional on all other observables XC
(“control” variables). Nonetheless, a sequence of ordinary regressions that add or drop
variables can be instructive as to the nature of various forms of omitted-variable bias
in the available data.
A complete discussion of panel methods would not ﬁt in any one book, much less
this article. However, the idea can be illuminated with one short example using linear
regression.
Suppose that our theory dictates a model is of the form
y = β0 + XT βT + XUβU + ε
where we do not observe XU. The omitted variables XU vary only across groups, where
group membership is indexed by i, so a representative observation can be written as
yit = β0 + XT
itβT + ui + εit
where ui = XU
i βU. Then we can eliminate the bias arising from omission of XU by
diﬀerencing
yit −yis = (XT
is)βT + (εit −εis)
using various deﬁnitions of s.
The idea of using panel methods to identify a causal impact is to use an individual
panel i as its own control group, by including information from multiple points in time.
The second dimension of the data indexed by t need not be time, but it is a convenient
viewpoint.
A ﬁxed-eﬀects (FE) model such as xtreg, fe eﬀectively subtracts the within-i mean
values of each variable, so, for example, X
is, and the model
yit −yi = (XT
i )βT + (εit −εi)
can be estimated with OLS. This is also called the “within estimator” and is equivalent to
a regression that includes an indicator variable for each panel i, allowing for a diﬀerent
intercept term for each panel.
An alternative to the FE model is to use the ﬁrst diﬀerence (FD), i.e., s = (t −1) or
yit −yi(t−1) = (XT
i(t−1))βT + (εit −εi(t−1))
which is regress d.y d.x in tsset data or xtivreg2 y x, fd , which oﬀers more standard error (SE) corrections beyond cluster() and robust.
A. Nichols
A third option is to use the long diﬀerence (LD), keeping only two observations per
group. For a balanced panel, if t = b is the last observation and t = a is the ﬁrst, the
yib −yia = (XT
ia)βT + (εib −εia)
producing only one observation per group (the diﬀerence of the ﬁrst and last observations).
Figure 1 shows the interpretation of these three types of estimates by showing one
panel’s contribution to the estimated eﬀect of an indicator variable that equals one for
all t > 3 (t in 0, . . . , 10) and equals zero elsewhere—e.g., a policy that comes into eﬀect
at some point in time (at t = 4 in the example). The FE estimate compares the mean
outcomes before and after, the FD estimate compares the outcome just prior to and just
after the change in policy, and the LD estimate compares outcomes well before and well
after the change in policy.
Figure 1: One panel’s contributions to FE/FD/LD estimates
Clearly, one must impose some assumptions on the speed with which XT aﬀects y
or have some evidence as to the right time frame for estimation. This type of choice
comes up frequently when stock prices are supposed to have adjusted to some news,
especially given the frequency of data available; economists believe the new information
is capitalized in prices, but not instantaneously.
Taking a diﬀerence in stock prices
between 3 p.m. and 3:01 p.m. is inappropriate but taking a diﬀerence over a year is
clearly inappropriate as well, because new information arrives continuously.
In panel models, one must usually think carefully about within-panel trends and the
frequency of measurement. (We cannot usually obtain consistent estimates of withinpanel trends for the same reason that we cannot usually obtain consistent estimates of
Causal inference with observational data
FE: the number of parameters increases linearly in the number of panels, N.) Baum
 discussed some ﬁltering techniques to get diﬀerent frequency “signals” from noisy
data. A simple method used in Baker, Benjamin, and Stanger is often attractive,
because it oﬀers an easy way to decompose any variable Xt into two orthogonal components: a high-frequency component (Xt −Xt−1)/2 and a low-frequency component
(Xt + Xt−1)/2 that together sum to Xt.
A simple example of all three (FE, FD, and LD) is
webuse grunfeld
xtreg inv ks, fe vce(cluster company)
regress d.inv d.ks, vce(cluster company)
summarize time, meanonly
generate t=time if time==r(min) | time==r(max)
tsset company t
regress d.inv d.ks, vce(cluster company)
Clearly, diﬀerent assumptions about the error process apply in each case, in addition to
assumptions about the speed with which XT aﬀects y. The FD and LD models require
an ordered t index (such as time).
The vce(cluster clustvar) option used above
should be considered nearly de rigeur in panel models to allow for errors that may be
correlated within group and not identically distributed across groups. The performance
of the cluster–robust estimator is good with 50 or more clusters, or fewer if the clusters
are large and balanced . For LD, the vce(cluster clustvar)
option is equivalent to the vce(robust) option, because each group is represented by
one observation.
Having eliminated bias due to unobservable heterogeneity across i units, it is often
tempting to diﬀerence or demean again. It is common to include indicator variables for
t in FE models, for example,
webuse grunfeld
quietly tabulate year, generate(d)
xtreg inv ks d*, fe vce(cluster company)
The above commands create a two-way FE model. If individuals, i, are observed in
diﬀerent settings, j—for example, students who attend various schools or workers who
reside in various locales over time—we can also include indicator variables for j in
an FE model. Thus we can consider various n-way FE models, though models with
large numbers of dimensions for FE may rapidly become unstable or computationally
challenging to ﬁt.
The LD, FD, and FE estimators use none of the cross-sectional diﬀerences across
groups (individuals), i, which can lead to lower eﬃciency (relative to an estimator that
exploits cross-sectional variation). They also drop any variables that do not vary over
t within i, so the coeﬃcients on some variables of interest may not be estimated with
these methods.
The random-eﬀects estimator (RE) available with xtreg exploits cross-sectional variation and reports coeﬃcients on variables that do not vary over t within i, but it requires
strong assumptions about error terms that are often violated in practice. Particularly,
A. Nichols
for RE to be unbiased in situations where FE is unbiased, we must assume that ui is
uncorrelated with XT
it (which contradicts our starting point above, where we worried
about a XU correlated with XT ). There is no direct test of this assumption about
an unobservable disturbance term, but hausman and xtoverid oﬀer a test that the coeﬃcients estimated in both the RE and FE models are the
same, e.g.,
ssc install xtoverid
webuse grunfeld
egen ik=max ), by(company)
xtreg inv ks ik, re vce(cluster company)
where a rejection casts doubt on whether RE is unbiased when FE is biased.
Other xt commands, such as xtmixed (see [XT] xtmixed) and xthtaylor (see
[XT] xthtaylor), oﬀer a variety of other panel methods that generally make further
assumptions about the distribution of disturbances and sources of endogeneity. Typically, there is a tradeoﬀbetween improved eﬃciency bought by making assumptions
about the data-generating process versus robustness to various violations of assumptions. See also Griliches and Hausman for more considerations related to all the
above panel methods. Rothstein oﬀers a useful applied examination of identifying
assumptions in FE models and correlated RE models.
Generally, panel methods eliminate the bias because of some unobserved factors and
not others. Considering the FE, FD, and LD models, it is often hard to believe that all
the selection on unobservables is because of time-invariant factors. Other panel models
often require unpalatable distributional assumptions.
Matching estimators
For one discrete set of treatments, XT , we want to compare means or proportions much
as we would in an experimental setting. We may be able to include indicators and interactions for factors (in XC) that aﬀect selection into the treatment group (say, deﬁned
by XT = 1), to estimate the impact of treatment within groups of identical XC using
a fully saturated regression. There are also matching estimators that compare observations with XC by pairing observations that are close by some metric .
alternative approaches involve reweighting so the joint or marginal distributions of XC
are identical for diﬀerent groups.
Matching or reweighting approaches can give consistent estimates of a huge variety of
ATEs, but only under the assumptions that the selection process depends on observables
and that the model used to match or reweight is a good one. Often we push the problems
associated with observational data from estimating the eﬀect of XT on y down onto
estimating the eﬀect of XC on XT . For this reason, estimates based on reweighting or
matching are unlikely to convince someone unconvinced by OLS results. Selection on
observables is not the type of selection most critics have in mind.
Causal inference with observational data
Nearest-neighbor matching
Nearest-neighbor matching pairs observations in the treatment and control groups and
computes the diﬀerence in outcome y for each pair and then the mean diﬀerence across
pairs. The Stata command nnmatch was described by Abadie et al. . Imbens
 covered details of nearest-neighbor matching methods. The downside to nearestneighbor matching is that it can be computationally intensive, and bootstrapped SEs
are infeasible owing to the discontinuous nature of matching .
Propensity-score matching
Propensity-score matching essentially estimates each individual’s propensity to receive
a binary treatment (with a probit or logit) as a function of observables and matches
individuals with similar propensities. As Rosenbaum and Rubin showed, if the
propensity was known for each case, it would incorporate all the information about selection, and propensity-score matching could achieve optimal eﬃciency and consistency.
In practice, the propensity must be estimated and selection is not only on observables,
so the estimator will be both biased and ineﬃcient.
Morgan and Harding provide an excellent overview of practical and theoretical issues in matching and comparisons of nearest-neighbor matching and propensityscore matching. Their expositions of diﬀerent types of propensity-score matching and
simulations showing when it performs badly are particularly helpful. Stuart and Rubin
 oﬀer a more formal but equally helpful discussion of best practices in matching.
Typically, one treatment case is matched to several control cases, but one-to-one
matching is also common and may be preferred .
One Stata command psmatch2 is available from the Statistical Software Components (SSC) archive (ssc describe psmatch2) and has a useful help
ﬁle. There is another useful Stata command pscore . psmatch2 will perform one-to-one (nearest neighbor or within caliper,
with or without replacement), k-nearest neighbors, radius, kernel, local linear regression,
and Mahalanobis matching.
Propensity-score methods typically assume a common support; i.e., the range of
propensities to be treated is the same for treated and control cases, even if the density
functions have diﬀerent shapes.
In practice, it is rare that the ranges of estimated
propensity scores are the same for both the treatment and control groups, but they
do nearly always overlap. Generalizations about treatment eﬀects should probably be
limited to the smallest connected area of common support.
Often a density estimate below some threshold greater than zero deﬁnes the end of
common support; see Heckman, Ichimura, and Todd for more discussion. This
is because the common support is the range where both densities are nonzero, but
the estimated propensity scores take on a ﬁnite number of values. Thus the empirical
densities will be zero almost everywhere. Generally, we need to use a kernel density
estimator like kdensity to obtain smooth estimated densities of the propensity score
A. Nichols
for both treatment and control groups, but then areas of zero density will have positive
density estimates. Thus some small value f0 is redeﬁned to be eﬀectively zero, and
the smallest connected range of estimated propensity scores λ with f(λ) ≥f0 for both
treatment and control groups is used in the analaysis, and observations outside this
range are discarded.
Regardless of whether the estimation or extrapolation of estimates is limited to a
range of propensities or ranges of XC variables, the analyst should present evidence
on how the treatment and control groups diﬀer and on which subpopulation is being
studied. The standard graph here is an overlay of kernel density estimates of propensity
scores for treatment and control groups. This is easy to create in Stata with twoway
Sensitivity testing
Matching estimators have perhaps the most detailed literature on formal sensitivity
Rosenbaum bounds on treatment eﬀects may be constructed by using psmatch2 and rbounds, a user-written command by DiPrete and Gangl ,
who compare Rosenbaum bounds in a matching model with IV estimates. sensatt by
Nannicini and mhbounds by Becker and Caliendo are also Stata programs
for sensitivity testing in matching models.
Reweighting
The propensity score can also be used to reweight treatment and control groups so the
distribution of XC looks the same in both groups. The basic idea is to use a probit or
logit regression of treatment on XC to estimate the conditional probability λ of being
in the treatment group and to use the odds λ/(1 −λ) as a weight. This is like inverting
the test of randomization used in experimental designs to make the group status look
as if it were randomly assigned.
As Morgan and Harding point out, all the matching estimators can also be
thought of various reweighting schemes whereby treatment and control observations are
reweighted to allow causal inference on the diﬀerence in means. A treatment case i
matched to k cases in an interval, or k-nearest neighbors, contributes yi −k−1 k
the estimate of a treatment eﬀect. One could easily rewrite the estimate of a treatment
eﬀect as a weighted-mean diﬀerence.
The reweighting approach leads to a whole class of weighted least-squares estimators and is connected to techniques described by DiNardo, Fortin, and Lemieux ,
Autor, Katz, and Kerney , Leibbrandt, Levinsohn, and McCrary , and
Machado and Mata .
These techniques are related to various decomposition
techniques in Blinder , Oaxaca , Yun , Gomulka and Stern
 , and Juhn, Murphy, and Pierce . DiNardo usefully outlines
some connections between propensity-score methods and the decomposition techniques.
Causal inference with observational data
The dfl , oaxaca , and jmpierce commands available from the SSC archive are useful for the latter.
The decomposition
techniques seek to attribute observed diﬀerences in an outcome y both to diﬀerences
in XC variables and diﬀerences in the associations between XC variables and y. They
are most useful for comparing two distributions where the binary variable deﬁning the
group to which an observation belongs is properly considered exogenous, e.g., sex or
calendar year. See also Rubin .
The reweighting approach is particularly useful in combining matching-type estimators with other methods, e.g., FE regression. After constructing weights w = λ/(1 −λ)
(or the product of weights w = w0λ/(1 −λ), where w0 is an existing weight on the data
used in the construction of λ) that equalize the distributions of XC, other commands
can be run on the reweighted data, e.g., areg for a FE estimator.
Imagine the outcome is wage and the treatment variable is union membership. One
can reweight union members to have distributions of education, age, race/ethnicity, and
other job and demographic characteristics equivalent to nonunion workers (or a subset
of nonunion workers). One could compare otherwise identical persons within occupation
and industry cells by using a regression approach or nnmatch with exact matching on
some characteristics. An example comparing several regressions with propensity-score
matching is
ssc install psmatch2
webuse nlswork
xi i.race i.ind i.occ
local x "union coll age ten not_s c_city south nev_m _I*"
regress ln_w union
regress ln_w `x´
generate u=uniform()
psmatch2 `x´, out(ln_w) ate
twoway kdensity _ps if _tr || kdensity _ps if !_tr
generate w=_ps/(1-_ps)
regress ln_w `x´ [pw=w] if _ps<.3
regress ln_w `x´ [pw=w]
The estimated union wage premium is about 13% in a regression but about 15% in the
matching estimate of the average beneﬁt to union workers (the ATE on the treated) and
about 10% on average for everyone (the ATE). The reweighted regressions give diﬀerent estimates: for the more than 70% of individuals who are unlikely to be unionized
(propensity under 30%), the wage premium is about 9%, and for the full sample, it is
about 18%.
Arguably none of these estimates of wage premiums correspond to a readily speciﬁed
thought experiment, such as “what is the estimated eﬀect on wages of being in a union
for a randomly chosen individual?” (the ATE) or “what is the estimated eﬀect on wages
of being in a union for an individual just on the margin of being in a union or not?” (the
A. Nichols
LATE). DiNardo and Lee oﬀer a much more convincing set of causal estimates of
the LATE by using an RD design (see below).
We could also have estimated the wage premium of a college education by switching
coll and union in the above syntax (to ﬁnd a wage premium of 25% in a regression or
27% using psmatch2). We could use data from Card on education and wages
to ﬁnd a college wage premium of 29% using a regression or 30% using psmatch2.
use 
generate byte coll=educ>15
local x "coll age exper* smsa* south mar black reg662-reg669"
regress lw `x´
psmatch2 `x´, out(lw) ate
We return to this example in the next section.
Instrumental variables
An alternative to panel methods and matching estimators is to ﬁnd another set of
variables Z correlated with XT but not correlated with the error term, e.g., e in
y = XT βT + XCβC + e
so Z must satisfy E(Z′e) = 0 and E(Z′XT ) ̸= 0. The variables Z are called excluded
instruments, and a class of IV methods can then be used to consistently estimate an
impact of XT on y.
Various interpretations of the IV estimate have been advanced, typically as the LATE
 , meaning the eﬀect of XT on y for those who are
induced by their level of Z to have higher XT . For the college-graduate example, this
might be the average gain Ei{yi(t) −yi(0)} over all those i in the treatment group with
Z = 1 (where Z might be “lived close to a college” or “received a Pell grant”), arising
from an increase from XT = 0 to XT = t in treatment, i.e., the wage premium due to
college averaged over those who were induced to go to college by Z.
The IV estimators are generally only as good as the excluded instruments used, so
naturally criticisms of the predictors in a standard regression model become criticisms
of the excluded instruments in an IV model.
Also, the IV estimators are biased, but consistent, and are much less eﬃcient than
OLS. Thus failure to reject the null should not be taken as acceptance of the alternative. That is, one should never compare the IV estimate with only a zero eﬀect; other
plausible values should be compared as well, including the OLS estimate. Some other
common pitfalls discussed below include improper exclusion restrictions (addressed with
overidentiﬁcation tests) and weak identiﬁcation (addressed with diagnostics and robust
inference).
Since IV estimators are biased in ﬁnite samples, they are justiﬁed only for large
samples. Nelson and Startz showed how strange the ﬁnite sample behavior of an
Causal inference with observational data
IV estimator can be. Bound, Jaeger, and Baker showed that even large samples
of millions of observations are insuﬃcient for asymptotic justiﬁcations to apply in the
presence of weak instruments .
Key assumptions
Because IV can lead one astray if any of the assumptions is violated, anyone using an
IV estimator should conduct and report tests of the following:
• instrument validity (overidentiﬁcation or overid tests)
• endogeneity
• identiﬁcation
• presence of weak instruments
• misspeciﬁcation of functional form (e.g., RESET)
Further discussion and suggestions on what to do when a test is failed appear in the
relevant sections below.
Forms of IV
The standard IV estimator in a model
y = XT βT + XCβC + e
where we have Z satisfying E(Z′e) = 0 and E(Z′XT ) ̸= 0 is
⎠= (X′PZX)−1X′PZy
(ignoring weights), where X = (XT XC) and PZ is the projection matrix Za(Z′
with Za = (ZXC). We use the component of XT along Z, which is exogenous, as the
only source of variation in XT that we use to estimate the eﬀect on y.
These estimates are easily obtained in Stata 6–9 with the syntax ivreg y xc* (xt*
= z*), where xc* are all exogenous “included instruments” XC and xt* are endogenous
variables XT .
In Stata 10, the syntax is ivregress 2sls y xc* (xt* = z*).
Stata 9 and later, the ivreg2 command would be
ssc install ivreg2
ivreg2 y xc* (xt* = z*)
A. Nichols
Example data for using these commands can be easily generated, e.g.,
use clear
rename lw y
rename nearc4 z
rename educ xt
rename exper xc
The standard IV estimator is equivalent to two forms of two-stage estimators. The
ﬁrst, which gave rise to the moniker two-stage least squares (2SLS), has you regress XT
on XC and Z, predict 
XT , and then regress y on 
XT and XC. The coeﬃcient on 
foreach xt of varlist xt* {
regress `xt´ xc* z*
predict `xt´_hat
regress y xt*_hat xc*
will give the same estimates as the above IV commands. However, the reported SEs
will be wrong as Stata will use 
XT rather than XT to compute them. Even though IV
is not implemented in these two stages, the conceptual model of these ﬁrst-stage and
second-stage regressions is pervasive, and the properties of said ﬁrst-stage regressions
are central to the section on identiﬁcation and weak instruments below.
The second two-stage estimator that generates identical estimates is a controlfunction approach. Regress each variable in XT on the other variables in XT , XC,
and Z to predict the errors vT = XT −
XT and then regress y on XT , vT , and XC.
You will ﬁnd that the coeﬃcient on XT is βIV
T , and tests of signiﬁcance on each vT are
tests of endogeneity of each XT . Thus
capture drop *_hat
unab xt: xt*
foreach v of loc xt {
local otht: list xt-v
regress `v´ xc* z* `otht´
predict v_`xt´, resid
regress y xt* xc* v_*
will give the IV estimates, though again the standard errors will be wrong. However,
the tests of endogeneity (given by the reported p-values on variables v * above) will
be correct. A similar approach works for nonlinear models such as probit or poisson
(help ivprobit and findit ivpois for relevant commands). The tests of endogeneity
in nonlinear models given by the control-function approach are also robust .
The third two-stage version of the IV strategy, which applies for one endogenous
variable and one excluded instrument, is sometimes called the Wald estimator. First,
regress XT on XC and Z (let π be the estimated coeﬃcient on Z) and then regress y
on Z and XC (let γ be the estimated coeﬃcient on Z). The ratio of coeﬃcients on Z
(γ/π) is βIV, so
Causal inference with observational data
regress xt z xc*
local p=_b[z]
regress y z xc*
local g=_b[z]
display `g´/`p´
will give the same estimate as the IV command ivreg2 y xc* (xt=z). The regression
of y on Z and XC is sometimes called the reduced-form regression. This name is often
applied to other regressions, so I will avoid using the term.
The generalized method of moments, limited-information maximum likelihood, and
continuously updated estimation and generalized method of moments forms of IV are
discussed at length in Baum, Schaﬀer, and Stillman . Various implementations
are available with the ivregress and ivreg2 commands. Some forms of IV may be
expressed as k-class estimation, available from ivreg2, and there are many other forms
of IV models, including oﬃcial Stata commands, such as ivprobit, treatreg, and
ivtobit, and user-written additions, such as qvf , jive , and ivpois (on SSC).
Finding excluded instruments
The hard part of IV is ﬁnding a suitable Z matrix. The excluded instruments in Z
have to be strongly correlated with the endogenous XT and uncorrelated with the
unobservable error e. However, the problem we want to solve is that the endogenous
XT is correlated with the unobservable error e. A good story is the crucial element in
any plausible IV speciﬁcation. We must believe that Z is strongly correlated with the
endogenous XT but has no direct impact on y (is uncorrelated with the unobservable
error e), because the assumptions are not directly testable. However, the tests discussed
in the following sections can help support a convincing story and should be reported
Generally, speciﬁcation search in the ﬁrst-stage regressions of XT on some Z does
not bias estimates or inference nor does using generated regressors. However, it is easy
to produce counterexamples to this general rule. For example, taking Z = XT + ν,
where ν is a small random error, will produce strong identiﬁcation diagnostics—and
might pass overidentiﬁcation tests described in the next section—but will not improve
estimates (and could lead to substantially less accurate inference).
If some Z are weak instruments, then regressing XT on Z to get 
XT and using
XT as the excluded instruments in an IV regression of y on XT and XC will likewise
produce strong identiﬁcation diagnostics but will not improve estimates or inference.
Hall, Rudebusch, and Wilcox reported that choosing instruments based on measures of the strength of identiﬁcation could actually increase bias and size distortions.
Exclusion restrictions in IV
The exclusion restrictions E(Z′e) = 0 cannot be directly tested, but if there are more
excluded instruments than endogenous regressors, an overidentiﬁcation (overid) test
A. Nichols
is feasible and the result should be reported. If there are exactly as many excluded
instruments as endogenous regressors, the equation is exactly identiﬁed, and no overid
test is feasible.
However, if Z is truly exogenous, it is likely also true that E(W ′e) = 0, where W
contains Z, squares, and cross products of Z. Thus there is always a feasible overid
test by using an augmented set of excluded instruments, though E(W ′e) = 0 is a
stronger condition than E(Z′e) = 0.
For example, if you have two good excluded
instruments, you might multiply them together and square each to produce ﬁve excluded
instruments.
Testing the three extra overid restrictions is like Ramsey’s regression
speciﬁcation-error (RESET) test of excluded instruments. Interactions of Z and XC may
also be good candidates for excluded instruments. For reasons discussed below, adding
excluded instruments haphazardly is a bad idea, and with many weak instruments,
limited-information maximum likelihood or continuously updated estimation is preferred
to standard IV/2SLS.
Baum, Schaﬀer, and Stillman discuss the implementation of overid tests in
ivreg2 . Passing the overid test (i.e., failing
to reject the null of zero correlation) is neither necessary nor suﬃcient for instrument
validity, E(Z′e) = 0, but rejecting the null in an overid test should lead you to reconsider
your IV strategy and perhaps to look for diﬀerent excluded instruments.
Tests of endogeneity
Even if we have an excluded instrument that satisﬁes E(Z′e) = 0, there is no guarantee
that E(XT ′ε) ̸= 0 as we have been assuming. If E(XT ′ε) = 0, we prefer ordinary
regression to IV. Thus we should test the null that E(XT ′ε) = 0 (a test of endogeneity),
though this test requires instrument validity, E(Z′e) = 0, so it should follow any feasible
overid tests.
Baum, Schaﬀer, and Stillman describe several methods to test the endogeneity of a variable in XT , including the endog() option of ivreg2 and the standalone
ivendog command (both available from SSC archive, with excellent help ﬁles). Section 4.2 also shows how the control function form of IV can be used to test endogeneity
of a variable in XT .
Identiﬁcation and weak instruments
This is the second of the two crucial assumptions and presents problems of various
sizes in almost all IV speciﬁcations. The extent to which E(Z′XT ) ̸= 0 determines the
strength of identiﬁcation. Baum, Schaﬀer, and Stillman describe tests of identiﬁcation, which amount to tests of the rank of E(Z′XT ). These rank tests address
the concern that a number of excluded instruments may generate exogenous variation
in one endogenous variable and be uncorrelated with another endogenous variable, so
the equation is not identiﬁed even though it satisﬁes the order condition (the number
of excluded instruments is at least as great as the number of endogenous variables).
Causal inference with observational data
For example, if we have two endogenous variables X1 and X2 and three excluded instruments, all three excluded instruments may be correlated with X1 and not with X2.
The identiﬁcation tests look at the least partial correlation, or the minimum eigenvalue
of the Cragg–Donald statistic (?), for example, and measures of whether at least one
endogenous variable has no correlation with the excluded instruments.
Even if we reject the null of underidentiﬁcation and conclude E(Z′XT ) ̸= 0, we can
still face a “weak-instruments” problem if some elements of E(Z′XT ) are close to zero.
Even if we have an excluded instrument that satisﬁes E(Z′e) = 0, there is no guarantee that E(Z′XT ) ̸= 0. The IV estimate is always biased but is less biased than
OLS to the extent that identiﬁcation is strong. In the limit of weak instruments, there
would be no improvement over OLS for bias and the bias would be 100% of OLS. In the
other limit, the bias would be 0% of the OLS bias (though this would require that the
correlation between XT and Z be perfect, which is impossible since XT is endogenous
and Z is exogenous). In applications, you would like to know where you are on that
spectrum, even if only approximately.
There is also a distortion in the size of hypothesis tests. If you believe that you are
incorrectly rejecting a null hypothesis about 5% of the time (i.e., you have chosen a size
α = 0.05), you may actually face a size of 10% or 20% or more.
Stock and Yogo reported rule-of-thumb critical values to measure the extent
of both of these problems. Their table 1 shows the value of a statistic measuring the
predictive power of the excluded instruments that will imply a limit of the bias to some
percentage of OLS. For two endogenous variables and three excluded instruments (n = 2,
K2 = 5), the minimum value to limit the bias to 20% of OLS is 5.91. ivreg2 reports
these values as Stock–Yogo weak ID test critical values: one set for various percentages
of “maximal IV relative bias” (largest bias relative to OLS) and one set for “maximal IV
size” (the largest size of a nominal 5% test).
The key point is that all IV and IV-type speciﬁcations can suﬀer from bias and
size distortions, not to mention ineﬃciency and sometimes failures of exclusion restrictions. The Stock and Yogo approach measures how strong identiﬁcation is in
your sample, and ranktest oﬀers a similar statistic for
cases where errors are not assumed to be independently and identically distributed.
Neither provides solutions in the event that weak instruments appear to be a problem.
A further limitation is that these identiﬁcation statistics only apply to the linear case,
not the nonlinear analogs, including those estimated with generalized linear models.
In practice, researchers should report the identiﬁcation statistics for the closest linear
analog; i.e., run ivreg2 and report the output alongside the output from ivprobit,
ivpois, etc.
If you suspect weak instruments may be producing large bias or size distortions, you
have several options. You can ﬁnd better excluded instruments, possibly by transforming your existing instruments. You can use limited-information maximum likelihood
or continuously updated estimation, which are more robust to many weak instruments
than standard IV. Perhaps best of all, you can conduct inference that is robust to
A. Nichols
weak instruments: with one endogenous variable, use condivreg , or with more than one, use tests described by Anderson and Rubin and
Baum, Schaﬀer, and Stillman .
Functional form tests in IV
As Baum, Schaﬀer, and Stillman and Wooldridge discuss, the
RESET test regressing residuals on predicted y and powers thereof is properly a test of
a linearity assumption or a test of functional-form restrictions. ivreset performs the
IV version of the test in Stata. A more informative speciﬁcation check is the graphical
version of RESET: predict 
XT after the ﬁrst-stage regressions, compute forecasts y =
T + XC βC and yf = 
T + XC βC, and graph a scatterplot of the residuals
ε = y −y against yf. Any unmodeled nonlinearities may be apparent as a pattern in
the scatterplot.
Standard errors in IV
The largest issue in IV estimation is often that the variance of the estimator is much
larger than ordinary regression. Just as with ordinary regression, the SEs are asymptotically valid for inference under the restrictive assumptions that the disturbances are
independently and identically distributed. Getting SEs robust to various violations of
these assumptions is easily accomplished by using the ivreg2 command . Many other commands ﬁtting IV models oﬀer no equivalent
robust SE estimates, but it may be possible to assess the size and direction of SE corrections by using the nearest linear analog in the spirit of using estimated design eﬀects
in the survey regression context.
Inference in IV
Assuming that we have computed consistent SEs and the best IV estimate we can by
using a good set of Z and XC variables, there remains the question of how we interpret
the estimates and tests. Typically, IV identiﬁes a particular LATE, namely the eﬀect of
an increase in XT due to an increase in Z. If XT were college and Z were an exogenous
source of ﬁnancial aid, then the IV estimate of the eﬀect of XT on wages would be the
college wage premium for those who were induced to attend college by being eligible for
the marginally more generous aid package.
Angrist and Krueger estimated the eﬀect of education on earnings by using
compulsory schooling laws as a justiﬁcation for using quarter of birth dummies as instruments. Even if the critiques of Bound, Jaeger, and Baker did not apply, the
identiﬁed eﬀect would be for an increase in education due to being forced to remain
in school a few months more. That is, the measured wage eﬀect of another year of
education is roughly for the eleventh grade and only for those who would have dropped
out if not for compulsory schooling laws.
Causal inference with observational data
Sometimes a LATE of this form is exactly the estimate desired. If, however, we cannot
reject that the IV estimate diﬀers from the OLS estimate or the IV conﬁdence region
includes the OLS conﬁdence region, we may not have improved estimates but merely
produced noisier ones. Only where the IV estimate diﬀers can we hope to ascertain the
nature of selection bias.
We can use the data from Card to estimate the impact of education on wages,
where nearness to a college is used as a source of exogenous variation in educational
attainment:
use 
local x "exper* smsa* south mar black reg662-reg669"
regress lw educ `x´
ivreg2 lw `x´ (educ=nearc2 nearc4), first endog(educ)
ivreg2 lw `x´ (educ=nearc2 nearc4), gmm
ivreg2 lw `x´ (educ=nearc2 nearc4), liml
The return to another year of education is found to be about 7% by using ordinary
regression or 16% or 17% by using IV methods. The Sargan statistic fails to reject that
excluded instruments are valid, the test of endogeneity is marginally signiﬁcant (giving
diﬀerent results at the 95% and 90% levels), and the Anderson–Rubin and Stock–Wright
tests of identiﬁcation strongly reject that the model is underidentiﬁed.
The test for weak instruments is the F test on the excluded instruments in the
ﬁrst-stage regression, which at 7.49 with a p-value of 0.0006 seems to indicate that the
excluded instruments inﬂuence educational attainment, but the size of Wald tests on
educ, which we specify as 5%, might be roughly 25%. To construct an Anderson–Rubin
conﬁdence interval, we can type
generate y=.
foreach beta in .069 .0695 .07 .36 .365 .37 {
quietly replace y=lw-`beta´*educ
quietly regress y `x´ nearc2 nearc4
display as res "Test of beta=" `beta´
test nearc2 nearc4
This gives a conﬁdence interval of (.07, .37); see Nichols and Baum, Schaﬀer,
and Stillman . Thus the IV conﬁdence region includes the OLS estimate and
nearly includes the OLS conﬁdence interval, so the evidence on selection bias is weak.
Still, if we accept the exclusion restrictions as valid, the evidence does not support a
story where omitting ability (causing both increased wages and increased education)
leads to positive bias. If anything, the bias seems likely to be negative, perhaps due to
unobserved heterogeneity in discount rates or credit market failures. In the latter case,
the omitted factor may be a social or economic disadvantage observable by lenders.
A similar set of conclusions apply if we model the education response as a binary
treatment, college:
A. Nichols
generate byte coll=educ>15
regress lw coll `x´
treatreg lw `x´, treat(coll=nearc2 nearc4)
ivreg2 lw `x´ (coll=nearc2 nearc4), first endog(coll)
ivreg2 lw `x´ (coll=nearc2 nearc4), gmm
ivreg2 lw `x´ (coll=nearc2 nearc4), liml
These regressions also indicate that the OLS estimate may be biased downward, but the
OLS conﬁdence interval is contained in the treatreg and IV conﬁdence intervals. Thus
we cannot conclude much with conﬁdence.
RD designs
The idea of the RD design is to exploit an observable discontinuity in the level of treatment related to an assignment variable Z, so the level of treatment XT jumps discontinuously at some value of Z, called the cutoﬀ. Let Z0 denote the cutoﬀ. In the
neighborhood of Z0, under some often plausible assumptions, a discontinuous jump in
the outcome y can be attributed to the change in the level of treatment. Near Z0, the
level of treatment can be treated as if it is randomly assigned. For this reason, the RD
design is generally regarded as having the greatest internal validity of the quasiexperimental estimators.
Examples include share of votes received in a U.S. Congressional election by the
Democratic candidate as Z, which induces a clear discontinuity in XT , the probability
of a Democrat occupying oﬃce the following term, and XT may aﬀect various outcomes
y, if Democratic and Republican candidates actually diﬀer in close races .
DiNardo and Lee use the share of votes received for a union as Z, and unions
may aﬀect the survival of a ﬁrm (but do not seem to). They point out that the union
wage premium, y, can be consistently estimated only if survival is not aﬀected (no
diﬀerential attrition around Z0), and they ﬁnd negligibly small eﬀects of unions on
The standard treatment of RD is Hahn, Todd, and van der Klaauw , who clarify the link to IV methods. Recent working papers by Imbens and Lemieux and
McCrary focus on some important practical issues related to RD designs.
Many authors stress a distinction between “sharp” and “fuzzy” RD. In sharp RD
designs, the level of treatment rises from zero to one at Z0, as in the case where treatment
is having a Democratic representative in the U.S. Congress or establishing a union, and
a winning vote share deﬁnes Z0. In fuzzy RD designs, the level of treatment increases
discontinuously, or the probability of treatment increases discontinuously, but not from
zero to one. Thus we may want to deﬂate by the increase in XT at Z0 in constructing
our estimate of the causal impact of a one-unit change in XT .
In sharp RD designs, the jump in y at Z0 is the estimate of the causal impact of
XT . In a fuzzy RD design, the jump in y divided by the jump in XT at Z0 is the local
Wald estimate (equivalent to a local IV estimate) of the causal impact. The local Wald
estimate reduces to the jump in y at Z0 in a sharp RD design as the jump in XT is one,
Causal inference with observational data
so the distinction between fuzzy and sharp RD is not that sharp. Some authors, e.g.,
Shadish, Cook, and Campbell , seem to characterize as fuzzy RD a wider
class of problems, where the cutoﬀitself may not be sharply deﬁned. However, without
a true discontinuity, there can be no RD. The fuzziness in fuzzy RD arises only from
probabilistic assignment of XT in the neighborhood of Z0.
Key assumptions and tests
The assumptions that allow us to infer a causal eﬀect on y because of an abrupt change in
XT at Z0 are the change in XT at Z0 is truly discontinuous, Z is observed without error
 , y is a continuous function of Z at Z0 in the absence of treatment
(for individuals), and that individuals are not sorted across Z0 in their responsiveness
to treatment. None of these assumptions can be directly tested, but there are diagnostic
tests that should always be used.
The ﬁrst is to test the null that no discontinuity in treatment occurs at Z0, since
without identifying a jump in XT we will be unable to identify the causal impact of said
jump. The second is to test that there are no other extraneous discontinuities in XT or
y away from Z0, as this would call into question whether the functions would be smooth
through Z0 in the absence of treatment. The third and fourth test that predetermined
characteristics and the density of Z exhibit no jump at Z0, since these call into question
the exchangeability of observations on either side of Z0. Then the estimate itself usually
supplies a test that the treatment eﬀect is nonzero (y jumps at Z0 because XT jumps
Abusing notation somewhat so that Δ is an estimate of the discontinuous jump in
a variable, we can enumerate these tests as
• (T1) ΔXT (Z0) ̸= 0
• (T2) ΔXT (Z ̸= Z0) = 0 and Δy(Z ̸= Z0) = 0
• (T3) ΔXC(Z0) = 0
• (T4) Δf(Z0) = 0
• (T5) Δy(Z0) ̸= 0 or
Methodological choices
Estimating the size of a discontinuous jump can be accomplished by comparing means
in small bins of Z to the left and right of Z0 or with a regression of various powers of
Z, an indicator D for Z > Z0, and interactions of all Z terms with D (estimating a
polynomial in Z on both sides of Z0, and comparing the intercepts at Z0). However,
since the goal is to compute an eﬀect at precisely one point (Z0) using only the closest
observations, the standard approach is to use local linear regression, which minimizes
A. Nichols
bias . In Stata 10, this is done with the lpoly command; users
of previous Stata versions can use locpoly .
Having chosen to use local linear regression, other key issues are the choice of bandwidth and kernel. Various techniques are available for choosing bandwidths , and the triangle kernel has good properties in
the RD context, due to being boundary optimal .
There are several rule-of-thumb bandwidth choosers and cross-validation techniques
for automating bandwidth choice, but none is foolproof. McCrary contains a
useful discussion of bandwidth choice and claims that there is no substitute for visual
inspection comparing the local polynomial smooth with the pattern in a scatterplot.
Because diﬀerent bandwidth choices can produce diﬀerent estimates, the researcher
should report at least three estimates as an informal sensitivity test: one using the
preferred bandwidth, one using twice the preferred bandwidth, and another using half
the preferred bandwidth.
(T1) XT jumps at Z0
The identifying assumption is that XT jumps at Z0 because of some known legal or
program-design rules, but we can test that assumption easily enough. The standard
approach to computing SEs is to bootstrap the local linear regression, which requires
wrapping the estimation in a program, for example,
program discont, rclass
version 10
syntax [varlist(min=2 max=2)] [, *]
tokenize `varlist´
tempvar z f0 f1
quietly generate `z´=0 in 1
local opt "at(`z´) nogr k(tri) deg(1) `options´"
lpoly `1´ `2´ if `2´<0, gen(`f0´) `opt´
lpoly `1´ `2´ if `2´>=0, gen(`f1´) `opt´
return scalar d=`=`f1´ -`f0´ ´
display as txt "Estimate: " as res `f1´ -`f0´ 
ereturn clear
In the program, the assignment variable Z is assumed to be deﬁned so that the cutoﬀ
Z0 = 0 (easily done with one replace or generate command subtracting Z0 from Z).
The triangle kernel is used and the default bandwidth is chosen by lpoly, which is
probably suboptimal for this application. The local linear regressions are computed
twice: once using observations on one side of the cutoﬀfor Z < 0 and once for Z ≥0.
The estimate of a jump uses only the predictions at the cutoﬀZ0 = 0, so these are the
only values computed by lpoly.
Causal inference with observational data
We can easily generate data to use this example program:
ssc install rd, replace
net get rd
use votex if i==1
rename lne y
rename win xt
rename d z
foreach v of varlist pop-vet {
rename `v´ xc_`v´
bs: discont y z
In a more elaborate version of this program called rd (which also supports earlier
versions of Stata), available by typing ssc inst rd in Stata, the default bandwidth is
selected to include at least 30 observations in estimates at both sides of the boundary.
Other options are also available. Try findit bandwidth to ﬁnd more sophisticated
bandwidth choosers for Stata. The key point is to use the at() option of lpoly so that
the diﬀerence in local regression predictions can be computed at Z0.
A slightly more elaborate version of this program would save local linear regression
estimates at a number of points and oﬀer a graph to assess ﬁt:
program discont2, rclass
version 10
syntax [varlist(min=2 max=2)] [, s(str) Graph *]
tokenize `varlist´
tempvar z f0 f1 se0 se1 ub0 ub1 lb0 lb1
summarize `2´, meanonly
local N=round(100*(r(max)-r(min)))
cap set obs `N´
quietly generate `z´=(_n-1)/100 in 1/50
quietly replace `z´=-(_n-50)/100 in 51/`N´
local opt "at(`z´) nogr k(tri) deg(1) `options´"
lpoly `1´ `2´ if `2´<0, gen(`f0´) se(`se0´) `opt´
quietly replace `f0´=. if `z´>0
quietly generate `ub0´=`f0´+1.96*`se0´
quietly generate `lb0´=`f0´-1.96*`se0´
lpoly `1´ `2´ if `2´>=0, gen(`f1´) se(`se1´) `opt´
quietly replace `f1´=. if `z´<0
quietly generate `ub1´=`f1´+1.96*`se1´
quietly generate `lb1´=`f1´-1.96*`se1´
return scalar d=`=`f1´ -`f0´ ´
return scalar f1=`=`f1´ ´
return scalar f0=`=`f0´ ´
forvalues i=1/50 {
return scalar p`i´=`=`f1´[`i´]´
forvalues i=51/`N´ {
return scalar n`=`i´-50´=`=`f0´[`i´]´
display as txt "Estimate: " as res `f1´ -`f0´ 
if "`graph´"!="" {
label var `z´ "Assignment Variable"
local lines "|| line `f0´ `f1´ `z´"
local a "tw rarea `lb0´ `ub0´ `z´ || rarea `lb1´ `ub1´ `z´"
`a´ || sc `1´ `2´, mc(gs14) leg(off) sort `lines´
A. Nichols
if "`s´"!="" {
rename `z´ `s´`2´
rename `f0´ `s´`1´0
rename `lb0´ `s´`1´lb0
rename `ub0´ `s´`1´ub0
rename `f1´ `s´`1´1
rename `lb1´ `s´`1´lb1
rename `ub1´ `s´`1´ub1
ereturn clear
In this version, the local linear regressions are computed at a number of points on
either side of the cutoﬀZ0 (in the example, the maximum of Z is assumed to be 0.5, so
the program uses hundredths as a convenient unit for Z), but the estimate of a jump
still uses only the two estimates at Z0. The s() option in the above program saves the
local linear regression predictions (and lpoly conﬁdence intervals) to new variables that
can then be graphed. Graphs of all output are advisable to assess the quality of the
ﬁt for each of several bandwidths. This program may also be bootstrapped, although
recovering the standard errors around each point estimate from bootstrap for graphing
the ﬁt is much more work than using the output of lpoly as above.
(T2) y and XC continuous away from Z0
Although we need only assume continuity at Z0 and need no assumption that the
outcome and treatment variables are continuous at values of Z away from the cutoﬀZ0
(i.e., ΔXT (Z ̸= Z0) = 0 and Δy(Z ̸= Z0) = 0), it is reassuring if we fail to reject the
null of a zero jump at various values of Z away from the cutoﬀZ0 (or reject the null
only in 5% of cases or so). Having deﬁned a program discont, we can easily randomly
choose 100 placebo cutoﬀpoints Zp ̸= Z0, without replacement in the example below,
and test the continuity of XT and y at each.
by z, sort: generate f=_n>1 if z!=0
generate u=uniform()
replace u=(_n<=100)
levelsof z if u, loc(p)
foreach val of local p {
capture drop newz
generate newz=z-`val´
bootstrap r(d), reps(100): discont y znew
bootstrap r(d), reps(100): discont xt znew
(T3) XC continuous around Z0
If we can regard an increase in treatment XT as randomly assigned in the neighborhood
of the cutoﬀZ0, then predetermined characteristics XC such as race or sex of treated
individuals should not exhibit a discontinuity at the cutoﬀZ0. This is equivalent to the
standard test of randomization in an experimental design, using a test of the equality
Causal inference with observational data
of the mean of every variable in XC across treatment and control groups (see help
hotelling in Stata), or the logically equivalent test that all the coeﬃcients on XC in a
regression of XT on XC are zero. As in the experimental setting, in practice the tests
are usually done one at a time with no adjustment for multiple hypothesis testing (see
mtest in Stata).
In the RD setting, this is simply a test that the measured jump in each predetermined
XC is zero at the cutoﬀZ0 or ΔXC(Z0) = 0 for all XC. If we fail to reject that the
measured jump in XC is zero, for all XC, we have more evidence that observations on
both sides of the cutoﬀare exchangeable, at least in some neighborhood of the cutoﬀ, and
we can treat them as if they were randomly assigned treatment in that neighborhood.
Having deﬁned the programs discont and discont2, we can simply type
foreach v of varlist xc* {
bootstrap r(d), reps(100): discont `v´ z
discont2 `v´ z, s(h)
scatter `v´ z, mc(gs14) sort || line h`v´0 h`v´1 hz, name(`v´)
(T4) Density of Z continuous at cutoﬀ
McCrary gives an excellent account of a violation of exchangability of observations around the cutoﬀ. If individuals have preferences over treatment and can manipulate assignment, for instance by altering their Z or misreporting it, then individuals
close to Z0 may shift across the boundary. For example, some nonrandomly selected
subpopulation of those who are nearly eligible for food stamps may misreport income,
whereas those who are eligible do not. This creates a discontinuity in the density of Z
at Z0. McCrary points out that the absence of a discontinuity in the density
of Z at Z0 is neither necessary nor suﬃcient for exchangability. However, a failure to
reject the null hypothesis, which indicates the jump in the density of Z at Z0 is zero, is
reassuring nonetheless.
McCrary discussed a test in detail and advocated a bandwidth chooser. We
can also adapt our existing program to this purpose by using multiple kdensity commands to estimate the density to the left and right of Z0:
kdensity z if z<0, gen(f0) at(z) tri nogr
count f0 if z>=0
replace f0=f0/r(N)*`=_N´/4
kdensity z if z>=0, gen(f1) at(z) tri nogr
count f1 if z<0
replace f1=f1/r(N)*`=_N´/4
generate f=cond(z>=0,f1,f0)
bootstrap r(d), reps(100): discont f z
discont2 f z, s(h) g
We could also wrap the kdensity estimation inside the program that estimates
the jump, so that both are bootstrapped together; this approach is taken by the rd
command available by typing ssc inst rd.
A. Nichols
(T5) Treatment-eﬀect estimator
Having deﬁned the program discont, we can type
bootstrap r(d), reps(100): discont y z
to get an estimate of the treatment eﬀect in a sharp RD setting, where XT jumps from
zero to one at Z0. For a fuzzy RD design, we want to compute the jump in y scaled by
the jump in XT at Z0, or the local Wald estimate, for which we need to modify our
program to estimate both discontinuities. The program rd available by typing ssc inst
rd does this, but the idea is illustrated in the program below by using the previously
deﬁned discont program twice.
program lwald, rclass
version 10
syntax varlist [, w(real .06) ]
tokenize `varlist´
display as txt "Numerator"
discont `1´ `3´, bw(`w´)
local n=r(d)
return scalar numerator=`n´
display as txt "Denominator"
discont `2´ `3´, s(`sd´) bw(`w´)
local d=r(d)
return scalar denominator=`d´
return scalar lwald=`n´/`d´
display as txt "Local Wald Estimate:" as res `n´/`d´
ereturn clear
This program takes three arguments—the variables y, XT , and Z—assumes Z0 = 0,
and uses a hardwired default bandwidth of 0.06. The default bandwidth selected by
lpoly is inappropriate for these models, because we do not use a Gaussian kernel and
are interested in boundary estimates. The rd program from SSC archive is similar to the
above; however, it oﬀers more options—particularly with regard to bandwidth selection.
Voting examples abound. A novel estimate in Nichols and Rader measures the
eﬀect of electing as a Representative a Democratic incumbent versus a Republican
incumbent on a district’s receipt of federal grants:
ssc install rd
net get rd
use votex if i==1
rd lne d, gr
bs: rd lne d, x(pop-vet)
The above estimates that the marginally victorious Democratic incumbent brings 20%
less to his home district than a marginally victorious Republican incumbent. However,
we cannot reject the null of zero diﬀerence. This is true for a variety of bandwidth
choices (ﬁgure 2 shows the small insigniﬁcant eﬀect). The above is a sharp RD design,
Causal inference with observational data
but the Wald estimator can be used to estimate eﬀect, because the jump in win at 50%
of vote share is one and dividing by one has no impact on estimates.
Spending in District, from ZIP Code Match
Local Linear Regression for Democratic Incumbents
Local Linear Regression for Republican Incumbents
Federal Spending in Districts, 102nd U.S. Congress
Figure 2: RD example
Many good examples of fuzzy RD designs concern educational policy or interventions . Many educational grants
are awarded by using deterministic functions of predetermined characteristics, lending
themselves to evaluation using RD. For example, some U.S. Department of Education
grants to states are awarded to districts with a poverty (or near-poverty) rate above
a threshold, as determined by data from a prior Census, which satisﬁes all of the requirements for RD. The size of the discontinuity in funding may often be insuﬃcient
to identify an eﬀect. Often a power analysis is warranted to determine the minimum
detectable eﬀect.
Returning to the Card example of the eﬀect of education on earnings,
we can imagine exploiting a discontinuity in the availability of college to residents of
certain U.S. states at the state boundary. College applicants who live 4.8 miles and
5 miles from a college may look similar in various observable characteristics, but if a
state boundary separates them at 4.9 miles from the college, and the college is a state
institution, they may face diﬀerent probabilities of admission or tuition costs. The data
in Card do not support this strategy, of course, because we would need to
know the exact locations of all individuals relative to state boundaries. However, it
helps to clarify the assumptions that justify the IV approach. We need to assume that
location relative to colleges is randomly sprinkled over potential applicants, which seems
questionable , especially when one considers including parental education
in the model.
A. Nichols
Conclusions
Often exploring data using quasiexperimental methods is the only option for estimating
a causal eﬀect when experiments are infeasible, and may sometimes be preferred even
when an experiment is feasible, particularly if a MTE is of interest. However, the methods
can suﬀer several severe problems when assumptions are violated, even weakly. For this
reason, the details of implementation are frequently crucial, and a kind of cookbook or
checklist for verifying that essential assumptions are satisﬁed has been provided above
for the interested researcher. As the topics discussed continue to be active research
areas, this cookbook should be taken merely as a starting point for further explorations
of the applied econometric literature on the relevant subjects.