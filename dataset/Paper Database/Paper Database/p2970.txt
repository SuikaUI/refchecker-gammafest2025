Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3615–3620,
Hong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics
SCIBERT: A Pretrained Language Model for Scientiﬁc Text
Iz Beltagy
Arman Cohan
Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA
{beltagy,kylel,armanc}@allenai.org
Obtaining large-scale annotated data for NLP
tasks in the scientiﬁc domain is challenging
and expensive. We release SCIBERT, a pretrained language model based on BERT to address the lack of highquality, large-scale labeled scientiﬁc data.
SCIBERT leverages unsupervised pretraining
on a large multi-domain corpus of scientiﬁc publications to improve performance on
downstream scientiﬁc NLP tasks.
We evaluate on a suite of tasks including sequence
tagging, sentence classiﬁcation and dependency parsing, with datasets from a variety
of scientiﬁc domains.
We demonstrate statistically signiﬁcant improvements over BERT
and achieve new state-of-the-art results on several of these tasks. The code and pretrained
models are available at 
com/allenai/scibert/.
Introduction
The exponential increase in the volume of scientiﬁc publications in the past decades has made
NLP an essential tool for large-scale knowledge
extraction and machine reading of these documents. Recent progress in NLP has been driven
by the adoption of deep neural models, but training such models often requires large amounts of
labeled data. In general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientiﬁc domains, annotated data
is difﬁcult and expensive to collect due to the expertise required for quality annotation.
As shown through ELMo ,
GPT and BERT , unsupervised pretraining of language
models on large corpora signiﬁcantly improves
performance on many NLP tasks. These models
return contextualized embeddings for each token
which can be passed into minimal task-speciﬁc
neural architectures. Leveraging the success of unsupervised pretraining has become especially important especially when task-speciﬁc annotations
are difﬁcult to obtain, like in scientiﬁc NLP. Yet
while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia.
In this work, we make the following contributions:
(i) We release SCIBERT, a new resource demonstrated to improve performance on a range of NLP
tasks in the scientiﬁc domain. SCIBERT is a pretrained language model based on BERT but trained
on a large corpus of scientiﬁc text.
(ii) We perform extensive experimentation to
investigate the performance of ﬁnetuning versus task-speciﬁc architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.
(iii) We evaluate SCIBERT on a suite of tasks
in the scientiﬁc domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks.
Background
The BERT model architecture is based on a multilayer bidirectional Transformer . Instead
of the traditional left-to-right language modeling
objective, BERT is trained on two tasks: predicting
randomly masked tokens and predicting whether
two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead
pretrained on scientiﬁc text.
Vocabulary
BERT uses WordPiece for unsupervised tokenization of the input
text. The vocabulary is built such that it contains
the most frequently used words or subword units.
We refer to the original vocabulary released with
BERT as BASEVOCAB.
We construct SCIVOCAB, a new WordPiece vocabulary on our scientiﬁc corpus using the SentencePiece1 library. We produce both cased and
uncased vocabularies and set the vocabulary size
to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and
SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientiﬁc and general domain texts.
We train SCIBERT on a random sample
of 1.14M papers from Semantic Scholar . This corpus consists of 18% papers
from the computer science domain and 82% from
the broad biomedical domain. We use the full text
of the papers, not just the abstracts. The average
paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to
the 3.3B tokens on which BERT was trained. We
split sentences using ScispaCy ,2 which is optimized for scientiﬁc text.
Experimental Setup
We experiment on the following core NLP tasks:
1. Named Entity Recognition (NER)
2. PICO Extraction (PICO)
3. Text Classiﬁcation (CLS)
4. Relation Classiﬁcation (REL)
5. Dependency Parsing (DEP)
PICO, like NER, is a sequence labeling task where
the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes
in a clinical trial paper . REL
is a special case of text classiﬁcation where the
model predicts the type of relation expressed between two entities, which are encapsulated in the
sentence by inserted special tokens.
For brevity, we only describe the newer datasets
here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP annotates PICO spans in clinical trial abstracts. SciERC annotates entities and relations from computer science abstracts.
1 
sentencepiece
2 
ACL-ARC and SciCite assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientiﬁc papers that cite other papers. The Paper
Field dataset is built from the Microsoft Academic
Graph 3 and maps paper titles
to one of 7 ﬁelds of study. Each ﬁeld of study
(i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.
Pretrained BERT Variants
We use the pretrained weights for
BERT-Base released with the
original BERT code.4 The vocabulary is BASE-
VOCAB. We evaluate both cased and uncased versions of this model.
We use the original BERT code to
train SCIBERT on our corpus with the same con-
ﬁguration and size as BERT-Base.
We train 4
different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB. The
two models that use BASEVOCAB are ﬁnetuned
from the corresponding BERT-Base models. The
other two models that use the new SCIVOCAB are
trained from scratch.
Pretraining BERT for long sentences can be
slow. Following the original BERT code, we set a
maximum sentence length of 128 tokens, and train
the model until the training loss stops decreasing.
We then continue training the model allowing sentence lengths up to 512 tokens.
We use a single TPU v3 with 8 cores. Training
the SCIVOCAB models from scratch on our corpus
takes 1 week5 (5 days with max length 128, then
2 days with max length 512). The BASEVOCAB
models take 2 fewer days of training because they
aren’t trained from scratch.
All pretrained BERT models are converted to
be compatible with PyTorch using the pytorchtransformers library.6
All our models (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP .
3 
4 
5BERT’s largest model was trained on 16 Cloud TPUs for
4 days. Expected 40-70 days on an 8-GPU
6 
pytorch-transformers
We follow Devlin et al. in using
the cased models for NER and the uncased models
for all other tasks. We also use the cased models
for parsing. Some light experimentation showed
that the uncased models perform slightly better
(even sometimes on NER) than cased models.
Finetuning BERT
We mostly follow the same architecture, optimization, and hyperparameter choices used in Devlin
et al. .
For text classiﬁcation (i.e.
and REL), we feed the ﬁnal BERT vector for the
[CLS] token into a linear classiﬁcation layer. For
sequence labeling (i.e. NER and PICO), we feed
the ﬁnal BERT vector for each token into a linear
classiﬁcation layer with softmax output. We differ slightly in using an additional conditional random ﬁeld, which made evaluation easier by guaranteeing well-formed entities. For DEP, we use
the model from Dozat and Manning with
dependency tag and arc embeddings of size 100
and biafﬁne matrix attention over BERT vectors instead of stacked BiLSTMs.
In all settings, we apply a dropout of 0.1 and
optimize cross entropy loss using Adam . We ﬁnetune for 2 to 5 epochs using
a batch size of 32 and a learning rate of 5e-6, 1e-
5, 2e-5, or 5e-5 with a slanted triangular schedule
 which is equivalent to
the linear warmup followed by linear decay . For each dataset and BERT variant, we pick the best learning rate and number of
epochs on the development set and report the corresponding test results.
We found the setting that works best across
most datasets and models is 2 or 4 epochs and a
learning rate of 2e-5. While task-dependent, optimal hyperparameters for each task are often the
same across BERT variants.
Frozen BERT Embeddings
We also explore the usage of BERT as pretrained
contextualized word embeddings, like ELMo , by training simple task-speciﬁc
models atop frozen BERT embeddings.
For text classiﬁcation, we feed each sentence of
BERT vectors into a 2-layer BiLSTM of size 200
and apply a multilayer perceptron (with hidden
size 200) on the concatenated ﬁrst and last BiL-
STM vectors. For sequence labeling, we use the
same BiLSTM layers and use a conditional random ﬁeld to guarantee well-formed predictions.
For DEP, we use the full model from Dozat and
Manning with dependency tag and arc embeddings of size 100 and the same BiLSTM setup
as other tasks. We did not ﬁnd changing the depth
or size of the BiLSTMs to signiﬁcantly impact results .
We optimize cross entropy loss using Adam,
but holding BERT weights frozen and applying a
dropout of 0.5. We train with early stopping on
the development set (patience of 10) using a batch
size of 32 and a learning rate of 0.001.
We did not perform extensive hyperparameter
search, but while optimal hyperparameters are going to be task-dependent, some light experimentation showed these settings work fairly well across
most tasks and BERT variants.
Table 1 summarizes the experimental results. We
observe that SCIBERT outperforms BERT-Base
on scientiﬁc tasks (+2.11 F1 with ﬁnetuning and
+2.43 F1 without)8. We also achieve new SOTA
results on many of these tasks using SCIBERT.
Biomedical Domain
We observe that SCIBERT outperforms BERT-
Base on biomedical tasks (+1.92 F1 with ﬁnetuning and +3.59 F1 without). In addition, SCIB-
ERT achieves new SOTA results on BC5CDR and
ChemProt , and EBM-NLP .
SCIBERT performs slightly worse than SOTA
on 3 datasets. The SOTA model for JNLPBA is a
BiLSTM-CRF ensemble trained on multiple NER
datasets not just JNLPBA . The
SOTA model for NCBI-disease is BIOBERT , which is BERT-Base ﬁnetuned on
18B tokens from biomedical papers. The SOTA
result for GENIA is in
Nguyen and Verspoor
 which uses the model from Dozat and
Manning with part-of-speech (POS) features, which we do not use.
In Table 2, we compare SCIBERT results
with reported BIOBERT results on the subset of
datasets included in . Interesting, SCIBERT outperforms BIOBERT results on
7The SOTA paper did not report a single score.
compute the average of the reported results for each class
weighted by number of examples in each class.
8For rest of this paper, all results reported in this manner
are averaged over datasets excluding UAS for DEP since we
already include LAS.
BC5CDR 
JNLPBA 
NCBI-disease 
EBM-NLP 
GENIA - LAS
GENIA - UAS
ChemProt 
SciERC 
SciERC 
ACL-ARC 
Paper Field
SciCite 
Table 1: Test performances of all BERT variants on all tasks and datasets. Bold indicates the SOTA result (multiple
results bolded if difference within 95% bootstrap conﬁdence interval). Keeping with past work, we report macro
F1 scores for NER (span-level), macro F1 scores for REL and CLS (sentence-level), and macro F1 for PICO
(token-level), and micro F1 for ChemProt speciﬁcally. For DEP, we report labeled (LAS) and unlabeled (UAS)
attachment scores (excluding punctuation) for the same model with hyperparameters tuned for LAS. All results
are the average of multiple runs with different random seeds.
NCBI-disease
Comparing SCIBERT with the reported
BIOBERT results on biomedical datasets.
BC5CDR and ChemProt, and performs similarly
on JNLPBA despite being trained on a substantially smaller biomedical corpus.
Computer Science Domain
We observe that SCIBERT outperforms BERT-
Base on computer science tasks (+3.55 F1 with
ﬁnetuning and +1.13 F1 without).
In addition,
SCIBERT achieves new SOTA results on ACL-
ARC , and the NER part of
SciERC . For relations in Sci-
ERC, our results are not comparable with those in
Luan et al. because we are performing relation classiﬁcation given gold entities, while they
perform joint entity and relation extraction.
Multiple Domains
We observe that SCIBERT outperforms BERT-
Base on the multidomain tasks (+0.49 F1 with
ﬁnetuning and +0.93 F1 without).
In addition,
SCIBERT outperforms the SOTA on SciCite . No prior published SOTA results
exist for the Paper Field dataset.
Discussion
Effect of Finetuning
We observe improved results via BERT ﬁnetuning
rather than task-speciﬁc architectures atop frozen
embeddings (+3.25 F1 with SCIBERT and +3.58
with BERT-Base, on average). For each scientiﬁc
domain, we observe the largest effects of ﬁnetuning on the computer science (+5.59 F1 with SCIB-
ERT and +3.17 F1 with BERT-Base) and biomedical tasks (+2.94 F1 with SCIBERT and +4.61 F1
with BERT-Base), and the smallest effect on multidomain tasks (+0.7 F1 with SCIBERT and +1.14
F1 with BERT-Base).
On every dataset except
BC5CDR and SciCite, BERT-Base with ﬁnetuning
outperforms (or performs similarly to) a model using frozen SCIBERT embeddings.
Effect of SCIVOCAB
We assess the importance of an in-domain scientiﬁc vocabulary by repeating the ﬁnetuning experiments for SCIBERT with BASEVOCAB. We
ﬁnd the optimal hyperparameters for SCIBERT-
BASEVOCAB often coincide with those of SCIB-
ERT-SCIVOCAB.
Averaged across datasets, we observe +0.60 F1
when using SCIVOCAB. For each scientiﬁc do-
main, we observe +0.76 F1 for biomedical tasks,
+0.61 F1 for computer science tasks, and +0.11 F1
for multidomain tasks.
Given the disjoint vocabularies (Section 2) and
the magnitude of improvement over BERT-Base
(Section 4), we suspect that while an in-domain
vocabulary is helpful, SCIBERT beneﬁts most
from the scientiﬁc corpus pretraining.
Related Work
Recent work on domain adaptation of BERT includes BIOBERT and CLINI-
CALBERT .
contrast, SCIBERT is trained on the full text of
1.14M biomedical and computer science papers
from the Semantic Scholar corpus . Furthermore, SCIBERT uses an in-domain
vocabulary (SCIVOCAB) while the other abovementioned models use the original BERT vocabulary (BASEVOCAB).
Conclusion and Future Work
We released SCIBERT, a pretrained language
model for scientiﬁc text based on BERT. We evaluated SCIBERT on a suite of tasks and datasets from
scientiﬁc domains. SCIBERT signiﬁcantly outperformed BERT-Base and achieves new SOTA results on several of these tasks, even compared to
some reported BIOBERT results
on biomedical tasks.
For future work, we will release a version of
SCIBERT analogous to BERT-Large, as well as experiment with different proportions of papers from
each domain. Because these language models are
costly to train, we aim to build a single resource
that’s useful across multiple domains.
Acknowledgment
We thank the anonymous reviewers for their comments and suggestions.
We also thank Waleed
Ammar, Noah Smith, Yoav Goldberg, Daniel
King, Doug Downey, and Dan Weld for their helpful discussions and feedback.
All experiments
were performed on beaker.org and supported
in part by credits from Google Cloud.