Multiscale Conditional Random Fields for Image Labeling
Richard S. Zemel
Miguel ´A. Carreira-Perpi˜n´an
Department of Computer Science, University of Toronto
{hexm,zemel,miguel}@cs.toronto.edu
We propose an approach to include contextual features for
labeling images, in which each pixel is assigned to one of
a ﬁnite set of labels. The features are incorporated into
a probabilistic framework which combines the outputs of
several components. Components differ in the information
they encode. Some focus on the image-label mapping, while
others focus solely on patterns within the label ﬁeld. Components also differ in their scale, as some focus on ﬁneresolution patterns while others on coarser, more global
structure.
A supervised version of the contrastive divergence algorithm is applied to learn these features from labeled image data.
We demonstrate performance on two
real-world image databases and compare it to a classiﬁer
and a Markov random ﬁeld.
1. Introduction
We consider the following problem, that we will call image
labeling: to classify every pixel of a given image into one
of several predeﬁned classes. For example, we might consider images of wildlife in the savanna, and we would like
to classify each pixel as either animal, ground, vegetation,
water or sky. The result is both a segmentation of the image
and a recognition of each segment as a given object class.
Automatically labeled images can also be useful for other
purposes, such as for image database querying (e.g. “ﬁnd
all images with animals in the water”). In this paper, we
propose a model that is learned from labeled images.
Labeling requires contextual information, because the labels are dependent across pixels. Further, an image contains
information that is useful for labeling at several levels. At
a local level (a few pixels wide), the color and texture can
sometimes be enough to identify the pixel class—e.g. the
sky tends to be uniformly blue. However, typically this is
complicated by the large overlap between classes (the water
can also be blue) and the noise in the image. An example
is given in Figure 1: two small image patches are ambiguous at a very local scale but clearly identiﬁable inside their
context. Aspects of this context concern the geometric relationships between objects—e.g. ﬁsh tend to be in water and
airplanes in the sky; while other aspects concern the loca-
Figure 1: Top: two small image patches that are difﬁcult to
label based on local information. Bottom: images containing the patches. The global context makes it clear what the
patches are (left: water; right: sky).
tion of objects in the image—e.g. the sky tends to be at the
top of the image and the water at the bottom. Thus, context
at a level more than a few pixels wide can help disambiguate
very local image information.
We thus have information about the labeling coming
from different scales (local and global). This presents two
problems: First, how can we extract and represent the information at each level? Second, how should we combine the
possibly conﬂicting information from the different levels?
1.1. Previous Approaches
One response to these questions is offered by a common
approach to region classiﬁcation, Markov random ﬁelds
(MRFs). MRFs are typically formulated in a probabilistic generative framework modeling the joint probability of
the image and its corresponding labels . MRFs suffer
from two key limitations with respect to the labeling problem. The ﬁrst drawback concerns their locality. Generally,
due to the complexity of inference and parameter estimation, only local relationships between neighboring nodes
are incorporated into the model. This allows the model to
locally smooth the assigned labels, based on very local regularities, but makes it highly inefﬁcient at capturing longrange interactions. However, as discussed above, the conditional probability of a labeling will likely depend on structure at different levels of granularity in the image. We seek
a model that can capture both local as well as more global
relationships. Hierarchical MRFs offer one way of
capturing label relationships at different scales, but still suffer from the second main drawback of MRFs, which lies in
their generative nature. Many labeled images are required
to estimate the parameters of the model of labels and images. We are interested in estimating the posterior over labels given the observed image; even when this posterior is
simple, the true underlying generative model may be quite
complex. Because we are only interested in the distribution of labels given images, devoting model resources and
degrees-of-freedom to the generative image model is unnecessary.
A very different non-generative approach is to directly
model the conditional probability of labels given images:
fewer labeled images will be required, and the resources
will be directly relevant to the task of inferring labels. This
is the key idea underlying the conditional random ﬁeld
(CRF) . Originally proposed for segmenting and labeling 1-D text sequences, CRFs directly model the posterior
distribution as a Gibbs ﬁeld. This conditional probability
model can depend on arbitrary non-independent characteristics of the observation, unlike a generative image model
which is forced to account for dependencies in the image,
and therefore requires strict independence assumptions to
make inference tractable. CRFs have been shown to outperform traditional hidden Markov model labeling of text
sequences .
In this paper, we aim to generalize the CRF approach
to the image labeling problem, which is considerably more
complicated due to the 2-D nature of images versus the 1-D
nature of text. We also aim to learn features in the random
ﬁeld that operate at different scales of the image. We adopt
a statistical learning approach, where such information is
learned from a training set of labeled images, and combined
in a probabilistic manner.
2. Multiscale Conditional Random Field
Let X = {xi}i∈S be the observed data from an input image
where S is a set of image sites to be labeled. We use the
term sites to refer to elements of the label ﬁeld, while pixels
refer to elements of the image. The local observation xi at
site i is the response of a set of ﬁlters applied to the image
at that site. The site has an associated label li from a ﬁnite
label set L.
Standard CRFs employ two forms of feature functions, which would be deﬁned in a 2D image as follows:
state feature functions, f(li, X, i), of the label at a site i
and the observed image; and transition feature functions
f(li, lj, X, i), of the image and labels at site i and a neighboring site j in the image. We extend this to label features, which encode particular patterns within a subset of
label variables. The label features are a form of potential
function, encoding a particular constraint between the image and the labels within a region of the image. Examples
are shown in Figure 2. Here the smaller (regional) label feature encodes a pattern of ground pixels above water pixels,
while the bigger (global) label feature encodes sky pixels at
the top of the image, rhino/hippo pixels in the middle, and
water pixels near the bottom. The global features can operate at a coarser resolution, specifying common value for a
patch of sites in the label ﬁeld. Our model learns these label
features based on a set of labeled images.
(color, edges,
texture. . . )
Regional feature
Figure 2: Above: An image patch at site i is represented
by the outputs of several ﬁlters. The aim is to associate the
patch with one of a predeﬁned set of labels. Below: Example label features: regional (each cell corresponds to one
site), which matches a boundary with ground (brown) above
water (cyan); and global (each cell corresponds to 10 × 10
sites), which matches a rhino or hippo (red) in the water
(cyan) with sky (blue) above the horizon. “Don’t care” cells
are blank (gray color). For label colors and abbreviations,
see key in Fig. 5.
Associated with each label feature is a binary hidden
variable, that acts as a switch for that feature. The feature
encodes a particular label pattern through a parametrized
conditional probability table (CPT) to the label sites within
This CPT speciﬁes a multinomial probability
distribution over the label values of each site.
The hidden variables are assumed to be conditionally independent
given the corresponding label variables, and vice versa (see
Fig. 3). This structure has the form of a restricted Boltzmann machine (RBM) , in which inference and learning
are greatly simpliﬁed.
Our multiscale conditional random ﬁeld (mCRF) deﬁnes
a conditional distribution over the label ﬁeld L = {li}i∈S
Input Image
Classifier
Label Field
Figure 3: Graphical model representation. The local classiﬁer maps image regions to label variables, while the hidden variables corresponding to regional and global features
form an undirected model with the label variables. Note
that features and labels are fully inter-connected, with no
intra-layer connections (restricted Boltzmann machine).
given input image X by multiplicatively combining component conditional distributions that capture statistical structure at different spatial scales s:
P(L|X) = 1
where Z = 
s Ps(L|X) is a normalization factor
(summed over all labelings). An mCRF is therefore a conditional form of the product-of-experts model .
that the model architecture makes the computation of Z
tractable when conditioned on the image X and hidden variables, as the label ﬁeld distribution can be factored across
the sites given the values of the hidden variables and X.
Each label feature in our model operates at a particular scale in the label ﬁeld. For a given site, there are thus
multiple predictors of its label conditioned on the image.
Our model deﬁned above, as in a standard CRF, combines
the predictions of the various features multiplicatively. The
product form of this combination has two chief effects on
the system. First, label features need not specify the label of
every site within the region. If a feature has uniform values
for each possible label, it will play no role in the combination. We call this a “don’t care” prediction. This enables a
feature to focus its prediction on particular sites in the region. Second, the label of a site may be sharper than any
of the component distributions. If two multinomials favor
a particular value, then their product will be more sharply
peaked on that value. Hence unconﬁdent predictions that
agree can produce a conﬁdent labeling.
In this paper, we instantiate the mCRF framework with
three separate components, operating at three different
scales s: a local classiﬁer, regional features, and global features, as shown in Fig. 3.
1. Local Classiﬁer. One powerful way of classifying
a pixel of an image using information at a local level only
is to use a statistical classiﬁer, such as a neural network.
Independently at each site i, the local classiﬁer produces
a distribution over label variable li given ﬁlter outputs xi
within an image patch centered on pixel i:
PC(L|X, λ) =
PC(li|xi, λ)
where λ are the classiﬁer parameters. We use a multilayer
perceptron as the classiﬁer. Note that the classiﬁer’s performance is limited by class overlap and image noise .
2. Regional Label Features. This second component
is intended to represent local geometric relationships between objects, such as edges, corners or T-junctions. Note
that these are more than edge detectors: they specify the
actual objects involved, thus avoiding impossible combinations such as a ground-above-sky border. We learn a collection of regional features from the training data.
We achieve a degree of translation invariance in the regional features by dividing the label ﬁeld for the whole image into overlapping regions of the same size, on which
these features are deﬁned. The feature for a given region
has its own hidden variables but share the CPT with other
Let r index the regions, a index the different regional
features within each region, and j = {1, . . . , J} index the
label nodes (sites) within region r.
The parameter wa,j
connecting hidden regional variable fr,a and label node lr,j
speciﬁes preferences for the possible label value of lr,j. So
wa,j can be represented as a vector with |L| elements. We
also represent the label variable lr,j as a vector with |L| elements, in which the vth element is 1 and the other is 0 when
lr,j = v. Thus, the probabilistic model describing regional
label features has the following joint distribution:
PR(L, f) ∝exp
r,a fr,awT
{fr,a} represents all the binary hidden
regional variables, wa
[wa,1, . . . , wa,J, αa], lr
[lr,1, . . . , lr,J, 1], and αa is a bias term. Here the sites i are
indexed by (r, j), because site i corresponds to a different
node j in region r based on the position of that region in the
Intuitively, the most probable conﬁguration of each feature is either the label pattern lr in region r matching wa
and fr,a = 1, or the label pattern lr does not match wa
and fr,a = 0. Given the hidden regional variables, the label
variables are conditionally independent and the distribution
of each label node can be written as
PR(li = v|f) =
a,(r,j)=i fr,awa,j,v]
a,(r,j)=i fr,awa,j,v′]
where the site is indexed by i and the summation ranges
over all features deﬁned on regions that contain i. Thus,
the features specify a multinomial distribution over the label
of each site. Finally, the regional component of our model
is formed by marginalizing out the hidden variables in this
sub-model: PR(L) ∝
r,a[1 + exp(wT
Global Label Features.
Each coarse-resolution
global feature has as its domain the label ﬁeld for the whole
image (though in principle we could use smaller ﬁelds anchored at speciﬁc locations, as in Fig. 2). These global features are also conﬁgured as an RBM, with undirected links
between the hidden global variables and the label variables.
Let b index the global label patterns encoded in the parameters {ub} and g = {gb} be the binary hidden global variables. In order to encourage these variables to represent
coarse aspects of the label ﬁeld, we divide the label ﬁeld
into non-overlapping patches pm, m ∈{1, ..., M}, and for
each hidden global variable gb, its connections with the label nodes within patch pm are assigned a single parameter
vector ub,pm. These tied parameters effectively specify the
same distribution for each label node within the patch (and
reduce the number of free parameters). Like the regional
component, the global label feature model has a joint distribution
PG(L, g) ∝exp
The global features also specify a multinomial distribution over each label node by their parameters. Note that a
global feature, as well as a regional feature, can specify that
it effectively “doesn’t care” about the label of a given node
or patch of nodes p, if its parameters ubp(v), v = 1, . . . , |L|
are equal across label values v. This enables a feature to
be sparse, and focus on labels in particular regions, allowing other features to determine the other labels. The joint
model is marginalized to obtain the global feature component: PG(L) ∝
b[1 + exp(uT
4. Combining the Components. The multiplicatively
combined probability distribution over the label ﬁeld has a
simple closed form (see Eqn. 1):
P(L|X; θ) = 1
C (li|xi, λ) ×
[1 + exp(wT
[1 + exp(uT
where θ = {λ, {wa}, {ub}, γ} is the set of parameters in
the model. We include a tradeoff parameter γ because the
classiﬁer is learned before the other components, and the
model needs to modulate the effect of over-conﬁdent incorrect classiﬁer outputs.
Equation 1 shows that the model forms redundant representations of the label ﬁeld. A key attribute of our model,
as in boosting and other expert combination approaches, is
complementarity: each component should learn to focus on
aspects modeled less well by others. Also, the labeling of
an image must maximally satisfy all relevant predictions
(the classiﬁer’s and the features’) at every site. In particular, we expect the global features to help disambiguate (or
even override) the classiﬁer’s judgment.
2.1. Parameter Estimation
For estimating the parameters θ, we assume a set of labeled
images D = {(Lt, Xt), t = 1, ..., N} is available. We train
the conditional model discriminatively based on the Conditional Maximum Likelihood (CML) criterion, which maximizes the log conditional likelihood:
θ∗= arg max
log P(Lt|Xt; θ).
A gradient-based algorithm can be applied to maximize the
conditional log likelihood.
Calling hs the unnormalized
Ps(L|X), we obtain the following learning rule:
where θs are the parameters in component Ps, P0(L|X)
is the data distribution deﬁned by D, and Pθ(L|X) is the
model distribution. However, we need to calculate expectations under the model distribution, which is difﬁcult due to
the normalization factor Z. One possible approach is to approximate the expectations by Markov chain Monte Carlo
(MCMC) sampling, but this requires extensive computation
and the estimated gradients tend to be very noisy.
In this paper, we apply the contrastive divergence (CD)
algorithm . CD is an approximate learning method that
overcomes the difﬁculty of computing expectations under
the model distribution.
The key beneﬁt of applying CD
to learning parameters in a random ﬁeld is that rather than
requiring convergence to equilibrium, such as in MCMC,
one only needs to take a few steps in the Markov chain
to approximate the gradients, which can be a huge savings, particularly during learning when the gradients must
be updated repeatedly. In addition, because our model is
a form of additive random ﬁeld, a block Gibbs sampling
chain can be implemented efﬁciently, simply computing the
conditional probabilities of the feature sets f and g given
L and vice versa. The original CD algorithm optimizes the
parameters of a model by approximately maximizing data
likelihood; we extend it here to the objective of maximizing
conditional likelihood.
2.2. Inference for Labeling an Image
To label a new image X, we need to infer the optimal label
conﬁguration L given X. There are two main criteria for
inferring labels from the posterior distribution : maximum a posteriori (MAP) and maximum posterior marginals
(MPM). Exact MAP is difﬁcult to compute due to the high
dimensionality and discrete domain of L. Also, it can be too
conservative in searching approximate solutions because it
only considers the most probable case and disregards the
difference between other solutions.
The MPM criterion,
which minimizes the expected number of the mislabeled
sites by taking the modes of posterior marginals:
i = arg max
usually produces a better solution. In this paper, we adopt
MPM. Evaluating P(li|X) in our model is intractable due to
its loopy structure, so we must resort to approximate inference methods. We use Gibbs sampling due to its simplicity
and fast convergence. Note that we can take advantage of
our architecture to start sampling the chain in a reasonable
initial point, given by the label distribution output by the
classiﬁer.
3. Experimental Results
3.1. Data Sets
We applied our mCRF to two natural image datasets. The
ﬁrst dataset is a 100-image subset of the Corel image
database, consisting of African and Arctic wildlife natural scenes.
We labeled them manually into 7 classes:
’rhino/hippo’, ’polar bear’, ’vegetation’, ’sky’, ’water’,
’snow’ and ’ground’. The training set includes 60 randomly
selected images and the remaining 40 for testing; each image is 180 × 120 pixels.
The second dataset, the Sowerby Image Database of
British Aerospace, is a set of color images of out-door
scenes and their associated labels.
The images contain
many typical objects near roads in rural and suburban area.
After preprocessing the images as in , we obtain 104 images with 8 labels: ’sky’, ’vegetation’, ’road marking’, ’road
surface’, ’building’, ’street objects’, ’cars’ and ’unlabeled’.
During testing, we do not consider the unlabeled sites and
the model’s output for them. We randomly select 60 images
as training data and use the remaining 44 for testing; each
image is 96 × 64 pixels.
We extract a set of image statistics xi at each image site
i, including color, edge and texture information. In these
experiments, each site corresponds to a single image pixel.
For the color information, we transform the RGB values
into CIE Lab* color space, which is perceptually uniform.
The edge and texture are extracted by a set of ﬁlterbanks
including difference-of-Gaussian ﬁlter at 3 different scales,
and quadrature pairs of oriented even- and odd-symmetric
ﬁlters at 4 orientations (0, π/4, π/2, 3π/4) and 3 scales.
Thus each pixel is represented by 30 image statistics.
3.2. Model Training
We train the system sequentially: ﬁrst we train the local
classiﬁer; then we ﬁx the classiﬁer and train the label features. Although potentially suboptimal with respect to a
joint training of all parameters, the sequential approach is
more efﬁcient. The classiﬁer is a 3-layer multilayer perceptron (MLP) with sigmoid hidden units and |L| outputs with
softmax activation function (so we can interpret the output
as the posterior distribution over labels). For each image
site, the input of the MLP is the image statistics within a local 3 × 3 pixel window centered at that site. Larger window
sizes (e.g. 5 × 5) produced only small improvements in the
classiﬁcation rate but need much longer training. The MLP
is trained to minimize the cross-entropy for multiple classes
with a scaled conjugate gradient algorithm. In the CD algorithm, we always run a Markov chain for 3 steps from the
correct label conﬁguration.
We compare our approach with an MRF, deﬁned as a
generative model P(L, X) = 
i P(xi|li)P(L), where xi
is the image statistics vector at image site i. The classconditional density P(xi|li) is modeled by a Gaussian mixture with diagonal covariance matrices. We learn the Gaussian mixture with the EM algorithm and choose the number
of mixture components using a validation set. The label
ﬁeld P(L) is modeled by a homogeneous random ﬁeld de-
ﬁned on a lattice:
µu,vδ(li −u)δ(lj −v)
where the parameter µu,v measures the compatibility between neighboring nodes (li, lj) when they take the value
(u, v). We trained the random ﬁeld model P(L) using the
pseudo-likelihood algorithm . To infer the optimal labeling given a new image, we use the same MPM criterion
where the marginal distribution is calculated by the loopy
belief propagation algorithm .
3.3. Performance Evaluation
We evaluate the performance of our model by comparing
with the generative MRF and the local classiﬁer over the
Sowerby and Corel datasets. The correct classiﬁcation rates
on the test sets of both datasets are shown in Table 1. For the
Corel dataset, the local classiﬁer is an MLP with 80 hidden
nodes, and the regional features are deﬁned on 8×8 regions
with overlap 4 in each direction, while the global features
are deﬁned on the whole label ﬁeld with patch size 18 × 12.
There are 30 regional features and 15 global features. For
the Sowerby data, the local classiﬁer has 50 hidden units
and the regional features are deﬁned on 6 × 4 regions overlapped by 2 horizontally and 3 vertically. The global features are deﬁned on 8×8 patches of label sites. There are 10
global features and 20 regional features. For both mCRFs,
we set the classiﬁer weighting parameter (γ = 0.9) and the
model structure—number of regional and global features,
and region sizes—using a small validation set.
Table 1: Classiﬁcation rates for the models.
Table 2: Confusion matrix in percentage for Corel data. Entry (row i, column j) means true label i was estimated as j.
From Table 1, we can see that the performance of the
MLP classiﬁer is comparable to the MRF, while our model
provides a signiﬁcant improvement. The result shows the
advantage of discriminative over generative modeling and
the weakness of local interactions captured by the MRF
model. The confusion matrix for the testing results on our
mCRF model is shown in Tables 2–3, where the values
show the percentage of labels in the whole testing data. The
tables show that the errors made by our model are consistent across the classes. For the Sowerby data, the overall
performance is comparable to the best result in published
classiﬁcation result on this dataset: 90.7% in .
We also show the outputs of the local classiﬁer, MRF and
our model on some test images in Figure 5. The classiﬁer
works reasonably well but can be easily fooled since no contextual information is included. The MRF produces quite
smooth label conﬁgurations but it may smooth in a wrong
way because it captures only local context, which can be
misleading. Our mCRF model generates more reasonable
labelings in which the contextual information provided by
regional and global features corrects most of the wrong predictions from the local classiﬁer—even when these occupy
large, scattered portions in the image. We can take the probability of labeling for each site as a conﬁdence measure, and
form a conﬁdence map of the labeling (see Fig. 5, rightmost
column). This conﬁdence measures the quality of the prediction in a consistent way: note how it tends to be low
around boundaries and where the model cannot reverse the
classiﬁer’s wrong labeling due to confusion by highlights
or shadows. The model performance in this case could be
improved by letting the label features have access to image
statistics as well.
Table 3: Confusion matrix in percentage for Sowerby data.
Figure 4 shows a subset of the parameters learned, i.e.,
the conditional probability tables in the regional and global
features. For legibility, only the most probable labels are
shown for each site and each feature pattern is displayed as
a matrix of blocks. The color of each block represents the
label value with the highest probability (cf. the key in Fig. 5)
and the block size is proportional to the probability values. Figure 4 shows 5 regional features from the Sowerby
data and 5 global features from the Corel data. We can see
that the regional features capture within-label regularities as
well as cross-label boundary regularities. For example, the
ﬁrst regional feature is mostly devoted to ’ground’, while
the fourth one represents the boundary between ’vegetation’
and ’sky’. The global features capture coarser patterns in the
entire label ﬁeld and reﬂect the global context in the data.
For instance, the second global feature shows the rhino or
hippo is usually surrounded by vegetation and water, and
the sky is above them, while the fourth one shows the bear
is often surrounded by snow.
4. Discussion
The method proposed here is similar to earlier approaches
to the problem of object detection, or the more general
task of image labeling, in that it combines local classiﬁers
with probabilistic models of label relationships. Insight into
these various models can be gained by comparing the solutions to the basic problem posed in the introduction: how
can information at different scales be represented, learned,
Figure 4: Examples of learned regional label features from
the Sowerby dataset (above, 6 × 4 sites) and global label
features on the Corel dataset (below, 10×10 blocks each of
18 × 12 sites).
rhino/hippo
polar bear
vegetation
Hand-labeling
mCRF conﬁdence
vegetation
road marking
road surface
street object
Figure 5: Some labeling results for the Corel (4 top rows) and Sowerby (3 bottom rows) datasets, using the classiﬁer, MRF
and mCRF models. The color keys for the labels are on the left. The mCRF conﬁdence is low/high in the dark/bright areas.
and combined?
A primary difference between these earlier models and
our model is the form of the representation over labels. One
method of capturing label relationships is through a more
conceptual graphical model, such as an abstraction hierarchy consisting of scenes, objects, and features . The distribution over labels can also be obtained based on pairwise
relationships between labels at different sites. Recently, Kumar and Hebert extended earlier MRF approaches by
including image information in the learned pairwise compatibilities between labels of different sites. Training their
model discriminatively as opposed to generatively led to
signiﬁcant improvements in detecting man-made structures
in images over traditional MRF approaches.
An alternative to a pairwise label model is a tree-based
model . Tree-based models have the potential to represent label relationships at different scales, corresponding
to conditional probability tables at different levels in the
tree. Static tree-based models are limited in their ﬂexibility due to the ﬁxed nature of the tree, which tends to lead to
blocky labelings. The dynamic tree model elegantly
overcomes this approach by constructing the tree on-line
for a given image; however, inference is quite complicated
in this model, necessitating complicated variational techniques. Thus the CPTs learned in this model were restricted
to very simple label relationships.
In our model, a wide variety of patterns of labels, at different scales, are represented by the features, and the features all interact at the label layer. The mCRF model is ﬂatter than the trees, and the features redundantly specify label
predictions. The model is therefore searching for a single
labeling for a given image that maximally satisﬁes the constraints imposed by the active learned features. In the treebased models, alternative hypotheses are represented as different trees, and inference considers distributions over trees.
Our method instead combines the probabilistic predictions
of different features at various scales using a product model,
which naturally takes into account the conﬁdence of each
feature’s prediction.
Our model is an instantiation of a larger framework,
where individual sub-models specialize on tasks and have
access to particular information. Further work can consider,
for example, label features over a range of scales (rather
than just local and global), or label features that have also
access to some image statistics. Generative models cannot include image information as well as label patterns into
learned features. We expect that this will enable the features
to localize boundaries between objects in a more precise
Ideally the system we described would be applied to a
higher level of input image representation, to apply to labeled image features rather than individual pixels. However, this requires a consistent and reliable method for extracting such representations from images.
Finally, automatic image labeling has several direct applications, including video surveying or object detection
and tracking. A primary application is content-based image
retrieval. Many current content-based query methods rely
on global image properties, which do not handle searches
for speciﬁc objects in a variety of scenes . As the quality of image data increases, it becomes more important to
have a mechanism for classifying images as fully as possible prior to insertion into a database. After learning our
model on a small, representative data set, the entire database
can be labeled automatically. Then, user queries such as
“ﬁnd images with hippos in water” can be processed very
quickly. Indexes for the classes associated with each image
could be generated for each image, which would allow rapid
retrieval; alternatively, more speciﬁc regions of images can
be retrieved based on the pixel labels.
5. Conclusions
We have presented a novel probabilistic model for labeling
images into a predeﬁned set of class labels. The model is a
product combination of individual models, each providing
labeling information from different aspects of the image: a
classiﬁer that looks at local image statistics; regional label
features that look at local label patterns; and global label
features that look at large, coarse label patterns. Both the
classiﬁer and the label features are learned from a training
set of labeled images. This strategy results in consensual
labelings that have to agree with the image statistics but at
the same time respect geometric relationships between objects at a local and global scale. The main reasons for our
model’s success are its direct representation of large-scale
interactions and its devoting resources to modelling the label space but not the image space. A chief novelty of the
work is that we generalize the standard form of feature functions used in CRFs to use hidden variables, each encoding
a learned pattern within a subset of label variables.
Acknowledgments
We thank Max Welling and Geoff Hinton for discussions on
contrastive divergence, the anonymous reviewers, and BAE
Systems for letting us use their Sowerby Image Database.
Funded by grants from CIHR New Emerging Teams program and the Institute for Robotics and Intelligent Systems.