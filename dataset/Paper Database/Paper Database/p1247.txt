Synthesized Classiﬁers for Zero-Shot Learning
Soravit Changpinyo∗, Wei-Lun Chao∗
U. of Southern California
Los Angeles, CA
schangpi, 
Boqing Gong
U. of Central Florida
Orlando, FL
 
U. of California
Los Angeles, CA
 
Given semantic descriptions of object classes, zeroshot learning aims to accurately recognize objects of the
unseen classes, from which no examples are available
at the training stage, by associating them to the seen
classes, from which labeled examples are provided. We
propose to tackle this problem from the perspective of
manifold learning. Our main idea is to align the semantic space that is derived from external information to the
model space that concerns itself with recognizing visual
features. To this end, we introduce a set of “phantom”
object classes whose coordinates live in both the semantic space and the model space. Serving as bases in a
dictionary, they can be optimized from labeled data such
that the synthesized real object classiﬁers achieve optimal discriminative performance. We demonstrate superior accuracy of our approach over the state of the art
on four benchmark datasets for zero-shot learning, including the full ImageNet Fall 2011 dataset with more
than 20,000 unseen classes.
1. Introduction
Visual recognition has made signiﬁcant progress due
to the widespread use of deep learning architectures that are optimized on large-scale datasets of humanlabeled images . Despite the exciting advances, to
recognize objects “in the wild” remains a daunting challenge. Many objects follow a long-tailed distribution:
in contrast to common objects such as household items,
they do not occur frequently enough for us to collect and
label a large set of representative exemplar images.
For example, this challenge is especially crippling
for ﬁne-grained object recognition (classifying species
of birds, designer products, etc.). Suppose we want to
carry a visual search of “Chanel Tweed Fantasy Flap
∗Equal contributions
Handbag”. While handbag, ﬂap, tweed, and Chanel are
popular accessory, style, fabric, and brand, respectively,
the combination of them is rare — the query generates
about 55,000 results on Google search with a small number of images. The amount of labeled images is thus far
from enough for directly building a high-quality classi-
ﬁer, unless we treat this category as a composition of
attributes, for each of which more training data can be
easily acquired .
It is thus imperative to develop methods for zero-shot
learning, namely, to expand classiﬁers and the space of
possible labels beyond seen objects, of which we have
access to the labeled images for training, to unseen ones,
of which no labeled images are available . To
this end, we need to address two key interwoven challenges : (1) how to relate unseen classes to seen
ones and (2) how to attain optimal discriminative performance on the unseen classes even though we do not
have their labeled data.
To address the ﬁrst challenge, researchers have been
using visual attributes and word vectors to associate seen and unseen classes. We call
them the semantic embeddings of objects. Much work
takes advantage of such embeddings directly as middle
layers between input images and output class labels , whereas others derive new
representations from the embeddings using, for example, Canonical Correlation Analysis (CCA) 
or sparse coding .
For the second challenge, the hand-designed probabilistic models in 
have been competitive baselines. More recent studies
show that nearest neighbor classiﬁers in the semantic
space are very effective . Finally,
classiﬁers for the unseen classes can directly be constructed in the input feature space .
In this paper, we tackle these two challenges with
ideas from manifold learning , converging to a
two-pronged approach. We view object classes in a se-
 
Semantic space
Model space
Figure 1: Illustration of our method for zero-shot learning. Object classes live in two spaces. They are characterized in the semantic
space with semantic embeddings (as) such as attributes and word vectors of their names. They are also represented as models for
visual recognition (ws) in the model space. In both spaces, those classes form weighted graphs. The main idea behind our approach
is that these two spaces should be aligned. In particular, the coordinates in the model space should be the projection of the graph
vertices from the semantic space to the model space — preserving class relatedness encoded in the graph. We introduce adaptable
phantom classes (b and v) to connect seen and unseen classes — classiﬁers for the phantom classes are bases for synthesizing
classiﬁers for real classes. In particular, the synthesis takes the form of convex combination.
mantic space as a weighted graph where the nodes correspond to object class names and the weights of the
edges represent how they are related. Various information sources can be used to infer the weights — humandeﬁned attributes or word vectors learnt from language
corpora. On the other end, we view models for recognizing visual images of those classes as if they live in a
space of models. In particular, the parameters for each
object model are nothing but coordinates in this model
space whose geometric conﬁguration also reﬂects the relatedness among objects. Fig. 1 illustrates this idea conceptually.
But how do we align the semantic space and the
model space? The semantic space coordinates of objects are designated or derived based on external information (such as textual data) that do not directly examine visual appearances at the lowest level, while the
model space concerns itself largely for recognizing lowlevel visual features. To align them, we view the coordinates in the model space as the projection of the
vertices on the graph from the semantic space — there
is a wealth of literature on manifold learning for computing (low-dimensional) Euclidean space embeddings
from the weighted graph, for example, the well-known
algorithm of Laplacian eigenmaps .
To adapt the embeddings (or the coordinates in the
model space) to data, we introduce a set of phantom object classes — the coordinates of these classes in both
the semantic space and the model space are adjustable
and optimized such that the resulting model for the real
object classes achieve the best performance in discriminative tasks. However, as their names imply, those phantom classes do not correspond to and are not optimized
to recognize any real classes directly. For mathematical convenience, we parameterize the weighted graph in
the semantic space with the phantom classes in such a
way that the model for any real class is a convex combinations of the coordinates of those phantom classes. In
other words, the “models” for the phantom classes can
also be interpreted as bases (classiﬁers) in a dictionary
from which a large number of classiﬁers for real classes
can be synthesized via convex combinations. In particular, when we need to construct a classiﬁer for an unseen
class, we will compute the convex combination coefﬁcients from this class’s semantic space coordinates and
use them to form the corresponding classiﬁer.
To summarize, our main contribution is a novel idea
to cast the challenging problem of recognizing unseen
classes as learning manifold embeddings from graphs
composed of object classes. As a concrete realization
of this idea, we show how to parameterize the graph
with the locations of the phantom classes, and how to
derive embeddings (i.e., recognition models) as convex
combinations of base classiﬁers. Our empirical studies extensively test our synthesized classiﬁers on four
benchmark datasets for zero-shot learning, including the
full ImageNet Fall 2011 release with 20,345 unseen
classes. The experimental results are very encouraging;
the synthesized classiﬁers outperform several state-ofthe-art methods, including attaining better or matching
performance of Google’s ConSE algorithm in the
large-scale setting.
The rest of the paper is organized as follows. We give
an overview of relevant literature in Section 2, describe
our approach in detail in Section 3, demonstrate its effectiveness in Section 4, and conclude in Section 5.
2. Related Work
In order to transfer knowledge between classes, zeroshot learning relies on semantic embeddings of class
labels, including attributes (both manually deﬁned and discriminatively learned ), word
vectors , knowledge mined from the
Web , or a combination of several embeddings .
Given semantic embeddings, existing approaches to
zero-shot learning mostly fall into embedding-based and
similarity-based methods. In the embedding-based approaches, one ﬁrst maps the input image representations
to the semantic space, and then determines the class labels in this space by various relatedness measures implied by the class embeddings . Our work as well as some recent work
combine these two stages , leading to a uniﬁed framework empirically shown to have,
in general, more accurate predictions.
In addition to
directly using ﬁxed semantic embeddings, some work
maps them into a different space through CCA and sparse coding .
In the similarity-based approaches, in contrast, one
builds the classiﬁers for unseen classes by relating them
to seen ones via class-wise similarities . Our approach shares a similar spirit to these
models but offers richer modeling ﬂexibilities thanks to
the introduction of phantom classes.
Finally, our convex combination of base classiﬁers
for synthesizing real classiﬁers can also be motivated
from multi-task learning with shared representations .
While labeled examples of each task are required in ,
our method has no access to data of the unseen classes.
3. Approach
We describe our methods for addressing zero-shot
learning where the task is to classify images from unseen classes into the label space of unseen classes.
Suppose we have training data D
{(xn ∈RD, yn)}N
n=1 with the labels coming from the
label space of seen classes S = {1, 2, · · · , S}. Denote
by U = {S + 1, · · · , S + U} the label space of unseen
We focus on linear classiﬁers in the visual feature
space RD that assign a label ˆy to a data point x by
ˆy = arg max
where wc ∈RD, although our approach can be readily
extended to nonlinear settings by the kernel trick .
3.1. Main idea
Manifold learning
The main idea behind our approach is shown by the conceptual diagram in Fig. 1.
Each class c has a coordinate ac and they live on a manifold in the semantic embedding space. In this paper,
we explore two types of such spaces: attributes 
and class name embeddings via word vectors . We
use attributes in this text to illustrate the idea and in the
experiments we test our approach on both types.
Additionally,
classes associated with semantic embeddings br, r =
1, 2, . . . , R.
We stress that they are phantom as they
themselves do not correspond to any real objects — they
are introduced to increase the modeling ﬂexibility, as
shown below.
The real and phantom classes form a weighted bipartite graph, with the weights deﬁned as
exp{−d(ac, br)}
r=1 exp{−d(ac, br)}
to correlate a real class c and a phantom class r, where
d(ac, br) = (ac −br)T Σ−1(ac −br),
and Σ−1 is a parameter that can be learned from data,
modeling the correlation among attributes. For simplicity, we set Σ = σ2I and tune the scalar free hyperparameter σ by cross-validation. The more general Mahalanobis metric can be used and we propose one way
of learning such metric as well as demonstrate its effectiveness in the Suppl.
The speciﬁc form of deﬁning the weights is motivated
by several manifold learning methods such as SNE .
In particular, scr can be interpreted as the conditional
probability of observing class r in the neighborhood of
class c. However, other forms can be explored and are
left for future work.
In the model space, each real class is associated with
a classiﬁer wc and the phantom class r is associated with
a virtual classiﬁer vr. We align the semantic and the
model spaces by viewing wc (or vr) as the embedding
of the weighted graph. In particular, we appeal to the
idea behind Laplacian eigenmaps , which seeks the
embedding that maintains the graph structure as much
as possible; equally, the distortion error
wc,vr ∥wc −
is minimized. This objective has an analytical solution
∀c ∈T = {1, 2, · · · , S + U} (4)
In other words, the solution gives rise to the idea of synthesizing classiﬁers from those virtual classiﬁers vr. For
conceptual clarity, from now on we refer to vr as base
classiﬁers in a dictionary from which new classiﬁers can
be synthesized. We identify several advantages. First,
we could construct an inﬁnite number of classiﬁers as
long as we know how to compute scr. Second, by making R ≪S, the formulation can signiﬁcantly reduce the
learning cost as we only need to learn R base classiﬁers.
3.2. Learning phantom classes
Learning base classiﬁers
We learn the base classi-
ﬁers {vr}R
r=1 from the training data (of the seen classes
only). We experiment with two settings. To learn oneversus-other classiﬁers, we optimize,
v1,··· ,vR
ℓ(xn, Iyn,c; wc) + λ
∀c ∈T = {1, · · · , S}
where ℓ(x, y; w) = max(0, 1 −ywTx)2 is the squared
hinge loss.
The indicator Iyn,c ∈{−1, 1} denotes
whether or not yn = c.
Alternatively, we apply the
Crammer-Singer multi-class SVM loss , given by
ℓcs(xn, yn; {wc}S
c∈S−{yn} ∆(c, yn) + wc
We have the standard Crammer-Singer loss when the
structured loss ∆(c, yn) = 1 if c ̸= yn, which, however,
ignores the semantic relatedness between classes. We
additionally use the ℓ2 distance for the structured loss
∆(c, yn) = ∥ac −ayn∥2
2 to exploit the class relatedness
in our experiments. These two learning settings have
separate strengths and weaknesses in empirical studies.
Learning semantic embeddings
The weighted graph
eq. (2) is also parameterized by adaptable embeddings
of the phantom classes br. For this work, however, for
simplicity, we assume that each of them is a sparse linear
combination of the seen classes’ attribute vectors:
βrcac, ∀r ∈{1, · · · , R},
Thus, to optimize those embeddings, we solve the following optimization problem
r=1,{βrc}R,S
ℓ(xn, Iyn,c; wc)
∀c ∈T = {1, · · · , S},
where h is a predeﬁned scalar equal to the norm of real
attribute vectors (i.e., 1 in our experiments since we perform ℓ2 normalization). Note that in addition to learning {vr}R
r=1, we learn combination weights {βrc}R,S
Clearly, the constraint together with the third term in
the objective encourages the sparse linear combination
of the seen classes’ attribute vectors. The last term in
the objective demands that the norm of br is not too far
from the norm of ac.
We perform alternating optimization for minimizing the objective function with respect to {vr}R
r,c=1. While this process is nonconvex, there are
useful heuristics to initialize the optimization routine.
For example, if R = S, then the simplest setting is to let
br = ar for r = 1, . . . , R. If R ≤S, we can let them be
(randomly) selected from the seen classes’ attribute vectors {b1, b2, · · · , bR} ⊆{a1, a2, · · · , aS}, or ﬁrst perform clustering on {a1, a2, · · · , aS} and then let each
br be a combination of the seen classes’ attribute vectors in cluster r. If R > S, we could use a combination
of the above two strategies. We describe in more detail
how to optimize and cross-validate hyperparameters in
the Suppl.
3.3. Comparison to several existing methods
We contrast our approach to some existing methods.
 combines pre-trained classiﬁers of seen classes to
construct new classiﬁers. To estimate the semantic embedding (e.g., word vector) of a test image, uses
the decision values of pre-trained classiﬁers of seen objects to weighted average the corresponding semantic
embeddings.
Neither of them has the notion of base
classiﬁers, which we introduce for constructing the classiﬁers and nothing else.
We thus expect them to be
more effective in transferring knowledge between seen
and unseen classes than overloading the pretrained and
ﬁxed classiﬁers of the seen classes for dual duties. We
note that can be considered as a special case of our
method. In , each attribute corresponds to a base and
each “real” classiﬁer corresponding to an actual object
is represented as a linear combination of those bases,
where the weights are the real objects’ “descriptions” in
the form of attributes. This modeling is limiting as the
number of bases is fundamentally limited by the number
of attributes. Moreover, the model is strictly a subset of
our model.1 Recently, propose similar ideas of
aligning the visual and semantic spaces but take different approaches from ours.
4. Experiments
We evaluate our methods and compare to existing
state-of-the-art models on several benchmark datasets.
While there is a large degree of variations in the current
empirical studies in terms of datasets, evaluation protocols, experimental settings, and implementation details,
we strive to provide a comprehensive comparison to as
many methods as possible, not only citing the published
results but also reimplementing some of those methods
to exploit several crucial insights we have discovered in
studying our methods.
We summarize our main results in this section. More
extensive details are reported in the Suppl. We provide
not only comparison in recognition accuracy but also
analysis in an effort to understand the sources of better
performance.
4.1. Setup
We use four benchmark datasets in our experiments: the Animals with Attributes (AwA) ,
CUB-200-2011 Birds (CUB) , SUN Attribute
(SUN) , and the ImageNet (with full 21,841
classes) . Table 1 summarizes their key characteristics. The Suppl. provides more details.
Semantic spaces
For the classes in AwA, we use 85dimensional binary or continuous attributes , as well
as the 100 and 1,000 dimensional word vectors ,
derived from their class names and extracted by Fu
et al. . For CUB and SUN, we use 312 and
102 dimensional continuous-valued attributes, respectively. We also thresh them at the global means to obtain
binary-valued attributes, as suggested in . Neither
datasets have word vectors for their class names. For
ImageNet, we train a skip-gram language model [31,
1For interested readers, if we set the number of attributes as the
number of phantom classes (each br is the one-hot representation of
an attribute), and use Gaussian kernel with anisotropically diagonal
covariance matrix in eq. (3) with properly set bandwidths (either very
small or very large) for each attribute, we will recover the formulation
in when the bandwidths tend to zero or inﬁnity.
Table 1: Key characteristics of studied datasets
# of unseen
14,197,122
†: Following the prescribed split in .
‡: 4 (or 10, respectively) random splits, reporting average.
§: Seen and unseen classes from ImageNet ILSVRC 2012
1K and Fall 2011 release .
32] on the latest Wikipedia dump corpus2 (with more
than 3 billion words) to extract a 500-dimensional word
vector for each class. Details of this training are in the
Suppl. We ignore classes without word vectors in the
experiments, resulting in 20,345 (out of 20,842) unseen
classes. For both the continuous attribute vectors and the
word vector embeddings of the class names, we normalize them to have unit ℓ2 norms unless stated otherwise.
Visual features
Due to variations in features being
used in literature, it is impractical to try all possible
combinations of features and methods. Thus, we make
a major distinction in using shallow features (such as
color histograms, SIFT, PHOG, Fisher vectors) and deep learning features in several recent studies of zero-shot learning. Whenever possible,
we use (shallow) features provided by those datasets or
prior studies. For comparative studies, we also extract
the following deep features: AlexNet for AwA and
CUB and GoogLeNet for all datasets (all extracted
with the Caffe package ). For AlexNet, we use the
4,096-dimensional activations of the penultimate layer
(fc7) as features. For GoogLeNet, we take the 1,024dimensional activations of the pooling units, as in .
Details on how to extract those features are in the Suppl.
Evaluation protocols
For AwA, CUB, and SUN, we
use the (normalized, by class-size) multi-way classiﬁcation accuracy, as in previous work. Note that the accuracy is always computed on images from unseen classes.
Evaluating zero-shot learning on the large-scale ImageNet requires substantially different components from
evaluating on the other three datasets. First, two evaluation metrics are used, as in : Flat hit@K (F@K) and
Hierarchical precision@K (HP@K).
2 
enwiki-latest-pages-articles.xml.bz2 on September
F@K is deﬁned as the percentage of test images for
which the model returns the true label in its top K predictions. Note that, F@1 is the multi-way classiﬁcation
accuracy. HP@K takes into account the hierarchical organization of object categories. For each true label, we
generate a ground-truth list of K closest categories in the
hierarchy and compute the degree of overlapping (i.e.,
precision) between the ground-truth and the model’s top
K predictions. For the detailed description of this metric, please see the Appendix of and the Suppl.
Secondly, following the procedure in , we
evaluate on three scenarios of increasing difﬁculty:
• 2-hop contains 1,509 unseen classes that are within
two tree hops of the seen 1K classes according to
the ImageNet label hierarchy3.
• 3-hop contains 7,678 unseen classes that are within
three tree hops of seen classes.
• All contains all 20,345 unseen classes in the ImageNet 2011 21K dataset that are not in the ILSVRC
2012 1K dataset.
The numbers of unseen classes are slightly different
from what are used in due to the missing semantic embeddings (i.e., word vectors) for certain class
In addition to reporting published results,
have also reimplemented the state-of-the-art method
ConSE on this dataset, introducing a few improvements. Details are in the Suppl.
Implementation details
We cross-validate all hyperparameters — details are in the Suppl. For convenience,
we set the number of phantom classes R to be the same
as the number of seen classes S, and set br = ac for
r = c. We also experiment setting different R and learning br. Our study (cf. Fig. 2) shows that when R is about
60% of S, the performance saturates. We denote the
three variants of our methods in constructing classiﬁers
(Section 3.2) by Ourso-vs-o (one-versus-other), Ourscs
(Crammer-Singer) and Oursstruct (Crammer-Singer with
structured loss).
4.2. Experimental results
Main results
Table 2 compares the proposed methods to the state-ofthe-art from the previously published results on benchmark datasets. While there is a large degree of variations
3 
released.xml
Table 2: Comparison between our results and the previously
published results in multi-way classiﬁcation accuracies (in %)
on the task of zero-shot learning. For each dataset, the best is
in red and the 2nd best is in blue.
ESZSL 
SSE-ReLU ⋆
Ourso-vs-o
Oursstruct
†: Results reported on a particular seen-unseen split.
⋆: Results were just brought to our attention. Note that VGG
 instead of GoogLeNet features were used, improving on
AwA but worsening on CUB. Our results using VGG will
appear in a longer version of this paper.
in implementation details, the main observation is that
our methods attain the best performance in most scenarios. In what follows, we analyze those results in detail.
We also point out that the settings in some existing work are highly different from ours; we do not include their results in the main text for fair comparison
 — but we include them in
the Suppl. In some cases, even with additional data and
attributes, those methods underperform ours.
Large-scale zero-shot learning
One major limitation of most existing work on zero-shot
learning is that the number of unseen classes is often
small, dwarfed by the number of seen classes. However,
real-world computer vision systems need to face a very
large number of unseen objects. To this end, we evaluate
our methods on the large-scale ImageNet dataset.
Table 3 summarizes our results and compares to the
ConSE method , which is, to the best of our knowledge, the state-of-the-art method on this dataset.4 Note
that in some cases, our own implementation (“ConSE
by us” in the table) performs slightly worse than the reported results, possibly attributed to differences in visual
features, word vector embeddings, and other implementation details. Nonetheless, the proposed methods (using
4We are aware of recent work by Lu that introduces a novel
form of semantic embeddings.
Table 3: Comparison between results by ConSE and our method on ImageNet. For both types of metrics, the higher the better.
Flat Hit@K
Hierarchical precision@K
ConSE 
ConSE by us
Ourso-vs-o
Oursstruct
ConSE 
ConSE by us
Ourso-vs-o
Oursstruct
ConSE 
ConSE by us
Ourso-vs-o
Oursstruct
the same setting as “ConSE by us”) always outperform
both, especially in the very challenging scenario of All
where the number of unseen classes is 20,345, signiﬁcantly larger than the number of seen classes. Note also
that, for both types of metrics, when K is larger, the
improvement over the existing approaches is more pronounced. It is also not surprising to notice that as the
number of unseen classes increases from the setting 2hop to All, the performance of both our methods and
ConSE degrade.
Detailed analysis
We experiment extensively to understand the beneﬁts of
many factors in our and other algorithms. While trying
all possible combinations is prohibitively expensive, we
have provided a comprehensive set of results for comparison and drawing conclusions.
Advantage of continuous attributes
It is clear from
Table 4 that, in general, continuous attributes as semantic embeddings for classes attain better performance
than binary attributes. This is especially true when deep
learning features are used to construct classiﬁers. It is
somewhat expected that continuous attributes provide
a more accurate real-valued similarity measure among
classes. This presumably is exploited further by more
powerful classiﬁers.
Advantage of deep features
It is also clear from Table 4 that, across all methods, deep features signiﬁcantly
boost the performance based on shallow features. We
use GoogLeNet and AlexNet (numbers in parentheses)
and GoogLeNet generally outperforms AlexNet. It is
worthwhile to point out that the reported results under
Table 5: Effect of types of semantic embeddings on AwA.
Semantic embeddings
Dimensions
Accuracy (%)
word vectors
word vectors
attributes
attributes + word vectors
attributes + word vectors
deep features columns are obtained using linear classi-
ﬁers, which outperform several nonlinear classiﬁers that
use shallow features. This seems to suggest that deep
features, often thought to be speciﬁcally adapted to seen
training images, still work well when transferred to unseen images .
Which types of semantic space?
In Table 5, we show
how effective our proposed method (Ourso-vs-o) exploits
the two types of semantic spaces: (continuous) attributes
and word-vector embeddings on AwA (the only dataset
with both embedding types).
We ﬁnd that attributes
yield better performance than word-vector embeddings.
However, combining the two gives the best result, suggesting that these two semantic spaces could be complementary and further investigation is ensured.
Table 6 takes a different view on identifying the best
semantic space. We study whether we can learn optimally the semantic embeddings for the phantom classes
that correspond to base classiﬁers. These preliminary
studies seem to suggest that learning attributes could
have a positive effect, though it is difﬁcult to improve
over word-vector embeddings. We plan to study this issue more thoroughly in the future.
How many base classiﬁers are necessary?
In Fig. 2,
we investigate how many base classiﬁers are needed —
Table 4: Detailed analysis of various methods: the effect of feature and attribute types on multi-way classiﬁcation accuracies (in
%). Within each column, the best is in red and the 2nd best is in blue. We cite both previously published results (numbers in bold
italics) and results from our implementations of those competing methods (numbers in normal font) to enhance comparability and
to ease analysis (see texts for details). We use the shallow features provided by , , for AwA, CUB, SUN, respectively.
Shallow features
Deep features
60.5 (50.0)
39.1 (34.8)
57.2 (53.2)
36.7 (32.7)
53.8 (48.8)
40.8 (35.3)
continuous
66.7 (61.9)
50.1 (40.3)†
continuous
66.3 (63.3)
46.5 (42.8)
ESZSL §
continuous
59.6 (53.2)
44.0 (37.2)
continuous
64.5 (59.4)
34.5 (28.0)
ConSE 
continuous
63.3 (56.5)
36.2 (32.6)
COSTA ♯
continuous
61.8 (55.2)
40.8 (36.9)
Ourso-vs-o
continuous
69.7 (64.0)
53.4 (46.6)
continuous
68.4 (64.8)
51.6 (45.7)
Oursstruct
continuous
72.9 (62.8)
54.5 (47.1)
†: Results reported by the authors on a particular seen-unseen split.
‡: Based on Fisher vectors as shallow features, different from those provided in .
§: On the attribute vectors without ℓ2 normalization, while our own implementation shows that normalization helps in some cases.
♯: As co-occurrence statistics are not available, we combine pre-trained classiﬁers with the weights deﬁned in eq. (2).
Table 6: Effect of learning semantic embeddings
Types of embeddings
w/o learning
w/ learning
attributes
100-d word vectors
1000-d word vectors
attributes
attributes
so far, we have set that number to be the number of seen
classes out of convenience. The plot shows that in fact,
a smaller number (about 60% -70%) is enough for our
algorithm to reach the plateau of the performance curve.
Moreover, increasing the number of base classiﬁers does
not seem to have an overwhelming effect. Further details
and analysis are in the Suppl.
5. Conclusion
We have developed a novel classiﬁer synthesis mechanism for zero-shot learning by introducing the notion
of “phantom” classes. The phantom classes connect the
dots between the seen and unseen classes — the classiﬁers of the seen and unseen classes are constructed
from the same base classiﬁers for the phantom classes
and with the same coefﬁcient functions. As a result,
we can conveniently learn the classiﬁer synthesis mechanism leveraging labeled data of the seen classes and
Ratio to the number of seen classes (%)
Relative accuracy (%)
Figure 2: We vary the number of phantom classes R as a
percentage of the number of seen classes S and investigate how
much that will affect classiﬁcation accuracy (the vertical axis
corresponds to the ratio with respect to the accuracy when R =
S). The base classiﬁers are learned with Ourso-vs-o.
then readily apply it to the unseen classes. Our approach
outperforms the state-of-the-art methods on four benchmark datasets in most scenarios.
Acknowledgments
B.G. is partially supported by NSF IIS-1566511.
Others are partially supported by USC Annenberg Graduate Fellowship, NSF IIS-1065243, 1451412, 1513966,
1208500, CCF-1139148, a Google Research Award,
an Alfred.
P. Sloan Research Fellowship and ARO#
W911NF-12-1-0241 and W911NF-15-1-0484.