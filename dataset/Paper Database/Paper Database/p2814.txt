ABCNN: Attention-Based Convolutional Neural Network
for Modeling Sentence Pairs
Wenpeng Yin, Hinrich Sch¨utze
Center for Information and Language Processing
LMU Munich, Germany
 
Bing Xiang, Bowen Zhou
IBM Watson
Yorktown Heights, NY, USA
bingxia, 
How to model a pair of sentences is a critical
issue in many NLP tasks such as answer selection (AS), paraphrase identiﬁcation (PI) and
textual entailment (TE). Most prior work (i)
deals with one individual task by ﬁne-tuning
a speciﬁc system; (ii) models each sentence’s
representation separately, rarely considering
the impact of the other sentence; or (iii) relies fully on manually designed, task-speciﬁc
linguistic features. This work presents a general Attention Based Convolutional Neural
Network (ABCNN) for modeling a pair of
sentences. We make three contributions. (i)
The ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs.
(ii) We propose three attention
schemes that integrate mutual inﬂuence between sentences into CNNs; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more
powerful than isolated sentence representations. (iii) ABCNNs achieve state-of-the-art
performance on AS, PI and TE tasks.
release code at: 
yinwenpeng/Answer_Selection.
Introduction
How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection
(AS) , paraphrase
identiﬁcation (PI) , textual entailment (TE) etc.
s0 how much did Waterboy gross?
1 the movie earned $161.5 million
1 this was Jerry Reed’s ﬁnal ﬁlm appearance
s0 she struck a deal with RH to pen a book today
1 she signed a contract with RH to write a book
1 she denied today that she struck a deal with RH
s0 an ice skating rink placed outdoors is full of people
1 a lot of people are in an ice skating park
1 an ice skating rink placed indoors is full of people
Figure 1: Positive (<s0, s+
1 >) and negative (<s0, s−
examples for AS, PI and TE tasks. RH = Random House
Most prior work derives each sentence’s representation separately, rarely considering the impact of
the other sentence. This neglects the mutual inﬂuence of the two sentences in the context of the task.
It also contradicts what humans do when comparing
two sentences. We usually focus on key parts of one
sentence by extracting parts from the other sentence
that are related by identity, synonymy, antonymy
and other relations. Thus, human beings model the
two sentences together, using the content of one sentence to guide the representation of the other.
Figure 1 demonstrates that each sentence of a pair
partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+
1 contains
a corresponding unit (“earned”) while s−
1 does not.
For PI, focus should be removed from “today” to
correctly recognize < s0, s+
1 > as paraphrases and
1 > as non-paraphrases. For TE, we need
to focus on “full of people” (to recognize TE for
1 >) and on “outdoors” / “indoors” (to recognize non-TE for < s0, s−
1 >). These examples show
the need for an architecture that computes different
representations of si for different s1−i (i ∈{0, 1}).
Transactions of the Association for Computational Linguistics, vol. 4, pp. 259–272, 2016. Action Editor: Brian Roark.
Submission batch: 12/2015; Revision batch: 3/2016; Published 6/2016.
c⃝2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.
Downloaded from by guest on 26 March 2025
Convolutional Neural Networks (CNNs) are widely used to model sentences
 and sentence pairs , especially in classiﬁcation tasks.
are supposed to be good at extracting robust and
abstract features of input. This work presents the
ABCNN, an attention-based convolutional neural
network, that has a powerful mechanism for modeling a sentence pair by taking into account the
interdependence between the two sentences.
ABCNN is a general architecture that can handle a
wide variety of sentence pair modeling tasks.
Some prior work proposes simple mechanisms
that can be interpreted as controlling varying attention; e.g., Yih et al. employ word alignment
to match related parts of the two sentences. In contrast, our attention scheme based on CNNs models
relatedness between two parts fully automatically.
Moreover, attention at multiple levels of granularity,
not only at word level, is achieved as we stack multiple convolution layers that increase abstraction.
Prior work on attention in deep learning (DL)
mostly addresses long short-term memory networks
(LSTMs) . It is not clear
whether this is the best strategy; e.g., in the AS example in Figure 1, it is possible to determine that
“how much” in s0 matches “$161.5 million” in s1
without taking the entire sentence contexts into account.
This observation was also investigated by
Yao et al. where an information retrieval
system retrieves sentences with tokens labeled as
DATE by named entity recognition or as CD by POS
tagging if there is a “when” question. However, labels or POS tags require extra tools. CNNs beneﬁt
from incorporating attention into representations of
local phrases detected by ﬁlters; in contrast, LSTMs
encode the whole context to form attention-based
word representations – a strategy that is more complex than the CNN strategy and (as our experiments
suggest) performs less well for some tasks.
Apart from these differences, it is clear that attention has as much potential for CNNs as it does for
LSTMs. As far as we know, this is the ﬁrst NLP
paper that incorporates attention into CNNs. Our
ABCNNs get state-of-the-art in AS and TE tasks,
and competitive performance in PI, then obtains further improvements over all three tasks when linguistic features are used.
Related Work
Non-DL on Sentence Pair Modeling.
pair modeling has attracted lots of attention in the
past decades. Many tasks can be reduced to a semantic text matching problem. Due to the variety
of word choices and inherent ambiguities in natural language, bag-of-word approaches with simple
surface-form word matching tend to produce brittle results with poor prediction accuracy . As a result, researchers put more emphasis on exploiting syntactic and semantic structure.
Representative examples include methods based on
deeper semantic analysis , tree edit-distance and
quasi-synchronous grammars 
that match the dependency parse trees of the two
sentences. Instead of focusing on the high-level semantic representation, Yih et al. turn their
attention to improving the shallow semantic component, lexical semantics, by performing semantic
matching based on a latent word-alignment structure ). Lai and Hockenmaier
 explore ﬁner-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al.
 extend word-to-word alignment to phraseto-phrase alignment by a semi-Markov CRF. However, such approaches often require more computational resources. In addition, employing syntactic or
semantic parsers – which produce errors on many
sentences – to ﬁnd the best match between the structured representations of two sentences is not trivial.
DL on Sentence Pair Modeling.
To address
some of the challenges of non-DL work, much recent work uses neural networks to model sentence
pairs for AS, PI and TE.
For AS, Yu et al. present a bigram CNN to
model question and answer candidates. Yang et al.
 extend this method and get state-of-the-art
performance on the WikiQA dataset (Section 5.1).
Downloaded from by guest on 26 March 2025
Feng et al. test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan
et al. explore bidirectional LSTMs on the
same dataset. Our approach is different because we
do not model the sentences by two independent neural networks in parallel, but instead as an interdependent sentence pair, using attention.
For PI, Blacoe and Lapata form sentence
representations by summing up word embeddings.
Socher et al. use recursive autoencoders
(RAEs) to model representations of local phrases
in sentences, then pool similarity values of phrases
from the two sentences as features for binary classi-
ﬁcation. Yin and Sch¨utze similarly replace
an RAE with a CNN. In all three papers, the representation of one sentence is not inﬂuenced by the
other – in contrast to our attention-based model.
For TE, Bowman et al. use recursive neural networks to encode entailment on SICK . Rockt¨aschel et al. present an
attention-based LSTM for the Stanford natural language inference corpus . Our
system is the ﬁrst CNN-based work on TE.
Some prior work aims to solve a general sentence matching problem. Hu et al. present
two CNN architectures, ARC-I and ARC-II, for sentence matching. ARC-I focuses on sentence representation learning while ARC-II focuses on matching features on phrase level.
Both systems were
tested on PI, sentence completion (SC) and tweetresponse matching. Yin and Sch¨utze propose the MultiGranCNN architecture to model general sentence matching based on phrase matching on
multiple levels of granularity and get promising results for PI and SC. Wan et al. try to match
two sentences in AS and SC by multiple sentence
representations, each coming from the local representations of two LSTMs.
Our work is the ﬁrst
one to investigate attention for the general sentence
matching task.
Attention-Based DL in Non-NLP Domains.
Even though there is little if any work on attention mechanisms in CNNs for NLP, attention-based
CNNs have been used in computer vision for visual
question answering , image classiﬁcation , caption generation , image segmentation 
and object localization .
description
sentence or sentence length
ﬁlter width
dimensionality of input to layer i + 1
weight matrix
Table 1: Notation
Mnih et al. apply attention in recurrent
neural networks (RNNs) to extract information from
an image or video by adaptively selecting a sequence of regions or locations and only processing
the selected regions at high resolution. Gregor et al.
 combine a spatial attention mechanism with
RNNs for image generation. Ba et al. investigate attention-based RNNs for recognizing multiple objects in images. Chorowski et al. and
Chorowski et al. use attention in RNNs for
speech recognition.
Attention-Based DL in NLP. Attention-based
DL systems have been applied to NLP after their
success in computer vision and speech recognition.
They mainly rely on RNNs and end-to-end encoderdecoders for tasks such as machine translation and text reconstruction . Our
work takes the lead in exploring attention mechanisms in CNNs for NLP tasks.
BCNN: Basic Bi-CNN
We now introduce our basic (non-attention) CNN
that is based on the Siamese architecture , i.e., it consists of two weightsharing CNNs, each processing one of the two sentences, and a ﬁnal layer that solves the sentence pair
task. See Figure 2. We refer to this architecture as
the BCNN. The next section will then introduce the
ABCNN, an attention architecture that extends the
BCNN. Table 1 gives our notational conventions.
In our implementation and also in the mathematical formalization of the model given below, we
pad the two sentences to have the same length s =
max(s0, s1). However, in the ﬁgures we show different lengths because this gives a better intuition of
how the model works.
We now describe the BCNN’s four types of layers: input, convolution, average pooling and output.
Input layer. In the example in the ﬁgure, the two
input sentences have 5 and 7 words, respectively.
Downloaded from by guest on 26 March 2025
Figure 2: BCNN: ABCNN without Attention
Each word is represented as a d0-dimensional precomputed word2vec embedding, d0 = 300. As a result, each sentence is represented as a feature map of dimension d0 × s.
Convolution layer.
Let v1, v2, . . . , vs be the
words of a sentence and ci ∈Rw·d0, 0 < i < s +
w, the concatenated embeddings of vi−w+1, . . . , vi
where embeddings for vj are set to zero when j < 1
We then generate the representation
pi ∈Rd1 for the phrase vi−w+1, . . . , vi using the
convolution weights W ∈Rd1×wd0 as follows:
pi = tanh(W · ci + b)
where b ∈Rd1 is the bias.
Average pooling layer. Pooling (including min,
max, average pooling) is commonly used to extract
robust features from convolution. In this paper, we
introduce attention weighting as an alternative, but
use average pooling as a baseline as follows.
For the output feature map of the last convolution layer, we do column-wise averaging over all
columns, denoted as all-ap. This generates a representation vector for each of the two sentences,
shown as the top “Average pooling (all-ap)” layer
below “Logistic regression” in Figure 2. These two
vectors are the basis for the sentence pair decision.
For the output feature map of non-ﬁnal convolution layers, we do column-wise averaging over windows of w consecutive columns, denoted as w-ap;
shown as the lower “Average pooling (w-ap)” layer
in Figure 2. For ﬁlter width w, a convolution layer
transforms an input feature map of s columns into
a new feature map of s + w −1 columns; average
pooling transforms this back to s columns. This architecture supports stacking an arbitrary number of
convolution-pooling blocks to extract increasingly
abstract features. Input features to the bottom layer
are words, input features to the next layer are short
phrases and so on. Each level generates more abstract features of higher granularity.
The last layer is an output layer, chosen according to the task; e.g., for binary classiﬁcation tasks,
this layer is logistic regression (see Figure 2). Other
types of output layers are introduced below.
We found that in most cases, performance is
boosted if we provide the output of all pooling layers as input to the output layer. For each non-ﬁnal
average pooling layer, we perform w-ap (pooling
over windows of w columns) as described above, but
we also perform all-ap (pooling over all columns)
and forward the result to the output layer.
improves performance because representations from
different layers cover the properties of the sentences
at different levels of abstraction and all of these levels can be important for a particular sentence pair.
ABCNN: Attention-Based BCNN
We now describe three architectures based on the
BCNN, the ABCNN-1, the ABCNN-2 and the
ABCNN-3, that each introduces an attention mechanism for modeling sentence pairs; see Figure 3.
The ABCNN-1 (Figure 3(a)) employs an attention feature matrix A to inﬂuence convolution. Attention features are intended to weight
those units of si more highly in convolution that are
relevant to a unit of s1−i (i ∈{0, 1}); we use the
term “unit” here to refer to words on the lowest level
and to phrases on higher levels of the network. Figure 3(a) shows two unit representation feature maps
in red: this part of the ABCNN-1 is the same as
in the BCNN (see Figure 2). Each column is the
Downloaded from by guest on 26 March 2025
(a) One block in ABCNN-1
(b) One block in ABCNN-2
(c) One block in ABCNN-3
Figure 3: Three ABCNN architectures
Downloaded from by guest on 26 March 2025
representation of a unit, a word on the lowest level
and a phrase on higher levels. We ﬁrst describe the
attention feature matrix A informally (layer “Conv
input”, middle column, in Figure 3(a)). A is generated by matching units of the left representation feature map with units of the right representation feature map such that the attention values of row i in
A denote the attention distribution of the i-th unit of
s0 with respect to s1, and the attention values of column j in A denote the attention distribution of the
j-th unit of s1 with respect to s0. A can be viewed as
a new feature map of s0 (resp. s1) in row (resp. column) direction because each row (resp. column) is a
new feature vector of a unit in s0 (resp. s1). Thus, it
makes sense to combine this new feature map with
the representation feature maps and use both as input to the convolution operation. We achieve this by
transforming A into the two blue matrices in Figure
3(a) that have the same format as the representation
feature maps. As a result, the new input of convolution has two feature maps for each sentence (shown
in red and blue). Our motivation is that the attention feature map will guide the convolution to learn
“counterpart-biased” sentence representations.
More formally, let Fi,r ∈Rd×s be the representation feature map of sentence i (i ∈{0, 1}). Then
we deﬁne the attention matrix A ∈Rs×s as follows:
Ai,j = match-score(F0,r[:, i], F1,r[:, j])
The function match-score can be deﬁned in a variety
of ways. We found that 1/(1 + |x −y|) works well
where | · | is Euclidean distance.
Given attention matrix A, we generate the attention feature map Fi,a for si as follows:
F0,a = W0 · A⊤,
F1,a = W1 · A
The weight matrices W0 ∈Rd×s, W1 ∈Rd×s are
parameters of the model to be learned in training.1
We stack the representation feature map Fi,r and
the attention feature map Fi,a as an order 3 tensor
and feed it into convolution to generate a higherlevel representation feature map for si (i ∈{0, 1}).
In Figure 3(a), s0 has 5 units, s1 has 7. The output
of convolution (shown in the top layer, ﬁlter width
1The weights of the two matrices are shared in our implementation to reduce the number of parameters of the model.
w = 3) is a higher-level representation feature map
with 7 columns for s0 and 9 columns for s1.
ABCNN-2. The ABCNN-1 computes attention
weights directly on the input representation with the
aim of improving the features computed by convolution. The ABCNN-2 (Figure 3(b)) instead computes
attention weights on the output of convolution with
the aim of reweighting this convolution output. In
the example shown in Figure 3(b), the feature maps
output by convolution for s0 and s1 (layer marked
“Convolution” in Figure 3(b)) have 7 and 9 columns,
respectively; each column is the representation of a
unit. The attention matrix A compares all units in s0
with all units of s1. We sum all attention values for a
unit to derive a single attention weight for that unit.
This corresponds to summing all values in a row of
A for s0 (“col-wise sum”, resulting in the column
vector of size 7 shown) and summing all values in a
column for s1 (“row-wise sum”, resulting in the row
vector of size 9 shown).
More formally, let A ∈Rs×s be the attention
matrix, a0,j = P A[j, :] the attention weight of unit
j in s0, a1,j = P A[:, j] the attention weight of
unit j in s1 and Fc
i,r ∈Rd×(si+w−1) the output of
convolution for si. Then the j-th column of the new
feature map Fp
i,r generated by w-ap is derived by:
i,r[:, j]=
i,r[:, k],
j = 1 . . . si
Note that Fp
i,r ∈Rd×si, i.e., ABCNN-2 pooling
generates an output feature map of the same size as
the input feature map of convolution. This allows
us to stack multiple convolution-pooling blocks to
extract features of increasing abstraction.
There are three main differences between the
ABCNN-1 and the ABCNN-2. (i) Attention in the
ABCNN-1 impacts convolution indirectly while attention in the ABCNN-2 inﬂuences pooling through
direct attention weighting. (ii) The ABCNN-1 requires the two matrices Wi to convert the attention
matrix into attention feature maps; and the input to
convolution has two times as many feature maps.
Thus, the ABCNN-1 has more parameters than the
ABCNN-2 and is more vulnerable to overﬁtting.
(iii) As pooling is performed after convolution, pooling handles larger-granularity units than convolution; e.g., if the input to convolution has word level
Downloaded from by guest on 26 March 2025
granularity, then the input to pooling has phrase level
granularity, the phrase size being equal to ﬁlter size
w. Thus, the ABCNN-1 and the ABCNN-2 implement attention mechanisms for linguistic units of
different granularity. The complementarity of the
ABCNN-1 and the ABCNN-2 motivates us to propose the ABCNN-3, a third architecture that combines elements of the two.
ABCNN-3 (Figure 3(c)) combines the ABCNN-1
and the ABCNN-2 by stacking them; it combines the
strengths of the ABCNN-1 and -2 by allowing the
attention mechanism to operate (i) both on the convolution and on the pooling parts of a convolutionpooling block and (ii) both on the input granularity
and on the more abstract output granularity.
Experiments
We test the proposed architectures on three tasks:
answer selection (AS), paraphrase identiﬁcation (PI)
and textual entailment (TE).
Common Training Setup. Words are initialized
by 300-dimensional word2vec embeddings and not
changed during training. A single randomly initialized embedding is created for all unknown words by
uniform sampling from [-.01,.01]. We employ Adagrad and L2 regularization.
Network Conﬁguration.
Each network in the
experiments below consists of (i) an initialization
block b1 that initializes words by word2vec embeddings, (ii) a stack of k −1 convolution-pooling
blocks b2, . . . , bk, computing increasingly abstract
features, and (iii) one ﬁnal LR layer (logistic regression layer) as shown in Figure 2.
The input to the LR layer consists of kn features
– each block provides n similarity scores, e.g., n
cosine similarity scores.
Figure 2 shows the two
sentence vectors output by the ﬁnal block bk of the
stack (“sentence representation 0”, “sentence representation 1”); this is the basis of the last n similarity
scores. As we explained in the ﬁnal paragraph of
Section 3, we perform all-ap pooling for all blocks,
not just for bk. Thus we get one sentence representation each for s0 and s1 for each block b1, . . . , bk. We
compute n similarity scores for each block (based
on the block’s two sentence representations). Thus,
we compute a total of kn similarity scores and these
scores are input to the LR layer.
.08 4 .0004 .08 3 .0002 .08 3 .0006
.085 4 .0006 .085 3 .0003 .085 3 .0006
.05 4 .0003 .085 3 .0001 .09 3 .00065
.06 4 .0006 .085 3 .0001 .085 3 .0007
.05 4 .0003 .05 3 .0003 .09 3 .0007
.06 4 .0006 .055 3 .0005 .09 3 .0007
Table 2: Hyperparameters. lr: learning rate. #CL: number convolution layers. w: ﬁlter width. The number of
convolution kernels di (i > 0) is 50 throughout.
Depending on the task, we use different methods
for computing the similarity score: see below.
Layerwise Training.
In our training regime,
we ﬁrst train a network consisting of just one
convolution-pooling block b2.
We then create a
new network by adding a block b3, initialize its b2
block with the previously learned weights for b2 and
train b3 keeping the previously learned weights for
b2 ﬁxed. We repeat this procedure until all k −1
convolution-pooling blocks are trained. We found
that this training regime gives us good performance
and shortens training times considerably. Since similarity scores of lower blocks are kept unchanged
once they have been learned, this also has the nice
effect that “simple” similarity scores (those based
on surface features) are learned ﬁrst and subsequent
training phases can focus on complementary scores
derived from more complex abstract features.
Classiﬁer. We found that performance increases
if we do not use the output of the LR layer as the
ﬁnal decision, but instead train a linear SVM or a
logistic regression with default parameters2 directly
on the input to the LR layer (i.e., on the kn similarity
scores that are generated by the k-block stack after
network training is completed). Direct training of
SVMs/LR seems to get closer to the global optimum
than gradient descent training of CNNs.
Table 2 shows hyperparameters, tuned on dev.
We use addition and LSTMs as two shared baselines for all three tasks, i.e., for AS, PI and TE. We
now describe these two shared baselines.
(i) Addition.
We sum up word embeddings
element-wise to form each sentence representation.
The classiﬁer input is then the concatenation of the
two sentence representations.
(ii) A-LSTM. Before this work, most attention mechanisms in NLP
2 for both.
Downloaded from by guest on 26 March 2025
were implemented in recurrent neural networks for
text generation tasks such as machine translation
 , Luong et al. ).
Rockt¨aschel et al. present an attention-LSTM
for natural language inference. Since this model is
the pioneering attention based RNN system for sentence pair classiﬁcation, we consider it as a baseline
system (“A-LSTM”) for all our three tasks. The A-
LSTM has the same conﬁguration as our ABCNNs
in terms of word initialization (300-dimensional
word2vec embeddings) and the dimensionality of all
hidden layers (50).
Answer Selection
We use WikiQA,3 an open domain question-answer
dataset. We use the subtask that assumes that there
is at least one correct answer for a question. The
corresponding dataset consists of 20,360 questioncandidate pairs in train, 1,130 pairs in dev and 2,352
pairs in test where we adopt the standard setup of
only considering questions with correct answers in
test. Following Yang et al. , we truncate answers to 40 tokens.
The task is to rank the candidate answers based
on their relatedness to the question. Evaluation measures are mean average precision (MAP) and mean
reciprocal rank (MRR).
Task-Speciﬁc Setup. We use cosine similarity as
the similarity score for AS. In addition, we use sentence lengths, WordCnt (count of the number of nonstopwords in the question that also occur in the answer) and WgtWordCnt (reweight the counts by the
IDF values of the question words). Thus, the ﬁnal
input to the LR layer has size k + 4: one cosine for
each of the k blocks and the four additional features.
We compare with seven baselines. The ﬁrst three
are considered by Yang et al. : (i) WordCnt;
(ii) WgtWordCnt; (iii) CNN-Cnt (the state-of-theart system): combine CNN with (i) and (ii). Apart
from the baselines considered by Yang et al. ,
we compare with two Addition baselines and two
LSTM baselines.
Addition and A-LSTM are the
shared baselines described before. We also combine
both with the four extra features; this gives us two
additional baselines that we refer to as Addition(+)
and A-LSTM(+).
3 
WgtWordCnt 0.5099
Addition(+)
ABCNN-1 one-conv
0.6810∗0.6979∗
0.6855∗0.7023∗
ABCNN-2 one-conv
0.6885∗0.7054∗
0.6879∗0.7068∗
ABCNN-3 one-conv
0.6914∗0.7127∗
0.6921∗0.7108∗
Table 3: Results on WikiQA. Best result per column
is bold. Signiﬁcant improvements over state-of-the-art
baselines (underlined) are marked with ∗(t-test, p < .05).
Results. Table 3 shows performance of the baselines, of the BCNN and of the three ABCNNs. For
CNNs, we test one (one-conv) and two (two-conv)
convolution-pooling blocks.
The non-attention network BCNN already performs better than the baselines. If we add attention
mechanisms, then the performance further improves
by several points. Comparing the ABCNN-2 with
the ABCNN-1, we ﬁnd the ABCNN-2 is slightly
better even though the ABCNN-2 is the simpler architecture. If we combine the ABCNN-1 and the
ABCNN-2 to form the ABCNN-3, we get further
improvement.4
This can be explained by the ABCNN-3’s ability to take attention of ﬁner-grained granularity into
consideration in each convolution-pooling block
while the ABCNN-1 and the ABCNN-2 consider attention only at convolution input or only at pooling
input, respectively. We also ﬁnd that stacking two
convolution-pooling blocks does not bring consistent improvement and therefore do not test deeper
architectures.
Paraphrase Identiﬁcation
We use the Microsoft Research Paraphrase (MSRP)
corpus . The training set contains
2753 true / 1323 false and the test set 1147 true /
578 false paraphrase pairs. We randomly select 400
4If we limit the input to the LR layer to the k similarity
scores in the ABCNN-3 (two-conv), results are .660 (MAP) /
.677 (MRR).
Downloaded from by guest on 26 March 2025
pairs from train and use them as dev; but we still
report results for training on the entire training set.
For each triple (label, s0, s1) in the training set, we
also add (label, s1, s0) to the training set to make
best use of the training data. Systems are evaluated
by accuracy and F1.
Task-Speciﬁc Setup.
In this task, we add the
15 MT features from and
the lengths of the two sentences. In addition, we
compute ROUGE-1, ROUGE-2 and ROUGE-SU4
 , which are scores measuring the match
between the two sentences on (i) unigrams, (ii) bigrams and (iii) unigrams and skip-bigrams (maximum skip distance of four), respectively.
task, we found transforming Euclidean distance into
similarity score by 1/(1 + |x −y|) performs better
than cosine similarity. Additionally, we use dynamic
pooling of the attention
matrix A in Equation (1) and forward pooled values of all blocks to the classiﬁer. This gives us better performance than only forwarding sentence-level
matching features.
We compare our system with representative DL
approaches:
(i) A-LSTM; (ii) A-LSTM(+):
LSTM plus handcrafted features; (iii) RAE , recursive autoencoder; (iv) Bi-CNN-
MI , a bi-CNN architecture; and (v) MPSSM-CNN , the
state-of-the-art NN system for PI, and the following four non-DL systems: (vi) Addition; (vii) Addition(+): Addition plus handcrafted features; (viii)
MT , a system that combines
machine translation metrics;5 (ix) MF-TF-KLD , the state-of-the-art non-NN
Results. Table 4 shows that the BCNN is slightly
worse than the state-of-the-art whereas the ABCNN-
1 roughly matches it.
The ABCNN-2 is slightly
above the state-of-the-art. The ABCNN-3 outperforms the state-of-the-art in accuracy and F1.6 Two
convolution layers only bring small improvements
5For better comparability of approaches in our experiments,
we use a simple SVM classiﬁer, which performs slightly worse
than Madnani et al. ’s more complex meta-classiﬁer.
6Improvement of .3 (acc) and .1 (F1) over state-of-the-art is
not signiﬁcant. The ABCNN-3 (two-conv) without “linguistic”
features (i.e., MT and ROUGE) achieves 75.1/82.7.
majority voting 66.5 79.9
Addition (+)
A-LSTM (+)
ABCNN-1 one-conv
ABCNN-2 one-conv
ABCNN-3 one-conv
Table 4: Results for PI on MSRP
Textual Entailment
SemEval 2014 Task 1 evaluates system predictions of textual entailment (TE)
relations on sentence pairs from the SICK dataset
 . The three classes are entailment, contradiction and neutral. The sizes of SICK
train, dev and test sets are 4439, 495 and 4906 pairs,
respectively. We call this dataset ORIG.
We also create NONOVER, a copy of ORIG in
which words occurring in both sentences are removed. A sentence in NONOVER is denoted by the
special token <empty> if all words are removed.
Table 5 shows three pairs from ORIG and their transformation in NONOVER. We observe that focusing
on the non-overlapping parts provides clearer hints
for TE than ORIG. In this task, we run two copies of
each network, one for ORIG, one for NONOVER;
these two networks have a single common LR layer.
Like Lai and Hockenmaier , we train our
ﬁnal system (after ﬁxing hyperparameters) on train
and dev (4934 pairs). Eval measure is accuracy.
Task-Speciﬁc Setup. We found that for this task
forwarding two similarity scores from each block
(instead of just one) is helpful. We use cosine similarity and Euclidean distance. As we did for paraphrase identiﬁcation, we add the 15 MT features for
each sentence pair for this task as well; our motivation is that entailed sentences resemble paraphrases
more than contradictory sentences do.
Downloaded from by guest on 26 March 2025
children in red shirts are
children red shirts
playing in the leaves
three kids are sitting in the leaves
three kids sitting
1 three boys are jumping in the leaves
three kids are jumping in the leaves
2 a man is jumping into an empty pool an empty
a man is jumping into a full pool
Table 5: SICK data: Converting the original sentences
(ORIG) into the NONOVER format
We use the following linguistic features. Negation is important for detecting contradiction. Feature NEG is set to 1 if either sentence contains “no”,
“not”, “nobody”, “isn’t” and to 0 otherwise. Following Lai and Hockenmaier , we use Word-
Net to detect nyms: synonyms, hypernyms and antonyms in the pairs. But we do this
on NONOVER (not on ORIG) to focus on what
is critical for TE. Speciﬁcally, feature SYN is the
number of word pairs in s0 and s1 that are synonyms. HYP0 (resp. HYP1) is the number of words
in s0 (resp. s1) that have a hypernym in s1 (resp.
s0). In addition, we collect all potential antonym
pairs (PAP) in NONOVER. We identify the matched
chunks that occur in contradictory and neutral, but
not in entailed pairs. We exclude synonyms and hypernyms and apply a frequency ﬁlter of n = 2. In
contrast to Lai and Hockenmaier , we constrain the PAP pairs to cosine similarity above 0.4
in word2vec embedding space as this discards many
noise pairs. Feature ANT is the number of matched
PAP antonyms in a sentence pair. As before we use
sentence lengths, both for ORIG (LEN0O: length
s0, LEN1O: length s1) and for NONOVER (LEN0N:
length s0, LEN1N: length s1).
On the whole, we have 24 extra features: 15
MT metrics, NEG, SYN, HYP0, HYP1, ANT, LEN0O,
LEN1O, LEN0N and LEN1N.
Apart from the Addition and LSTM baselines, we
further compare with the top-3 systems in SemEval
and TrRNTN , a recursive
neural network developed for this SICK task.
Results. Table 6 shows that our CNNs outperform A-LSTM (with or without linguistic features
added) and the top three SemEval systems. Comparing ABCNNs with the BCNN, attention mechanisms
consistently improve performance. The ABCNN-1
has performance comparable to the ABCNN-2 while
 
 
 84.6
 
no features
plus features
no features
plus features
ABCNN-1 one-conv
ABCNN-2 one-conv
ABCNN-3 one-conv
Table 6: Results on SICK. Signiﬁcant improvements over
 are marked with ∗(test of
equal proportions, p < .05).
the ABCNN-3 is better still: a boost of 1.6 points
compared to the previous state of the art.7
Visual Analysis. Figure 4 visualizes the attention
matrices for one TE sentence pair in the ABCNN-
2 for blocks b1 (unigrams), b2 (ﬁrst convolutional
layer) and b3 (second convolutional layer). Darker
shades of blue indicate stronger attention values.
In Figure 4 (top), each word corresponds to exactly one row or column. We can see that words in
si with semantic equivalents in s1−i get high attention while words without semantic equivalents get
low attention, e.g., “walking” and “murals” in s0 and
“front” and “colorful” in s1. This behavior seems
reasonable for the unigram level.
Rows/columns of the attention matrix in Figure 4
(middle) correspond to phrases of length three since
ﬁlter width w = 3. High attention values generally
correlate with close semantic correspondence: the
phrase “people are” in s0 matches “several people
are” in s1; both “are walking outside” and “walking
outside the” in s0 match “are in front” in s1; “the
building that” in s0 matches “a colorful building” in
s1. More interestingly, looking at the bottom right
corner, both “on it” and “it” in s0 match “building”
in s1; this indicates that ABCNNs are able to detect
some coreference across sentences. “building” in
s1 has two places in which higher attentions appear,
one is with “it” in s0, the other is with “the building
7If we run the ABCNN-3 (two-conv) without the 24 linguistic features, performance is 84.6.
Downloaded from by guest on 26 March 2025
several people
colorful building
several people
several people are
people are in
are in front
in front of
front of a
of a colorful
a corlorful building
corlorful building
people are
people are walking
are walking outside
walking outside the
outside the building
the building that
building that has
that has several
has several murals
several murals on
murals on it
several people
several...are
several...in
several...front
people...of
in...colorful
front...building
of...building
a...building
people are
people...walking
people...outside
people...the
are...building
walking...that
outside...has
the...several
building...murals
several...it
murals...it
Figure 4: Attention visualization for TE. Top: unigrams,
b1. Middle: conv1, b2. Bottom: conv2, b3.
that” in s0. This may indicate that ABCNNs recognize that “building” in s1 and “the building that” /
“it” in s0 refer to the same object. Hence, coreference resolution across sentences as well as within a
sentence both are detected. For the attention vectors
on the left and the top, we can see that attention has
focused on the key parts: “people are walking outside the building that” in s0, “several people are in”
and “of a colorful building” in s1.
Rows/columns of the attention matrix in Figure 4
(bottom, second layer of convolution) correspond
to phrases of length 5 since ﬁlter width w = 3 in
both convolution layers (5 = 1 + 2 ∗(3 −1)). We
use “. . .” to denote words in the middle if a phrase
like “several...front” has more than two words. We
can see that attention distribution in the matrix has
focused on some local regions. As granularity of
phrases is larger, it makes sense that the attention
values are smoother.
But we still can ﬁnd some
interesting clues: at the two ends of the main diagonal, higher attentions hint that the ﬁrst part of
s0 matches well with the ﬁrst part of s1; “several
murals on it” in s0 matches well with “of a colorful building” in s1, which satisﬁes the intuition that
these two phrases are crucial for making a decision
on TE in this case. This again shows the potential
strength of our system in ﬁguring out which parts of
the two sentences refer to the same object. In addition, in the central part of the matrix, we can see
that the long phrase “people are walking outside the
building” in s0 matches well with the long phrase
“are in front of a colorful building” in s1.
We presented three mechanisms to integrate attention into CNNs for general sentence pair modeling
Our experiments on AS, PI and TE show that
attention-based CNNs perform better than CNNs
without attention mechanisms. The ABCNN-2 generally outperforms the ABCNN-1 and the ABCNN-
3 surpasses both.
In all tasks, we did not ﬁnd any big improvement
of two layers of convolution over one layer. This is
probably due to the limited size of training data. We
expect that, as larger training sets become available,
deep ABCNNs will show even better performance.
In addition, linguistic features contribute in all
three tasks: improvements by 0.0321 (MAP) and
0.0338 (MRR) for AS, improvements by 3.8 (acc)
and 2.1 (F1) for PI and an improvement by 1.6 (acc)
for TE. But our ABCNNs can still reach or surpass
state-of-the-art even without those features in AS
and TE tasks. This indicates that ABCNNs are generally strong NN systems.
Attention-based LSTMs are especially successful
in tasks with a strong generation component like machine translation (discussed in Sec. 2). CNNs have
not been used for this type of task. This is an interesting area of future work for attention-based CNNs.
Downloaded from by guest on 26 March 2025
Acknowledgments
We gratefully acknowledge the support of Deutsche
Forschungsgemeinschaft
We would like to thank the anonymous reviewers
for their helpful comments.