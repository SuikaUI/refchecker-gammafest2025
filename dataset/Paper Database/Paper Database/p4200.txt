Machine Learning, 39, 287–308, 2000.
c⃝2000 Kluwer Academic Publishers. Printed in The Netherlands.
Convergence Results for Single-Step On-Policy
Reinforcement-Learning Algorithms
SATINDER SINGH
 
AT&T Labs-Research, 180 Park Avenue, Florham Park, NJ 07932, USA
TOMMI JAAKKOLA
 
Department of Computer Science, Massachusetts Institute of Technology, Cambridge, MA 02139, USA
MICHAEL L. LITTMAN
 
Department of Computer Science, Duke University, Durham, NC 27708-0129, USA
CSABA SZEPESV ´ARI
 
Mindmaker Ltd., Konkoly Thege M. u. 29-33, Budapest 1121, Hungary
Editor: Sridhar Mahadevan
An important application of reinforcement learning (RL) is to ﬁnite-state control problems and one
of the most difﬁcult problems in learning for control is balancing the exploration/exploitation tradeoff. Existing
theoretical results for RL give very little guidance on reasonable ways to perform exploration. In this paper, we
examine the convergence of single-step on-policy RL algorithms for control. On-policy algorithms cannot separate
exploration from learning and therefore must confront the exploration problem directly. We prove convergence
results for several related on-policy algorithms with both decaying exploration and persistent exploration. We also
provide examples of exploration strategies that can be followed during learning that result in convergence to both
optimal values and optimal policies.
reinforcement-learning, on-policy, convergence, Markov decision processes
Introduction
Most reinforcement-learning (RL) algorithms 
for solving discrete optimal control problems use evaluation or value functions to cache
the results of experience. This is useful because close approximations to optimal value
functions lead directly to good control policies . Different RL algorithms combine new experience with old value functions to produce
new and statistically improved value functions in different ways. All such algorithms face
a tradeoff between exploitation and exploration , i.e., between choosing actions that are best according to the
current state of knowledge, and actions that are not the current best but improve the state
of knowledge and potentially yield higher payoffs in the future.
Following Sutton and Barto , we distinguish between two types of RL algorithms:
on-policy and off-policy. Off-policy algorithms may update estimated value functions on the
SINGH ET AL.
basis of hypothetical actions, i.e., actions other than those actually executed—in this sense
Q-learning is an off-policy algorithm. On-policy algorithms, on
the other hand, update value functions strictly on the basis of the experience gained from
executing some (possibly non-stationary) policy. This distinction is important because offpolicy algorithms can (at least conceptually) separate exploration from control while onpolicy algorithms cannot. More precisely, in the case of on-policy algorithms, a convergence
proof requires more details of the exploration to be speciﬁed than for off-policy algorithms,
since the update rule depends a great deal on the actions taken by the system.
On-policy algorithms may prove to be important for several reasons. The analogue of the
on-policy/off-policydistinctionforRLpredictionproblemsisthetrajectory-based/trajectoryfree distinction. Trajectory-based algorithms appear superior to trajectory-free algorithms
for prediction when parameterized function approximators are used . These results carry over empirically to the control case as well . In addition, multi-step prediction algorithms such as TD(λ) are more ﬂexible and data efﬁcient than single-step algorithms (TD(0)), and most
natural multi-step algorithms for control are on-policy.
Another motivation for studying on-policy algorithms is the consideration of the interaction between exploration and optimal actions, identiﬁed by Sutton and Barto and
John . Consider a robot learning to maximize reward in a dangerous environment.
Throughout its existence, it will need to execute exploration actions to help it learn about
its options. However, some of these exploration actions will lead to bad outcomes. An onpolicy learner will factor in the costs of exploration, and tend to avoid entering parts of the
state space where exploration is more dangerous.
A suggestive example appears in ﬁgure 1. This 3-state deterministic MDP has two actions:
l and r. For a discount factor of γ = 0.9, the optimal action choice from state y is r (value
75.8 as opposed to 74.9 for action r). On the other hand, if exploratory actions are taken
50% of the time, the risk of picking “dangerous” action r in state z becomes too great. Now,
for a discount factor of γ = 0.9, the optimal action choice from state y is l (value 58.8 as
opposed to 58.6 for action r). In other environments, the difference can be even greater,
necessitating the application of on-policy learning methods.
In this paper, we examine the convergence of single-step (value updates based on the value
of the “next” timestep only), on-policy RL algorithms for control. We do not address either
The optimal action to take in this small, deterministic MDP depends on the exploration strategy. If no
exploration actions are taken, the optimal action from state y is r. If exploration actions are taken, costly action r
will sometimes be chosen from state z, making l the best choice from y.
CONVERGENCE OF ON-POLICY RL ALGORITHMS
function approximation or multi-step algorithms; this is the subject of our ongoing research.
Earlier work has shown that there are off-policy RL algorithms that converge to optimal
value functions ; we prove convergence results
for several related on-policy algorithms. We also provide examples of policies that can be
followed during learning that result in convergence to both optimal values and optimal
policies. These results generalize naturally to off-policy algorithms, such as Q-learning,
showing the convergence of many RL algorithms to optimal policies.
Solving Markov decision problems
Markov decision processes (MDPs) are widely used to model controlled dynamical systems
in control theory, operations research and artiﬁcial intelligence . Let S = 1, 2, . . . , N denote the discrete
set of states of the system, and let A be the discrete set of actions available to the system.
The probability of making a transition from state s to state s′ on action a is denoted Pa
and the random payoff associated with that transition is denoted r(s, a). A policy maps
each state to a probability distribution over actions—this mapping can be invariant over
time (stationary) or change as a function of the interaction history (non-stationary). For any
policy π, we deﬁne a value function
V π(s) = Eπ
¯¯¯¯¯s0 = s
which is the expected value of the inﬁnite-horizon sum of the discounted payoffs when the
system is started in state s and the policy π is followed forever. Note that rt and st are
the payoff and state respectively at timestep t, and (rt, st) is a stochastic process, where
(rt, st+1) depends only on (st, at) governed by the rules that rt is distributed as r(st, at) and
the probability that st+1 = s is Pat
sts. Here, at is the action taken by the system at timestep
t. The discount factor, 0 ≤γ < 1, makes payoffs in the future less valuable than more
immediate payoffs.
The solution of an MDP is an optimal policy π∗that simultaneously maximizes the value
of every state s ∈S. It is known that a stationary deterministic optimal policy exists for every
MDP ). Hereafter, unless explicitly noted, all policies are assumed to
be stationary. The value function associated with π∗is denoted V ∗. Often it is convenient
to associate values not with states but with state-action pairs, called Q values as in Watkins’
Q-learning :
Qπ(s, a) = R(s, a) + γ E{V π(s′)},
Q∗(s, a) = R(s, a) + γ E{V ∗(s′)},
SINGH ET AL.
where s′ is the random next state on executing action a in state s, and R(s, a) is expected
value of r(s, a). Clearly, π∗(s) = argmaxa Q∗(s, a), and V ∗(s) = maxa Q∗(s, a). The
optimal Q values satisfy the recursive Bellman optimality equations ,
Q∗(s, a) = R(s, a) + γ
Q∗(s′, b), ∀s, a.
In reinforcement learning, the quantities that deﬁne the MDP, P and R, are not known in
advance. An RL algorithm must ﬁnd an optimal policy by interacting with the MDP directly;
because effective learning typically requires the algorithm to revisit every state many times,
we assume the MDP is “communicating” (every state can be reached from every other state).
Off-policy and on-policy algorithms
Most RL algorithms for solving MDPs are iterative, producing a sequence of estimates of
either the optimal (Q-)value function or the optimal policy or both by repeatedly combining
old estimates with the results of a new trial to produce new estimates.
An RL algorithm can be decomposed into two components. The learning policy is a nonstationary policy that maps experience (states visited, actions chosen, rewards received)
into a current choice of action. The update rule is how the algorithm uses experience to
change its estimate of the optimal value function.
In an off-policy algorithm, the update rule need not have any relation to the learning
policy. Q-learning is an off-policy algorithm that estimates the optimal
Q-value function as follows:
Qt+1(st, at) = (1 −αt(st, at))Qt(st, at)
+ αt(st, at)
rt + γ max
b (Qt(st+1, b))
where Qt is the estimate at the beginning of the tth timestep, and st, at, rt, and αt are
the state, action, reward, and step size (learning rate) at timestep t. Equation (2) is an offpolicy algorithm as the update of Qt(st, at) depends on maxb(Qt(st+1, b)), which relies on
comparing various “hypothetical” actions b. The convergence of the Q-learning algorithm
does not put any strong requirements on the learning policy other than that every action is
experienced in every state inﬁnitely often. This can be accomplished, for example, using
the random-walk learning policy, which chooses actions uniformly at random. Later, we
describe several other learning policies that result in convergence when combined with the
Q-learning update rule.
The update rule for SARSA(0) :
Qt+1(st, at) = (1 −αt(st, at))Qt(st, at)
+ αt(st, at)[rt + γ Qt(st+1, at+1)],
CONVERGENCE OF ON-POLICY RL ALGORITHMS
a special case of SARSA(λ) with λ = 0, is quite similar to the update rule for Q-learning.
The main difference is that Q-learning makes an update based on the greedy Q value
of the successor state, st+1, while SARSA(0)1 uses the Q value of the action at+1 actually
chosen by the learning policy. This makes SARSA(0) an on-policy algorithm, and therefore its
conditions for convergence depend a great deal on the learning policy. In particular, because
SARSA(0) learns the value of its own actions, the Q values can converge to optimality in the
limit only if the learning policy chooses actions optimally in the limit. Section 3 provides
some positive convergence results for two signiﬁcant classes of learning policies.
Under a greedy learning policy (i.e., always select the action that is best according
to the current estimate), the update rules for Q-learning and SARSA(0) are identical. The
resulting RL algorithm would not converge to optimal solutions, in general, because the
need for inﬁnite exploration would not be satisﬁed. This helps illustrate the tension between
adequate exploration and exploitation with regard to convergence to optimality.
It is worth noting, however, that the approach of using a greedy learning policy has
yielded some impressive successes, including the world’s ﬁnest backgammon-playing program , and state-of-the-art systems for space shuttle scheduling , elevator control , and cellular telephone resource
allocation . All these applications can be viewed as exploiting
on-policy algorithms, although the on-policy versus off-policy distinction is not meaningful
when no explicit exploration is used.
Learning policies
A learning policy selects an action at timestep t as a function of the history of states, actions,
and rewards experienced so far. In this paper, we consider several learning policies that make
decisions based on a summary of history consisting of the current timestep t, the current
state s, the current estimate Q of the optimal Q-value function, and the number of times
state s has been visited before time t, nt(s). Such a learning policy can be expressed as the
probabilities Pr(a|s, t, Q, nt(s)), the probability that action a is selected given the history.
We divide learning policies for MDPs into two broad categories: a decaying exploration
learning policy that becomes more and more like the greedy learning policy over time, and a
persistent exploration learning policy that does not. The advantage of decaying exploration
policies is that the actions taken by the system may converge to the optimal ones eventually,
but with the price that their ability to adapt slows down. In contrast to this, persistent
exploration learning policies can retain their adaptivity forever, but with the price that the
actions of the system will not converge to optimality in the standard sense. We prove the
convergence of SARSA(0) to optimal policies in the standard sense for a class of decaying
exploration learning policies, and to optimal policies in a special sense deﬁned below for a
class of persistent exploration learning policies.
Consider the class of decaying exploration learning policies characterized by the following two properties:
1. each action is executed inﬁnitely often in every state that is visited inﬁnitely often, and
2. in the limit, the learning policy is greedy with respect to the Q-value function with
probability 1.
SINGH ET AL.
We label learning policies satisfying the above conditions as GLIE, which stands for “greedy
in the limit with inﬁnite exploration.” An example of such a learning policy is a form of
Boltzmann exploration:
Pr(a|s, t, Q) =
eβt(s)Q(s,a)
b∈A eβt(s)Q(s,b) ,
where βt(s) is the state-speciﬁc exploration coefﬁcient for time t, which controls the rate of
exploration in the learning policy. To meet Condition 2 above, we would like βt to be inﬁnite
in the limit, while to meet Condition 1 above we would like βt to not approach inﬁnity too
fast. In Appendix B, we show that βt(s) = ln nt(s)/Ct(s) satisﬁes the above requirements
(where nt(s) ≤t is the number of times state s has been visited in t timesteps, and Ct(s) is
deﬁned in Appendix B). Another example of a GLIE learning policy is a form of ϵ-greedy
exploration , which at timestep t in state s picks a random exploration action
with probability ϵt(s) and the greedy action with probability 1 −ϵt(s). In Appendix B, we
show that if ϵt(s) = c/nt(s) for 0 < c < 1, then ϵ-greedy exploration is GLIE.
We also analyze “restricted rank-based randomized” (RRR) learning policies, a class of
persistent exploration learning policies commonly used in practice. An RRR learning policy
selects actions probabilistically according to the ranks of their Q values, choosing the greedy
action with the highest probability and the action with the lowest Q value with the lowest
probability. Different learning policies can be speciﬁed by different choices of the function
T : {1, . . . , m} →ℜthat maps action ranks to probabilities. Here, m is the number of
actions. For consistency, we require that T (1) ≥T (2) ≥· · · ≥T (m) and Pm
i=1 T (i) = 1.
Attimestept,theRRRlearningpolicychoosesanactionbyﬁrstrankingtheavailableactions
according to the Q values assigned by the current Q-value function Qt for the current state
st. We use the notation ρ(Q, s, a) to be the rank of action a in state s based on Q(s, ·) (e.g.,
if ρ(Q, s, a) = 1 then a = argmaxb Q(s, b)), with ties broken arbitrarily. Once the actions
are ranked, the ith ranked action is chosen with probability T (i); that is, action a is chosen
with probability T (ρ(Q, s, a)). The RRR learning policy is “restricted” in that it does not
directly choose actions—it simply assigns probabilities to actions according to their ranks.
Therefore, an RRR learning policy has the form Pr(a|s, t, Q) = T (ρ(Qt, s, a)).
To illustrate the use of the T function, we specify three well-known learning policies as
RRR learning policies by the appropriate deﬁnition of T . The random-walk learning policy
chooses action a in state s with probability 1/m. To achieve this behavior with the RRR
learning policy, simply deﬁne T (i) = 1/m for all i; actions will be chosen uniformly at
random regardless of their rank. The greedy learning policy can be speciﬁed by T (1) = 1,
T (i) = 0 when 1 < i ≤m; it deterministically selects the action with the highest Q
value. Similarly, ϵ-greedy exploration can be speciﬁed by deﬁning T (1) = 1 −ϵ + ϵ/m,
T (i) = ϵ/m, 1 < i ≤m. This policy takes the greedy action with probability 1 −ϵ and
a random action otherwise. To satisfy the condition that T (1) ≥T (2) ≥· · · ≥T (m), we
require that 0 ≤ϵ ≤1.
Another commonly used persistent exploration learning policy is Boltzmann exploration
with a ﬁxed exploration parameter. Note there is no choice of T that speciﬁes Boltzmann exploration; Boltzmann exploration is not an RRR learning policy as the probability of choosing an action depends on the actual Q values and not only on the ranks of actions in Q(·).
CONVERGENCE OF ON-POLICY RL ALGORITHMS
Below we prove results on the convergence of SARSA(0) under the two separate cases of
GLIE and RRR learning policies.
Convergence of SARSA(0) under GLIE learning policies
To ensure the convergence of SARSA(0), we require a lookup-table representation for the Q
values and inﬁnite visits to every state-action pair, just as for Q-learning. Unlike Q-learning,
however, SARSA(0) is an on-policy algorithm and, in order to achieve its convergence to
optimality, we have to further assume that the learning policy becomes greedy in the limit.
To state these assumptions and the resulting convergence more formally, we note ﬁrst
that due to the dependence on the learning policy, SARSA(0) does not directly fall under
the previously published convergence theorems . Only a slight extension is needed,
however, and this is presented in the form of Lemma 1 below ). For clarity, we will
not present the lemma in full generality.
Consider a stochastic process (αt, 1t, Ft), t ≥0, where αt, 1t, Ft : X →ℜ
satisfy the equations
1t+1(x) = (1 −αt(x))1t(x) + αt(x)Ft(x),
x ∈X, t = 0, 1, 2, . . . .
Let Pt be a sequence of increasing σ-ﬁelds such that α0 and 10 are P0-measurable and
αt, 1t and Ft−1 are Pt-measurable, t = 1, 2, . . . . Assume that the following hold:
1. the set X is ﬁnite.
2. 0 ≤αt(x) ≤1, P
t αt(x) = ∞, P
t (x) < ∞w.p.1.
3. ∥E{Ft(·)|Pt}∥W ≤κ∥1t∥W + ct, where κ ∈[0, 1) and ct converges to zero w.p.1.2
4. Var{Ft(x)|Pt} ≤K(1 + ∥1t∥W)2, where K is some constant.
Then, 1t converges to zero with probability one (w.p.1).
Let us ﬁrst clarify how this lemma relates to the learning algorithms that are the focus
of this paper. We can capture the sequence of visited states st and selected actions at in the
deﬁnition of the learning rates αt as follows: deﬁne xt = (st, at) and further require that
αt(x) = 0 whenever x ̸= xt. With these deﬁnitions, the iterative process reduces to
1t+1(st, at) = (1 −αt(st, at))1t(st, at) + αt(st, at)Ft(st, at),
which resembles more closely the updates of the on-line algorithms such as SARSA(0)
(Eq. (3)). Also, note that the lemma shows the convergence of 1 to zero rather than to some
non-zero optimal values. The intended meaning of 1 is Qt −Q∗, i.e., the difference between
the current Q values, Qt, and the target Q values, Q∗, that are attained asymptotically.
SINGH ET AL.
The extension provided by our formulation of the lemma is the fact that the contraction
property (the third condition) need not be strict; strict contraction is now required to hold
only asymptotically. This relaxation makes the theorem more widely applicable.
While we have stated that the lemma extends previous results such as the Theorem 1 of Jaakkola et al. and Lemma 12 of Szepesv´ari & Littman , the proof
of our lemma is, however, already almost fully contained in the proofs of these results
(requiring only minor, largely notational changes). Moreover, the lemma also follows from
Proposition 4.5 of Bertsekas , and in Appendix A we present a proof based on this
proposition.
We can now use Lemma 1 to show the convergence of SARSA(0).
Theorem 1.
Consider a ﬁnite state-action MDP and ﬁx a GLIE learning policy π given as
a set of probabilities Pr(a | s, t, nt(s), Q). Assume that at is chosen according to π and at
time step t, π uses Q = Qt, where the Qt values are computed by the SARSA(0) rule (see
Eq. (3)). Then Qt converges to Q∗and the learning policy πt converges to an optimal policy
π∗provided that the conditions on the immediate rewards, state transitions and learning
rates listed in Section 2 hold and if the following additional conditions are satisﬁed:
1. The Q values are stored in a lookup table.
2. The learning rates satisfy 0 ≤αt(s, a) ≤1, P
t αt(s, a) = ∞and P
t (s, a) < ∞
and αt(s, a) = 0 unless (s, a) = (st, at).
3. Var{r(s, a)} < ∞.
The correspondence to Lemma 1 follows from associating X with the set of stateaction pairs (s, a), αt(x) with αt(s, a) and 1t(s, a) with Qt(s, a) −Q∗(s, a). It follows
1t+1(st, at) = (1 −αt(st, at))1t(st, at) + αt(st, at)Ft(st, at),
Ft(st, at) = rt + γ max
b∈A Qt(st+1, b) −Q∗(st, at)
Qt(st+1, at+1) −max
b∈A Qt(st+1, b)
= rt + γ max
b∈A Qt(st+1, b) −Q∗(st, at) + Ct(Q)
t (st, at) + Ct(st, at),
would be the corresponding Ft in Lemma 1 if the algorithm under consideration
were Q-learning. We deﬁne Ft(s, a) = F Q
t (s, a) = Ct(s, a) = 0 if (s, a) ̸= (st, at)
(so Ft(s, a) = F Q
t (s, a) + Ct(s, a) for all (s, a)) and denote the σ-ﬁeld generated by the
CONVERGENCE OF ON-POLICY RL ALGORITHMS
random variables {st, αt, at,rt−1, . . . , s1, α1, a1, Q0} by Pt. Note that Qt, Qt−1, . . . , Q0 are
Pt-measurable and, thus, both 1t and Ft−1 are Pt-measurable, satisfying the measurability
conditions of Lemma 1.
It is well-known that for Q-learning ∥E{F Q
t (·, ·) | Pt}∥≤γ ∥1t∥for all t, where ∥· ∥is
the maximum norm. In other words, the expected update operator is a contraction mapping.
The only difference between the current Ft and F Q
for Q-learning is the presence of Ct.
Therefore,
∥E{Ft(·, ·)|Pt}∥≤
t (·, ·) | Pt
ª°° + ∥E{Ct(·, ·) | Pt}∥
≤γ ∥1t∥+ ∥E{Ct(·, ·) | Pt}∥.
Identifying ct = ∥E{Ct(·, ·) | Pt}∥in Lemma 1, we are left with showing that ct converges
tozerow.p.1.This,however,follows(a)fromourassumptionofaGLIEpolicy(i.e.,thatnongreedy actions are chosen with vanishing probabilities), (b) the assumption of ﬁniteness
of the MDP, and (c) the fact that Qt(s, a) stays bounded during learning. To verify the
boundedness property, we note that the SARSA(0) Q values can be upper bounded by the
Q values of a Q-learning process that updates exactly the same state-action pairs in the
same order as the SARSA(0) process. Similarly, the SARSA(0) Q values are lower bounded
by the Q values of a Q-learning process that uses a min instead of a max in the update
rule (c.f. Eq. (2)) and updates exactly the same state-action pairs in the same order as the
SARSA(0) process. Both the lower-bounding and the upper-bounding Q-learning processes
are convergent and have bounded Q values.
The condition on the variance of Ft follows from the similar property of F Q
Note that if a GLIE learning policy is used with the Q-learning update rule, one gets
convergence to both the optimal Q-value function and an optimal policy. This begins to
address a signiﬁcant outstanding question in the theory of reinforcement learning: How do
you a learn a policy that achieves high reward in the limit and during learning? Previous
convergence results for Q-learning guarantee that the optimal Q-value function is reached
in the limit; this is important because the longer the learning process goes on, the closer to
optimal the greedy policy with respect to the learned Q-value function will be. However, this
provides no useful guidance for selecting actions during learning. Our results, in contrast,
show that it is possible to follow a policy during learning that approaches optimality over
The properties of GLIE policies imply that for any RL algorithm that converges to the
optimal value function and whose estimates stay bounded ), using GLIE learning policies will ensure a concurrent convergence to
an optimal policy. However, to get an implementable RL algorithm, one still has to specify a
suitable learning policy that guarantees that every action is attempted in every state inﬁnitely
often (i.e., P
t αt(s, a) = ∞). In Appendix B, we prove that, if the probability of choosing
any particular action in any given state sums up to inﬁnity, then the above condition is
indeed satisﬁed. To illustrate this, in Appendix B we derive two learning strategies that are
SINGH ET AL.
Convergence of SARSA(0) under RRR learning policies
This section proves two separate results concerning a class of persistent exploration learning
policies: (1) the SARSA(0) update rule combined with an RRR learning policy converges
to a well-deﬁned Q-value function and policy, and (2) the resulting policy is optimal, in a
sense we will deﬁne.
As mentioned earlier, an RRR learning policy chooses actions probabilistically by their
ranking according to the current Q-value function; a speciﬁc learning policy is speciﬁed
by the function T , a probability distribution over action ranks. A restricted policy ¯π :
S →5(A, {1, . . . , m}) ranks actions in each state (recall that m denotes the number of
actions), i.e., ¯π(s) is a bijection between A and {1, . . . , m}. For convenience, we use the
notation ¯π(s, a) to denote the assigned rank of action a in state s, i.e., to denote ¯π(s)(a).
The mapping ¯π represents a policy in the sense that an agent following restricted policy ¯π
from state s chooses action a with probability T ( ¯π(s, a)), the probability assigned to the
rank, ¯π(s, a), of action a in state s.
Consider what happens when the SARSA(0) update rule is used to learn the value of a
ﬁxed restricted policy ¯π. Standard convergence results for Q-learning can easily be used to
show that the Qt values will converge to the Q-value function of ¯π. Speciﬁcally, Qt will
converge to Q ¯π, deﬁned as the unique solution to
Q ¯π(s, a) = R(s, a) + γ
T ( ¯π(s′, a′))Q ¯π(s′, a′), (s, a) ∈S × A.
When an RRR learning policy is followed, the situation becomes a bit more complex.
Upon entering state s, the probability that the learning policy will choose, for example, the
rank 1 action is ﬁxed at T (1); however, the identity of that action changes according to
the current Q-value function estimate Qt(·, ·). The natural extension of Eq. (6) to an RRR
learning policy would be for the target of convergence of Qt in SARSA(0) to be
¯Q(s, a) = R(s, a) + γ
T (ρ( ¯Q, s′, a′)) ¯Q(s′, a′), (s, a) ∈S × A.
Recall that ρ( ¯Q, s′, a′) represents the rank of action a′ according to the Q values ¯Q of state
s′. The only change between Eqs. (6) and (7) is that the latter uses an assignment of ranks
that is based upon the recursively deﬁned Q-value function ¯Q, whereas the former uses a
ﬁxed assignment of ranks. Using the theory of generalized MDPs , we can show that this difference is not important from the perspective of proving the
existence and uniqueness of the solution to Eq. (7).
T (ρ(Q, s, a))Q(s, a);
now Eq. (7) can be rewritten
¯Q(s, a) = R(s, a) + γ
¯Q(s′, a′), (s, a) ∈S × A.
CONVERGENCE OF ON-POLICY RL ALGORITHMS
As long as N satisﬁes the non-expansion property that
|Q(s, a) −Q′(s, a)|
for all Q-value functions Q and Q′ and all states s, then Eq. (9) has a solution and it is
unique ; this is proven in Appendix C. The non-expansion
property of N can be veriﬁed by the following argument.
• Consider a family of operators N
i Q(s, a) = ith largest value of Q(s, a) for each
1 ≤i ≤m. These are all non-expansions (see Appendix C).
• Deﬁne N′
a Q(s, a) = P
i Q(s, a); it is a non-expansion as long as every N
is and T is a ﬁxed probability distribution (see Appendix C).
• It is clear that N′
a Q(s, a) = N
a Q(s, a) as deﬁned in Eq. (8), so N is a non-expansion
Therefore, ¯Q exists and is unique. We next show that ¯Q is, in fact, the target of convergence
for SARSA(0).
Theorem 2.
In ﬁnite state-action MDPs, the Qt values computed by the SARSA(0) rule (see
Eq. (3)) convergeto ¯Q ifthelearningpolicyisRRR, theconditionsontheimmediaterewards
and state transitions listed in Section 2 hold, and if the following additional conditions are
1. Pr(at+1 = a | Qt, st+1) = T (ρ(Qt, st+1, at+1)).
2. The Q values are stored in a lookup table.
3. The learning rates satisfy 0 ≤αt(s, a) ≤1, P
t αt(s, a) = ∞, P
t (s, a) < ∞, and
αt(s, a) = 0 unless (s, a) = (st, at).
4. Var{r(s, a)} < ∞.
The result readily follows from Lemma 1 )
and the proof follows nearly identical lines as that of Theorem 1. First, we associate X (of
Lemma 1) with the set of state-action pairs (s, a) and αt(x) with αt(s, a), but here we set
1t(s, a) = Qt(s, a) −¯Q(s, a). Again, it follows that
1t+1(st, at) = (1 −αt(st, at))1t(st, at) + αt(st, at)Ft(st, at),
Ft(st, at) = rt + γ Qt(st+1, at+1) −¯Q(st, at).
Further, we deﬁne Ft(s, a) = Ct(s, a) = 0 if (s, a) ̸= (st, at) and denote the σ-ﬁeld
generated by the random variables {st, αt, at,rt−1, . . . , s1, α1, a1, Q0} by Pt. Note that
Qt, Qt−1, . . . , Q0 are Pt-measurable and, thus, both 1t and Ft−1 are Pt-measurable, satisfying the measurability conditions of Lemma 1.
SINGH ET AL.
Substituting the right-hand side of Eq. (7) for ¯Q(st, at) in the deﬁnition of Ft together
with the properties of sampling rt, st+1 and at+1 yields that
E{Ft(st, at) | Pt}
E{Qt(st+1, at+1) | Pt}−
T (ρ( ¯Q, s′, a′)) ¯Q(s′, a′)
T (ρ(Qt, s′, a′))Qt(s′, a′)
T (ρ( ¯Q, s′, a′)) ¯Q(s′, a′)
≤γ ∥Qt −¯Q∥
where in the ﬁrst equation we have exploited the fact that
E{rt | st, at} = R(st, at),
in the second equation that
Pr(st+1 | st, at) = Pat
Pr(at+1 = a | Qt, st+1) = T (ρ(Qt, st+1, a)) (Condition 1),
whereas the inequality comes from the properties of rank-based averaging , also Appendix C). Finally, it is not
hard to prove that the variance of Ft given the past Pt satisﬁes Condition 4 and, therefore,
we do not include it here.
We have shown that SARSA(0) with an RRR learning policy converges to ¯Q. Next, we
show that ¯Q is, in a sense, an optimal Q-value function.
An optimal restricted policy is one that has the highest expected total discounted reward
of all restricted policies. The greedy restricted policy for a Q-value function Q is ¯π(s, a) =
ρ(Q, s, a); it assigns each action the rank of its corresponding Q value. Note that this is the
policy dictated by the RRR learning policy for a ﬁxed Q-value function Q.
The greedy restricted policy for Q∗(the optimal Q-value function of the MDP) is not
an optimal restricted policy in general, so the Q-learning rule in Eq. (2) does not ﬁnd an
optimal restricted policy. However, the next theorem shows that the greedy restricted policy
for ¯Q (Eq. (7)) is an optimal restricted policy. This ¯Q function is very similar to Q∗, except
that actions are weighted according to the greedy restricted policy instead of the standard
greedy policy.
CONVERGENCE OF ON-POLICY RL ALGORITHMS
Theorem 3.
The greedy restricted policy with respect to ¯Q is an optimal restricted policy.
We construct an alternate MDP so that every restricted policy in the original MDP
is in one-to-one correspondence with (and has the same value as) a deterministic stationary
policy in the alternate MDP. Note that, as a result of the equality of value functions, the
optimal policy of the alternate MDP will correspond to an optimal restricted policy of the
original MDP (the restricted policy that achieves the best values for each of the states)
and, thus, the theorem will follow if we show that the optimal policy in the alternate MDP
corresponds to the greedy restricted policy with respect to ¯Q.
The alternate MDP is deﬁned by ⟨S, ¯A, ¯R, ¯P, γ ⟩. Its action space, ¯A, is the set of all
bijections from A to {1, . . . , m}, i.e., ¯A = 5(A, {1, . . . , m}). The rewards are
¯R(s, µ) =
T (µ(a))R(s, a),
and the transition probabilities are given by ¯Pµ
a∈A T (µ(a))Pa
ss′. Here, µ is an element
of ¯A. One can readily check that the value of a restricted policy ¯π is just the value of ¯π in
the alternate MDP.
The value of the greedy restricted policy with respect to ¯Q in the original MDP is
T (ρ( ¯Q, s, a)) ¯Q(s, a).
Substituting the deﬁnition of ¯Q from Eq. (7) into Eq. (10) results in
T (ρ( ¯Q, s, a))
R(s, a) + γ
T (ρ( ¯Q, s′, a′)) ¯Q(s′, a′)
Using Eq. (10) once again, we ﬁnd that ¯V satisﬁes the recurrence equation
T (ρ( ¯Q, s, a))
R(s, a) + γ
ss′ ¯V (s′)
Meanwhile, the optimum value of the alternate MDP satisﬁes
¯V ∗(s) = max
¯R(s, µ) + γ
ss′ ¯V ∗(s′)
T (µ(a))R(s, a) + γ
T (µ(a))Pa
R(s, a) + γ
ss′ ¯V ∗(s′)
SINGH ET AL.
The highest value permutation is the one that assigns the highest probabilities to the actions
with the highest Q values and the lowest probabilities to the actions with the lowest Q values.
Therefore, the recurrence in Eq. (12) is the same as that in Eq. (11), so, by uniqueness,
¯V ∗= ¯V . This means the greedy restricted policy with respect to ¯Q is the optimal restricted
As a corollary of Theorem 2, given a communicating MDP and an RL algorithm that
follows an RRR learning policy speciﬁed by T where T (i) > 0 for all 1 ≤i ≤m,
SARSA(0) converges to an optimal restricted policy.3
The results of this section show that RRR learning policies with the SARSA(0) update rule
converge to optimal restricted policies. In contrast to Q-learning, this means that the learner
can adopt its asymptotic policy at any time during learning and still converge to optimality
in this modiﬁed sense. However, the fact that convergence depends on decaying the learning
rate to zero means that this approach is somewhat self-contradictory; in the limit, the learner
is still exploring, but it is not able to learn anything new from its discoveries.
Conclusion
In this paper, we have provided convergence results for SARSA(0) under two different
learning policy classes; one ensures optimal behavior in the limit and the other ensures
behavior optimal with respect to constraints imposed by the exploration strategy. To the
best of our knowledge, these constitute the ﬁrst convergence results for any on-policy
algorithm. However, these are very basic results because they apply only to the lookuptable case, and more importantly because they do not seem to extend naturally to general
multi-step on-policy algorithms.
Appendix A: proof of Lemma 1
For completeness we present Proposition 4.5 of Bertsekas .
rt+1(i) = (1 −γt(i))rt(i) + γt(i){(Htrt)(i) + wt(i) + ut(i)},
where i = 1, 2, . . . , n and t = 0, 1, 2, . . . , let Ft be an increasing sequence of σ-ﬁelds,
and assume the following:
1. γt ≥0, P∞
t=0 γt(i) = ∞, P∞
t (i) < ∞(a.s.);
2. (A) for all i, t : E[wt(i)|Ft] = 0;
3. (B) there exist A, B ∈ℜs.t. for all i, t : E[w2
t (i) | Ft] ≤A + B∥rt∥2;
4. there exists an r∗∈ℜn, a positive vector ξ, and a scalar β ∈[0, 1) s.t. for all t ≥0
∥Htrt −r∗∥ξ ≤β∥rt −r∗∥ξ;
5. there exists θt ≥0, θt →0 w.p.1 and for all i, t : |ut(i)| ≤θt(∥rt∥ξ + 1).
Then rt →r∗w.p.1.
For convenience, we repeat Lemma 1.
CONVERGENCE OF ON-POLICY RL ALGORITHMS
Consider a stochastic process (αt, 1t, Ft), t ≥0, where αt, 1t, Ft : X →ℜ,
which satisﬁes the equations
1t+1(x) = (1 −αt(x))1t(x) + αt(x)Ft(x),
x ∈X, t = 0, 1, 2, . . . .
Let Pt be a sequence of increasing σ-ﬁelds such that α0, 10 are P0-measurable and αt, 1t
and Ft−1 are Pt-measurable, t = 1, 2, . . . . Assume that the following hold:
1. the set of possible states X is ﬁnite.
2. 0 ≤αt(x) ≤1, P
t αt(x) = ∞, P
t (x) < ∞w.p.1.
3. ∥E{Ft(·)|Pt}∥W ≤κ∥1t∥W + ct, where κ ∈[0, 1) and ct converges to zero w.p.1.
4. Var{Ft(x)|Pt} ≤K(1 + ∥1t∥W)2, where K is some constant.
Then, 1t converges to zero with probability one (w.p.1).
We apply Lemma 2. For simplicity, we present the proof for the case when W =
(1, 1, . . . , 1). Let
if |E[Ft|Pt]| ≤κ∥1t∥;
sign(E[Ft|Pt])κ∥1t∥,
otherwise.
Further, let bt = Ft −˜Ft. Then, by the construction of ˜Ft, ∥E[ ˜Ft|Pt]∥≤κ∥1t∥and
∥E[bt|Pt]∥≤ct. Now, if we identify {1, 2, . . . , n} with X, and deﬁne Ft = Pt, γt = αt,
rt = 1t, Htrt = E[ ˜Ft|Pt], wt = ˜Ft −E[ ˜Ft|Pt]+bt −E[bt|Pt], ut = E[bt|Pt] and r∗= 0,
then we see that the conditions of Lemma 2 are satisﬁed and thus rt = 1t converges to
r∗= 0 w.p.1.
Appendix B: GLIE learning policies
Here, we present conditions on the exploration parameter in the commonly used Boltzmann
exploration and ϵ-greedy exploration strategies to ensure that both inﬁnite exploration and
greedy in the limit conditions are satisﬁed.
In a communicating MDP, every state gets visited inﬁnitely often as long as each action
is chosen inﬁnitely often in each state ; all we have to ensure is that in each state each action gets chosen inﬁnitely
often in the limit. Consider some state s. Let ts(i) represent the timestep at which the ith visit
to state s occurs. Consider some action a. The probability with which action a is executed
at the ith visit to state s is denoted Pr(a | s, ts(i)) (i.e, Pr(a = at | st = s, ts(i) = t)).
We would like to show that if the sum of the probabilities with which action a is chosen
is inﬁnite, i.e., P∞
i=1 Pr(a | s, ts(i)) = ∞, then the number of times action a gets executed
in state s is inﬁnite w.p.1. This would follow directly from the Borel-Cantelli Lemma if the
probabilities of selecting action a at the different i were independent. However, in our case
the random choice of action at the ith visit to state s affects the probabilities at the i + 1st
visit to state s (through the evolution of the Q-value function), so we need an extension of
the Borel-Cantelli Lemma ):
SINGH ET AL.
Lemma 3 (Extended Borel-Cantelli Lemma).
Let Fi be an increasing sequence of
σ-ﬁelds and let Ai be Fi-measurable. Then
Pr(Ai|Fi−1) = ∞
= {ω : ω ∈Ai i.o.}
holds w.p.1.
We have the following:
Consider a communicating MDP and the reinforced decision process
(x0, a0,r0, . . . , xt, at,rt, . . .).
Let nt(s) denote the number of visits to state s up to time t, nt(s, a) denote the number of
times action a has been chosen in state s during the ﬁrst t timesteps (nt(s, a) ≤nt(s)), and
ts(i) denote the time when state s was visited the ith time. Assume that the action at time
step t, at, is selected purely on the basis of the statistics Dt:
Pr(at = a | Dt, at−1, Dt−1, . . . , a0, D0) = Pr(at = a | Dt),
where Dt is computed from the full t-step history (x0, a0,r0, . . . , xt). Further, assume that
the action selection policy π is such that
t→∞nt(s)(ω) = ∞
ats(i) = a | Dts(i)
Then, for all (s, a) pairs nt(s) →∞a.s. and nt(s, a) →∞a.s.
The statistics Dt could be for example (st, t, nt(s), Qt), where Qt is computed by the
SARSA(0) update rule (3).
Fix an arbitrary pair (s, a) and let Fi be the sigma ﬁeld generated by the random variables {Dts(i+1), ats(i), Dts(i), . . . , ats(0), Dts(0)}. Let Ai = {ats(i) = a}. Then Ai is
Fi-measurable. Further, by Eq. (B.1)
Pr(Ai|Fi−1) = Pr
ats(i) = a
¯¯Dts(i), ats(i−1), Dts(i−1), . . . , ats(0), Dts(0)
ats(i) = a
CONVERGENCE OF ON-POLICY RL ALGORITHMS
and thus, by Eq. (B.2) and Lemma 3, almost surely
t→∞nt(s)(ω) = ∞
ats(i) = a
= {ω : ω ∈Ai for inﬁnitely many is}
t→∞nt(s, a) = ∞
This proves that if state s is visited inﬁnitely often then action a is also chosen inﬁnitely often
in that state. Now let S∞be the set of states visited i.o. by st, i.e., if S∞(ω) = S0 then S0 is
the set of states which occur i.o. in the sequence {s0(ω), s1(ω), . . . , st(ω), . . .}. Clearly, the
events {S∞= S0}, S0 ⊆S form a complete event system. Thus, P
S0⊆S P(S∞= S0) = 1.
Now let S0 ̸= ∅be a nontrivial subset of S. Then, since the MDP is communicating, there
exists a pair of states s, s′ and an action a, such that s ∈S0, s′ ̸∈S0 and Pa
ss′ > 0. Then,
Pr(S∞= S0) = Pr(S∞= S0, s′ ∈S∞) + Pr(S∞= S0, s′ ̸∈S∞). Here, both events are
impossible, so Pr(S∞= S0) = 0. Since the MDP is ﬁnite, also Pr(S∞= ∅) = 0 and so
Pr(S∞= S) = 1. This yields that Pr(limt→∞nt(s) = ∞) = 1 for all s, thus, ﬁnishing the
Boltzmann exploration
In Boltzmann exploration,
Pr(a | s, t, Q, nt(s)) =
eβt(s)Q(s,a)
b∈A eβt(s)Q(s,b) ,
where βt(s) is the state-speciﬁc exploration coefﬁcient for time t. Let the number of visits
to state s in timestep t be denoted as nt(s) and assume that r(s, a) has a ﬁnite range. We
know that P∞
i=1 c/i = ∞; therefore, to meet the conditions of Lemma 4, we will ensure
that for all actions a ∈A, Pr(a|s, ts(i)) ≥c/i (with c ≤1). To do that we need for all a:
eβt(s)Qt(s,a)
b∈A eβt(s)Qt(s,b) ≥
nt(s)eβt(s)Qt(s,a) ≥c
eβt(s)Qt(s,b)
nt(s)eβt(s)Qt(s,a) ≥cmeβt(s)Qt(s,bmax)
≥eβt(s)(Qt(s,bmax)−Qt(s,a))
ln nt(s) −ln cm ≥βt(s)(Qt(s, bmax) −Qt(s, a)),
where bmax = argmaxb∈A Qt(s, b) above and m is the number of actions. Further, let
c = 1/m. Taken together, this means that we want βt(s) ≤ln nt(s)/Ct(s) where Ct(s) =
SINGH ET AL.
maxa |Qt(s, bmax) −Qt(s, a)|. Note that Ct(s) is bounded because the Q values remain
bounded (since r(s, a) has a bounded range).
Since for every s, limt→∞nt(s) = ∞, also
t→∞βt(s) ≤lim
Ct(s) = ∞;
this means that Boltzmann exploration with βt(s) = ln nt(s)/Ct(s) will be greedy in the
ϵ-greedy exploration
In ϵ-greedy exploration we pick a random exploration action with probability ϵt(s) and
the greedy action with probability 1 −ϵt(s). Let ϵt(s) = c/nt(s) with 0 < c < 1. Then,
Pr(a|s, ts(i)) ≥ϵt(s)/m, where m is the number of actions. Therefore, Lemma 4 combined
with the fact that P∞
i=1 c/i = ∞implies that for all s, P∞
i=1 Pr(a|s, ts(i)) = ∞. Since
also by Lemma 4 for all s, limt→∞nt(s) = ∞, and, therefore, limt→∞ϵt(s) = 0, ensuring
that the learning policy is greedy in the limit. Therefore, if ϵt(s) = c/nt(s) then ϵ-greedy
exploration is GLIE for 0 < c < 1.
Appendix C: generalized Markov decision processes
In this section, we give proofs of several properties associated with generalized MDPs, which
are described in more detail by Szepesv´ari & Littman .
Deﬁne the Q-value function
Q(s, a) = R(s, a) + γ
Q(s′, a′), (s, a) ∈S × A.
Here, we assume 0 ≤γ < 1.
The important property for N to satisfy is the non-expansion property:
¯¯¯¯¯ ≤max
|Q(s, a) −Q′(s, a)|
for all Q-value functions Q and Q′ and all states s.
We begin by showing that an average over actions with a ﬁxed set of weights satisﬁes the
non-expansion property.
The function N Q(s, a) = P pa Q(s, a) satisﬁes the non-expansion property,
where 0 ≤pa ≤1 and P
CONVERGENCE OF ON-POLICY RL ALGORITHMS
This follows directly from deﬁnitions. If Q and Q′ are Q-value functions, we have
pa(Q(s, a) −Q′(s, a))
pa|Q(s, a) −Q′(s, a)|
|Q(s, a) −Q′(s, a)|.
A corollary is that a ﬁxed-weight average of functions that satisfy the non-expansion
property also satisﬁes the non-expansion property.
We can use Lemma 5 to prove the existence and uniqueness of the Q-value function.
As long as N satisﬁes the non-expansion property, Eq. (C.1) has a solution
and it is unique.
Deﬁne the operator L on Q-value functions as
(LQ)(s, a) = R(s, a) + γ
Q(s′, a′),
for all (s, a) ∈S × A. We can rewrite Eq. (C.1) as Q(s, a) = (LQ)(s, a), which has a
unique solution if L is contraction with respect to the max norm.
To see that L is a contraction, consider two Q-value functions Q and Q′. We have
|LQ −LQ′| ≤γ maxs′ | N
a′ Q(s′, a′) −N
a′ Q′(s′, a′)| < |Q −Q′|, where we have used
Lemma 5, the fact that γ < 1, and the non-expansion property of N.
Finally, deﬁne a family of rank-based operators:
i Q(s, a) = ith largest value of Q(s, a), for each 1 ≤i ≤m.
We show that these operators satisfy the non-expansion property.
a Q(s, a) operators satisfy the non-expansion property.
Let Q and Q′ be Q-value functions and ﬁx s ∈S. Without loss of generality,
a Q(s, a) ≥Ni
a Q′(s, a). Let a∗be the ith largest value of Q(s, a): Q(s, a∗) =
a Q(s, a).
We examine two cases separately and show that the non-expansion property is satisﬁed
either way. If Q′(s, a∗) ≤Ni
a Q′(s, a), then
SINGH ET AL.
i Q(s, a) −
i Q′(s, a)
¯¯¯¯¯ = Q(s, a∗) −
i Q′(s, a)
≤Q(s, a∗) −Q′(s, a∗)
|Q(s, a) −Q′(s, a)|.
On the other hand, if Q′(s, a∗) > Ni
a Q′(s, a), that means that the rank of a∗in Q′,
ρ(Q′, s, a∗) is smaller than i. This implies that there is some a′ such that ρ(Q, s, a′) < i
and ρ(Q′, s, a′) ≥i (otherwise there would be i actions with ranks less than i in Q′). For
i Q(s, a) −
i Q′(s, a)
i Q(s, a) −
i Q′(s, a)
≤Q(s, a′) −Q′(s, a′)
|Q(s, a) −Q′(s, a)|.
Acknowledgments
We thank Richard S. Sutton for help and encouragement. We also thank Nicolas Meuleau
and the anonymous reviewers for comments and suggestions. This research was partially supported by NSF grant IIS-9711753 (Satinder Singh), OTKA Grant No. F20132
(Csaba Szepesv´ari), Hungarian Ministry of Education Grant No. FKFP 1354/1997 (Csaba
Szepesv´ari), and NSF CAREER grant IRI-9702576 (Michael Littman).
1. The name is a reference to the fact that it is a single-step algorithm that makes updates on the basis of a State,
Action, Reward, State, Action 5-tuple.
2. Here ∥· ∥W denotes a weighted maximum norm with weight W = (w1, . . . , wn), wi > 0: if x ∈ℜn then
∥x∥W = maxi(|xi|/wi).
3. We conjecture that the same result does not hold for persistent Boltzmann exploration because related synchronous algorithms do not have a unique target of convergence .