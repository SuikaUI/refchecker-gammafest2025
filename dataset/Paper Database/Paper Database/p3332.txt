Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 328–339
Melbourne, Australia, July 15 - 20, 2018. c⃝2018 Association for Computational Linguistics
Universal Language Model Fine-tuning for Text Classiﬁcation
Jeremy Howard∗
University of San Francisco
Sebastian Ruder∗
Insight Centre, NUI Galway
Aylien Ltd., Dublin
 
Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-speciﬁc
modiﬁcations and training from scratch.
We propose Universal Language Model
Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to
any task in NLP, and introduce techniques
that are key for ﬁne-tuning a language
model. Our method signiﬁcantly outperforms the state-of-the-art on six text classiﬁcation tasks, reducing the error by 18-
24% on the majority of datasets. Furthermore, with only 100 labeled examples, it
matches the performance of training from
scratch on 100× more data.
We opensource our pretrained models and code1.
Introduction
Inductive transfer learning has had a large impact
on computer vision (CV). Applied CV models (including object detection, classiﬁcation, and segmentation) are rarely trained from scratch, but instead are ﬁne-tuned from models that have been
pretrained on ImageNet, MS-COCO, and other
datasets .
Text classiﬁcation is a category of Natural Language Processing (NLP) tasks with real-world applications such as spam, fraud, and bot detection
 , emergency response ,
and commercial document classiﬁcation, such as
for legal discovery .
1 
⋆Equal contribution. Jeremy focused on the algorithm development and implementation, Sebastian focused on the experiments and writing.
While Deep Learning models have achieved
state-of-the-art on many NLP tasks, these models
are trained from scratch, requiring large datasets,
and days to converge. Research in NLP focused
mostly on transductive transfer ,
a simple transfer technique that only targets a
model’s ﬁrst layer, has had a large impact in practice and is used in most state-of-the-art models.
Recent approaches that concatenate embeddings
derived from other tasks with the input at different
layers still train the main task model
from scratch and treat pretrained embeddings as
ﬁxed parameters, limiting their usefulness.
In light of the beneﬁts of pretraining , we should be able to do better than
randomly initializing the remaining parameters of
our models. However, inductive transfer via ﬁnetuning has been unsuccessful for NLP ﬁrst proposed ﬁnetuning a language model (LM) but require millions
of in-domain documents to achieve good performance, which severely limits its applicability.
We show that not the idea of LM ﬁne-tuning but
our lack of knowledge of how to train them effectively has been hindering wider adoption. LMs
overﬁt to small datasets and suffered catastrophic
forgetting when ﬁne-tuned with a classiﬁer. Compared to CV, NLP models are typically more shallow and thus require different ﬁne-tuning methods.
We propose a new method, Universal Language
Model Fine-tuning (ULMFiT) that addresses these
issues and enables robust inductive transfer learning for any NLP task, akin to ﬁne-tuning ImageNet
models: The same 3-layer LSTM architecture—
with the same hyperparameters and no additions other than tuned dropout hyperparameters—
outperforms highly engineered models and trans-
fer learning approaches on six widely studied text
classiﬁcation tasks. On IMDb, with 100 labeled
examples, ULMFiT matches the performance of
training from scratch with 10× and—given 50k
unlabeled examples—with 100× more data.
Contributions
Our contributions are the following: 1) We propose Universal Language Model
Fine-tuning (ULMFiT), a method that can be used
to achieve CV-like transfer learning for any task
for NLP. 2) We propose discriminative ﬁne-tuning,
slanted triangular learning rates, and gradual
unfreezing, novel techniques to retain previous
knowledge and avoid catastrophic forgetting during ﬁne-tuning. 3) We signiﬁcantly outperform the
state-of-the-art on six representative text classiﬁcation datasets, with an error reduction of 18-24%
on the majority of datasets. 4) We show that our
method enables extremely sample-efﬁcient transfer learning and perform an extensive ablation
analysis. 5) We make the pretrained models and
our code available to enable wider adoption.
Related work
Transfer learning in CV
Features in deep neural networks in CV have been observed to transition from task-speciﬁc to general from the ﬁrst
to the last layer . For this
reason, most work in CV focuses on transferring
the last layers of the model .
Sharif Razavian et al. achieve state-of-theart results using features of an ImageNet model as
input to a simple classiﬁer. In recent years, this
approach has been superseded by ﬁne-tuning either the last or several of
the last layers of a pretrained model and leaving
the remaining layers frozen .
Hypercolumns
In NLP, only recently have
methods been proposed that go beyond transferring word embeddings. The prevailing approach
is to pretrain embeddings that capture additional
context via other tasks. Embeddings at different
levels are then used as features, concatenated either with the word embeddings or with the inputs at intermediate layers. This method is known
as hypercolumns in CV2
and is used by Peters et al. , Peters et al.
 , Wieting and Gimpel , Conneau
2A hypercolumn at a pixel in CV is the vector of activations of all CNN units above that pixel. In analogy, a hypercolumn for a word or sentence in NLP is the concatenation of
embeddings at different layers in a pretrained model.
et al. , and McCann et al. who use
language modeling, paraphrasing, entailment, and
Machine Translation (MT) respectively for pretraining. Speciﬁcally, Peters et al. require
engineered custom architectures, while we show
state-of-the-art performance with the same basic
architecture across a range of tasks. In CV, hypercolumns have been nearly entirely superseded by
end-to-end ﬁne-tuning .
Multi-task
multi-task learning (MTL) . This
is the approach taken by Rei and Liu et al.
 who add a language modeling objective
to the model that is trained jointly with the main
task model. MTL requires the tasks to be trained
from scratch every time, which makes it inefﬁcient
and often requires careful weighting of the taskspeciﬁc objective functions .
Fine-tuning
Fine-tuning has been used successfully to transfer between similar tasks, e.g. in QA
 , for distantly supervised sentiment analysis , or
MT domains but has been
shown to fail between unrelated ones . Dai and Le also ﬁne-tune a language model, but overﬁt with 10k labeled examples and require millions of in-domain documents
for good performance. In contrast, ULMFiT leverages general-domain pretraining and novel ﬁnetuning techniques to prevent overﬁtting even with
only 100 labeled examples and achieves state-ofthe-art results also on small datasets.
Universal Language Model Fine-tuning
We are interested in the most general inductive
transfer learning setting for NLP : Given a static source task TS and any target task TT with TS ̸= TT , we would like to improve performance on TT .
Language modeling
can be seen as the ideal source task and a counterpart of ImageNet for NLP: It captures many facets
of language relevant for downstream tasks, such as
long-term dependencies , hierarchical relations , and
sentiment . In contrast to
tasks like MT and entailment , it provides data in
near-unlimited quantities for most domains and
languages. Additionally, a pretrained LM can be
easily adapted to the idiosyncrasies of a target
(a) LM pre-training
(b) LM ﬁne-tuning
(c) Classiﬁer ﬁne-tuning
Figure 1: ULMFiT consists of three stages: a) The LM is trained on a general-domain corpus to capture
general features of the language in different layers. b) The full LM is ﬁne-tuned on target task data using
discriminative ﬁne-tuning (‘Discr’) and slanted triangular learning rates (STLR) to learn task-speciﬁc
features. c) The classiﬁer is ﬁne-tuned on the target task using gradual unfreezing, ‘Discr’, and STLR to
preserve low-level representations and adapt high-level ones (shaded: unfreezing stages; black: frozen).
task, which we show signiﬁcantly improves performance (see Section 5).
Moreover, language
modeling already is a key component of existing
tasks such as MT and dialogue modeling. Formally, language modeling induces a hypothesis
space H that should be useful for many other NLP
tasks .
We propose Universal Language Model Finetuning (ULMFiT), which pretrains a language
model (LM) on a large general-domain corpus and
ﬁne-tunes it on the target task using novel techniques. The method is universal in the sense that
it meets these practical criteria: 1) It works across
tasks varying in document size, number, and label
type; 2) it uses a single architecture and training
process; 3) it requires no custom feature engineering or preprocessing; and 4) it does not require additional in-domain documents or labels.
In our experiments, we use the state-of-theart language model AWD-LSTM , a regular LSTM (with no attention,
short-cut connections, or other sophisticated additions) with various tuned dropout hyperparameters. Analogous to CV, we expect that downstream
performance can be improved by using higherperformance language models in the future.
ULMFiT consists of the following steps, which
we show in Figure 1:
a) General-domain LM
pretraining (§3.1); b) target task LM ﬁne-tuning
(§3.2); and c) target task classiﬁer ﬁne-tuning
(§3.3). We discuss these in the following sections.
General-domain LM pretraining
An ImageNet-like corpus for language should be
large and capture general properties of language.
We pretrain the language model on Wikitext-103
 consisting of 28,595 preprocessed Wikipedia articles and 103 million words.
Pretraining is most beneﬁcial for tasks with small
datasets and enables generalization even with 100
labeled examples.
We leave the exploration of
more diverse pretraining corpora to future work,
but expect that they would boost performance.
While this stage is the most expensive, it only
needs to be performed once and improves performance and convergence of downstream models.
Target task LM ﬁne-tuning
No matter how diverse the general-domain data
used for pretraining is, the data of the target task
will likely come from a different distribution. We
thus ﬁne-tune the LM on data of the target task.
Given a pretrained general-domain LM, this stage
converges faster as it only needs to adapt to the idiosyncrasies of the target data, and it allows us to
train a robust LM even for small datasets. We propose discriminative ﬁne-tuning and slanted triangular learning rates for ﬁne-tuning the LM, which
we introduce in the following.
Discriminative ﬁne-tuning
As different layers
capture different types of information , they should be ﬁne-tuned to different extents. To this end, we propose a novel ﬁne-
tuning method, discriminative ﬁne-tuning3.
Instead of using the same learning rate for all
layers of the model, discriminative ﬁne-tuning allows us to tune each layer with different learning
rates. For context, the regular stochastic gradient
descent (SGD) update of a model’s parameters θ at
time step t looks like the following :
θt = θt−1 −η · ∇θJ(θ)
where η is the learning rate and ∇θJ(θ) is the gradient with regard to the model’s objective function. For discriminative ﬁne-tuning, we split the
parameters θ into {θ1, . . . , θL} where θl contains
the parameters of the model at the l-th layer and
L is the number of layers of the model. Similarly,
we obtain {η1, . . . , ηL} where ηl is the learning
rate of the l-th layer.
The SGD update with discriminative ﬁnetuning is then the following:
t−1 −ηl · ∇θlJ(θ)
We empirically found it to work well to ﬁrst
choose the learning rate ηL of the last layer by
ﬁne-tuning only the last layer and using ηl−1 =
ηl/2.6 as the learning rate for lower layers.
Slanted triangular learning rates
For adapting
its parameters to task-speciﬁc features, we would
like the model to quickly converge to a suitable
region of the parameter space in the beginning
of training and then reﬁne its parameters. Using
the same learning rate (LR) or an annealed learning rate throughout training is not the best way
to achieve this behaviour.
Instead, we propose
slanted triangular learning rates (STLR), which
ﬁrst linearly increases the learning rate and then
linearly decays it according to the following update schedule, which can be seen in Figure 2:
cut = ⌊T · cut frac⌋
if t < cut
cut·(ratio−1),
ηt = ηmax · 1 + p · (ratio −1)
where T is the number of training iterations4,
cut frac is the fraction of iterations we increase
3 An unrelated method of the same name exists for deep
Boltzmann machines .
4In other words, the number of epochs times the number
of updates per epoch.
the LR, cut is the iteration when we switch from
increasing to decreasing the LR, p is the fraction of
the number of iterations we have increased or will
decrease the LR respectively, ratio speciﬁes how
much smaller the lowest LR is from the maximum
LR ηmax, and ηt is the learning rate at iteration t.
We generally use cut frac = 0.1, ratio = 32 and
ηmax = 0.01.
STLR modiﬁes triangular learning rates with a short increase and a long decay period, which we found key for good performance.5
In Section 5, we compare against aggressive cosine annealing, a similar schedule that has recently
been used to achieve state-of-the-art performance
in CV .6
The slanted triangular learning rate
schedule used for ULMFiT as a function of the
number of training iterations.
Target task classiﬁer ﬁne-tuning
Finally, for ﬁne-tuning the classiﬁer, we augment
the pretrained language model with two additional
linear blocks.
Following standard practice for
CV classiﬁers, each block uses batch normalization and dropout, with
ReLU activations for the intermediate layer and a
softmax activation that outputs a probability distribution over target classes at the last layer. Note
that the parameters in these task-speciﬁc classi-
ﬁer layers are the only ones that are learned from
scratch. The ﬁrst linear layer takes as the input the
pooled last hidden layer states.
Concat pooling
The signal in text classiﬁcation
tasks is often contained in a few words, which may
5We also credit personal communication with the author.
6While Loshchilov and Hutter use multiple annealing cycles, we generally found one cycle to work best.
occur anywhere in the document. As input documents can consist of hundreds of words, information may get lost if we only consider the last hidden state of the model. For this reason, we concatenate the hidden state at the last time step hT
of the document with both the max-pooled and the
mean-pooled representation of the hidden states
over as many time steps as ﬁt in GPU memory
H = {h1, . . . , hT }:
hc = [hT , maxpool(H), meanpool(H)]
where [] is concatenation.
Fine-tuning the target classiﬁer is the most critical part of the transfer learning method. Overly
aggressive ﬁne-tuning will cause catastrophic forgetting, eliminating the beneﬁt of the information
captured through language modeling; too cautious
ﬁne-tuning will lead to slow convergence (and resultant overﬁtting). Besides discriminative ﬁnetuning and triangular learning rates, we propose
gradual unfreezing for ﬁne-tuning the classiﬁer.
Gradual unfreezing
Rather than ﬁne-tuning all
layers at once, which risks catastrophic forgetting,
we propose to gradually unfreeze the model starting from the last layer as this contains the least
general knowledge : We
ﬁrst unfreeze the last layer and ﬁne-tune all unfrozen layers for one epoch. We then unfreeze the
next lower frozen layer and repeat, until we ﬁnetune all layers until convergence at the last iteration. This is similar to ‘chain-thaw’ , except that we add a layer at a time to the
set of ‘thawed’ layers, rather than only training a
single layer at a time.
While discriminative ﬁne-tuning, slanted triangular learning rates, and gradual unfreezing all
are beneﬁcial on their own, we show in Section
5 that they complement each other and enable our
method to perform well across diverse datasets.
BPTT for Text Classiﬁcation (BPT3C)
Language models are trained with backpropagation
through time (BPTT) to enable gradient propagation for large input sequences.
In order to
make ﬁne-tuning a classiﬁer for large documents
feasible, we propose BPTT for Text Classiﬁcation (BPT3C): We divide the document into ﬁxedlength batches of size b. At the beginning of each
batch, the model is initialized with the ﬁnal state
of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients
# examples
Table 1: Text classiﬁcation datasets and tasks with
number of classes and training examples.
are back-propagated to the batches whose hidden
states contributed to the ﬁnal prediction. In practice, we use variable length backpropagation sequences .
Bidirectional language model
Similar to existing work , we are not
limited to ﬁne-tuning a unidirectional language
model. For all our experiments, we pretrain both a
forward and a backward LM. We ﬁne-tune a classiﬁer for each LM independently using BPT3C
and average the classiﬁer predictions.
Experiments
While our approach is equally applicable to sequence labeling tasks, we focus on text classiﬁcation tasks in this work due to their important realworld applications.
Experimental setup
Datasets and tasks
We evaluate our method on
six widely-studied datasets, with varying numbers
of documents and varying document length, used
by state-of-the-art text classiﬁcation and transfer
learning approaches as instances of three common text classiﬁcation tasks:
sentiment analysis, question classiﬁcation, and topic classiﬁcation. We show the statistics for each dataset and
task in Table 1.
Sentiment Analysis
For sentiment analysis, we
evaluate our approach on the binary movie review
IMDb dataset and on the binary
and ﬁve-class version of the Yelp review dataset
compiled by Zhang et al. .
Question Classiﬁcation
We use the six-class
version of the small TREC dataset dataset of open-domain, fact-based
questions divided into broad semantic categories.
CoVe 
CoVe 
oh-LSTM 
TBCNN 
Virtual 
LSTM-CNN 
ULMFiT (ours)
ULMFiT (ours)
Table 2: Test error rates (%) on two text classiﬁcation datasets used by McCann et al. .
Char-level CNN 
CNN 
DPCNN 
ULMFiT (ours)
Table 3: Test error rates (%) on text classiﬁcation datasets used by Johnson and Zhang .
Topic classiﬁcation
For topic classiﬁcation, we
evaluate on the large-scale AG news and DBpedia
ontology datasets created by Zhang et al. .
Pre-processing
We use the same pre-processing
as in earlier work . In addition, to allow the language model to capture aspects that might be relevant for classiﬁcation, we add special tokens for
upper-case words, elongation, and repetition.
Hyperparameters
We are interested in a model
that performs robustly across a diverse set of tasks.
To this end, if not mentioned otherwise, we use the
same set of hyperparameters across tasks, which
we tune on the IMDb validation set.
the AWD-LSTM language model with an embedding size of 400, 3 layers,
1150 hidden activations per layer, and a BPTT
batch size of 70.
We apply dropout of 0.4 to
layers, 0.3 to RNN layers, 0.4 to input embedding layers, 0.05 to embedding layers, and weight
dropout of 0.5 to the RNN hidden-to-hidden matrix. The classiﬁer has a hidden layer of size 50.
We use Adam with β1 = 0.7 instead of the default β1 = 0.9 and β2 = 0.99, similar to . We use a batch size of 64,
a base learning rate of 0.004 and 0.01 for ﬁnetuning the LM and the classiﬁer respectively, and
tune the number of epochs on the validation set of
each task7. We otherwise use the same practices
7On small datasets such as TREC-6, we ﬁne-tune the LM
only for 15 epochs without overﬁtting, while we can ﬁne-tune
longer on larger datasets. We found 50 epochs to be a good
default for ﬁne-tuning the classiﬁer.
used in .
Baselines and comparison models
task, we compare against the current state-of-theart. For the IMDb and TREC-6 datasets, we compare against CoVe , a stateof-the-art transfer learning method for NLP. For
the AG, Yelp, and DBpedia datasets, we compare against the state-of-the-art text categorization
method by Johnson and Zhang .
For consistency, we report all results as error rates
(lower is better).
We show the test error rates
on the IMDb and TREC-6 datasets used by Mc-
Cann et al. in Table 2. Our method outperforms both CoVe, a state-of-the-art transfer learning method based on hypercolumns, as well as the
state-of-the-art on both datasets. On IMDb, we
reduce the error dramatically by 43.9% and 22%
with regard to CoVe and the state-of-the-art respectively. This is promising as the existing stateof-the-art requires complex architectures , multiple forms of attention and sophisticated embedding schemes
 , while our method employs a regular LSTM with dropout.
that the language model ﬁne-tuning approach of
Dai and Le only achieves an error of 7.64
vs. 4.6 for our method on IMDb, demonstrating
the beneﬁt of transferring knowledge from a large
ImageNet-like corpus using our ﬁne-tuning techniques. IMDb in particular is reﬂective of realworld datasets: Its documents are generally a few
Figure 3: Validation error rates for supervised and semi-supervised ULMFiT vs. training from scratch
with different numbers of training examples on IMDb, TREC-6, and AG (from left to right).
paragraphs long—similar to emails (e.g for legal
discovery) and online comments (e.g for community management); and sentiment analysis is similar to many commercial applications, e.g. product
response tracking and support email routing.
On TREC-6, our improvement—similar as the
improvements of state-of-the-art approaches—is
not statistically signiﬁcant, due to the small size of
the 500-examples test set. Nevertheless, the competitive performance on TREC-6 demonstrates
that our model performs well across different
dataset sizes and can deal with examples that range
from single sentences—in the case of TREC-6—
to several paragraphs for IMDb. Note that despite
pretraining on more than two orders of magnitude
less data than the 7 million sentence pairs used by
McCann et al. , we consistently outperform
their approach on both datasets.
We show the test error rates on the larger AG,
DBpedia, Yelp-bi, and Yelp-full datasets in Table
Our method again outperforms the state-ofthe-art signiﬁcantly. On AG, we observe a similarly dramatic error reduction by 23.7% compared
to the state-of-the-art. On DBpedia, Yelp-bi, and
Yelp-full, we reduce the error by 4.8%, 18.2%,
2.0% respectively.
In order to assess the impact of each contribution,
we perform a series of analyses and ablations. We
run experiments on three corpora, IMDb, TREC-
6, and AG that are representative of different tasks,
genres, and sizes. For all experiments, we split off
10% of the training set and report error rates on
this validation set with unidirectional LMs. We
ﬁne-tune the classiﬁer for 50 epochs and train all
methods but ULMFiT with early stopping.
Low-shot learning
One of the main beneﬁts of
transfer learning is being able to train a model for
Pretraining
Without pretraining
With pretraining
Table 4: Validation error rates for ULMFiT with
and without pretraining.
a task with a small number of labels. We evaluate ULMFiT on different numbers of labeled examples in two settings: only labeled examples are
used for LM ﬁne-tuning (‘supervised’); and all
task data is available and can be used to ﬁne-tune
the LM (‘semi-supervised’). We compare ULM-
FiT to training from scratch—which is necessary
for hypercolumn-based approaches. We split off
balanced fractions of the training data, keep the
validation set ﬁxed, and use the same hyperparameters as before. We show the results in Figure 3.
On IMDb and AG, supervised ULMFiT with
only 100 labeled examples matches the performance of training from scratch with 10× and 20×
more data respectively, clearly demonstrating the
beneﬁt of general-domain LM pretraining. If we
allow ULMFiT to also utilize unlabeled examples (50k for IMDb, 100k for AG), at 100 labeled
examples, we match the performance of training
from scratch with 50× and 100× more data on AG
and IMDb respectively.
On TREC-6, ULMFiT
signiﬁcantly improves upon training from scratch;
as examples are shorter and fewer, supervised and
semi-supervised ULMFiT achieve similar results.
Impact of pretraining
We compare using no
pretraining with pretraining on WikiText-103
 in Table 4. Pretraining is
most useful for small and medium-sized datasets,
which are most common in commercial applications. However, even for large datasets, pretraining improves performance.
Vanilla LM
AWD-LSTM LM
Table 5: Validation error rates for ULMFiT with a
vanilla LM and the AWD-LSTM LM.
LM ﬁne-tuning
No LM ﬁne-tuning
Full + discr
Full + discr + stlr
Table 6: Validation error rates for ULMFiT with
different variations of LM ﬁne-tuning.
Impact of LM quality
In order to gauge the importance of choosing an appropriate LM, we compare a vanilla LM with the same hyperparameters without any dropout8 with the AWD-LSTM
LM with tuned dropout parameters in Table 5.
Using our ﬁne-tuning techniques, even a regular
LM reaches surprisingly good performance on the
larger datasets. On the smaller TREC-6, a vanilla
LM without dropout runs the risk of overﬁtting,
which decreases performance.
Impact of LM ﬁne-tuning
We compare no ﬁnetuning against ﬁne-tuning the full model (‘Full’), the most commonly used
ﬁne-tuning method, with and without discriminative ﬁne-tuning (‘Discr’) and slanted triangular
learning rates (‘Stlr’) in Table 6. Fine-tuning the
LM is most beneﬁcial for larger datasets. ‘Discr’
and ‘Stlr’ improve performance across all three
datasets and are necessary on the smaller TREC-6,
where regular ﬁne-tuning is not beneﬁcial.
Impact of classiﬁer ﬁne-tuning
We compare
training from scratch, ﬁne-tuning the full model
(‘Full’), only ﬁne-tuning the last layer (‘Last’)
 , ‘Chain-thaw’ , and gradual unfreezing (‘Freez’). We furthermore assess the importance of discriminative
ﬁne-tuning (‘Discr’) and slanted triangular learning rates (‘Stlr’).
We compare the latter to an
alternative, aggressive cosine annealing schedule
(‘Cos’) . We use a
learning rate ηL = 0.01 for ‘Discr’, learning rates
8To avoid overﬁtting, we only train the vanilla LM classi-
ﬁer for 5 epochs and keep dropout of 0.4 in the classiﬁer.
Classiﬁer ﬁne-tuning
From scratch
Full + discr
Chain-thaw
Freez + discr
Freez + stlr
Freez + cos
Freez + discr + stlr
Table 7: Validation error rates for ULMFiT with
different methods to ﬁne-tune the classiﬁer.
of 0.001 and 0.0001 for the last and all other layers
respectively for ‘Chain-thaw’ as in , and a learning rate of 0.001 otherwise. We
show the results in Table 7.
Fine-tuning the classiﬁer signiﬁcantly improves
over training from scratch, particularly on the
small TREC-6.
‘Last’, the standard ﬁne-tuning
method in CV, severely underﬁts and is never
able to lower the training error to 0.
‘Chainthaw’ achieves competitive performance on the
smaller datasets, but is outperformed signiﬁcantly
on the large AG. ‘Freez’ provides similar performance as ‘Full’.
‘Discr’ consistently boosts
the performance of ‘Full’ and ‘Freez’, except
for the large AG. Cosine annealing is competitive with slanted triangular learning rates on large
data, but under-performs on smaller datasets. Finally, full ULMFiT classiﬁer ﬁne-tuning (bottom
row) achieves the best performance on IMDB and
TREC-6 and competitive performance on AG. Importantly, ULMFiT is the only method that shows
excellent performance across the board—and is
therefore the only universal method.
Classiﬁer ﬁne-tuning behavior
While our results demonstrate that how we ﬁne-tune the classiﬁer makes a signiﬁcant difference, ﬁne-tuning
for inductive transfer is currently under-explored
in NLP as it mostly has been thought to be unhelpful . To better understand
the ﬁne-tuning behavior of our model, we compare
the validation error of the classiﬁer ﬁne-tuned with
ULMFiT and ‘Full’ during training in Figure 4.
On all datasets, ﬁne-tuning the full model leads
to the lowest error comparatively early in training, e.g. already after the ﬁrst epoch on IMDb.
Figure 4: Validation error rate curves for ﬁnetuning the classiﬁer with ULMFiT and ‘Full’ on
IMDb, TREC-6, and AG (top to bottom).
The error then increases as the model starts to
overﬁt and knowledge captured through pretraining is lost.
In contrast, ULMFiT is more stable and suffers from no such catastrophic forgetting; performance remains similar or improves until late epochs, which shows the positive effect of
the learning rate schedule.
Impact of bidirectionality
At the cost of training a second model, ensembling the predictions of
a forward and backwards LM-classiﬁer brings a
performance boost of around 0.5–0.7. On IMDb
we lower the test error from 5.30 of a single model
to 4.58 for the bidirectional model.
Discussion and future directions
While we have shown that ULMFiT can achieve
state-of-the-art performance on widely used text
classiﬁcation tasks, we believe that language
model ﬁne-tuning will be particularly useful in the
following settings compared to existing transfer
learning approaches : a) NLP for
non-English languages, where training data for supervised pretraining tasks is scarce; b) new NLP
tasks where no state-of-the-art architecture exists;
and c) tasks with limited amounts of labeled data
(and some amounts of unlabeled data).
Given that transfer learning and particularly
ﬁne-tuning for NLP is under-explored, many future directions are possible. One possible direction is to improve language model pretraining and
ﬁne-tuning and make them more scalable:
ImageNet, predicting far fewer classes only incurs a small performance drop ,
while recent work shows that an alignment between source and target task label sets is important —focusing on predicting a subset of words such as the most frequent
ones might retain most of the performance while
speeding up training. Language modeling can also
be augmented with additional tasks in a multi-task
learning fashion or enriched with
additional supervision, e.g. syntax-sensitive dependencies to create a model
that is more general or better suited for certain
downstream tasks, ideally in a weakly-supervised
manner to retain its universal properties.
Another direction is to apply the method to
novel tasks and models. While an extension to
sequence labeling is straightforward, other tasks
with more complex interactions such as entailment
or question answering may require novel ways to
pretrain and ﬁne-tune.
Finally, while we have
provided a series of analyses and ablations, more
studies are required to better understand what
knowledge a pretrained language model captures,
how this changes during ﬁne-tuning, and what information different tasks require.
Conclusion
We have proposed ULMFiT, an effective and extremely sample-efﬁcient transfer learning method
that can be applied to any NLP task. We have also
proposed several novel ﬁne-tuning techniques that
in conjunction prevent catastrophic forgetting and
enable robust learning across a diverse range of
tasks. Our method signiﬁcantly outperformed existing transfer learning techniques and the stateof-the-art on six representative text classiﬁcation
tasks. We hope that our results will catalyze new
developments in transfer learning for NLP.
Acknowledgments
We thank the anonymous reviewers for their valuable feedback.
Sebastian is supported by Irish
Research Council Grant Number EBPPG/2014/30
and Science Foundation Ireland Grant Number
SFI/12/RC/2289.