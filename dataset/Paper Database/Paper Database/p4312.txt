Mitigating the eﬀects of measurement noise on Granger causality
Hariharan Nalatore,1, ∗Govindan Rangarajan,2, † and Mingzhou Ding1, ‡
1The J. Crayton Pruitt Family Department of Biomedical Engineering,
University of Florida, Gainesville, FL 32611, USA
2 Department of Mathematics, Indian Institute of Science, Bangalore - 560 012, India
Computing Granger causal relations among bivariate experimentally observed time series has
received increasing attention over the past few years. Such causal relations, if correctly estimated,
can yield signiﬁcant insights into the dynamical organization of the system being investigated.
Since experimental measurements are inevitably contaminated by noise, it is thus important to
understand the eﬀects of such noise on Granger causality estimation. The ﬁrst goal of this paper
is to provide an analytical and numerical analysis of this problem. Speciﬁcally, we show that, due
to noise contamination, (1) spurious causality between two measured variables can arise and (2)
true causality can be suppressed. The second goal of the paper is to provide a denoising strategy to
mitigate this problem. Speciﬁcally, we propose a denoising algorithm based on the combined use of
the Kalman ﬁlter theory and the Expectation-Maximization (EM) algorithm. Numerical examples
are used to demonstrate the eﬀectiveness of the denoising approach.
PACS numbers: 05.40.a, 87.19.La, 84.35.+i, 02.50.Sk
INTRODUCTION
Granger causality has become the method of choice to determine whether and how two time series exert causal
inﬂuences on each other. In this method one starts by modeling simultaneously acquired time series as coming from
a multivariate or vector autoregressive (VAR) stochastic process. One time series is said to have a causal inﬂuence on
the other if the residual error in the autoregressive model of the second time series (at a given point of time) is reduced
by incorporating past measurements from the ﬁrst. This method and related methods have found applications in a
wide variety of ﬁelds including physics , economics and neuroscience . Its nonlinear
extension has recently appeared in and has been applied to study problems in condensed matter physics .
The statistical basis of Granger causality estimation is linear regression. It is known that regression analysis is sensitive to the impact of measurement noise . Given the inevitable occurrence of such noise in experimental time series,
it is imperative that we determine whether and how such added noise can adversely aﬀect Granger causality estimation. Previous studies have suggested that such adverse eﬀects can indeed occur. In this paper, we make further
progress by obtaining analytical expressions that explicitly demonstrate how the interplay between measurement noise
and system parameters aﬀects Granger causality estimation. Moreover, we show how this deleterious eﬀect of noise
can be reduced by a denoising method, which is based on the Kalman ﬁlter theory and the Expectation-Maximization
(EM) algorithm. We refer to our denoising algorithm as the KEM (Kalman EM) denoising algorithm.
The organization of this paper is as follows. In Section 2, we start by introducing an alternative formulation of
Granger causality and proceed to outline a framework within which the eﬀects of added (measurement) noise on
the estimation of directional inﬂuences in bivariate autoregressive processes can be addressed. To simplify matters,
we then consider a bivariate ﬁrst order autoregressive (AR(1)) process in Section 3. Here explicit expressions for the
eﬀect of noise on Granger causality are derived. These expressions allow us to show that, for two time series that are
unidirectionally coupled, spurious causality can arise when noise is added to the driving time series and true causality
can be suppressed by the presence of noise in either time series. The theoretical results are illustrated by numerical
simulations. In Section 4, we brieﬂy introduce the KEM denoising algorithm and apply it to the example considered
in Section 3. Our results show that the KEM algorithm can mitigate the eﬀects of noise and restore the true causal
relations between the two time series. In section 5, we consider a coupled neuron model which produces time series
that closely resemble that recorded in neural systems. The eﬀect of noise on Granger causality and the eﬀectiveness
of the KEM algorithm in mitigating the noise eﬀect are illustrated numerically. Our conclusions are given in Section
∗Electronic address: 
†Electronic address: 
‡Electronic address: 
THEORETICAL FRAMEWORK
Consider two time series X(t) and Y (t). To compute Granger causality, we model them as a combined bivariate
autoregressive process of order p. In what follows, the model order p is assumed to be known, since this aspect is not
central to our analysis. The bivariate autoregressive model can then be represented as:
[akX(t −k) + bkY (t −k)] = E1(t),
[ckX(t −k) + dkY (t −k)] = E2(t),
where ak, bk, ck, and dk are the AR coeﬃcients and Ei(t) are the temporally uncorrelated residual errors.
For our purposes, it is more convenient to rewrite the above bivariate process as two univariate processes (this can
always be done according to ):
P1(B)X(t) = ξ(t);
P2(B)Y (t) = η(t),
where B is the lag operator deﬁned as BkX(t) = X(t −k) and P1 and P2 are polynomials (of possibly inﬁnite order)
in the lag operator B. It should be noted that the new noise terms ξ(t) and η(t) are no longer uncorrelated. Let
γ12(k) denote the covariance at lag k between these two noises.
γ12(k) ≡cov(ξ(t)), η(t −k))
k = ..., −1, 0, 1... .
A theorem by Pierce and Haugh states that Y (t) causes X(t) in Granger sense if and only if
γ12(k) ̸= 0 for some k > 0.
Similarly X(t) causes Y (t) if and only if γ12(k) ̸= 0 for some k < 0.
Now we add measurement noises ξ′(t) and η′(t) to X(t) and Y (t) respectively:
X(c)(t) = X(t) + ξ′(t),
Y (c)(t) = Y (t) + η′(t).
Here ξ′(t), η′(t) are uncorrelated white noises that are uncorrelated with X(t), Y (t), ξ(t) and η(t). Following Newbold
 , the above equations can be rewritten as
P1(B)X(c)(t) = P1(B)X(t) + P1(B)ξ′(t),
P2(B)Y (c)(t) = P2(B)Y (t) + P2(B)η′(t).
Using Eq. (3) we get
P1(B)X(c)(t) = ξ(t) + P1(B)ξ′(t),
P2(B)Y (c)(t) = η(t) + P2(B)η′(t).
Following the procedure in Granger and Morris , the linear combination of white noise processes on the right
hand sides can be rewritten in terms of invertible moving average processes :
ξ(t) + P1(B)ξ′(t) = P3(B)ξ(c)(t),
η(t) + P2(B)η′(t) = P4(B)η(c)(t),
where ξ(c) and η(c) are again uncorrelated white noise processes. Thus we get
(B)P1(B)X(c)(t) = ξ(c)(t),
(B)P2(B)Y (c)(t) = η(c)(t).
This is again in the form of two univariate AR processes. Therefore the theorem of Pierce and Haugh can be applied
to yield the result that the noisy signal Y (c)(t) causes X(c)(t) in Granger sense if and only if
12 (k) ≡cov(ξ(c)(t), η(c)(t −k)) ̸= 0,
for some k > 0. Similarly X(c)(t) cause Y (c)(t) if and only if
12 (k) ̸= 0,
for some k < 0.
We can relate γ(c)
12 to γ12 as follows. Consider the corresponding covariance generating functions (which are nothing
but the z-transforms of the cross-covariances)
We can show that 
12 (z) = P −1
(z−1)¯γ12(z).
Even if γ12(k) = 0 for all k < 0 (i.e. X does not cause Y ) it is possible that γ(c)
12 (k) ̸= 0 for some negative k because
of the additional term P −1
(z−1) that has been introduced by the measurement noise. This gives rise to the
spurious Granger causality, (X(c) causes Y (c)), which is a consequence of the added measurement noise.
A BIVARIATE AR(1) PROCESS
In the previous section, we demonstrated that measurement noise can aﬀect Granger causality. But the treatment
given was quite general in nature. In this section we specialize to a simple bivariate AR(1) process and obtain explicit
expressions for the eﬀect of noise on Granger causality.
Consider the following bivariate AR(1) process
X(t) = aX(t −1) + bY (t −1) + E1(t),
Y (t) = dY (t −1) + E2(t).
From the above expressions, it is clear that Y drives X for nonzero values of b and X does not drive Y in this model.
More speciﬁcally, we see that Y at an earlier time t−1 aﬀects X at the current time t. There is no such corresponding
inﬂuence of X on Y .
When noises ξ′(t) and η′(t) with variances σ2
η′, respectively, are added to the data generated by Eq. (19),
after some algebra (see Appendix for details), we ﬁnd the following expressions for P3(B) and P4(B):
P3(B) = 1 + a′
P4(B) = 1 −d
d + d) + 1
The expressions for a′
2 are very long and for our purposes it is suﬃcient to note that they go to zero as the
added noise goes to zero (as expected). We see that |s| > 2 for any value of d, σ2
η′. Therefore
hence d ′ are well deﬁned. We also have the following results:
a) As |d| →0, |d
′| < |d| →0;
b) As d →1, d
c) As the ratio
d) As the ratio
Substituting the expressions for P3(B) and P4(B) in Eq. (18) we get
12 (z) = (1 + a
2z2)−1(1 −d
′z−1)−1¯γ12(z).
We now expand both sides in powers of z:
· · · + γ(c)
12 (−1)z−1 + γ(c)
12 (0) + γ(c)
12 (1)z + · · · = (1 −a
2)z2 + · · ·)
′2z−2 + · · ·)(· · · + γ12(−1)z−1 + γ12(0) + γ12(1)z + · · ·).
Collecting terms proportional to z−1, z0, z1 etc., we obtain the following expressions for the cross covariances at lag
-1, 0 and 1:
12 (−1) = d
′ + . . .)(γ12(0) + d
′γ12(1) + . . .),
12 (0) = (1 −a
′ + . . .)(γ12(0) + d
′γ12(1) + . . .),
12 (1) = γ12(1) −a
1γ12(0) −a
′γ12(1) + . . . .
We observe that γ(c)
12 (k) for k < 0 (and in particular, γ(c)
12 (−1)) is no longer zero, implying that the X(c) drives Y (c),
thus giving rise to a spurious causal direction. The spurious causality term γ(c)
12 (−1) is proportional to d
′. This can
be shown to be true for all the other spurious terms γ(c)
12 (k), k < −1 as well. Hence they all go to zero if d
(i.e. if Y has no measurement noise). This happens even if a
2 are non-zero (i.e. even if X measurement is
contaminated by noise). Hence we arrive at an important conclusion that if Y is driving X, only measurement noise
in Y can cause spurious causality. If Y has no measurement noise, no amount of measurement noise in X can lead to
spurious causality. Further, using the asymptotic properties of d
′ listed earlier, we can easily see that the magnitude
of the spurious causality increases as d →1 and as the ratio σ2
The foregoing demonstrates that noise can lead to spurious causal inﬂuences that are not part of the underlying
processes. Here we show that the true causality terms (γ12(k) for k > 0) are also modiﬁed by the presence of noise.
They undergo a change even if d
′ = 0. For example, γ12(1) is changed to γ12(1)−a
1γ12(0) even if d
′ = 0. Therefore,
it is quite possible that even a true causal direction can be masked by added noise and the measurement noises in
both time series contribute to this suppression. As the ratios σ2
′ all go to zero and
12 →γ12, as expected.
We make one ﬁnal observation. If we replace z by ei2πf (where f is the frequency) in the covariance generating
function [cf. Eq. (17)] we obtain the cross spectrum. Hence all the above results carry over to the spectral/frequency
To illustrate the above theoretical results, we estimate Granger causality spectrum (in the frequency domain) for a
bivariate AR process numerically. First, we brieﬂy summarize the theory behind this computation . The bivariate
AR process given in Eq. (1) can be written as:
A(k)Z(t −k) = E(t),
where Z(t) = [X(t), Y (t)]T ; E(t) = [E1(t), E2(t)]T and
for 1 ≤k ≤p. A(0) is the 2×2 identity matrix. Here, E(t) is a temporally uncorrelated residual error with covariance
matrix Σ. We obtain estimates of the coeﬃcient matrices A(k) by solving the multivariate Yule-Walker equations 
using the Levinson-Wiggins-Robinson (LWR) algorithm . From A(k) and Σ we estimate the spectral matrix S(f)
by the relation
S(f) = H(f)ΣH∗(f),
where H(f) = [Pp
k=0 A(k)e−2πikf ]−1 is the transfer function of the system.
The Granger causality spectrum from Y to X is given by (see also )
IY →X(f) = −ln[1 −
(Σ22 −Σ122
Σ11 )|H12(f)|2
Here, Σ11, Σ22 and Σ12 are the elements of Σ and S11(f) is the power spectrum of X at frequency f. Hij(f) is the
{ij}th element of the transfer function matrix H(f). Similarly, the Granger causality spectrum from X to Y is deﬁned
IX→Y (f) = −ln[1 −
)|H21(f)|2
and S22(f) is the power spectrum of Y at frequency f.
We now estimate the Granger causality spectrum for the speciﬁc AR(1) process given in Eq. (19) where Y drives
X and X does not drive Y . The parameter values used are a = 0.4, b = 0.6, d = 0.9, σξ = 0.2 and ση = 1.0. We
obtain two time series X and Y by numerically simulating the VAR model and then adding Gaussian measurement
noise with σξ′ = 0.2 and ση′ = 2.5. For concreteness we assume that each time unit corresponds to 5 ms. In other
words, the sampling rate is 200 Hz, and thus the Nyquist frequency is 100 Hz. The dataset consists of one hundred
realizations, each of length 250 ms (50 points). These 100 realizations are used to obtain expected values of the
covariance matrices in the LWR and KEM algorithms (see next section). The Granger causality spectra IX→Y (f)
and IY →X(f) are plotted in Figure 1. The solid lines represents the true causality spectra while the dashed lines
represent the noisy causality spectra.
Similarly, we also simulated the following bivariate AR(2) process:
X(t) = aX(t −1) + bY (t −1) + E1(t),
Y (t) = d1Y (t −1) + d2Y (t −2) + E2(t).
The values of the parameters a and b used were the same as in the previous AR(1) process example (Eq. 19) except
for the values of the new parameters d1 and d2 which were chosen to be 0.4 and 0.5 respectively. We again obtain
two time series X and Y and then added Gaussian measurement noise with σξ′ = 0.2 and ση′ = 2.5 to X and Y
respectively. The Granger causality spectra IX→Y (f) and IY →X(f) are plotted in Figure 2. As before, the solid lines
and dashed lines represent the true causality spectra and noisy causality spectra, respectively.
We observe that the measurement noise has a dramatic eﬀect in both of these cases: It completely reverses the true
causal directions. For the noisy data, X appears to drive Y and Y does not appear to drive X.
The above theoretical and numerical results bring out clearly the adverse eﬀect that noise can have on correctly
determining directional inﬂuences. The same is also true for other quantities like power spectrum and coherence.
Therefore it is imperative that the eﬀect of noise be mitigated to the extent possible.
THE KEM DENOISING ALGORITHM
In the previous section we have seen that noisy data can lead to grave misinterpretation of directional inﬂuences.
We now provide a practical solution to this problem by combining the Kalman smoother with the Expectation-
Maximization algorithm . The detailed algorithm is long and tedious. We outline the main logical steps below.
Kalman ﬁlter is a standard algorithm for denoising noisy data. To apply this, we ﬁrst need to recast a VAR
process with measurement noise in the so-called state-space form. This is nothing but the diﬀerence equation analogue
of converting a higher order diﬀerential equation to a system of ﬁrst order diﬀerential equations. Once this is done,
our VAR model takes on the following form:
xt+1 = Axt + wt+1,
yt = Cxt + vt.
Here xt is an M × 1 (“true”) state vector at time t. A is an M × M state matrix. wt is a zero mean Gaussian
independent and identically distributed random variable with covariance matrix Q.
Bivariate AR(p) models can
be put in the form xt+1 = Axt + wt+1 by deﬁning M = 2p auxiliary variables xi,t. The N × 1 vector yt is the
observed/measured value of xt in N channels. C is an N × M observation matrix and is a ﬁxed, known matrix for
VAR models. Hence we will ignore this in future discussions. The N × 1 vector vt is the measurement noise which is
zero mean, Gaussian, independent and identically distributed with covariance matrix R.
Kalman ﬁlter, however, can not be directly applied to denoise experimental or observed data since it assumes the
knowledge of the model describing the state space dynamics. In practice, such knowledge is often not available. To
get around this problem, we apply the Kalman smoother in conjunction with the Expectation and Maximization
algorithm . Thus, this denoising algorithm will henceforth be called the KEM algorithm. In this
algorithm, one follows the standard procedure for estimating state space parameters from data using the maximum
likelihood method. The appropriate likelihood function in our case is the joint log likelihood logP({x}, {y}) where
{x} denotes {xt} (for all t) and similarly for {y}. In the usual maximum likelihood method, P would not depend on
x and we would therefore maximize the above quantity directly (conditioned on the observed yt values) and obtain
the unknown state space parameters. But in our case, P depends on x which is also unknown. To get rid of x, we
take the expected value of the log likelihood
O = E[log P({x}, {y}) | {y}].
As usual, we have conditioned the expectation on the known observations {y}.
To compute O, it turns out we need the expectations of x and xxT (where T denotes the transpose) conditioned
on y. These expectations are obtained by applying the Kalman smoother on the noisy data. We use the Kalman
smoother and not the Kalman ﬁlter since we are utilizing all the observations y instead of only the past observations.
This is the appropriate thing to do in our case since we are performing an oﬀ-line analysis where all observations are
known. In other words, in Kalman smoother, we perform both a forward pass and a backward pass on the data in
order to make use of all observations.
To apply the Kalman smoother, however, we still need the state space model parameters (just as in the Kalman
ﬁlter case). To circumvent this problem, we start with initial estimates for these parameters (A, Q and R) as follows.
From the noisy data, using the LWR algorithm, we obtain the VAR model coeﬃcient matrices . Then a standard
transformation is used to put these matrices in the state space form giving the initial estimate for A. The initial
estimate of Q is taken to be the identity matrix following the standard procedure . The initial estimate of R is
taken to be half the covariance matrix at lag zero of the noisy data. The approximate model order can be determined
by applying the AIC criterion in the LWR algorithm. This step is admittedly rather ad hoc. Further studies to
optimize the above initial estimates and the VAR model order p are currently being carried out. Once we have initial
estimates of the model parameters, we can apply the Kalman smoother to obtain the various conditional expectations
and evaluate the expected log likelihood O. This is called the expectation (E) step.
Next, we go to the maximization (M) step. Each of the parameters A, Q, R etc is re-estimated by maximizing
O. Using these improved estimates, we can apply the E step again followed by the M step. This iterative process
is continued till the value of log likelihood function converges to a maximum. We could now directly use the VAR
parameters estimated from the KEM algorithm for further analysis as is usually done. But here we prefer to use the
following procedure which was found to yield better performance. The ﬁnal denoised data (that is, the estimate of x
obtained from the KEM algorithm) is treated as the new experimental time series and subjected to parametric spectral
analysis from which Granger causality measures can be derived. The Matlab code implementing this algorithm for
our applications is available from the authors upon request.
We have compared the denoising capabilities of the KEM algorithm with two widely used algorithms, the higherorder Yule-Walker (HOY) method and the overdetermined higher-order Yule-Walker method . We ﬁnd that
the denoising capabilities of the KEM algorithm is superior. Detailed results will be presented elsewhere. In Figure
3, we explicitly show that KEM algorithm performs better than the HOY method (see below).
The KEM algorithm is applied to denoise the data shown in Figures 1 and 2. Figure 3 displays the same exact
Granger causality spectra (solid lines) as that in Figure 1 and the Granger causality spectra (dashed lines) obtained
from the denoised data using KEM algorithm. Causality spectra obtained using HOY method is also shown (as dotted
lines). It is clear that the KEM method performs better. In Figure 4, the solid lines again represent the same exact
Granger causality as that in Figure 2 and the dashed lines represent the Granger causality spectra obtained from
the denoised data of a bivariate AR(2) process. We see that the correct causal directions are recovered and that the
denoised spectra are reasonably close to the true causality spectra for both AR(1) and AR(2) process. We stress that
these results are achieved without assuming any knowledge of the VAR models [Eqs. 19 and 33] that generated the
original time series data.
CAUSAL RELATIONS IN A NEURAL NETWORK MODEL
In this section, we analyze the eﬀect of noise on time series generated by a neural network model.
demonstrate the eﬀect of measurement noise on causality directions and then the eﬀect of applying the KEM algorithm
on the noisy data.
Our simulation model comprises two coupled cortical columns where each column is made up of an excitatory and
an inhibitory neuronal population . The equations governing the dynamics of the two columns are given by
¨xi + (a + b) ˙xi + abxi = −keiQ(yi(t), Qm0) + kijQ(xj(t), Qm0) + ξxi(t),
¨yi + (a + b) ˙yi + abyi = kieQ(xi(t), Qm0) + ξyi(t),
where i ̸= j = 1, 2. Here x and y represent local ﬁeld potentials (LFP) of the excitatory and inhibitory populations
respectively, kie > 0 gives the coupling gain from the excitatory (x) to the inhibitory (y) population, and kei > 0 is the
strength of the reciprocal coupling. The neuronal populations are coupled through a sigmoidal function Q(x, Qm0)
which represents the pulse densities converted from x with Qm0 a modulatory parameter. The function Q(x, Qm0) is
Q(x, Qm0) =
Qm0[1 −e−(ex−1)/Qm0] if x > −u0
where u0 = −ln[1 + ln(1 +
Qm0 )]. The coupling strength kij is the gain from the excitatory population of column j to
the excitatory population of column i, with kij = 0 for i = j. The terms ξ(t) represent independent Gaussian white
noise inputs given to each neuronal population.
The parameter values used were: a = 0.22/ms, b = 0.72/ms, kie = 0.1, kei = 0.4, k12 = 0, k21 = 0.25 and Qm0 = 5.
The standard deviation for the Gaussian white noise was chosen as 0.2. Assuming a sampling rate of 200Hz, two
hundred realizations of the signals were generated, each of length 30 s (6,000 points).
We now restrict our attention to the variables x1(t) and x2(t). Measurement noises (Gaussian white noises with
standard deviations 2.0 and 3.0 respectively) were added to these variables. From the model it is clear that x1(t)
should drive x2(t) since k12 = 0 while k21 = 0.25. The results of applying Granger causality analysis (using a VAR
model of order 7) on these two variables is shown in Figure 5. The solid lines represent the causality spectra for the
noise-free data. The dashed lines represents the causality spectra for the noisy data. It is clear that the measurement
noise has an eﬀect on the causal relations by signiﬁcantly reducing the true causality magnitude. In contrast to the
example in Section 3, however, no spurious causal direction is generated here, despite the fact that both time series
are contaminated by measurement noise. Next, we applied the KEM algorithm to denoise the noisy data. When
Granger causality analysis is performed on the denoised data, we obtain causality spectra that are closer to the true
causality spectra (see Figure 6). We note that the KEM algorithm is not able to completely remove the noise as the
denoised spectra are still quite diﬀerent from the true spectra.
To show that the denoised Granger spectrum is signiﬁcantly diﬀerent from that of the noisy data we use the
bootstrap approach to establish the signiﬁcant diﬀerence between the two peaks observed in Granger causality
spectrum of Figures 5 and 6 (shown by dashed lines in these Figures). One thousand resamples of noisy data and the
denoised data were generated by randomly selecting trials with replacement. It should be noted that in any selected
trial, the entire multichannel data is taken as it is thus preserving the auto and cross correlation structures. Thus,
we employ a version of block bootstrap method . The peak values of Granger causality were computed for each
resample using both noisy data and denoised data. Let us denote these peak values by the random variables Z1 and
Z2 respectively. The two population Student t-test was performed to determine whether the means of Z1 and Z2 are
diﬀerent at a statistically signiﬁcant level.
The null hypothesis was that the means of the two populations Z1 and Z2 are equal. The t value was found to
be very large: 4.6446 ∗103 and corresponds to a two-tailed p value less than 0.0001. Thus the null hypothesis that
the two groups do not diﬀer in mean is rejected. This establishes the fact that the peak of the Granger causality
spectrum of the denoised data is signiﬁcantly higher than that of the noisy data. Figure 7 shows the plot of Granger
causality for the direction x1 →x2 along with 95% conﬁdence intervals. The 95% conﬁdence intervals are calculated
as Ix1→x2(f) ± 1.96σB (for each frequency f) where σB is the sample standard deviation of the 1000 bootstrap
replications of Ix1→x2(f).
CONCLUSIONS
Our contributions in this paper are two fold.
First, we demonstrate that measurement noise can signiﬁcantly
impact Granger causality analysis.
Based on analytical expressions linking noise strengths and the VAR model
parameters, it was shown that spurious causality can arise and that true causality can be suppressed due to noise
contamination. Numerical simulations were performed to illustrate the theoretical results. Second, a practical solution
to the measurement noise problem, called the KEM algorithm, was outlined, which combines the Kalman ﬁlter theory
with the Expectation and Maximization (EM) algorithm. It was shown that the application of this algorithm to
denoise the noisy data can signiﬁcantly mitigate the deleterious eﬀects of measurement noise on Granger causality
estimation. It is worth noting that, despite the fact that the adverse eﬀect of measurement noise on Granger causality
has been known since 1978 , mitigation of such eﬀect has received little attention. The KEM algorithm described
in this paper is our attempt at addressing this shortcoming.
Acknowledgements
This work was supported by NIH grant MH071620. GR was supported in part by grants from DRDO and UGC
(under DSA-SAP Phase IV). GR is also a Honorary Faculty Member of the Jawaharlal Nehru Centre for Advanced
Scientiﬁc Research, Bangalore.
In this appendix, we derive the expressions for P3(B) and P4(B) given in Eq. (20). We ﬁrst determine P4(B).
When a zero mean white noise process η′(t) with variance σ2
η′ is added to Y (t) we get
Y (c)(t) = Y (t) + η′(t).
Applying (1 −dB) on both sides of the above equation we get
(1 −dB)Y (c)(t) = (1 −dB)Y (t) + (1 −dB)η′(t)
= η(t) + (1 −dB)η′(t).
We now determine a white noise process η(c)(t) such that
η(t) + (1 −dB)η′(t) = (1 −d′B)η(c)(t).
We need to determine d′ and σ2
Taking variances on both sides of the above equation we get
η + (1 + d2)σ2
η′ = (1 + d′2)σ2
Taking autocovariance at lag 1 on both sides we obtain
Since η(c) is a sum of η and (1 −dB)η′, we have σ2
η′. This implies that |d′| < |d|. Since stationarity of the AR
process requires 0 < |d| < 1, we obtain the inequality 0 < |d′| < |d′| < 1. Further d′ has the same sign as d.
Substituting in the variance equation we get
η′ = (1 + d2)σ2
d′ + d′) = (1
d + d) + 1
d + d) + 1
This gives
d′ + d′) = s.
Note that |s| > 2 for any value of d, σ2
η′. Therefore
s2 −4 and hence d′ are well deﬁned. Further, since
|d′| < |d| if d is positive, d′ = (s −
s2 −4)/2 is the only valid solution. If d is negative, d′ = (s +
s2 −4)/2 is the
only valid solution.
Next, we derive the expression for P3(B). First, we ﬁrst need to rewrite X(t) as an univariate process i.e. we need
to determine P1(B):
P1(B)X(t) = ξ(t),
where ξ(t) is a zero mean white noise process and
X(t) = aX(t −1) + bY (t −1) + E1(t).
Here E1(t) is a zero mean white noise process with variance σ2
ǫ. We have already seen that
(1 −dB)Y (t) = η(t).
The equation for X(t) can be written as
(1 −aB)X(t) = bY (t −1) + E1(t).
Substituting the expression for Y (t −1) we obtain
(1 −aB)X(t) = b(1 −dB)−1η(t −1) + E1(t).
We now ﬁnd a white noise process ξ(t) with variance σ2
ξ such that
b(1 −dB)−1η(t −1) + E1(t) = (1 −rB)−1ξ(t).
To determine r and σ2
ξ, we take variance and autocovariance at lag 1 on both sides. Taking variance we obtain
(1 −d2) + σ2
Taking autocovariance at lag 1 and assuming that σǫη (the cross-covariance between E1 and ξ) is zero for simplicity,
which can be written as
(1 −d2)r .
Substituting in the variance equation we obtain
(1 −d2) + σ2
b2σ2η + (1 −d2)σ2
If b = 0, we get r = 0 and σ2
1 as expected. Similarly if d = 0, we get r = 0 and σ2
η as expected.
Once r is known, σ2
ξ is given by
ξ = (1 −r2)
(1 −d2) + σ2
We ﬁnally have
(1 −aB)X(t) = (1 −rB)−1ξ(t).
P1(B)X(t) = ξ(t),
P1(B) = (1 −rB)(1 −aB).
Consider a white noise process ξ′(t) (which is uncorrelated with X(t)) and has variance σ2
ξ′. This is added to X(t)
to obtain the noisy process X(c)(t):
X(c)(t) = X(t) + ξ′(t).
Applying P1(B) on both sides of the above equation,
P1(B)X(c)(t) = ξ(t) + P1(B)ξ′(t).
We need to ﬁnd a zero mean white noise process ξ(c)(t) with variance σ2
ξ(c) such that
ξ(t) + P1(B)ξ′(t) = P3(B)ξ(c)(t).
P3(B) = 1 + a
ξ(t) + (1 −(a + r)B + arB2)ξ′(t) = [1 + a
2B2]ξ(c)(t).
Taking variances on both sides we get
ξ + (1 + (a + r)2 + a2r2)σ2
ξ′ = [1 + a′ 2
Taking autocovariance at lag 1 on both sides we obtain
−(a + r)σ2
ξ′ −ar(a + r)σ2
This can be rewritten as
−(a + r)(1 + ar)σ2
Taking autocovariance at lag 2 on both sides
which gives
ξ′, we see that |a
2| < |ar| and a
2 has the same sign as ar.
Substituting the last equation in Eqs. (70) and (68) we obtain
−(a + r)(1 + ar)σ2
ξ + [1 + (a + r)2 + a2r2]σ2
ξ′ = [1 + a′2
Thus we get
= −(a + r)(1 + ar)
= [1 + (a + r)2 + a2r2]
We can solve these two equations for a
2. There will be multiple solutions. We choose that solution for which
2| < |ar|. Further the solution has to be such that the roots of 1 + a
2B2 = 0 lie outside the unit circle. The
last condition is required for the invertibility of the MA process (1 + a
2B2)ξ(c)(t). The expressions for a
2 obtained by solving the above equations are very long and therefore we do not list them here. However, we can
easily obtain the asymptotic behaviour of these solutions as follows.
For our bivariate AR(1)process to be stable, we require that the roots of
det[λI −A(1)] = 0
lie within the unit circle i.e., the eigenvalues of A(1) should have absolute value less than 1. In our case
which is an upper triangular matrix. Hence eigenvalues are a and d. Therefore, for stability we require that |a| < 1
and |d| < 1.
As already derived, we have
b2σ2η + (1 −d2)σ2
Since |d| < 1, the term within brackets is always positive and less than 1. It becomes zero only when b = 0. Hence
|r| < |d| and r has same sign as d. As |d| →1, |r| →1. As |d| →0 or |b| →0, we see that |r| →0.
We have already seen that |a
2| < |ar|. Since |r| < |d|, we obtain further results that |a
2| < |a||d| and a
2 has same
sign as ad. Since |a|, |d| < 1, we get
2| < |a||d| < 1.
As |a|, |d| →1, |a
2| also →1. As a →1, d →1 and the ratio σ2
ξ′ →0, we have
As the variance ratio →∞
as expected. The parameter a
1 is hardly aﬀected by the value of the parameter b. On the other hand, a
b →0 and saturates rapidly for b > 0.5.
 C. W. J. Granger, Econometrica 37, 424 .
 K. J. Blinowska, R. Kus and M. Kaminski, Phys. Rev. E 70, 050902 .
 D. Marinazzo, M. Pellicoro and S. Stramaglia, Phys. Rev. E 73 066216 .
 P. F. Verdes, Phys. Rev. E 72, 026222 .
 N. G. Rosenblum and A. S. Pikovsky, Phys. Rev. E 64 045202 .
 X. Hu and V. Nenov, Phys. Rev. E 69 026206 .
 L. M. Xu, Z. Chen, K. Hu, H. E. Stanley and P. Ch. Ivanov, Phys. Rev. E 73 065201 .
 M. Ding, S. L. Bressler, W. Yang, and H. Liang, Biol. Cyber. 84, 463 .
 A. Brovelli, M. Ding, A. Ledberg, Y. Chen, R. Nakamura, and S. L. Bressler, Proc. Natl. Acad. Sci. USA 101, 9849 .
 D. L. Thornton and D. S. Batten, Journal of Money, Credit and Banking 17, 164 .
 T. E. Hall and N. R. Noble, Journal of Money, Credit and Banking 19, 112 .
 C. Hiemstra and J. D. Jones, Journal of Finance 49, 1639 .
 Y. Chen, G. Rangarajan, J. Feng, and M. Ding, Physics Letters A 324, 26 .
 R. Ganapathy, G. Rangarajan, and A. K. Sood, Phys. Rev. E 75, 016211 .
 W. A. Fuller, Measurement Error Models, .
 P. Newbold, Int. Econ. Rev. 19, 787 .
 D. A. Pierce and L. D. Haugh, J. Econometrics 5, 265 .
 C. W. J. Granger and M. J. Morris, J. Royal Statist. Soc. Ser. A 139, 246 .
 A. Maravall and A. Mathis, J. Econometrics 61, 197 .
 C. Chatﬁeld, The Analysis of Time Series, .
 M. Morf, A. Vieira, D. Lee, and T. Kailath, IEEE Trans Geoscience Electronics 16, 85 .
 J. Geweke, J. Amer. Statist. Assoc. 77, 304 .
 Y. Hosoya, Prob. Th. Related Fields 88, 429 .
 A. P. Dempster, N. M. Laird, and D. B. Rubin, J. Royal Statist. Soc. Ser. B 39, 1 .
 S. Haykin, Adaptive Filter Theory .
 Z. Gahramani and G. E. Hinton, Technical Report CRG-TR-96-2, 1996.
 E. Weinstein, A. V. Oppenheim, M. Feder, and J. R. Buck, IEEE Trans Signal Proc. 42, 846 .
 V. Digalakis, J. R. Rohlicek, and M. Ostendorf, IEEE Trans Speech Audio Proc. 1, 431 .
 H. Akaike, IEEE Trans Autom Control AC-19, 716 .
 Y. T. Chan and R. Langford, IEEE Trans. Acoustics, Speech and Signal Proc. 30, 689 .
 J. A. Cadzow, Proc. IEEE 70, 907 .
 M. Kaminski, M. Ding, W. A. Truccolo, and S. L. Bressler, Biol. Cybern. 85, 145 .
 B. Efron, The Jackknife,the Bootstrap, and Other Repsampling Plans .
Granger Causality
Frequency (Hz)
Granger Causality
Before denoising
Before denoising
FIG. 1: Granger causality spectra for a bivariate AR(1) process (a) Causality of X →Y (b) Causality of Y →X. The solid
lines represent true causality spectra and the dashed lines represent spectra from noisy data.
Granger Causality
Frequency (Hz)
Granger Causality
Before denoising
Before denoising
FIG. 2: Granger causality spectra for a bivariate AR(2) process (a) Causality of X →Y (b) Causality of Y →X. The solid
lines represent true causality spectra and the dashed lines represent spectra from noisy data.
Granger Causality
After denoising
Denoising by HOY
Frequency (Hz)
Granger Causality
After denoising
Denoising by HOY
FIG. 3: Granger causality spectra for the bivariate AR(1) process in Fig 1. (a) Causality of X →Y (b) Causality of Y →X.
The solid lines represent true causality spectra and the dashed lines represent spectra obtained from the denoised data using
the KEM algorithm. The dotted lines represent spectra obtained using HOY algorithm.
Granger Causality
Frequency (Hz)
Granger Causality
After denoising
After denoising
FIG. 4: Granger causality spectra for the bivariate AR(2) process in Fig 2. (a) Causality of X →Y (b) Causality of Y →X.
The solid lines represent true causality spectra and the dashed lines represent spectra obtained from the denoised data using
the KEM algorithm.
Granger Causality
Frequency (Hz)
Granger Causality
Before denoising
Before denoising
FIG. 5: Granger causality spectra for noisy data from a neural network model (a) Causality of x1 →x2 (b) Causality of
x2 →x1. The solid lines represent true causality spectra (noise-free data) and the dashed lines represent spectra from noisy
Granger Causality
Frequency (Hz)
Granger Causality
After denoising
After denoising
FIG. 6: Granger causality spectra of the neural network model (a) Causality of x1 →x2 (b) Causality of x2 →x1. The solid
lines represent true causality spectra (noise-free data) and the dashed lines represent spectra obtained from denoised data using
the KEM algorithm.
Frequency (Hz)
Granger Causality
After Denoising
Before Denoising
FIG. 7: Granger causality spectra of the neural network model for the direction x1 →x2. The solid line represents the Granger
causality for denoised data, while the dashed line represents the Granger causality for noisy data. 95% conﬁdence intervals are
also given..