Characterizing and Predicting Blocking Bugs in Open Source Projects
Harold Valdivia-Garcia, Emad Shihab, Mei Nagappan
S0164-1212(18)30053-0
10.1016/j.jss.2018.03.053
Reference:
To appear in:
The Journal of Systems & Software
Received date:
20 March 2017
Revised date:
17 February 2018
Accepted date:
21 March 2018
Please cite this article as: Harold Valdivia-Garcia, Emad Shihab, Mei Nagappan, Characterizing and
Predicting Blocking Bugs in Open Source Projects, The Journal of Systems & Software , doi:
10.1016/j.jss.2018.03.053
This is a PDF ﬁle of an unedited manuscript that has been accepted for publication. As a service
to our customers we are providing this early version of the manuscript. The manuscript will undergo
copyediting, typesetting, and review of the resulting proof before it is published in its ﬁnal form. Please
note that during the production process errors may be discovered which could affect the content, and
all legal disclaimers that apply to the journal pertain.
 
 
Characterizing and Predicting Blocking Bugs in Open
Source Projects
Harold Valdivia-Garciaa,∗, Emad Shihabb, Mei Nagappanc
aBloomberg LP
bConcordia University, Montreal, Canada
cUniversity of Waterloo Waterloo, Ontario Canada
Software engineering researchers have studied speciﬁc types of issues such reopened bugs, performance bugs, dormant bugs, etc. However, one special type
of severe bugs is blocking bugs. Blocking bugs are software bugs that prevent
other bugs from being ﬁxed. These bugs may increase maintenance costs, reduce
overall quality and delay the release of the software systems. In this paper,
we study blocking bugs in eight open source projects and propose a model to
predict them early on. We extract 14 diﬀerent factors (from the bug repositories)
that are made available within 24 hours after the initial submission of the bug
reports. Then, we build decision trees to predict whether a bug will be a blocking
bugs or not. Our results show that our prediction models achieve F-measures of
21%-54%, which is a two-fold improvement over the baseline predictors. We also
analyze the ﬁxes of these blocking bugs to understand their negative impact. We
ﬁnd that ﬁxing blocking bugs requires more lines of code to be touched compared
to non-blocking bugs. In addition, our ﬁle-level analysis shows that ﬁles aﬀected
by blocking bugs are more negatively impacted in terms of cohesion, coupling
complexity and size than ﬁles aﬀected by non-blocking bugs.
Process Metrics, Code Metrics. Post-release Defects
∗Corresponding author
Email addresses: (Harold Valdivia-Garcia),
 (Emad Shihab), found that software bugs cost $59
billions annually to the US economy .
Therefore, in recent years, researchers and industry have put a large amount of
eﬀort in developing tools and prediction models to reduce the impact of software
defects (e.g., ). This work usually leverages data from bug reports in
bug tracking systems to build their prediction models. Other work proposed
methods for detecting duplicate bug reports , automatic assignment of
bug severity/priority , predicting ﬁxing time and assisting
in bug triaging . More recently, prior work focused on speciﬁc types of
issues such as reopened bugs, performance bugs and enhancement requests .
In the normal ﬂow of the bug process, someone discovers a bug and creates the
respective bug report1, then the bug is assigned to a developer who is responsible
for ﬁxing it and ﬁnally, once it is resolved, another developer veriﬁes the ﬁx
and closes the bug report. Sometimes, however, the ﬁxing process is stalled
because of the presence of a blocking bug. Blocking bugs are software defects
that prevent other defects from being ﬁxed. In this scenario, the developers
cannot go further ﬁxing their bugs, not because they do not have the skills or
resources (e.g., time) needed to do it, but because the components they are ﬁxing
1We use the terms “bug” or “bug report” to refer to an issue report (e.g., corrective and
non-corrective requests) stored in the bug tracking system.
 
 
depend on other components that have unresolved bugs. These blocking bugs
considerably lengthen the overall ﬁxing time of the software bugs and increase
the maintenance cost. In fact, we found that blocking bugs can take up 2 times
longer to be ﬁxed compared to non-blocking bugs. For example, in one of our
case studies, the median number of days to resolve a blocking bug is 129, whereas
the median for non-blocking bugs is 69 days.
In our earlier work we found that the manual identiﬁcation of blocking bugs
takes 3-18 days on median . To reduce such impact, we built prediction
models to ﬂag blocking bugs early on for developers. In particular, we mined
the bug repositories from six open source projects to extract 14 diﬀerent factors
related to the textual information of the bug, the location the bug is found
and the people who reported the bug. Based on these factors and employing a
decision tree-based technique (C4.5), we built our prediction models. Then, we
compared our proposed models with many other machine learning techniques.
In addition, we performed a Top Node analysis in order to determine which
factors best identify blocking bugs.
In this paper, we extended the work on blocking bugs in a number of ways.
First, to reduce the threat to external validity, we added another 2 projects to
our data set. Second, we enhanced our prediction models by using bug report
information available within 24 hours after the initial submission of the bug
reports. This change has a signiﬁcant impact on the practical value of our work,
since it means that our new approach can be applied much earlier than our
previously proposed approach. Third, we analyzed the ﬁxes of the blocking
bugs to empirically examine their negative impact on the bug-ﬁxing process. In
particular, we link the bug-ﬁxes to their corresponding bug-reports. Then, we
divide the bug-ﬁxes into blocking/non-blocking bug-ﬁxes in order to compare
their size. We also compared the ﬁles related to blocking and non-blocking
bugs in terms of cohesion, coupling, complexity and lines of code. We note that
our examination of the ﬁxes is not done to improve the predictions, nor are we
suggesting that ﬁxing information can be used to predict blocking bugs; we study
 
 
the ﬁxes of blocking bugs to empirically validate their impact. In particular, we
would like to answer the following research questions:
RQ1 What is the impact of blocking bugs? By analyzing bug reports and
bug-ﬁx commits, we ﬁnd that blocking bugs take up 2 times longer and
require 1.2-4.7 times more lines of code to be ﬁxed than non-blocking bugs.
RQ2 Do ﬁles with blocking bugs have higher complexity than ﬁles
with non-blocking bugs? We ﬁnd that ﬁles aﬀected by blocking bugs
are bigger (in LOC), have higher complexity, higher coupling and less
cohesion than not aﬀected by non-blocking bugs.
RQ3 Can we build highly accurate models to predict whether a new
bug will be a blocking bug? We use 14 diﬀerent factors extracted from
bug databases to build accurate prediction models that predict whether
a bug will be a blocking bug or not.
Our models achieve F-measure
values between 21%-54%. Additionally, we ﬁnd that the bug description,
the comments and the experience of the reporter in identifying previous
blocking bugs are the best indicators of whether or not a bug will be
blocking bug.
The rest of the paper is organized as follows. Section 2 describes the approach
used in this work, including the data collection, preprocessing and a brief
description of the machine learning techniques used to predict blocking bugs.
Section 3 presents the ﬁndings of our case study. We discuss the implications of
relaxing the data collection process in Section 4. Section 5 highlights the threats
to validity. We discuss the related work in Section 6. Section 7 concludes the
paper and discusses future work.
2. Approach
In this section, we ﬁrst provide a deﬁnition of blocking bugs. Second, we
present details of the data collection process. We leveraged data from three
 
 
sources: bug reports, bug-ﬁxing commits and source-code ﬁles. Third, we discuss
the bug report factors used in our prediction models. Forth, we brieﬂy discuss the
machine learning techniques, as well as, the evaluation criteria used to examine
the performance of our prediction models.
2.1. Deﬁning Blocking and Non-Blocking bugs
When a user or developer ﬁnds a bug in a software system, she/he creates
the respective report (bug report) in the bug tracking system. Typically, a bug
assigned to a developer who is responsible for ﬁxing it. Once the bug is marked
as resolved, another developer veriﬁes the ﬁx and closes the bug report. There
are cases in which the ﬁxing of a bug prevents (blocks) other bugs (in the same
or related component) from being ﬁxed. We refer to such bugs as blocking
bugs. Developers of blocked bugs will record the blocking dependency in the
“Blocks” ﬁeld of the bug that is blocking them. More precisely, in this work we
consider a blocking bug as a bug report whose “Blocks” ﬁeld contains at least
one reference to another bug. Similarly, we consider a non-blocking bug as a
bug report whose “Blocks” ﬁeld is empty.
2.2. Data Collection
We used the bug report, bug-ﬁx and ﬁle history from eight diﬀerent projects
listed in Table 1. We chose these projects because they are mature and long-lived
open sources projects, with a large amount of bug reports. Below we explain
how we get the bug report and bug-ﬁx data sets from the studied projects.
2.2.1. Bug Report Collection
We collected bug reports from the bug repository of each project. We only
considered those bug reports with status equal to veriﬁed or closed. Bug reports
closed in less than one day were also ﬁltered out, because we want to analyze
non-trivial bug reports. The left-hand side of Table 2 shows a summary of
our data set of bug reports. We extracted 857,581 bug reports and discarded
247,781 of them. In brief, after the preprocessing step, we have that: (a) the
 
 
Table 1: Description of the case study projects
Description
Web browser developed by Google and used as the development
branch of Google Chrome.
A popular multi-language IDE written in Java, well known for its
system of plugins that allows customization of its programming
environment.
FreeDesktop
Umbrella project hosting sub projects such as Wayland (display
protocol to replace X11 ), Mesa (free implementation of the OpenGL
speciﬁcation), etc.
Framework and umbrella project that hosts and develops products
such as Firefox, Thunderbird, Bugzilla, etc.
Another popular IDE written in Java. Although it is meant for
java development, it also provides support for PHP and C/C++
development.
Oﬃce suite initiated by Sun Microsystem and currently developed
by Apache.
Operating system distribution built on top of either GNU/Linux or
FreeBSD. At the time of writing this paper, Gentoo contains over
17,000 packages.
GNU/Linux distribution developed by the Fedora-Project under the
sponsorship of Red Hat.
total number of valid bugs was 609,800, of which 77,448 were blocking bugs and
532,352 were non-blocking bugs; (b) in all projects, the percentages of blocking
bugs range from 6%-21% with an overall percentage of 12% and (c) the number
of bugs blocked by blocking bugs is ≈57,000 (details in RQ1).
2.2.2. Bug-ﬁx Collection
We summarize the extracted bug-ﬁxing commits in the right-hand side of
Table 2. We link the bug-reports (in the bug repositories) to their bug-ﬁxing
commits (in the code-repositories) using an approach similar to previous studies
 
 
Table 2: Summary of the collected bug reports
Bug-report Dataset
Bug-ﬁx Dataset
# Blocking
# Non-blocking
linked to bugs
3,468 [6.1%]
53,600 [93.9%]
8,022 [6.2%]
121,779 [93.8%]
FreeDesktop
605 [11.4%]
4,687 [88.6%]
13,994 [20.3%]
54,832 [79.7%]
5,101 [6.6%]
72,303 [93.4%]
4,164 [5.6%]
70,775 [94.4%]
531 [7.9%]
6,169 [92.1%]
41,563 [21.9%]
148,207 [78.1%]
All Projects
77,448 [12.7%]
532,352 [87.3%]
 . First, we checked out the code repositories of each of the projects. The
projects studied in this work are comprised of many products and components
that use tens or even hundreds code-repositories (e.g., the Fedora website2 lists
18,000 GIT repositories). However, processing the commits from all of these
repositories would be impractical and of little beneﬁt, since many of them have
a small number of commits. To select the most representative code-repositories,
we use the following two approaches:
• When we were able to identify the products and their code repositories, we
manually downloaded the repositories of the 20 most buggiest products. For
example, the Bugzilla repository of Eclipse lists ≈230 diﬀerent products,
out of which we downloaded the code-repositories of the 20 products (84
repositories) with the highest number of bug-reports.
• On the other hand, when we were not able to match the products and the
code-repositories, we downloaded all the code-repositories, ranked them
by the number of commits and selected the 100 largest repositories. We
also tried diﬀerent number of repositories (50, 100 and 150), however in
most of the cases the number of links only slightly improved (less than 1%)
after 100 repositories.
2Fedora Git Repositories: 
 
 
In total, we downloaded more than 400 repositories. We refer the reader
to our online appendix for a detailed list of the code-repositories used in
this study. Once we obtained all the commits, we extracted those commits
that contain bug-related words (e.g., bug, ﬁxed, failed, etc) and potential bugs
identiﬁers (e.g., bug#700, rhbz:800, etc) in their commit messages. To validate
the collected commits, we checked that the bug-identiﬁers in the commits are
present in our bug report data set. In total, we extracted ≈2.4 million commits,
out of which approximately 263,345 commits were successfully linked to one
or more bug-reports in our data set. Of these linked commits, 61,052 (23%)
were commits ﬁxing blocking bugs and about 202,293 (77%) were commits ﬁxing
non-blocking bugs.
Table 3: Distribution of the number of blocking and non-blocking ﬁles
# Blocking
# Non-Blocking
34,430 [36%]
60,282 [64%]
74,580 [43%]
97,375 [57%]
FreeDesktop
1,074 [22%]
3,774 [78%]
34,939 [78%]
9,612 [22%]
3,876 [19%]
16,833 [81%]
1,752 [4%]
48,183 [96%]
4,182 [33%]
8,510 [67%]
1,674 [55%]
1,347 [45%]
156,507 [39%]
245,916 [61%]
2.2.3. Code-metrics Collection
We used Understand from Scitools3 to extract four metrics from the sourcecode ﬁles in the code repositories: Lack of Cohesion, Coupling Between Objects,
Cyclomatic Complexity and LOC. In our analysis, we take into account Java, C,
C++, Python, Javascript, PHP, Bash and Patch source code ﬁles.
3 
 
 
From the bug-ﬁxing commits obtained in the previous section, we identiﬁed
402,423 buggy ﬁles. Then, we analyzed the distribution of the number of bugs
per ﬁle and we found that ≈90% of the buggy ﬁles have at most 5 bugs and
usually just 1 bug on median. Therefore, in this work, we split the buggy ﬁles
into two groups: (a) ﬁles aﬀected by at least one blocking bug (blocking ﬁles
for brevity) and (b) ﬁles aﬀected only by non-blocking bugs (non-blocking ﬁles
for brevity). Table 3 shows the distribution of the blocking ﬁles and non-blocking
ﬁles across all of the projects. We can see that 39% of the ﬁles are blocking ﬁles
(156,507 ﬁles), whereas 61% are non-blocking ﬁles (245,916 ﬁles).
To better understand the ﬁles aﬀected by blocking and non-blocking bugs, we
analyzed the distribution of their programming languages. In Table 4, we show
the percentage of blocking ﬁles (third column) and non-blocking ﬁles (fourth
column) across the top programming languages in each of the projects. For
example, in Fedora 49% of the blocking ﬁles and 19% of the non-blocking ﬁles are
written in Bash. Additionally, from the ﬁfth column, we can observe that about
98% of the buggy ﬁles in Fedora are Patch or Bash ﬁles. As we will discuss in
RQ2, this situation will prevent us from extracting two of the four code metrics
for Fedora.
2.3. Factors Used to Predict Blocking Bugs
Since our goal is to be able to predict blocking bugs, we extracted diﬀerent
factors from the bug reports so the blocking bugs can be detected early on. In ad-
dition, we would like to determine which factors best identify these blocking bugs.
We consider 14 diﬀerent factors to help us discriminate between blocking and
non-blocking bugs. To come up with a list of factors, we surveyed prior work. For
example, Sun et al. included factors such product, component, priority, etc in
their models to detect duplicate bugs. Lamkanﬁet al. used textual infor-
mation to predict bug severities. Wang et al. and Jalbert et al. used text
mining to identify duplicate bug reports. Zimmermann et al. showed that
the reporter’s reputation is negatively correlated with reopened bugs in Windows
Vista. Furthermore, many of our factors are inspired in the metrics used by our
 
 
Table 4: Distribution of source code ﬁles across diﬀerent programming languages.
In each of columns three to ﬁve, we report the percentage of ﬁles that belong to
a particular programming language.
% Blocking
% Non-Blocking
FreeDesktop
prior work , predicting reopened bugs. We list each factor and provide a brief
description for each below:
 
 
1. Product: The product where the bug was found (e.g., Firefox OS, Bugzilla,
etc). Some products are older or more complex than others and therefore,
are more likely to have blocking bugs.
For example, Firefox OS and
Bugzilla are two Mozilla products with approximately the same number of
bugs (≈880), however there were more blocking bugs in Firefox OS (250
bugs) than in Mozilla (30 bugs).
2. Component: The component in which the bug was found (e.g., Core,
Editor, UI, etc). Some components are more/less critical than others and
as a consequence more/less likely to have blocking bugs than others. For
example, it might be the case that bugs in critical components prevent
bugs in other components from being ﬁxed. Note that we were not able to
have this factor for Chromium because its issue tracking system does not
support it.
3. Platform: The operating system in which the bug was found (e.g., Win-
dows, Android, GNU/Linux etc). Some platforms are more/less prone to
have bugs than others. It is more/less likely to ﬁnd blocking/non-blocking
bugs for speciﬁc platforms.
4. Severity: The severity describes the impact of the bug. We anticipate that
bugs with a high severity tend to block the development and debugging
process. On the other hand, bugs with a low severity are related to minor
issues or enhancement requests.
5. Priority: Refers to the order in which a bug should be attended with
respect to other bugs. For example, bugs with low priority values (i.e.,
P1) should be prioritized instead of bugs with high priority values (i.e.,
It might be the case that a high/low priority is indicative of a
blocking/non-blocking bugs.
6. Number in the CC list: The number of developers in the CC list of the
bug. We think that bugs followed by a large number of developers might
 
 
indicate bottlenecks in the maintenance process and therefore are more
likely to be blocking bugs.
7. Description size: The number of words in the description. It might be
the case that long/short descriptions can help to discriminate between
blocking and non-blocking bugs.
8. Description text: Textual content that summarize the bug report. We
think that some words in the description might be good indicators of
blocking bugs.
9. Comment size: The number of words of all comments of a bug. Longer
comments might be indicative of bugs that get discussed heavily since they
are more diﬃcult to ﬁx. Therefore, they are more likely to be blocking
10. Comment text: The comments posted by the developers during the life
cycle of a bug. We think that some words in the comments might be good
indicators of blocking bugs.
11. Priority has Increased: Indicates whether the priority of a bug has
increased after the initial report.
Increasing priorities of bugs might
indicate increased complexity and can make a bug more likely to be a
blocking bug. Note that we were unable to obtain this information for
12. Reporter Name: Name of the developer or user that ﬁles the bug. We
include this factor to investigate whether bugs ﬁled by a speciﬁc reporter
are more/less likely to be blocking bugs.
13. Reporter Experience: Counts the number of previous bug reports ﬁled
by the reporter. We conjecture that more/less experienced reporters may
be more/less likely to report blocking bugs.
 
 
14. Reporter Blocking Experience: Measures the experience of the reporter in identifying blocking bugs. It counts the number of blocking bugs
ﬁled by the reporter previous to this bug.
In order to extract information for the factors, we ﬁrst obtained the closingdates and blocking-dates of the bug-reports. Closing-date refers to the latest
date in which a bug was closed. To obtain this information, we inspect the
history of the bugs looking for the date of the last appearance of the tag “status”
with a value equal to “closed”. Blocking-date refers to the earliest date in
which a bug was marked as blocking bug. To calculate this information, we look
for the date of the ﬁrst appearance of the tag “Blocks” in the history of the bugs.
For the non-blocking bugs, we extracted the last values of the factors prior
to their closing-dates and within 24 hours after the submission. On the other
hand, for the blocking bugs, we extracted the last values of the factors prior
to their blocking-dates and within 24 hours after the submission. The rationale
for this approach is that, although the data after the blocking-date is useful
information about the ﬁxing process in general, it is not useful to identify a
blocking bug because we already know that the bug is a blocking bug (i.e., by
then no prediction is needed). Since our aim is to identify potential blocking
bugs early on, then we can only rely on data before the blocking phenomenon
happens. That way we can shorten the overall ﬁxing-time.
As we mentioned above, these 14 factors have been used in prior studies
and most of them are easy to extract through software repositories. Because
our goal is to help developers to identify blocking bugs early on, we only use
bug report information available within 24 hours after the initial submission
of the bug reports. When a factor was empty, we set its value to NA (or zero
for numeric factors). That said, it is important to note that 3 of our factors
(product, component and reporter’s name) are project-speciﬁc. Therefore, if a
practitioner would like to predict blocking bugs in a cross-project setting, she/he
might not able to reuse models on new projects. In that situation, the simpler
approach would be to remove the project-speciﬁc factors from the model or
 
 
adapt these factors from their speciﬁc project in order to have a more ﬂexible
Finally, another important observation to note is that the description text
and the comment text factors need special treatment before being included in
our prediction models. We describe this special preprocessing in detail in the
next sub-section.
2.4. Textual Factor Preprocessing
The description and comments in bug reports are two rich sources of unstructured information that require special preprocessing. These factors contain
discussions about the bugs and can also provide snapshots of the progress and
status of such bugs. One way to deal with text based factors is using a vector
representation. In this kind of representation, a new factor is created for each
unique word in the data set. Similar to prior work , we followed this
simple approach. In Figure 1, we show our adapted approach to convert textual
factors into numerical values. We used a Naive Bayes classiﬁer to calculate the
Bayesian-score of these two factors. Basically this metric indicates the likelihood
First training set (D0)
(nonblocking)
(blocking)
Second training set (D1)
(nonblocking)
(blocking)
the classifier
the classifier
Figure 1: Converting textual factor into Bayesian-score
 
 
that a description or comment belongs to certain kind of bug (i.e., blocking or
non-blocking).
We divide the entire data set into two training sets (D0 and D1) using
stratiﬁed random sampling. This ensures that we have the same number of
blocking and non-blocking bugs in both training sets. We train a classiﬁer (C0)
with the ﬁrst training set and use it to obtain the Bayesian-scores on the second
training set. We also do the same in the opposite direction. We build a classiﬁer
(C1) using the second training set and apply it on the ﬁrst training set. This
strategy is used in order to avoid the classiﬁers from being biased toward their
training sets; otherwise, it will lead to optimistic (unrealistic) values for the
Bayesian-scores.
In our classiﬁer implementation, each training set is split into two corpora
(corpus1 and corpus0). The ﬁrst corpus contains the descriptions/comments
of the blocking bugs. The second corpus contains the description/comments
of the non-blocking bugs. We create a word frequency table for each corpus.
The textual content is tokenized in order to calculate the occurrence of each
word within a corpus. Based on these two frequency tables, the next step is to
calculate the probabilities of all the words to be in corpus1 (i.e., blocking bugs),
because we are interested in identifying these kinds of bugs. The probability
is calculated as follow: if a word is in corpus1 and not in corpus0, then its
probability is close to 1. If a word is not in corpus1 but in corpus0, then its
probability is close to 0. On the other hand, if the word is in both corpora, then
its probability is given by p(w) =
%w in corpus1
%w in corpus1+%w in corpus0 .
Once the classiﬁers are trained, we can obtain the Bayesian-score of a text
based factor by mapping its words to their probabilities and combining them.
The formula for the Bayesian-score is p(text) =
Q p(wi)+Q(1−p(wi)). For this
calculation, the ﬁfteen most relevant words are considered . Here, “relevant”
means those words with probability close to 1 or 0.
 
 
2.5. Prediction Models
For each of our case study projects, we use our proposed factors to train a
decision tree model to predict whether a bug will be a blocking bug or not. We
also compare our prediction model with four other classiﬁers namely: Naive Bayes,
kNN, Zero-R, Logistic Regression, Random Forests and Stacked Generalization.
2.5.1. Decision Tree Model
We use a tree-based learning algorithm to perform our predictions. One
of the beneﬁts of decision trees is that they provide explainable models. Such
models intuitively show to the users (i.e., developers or managers) the decisions
taken during the prediction process. The C4.5 algorithm belongs to this type
of data mining technique and like other tree-based classiﬁers, it follows a greedy
divide and conquer strategy in the training stage. The algorithm recursively
splits data into subsets with rules that maximize the information gain. The
rules are of the form Xi < b if the feature is numeric or into multiple subsets if
the feature is nominal. In Figure 2, we provide an example of a tree generated
from the extracted factors in our data set. The sample tree indicates that a bug
report will be predicted as blocking bug if the Bayesian-score of its comment is
> 0.74, there are more than 6 developers in the CC list and the number of words
in the comments is greater than 20. On the other hand, if the Bayesian-score of
bayes-score
Blocking Blocking
NonBlocking
experience
NonBlocking
Figure 2: Example of a Decision Tree
 
 
its comment is ≤0.74 and the reporter’s experience is less than 5, then it will
be predicted as a non-blocking bug.
2.5.2. Naive Bayes Model
We use this machine learning method for two purposes: to convert textual
information into numerical values (i.e., to obtain the probability that a description/comment belongs to a blocking-bug), and to build a prediction model and
compare its performance with that of our decision tree model. This simple model
is based on the Bayes theorem and the assumption that the factors are randomly
independent. For a given record x, the model predicts the class k that maximizes
the conditional joint distribution of the data set. Mathematically, the model can
be written as:
f(x) = arg max
P(C = k) Q
i P(xi|C = k)
Here, the prior-probability P(C = k) can be estimated with the percentage
of training records labeled as k (e.g., percentage of blocking or non-blocking).
The conditional probabilities P(xi|C = k) can be estimated with Nk,i
Nk , where
the numerator is the number of records labeled as k for which the ith-factor is
equal to xi and the denominator is the number of records labeled as k. The
probability P(X = x) can be neglected because it is constant with respect to
the classes.
2.5.3. K-Nearest Neighbor Model
The k-nearest neighbor model is a simple, yet powerful memory-based tech-
nique, which has been used with relative success in previous bug prediction works
 . The idea of the method is as follows: given an unseen record ˆx (e.g., an
incoming bug report), we calculate the distance of all records x in the training
set (e.g., already-reported bugs) to ˆx, then we select the k closest instances and
ﬁnally classify ˆx to the most frequent class among these k neighbors. In this
work, we considered k = 5 as the number of neighbors, used the euclidean metric
for numerical factors and the overlap metric for nominal factors. Under the
 
 
overlap metric, the distance is zero if the values of the factors are equal and one
otherwise.
2.5.4. Zero-R Model
Zero-R (no rule) is the simplest prediction model because it always predicts
the majority class in the training set. We use this classiﬁer as one of our baseline
models in the comparison section.
2.5.5. Logistic Regression
Logistic regression is statistical binary classiﬁcation model extensively used
in the literature on software bug prediction . For a given record
x = x1, x2, · · · , xp, this prediction model estimates the probability that such
a record belongs to the class k = 1 (e.g., blocking-bug) using the following
P(k = 1|x) =
eβ0+β1x1+···+βpxp
1 + eβ0+β1x1+···+βpxp
where the regression coeﬃcients βi are found during the training phase. For
a detailed description of the logistic regression model, we refer readers to .
2.5.6. Random Forests Model
Random Forests is an ensemble classiﬁcation approach that makes its
prediction based on the majority vote of a set of weak decision trees. This
approach reduces the variance of the individual trees and makes the model
more resilient to noise in the data set. In general, the random forests model
outperforms simple decision trees in terms of prediction accuracy .
2.5.7. Stacked Generalization
Stacked Generalization is an ensemble classiﬁcation approach, which
attempts to increase the performance of individual machine learning methods
by combining their outputs (i.e., individual predictions) using another machine
learning method referred to as the meta-learner. In this work, we use C4.5, Naive
Bayes and kNN algorithm as our individual models, and Logistic regression as
the meta-learner.
 
 
2.6. Performance Evaluation
A common metric used to measure the eﬀectiveness of a prediction model
is its accuracy (fraction of correctly classiﬁed records). However, this metric
might not be appropriate when the data set is extremely skewed towards one
of the classes . If a classiﬁer tends to maximize the accuracy, then it can
perform very well by simply ignoring the minority class . Since our data
set suﬀers from the class imbalance problem, the accuracy is not enough and
therefore we include three other performance measures: precision, recall and
f-measure. These measures are widely used to evaluate the quality of models
trained on imbalanced data.
1. Precision: The ratio of correctly classiﬁed blocking bugs over all the bugs
classiﬁed as blocking.
2. Recall: The ratio of correctly classiﬁed blocking bugs over all of the
actually blocking bugs.
3. F-measure: Measures the weighted harmonic mean of the precision and
recall. It is calculated as F-measure = 2∗Precision∗Recall
Precision+Recall .
4. Accuracy: The ratio between the number of correctly classiﬁed bugs
(both the blocking and the non-blocking) over the total number of bugs.
A precision value of 100% would indicate that every bug we classiﬁed as
blocking bug was actually a blocking bug. A recall value of 100% would indicate
that every actual blocking bug was classiﬁed as blocking bug.
We use stratiﬁed 10-fold cross-validation to estimate the accuracy of our
models. This validation method splits the data set into 10 parts of the same size
preserving the original distribution of the classes. At the i-th iteration (i.e., fold),
it creates a testing set with the i-th part and a training set with the remaining
9 parts. Then, it builds a decision tree using the training set and calculate its
accuracy with the testing set. We report the average performance of the 10
folds. Since our data sets have a low number of blocking bugs, the stratiﬁed
 
 
sampling prevents us from having parts without blocking bugs. Additionally, we
use re-sampling on the training data only in order to reduce the impact of the
class imbalance problem (i.e., the fact that there are many non-blocking bugs
and very few blocking bugs) of our data sets.
3. Case Study
This section reports the results of our study on eight open source projects
and answers our three research questions. First, we characterized the impact of
blocking bugs in terms of their ﬁxing time, blocking dependency and bug-ﬁxing
commits (i.e., bug-ﬁx size). Second, we inspected the ﬁles aﬀected by blocking
and non-blocking bugs and measure their complexity to better understand the
blocking phenomenon. Third, we built diﬀerent prediction models to detect
whether a bug will be or not a blocking bug and performed Top Node analysis to
determine which of the collected factors are good indicators to identify blocking
RQ1. What is the impact of blocking bugs?
Motivation. Since blocking bugs delay the repair of other bugs (i.e.,
blocked bugs), they are harmful for the maintenance process. For example,
if blocking bugs take longer than other ordinary bugs, then the overall
ﬁxing time of the system might increase. Similarly, the presence of blocking
bugs that block a large number of other bugs (high dependency) might
become bottlenecks for maintenance, and impact the quality of the system.
Although there are diﬀerent ways to deﬁne the impact of software bugs on
software projects, in this RQ we are interested in quantifying the eﬀects
caused by blocking bugs during bug triaging. Therefore, in this RQ, we
deﬁne the impact in terms of two proxy metrics collected at bug-report
level, namely ﬁxing-time and degree of dependency
Approach. First, we calculate the ﬁxing time for both blocking and nonblocking bugs as the time period between the date when a bug is reported
 
 
until its closing date. Then, we performed an unpaired Wilcoxon rank-sum
test (also called Mann-Whitney U test) for the alternative hypothesis
Ha : tblock > tnonblock, in order to determine whether blocking bugs take
longer to be ﬁxed compared to non-blocking bugs. On the other hand, we
analyze the degree of blocking dependency as the number of bugs that
depend on the same blocking bugs.
Fixing time.
Table 5 reports the median ﬁxing-time for
blocking/non-blocking bugs. For all of the projects, we observe that the
ﬁxing-time for blocking bugs is 1.1 - 1.9 times longer than for the nonblocking bugs. In addition, the results of the Wilcoxon test conﬁrm that
there is a statistically signiﬁcant diﬀerence between the blocking and nonblocking bugs for all of the projects (p-value < 0.001), meaning that the
ﬁxing-time for blocking bugs is statistically signiﬁcantly longer than the
ﬁxing-time for non-blocking bugs.
Table 5: Median ﬁxing time in days and the result of the Wilcoxon rank-sum test
for blocking and non-blocking bugs
Chromium ***
Eclipse ***
129 [1.9X]
FreeDesktop ***
Mozilla ***
NetBeans ***
204 [1.4X]
OpenOﬃce ***
129 [1.1X]
Gentoo ***
Fedora ***
119 [1.1X]
(***) p < 0.001
Dependency of Blocking Bugs. In our study, we found that blocking
bugs represent 12% of all bugs in our data set (77,448 bugs). In order to
 
 
Table 6: Degree of Blocking Dependency
FreeDesktop
assess the impact of the dependency of these blocking bugs, we extracted
the list of blocked bugs contained in the “Blocks” ﬁeld of each blocking
bug. In total, we identiﬁed 57,015 diﬀerent bug reports that were blocked
by blocking bugs. At the time of the data collection, many of these blocked
bugs were still in progress (and therefore were not included in our data
set). Hence, we cannot claim that they account for about 9% of our data
set. Table 6 reports the distribution of the degree of dependency between
one and six. Furthermore, we include a category “≥7” for those blocking
bugs that block seven or more bugs.
At ﬁrst sight, it is easy to see that approximately 89-98% of the blocking
bugs for all projects only block 1 or 2 bugs. As a consequence, blocking
bugs with high dependency are uncommon. To better understand the
severity of these bugs with degree of dependency greater than or equal to
7, we performed a manual inspection, and we found inconclusive results.
For example, in the Eclipse project, many bugs with high dependency
were actual enhancements with low priority (e.g., P3 or P4) instead of
real defects. On the other hand, in NetBeans, we found that indeed these
blocking bugs were real defects with high priority (e.g., P1 or P2).
Discussion Although, we found that blocking bugs take longer to be ﬁxed compared
to non-blocking bugs, the evidence is still unclear whether or not blocking
bugs are more complex to ﬁx. Blocking bugs may be easy to ﬁx, but take a
long time to ﬁnd the right developers to solve them, or many blocking bugs
 
 
are actually enhancements that while desirable, are not a priority, so the
developers postpone them in favor of more important bugs. Therefore, we
analyze the size of bug-ﬁxes, to determine whether blocking bugs require
more eﬀort to ﬁx than non-blocking bugs. First, we calculate the bug-ﬁx
size as the number of lines modiﬁed (LM) from all the commits related
to the bug. Then, we check whether blocking bug-ﬁxes are larger than
the non-blocking bug-ﬁxes by using a Wilcoxon rank-sum test for the the
hypothesis Ha : LMblock > LMnonblock.
Table 7: Median bug-ﬁx size and the result of the Wilcoxon rank-sum test for
blocking and non-blocking bugs
LMnonblock
Chromium ***
205 [4.7X]
Eclipse ***
107 [3.3X]
FreeDesktop ***
Mozilla ***
NetBeans ***
OpenOﬃce **
Gentoo ***
Fedora ***
(***) p < 0.001;
(**) p < 0.01
In Table 7, we report the median bug-ﬁx size (code-churn) of blocking and
non-blocking bugs. We can observe that for all of the projects, blocking
bug-ﬁxes are 1.2 - 4.7 times larger than non-blocking bug-ﬁxes. The result
of the Wilcoxon rank-sum test verify that blocking bugs have statistically
signiﬁcantly larger bug-ﬁxes than non-blocking bugs.
The time to address a blocking bug is 1.1 - 1.9 times longer
than the time it takes to address a non-blocking bug. Simultaneously, ﬁxing blocking bugs requires 1.2 - 4.7 times more
lines of code to be modiﬁed than ﬁxing non-blocking bugs.
 
 
RQ2. Do ﬁles with blocking bugs have higher complexity than
ﬁles with non-blocking bugs?
Motivation We found that ﬁxing blocking bugs require more eﬀort
and time (RQ1). However, it is not clear whether ﬁles with blocking
bugs (blocking ﬁles) are diﬀerent from ﬁles with non-blocking bugs
(non-blocking ﬁles).
In this RQ, we would like to analyze and
quantify the blocking phenomenon at ﬁle level.
To answer this question, ﬁrst we extract four metrics from the sourcecode ﬁles in the code-repositories: size (LOC), Cyclomatic Complexity
(CC), Lack of Cohesion (LCOM) and Coupling Between Objects
Results Lack of cohesion. Table 8 reports the median of LCOM
for blocking/non-blocking ﬁles.
We see that blocking ﬁles have
slightly higher LCOM (1.02-1.18 times higher) than non-blocking
We compared these two groups of ﬁles using a Wilcoxon
rank-sum test in order to determine if the diﬀerence is statistically
signiﬁcant. For four projects (Chromium, Eclipse, Netbeans and
OpenOﬃce), we ﬁnd that ﬁles with blocking bugs have statistically
less cohesion than ﬁles with non-blocking bugs. For FreeDesktop
and Mozilla, we ﬁnd no evidence that blocking ﬁles have higher
LCOM than non-blocking ﬁles. Although these projects have a relative large number of buggy ﬁles, the Understand tool was able to
extract the LCOM metric from only a small fraction of the buggy
ﬁles. For both FreeDesktop and Mozilla, we obtained the LCOM
metric from 7% and 25% of the buggy ﬁles respectively. In contrast,
we obtained the LCOM metric from about 44%-93% of buggy ﬁles for
the other projects. This is not surprising since, most of the buggy ﬁles
in FreeDesktop and Mozilla are written in C and Javascript. From
Table 4, we can see that for FreeDesktop, about 84% of the buggy ﬁles
 
 
are written in C, whereas for Mozilla about 55% of the buggy ﬁles
are written in C and Javascript.
Table 8: Median lack of cohesion for blocking and non-blocking ﬁles
LCOMnonblock
Chromium ***
67% [1.10X]
Eclipse ***
58% [1.16X]
FreeDesktop
71% [0.86X]
87% [1.02X]
NetBeans ***
79% [1.03X]
OpenOﬃce ***
71% [1.18X]
(***) p < 0.001;
(**) p < 0.01;
(*) p < 0.05
Coupling between objects. In Table 9, we show the median of
CBO for blocking/non-blocking ﬁles. For four projects (Chromium,
Eclipse, Netbeans and OpenOﬃce), we ﬁnd that blocking ﬁles are
coupled to other classes 1.15-1.43 times more than non-blocking ﬁles.
The result of the Wilcoxon rank-sum test shows that, there is a
statistically signiﬁcant diﬀerence in terms of CBO between blocking
and non-blocking ﬁles. Similar to the previous metric, we ﬁnd no
evidence that blocking ﬁles have higher CBO than non-blocking ﬁles
for FreeDesktop and Mozilla.
Cyclomatic Complexity.
Prior work showed that OO metrics
such as LCOM and CBO are signiﬁcantly associated with bugs
 . These OO metrics are useful for architectural and
design evaluation . However, ﬁrst, they cannot be extracted from
non-object oriented languages (e.g., C, Bash). Second, they might
not be easily computed by practitioners. In such cases, other code
metrics that assess the quality of the software systems should be
 
 
considered (e.g., CC and LOC). In Table 10 we compare the median
CC between blocking and non-blocking ﬁles. For the ﬁrst six projects,
we ﬁnd that blocking ﬁles have ≈1.2-7.6 times more execution paths
than non-blocking ﬁles. The Wilcoxon rank-sum test conﬁrms that
Table 9: Median coupling for the blocking and non-blocking ﬁles
CBOnonblock
Chromium ***
10 [1.43X]
Eclipse ***
11 [1.22X]
FreeDesktop
12 [1.26X]
NetBeans ***
23 [1.15X]
OpenOﬃce ***
11 [1.38X]
(***) p < 0.001;
(**) p < 0.01;
(*) p < 0.05
Table 10: Median cyclomatic complexity for blocking and non-blocking ﬁles
CCnonblock
Chromium ***
Eclipse ***
FreeDesktop ***
Mozilla ***
NetBeans ***
OpenOﬃce ***
(***) p < 0.001;
(**) p < 0.01;
(*) p < 0.05
 
 
the diﬀerence is signiﬁcant. For Gentoo, there is no evidence that
CCblock > CCnonblock. However, this does not necessarily mean that
blocking/nonblocking ﬁles have the same complexity. After performing
the opposite hypothesis (CCblock < CCnonblock), we ﬁnd that blocking
ﬁles have statistically less complexity than non-blocking ﬁles. After
a manual inspection, we ﬁnd that blocking and non-blocking ﬁles
in Gentoo are quite diﬀerent in terms of functionality provided and
programming language distribution. Approximately 68% of the blocking ﬁles comes from Portage (Gentoo’s package management system),
which is mostly written in Python, whereas 40% of the non-blocking
ﬁles comes from Quagga (routing suite) and X-Server (window system
server) which are mostly written in C. For Fedora, we did not have
enough data to extract the CC metric. Approximately 98% of the
ﬁles in Fedora are Patch/Bash ﬁles and our metric extraction tool
does not support these kind of ﬁles.
Lines of Code.
Although CC is a good measure of structural
complexity of a program, it cannot be easily calculated for Bash/Patch
ﬁles. On the other hand, LOC can be calculated easier than CC for
any kind of source code ﬁle. Table 11 presents the median LOC of
blocking and non-blocking ﬁles. Similar to our previous ﬁndings, we
observe that for most of the projects (the ﬁrst six projects and Fedora),
blocking ﬁles have statistically more lines of code (1.3X-12.2X) than
non-blocking ﬁles. The only exception was Gentoo, for which we ﬁnd
that blocking ﬁles are smaller than non-blocking ﬁles.
Discussion. Our ﬁndings so far indicate that there is a negative
impact on the quality of the ﬁles aﬀected by blocking bugs. Therefore,
practitioners should plan to allocate more QA eﬀort when ﬁxing
blocking ﬁles.
In order to help with the resource allocation, we
would like to provide practitioners with a subset of ﬁles that are
most susceptible to blocking bugs. More precisely, we would like to
 
 
Table 11: Median LOC for blocking and non-blocking ﬁles
LOCnonblock
Chromium ***
142 [1.6X]
Eclipse ***
122 [1.4X]
FreeDesktop ***
588 [1.4X]
Mozilla ***
174 [1.4X]
NetBeans ***
284 [1.3X]
OpenOﬃce ***
513 [3.7X]
121 [0.9X]
Fedora ***
755 [12.2X]
(***) p < 0.001;
(**) p < 0.01;
(*) p < 0.05
investigate whether we can build accurate models (trained on ﬁle
metrics) to predict which buggy ﬁles will contain blocking bugs in the
future. First, we extract two process metrics (Num. lines modiﬁed and
Num. commits) and four code metrics (LOC, Cyclomatic, Coupling
and Cohesion) for both blocking and non-blocking ﬁles analyzed in this
RQ. Then, we train decision tree models using such ﬁle-metrics and
evaluate their performance using the precision, recall and F-measure
metrics. For Gentoo and Fedora, we do not consider Cyclomatic,
Coupling and Cohesion metrics, since most of the ﬁles in these projects
are Patch/Bash ﬁles. In Table 12, we report the models’ performance
for each of the projects. The results indicate that our blocking ﬁles
prediction models can achieve moderate and high F-measure values
ranging from 45.3% to 86.3%, while at the same time achieving high
accuracy values ranging from 64.9% to 96.7%. It is important to
emphasize that our models are not general models that aim to predict
buggy ﬁles, but specialized models to predict whether a buggy ﬁle will
be a blocking ﬁle. Therefore, our proposed models should be used in
 
 
conjunction with traditional bug prediction/localization models to
ﬁrst identify buggy ﬁles .
Files aﬀected by blocking bugs have
1.02-1.18 times less cohesion,
1.15-1.43 times higher coupling,
1.2-7.6 times higher complexity and
1.3-12.2 times more lines of code
than ﬁles aﬀected by non-blocking bugs.
RQ3. Can we build highly accurate models to predict whether
a new bug will be a blocking bug?
Motivation. We observed that blocking bugs not only take much
longer and require more lines of code to be ﬁxed than non-blocking
bugs, but also they negatively impact the aﬀected ﬁles in terms of
cohesion, coupling, complexity and size.
Because of these severe
consequences, it is important to identify blocking bugs in order to
reduce their impact. Therefore, in this RQ, we want to build prediction
models that can help developers to ﬂag blocking bugs early on, so they
can shorten the overall ﬁxing time. Additionally, we want to know if
Table 12: Performance of blocking ﬁles prediction models
FreeDesktop
 
 
we can accurately predict these blocking bugs using the factors that
we proposed in Section 2.3.
Approach. We use decision trees based on the C4.5 algorithm as
our prediction model, because it is an explainable model that can
easily be understood by practitioners. We use stratiﬁed 10-fold crossvalidation to estimate the accuracy of our models. To evaluate their
performance, we use the precision, recall and F-measure metrics.
The reported performances of the models are the average of the
10 folds. Baseline: In order to have a point of reference for our
performance evaluation, we use a random classiﬁer that has a 50/50
chance of predicting two outcomes (e.g., blocking and non-blocking
bugs). Prior studies have also used this theoretical model as their
baseline . Given a 50/50 random classiﬁer, if an inﬁnite
number of random predictions are performed, then the precision
will be to the percentage of blocking bugs in the data set, and the
recall will be to 50%. Additionally, we further compare them to six
other machine learning techniques.
Results. In Table 13, we present the performance results of our
prediction models. Our models present precision values ranging from
13.7% to 45.8%. Comparing these results with those of the baseline
models (6.1%-21.9%), our models provide a approximately two-fold
improvement over the baseline models in terms of precision.
In terms of recall, our models present better results for six projects
with values ranging from 52.9% to 66.7%. For the other projects
(Eclipse and Gentoo), the recalls were bellow the baseline recall (50%)
with values of ≈47%-49%. Although, we achieved low recall values
for some of our projects, what really matters for comparing the performance of the two models is the F-measure, which is a trade-oﬀbetween
precision and recall.
 
 
Our results show that the F-measure values of our prediction models
represent an improvement over those of the baseline models for all
of the projects. Our F-measure values range from 21.2% to 54.3%,
whereas the F-measure values of the baseline models range from
10.8% to 30.5%. The improvement ratio of our F-measure values vary
from ≈1.5 to 2.3 folds.
Table 13: Performance of the decision tree models
Decision Tree model
Baseline model
24.8% [2.3X]
21.9% [2.0X]
FreeDesktop
35.2% [1.9X]
46.0% [1.6X]
24.2% [2.1X]
23.1% [2.3X]
21.2% [1.5X]
54.3% [1.8X]
The above results give an idea of the eﬀectiveness of our models with
respect to a random classiﬁer. However, there are other popular
machine learning techniques besides decision trees that can be used to
predict blocking bugs. In Table 14, we compare the performance of our
model to six other machine learning techniques namely: Zero-R, Naive
Bayes, kNN, Logistic Regression, Stacked Generalization and Random
Forests. The Zero-R model presents the highest accuracy across most
of the projects (except for Fedora). This happens because the Zero-R
always predicts the majority class (e.g., non-blocking bugs), which in
our case account for approximately 87% of the bugs in most of the
projects. Clearly, it is useless to have a highly accurate model that
cannot detect blocking bugs. Therefore, we use the F-measure metric
to perform the comparisons. The Naive Bayes model is only slightly
 
 
Table 14: Predictions diﬀerent algorithms
Naive Bayes
Logistic Regression
Stacked Gen.
Rand. Forest
Decision Tree
Naive Bayes
Logistic Regression
Stacked Gen.
Rand. Forest
Decision Tree
FreeDesktop
Naive Bayes
Logistic Regression
Stacked Gen.
Rand. Forest
Decision Tree
Naive Bayes
Logistic Regression
Stacked Gen.
Rand. Forest
Decision Tree
Naive Bayes
Logistic Regression
Stacked Gen.
Rand. Forest
Decision Tree
Naive Bayes
Logistic Regression
Stacked Gen.
Rand. Forest
Decision Tree
Naive Bayes
Logistic Regression
Stacked Gen.
Rand. Forest
Decision Tree
Naive Bayes
Logistic Regression
Stacked Gen.
Rand. Forest
Decision Tree
 
 
better for Chromium, Mozilla and Gentoo with F-measure values
ranging from 22.1% to 46.6%. In the other ﬁve projects, Naive Bayes
performs worse than our model (specially for OpenOﬃce). The kNN
model is slightly worse for all of the projects. For example, in Mozilla,
kNN achieves a F-measure of 43%, whereas our model achieves a
F-measure of 46%. The Logistic Regression model performs slightly
worse for FreeDesktop, OpenOﬃce and Fedora, whereas in the other
projects, it performs better than our model. For example, in Mozilla,
Logistic Regression and Decision Trees achieve F-measures of 49% and
46% respectively. Random Forests and Stacked Generalization models
perform better in all of the projects. In particular, Random Forests
signiﬁcantly outperforms our models with an improvement of 7%-9%
for four projects (Chromium, Eclipse, NetBeans and OpenOﬃce). For
example, for the Chromium project, we observe that the F-measure
improves from 24.8% to 31.7%. However, these two ensemble models
do not provide easily explainable models. Practitioners often prefer
easy-to-understand models such as decision trees because they can
explain why the predictions are the way they are. What we observe
is that the decision trees are close to the Random Forests (or Stacked
Generalization) in terms of F-measure in many projects, however if
one is more concerned about accuracy to detect blocking bugs, the
Random Forests would be the best prediction model. If one wants
accurate models that are easily explainable, then they would need to
sacriﬁce a bit of accuracy and use the decision tree model.
Discussion. Besides warning about blocking bugs, we would like
to advise developers to be careful of factors (in the bug reports)
that potentially indicate the presence of blocking bugs. Therefore,
we investigate which factor or group of factors have a signiﬁcant
impact on the determination of blocking bugs.
We perform Top
Node analysis in order to determine which factors are the best indi-
 
 
cators of whether a bug will be a blocking bug or not. In the Top
Node analysis, we examine the decision trees created by the 10-fold
cross validation and we count the occurrences of the factors at each
level of the trees. The most relevant factors are always close to the
root node (level 0, 1 and 2). As we traverse down the tree, the factors become less relevant. For example, in Figure 2, the comment
is the most relevant factor because it is the root of the tree (level
0). The next two most relevant factors are num-CC and reporter’s
experience (both in level 1) and so on. In the Top Node analysis, the
combination of the level in which a factor is found along with its occurrences determines the importance of such as factor. If, for example,
the product factor appears as the root in seven of the ten trees and
the platform factor appears as the root in the remaining, we would
report product as the ﬁrst most important factor and platform as the
second most important factor.
Table 15 reports the Top Node analysis results for our eight projects.
The description and the comments included in the bugs are the most
important factors. For example, the description text is the most
important factor in Chromium, FreeDesktop, NetBeans and Gentoo;
and the second most important factor in Eclipse, Mozilla, OpenOﬃce
and Fedora. Likewise, the comment text is the most important factor
in Mozilla, OpenOﬃce and Fedora; and the third most important
in NetBeans.
Words such as “dtrace”, “pthreads”, “scheduling”,
“glitches” and “underestimate” are associated with blocking bugs
by the Naive Bayes Classiﬁer. On the other hand, words such as
“duplicate”, “harmless”, “evolution”, “enhancement” and “upgrading”
are associated with non-blocking bugs.
 
 
Table 15: Top Node analysis results
Description text
Rep. Blocking experience
Description text
Rep. Blocking experience
Description text
Comment size
Rep. Blocking experience
Comment text
Comment size
Rep. Blocking experience
Description text
Description text
FreeDesktop
Description text
Comment text
Description text
Description text
Rep. Blocking experience
Comment text
Description text
Rep. Blocking experience
Rep. Blocking experience
Rep. experience
Comment size
Description text
Comment text
Description text
Rep. Blocking experience
Description text
Comment text
Rep. Blocking experience
Rep. Blocking experience
Description text
Rep. experience
Rep. Blocking experience
Description text
Comment text
Description text
Rep. Blocking experience
Description text
Comment text
Rep. Blocking experience
Rep. experience
Rep. Blocking experience
Description size
Comment text
 
 
The experience of reporting previous blocking bugs (Rep. Blocking
experience) is the most important factor for Eclipse, and the second most important for Chromium and NetBeans. It also appears
consistently in the second and third levels of all the projects.
Other factors such as priority, component, number of developers in the
CC list, reporter’s name, reporter’s experience, and description-size
are only present in the second and third levels of two or less projects.
This means that among the factors reported in Table 15, such factors
are the less important.
We can build prediction models that can achieve F-measure
values ranging from 21% to 54% when detecting blocking bugs.
In addition, we ﬁnd that the
description and
comment text are
the most important factors in determining blocking bugs for the
majority of the projects, followed by the Rep. Blocking experience.
4. Relaxing the Data Collection Process
4.1. Prediction models using data available after 24 hours after the bug report
submission
So far, we trained our prediction models with bug report information collected
within the 24 hours after the initial submission. One limitation of this approach
is that a large number of bug reports do not have any information recorded for
some of the factors. For example, we found that around 92%-98% of the bug
reports have empty values for severity, priority, priority has increased, platform
and product. Therefore, it is worth investigating whether relaxing the data
collection period could improve the performance of our prediction models.
In Table 16, we present the performance of prediction models trained on data
collected without the “24 hours restriction”. More precisely, for non-blocking
bugs we used data before their blocking-dates and for blocking-bugs we used data
before their blocking-date. From Table 16, it can be seen that the F-measures
 
 
range from 14.1% to 42.1%. These values are lower that the F-measures of our
original models (21.2% to 54.3%) presented in Table 13. This suggests that
collecting data after the blocking-date and closing-date is not worth the eﬀort.
One possible explanation for the performance degradation of the prediction
models is that relaxing the data collection process introduces noise into the data
Table 16: Performance of the decision tree models using data collected after 24
hours after the initial submission
FreeDesktop
4.2. Dealing with the Reporter’s name factor
While building our prediction models, we faced computational issues caused by
the reporter’s name factor. In our data set, we found approximately 100,000 different reporters. We summarize the number of unique reporters for all of the
projects in Table 17. Having a nominal factor with such high number of levels is
computational expensive and impractical. For example, a logistic model trained
on the Chromium data would create 16,209 dummy variable to account for the
diﬀerent levels of the nominal factor reporter’s name. To overcome this issue and
because we are interested in the impact of non-sporadic developers, we reduced
the number of levels by considering the top K reporters (of each project) with
the highest number of reported bugs. The remaining reporters were grouped
 
 
into a level named “others”. In our work, we considered a value of K = 200 (i.e.,
the top 200 reporters) for the prediction models in RQ3.
Table 17: Number of diﬀerent reporters (nominal levels)
Num. unique
FreeDesktop
Instead of performing a sensitivity analysis to determine whether other values
of K (e.g., 50, 100, 300, etc.) have a potential eﬀect on the models’ performance,
we followed a slightly diﬀerent approach. First, we removed the reporter’s name
from the data set, and then re-built the prediction models. Our experiments show
that these models achieved F-measures of 19.7% to 53.2%, which are similar to the
performance of models considering the reporter’s name built in RQ3 (F-measures
of 21.2% to 54.3%). These ﬁndings suggest that the reporter’s name does not
play a signiﬁcant role in predicting blocking bugs. A detailed information about
the models built in this section (precision, recall and F-measure) for all of the
projects can be found in our online appendix .
5. Threats to Validity
Internal Validity We used standard statistical libraries and methods
to perform our predictions and statistical analysis (e.g., Weka and R
programming). We also rely on a commercial tool (Scitools Understand)
to extract the code metrics. Although these tools are not perfect, they have
been used by other researchers in the past for bug prediction .
 
 
Construct Validity The main threat here is the quality of the ground
truth for blocking bugs. We used the information in the “Blocks” ﬁeld of
the bug reports to determine blocking and non-blocking bugs. In some
cases, developers could have mistakenly ﬁlled that ﬁeld. We inspected
a subset of the blocking bugs in each of our projects and we found no
evidence of such a mistake.
For the nominal factor: reporter name, we considered the top K = 200
reporters and grouped the remaining reporters into one level. This approach
signiﬁcantly reduced the number of diﬀerent levels for that factor. Although
using a diﬀerent number K for the top reporters may change our results, we
found that reporter name does not play a signiﬁcant role in the prediction
models. In addition, we used the number of previous reported bugs as
the experience of a reporter. In some cases, using the number of previous
reported bugs may not be indicative of actual developer experience, however
similar measures were used in prior studies .
In RQ2, we used Lack of Cohesion, Coupling between Objects, Cyclomatic
Complexity and LOC as proxy metrics to quantify the impact of blocking
bugs at ﬁle level. Although these metrics have also been reported to be
useful for architectural evaluation, other architectural and design metrics
such code smell metrics may quantify diﬀerently the eﬀects of blocking
bugs on software systems.
Our data set suﬀers from the class imbalance problem. In most of the
projects, the percentage of blocking bugs account for less than 12% of
the total data.
This causes the classiﬁer not to learn to identify the
blocking bugs very well. To mitigate this problem, we use re-sampling
of our training data and stratiﬁed cross-validation.
To calculate the
Bayesian-scores, we ﬁltered out all the words with less than ﬁve occurrences
in the corpora. Increasing this threshold will produce diﬀerent scores,
however, it will introduce more noise. Furthermore, the Bayesian-score of
a description/comment is based on the combined probability of the ﬁfteen
 
 
most important words of the description/comment. Changing this number
may impact our ﬁnding.
Our work did not considered bugs with status other than resolved or
closed, because we wanted to investigate only well identiﬁed blocking and
non-blocking bugs. However, unlike non-blocking bugs, the blocking bugs
may not be restricted to veriﬁed or closed bugs. In most of the cases, bugs
marked as blocking bugs remain that way until their closed-date. In the
future, we plan to include these blocking bugs in order to improve the
accuracy of our model.
Many of the projects do not follow any formal guidelines to label bug
reports in the commits. To extract the links between bug reports and
commits, we tried to match the bug-IDs in the messages of the commits with
diﬀerent regular expressions that may not consider all possible patterns.
Therefore, our data set might not be complete and/or contain false positive
bug-ﬁxes. To reduce the impact of this threat, we manually inspected a
subset of the linked commits and their respective bug reports generated
by each regular expression. Additionally, we might miss actual bug-ﬁxes
in which the developer did not include the related bug-report. Although
more sophisticated methods (Wu et al. and Nguyen et al. ) can
improve the identiﬁcation of bug-ﬁxes, our approach was able to extract a
large number of bug-ﬁxes (263,345) which is a rich data set suitable for
the purpose of this study.
In Software Bug Prediction research area, there are two well-known model
evaluation settings: Cross-validation and Forward-release/Crossrelease validation (i.e., train models with data from previous
releases and test them with data of the next release). Although, Forwardrelease is closer to what can be deployed in a real environment, both
approaches (and most of the studies in the Software Bug Prediction area)
assume little or no autocorrelation in the dataset. In other words, the
instances in the dataset do not have any signiﬁcant temporal dependency
 
 
among them. Since we are using cross-validation to evaluate our models
performance, we are implicitly assuming no autocorrelation in our dataset
and as a result, our prediction models do no account for this kind of
correlation. Therefore, if there was a high autocorrelation in our dataset,
then other techniques such time series analysis could potentially improve
the performance of our models.
External Validity Including more software systems improves the gen-
erality of research ﬁndings (which is a diﬃcult and important aspect in
SE research). In this work, we increase the generality of our results by
studying 609,800 bug reports and 263,345 bug-ﬁxing commits from eight
projects. That said, not always having a set of diverse projects is better
because it might introduce outliers that can impact the generality of the
ﬁndings. To combat this, we considered long-live and large open-source
mostly written in Java and C++.
6. Related Work
Re-opened bug prediction: Similar to our work, however focusing on
diﬀerent types of bugs, prior work by Shihab et al. studied re-opened bugs
on three open-source projects and proposed prediction models based on decision
trees in order to detect such type of bugs. In their work, they used 22 diﬀerent
factors from 4 dimensions to train their models. Xia et al. in compared the
performance of diﬀerent machine learning methods to predict re-opened bugs.
They found that Bagging and Decision Table algorithms presents better results
than decision trees when predicting re-opened bugs. Zimmermann et al. 
also investigated and characterized re-opened bugs in Windows. They performed
a survey to identify possible causes of reopened bugs and built statistical models
to determine the impact of various factors. The extracted factors in our data
sets are similar to those used in the previous works (specially in ). Addi-
tionally, we also use decision trees as our prediction models. However our work
 
 
diﬀers in that we are not interested in predicting reopened bugs, but instead in
predicting blocking bugs.
Fix-time prediction: A prediction model for estimating the bug’s ﬁxing eﬀort
based on previous bugs with similar textual information has been proposed
by Weiss et al. . Given a new bug report, they use kNN along with text
similarity techniques for ﬁnding the bugs with closely related descriptions. The
average eﬀort of these bugs are used to estimate the ﬁxing eﬀort of the given
bug report. Panjer et al. in used decision trees and other machine learning
methods to predict the lifetime of Eclipse bugs. Since the classiﬁers do not
deal with a continuous response variable, they discretized the lifetime into seven
categories. Their models considered only primitive factors taken directly from
the bug database (e.g., ﬁxer, severity, component, number of comments, etc.)
and achieved accuracies of 31-34%. Marks et al. used Random Forest
to predict bug’s ﬁxing time. Using the bugs from Eclipse and Mozilla, they
examined the ﬁxing time along 3 dimensions: location, reporter and description.
Following an approach similar to Panjer, Marks et al. discretized the ﬁxing
time into 3 categories (within 1 month, within 1 year, more than a year). For
both projects their method was able to yield an accuracy of about 65%. In our
work, we also used decision trees as prediction models, but instead of predicting
the bug’s lifetime, we try to predict blocking bugs. Bhattacharya et al. 
performed multivariate regression testing to determine the relationship strength
between various bug report factors and the ﬁxing time. They found that the
dependency among software bugs (i.e., blocking dependency) is an important
factor that contributes to predict the ﬁxing time. Our work is not directly related
to bug-ﬁxing time prediction, but the results in motivate the study and
characterization of blocking bugs.
Severity/Priority prediction: Other works focused on the prediction of
speciﬁc bug report ﬁelds . Lamkanﬁet al. trained Naive
Bayes classiﬁers with textual information from bug reports on Eclipse and
Mozilla to determine the severity of such bugs.
In another paper , the
authors compared the performance of four machine learning algorithms (Naive
 
 
Bayes, Naive Bayes Multinomial, kNN and SVM) for predicting the bug severity
and found that Naive Bayes Multinomial is the fastest and most accurate.
Menzies et al. used a rule-based algorithm for predicting the severity of
bug reports using their textual descriptions. They evaluated their method using
data from a NASA’s bug tracking system. Sharma et al. evaluated diﬀerent
classiﬁers for predicting the priority of bugs in OpenOﬃce and Eclipse. Their
prediction models achieved accuracies above 70%. Our work diﬀers from the
previous studies in that we used that information to predict blocking bug rather
than the severity/priority. In fact, we used the severity and priority of the
bug reports in our factors.
Bug triaging and Duplicate bug detection: Other studies use textual information from bug reports such as summary, description and execution trace
for semi-automatic triage process and bug duplicate detection
 . The key idea in the majority of these works is to apply natural language processing (NLP) and information retrieval techniques in order to ﬁnd a set
of bug reports that are similar to a target bug (new bug). Based on this suggested
list of similar bugs, the triager can, for example, recommend the appropriate
developer to incoming bugs or ﬁlter out those already-reported bugs. Similar to
these works, we included textual-based factors (comments and description) in our
prediction models with the diﬀerence that instead of using a vector space representation, we converted them into numerical factors following the same approach
used by , .
Bug localization: Prior studies have proposed method to localize buggy ﬁles of
a given new bug report . Nguyen et al. proposed BugScout, a new
topic model based on Latent Dirichlet Allocation that can assist practitioners in
automatically locating buggy ﬁles associated to a bug report. They exploited
the technical aspects shared by the textual content of ﬁles between code and
bug reports in order to correlate buggy ﬁles and bugs. Zhou et al. proposed
BugLocator, a method based on a revised Vector Space Model for locating source
code ﬁles relevant to a initial bug report. To rank potential buggy ﬁles, the
method uses (a) text similarity between a new bug report and the source code
 
 
ﬁles, (b) historical data of prior ﬁxed reports and (c) source code size. Kim et
al. proposed a two-phase machine learning model to suggests the ﬁles that
are likely to be ﬁxed in response to a given bug report. In the ﬁrst phase, their
model assesses whether the bug report has suﬃcient information. In the second
phase, the model proceeds to predict ﬁles to be ﬁxed only if it believes that
the bug report is predictable. To train the model, the authors considered only
basic metadata and initial comments posted within 24 from the bug submission.
Our work diﬀers from these previous studies in that their goal is to recommend
relevant ﬁles related to a given bug report, whereas our main goal is to predict
whether a given bug report will be a blocking bug or not. That said, since these
bug localization techniques use textual information to do their recommendations,
they can easily be used in conjunction with our prediction models to identify
potential buggy ﬁles with blocking bugs (as we pointed out at the end of RQ2).
7. Conclusion and Future Work
Blocking bugs increase the maintenance cost, cause delays in the release of
software projects, and may result in a loss of market share. Our empirical study
shows that blocking bugs take up 2 times longer and require 1.2-4.7 times more
lines of code to be ﬁxed than non-blocking bugs. On further analysis, we found
that ﬁles aﬀected by blocking bugs are more negatively impacted in terms of
cohesion, coupling complexity and size than ﬁles aﬀected by non-blocking bugs.
For example, we ﬁnd that ﬁles with blocking bugs are 1.3-12.2 times bigger (in
LOC) than ﬁles with non-blocking bugs. Based on our ﬁndings, we suggest that
practitioners should allocate more QA eﬀort when ﬁxing blocking bugs and ﬁles
related to them.
Since these bugs have such severe consequences, it is important to identify
them early on in order to reduce their impact. In this paper, we build prediction
models based on decision trees to predict whether a bug will be a blocking bug
or not. As our data set, we used 14 factors extracted from the bug repositories
of eight large open source projects.
The results of our investigation shows
 
 
that our models achieve 13%-45% precision, 47%-66% recall and 21%-54% Fmeasure when predicting blocking bugs. On the other hand, our Top Node
analysis shows that the most important factors to determine blocking bugs
are the description, comments and the reporter’s blocking experience. In the
future, we plan to model the blocking dependency of the bug reports as a graph
structure and study it using network analysis. Particularly, we are interested in
deriving network measures to incorporate them in our prediction models and
examine whether they improve the prediction performance (Zimmermann et
al. followed a similar approach in ). We also plan to extend this work by
performing feature selection on our factors. Employing feature selection may
improve the performance of our models since it removes redundant factors. From
the architectural point of view, we would like to exploit the source code topology
to identify hotspots in the architecture of software systems caused by blocking
bugs. Furthermore, we plan to perform qualitative analyses similar to at
factor and ﬁle level to better understand (a) the inﬂuence of certain factors and
(b) the characteristics of buggy ﬁles aﬀected by blocking bugs.