MITSUBISHI ELECTRIC RESEARCH LABORATORIES
 
Ensemble Learning for Speech
Enhancement
Le Roux, J.; Watanabe, S.; Hershey, J.R.
TR2013-098
October 2013
Over the years, countless algorithms have been proposed to solve the problem of speech enhancement from a noisy mixture. Many have succeeded in improving at least parts of the signal, while
often deteriorating others. Based on the assumption that different algorithms are likely to enjoy
different qualities and suffer from different ﬂaws, we investigate the possibility of combining
the strengths of multiple speech enhancement algorithms, formulating the problem in an ensemble learning framework. As a ﬁrst example of such a system, we consider the prediction of a
time-frequency mask obtained from the clean speech, based on the outputs of various algorithms
applied on the noisy mixture. We consider several approaches involving various notions of context and various machine learning algorithms for classiﬁcation, in the case of binary masks, and
regression, in the case of continuous masks. We show that combining several algorithms in this
way can lead to an improvement in enhancement performance, while simple averaging or voting
techniques fail to do so.
IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)
This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part
without payment of fee is granted for nonproﬁt educational and research purposes provided that all such whole or partial copies include
the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of
the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or
republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All
rights reserved.
Copyright c⃝Mitsubishi Electric Research Laboratories, Inc., 2013
201 Broadway, Cambridge, Massachusetts 02139
MERLCoverPageSide2
2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
October 20-23, 2013, New Paltz, NY
ENSEMBLE LEARNING FOR SPEECH ENHANCEMENT
Jonathan Le Roux, Shinji Watanabe, John R. Hershey
Mitsubishi Electric Research Laboratories (MERL), 201 Broadway, Cambridge, MA 02139, USA,
{leroux,watanabe,hershey}@merl.com
Over the years, countless algorithms have been proposed to solve
the problem of speech enhancement from a noisy mixture. Many
have succeeded in improving at least parts of the signal, while often
deteriorating others. Based on the assumption that different algorithms are likely to enjoy different qualities and suffer from different ﬂaws, we investigate the possibility of combining the strengths
of multiple speech enhancement algorithms, formulating the problem in an ensemble learning framework. As a ﬁrst example of such
a system, we consider the prediction of a time-frequency mask obtained from the clean speech, based on the outputs of various algorithms applied on the noisy mixture. We consider several approaches involving various notions of context and various machine
learning algorithms for classiﬁcation, in the case of binary masks,
and regression, in the case of continuous masks. We show that combining several algorithms in this way can lead to an improvement in
enhancement performance, while simple averaging or voting techniques fail to do so.
Index Terms— Ensemble learning, Speech enhancement,
Time-frequency mask, Classiﬁcation, Stacking
1. INTRODUCTION
Speech enhancement methods attempt to improve the quality and
intelligibility of speech that has been degraded by interfering noise
or other processes. One thing that makes this problem difﬁcult is
that the interference can come in many different varieties. To further complicate matters, often the operational constraints on computation and latency preclude the use of complex models that can
represent and adapt to many different noise types. As it is difﬁcult
for a simple algorithm to accommodate the variety of conditions,
some assumptions about the statistical properties of the target and
interference signals have to be made. Over the years, many different algorithms have been proposed, each having different explicit
or implicit assumptions about the nature of the speech and interference . Assuming that the strengths and weaknesses of a set of
algorithms differ, it would be desirable to combine them in a way
that takes advantage of all their strengths.
Ensemble machine learning methods aim at combining different models, and exploit the independence of the errors made by each
classiﬁer to reduce the estimation variance, and hence the error rate.
These methods range from simple voting procedures, where the
quantities inferred by each model are averaged together, to stacking, in which a secondary model is trained to perform the combination in a way that is tuned to training data. An advantage of voting
methods is that they can be applied without consideration of the test
conditions. However, stacking methods can learn more complex
combination functions, potentially leading to better performance.
Ensemble methods have been used extensively in automatic
speech recognition (ASR) to fuse speech recognition hypotheses
of different recognizers via voting procedures such as recognizer
output voting error reduction (ROVER) . Particularly relevant to
our work here are ensemble ASR methods in which the recognizers
differ according to the enhancement or robustness algorithms used
in their front end . A chief advantage of ensemble methods is
that they can build upon a variety of existing algorithms to improve
performance.
To make use of ensemble learning in the speech enhancement
paradigm, we consider a more direct integration of the enhancement algorithms. We compute the time-frequency masking functions that, when applied to the noisy spectrogram, yield the spectrum of the enhanced signals. The result of their combination is to
produce an ensemble time-frequency masking function. Here, for
simplicity, we primarily focus on the estimation of binary masking
functions, and only touch upon the estimation of continuous masking functions. We investigate both simple voting as well as stacking, in which a variety of classiﬁcation algorithms, such as support
vector machines (SVM) , naive Bayes classiﬁers (NB) , decision trees (DT) , and random forests (RF) , are used to infer
the binary masking function. Estimation of binary masks for enhancement and separation has been considered in a machine learning context before , but not in an ensemble learning
framework.
In experiments with difﬁcult interference conditions, we show
that a combination of several enhancement algorithms using stacking can lead to an improvement in enhancement performance,
whereas simple averaging or voting techniques fail to do so.
2. GENERAL FRAMEWORK
We assume an ensemble of speech enhancement algorithms that are
to be treated as “black boxes” in the sense that we only use the
enhanced signals for combination. It would also be reasonable to
combine enhancement algorithms at the “decision” level using some
internal representations. However, we would like to allow the use
of arbitrary models and avoid the use of heterogeneous features.
We thus perform the combination in a domain that is independent of the particular formulation of each enhancement algorithm.
A good choice for such a domain is the short-time power spectrum,
which is widely used in signal processing because of its relative
insensitivity to phase and its ability to reveal time and frequency
patterns in the signal. Regardless of the internal representation they
use, speech enhancement algorithms take as input a noisy signal
y[t] in the time domain and transform it to an enhanced estimate
ˆx[t] of the clean signal. In the short-time power spectrum domain,
this enhancement process can be approximated as applying a timefrequency masking function to the spectrogram of the noisy input
signal. If the optimal masking function were known, the speech sig-
2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
October 20-23, 2013, New Paltz, NY
Enhancement
Algorithm 1
Enhancement
Algorithm J
Computation
Computation
Combination
Resynthesis
Figure 1: Overview of the framework.
nal could be reconstructed almost perfectly by applying the masking function to the noisy power spectrum and inverting the representation. Our method is thus to combine time-frequency masking
functions obtained from the enhancement algorithms, in order to estimate an optimal masking function to better reconstruct the speech.
For a given enhancement algorithm j in our ensemble, we de-
ﬁne an equivalent continuous masking function, w(j)
n,f, for time
frame n and frequency f. We also formulate a target masking function w∗
n,f as that which transforms the noisy spectrum into the clean
spectrum. For simplicity, the masking functions can be approximated as binary masking functions, m(j)
n,f, and m∗
The binary target mask m∗
n,f is convenient in that the ensemble
inference problem can be posed as binary classiﬁcation, where a
classiﬁer computes a binary mask estimate ˆmn,f using as input the
masking functions {w(1)
n,f, ..., w(J)
n,f}, or their binary counterparts,
derived from each of the enhancement algorithms.
Simple voting or averaging procedures on the input signals or
their masking functions could be used, but here we also investigate
stacking approaches in which the method of combination is learned
from training data. In the context of stacking, we can also consider
the temporal and frequency context in the neighborhood of each
masking function value to be estimated.
Once the combined mask is inferred, it can be applied to the
noisy signal spectrum, and combined with noisy phases to produce the estimated speech signal, ˆx[t], via an inverse transform such
as the overlap-add procedure for the short-time Fourier transforms
(STFT). The overall system architecture is shown in Fig. 1.
3. TARGETS
Time-frequency masking functions estimated from the noisy mixture have often been used as a means to perform source separation
or speech enhancement . Time-frequency masks apply a weight
to each bin of a time-frequency representation of the acoustic input, such as cochleograms, short-time Fourier transforms, wavelet
transforms, and so on, to emphasize regions which are dominated
by the target source and suppress regions which are dominated by
other sources. The weight values can be either binary or continuous.
Continuous values can be interpreted as the energy ratio between the
target and the mixture, as in a Wiener ﬁlter, or as the probability that
the corresponding bin belongs to the target source.
Restricting the mask to take only binary values has been shown
to be a reasonable proxy for the optimal masking function in general conditions . Binary masks have the disadvantage that they
cannot account for cancellation effects and may introduce strong artifacts depending on the interfering noise. However, advantages in
our setting include the ease of estimation of the two possible values
instead of a continuum, as well as their potential for computational
savings. We thus here mainly focus on the binarized continuous
mask obtained from the clean speech as the target for our method,
and only touch upon the use of continuous masks in a regression
framework.
As mentioned above, each enhancement algorithm may be processing the noisy input signal in various domains, whether directly in the
time domain or more likely in some time-frequency representation
such as the STFT or a Gammatone-based transform, with various
ﬁlterbank settings. Instead of directly attempting to combine these
inner representations, we choose here to use the ﬁnal outputs, the
enhanced time-domain signals ˆx(j), j = 1, . . . , J. This enables us
to consider any speech enhancement algorithm as a potential input
to our system, regardless of its implementational details.
From these enhanced signals, we could consider deriving any
type of features for combination.
For convenience and simplicity, we consider here re-analysing all enhanced signals using the
same common time-frequency representation used to derive the target. This enables us to have a direct correspondence between the
time-frequency bins of the input features and those of the target.
To avoid feature-scaling issues, we do not directly use features
such as the power spectrogram or log-power spectrogram, but deﬁne
an equivalent continuous mask w(j) for each algorithm as the ratio
of the power spectrogram of the enhanced signal ˆ
X(j) to that of the
noisy mixture Y :
and similarly for w∗
n,f. This approximates each algorithm as a
reweighting method in a common time-frequency representation.
Finally, we also derive binary mask features m(j) from the continuous masks: m(j)
n,f = [w(j)
n,f > 0.5], and m∗
n,f > 0.5],
where [a > b] = 1 if a > b and 0 otherwise. The motivation for
considering binary masks as inputs is two-fold: they may lead to
more robust estimators; and their use can reduce the computational
cost with regard to the continuous masks, for example with support
vector machines and decision trees.
5. INFERENCE ALGORITHMS
5.1. Voting
Voting or averaging is an ensemble combination strategy that simply combines outputs of the models by taking an average of their
2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
October 20-23, 2013, New Paltz, NY
values. In the case of classiﬁcation, the output is usually the mode
of the distribution over classes, whereas in regression, the output
would be the mean or some other average of the output values. Uncertainty within each model can also be considered, but here since
we derive the mask values from an ensemble of arbitrary enhancement methods, we do not consider the uncertainty within each enhancement algorithm.
In voting, continuous or binary mask values for all algorithms
at time-frequency bin (n, f) are used to estimate the target mask
(either w∗
n,f) at the same bin. The input feature vectors are
thus typically zn,f = (w(1)
n,f, ..., w(J)
n,f)T for the continuous masks
and zn,f = (m(1)
n,f, ..., m(J)
n,f)T for the binary masks.
For the continuous masking function inputs, we consider the
mean of the masking values as a continuous mask estimate, which
corresponds to averaging the original power spectrum estimates.
We also consider the median in a similar way.
For the binary masking function inputs, voting considers the
mode of the masking value distribution:
n,f > 0.5].
Since there are no learned parameters, voting methods cannot
over-ﬁt the training data. To the extent that the masking values make
uncorrelated errors, then voting and averaging procedures tend to
recover from these errors. In other words, the variance across classiﬁers can be reduced by the voting procedure. However, whenever
the errors are correlated, the averaging just reinforces the errors, so
the classiﬁer can remain biased.
5.2. Stacking
Stacking is an ensemble learning strategy in which multiple estimation algorithms for the same task are used as input into a ﬁnal
algorithm that is trained on data to combine their results. This procedure can reduce the bias even when the outputs of the ensemble
are correlated; however, the learning may also over-ﬁt the training
data. The case of binary mask targets allows us to use simple binary
classiﬁers to produce mask estimates. One can also use different
forms of regression to produce continuous mask estimates, but here
we mainly focus on a classiﬁcation-based approach. We investigated a variety of classiﬁers, such as SVM, NB, DT, and RF.
We here consider separate classiﬁers Cf
Θf for each frequency f,
with parameters Θf. At each frame n, given an input vector in,f,
the classiﬁer produces a mask estimate ˆmn,f = Cf
Θf (in,f). We
ﬁrst learn the parameters Θf so that they minimize a loss function
L with respect to the target mask m∗
n,f on training data T :
¯Θf = argmin
Θf (in,f), m∗
n,f), n ∈T ],
At test time, we estimate the mask using the learned parameters ¯Θf:
ˆmn,f = Cf
¯Θf (in,f),
The loss function L is determined by the classiﬁer type.
In the framework of stacking, we can consider including time
and/or frequency context information into the input feature vectors.
Here, we extend the features in the time direction by c(n) frames to
the left and to the right, and in the frequency direction by c(f) frequency bins below and above. The input feature vector to estimate
n,f thus consists of the concatenation of time-frequency patches
with (2c(n) + 1) × (2c(f) + 1) elements in the neighborhood of the
bin (n, f) for each algorithm. The boundary cases in both directions are handled appropriately.
6. EVALUATION
6.1. Setup
We used audio data from the medium vocabulary task (Track
2) of the 2nd CHiME Speech Separation and Recognition Challenge . The speech is taken from the Wall Street Journal (WSJ0)
5k vocabulary read speech corpus, and convolved with binaural
room impulse responses before mixing with binaural recordings of
a noisy domestic environment. The RT60 of the room is 300 ms.
The noise excerpts are selected as to obtain input signal-to-noise ratio (SNR) ranges of −6, −3, 0, 3, 6, and 9 dB without rescaling.
Noises are highly non-stationary, such as speech by other speakers,
home noises, or music, making the denoising task very challenging.
As we need parallel data to train our system as well as to evaluate its enhancement performance, we randomly sample utterances
across all input SNRs from the development set data (si_dt_05),
for which scaled reverberated clean speech data are provided on top
of the noisy mixture data, to build a training set of 100 utterances
with random input SNR, and an evaluation set of 600 utterances,
100 for each input SNR.
The sampling rate was 16 kHz. The common time-frequency
representation for the target and all enhancement output signals was
obtained using the short-time Fourier transform with a frame length
of 640 samples, 50% overlap and a sine window for analysis and
re-synthesis.
Performance was evaluated in terms of averaged signal-todistortion ratio (SDR), using the bss_eval toolbox . The
SDR averaged over the noisy mixtures was 1.85 dB. Resynthesizing
the clean speech from its equivalent continuous mask w∗led to an
average SDR of 17.54 dB, and the binarized continuous mask m∗
led to 17.01 dB, which represents the ideal performance that could
be expected from this implementation of our method.
We considered the following speech enhancement algorithms,
which constitute a varied set of techniques, including state-of-theart methods: vector-Taylor series (VTS) , indirect VTS ,
OMLSA-IMCRA , as well as implementations of the classical
MMSE and log-MMSE algorithms taken from . Results for these
input algorithms are shown in Table 1. CMask and BMask denote
the result of applying respectively the equivalent continuous mask
and its binarized version to the noisy complex spectrum, and resynthesizing to the time domain. The continuous mask was truncated
to values between 0 and 1. Note that, differing from , VTS and
OMLSA performed better than indirect VTS on this data.
6.2. Results
We ﬁrst investigate the performance of averaging on the input continuous masks, both using mean and median, and of voting on the
binarized masks. As shown in Table 2, none of these methods led to
improvements compared to the input algorithms, the performance
actually decreasing with respect to the best ones. While voting in
particular is known to help when combining complex systems such
as in ASR, the poor performance here could be due to the fact that
the combination is impacted by the poorly performing algorithms
in a direct way, while processing by complex systems may still lead
to interesting hypotheses prior to combination.
2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
October 20-23, 2013, New Paltz, NY
Table 1: SDR (dB) for each input speech enhancement algorithm
indirectVTS
∗VTS did not use the truncation explained in .
Table 2: SDR (dB) for averaging and voting methods
We now turn to ensemble learning methods. First, Table 3 investigates the performance of each classiﬁer (linear SVM , DT:
decision tree, RF: random forest, NB: naive Bayes) under the simple experimental condition “B →B”, i.e., where both input features
and output targets are binary masks. In this preliminary experiment,
we did not attempt to tune the regularization parameters of the classiﬁers. Every approach was comparable to or outperformed the best
performing single speech enhancement algorithm, here VTS. Although the random forest achieved the best performance (SDR =
5.92 dB) without considering time-frequency contexts, adding them
did not seem to improve performance for DT, RF and NB. As it also
drastically increased computational cost for RF and NB, we did not
consider large contexts for these classiﬁers. On the other hand, we
found that the performance of SVM improved consistently when
the feature dimensionality increased by considering contexts. This
is reasonable, since SVM can make use of redundant features to set
accurate and robust classiﬁcation bounds, while the other classiﬁers
face over-training problems. Based on the results of Table 3, the
subsequent experiments focus on SVM classiﬁer results using the
whole frame as frequency context in the input features.
Table 4 compares the SVM results with binary masks (B →
B) and continuous masks (C →B) as features to estimate binary
masks. We also estimated continuous masks in the output by using support vector regression with continuous mask features (C →
C). Table 4 shows that the continuous feature case outperformed the
binary feature case by up to 1.32 dB, which indicates that the continuous values are informative features to combine speech enhancement algorithms. The result of the continuous mask estimation did
not outperform that of the binary mask estimation in this setting,
although other regression methods may lead to better results.
Finally, we scaled up the experiments by increasing the size of
the training data (100 →1260 utterances), as shown in Table 4.
We ﬁnally obtained 7.97 dB, which improved from the best single
system (VTS) by 2.36 dB, and from the voting method by 3.21 dB.
This conﬁrms the effectiveness of our system combination approach
based on ensemble learning.
7. CONCLUSION
We presented an ensemble learning approach to speech enhancement. By learning how to combine the outputs of multiple enhancement algorithms, we were able to signiﬁcantly outperform the original algorithms. Future work will investigate further the use of regression to estimate continuous masking functions, as well as the
inﬂuence of the proposed system on ASR performance.
8. REFERENCES
 P. C. Loizou, Speech Enhancement, Theory and Practice.
Boca Raton, FL: CRC Press, 2007.
Table 3: SDR (dB) for each classiﬁer (B →B, i.e., Input feature:
binary feature. Output target: binary mask)
Context (time c(n), bin c(f))
No context (0, 0)
Table 4: SDR (dB) for various types of inputs/outputs using SVM
C→B (1260 utt.)
 J. G. Fiscus, “A post-processing system to yield reduced word error
rates: Recognizer output voting error reduction (ROVER),” in Proc.
ASRU, 1997, pp. 347–354.
 J. Barker, E. Vincent, N. Ma, H. Christensen, and P. Green, “The PAS-
CAL CHiME speech separation and recognition challenge,” Computer Speech & Language, 2012.
 C. Cortes and V. Vapnik, “Support vector machine,” Machine learning, vol. 20, no. 3, pp. 273–297, 1995.
 D. D. Lewis, “Naive (Bayes) at forty: The independence assumption
in information retrieval,” in Proc. ECML, 1998, pp. 4–15.
 L. Olshen, J. H. Breiman, R. A. Friedman, and C. J. Stone, Classiﬁcation and Regression Trees.
Wadsworth International Group, 1984.
 L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.
5–32, 2001.
 F. R. Bach and M. I. Jordan, “Learning spectral clustering, with application to speech separation,” JMLR, vol. 7, pp. 1963–2001, 2006.
 R. J. Weiss and D. P. W. Ellis, “Estimating single-channel source separation masks: Relevance vector machine classiﬁers vs. pitch-based
masking,” in Proc. SAPA, 2006, pp. 31–36.
 G. Kim, Y. Lu, Y. Hu, and P. C. Loizou, “An algorithm that improves
speech intelligibility in noise for normal-hearing listeners,” J. Acoust.
Soc. Am., vol. 126, no. 3, pp. 1486–1494, 2009.
 K. Han and D. Wang, “A classiﬁcation based approach to speech segregation,” J. Acoust. Soc. Am., vol. 132, no. 5, pp. 3475–3483, 2012.
 D. Wang and G. J. Brown, Eds., Computational Auditory Scene Analysis: Principles, Algorithms, and Applications.
Wiley, 2006.
 D. Wang, “On ideal binary mask as the computational goal of auditory
scene analysis,” in Speech separation by humans and machines, P. Divenyi, Ed.
Kluwer Academic Publishers, 2005, ch. 12, pp. 181–197.
 E. Vincent, J. Barker, S. Watanabe, J. Le Roux, F. Nesta, and
M. Matassoni, “The second CHiME speech separation and recognition challenge: Datasets, tasks and baselines,” in Proc. ICASSP, May
 E. Vincent, R. Gribonval, and C. F´evotte, “Performance measurement
in blind audio source separation,” IEEE Trans. ASLP, vol. 14, no. 4,
pp. 1462–1469, 2006.
 P. J. Moreno, B. Raj, and R. M. Stern, “A vector Taylor series approach for environment-independent speech recognition,” in Proc.
ICASSP, vol. 2, May 1996, pp. 733–736.
 J. Le Roux and J. R. Hershey, “Indirect model-based speech enhancement,” in Proc. ICASSP, Mar. 2012, pp. 4045–4048.
 I. Cohen, “Noise spectrum estimation in adverse environments: Improved minima controlled recursive averaging,” IEEE Trans. SAP,
vol. 11, no. 5, pp. 466–475, 2003.
 R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin, “LI-
BLINEAR: A library for large linear classiﬁcation,” JMLR, vol. 9, pp.
1871–1874, 2008.