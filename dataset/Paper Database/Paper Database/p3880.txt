MIT Open Access Articles
AMC: AutoML for Model Compression
and Acceleration on Mobile Devices
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: He, Yihui et al. "AMC: AutoML for Model Compression and Acceleration on Mobile
Devices." Computer vision -- ECCV 2018 : 15th European Conference, Lecture Notes in
Computer Science, 11211, Springer, 2018, 815-832 © 2018 The Author(s)
As Published: 10.1007/978-3-030-01234-2_48
Publisher: Springer Science and Business Media LLC
Persistent URL: 
Version: Author's final manuscript: final author's manuscript post peer review, without
publisher's formatting or copy editing
Terms of use: Creative Commons Attribution-Noncommercial-Share Alike
AMC: AutoML for Model Compression
and Acceleration on Mobile Devices
Yihui He‡∗, Ji Lin†∗, Zhijian Liu†, Hanrui Wang†, Li-Jia Li↕, and Song Han†
{jilin, songhan}@mit.edu
†Massachusetts Institute of Technology
‡Carnegie Mellon University
Abstract. Model compression is a critical technique to eﬃciently deploy
neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression
techniques rely on hand-crafted heuristics and rule-based policies that
require domain experts to explore the large design space trading oﬀ
among model size, speed, and accuracy, which is usually sub-optimal and
time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverage reinforcement learning to provide the model
compression policy. This learning-based compression policy outperforms
conventional rule-based compression policy by having higher compression
ratio, better preserving the accuracy and freeing human labor. Under
4× FLOPs reduction, we achieved 2.7% better accuracy than the handcrafted model compression policy for VGG-16 on ImageNet. We applied
this automated, push-the-button compression pipeline to MobileNet and
achieved 1.81× speedup of measured inference latency on an Android
phone and 1.43× speedup on the Titan XP GPU, with only 0.1% loss of
ImageNet Top-1 accuracy.
Keywords: AutoML · Reinforcement learning · Model compression ·
CNN acceleration · Mobile vision.
Introduction
In many machine learning applications such as robotics, self-driving cars, and
advertisement ranking, deep neural networks are constrained by latency, energy
and model size budget. Many approaches have been proposed to improve the
hardware eﬃciency of neural networks by model compression . The
core of model compression technique is to determine the compression policy
for each layer as they have diﬀerent redundancy, which conventionally requires
hand-crafted heuristics and domain expertise to explore the large design space
trading oﬀamong model size, speed, and accuracy. The design space is so large
that human heuristic is usually sub-optimal, and manual model compression
is time-consuming. To this end, we aim to automatically ﬁnd the compression
∗Indicates equal contributions. Published as a conference paper in ECCV’18.
 
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li and Song Han
Reward= -Error*log(FLOP)
Agent: DDPG
Action: Compress with
Sparsity ratio at (e.g. 50%)
Embedding st=[N,C,H,W,i…]
Environment: Channel Pruning
Original NN
Model Compression by Human:
Labor Consuming, Sub-optimal
Model Compression by AI:
Automated, Higher Compression Rate, Faster
Compressed NN
AMC Engine
Original NN
Compressed NN
Fig. 1. Overview of AutoML for Model Compression (AMC) engine. Left: AMC replaces
human and makes model compression fully automated while performing better than
human. Right: Form AMC as a reinforcement learning problem. We process a pre-trained
network (e.g., MobileNet) in a layer-by-layer manner. Our reinforcement learning agent
(DDPG) receives the embedding st from a layer t, and outputs a sparsity ratio at. After
the layer is compressed with at, it moves to the next layer Lt+1. The accuracy of the
pruned model with all layers compressed is evaluated. Finally, as a function of accuracy
and FLOP, reward R is returned to the reinforcement learning agent.
policy for an arbitrary network to achieve even better performance than human
designed, rule-based model compression methods.
Previously there were many rule-based model compression heuristics .
For example, prune less parameters in the ﬁrst layer which extracts low level
features and have the least amount of parameters; prune more parameters in
the FC layer since FC layers have the most parameters; prune less parameters
in the layers that are sensitive to pruning, etc. However, as the layers in deep
neural networks are not independent, these rule-based pruning policies are nonoptimal, and doesn’t transfer from one model to another model. Neural network
architectures are evolving fast, and we need an automated way to compress them
to improve engineer eﬃciency. As the neural network becomes deeper, the design
space has exponential complexity , which is infeasible to be solved by greedy,
rule-based methods. Therefore, we propose AutoML for Model Compression
(AMC) which leverages reinforcement learning to automatically sample the
design space and improve the model compression quality. Figure 1 illustrates
our AMC engine. When compressing a network, our AMC engine automates this
process by learning-based policies, rather than relying on rule-based policy and
experienced engineers.
We observe that the accuracy of the compressed model is very sensitive to the
sparsity of each layer, requiring a ﬁne-grained action space. Therefore, instead
of searching over a discrete space, we come up with a continuous compression
ratio control strategy with a DDPG agent to learn through trial and error:
penalizing accuracy loss while encouraging model shrinking and speedup. The
actor-critic structure also helps to reduce variance, facilitating stabler training.
AMC: AutoML for Model Compression and Acceleration on Mobile Devices
Speciﬁcally, our DDPG agent processes the network in a layer-wise manner. For
each layer Lt, the agent receives a layer embedding st which encodes useful
characteristics of this layer, and then it outputs a precise compression ratio at.
After layer Lt is compressed with at, the agent moves to the next layer Lt+1. The
validation accuracy of the pruned model with all layers compressed is evaluated
without ﬁne-tuning, which is an eﬃcient delegate of the ﬁne-tuned accuracy. This
simple approximation can improve the search time not having to retrain the
model, and provide high quality search result. After the policy search is done,
the best-explored model is ﬁne-tuned to achieve the best performance.
We proposed two compression policy search protocols for diﬀerent scenarios.
For latency-critical AI applications (e.g., mobile apps, self-driving cars, and
advertisement rankings), we propose resource-constrained compression to achieve
the best accuracy given the maximum amount of hardware resources (e.g.,
FLOPs, latency, and model size), For quality-critical AI applications (e.g., Google
Photos) where latency is not a hard constraint, we propose accuracy-guaranteed
compression to achieve the smallest model with no loss of accuracy.
We achieve resource-constrained compression by constraining the search space,
in which the action space (pruning ratio) is constrained such that the model
compressed by the agent is always below the resources budget. For accuracyguaranteed compression, we deﬁne a reward that is a function of both accuracy
and hardware resource. With this reward function, we are able to explore the
limit of compression without harming the accuracy of models.
To demonstrate the wide and general applicability, we evaluate our AMC
engine on multiple neural networks, including VGG , ResNet , and MobileNet , and we also test the generalization ability of the compressed model
from classiﬁcation to object detection. Extensive experiments suggest that AMC
oﬀers better performance than hand-crafted heuristic policies. For ResNet-50, we
push the expert-tuned compression ratio from 3.4× to 5× with no loss of
accuracy. Furthermore, we reduce the FLOPs of MobileNet by 2×, achieving
top one accuracy of 70.2%, which is on a better Pareto curve than 0.75 MobileNet,
and we achieve a speedup of 1.53× on the Titan XP and 1.95× on an Android
Related Work
CNN Compression and Acceleration. Extensive works 
have been done on accelerating neural networks by compression. Quantization and special convolution implementations can also
speed up the neural networks. Tensor factorization decomposes
weights into light-weight pieces, for example proposed to accelerate
the fully connected layers with truncated SVD; Jaderberg et al. proposed to
factorize layers into 1×3 and 3×1; and Zhang et al. proposed to factorize
layers into 3×3 and 1×1. Channel pruning removes the redundant
channels from feature maps. A common problem of these methods is how to
determine the sparsity ratio for each layer.
Neural Architecture Search and AutoML. Many works on searching models
with reinforcement learning and genetic algorithms greatly improve
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li and Song Han
Table 1. Comparisons of reinforcement learning approaches for models searching (NAS:
Neural Architecture Search , NT: Network Transformation , N2N: Network to
Network , and AMC: AutoML for Model Compression. AMC distinguishes from other
works by getting reward without ﬁne-tuning, continuous search space control, and can
produce both accuracy-guaranteed and hardware resource-constrained models.
optimize for accuracy
optimize for latency
simple, non-RNN controller
fast exploration with few GPUs
continuous action space
the performance of neural networks. NAS aims to search the transferable
network blocks, and its performance surpasses many manually designed architectures . Cai et al. proposed to speed up the exploration via network
transformation . Inspired by them, N2N integrated reinforcement learning
into channel selection. In Table 1, we demonstrate several merits of our AMC
engine. Compared with previous work, AMC engine optimizes for both accuracy
and latency, requires a simple non-RNN controller, can do fast exploration with
fewer GPU hours, and also support continuous action space.
Methodology
We present an overview of our AutoML for Model Compression(AMC) engine
in Figure 1. We aim to automatically ﬁnd the redundancy for each layer, characterized by sparsity. We train an reinforcement learning agent to predict the
action and give the sparsity, then perform form the pruning. We quickly evaluate
the accuracy after pruning but before ﬁne-tuning as an eﬀective delegate of ﬁnal
accuracy. Then we update the agent by encouraging smaller, faster and more
accurate models.
Problem Deﬁnition
Model compression is achieved by reducing the number of parameters and computation of each layer in deep neural networks. There are two categories of pruning:
ﬁne-grained pruning and structured pruning. Fine-grained pruning aims to
prune individual unimportant elements in weight tensors, which is able to achieve
very high compression rate with no loss of accuracy. However, such algorithms
result in an irregular pattern of sparsity, and it requires specialized hardware such
as EIE for speed up. Coarse-grained / structured pruning aims to prune
entire regular regions of weight tensors (e.g., channel, row, column, block, etc.).
The pruned weights are regular and can be accelerated directly with oﬀ-the-shelf
hardware and libraries. Here we study structured pruning that shrink the input
channel of each convolutional and fully connected layer.
Our goal is to precisely ﬁnd out the eﬀective sparsity for each layer, which
used to be manually determined in previous studies . Take convolutional
AMC: AutoML for Model Compression and Acceleration on Mobile Devices
layer as an example. The shape of a weight tensor is n × c × k × k, where n, c
are output and input channels, and k is the kernel size. For ﬁne-grained pruning,
the sparsity is deﬁned as the number of zero elements divided by the number of
total elements, i.e. #zeros/(n × c × k × h). For channel pruning, we shrink the
weight tensor to n × c′ × k × k (where c′ < c), hence the sparsity becomes c′/c.
Automated Compression with Reinforcement Learning
AMC leverages reinforcement learning for eﬃcient search over action space. Here
we introduce the detailed setting of reinforcement learning framework.
The State Space For each layer t, we have 11 features that characterize the
(t, n, c, h, w, stride, k, FLOPs[t], reduced, rest, at−1)
where t is the layer index, the dimension of the kernel is n×c×k×k, and the input
is c × h × w. FLOPs[t] is the FLOPs of layer Lt. Reduced is the total number
of reduced FLOPs in previous layers. Rest is the number of remaining FLOPs
in the following layers. Before being passed to the agent, they are scaled within
 . Such features are essential for the agent to distinguish one convolutional
layer from another.
The Action Space Most of the existing works use discrete space as coarsegrained action space (e.g., {64, 128, 256, 512} for the number of channels). Coarsegrained action space might not be a problem for a high-accuracy model architecture search. However, we observed that model compression is very sensitive to
sparsity ratio and requires ﬁne-grained action space, leading to an explosion of
the number of discrete actions (Sec. 4.2). Such large action spaces are diﬃcult to
explore eﬃciently . Discretization also throws away the order: for example,
10% sparsity is more aggressive than 20% and far more aggressive than 30%.
As a result, we propose to use continuous action space a ∈(0, 1], which
enables more ﬁne-grained and accurate compression.
DDPG Agent As illustrated in Figure 1, the agent receives an embedding state
st of layer Lt from the environment and then outputs a sparsity ratio as action
at. The underlying layer is compressed with at (rounded to the nearest feasible
fraction) using a speciﬁed compression algorithm (e.g., channel pruning). Then
the agent moves to the next layer Lt+1, and receives state st+1. After ﬁnishing
the ﬁnal layer LT , the reward accuracy is evaluated on the validation set and
returned to the agent. For fast exploration, we evaluate the reward accuracy
without ﬁne-tuning, which is a good approximation for ﬁne-tuned accuracy
(Sec. 4.1).
We use the deep deterministic policy gradient (DDPG) for continuous control
of the compression ratio, which is an oﬀ-policy actor-critic algorithm. For the
exploration noise process, we use truncated normal distribution:
µ′(st) ∼TN
 µ (st | θµ
t ) , σ2, 0, 1
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li and Song Han
During exploitation, noise σ is initialized as 0.5 and is decayed after each
episode exponentially.
Following Block-QNN , which applies a variant form of Bellman’s Equation , each transition in an episode is (st, at, R, st+1), where R is the reward
after the network is compressed. During the update, the baseline reward b is
subtracted to reduce the variance of gradient estimation, which is an exponential
moving average of the previous rewards :
 si, ai | θQ2
yi = ri −b + γQ(si+1, µ(si+1) | θQ)
The discount factor γ is set to 1 to avoid over-prioritizing short-term rewards .
Search Protocols
Resource-Constrained Compression By limiting the action space (the sparsity ratio for each layer), we can accurately arrive at the target compression ratio.
Following , we use the following reward:
Rerr = −Error
This reward oﬀers no incentive for model size reduction, so we the achieve target
compression ratio by an alternative way: limiting the action space. Take ﬁnegrained pruning for model size reduction as an example: we allow arbitrary action
a at the ﬁrst few layers; we start to limit the action a when we ﬁnd that the
budget is insuﬃcient even after compressing all the following layers with most
aggressive strategy. Algorithm 1 illustrates the process. (For channel pruning, the
code will be longer but similar, since removing input channels of layer Lt will also
remove the corresponding output channels of Lt−1, reducing parameters/FLOPs
of both layers). Note again that our algorithm is not limited to constraining
model size and it can be replaced by other resources, such as FLOPs or the actual
inference time on mobile device. Based on our experiments (Sec. 4.1), as the
agent receives no incentive for going below the budget, it can precisely arrive at
the target compression ratio.
Accuracy-Guaranteed Compression By tweaking the reward function, we
can accurately ﬁnd out the limit of compression that oﬀers no loss of accuracy.
We empirically observe that Error is inversely-proportional to log(FLOPs) or
log(#Param) . Driven by this, we devise the following reward function:
RFLOPs = −Error · log(FLOPs)
RParam = −Error · log(#Param)
This reward function is sensitive to Error; in the meantime, it provides a
small incentive for reducing FLOPs or model size. Based on our experiments in
Figure 4.1, we note that our agent automatically ﬁnds the limit of compression.
AMC: AutoML for Model Compression and Acceleration on Mobile Devices
Algorithm 1 Predict the sparsity ratio actiont for layer Lt with constrained
model size (number of parameters) using ﬁne-grained pruning
▷Initialize the reduced model size so far
if t is equal to 0 then
Wreduced ←0
▷Compute the agent’s action and bound it with the maximum sparsity ratio
actiont ←µ′(st)
actiont ←min(actiont, actionmax)
▷Compute the model size of the whole model and all the later layers
▷Compute the number of parameters we have to reduce in the current layer if
all the later layers are pruned with the maximum sparsity ratio. α is the target
sparsity ratio of the whole model.
Wduty ←α · Wall −actionmax · Wrest −Wreduced
▷Bound actiont if it is too small to meet the target model size reduction
actiont ←max(actiont, Wduty/Wt)
▷Update the accumulation of reduced model size
Wreduced ←Wreduced + actiont · Wt
return actiont
Experimental Results
For ﬁne-grained pruning , we prune the weights with least magnitude. The
maximum sparsity ratio amax for convolutional layers is set to 0.8, and amax for
fully connected layer is set to 0.98. For channel pruning, we use max response
selection (pruning the weights according to the magnitude ), and preserve
Batch Normalization layers during pruning instead of merging them into
convolutional layers. The maximum sparsity ratios amax for all layers are set to
0.8. Note that the manual upper bound amax is only intended for faster search,
one can simply use amax = 1 which also produces similar results. Our actor
network µ has two hidden layers, each with 300 units. The ﬁnal output layer is a
sigmoid layer to bound the actions within (0, 1). Our critic network Q also had
two hidden layers, both with 300 units. Actions arr included in the second hidden
layer. We use τ = 0.01 for the soft target updates and train the network with 64
as batch size and 2000 as replay buﬀer size. Our agent ﬁrst explores 100 episodes
with a constant noise σ = 0.5, and then exploits 300 episodes with exponentially
decayed noise σ.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li and Song Han
Table 2. Pruning policy comparison of Plain-20, ResNets on CIFAR-10 .
RErr corresponds to FLOPs-constrained compression with channel pruning, while
RParam corresponds to accuracy guaranteed compression with ﬁne-grained pruning.
For both shallow network Plain-20 and deeper network ResNets, AMC outperforms
hand-crafted policies by a large margin. This enables eﬃcient exploration without
ﬁne-tuning. Although AMC makes many trials on model architecture, we have separate
validation and test dataset. No over-ﬁtting is observed.
Val Acc. Test Acc. Acc. after FT.
deep (handcraft)
shallow (handcraft)
uniform (handcraft)
AMC (RErr)
uniform (handcraft)
deep (handcraft)
AMC (RErr)
AMC (RParam)
60% Params
CIFAR-10 and Analysis
We conduct extensive experiments and fully analyze our AMC on CIFAR-10 to
verify the eﬀectiveness of the 2 search protocols. CIFAR dataset consists of
50k training and 10k testing 32 × 32 tiny images in ten classes. We split the
training images into 45k/5k train/validation. The accuracy reward is obtained on
validation images. Our approach is computationally eﬃcient: the RL can ﬁnish
searching within 1 hour on a single GeForce GTX TITAN Xp GPU.
FLOPs-Constrained Compression. We conducted FLOPs-constrained experiments on CIFAR-10 with channel pruning. We compare our approach with
three empirical policies illustrated in Figure 2: uniform sets compression
ratio uniformly, shallow and deep aggressively prune shallow and deep layers
respectively. Based on sparsity distribution of diﬀerent networks, a diﬀerent
strategy might be chosen.
In Table 2, we show us using reward Rerr to accurately ﬁnd the sparsity ratios
for pruning 50% for Plain-20 and ResNet-56 and compare it with empirical
policies. We outperform empirical policies by a large margin. The best pruning
setting found by AMC diﬀers from hand-crafted heuristic (Figure 2). It learns a
bottleneck architecture .
Accuracy-Guaranteed Compression. By using the RParam reward, our agent
can automatically ﬁnd the limit of compression, with smallest model size and
little loss of performance. As shown in Table 2, we compress ResNet-50 with ﬁnegrained pruning on CIFAR-10. The result we obtain has up to 60% compression
ratio with even a little higher accuracy on both validation set and test set, which
might be in light of the regularization eﬀect of pruning.
AMC: AutoML for Model Compression and Acceleration on Mobile Devices
compression ratio
uniform(handcraft,89.7%)
shallow(handcraft,89.2%)
deep(handcraft,88.3%)
ours(90.2%)
Fig. 2. Comparisons of pruning strategies for Plain-20 under 2×. Uniform policy
sets the same compression ratio for each layer uniformly. Shallow and deep policies
aggressively prune shallow and deep layers respectively. Policy given by AMC looks
like sawtooth, which resembles the bottleneck architecture . The accuracy given by
AMC outperforms hand-crafted policies. (better viewed in color)
Since our reward RParam focuses on Error and oﬀers very little incentive
to compression in the meantime, it prefers the high-performance model with
harmless compression. To shorten the search time, we obtain the reward using
validation accuracy without ﬁne-tuning. We believe if reward were ﬁne-tuned
accuracy, the agent should compress more aggressively, because the ﬁne-tuned
accuracy is much closer to the original accuracy.
Speedup policy exploration. Fine-tuning a pruned model usually takes a
very long time. We observe a correlation between the pre-ﬁne-tune accuracy and
the post ﬁne-tuning accuracy . As shown in Table 2, policies that obtain
higher validation accuracy correspondingly have higher ﬁne-tuned accuracy. This
enables us to predict ﬁnal model accuracy without ﬁne-tuning, which results in
an eﬃcient and faster policy exploration.
The validation set and test set are separated, and we only use the validation
set to generate reward during reinforcement learning. In addition, the compressed
models have fewer parameters. As shown in Table 2, the test accuracy and
validation accuracy are very close, indicating no over-ﬁtting.
On ImageNet, we use 3000 images from the training set to evaluate the reward
function in order to prevent over-ﬁtting. The latency is measured with 224 × 224
input throughout the experiments.
Push the Limit of Fine-grained Pruning. Fine-grained pruning method
prunes neural networks based on individual connections to achieve sparsity in both
weights and activations, which is able to achieve higher compression ratio and can
be accelerated with specialized hardware such as . However, it requires
iterative prune & ﬁne-tune procedure to achieve decent performance , and
single-shot pruning without retraining will greatly hurt the prediction accuracy
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li and Song Han
Residual Block 1
Residual Block 2
Residual Block 3
Residual Block 4
Crests: our RL agent automatically learns 3x3 convolutions have
more redundancy and can be pruned more.
Peaks: our RL agent automatically learns 1x1 convolutions
have less redundancy and can be pruned less.
Fig. 3. The pruning policy (sparsity ratio) given by our reinforcement learning agent
for ResNet-50. With 4 stages of iterative pruning, we ﬁnd very salient sparsity pattern
across layers: peaks are 1 × 1 convolution, crests are 3 × 3 convolution. The reinforcement learning agent automatically learns that 3 × 3 convolution has more
redundancy than 1 × 1 convolution and can be pruned more.
Fig. 4. Our reinforcement learning agent (AMC) can prune the model to a lower
density compared with human experts without losing accuracy. (Human expert: 3.4×
compression on ResNet50. AMC : 5× compression on ResNet50.)
with large compression rate (say 4×), which cannot provide useful supervision
for reinforcement learning agent.
To tackle the problem, we follow the settings in to conduct 4-iteration
pruning & ﬁne-tuning experiments, where the overall density of the full model is
set to [50%, 35%, 25% and 20%] in each iteration. For each stage, we run AMC to
determine the sparsity ratio of each layer given the overall sparsity. The model is
then pruned and ﬁne-tuned for 30 epochs following common protocol. With that
framework, we are able to push the expert-tuned compression ratio of ResNet-50
on ImageNet from 3.4× to 5× (see Figure 4) without loss of performance on
ImageNet (original ResNet50’s [top-1, top-5] accuracy=[76.13%, 92.86%]; AMC
pruned model’s accuracy=[76.11%, 92.89%]). The density of each layer during
each stage is displayed in Figure 3. The peaks and crests show that the RL agent
automatically learns to prune 3 × 3 convolutional layers with larger sparsity,
AMC: AutoML for Model Compression and Acceleration on Mobile Devices
Table 3. Learning-based model compression (AMC) outperforms rule-based model
compression. Rule-based heuristics are suboptimal. (for reference, the baseline top-1
accuracy of VGG-16 is 70.5%, MobileNet is 70.6%, MobileNet-V2 is 71.8%).
FLOPs ∆Acc %
FP (handcraft) 
RNP (handcraft) 
SPP (handcraft) 
CP (handcraft) 
AMC (ours)
uniform (0.75-224) 
AMC (ours)
uniform (0.75-192) 
AMC (ours)
MobileNet-V2 uniform (0.75-224) 
AMC (ours)
since they generally have larger redundancy; while prunes more compact 1 × 1
convolutions with lower sparsity. The density statistics of each block is provided
in Figure 4. We can ﬁnd that the density distribution of AMC is quite diﬀerent
from human expert’s result shown in Table 3.8 of , suggesting that AMC can
fully explore the design space and allocate sparsity in a better way.
Comparison with Heuristic Channel Reduction. Here we compare AMC
with existing state-of-the-art channel reduction methods: FP , RNP and
SPP . All the methods proposed a heuristic strategy to design the pruning
ratio of each layer. FP proposed a sensitive analysis scheme to estimate the
sensitivity of each layer by evaluating the accuracy with single layer pruned.
Layers with lower sensitivity are pruned more aggressively. Such method assumes
that errors of diﬀerent pruned layers can be summed up linearly, which does not
stand according to our experiments. RNP groups all convolutional channels
into 4 sets and trains a RL agent to decide on the 4 sets according to input image.
However, the action space is very rough (only 4 actions for each layer), and it
cannot reduce the model size. SPP applies PCA analysis to each layer and
takes the reconstruction error as the sensitivity measure to determine the pruning
ratios. Such analysis is conducted based on one single layer, failing to take the
correlations between layers into consideration. We also compare our method to
the original channel pruning paper (CP in Table 3), in which the sparsity ratios
of pruned VGG-16 are carefully tuned by human experts (conv5 are skipped,
sparsity ratio for conv4 and remaining layers is 1 : 1.5). The results of pruned
VGG-16 are presented in Table 3. Consistent with our CIFAR-10 experiments
(Sec. 4.1), AMC outperforms all heuristic methods by more than 0.9%, and
beats human expert by 0.3% without any human labor.
Apart from VGG-16, we also test AMC on modern eﬃcient neural networks
MobileNet and MobileNet-V2 . Since the networks have already been
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li and Song Han
(a) Accuracy v.s. MACs
(b) Accuracy v.s. Inference time
Fig. 5. (a) Comparing the accuracy and MAC trade-oﬀamong AMC, human expert,
and unpruned MobileNet. AMC strictly dominates human expert in the pareto optimal
curve. (b) Comparing the accuracy and latency trade-oﬀamong AMC, NetAdapt,
and unpruned MobileNet. AMC signiﬁcantly improves the pareto curve of MobileNet.
Reinforcement-learning based AMC surpasses heuristic-based NetAdapt on the pareto
curve (inference time both measured on Google Pixel 1).
very compact, it is much harder to further compress them. The easiest way to
reduce the channels of a model is to use uniform channel shrinkage, i.e. use
a width multiplier to uniformly reduce the channels of each layer with a ﬁxed
ratio. Both MobileNet and MobileNet-V2 present the performance of diﬀerent
multiplier and input sizes, and we compare our pruned result with models of same
computations. The format are denoted as uniform (depth multiplier - input size).
We can ﬁnd that our method consistently outperforms the uniform baselines.
Even for the current state-of-the-art eﬃcient model design MobileNet-V2, AMC
can still improve its accuracy by 1.0% at the same computation (Table 3). The
pareto curve of MobileNet is presented in Figure 5a.
Speedup Mobile Inference. Mobile inference acceleration has drawn people’s
attention in recent years. Not only can AMC optimize FLOPs and model size, it
can also optimize the inference latency, directly beneﬁting mobile developers. For
all mobile inference experiments, we use TensorFlow Lite framework for timing
evaluation.
We prune MobileNet , a highly compact network consisting of depth-wise
convolution and point-wise convolution layers, and measure how much we can
improve its inference speed. Previous attempts using hand-crafted policy to prune
MobileNet led to signiﬁcant accuracy degradation : pruning MobileNet to
75.5% original parameters results in 67.2% top-1 accuracy⋆, which is even worse
than the original 0.75 MobileNet (61.9% parameters with 68.4% top-1 accuracy).
However, our AMC pruning policy signiﬁcantly improves pruning quality: on
ImageNet, AMC-pruned MobileNet achieved 70.5% Top-1 accuracy with 285
⋆ 
AMC: AutoML for Model Compression and Acceleration on Mobile Devices
Table 4. AMC speeds up MobileNet. Previous attempts using rule-based policy to
prune MobileNet lead to signiﬁcant accuracy degradation while AMC use learningbased pruning which well preserve the accuracy. On Google Pixel-1 CPU, AMC achieves
1.95× measured speedup with batch size one, while saving the memory by 34%. On
NVIDIA Titan XP GPU, AMC achieves 1.53× speedup with batch size of 50. The
input image size is 224×224 for all experiments and no quantization is applied for
apple-to-apple comparison.
70.6% 89.5% 0.46ms
68.4% 88.2% 0.34ms
NetAdapt 
(50% FLOPs)
70.5% 89.3% 0.32ms 3127 fps
(1.81×) 14.3MB
(50% Latency)
70.2% 89.2% 0.30ms 3350 fps
(1.95×) 13.2MB
MFLOPs, compared to the original 0.75 MobileNet’s 68.4% Top-1 accuracy with
325 MFLOPs. As illustrated in Figure 5a, human expert’s hand-crafted policy
achieves slightly worse performance than that of the original MobileNet under 2×
FLOPs reduction. However, with AMC, we signiﬁcantly raise the pareto curve,
improving the accuracy-MACs trade-oﬀof original MobileNet.
By substituting FLOPs with latency, we can change from FLOPs-constrained
search to latency-constrained search and directly optimize the inference time.
Our experiment platform is Google Pixel 1 with Qualcomm Snapdragon 821
SoC. As shown in Figure 5b, we greatly reduce the inference time of MobileNet
under the same accuracy. We also compare our learning based policy with a
heuristic-based policy , and AMC better trades oﬀaccuracy and latency.
Furthermore, since AMC uses the validation accuracy before ﬁne-tuning as the
reward signal while needs local ﬁne-tuning after each step, AMC is more
sampling-eﬃcient, requiring fewer GPU hours for policy search.
We show the detailed statistics of our pruned model in Table 4. Model searched
with 0.5× FLOPs and 0.5× inference time are proﬁled and displayed. For 0.5×
FLOPs setting, we achieve 1.81× speed up on a Google Pixel 1 phone, and
for 0.5× FLOPs setting, we accurately achieve 1.95× speed up, which is very
close to actual 2× target, showing that AMC can directly optimize inference
time and achieve accurate speed up ratio. We achieve 2.01× speed up for 1×1
convolution but less signiﬁcant speed up for depth-wise convolution due to
the small computation to communication ratio. AMC compressed models also
consumes less memory. On GPUs, we also achieve up to 1.5× speedup, less than
mobile phone, which is because a GPU has higher degree of parallelism than a
mobile phone.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li and Song Han
Table 5. Compressing Faster R-CNN with VGG16 on PASCAL VOC 2007. Consistent with classiﬁcation task, AMC also results in better performance under the same
compression ratio on object detection task.
mAP [0.5, 0.95] (%)
2× handcrafted 
68.3 (-0.4)
36.7 (-0.0)
4× handcrafted 
66.9 (-1.8)
35.1 (-1.6)
4× handcrafted 
67.8 (-0.9)
36.5 (-0.2)
4× AMC (ours)
68.8 (+0.1)
37.2 (+0.5)
Generalization Ability. We evaluate the generalization ability of AMC on
PASCAL VOC object detection task . We use the compressed VGG-16 (from
Sec 4.2) as the backbone for Faster R-CNN . In Table 5, AMC achieves 0.7%
better mAP[0.5, 0.95]) than the best hand-crafted pruning methods under the
same compression ratio. AMC even surpasses the baseline by 0.5% mAP. We
hypothesize this improvement as that the optimal compression policy found by
the RL agent also serves as eﬀective regularization.
Conclusion
Conventional model compression techniques use hand-crafted features and require
domain experts to explore a large design space and trade oﬀbetween model size,
speed, and accuracy, which is usually suboptimal and labor-consuming. In this
paper, we propose AutoML for Model Compression (AMC), which leverages reinforcement learning to automatically search the design space, greatly improving the
model compression quality. We also design two novel reward schemes to perform
both resource-constrained compression and accuracy-guaranteed compression.
Compelling results have been demonstrated for MobileNet, MobileNet-V2, ResNet
and VGG on Cifar and ImageNet. The compressed model generalizes well from
classiﬁcation to detection tasks. On the Google Pixel 1 mobile phone, we push the
inference speed of MobileNet from 8.1 fps to 16.0 fps. AMC facilitates eﬃcient
deep neural networks design on mobile devices.
Acknowledgements
We thank Quoc Le, Yu Wang and Bill Dally for the supportive feedback. We
thank Jiacong Chen for drawing the cartoon on the left of Figure 1.
AMC: AutoML for Model Compression and Acceleration on Mobile Devices