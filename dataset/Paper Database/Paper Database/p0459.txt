Bottom-up/Top-Down Image Parsing by Attribute Graph Grammar
Feng Han and Song-Chun Zhu
Departments of Computer Science and Statistics
University of California, Los Angeles
Los Angeles, CA 90095
 , 
In this paper, we present an attribute graph grammar for
image parsing on scenes with man-made objects, such as
buildings, hallways, kitchens, and living rooms. We choose
one class of primitives – 3D planar rectangles projected on
images, and six graph grammar production rules. Each production rule not only expands a node into its components,
but also includes a number of equations that constrain the
attributes of a parent node and those of its children. Thus
our graph grammar is context sensitive. The grammar rules
are used recursively to produce a large number of objects
and patterns in images and thus the whole graph grammar
is a type of generative model. The inference algorithm integrates bottom-up rectangle detection which activates topdown prediction using the grammar rules. The ﬁnal results
are validated in a Bayesian framework. The output of the
inference is a hierarchical parsing graph with objects, surfaces, rectangles, and their spatial relations. In the inference, the acceptance of a grammar rule means a recognition of an object, and actions are taken to pass the attributes
between a node and its parent through the constraint equations associated with this production rule.
When an attribute is passed from a child node to a parent node, it is
called bottom-up, and the opposite is called top-down.
1. Introduction
In real world images, especially man-made scenes, such
as buildings, ofﬁces, and living spaces, a large number of
complex patterns (objects) are composed of a small set of
primitives using few operational relations. This is very similar to language where a huge set of sentences can be generated with a relatively small vocabulary and a few grammar
rules in a hierarchy from words, phrases, clauses, and to
sentences. The objective of image parsing is to decompose
an image into its constituent components (objects, parts,
surfaces, primitives) in a hierarchical structure represented
surfaces-rectangles
Figure 1. An example of generative scene
modeling with graph grammar.
scene consists of a number of objects and
each object is generated by a number of rectangles (transformed) using one of the four
graph grammar rules.
The grammar rules
specify the spatial relations between the constituent rectangles.
by a parsing graph.
Fig. 1 shows a parsing graph for a
kitchen scene using the rectangle primitives. We only show
a subset of the rectangles for clarity. Its vertical edges show
the decomposition of scene and objects, and its horizontal
links specify the spatial relations between objects and components. Thus a parsing graph is more general than a parsing tree in language.
In the literature, the study of syntactic pattern recognition was pioneered by Fu et al in the 1970-80s. Unfortunately, its applicability was very much limited by two
difﬁculties. (i) The primitive patterns (terminators in their
visual languages) could not be computed reliably from real
images. Thus their parsing algorithms achieved little success in real images. (ii) The string grammar (ﬁnite state automation) and stochastic context free grammars (SCFG) in
early work are not strong enough to express complex visual
In recent years, two new developments are encouraging and thus motivate our research. Firstly, more powerful
grammars, such as attribute graph grammars and context sensitive graph grammars have been developed in
visual diagrams parsing, such as music notes and diagrams
for relational databases. These grammars allow attributes
to be passed among the nodes in the parsing process in
three directions: top-down, bottom-up, and horizontal context. Such mechanisms are much desired in visual inference.
Secondly, more powerful inference tools, such as
the data-driven Markov chain Monte Carlo (DDMCMC)
 , were developed for passing the hypotheses up and
down the parsing tree (in SCFG) probabilistically in a principled way. This makes it possible to apply the grammar
parsing algorithms to real images.
In this paper, we study an attribute graph grammar for
image parsing and focus on man-made scenes, such as
buildings, hallways, kitchens, living rooms, etc. We choose
one primitive (terminator) – 3D planar rectangles projected
on images as shown in Fig. 2. We choose six graph grammar production rules as displayed in Fig. 3. For example,
a “line rule” aligns a number of rectangles in one row, a
“nesting rule” has one rectangle containing the other, and a
“cube rule” has three rectangles forming a rectangular box.
Each production rule is associated with a number of equations that constrain the attributes of a parent node and those
of its children. Thus our graph grammar is both attributed
and context sensitive. These rules can be used recursively
to generate a large set of complex conﬁgurations – called
the “language” of the grammar.
The paper is mainly focused on the top-down and
bottom-up inference.
Our algorithm proceeds in two
Phase I is bottom-up computation. We ﬁrst compute a
number of edge segments from the input image, and estimate a number of vanish points (usually three) using a
method studied in . Thus the line segments are divided
into three line sets illustrated in Fig. 4. Then we generate a number of rectangle hypotheses in a method similar
to RANSAC , which are weighted by their goodness of
ﬁtting to the input image. Thus an excessive number of
rectangles are proposed as bottom-up particles, and some
of them may overlap or conﬂict with each other.
bottom-up particles are tested and pursued one by one in
a procedure similar to the matching pursuit algorithm 
used in wavelets. With a high stopping threshold, we obtain
a number of “strong” hypotheses (rectangles) as initialization.
Phase II integrates top-down/bottom-up inference. Each
rectangle in the current state of solution matches to a production rule (rule 5) with attributes passed to the nonterminal node.
These non-terminal nodes are in turn
matched to other production rules (rule 2,3,4,6) which then
propose a number of top-down particles. Some of these
top-down particles may have existed in the bottom-up particles with weak scores. Then each rule is evaluated in a
Bayesian framework. The acceptance of a production rule
means a recognition of an object, and actions are taken to
pass the attributes between a node and its parent through the
constraint equations associated with this production rule.
When an attribute is passed from a child node to a parent
node, it is called bottom-up, and the opposite is called topdown.
In most of the images, the parsing graph has 2-3 layers
with fewer than 20 nodes, so the computation can be done
by AI search algorithms, such as best ﬁrst search. The topdown and bottom-up computing is illustrated in Fig. 5 for
the kitchen scene. In our experiments, we observed that the
top-down and context information helps the algorithm detect weak rectangles which are missing in bottom-up detection. Some “illusory rectangles” could also be hallucinated,
especially due to the line and mesh grammar rules.
Although this paper is focused on rectangular scenes, we
argue that the method could be extended to other classes of
objects. To our best knowledge, this paper is perhaps the
ﬁrst work applying attributed context sensitive graph grammars to parse real world scenes.
The remaining of the paper is organized as follows. We
ﬁrst present the graph grammar representation in Section 2,
and then the top-down/bottom-up inference in Section 3.
We show some results in Section 4 and conclude the paper
with a discussion of future work in Section 5.
Graph grammar representation
We brieﬂy introduce the attribute graph grammar, then
present our primitives and grammar rules for the rectangle
scene, and formulate the problem in Bayesian inference.
Attribute graph grammar
We denote an attribute graph grammar by a 4-tuple
G = (VN, Vt, R, Σ).
VN is a set of non-terminal nodes, often denoted by capital
letters A, A1, A2, etc. Vt is a set of terminal nodes denoted
by lower case letters a, b, c, a1, a2, etc. Both non-terminal
and terminal nodes have a vector of attributes denoted by
X(A) and x(a) respectively. Within the attribute space, the
terminal and non-terminal set could be very large. R is a
set of production rules
R = {r1, r2, ..., rm}.
Each production rule speciﬁes how a non-terminal node in
VN is expanded in VN ∪VT , and is associated with a number
of constraint equations. For example, the following is a rule
that expands one node A into two nodes A1, A2 ∈VN.
r : A →(A1, A2).
The associated equations are constraints on the attributes.
gi(X(A)) = fi(X(A1), X(A2)), i = 1, 2, ..., n(r).
gi() and fi() are usually projection functions that take
some elements from the attribute vectors. For example, let
X(A) = (X1, X2, X3) and X(A1) = (X11, X12), then an
equation could be simply an equivalence constraint,
Other rules may instantiate a non-terminal node to a terminal node
with constraints
gi(X(A)) = fi(x(a)), i = 1, 2, ..., n(r).
In a parsing process, sometimes we know the attributes of
a child node X11 and then pass it to X1 as in rule r. This
is called “bottom-up message passing”. Then X1 may be
passed to another child node’s attribute X2 with X21 = X1.
This is called “top-down message passing”.
Finally Σ is a set of instances (or conﬁgurations) that
can be produced by the production rules starting from a root
node S ∈VN,
Σ(G) = {(C, X(C)) : S
rc(1),...,rc(k)
Each conﬁguration C has attributes X(C) and corresponds
to a speciﬁc scene structure. The grammar rules are applied
in a certain order and form the parsing graph G,
G(C) = (rc(1), ..., rc(k)).
Fig.1 illustrates a parsing graph for a kitchen scene. Most
grammars are ambiguous, and thus a conﬁguration C may
have more than one parsing graphs. In such case, we should
choose the shortest or most probable one. We will discuss
this in the Bayesian formulation below. In the following, we
explain the primitive and production rule of our grammar
for rectangle scenes.
Figure 2. A planar rectangle (ABCD) is described by 8 variables. The two vanish points
p1(x, y) and p2(x, y) and four directions θ1, θ2,
θ3 and θ4 for the four boundaries.
A primitive — rectangles
In this paper, we use only one class of primitives – 3D
planar rectangles projected on images illustrated in Fig. 2.
The two pairs of parallel line segments in 3D may form two
vanishing points p1, p2 after projected to the image plane.
VT = {(a, x(a)) : x(a) ∈Ωa}.
There are many equivalent ways to deﬁne the attributes x(a)
for a rectangle. We choose the two vanishing points p1(x, y)
and p2(x, y), and the four directions, θ1, θ2, θ3, θ4, of the
four lines corresponding to the four boundaries of the rectangle. Thus a rectangle is described by 8-variables,
x(a) = (p1(x, y), p2(x, y), θ1, θ2, θ3, θ4).
We choose the attributes in a way that they can be shared
with the non-terminal nodes. VT is a rather large set because of the large space of rectangles denoted by Ωa. One
may denote some special rectangles with smaller number of
variables.
Six production rules
In our graph grammar for rectangle scenes, there are only
two types of non-terminal nodes – the root node S for the
scene and a node A for objects.
VN = {(S, X(S)), (A, X(A)) : X(S) = n, X(A) ∈ΩA}.
The scene node generates n independent objects. It may
also include a label ℓfor the scene category, but we do not
deal with scene classiﬁcation in this paper. The object node
A can be instantiated (assigned) to a rectangle (rule r5), otherwise it will be used recursively by other four production
line production rule
nesting production rule
cube production rule
Figure 3. Six attribute grammar rules that can be used recursively. Attributes will be passed between a
node to its children and the horizontal lines shows constraints on attributes. See text for explanation.
rules: r2 – the line production rule, r3– the mesh production rule, r4– the nesting production rule, and r6 –the cube
production rule.
The six production rules are summarized in Fig. 3,
R = {r1, r2, r3, r4, r5, r6}.
Each of the production rules produces a structure and is associated with a number of constraints on attributes. The
attribute X(A) = (ℓ(A), Xo(A)) includes a label ℓfor the
type of object (structure) represented by A, and Xo(A) for
other attributes.
ℓ(A) ∈{line, mesh, nest, rect, cube}
Thus accepting a production rule is recognizing one of the
ﬁve patterns in inference. The other attributes Xo(A) are
different for different object types, so we have the space of
A as the union of the ﬁve different subspaces.
ΩA = Ωline
In the following, we explain some of the production rules.
The simplest rule is r5.
r5 : A →a; Xo(A) = x(a).
This rule terminates the recursion and assigns a rectangle
and its attributes to the non-terminal node A. Therefore
Xo(A) = x(a) is the constraint equation.
The second simplest one is the line production rule r2
we choose m = 3 for simplicity.
r2 : A →(A1, A2, A3);
gi(Xo(A)) = fi(Xo(A1), Xo(A2), Xo(A3)).
As illustrated in Fig. 3 (bottom-left corner), Xo(A) has 8
attributes describing the bounding box (blue) of A1, A2, A3.
Given Xo(A), the three rectangles have only 4 degrees of
freedom for the two intervals, all the other 3 × 8 −4 = 20
attributes are decided by the above attribute equations. As
one can see, the use of grammar rule reduces the description
For space limit, we omit the tedious speciﬁcation of the
constraint equations for all production rules.
Bayesian formulation and Probability models
Let I be an input image, our goal is to compute parsing
graph G as our recognition of the scene using grammar G.
The terminal nodes in G, i.e. all rectangles, compose a 2D
conﬁguration C = C(G) – a reduced representation of G.
In a Bayesian framework this is to optimize a posterior
probability,
G∗= arg max p(I|C(G))p(G).
Let ∆N(G) and ∆T (G) be the set of non-terminal nodes
and terminal nodes in the parsing graph G respectively. The
prior probability is the product of the probabilities for all the
rules it used.
p(X(A1), ..., X(Am)|X(A)).
One may easily calculate these probabilities by computing
the coding length L
log2 p(X(A1), ..., X(Am)|X(A)) = L(X(A1), ..., X(Am)|X(A)).
L is the number of bits coding the attributes of the children
A1, ...Am given the attributes of their parent A. In our experiments, we compute the degree-of-freedom and calculate
the bits after discretization of the attribute spaces.
C(G) is a set of rectangles produced by the parsing
graph. For example, Fig. 8 shows the computed rectangles
for four scenes. These rectangles are a sketch representation
of the image. Therefore, we adopt a likelihood model used
in the primal sketch . We avoid a region based model for
the following two reasons. (i) The rectangles are occluding each other. We have to infer an partial order between
the rectangles so that the region based model can be applied
properly. This is an extra computational burden. (ii) The
intensity inside rectangle could be non-trivial and includes
shading effects, see the kitchen scene. Thus it is rather complex for region based model. In contrast, the primal sketch
model only needs to record the image primitives along
the line segments (5 pixels perpendicular to the line segment), and the rest of the image is ﬁlled in with simple texture.
In the primal sketch model, the image lattice Λ is divided
into two parts: the structural part along the boundaries of
rectangles and the textural part for the remaining areas. Λ =
Λsk ∪Λnsk. We divide the line segments of the rectangles
in C(G) into short segments (say 7 pixels long with 5 pixel
width). We denote them by Λsk,k, k = 1, 2, ..., N, thus Λsk
consists of all such 5 × 7 image patches,
Each patch is ﬁtted to an image primitive Bk from a dictionary ∆B with Gaussian residue n.
I(x, y) = Bk(x, y) + n(x, y), (x, y) ∈Λsk,k.
Thus the sketchable part follows a likelihood model
p(Isk,k|C; ∆B) ∝exp{−
(x,y)∈Λsk,k
(I(x, y) −Bk(x, y))2
The textural part follows Markov random ﬁeld models
p(Insk|Isk; β) with parameters β conditioning on Isk. So,
p(I|C(G)) = p(Insk|Isk; β)
p(Isk,k|C; ∆B)
independent rectangles
line segments in three groups
Figure 4. A simpliﬁed two layer model for
bottom-up rectangle detection. Each rectangle consists of 2 pairs of nearly parallel line
segments, and the rectangles are treated independently.
We refer to for details of the primal sketch model.
Inference algorithm
Given our objective to compute the best parsing graph G
for an input image, the computation algorithm must achieve
three difﬁcult tasks: (1). constructing the parsing graph,
whose structure, like parse trees in natural languages, is not
pre-determined but depends on the input image; (2). estimating the attributes of graph nodes (label and model parameters); and (3). computing the spatial relations between
nodes on the same level.
There are several ways to infer the best parsing graph and
Markov Chain Monte Carlo (MCMC) methods have been
used in . In this paper, as our domain is limited to
rectangle scenes and the parsing graph is not too big, the AI
best ﬁrst search algorithm can be directly applied to maximize the posterior probability in a steepest ascent way.
Our algorithm consists of two phases. In the ﬁrst phase,
a number of independent rectangle proposals are generated
from the input image and an initialization for the parsing graph is achieved in a greedy way.
In the second
phase, these rectangle proposals are used with the grammars
rules to efﬁciently construct the parsing graph in a bottomup/top-down mechanism.
Generate bottom-up rectangle proposals and
initialization
We start with edge detection and edge tracing to get a
number of long contours, which are cut into a number of
straight line segments by polygon-approximation. Then we
run the vanishing point estimation algorithm in to group
all the line segments into three groups corresponding to the
principal directions. Given the three orthogonal groups of
top-down proposals
bottom-up proposals
Figure 5. Bottom-up detection of rectangles instantiate some terminal nodes (shaded
squares), which in turn activate graph grammar rules. Using context information, these
rules generate top-down proposals for new
rectangles which will be validated from the
data (edge map).
line segments, we generate the rectangle proposals in the
spirit of RANSAC . We exhaustively draw two pairs of
line segments from two out of the three line groups, and
compute their intersection points as shown in Figure 4. In
case the selected four lines indeed delineate a rectangle, we
treat this as a valid proposal. However, these proposals are
not equally good. So we weight them by their ﬁtness to
the given image. These weights can help us to sample the
better proposals into the computation process ﬁrst, and thus
achieve speeding up.
Now we have an excessive number of weighted bottomup particles for rectangle proposals. Some of them are con-
ﬂicting with each other (e.g. two rectangle proposals share
two or more edge segments) and only one of them should
appear. We mark this conﬂicting relation among all the proposals and when one proposal is accepted, this can be used
to shut down those conﬂicting with it.
Then these bottom-up particles are tested and pursued
one by one in a procedure like the matching pursuit algorithm used in wavelets. With a high stopping threshold,
we obtain a number of “strong” hypotheses (rectangles) as
initialization G0.
............
Figure 6. The proposed production rules are
organized in four candidate rows, each has a
posterior probability change it produces on
current parsing graph as its importance.
Bottom-up and top-down construction of
parsing graph with graph grammar
After the initialization, the following two steps are iteratively performed to construct the parsing graph by maximizing its posterior probability in a steepest ascend manner:
Step1: Each newly added rectangle in current state Gt
matches to active one production rule (rule 5) with attributes
passed to the non-terminal node. Then these non-terminal
nodes in turn match to activate other production rules (rule
2,3,4,6) and so on, which propose a number of top-down
particles. One example is shown in Figure 5. In this example, the four red rectangles instantiate four terminal nodes
which activate four production rule 5. The left two further
activate one production rule 2, which generates two new
blue rectangles in a line with the two exiting red ones. The
middle one further actives one production rule 6 to generate
two new blue rectangles to form a cube aspect, while the
left one activates one production rule 4 to generate one new
blue rectangle to form a nested group.
All of these activated rules are put into four rows as
shown in Figure 6. Each rule is weighted by the posterior probability change it achieves when being applied to
the current parsing graph, which can be easily computed
since each rule only affects the parsing graph locally. In addition, this posterior probability change depends on the state
of the parsing graph. Therefore, we should on-line update
the weights for all the rules in the four rows.
Step2: Select the rule with the biggest weight from all the
rules in the four rows and accept it if its weight is positive. If accepted, it means a recognition of an object, and
actions are taken to pass the attributes between a node and
its parent through the constraint equations associated with
this production rule. At the same time, this rule is removed
from the rule rows. If not accepted, the whole process stops.
Figure 7. Running example for the kitchen scene. We ﬁrst compute the edge map in (a), and generate
a large number of rectangle hypotheses in (b). Some of them are validated by the generative model
and we obtain the current state in (c). All the rectangles in (c) are used to activate the graph grammar
rules. (d-e-f) are the top-down hypotheses (shown in blue) which are then feed to the generative
model for validation iteratively. Not all top-down proposals are accepted.
4. Experiments
We run our algorithm on four typical scenes with rectangle structures. One running example in Figure 7 shows
details of the algorithm. The ﬁnal experimental results are
shown in Figure 8. Some rectangles are missing due to
the strong occlusion. In addition, it clearly shows that the
high level knowledge introduced by the grammar greatly
improves the results.
5. Discussion
In this paper, we study an attribute graph grammar for
image parsing. The paper makes two main contributions
to the vision literature.
Firstly, it uses attributed graph
grammar which is context sensitive for scene representation. With more powerful class of grammars, we should
rejuvenate the syntactic pattern recognition research originally pursued in the 70s-80s .
Such grammar representations are long desired for high level vision, especially
scene understanding and parsing. Secondly, it integrates
top-down/bottom-up procedure for computing the parsing
graph. For future work, we should study general frameworks for learning the graph grammars and for systematic
parsing algorithm with large set of graph grammar rules.
Acknowledgements
This work was supported in part by National Science
Foundation grant IIS-0413214.