Received February 20, 2019, accepted March 24, 2019, date of publication April 1, 2019, date of current version April 15, 2019.
Digital Object Identifier 10.1109/ACCESS.2019.2908489
A Comparative Study of Meta-Heuristic
Optimization Algorithms for 0 - 1 Knapsack
Problem: Some Initial Results
ABSALOM E. EZUGWU
1, VEROSHA PILLAY2, DIVYAN HIRASEN2,
KERSHEN SIVANARAIN2, AND MELVIN GOVENDER2
1School of Computer Science, University of KwaZulu-Natal, Pietermaritzburg 3201, South Africa
2School of Mathematics, Statistics, and Computer Science, University of KwaZulu-Natal, Durban 4000, South Africa
Corresponding author: Absalom E. Ezugwu ( )
ABSTRACT In this paper, we present some initial results of several meta-heuristic optimization algorithms,
namely, genetic algorithms, simulated annealing, branch and bound, dynamic programming, greedy search
algorithm, and a hybrid genetic algorithm-simulated annealing for solving the 0-1 knapsack problems. Each
algorithm is designed in such a way that it penalizes infeasible solutions and optimizes the feasible solution.
The experiments are carried out using both low-dimensional and high-dimensional knapsack problems. The
numerical results of the hybrid algorithm are compared with the results achieved by the individual algorithms.
The results revealed the superior performances of the branch and bound dynamic programming, and hybrid
genetic algorithm with simulated annealing methods over all the compared algorithms. This performance was
established by taking into account both the algorithm computational time and the solution quality. In addition,
the obtained results also indicated that the hybrid algorithm can be applied as an alternative to solve smalland large-sized 0-1 knapsack problems.
INDEX TERMS Knapsack problem, genetic algorithms, simulated annealing, branch and bound, dynamic
programming, greedy search algorithm, hybrid IGA-SA.
I. INTRODUCTION
Various combinatorial optimization problems are intrinsically NP-hard because their deterministic polynomial time
algorithms are very unlikely to exist . Heuristic approaches
for these NP-hard problems have been the focus of the
research community. The 0-1 Knapsack Problem (KP01) is
popular and widely studied example of an NP-hard combinatorial optimization problem , , where we ﬁnd the
optimal solution of the given problem such that it satisﬁes the
given constraint. The KP01s appear in real-world decisionmaking processes in a wide variety of ﬁelds . Some
examples of KP01s real-world applications includes capital
budgeting allocation problem , real estate property maintenance problem , cargo loading problem , resource
allocation problem , project selection problem , combinatorial auctions , available-to-promise problem ,
cutting stock problem , investment decision making .
In the KP01 problem, set of items are provided, each with a
The associate editor coordinating the review of this manuscript and
approving it for publication was Mahammad Abdul Hannan.
weight and a value. The idea is to determine the number of
each item to include in a collection, so that the total weight is
less than or equal to a maximum limit and the total value is
as large as possible – .
In this study, we use a variety of heuristic and metaheuristic
algorithms to solve the KP01 problem where one has to maximize the beneﬁt of objects in a knapsack without exceeding it
is capacity. The algorithms studied in solving the KP01 problems includes: Greedy Search Algorithm (GSA), Dynamic
Programming (DP), Branch and Bound (BB), Genetic Algorithm (GA) and Simulated Annealing (SA). Both SA and GA
are nature-inspired algorithms with wide applications in solving real-world optimization problems – . SA is based
on thermodynamics principle and GA is based on natural
evolution , . The major advantages of these natureinspired algorithms are their broad applicability, ﬂexibility,
ease of implementation, and the potential of ﬁnding nearoptimal solutions .
The ﬁrst appearance of KP01 was in 1957 in two publications. The ﬁrst was a research conducted by Dantzig ,
the founder of operations research and a developer of linear
VOLUME 7, 2019
2019 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See for more information.
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
programming. The second research is ﬂawlessly maximized
by selecting items by bang-for-buck , . Furthermore,
in , Dantzig proposed a greedy approximation algorithm to solve the unbounded knapsack problem , .
His version sorts the items in decreasing order of value per
unit of weight vi/wi. A greedy algorithm is an algorithmic
paradigm that follows the solving heuristic of making the
locally optimal choice at each stage. The dynamic programming concept can be considered as both mathematical optimization and computer programming methods , . The
method was developed by Richard Bellman in the 1950s and
has found applications in numerous ﬁeld . The Branch
and Bound method was ﬁrst proposed by Land and Doig 
whilst carrying out research for discrete programming, and
has become the most commonly used tool for solving
NP-hard optimization problems such as the KP01 and travelling salesman problem . GA is an algorithm that search
for good solutions to a problem from among a number of
possible solutions. The GA and its variants were pioneered
and developed in the 1960s by John Holland, his students, and
colleagues . SA is a stochastic based general search tool
that mimics the natural process of metals annealing . The
SA algorithm has the ability of escaping from local minima
and has been widely applied in different domain . The
performance of the SA is dependent on the cooling schedule.
The main focus and contribution of this paper is to evaluate
the capabilities of the selected meta-heuristics algorithms to
solve the KP01 problem. More speciﬁcally, to investigate the
effectiveness and efﬁciency of those well-known optimization algorithms namely, GA, SA, BB, DP, GSA and a Hybrid
GA and SA (IGA-SA) that have notable track records in
ﬁnding good quality solutions for the KP01 problems. Furthermore, to also carry out comparisons on the various levels
of difﬁculties for the Knapsack datasets and to determine
how well each of the aforementioned algorithms perform on
different dimension of the KP01.
The remainder of the paper is organized as follows.
In Section 2, the KP01 problem description and related work
are discussed. In Section 3, the proposed heuristics and metaheuristics algorithms are introduced in detail. In Section 4,
the performance comparisons of the proposed GA, SA, BB,
DP, GSA, and IGA-SA on different types of large-scale
KP01 instances are conducted. Finally, conclusions are drawn
in Section 5.
II. 0-1 KNAPSACK PROBLEM DESCRIPTION
The KP01 is an example of a combinatorial optimization
problem. It searches for the best solution among many other
solutions. The objective of this problem is to maximize the
total value of items in the knapsack while the constraint
ensures the sum of the weights is less than or equal to
the knapsack capacity. There is only one item of each type
and only two options for each item, that is included in the
knapsack or not. Each item cannot be put into the knapsack
more than once or be partially included in the knapsack. The
KP01 restricts the number xi copies of each kind of item to
TABLE 1. Sample KP01 problem.
zero or one. Given a set of n items numbered from 1 up to
n each with a weight wi and a value vi, along with a maximum
weight capacity W. The KP01 can be formulated as follows
(See Equation 1 and 2) :
if item is in the bag
subject to
vixi ≤W and xiϵ {0, 1}
Here, xi represents the number of instances of item i to be
included in the knapsack.
A. EXAMPLE OF 0-1 KNAPSACK PROBLEM
Suppose we have a knapsack that has a capacity of ﬁfteen
cubic inches and several items of different sizes and different
beneﬁts . We want to include in the knapsack only these
items that will have the greatest total beneﬁt within the constraint of the knapsack’s capacity. There are three potential
items: A, B, and C. Their volumes and beneﬁts are as shown
in Table 1.
We want to maximize the total beneﬁt:
vixi = 2x1 + 5x2 + 4x3
Subject to the following constraints:
vixi = 9x1 + 6x2 + 7x3
and xi ϵ {0, 1} ,
for i = 1, 2..n
To ﬁnd the best solution, we have to identify a subset that
meets the constraint and has the maximum total beneﬁt. For
this problem, there are 23 possible subsets of items as shown
in Table 2.
The highlighted row satisﬁes the constraint. Hence,
the optimal beneﬁt for the given constraint (V = 15) can
only be obtained with one quantity of A, one quantity of B,
and zero quantity of C, and it has a resulting beneﬁt of 7.
B. RELATED WORK
Several algorithms have been proposed in the literature for
solving the KP01 problem. Most of the algorithms deals
with exact approaches. In this section, we present various
approaches that have been used to solve the KP problem.
Before further discussing each of the methods, we categorize
the approaches into three groups i.e.: Exact, Metaheuristic
and Hybrid Algorithms.
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
TABLE 2. Possible subsets of solution for KP01 Problem.
1) EXACT ALGORITHMS
The BB algorithm was ﬁrst proposed in and is a general
exact technique. The BB algorithm is essentially an enumeration strategy that prunes the non-promising regions of a
search space . One of the ﬁrst BB algorithms proposed for
the KP01 problem was presented in and later on ameliorated version in , . Bettinelli et al. presented a
BB based algorithm for Knapsack Problem with Conﬂict
Graph (KPCG) which is an extension of the classic KP01
problem. The study employed the BB algorithm to obtain
optimal solutions to the KPCG in short computational time.
The reduction of a solution space and enumeration of a
smaller number of nodes in BB based algorithms have been
onerous in ﬁnding solutions for various knapsack problems.
However, Tari presented an algorithmic procedure based
on the BB with three different selective branching mechanisms for the reduction of the solution space to derive
an optimal solution of the Multi-dimensional Multi-choice
Knapsack problem.
Another widely utilized exact approach is DP. The
DP algorithm was introduced by Richard Bellman , 
who ﬁrst used the term DP in 1957. The proposed DP heuristic has an improved algorithmic complexity of (nW), where
W is the total capacity of the knapsack problem. Later on, two
new algorithms were presented in that proved to outperform all previous exact methods that were implemented for
the KP01 problem. Detailed summaries of the DP and BB
approaches can be found in .
A review of the GSA, BB and DP algorithms are conducted
by . In their work, the authors compared and contrasted
each of the three algorithms, concluding that the most efﬁcient technique is the GSA. However, it is inappropriate under
certain conditions as it does not result in the optimal solution.
Furthermore, they noted that the DP algorithm proved to be
immensely efﬁcient in terms of the number of computations
for KP01 problems with lesser capacities. However, as the
capacity of the knapsack increases, the DP proved to be
inefﬁcient.
2) META-HEURISTIC ALGORITHMS
A variety of meta-heuristic algorithms with wide range
of applications to several real-world problems exist in the
literature – , this paper mainly discussed the SA and
the GA because both algorithms are fundamentals to other
recently proposed meta-heuristic algorithms. Most importantly, the SA and GA have an excellent record of ﬁnding
good quality solutions for the KP01 problems. The GA is
a commonly used algorithm for solving the KP01 problem. The characteristics of the GA for the KP01 problem
involves the encoding of solutions as an array of bits or character strings i.e. chromosomes, the manipulation of these
strings by genetic operators and a selection based on their
ﬁtness to ﬁnd the optimal solution to a given problem.
Stripling et al. reviewed how a simple GA can be applied
to solve the knapsack problem. They outlined the similarities
to the feature selection problem that frequently occur in the
context of an analytical model. Yadav and Singh used the
GA to solve the KP01 problem by utilizing corrupted renewal
and focal improvement operators which they apply to every
recent generated solution. Furthermore, Yadav and Singh 
conducted a review on the applications of GA for solving the
KP01 problem.
The SA algorithm on the other hand was developed by
Sonuc et al. for solving economic activities problems in 1983. The purpose of the SA algorithm is to ﬁnd
a global maximum or minimum point of a function that
has more than one local maximum or minimum point.
Kirkpatrick and Vecchi proposed an effective SA algorithm for the KP01 problem which runs on CPU and GPU.
Their method runs in parallel on a GPU platform with multistart technique to enhance the quality of solutions. They
deduced that their method was capable of delivering good
quality solution for both the low-dimensional and mediumdimensional instances within a short period of time.
3) HYBRID ALGORITHMS
Manaseer and Almogdady hybridized DP and GSA to
solve KP01 problem. It was found that the hybrid DP and
GSA is better than the constituent GSA and other compared
algorithms. However, the hybrid algorithm is found to be
inferior in terms of time and space complexity, it achieved
a time complexity of O(n log (n)). Lin et al. proposed
a new stochastic approach for solving the well-known combinatorial optimization problems. The study integrates the
GA into the SA algorithm to boost the performance of the SA.
Lin et al. highlighted some of the major drawbacks of the SA
and analysed the characteristics of the GA, noting that one of
the various challenges faced by SA schedules is the concept
of the quasi-equilibrium , . To combat the previously
proposed algorithms falling into local optimal solutions for
solving the KP01 problem, Zhou et al. proposed a binary
version of the monkey algorithm where the GSA is employed
to reinforce the local search ability. The algorithm used the
GSA to correct the infeasible solutions (solutions whose sum
of weights exceed the speciﬁed capacity). They concluded
that the proposed algorithm has strong advantages in solving KP01 problems for testing ﬁxed and random problems,
small and large-scale problems. Rezoug et al. presented
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
TABLE 3. Choice of algorithm selection for 0-1 Knapsack problem.
a hybrid heuristic algorithm which is referred to as the Guided
GA (GGA) for solving the Multidimensional Knapsack Problem (MKP). It is a two-step memetic algorithm composed of
a data pre-analysis and a modiﬁed GA. They deduced that the
GGA outperformed the basic GA on several instances. Similarly, the result showed that there is an improved performance
achieved by the GA and accelerated speed of the convergence
when guidance is provided to the algorithm.
C. CHOICE OF TECHNIQUES
After conducting extensive research (See Section II-B). Furthermore, the reasons for our choice of the representative
algorithms are as highlighted in Table 3. It is noteworthy to
emphasis here that the main focus of the current study is to
present an initial result based on the results obtained from the
comparative analyses of all the aforementioned algorithms:
GSA, DP, BB, SA and GA. More so, we have also tried to
establish some performance proﬁle for the ﬁve representative
algorithms which are supported by the detailed literature
III. HEURISTICS AND METAHEURISTICS ALGORITHMS
FOR SOLVING 0-1 KNAPSACK PROBLEM
In this section, we discuss in detail the ﬁve heuristics algorithms chosen to solve the 0-1 KP and subsequently provide an in-depth comparative analysis of the algorithms with
respect to their capability to provide feasible and near optimum solutions to the problem. In addition, the structural
design, limitations and advantages for each of the algorithms
are provided. Furthermore, algorithmic design and implementation steps are discussed.
A. GREEDY SEARCH ALGORITHM
A GSA is an algorithm that uses a heuristic technique for
making locally optimal choices at each stage with the hope
of ﬁnding a global optimum. Greedy algorithm often fails to
ﬁnd the globally optimum solution because it usually does
not operate exhaustively on all the data. This algorithm can
make commitments to certain choices too early which prevent
it from ﬁnding the best overall solution later . Assume
that there is an objective function that needs to be optimized
(either maximized or minimized) at a given point. A GSA
makes greedy choices at each step to ensure that the objective
function is optimized. The GSA has only one shot to compute
the optimal solution so that it never goes back and reverses
the decision. Some of the notable limitations of GSA are as
• The GSA does not always reach the global optimum
• The difﬁcult part is that, a greedy algorithm requires to
work harder to understand the correct issues. Even with
the correct algorithm, it is hard to prove why it is correct.
• It may produce the unique worst possible solution.
As regards the GSA and with respect to the KP01 problem,
there are several other greedy algorithms been proposed
to solve the KP01 problem and its other variants as
well. The most efﬁcient technique follows the following
procedures , 
• Compute the proﬁt-weight ratio of the given items.
• Sort the array containing the ratio of the items in decreasing order.
• Place the item with the highest ratio into the Knapsack
if it does not exceed the capacity of the Knapsack, else
proceed to the next item.
Our implementation using the GSA discussed in this paper
adopts the following set of steps:
• Sort the items in descending order according to their
• If an item can ﬁt in the knapsack, then it is added.
• Termination occurs when no more items can occupy the
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
B. DYNAMIC PROGRAMMING
The DP is used for problems requiring a sequence of interrelated decisions, that is, to take another decision, this
would depend on the previous decision or solution. The
DP method is a technique for solving problems whose
solutions satisﬁes recurrence relations with overlapping
sub-problems , . In terms of mathematical optimization, the DP usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time. Some
of the major limitations of the DP includes:
• No general formation of DP is available; every problem
has to be solved in its own way.
• Divide problem into sub-problems and store intermediate results that consumes memory.
The DP with respect to the KP01 problem considers an
instance of the problem deﬁned by the ﬁrst i items,
1 ≤i ≤N, with:
weights w1, w2 . . . , wi,
values v1, v2, . . . , vi,
and knapsack capacity j, 1 ≤j ≤Capacity.
Let Table [i, j] be the optimal solution of this instance (that
is, the value of the most valuable subsets of the ﬁrst i items
that ﬁt into the knapsack capacity of j). The Table can be populated by the Algorithm 1. The last value of the array Table,
that is, Table [N, Capacity] contains the optimal solution of
the problem. The implementation details of using DP to solve
the KP01 is described as follows:
• Algorithm 1 was used to ﬁnd the optimal value.
• Since the algorithm 1 only returns the optimal value,
we had to employ some form of backtracking. The backtracking technique used is shown in Algorithm 2.
C. BRANCH AND BOUND
The BB algorithm is used to ﬁnd the optimal solution by
keeping the best solution found so far. The BB constructs
candidate solutions one component at a time and evaluates
the partly constructed solutions. If a partial solution can not
improve on the currently optimal solution, it is abandoned.
This approach makes it possible to solve some large instances
of difﬁcult combinatorial problems. This algorithm is based
on the construction of a state space tree. A state space tree
is a rooted tree where each level represents a choice in the
solution space that depends on the level above and any possible solution is represented by some path starting out at the
root and ending at a leaf. A node’s bound value is compared
with the value of the best current solution obtained. If the
bound value is not better than the best current solution, that is,
not smaller for minimization and not larger for maximization,
the node is non-promising and can be terminated in view of
the fact that no solution obtained from it can yield a better
solution than the one already available solution. This is the
principle idea of the BB algorithm , . Next we discuss
the branching strategy.
According to Gupta and Ravindran , there are generally two ways of implementing the branching:
i. Branching on the node with the smallest bound: Search
all the nodes and ﬁnd the one with the smallest bound
and set it as the next branching node.
Advantage: Generally, it will inspect less sub-problems
and thus saves computational time.
Disadvantage: Normally, it will require more storage.
ii. Branching on the newly created node with the smallest
bound: Search the newly created nodes and ﬁnd the one
with the smallest bound and set it as the next branching
Advantage: Saves storage space.
Disadvantage: Require more branching effort and
therefore not computationally efﬁcient.
In the context of BB with respect to solving the KP01 problem, if there are N possible items to choose from, then, the
kth level represents the state where it has been decided which
of the ﬁrst k items have or have not been included in the
knapsack. In this case, there are 2 k nodes on the kth level
and the state space tree’s leaves are all on level N.
As regards our implementation using the BB algorithm,
we implemented the BB algorithm using a priority queue.
The solution is built stepwise. The upper bound (ub) is ﬁrst
calculated. This is computed by adding the total proﬁt of the
items that are already selected, p, the product of the remaining
capacity of the knapsack, M −w, and the best proﬁt-weight
ratio, which is
Wi+1 . Hence, the formula used (Equation 5)
adopted from :
ub = p + (M −w)
i. Using a heuristic, ﬁnd a solution xs to the KP01 problem. Next store it’s value, Best = f (xs), where Best
denotes the best found current solution.
ii. Initialize a queue to the partial solution with none of
the variables.
iii. Loop until the queue is empty:
a. Take a Node N off the queue
b. If N represents a single candidate solution x and
f (x) < Best, then x is the best current solution.
Record it and set Best ←f (x)
c. Else , branch on N to produce new nodes Ni.
d. For each of these:
• If g(Ni) > Best, do nothing since the lower
bound on this node is greater than the upper
bound, since it will never lead to an optimal
solution and therefore can be disregarded
• Else, store Ni on the queue
D. GENETIC ALGORITHM
The GA is a heuristic search and optimization algorithm
inspired by natural evolution . The GA begin with a set of
candidate solutions (chromosomes) called population. A new
population is created from solutions of an old population
with the hope of getting a better population. Solutions which
are chosen to form new solutions (offspring) are selected
according to their ﬁtness. The more suitable the solutions are
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
Algorithm 1 Pseudocode of the Implemented Dynamic Programming Method
for i = 0 to N do
for j = 0 to Capacity
if j < Weights[i] then
Table[i, j] ←Table[i −1, j]
Table[i, j] ←maximum {Table[i −1, j]
Values[i] + Table[i −1, j −Weights[i]]}
return Table[N, Capacity]
Algorithm 2 Pseudocode of the Backtracking Technique
c ←Capacity
Start at position Table[n, c]
while remaining Capacity > 0 do
if Table[n, c] = Table[n −1, c] then
Item n has not been included in the optimal solution
Item n has been included in the optimal solution
Process item n
Move one row up to n −1
Move to column c−weight (n)
the bigger chances they have to reproduce. This process is
repeated until some condition is satisﬁed. The main components of the GA are the chromosome encoding, the ﬁtness function, selection, recombination and the evolution
scheme .
1) CHROMOSOME ENCODING
The GA manipulates population of the chromosomes. The
chromosomes are string representations of solutions to a
particular problem . For the KP, we use binary encoding,
where every chromosome is a string of bits, 0 or 1 .
2) FITNESS FUNCTION
The ﬁtness function is a computation that evaluates the
quality of the chromosome as a solution to a particular
problem . The ﬁtness function allocates a score to each
chromosome in the population. This will help determine how
well a particular solution solves a problem.
3) SELECTION PROCESS
In this process, chromosomes are selected for recombination
on the basis of the ﬁtness function. An evaluation of the
current population members is done. A selection of the subset
with the best ﬁtness values to act as parents for the next
generation is carried out. Those chromosomes with a higher
ﬁtness score have a signiﬁcantly higher chance of selection
than those with a lower ﬁtness score. Hence, creating a basis
for more highly ﬁt solutions.
4) CROSSOVER
Crossover takes chromosome pairs that have been chosen
from the selection process. These selected pairs are merged
together to generate the new successor population. The idea is
to simulate the mixing of genetic material that can occur when
organisms reproduce . Consider the following parents and
a crossover point at position 3:
Parent 1: 1 0 0 | 0 1 1 1
Parent 2: 1 1 1 | 1 0 0 0
Offspring 1: 1 0 0 1 0 0 0
Offspring 2: 1 1 1 0 1 1 1
These offspring are created by exchanging the genes of
parents until the crossover point is reached. Hence, offspring
1 receives the ﬁrst 3 bits to the left of the parent 1’s crossover
point and the remaining bits from the right of parent 2’s
crossover point. Similarly, offspring 2 receives the ﬁrst 3 bits
to the right of the parent 1’s crossover point and the remaining
bits from the left of parent 2’s crossover point.
5) MUTATION
In terms of biology, mutation may allow for the probability
of a child to inherit a characteristic (feature) that was not
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
FIGURE 1. Single point crossover.
inherited from the parents. To maintain generic diversity from
one generation of the population to the next, the mutation
process ﬂips bits from 0 to 1 or from 1 to 0.
6) LIMITATIONS OF THE GENETIC ALGORITHM
• It is really difﬁcult for a researcher to come up with a
good heuristic that actually reﬂects what we want the
algorithm to do.
• GA’s cannot always ﬁnd the exact solution but they
always ﬁnd the best solution.
• GA’s are computationally expensive, that is, they are
time consuming.
• GA’s requires less information about the problem but
designing an objective function, getting the representation and operators right can be fairly difﬁcult.
7) GENETIC ALGORITHM IMPLEMENTATION
The GA implementation steps centres around good formulation of the algorithm’s solution representation, ﬁtness function, genetic operators and termination condition.
Solution representation: A binary representation is used
where a value of 1 indicates that an object is placed in the
knapsack, whilst a value of 0 indicates that the object is left
Fitness function: The GA ﬁtness function denotes thesum
of values represented by the chromosome.
8) GA OPERATORS
i. Single point crossover- in this crossover, a single
crossover point on both parent chromosomes is selected
by choosing a random number. Both the parent chromosomes are split at the crossover point chosen and
all data beyond that point in either chromosome
is swapped between the two parent chromosomes.
A crossover rate of 0.1 is used. An example is depicted
in Figure 1:
ii. Bit-Flip Mutation: Select one random bit and ﬂip it.
This is used for binary encoded GAs. A mutation rate
of 0.3 is used. Figure 2 depicts an example of Bit-ﬂip
iii. Roulette wheel selection: In this technique, all the chromosomes in the population are placed on the roulette
wheel according to their ﬁtness value. Each individual
is assigned a segment of the roulette wheel whose
FIGURE 2. Bit-flip mutation.
size is proportional to the value of the ﬁtness of the
individual. The bigger the ﬁtness value is, the larger
the segment. Then, the virtual roulette wheel is span.
The individual corresponding to the segment on which
roulette wheel stops are then selected. The process
is repeated until the desired number of individuals is
iv. Termination Criteria: There are two termination criteria: Firstly, it occurs if 90% of the population fall within
convergence of the ﬁttest individual in the population.
Secondly, it occurs if the ﬁttest individual reﬂects no
change for 700 generations. Either of these criterion
will result in termination. A ﬂowchart of the GA is
depicted in Figure 3.
E. SIMULATED ANNEALING ALGORITHM
The SA is a stochastic computational algorithm for ﬁnding global extremums to large optimization problems. The
name and inspiration of the algorithm was originally inspired
from the process of annealing in metal work. Annealing
involves heating and cooling a material to alter its physical
properties due to the changes in its internal structure. It is
generally used when the search space is discrete. At each
iteration of a SA algorithm applied to a discrete optimization problem, the objective function values for two solutions
(the current solution and a newly generated neighbouring
solution) are compared. Better solutions are always accepted,
while a fraction of inferior solutions are accepted in the
hope of escaping local optima in search of global optima.
The key feature of the SA is that it is able to escape local
optima by allowing worse moves (i.e. moves to a solution
that corresponds to a worse objective function value) .
The basic ingredients for the SA process include solution
space, neighbourhood structure, cost function and Annealing
There have been very few existing implementations of
the SA for the KP01 problem. Hence, this project attempts
to explore this largely unexplored area. A common practice
when using evolutionary algorithms is to disregard solutions
that are infeasible (i.e. solutions where the weight exceeds the
maximum capacity of the knapsack) and ‘‘drop’’ an item from
the knapsack until it’s feasible. However, due to the nature
of SA, our implementation gives the infeasible solutions a
chance to improve in future iterations. Empirically, this technique has been evaluated to be more optimum, hence it is used
in our implementation. Below are two main limitations of
• There is a clear trade-off between the quality of
the solution and the time required to produce the
• The precision of numbers used can have a signiﬁcant
effect upon the quality of the outcome.
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
FIGURE 3. Flowchart for the genetic algorithm.
The implementation of the SA is described as follows:
Representation: A binary representation is used, where a
value of 1 indicates that an object is placed in the knapsack,
whilst a value of 0 indicates that the object is left behind.
Pseudocode: The pseudocode of our implementation is
shown in Algorithm 3. It is inspired by the basic concept
of the SA which is now extended to the knapsack problem.
A ﬁnite-length Markov chain is generated at a certain temperature Tk and a temperature control parameter α, that slowly
cools the system until a certain temperature at which the
system is frozen.
State Generation: The state generation is performed by
randomly selecting a bit of the current_state and ﬂipping it
to form the next_state. Figure 4 shows the state generation.
Termination Criteria: The algorithm terminates when the
temperature reaches the minimum allowed temperature of
the system, indicating that the system is frozen. Our implementation uses an initial temperature of 1000 and is slowly
decreased to a frozen state of 0.0001. The temperature change
is accomplished by multiplying the system temperature by
α = 0.9999.
F. HYBRID METAHEURISTIC ALGORITHMS FOR SOLVING
0-1 KNAPSACK PROBLEM
In an effort to improve the quality of solutions obtained
by the basic meta-heuristic algorithms speciﬁcally the GA
and SA algorithms, a hybrid approach was adopted. The
IGA-SA comprises of SA integrated in GA. The SA is
expected to reﬁne the population generated by the GA. This is
FIGURE 4. Simulated Annealing state generation.
accomplished by searching the neighbourhood of a candidate solution for better ﬁt solutions. Majority of the
hybrid approaches involving these two algorithms apply
the SA algorithm to each individual in the population.
However, this is computationally expensive and may ultimately prove to be useless as only half of the current
population are selected to reproduce; and the other half
is discarded. We adopt the approach in where the
SA algorithm is only applied to the ﬁttest individual in each
generation (see Figure 4). The GA and SA algorithms used
in our hybrid IGA-SA implementation are of the simpler
variations of each of the standard GA and SA algorithms,
respectively. Therefore, we opted for simpler algorithms to be
sure that the solutions generated by the hybrid approach are
the direct result of combining the performance capabilities
of the two algorithms and not narrowing down to the numerous improvements made on each algorithm by the research
community.
The conﬁguration of each algorithm is shown in Table 4,
the hybrid algorithm adopts majority of its conﬁguration values from the parameter settings of both the GA and SA shown
in Table 4. Though, the SA executes for only 250 iterations in
the hybrid implementation whereas it executes for 2000 iterations in the standalone implementation. This is due to the
SA algorithm acting as a local search technique in the hybrid
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
Algorithm 3 Pseudocode of the Implemented Simulated Annealing Algorithm
Initialize parameters of the annealing schedule;
Generate an initial state as the current_state;
Generate the next_state;
1 = value of next_state – value of current_state;
if 1 > 0 and solution is feasible then
current_state = next_state
if value of current_state >value of best_state then
best_state = current_state;
acceptance_function = exp (−1/Tk);
if acceptance_function > random [0,1) then
current_state = next_state;
until system equilibrium at Tk
Tk+1 = Tk ∗α
until system has been frozen
print out the current_state as the ﬁnal state.
implementation and only operating on the best individual in
the current population, 2000 iterations essentially means the
generation of 2000 neighbour solutions which is deemed too
excessive.
The deﬁnition of the neighbourhood of a solution is pivotal
in the performance of the SA algorithm. Since the SA algorithm acts as a local search technique here, it is beneﬁcial
that we generate neighbour solutions in such a way that they
are of better ﬁtness than the current solution. A neighbour
solution is generated depending on the sum of its weights,
if the sum of its weights exceeds the speciﬁed capacity of
the knapsack then the neighbour solution is deﬁned to be the
solution with one item randomly removed. If the sum of its
weights is below the speciﬁed capacity, then the neighbour
solution is deﬁned to be the solution with one item randomly
added. In evaluating an individual, the same ﬁtness function
is applied to all three algorithms and is designed in such a
way that it penalizes infeasible solutions.
GAs have shown to be well suited for high-quality solutions to larger NP problems and currently they are the most
efﬁcient methods for ﬁnding an approximately optimal solution for optimization problems. They do not involve extensive
search techniques and do not try to ﬁnd the best solution.
The GA can reserve excellent individuals for the next generation in the genetic operation process and guarantee the
diversity of population. The SA algorithm has strong local
search ability and is capable of escaping from local optimal
solutions. However, GA is liable to converge prematurely and
be trapped in local optimal solutions. In addition, the SA is
bottlenecked with high computational time. Therefore, by the
combination of the two algorithms, an IGA-SA is selected for
hybridization to escape from the limitations of the GA and SA
pointed out to form a strong hybrid algorithm.
IV. COMPUTATIONAL EXPERIMENT
The experiments were carried out using a 2.80 GHz intel
core i7 processor and 4GB memory. The entire algorithms
were coded in Java with Eclipse integrated development
environment (or IDE). The parameter settings for the two
population based algorithms are presented in Table 4.
A. DATASETS
The datasets used for the 0-1 knapsack problem was obtained
from David Pisinger’s optimization codes . There are four
different categorical datasets. Low dimensional uncorrelated
(LD-UC), high dimensional uncorrelated (HD-UC), high
dimensional weakly correlated (HD-WC) and high dimensional strongly correlated (HD-SC). The variations within the
correlation of instances in these datasets will provide an overall performance overview on each of the implemented algorithms. The details of the datasets together with their optimum
values recorded are shown in Table 5, while the respective
characteristics of the datasets used are shown in Table 6.
B. EXPERIMENT 1: RESULTS AND DISCUSSION
The ﬁve algorithms: BB, DP, GA, GSA and SA were all
tested on the four different categorical datasets and a wide
range of results were formulated from the experiments carried
out. A summary of these results and key observations made
are discussed in the next section. However, the discussion
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
TABLE 4. Genetic Algorithm and Simulated Annealing parameter configuration.
TABLE 5. Datasets used with their optimum recorded values.
TABLE 6. Dataset characteristics.
of results is presented according to the following dataset
structure:
• LD-UC datasets
• HD-UC datasets
• HD-WC datasets
• HD-SC datasets
• SA and GA iterations
• Overall comparisons and summary of results
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
TABLE 7. Results obtained on LD-UC datasets.
1) LOW-DIMENSIONAL UNCORRELATED DATASETS
Table 7 shows all the results obtained on LD-UC datasets.
From Table 7 and Figure 5, it can be seen that on the
initial low-dimensional datasets, all algorithms performed
well and obtained results in the same range, except for the
BB algorithm. The GSA had obtained near optimal results
on this dataset. This could be a special case such that by
selecting the items of highest value the weight requirements
would increase accordingly and the results obtained becomes
convincing. The GA, SA and DP all performed in the
same range. Moreover, their execution time was similar,
while DP had run in marginally less time as the rest. Even
though the SA had obtained comparable results, it has
the highest operational time. This is likely because of the
gradual temperature decrease in the SA algorithm, which
can take up larger amounts of computational resources.
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
FIGURE 5. A representation of the performances on the LD-UC datasets
in terms of the best values produced.
FIGURE 6. A representation of the performances on the LD-UC datasets
in terms of the algorithm computational time.
The computational time for the ﬁve algorithms are shown
in Figure 6.
2) HIGH-DIMENSIONAL UNCORRELATED DATASETS
Table 8 shows all the results obtained based on the HD-UC
datasets. Figure 7 and 8 show performances of the algorithms
on a HD-UC dataset. It should be noted that the numbers
below the bar indicate the ’shortfall’ (i.e. how much short the
value is from the optimal solution recorded in the literature).
For example, it can be seen from Figure 8 that in this instance,
DP had a shortfall of zero (i.e. produced the same results as
obtained in the literature) whereas BB had a shortfall of 1091
(i.e. compared results from literature had produced a solution
with a value of 1091 greater than that produced by the BB).
An important observation is that the DP algorithm was
the only algorithm to produce optimal results as compared
to those found in the literature. Hence, the DP had achieved
perfect outcomes on all datasets within this category. On the
FIGURE 7. Performances of all the algorithms on a HD-UC dataset with
1000 items and a knapsack capacity of 1000.
FIGURE 8. Performances of all the algorithms on a HD-UC dataset with
10,000 items and a knapsack capacity of 1000.
FIGURE 9. A representation of the performances of all the algorithms on
HD-UC datasets in terms of the best values produced.
other hand, BB also performed consistently well throughout these datasets, followed closely behind DP as shown in
Figure 9. Interesting observations can be picked up from
the results produced by the GA. The performance seems to
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
TABLE 8. Results obtained on HD-UC datasets.
ﬂuctuate, often based on the size of the datasets. It can be seen
from Figure 7 and Figure 8 that when used on datasets that
are much larger in size, the GA performs extremely poor. This
may be attributed to the fact that the GA can easily get stuck
in a local optimum. Therefore, when much larger datasets
are used, the chance of getting stuck in a local optimum is
much higher. It can be seen from Figure 9 that the GSA
had performed consistently poor whereas the SA showed
relatively average performances on these datasets.
3) HIGH-DIMENSIONAL WEAKLY CORRELATED DATASETS
Table 9 shows all the results obtained on HD-WC datasets.
The main observation from this dataset was that the shortfall
in terms of literature recorded the best value of zero in total for
the DP algorithm. Hence, DP had achieved perfect outcomes
on all datasets within this category. Moreover, we can infer
from Figure 10 that BB performed second best on these
datasets. The computational time is shown in Figure 11.
When comparing BB and DP in terms of computational time,
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
FIGURE 10. A representation of all the algorithms performances on the
HD-WC datasets in terms of the best values produced.
FIGURE 11. A representation of the performances of all the algorithms on
the HD-WC datasets in terms of computational time .
it can be seen that the BB completed the dataset in 4.175s
faster than the DP. Therefore, the BB algorithm did provide
faster computational time for marginally lower results than
the DP. While DP had taken a few seconds time longer
to obtain marginally higher results. GA and SA had taken
approximately 10min and 16min to complete its execution
while still providing results lower than that of the BB or DP.
This can be largely attributed to the time taken to create and
evaluate generations in the GA algorithm and the iterative
temperature changes implemented in the SA. These aspects
of larger datasets increase the amount of time needed to
complete execution. Amongst the ﬁve algorithms evaluated,
GSA and SA performed the lowest.
4) HIGH-DIMENSIONAL STRONGLY CORRELATED DATASETS
Table 10 shows all the results obtained on HD-SC datasets.
On this dataset, BB, DP and GA all performed amongst the
best, while GSA and the SA performed with the lowest total
FIGURE 12. A representation of all the algorithms performances on the
HD-SC datasets in terms of the best values produced.
FIGURE 13. A representation of all the algorithms performances on the
HD-WC datasets in terms of computational time.
values achieved. The results are illustrated in Figure 12 and
the computational times are shown in Figure 13. The high
performances of BB and DP along with their low computation
time are trends seen inclusive of previous datasets. It can
be seen that GSA had achieved the fastest convergence time
on the dataset, but had produced the worst results. This
is likely because of the algorithm accepting items without
weight implications. This quickly leads to the GSA obtaining
the best values items but not making optimal usage of it is
weight, leading to an overall low value. On all datasets in this
category, the DP algorithm had achieved most optimal results,
followed by the BB algorithm. In this dataset category, we can
still observe that BB and DP had performed in a shorter time
compared to GA and SA.
5) COMPARISON BETWEEN GA AND SA
The SA and GA algorithms both have a degree of randomness
associated with its performance. Therefore, these algorithms
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
TABLE 9. Results obtained on HD-WC datasets.
were run with additional 10 times number of iterations on
each of the 31 datasets. This allows us to further analyse the
average performance of the algorithms and helps determine
the individual algorithm performance consistency. Furthermore, from these 10 iterations we can observe the highest and
lowest value obtained.
It can be seen from Figure 14 that for the LD-UC dataset,
both the SA and GA performed the same on each of their
iterations. This can be due to the smaller dataset dimensions making it easier to obtain the optimum values. It can
also be noted that the GA had obtained a higher value than
For the larger and more complex datasets, such as the
HD-UC we can observe the ﬂuctuations between the two
algorithms. This can be seen in Figure 15. It is apparent that
the GA has two type of trends. It is either it converges to a
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
TABLE 10. Results obtained on HD-SC datasets.
globally optimum value on the dataset indicated by the taller
bars, or it converges to a local optimum and terminates, which
are indicated by the shorter bars. This creates two very drastic
changes in the results obtained on the same dataset. On the
other hand, even though the SA did ﬂuctuate between the
10 iterations, its results were more consistent with working
towards higher values, while the bar graph shows no sign
of repeated convergence towards some local value. Despite
these trends in both the GA and SA, the GA still manages
to obtain higher results if we compare the highest values
achieved over the 10 iterations.
A similar trend can be seen in Figure 16 with the
HD-SC datasets. The ﬂuctuations on the GA graph point
towards some form of global optimum and local convergences, while the ﬂuctuations on the SA graph seem to be
more randomised and doesn’t indicate any form of local
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
FIGURE 14. Multiple iterations of the GA and SA on a LD-UC dataset with 4 items.
FIGURE 15. Multiple iterations of the GA and SA on a high-dimensional uncorrelated dataset with 2000 items.
FIGURE 16. Multiple iterations of the GA and SA on a HD-SC dataset with 5000 items.
convergence. Furthermore, we can still see that the GA had
obtained higher results than the SA on its iterations for all
4 types of datasets.
6) OVERALL EVALUATION STUDY FOR THE ALGORITHMS
The GA, GSA, SA, DP and BB algorithms were all
implemented to evaluate their effectiveness on solving the
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
FIGURE 17. A representation of the performances on all the datasets in
terms of the best values produced.
FIGURE 18. A representation of the performances on all the datasets in
terms of the algorithm computational time.
KP01 problem. Figures 17 and 18 show the performances of
these algorithms on all datasets in terms of the best values
produced and computational time respectively. Furthermore,
it was observed that the GA did produce exceptional good
quality solution on the LD-UC dataset but fell short compared
to the other algorithms on the high dimensional datasets. This
can be due to the fact that the algorithm can easily get trapped
in local optimum states. Moreover, the difﬁculty of obtaining
effective heuristics can alter the outcomes of speciﬁc results.
On the high dimensional datasets, the GA had taken up
the second position most time compared to all the algorithms.
This is likely due to the generation of populations for each
iteration. In some high dimensional datasets this process
becomes lengthy. Moreover, when the average result of each
dataset was observed over 10 iterations we could distinctly
observe a pattern in the results. The GA had either converged
to a global optimum or a local optimum. Overall the GA has
the potential to ﬁnd high values solutions but does take extra
computational time. The GSA had in all cases performed
the fastest. This is simply due to it is nature of selecting the
highest valued items without considering future implications
of the weight obtained. Through all the datasets, the GSA
had performed below average and with the fastest amount of
time. The GSA is not an optimal algorithm in terms of solving
the KP01 problem. In terms of the SA, we can observe a
similar pattern to that of the GA. We had obtained comparable
results in all datasets. The only exception was that the SA
had taken the longest time to execute on all datasets and in
some cases not achieving the most optimal result. This can
also be constrained to the difﬁculty of ﬁne turning all the
parameter settings which could be different for each dataset.
In terms of consistency, it showed improvements compared
to the GA when we observe the results over 10 iterations
on each dataset. Unlike the obvious convergence to local
minimums experienced by the GA, there are no indications
of local convergence from the SA algorithm. The DP and
BB both performed very similar in terms of both the optimal
value being obtained and the execution time required. On all
the dataset dimensionalities both algorithms produced the
most competitive results. Furthermore, we can observe that
the execution time of these algorithms were far lower than the
GA and SA, with the exception of GSA. On a marginal note,
we can deduce that the DP had obtained an overall higher
value than the BB. The important fact to note is that the
DP algorithm had taken an overall time, approximately
23 times larger than the BB.
C. EXPERIMENT 2: RESULTS AND DISCUSSION
The second experiment considers an improvement strategy
that would enhance the performance of the standard GA and
SA. Therefore, the results of a hybrid implementation of the
IGA-SA is presented in this section. The experimentation in
this case comprises of three algorithms namely, GA, SA, and
IGA-SA. More so, all the three algorithms were tested using
the parameter conﬁgurations described in Table 4 and under
the same experimental conditions, that is, each algorithm was
tested on the exact same machine using the same dataset.
The machine was also left idle in an effort to speed up
execution and prevent the execution time of an algorithm
being inﬂuenced by external factors. Due to the stochastic nature of metaheuristic algorithms, several iterations are
required to gain a near optimum solution, we executed the
three algorithms for 10 iterations and the best solution from
those 10 iterations was chosen.
In the results shown in Table 11, the GA and IGA-SA
(denoted as hybrid in the ﬁgures) algorithms produced nearoptimum and even optimum solutions on datasets of low
dimensionality but performance worsens as the size of the
dimensionality of the problem increases, this is likely due to
the complexity of the problem increasing. Overall, the simulated annealing algorithm performed the worst – it even produced infeasible solutions on the low dimensionality datasets,
therefore, its results was omitted in Figures 19, 20, 21, and 22.
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
TABLE 11. Overall results.
The GA outperforms the hybrid algorithm on a few datasets
but overall the hybrid algorithm outperforms the GA, as the
dimensionality of the dataset increases, the GA struggles to
produce feasible solutions whilst the hybrid algorithm always
produces a feasible solution albeit that solution isn’t optimal.
The poor performance of the GA on an uncorrelated dataset
with a relatively large dimensionality can be seen in Figure 19, all 20 solutions produced are infeasible whereas all
20 solutions produced by the hybrid algorithm are feasible
with one near-optimum solution (optimum value: 11238).
The GA outperform the hybrid algorithm on weakly correlated datasets (Figure 20). However, the hybrid algorithm
still produces feasible solutions of sufﬁcient quality. The
optimum value for the dataset used in Figure 21 was 1514.
Interestingly, the results were mixed for the experiments
conducted on the strongly correlated datasets. On the dataset
knapPI_3_100_1000_1 (see Table 11), the GA produced a
near-optimum solution whereas the hybrid algorithm produced a solution well below the optimum solution but on the
dataset knapPI_3_200_1000_1, we see the usual trend, the
GA produces infeasible solution whereas the hybrid algorithm still produces feasible solutions (see Figure 22). It is
worth noting that the second dataset has a higher dimensionality which might be the reason for the GA to perform poorly on
the second dataset despite producing a near-optimum solution
on the ﬁrst dataset.
From the above graphs (Figures 19 to 22) we can gain
an overall idea of the performance of the IGA-SA when
compared to the SA and GA. It has been noted that the
GA had produced infeasible solutions. The comparison
in Figure 23 shows the comparison between the best values obtained by each of the three algorithms on the entire
dataset. From this it can be assumed that the GA had greatly
outperformed the SA and even the hybrid IGA-SA. Now we
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
FIGURE 19. Scatter plot of solutions produced by the hybrid and GA on
the uncorrelated dataset knapPI_1_200_1000_1 (Max Weight: 1008).
FIGURE 20. Scatter plot of solutions produced by the hybrid and GA on
the weakly correlated dataset knapPI_2_100_1000_1 (Max Weight: 995).
consider the removal of invalid solutions. Figure 24 represents the best values achieved by the algorithms once the
invalid solutions are removed. We can now compare the
actual performance of the GA. It can be observed that the
GA had produced inferior results compared to the SA and
IGA-SA. It can be deduced that most likely the hybridisation
of GA with SA allowed the properties of SA, speciﬁcally
the property of escaping local convergence, to aid the GA
in avoiding premature convergence. Therefore, this property
combined with the GA’s ability to explore large amounts of
solution spaces allowed for an improved IGA-SA that only
created feasible solutions.
Lastly, we consider the operational and computational time
of the hybrid IG-SA algorithm. It can be seen in Figure 25 that
FIGURE 21. Scatter plot of solutions produced by the hybrid and GA on
the strongly correlated dataset knapPI_3_100_1000_1 (Max Weight: 997).
FIGURE 22. Scatter plot of solutions produced by the hybrid and GA on
the strongly correlated dataset knapPI_3_200_1000_1 (Max Weight: 997).
the operation of the IGA-SA is much larger than the individual GA and SA algorithms. This is due to the fact that the
SA reﬁnes the population generated by the GA by searching
the neighbourhood of candidate solutions for more ﬁt individuals. Hence, the operational cost is much higher. Figure 26 shows the time expenditure which directly correlates
with the operational cost.
We can observe independently that GA algorithm seemed
to perform well but had incorporated infeasible solutions.
When combined with the SA we observe that only feasible
solutions were obtained. Moreover, the SA had been able to
generate populations of better quality for the GA by reﬁning
the population generated and searching the neighbourhood
of a candidate solution for better ﬁt solutions. The property of the SA which allows it to prevent accepting solutions which aren’t global had further improved the GA’s
performance and worked in tangent with the mutation and
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
FIGURE 23. Bar graph representing best values achieved inclusive of
infeasible solutions.
FIGURE 24. Bar graph representing best values achieved exclusive of
infeasible solutions.
FIGURE 25. Bar graph representing total operations done by each of the
constituent algorithm.
crossover properties of GA. In summary, it is clear that the
hybrid IGA-SA algorithm is a suitable algorithm in solving
the KP01 problem. Furthermore, it outperforms the GA and
SA algorithm on majority of datasets and produces feasible
FIGURE 26. Bar graph representing total time taken by each the
constituent algorithm.
solutions to the large-scale datasets whereas the GA and
SA algorithms produce infeasible solutions to these datasets.
However, the hybrid IGA-SA is signiﬁcantly more computationally expensive since it does combine the logical components of both the GA and SA.
V. CONCLUSIONS
This paper presents some initial results of DP, BB, GSA,
SA, GA and hybrid IGA-SA algorithms for solving the
KPO1 problem. In the comparison of these algorithms,
we had observed multiple trends and trade-offs. Some trends
included the observation of the GA over 10 iterations either
getting stuck in a common local optimum or achieving the
global optimum. Moreover, a notable trend was seen with
the BB and DP algorithms achieving global optimum results
in low times. It was also observed that the population based
algorithms namely GA, IGA-SA, and SA resulted in the highest computational time, simply based on the iterative nature
of these algorithms. The GSA showed low time complexity,
however performed poorly in terms of generating optimum
values. Overall, we can deduce that both the DP and BB
are the most effective approaches which were evaluated to
solve the KP01 problem. DP had produced slightly more
optimal values than BB, whereas BB showed a slightly lower
computational time than the DP. These two algorithms performed exceptionally well on both low and high dimensional
datasets with different levels of correlation. This performance
was established taking into account both the algorithm time
complexity as well as the optimum values found. More consideration was also given to the GA and SA due to both
algorithms being nature-inspired algorithms with seemingly
complementary properties. The SA has strong local search
ability and is capable of escaping from local optimal solutions. However, GAs are liable to converge prematurely
and be trapped in local optimal solutions. In addition, the
SA is bottlenecked with high computational time. Taking
into account the properties mentioned above, a hybrid of the
two algorithms was implemented and the result proved to be
VOLUME 7, 2019
A. E. Ezugwu et al.: Comparative Study of Meta-Heuristic Optimization Algorithms for 0-1 Knapsack Problem: Some Initial Results
more superior than that of the GA and SA respectively. In
the future, we intend to carry out more complex performance
study on multiple greedy problems.