Efﬁcient Training for Positive Unlabeled Learning
Emanuele Sansone, Francesco G. B. De Natale, Senior Member, IEEE and Zhi-Hua Zhou, Fellow, IEEE
Abstract—Positive unlabeled (PU) learning is useful in various practical situations, where there is a need to learn a classiﬁer for a
class of interest from an unlabeled data set, which may contain anomalies as well as samples from unknown classes. The learning task
can be formulated as an optimization problem under the framework of statistical learning theory. Recent studies have theoretically
analyzed its properties and generalization performance, nevertheless, little effort has been made to consider the problem of scalability,
especially when large sets of unlabeled data are available. In this work we propose a novel scalable PU learning algorithm that is
theoretically proven to provide the optimal solution, while showing superior computational and memory performance. Experimental
evaluation conﬁrms the theoretical evidence and shows that the proposed method can be successfully applied to a large variety of
real-world problems involving PU learning.
Index Terms—Machine learning, one-class classiﬁcation, positive unlabeled learning, open set recognition, kernel methods
INTRODUCTION
OSITIVE unlabeled (PU) learning refers to the task of
learning a binary classiﬁer from only positive and unlabeled data . This classiﬁcation problem arises in various
practical situations, such as:
Retrieval , where the goal is to ﬁnd samples in an
unlabeled data set similar to user-provided ones.
Inlier-based outlier detection , where the goal is to
detect outliers from an unlabeled data set, based on
inlier samples.
One-vs-rest classiﬁcation , where the negative
class is too diverse, thus being difﬁcult to collect and
label enough negative samples.
Open set recognition , where testing classes are
unknown at training time and the exploitation of unlabeled data may help learning more robust concepts.
Naive approaches are proposed to address PU learning.
In particular, it is possible to distinguish between solutions
that heuristically identify reliable negative samples from
unlabeled data and use them to train a standard binary
classiﬁer, and solutions based on binary classiﬁers using all
unlabeled data as negative. The former are heavily dependent on heuristics, while the latter suffer the problem of
wrong label assignment.
The recent works in and formulate PU learning as
an optimization problem under the framework of statistical
learning theory . Both works theoretically analyse the
problem, deriving generalization error bounds and studying
the optimality of the obtained solutions. Even though these
methods are theoretically grounded, they are not scalable.
In this work we present a method that provides better
scalability, while maintaining the optimality of the above
approaches for what concerns the generalization. In particular, starting from the formulation of the convex optimization
Emanuele Sansone and Francesco De Natale are with the Department
of Information Engineering and Computer Science (DISI), University of
Trento, Trento 38123, Italy.
E-mail: 
Zhi-Hua Zhou is with the National Key Laboratory for Novel Software
Technology, Nanjing University, Nanjing 210023, China.
problem in , we derive an algorithm that requires signiﬁcantly lower memory and computation, while being proven
to converge to the optimal solution.
The rest of the paper is organized as follows. Section 2 reviews related works, starting from the comparison of PU learning with one-class classiﬁcation and semisupervised learning and describing the main theoretical
results achieved in PU learning. Section 3 provides the formulation of the optimization problem under the framework
of statistical learning theory and enunciates the representer
theorem for PU learning. Section 4 and Section 5 describe
USMO (Unlabeled data in Sequential Minimal Optimization) algorithm and prove its convergence, respectively. Section 6 provides a comprehensive evaluation of the proposed
algorithm on a large collection of real-world datasets.
LITERATURE REVIEW
PU learning is well known in the machine learning community, being used in a variety of tasks such as matrix completion , multi-view learning , and semi-supervised
learning . It is also applied in data mining to classify
data streams or time series and to detect events,
like co-occurrences, in graphs .
PU learning approaches can be classiﬁed in two broad
categories, according to the use of unlabeled data: twostage and single stage approaches. The former extract a
set of reliable negative samples from unlabeled data and
use them, together with the available positive data, to train
a binary classiﬁer , , , . These ﬁrst step is
heuristic and strongly inﬂuences the ﬁnal result. The latter
regard all unlabeled data as negative samples. Positive
and negative data are then used to train different classiﬁers
based on SVM , , neural networks or kernel density estimators . These approaches suffer the problem
of wrong label assignment, whose effect depends on the
proportion of positive samples in the unlabeled dataset. We
will see later, in the discussion about theoretical studies
of PU learning, how critical this issue is. For the moment,
we focus on analyzing the relations of PU learning with
one-class classiﬁcation (OCC) and semi-supervised learning,
which allows us drawing some clear boundaries between
these tasks and highlighting the novelty of this work.
Comparison with one-class classiﬁcation
The main goal of OCC is to estimate the support of data
distribution, which is extremely useful in unsupervised
learning, especially in high-dimensional feature spaces,
where it is very difﬁcult to perform density estimation.
OCC is applied to many real-world problems, such as
anomaly/novelty detection (see for a recent survey
and deﬁnition of anomaly detection and , for novelty detection). Other possible applications of OCC include
author veriﬁcation in text documents , document retrieval , and collaborative ﬁltering in social networks .
Authors is , are among the ﬁrst to develop
OCC algorithms.1 In particular,
 proposes a classiﬁer
that ﬁnds the hyperplane separating data from the origin
with the maximum margin, while proposes a classiﬁer
that ﬁnds the mimimum radius hypersphere enclosing data.
Despite the difference between these two approaches, 
proves that, for translation-invariant kernels such as the
Gaussian kernel, they obtain the same solutions. Extensions
of these two pioneering works, falling in the category of
kernel methods, are proposed few years later. modiﬁes
the model of by incorporating a small training set of
anomalies and using the centroid of such set as the reference
point to ﬁnd the hyperplane. proposes a strategy to automatically select the hyperparameters deﬁned in to increase the usability of the framework. Rather than repelling
samples from a speciﬁc point, as in , , suggests
a strategy to attract samples towards the centroid, solving
a linear programming problem that minimizes the average
output of the target function computed on the training
samples. Authors in propose a similar strategy based
on linear programming, where data are represented in a
similarity/dissimilarity space. The framework is well suited
for OCC applications involving strings, graphs or shapes.
Solutions different from kernel methods are also proposed.
To mention a few,
 proposes a neural network-based
approach, where the goal is to learn the identity function.
New samples are fed into the network and tested against
their corresponding outputs. The test sample is considered
as part of the class of interest only when the input and the
output of the network are similar.
 proposes a oneclass nearest neighbour, where a test sample is accepted as a
member of the target class only when the distance from its
neighbours is comparable to their local density. It is worth
noting that most of the works in OCC focus on increasing
classiﬁcation performance, rather than improving scalability. This is arguably motivated by the fact that it is usually
difﬁcult to collect large amounts of training samples for the
concept/class of interest. Solutions to improve classiﬁcation
performance rely on classical strategies such as ensemble
methods , bagging or boosting . Authors in 
argue that existing one-class classiﬁers fail when dealing
with mixture distributions. Accordingly, they propose a
multi-class classiﬁer exploiting the supervised information
of all classes of interest to reﬁne support estimation.
A promising solution to improve OCC consists of exploiting unlabeled data, which are in general largely avail-
1. More precisely, the term OCC was coined in 1996 .
able. As discussed in , standard OCC algorithms are not
designed to use unlabeled data, thus making the implicit
assumption that they are uniformly distributed over the
support of nominal distribution, which does not hold in
general. The recent work in proves that, under some
simple conditions,2 large amounts of unlabeled data can
boost OCC even compared to fully supervised approaches.
Furthermore, unlabeled data allow building OCC classiﬁers
in the context of open set recognition , where it is essential
to learn robust concepts/functions. Since the primary goal
of PU learning is to exploit this unsupervised information,
it can be regarded as a generalization of OCC , in the
sense that it can manage unlabeled data coming from more
general distributions than the uniform one.
Comparison with semi-supervised learning
The idea of exploiting unlabeled data in semi-supervised
learning was originally proposed in . Earlier studies
do not thoroughly explain why unlabeled data can be
beneﬁcial. Authors of are among the ﬁrst to analyze
this aspect from a generative perspective. In particular, they
assume that data are distributed according to a mixture of
Gaussians and show that the class posterior distribution
can be decomposed in two terms, depending one on the
class labels and the other on the mixture components. The
two terms can be estimated using labeled and unlabeled
data, respectively, thus improving the performance of the
learnt classiﬁer.
 extends this analysis assuming that
data can be correctly described by a more general class
of parametric models. The authors shows that if both the
class posterior distribution and the data prior distribution
are dependent on model parameters, unlabeled examples
can be exploited to learn a better set of parameters. Thus,
the key idea of semi-supervised learning is to exploit the
distributional information contained in unlabeled samples.
Many approaches have been proposed. The work in 
provides a taxonomy of semi-supervised learning algorithms. In particular, it is possible to distinguish ﬁve types
of approaches: generative approaches (see, e.g.,
 ), exploit unlabeled data to better estimate the class-conditional
densities and infer the unknown labels based on the learnt
model; low-density separation methods (see, e.g., , look
for decision boundaries that correctly classify labeled data
and are placed in regions with few unlabeled samples (the
so called low-density regions); graph-based methods (see,
e.g., ), exploit unlabeled data to build a similarity graph
and then propagate labels based on smoothness assumptions; dimensionality reduction methods (see, e.g.,
use unlabeled samples for representation learning and then
perform classiﬁcation on the learnt feature representation;
and disagreement-based methods (discussed in ), exploit
the disagreement among multiple base learners to learn
more robust ensemble classiﬁers.
The scalability issue is largely studied in the context of
semi-supervised learning. For example, the work in proposes a framework to solve a mixed-integer programming
problem, which runs multiple times the SVM algorithm.
State-of-art implementations of SVM (see, e.g., LIBSVM )
are based mainly on decomposition methods , like our
2. The conditions are based on class prior and size of positive (class
of interest) and negative (the rest) data.
proposed approach. Other semi-supervised approaches use
approximations of the ﬁtness function to simplify the optimization problem (see , ).
Both semi-supervised and PU learning exploit unlabeled
data to learn better classiﬁers. Nevertheless, substantial
differences hold that make semi-supervised learning not
applicable to PU learning tasks. An important aspect is
that most semi-supervised learning algorithms assume that
unlabeled data are originated from a set of known classes
(close set environment), thus not coping with the presence
of unknown classes in training and test sets (open set
environment). To the best of our knowledge, only few works
 ) propose semi-supervised methods able to
handle such situation. Another even more relevant aspect
is that semi-supervised learning cannot learn a classiﬁer
when only one known class is present, since it requires at
least two known classes to calculate the decision boundary.
On the contrary, recent works show that it is possible to
apply PU learning to solve semi-supervised learning tasks,
even in the case of open set environment , .
Theoretical studies about PU learning
Inspired by the seminal work and by the ﬁrst studies on
OCC , , the work in and its successive extension
 are the ﬁrst to deﬁne and theoretically analyze the problem of PU learning. In particular, they propose a framework
based on the statistical query model to theoretically assess the classiﬁcation performance and to derive algorithms
based on decision trees. The authors study the problem of
learning functions characterized by monotonic conjuctions,
which are particularly useful in text classiﬁcation, where
documents can be represented as binary vectors that model
the presence/absence of words from an available dictionary.
Instead of considering binary features,
 proposes a
Naive-Bayes classiﬁer for categorical features in noisy environments. The work assumes the attribute independence,
which eases the estimate of class-conditional densities in
high-dimensional spaces, but is limiting as compared to discriminative approaches, directly focusing on classiﬁcation
and not requiring density estimations .
As already mentioned at the beginning of this section,
PU learning studies can be roughly classiﬁed in two-stage
and single stage approaches. The former are based on
heuristics to select a set of reliable negative samples from
the unlabeled data and are not theoretically grounded,
while the latter are subject to the problem of wrong label
assignment. In order to understand how this issue is critical,
let consider the theoretical result of consistency presented
in .3 For any set of classiﬁers F of Vapnik-Chervonenkis
(VC) dimension V and any δ > 0, there exists a constant c
such that the following bounds hold with probability 1 −δ:
FNR(f) −FNR(f ∗) ≤cǫn,
FPR(f) −FPR(f ∗) ≤
1 −π (ǫn + ǫp),
where FPR, FNR are the false positive/negative rates,
f ∈F is the function obtained by the above-mentioned
strategy, f ∗∈F is the optimal function having access
to the ground truth, π is the positive class prior, ǫ· =
V log(·)−log(δ)
and p and n are the number of positive
and unlabeled samples, respectively. In particular, if one
considers a simple scenario where the feature space is R100
3. It is rewritten to be more consistent with the notation in this paper.
and V = 101 (in the case of linear classiﬁers), it is possible to
learn a classiﬁer such that, with probability of 90%, the performance does not deviate from the optimal values for more
than 10% (which is equivalent to setting δ, ǫp, ǫn = 0.1). This
is guaranteed when both positive and unlabeled sets consist
of at least 105 training samples each. This is impractical
in real world applications, since collecting and labelling
so many data is very expensive. The effect of wrong label
assignment is even more evident for larger values of positive
class prior.
 , proposed two frameworks based on
the statistical learning theory . These works are free from
heuristics, do not suffer the problem of wrong label assignment, and provide theoretical bounds on the generalization
error. Another theoretical work is the one in , which
speciﬁcally addresses the matrix completion problem, motivated by applications like recovering friendship relations in
social networks based on few observed connections. This
work however is unable to deal with the more general
problem of binary PU learning, and formulates the optimization problem using the squared loss, which is known
to be subobtimal according to the theoretical ﬁndings in ,
 .4 Overall, the analysis of the literature in the ﬁeld makes
evident the lack of theoretically-grounded PU learning
approaches with good scalability properties.
PU LEARNING FORMULATION
Assume that we are given a training dataset Db =
{(xi, yi) : xi ∈X, yi ∈Y }m
i=1, where X ⊆Rd, Y = {−1, 1}
and each pair of samples in Db is drawn independently from
the same unknown joint distribution P deﬁned over X and
Y . The goal is to learn a function f that maps the input space
X into the class set Y , known as the binary classiﬁcation
problem. According to statistical learning theory , the
function f can be learnt by minimizing the risk functional
ℓ(f(x), y)P(x, y)dx
ℓ(f(x), 1)P(x|y = 1)dx
ℓ(f(x), −1)P(x|y = −1)dx
where π is the positive class prior and ℓis a loss function
measuring the disagreement between the prediction and the
ground truth for sample x, viz. f(x) and y, respectively.
In PU learning, the training set is split into two parts: a
set of samples Dp = {xi ∈X}p
i=1 drawn from the positive
class and a set of unlabeled samples Dn = {xi ∈X}n
drawn from both positive and negative classes. The goal
is the same of the binary classiﬁcation problem, but this
time the supervised information is available only for one
class. The learning problem can be still formulated as a risk
minimization. In fact, since P(x) = πP(x|y = 1) + (1 −
π)P(x|y = −1), (1) can be rewritten in the following way:
ℓ(f(x), 1)P(x|y = 1)dx
ℓ(f(x), −1)P(x) −πP(x|y = 1)
˜ℓ(f(x), 1)P(x|y = 1)dx+
ℓ(f(x), −1)P(x)dx (2)
4. In fact, the best convex choice is the double Hinge loss, see
Section 3 for further details.
where ˜ℓ(f(x), 1) = ℓ(f(x), 1) −ℓ(f(x), −1) is called the
composite loss .
The risk functional in (2) cannot be minimized since the
distributions are unknown. In practice, one considers the
empirical risk functional in place of (2), where expectation
integrals are replaced with the empirical mean estimates
computed over the available training data, namely
Remp(f) = π
˜ℓ(f(xi), 1) + 1
ℓ(f(xi), −1)
The minimization of Remp is in general an ill-posed problem. A regularization term is usually added to Remp to
restrict the solution space and to penalize complex solutions.
The learning problem is then stated as an optimization task:
f ∗= arg min
Remp(f) + λ∥f∥2
where λ is a positive real parameter weighting the relative
importance of the regularizer with respect to the empirical
risk and ∥· ∥Hk is the norm associated with the function
space Hk. In this case, Hk refers to the Reproducing Kernel
Hilbert Space (RKHS) associated with its Mercer kernel k :
X ×X →R.5 We can then enunciate the representer theorem
for PU learning (proof in Supplementary Material):
Representer Theorem 1. Given the training set D = Dp ∪
Dn and the Mercer kernel k associated with the RKHS
Hk, any minimizer f ∗∈Hk of (4) admits the following
representation
αik(x, xi)
where αi ∈R for all i.
It is worth mentioning that many types of representer theorem have been proposed, but none of them can be applied
to the PU learning problem. Just to mention a few,
provides a generalized version of the classical representer
theorem for classiﬁcation and regression tasks, while 
derives the representer theorem for semi-supervised learning. Recently, proposed a uniﬁed view of existing representer theorems, identifying the relations between these
theorems and certain classes of regularization penalties.
Nevertheless, the proposed theory does not apply to (4) due
to the hypotheses made on the empirical risk functional.
This theorem shows that it is possible to learn functions
deﬁned on possibly-inﬁnite dimensional spaces, i.e., the
ones induced by the kernel k, but depending only on a ﬁnite
number of parameters αi. Thus, training focuses on learning such restricted set of parameters. Another important
aspect is that the representer theorem does not say anything
about the uniqueness of the solution, as it only says that
every minimum solution has the same parametric form.
In other words, different solutions have different values of
parameters. The uniqueness of the solution is guaranteed
only when the empirical risk functional in (4) is convex. A
proper selection of the loss function is then necessary to
fulﬁll this condition. Authors in analysed the properties
of loss functions for the PU learning problem, showing that
a necessary condition for convexity is that the composite
loss function in (3) is afﬁne. This requirement is satisﬁed by
some loss functions, such as the squared loss, the modiﬁed
Huber loss, the logistic loss, the perceptron loss, and the
double Hinge loss. The latter ensures the best generalization
5. For an overview of RKHS and their properties, see the work in 
performance .6 Even, the comparison with non-convex
loss functions , suggests to use the double Hinge loss
for the PU learning problem, with a twofold advantage:
ensuring that the obtained solution is globally optimal, and
allowing the use of convex optimization theory to perform
a more efﬁcient training.
These considerations, together with the result stated by
the representer theorem, allow us rewriting problem (4)
in an equivalent parametric form. In particular, deﬁning
α ∈R(p+n) as the vector of alpha values, ξ ∈Rn as the
vector of slack variables, K ∈R(p+n)×(p+n) as the Gram
matrix computed using the training set D, and considering,
without loss of generality, target functions in the form
i αik(x, xi) + β, where β is the bias of f, it
is possible to derive the following optimization problem
(derivation in Supplementary Material):
−c1˜1T Kα −c1˜1T 1β + c21T
s.t. ξ ⪰0n,
ξ ⪰UKα + β1n,
where ˜1 = [1, . . . , 1, 0, . . ., 0]T is a vector of size p + n
with p non-zero entries, 1 and 1n are unitary vectors of size
p + n and n, respectively, U is a n × (p + n) matrix obtained
through the concatenation of a n × p null matrix and an
identity matrix of size n, ⪰is an element-wise operator, c1 =
2λp and c2 =
The equivalent dual problem of (5) is more compactly
expressed as:
2σT UKUT σ −c1˜1T KUT σ −1
c1˜1 −UT σ
0n ⪯δ ⪯c21n,
where σ, δ ∈Rn and are related to the Lagrange multipliers
introduced during the derivation of the dual formulation
(see Supplementary Material for details).
Due to linearity of constraints in (5), Slater’s condition
is trivially satisﬁed7, thus strong duality holds. This means
that (6) can be solved in place of (5) to get the primal
solution. The optimal α can be obtained from one of the
stationarity conditions used during the Lagrangian formulation (details in Supplementary Material), namely using the
following relation
α = c1˜1 −UT σ
Note that the bias β has to be considered separately,
since problem (6) does not give any information on how
to compute it (this will be discussed in the next section).
It is to be pointed out that (6) is a quadratic programming
(QP) problem that can be solved by existing numerical QP
optimization libraries. Nevertheless, it is memory inefﬁcient, as it requires storing the Gram matrix, which scales
quadratically with the number of training samples. Thus,
6. Double Hinge loss can be considered as the equivalent of Hinge
loss function for the binary classiﬁcation problem.
7. See, e.g., 
modern computers cannot manage to solve (6) even for a
few thousands samples. A question therefore arises: is it
possible to efﬁciently ﬁnd an exact solution to problem (6)
without storing the whole Gram matrix?
USMO ALGORITHM
In order to avoid the storage problem, we propose an
iterative algorithm that converts problem (6) into a sequence
of smaller QP subproblems associated to subsets of training
samples (working sets), which require the computation and
temporary storage of much smaller Gram matrices.
Algorithm 1 General USMO algorithm
2: Initialize (σ1, δ1).
3: while (σk, δk) is not a stationary point of (6) do
Select the working set S ⊂U = {u : xu ∈Dn} with
Compute KSS, KSP and KS ¯S where P = {u : xu ∈
Dp} and ¯S = U\S.
s.t. 1T σk
S = c1p −1T σk
S −c1KSP 1p
K, ˜σk and ˜δk are permutations of K, σk and δk,
respectively. In general, KV W is used to denote a
matrix containing rows of K indexed by elements in
set V and columns of K indexed by elements in set
9: end while
Each iteration of USMO is made of three steps: selection
of the working set S, computation of the Gram matrix for
samples associated to indices in S, solution of a QP subproblem, where only terms depending on S are considered.
Details are provided in Algorithm 1. It is to be mentioned
that in principle this strategy allows decreasing the storage
requirement at the expense of a heavier computation. In
fact, the same samples may be selected multiple times over
iterations, thus requiring recomputing matrices KSS, KSP
and KS ¯S. We will see later how to deal with this inefﬁciency.
Another important aspect is that at each iteration only few
parameters are updated (those indexed by the working set
S, namely σk
S), while the others are kept ﬁxed.
Here, we consider a working set of size two, as this allows
solving the QP subproblem (8) analytically, without the
need for further optimization. This is discussed in the next
subsection.
Equations and Conditions Used to Solve the Four QP Subproblems.
j =(ak(k(xi, xi) −k(xi, xj)) + e1 −e2)η
j =(ak(k(xi, xi) −k(xi, xj)) + e1 −e2 + 2)/η
j =(ak(k(xi, xi) −k(xi, xj)) + e1 −e2 −2)/η
j =(ak(k(xi, xi) −k(xi, xj)) + e1 −e2)/η
Conditions
max{c2/2, ak−c2}≤σk
j ≤min{c2, ak−c2/2}
max{0, ak−c2}≤σk
j ≤min{c2/2, ak−c2/2}
max{c2/2, ak−c2/2}≤σk
j ≤min{c2, ak}
max{0, ak−c2/2}≤σk
j ≤min{c2/2, ak}
ak = c1p −1T σk
S, e = [e1, e2]T and
η = k(xi, xi) + k(xj, xj) −2k(xi, xj).
QP Subproblem
We start by considering the following Lemma (proof in
Supplementary Material):
Lemma 1. Given S = {i, j}, any optimal solution σ∗
j ]T of the QP subproblem (8) has to
satisfy the following condition: ∀u : xu ∈S ∧0 ≤δ∗
c2 either σ∗
u = c2 −δ∗
This tells us that the optimal solution (σ∗
S) assumes a
speciﬁc form and can be calculated by searching in a smaller
space. In particular, four subspaces can be identiﬁed for
Case 1: σk
i = c2 −δk
j = c2 −δk
Case 2: σk
i = c2 −δk
Case 3: σk
j = c2 −δk
Case 4: σk
Then, in order to solve the QP subproblem (8), one can
solve four optimization subproblems, where the objective
function is the same as (8), but the inequality constraints
of (8) are simpliﬁed to (9). Each of these subproblems can
be expressed as an optimization of just one variable, by
exploiting the equality constraints of both (8) and (9). It
can therefore be solved analytically, without the need for
further optimization. Table 1 reports the equations used to
solve the four subproblems (we omit the derivation, which
is straightforward), where σk
j is computed for all four cases.
All other variables, namely σk
j , can be obtained in
a second phase by simply exploiting the equality constraints
in (8) and (9).
These equations do not guarantee that the inequalities
in (9) are satisﬁed. To verify this, one can rewrite these inequalities as equivalent conditions of only σk
j (by exploiting
the equality constraints in (8) and (9)), and check σk
them, as soon as all σk
j are available. If these conditions
are violated, a proper clipping is applied to σk
j to restore
feasibility. Table 1 summarizes the checking conditions.
Finally, the minimizer of the QP subproblem (8) can be
obtained by retaining only the solution with the lowest level
of objective. At each iteration, the output of the algorithm
is both optimal and feasible for the QP subproblem (8). The
question now is: when is it also optimal for the problem (6)?
Optimality Conditions
A problem of any optimization algorithm is to determine
the stop condition. In Algorithm 1, the search of the solution
is stopped as soon as some stationarity conditions are met.
These conditions, called Karush-Kuhn-Tucker (KKT) conditions, represent the certiﬁcates of optimality for the obtained
solution. In case of (6) they are both necessary and sufﬁcient
conditions, since the objective is convex and the constraints
are afﬁne functions . More in detail, an optimal solution
has to satisfy the following relations:
−β = −λu + µu,
2 −ξu + ηu,
λu, µu, ξu, ηu ≥0,
and this is valid for any component of the optimal solution,
namely ∀u : xu ∈Dn. In (10) F(σ, δ) is used as an abbreviation of the objective function of (6), while β, λu, µu, ξu, ηu
are the Lagrange multipliers introduced to deal with the
constraints in (6). These conditions can be rewritten more
compactly as:
0 ≤δu<c2 ∧σu=δu
⇒f(xu) ≤−1,
0 ≤δu<c2 ∧σu=c2−δu
⇒f(xu) ≥1,
δu=c2 ∧σu=c2
⇒−1 ≤∂F(σ, δ)
⇒−1 ≤f(xu) ≤1,
In order to derive both (10) and (11), one can follow a
strategy similar to the proof of Lemma 1. It is easy to
verify that
= −f(xu) + β. Thus, (11) provides
also conditions on the target function f. From now on,
we will refer to (11) as the optimality conditions, to distinguish from approximate conditions used to deal with
numerical approximations of calculators. To this aim, the
τ−optimality conditions are introduced, namely:
0 ≤δu<c2 ∧σu=δu
⇒f(xu) ≤−1 + τ
0 ≤δu<c2 ∧σu=c2−δu
−β ≤−1 + τ
⇒f(xu) ≥1 −τ
δu=c2 ∧σu=c2
2 ≤∂F(σ, δ)
2 ≤f(xu) ≤1+τ
where τ is a real-positive scalar used to perturb the optimality conditions.
By introducing the sets D1(σ, δ) = 
xu ∈Dn : 0 ≤δu <
c2 ∧σu = δu
, D2(σ, δ) =
xu ∈Dn : 0 ≤δu < c2 ∧σu = c2 −
and D3(σ, δ) = 
xu ∈Dn : 0 < δu ≤c2 ∧ σu = δu
 and the quantities mmax
(σ, δ) = maxxu∈D1 f(xu),
(σ, δ) = minxu∈D2 f(xu), mmin
(σ, δ) = minxu∈D3 f(xU)
(σ, δ) = maxxu∈D3 f(xu), called the most critical
values, it is possible to rewrite conditions (12) in the following equivalent way:
(σ, δ) −mmin
(σ, δ) ≤τ,
(σ, δ) −mmin
(σ, δ) ≤τ,
(σ, δ) −mmin
(σ, δ) + 2 ≤τ,
Apart from being written more compactly than (12), conditions (13) have the advantage that they can be computed
without knowing the bias β. Due to the dependence on σ
and δ, mmax
need to be tracked and
computed at each iteration in order to check τ−optimality
and to decide whether to stop the algorithm.
Working Set Selection
A natural choice for selecting the working set is to look
for pairs violating the τ−optimality conditions. In particular,
Deﬁnition 1. Any pair (xi, xj) from Dn is a violating pair,
if and only if it satisﬁes the following relations:
xi ∈D1, xj ∈D3 ⇒f(xi) −f(xj) > τ,
xi ∈D3, xj ∈D1 ⇒f(xi) −f(xj) < −τ,
xi ∈D2, xj ∈D3 ⇒f(xi) −f(xj) < −τ,
xi ∈D3, xj ∈D2 ⇒f(xi) −f(xj) > τ,
xi ∈D1, xj ∈D2 ⇒f(xi) −f(xj) + 2 > τ,
xi ∈D2, xj ∈D1 ⇒f(xi) −f(xj) −2 < −τ,
Conditions (13) are not satisﬁed as long as violating pairs are
found. Therefore, the algorithm keeps looking for violating
pairs and use them to improve the objective function until
τ−optimality is reached.
The search of violating pairs as well as the computation
of the most critical values go hand in hand in the optimization and follow two different approaches. The former
consists of ﬁnding violating pairs and computing the most
critical values based only on a subset of samples called the
non-bound set, namely D−
n = (D1 ∩D3) ∪(D2 ∩D3),8
while the latter consists of looking for violating pairs based
on the whole set Dn by scanning all samples one by one.
In this second approach, the most critical values are updated using the non-bound set together with the current
examined sample. Only when all samples are examined, it
is possible to check conditions (13), since the most critical
values correspond to the original deﬁnition. The algorithm
keeps using the ﬁrst approach until τ−optimality for the
non-bound set is reached, after that the second approach is
used. This process is repeated until the τ−optimality for the
whole set Dn is achieved. On the one hand, checking these
conditions only on the non-bound set is very efﬁcient but
does not ensure the global τ optimality; on the other hand,
the use of the whole unlabeled set is much more expensive,
while ensuring the global τ optimality
The motivation of having two different approaches for
selecting the violating pairs and computing the most critical
values is to enhance efﬁciency in computation. This will be
clariﬁed in the next subsection.
Function Cache and Bias Update
Recall that each iteration of the USMO algorithm is
composed by three main operations, namely: the working
8. The term non-bound comes from the fact that 0 < δu < c2 for all
set selection, the resolution of the associated QP subproblem, and the update of the most critical values based
on the obtained solution. It is interesting to note that all
these operations require to compute the target function
f(xu) for all xu belonging either to the non-bound set
or to the whole set Dn, depending on the approach selected by that iteration. In fact, the stage of working set
selection requires to evaluate conditions in (14) for pairs
of samples depending on f; the equations used to solve
the QP subproblem, shown in Table 1, depend on vector
e = KS ¯Sσk¯S −c1KSP 1p + KSSσk−1
−[f(xi) −β, f(xj) −β]T −KSSσk−1
, which is also in-
ﬂuenced by f; ﬁnally, the computation of the most critical
values requires to evaluate the target function f. Therefore,
it is necessary to deﬁne a strategy that limits the number of
times the target function is evaluated at each iteration. This
can be achieved by considering the fact that the algorithm
performs most of the iterations on samples in the nonbound set, while the whole set is used mainly to check if
τ−optimality is reached, and then the values of the target
function for those samples can be stored in a cache, called
the function cache. Since usually |D−
n | ≪|Dn|, storing
f(xu) for all xu ∈D−
n is a cheap operation, which allows
to save a huge amount of computation, thus increasing the
computational efﬁciency.
At each iteration the function cache has to be updated in
order to take into account the changes occured at some of
the entries of vectors σ and δ, or equivalently at some of the
entries of α . By deﬁning Fk(xu) as the function cache for
sample xu at iteration k, it is possible to perform the update
operation using the following relation:
Fk(xu) =Fk−1(xu) + (αk
)k(xi, xu)
)k(xj, xu)
Since all operations at each iteration are invariant with
respect to β (because they require to evaluate differences
between target function values), β can be computed at the
end of the algorithm, namely when τ−optimality is reached.
By exploiting the fact that the inequalities in (11) become
simply equalities for samples in the non-bound set, meaning
that the target function evaluated at those samples can
assume only two values, 1 or -1,9 it is possible to compute β
for each of these samples in the following way:
n −1 −F(xu),
∀xu ∈D1 ∩D3
∀xu ∈D2 ∩D3
The ﬁnal β can be computed by averaging of (16) over all
samples in the non-bound set, in order to reduce the effect
of wrong label assignment.
Initialization
As previously mentioned, the proposed algorithm is
characterized by iterations focusing either on the whole
training data set or on a smaller set of non-bound samples.
The formers are computationally more expensive, not only
because a larger amount of samples is involved in the
optimization, but also because the algorithm has to perform
many evaluation operations, which can be skipped in the
latter case, thanks to the exploitation of the function cache.
An example of this is provided in Figure 1: the chart on the
left plots the time required by the algorithm to complete
the corresponding set of iterations. Peaks correspond to
9. The same principle holds for conditions (12), but in this case the
inequalities are deﬁned over arbitrary small intervals centered at 1 and
-1 rather than being equalities.
Iterations
Time (secs)
Iterations
Objective function
Fig. 1. (a) plot of training time over iterations, (b) learning curve expressed in terms of objective function.
cases where the whole training data set is considered, while
valleys represent the cases where iterations are performed
over the non-bound samples. The chart on the right plots
the overall objective score vs. the different sets of iterations.
Jumps are associated with cases where all training samples
are involved in the optimization. As soon as the algorithm
approaches convergence, the contribution of the ﬁrst kind
of iterations becomes less and less relevant. Therefore, the
selection of a good starting point is important to limit the
number of iterations requested over the whole unlabeled
dataset. Given the convex nature of the problem, any suboptimal solution achievable with a low complexity can be used
as a starting point. In our method, we propose the following
heuristic procedure. Labeled samples are used to train a oneclass SVM , that is in turn used to rank the unlabeled
samples according to their value of estimated target function. From this ordered list it is possible to identify groups
of samples that can be associated with the cases in (11). In
particular, we identify ﬁve groups of samples corresponding
to the following ﬁve cases:
< c2 ∧σu = δ(2)
= c2 ∧σ(3)
< c2 ∧σ(4)
The size of each group as well as the initial parameters for
cases in (17) can be computed by solving an optimization
problem, whose objective function is deﬁned starting from
the equality constraint in (6). In particular, by deﬁning
n1, n2, n3, n4, n5 as the sizes of the groups for the different
cases and by assuming that n1 = (1 −π)an, n2 = bn,
n3 = (1−a−b−c)n, n4 = cn, n5 = πan, where a, b, c ∈R+,
and that the parameters for the second and the fourth cases
in (17), namely σ(2)
u , are the same, the optimization
problem can be formulated in the following way:
c1p −bnσ(2)
πa + 1 −a −b −c
s.t. 0 ≤a + b + c ≤1 −1
n ≤b, c ≤log(n)
where the constraints in (18) can be obtained by imposing
n1 + n2 + n3 + n4 + n5 = n and 1 ≤n2, n3, n4, n5 ≤n.
σj = ak −σi
Fig. 2. Subdivision of the feasible region in the plane deﬁned by the
variables σi and σj. The red solid line represents the feasible region
including the equality constraint in (8).
Furthermore, we decide to have some upper bounds for b
and c to limit the size of the initial non-bound set.
In practice, after ranking the unlabeled samples through
the one-class SVM and solving the optimization problem
in (18), the initial solution is obtained by assigning to each
sample the value of parameters corresponding to the case
that sample belongs to. For example, if the samples are
ranked in ascending order, then the ﬁrst n1 samples in the
list have σu = 0 and δu = 0, the next n2 samples have
and δu = 2σu and the others follow the same
THEORETICAL ANALYSIS
In this section, we present the main theoretical result, namely, we prove that Algorithm 1 converges to a
τ−optimal solution.
It is important to recall that each iteration of USMO
requires to solve an optimization subproblem, that depends
on a single variable. In particular, if xi and xj correspond to
the selected pair of points at one iteration, then the solution
space corresponds to a line lying in the two-dimensional
plane deﬁned by the variables σi and σj. The feasible region
in that plane can be subdivided into four parts, as deﬁned
according to Figure 2. These regions are considered closed
sets, therefore including boundary points, like edges and
corners. To consider only the interior of any set U, we
use the notation int U. Based on these considerations, it is
possible to prove the following lemma.
Lemma 2. Let the vector z′ = [σ′; δ′] be in the feasible set
of (6) and (xi, xj) be a violating pair at point z′. Let also
z∗= [σ∗; δ∗] be the solution obtained after applying one
iteration of the Algorithm 1 using the working set S =
{i, j} and starting from z′. Then, the following results
After the minimization step, (xi, xj) is no more a
violating pair,
j ) ∈int R1 ∪int R3 ⇒fz∗(xj)−fz∗(xi) = 0,
j ) ∈int R2 ⇒fz∗(xj)−fz∗(xi) = 2,
j ) ∈int R4 ⇒fz∗(xj)−fz∗(xi) = −2,
j ) ∈BE ⇒0 ≤fz∗(xj)−fz∗(xi) ≤2,
j ) ∈DE ⇒0 ≤fz∗(xj)−fz∗(xi) ≤2,
j ) ∈EF ⇒−2 ≤fz∗(xj)−fz∗(xi) ≤0,
j ) ∈EH ⇒−2 ≤fz∗(xj)−fz∗(xi) ≤0,
j ) ∈AB ∪DI ⇒fz∗(xj)−fz∗(xi) ≥0,
j ) ∈AF ∪HI ⇒fz∗(xj)−fz∗(xi) ≤0,
j ) ∈BC ∪CD ⇒fz∗(xj)−fz∗(xi) ≥2,
j ) ∈FG ∪GH ⇒fz∗(xj)−fz∗(xi) ≤−2,
F(σ′, δ′) −F(σ∗, δ∗) >
2∥σ′ −σ∗∥2,
where fz∗represents the target function with coefﬁcients αi
computed according to (7) using z∗(see Figure 2).
Proof: Note that the feasible region for the QP subproblem (8) is a portion of line with negative slope lying
on the plane deﬁned by variables σi and σj (see Figure 2).
Thus, any point (σi, σj) on this line can be expressed using
the following relationship:
where t ∈R. In particular, if t = 0, then (σi, σj) ≡(σ′
and, if t = t∗, then (σi, σj) ≡(σ∗
Considering (19) and the fact that δxi=2σi ∧δj=2σj
when (σi, σj) ∈R1, δxi=2σi ∧δj=2c2−2σj when (σi, σj) ∈
R2, δxi=c2 −2σi ∧δj=c2 −2σj when (σi, σj) ∈R3 and
δxi=c2 −2σi ∧δj=2σj when (σi, σj) ∈R4, it is possible to
rewrite the objective in (8) as a function of t, namely:
i + t)2k(xi, xi) + 1
j −t)2k(xj, xj)+
j −t)k(xi, xj) + hz(t)
where hz is a function deﬁned in the following way:
i+t)+(e2−1)(σ′
(σi,σj)∈R1,
i+t)+(e2+1)(σ′
j−t)−c2, (σi,σj)∈R2,
i+t)+(e2+1)(σ′
j−t)−2c2,(σi,σj)∈R3,
i+t)+(e2−1)(σ′
j−t)−c2, (σi,σj)∈R4,
Note that d2φ(t)
= k(xi, xi) + k(xj, xj) −2k(xi, xj) ≥0
(k is a Mercer kernel), meaning that (20) is convex.
j ) ∈int R1, then (σ∗
j ) is the minimum and
= 0. Since dφ(t∗)
= fz∗(xj) −fz∗(xi) = 0, the ﬁrst
and the second conditions in (14), which are the only possibilities to have a violating pair, are not satisﬁed. Therefore,
(xi, xj) is not violating at point z∗, but it is violating at
z′, implying that z∗̸= z′. The same situation holds for
j ) ∈int R3 and this proves statement (c).10 Statements
(d) and (e) can be proven in the same way, considering that
the admissible conditions to have a violating pair are the
ﬁrst, the fourth and the ﬁfth conditions for the former case
and the second, the third and the sixth ones for the latter
j ) ∈BE, there are two possibilities to compute the derivative depending on the position of (σ′
namely approaching (σ∗
j ) from the bottom or from the
top of the constraint line. In the ﬁrst case, the derivative
is identiﬁed by dφ(t∗)
dt−, while in the second case by dφ(t∗)
j ) is the minimum and due to the convexity
of function φ(t), dφ(t∗)
≥0 and dφ(t∗)
≤0. Furthermore,
it is easy to verify that
= fz∗(xj)−fz∗(xi) and
= fz∗(xj)−fz∗(xi) −2. By combining these results,
we obtain that 0 ≤fz∗(xj)−fz∗(xi) ≤2. This, compared
with the ﬁrst condition in (14), guarantees that (xi, xj) is
not a violating pair at z∗and therefore that z∗̸= z′. The
same strategy can be applied to derive statements (g)-(o).
For the sake of notation compactness, we use φ′(t) to
identify both the classical and the directional derivatives of
φ(t), viz. dφ(t)
dt , dφ(t∗)
and dφ(t∗)
dt+ , respectively. Therefore, it
10. In this case, the admissible conditions for violation are the second
and the third conditions in (14).
is possible to show that φ(t) = φ(0) + φ′(0)t + φ′′(0)
Furthermore, due to the convexity of φ(t), we have that
φ′(0) < 0 ⇒tq ≥t∗> 0,
φ′(0) > 0 ⇒tq ≤t∗< 0,
where tq = −φ′(0)
φ′′(0) is the unconstrained minimum of φ(t).
From all these considerations, we can derive the following
φ(t∗) ≤φ(0) + φ′(0)
In fact, if φ′′(0) = 0, then (22) trivially holds. If φ′′(0) > 0,
φ(t∗) −φ(0) = φ′(0)
where the last inequality of (23) is valid because
1, by simply applying (21).
Note also that (19) can be used to derive the following
result, namely:
∥σ′ −σ∗∥2 = |t∗|
By combining (23) and (24) and considering that conditions (14) can be compactly rewritten as |φ′(0)| > τ, we
obtain that
φ(0) −φ(t∗) ≥−φ′(0)
t∗= |φ′(0)|
∥σ′ −σ∗∥2,
Finally, statement (p) is obtained from (25), by taking into
account that φ(t∗) = F(σ∗, δ∗) and φ(0) = F(σ′, δ′).
Lemma 2 states that each iteration of Algorithm 1 generates a solution that is τ−optimal for the indices in the
working set S.
The convergence of USMO to a τ−optimal solution can
be proven by contradiction by assuming that the algorithm
proceeds indeﬁnitely. This is equivalent to assume that
(xik, xjk) is violating ∀k ≥0, where (ik, jk) represents the
pair of indices selected at iteration k.
Since {F(σk, δk)} is a decreasing sequence (due to
the fact that zk ̸= zk+1 ∀k ≥011 and that the algorithm minimizes the objective function at each iteration)
and bounded below (due to the existence of an unknown
global optimum), it is convergent. By exploiting this fact
and by considering that
τ [F(σk, δk)−F(σk+l, δk+l)]
∥σk−σk+l∥2, ∀k, l ≥0, which can be obtained from (p) of
Lemma 2 by applying l times the triangle inequality, it
is possible to conclude that {σk} is a Cauchy sequence.
Therefore, since the sequence lies also in a closed feasible set,
it is convergent. In other words, we have that σk →¯σ for
k →∞, meaning that Algorithm 1 produces a convergent
sequence of points. Now, it is important to understand if
this sequence converges to a τ−optimal solution.
First of all, let us deﬁne the set of indices that are encountered/selected by the algorithm inﬁnitely many times:
I∞= {(µ, ν) : ∃{kt} ⊂{k}, (ikt, jkt) = (µ, ν), ∀t ∈N}
{kt} is therefore a subsequence of {k}. It is also important
to mention that since the number of iterations is inﬁnite and
the number of samples is ﬁnite, I∞cannot be an empty set.
Based on this consideration, we deﬁne vµν as the vector,
11. Statement (a) of Lemma 2.
Fig. 3. Example of transitions performed by a minimization step of
Algorithm 1 for different locations of ¯σµ,ν (highlighted by blue points)
and for sufﬁciently large number of iterations.
whose elements are the entries at position µ and ν of a
general vector v, and provide the following lemma.
Lemma 3. Assume (µ, ν) ∈I∞and let {kt} be the sequence
of indices for which (ikt, jkt) = (µ, ν). Then,
∀ǫ > 0, ∃ˆt > 0: ∀t ≥ˆt, ∥σkt
µν−¯σµν∥< ǫ and ∥σkt+1
fσkt (xµ)−fσkt (xν) > τ ⇒f¯σ(xµ)−f¯σ(xν) ≥τ
fσkt (xµ)−fσkt (xν) < −τ ⇒f¯σ(xµ)−f¯σ(xν) ≤−τ
fσkt (xµ)−fσkt (xν) > τ−2 ⇒f¯σ(xµ)−f¯σ(xν) ≥τ−2
fσkt (xµ)−fσkt (xν)<−τ+2 ⇒f¯σ(xµ)−f¯σ(xν)≤−τ+2
where fσkt , f¯σ represent the target function with coefﬁcients αi computed according to (7) using σkt and ¯σ,
respectively.
Proof: Since {σk} is convergent and {kt}, {kt +1} are
subsequences of {k}, {σkt} and {σkt+1} are also convergent sequences. In other words, ∃ˆt>0 such that ∥σkt −¯σ∥<ǫ
and ∥σkt+1 −¯σ∥<ǫ. Furthemore, ∥σkt −¯σ∥≥∥σkt
and ∥σkt+1 −¯σ∥≥∥σkt+1
−¯σµν∥. By combining these two
results, we obtain statement (a).
Concerning
fσkt (xµ)−fσkt (xν)>τ.
Furthermore,
convergence
of {σkt} and continuity of f, we obtain that ∀ǫ>0, ∃˜t≥ˆt:
∀t≥˜t, −ǫ≤fσkt (xµ)−f¯σ(xµ)≤ǫ and −ǫ≤fσkt (xν)−f¯σ(xν)≤ǫ,
{fσkt(xµ)}
{fσkt (xν)}
convergent.
Therefore,
fσkt (xµ)−fσkt (xν)>τ
rewritten as
fσkt (xµ)−fσkt (xν)+f¯σ(xµ)−f¯σ(xµ)+f¯σ(xν)−f¯σ(xν)>τ
and by applying the information about the convergence of
both {fσkt (xµ)} and {fσkt (xν)}, we get that
f¯σ(xµ) −f¯σ(xν) > τ −2ǫ
which is valid ∀ǫ > 0 and therefore proves statement (b).
All other statements, namely (c)-(e), can be proven using the
same approach.
Lemma 3 states some conditions about the ﬁnal target function and also states that the sequence output by Algorithm 1,
after a sufﬁciently large number of iterations, is enclosed in
a ball centered at ¯σ. This aspect is shown in Figure 3 for
R1 and for different possible locations of ¯σµ,ν. The same
picture shows also the possible transitions that may happen
at each iteration. In particular, we see that for ¯σµ,ν lying
on corners and edges, different kinds of transitions exist. In
fact, we ﬁnd transitions from border to border, transitions
from border to inner points and viceversa, and transitions
from inner points to inner points. These are indetiﬁed as
bd →bd, bd →int, int →bd and int →int, respectively.
Note that for ¯σµ,ν not lying on borders, int →int is the only
available kind of transition. Based on these considerations,
it is possible to prove the following lemma.
Lemma 4. Let (µ, ν), {kt}, ˆt and ǫ be deﬁned according to
Lemma 3. Then, ∃¯t ≥ˆt such that ∀t ≥¯t and for sequence
{kt} the only allowed transitions are int →bd and bd →
Proof: Consider region R1 and (¯σµ, ¯σν) ∈int R1.
Then, the only admissible type of transitions for this case is
int→int. Therefore, based on statement (c) of Lemma 2 (and
thanks also to statement (a) of Lemma 3), we obtain that
∀t≥¯t, fσkt+1(xµ)−fσkt+1(xν)=0. By exploiting this fact,
the continuity of f and the convergence of {σkt+1}, it is
possible to show that
f¯σ(xµ)−f¯σ(xν) = 0
Furthermore, since (xµ, xν) is a violating pair at all iterations and ∀t≥¯t, σkt
µν ∈int R1 (due to statement (a) of
Lemma 3), (xµ, xν) has to satisfy conditions (b) or (c) of
Lemma 3. These conditions are in contradiction with (27),
meaning that the int→int transition is not allowed in this
Consider now region R1 and (¯σµ, ¯σν) ∈E, or equivalently (¯σµ, ¯σν) ∈A. This time, the potential transitions
are bd →bd, bd →int, int →bd and int →int.
Nevertheless, it is always possible to deﬁne a subsequence
containing only either int →int or bd →int and obtain
therefore conclusions similar to the previous case. In fact,
both int→int and bd →int are not allowed transitions.
The same results can be obtained in a similar way for
other edges, corners of R1 as well as for points in R3, upon
selection of the proper conditions in Lemma 2.
Consider now region R2 and (¯σµ, ¯σν) ∈int R2. The
only admissible transition in this case is int
From statement (d) of Lemma 2, we have that ∀t≥¯t,
fσkt+1(xµ)−fσkt+1(xν)= −2 and, from the continuity of
f and the convergence of {σkt+1}, it is possible to show
f¯σ(xµ)−f¯σ(xν) = −2
Furthermore, since (xµ, xν) is a violating pair at all iterations and ∀t≥¯t, σkt
µν ∈int R2, (xµ, xν) has to satisfy
conditions (b) or (d) of Lemma 3. These conditions are in
contradiction with (28), meaning that the int→int transition
is not valid.
For all corners and edges of R2, as well as for all points in
R4, it is possible to show that int→int and bd →int are not
valid transitions. The proof is similar to the previous cases.
Therefore, the only admissible transitions after a sufﬁciently
large number of iterations are int →bd and bd →bd.
It is interesting to note that each transition int →bd increases the number of components of σ belonging to borders
of the four regions, by one or two, while each transition
bd →bd leaves it unchanged. Since this number is bounded
by n, transition int →bd cannot appear inﬁnitely many
times. Therefore, ∃t∗≥¯t, ∀t ≥t∗, bd →bd is the only valid
transition.
Note that bd →bd may happen only when (¯σµ, ¯σν) is located at some speciﬁc corners of the feasible region, namely
corners A or E for region R1, corners B or C for region R2,
corners E or I for region R3 and corners F or H for region
R4. For all cases, it is possible to deﬁne a subsequence that
goes only from a vertical to a horizontal border and a subsequence that goes only from a horizontal to a vertical border.
Without loss of generality, we can consider a speciﬁc case,
namely (¯σµ, ¯σν) ∈A. Note that for the ﬁrst subsequence,
fσkt+1(xµ)−fσkt+1(xν)< −τ, since (xµ, xν) has to be a violating pair in order not to stop the iterations and therefore,
from statement (c) of Lemma 3, f¯σ(xµ)−f¯σ(xν)< −τ. For
the second subsequence, fσkt+1(xµ)−fσkt+1(xν)>τ and consequently f¯σ(xµ)−f¯σ(xν)>τ. This leads to a contradiction
which holds ∀(µ, ν) ∈I∞. Therefore, the assumption that
Algorithm 1 proceeds indeﬁnitely is not veriﬁed. In other
words, there exists an iteration at which the algorithm stops
because a τ−optimal solution is obtained.
EXPERIMENTAL RESULTS
Characteristics of data sets.
# Instances
# Features
Australian
Heart-statlog
House-votes
Ionosphere
Liverdisorders
Bank-marketing
Statlog (shuttle)
Poker-hand
In this section, comprehensive evaluations are presented
to demonstrate the effectiveness of the proposed approach.
USMO is compared with and . The three methods have
been implemented in MATLAB, to ensure fair comparison.
12 The method in solves problem (6) using the MATLAB
built-in function quadprog, combined with the second-order
primal-dual interior point algorithm , while the method
in solves problem (4) with the ramp loss function using
the quadprog function combined with the concave-convex
procedure . Experiments were run on a PC with 16
2.40 GHz cores and 64GB RAM.
A collection of 17 realworld datasets from the UCI repository was used, 12 of
which contain few hundreds/thousands of samples, while
the remaining 5 are signiﬁcantly bigger. Table 2 shows some
of their statistics.
Since USMO and solve the same optimization problem, we ﬁrst verify that both achieve the same solution. We
consider the F-measure in a transductive setting, to assess
the generalization performance on all small-scale datasets
and under different conﬁgurations of hyper-parameters and
kernel functions. In particular, we consider different values
of λ, viz. 0.0001, 0.001, 0.01, 0.1, using linear and Gaussian kernels. 13 In these experiments, only 20% of positive
samples are labeled. Tables 3-4 show the results for linear
and Gaussian kernels, respectively. Both algorithms achieve
almost identical performance, with small differences due
to numerical approximations. This fact conﬁrms the theory
12. Code available at 
13. The positive class prior π is set to the class proportion in the
training data sets. Methods like , , can be used to estimate
Comparative results (F-measure) on different small-scale datasets and on different values of hyperparameters using the linear kernel. 20% of
positive examples are labeled, while the remaining are unlabeled.
λ = 0.0001
Australian
Heart-statlog
House-votes
Ionosphere
Liverdisorders
* Results obtained using only our proposed initialization
Comparative results (F-measure) on different small-scale datasets and on different values of hyperparameters using the Gaussian kernel (scale
parameter equal to 1). 20% of positive examples are labeled, while the remaining are unlabeled.
λ = 0.0001
Australian
Heart-statlog
House-votes
Ionosphere
Liverdisorders
* Results obtained using only our proposed initialization
proven in Section 5, according to which USMO is guaranteed to converge to the same value of objective function
obtained by . Note that ramp loss never achieves the
best average performance. Furthermore, it is inﬂuenced by
the starting point due to a non-convex objective function,
thus making double Hinge loss preferable in practical applications.
Secondly, we investigate the complexity of USMO with
respect to , . As to the storage requirements, USMO
behaves linearly instead of quadratically as
 , . Concerning the computational complexity, it can be easily found
that each iteration has, in the worst case (i.e., an iteration
over the whole unlabeled dataset), a complexity O(|Dn|).
As to the number of iterations, it is difﬁcult to determine
a theoretical limit and it has been experimentally observed
over a large and variate set of tests that it is possible to
establish a linear upper bound with very low slope (less
than 40 iterations for 6000 samples). Therefore, we can state
that a quadratic dependence represents a very conservative
upper limit for the complexity of USMO. In particular,
we measured the processing time of all methods for an
increasing number of unlabeled samples and with different
kernel functions. Figures 4 and 5 show elapsed time and
generalization performance with the linear kernel, while
Figures 6 and 7 show the results achieved with a Gaussian
kernel. In most cases, and especially for linear kernel, USMO
outperforms all competitors. For Gaussian kernel and few
unlabeled samples USMO may require higher computation
than ramp loss , however, its performance consistently
increases with the number of unlabeled samples and its
lower storage requirements allow using it also when other
methods run out of memory (see results for MNIST in
Figure 6).
CONCLUSIONS
In this work an efﬁcient algorithm for PU learning is proposed. Theoretical analysis is provided to ensure that the
obtained solution recovers the optimum of the objective
function. Experimental validation conﬁrms that the proposed solution can be applied to real-world problems.
ACKNOWLEDGMENTS
This research was partially supported by NSFC (61333014)
and the Collaborative Innovation Center of Novel Software Technology and Industrialization. Part of this research
was conducted when E. Sansone was visiting the LAMDA
Group, Nanjing University. We gratefully acknowledge the
support of NVIDIA Corporation with the donation of a Titan
X Pascal machine to support this research.