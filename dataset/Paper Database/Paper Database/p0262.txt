Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 479–489,
Austin, Texas, November 1-5, 2016. c⃝2016 Association for Computational Linguistics
How Transferable are Neural Networks in NLP Applications?
Lili Mou,1 Zhao Meng,1 Rui Yan,2 Ge Li,1,† Yan Xu,1,∗Lu Zhang,1 Zhi Jin1,†
1Key Laboratory of High Conﬁdence Software Technologies (Peking University), MoE, China
Institute of Software, Peking University, China
†Corresponding authors
2Insitute of Computer Science and Technology of Peking University, China
{doublepower.mou,rui.yan.peking}@gmail.com, 
{lige,xuyan14,zhanglu,zhijin}@sei.pku.edu.cn
Transfer learning is aimed to make use of
valuable knowledge in a source domain to
help model performance in a target domain.
It is particularly important to neural networks,
which are very likely to be overﬁtting.
some ﬁelds like image processing, many studies have shown the effectiveness of neural
network-based transfer learning.
For neural
NLP, however, existing studies have only casually applied transfer learning, and conclusions are inconsistent. In this paper, we conduct systematic case studies and provide an
illuminating picture on the transferability of
neural networks in NLP.1
Introduction
Transfer learning, or sometimes known as domain
adaptation,2 plays an important role in various natural language processing (NLP) applications, especially when we do not have large enough datasets
for the task of interest (called the target task T ). In
such scenarios, we would like to transfer or adapt
knowledge from other domains (called the source
domains/tasks S) so as to mitigate the problem of
overﬁtting and to improve model performance in
T . For traditional feature-rich or kernel-based models, researchers have developed a variety of elegant methods for domain adaptation; examples include EasyAdapt , instance weighting , and structural correspondence
learning 
parameters from a source task to initialize the network in the target task; alternatively, we may also
train two tasks simultaneously with some parameters shared. But their performance should be veriﬁed
by empirical experiments.
Existing studies have already shown some evidence of the transferability of neural features. For
example, in image processing, low-level neural layers closely resemble Gabor ﬁlters or color blobs
 ;
they can be transferred well to different tasks. Donahue et al. suggest that high-level layers
are also transferable in general visual recognition;
Yosinski et al. further investigate the transferability of neural layers in different levels of abstraction.
Although transfer learning is promising in image
processing, conclusions appear to be less clear in
NLP applications. Image pixels are low-level signals, which are generally continuous and less related
to semantics. By contrast, natural language tokens
are discrete: each word well reﬂects the thought
of humans, but neighboring words do not share as
much information as pixels in images do.
Previous neural NLP studies have casually applied transfer techniques, but their results are not consistent.
Collobert and Weston apply multi-task learning to SRL, NER, POS, and CHK,3 but obtain only
0.04–0.21% error reduction4 (out of a base error rate
of 16–18%). Bowman et al. , on the contrary,
improve a natural language inference task from an
accuracy of 71.3% to 80.8% by initializing parameters with an additional dataset of 550,000 samples.
Therefore, more systematic studies are needed to
shed light on transferring neural networks in the ﬁeld
Our Contributions
In this paper, we investigate the question “How
transferable are neural networks in NLP applications?”
We distinguish two scenarios of transfer:
transferring knowledge to a semantically similar/equivalent task but with a different dataset; (2)
transferring knowledge to a task that is semantically different but shares the same neural topology/architecture so that neural parameters can indeed be transferred.
We further distinguish two
transfer methods: (1) using the parameters trained
on S to initialize T (INIT), and (2) multi-task learning (MULT), i.e., training S and T simultaneously.
(Please see Sections 2 and 4). Our study mainly focuses on the following research questions:
RQ1: How transferable are neural networks between two tasks with similar or different semantics in NLP applications?
RQ2: How transferable are different layers of NLP
neural models?
RQ3: How transferable are INIT and MULT, respectively?
What is the effect of combining
these two methods?
3The acronyms refer to semantic role labeling, named entity
recognition, part-of-speech tagging, and chunking, respectively.
4Here, we quote the accuracies obtained by using unsupervised pretraining of word embeddings. This is the highest performance in that paper; using pretrained word embeddings is
also a common practice in the literature.
We conducted extensive experiments over six
datasets on classifying sentences and sentence pairs.
We leveraged the widely-used convolutional neural network (CNN) and long short term memory
(LSTM)-based recurrent neural network (RNN) as
our models.
Based on our experimental results, we have the
following main observations, some of which are unexpected.
• Whether a neural network is transferable in
NLP depends largely on how semantically
similar the tasks are, which is different from
the consensus in image processing.
• The output layer is mainly speciﬁc to the
dataset and not transferable.
Word embeddings are likely to be transferable to semantically different tasks.
• MULT and INIT appear to be generally comparable to each other; combining these two
methods does not result in further gain in our
The rest of this paper is organized as follows. Section 2 introduces the datasets that neural models are
transferred across; Section 3 details the neural architectures and experimental settings. We describe two
approaches (INIT and MULT) to transfer learning in
Section 4. We present experimental results in Sections 5–6 and have concluding remarks in Section 7.
In our study, we conducted two series of experiments using six open datasets as follows.
• Experiment I: Sentence classiﬁcation
−IMDB. A large dataset for binary sentiment
classiﬁcation (positive vs. negative).5
−MR. A small dataset for binary sentiment classiﬁcation.6
−QC. A (small) dataset for 6-way question
classiﬁcation (e.g., location, time, and
5 
0B8yp1gOBCztyN0JaMDVoeXhHWm8/
6 
movie-review-data/
7 
Statistics (# of Samples)
Experiment I
Experiment II
Examples in Experiment I
Sentiment Analysis (IMDB and MR)
An idealistic love story that brings out
the latent 15-year-old romantic in everyone.
Its mysteries are transparently obvious,
and its too slowly paced to be a thriller.
Question Classiﬁcation (QC)
What is the temperature at the center of the earth?
What state did the Battle of Bighorn take place in?
Examples in Experiment II
Natural Language Inference (SNLI and SICK)
Two men on bicycles competing in a race.
People are riding bikes.
Hypothesis
Men are riding bicycles on the streets.
A few people are catching ﬁsh.
Paraphrase Detection (MSRP)
The DVD-CCA then appealed to the state
Paraphrase
Supreme Court.
The DVD CCA appealed that decision
to the U.S. Supreme Court.
Earnings per share from recurring operations
will be 13 cents to 14 cents.
That beat the company’s April earnings
Paraphrase
forecast of 8 to 9 cents a share.
Table 1: Statistics and examples of the datasets.
• Experiment II: Sentence-pair classiﬁcation
−SNLI. A large dataset for sentence entailment recognition.
The classiﬁcation objectives are entailment, contradiction,
and neutral.8
−SICK. A small dataset with exactly the same
classiﬁcation objective as SNLI.9
−MSRP. A (small) dataset for paraphrase detection. The objective is binary classiﬁcation:
judging whether two sentences have the same
meaning.10
In each experiment, the large dataset serves as the
source domain and small ones are the target domains.
Table 1 presents statistics of the above
We distinguish two scenarios of transfer regarding semantic similarity: (1) semantically equivalent
transfer (IMDB→MR, SNLI→SICK), that is, the
tasks of S and T are deﬁned by the same meaning,
8 
9 
10 
and (2) semantically different transfer (IMDB→QC,
SNLI→MSRP). Examples are also illustrated in Table 1 to demonstrate semantic relatedness.
It should be noticed that in image or speech processing , the input of neural networks pretty much consists of raw signals; hence, low-level feature detectors are almost always transferable, even if Yosinski
et al. manually distinguish artiﬁcial objects
and natural ones in an image classiﬁcation task.
Distinguishing
relatedness—which
emerges from very low layers of either word embeddings or the successive hidden layer—is speciﬁc
to NLP and also a new insight of our paper.
we shall see in Sections 5 and 6, the transferability
of neural networks in NLP is more sensitive to
semantics than in image processing.
Neural Models and Settings
In each group, we used a single neural model to
solve three problems in a uniﬁed manner. That is
to say, the neural architecture is the same among the
three datasets, which makes it possible to investigate transfer learning regardless of whether the tasks
are semantically equivalent. Concretely, the neural
models are as follows.
• Experiment I: LSTM-RNN. To classify a
sentence according to its sentiment or question type, we use a recurrent neural network
(RNN, Figure 1a) with long short term memory (LSTM) units . A softmax layer is added to the last
word’s hidden state for classiﬁcation.
• Experiment II: CNN-pair. In this group, we
use a “Siamese” architecture to classify the relation of two sentences.
We ﬁrst apply a convolutional neural network
(CNN, Figure 1b) with a window size of 5 to
model local context, and a max pooling layer
gathers information to a ﬁxed-size vector. Then
the sentence vectors are concatenated and fed
to a hidden layer before the softmax output.
In our experiments, embeddings were pretrained
by word2vec ; all embeddings and hidden layers were 100 dimensional. We
Convolution
an        idealistic
Hidden LSTM
Embedding            Hidden layers            Output
Figure 1: The models in our study. (a) Experiment I: RNNs
with LSTM units for sentence classiﬁcation. (b) Experiment II:
CNN for sentence pair modeling.
applied stochastic gradient descent with a minibatch size of 50 for optimization. In each setting, we
tuned the hyperparameters as follows: learning rate
from {3, 1, 0.3, 0.1, 0.03}, power decay of learning
rate from {fast, moderate, low} (deﬁned by how
much, after one epoch, the learning rate residual is:
0.1x, 0.3x, 0.9x, resp). We regularized our network
by dropout with a rate from {0, 0.1, 0.2, 0.3}. Note
that we might not run nonsensical settings, e.g., a
larger dropout rate if the network has already been
underﬁtting (i.e., accuracy has decreased when the
dropout rate increases). We report the test performance associated with the highest validation accuracy.
To setup a baseline, we trained our models without transfer 5 times by different random parameter
initializations (Table 2). We have achieved reasonable performance that is comparable to similar models reported in the literature with all six datasets.
Therefore, our implementation is fair and suitable
for further study of transfer learning.
It should be mentioned that the goal of this paper
is not to outperform state-of-the-art results; instead,
Avg acc.±std.
Related model
89.3 
75.1 ± 0.6
77.7 
90.8 ± 0.9
90.2 
77.6 
70.9 ± 1.3
71.3 
69.0 ± 0.5
69.6 
Table 2: Accuracy (%) without transfer. We also include related models for comparison ,
showing that we have achieved comparable results, and thus are
ready to investigate transfer learning. The models were run one
only once in source domains, because we could only transfer a
particular model instead of an average of several models.
we would like to conduct a fair comparison of different methods and settings for transfer learning in
Transfer Methods
Transfer learning aims to use knowledge in a source
domain to aid the target domain.
As neural networks are usually trained incrementally with gradient descent (or variants), it is straightforward to use
gradient information in both source and target domains for optimization so as to accomplish knowledge transfer. Depending on how samples in source
and target domains are scheduled, there are two
main approaches to neural network-based transfer
• Parameter initialization (INIT). The INIT approach ﬁrst trains the network on S, and then directly uses the tuned parameters to initialize the
network for T .
After transfer, we may ﬁx ()
the parameters in the target domain , i.e., no training is performed on T . But
when labeled data are available in T , it would be
better to ﬁne-tune (1) the parameters.
INIT is also related to unsupervised pretraining
such as word embedding learning and autoencoders . In
these approaches, parameters that are (pre)trained
in an unsupervised way are transferred to initialize the model for a supervised task . However, our paper focuses on
“supervised pretraining,” which means we transfer knowledge from a labeled source domain.
• Multi-task learning (MULT). MULT, on the other
hand, simultaneously trains samples in both domains . The overall cost function is given by
J = λJT + (1 −λ)JS
where JT and JS are the individual cost function
of each domain. (Both JT and JS are normalized
by the number of training samples.) λ ∈(0, 1) is
a hyperparameter balancing the two domains.
It is nontrivial to optimize Equation 1 in practice
by gradient-based methods. One may take the partial derivative of J and thus λ goes to the learning
rate , but the model is then vulnerable because it is likely to blow up with large
learning rates (multiplied by λ or 1 −λ) and be
stuck in local optima with small ones.
Collobert and Weston alternatively choose
a data sample from either domain with a certain
probability (controlled by λ) and take the derivative for the particular data sample. In this way, domain transfer is independent of learning rates, but
we may not be able to fully use the entire dataset
of S if λ is large. We adopted the latter approach
in our experiment for simplicity. (More in-depth
analysis may be needed in future work.) Formally,
our multi-task learning strategy is as follows.
1 Switch to T with prob. λ, or to S with
prob. 1 −λ.
2 Compute the gradient of the next data sample
in the particular domain.
Further, INIT and MULT can be combined
straightforwardly, and we obtain the third setting:
• Combination (MULT+INIT). We ﬁrst pretrain on
the source domain S for parameter initialization,
and then train S and T simultaneously.
From a theoretical perspective, INIT and MULT
work in different ways. In the MULT approach, the
source domain regularizes the model by “aliasing”
the error surface of the target domain; hence the
neural network is less prone to overﬁtting. In INIT,
T ’s error surface remains intact. Before training on
the target dataset, the parameters are initialized in
such a meaningful way that they contain additional
knowledge in the source domain. However, in an extreme case where T ’s error surface is convex, INIT
is ineffective because the parameters can reach the
global optimum regardless of their initialization. In
practice, deep neural networks usually have highly
complicated, non-convex error surfaces. By properly initializing parameters with the knowledge of
S, we can reasonably expect that the parameters are
in a better “catchment basin,” and that the INIT approach can transfer knowledge from S to T .
Results of Transferring by INIT
We ﬁrst analyze how INIT behaves in NLP-based
transfer learning. In addition to two different transfer scenarios regarding semantic relatedness as described in Section 2, we further evaluated two settings: (1) ﬁne-tuning parameters 1, and (2) freezing parameters after transfer . Existing evidence
shows that frozen parameters would generally hurt
the performance , but this setting
provides a more direct understanding on how transferable the features are (because the factor of target
domain optimization is ruled out). Therefore, we
included it in our experiments. Moreover, we transferred parameters layer by layer to answer our second research question.
Through Subsections 5.1–5.3, we initialized the
parameters of T with the ones corresponding to
the highest validation accuracy of S.
In Subsection 5.4, we further investigated when the parameters are ready to be transferred during the training
Overall Performance
Table 3 shows the main results of INIT. A quick
observation is that, in both groups, transfer learning of semantically equivalent tasks (IMDB→MR,
SNLI→SICK) appears to be successful with an improvement of ∼6%. The results are not surprising
and also reported in Bowman et al. .
For IMDB→QC and SNLI→MSRP, however,
there is no improvement of transferring hidden layers (embeddings excluded), namely LSTM-RNN
units and CNN feature maps.
The E1H1O2
setting yields a slight degradation of 0.2–0.4%,
The incapability of transferring is also
proved by locking embeddings and hidden layers
We see in this setting, the test performance is very low in QC or even worse than
majority-class guess in MSRP. By further examining its training accuracy, which is 48.2% and 65.5%,
respectively, we conclude that extracted features by
LSTM-RNN and CNN models in S are almost irrelevant to the ultimate tasks T (QC and MSRP).
Although in previous studies, researchers have
mainly drawn positive conclusions about transfer
learning, we ﬁnd a negative result similar to ours
upon careful examination of Collobert and Weston , and unfortunately, their results may be
somewhat misinterpreted. In that paper, the authors
report transferring NER, POS, CHK, and pretrained
word embeddings improves the SRL task by 1.91–
3.90% accuracy (out of 16.54–18.40% error rate),
but their gain is mainly due to word embeddings.
In the settings that use pretrained word embeddings
(which is common in NLP), NER, POS, and CHK
together improve the SRL accuracy by only 0.04–
The above results are rather frustrating, indicating for RQ1 that neural networks may not be transferable to NLP tasks of different semantics. Transfer learning for NLP is more prone to semantics
than the image processing domain, where even highlevel feature detectors are almost always transferable .
Layer-by-Layer Analysis
To answer RQ2, we next analyze the transferability of each layer. First, we freeze both embeddings
and hidden layers (EH).
Even in semantically
equivalent settings, if we further freeze the output
layer (O), the performance in both IMDB→MR and
SNLI→SICK drops, but by randomly initializing
the output layer’s parameters (O2), we can obtain a
similar or higher result compared with the baseline
(E4H2O2). The ﬁnding suggests that the output
layer is mainly speciﬁc to a dataset. Transferring the
output layer’s parameters yields little (if any) gain.
Regarding embeddings and hidden layers (in
the settings E1H1O2/E1H2O2 vs. E4H2O2),
the IMDB→MR experiment suggests both of embeddings and the hidden layer play an important
role, each improving the accuracy by 3%.
SNLI→SICK, however, the main improvement lies
in the hidden layer. A plausible explanation is that
Experiment I
Experiment II
SNLI→SICK SNLI→MSRP
Table 3: Main results of neural transfer learning by INIT. We
report test accuracies (%) in this table. E: embedding layer;
H: hidden layers; O: output layer. 4: Word embeddings are
pretrained by word2vec; 2: Parameters are randomly initialized); : Parameters are transferred but frozen; 1: Parameters are transferred and ﬁne-tuned. Notice that the EHO
and E1H1O1 settings are inapplicable to IMDB→QC and
SNLI→MSRP, because the output targets do not share same
meanings and numbers of target classes.
in sentiment classiﬁcation tasks (IMDB and MR), information emerges from raw input, i.e., sentiment
lexicons and thus their embeddings, but natural language inference tasks (SNLI and SICK) address
more on semantic compositionality and thus hidden
layers are more important.
semantically
(IMDB→QC and SNLI→MSRP), the embeddings
are the only parameters that have been observed to
be transferable, slightly beneﬁting the target task by
2.7x and 1.8x std, respectively.
How does learning rate affect transfer?
Bowman et al. suggest that after transferring,
a large learning rate may damage the knowledge
stored in the parameters; in their paper, they transfer
the learning rate information (AdaDelta) from S to
T in addition to the parameters.
Experiment I
Accuracy (%)
Experiment II
Accuracy (%)
Learning curves of different learning rates (denoted as α). (a) Experiment I: IMDB→MR; (b) Experiment II:
SNLI→SICK.
Although the rule of the thumb is to choose all
hyperparameters—including the learning rate—by
validation, we are curious whether the above conjecture holds. Estimating a rough range of sensible
hyperparameters can ease the burden of model selection; it also provides evidence to better understand
how transfer learning actually works.
We plot the learning curves of different learning
rates α in Figure 2 (IMDB→MR and SNLI→SICK,
E1H1O2). (In the ﬁgure, no learning rate decay is
applied.) As we see, with a large learning rate like
α = 0.3, the accuracy increases fast and peaks at
earlier epochs. Training with a small learning rate
(e.g., α = 0.01) is slow, but its peak performance is
comparable to large learning rates when iterated by,
say, 100 epochs. The learning curves in Figure 2 are
similar to classic speed/variance trade-off, and we
have the following additional discovery:
In INIT, transferring learning rate information
is not necessarily useful. A large learning rate
does not damage the knowledge stored in the
pretrained hyperparameters, but accelerates the
training process to a large extent. In all, we may
need to perform validation to choose the learning
rate if computational resources are available.
Experiment I
IMDB Acc. (%)
Learning curve of IMDB
Experiment II
SNLI Acc. (%)
Learning curve of SNLI
Figure 3: (a) and (c): Learning curves of S. (b) and (d): Accuracies of T when parameters are transferred at a certain epoch
during the training of S.
Dotted lines refer to non-transfer,
which can be equivalently viewed as transferring before training on S, i.e., epoch = 0. Note that the x-axis shares across
different subplots.
When is it ready to transfer?
In the above experiments, we transfer the parameters when they achieve the highest validation performance on S. This is a straightforward and intuitive
However, we may imagine that the parameters
well-tuned to the source dataset may be too speciﬁc
to it, i.e., the model overﬁts S and thus may underﬁt
T . Another advantage of early transfer lies in com-
putational concerns. If we manage to transfer model
parameters after one or a few epochs on S, we can
save much time especially when S is large.
We therefore made efforts in studying when the
neural model is ready to be transferred. Figures 3a
and 3c plot the learning curves of the source tasks.
The accuracy increases sharply from epochs 1–5;
later, it reaches a plateau but is still growing slowly.
We then transferred the parameters at different
stages (epochs) of training to target tasks (also with
the setting E1H1O2). Their accuracies are plotted
in Figures 3b and 3d.
In IMDB→MR, the source performance and transferring performance align well. The SNLI→SICK
experiment, however, produces interesting yet unexpected results. Using the second epoch of SNLI’s
training yields the highest transfer performance on
SICK, i.e., 78.98%, when the SNLI performance
itself is comparatively low (72.65% vs. 76.26% at
epoch 23). Later, the transfer performance decreases
gradually by ∼2.7%. The results in these two experiments are inconsistent and lack explanation.
MULT, and its Combination with INIT
To answer RQ3, we investigate how multi-task
learning performs in transferring knowledge, as well
as the effect of the combination of MULT and INIT.
In this section, we applied the setting: sharing embeddings and hidden layers (denoted as E♥H♥O2),
analogous to E1H1O2 in INIT. When combining
MULT and INIT, we used the pretrained parameters
of embeddings and hidden layers on S to initialize
the multi-task training of S and T , visually represented by E1♥H1♥O2.
In both MULT and MULT+INIT, we had a hyperparameter λ ∈(0, 1) balancing the source and
target tasks (deﬁned in Section 4). λ was tuned with
a granularity of 0.1. As a friendly reminder, λ = 1
refers to using T only; λ = 0 refers to using S only.
After ﬁnding that a small λ yields high performance
of MULT in the IMDB+MR and SNLI+SICK experiments (thick blue lines in Figures 4a and 4c), we
further tuned the λ from 0.01 to 0.09 with a ﬁnegrained granularity of 0.02.
The results are shown in Figure 4. From the green
curves in the 2nd and 4th subplots, we see MULT
(with or without INIT) does not improve the accu-
Experiment I
Accuracy (%)
IMDB+MR, MULT
IMDB+MR, MULT+INIT
Accuracy (%)
IMDB+QC, MULT
IMDB+QC, MULT+INIT
Experiment II
Accuracy (%)
SNLI+SICK, MULT
SNLI+SICK, MULT+INIT
Accuracy (%)
SNLI+MSRP, MULT
SNLI+MSRP, MULT+INIT
Figure 4: Results of MULT and MULT+INIT, where we share
word embeddings and hidden layers. Dotted lines are the nontransfer setting; dashed lines are the INIT setting E1H1O2,
transferred at the peak performance of IMDB and SNLI.
racy of target tasks (QC and MSRP); the inability
to transfer is cross-checked by the INIT method in
Section 5. For MR and SICK, on the other hand,
transferability of the neural model is also consistently positive (blue curves in Figures 4a and 4c),
supporting our conclusion to RQ1 that neural trans-
fer learning in NLP depends largely on how similar
in semantics the source and target datasets are.
Moreover, we see that the peak performance of
MULT is slightly lower than INIT in Experiment I
(Figure 4a), but higher in Experiment II (Figure 4c);
they are in the same ballpark.
In MULT+INIT (E1♥H1♥O2), the transfer
performance of MULT+INIT remains high for different values of λ. Because the parameters given
by INIT have already conveyed sufﬁcient information about the source task, MULT+INIT consistently outperforms non-transferring by a large margin. Its peak performance, however, is not higher
than MULT or INIT. In summary, we answer our
RQ3 as follows: in our experiments, MULT and
INIT are generally comparable; we do not obtain
further gain by combining MULT and INIT.
Concluding Remarks
In this paper, we addressed the problem of transfer learning in neural network-based NLP applications. We conducted two series of experiments on
six datasets, showing that the transferability of neural NLP models depends largely on the semantic relatedness of the source and target tasks, which is
different from other domains like image processing.
We analyzed the behavior of different neural layers.
We also experimented with two transfer methods:
parameter initialization (INIT) and multi-task learning (MULT). Besides, we reported two additional
studies in Sections 5.3 and 5.4 (not repeated here).
Our paper provides insight on the transferability of
neural NLP models; the results also help to better
understand neural features in general.
How transferable are the conclusions in this
paper? We have to concede that empirical studies
are subject to a variety of factors (e.g., models, tasks,
datasets), and that conclusions may vary in different
scenarios. In our paper, we have tested all results
on two groups of experiments involving 6 datasets
and 2 neural models (CNN and LSTM-RNN). Both
models and tasks are widely studied in the literature,
and not chosen deliberately. Results are mostly consistent (except Section 5.4). Along with analyzing
our own experimental data, we have also collected
related results in previous studies, serving as additional evidence in answering our research questions.
Therefore, we think the generality of this work is
fair and that the conclusions can be generalized to
similar scenarios.
Future work. Our work also points out some future directions of research. For example, we would
like to analyze the effect of different MULT strategies. More efforts are also needed in developing an
effective yet robust method for multi-task learning.
Acknowledgments
We thank all reviewers for their constructive comments, Sam Bowman for helpful suggestion, and
Vicky Li for discussion on the manuscript.
research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201 and the National
Natural Science Foundation of China under Grant
Nos. 61232015, 91318301, 61421091, 61225007,
and 61502014.