This is the accepted manuscript made available via CHORUS. The article has been
published as:
Neural Decoder for Topological Codes
Giacomo Torlai and Roger G. Melko
Phys. Rev. Lett. 119, 030501 — Published 18 July 2017
DOI: 10.1103/PhysRevLett.119.030501
A Neural Decoder for Topological Codes
Giacomo Torlai and Roger G. Melko
Department of Physics and Astronomy, University of Waterloo, Ontario N2L 3G1, Canada
Perimeter Institute for Theoretical Physics, Waterloo, Ontario N2L 2Y5, Canada
 
We present an algorithm for error correction in topological codes that exploits modern machine
learning techniques. Our decoder is constructed from a stochastic neural network called a Boltzmann
machine, of the type extensively used in deep learning. We provide a general prescription for the
training of the network and a decoding strategy that is applicable to a wide variety of stabilizer codes
with very little specialization. We demonstrate the neural decoder numerically on the well-known
two dimensional toric code with phase-ﬂip errors.
Introduction: Much of the success of modern machine
learning stems from the ﬂexibility of a given neural network architecture to be employed for a multitude of different tasks. This generalizability means that neural networks can have the ability to infer structure from vastly
diﬀerent data sets with only a change in optimal hyperparameters. For this purpose, the machine learning community has developed a set of standard tools, such as
fully-connected feed forward networks and Boltzmann
machines . Specializations of these underlie many of
the more advanced algorithms, including convolutional
networks and deep learning , encountered in
real-world applications such as image or speech recognition .
These machine learning techniques may be harnessed
for a multitude of complex tasks in science and engineering .
An important application lies in quantum
computing. For a quantum logic operation to succeed,
noise sources which lead to decoherence in a qubit must
be mitigated. This can be done through some type of
quantum error correction – a process where the logical
state of a qubit is encoded redundantly so that errors
can be corrected before they corrupt it . A leading
candidate for this is the implementation of fault-tolerant
hardware through surface codes, where a logical qubit
is stored as a topological state of an array of physical
qubits . Random errors in the states of the physical qubits can be corrected before they proliferate and
destroy the logical state.
The quantum error correction protocols that perform this correction are termed
“decoders”, and must be implemented by classical algorithms running on conventional computers .
In this paper we demonstrate how one of the simplest
stochastic neural networks for unsupervised learning, the
restricted Boltzmann machine , can be used to construct a general error-correction protocol for stabilizer
codes. Give a syndrome, deﬁned by a measurement of
the end points of an (unknown) chain of physical qubit
errors, we use our Boltzmann machine to devise a protocol with the goal of correcting errors without corrupting
the logical bit.
Our decoder works for generic degenerate stabilizers codes that have a probabilistic relation
between syndrome and errors, which does not have to be
FIG. 1. Several operations on a 2D toric code. Logical operators ˆZ(1)
(orange) are non-trivial cycles on the real
lattice. A physical error chain e (purple) and its syndrome
S(e) (black squares). A recovery chain r′ (green), with the
combined operator on the cycle e ⊕r′ being a product of
stabilizers ˆZα ˆZβ ˆZγ (recovery success). A recovery chain r′′
(red) whose cycle has non-trivial homology and acts on the
code state as ˆZ(1)
(logical failure).
a priori known. Importantly, it is very simple to implement, requiring no specialization regarding code locality,
dimension, or structure. We test our decoder numerically
on a simple two-dimensional surface code with phase-ﬂip
The 2D Toric Code.
Most topological codes can be
described in terms of the stabilizer formalism . A stabilizer code is a particular class of error-correcting code
characterized by a protected subspace C deﬁned by a stabilizer group S. The simplest example is the 2D toric
code, ﬁrst introduced by Kitaev . Here, the quantum information is encoded into the homological degrees
of freedom, with topological invariance given by the ﬁrst
homology group . The code features N qubits placed
on the links of a L × L square lattice embedded on a
torus. The stabilizers group is S = { ˆZp, ˆXv}, where the
plaquette and vertex stabilizers are deﬁned respectively
as ˆZp = N
ℓand ˆXv = N
ℓ, with ˆσz
acting respectively on the links contained in the plaque-
tte p and the links connected to the vertex v. There are
two encoded logical qubits, manipulated by logical operators ˆZ(1,2)
as ˆσz acting on the non-contractible loops on
the real lattice and logical ˆX(1,2)
as the non-contractible
loops on the dual lattice (Fig 1).
Given a reference state |ψ0⟩∈C, let us consider the
simple phase-ﬂip channel described by a Pauli operator
where ˆσz is applied to each qubit with probability perr.
This operator can be eﬃciently described by a mapping
between the links and Z2, called an error chain e, whose
boundary is called a syndrome S(e). In a experimental implementation, only the syndrome (and not the error chain) can be measured. Error correction (decoding)
consists of applying a recovery operator whose chain r
generates the same syndrome, S(e) = S(r). The recovery succeeds only if the combined operation is described
by a cycle (i.e. a chain with no boundaries) e ⊕r that
belongs to the trivial homology class h0, describing contractible loops on the torus. On the other hand, if the
cycle belongs to a non-trivial homology class (being noncontractible on the torus), the recovery operation directly
manipulates the encoded logical information, leading to
a logical failure (Fig 1).
Several decoders have been proposed for the 2D toric
code, based on diﬀerent strategies .
likelihood decoding consists of ﬁnding a recovery chain
r with the most likely homology class . A diﬀerent recovery strategy, designed to reduce computational
complexity, consists of generating the recovery chain r
compatible with the syndrome simply by using the minimum number of errors. Such a procedure, called Minimum Weight Perfect Matching (MWPM), has the
advantage that can be performed without the knowledge
of the error probability perr. This algorithm is however
sub-optimal (with lower threshold probability ) since
it does not take into account the high degeneracy of the
error chains given a syndrome.
The Neural Decoder. Neural networks are commonly
used to extract features from raw data in terms of probability distributions.
In order to exploit this for error
correction, we ﬁrst build a dataset made of error chains
and their syndromes D = {e, S}, and train a neural network to model the underlying probability distribution
pdata(e, S).
Our goal is to then generate error chains
to use for the recovery. We use a generative model called
a Boltzmann machine, a powerful stochastic neural network widely used in the pre-training of the layers of deep
neural networks . The network architecture features three layers of stochastic binary neurons, the syndrome layer S ∈{0, 1}N/2, the error layer e ∈{0, 1}N,
and one hidden layer h ∈{0, 1}nh (Fig. 2).
Symmetric edges connect both the syndrome and the error layer
with the hidden layer. We point out the this network is
equivalent to a traditional bilayer restricted Boltzmann
machine, where we have here divided the visible layer into
FIG. 2. The neural decoder architecture. The hidden layer h
is fully-connected to the syndrome and error layers S and e
with weights U and W respectively.
two separate layers for clarity. The weights on the edges
connecting the network layers are given by the matrices
U and W . Moreover, we also add external ﬁelds b, c and
d coupled to the every neuron in each layer. The probability distribution that the probabilistic model associates
to this graph structure is the Boltzmann distribution 
pλ(e, S, h) = 1
e−Eλ(e,S,h)
where Zλ = Tr{h,S,e} e−Eλ(e,S,h) is the partition function, λ = {U, W , b, c, d} is the set of parameters of the
model, and the energy is
Eλ(e, S, h) = −
The joint probability distribution over (e, S) is obtained
after integrating out the hidden variables from the full
distribution
pλ(e, S) =
pλ(e, S, h) = 1
where the eﬀective energy Eλ(e, S) can be computed
Moreover, given the structure of the network, the conditional probabilities pλ(e | h), pλ(S | h)
and pλ(h | e, S) are also known exactly.
The training
of the machine consists of tuning the parameters λ until the model probability pλ(e, S) becomes close to the
target distribution pdata(e, S) of the dataset. This translates into solving an optimization problem over the parameters λ by minimizing the distance between the two
distribution, deﬁned as the Kullbach-Leibler (KL) divergence, KL ∝−P
(e,S)∈D log pλ(e, S). Details about the
Boltzmann machine and its training algorithm are reported in the Supplementary Materials.
We now discuss the decoding algorithm, which proceeds assuming that we successfully learned the distribution pλ(e, S). Given an error chain e0 with syndrome
S0 we wish to use the Boltzmann machine to generate
an error chain compatible with S0 to use for the recovery. To achieve this goal we separately train networks on
diﬀerent datasets obtained from diﬀerent error regimes
Assuming we know the error regimes that generated e0, the recovery procedure consists of sampling
a recovery chain from the distribution pλ(e | S0) given
by the network trained at the same probability perr of
Although the Boltzmann machine does not learn
this distribution directly, by sampling the error and hidden layers while keeping the syndrome layer ﬁxed to S0,
since pλ(e, S0) = pλ(e | S0)p(S0), we are enforcing sampling from the desired conditional distribution. An advantage of this procedure over decoders that employ conventional Monte Carlo on speciﬁc stabilizer codes
is that specialized sampling algorithms tied to the stabilizer structure, or multi-canonical methods such as parallel tempering, are not required. Finally, note that the
assumption of perfect learning is not critical, since the
above sampling routine can be modiﬁed with an extra
rejection step as discussed in Ref. to ensure sampling
occurs from the proper physical distribution.
An error correction procedure can be deﬁned as follows (Alg. 1): we ﬁrst initialize the machine into a random state of the error and hidden layers (see Fig. 2)
and to S0 for the syndrome layer. We then let the machine equilibrate by repeatedly performing block Gibbs
sampling. After some amount of equilibration steps, we
begin checking the syndrome of the error state e in the
machine and, as soon as S(e) = S0 we select it for the
recovery operation. If such a condition is not met before
a ﬁxed amount of sampling steps, the recovery attempt
is stopped and considered failed. This condition makes
the precise computational requirements of the algorithm
ill-deﬁned, since the cut-oﬀtime can always be increased
resulting in better performance for a higher computational cost.
Algorithm 1 Neural Decoding Strategy
1: e0: physical error chain
2: S0 = S(e0)
▷Syndrome Extraction
3: RBM = {e, S = S0, h}
▷Network Initialization
4: while S(e) ̸= S0 do
Sample h ∼p(h | e, S0)
Sample e ∼p(e | h)
7: end while
We train neural networks in diﬀerent error
regimes by building several datasets Dp = {ek, Sk}M
at elementary error probabilities p = {0.5, 0.6, . . . , 0.15}
of the phase-ﬂip channel. For a given error probability,
the network hyper-parameters are individually optimized
via a grid search (for details see the Supplementary Material). Once training is complete, we perform decoding
following the procedure laid out in Alg. 1. We generate a
L = 4, NeD
L = 6, NeD
L = 6, MWPM
L = 4, MWPM
FIG. 3. Logical failure probability as a function of elementary
error probability for MWPM (lines) and the neural decoder
(markers) of size L = 4 (red) and L = 6 (green).
test set Tp = {ek}M
k=1 and for each error chain ek ∈Tp, after a suitable equilibration time (usually Neq ∝102 sampling steps), we collect the ﬁrst error chain e compatible
with the original syndrome, S(e) = S(ek). We use this
error chain for the recovery, r(k) = e. Importantly, error
recovery with r(k) chosen from the ﬁrst compatible chain
means that the cycle ek + r(k) is sampled from a distribution that includes all homology classes. By computing
the Wilson loops on the cycles we can measure their homology class. This allows us to gauge the accuracy of the
decoder in term of the logical failure probability, deﬁned
as Pfail = nfail
where nfail is the number of cycles with
non-trivial homology. Because of the fully-connected architecture of the network, and the large complexity of
the probability distribution arising from the high degeneracy of error chains given a syndrome, we found that
the dataset size required to accurately capture the underlying statistics must be relatively large (|Dp| ∝105).
In Fig. 3 we plot the logical failure probability Pfail as a
function of the elementary error probability for the neural
decoding scheme. We note that at low perr, our logical
failure probabilities follow the expected scaling form
err (not plotted).
To compare our numerical results we also perform
error correction using the recovery scheme given by
MWPM . This algorithm creates a graph whose vertices corresponds to the syndrome and the edges connect
each vertex with a weight equal to the Manhattan distance (the number of links connecting the vertices in the
original square lattice). MWPM then ﬁnds an optimal
matching of all the vertices pairwise using the minimum
weight, which corresponds to the minimum number of
edges in the lattice .
Fig. 3 displays the comparison between a MWPM decoder (line) and our neural
decoder (markers). As is evident, the neural decoder has
an almost identical logical failure rate for error proba-
bilities below the threshold (perr ≈10.9 ), yet a signiﬁcant higher probability above. Note that by training
the Boltzmann machine on diﬀerent datasets we have enforced in the neural decoder a dependence on the error
probability. This is in contrast to MWPM which is performed without such knowledge. Another key diﬀerence
is that the distributions learned by the Boltzmann machine contain the entropic contribution from the high degeneracy of error chains, which is directly encoded into
the datasets. It will be instructive to explore this further, to determine whether the diﬀerences in Fig. 3 come
from ineﬃciencies in the training, the diﬀerent decoding
model of the neural network, or both. Finite-size scaling
on larger L will allow calculation of the threshold deﬁned
by the neural decoder.
In the above algorithm, which amounts to a simple
and practical implementation of the neural decoder, our
choice to use the ﬁrst compatible chain for error correction means that the resulting logical operation is sampled
from a distribution that includes all homology classes.
This is illustrated in Fig. 4, where we plot the histogram
of the homology classes for several diﬀerent elementary
error probabilities. Accordingly, our neural decoder can
easily be modiﬁed to perform Maximum Likelihood (ML)
optimal decoding. For a given syndrome, instead of obtaining only one error chain to use in decoding, one could
sample many error chains and build up the histogram
of homology classes with respect to any reference error state. Then, choosing the recovery chain from the
largest histogram bin will implement, by deﬁnition, ML
decoding. Although the computational cost of this procedure will clearly be expensive using the current fullyconnected restricted Boltzmann machine, it would be interesting to explore specializations of the neural network
architecture in the future to see how its performance may
compare to other ML decoding algorithms 
Conclusions. We have presented a decoder for topological codes using a simple algorithm implemented with a
restricted Boltzmann machine, a common neural network
used in many machine learning applications. Our neural
decoder is easy to program using standard machine learning software libraries and training techniques, and relies
on the eﬃcient sampling of error chains distributed over
all homology classes. Numerical results show that our
decoder has a logical failure probability that is close to
MWPM, but not identical, a consequence of our neural
network being trained separately at diﬀerent elementary
error probabilities. This leads to the natural question of
the relationship between the neural decoder and optimal
decoding, which could be explored further by a variation
of our algorithm that implements maximum likelihood
In its current implementation, the Boltzmann machine
is restricted within a given layer of neurons, but fullyconnected between layers. This means that our decoder
does not depend on the speciﬁc geometry used to imh0
perr = 0.05
perr = 0.08
perr = 0.12
perr = 0.15
FIG. 4. Histogram of the homology classes returned by our
neural decoder for various elementary error probabilities perr.
The green bars represent the trivial homology class h0 corresponding to contractible loops on the torus. The other three
classes correspond respectively to the logical operations ˆZ(1)
plement the code, nor on the structure of the stabilizer
group; it is trained simply using a raw data input vector, with no information on locality or dimension. Such
a high degree of generalizability, which is one of the core
advantages of this decoder, also represents a challenge for
investigating bigger systems. For example, a bottleneck
in our scheme to decode larger sizes is ﬁnding an error
chain compatible with the syndrome within a reasonable
cut-oﬀtime.
In order to scale up our system sizes on the 2D toric
code (as required e.g. to calculate the threshold), one
could relax some of the general fully-connected structure of the network, and specialize it to accommodate
the speciﬁc details of the code. Geometric specialization
such as this has been explicitly demonstrated to improve
the representational eﬃciency of neural networks in the
case of the toric code . This specialization should
be explored in detail, before comparisons of computational eﬃciency can be made between our neural decoder,
MWPM, and other decoding schemes. Note that, even
with moderate specialization, the neural decoder as we
have presented above can immediately be extended to
other choices of error models , such as the more realistic case of imperfect syndrome measurement , or
transferred to other topological stabilizer codes, such as
color codes . We also point out that the training of the networks are performed oﬀ-line and have to be
carried out only once. As such, the high computational
cost of the training need not be considered when evaluating the decoder computational eﬃciency for any of these
Finally, it would be interesting to explore the improvements in performance obtained by implementing standard tricks in machine learning, such as convolutions,
adaptive optimization algorithms, or the stacking of multiple Boltzmann machines into a network with deep structure. Given the rapid advancement of machine learning
technology within the world’s information industry, we
expect that such tools will be the obvious choice for the
real-world implementation of decoding schemes on future
topologically fault-tolerant qubit hardware.
Acknowledgements. The authors thank J. Carrasquilla,
D. Gottesman, M. Hastings, C. Herdmann, B. Kulchytskyy, M. Mariantoni and D. Poulin for enlightening discussions. This research was supported by NSERC, the
CRC program, the Ontario Trillium Foundation, the
Perimeter Institute for Theoretical Physics, and the National Science Foundation under Grant No. NSF PHY-
1125915. Simulations were performed on resources provided by SHARCNET. Research at Perimeter Institute is
supported through Industry Canada and by the Province
of Ontario through the Ministry of Research & Innovation.
 K. Hornik, M. Stinchcombe, and H. White, Neural Networks 2, 359 .
 R. Salakhutdinov, Technical Report UTML, Dep. Comp.
Sc., University. of Toronto , 002 .
 A. Krizhevsky, I. Sutskever,
and G. Hinton, Proc. Advances in Neural Information Processing Systems 25,
1090 .
 G. Hinton, Trends in Cognitive Science 10, 428 .
 Y. LeCun, Y. Bengio, and G. Hinton, Nature 521, 436
 G. Hinton and et al, IEEE Signal Processing Magazine
29, 82 .
 G. Torlai and R. Melko, Physical Review B 94, 165134
 J. Carrasquilla and R. G. Melko, Nat Phys 13, 431
 L. Wang, Physical Review B 94, 195105 .
 G. Carleo and M. Troyer, Science 355, 602 .
 P. Broecker, J. Carrasquilla, R. G. Melko, and S. Trebst,
 
 K. Ch’ng, J. Carrasquilla, R. G. Melko, and E. Khatami,
 
 D.-L. Deng, X. Li,
and S. D. Sarma, arXiv:1609.09060
 L. Huang and L. Wang, Physical Review B 95, 035105
 J. Liu, Y. Qi, Z. Y. Meng, and L. Fu, Physical Review
B 95, 041101 .
 E. M. Stoudenmire and D. J. Schwab, arXiv:1605.05775
 G. Torlai, G. Mazzola, J. Carrasquilla, M. Troyer, R. G.
Melko, and G. Carleo, arXiv:1703.05334 .
 D. Nigg, M. Mueller, E. A. Martinez, P. Schindler,
M. Hennrich, T. Monz, M. A. Martin-Delgado,
R. Blatt, Science 345, 302 .
 H. Bombin, in Quantum Error Correction, edited by
D. A. Lidar and T. A. Brun Chap. 19.
 A. G. Fowler, M. Mariantoni, J. M. Martinis, and A. N.
Cleland, Phys. Rev. A 86, 032324 .
 H. Bombin and M. A. Martin-Delgado, J. Phys. A:Math.
Theor. 42, 095302 .
 G. Hinton, Neural Networks: Tricks of the Trade , 599
 D. Gottesman, arXiv:quant-ph/9705052 .
 A. Y. Kitaev, Annals of Physics 1, 2 .
 E. Dennis, A. Kitaev, A. Landahl, and J. Preskill, Journal of Mathematical Physics 43, 4452 .
 G. Duclos-Cianci and D. Poulin, Phys. Rev. Lett. 104,
050504 .
 G. Duclos-Cianci and D. Poulin, Quant. Inf. Comp. 14,
0721 .
 J. R. Wootton and D. Loss, Phys. Rev. Lett. 109, 160503
 A. Hutter, J. R. Wootton,
and D. Loss, Phys. Rev. A
89, 022326 .
 A. Fowler, arXiv:1310.0863 .
 S. Bravyi, M. Suchara, and A. Vargo, Phys. Rev. A 90,
032326 .
 
 J. Edmonds, Canadian Journal of Mathematics 17, 449
 G. Hinton, S. Osindero, and Y. Teh, Neural computation
18, 1527 .
 R. Salakhutdinov and I. Murray, ICML’08 Proceedings
of the 25th international conference on machine learning
, 872 .
 A. Fischer and C. Igel, Progress in Pattern Recognition,
Image Analysis, Computer Vision, and Applications , 14
 F. H. E. Watson and S. D. Barrett, New Journal of
Physics 16, 093045 .
 V. Kolmogorov, Math. Prog. Comp. 1, 43 .
 A. G. Fowler, A. C. Whiteside, and L. C. L. Hollenberg,
Phys. Rev. Lett. 108, 180501 .
 E. Novais and E. R. Mucciolo, Phys. Rev. Lett. 110,
010502 .
 C. Wang, J. Harrington,
and J. Preskill, Annals of
Physics 1, 31 .
 H. G. Katzgraber, H. Bombin,
and M. A. Martin-
Delgado, Phys. Rev. Lett 103, 090501 .
 B. J. Brown, N. H. Nickerson, and D. E. Browne, Nature
Communications 7 .