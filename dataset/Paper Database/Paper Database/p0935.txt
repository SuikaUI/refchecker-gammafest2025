IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
A Discriminative Kernel-based Model to Rank
Images from Text Queries
David Grangier and Samy Bengio
Abstract— This paper introduces a discriminative model for the
retrieval of images from text queries. Our approach formalizes
the retrieval task as a ranking problem, and introduces a
learning procedure optimizing a criterion related to the ranking
performance. The proposed model hence addresses the retrieval
problem directly and does not rely on an intermediate image
annotation task, which contrasts with previous research. Moreover, our learning procedure builds upon recent work on the
online learning of kernel-based classiﬁers. This yields an efﬁcient,
scalable algorithm, which can beneﬁt from recent kernels developed for image comparison. The experiments performed over
stock photography data show the advantage of our discriminative
ranking approach over state-of-the-art alternatives (e.g. our
model yields 26.3% average precision over the Corel dataset,
which should be compared to 22.0%, for the best alternative
model evaluated). Further analysis of the results shows that our
model is especially advantageous over difﬁcult queries such as
queries with few relevant pictures or multiple-word queries.
Index Terms— image retrieval, ranking, discriminative learning, kernel-based classiﬁer, large margin
I. INTRODUCTION
In this paper, we address the problem of retrieving pictures
from text queries. In this task, the retrieval system is given a
set of pictures and a few word query, it then outputs a picture
ranking in which the pictures relevant to the query should
appear above the others. This type of setup is common in several application domains, including web search engines, news
wire services or stock photography providers. So far, the most
widely-used approach to this problem consists in applying text
retrieval techniques over a set of manually-produced captions
that describe each picture. Although effective, this solution is
expensive, as it requires a signiﬁcant manual labeling effort.
Consequently, several automatic annotation approaches have
been proposed in the literature. These approaches rely on a
set of captioned pictures to learn a model, which can then
predict textual annotations for any unlabeled picture. Two main
types of auto-annotation models have been introduced: concept
classiﬁcation models and bi-modal generative models. In the
case of concept classiﬁcation, a classiﬁer is learned for each
vocabulary term, or concept, t. This classiﬁer takes as input
a picture and outputs a conﬁdence value indicating whether
the term t should occur in the predicted picture caption. This
classiﬁcation problem is typically addressed using Support
Vector Machine (SVM) , or boosting classiﬁers ,
Manuscript received December 22, 2006; revised September 12, 2007.
David Grangier is with the IDIAP Research Institute (Switzerland), Samy
Bengio is with Google Inc. (USA). Part of this research has been performed
while Samy Bengio was at the IDIAP Research Institute, part of this research
has been performed while David Grangier was at Google Inc.
as these large margin approaches enjoy good generalization
properties . In the case of bi-modal generative models,
the training procedure learns a distribution estimating the joint
probability P(p, c) of a picture p (i.e. a set of visual features)
and a caption c (i.e. a set of terms describing the picture).
Given a test picture p, the learned distribution can then be used
to infer the most likely caption, or a distribution over the whole
vocabulary. Compared to concept classiﬁcation, this generative
approach hence learns a single model for all vocabulary terms,
which notably yields a better modeling of term dependencies.
Several bi-modal generative models have been proposed in
the recent years, each model relying on different conditional
independence assumptions between the observation of the text
and the visual features.
Besides their differences, both concept classiﬁcation and bimodal generative models address the image retrieval problem
through an intermediate task, auto-annotation. Image retrieval
is performed by applying text retrieval techniques over the
textual outputs of the auto-annotation model. Therefore, their
learning procedure does not maximize a criterion related to the
ﬁnal retrieval performance, instead it maximizes a criterion related to the annotation performance. In this work, we adopt an
alternative approach and introduce a model to learn an image
retrieval model directly, without relying on auto-annotation.
The proposed model, Passive-Aggressive Model for Image
Retrieval (PAMIR), adopts a learning criterion related to
the ﬁnal retrieval performance, based on recent advances on
discriminative learning for text retrieval , , . PAMIR
learning approach hence takes as input a set of training queries,
as well as a set of pictures, and outputs a trained model likely
to achieve high ranking performance on new data. Moreover,
PAMIR also enjoys an efﬁcient learning algorithm, which
builds upon recent work on online learning of kernel-based
classiﬁers . The advantages of the proposed approach are
several: our model parameterization can beneﬁt from effective kernels for pictures comparison, while its optimization
procedure permits an efﬁcient learning over large training
sets. Furthermore, our ranking criterion yields a discriminative
retrieval model that does not rely on an intermediate annotation
task, which is theoretically appealing . These advantages
are actually supported by our experiments, in which PAMIR is
shown to outperform various state-of-the-art alternatives. For
instance, the precision at top 10 of PAMIR reaches 10% over
the Corel dataset , which should be compared to 9.3% for
SVM for concept classiﬁcation, the best alternative.
The remainder of this paper is organized as follows. Section II brieﬂy describes previous related research. Section III
introduces the proposed approach. Section IV presents the
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
features used for image and query representation. This section
also describes different picture kernels from which PAMIR
could beneﬁt. Section V reports the experiments comparing
PAMIR to the alternative approaches. Finally, Section VI
draws some conclusions.
II. RELATED WORK
With the advent of the digital photography era, image
retrieval has increasingly received attention. This study focuses
on an important part of this research domain, the query-by-text
task. This task aims at identifying the pictures relevant to a few
word query, within a large picture collection. Solving such a
problem is of particular interest from a user perspective since
most people are used to efﬁciently access large textual corpora
through text querying and would like to beneﬁt from a similar
interface to search collections of pictures. In this section, we
brieﬂy describe prior work focussing on this task.
So far, the query-by-text problem has mainly been addressed
through automatic annotation approaches. In this case, the
objective is to learn a model that can predict textual annotations from a picture. Such a model permits the retrieval
of unlabeled pictures through the application of text retrieval
techniques over the auto-annotator outputs. In the following,
we brieﬂy describe the two main types of approaches adopted
in this context, concept classiﬁcation and bi-modal generative
A. Concept Classiﬁcation
Concept classiﬁcation formulates auto-annotation within a
classiﬁcation framework. Each vocabulary term t, also referred
as a concept, deﬁnes a binary classiﬁcation problem, whose
positive examples are the pictures for which the term t should
appear in the predicted annotation. In this case, the learning
procedure hence consists in training a binary classiﬁer for each
vocabulary term, and each classiﬁer is learned to minimize the
error rate of its concept classiﬁcation problem.
Efforts in concept classiﬁcation started with the detection
of simple concepts such as indoor/outdoor , or landscape/cityscape . Then, signiﬁcant research has been directed towards detecting more challenging concepts, notably
in the context of the TREC video benchmark . Large sets
of various concepts have then been addressed in recent work,
such as , . Nowadays, popular approaches in concept
classiﬁcation mainly relies on large margin classiﬁers, such as
Support Vector Machines (SVM) , , or boosting
approaches . SVM for concept classiﬁcation constitutes
the state-of-the-art for single word queries. In this application
scenario, the images of the test corpus are ranked according to
the conﬁdence scores outputted by the classiﬁer corresponding
to the query term , . However, in the case of multiple
word queries, concept classiﬁers are more difﬁcult to apply
since the independent training of each concept classiﬁer requires to further deﬁne fusion rules to combine the scores
of the different concept classiﬁers , . compares
different fusion strategies and concludes that, for query-bytext tasks, it is generally effective to compute the average
of the score of the concept classiﬁers corresponding to the
query terms, after having normalized their mean and variance.
Therefore, we will adopt this fusion procedure latter in our
experiments. As an alternative to such ad-hoc fusion strategies,
bi-modal generative approaches have been introduced to learn
a single model over the whole vocabulary, yielding a solution
which can natively handle multiple-word queries.
B. Bi-Modal Generative Models
Contrary to concept classiﬁcation, bi-modal generative approaches do not consider the different vocabulary words in
isolation. Instead, these approaches model the joint distribution
P(c, p) of the textual caption (c) and the picture visual features
(p), P(c, p). The parameters of such a distribution are typically
learned through maximum likelihood training, relying on a
set of picture/caption pairs. After this learning phase, the
retrieval of unlabeled pictures can be performed by ranking
the pictures according to their likelihood P(p|q) given query
q, which is derived from the joint P(q, p) through Bayes
rule. Alternatively, it is also possible to estimate a conditional
multinomial over the vocabulary {P(t|p), ∀t ∈V }, for each
unlabeled picture. This enables to retrieve pictures through the
application of text retrieval techniques over the inferred multinomials. In this case, each multinomial P(·|p) is considered to
represent a textual item, in which the number of occurrences
of term t is proportional to P(t|p). This alternative retrieval
technique is generally preferred since it is more efﬁcient (the
multinomials need to be inferred only once for all queries)
and it has shown to be more effective .
Several approaches based on the bi-modal generative framework have been proposed in the recent years. These models mainly differ in the types of distributions chosen to
model textual and visual features, as well as in the way
they model the dependencies between both modalities. In
the following, we have chosen to brieﬂy describe three such
models, Cross-Media Relevance Model (CMRM) , Cross-
Media Translation Table (CMTT) and Probabilistic Latent
Semantic Analysis (PLSA) . A longer survey could also
have described alternative models such as Multimodal Hierarchical Aspect Model , , Multiple Bernoulli Relevance
Model or Latent Dirichlet Allocation . However, for
the sake of brevity, we decided to focus on models that have
shown to be the most effective over the Corel dataset .
Cross Media Relevance Model (CMRM) , is inspired
by Cross-Lingual Relevance Model , considering caption
of an image as the translation of its visual properties into
words. In this model, it is assumed that the visual properties
of an image are summarized as a set of discrete visual features.
Formally, the visual features of a picture p are represented as
1,p, . . . , tf v
where tf v
i,p refers to the number of features of type i in picture
p and |C| is the total number of feature types.
Given such a representation, CMRM infers a multinomial
P(t|ptest) over the vocabulary for any test picture ptest. For
that purpose, the joint probability of term t and all the visual
elements of ptest is estimated by its expectation over the
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
training pictures in Ptrain,
P(t, ptest) =
P(j) · P(t, ptest|j).
It is then assumed that terms and visual elements are independent given a training picture, leading to
P(t, ptest) =
P(j) · P(t|j)
P(v|j)tf v
In this equation, the probability of a training picture P(j)
is assumed to be uniform over the training set, i.e. P(j) =
1/|Ptrain|, while the probability of a term given a training
picture P(t|j) and the probability of a visual element given
a training pictures P(v|j) are estimated through maximum
likelihood, smoothed with the Jelinek-Mercer method .
From (1), P(t|ptest) can then be estimated through Bayes
rule, P(t|ptest) = P(t, ptest)/P(ptest). Although simple, this
approach has shown to be more effective compared to other
approaches inspired by translation models, e.g. .
Cross Media Translation Table (CMTT) also builds upon
cross-lingual retrieval techniques . This model considers
textual terms and discrete visual features, or visterms, as words
originating from two different languages and constructs a
translation table containing P(t|v) for any pair of term/visterm
(t, v). This table allows for the estimation of P(t|ptest) for any
term t and any picture ptest:
P(t|ptest) =
P(t|vi)P(vi|ptest),
where P(vi|ptest) =
i,ptest , and v1, . . . , vm are the
visterms of ptest.
The translation table {P(t|v), ∀t, v} is built from the training data Dtrain according to the following process. First,
each term i (and each visterm j) is represented by a |Dtrain|
dimensional vector, ti (vj), in which each component k is the
weight of term i (visterm j) in the kth training example. As
a noise removal step, the matrix M = [t1, . . . , tT , v1, . . . , vV ]
containing all term and visterm vectors is approximated with
a lower rank matrix, M ′ = [t′
1, . . . , t′
1, . . . , v′
V ], through
Singular Value Decomposition, and P(j|i) is ﬁnally deﬁned
k=1 cos(t′
Like CMRM, this method has also been evaluated over the
Corel corpus , where it has shown to be effective. The
use of Singular Value Decomposition has notably shown to
improve noise robustness. However, CMTT has also some limitations. In particular, cosine similarity can only model simple
relationships between terms and visual features. Approaches
modeling more complex relationships, such as Probabilistic
Latent Semantic Analysis , have subsequently been introduced.
Probabilistic Latent Semantic Analysis (PLSA) has ﬁrst
been introduced for text retrieval , before being extended
to image retrieval . This model introduces the following
conditional independence assumption: “terms and discrete
visual features are independent from pictures conditionally to
an unobserved discrete variable zk ∈{z1, . . . , zK}” (zk is
called an aspect variable and the hyperparameter K is referred
to as the number of aspects). In this framework, the probability
of observing a term t or a visual feature v in a picture p follows
P(zk|p)P(t|zk),
P(zk|p)P(v|zk).
The different parameters of the model can be estimated relying
on a two-step process. First, the probabilities P(p), P(zk|p)
and P(t|zk) for all p ∈Ptrain are estimated to maximize
the training caption likelihood through the Expectation Maximization algorithm. Then the probabilities P(v|zk), ∀v, k are
ﬁtted to maximize the training picture likelihood, keeping
P(p) and P(zk|p) ﬁxed. For test pictures without caption,
the probabilities P(p), P(zk|p) are estimated to maximize
the picture likelihood, keeping P(v|zk), ∀(v, k) to the values
estimated during training. After this procedure, (2) is applied
to infer P(p, t) for any test picture p and any term t. Similarly
to CMRM, Bayes rule can then derive P(t|p) from P(p, t).
This model has several strengths: the latent aspect assumption allows one to model more complex dependencies between
term and visual features, compared to CMRM or CMTT.
Moreover, the two step training procedure biases the latent
space toward the text modality, yielding better performance
than less constrained latent models .
In absence of manual annotations, bi-modal generative
models constitute the state-of-the-art for the retrieval of images from multiple-word queries, while, as mentioned above,
concept classiﬁcation is generally preferred for single word
queries. However, one could wonder whether it is possible
to provide a single solution for both settings. More fundamentally, one can also question the auto-annotation framework
on which both types of approaches are based. In both cases,
model training aims at solving an auto-annotation problem:
for concept classiﬁcation, the learning objective is to minimize
the number of false positives (predicting a word which does
not occur in the reference annotation) and false negatives
(not predicting a word occurring in the reference annotation),
while, for bi-modal generative models, the learning objective
is to maximize the likelihood of the training picture/caption
pairs. None of those criteria is tightly related to the ﬁnal
retrieval performance and there is hence no guarantee that a
model optimizing such annotation objectives also yields good
retrieval rankings.
In order to address those issues, we propose a discriminative
ranking model for the query-by-text problem. The proposed
approach is based on recent work on discriminative learning
for the retrieval of text documents , , . It learns
a retrieval model with a criterion related to the ranking
performance over a set of training queries. To the best of our
knowledge, this is the ﬁrst attempt to address the query-by-text
problem directly, without solving an intermediate annotation
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
III. PASSIVE-AGGRESSIVE MODEL FOR IMAGE
This section introduces our discriminative model for the retrieval of images from text queries, Passive Aggressive Model
for Image Retrieval (PAMIR). It ﬁrst formalizes the query-bytext problem before introducing PAMIR parameterization and
learning objective. Finally, it explains how the proposed linear
model can be applied to infer non-linear decision functions
relying on kernels.
A. Formalizing the Query-by-Text Problem
In the query-by-text problem, the retrieval system receives
a text query q, from the text space T , and a set of pictures
P, from the picture space P. It should then output a picture
ranking in which the pictures relevant to q would ideally
appear above the others, i.e.
∀p+ ∈R(q, P), ∀p−∈R(q, P), rk(q, p+) < rk(q, p−) (4)
where R(q, P) refers to the set of pictures of P that are
relevant to q, R(q, P) refers to the set of pictures of P that are
not relevant to q and rk(q, p) refers to the position of picture
p in the ranking outputted for query q. Our goal is hence
to learn a ranking model from training pictures Ptrain and
queries Qtrain such that the constraints of type (4) are likely
to be veriﬁed over new pictures Ptest and queries Qtest.
Similarly to most text retrieval approaches , we address
this ranking problem relying on a scoring function F. This
function F
: T × P →R assigns a real value F(q, p)
expressing the match between any query q and any picture
p. Our ranking approach is then simple: given a query q,
we compute the score of each picture p in the picture set
P, {F(q, p), ∀p ∈P}, and order the pictures by decreasing
scores. In this context, condition (4) translates to
∀p+ ∈R(q, P), ∀p−∈R(q, P), F(q, p+) > F(q, p−),
and our objective comes down to learning a function F
likely to verify (5) for unseen pictures Ptest and queries
Qtest. For that purpose, we introduce a parametric function
Fw along with an algorithm to infer the parameter w from
(Ptrain, Qtrain), so that Fw is likely to achieve this objective.
B. Model Parameterization
The parameterization of Fw is inspired from text retrieval,
Fw : T × P →R,
Fw(q, p) = q · fw(p),
fw refers to a parametric mapping from the picture space
P to the text space T , and · refers to the dot product in
the text space, which is commonly used to measure the
matching between textual vectors . In other words, our
scoring function Fw measures the match between a picture
p and a query q by ﬁrst projecting the picture into the text
space according to fw, before measuring the match between
the obtained textual vector fw(p) and the query q.
In the following, the form of fw is ﬁrst limited to linear
fw : P →T ,
fw(p) = (w1 · p, . . . , wT · p)
Fig. 1. PAMIR ranking strategy: in this example, the pictures of {p1, p2, p3}
are ranked p2, p3, p1 in answer to the query q. This ﬁgure illustrates
that the pictures are ranked according to the order of the projections of
{γ(q, p1), γ(q, p2), γ(q, p3)} along the direction of w.
and w = (w1, . . . , wT ) is a vector of PT , T being the
dimension of the text space T . Section III-E then shows that
the training procedure proposed thereafter can be extended to
non-linear mappings through the kernel trick.
C. Large Margin Learning for our Ranking Problem
Our goal is to learn the parameter w such that Fw yields
high ranking performance over unseen test queries. For that
purpose, we ﬁrst introduce a geometric interpretation of Fw,
from which we can derive a margin maximization objective
suitable to our ranking task.
For any query q = (q1, . . . , qT ) ∈T and picture p ∈P, we
deﬁne γ(q, p) as the vector (q1p, . . . , qT p) of PT and rewrite
Fw(q, p) as w · γ(q, p), since
q · fw(p) = q · (w1 · p, . . . , wT · p)
wt · (qtp) = w · γ(q, p).
Hence, we can interpret Fw(q, p) as the projection of γ(q, p)
onto the vector w. This means that PAMIR ranks the pictures
of P according to the order of the projections of {γ(q, p), ∀p ∈
P} along the direction of w, see Figure 1. With such an
interpretation, one can easily remark that only the direction
of w determines whether the constraints of type (5), ∀q ∈
T , ∀p+ ∈R(q, P),
∀p−∈R(q, P), w · γ(q, p+) −w · γ(q, p−) > 0,
are veriﬁed since the norm of w has no inﬂuence on the sign
of w · γ(q, p+) −w · γ(q, p−).
Hence, we can arbitrarily constrain the weight vector to lie
on the unit circle U, and solve our learning problem by ﬁnding
a vector u ∈U that veriﬁes all training constraints. In other
words, we want to select the weight vector in the set
∀(q, p+, p−) ∈Dtrain,
u · γ(q, p+) −u · γ(q, p−) > 0}
where Dtrain refers to all triplets (q, p+, p−) such that q ∈
Qtrain, p+ ∈R(q, Ptrain), p−∈R(q, Ptrain).
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
When the training constraints are feasible (S ̸= ∅), any
weight vector of S yields perfect retrieval performance over
the training set. However, not all these solutions will yield
the same results over some new test data. In order to select
a vector of S likely to yield high generalization performance,
we introduce the notion of margin for our ranking problem.
For any vector u ∈S, we deﬁne its margin as
(q,p+,p−)∈Dtrain
u · γ(q, p+) −u · γ(q, p−),
which is, by deﬁnition of S, a positive quantity. This notion
of margin is inspired from the deﬁnition introduced in in
the context of ranked categorization.
Equipped with this deﬁnition, we now explain why large
margin solutions are preferable to ensure good generalization
performance. Given a test triplet (qtest, p+
test) composed
of a query qtest, a picture p+
test relevant to qtest and a picture
test non-relevant to qtest, we deﬁne R(qtest, p+
the smallest quantity that satisﬁes ∃(qtrain, p+
Dtrain s.t.
∥γ(qtrain, p+
train) −γ(qtest, p+
test)∥< R(qtest, p+
∥γ(qtrain, p−
train) −γ(qtest, p−
test)∥< R(qtest, p+
This deﬁnition implies that, ∀u ∈S,
|u · γ(qtrain, p+
train) −u · γ(qtest, p+
test)| < R(qtest, p+
|u · γ(qtrain, p−
train) −u · γ(qtest, p−
test)| < R(qtest, p+
since ∥u∥= 1. Therefore,
u · γ(qtest, p+
test) −u · γ(qtest, p−
(u · γ(qtest, p+
test) −u · γ(qtrain, p+
(u · γ(qtest, p−
test) −u · γ(qtrain, p−
(u · γ(qtrain, p+
train) −u · γ(qtrain, p−
can be bounded as,
test)−u·γ(q, p−
test) > −2R(qtest, p+
test)+m(u)
since u · γ(q, p+
train) −u · γ(q, p−
train) > m(u) by deﬁnition
of m(u). Consequently, any solution u ∈S for which the
margin m(u) is greater than 2R(qtest, p+
test) satisﬁes the
test constraint u · γ(q, p+
test) −u · γ(q, p−
test) > 0.
Therefore, we decide to focus on the selection of the weight
vector of S with the largest margin, as this weight is the most
likely to satisfy all the constraints of a given test set,
u∗= argmax
This maximization problem is actually equivalent to the following minimization problem
m(u)2 , s.t.
∀(q, p+, p−) ∈Dtrain, u · γ(q, p+) −u · γ(q, p−) > m(u)
and the introduction of the vector w =
m(u)u yields the
following formulation of the same problem,
w∈PT ∥w∥2,
s.t. ∀(q, p+, p−) ∈Dtrain, w · γ(q, p+) −w · γ(q, p−) > 1.
This formulation of our retrieval problem is similar to the
Ranking Support Vector Machine (RSVM) problem introduced in the context of text retrieval, even if the notion of
margin was not formalized as such in the case of RSVM.
Like for RSVM, we need to relax the training constraints
for the non-feasible case (S = ∅), which yields the following
optimization problem,
(q,p+,p−)∈Dtrain
∀(q, p+, p−) ∈Dtrain,
w · γ(q, p+) −w · γ(q, p−) > 1 −ξq,p+,p−
ξq,p+,p−≥0
where the hyperparameter C controls the trade-off between
maximizing the margin and satisfying all the training constraints. This problem (7) can equivalently be written as,
w∈PT ∥w∥2 + C
(q,p+,p−)∈Dtrain
l(w; q, p+, p−),
where ∀(q, p+, p−) ∈Dtrain,
l(w; q, p+, p−) = max(0, 1 −w · γ(q, p+) + w · γ(q, p−)),
D. An Efﬁcient Learning Algorithm
The resolution of problem (7) involves a costly optimization
procedure, if the RSVM approach is adopted. In fact, stateof-the-art techniques to solve this problem have a timecomplexity greater than O(|Dtrain|2) , where |Dtrain|
denotes the number of training constraints. As we would like
to handle large constraint sets, we derive an efﬁcient training
procedure by adapting the Passive-Aggressive (PA) algorithm,
originally introduced for classiﬁcation and regression problems . For our ranking problem, PA should minimize
L(w; Dtrain) =
(q,p+,p−)∈Dtrain
l(w; q, p+, p−).
while keeping ||w||2 small.
For that purpose, the algorithm constructs a sequence of
weight vectors (w0, . . . , wn) according to the following iterative procedure: the ﬁrst vector is set to be zero, w0 = 0 and, at
the ith iteration, the weight wi is selected according to the ith
training example (qi, pi+, pi−) and the previous weight wi−1,
wi = argmin
2∥w −wi−1∥2 + c l(w; (qi, pi+, pi−)).
Hence, at each iteration, we select the weight wi as a tradeoff between minimizing the loss on the current example
l(w; (qi, pi+, pi−)) and remaining close to the previous weight
vector wi−1. The aggressiveness parameter c controls this
trade-off. Based on , it can be shown that the solution
wi = wi−1 + τivi,
c, l(wi−1; (qi, pi+, pi−))
vi = γ(qi, pi+) −γ(qi, pi−).
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
The hyperparameter c is selected to maximize the performance
over some validation data Dvalid. The number of iterations n
is also validated: training is stopped as soon as the validation
performance stops improving. This early stopping procedure
actually allows one to select a good trade-off between satisfying all training constraints (i.e. minimizing the training loss
L(w; Dtrain)) and maximizing the margin (i.e. minimizing
∥w∥2). During the training process, it can be shown that,
while the training error is decreasing , ∥w∥2 tends to
increase, see Appendix. Hence, the number of iterations n
plays a role similar to C in RSVM (7), setting the trade-off
between margin maximization and training error minimization.
The introduced PA algorithm therefore solves our learning
problem with a time-complexity growing linearly with the
number of iterations n. The observed complexity, reported
later in Section V, actually shows that n grows much slower
than |Dtrain|2, a lower bound on RSVM time-complexity,
enabling PAMIR to address much larger constraint sets.
E. Non-Linear Extension
Our model parameterization is based on a linear mapping
fw from the picture space P to the text space T , see Eq. (6).
This parameterization can be extended to non-linear mappings
through the kernel trick, which allows PAMIR to beneﬁt from
effective picture kernels recently introduced in the computer
vision literature, e.g. , , . To kernelize PAMIR, we
show that its parameterization solely requires the evaluation
of dot products between picture vectors. For that purpose,
we prove that, in the weight vector w = (w1, . . . , wT ), each
subvector wt, ∀t, is a linear combination of training pictures.
This then implies that the evaluation of
fw(p) = (w1 · p, . . . , wT · p), ∀p ∈P,
only requires to compute the dot product between p and any
training picture. The proof that, ∀t, the vector wt is a linear
combination of training pictures is performed by induction
over the iterations of our training procedure: at the ﬁrst
iteration, the property is obviously veriﬁed since w0
t = 0, then
the update preserves the property since, wi
t is itself a linear combination of training pictures,
t (pi+ −pi−), see Eq. (10). Hence, at the last iteration
n, wt = wn
t veriﬁes the property. This means that we can
rewrite wt as wt = P|Ptrain|
αt,jpj, where ∀j, αt,j ∈R.
Consequently, we can introduce any kernel function k : P ×
P →R, and rewrite fw as,
∀p ∈P, [fw(p)]t =
αt,jk(pj, p),
where [fw(p)]t denotes the tth component of the fw(p) vector.
Practically, in this kernelized case, each wt is stored as as
a support set, consisting of pairs (αt,j, pj). The following
section notably discusses different types of kernels suitable
for our task.
This section has introduced PAMIR, a model suitable for
image retrieval from text queries. This model has several
advantages compared to the previous approaches presented
in Section II: unlike SVM for concept classiﬁcation, PAMIR
can natively handle multiple-word queries, without requiring
any fusion strategy; unlike bi-modal generative models, it
relies on margin maximization training and hence enjoys
good generalization properties . More importantly, unlike
both SVM for concept classiﬁcation and bi-modal generative
models, PAMIR training relies on a ranking criterion related
to the ﬁnal retrieval performance of the model. This criterion
yields a discriminative retrieval model, which does not learn
from textual annotations, but directly from training queries
with pictures assessed for relevance.
IV. TEXT AND VISUAL FEATURES
This section introduces both the representation of text
queries, and the representation of pictures, along with kernel
functions suitable for picture comparison.
A. Query Representation
The bag-of-words framework is borrowed from text retrieval
 for query representation. In this context, a vocabulary V
is given prior to training to deﬁne the set of allowed words.
Then, the bag-of-words representation neglects word ordering
and assigns each query as a vector q ∈RT , where T denotes
the vocabulary size. The ith component qi of this vector is
referred to as the weight of term i in the query q. In our case,
it is deﬁned as the normalized idf weighting scheme ,
j=1(bj,q idfj)2
where bi,q is a binary weight, denoting the presence (bi,q = 1)
or absence (bi,q = 0) of i in q, and idfi is the inverse document
frequency of i. This latter quantity is deﬁned based on a
reference corpus, such as an encyclopedia, and corresponds
to idfi = −log(ri), where ri refers to fraction of corpus
documents containing term i. This weighting hypothesizes
that, among the terms present in q, the terms appearing rarely
in the reference corpus are more discriminant and should be
assigned higher weights.
B. Picture Representation
The representation of pictures for image retrieval is a
research topic in itself, and different approaches have been
proposed in the recent years, e.g. , , . Contrary to
the well-established bag-of-words representation for text data,
there is not yet a single image representation that would be
adequate for a wide variety of retrieval problems. However,
among the proposed representations, a consensus is emerging
on using local descriptors for various tasks, e.g. , .
This type of representation segments the picture into regions
of interest, and extracts visual features from each region.
The segmentation algorithm as well as the region features
vary among approaches, but, in all cases, the image is then
represented as a set of feature vectors describing the regions of
interest. Such a set is often called a bag-of-local-descriptors.
This study also adopts the local descriptor framework. Our
features are extracted by dividing each picture into overlapping
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
Neighborhood Intensities
Binary Tests
8-bit Sequence
An example of Local Binary Pattern (LBP8,2). For a given pixel,
the Local Binary Pattern is a 8-bit code obtained by verifying whether the
intensity of the pixel is greater or lower than its 8 neighbors.
square blocks, and each block is then described with edge and
color histograms. For edge histograms, we rely on uniform
Local Binary Patterns . These texture descriptors have
shown to be effective on various tasks in the computer vision
literature , , certainly due to their robustness with
respect to changes in illumination and other photometric
transformations . Local Binary Patterns assign the texture
histogram of a block by considering differences in intensity
at circular neighborhoods centered on each pixel. Precisely,
we use LBP8,2 patterns, which means that a circle of radius
2 is considered centered on each block. For each circle, the
intensity of the center pixel is compared to the interpolated
intensities located at 8 equally-spaced locations on the circle,
as shown on Figure 2, left. These eight binary tests (lower or
greater intensity) result in an 8-bit sequence, see Figure 2,
right. Hence, each block pixel is mapped to a sequence
among 28 = 256 possible sequences and each block can
therefore be represented as a 256-bin histogram. In fact, it
has been observed that the bins corresponding to non-uniform
sequences (sequences with more than 2 transitions 1 →0
or 0 →1) can be merged, yielding more compact 59-bin
histograms without performance loss .
Color histograms are obtained by k-means clustering. The
color codebook is learned from the Red-Green-Blue pixels of
the training pictures, and the histogram of a block is obtained
by mapping each block pixel to the closest codebook color.
Finally, the histograms describing color and edge statistics
of each block are concatenated, which yields a single vector
descriptor per block. Our local descriptor representation is
therefore simple, relying on both a basic segmentation approach and simple features. Of course, alternative representations could have been used, e.g. , , . However,
this paper focuses on the learning model, and a benchmark of
picture representations is beyond the topic of this research.
C. Picture Kernels
Our model relies on a kernel function k : P × P →R over
the picture space P, as explained in Section III. Given our
picture representation, we hence need a kernel to compare bags
of local descriptors. Fortunately, several kernels comparing
sets of feature vectors have been proposed along with the
development of local descriptors , , .
Distribution Kernel approaches ﬁt a distribution p(v|p)
over the space of local descriptors for each picture p, and
then apply a kernel between distributions to compare pictures.
Such kernels includes the Bhattacharya kernel or the expected
likelihood kernel .
In this study, we ﬁt a Gaussian Mixture Model for each picture p through Expectation-Maximization, as proposed in .
Motivated by scalability issues, we ﬁt standard Gaussians on
the input space, not kernelized Gaussian mixtures like .
The learned distributions are then compared with the Expected
Likelihood Kernel (ELK),
kELK(p, p′) =
p(v|p) p(v|p′)dv,
which can be computed in closed form for Gaussian mixtures , .
Matching Kernel approaches rely on a minor kernel,
kl, that compares local descriptors. The kernel between two
sets of local descriptors, p = {dp,i}|p|
i=1 and p′ = {dp′,i}|p′|
is deﬁned as the average of the best-match-score between the
descriptors of p and p′,
kmatch(p, p′) = 1
ˆk(p, p′) + ˆk(p′, p)
ˆk(p, p′) = 1
kl(dp,i, dp′,j).
Formally, this function kmatch is not a true Mercer kernel, since
its Gram matrix is not always positive deﬁnite . However, in
practice, it can be used with SVM or PAMIR, without enjoying
the same theoretical guarantee as a true kernel . Empirically,
SVMs relying on this kernel have shown to be effective over
several object categorization tasks , , .
Visterm Kernel approaches explicitly represent the pictures
in a high dimensional vector space, where the linear kernel is
applied. For that purpose, each local descriptor of a picture p is
represented as a discrete index, called visual term or visterm,
and, like for text data, the picture is represented as a bag-ofvisterms vector, in which each component pi is related to the
presence or absence of visterm i in p.
The mapping of the descriptors to discrete indexes is performed according to a codebook C, which is typically learned
from the local descriptors of the training pictures through the
k-means algorithm , , . This study also applies this
standard strategy. The assignment of the weight pi of visterm
i in picture p is classical as well,
where tf v
i , the term frequency of i in p, refers to the number
of occurrences of i in p, while idf v
i , the inverse document
frequency of i, is deﬁned as −log(rv
i being the fraction
of training pictures containing at least one occurrence of i.
Each of the presented kernels proposes a different technique
to compare bags of local descriptors, whose effectiveness
highly depends on the application context. For our task, we
selected the most appropriate kernel through validation, as
explained in the next section.
V. EXPERIMENTS AND RESULTS
In this section, we present the experiments performed to
evaluate PAMIR. We ﬁrst describe our experimental setup,
and then discuss the various issues related to hyperparameter
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
coast water
people market
lighthouse cliff
food parasol
Examples of Corel pictures along with the associated captions.
selection, including the choice of a suitable kernel. Finally,
we report the experimental results comparing PAMIR to the
alternative models presented in Section II.
A. Experimental Setup
The datasets used for evaluation originate from stock photography, one of the application context of query-by-text image
retrieval. Data from other domains, such as web search engine
or newspaper archive, could also have been used. However,
we decided to focus on stock photography, since the annotations associated with such pictures are generally produced
by professional assessors with well deﬁned procedures, which
guarantees a reliable evaluation.
Two datasets are used in our experiments, CorelSmall
and CorelLarge. Both sets originate from the Corel stock
photography collection1, which offers a large variety of pictures, ranging from wilderness scenes to architectural building
pictures or sport photographs. Each picture is associated with
a textual caption that depicts the main objects present in the
picture, see Figure 3.
CorelSmall corresponds to the 5, 000-picture set presented
in . This set, along with the provided split between
development and test data, has been used extensively in the
query-by-text literature, e.g. , , . It is composed of
a 4, 500-picture development set P s
dev and a 500-picture test
test. For model training and hyperparameter selection, we
further divided the development set into a 4, 000-picture train
train and a 500-picture validation set P s
valid (see Table I).
The queries needed to train and evaluate our model originate
from the caption data. For that purpose, we ﬁrst deﬁned the
relevance assessments considering that a picture p is relevant
to a query q if and only if the caption of p contains all query
words. Then, we deﬁned the query set, Qs
valid, or Qs
the set containing all the queries for which there is at least one
relevant picture in the picture set, P s
train, P s
valid, or P s
test. This
strategy deﬁning queries and relevance assessments is hence
not identical to a labeling in which a human assessor issues
queries and labels pictures. However, it is based on manually
produced captions and the resulting relevance information can
be considered as reliable. In fact, there is no doubt that the
pictures marked as relevant according to the deﬁnition above
are indeed relevant, e.g. if the words beach, sky are present in
a caption, it can conﬁdently be claimed that the corresponding
1Corel data are distributed through
 
CO R E LSM A L L STATISTICS
Number of pictures
Picture size
384x256 or 256x384
Number of queries
Avg. # of rel. pic. per q.
Vocabulary size
Avg. # of words per query
CO R E LLA R G E STATISTICS
Number of pictures
Picture size
384x256 or 256x384
Number of queries
Avg. # of rel. pic. per q.
Vocabulary size
Avg. # of words per query
picture is relevant to the queries “beach”, “sky” and “beach
sky”. The only problem that could affect our relevance data
is due to the possible incompleteness of some captions. If a
word is missing from a caption, the corresponding picture will
wrongly be marked as non-relevant for all queries containing
this word. This weakness is however not speciﬁc to our labeling process. For instance, system pooling, the semi-automatic
technique used for labeling data in retrieval benchmarks, also
underestimates the number of relevant documents .
CorelSmall statistics are summarized in Table I. The
datasets are used as follows: the parameter vector w is learned
train) through the training procedure deﬁned in
Section III. Hyperparameters, such as the number of training iterations, or the type of kernel used, are selected over
valid). Final evaluation is conducted over (P s
The training and evaluation of the alternative models is also
performed over to the exact same data split, as it is the only
way to conduct a fair comparison between the models .
The second dataset, CorelLarge, contains 35, 379 images
and hence corresponds to a more challenging retrieval problem
than CorelSmall. Like for the smaller set, CorelLarge
pictures originate from the Corel collection and CorelLarge
queries have been deﬁned relying on the picture captions as
explained above. The statistics of the training, validation and
test sets of CorelLarge are reported in Table II.
For both datasets, performance evaluation has been conducted relying on standard information retrieval measures: average precision, precision at top 10, and break-even point .
For any query q, these measures evaluate the picture ranking
outputted by the retrieval system as follows.
Precision at top 10 pictures (P10) measures the percentage
Pr(10) of relevant pictures within the top 10 positions of
the ranking. P10 hence evaluates the percentage of relevant
material a user would encounter on the ﬁrst 10–result page of
a search engine. Although it is easy to interpret, this measure
tends to overweight simple queries with many relevant pictures
when averaging over a query set. For such queries, it is easier
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
to rank some relevant pictures within the top 10, simply
because the relevance set is larger and not because of any
property of the ranking approach.
Break-Even Point (BEP), often called R-Precision, measures
the percentage Pr(|R(q)|) of relevant pictures within the
top |R(q)| ranking positions, where |R(q)| is the number of
relevant pictures for the evaluated query q. Contrary to P10,
this measure does not overweight queries with many relevant
Average Precision (AvgP) is the standard measure used for
retrieval benchmark , and it corresponds to the average of
the precision at each position where a relevant picture appears,
p∈R(q) Pr(rk(q, p)), where rk(q, p) is the
rank of picture p for query q.
In the following, we report the performance of PAMIR and
the alternative models as the average of these measures over
the sets of test queries Qs
test and Ql
B. Hyperparameter Selection
This section studies the enﬂuence of the hyperparameters
on PAMIR performance. The feature extractor parameters, the
type of kernel used, and the learning algorithm parameters are
selected through validation: the model is trained with different
parameter values over the training set and the parameters
achieving the highest average precision over the validation
set. For CorelSmall, all types of parameters are validated.
For CorelLarge, only the learning parameters are validated
for efﬁciency reasons, keeping the feature extractor and kernel
parameters to the value selected over CorelSmall.
Feature extraction requires to select the block segmentation
parameters (block size and block overlap) and the number of
clusters used for color quantization. The block size determines
the trade-off between obtaining local information (with small
blocks) and extracting reliable statistics for each block (with
large blocks), this parameter is selected through validation.
Block overlap is set to half the block size such that all
pixels belong to the same number of blocks, to avoid the
predominance of pixels located at the block borders. The
number of color bins is set to 50, as a trade-off between
extracting a compact block representation and obtaining a
perceptually good image reconstruction. Table III reports the
validation performance for different block sizes. These results
show that large blocks (> 128 pixels) are not suitable for our
retrieval problem. In fact, it seems that considering less than
15 local descriptors per image does not provide PAMIR with
enough statistics to address the retrieval task. The performance
is stable for small blocks, between 32 and 96 pixels, with a
slight advantage for 64 pixel blocks. We therefore pick this
latter value for evaluation.
The selection of the kernel is also performed through
validation. In fact, the different kernels comparing bag-oflocal descriptors have been proposed recently and few studies
focused on the empirical comparison of these approaches .
Table IV reports the best validation performance for each
kernel, along with its parameters. Among the three kernels
evaluated, the visterm kernel is clearly yielding the best performance, followed by the match kernel and then the Expected
Likelihood Kernel. These results yields several remarks.
SELECTING THE BLOCK SIZE OVER (Qs
VALID, P s
The other hyperparameters (kernel and learning parameters) are set to their
optimal validation value.
block size
blocks per pic.
AvgP (valid.)
The Expected Likelihood Kernel (ELK) over Gaussian
mixtures surprisingly yields its best results with only a single
Gaussian per picture. This observation is not in line with the
handwritten digit recognition experiments reported in .
Even if the differences in the datasets and the tasks performed
might explain this difference, we further investigated on this
point. In fact, the non-convex Expectation-Maximization procedure seems to explain the failure of ELK over Gaussian
mixtures. The ﬁtting of a mixture over the same picture with
different initializations yield similar distributions in terms of
data likelihood. However, these distributions are not equivalent
for ELK evaluations and large relative variations are observed
for a given pair of pictures, depending on the initialization
of the Expectation-Maximization procedure for these pictures.
This effect could possibly be reduced through averaging, if one
ﬁts multiple mixtures per picture. However, such a solution
would be too costly for large datasets.
The performance of the match kernel is reported to be higher
than the ELK. The match kernel relies on a minor kernel
to compare pairs of local descriptors. In our experiments,
the linear kernel, the Radial Basis Function (RBF) kernel,
and the polynomial kernel have been tried as minor kernels.
Table IV reports results only for the RBF kernel, which yielded
the highest validation performance. Regarding efﬁciency, the
match kernel is computationally demanding as it needs to
compare all pairs of local descriptors between two pictures.
The visterm kernel is reported to yield the highest validation
performance and optimal performance is reached with a codebook of 10, 000 prototypes. Moreover, the visterm approach
also yields a more efﬁcient model, compared to the other
kernels. In fact, the visterm framework represents the pictures
as bag-of-visterms vectors, where the linear kernel is applied.
This means that the picture vectors can be pre-computed,
as soon as the pictures are available. Then, model training
and testing only require the evaluations of the linear kernel
between sparse vectors. Such an operation can be performed
efﬁciently as its complexity only depends on the number
of non-zero components of the vectors (bounded by 77, the
number of blocks per image), not on the data dimension
(10, 000, the codebook size) . Furthermore, the linear kernel
allows for handling w explicitly, which involves much less
computation than handling support sets.
The training parameters of PAMIR are the number of
iterations n and the aggressiveness c. Both of them sets the
trade-off between the two learning objectives, i.e. minimizing
the training loss and identifying a large margin model. Table V reports the selected values. For both CorelLarge and
CorelSmall, the number of iterations is signiﬁcantly lower
than the number of training constraints (e.g. for CorelSmall,
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
SELECTING THE KERNEL OVER (Qs
VALID, P s
The other hyperparameters (feature extractor and learning parameters) are set
to their optimal validation value.
Parameters
Exp. Likelihood
num. of Gaussians per picture (1)
stdv of the local RBF kernel (5)
Visterm-Linear
codebook size (10, 000)
SELECTING THE PARAMETERS OF THE LEARNING PROCEDURE.
The other hyperparameters (feature extractor and kernel parameters) are set
to their optimal CorelSmall validation value.
Aggressiveness c
Num. of iter. n
CorelSmall
2.53 × 106
CorelLarge
1.55 × 107
2.53×106 iterations should be compared to 1.45×108 training
constraints). The algorithm hence converges before examining
all the training set, which is certainly due to some redundancy
in the training data. This highlights the efﬁciency of the
PA approach, compared to other optimization techniques for
SVM-like problems, as discussed in Section III.
To conduct a fair comparison, the alternative models have
been trained over the same local descriptors and their hyperparameters have been selected with the same validation
procedure. Namely, we selected the block size (for all models),
the visual codebook size (for CMRM, CMTT and PLSA),
and the kernel along with the corresponding parameters (for
concept classiﬁcation SVM) based solely on the validation set
of CorelSmall, while all other parameters have been validated
for both CorelSmall and CorelLarge, see Table VI. Note
that Table VI does not report the regularization parameter (C)
for the SVM as it has been individually tuned for each term.
Before presenting the generalization performance, we
brieﬂy compare the computational time required by the different models, for both indexing and retrieval. Table VII reports
the indexing times needed by PAMIR and the alternative
models. Indexing corresponds to all the computations performed prior to the submission of the test queries, once the
test pictures are available, excluding the operations related to
feature extraction, such as visterm quantization. Indexing can
hence be performed off-line, before the user can interact with
the system. In the case of PAMIR, it includes the training step,
plus the mapping of each test picture to the text space. For
bi-modal generative models (CMRM, CMTT and PLSA), it
corresponds to model training, plus the inference of p(t|p) for
each vocabulary term t and each test picture p. In the case of
concept classiﬁcation SVM, it corresponds to the training of an
SVM for each vocabulary term, and the classiﬁcation of each
test image according to each of the trained SVMs. Table VII
shows that our efﬁcient training procedure yields an indexing
time of the same order as the most efﬁcient model, CMRM.
This table also shows that SVM for concept classiﬁcation
is especially costly: this approach involves training a model
for each vocabulary term, and each model training has a
complexity that grows at least quadratically with the training
INDEXING TIMES FOR PAMIR AND THE ALTERNATIVES MODELS
Execution times have all been measured in seconds on the same machine
(AMD Athlon64, 2.4Ghz, 2GB RAM).
CorelSmall
CorelLarge
TABLE VIII
RETRIEVAL TIMES FOR PAMIR AND THE ALTERNATIVES MODELS
All models have the same retrieval complexity. Execution times have all been
measured on the same machine (AMD Athlon64, 2.4Ghz, 2GB RAM).
CorelSmall
0.34 ms per que
CorelLarge
7.94 ms per que
set size . This makes the application of this technique
challenging for large datasets such as CorelLarge. Of course,
the reported times highly depend on implementation details
and optimization tricks2, and should be considered carefully.
It should also be noted that the reported times correspond to
a single run of training, while, in a real-world usage scenario,
a variable number of runs might be required depending on
the number of hyperparameter values selected for validation.
However, the results clearly indicate that indexing a corpus
with PAMIR is not more costly than indexing a corpus with the
other models. After indexing, all models then need to compute
the dot-product matching between the submitted query and the
textual representations inferred from the text pictures, before
ranking the obtained scores. All models hence yield the same
retrieval time, 0.34 msec per query for CorelSmall and 7.94
msec per query for CorelLarge, on our reference machine,
see Table VIII. This hence means that all models can be used
interactively by the user, without any perceived delay.
C. Experimental Results
This section evaluates PAMIR and the alternative models
over the test parts of CorelSmall and CorelLarge.
Table IX, which reports the results over CorelSmall, shows
that PAMIR outperforms all the alternative evaluated models.
Compared to the best alternative, SVM, a relative improvement
of 21% is reported for AvgP (26.3% for PAMIR versus
22.0% for SVM). Improvements are also observed for the
other measures, P10 and BEP, which means that the use of
PAMIR is advantageous for both users focussing on the ﬁrst
positions of the ranking (as shown by P10 results) or users
focussing on the whole ranking (as shown by AvgP results).
One should note that the relatively low values reported for
the P10 results does not indicate a failure of the models
but reﬂects the difﬁculty of the task: in fact, the optimal
value for P10 is 20.2% due to the low number of relevant
pictures per query. This therefore means that the PAMIR user
focussing only on the ﬁrst ten results will retrieve about half
the pictures he would have retrieved using the ideal ranker.
In order to verify whether the observed advantage on the
2Our implementation of PAMIR is available at www.idiap.ch/pamir/.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
HYPERPARAMETERS FOR CMRM, CMTT, PLSA AND SVM
Hyperparameters
CorelSmall
block size (192), visual codebook size (3,000), smoothing parameters (α = 0.5, β = 0.1)
CorelLarge
block size (192), visual codebook size (3,000), smoothing parameters (α = 0.2, β = 0.1)
CorelSmall
block size (256), visual codebook size (2,000), number of singular values kept (50)
CorelLarge
block size (256), visual codebook size (2,000), number of singular values kept (1,000)
CorelSmall
block size (32), visual codebook size (50,000), number of aspects (400)
CorelLarge
block size (32), visual codebook size (50,000), number of aspects (600)
CorelSmall
block size (48), kernel (visterm kernel with a 20,000-visterm codebook)
CorelLarge
block size (48), kernel (visterm kernel with a 20,000-visterm codebook)
AVERAGED PERFORMANCE ON CO R E LSM A L L TEST QUERIES
Bold numbers report when a model outperforms all others according to the
Wilcoxon test at the 95% conﬁdence level.
AvgP (%) FOR EASY AND DIFFICULT QUERIES OF CO R E LSM A L L
The ‘easy’ query set contains the 421 test queries with 3 or more relevant
pictures, while the ‘difﬁcult’ query set contains the 1, 820 test queries with
only 1 or 2 relevant pictures. Bold numbers report when a model outperforms
all others according to the Wilcoxon test at the 95% conﬁdence level.
Easy Queries
Difﬁcult Queries
average results could be due to a few queries, we further
ran the Wilcoxon signed rank test to compare PAMIR and
each alternative model . This test examines the distribution
of the differences in the score obtained for each query and
veriﬁes whether it is symmetric around zero, which would
mean that PAMIR has actually no advantage over the alternative approach. The test rejected this hypothesis at the 95%
conﬁdence level for all alternative models and all measures,
as indicated by the bold numbers in the tables.
In order to compare the models over difﬁcult and easy
queries, we split the set of test queries into an ‘easy’ set,
containing the queries with 3 or more relevant pictures in
test, and a ‘difﬁcult’ set, containing the queries with only one
or two relevant pictures in P s
test. Table X reports the average
precision obtained over the two sets. PAMIR is shown to be the
best model over both sets and its advantage is reported to be
greater over the ‘difﬁcult’ set (on this set, the relative AvgP
improvement compared to SVM, the second best model, is
+29%, as compared to +3.2% over the ‘easy’ set). This
outcome is certainly due to PAMIR ranking criterion, since
previous work showed that similar criteria for classiﬁcation are
especially adapted to unbalanced problems, i.e. classiﬁcation
tasks with a low percentage of positive examples .
As a further comparison, Table XI reports the average
precision obtained over single and multiple-word queries
separately. Several previous papers focused on single-word
queries only, e.g. , , , and reporting those results
allows for direct comparison with this literature. The single-
AvgP (%) ON SINGLE & MULTI-WORD QUERIES OF CO R E LSMALL
CorelSmall contains 179 test queries with a single word and 2, 062 queries
with more than one word. Bold numbers report when a model outperforms
all others according to the Wilcoxon test at the 95% conﬁdence level.
Single-Word Que.
Multi-Word Que.
word queries correspond to an easier task since the average
number of relevant pictures per query is 9.4 for the single-word
queries, compared to 1.8 for the multiple-word queries. The
results reported in Table XI agree with this observation and
all models are reported to reach higher performance on the
single-word queries compared to multiple-word queries. On
both query subsets, the advantage of PAMIR is conﬁrmed. The
PAMIR improvement is shown to be greater for multiple-word
queries (+22.3% relative improvement in AvgP compared
to the second best model, SVM) than for single-word queries
(+4.0% relative improvement in AvgP compared to SVM).
Two characteristics of PAMIR might explain this outcome:
PAMIR training criterion has shown to be adapted to retrieval
problems with few relevant pictures, which is the case of
multiple-word queries. Moreover, PAMIR is the only model
trained over multiple-word queries, which certainly helps
achieving better performance over such queries. In fact, we
observed that, for multiple-word queries, the other models
often favor one of the query terms at the expense of the
others. Figure 4 shows, for instance, that SVM favors the term
‘car’ at the expense of ‘building’ for the query ‘building car’.
On this example, the SVM ranking provides only one picture
containing both cars and buildings, while PAMIR succeed in
retrieving all the 3 relevant pictures in the top 5 positions. The
PAMIR results even provide a non-relevant picture that could
have been labeled relevant with looser labeling instructions
(see the ﬁfth picture of the ranking). The other example on
Figure 4 is a single word query, ‘petals’. It yields good results
for both models, which retrieve 3 relevant pictures out of 4
in the top 5 positions. One can note a slight advantage for
PAMIR that returns only ﬂower-related pictures. Of course,
these examples have limited statistical values but they give an
idea on the type of ranking the user is facing.
With our setup, some queries appear in both the test and
train sets (for instance, single-word queries are common to
both sets). In order to verify the ability of PAMIR to generalize
to new queries, we evaluated our model on the 601 test queries,
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
RESULTS OVER TEST-ONLY QUERIES OF CO R E LSM A L L QUERIES
Among the 2, 241 test queries of CorelSmall, 601 queries are not appearing
in the training or in the validation set. Bold numbers report when a model
outperforms all others according to the Wilcoxon test at the 95% conﬁdence
building car
(4 relevant pictures in P s
(3 relevant pictures in P s
Example: the top 5 pictures obtained with PAMIR and SVM, for
two queries over CorelSmall. Higher resolution images, as well as other
examples, are available at www.idiap.ch/pamir/.
which are not present in the training set. These queries can
be considered as difﬁcult, not only because the model has not
seen pictures relevant to them during training, but also because
they have very few relevant documents (1.03 on average).
This second aspect can easily by explained if one remark that
test queries with many relevant test pictures are also likely
to have at least one relevant picture within the training data,
which means that such queries are likely to belong to the
training set as well. The results over this set of queries conﬁrm
the results observed on the whole set (see Table XII) and
PAMIR is reported to outperform the alternative according
to all measures. Moreover, for all models, the performance
is much lower than for the ‘difﬁcult’ query set (see Table X),
which indicates that generalization to new queries deserves to
be investigated further in the future.
Overall, the results over CorelSmall outline the advantage
of PAMIR over the alternative solutions. This outcome is certainly due to our discriminative learning strategy. The training
of the other models either maximizes the joint picture/caption
likelihood (CMTT, CMRM and PLSA) or minimizes the error
rate of the per-term classiﬁcation problems (SVM for concept
classiﬁcation), while our model relies on a ranking criterion,
related to the ﬁnal retrieval performance. This difference
has shown to be especially helpful for both difﬁcult queries
(queries with few relevant pictures) and multiple-word queries.
TABLE XIII
AVERAGED PERFORMANCE ON CO R E LLA R G E QUERIES
Bold numbers report when a model outperforms all others according to the
Wilcoxon test at the 95% conﬁdence level.
Table XIII reports the results of the experiments performed
over CorelLarge. The reported performance over this set are
much lower than for CorelSmall, which is not surprising
considering the difﬁculty of the task. In CorelLarge, the
relevant pictures account for 0.27 per thousand on average,
which should be compared to 4.7 per thousand on average
for CorelSmall. Moreover, the limited amount of relevant
material present in the training set of CorelLarge also makes
this task more difﬁcult: in CorelLarge, the average number
of relevant pictures per training query is 3.79, which should
be compared to 5.33 for CorelSmall (see Table I and II).
Hence, the models trained over CorelLarge should address
a more difﬁcult ranking problem, while having seen less
relevant pictures to generalize from. In fact, the statistics of
CorelLarge make this task closer to real world applications,
such as image search for stock photography or news wire
services, and the results over CorelLarge are hence of a
greater interest from a user perspective.
Although low, the results over CorelLarge are much higher
than random performance for all models (e.g. random performance is ∼0.03% for P10 which is much lower than 1.44%,
the worst P10 results, obtained with CMRM). All approaches
can hence leverage from the training data. In fact, even if the
models are far from optimal performance, they can still be
useful to the user, as illustrated by the two queries shown on
Figure 5. The ﬁrst example ‘tree snow people’ corresponds to
a relatively easy query with 13 relevant pictures in the test
set. Like for the ‘building car’ example on CorelSmall, the
SVM solution is dominated by one of the concepts, ‘snow’, at
the expense of the others, and does not retrieve any relevant
picture in the top 5. On the contrary, PAMIR, which is directly
trained from multiple-word queries, yields high performance
with 3 relevant pictures within the top 5 positions. The second
query ‘zebra herd’ has less relevant pictures (4 in the test
set). The results show a slight advantage for PAMIR: our
model retrieves two relevant pictures at the third and fourth
positions, while the SVM retrieves one relevant picture at the
ﬁfth position. This example illustrates that both models are
often confused by similar pictures (savannah scenes in this
case) for concepts with few training instances (only 22 pictures
contain zebras among the 14,861 pictures of P l
Like for CorelSmall, the results in Table XIII clearly
show the advantage of PAMIR over the other approaches.
In fact, model comparison yields similar conclusions over
CorelSmall and CorelLarge: CMTT and CMRM reach
comparable performance levels, PLSA performs better than the
other generative models, but not as well as the SVM. Again,
PAMIR yields the best results. Furthermore, the Wilcoxon
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, XXXX
tree snow people
zebra herd
(13 relevant pictures in P l
(4 relevant pictures in P l
Example: the top 5 pictures obtained with PAMIR and SVM for
two queries over CorelLarge. Higher resolution images, as well as other
examples, are available at www.idiap.ch/pamir/.
test over CorelLarge concludes that PAMIR signiﬁcantly
outperforms each alternative, at the 95% conﬁdence level,
for P10 and BEP. For AvgP, the test concludes that
PAMIR outperforms all generative models (CMTT, CMRM
and PLSA), and yields an AvgP similar to the SVM’s.
Overall, the results over both sets are consistent and show the
advantage of our discriminative model over the alternatives.
VI. CONCLUSIONS
We have proposed a discriminative approach to the retrieval
of images from text queries. In such a task, the model receives
a picture corpus P and a text query q. It should then rank the
pictures of P such that the pictures relevant to q appear above
the others. Contrary to previous approaches that generally
rely on an image auto-annotation framework, our learning
procedure aims at selecting the model parameters likely to
yield a high ranking performance over the unseen test data.
For that purpose, we introduced a loss inspired from ranking
SVM and formalized the notion of margin for our retrieval
problem. We then introduced a learning algorithm building
upon Passive-Aggressive (PA) minimization . The resulting model, Passive-Aggressive Model for Image Retrieval
(PAMIR), has several advantages: its learning objective is
related to the ﬁnal retrieval performance, its training procedure
is efﬁcient for learning over large datasets, and the model
parameterization can beneﬁt from effective picture kernels
recently introduced in the computer vision literature , ,
 . These advantages actually yield a model effective in
practice, as shown by our experiments over stock photography
data. For instance, over the standard Corel benchmark ,
PAMIR yields 26.3% average precision, which should be
compared to 22.0% for SVM for concept classiﬁcation, the
best alternative. Our model has notably shown to be especially
advantageous over multiple-word queries and difﬁcult queries
with few relevant pictures.
Although it outperforms the alternative models, PAMIR is
far from reaching perfect performance, especially over the
challenging CorelLarge data. Therefore, we plan to investigate several directions to improve our model. First, we plan
to modify PAMIR loss function to focus mainly on the top
of the ranking, as most users examine only the ﬁrst results.
An approach derived from could be applied to optimize
measures like P10. The loss could also be modiﬁed to optimize measures considering relevance assessments with gradual
relevance levels, such as Discounted Cumulative Gain .
Another useful extension would be the prediction of a cutoff rank, that is, a ranking position below which the user
is unlikely to encounter any relevant documents. Solutions
inspired from could help solving this problem. Finally, we
also plan to investigate further on the use of kernels for local
features. We want to model the spacial relationships between
local features , and adopt a multi-resolution approach .
The proposed model, along with the reported results, hence
advocate for addressing the image retrieval problem through
a discriminative ranking approach, and open several possible
directions of research to fully beneﬁt from this formalism.
This appendix shows that an upper bound on the norm
∥wi∥grows with the number of iterations i of the Passive
Aggressive algorithm. Precisely, it shows that ∥wi∥≤2 c ρ i,
where ρ corresponds to the radius of the training data
ρ = max(q,p)∈Dtrain γ(q, p).
The proof, inspired from , is conducted by induction
over the iteration i. At the ﬁrst iteration, the property is
satisﬁed, since w0 = 0. The update rule of wt also preserves the property. If we assume the property to be veri-
ﬁed at iteration i −1, i.e. ∥wi−1∥≤2 c ρ (i −1), we have
∥wi∥≤2 c ρ (i −1) + ∥τivi∥, according to the update rule
(10). By deﬁnition, τi is positive and smaller than c and hence
∥wi∥≤2 c ρ (i −1) + c∥vi∥. Furthermore, vi is deﬁned
as γ(qi, pi+) −γ(qi, pi−), which implies that ∥vi∥≤2ρ.
Consequently, ∥wi∥≤2 c ρ i. This concludes the proof.
ACKNOWLEDGMENTS
The authors thank Florent Monay for his advice. This work
has been supported by the Swiss NSF through the MULTI
project and the Swiss OFES through the PASCAL Network.