Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3433–3442
Brussels, Belgium, October 31 - November 4, 2018. c⃝2018 Association for Computational Linguistics
Multi-grained Attention Network for Aspect-Level
Sentiment Classiﬁcation
Feifan Fan1
Yansong Feng 12∗Dongyan Zhao13
1Institute of Computer Science and Technology, Peking University, China
2MOE Key Laboratory of Computational Linguistics, Peking University, China
3Beijing Institute of Big Data Research, China
{fanff, fengyansong, zhaody}@pku.edu.cn
We propose a novel multi-grained attention
network (MGAN) model for aspect level sentiment classiﬁcation.
Existing approaches
mostly adopt coarse-grained attention mechanism, which may bring information loss if the
aspect has multiple words or larger context.
We propose a ﬁne-grained attention mechanism, which can capture the word-level interaction between aspect and context.
then we leverage the ﬁne-grained and coarsegrained attention mechanisms to compose the
MGAN framework. Moreover, unlike previous works which train each aspect with its
context separately, we design an aspect alignment loss to depict the aspect-level interactions among the aspects that have the same
context. We evaluate the proposed approach
on three datasets: laptop and restaurant are
from SemEval 2014, and the last one is a twitter dataset. Experimental results show that the
multi-grained attention network consistently
outperforms the state-of-the-art methods on all
three datasets. We also conduct experiments to
evaluate the effectiveness of aspect alignment
loss, which indicates the aspect-level interactions can bring extra useful information and
further improve the performance.
Introduction
Aspect level sentiment classiﬁcation is a fundamental task in sentiment analysis , which aims to infer the sentiment polarity (e.g. positive, neutral, negative) of
sentence with respect to the aspects. For example, in sentence “I like coming back to Mac OS
but this laptop is lacking in speaker quality compared to my $400 old HP laptop”, the polarity of
the sentence towards the aspect “Mac OS” is positive while the polarity is negative in terms of aspect “speaker quality”.
∗corresponding author.
Many statistical methods, such as support vector machine (SVM) , are employed with welldesigned handcrafted features.
In recent years,
neural network models are studied to automatically learn low-dimensional representations for aspects and their context. Attention
mechanism is also be studied to characterize the effect of aspect on enforcing the model
to pay more attention on the important words of
the context. Previous works mainly employed the simple averaged aspect vector to learn the attention weights
on the context words. Ma et al. further
proposed the bidirectional attention mechanism,
which interactively learns the attention weights on
context/aspect words, with respect to the averaged
vector of aspect/context, respectively.
These above attention methods are all at the
coarse-grained level, which simply averages the
aspect/context vector to guide learning the attention weights on the context/aspect words.
simple average pooling mechanism might cause
information loss, especially for the aspect with
multiple words or larger context. For example, in
sentence “I like coming back to Mac OS but this
laptop is lacking in speaker quality compared to
my $400 old HP laptop”, the simple averaged vector of long context might lose information when
steering the attention weights on aspect words.
Similarly, the simple averaged vector of aspect
(i.e. “speaker quality”) may deviate from the intuitive core meaning (i.e. “quality”) when enforcing
the model to pay varying attentions on the context
words. From another perspective, previous works
all regard the aspect and its context words as one
instance, and train each instance separately. However, they do not consider the relationship among
the instances that have the same context words.
The aspect-level interactions among the instances
with same context might bring extra useful information.
Considering the above example, intuitively, the aspect “speaker quality” should pay
more attention on “lacking” and less attention on
“like”, compared with aspect “Mac OS”, since
they have different sentiment polarities.
In this paper, we propose a multi-grained attention network to address the above two issues in aspect level sentiment classiﬁcation.
Speciﬁcally, we propose a ﬁne-grained attention mechanism (i.e.
F-Aspect2Context and F-
Context2Aspect), which is employed to characterize the word-level interactions between aspect and
context words, and relieve the information loss
occurred in coarse-grained attention mechanism.
In addition, we utilize the bidirectional coarsegrained attention (i.e. C-Aspect2Context and C-
Context2Aspect) and combine them with ﬁnegrained attention vectors to compose the multigrained attention network for the ﬁnal sentiment
polarity prediction, which can leverage the advantages of them. More importantly, in order to make
use of the valuable aspect-level interaction information, we design an aspect alignment loss in the
objective function to enhance the difference of the
attention weights towards the aspects which have
the same context and different sentiment polarities. As far as we know, we are the ﬁrst to explore
the interactions among the aspects with the same
To evaluate the proposed approach, we conduct
experiments on three datasets: laptop and restaurant are from the SemEval 2014 Task 4 and the
third one is a tweet collection. Experimental results show that our method achieves the best performance on all three datasets.
Related Work
Aspect-level sentiment analysis is a branch of sentiment classiﬁcation, which requires considering
both the sentence and aspect information.
Traditional approaches regard
this task as the text classiﬁcation problem and design effective features, which are utilized in statistical learning algorithms for training a classi-
ﬁer. Kiritchenko et al. proposed to use
SVM based on n-gram features, parse features and
lexicon features, which achieved the best performance in SemEval 2014. Vo and Zhang 
designed sentiment-speciﬁc word embedding and
sentiment lexicons as rich features for prediction.
These methods highly depend on the effectiveness
of the laborious feature engineering work and easily reach the performance bottleneck.
In recent works, there are growing studies on
neural network based methods due to their capability of encoding original features as continuous and low-dimensional vectors without feature
engineering. Recursive Neural Network are studied to conduct semantic compositions on tree structures, and generate representations for prediction. Methods on LSTM were proposed to
model the context information and use an aggregated vector for prediction. TD-LSTM adopted LSTM to model the left
context and right context of the aspect, and concatenate them as the representation for prediction.
However, these works only focused on modeling the contexts without considering the aspects,
which performed an important role in estimate the
sentiment polarity.
Attention mechanisms are studied to enhance
the inﬂuence of aspects on the ﬁnal representation for prediction. Many approaches adopted the averaged aspect vector to learn the attention weights on the
hidden vectors of context words. Ma et al. 
further proposed bidirectional attention mechanism, which also learns the attention weights on
aspect words towards the averaged vector of context words.
These attention methods only consider the coarse-grained level attention, through
using the simple averaged aspect/context vector
to steer the attention weights learning on the context/aspect words, which might cause some information loss on the long aspect or context case.
In contrast, motivated by the bidirectional attention ﬂow approaches in machine comprehension, we propose
a ﬁne-grained attention mechanism which is responsible for linking and fusing information from
the aspect and the context words. Furthermore,
we leverage both the coarse-grained and ﬁnegrained attentions to compose the multi-grained
attention network (MGAN). In addition, existing
works train each instance separately. However, we
observe that the interactions among the aspects,
which have the same context words, could bring
extra useful information. Thus we design the aspect alignment loss in the objective function to depict such kind of relationship, which is the ﬁrst
work to explore the aspect-level interactions.
Our Approach
Task Deﬁnition
Given a sentence s = {w1, w2, · · · , wN} consisting of N words, and an aspect list A
{a1, · · · , ak}, where the aspect list size is k and
each aspect ai = {wi1, · · · , wiM } is a subsequence of sentence s, which contains M ∈[1, N)
words. Aspect-level sentiment classiﬁcation evaluates sentiment polarity of the sentence s with respect to each aspect ai.
We present the overall architecture of the proposed Multi-grained Attention Network (MGAN)
model in Figure 1. It consists of the Input Embedding layer, the Contextual Layer, the Multigrained Attention Layer and the Output Layer.
Input Embedding Layer
Input Embedding Layer maps each word to a high
dimensional vector space.
We employ the pretrained word vector, GloVe , to obtain the ﬁxed word embedding of
each word. Speciﬁcally, we denote the embedding
lookup matrix as L ∈Rdv×|V |, where dv is the
word vector dimension and |V | is the vocabulary
Contextual Layer
We employ a bidirectional Long Short-Term
Memory Network (BiLSTM) on top of the embedding layer to capture the temporal interactions
among words. Speciﬁcally, at time step t, given
the input word embedding x, the update process
of forward LSTM network can be formalized as
W i · [−→h t−1, −→x t] + −→b i)
W f · [−→h t−1, −→x t] + −→b f)
W o · [−→h t−1, −→x t] + −→b o)
gt = tanh(−→
W g · [−→h t−1, −→x t] + −→b g)
−→c t = ft ∗−→c t−1 + it ∗gt
−→h t = ot ∗tanh(−→c t)
Where σ is the sigmoid activation function, it, ft,
ot are the input gate, forget gate and output gate,
respectively.
Rd∗(d+dv),
−→b i, −→b f, −→b o, −→b g ∈Rd, and d is the hidden dimension size. The backward LSTM does the similar process and we can get the concatenated output
ht = [−→h t, ←−h t] ∈R2d. Given the word embeddings of a context sentence s and a corresponding
aspect aj, we will employ the BiLSTM separately
and get the sentence contextual output H ∈R2d∗N
and aspect contextual output Q ∈R2d∗M.
In addition, considering that the context words
with closer distance to an aspect may have higher
inﬂuence to the aspect, we utilize the position
encoding mechanism to simulate the observation.
Formally, the weight for a context word wj, which
has l word-level distance from the aspect (here we
treat the aspect phrase as a single unit), is deﬁned
as follows:
Speciﬁcally, we treat the weights of words within
the aspect as 0 in order to focus on the context
words in the sentence. Then we can obtain the
ﬁnal contextual outputs of context words H =
[H1 ∗w1, · · · , HN ∗wN].
Multi-grained Attention Layer
Attention mechanism is a common way to capture
the interactions between the aspect and context
words. Previous methods only adopt coarsegrained attentions, which simply use the averaged aspect/context vector as the guide to learn
the attention weights on context/aspect.
However, the simple average pooling in generating the
guide vector might bring some information loss,
especially for the aspect with multiple words or
larger context.
We propose the ﬁne-grained attention mechanism, which is responsible for linking and fusing information from the aspect and
context words.
This mechanism is designed to
capture the word-level interactions which estimate how each aspect/context word affect each
context/aspect word.
In addition, we concatenate both the ﬁne-grained and coarse-grained attention vectors to obtain the ﬁnal representation.
From other perspective, we observe the relationship among aspects can introduce extra valuable
information. Hence, we propose an aspect alignment loss, which is designed to strengthen the at-
Location Encoding
Alignment Matrix
Aspect Sentiment
Input Embedding
Contextual Layer
Multi-grained Attention
Output Layer
C-Aspect2Context
F-Aspect2Context
C-Context2Aspect
F-Context2Aspect
Aspect Alignment Loss
Figure 1: The architecture of the proposed multi-grained attention network.
tention difference among aspects with same context and different sentiment polarities.
Coarse-grained Attention
Coarse-grained attention is a widely used mechanism to capture the interactions between aspect
and context, which utilizes an averaged aspect
vector to steer the attention weights on the context words. Follow the work in ,
we employ the bidirectional attention mechanism,
namely C-Aspect2Context and C-Context2Aspect.
(1) C-Aspect2Context learns to assign attention scores to the context words with respect to the
averaged aspect vector. Here we employ an average pooling layer above aspect contextual output
Q to generate the averaged aspect vector Qavg ∈
R2d. For each word vector Hi in context, we can
compute the attention score aca
i as follows:
sca(Qavg, Hi) = Qavg ∗Wca ∗Hi
exp(sca(Qavg, Hi))
k=1 exp(sca(Qavg, Hk))
Where the score function sca computes the weight
which indicates the importance of a context word
towards aspect sentiment. Wca ∈R2d∗2d is the
attention weight matrix. Then the weighted combination of the context output mca ∈R2d is calculated as follows:
(2) C-Context2Aspect learns to assign attention weights on aspects words, which follows the
similar learning process with C-Aspect2Context.
We utilize the average pooling mechanism to obtain the averaged context vector Havg, and compute the weights for each word wi in the aspect
phrase. We compute the ﬁnal weighted combination of aspect vector mcc ∈R2d as follows:
scc(Havg, Qi) = Havg ∗Wcc ∗Qi
exp(scc(Havg, Qi))
k=1 exp(scc(Havg, Qk))
where W cc ∈R2d∗2d is the attention weight matrix.
Fine-grained Attention
As introduced above, we propose a ﬁne-grained attention mechanism to characterize the word-level
interactions and evaluate how each aspect/context
word affect each context/aspect word. Considering the previous example “I like coming back to
Mac OS but this laptop is lacking in speaker quality compared to my $400 old HP laptop”, the word
“quality” in aspect “speaker quality” should have
more effect on the context words compared with
word “speaker”. Accordingly, the context words
should pay more attention on “quality” instead of
“speaker”.
Formally, we deﬁne an alignment matrix U ∈
RN∗M, between the contextual output of and the
context H and the aspect Q, where Uij indicates
the similarity between i-th context word and j-th
aspect word. The similarity matrix U is computed
Uij = Wu([Hi; Qj; Hi ∗Qj])
Where Wu ∈R1∗6d is the weight matrix, [; ] is the
vector concatenation across row, ∗is the elementwise multiplication. Then we use U to calculate
the attention vectors in both directions.
(1) F-Aspect2Context estimates which context
word has the closest similarity to one of the aspect
word and are hence critical for determining the
sentiment. We can compute the attention weights
afa on context words by
= max(Ui,:)
k=1 exp(sfa
obtains the maximum similarity across
column. And then we can get the attended vector
mfa ∈R2d as follows:
(2) F-Context2Aspect measures which aspect
words are most relevant to each context word. Let
∈RM be the attention weights on aspect
contextual output Q with respect to the i-th context word vector Hi. The attended aspect vector
qfc ∈R2d∗N is deﬁned as follows:
k=1 exp(Uik)
Then we use an average pooling layer on qfc to get
the attended vector mfc ∈R2d:
mfc = Pooling([qfc
1 , · · · , qfc
Output Layer
At last, we concatenate both the coarse-grained
and ﬁne-grained attention vectors as the ﬁnal representation m ∈R8d, which will be fed to a softmax layer for determining the aspect sentiment polarity.
m = [mca; mcc; mfa; mfc]
p = softmax(Wp ∗m + bp)
where p ∈RC is the probability distribution for
the polarity of aspect sentiment, Wp ∈RC∗8d and
bp ∈RC are the weight matrix and bias, respectively. Here we set C = 3, which is the number of
aspect sentiment classes.
Model Training
Aspect Alignment Loss
Existing approaches train each aspect with its context separately, without considering the relationship among the aspects. However, we observe the
aspect-level interactions can bring extra valuable
information. In order to enhance the attention differences of aspects, which have the same context
and different sentiment polarities, we design the
aspect alignment loss on the C-Aspect2Context attention weights. C-Aspect2Context is employed to
ﬁnd the important context words in terms of a speciﬁc aspect. With the constraint of aspect alignment loss, each aspect will pay more attention on
the important words through the comparisons with
other related aspects. In terms of the previous example, the aspect “speaker quality” should pay
more attention on “lacking” and less attention on
“like”, compared with aspect “Mac OS” due to
their different sentiment polarities.
Speciﬁcally, for each aspect pair ai and aj in
aspect list A, we compute the square loss on the
coarse-grained attention vector aca
also estimate the distance dij ∈ between ai
and aj as the loss weight.
dij = σ(Wd([Qi; Qj; Qi ∗Qj])
Lalign = −
j=i+1,yi̸=yj
dij · (aca
Where σ is the sigmoid function, Wd ∈R1∗6d is
weight matrix for computing the distance, yi and
yj are the true labels of the aspect ai and aj, aca
jk are the attention weights on k-th context
word towards aspect ai and aj, respectively.
multi-grained
the parameters Θ from the LSTM networks:
[Wi, Wo, Wf, Wg, bi, bo, bf, bg], the attention and
alignment loss parameters: [Wca, Wcc, Wu, Wd]
and softmax parameters: [Wp, bp]. The ﬁnal loss
function is consisting of the cross-entropy loss,
aspect alignment loss and regularization item as
yilog(pi) + βLalign + λ ∥Θ∥2
Where β ≥0 and λ ≥0 controls the inﬂuence
of the aspect alignment loss and the L2 regularization item, respectively. We employ the stochastic
gradient descent (SGD) optimizer to compute and
update the training parameters. In addition, we utilize dropout strategy to avoid overﬁtting.
Experiments
In this section, we conduct experiments to evaluate our two hypotheses: (1) whether the wordlevel interaction between aspect and context can
help relieve the information loss and improve the
performance. (2) whether the relationship among
the aspects, which have the same context and different sentiment polarities, can bring extra useful
information.
Experiment Setting
We conduct experiments on three datasets, as
shown in Table 1. The ﬁrst two are from the SemEval 2014 Task 41 , which
contains the reviews in laptop and restaurants, respectively.
The third one is a tweet collection,
which are gathered by . Each
aspect with the context is labeled by three sentiment polarities, namely Positive, Neutral and Negative. In addition, we adopt Accuracy and Macro-
F1 as the metrics to evaluate the performance
of aspect-level sentiment classiﬁcation, which is
widely used in previous works . The dimension
of word embedding dv and hidden state d are
introduction
 
Restaurant
Table 1: The statistics of the datasets.
set to 300. The weight matrix and bias are initialized by sampling from a uniform distribution
U(0.01, 0.01). The coefﬁcient λ of L2 regularization item is 10−5, the parameter β of aspect alignment loss and drop out rate are set to 0.5.
Compared Methods
To evaluate the performance of proposed approach, we compared with the following methods:
Majority is the basic baseline, which chooses the
largest sentiment polarity in the training set to each
instance in the test set.
Feature+SVM uses ngram features, parse features and lexicon features
based on SVM, which achieves the state-of-the-art
performance in SemEval 2014.
LSTM utilizes one LSTM network to learn the hidden states and obtain the averaged vector to predict the sentiment polarity.
ATAE-LSTM learns attention
embeddings and combine them with the LSTM
hidden states to predict the polarity.
TD-LSTM employs two directional LSTM networks, which estimate the left
context and right context of the target aspect, respectively. Finally it takes the last hidden states of
LSTM networks for prediction.
MemNet applys multi-hop attentions on the word embeddings, learns the attention weights on context word vectors with respect
to the averaged query vector.
IAN interactively learns the
coarse-grained attentions between the context and
aspect, and concatenate the vectors for prediction.
BILSTM-ATT-G models left and right context with two attention-based
LSTMs and utilizes gates to control the importance of left context, right context and the entire
sentence for prediction.
RAM learns multi-hop attentions on the hidden states of bidirectional LSTM
networks for context words, and proposes to use
GRU network to get the aggregated vector from
the attentions. Similar with MemNet, the atten-
tion weights on context words are steered by the
simple averaged aspect vector.
We also list the variants of MGAN model,
which are used to analyze the effects of coarsegrained attention, ﬁne-grained attention and aspect
alignment loss, respectively.
MGAN-C only employs the coarse-grained attentions for prediction, which is similar with IAN.
MGAN-F only utilizes the proposed ﬁne-grained
attentions for prediction.
MGAN-CF adopts both the coarse-grained and
ﬁne-grained attentions, while without applying the
aspect alignment loss.
MGAN is the complete multi-grained attention
network model.
Overall Performance Comparison
Table 2 shows the performance comparison results
of MGAN with other baseline methods. We can
have the following observations.
(1) Majority performs worst since it only utilizes the data distribution information.
Feature+SVM can achieve much better performance
on all the datasets, with the well-designed feature
engineering.
Our method MGAN outperforms
Majority and Feature+SVM since MGAN could
learn the high quality representation for prediction.
(2) ATAE-LSTM is better than LSTM since it
employs attention mechanism on the hidden states
and combines with attention embedding to generate the ﬁnal representation. TD-LSTM performs
slightly better than ATAE-LSTM, and it employs
two LSTM networks to capture the left and right
context of the aspect. TD-LSTM performs worse
than our method MGAN since it could not properly pay more attentions on the important parts of
the context.
(3) IAN achieves slightly better results with the
previous LSTM-based methods, which interactively learns the attended aspect and context vector
as ﬁnal representation. Our method consistently
performs better than IAN since we utilize the ﬁnegrained attention vectors to relieve the information loss in IAN. MemNet continuously learns
the attended vector on the context word embedding memory, and updates the query vector at each
BILSTM-ATT-G models left context and
right context using attention-based LSTMs, which
achieves better performance than MemNet. RAM
performs better than other baselines. It employs
bidirectional LSTM network to generate contextual memory, and learns the multiple attended vector on the memory. Similar with MemNet, it utilizes the averaged aspect vector to learn the attention weights on context words.
Our proposed MGAN consistently performs
better than MemNet, BILSTM-ATT-G and RAM
on all three datasets.
On one hand, they only
consider to learn the attention weights on context
towards the aspect, and do not consider to learn
the weights on aspect words towards the context.
On the other hand, they just use the averaged aspect vector to guide the attention, which will lose
some information, especially on the aspects with
multiple words.
From another perspective, our
method employs the aspect alignment loss, which
can bring extra useful information from the aspectlevel interactions.
Analysis of MGAN model
Table 3 shows the performance comparison among
the variants of MGAN model. We can have the
following observations.
(1) the proposed ﬁne-grained attention mechanism MGAN-F, which is responsible for linking and fusing the information between the context and aspect word, achieves competitive performance compared with MGAN-C, especially on
laptop dataset. To investigate this case, we collect the percentage of aspects with different word
lengths in Table 4. We can ﬁnd that laptop dataset
has the highest percentage on the aspects with
more than two words, and the second-highest percentage on two words. It demonstrates MGAN-
F has better performance on aspects with more
words, and make use of the word-level interactions
to relieve the information loss occurred in coarsegrained attention mechanism.
(2) MGAN-CF is better than both MGAN-C
and MGAN-F, which demonstrates the coarsegrained attentions and ﬁne-grained attentions
could improve the performance from different perspectives. Compared with MGAN-CF, the complete MGAN model gains further improvement
by bringing the aspect alignment loss, which is
designed to capture the aspect level interactions.
Speciﬁcally, we collect the statistics of sentencelevel with different aspect amounts, which is
shown in Table 5. We can observe that both laptop
and restaurant datasets have relatively high percentage on the sentences with multiple aspects.
Restaurant
Feature-SVM
BILSTM-ATT-G
Table 2: The performance comparisons of different methods on the three datasets, where the results of baseline
methods are retrieved from published papers. The best performances are marked in bold.
Restaurant
Table 3: The performance comparisons of MGAN variants. ∗means MGAN-CF and MGAN can be regarded as
the same method on twitter dataset.
Restaurant
Table 4: The percentage of aspects with different word
length on three datasets. Here we give the overall statistic of each dataset.
The improved performance on the two datasets
shows the importance of capturing the aspect-level
interactions. In terms of twitter dataset, almost all
of the sentences only has one aspect. In this case,
the method MGAN can be regarded as MGAN-
#aspects=1
#aspects=2
#aspects>2
Restaurant
Table 5: The percentage of sentences with different aspect numbers on three datasets. Aspects with the same
context are regarded as the same sentence.
Case Study
In order to demonstrate the effect of aspect alignment loss, we visualize the attention weights
of the C-Aspect2Context mechanism.
shows the attention weights of two aspects “resolution” and “fonts”, whose sentiment polarities
are positive and negative, respectively.
the above two bars, we can observe that the C-
Aspect2Context can enforce the model to pay
more attentions on the important words with respect to the aspect.
For example, in terms of
the aspect“resolution”, the words “has”, “higher”
and “but” have higher attention weights compared
with other words.
In contrast, aspect “fonts”
pays more attentions on words “but”, “fonts” and
“small”. In addition, we evaluate the effect of aspect alignment loss, which enhances the attention
difference between the aspect “resolution” and
“fonts”. For the two bars at bottom, we can ﬁnd
that aspect “fonts” has more attention on “small”
and less attention on “higher”, compared with the
aspect “resolution”. This phenomenon shows that
with the constraint of aspect alignment loss, C-
Aspect2Context can not only learn the important
context words for each aspect, but also can make
the attention gaps on the important words be as
large as possible for aspects with different polarities.
Conclusion
In this paper, we propose a multi-grained attention network (MGAN) for aspect-level sentiment
classiﬁcation.
Speciﬁcally, we propose a ﬁnegrained attention mechanism, which is responsible
for linking and fusing the words from the aspect
and context, to capture the word-level interaction.
And we combine it with the coarse-grained atten-
resolution
Aspect resolution
Aspect fonts
Aspect resolution
Aspect fonts
C-Aspect2Context
C-Aspect2Context with
Aspect Alignment Loss
Figure 2: The attention visualizations on aspect “resolution” and “fonts”. The above two bars are from the C-
Aspect2Context attention mechanism, and the two bars at bottom are from the C-Aspect2Context attention mechanism with the constraint of aspect alignment loss.
tion mechanism to compose the MGAN model.
In addition, we design an aspect alignment loss
to characterize the aspect-level interactions among
aspects, which have the same context and different sentiment polarities, to explore extra valuable information.
Experimental results demonstrate the effectiveness of our approach on three
public datasets.
Acknowledgments
This work is supported by the National Natural Science Foundation of China (61672057,
61672058); KLSTSPI Key Lab.
of Intelligent
Press Media Technology. For any correspondence,
please contact Yansong Feng.