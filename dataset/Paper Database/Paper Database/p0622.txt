DeepWalk: Online Learning of Social Representations
Bryan Perozzi
Stony Brook University
Department of Computer
Rami Al-Rfou
Stony Brook University
Department of Computer
Steven Skiena
Stony Brook University
Department of Computer
{bperozzi, ralrfou, skiena}@cs.stonybrook.edu
We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent
representations encode social relations in a continuous vector
space, which is easily exploited by statistical models. Deep-
Walk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning)
from sequences of words to graphs.
DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate
DeepWalk’s latent representations on several multi-label
network classiﬁcation tasks for social networks such as Blog-
Catalog, Flickr, and YouTube. Our results show that Deep-
Walk outperforms challenging baselines which are allowed
a global view of the network, especially in the presence of
missing information. DeepWalk’s representations can provide F1 scores up to 10% higher than competing methods
when labeled data is sparse. In some experiments, Deep-
Walk’s representations are able to outperform all baseline
methods while using 60% less training data.
DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially
parallelizable. These qualities make it suitable for a broad
class of real world applications such as network classiﬁcation, and anomaly detection.
Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications
- Data Mining; I.2.6 [Artiﬁcial Intelligence]: Learning;
I.5.1 [Pattern Recognition]: Model - Statistical
INTRODUCTION
The sparsity of a network representation is both a strength
and a weakness. Sparsity enables the design of eﬃcient discrete algorithms, but can make it harder to generalize in
statistical learning.
Machine learning applications in networks Input: Karate Graph
(b) Output: Representation
Figure 1: Our proposed method learns a latent space representation of social interactions in Rd.
The learned representation encodes community structure so it can be easily exploited by standard classiﬁcation methods. Here, our
method is used on Zachary’s Karate network to generate a latent representation in R2.
Note the correspondence between community structure in the input graph and
the embedding. Vertex colors represent a modularity-based
clustering of the input graph.
ommendation , anomaly detection , and missing link
prediction ) must be able to deal with this sparsity in
order to survive.
In this paper we introduce deep learning (unsupervised
feature learning) techniques, which have proven successful in natural language processing, into network analysis for
the ﬁrst time. We develop an algorithm (DeepWalk) that
learns social representations of a graph’s vertices, by modeling a stream of short random walks.
Social representations are latent features of the vertices that capture neighborhood similarity and community membership. These latent representations encode social relations in a continuous
vector space with a relatively small number of dimensions.
DeepWalk generalizes neural language models to process a
special language composed of a set of randomly-generated
These neural language models have been used to
capture the semantic and syntactic structure of human language , and even logical analogies .
DeepWalk takes a graph as input and produces a latent representation as an output. The result of applying our
method to the well-studied Karate network is shown in Figure 1. The graph, as typically presented by force-directed
layouts, is shown in Figure 1a. Figure 1b shows the output
of our method with 2 latent dimensions. Beyond the striking
similarity, we note that linearly separable portions of (1b)
correspond to clusters found through modularity maximization in the input graph (1a) (shown as vertex colors).
To demonstrate DeepWalk’s potential in real world scearXiv:1403.6652v2 [cs.SI] 27 Jun 2014
narios, we evaluate its performance on challenging multilabel network classiﬁcation problems in large heterogeneous
graphs. In the relational classiﬁcation problem, the links between feature vectors violate the traditional i.i.d. assumption. Techniques to address this problem typically use approximate inference techniques to leverage the dependency information to improve classiﬁcation results. We
distance ourselves from these approaches by learning labelindependent representations of the graph. Our representation quality is not inﬂuenced by the choice of labeled vertices, so they can be shared among tasks.
DeepWalk outperforms other latent representation methods for creating social dimensions , especially when
labeled nodes are scarce. Strong performance with our representations is possible with very simple linear classiﬁers
(e.g. logistic regression). Our representations are general,
and can be combined with any classiﬁcation method (including iterative inference methods). DeepWalk achieves
all of that while being an online algorithm that is trivially
parallelizable.
Our contributions are as follows:
• We introduce deep learning as a tool to analyze graphs,
to build robust representations that are suitable for
statistical modeling. DeepWalk learns structural regularities present within short random walks.
• We extensively evaluate our representations on multilabel classiﬁcation tasks on several social networks. We
show signiﬁcantly increased classiﬁcation performance
in the presence of label sparsity, getting improvements
5%-10% of Micro F1, on the sparsest problems we consider. In some cases, DeepWalk’s representations can
outperform its competitors even when given 60% less
training data.
• We demonstrate the scalability of our algorithm by
building representations of web-scale graphs, (such as
YouTube) using a parallel implementation. Moreover,
we describe the minimal changes necessary to build a
streaming version of our approach.
The rest of the paper is arranged as follows. In Sections 2
and 3, we discuss the problem formulation of classiﬁcation
in data networks, and how it relates to our work. In Section
4 we present DeepWalk, our approach for Social Representation Learning. We outline ours experiments in Section
5, and present their results in Section 6. We close with a
discussion of related work in Section 7, and our conclusions.
PROBLEM DEFINITION
We consider the problem of classifying members of a social
network into one or more categories. More formally, let G =
(V, E), where V are the members of the network, and E be
its edges, E ⊆(V × V ).
Given a partially labeled social
network GL = (V, E, X, Y ), with attributes X ∈R|V |×S
where S is the size of the feature space for each attribute
vector, and Y ∈R|V |×|Y|, Y is the set of labels.
In a traditional machine learning classiﬁcation setting, we
aim to learn a hypothesis H that maps elements of X to the
labels set Y. In our case, we can utilize the signiﬁcant information about the dependence of the examples embedded
in the structure of G to achieve superior performance.
In the literature, this is known as the relational classiﬁcation (or the collective classiﬁcation problem ). Traditional approaches to relational classiﬁcation pose the problem as an inference in an undirected Markov network, and
then use iterative approximate inference algorithms (such
as the iterative classiﬁcation algorithm , Gibbs Sampling , or label relaxation ) to compute the posterior
distribution of labels given the network structure.
We propose a diﬀerent approach to capture the network
topology information.
Instead of mixing the label space
as part of the feature space, we propose an unsupervised
method which learns features that capture the graph structure independent of the labels’ distribution.
This separation between the structural representation and
the labeling task avoids cascading errors, which can occur in
iterative methods . Moreover, the same representation
can be used for multiple classiﬁcation problems concerning
that network.
Our goal is to learn XE ∈R|V |×d, where d is small number of latent dimensions. These low-dimensional representations are distributed; meaning each social phenomena is
expressed by a subset of the dimensions and each dimension
contributes to a subset of the social concepts expressed by
the space.
Using these structural features, we will augment the attributes space to help the classiﬁcation decision. These features are general, and can be used with any classiﬁcation
algorithm (including iterative methods). However, we believe that the greatest utility of these features is their easy
integration with simple machine learning algorithms. They
scale appropriately in real-world networks, as we will show
in Section 6.
LEARNING SOCIAL REPRESENTATIONS
We seek learning social representations with the following
characteristics:
• Adaptability - Real social networks are constantly
evolving; new social relations should not require repeating the learning process all over again.
• Community aware - The distance between latent
dimensions should represent a metric for evaluating
social similarity between the corresponding members
of the network. This allows generalization in networks
with homophily.
• Low dimensional - When labeled data is scarce, lowdimensional models generalize better, and speed up
convergence and inference.
• Continuous - We require latent representations to
model partial community membership in continuous
In addition to providing a nuanced view of
community membership, a continuous representation
has smooth decision boundaries between communities
which allows more robust classiﬁcation.
Our method for satisfying these requirements learns representation for vertices from a stream of short random walks,
using optimization techniques originally designed for language modeling. Here, we review the basics of both random
walks and language modeling, and describe how their combination satisﬁes our requirements.
Vertex visitation count
# of Vertices
Frequency of Vertex Occurrence in Short Random Walks
(a) YouTube Social Graph
Word mention count
# of Words
Frequency of Word Occurrence in Wikipedia
(b) Wikipedia Article Text
Figure 2: The power-law distribution of vertices appearing
in short random walks (2a) follows a power-law, much like
the distribution of words in natural language (2b).
Random Walks
We denote a random walk rooted at vertex vi as Wvi. It is
a stochastic process with random variables W1
vi, . . . , Wk
such that Wk+1
is a vertex chosen at random from the neighbors of vertex vk. Random walks have been used as a similarity measure for a variety of problems in content recommendation and community detection . They are also
the foundation of a class of output sensitive algorithms which
use them to compute local community structure information
in time sublinear to the size of the input graph .
It is this connection to local structure that motivates us to
use a stream of short random walks as our basic tool for extracting information from a network. In addition to capturing community information, using random walks as the basis for our algorithm gives us two other desirable properties.
First, local exploration is easy to parallelize. Several random
walkers (in diﬀerent threads, processes, or machines) can simultaneously explore diﬀerent parts of the same graph. Secondly, relying on information obtained from short random
walks make it possible to accommodate small changes in the
graph structure without the need for global recomputation.
We can iteratively update the learned model with new random walks from the changed region in time sub-linear to the
entire graph.
Connection: Power laws
Having chosen online random walks as our primitive for
capturing graph structure, we now need a suitable method
to capture this information.
If the degree distribution of
a connected graph follows a power law (is scale-free), we
observe that the frequency which vertices appear in the short
random walks will also follow a power-law distribution.
Word frequency in natural language follows a similar distribution, and techniques from language modeling account
for this distributional behavior. To emphasize this similarity we show two diﬀerent power-law distributions in Figure
2. The ﬁrst comes from a series of short random walks on
a scale-free graph, and the second comes from the text of
100,000 articles from the English Wikipedia.
A core contribution of our work is the idea that techniques
which have been used to model natural language (where the
symbol frequency follows a power law distribution (or Zipf’s
law)) can be re-purposed to model community structure in
networks. We spend the rest of this section reviewing the
growing work in language modeling, and transforming it to
learn representations of vertices which satisfy our criteria.
Language Modeling
The goal of language modeling is estimate the likelihood
of a speciﬁc sequence of words appearing in a corpus. More
formally, given a sequence of words
1 = (w0, w1, · · · , wn)
where wi ∈V (V is the vocabulary), we would like to maximize the Pr(wn|w0, w1, · · · , wn−1) over all the training corpus.
Recent work in representation learning has focused on using probabilistic neural networks to build general representations of words which extend the scope of language modeling
beyond its original goals.
In this work, we present a generalization of language modeling to explore the graph through a stream of short random
walks. These walks can be thought of short sentences and
phrases in a special language. The direct analog is to estimate the likelihood of observing vertex vi given all the
previous vertices visited so far in the random walk.
 vi | (v1, v2, · · · , vi−1)
Our goal is to learn a latent representation, not only a
probability distribution of node co-occurrences, and so we
introduce a mapping function Φ: v ∈V 7→R|V |×d. This
mapping Φ represents the latent social representation associated with each vertex v in the graph.
(In practice, we
represent Φ by a |V | × d matrix of free parameters, which
will serve later on as our XE.)
The problem then, is to
estimate the likelihood:
 Φ(v1), Φ(v2), · · · , Φ(vi−1)
However as the walk length grows, computing this objective function becomes unfeasible.
A recent relaxation in language modeling turns
the prediction problem on its head. First, instead of using
the context to predict a missing word, it uses one word to
predict the context. Secondly, the context is composed of
the words appearing to right side of the given word as well
as the left side. Finally, it removes the ordering constraint
on the problem. Instead, the model is required to maximize
the probability of any word appearing in the context without
the knowledge of its oﬀset from the given word.
In terms of vertex representation modeling, this yields the
optimization problem:
 {vi−w, · · · , vi−1, vi+1, · · · , vi+w} | Φ(vi)
We ﬁnd these relaxations are particularly desirable for social representation learning. First, the order independence
assumption better captures a sense of ‘nearness’ that is provided by random walks. Moreover, this relaxation is quite
useful for speeding up the training time by building small
models as one vertex is given at a time.
Solving the optimization problem from Eq. 2 builds representations that capture the shared similarities in local graph
structure between vertices. Vertices which have similar neighborhoods will acquire similar representations (encoding cocitation similarity), and allowing generalization on machine
learning tasks.
By combining both truncated random walks and neural
language models we formulate a method which satisﬁes all
Algorithm 1 DeepWalk(G, w, d, γ, t)
Input: graph G(V, E)
window size w
embedding size d
walks per vertex γ
walk length t
Output: matrix of vertex representations Φ ∈R|V |×d
1: Initialization: Sample Φ from U|V |×d
2: Build a binary Tree T from V
3: for i = 0 to γ do
O = Shuﬄe(V )
for each vi ∈O do
Wvi = RandomWalk(G, vi,t)
SkipGram(Φ, Wvi, w)
9: end for
of our desired properties. This method generates representations of social networks that are low-dimensional, and exist in a continuous vector space. Its representations encode
latent forms of community membership, and because the
method outputs useful intermediate representations, it can
adapt to changing network topology.
In this section we discuss the main components of our
algorithm. We also present several variants of our approach
and discuss their merits.
As in any language modeling algorithm, the only required
input is a corpus and a vocabulary V. DeepWalk considers
a set of short truncated random walks its own corpus, and
the graph vertices as its own vocabulary (V = V ). While it
is beneﬁcial to know the V and the frequency distribution
of vertices in the random walks ahead of the training, it is
not necessary for the algorithm to work as we will show in
Algorithm: DeepWalk
The algorithm consists of two main components; ﬁrst a
random walk generator and second an update procedure.
The random walk generator takes a graph G and samples
uniformly a random vertex vi as the root of the random
A walk samples uniformly from the neighbors
of the last vertex visited until the maximum length (t) is
reached. While we set the length of our random walks in
the experiments to be ﬁxed, there is no restriction for the
random walks to be of the same length. These walks could
have restarts (i.e. a teleport probability of returning back
to their root), but our preliminary results did not show any
advantage of using restarts. In practice, our implementation
speciﬁes a number of random walks γ of length t to start at
each vertex.
Lines 3-9 in Algorithm 1 shows the core of our approach.
The outer loop speciﬁes the number of times, γ, which we
should start random walks at each vertex. We think of each
iteration as making a ‘pass’ over the data and sample one
walk per node during this pass. At the start of each pass we
generate a random ordering to traverse the vertices. This
is not strictly required, but is well-known to speed up the
convergence of stochastic gradient descent.
Algorithm 2 SkipGram(Φ, Wvi, w)
1: for each vj ∈Wvi do
for each uk ∈Wvi[j −w : j + w] do
J(Φ) = −log Pr(uk | Φ(vj))
Φ = Φ −α ∗∂J
6: end for
In the inner loop, we iterate over all the vertices of the
graph. For each vertex vi we generate a random walk |Wvi| =
t, and then use it to update our representations (Line 7). We
use the SkipGram algorithm to update these representations in accordance with our objective function in Eq. 2.
SkipGram is a language model that maximizes the cooccurrence probability among the words that appear within
a window, w, in a sentence .
Algorithm 2 iterates over all possible collocations in random walk that appear within the window w (lines 1-2). For
each, we map each vertex vj to its current representation
vector Φ(vj) ∈Rd (See Figure 3b). Given the representation of vj, we would like to maximize the probability of its
neighbors in the walk (line 3). We can learn such posterior
distribution using several choices of classiﬁers. For example, modeling the previous problem using logistic regression
would result in a huge number of labels that is equal to |V |
which could be in millions or billions. Such models require
large amount of computational resources that could span a
whole cluster of computers . To speed the training time,
Hierarchical Softmax can be used to approximate the
probability distribution.
Hierarchical Softmax
Given that uk ∈V , calculating Pr(uk | Φ(vj)) in line 3
is not feasible. Computing the partition function (normalization factor) is expensive. If we assign the vertices to the
leaves of a binary tree, the prediction problem turns into
maximizing the probability of a speciﬁc path in the tree
(See Figure 3c).
If the path to vertex uk is identiﬁed by
a sequence of tree nodes (b0, b1, . . . , b⌈log |V |⌉), (b0 = root,
b⌈log |V |⌉= uk) then
Pr(uk | Φ(vj)) =
⌈log |V |⌉
Pr(bl | Φ(vj))
Now, Pr(bl | Φ(vj)) could be modeled by a binary classiﬁer
that is assigned to the parent of the node bl. This reduces
the computational complexity of calculating Pr(uk | Φ(vj))
from O(|V |) to O(log |V |).
We can speed up the training process further, by assigning
shorter paths to the frequent vertices in the random walks.
Huﬀman coding is used to reduce the access time of frequent
elements in the tree.
Optimization
The model parameter set is {Φ, T} where the size of each
is O(d|V |). Stochastic gradient descent (SGD) is used
to optimize these parameters (Line 4, Algorithm 2). The
derivatives are estimated using the back-propagation algorithm. The learning rate α for SGD is initially set to 2.5%
at the beginning of the training and then decreased linearly
(a) Random walk generation.
(b) Representation mapping.
(c) Hierarchical Softmax.
Figure 3: Overview of DeepWalk. We slide a window of length 2w + 1 over the random walk Wv4, mapping the central
vertex v1 to its representation Φ(v1). Hierarchical Softmax factors out Pr(v3 | Φ(v1)) and Pr(v5 | Φ(v1)) over sequences of
probability distributions corresponding to the paths starting at the root and ending at v3 and v5. The representation Φ is
updated to maximize the probability of v1 co-occurring with its context {v3, v5}.
with the number of vertices that are seen so far.
Parallelizability
As shown in Figure 2 the frequency distribution of vertices
in random walks of social network and words in a language
both follow a power law. This results in a long tail of infrequent vertices, therefore, the updates that aﬀect Φ will be
sparse in nature. This allows us to use asynchronous version
of stochastic gradient descent (ASGD), in the multi-worker
case. Given that our updates are sparse and we do not acquire a lock to access the model shared parameters, ASGD
will achieve an optimal rate of convergence . While we
run experiments on one machine using multiple threads, it
has been demonstrated that this technique is highly scalable, and can be used in very large scale machine learning . Figure 4 presents the eﬀects of parallelizing Deep-
Walk. It shows the speed up in processing BlogCatalog
and Flickr networks is consistent as we increase the number of workers to 8 (Figure 4a). It also shows that there
is no loss of predictive performance relative to the running
DeepWalk serially (Figure 4b).
Algorithm Variants
Here we discuss some variants of our proposed method,
which we believe may be of interest.
One interesting variant of this method is a streaming approach, which could be implemented without knowledge of
the entire graph. In this variant small walks from the graph
are passed directly to the representation learning code, and
the model is updated directly. Some modiﬁcations to the
learning process will also be necessary. First, using a decaying learning rate will no longer be possible. Instead, we
can initialize the learning rate α to a small constant value.
This will take longer to learn, but may be worth it in some
applications. Second, we cannot necessarily build a tree of
parameters any more. If the cardinality of V is known (or
can be bounded), we can build the Hierarchical Softmax tree
for that maximum value. Vertices can be assigned to one of
the remaining leaves when they are ﬁrst seen. If we have
the ability to estimate the vertex frequency a priori, we can
# of Workers
Relative Time
BlogCatalog
(a) Running Time
# of Workers
Relative Change in Micro F1
BlogCatalog
(b) Performance
Figure 4: Eﬀects of parallelizing DeepWalk
also still use Huﬀman coding to decrease frequent element
access times.
Non-random walks
Some graphs are created as a by-product of agents interacting with a sequence of elements (e.g. users’ navigation
of pages on a website). When a graph is created by such a
stream of non-random walks, we can use this process to feed
the modeling phase directly. Graphs sampled in this way will
not only capture information related to network structure,
but also to the frequency at which paths are traversed.
In our view, this variant also encompasses language modeling. Sentences can be viewed as purposed walks through
an appropriately designed language network, and language
models like SkipGram are designed to capture this behavior.
This approach can be combined with the streaming variant (Section 4.4.1) to train features on a continually evolving network without ever explicitly constructing the entire
graph. Maintaining representations with this technique could
enable web-scale classiﬁcation without the hassles of dealing
with a web-scale graph.
EXPERIMENTAL DESIGN
In this section we provide an overview of the datasets and
methods which we will use in our experiments. Code and
data to reproduce our results will be available at the ﬁrst
author’s website.
BlogCatalog
Table 1: Graphs used in our experiments.
An overview of the graphs we consider in our experiments
is given in Figure 1.
• BlogCatalog is a network of social relationships
provided by blogger authors. The labels represent the
topic categories provided by the authors.
• Flickr is a network of the contacts between users
of the photo sharing website. The labels represent the
interest groups of the users such as ‘black and white
• YouTube is a social network between users of
the popular video sharing website.
The labels here
represent groups of viewers that enjoy common video
genres (e.g. anime and wrestling).
Baseline Methods
To validate the performance of our approach we compare
it against a number of baselines:
• SpectralClustering : This method generates a representation in Rd from the d-smallest eigenvectors of
eL, the normalized graph Laplacian of G. Utilizing the
eigenvectors of eL implicitly assumes that graph cuts
will be useful for classiﬁcation.
• Modularity : This method generates a representation in Rd from the top-d eigenvectors of B, the Modularity matrix of G.
The eigenvectors of B encode
information about modular graph partitions of G .
Using them as features assumes that modular graph
partitions will be useful for classiﬁcation.
• EdgeCluster : This method uses k-means clustering to cluster the adjacency matrix of G.
been shown to perform comparably to the Modularity
method, with the added advantage of scaling to graphs
which are too large for spectral decomposition.
• wvRN : The weighted-vote Relational Neighbor is
a relational classiﬁer. Given the neighborhood Ni of
vertex vi, wvRN estimates Pr(yi|Ni) with the (appropriately normalized) weighted mean of its neighbors
(i.e Pr(yi|Ni) =
vj∈Ni wij Pr(yj | Nj)).
shown surprisingly good performance in real networks,
and has been advocated as a sensible relational classi-
ﬁcation baseline .
• Majority: This na¨ıve method simply chooses the most
frequent labels in the training set.
EXPERIMENTS
In this section we present an experimental analysis of our
method. We thoroughly evaluate it on a number of multilabel classiﬁcation tasks, and analyze its sensitivity across
several parameters.
Multi-Label Classiﬁcation
To facilitate the comparison between our method and the
relevant baselines, we use the exact same datasets and experimental procedure as in . Speciﬁcally, we randomly
sample a portion (TR) of the labeled nodes, and use them
as training data. The rest of the nodes are used as test. We
repeat this process 10 times, and report the average performance in terms of both Macro-F1 and Micro-F1. When
possible we report the original results here directly.
For all models we use a one-vs-rest logistic regression implemented by LibLinear for classiﬁcation. We present
results for DeepWalk with (γ = 80, w = 10, d = 128). The
results for (SpectralClustering, Modularity, EdgeCluster)
use Tang and Liu’s preferred dimensionality, d = 500.
BlogCatalog
In this experiment we increase the training ratio (TR) on
the BlogCatalog network from 10% to 90%. Our results
are presented in Table 2.
Numbers in bold represent the
highest performance in each column.
DeepWalk performs consistently better than EdgeCluster,
Modularity, and wvRN. In fact, when trained with only 20%
of the nodes labeled, DeepWalk performs better than these
approaches when they are given 90% of the data. The performance of SpectralClustering proves much more competitive,
but DeepWalk still outperforms when labeled data is sparse
on both Macro-F1 (TR ≤20%) and Micro-F1 (TR ≤60%).
This strong performance when only small fractions of the
graph are labeled is a core strength of our approach. In the
following experiments, we investigate the performance of our
representations on even more sparsely labeled graphs.
In this experiment we vary the training ratio (TR) on the
Flickr network from 1% to 10%. This corresponds to having approximately 800 to 8,000 nodes labeled for classiﬁcation in the entire network. Table 3 presents our results,
which are consistent with the previous experiment. Deep-
Walk outperforms all baselines by at least 3% with respect
to Micro-F1. Additionally, its Micro-F1 performance when
only 3% of the graph is labeled beats all other methods
even when they have been given 10% of the data. In other
words, DeepWalk can outperform the baselines with 60%
less training data. It also performs quite well in Macro-F1,
initially performing close to SpectralClustering, but distancing itself to a 1% improvement.
The YouTube network is considerably larger than the
previous ones we have experimented on, and its size prevents two of our baseline methods (SpectralClustering and
Modularity) from running on it. It is much closer to a real
world graph than those we have previously considered.
The results of varying the training ratio (TR) from 1% to
10% are presented in Table 4. They show that DeepWalk
signiﬁcantly outperforms the scalable baseline for creating
graph representations, EdgeCluster.
When 1% of the labeled nodes are used for test, the Micro-F1 improves by
14%. The Macro-F1 shows a corresponding 10% increase.
This lead narrows as the training data increases, but Deep-
Walk ends with a 3% lead in Micro-F1, and an impressive
5% improvement in Macro-F1.
This experiment showcases the performance beneﬁts that
% Labeled Nodes
SpectralClustering
EdgeCluster
Micro-F1(%)
Modularity
SpectralClustering
EdgeCluster
Macro-F1(%)
Modularity
Table 2: Multi-label classiﬁcation results in BlogCatalog
% Labeled Nodes
SpectralClustering
Micro-F1(%)
EdgeCluster
Modularity
SpectralClustering
Macro-F1(%)
EdgeCluster
Modularity
Table 3: Multi-label classiﬁcation results in Flickr
% Labeled Nodes
SpectralClustering
Micro-F1(%)
EdgeCluster
Modularity
SpectralClustering
Macro-F1(%)
EdgeCluster
Modularity
Table 4: Multi-label classiﬁcation results in YouTube
(a1) Flickr, γ = 30
(a2) Flickr, TR = 0.05
(a3) BlogCatalog, γ = 30
(a4) BlogCatalog, TR = 0.5
(a) Stability over dimensions, d
(b1) Flickr, TR = 0.05
(b2) Flickr, d = 128
(b3) BlogCatalog, TR = 0.5
(b4) BlogCatalog, d = 128
(a) Stability over number of walks, γ
Figure 5: Parameter Sensitivity Study
can occur from using social representation learning for multilabel classiﬁcation. DeepWalk, can scale to large graphs,
and performs exceedingly well in such a sparsely labeled
environment.
Parameter Sensitivity
In order to evaluate how changes to the parameterization
of DeepWalk eﬀect its performance on classiﬁcation tasks,
we conducted experiments on two multi-label classiﬁcations
tasks (Flickr, and BlogCatalog). For this test, we have
ﬁxed the window size and the walk length to sensible values (w = 10, t = 40) which should emphasize local structure. We then vary the number of latent dimensions (d),
the number of walks started per vertex (γ), and the amount
of training data available (TR) to determine their impact on
the network classiﬁcation performance.
Effect of Dimensionality
Figure 5a shows the eﬀects of increasing the number of
latent dimensions available to our model.
Figures 5a1 and 5a3 examine the eﬀects of varying the
dimensionality and training rate. The performance is quite
consistent between both Flickr and BlogCatalog and
show that the optimal dimensionality for a model is dependent on the number of training examples. (Note that 1%
of Flickr has approximately as many labeled examples as
10% of BlogCatalog).
Figures 5a2 and 5a3 examine the eﬀects of varying the
dimensionality and number of walks per vertex.
The relative performance between dimensions is relatively stable
across diﬀerent values of γ. These charts have two interesting observations. The ﬁrst is that there is most of the
beneﬁt is accomplished by starting γ = 30 walks per node
in both graphs. The second is that the relative diﬀerence
between diﬀerent values of γ is quite consistent between the
two graphs. Flickr has an order of magnitude more edges
than BlogCatalog, and we ﬁnd this behavior interesting.
These experiments show that our method can make useful models of various sizes. They also show that the performance of the model depends on the number of random
walks it has seen, and the appropriate dimensionality of the
model depends on the training examples available.
Effect of sampling frequency
Figure 5a shows the eﬀects of increasing γ, the number of
random walks that we start from each vertex.
The results are very consistent for diﬀerent dimensions
(Fig. 5b1, Fig. 5b3) and the amount of training data (Fig.
5b2, Fig. 5b4).
Initially, increasing γ has a big eﬀect in
the results, but this eﬀect quickly slows (γ > 10). These
results demonstrate that we are able to learn meaningful
latent representations for vertices after only a small number
of random walks.
RELATED WORK
The main diﬀerences between our proposed method and
previous work can be summarized as follows:
1. We learn our latent social representations, instead of
computing statistics related to centrality or partitioning .
2. We do not attempt to extend the classiﬁcation procedure itself (through collective inference or graph
kernels ).
3. We propose a scalable online method which uses only
local information. Most methods require global information and are oﬄine .
4. We apply unsupervised representation learning to graphs.
In this section we discuss related work in network classiﬁcation and unsupervised feature learning.
Relational Learning
Relational classiﬁcation (or collective classiﬁcation) methods use links between data items as part of the
classiﬁcation process. Exact inference in the collective classiﬁcation problem is NP-hard, and solutions have focused on
the use of approximate inference algorithm which may not
be guaranteed to converge .
The most relevant relational classiﬁcation algorithms to
our work incorporate community information by learning
clusters , by adding edges between nearby nodes , by
using PageRank , or by extending relational classiﬁcation to take additional features into account . Our work
takes a substantially diﬀerent approach. Instead of a new
approximation inference algorithm, we propose a procedure
which learns representations of network structure which can
then be used by existing inference procedure (including iterative ones).
A number of techniques for generating features from graphs
have also been proposed . In contrast to these
methods, we frame the feature creation procedure as a representation learning problem.
Graph Kernels have been proposed as a way to use
relational data as part of the classiﬁcation process, but are
quite slow unless approximated . Our approach is complementary; instead of encoding the structure as part of a
kernel function, we learn a representation which allows them
to be used directly as features for any classiﬁcation method.
Unsupervised Feature Learning
Distributed representations have been proposed to model
structural relationship between concepts . These representations are trained by the back-propagation and gradient
descent. Computational costs and numerical instability led
to these techniques to be abandoned for almost a decade.
Recently, distributed computing allowed for larger models to
be trained , and the growth of data for unsupervised learning algorithms to emerge .
Distributed representations
usually are trained through neural networks, these networks
have made advancements in diverse ﬁelds such as computer
vision , speech recognition , and natural language processing .
CONCLUSIONS
We propose DeepWalk, a novel approach for learning
latent social representations of vertices. Using local information from truncated random walks as input, our method
learns a representation which encodes structural regularities. Experiments on a variety of diﬀerent graphs illustrate
the eﬀectiveness of our approach on challenging multi-label
classiﬁcation tasks.
As an online algorithm, DeepWalk is also scalable. Our
results show that we can create meaningful representations
for graphs too large to run spectral methods on. On such
large graphs, our method signiﬁcantly outperforms other
methods designed to operate for sparsity.
We also show
that our approach is parallelizable, allowing workers to update diﬀerent parts of the model concurrently.
In addition to being eﬀective and scalable, our approach
is also an appealing generalization of language modeling.
This connection is mutually beneﬁcial.
Advances in language modeling may continue to generate improved latent
representations for networks. In our view, language modeling is actually sampling from an unobservable language
graph. We believe that insights obtained from modeling observable graphs may in turn yield improvements to modeling
unobservable ones.
Our future work in the area will focus on investigating
this duality further, using our results to improve language
modeling, and strengthening the theoretical justiﬁcations of
the method.