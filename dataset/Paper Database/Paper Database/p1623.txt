Exploiting Unlabeled Data to Enhance Ensemble Diversity
Min-Ling Zhang∗,†
∗School of Computer Science and Engineering,
Southeast University, Nanjing 210096, China
Email: 
Zhi-Hua Zhou†
†National Key Laboratory for Novel Software Technology,
Nanjing University, Nanjing 210093, China
Email: 
Abstract—Ensemble learning aims to improve generalization
ability by using multiple base learners. It is well-known that
to construct a good ensemble, the base learners should be
accurate as well as diverse. In this paper, unlabeled data is
exploited to facilitate ensemble learning by helping augment
the diversity among the base learners. Speciﬁcally, a semisupervised ensemble method named UDEED is proposed. Unlike
existing semi-supervised ensemble methods where error-prone
pseudo-labels are estimated for unlabeled data to enlarge the
labeled data to improve accuracy, UDEED works by maximizing
accuracies of base learners on labeled data while maximizing
diversity among them on unlabeled data. Experiments show
that UDEED can effectively utilize unlabeled data for ensemble
learning and is highly competitive to well-established semisupervised ensemble methods.
Keywords-ensemble learning; unlabeled data; diversity
I. INTRODUCTION
In ensemble learning , a number of base learners are
trained and then combined for prediction to achieve strong
generalization ability. Numerous effective ensemble methods
have been proposed, such as BOOSTING , BAGGING ,
STACKING , etc., and most of these methods work under
the supervised setting where the labels of training examples
are known. In many real-world tasks, however, unlabeled
training examples are readily available while obtaining their
labels would be fairly expensive. Semi-supervised learning
 is a major paradigm to exploit unlabeled data together
with labeled training data to improve learning performance
automatically, without human intervention.
This paper deals with semi-supervised ensembles, that
is, ensemble learning with labeled and unlabeled data. In
contrast to the huge volume of literatures on ensemble
learning and on semi-supervised learning, only a few work
has been devoted to the study of semi-supervised ensembles.
As indicated by Zhou , this was caused by the different
philosophies of the ensemble learning community and the
semi-supervised learning community. The ensemble learning
community believes that it is able to boost the performance
of weak learners to strong learners by using multiple learners, and so there is no need to use unlabeled data; while the
semi-supervised learning community believes that it is able
to boost the performance of weak learners to strong learners
by exploiting unlabeled data, and so there is no need to use
multiple learners. However, as Zhou indicated , there are
several important reasons why ensemble learning and semisupervised learning are actually mutually beneﬁcial, among
which an important one is that by considering unlabeled data
it is possible to help augment the diversity among the base
learners, as explained in the following paragraph.
It is well-known that the generalization error of an ensemble is related to the average generalization error of the
base learners and the diversity among the base learners.
Generally, the lower the average generalization error (or,
the higher the average accuracy) of the base learners and
the higher the diversity among the base learners, the better
the ensemble . Previous ensemble methods work under
supervised setting, trying to achieve a high average accuracy
and a high diversity by using the labeled training set. It
is noteworthy, however, pursuing a high accuracy and a
high diversity may suffer from a dilemma. For example,
for two classiﬁers which have perfect performance on the
labeled training set, they would not have diversity since there
is no difference between their predictions on the training
examples. Thus, to increase the diversity needs to sacriﬁce
the accuracy of one classiﬁer. However, when we have
unlabeled data, we might ﬁnd that these two classiﬁers
actually make different predictions on unlabeled data. This
would be important for ensemble design. For example, given
two pairs of classiﬁers, (A, B) and (C, D), if we know that
all of them are with 100% accuracy on labeled training data,
then there will be no difference taking either the ensemble
consisting of (A, B) or the ensemble consisting of (C, D);
however, if we ﬁnd that A and B make the same predictions
on unlabeled data, while C and D make different predictions
on some unlabeled data, then we will know that the ensemble
consisting of (C, D) should be better. So, in contrast to
previous ensemble methods which focus on achieving both
high accuracy and high diversity using only the labeled data,
the use of unlabeled data would open a promising direction
for designing new ensemble methods.
In this paper, we propose the UDEED (Unlabeled Data to
Enhance Ensemble Diversity) approach. Experiments show
that by using unlabeled data for diversity augmentation,
UDEED achieves much better performance than its counterpart which does not consider the usefulness of unlabeled
data. Moreover, UDEED also achieves highly comparable
performance to other state-of-the-art semi-supervised ensemble methods.
The rest of this paper is organized as follows. Section II
brieﬂy reviews related work on semi-supervised ensembles.
Section III presents UDEED. Section IV reports our experimental results. Finally, Section V concludes.
II. RELATED WORK
As mentioned before, in contrast to the huge volume
of literatures on ensemble learning and on semi-supervised
learning, only a few work has been devoted to the study of
semi-supervised ensembles.
Zhou and Li proposed the TRI-TRAINING approach
which uses three classiﬁers and in each round if two classi-
ﬁers agree on an unlabeled instance while the third classiﬁer
disagrees, then the two classiﬁers, under a certain condition,
will label this unlabeled instance for the third classiﬁer;
the three classiﬁers are voted to make prediction. This
is a disagreement-based semi-supervised learning approach
 , which can be viewed as a variant of the famous
co-training method . Later, Li and Zhou extended
TRI-TRAINING to CO-FOREST, by including more base
classiﬁers and in each round the majority teach minority
strategy is still adopted.
In addition to TRI-TRAINING and CO-FOREST, there are
several semi-supervised boosting methods , , , ,
 . D’Alch´e Buc et al. proposed SSMBOOST to handle
unlabeled data within the margin cost functional optimization framework for boosting , where the margin of an ensemble H on unlabeled data x is deﬁned as either H(x)2 or
|H(x)|. Furthermore, SSMBOOST requires the base learners
to be semi-supervised algorithms themselves. Later, Bennett
et al. developed ASSEMBLE, which labels unlabeled
data x by the current ensemble as y = sign [H(x)], and
then iteratively puts the newly labeled examples into the
original labeled set to train a new base classiﬁer which is
then added to H. Following the same margin cost functional
optimization framework, Chen and Wang added a local
smoothness regularizer to the objective function used by
ASSEMBLE to help induce new base classiﬁer with a more
reliable self-labeling process. Other than the margin cost
functional formalization, MCSSB and SEMIBOOST 
estimate the labels of unlabeled instances by optimizing
an objective function containing two terms. The ﬁrst term
encodes the manifold assumption that unlabeled instances
with high similarities in input space should share similar labels, while the other term encodes the clustering assumption
that unlabeled instances with high similarities to a labeled
example should share its given label. The difference lies in
that MCSSB implemented the objective terms based on
Bregman divergence while SEMIBOOST implemented
them with traditional exponential loss.
A commonness of these existing semi-supervised ensemble methods is that they construct ensembles iteratively,
and in particular, the unlabeled data are exploited through
assigning pseudo-labels for them to enlarge labeled training
set. Speciﬁcally, pseudo-labels of unlabeled instances are
estimated based on the ensemble trained so far , ,
 , , or with speciﬁc form of smoothness or manifold regularization , , . After that, by regarding
the estimated labels as their ground-truth labels, unlabeled
instances are used in conjunction with labeled examples to
update the current ensemble iteratively.
Although various strategies have been employed to make
the pseudo-labeling process more reliable, such as by incorporating data editing , the estimated pseudo-labels may
still be prone to error, especially in initial training iterations
where the ensemble is only moderately accurate. In the next
section we will present the UDEED approach. Rather than
working with pseudo-labels to enlarge labeled training set,
UDEED utilizes unlabeled data in a different way, i.e., help
augment the diversity among base learners.
III. THE UDEED APPROACH
A. General Formulation
= Rd be the d-dimensional input space and
Y = {−1, +1} be the output space. Suppose L = {(xi, yi)|
1 ≤i ≤L} contains L labeled training examples and
U = {xi|L+1 ≤i ≤L+U} contains U unlabeled training
examples, where xi ∈X and yi ∈Y. In addition, we use
˜L = {xi|1 ≤i ≤L} to denote the unlabeled data set
derived from L.
We assume that the classiﬁer ensemble is composed of
m base classiﬁers {fk|1 ≤k ≤m}, where each of them
takes the form fk : X →[−1, +1]. Here, the real value
of fk(x) corresponds to the conﬁdence of x being positive.
Accordingly, (fk(x)+1)/2 can be regarded as the posteriori
probability of being positive given x, i.e. P(y = +1|x).
The basic idea of UDEED is to maximize the ﬁt of the classiﬁers on the labeled data, while maximizing the diversity
of the classiﬁers on the unlabeled data. Therefore, UDEED
generates the classiﬁer ensemble f = (f1, f2, · · · , fm) by
minimizing the following loss function:
V (f, L, D) = Vemp(f, L) + γ · Vdiv(f, D)
Here, the ﬁrst term Vemp(f, L) corresponds to the empirical
loss of f on the labeled data set L; the second term
Vdiv(f, D) corresponds to the diversity loss of f on a
speciﬁed data set D (e.g. D = U). Furthermore, γ is the
cost parameter balancing the importance of the two terms.
In this paper, UDEED calculates the ﬁrst term Vemp(f, L)
in Eq.(1) as:
Vemp(f, L) = 1
Here, l(fk, L) measures the empirical loss of the k-th base
classiﬁer fk on the labeled data set L.
As shown in Eq.(1), the second term Vdiv(f, D) is used
to characterize the diversity among the based learners.
However, it is well-known that diversity measurement is not
a straightforward task since there is no generally accepted
formal deﬁnition . In this paper, UDEED chooses to
calculate Vdiv(f, D) in a novel way as follows:
Vdiv(f, D) =
d(fp, fq, D)
where d(fp, fq, D) =
fp(x)fq(x)
Here, |D| returns the cardinality of data set D. Intuitively,
d(fp, fq, D) represents the prediction difference between any
pair of base classiﬁers on a speciﬁed data set D.1 In addition,
the prediction difference is calculated based on the concrete
output f(x) instead of the signed output sign[f(x)]. In this
way, the prediction conﬁdence of each classiﬁer other than
the simple binary prediction is fully utilized.
Then, UDEED aims to ﬁnd the target model f ∗which
minimizes the loss function in Eq.(1):
f ∗= arg min
V (f, L, D)
B. Logistic Regression Implementation
In this paper, we employ logistic regression to implement
the base classiﬁers. Speciﬁcally, each base classiﬁer fk (1 ≤
k ≤m) is modeled as:
fk(x) = 2 · gk(x) −1 = 2 ·
k ·x+bk) −1
Here, gk : X → is the standard logistic regression
function with weight vector wk ∈Rd and bias value bk ∈
R. Without loss of generality, in the rest of this paper, bk is
absorbed into wk by appending the input space X with an
extra dimension ﬁxed at value 1.
Correspondingly, the ﬁrst term Vemp(f, L) in Eq.(1) is
set to be the negative binomial likelihood function on the
labeled data set L, which is commonly used to measure the
empirical loss of logistic regression:
Vemp(f, L)
−BLH(fk(xi), yi)
Here, the term BLH(fk(xi), yi) calculates the binomial
likelihood of xi having label yi, when fk serves as the classi-
ﬁcation model. Note that the probabilities of P(y = +1|x)
1As reviewed in , most existing diversity measures are calculated
based on the oracle (correct/incorrect) outputs of base learners, i.e. the
ground-truth labels of the data set are assumed to be known. However,
considering that examples contained in the speciﬁed data set D may be
unlabeled, it is then infeasible to calculate d(fp, fq, D) by directly utilizing
existing diversity measures.
and P(y = −1|x) are modeled as
respectively, BLH(fk(xi), yi) then takes the following form
based on Eq.(5):
BLH(fk(xi), yi)
1 + fk(xi)
1 −fk(xi)
Note that the ﬁrst term Vemp(f, L) can also be evaluated in
other ways, such as l2 loss:
i=1 (fk(xi) −yi)2,
hinge loss:
i=1 1 −yifk(xi), etc.
The target model f ∗is found by employing gradient
descent-based techniques. Accordingly, the gradients of
V (f, L, D) with respect to the model parameters Θ =
{wk|1 ≤k ≤m} are determined as follows:2
, · · · , ∂V
, · · · , ∂V
∂BLH(fk(xi), yi)
k′=1, k′̸=k
∂d(fk, fk′, D)
∂BLH(fk(xi), yi)
(1 + yi)(1 −fk(xi))
−(1 −yi)(1 + fk(xi))
∂d(fk, fk′, D)
fk′(x) · (1 −fk(x)2) · x
To initialize the ensemble, each classiﬁer fk is learned from
a bootstrapped sample of L, namely Lk = {(xk
i ≤L}, by conventional maximum likelihood procedure.
Speciﬁcally, the corresponding model parameter wk is obtained by minimizing the objective function
2||wk||2 +
i=1 −BLH(fk(xk
i ). Here, λ balances the model
complexity and the binomial likelihood of fk on Lk. In
2Note that under logistic regression implementation, the loss function
V (f, L, D) is generally non-convex, and the target model f ∗returned by
the gradient descent process would correspond to a local optimal solution.
CHARACTERISTICS OF THE DATA SETS (d: DIMENSIONALITY, pos.: #POSITIVE EXAMPLES, neg.: #NEGATIVE EXAMPLES).
ionosphere 34
7841/24720
40 1527/1669
117 200/200
1479/48270
14 357/212
hepatitis 19
241 734/766
13565/128126
15 307/383
241 750/750
110384/220768
16 108/124
30 1310/1320
241 748/752
283301/297711
this paper, λ is set to the default value of 1. Note that
the ensemble can also be initialized in other ways, such as
instantiating each wk with random values, etc.
As shown in Eq.(1), the second term Vdiv(f, D) regarding
ensemble diversity is deﬁned on a speciﬁed data set D. Given
the labeled training set L and the unlabeled training set U,
we consider three possibilities of instantiating D:
• D = ∅: No data is employed to measure the diversity
among base learners (Vdiv(f, D)=0). The resulting implementation is called LC;
˜L: Labeled training examples are employed
to measure the diversity among base learners, and
the ensemble is optimized by exploiting only L. The
resulting implementation is called LCD;
• D = U: Unlabeled training examples are employed
to measure the diversity among base learners, and the
ensemble is optimized by exploiting both L and U. The
resulting implementation is called LCUD;
For LC and LCD, after the ensemble is initialized, a
series of gradient descent steps are performed to optimize
the model by minimizing the loss function V (f, L, D) as
deﬁned in Eq.(1). For LCUD however, instead of directly
minimizing V (f, L, D) in the straightforward way of setting
D = U, the loss function is ﬁrstly minimized by a series of
gradient descent steps with D = ˜L. After that, by using
the learned model as the starting point, a series of gradient
descent steps are further conducted to ﬁnely search the
model space with D = U. The purpose of this two-stage
process is to distinguish the priorities of the contribution
from labeled data and unlabeled data.3
For any gradient descent-based optimization process, it is
terminated if either the loss function V (f, L, D) or the diversity term Vdiv(f, D) does not decrease anymore. For each
implementation of UDEED, the label of an unseen example z
3Similar strategies have been adopted by some successful semisupervised ensemble methods , , where objective terms involving
labeled data are given much higher weight than those involving unlabeled
is predicted by the learned ensemble f ∗= (f ∗
2 , · · · , f ∗
via weighted voting:4 f ∗(z) = sign [Pm
Intuitively, if the ensemble does beneﬁt from the diversity augmented by the unlabeled training examples, LCUD
should achieve superior performance than LC and LCD.
IV. EXPERIMENTS
In this section, comparative studies between UDEED (i.e.
LCUD) and other semi-supervised ensemble methods are
ﬁrstly reported. More importantly, experimental analysis on
the three different implementations of UDEED are further
conducted to show whether unlabeled data do beneﬁt ensemble learning by helping augment the diversity among
base learners.
Twenty-ﬁve publicly-available binary data sets are used
for experiments, whose characteristics are summarized in
Table I. Fifteen of them are from UCI Machine Learning
Repository , ﬁve from UCI KDD Archive , four from
 and one from . Twenty regular-scale data sets (left
four columns) as well as ﬁve large-scale data sets (right
column) are included. The data set size varies from 57 to
581,012, the dimensionality varies from 8 to 300, and the
ratio between positive examples to negative examples varies
from 0.031 to 3.844.
For each data set, 50% of them are randomly selected
to form the test set T , and the rest is used to form the
training set of L S U. The percentage of labeled data in
training set (i.e. |L|/(|L| + |U|)) is set to be 0.25. For each
data set, 50 random L/U/T splits are performed. Hereafter,
the reported performance of each method corresponds to the
average result out of 50 runs on different splits.
Various ensemble sizes (i.e. m) are considered in the
experiments: a) m = 20 representing the case of small-scale
ensemble; b) m = 50 representing the case of medium-scale
ensemble; and c) m = 100 representing the case of largescale ensemble.5 In addition, as shown in Eq.(1), the cost
4Compared to unweighted voting where the label of z is predicted by
f ∗(z) = sign
k=1 sign[f∗
, the prediction conﬁdence of each
base learner could be fully utilized by weighted voting.
5Preliminary experiments show that, as the ensemble size increases from
10 to 100 within an interval of 100, the performance of UDEED does not
signiﬁcantly change within successive ensemble sizes and tends to converge
as the ensemble size approaches 100.
PREDICTIVE ACCURACY (MEAN±STD.) UNDER small-scale ENSEMBLE SIZE (m = 20). •/◦INDICATES WHETHER UDEED IS STATISTICALLY
SUPERIOR/INFERIOR TO THE COMPARED ALGORITHM (PAIRWISE t-TEST AT 95% SIGNIFICANCE LEVEL).
0.726±0.021
0.690±0.018•
0.728±0.029
0.700±0.031•
0.695±0.019•
0.793±0.040
0.779±0.043•
0.766±0.045•
0.744±0.072•
0.789±0.035
0.927±0.014
0.807±0.024•
0.934±0.025
0.898±0.070•
0.793±0.028•
0.834±0.023
0.810±0.024•
0.809±0.028•
0.801±0.038•
0.815±0.029•
0.921±0.028
0.922±0.027
0.849±0.156•
0.921±0.036
0.924±0.029
0.932±0.017
0.930±0.018•
0.906±0.106
0.928±0.019
0.932±0.017
0.916±0.019
0.914±0.021
0.916±0.064
0.921±0.029
0.886±0.026•
0.800±0.042
0.792±0.026
0.763±0.077•
0.788±0.041
0.796±0.026
0.809±0.072
0.801±0.074
0.646±0.142•
0.747±0.075•
0.810±0.071
0.944±0.007
0.942±0.008•
0.934±0.013•
0.939±0.010•
0.929±0.009•
ionosphere
0.795±0.043
0.721±0.023•
0.807±0.037
0.772±0.038•
0.746±0.027•
0.940±0.008
0.938±0.008•
0.941±0.009
0.942±0.010
0.936±0.008•
0.989±0.007
0.988±0.006
0.714±0.244•
0.985±0.010•
0.989±0.005
0.690±0.069
0.690±0.070
0.701±0.063
0.672±0.068
0.692±0.067
0.777±0.035
0.785±0.035◦
0.747±0.039•
0.748±0.037•
0.765±0.041•
0.690±0.024
0.710±0.019◦
0.678±0.023•
0.686±0.025
0.702±0.019◦
0.582±0.039
0.576±0.039•
0.606±0.040◦
0.575±0.037
0.569±0.049•
0.939±0.010
0.940±0.009
0.928±0.012•
0.927±0.012•
0.941±0.009◦
0.807±0.029
0.809±0.028
0.862±0.017◦
0.819±0.023◦
0.823±0.021◦
0.793±0.020
0.794±0.018
0.760±0.021•
0.751±0.020•
0.791±0.022
0.835±0.003
0.844±0.002◦
0.840±0.003◦
0.843±0.002◦
0.981±0.001
0.980±0.001•
0.980±0.001•
0.981±0.001◦
0.914±0.001
0.906±0.001•
0.910±0.004•
0.906±0.001•
0.920±0.001
0.850±0.001•
0.945±0.003◦
0.851±0.002•
0.706±0.002
0.703±0.002•
0.736±0.006◦
0.696±0.002•
win/tie/loss
parameter γ is set to the default value of 1. Note that better
performance can be expected if certain strategies such as
cross-validation are employed to optimize the value of γ.
A. Comparative Studies
In this subsection, UDEED (LCUD) is compared with two
popular ensemble methods BAGGING and ADABOOST
 , and two successful semi-supervised ensemble methods
ASSEMBLE and SEMIBOOST . For fair comparison,
logistic regression is employed as the base learner of each
compared method. For UDEED, the maximum number of
gradient descent steps is set to 25 and the learning rate is set
to 0.25. For the other compared methods, default parameters
suggested in respective literatures are adopted.
Tables II to IV report the detailed experimental results
under small-scale (m=20), medium-scale (m=50) and largescale (m=100) ensemble sizes respectively. SEMIBOOST
fails to work on the large-scale data sets, due to its demanding storage complexity (O((|L| + |U|)2)) to maintain
the similarity matrix for the training examples.
On each data set, the mean predictive accuracy as well
as the standard deviation of each algorithm (out of 50
runs) are recorded. Furthermore, to statistically measure the
signiﬁcance of performance difference, pairwise t-tests at
95% signiﬁcance level are conducted between the algorithms. Speciﬁcally, whenever UDEED achieves signiﬁcantly
better/worse performance than the compared algorithm on
any data set, a win/loss is counted and a maker •/◦is
shown. Otherwise, a tie is counted and no marker is given.
The resulting win/tie/loss counts for UDEED against the
compared algorithms are highlighted in the last line of each
In summary, when the ensemble size is small (Table II),
UDEED is statistically superior to BAGGING, ADABOOST,
ASSEMBLE and SEMIBOOST in 52%, 52%, 56% and 45%
cases, and is inferior to them in much less 12%, 20%,
12% and 15% cases; When the ensemble size is medium
(Table III), UDEED is statistically superior to BAGGING,
PREDICTIVE ACCURACY (MEAN±STD.) UNDER medium-scale ENSEMBLE SIZE (m = 50). •/◦INDICATES WHETHER UDEED IS STATISTICALLY
SUPERIOR/INFERIOR TO THE COMPARED ALGORITHM (PAIRWISE t-TEST AT 95% SIGNIFICANCE LEVEL).
0.710±0.020
0.691±0.019•
0.731±0.026◦
0.699±0.032•
0.696±0.019•
0.794±0.033
0.782±0.032•
0.766±0.037•
0.736±0.078•
0.794±0.033
0.885±0.017
0.806±0.022•
0.925±0.065◦
0.916±0.046◦
0.816±0.033•
0.828±0.024
0.812±0.028•
0.808±0.025•
0.815±0.036•
0.816±0.029•
0.921±0.030
0.920±0.030
0.793±0.195•
0.925±0.034
0.924±0.029◦
0.931±0.017
0.929±0.018•
0.868±0.151•
0.927±0.019
0.932±0.017
0.914±0.022
0.914±0.021
0.914±0.088
0.919±0.025
0.893±0.026•
0.796±0.031
0.792±0.022
0.737±0.106•
0.785±0.045
0.797±0.027
0.813±0.083
0.799±0.079•
0.681±0.142•
0.749±0.095•
0.804±0.083
0.944±0.006
0.942±0.007•
0.937±0.013•
0.939±0.011•
0.931±0.009•
ionosphere
0.797±0.042
0.722±0.022•
0.814±0.035◦
0.783±0.027•
0.748±0.028•
0.939±0.008
0.938±0.008•
0.943±0.011◦
0.943±0.009◦
0.935±0.008•
0.989±0.006
0.988±0.007•
0.672±0.232•
0.986±0.008•
0.990±0.005
0.687±0.069
0.690±0.072
0.714±0.059◦
0.679±0.070
0.696±0.068
0.783±0.033
0.783±0.036
0.744±0.043•
0.748±0.046•
0.763±0.040•
0.703±0.024
0.711±0.020◦
0.674±0.026•
0.689±0.025•
0.703±0.019
0.582±0.041
0.577±0.041
0.620±0.043◦
0.583±0.051
0.572±0.045•
0.941±0.010
0.940±0.010
0.929±0.012•
0.925±0.012•
0.941±0.009
0.808±0.027
0.812±0.024
0.867±0.016◦
0.821±0.022◦
0.820±0.022◦
0.796±0.019
0.794±0.018
0.762±0.023•
0.750±0.020•
0.791±0.022•
0.842±0.002
0.844±0.002◦
0.841±0.002•
0.842±0.002◦
0.981±0.001
0.980±0.001•
0.980±0.001
0.981±0.001◦
0.907±0.001
0.906±0.001•
0.906±0.001•
0.910±0.004◦
0.891±0.001
0.851±0.001•
0.945±0.003◦
0.851±0.003•
0.705±0.002
0.703±0.002•
0.737±0.006◦
0.698±0.003•
win/tie/loss
ADABOOST, ASSEMBLE and SEMIBOOST in 56%, 56%,
52% and 50% cases, and is inferior to them in much less 8%,
36%, 24% and 10% cases; When the ensemble size is large
(Table IV), UDEED is statistically superior to BAGGING,
ADABOOST, ASSEMBLE and SEMIBOOST in 48%, 52%,
52% and 40% cases, and is inferior to them in much
less 8%, 40%, 20% and 15% cases. These results indicate
that UDEED is highly competitive to the other compared
methods. Roughly speaking, as for the time complexity,
UDEED is slightly higher than BAGGING and ADABOOST
while fairly comparable to ASSEMBLE and SEMIBOOST.
B. The Helpfulness of Unlabeled Data
As motivated in Section I, UDEED aims to exploit unlabeled data to help ensemble learning in the particular way
of augmenting diversity among base learners. Therefore, in
addition to the above comparative experiments with other
(semi-supervised) ensemble methods, it is more important
to show whether UDEED (LCUD) does achieve better performance than its counterparts (LC and LCD) which do not
consider using unlabeled data for diversity augmentation.
Table V reports the performance improvement (i.e. increase of predictive accuracy) of LCUD against LC and
LCD under various ensemble sizes. On each data set, the
mean improved predictive accuracy as well as the standard
deviation (out of 50 runs) are recorded. In addition, to statistically measure the signiﬁcance of performance difference,
pairwise t-tests at 95% signiﬁcance level are conducted.
Speciﬁcally, whenever LCUD achieves signiﬁcantly superior/inferior performance than LC or LCD on any data set, a
win/loss is counted and a maker •/◦is shown in the Table.
Otherwise, a tie is counted and no marker is given. The
resulting win/tie/loss counts for LCUD against LC and LCD
are highlighted in the last line of Table V.
In summary, when the ensemble size is small, LCUD is
statistically superior to LC and LCD in 64% and 56% cases,
and is inferior to them in both only 12% cases; When the
ensemble size is medium, LCUD is statistically superior to
PREDICTIVE ACCURACY (MEAN±STD.) UNDER large-scale ENSEMBLE SIZE (m = 100). •/◦INDICATES WHETHER UDEED IS STATISTICALLY
SUPERIOR/INFERIOR TO THE COMPARED ALGORITHM (PAIRWISE t-TEST AT 95% SIGNIFICANCE LEVEL).
0.700±0.020
0.692±0.018•
0.726±0.032◦
0.694±0.031
0.696±0.018•
0.790±0.035
0.781±0.035•
0.757±0.041•
0.751±0.066•
0.792±0.036
0.852±0.021
0.805±0.019•
0.930±0.064◦
0.916±0.037◦
0.825±0.030•
0.824±0.025
0.812±0.024•
0.806±0.027•
0.808±0.038•
0.817±0.028•
0.921±0.028
0.921±0.029
0.831±0.180•
0.919±0.029
0.924±0.029◦
0.930±0.017
0.930±0.018
0.902±0.104
0.926±0.020
0.932±0.017◦
0.913±0.022
0.915±0.022
0.930±0.026◦
0.911±0.031
0.897±0.027•
0.797±0.027
0.790±0.023•
0.743±0.101•
0.782±0.040•
0.797±0.026
0.811±0.080
0.808±0.080
0.683±0.146•
0.756±0.098•
0.809±0.075
0.943±0.007
0.942±0.007
0.938±0.012•
0.939±0.011•
0.932±0.008•
ionosphere
0.780±0.032
0.721±0.023•
0.812±0.037◦
0.779±0.042
0.747±0.027•
0.939±0.008
0.938±0.007•
0.945±0.011◦
0.944±0.008◦
0.935±0.008•
0.989±0.006
0.989±0.006•
0.616±0.208•
0.984±0.012•
0.990±0.005
0.690±0.071
0.689±0.070
0.713±0.061◦
0.679±0.063
0.696±0.069
0.784±0.033
0.786±0.033
0.741±0.041•
0.745±0.051•
0.763±0.042•
0.706±0.021
0.711±0.021◦
0.679±0.024•
0.686±0.026•
0.703±0.019
0.580±0.041
0.578±0.042
0.620±0.043◦
0.588±0.041
0.572±0.046
0.940±0.009
0.940±0.010
0.927±0.013•
0.925±0.011•
0.941±0.009
0.807±0.027
0.811±0.024
0.870±0.016◦
0.819±0.027◦
0.820±0.021◦
0.795±0.018
0.796±0.018
0.760±0.023•
0.754±0.027•
0.792±0.022
0.844±0.002
0.844±0.002◦
0.840±0.002•
0.843±0.002•
0.981±0.001
0.980±0.001•
0.980±0.002
0.981±0.001◦
0.906±0.001
0.905±0.004•
0.906±0.001•
0.906±0.001◦
0.873±0.001
0.851±0.001•
0.945±0.003◦
0.851±0.003•
0.705±0.002
0.703±0.002•
0.737±0.006◦
0.698±0.003•
win/tie/loss
LC and LCD in both 52% cases, and is inferior to them
in both only 8% cases; When the ensemble size is large,
LCUD is statistically superior to LC and LCD in 52% and
56% cases, and is inferior to them in only 8% and 12% cases.
These results indicate that, by exploiting unlabeled data in
the speciﬁc way of helping augment ensemble diversity,
UDEED (LCUD) is capable of achieving better performance
than its counterparts (LC and LCD) which do not consider
employing unlabeled in ensemble generation.6
C. Diversity Analysis
To clearly verify that UDEED (LCUD) does increase the
diversity among base learners after generating ensemble by
utilizing unlabeled data, additional experiments are analyzed
in this subsection based on several existing diversity measures. Speciﬁcally, four diversity measures summarized in
6Note that although in a number of cases the accuracy difference between
two algorithms looks rather marginal (e.g. less than 1%), the difference may
still be statistically signiﬁcant according to the pairwise t-test.
 are considered, whose values are calculated based on
the oracle (correct/incorrect) outputs of base learners.
Suppose m denotes the number of base classiﬁers in the
ensemble and N denotes the number of examples in the
test set T . In addition, let O = [oij]m×N be the oracle
output matrix. Here, oij = 1 if the i-th base learner correctly
classiﬁes the j-th test example (1 ≤i ≤m, 1 ≤j ≤N).
Otherwise, oij = 0. The formal deﬁnitions of the four
diversity measures are as follows:
• Disagreement measure (DIS):
j=1 oij · (1 −okj) + PN
j=1(1 −oij) · okj
ACCURACY IMPROVEMENT (MEAN±STD.) FOR LCUD AGAINST LC AND LCD UNDER VARIOUS ENSEMBLE SIZES. •/◦INDICATES WHETHER LCUD IS
STATISTICALLY SUPERIOR/INFERIOR TO THE COMPARED IMPLEMENTATION (PAIRWISE t-TEST AT 95% SIGNIFICANCE LEVEL).
Accuracy Improvement of LCUD against
0.034±0.024•
0.019±0.013•
0.008±0.011•
0.011±0.012•
0.009±0.009•
0.004±0.007•
0.023±0.027•
0.009±0.016•
0.006±0.013•
0.009±0.016•
0.003±0.010•
0.004±0.009•
0.127±0.024•
0.075±0.012•
0.047±0.013•
0.033±0.014•
0.031±0.013•
0.023±0.008•
0.022±0.022•
0.015±0.013•
0.010±0.008•
0.004±0.012•
0.006±0.008•
0.005±0.005•
0.003±0.010•
-0.001±0.005
0.001±0.004•
0.002±0.007•
0.000±0.004
0.001±0.003•
0.002±0.005•
0.001±0.003•
0.001±0.003•
0.001±0.004
0.001±0.002•
0.001±0.001•
0.005±0.010•
0.002±0.005
0.001±0.004
0.003±0.007•
0.001±0.005
0.001±0.004
0.010±0.035
0.005±0.027
0.008±0.017•
0.003±0.027
0.001±0.019
0.005±0.012•
0.003±0.071
0.004±0.043
0.004±0.018
-0.007±0.041
0.007±0.032
0.004±0.012•
0.002±0.003•
0.001±0.002•
0.001±0.002•
0.001±0.002•
0.001±0.001•
0.001±0.001•
ionosphere
0.073±0.049•
0.076±0.049•
0.057±0.035•
0.015±0.034•
0.022±0.032•
0.029±0.024•
0.002±0.003•
0.001±0.002•
0.001±0.001•
0.001±0.001•
0.001±0.001•
0.001±0.001•
0.001±0.003•
0.001±0.002•
0.001±0.002
0.001±0.002
0.001±0.001•
0.001±0.001
0.001±0.036
0.003±0.022
0.001±0.015
0.002±0.016
-0.001±0.014
0.001±0.011
-0.006±0.014◦
-0.003±0.012
-0.001±0.008
-0.003±0.010◦
-0.003±0.009
0.001±0.006
-0.019±0.017◦
-0.008±0.010◦
-0.005±0.008◦
-0.009±0.010◦
-0.004±0.006◦
-0.002±0.006◦
0.006±0.015•
0.003±0.010
0.002±0.012
0.005±0.010•
0.002±0.010
0.002±0.011
0.001±0.005
0.001±0.002
0.001±0.004
0.001±0.005
0.001±0.002
0.001±0.003
-0.001±0.016
-0.004±0.016
-0.003±0.015
0.001±0.005
-0.001±0.006
-0.002±0.007◦
0.001±0.005
0.001±0.004
-0.001±0.004
-0.001±0.004
0.001±0.004
-0.001±0.004
-0.009±0.002◦
-0.002±0.002◦
-0.001±0.001◦
-0.006±0.001◦
-0.002±0.001◦
-0.001±0.001◦
0.001±0.001•
0.001±0.001•
0.000±0.000
0.001±0.001•
0.001±0.001•
0.000±0.000
0.008±0.001•
0.001±0.001•
0.001±0.001•
0.006±0.001•
0.001±0.001•
0.001±0.001•
0.069±0.001•
0.041±0.001•
0.023±0.001•
0.022±0.001•
0.018±0.001•
0.011±0.001•
0.003±0.001•
0.002±0.001•
0.001±0.001•
0.001±0.001•
0.001±0.001•
0.001±0.001•
win/tie/loss
• Double-fault measure (DF):
j=1(1 −oij) · (1 −okj)
• Entropy measure (ENT):
m −⌈m/2⌉min
• Coincident failure diversity (CFD):
j=1 1[i=Pm
k=1(1−okj)]
Here, DIS and DF are pairwise measures while ENT and
CFD are non-pairwise measures. In addition, 1-DF is used
instead of DF such that for all the measures, the greater the
value the higher the diversity. All the four measures vary
between 0 and 1.
Table VI compares UDEED’s initial diversity after ensemble initialization with its ﬁnal diversity after ensemble learning under various ensemble sizes. For each data set, pairwise
t-tests at 95% signiﬁcance level are conducted between the
initial and the ﬁnal ensemble diversities. Whenever the ﬁnal
THE WIN/TIE/LOSS RESULTS FOR FINAL ENSEMBLE AGAINST INITIAL ENSEMBLE IN TERMS OF THE FOUR DIVERSITY MEASURES UNDER VARIOUS
ENSEMBLE SIZES.
FINAL ensemble vs. INITIAL ensemble
ionosphere
win/tie/loss
ensemble achieves signiﬁcantly higher/lower diversity than
the initial one, a win/loss is recorded. Otherwise, a tie is
recorded. The resulting win/tie/loss counts are highlighted
in the last line of Table VI.
In summary, when the ensemble size is small, UDEED
statistically increases the initial ensemble diversity in 60%
(DIS), 56% (DF), 60% (ENT) and 60% (CFD) cases, but
decreases the initial ensemble diversity in only 16% (DIS),
20% (DF), 16% (ENT) and 8% (CFD) cases.
When the ensemble size is medium, UDEED statistically
increases the initial ensemble diversity in 56% (DIS), 56%
(DF), 56% (ENT) and 48% (CFD) cases, but decreases the
initial ensemble diversity in only 24% (DIS), 16% (DF),
16% (ENT) and 8% (CFD) cases;
Finally, when the ensemble size is large, UDEED statistically increases the initial ensemble diversity in 68% (DIS),
68% (DF), 64% (ENT) and 48% (CFD) cases, but decreases
the initial ensemble diversity in only 16% (DIS), 16% (DF),
16% (ENT) and 12% (CFD) cases.
These results clearly verify that UDEED can effectively
exploit unlabeled data to help augment ensemble diversity.
V. CONCLUSION
Previous ensemble methods try to obtain a high accuracy
of base learners and high diversity among base learners
by considering only labeled data. There were some studies
on using unlabeled data, but focusing on using unlabeled
data to improve accuracy. The major contribution of our
work is to use unlabeled data to augment diversity, which
suggests a new direction for ensemble design. Speciﬁcally,
a novel semi-supervised ensemble method named UDEED is
proposed, which works by maximizing accuracy on labeled
data while maximizing diversity on unlabeled data.
Experiments show that: a) UDEED achieves highly comparable performance against other successful semi-supervised
ensemble methods; b) UDEED does beneﬁt from unlabeled
data by using them to augment the diversity among base
learners. In the future, it is interesting to see whether UDEED
works well with other base learners. It would be insightful
to analyze why UDEED can achieve good performance theoretically. Furthermore, designing other ensemble methods
by exploiting unlabeled data to augment ensemble diversity
gracefully is a direction very worth studying.
ACKNOWLEDGMENT
The authors wish to thank the anonymous reviewers
for their helpful comments in improving this paper. This
work was supported by the National Science Foundation of China (60635030, 60805022), the National Fundamental Research Program of China (2010CB327903),
the Ph.D. Programs Foundation of Ministry of Education
of China (200802941009), the Jiangsu Science Foundation
(BK2008018) and the Jiangsu 333 Program.