Machine Learning, 30, 23–29 
c⃝1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
A Note on Learning from Multiple-Instance
AVRIM BLUM
avrim+@cs.cmu.edu
ADAM KALAI
akalai+@cs.cmu.edu
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213
Editor: Dana Ron
Abstract. We describe a simple reduction from the problem of PAC-learning from multiple-instance examples
to that of PAC-learning with one-sided random classiﬁcation noise. Thus, all concept classes learnable with onesided noise, which includes all concepts learnable in the usual 2-sided random noise model plus others such as the
parity function, are learnable from multiple-instance examples. We also describe a more efﬁcient (and somewhat
technically more involved) reduction to the Statistical-Query model that results in a polynomial-time algorithm
for learning axis-parallel rectangles with sample complexity ˜O(d2r/ϵ2), saving roughly a factor of r over the
results of Auer et al. .
Keywords: Multiple-instance examples, classiﬁcation noise, statistical queries
Introduction and Deﬁnitions
In the standard PAC learning model, a learning algorithm is repeatedly given labeled examples of an unknown target concept, drawn independently from some probability distribution.
The goal of the algorithm is to approximate the target concept with respect to this distribution. In the multiple-instance example setting, introduced in , the
learning algorithm is given only the following weaker access to the target concept: instead
of seeing individually labeled points from the instance space, each “example” is an r-tuple
of points together with a single label that is positive if at least one of the points in the
r-tuple is positive (and is negative otherwise). The goal of the algorithm is to approximate
the induced concept over these r-tuples. In the application considered by Dietterich et al.,
an example is a molecule and the points that make up the example correspond to different
physical conﬁgurations of that molecule; the label indicates whether or not the molecule
has a desired binding behavior, which occurs if at least one of the conﬁgurations has the
Formally, given a concept c over instance space X, let us deﬁne cmulti over X∗as:
cmulti(x1, x2, . . . , xr) = c(x1) ∨c(x2) ∨. . . ∨c(xr).
Similarly, given a concept class C, let Cmulti = {cmulti : c ∈C}. We will call ⃗x =
(x1, . . . , xr) an r-example or r-instance. Long and Tan give a natural PAC-style
formalization of the multiple-instance example learning problem, which we may phrase as
Deﬁnition 1.
An algorithm A PAC-learns concept class C from multiple-instance
examplesifforanyr > 0, andanydistributionD oversingleinstances, APAC-learnsCmulti
A. BLUM AND A. KALAI
over distribution Dr. (That is, each instance in each r-example is chosen independently
from the same distribution D.)
Previous work on learning from multiple-instance examples has focused on the problem
of learning d-dimensional axis-parallel rectangles. Dietterich et al. present several
algorithms and describe experimental results of their performance on a molecule-binding
domain. LongandTan describeanalgorithmthatlearnsaxis-parallelrectanglesinthe
above PAC setting, under the condition that D is a product distribution (i.e., the coordinates
of each single-instance are chosen independently), with sample complexity ˜O(d2r6/ϵ10).
Auer et al. give an algorithm that does not require D to be a product distribution and
has a much improved sample complexity ˜O(d2r2/ϵ2) and running time ˜O(d3r2/ϵ2). (The
˜O notation hides logarithmic factors.) Auer reports on the empirical performance
of this algorithm. Auer et al. also show that if we generalize Deﬁnition 1 so that the
distribution over r-examples is arbitrary (rather than of the form Dr) then learning axisparallel rectangles is as hard as learning DNF formulas in the PAC model.
In this paper we describe a simple general reduction from the problem of PAC-learning
from multiple-instance examples to that of PAC-learning with one-sided random classiﬁcation noise. Thus, all concept classes learnable from one-sided noise are PAC-learnable
from multiple-instance examples. This includes all classes learnable in the usual 2-sided
random noise model, such as axis-parallel rectangles, plus others such as parity functions.
We also describe a more efﬁcient reduction to the Statistical-Query model .
For the case of axis-parallel rectangles, this results in an algorithm with sample complexity
˜O(d2r/ϵ2), saving roughly a factor of r over the results in .
A simple reduction to learning with noise
Let us deﬁne 1-sided random classiﬁcation noise to be a setting in which positive examples
are correctly labeled but negative examples have their labels ﬂipped with probability η < 1,
and the learning algorithm is allowed time polynomial in
Theorem 1 If C is PAC-learnable from 1-sided random classiﬁcation noise, then C is
PAC-learnable from multiple-instance examples.
Corollary 1 If C is PAC-learnable from (2-sided) random classiﬁcation noise, then
C is learnable from multiple-instance examples. In particular, this includes all classes
learnable in the Statistical Query model.
Proof (of Theorem 1 and Corollary 1): Let D be the distribution over single instances,
so each multiple-instance example consists of r independent draws from D. Let pneg be
the probability a single instance drawn from D is a negative example of target concept c.
So, a multiple-instance example has probability qneg = (pneg)r of being labeled negative.
Let ˆqneg denote the fraction of observed multiple-instance examples labeled negative; i.e.,
ˆqneg is the observed estimate of qneg. Our algorithm will begin by drawing O( 1
examples and halting with the hypothesis “all positive” if ˆqneg < 3ϵ/4. Chernoff bounds
guarantee that if qneg < ϵ/2 then with high probability we will halt at this stage, whereas if
A NOTE ON LEARNING FROM MULTIPLE-INSTANCE EXAMPLES
qneg > ϵ then with high probability we will not. So, from now on we may assume without
loss of generality that qneg ≥ϵ/2.
Given a source of multiple-instance examples, we now convert it into a distribution over
single-instance examples by simply taking the ﬁrst instance from each example and ignoring
the rest. Notice that the instances produced are distributed independently according to D
and for each such instance x,
if x is a true positive, it is labeled positive with probability 1,
if x is a true negative, it is labeled negative with probability (pneg)r−1, independent of
the other instances and labelings in the ﬁltered distribution.
Thus, we have reduced the multiple-instance learning problem to the problem of learning
with 1-sided classiﬁcation noise, with noise rate η = 1 −(pneg)r−1. Furthermore, η is not
too close to 1, since
η = 1 −(pneg)r−1 ≤1 −qneg ≤1 −ϵ/2.
Wecannowreducethisfurthertothemorestandardproblemoflearningfrom2-sidednoise
by independently ﬂipping the label on each positive example with probability ν = η/(1+η)
(that is, the noise rate on positive examples, ν, equals the noise rate on negative examples,
η(1 −ν)). This results in 2-sided random classiﬁcation noise with noise rate
ν ≤(1 −ϵ/2)/(2 −ϵ/2) ≤1/2 −ϵ/8.
This reduction to 2-sided noise nominally requires knowing η; however, there are two easy
ways around this. First, if there are m+ positive examples, then for each i ∈{0, 1, . . . , m+}
we can just ﬂip the labels on a random subset of i positive examples and apply our 2-sided
noise algorithm, verifying the m+ hypotheses produced on an independent test set. The
desired experiment of ﬂipping each positive label with probability ν can be viewed as a
probability distribution over these m+ experiments, and therefore if the class is learnable
with 2-sided noise then at least one of these will succeed. A second approach is that we in
fact do have a good guess for η: η = 1 −(qneg)1−1/r, so ˆη = 1 −(ˆqneg)1−1/r provides a
good estimate for sufﬁciently large sample sizes. We discuss the details of this approach in
the next section.
Finally, notice that it sufﬁces to approximate c to error ϵ/r over single instances to achieve
an ϵ-approximation over r-instances.
While we can reduce 1-sided noise to 2-sided noise as above, 1-sided noise appears to be a
strictly easier setting. For instance, the class of parity functions, not known to be learnable
with 2-sided noise, is easily learnable with 1-sided noise because parity is learnable from
negative examples only. In fact, we do not know of any concept class learnable in the PAC
model that is not also learnable with 1-sided noise.
A more efﬁcient reduction
We now describe a reduction to the Statistical Query model of Kearns that is more
efﬁcient than the above method in that all of the r single instances in each r-instance are
A. BLUM AND A. KALAI
used. Our reduction turns out to be simpler than the usual reduction from classiﬁcation
noise for two reasons. First of all, we have a good estimate of the noise just based on the
observed fraction of negatively-classiﬁed r-instances, ˆqneg. Secondly, we have a source of
examples with known (negative) classiﬁcations.
Informally, a Statistical Query is a request for a statistic about labeled instances drawn
independently from D. For example, we might want to know the probability that a random
instance x ∈ℜis labeled negative and satisﬁes x < 2. Formally, a statistical query is a
pair (χ, τ), where χ is a function χ : X × {0, 1} →{0, 1} and τ ∈(0, 1). The statistical
query returns an approximation ˆ
Pχ to the probability Pχ = Prx∈D[χ(x, c(x)) = 1], with
the guarantee that Pχ −τ ≤ˆ
Pχ ≤Pχ + τ. We know, from Corollary 1, that anything
learnable in the Statistical Query model can be learned from multiple instance examples.
In this section we give a reduction which shows:
Theorem 2 Given any δ, τ ∈(0, 1/r) and a lower bound ˜qneg on qneg, we can use
a multiple-instance examples oracle to simulate n Statistical Queries of tolerance τ with
probabilityatleast1−δ, usingO( ln(n/δ)
rτ 2 ˜qneg ) r-instances, andintimeO( ln(n/δ)
˜qneg +nTχ)),
where Tχ is the time to evaluate a query.
We begin by drawing a set R of r-instances. Let S−be the set of single instances
from the negative r-instances in R, and let S+/−be the set of single instances from all
r-instances in R. Thus the instances in S+/−are drawn independently from D, and those
in S−are drawn independently from D−, the distribution induced by D over the negative
instances.
We now estimate ˆqneg = |S−|/|S+/−| and ˆpneg = (ˆqneg)1/r. Chernoff bounds guarantee
that so long as |R| ≥k ln(1/δ)
r2τ 2 ˜qneg for sufﬁciently large constant k, with probability at least
qneg(1 −rτ/12) ≤ˆqneg ≤qneg(1 + rτ/6).
This implies
pneg(1 −rτ/12)1/r ≤ˆpneg ≤pneg(1 + rτ/6)1/r,
pneg(1 −τ/6) ≤ˆpneg ≤pneg(1 + τ/6)
where the last line follows using the fact that τ/6 < 1/r.
Armed with S+/−, S−, and ˆpneg, we are ready to handle a query. Our method will be
similar in style to the usual simulation of Statistical Queries in the 2-sided noise model
 , but different in the details because we have 1-sided noise (and, in fact,
simpler because we have an estimate ˆpneg of the noise rate). Observe that, for an arbitrary
subset S ⊆X, we can directly estimate Prx∈D[x ∈S] from S+/−. Using examples from
S−, we can also estimate the quantity,
Prx∈D[x ∈S ∧c(x) = 0] = Prx∈D[c(x) = 0]Prx∈D[x ∈S|c(x) = 0]
= pnegPrx∈D−[x ∈S].
Suppose we have some query (χ, τ). Deﬁne two sets: X0 consists of all points x ∈X
such that χ(x, 0) = 1, and X1 consists of all points x ∈X such that χ(x, 1) = 1. Based
A NOTE ON LEARNING FROM MULTIPLE-INSTANCE EXAMPLES
on these deﬁnitions and (1), we can rewrite Pχ,
Pχ = Prx∈D[x ∈X1 ∧c(x) = 1] + Prx∈D[x ∈X0 ∧c(x) = 0]
= Prx∈D[x ∈X1] −Prx∈D[x ∈X1 ∧c(x) = 0]
+Prx∈D[x ∈X0 ∧c(x) = 0]
= Prx∈D[x ∈X1] + pneg(Prx∈D−[x ∈X0] −Prx∈D−[x ∈X1]).
Each of the three probabilities in the last equation is easily estimated from S+/−or S−as
Using k ln(n/δ)
examples from S+/−, estimate Prx∈D[x ∈X1].
Using k ln(n/δ)
examples from S−, estimate Prx∈D−[x ∈X0] and
Prx∈D−[x ∈X1].
Combine these with ˆpneg to get an estimate ˆPχ for
Pχ = Prx∈D[x ∈X1] + pneg(Prx∈D−[x ∈X0] −Prx∈D−[x ∈X1]).
We can choose k large enough so that, with probability at least 1−δ/2n, our estimates for
Prx∈D[x ∈X1], Prx∈D−[x ∈X0], and Prx∈D−[x ∈X1] are all within an additive τ/6
of their true values. From above, we already know that ˆpneg is within an additive τ/6 of
pneg. Now, since we have an additive error of at most τ/6 on all quantities in (2), and each
quantity is at most 1, our error on Pχ will be at most τ/6 + (1 + τ/6)(1 + 2τ/6) −1 < τ,
with probability at least 1 −δ for all n queries. The runtime for creating S+/−and S−is
O( ln(n/δ)
τ 2 ˜qneg ) and for each query is O( ln(n/δ)
Tχ). The total number of r-instances required
is O( ln(n/δ)
rτ 2 ˜qneg ).
As noted in Section 2, if we can approximate the target concept over single instances
to error ϵ/r, then we have an ϵ-approximation over multiple-instance examples. Again, if
we begin by drawing O( 1
δ ) examples and halting with the hypothesis “all positive” if
ˆqneg < 3ϵ/4, then we get (using the lower bound ϵ/2 for qneg),
Corollary 2 Suppose C is PAC-learnable to within error ϵ/r with n statistical queries
of tolerance τ < 1/r, which can each be evaluated in time Tχ (so n, τ, and Tχ depend on
ϵ/r). Then C is learnable from multiple-instance examples with probability at least 1 −δ,
using O( ln(n/δ)
rϵτ 2 ) r-instances, and in time O( ln(n/δ)
ϵ + nTχ)).
The following theorem for the speciﬁc case of axis-parallel
rectangles) gives a somewhat better bound on the error we need on single-instance examples.
Theorem 3 If qneg ≥ϵ
4 and errorD(c, h) < ϵ
4qneg , then errorDr(cmulti, hmulti) < ϵ.
Letp1 = Prx∈D[c(x) = 0∨h(x) = 0]andp2 = Prx∈D[c(x) = 0∧h(x) = 0]. So,
errorD(c, h) = p1−p2. Notice that Pr⃗x∈Dr[cmulti(⃗x) = hmulti(⃗x) = 0] = pr
2. Also note
that Pr⃗x∈Dr[cmulti(⃗x) = hmulti(⃗x) = 1] ≥1−pr
1 because all r-instances that fail to satisfy
this equality must have their components drawn from the region [c(x) = 0 ∨h(x) = 0].
Therefore,
errorDr(cmulti, hmulti) ≤pr
A. BLUM AND A. KALAI
= (p1 −p2)(pr−1
p2 + . . . + pr−1
≤(p1 −p2)rpr−1
Axis-Parallel Rectangles
The d-dimensional axis-parallel rectangle deﬁned by two points, (a1, . . . , ad) and (b1, . . . ,
bd), is {⃗x|xi ∈[ai, bi], i = 1, . . . , d}. The basic approach to learning axis-parallel rectangles with statistical queries is outlined in and is similar to .
Suppose we have some target rectangle deﬁned by two points, (a1, . . . , ad) and (b1, . . . , bd),
with ai < bi. Our strategy is to make estimates (ˆa1, . . . , ˆad) and (ˆb1, . . . ,ˆbd), with ˆai ≥ai
and ˆbi ≤bi so that our rectangle is contained inside the true rectangle but so that it is
unlikely that any point has ith coordinate between ai and ˆai or between ˆbi and bi. We
assume in what follows that ϵ/2 ≤qneg ≤1 −ϵ/2, and that we have estimates of pneg and
qneg good to within a factor of 2, which we may do by examining an initial sample of size
qneg . From Theorem 3, we see that if we have error less than τ per side of
the rectangle, then we will have less than ϵ error for the r-instance problem, and we are
done. For simplicity, the argument below will assume that τ is known; if desired one can
instead use an estimate of τ obtained from sampling, in a straightforward way.
We ﬁrst ask the statistical query Prx∈D[c(x) = 1] to tolerance τ/3. If the result is
less than 2τ/3 then 1 −pneg ≤τ, and (using Theorem 3) we can safely hypothesize that
all points are negative. Otherwise we know pneg ≤1 −τ/3. Deﬁne (a′
1, . . . , a′
1, . . . , b′
d) such that Prx∈D(ai ≤xi ≤a′
i) = τ/3 and Prx∈D(b′
i ≤x ≤bi) = τ/3. (If
the distribution is not continuous, then let a′
i be such that Prx∈D(ai ≤xi < a′
i) ≤τ/3 and
Prx∈D(ai ≤xi ≤a′
i) ≥τ/3, and similarly for b′
i.) We now explain how to calculate ˆa1,
for example, without introducing error of more than τ.
Take m = O(ln(d/δ)/τ) unlabeled sample points. With probability at least 1-δ/2d, one
of these points has its ﬁrst coordinate between a1 and a′
1 (inclusive) and let us assume this is
the case. We will now do a binary search among the ﬁrst coordinates of these points, viewing
each as a candidate for ˆa1 and asking the statistical query Prx∈D[c(x) = 1∧x1 < ˆa1] with
tolerance τ/3. If all of our log m queries are inside our tolerance, then we are guaranteed
that some ˆa1 ≥a1 will return a value at most 2τ/3. In particular, the largest such ˆa1 is at
least a1 and satisﬁes Prx∈D[a1 ≤x1 < ˆa1] ≤τ. We similarly ﬁnd the other ˆai and ˆbi. We
use the algorithm of Theorem 2 with conﬁdence parameter δ′ = δ/(4d log m) so that with
probability at least 1 −δ/2 none of our 2d log m queries fail.
A NOTE ON LEARNING FROM MULTIPLE-INSTANCE EXAMPLES
The total number of multiple-instance examples used is at most
r + ln((2d log m)/δ′)
The time for the algorithm is the time to sort these m points plus the time for the log m
calls per side of the rectangle, which by Theorem 2, is:
dm log m + ln((d log m)/δ′)
+ d log mln((d log m)/δ′)
This is almost exactly the same time bound as given in except that they
have an log( d
δ ) instead of log( d
ϵ)) for the last term. We use ˜O(rd2/ϵ2) r-instances
compared to ˜O(r2d2/ϵ2) r-instances.
Acknowledgments
We thank the Peter Auer and the anonymous referees for their helpful comments. This
research was supported in part by NSF National Young Investigator grant CCR-9357793,
a grant from the AT&T Foundation, and an NSF Graduate Fellowship.