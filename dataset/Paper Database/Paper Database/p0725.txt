Deep Matrix Factorization Models for Recommender Systems∗
Hong-Jian Xue, Xin-Yu Dai, Jianbing Zhang, Shujian Huang, Jiajun Chen
National Key Laboratory for Novel Software Technology; Nanjing University, Nanjing 210023, China
Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing 210023, China
 , {daixinyu,zjb,huangsj,chenjj}@nju.edu.cn
Recommender systems usually make personalized
recommendation with user-item interaction ratings,
implicit feedback and auxiliary information. Matrix factorization is the basic idea to predict a personalized ranking over a set of items for an individual user with the similarities among users and
items. In this paper, we propose a novel matrix
factorization model with neural network architecture. Firstly, we construct a user-item matrix with
explicit ratings and non-preference implicit feedback. With this matrix as the input, we present a
deep structure learning architecture to learn a common low dimensional space for the representations
of users and items. Secondly, we design a new loss
function based on binary cross entropy, in which
we consider both explicit ratings and implicit feedback for a better optimization. The experimental
results show the effectiveness of both our proposed
model and the loss function.
On several benchmark datasets, our model outperformed other stateof-the-art methods. We also conduct extensive experiments to evaluate the performance within different experimental settings.
Introduction
In the era of information explosion, information overload is
one of the dilemmas we are confronted with. Recommender
systems (RSs) are instrumental to address this problem as
they help determine which information to offer to individual
consumers and allow online users to quickly ﬁnd the personalized information that ﬁts their needs [Sarwar et al., 2001;
Linden et al., 2003].
RSs are nowadays ubiquitous in ecommerce platforms, such as recommendation of books at
Amazon, music at Last.com, movie at Netﬂix and reference
at CiteULike.
Collaborative ﬁltering (CF) recommender approaches are
extensively investigated in research community and widely
used in industry. They are based on the simple intuition that
∗Xin-Yu Dai is the corresponding author.
This work was
supported by the 863 program(2015AA015406) and the NSFC
(61472183,61672277).
if users rate items similarly in the past, they are likely to rate
other items similarly in the future [Sarwar et al., 2001; Linden
et al., 2003]. As the most popular approach among various
collaborative ﬁltering techniques, matrix factorization (MF)
which learns a latent space to represent a user or an item becomes a standard model for recommendation due to its scalability, simplicity, and ﬂexibility [Billsus and Pazzani, 1998;
Koren et al., 2009]. In the latent space, the recommender
system predicts a personalized ranking over a set of items for
each individual user with the similarities among the users and
Ratings in the user-item interaction matrix are explicit
knowledge which have been deeply exploited in early recommendation methods. Because of the variation in rating
values associated with users on items, biased matrix factorization
[Koren et al., 2009] are used to enhance the rating prediction. To overcome the sparseness of the ratings,
additional extra data are integrated into MF, such as social
matrix factorization with social relations [Ma et al., 2008;
Tang et al., 2013], topic matrix factorization with item
contents or reviews text
[McAuley and Leskovec, 2013;
Bao et al., 2014], and so on.
However, modeling only observed ratings is insufﬁcient to
make good top-N recommendations [Hu et al., 2008]. Implicit feedback, such as purchase history and unobserved ratings, is applied in recommender systems [Oard et al., 1998].
The SVD++ [Koren, 2008] model ﬁrstly factorizes the rating
matrix with the implicit feedback, and is followed by many
techniques for recommender systems [Rendle et al., 2009;
Mnih and Teh, 2012; He and McAuley, 2015].
Recently, due to the powerful representation learning abilities, deep learning methods have been successfully applied
including various areas of Computer Vision, Audio Recognition and Natural Language Processing. A few efforts have
also been made to apply deep learning models in recommender systems. Restricted Boltzmann Machines [Salakhutdinov et al., 2007] was ﬁrstly proposed to model users’ explicit ratings on items. Autoencoders and the denoising autoencoders have also been applied for recommendation [Li et
al., 2015; Sedhain et al., 2015; Strub and Mary, 2015]. The
key idea of these methods is to reconstruct the users’ ratings
through learning hidden structures with the explicit historical
ratings. Implicit feedback is also applied in this research line
of deep learning for recommendation. An extended work pre-
Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)
sented a collaborative denoising autoencoder (CDAE) [Wu et
al., 2016] to model user’s preference with implicit feedback.
Another work of neural collaborative ﬁltering (NCF) [He et
al., 2017] was proposed to model the user-item interactions
with a multi-layer feedforward neural network. Two recent
works above exploit only implicit feedback for item recommendations instead of explicit rating feedback.
In this paper, to make use of both explicit ratings and
implicit feedback, we propose a new neural matrix factorization model for top-N recommendation. We ﬁrstly construct a user-item matrix with both explicit ratings and nonpreference implicit feedback, which is different from other
related methods using either only explicit ratings or only implicit ratings. With this full matrix (explicit ratings and zero
of implicit feedback) as input, a neural network architecture is
proposed to learn a common latent low dimensional space to
represent the users and items. This architecture is inspired by
the deep structured semantic models which have been proved
to be useful for web search [Huang et al., 2013], where it can
map the query and document in a latent space through multiple layers of non-linear projections. In addition, we design a
new loss function based on cross entropy, which includes the
considerations of both explicit ratings and implicit feedback.
In sum, our main contributions are outlined as follows.
• We propose novel deep matrix factorization models with
a neural network that map the users and items into a
common low-dimensional space with non-linear projections. We use a matrix including both explicit ratings
and non-preference implicit feedback as the input of our
• We design a new loss function to consider both explicit
ratings and implicit feedback for better optimization.
• The experimental results show the effectiveness of our
proposed models which outperform other state-of-theart methods in top-N recommendation.
The organization of this paper is as follows. Problem statement is introduced in Section 2. In Section 3, we present the
architecture and details of the proposed models. In Section
4, we give empirical results on several benchmark datasets.
Concluding remarks with a discussion of some future work
are in the ﬁnal section.
Problem Statement
Suppose there are M users U = {u1, ..., uM}, N items
V = {v1, ..., vN}. Let R ∈RM×N denote the rating matrix, where Rij is the rating of user i on item j, and we mark
unk if it is unknown. There are two ways to construct the
user-item interaction matrix Y ∈RM×N from R with implicit feedback as,
Most of the existing solutions for recommendation apply
Equation 1 to construct the interaction matrix of Y [Wu et
al., 2016; He et al., 2017]. They consider all observed ratings the same as 1. In this paper, we construct the matrix of
Y with the Equation 2. The rating Rij of user ui on item
vj is still reserved in Y . We think that the explicit ratings in
Equation 2 is non-trivial for recommendation because they indicate the preference degree of a user on an item. Meanwhile,
we mark a zero if the rating is unknown, which is named as
non-preference implicit feedback in this paper.
The recommender systems are commonly formulated as
the problem of estimating the rating of each unobserved entry in Y , which are used for ranking the items. Model-based
approaches [Koren, 2008; Salakhutdinov and Mnih, 2007] assume that there is an underlying model which can generate all
ratings as follows.
ˆYij = F(ui, vj|Θ)
where ˆYij denotes the predicted score of interaction Yij
between user ui and item vj, Θ denotes the model parameters,
and F denotes the function that maps the model parameters to
the predicted scores. Based on this function, we can achieve
our goal of recommending a set of items for an individual
user to maximize the user’s satisfaction.
Now, the next question is how to deﬁne such a function F.
Latent Factor Model (LFM) simply applied the dot product
of pi, qj to predict the ˆYij as follows [Koren et al., 2009].
Here, pi and qj denote the latent representations of ui and vj,
respectively.
ˆYij = F LF M(ui, vj|Θ) = pT
Recently, neural collaborative ﬁltering (NCF) [He et al.,
2017] presented an approach with a multi-layer perceptron to
automatically learn the function of F. The motivation of this
method is to learn the non-linear interactions between users
and items.
In this paper, we follow the Latent Factor Model which
uses the inner product to calculate the interactions between
users and items. We do not follow the neural collaborative
ﬁltering because we try to get the non-linear connection between users and items through a deep representation learning
architecture.
We give the notations used in the following section. u indicates a user and v indicates an item. i and j index u and v,
respectively. Y denote the user-item interaction matrix transformed by Equation 2, Y + denotes the observed interactions,
Y −means all zero elements in Y and Y −
sampled denotes the
set of negative instances, which can be all (or sampled from)
Y −. Then Y + ∪Y −
sampled means all training interactions. We
denote the i-th row of matrix Y by Yi∗, j-th column by Y∗j
and its (i, j) −th element by Yij .
Our Proposed Model
In this section, we ﬁrstly brieﬂy introduce the deep structure
semantic model which inspires us to propose our method.
Then, we present our proposed architecture to represent the
users and items in a latent low-dimensional space. Lastly, we
give our designed loss function for optimization, followed by
the model training algorithm.
Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)
Deep Structure Semantic Model
Deep Structured Semantic Models (DSSM) were proposed in
[Huang et al., 2013] for web search. It uses a deep neural
network to rank a set of documents for a given query. DSSM
ﬁrstly maps the query and the documents to a common lower
semantic space with a non-linear multi-layer projection. And
then for web search ranking, the relevance of query to each
document is calculated by cosine similarity between the low
dimensional vectors of query and document. The deep neural
network are discriminatively trained to maximize the conditional likelihood of the query and matched documents.
DSSM has been applied for users modeling [Elkahky et
al., 2015]. Different from our work, it focused on modeling
the user with rich extra features, such as the web browsing
history and search queries. We only use the observed ratings
and observed feedback since we focus on the traditional top-
N recommendation problem.
Deep Matrix Factorization Models (DMF)
As mentioned in Section 2, we form a matrix Y according to
the Equation 2. With this matrix Y as the input, we propose
an architecture of deep neural network to project users and
items into a latent structured space. Figure 1 illustrates our
proposed architecture.
Figure 1: The architecture of Deep Matrix Factorization Models
From the matrix Y , each user ui is represented as a highdimensional vector of Yi∗, which represents the ith user’s ratings across all items. Each item vj is represented as a highdimensional vector of Y∗j, which represent the jth item’s
ratings across all users. In each layer, each input vector is
mapped into another vector in a new space. Formally, if we
denote the input vector by x , the output vector by y , the
intermediate hidden layers by li, i = 1, ..., N −1, the ith
weight matrix by Wi, the ith bias term by bi, and the ﬁnal
output latent representation by h. We have
li = f(Wi−1li−1 + bi), i = 2, ..., N −1
h = f(WNlN−1 + bN)
where we use the ReLU as the activation function at the
output layer and the hidden layers li, i = 2, ..., N −1:
f(x) = max(0, x)
In our architecture, we have two multi-layer networks
to transform the representations of u and v respectively.
Through the neural network, the user ui and item vj are ﬁnally mapped to a low-dimensional vector in a latent space as
shown in Equation 7. The similarity between the user ui and
item vj is then measured according to the Equation 8.
2 (Yi∗WU1))...)
∗jWV 1))...)
Here WU1 and WV 1 are the ﬁrst layer weighting matrix
for U and V , respectively, and WU2 and WV 2 for the second
layer, and so on.
ˆYij = F DMF (ui, vj|Θ) = cosine(pi, qj) =
∥pi∥∥qj∥(8)
In our architecture, besides the multi-layers representation
learning, we want to emphasize again that, to our best knowledge, it is the ﬁrst time to use the interaction matrix directly
as the input for representation learning. As we mentioned before, Yi∗represents a user’s ratings across all items. It can
to some extent indicate a user’s global preference. And Y∗j
represents an item’s ratings by all users. It can to some extent indicate an item’s proﬁle. We believe that these representations of users and items are very useful for their ﬁnal
low-dimensional representations.
Loss Function
Another key component for recommendation models is to de-
ﬁne a proper objective function for model optimization according to the observed data and unobserved feedback.
A general objective function is as follows.
l(y, ˆy) + λΩ(Θ)
Where l(·) denotes a loss function and Ω(Θ) is the regularizer.
For recommender systems, two types of objective functions are commonly used, point-wise and pair-wise, respectively. For simply, we use point-wise objective function in
this paper, and leave the pair-wise version to our future work.
Loss function is the most important part in the objective
function. Squared loss is largely performed in many existing
models [Salakhutdinov and Mnih, 2007; Koren et al., 2009;
Ning and Karypis, 2011; Hu et al., 2008].
(i,j)∈Y +∪Y −
wij(Yij −ˆYij)2
where wij denotes the weight of training instance (i, j). The
use of the squared loss is based on the assumption that observations are generated from a Gaussian distribution [Salakhutdinov and Mnih, 2007]. However, the square loss can not be
Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)
used well with implicit feedback, because for implicit data,
the target value Yij is a binarized 1 or 0 denoting whether i
has interacted with j or not. In what follows, a loss function
which pays special attention to the binary property of implicit
data was proposed by [He et al., 2017] as follows.
(i,j)∈Y +∪Y −
Yij + (1 −Yij)log(1 −ˆ
This loss is actually the binary cross-entropy loss (brieﬂy
as ce), addressing the recommendation with implicit feedback
as a binary classiﬁcation problem.
In sum, square loss pays attention to explicit ratings, while
cross entropy loss pays attention to implicit ratings. In this
paper, we design a new loss function to incorporate the explicit ratings into cross entropy loss, so that explicit and implicit information can be used together for optimization. We
name our new loss as normalized cross entropy loss (brieﬂy
as nce), which is presented in Equation 12.
(i,j)∈Y +∪Y −
max(R)log ˆYij
max(R))log(1 −ˆYij))
We use the max(R) (5 in a 5-star system) for normalization which is the max score in all ratings, so that different
values of Yij have different inﬂuences to the loss.
Algorithm 1 DMF Training Algorithm With Normalized
Cross Entropy
Input: Iter: # of training iterations,
neg ratio: Negative sampling ratio,
R: original rating matrix,
Output: WUi(i=1..N-1): weight matrix for user,
WV i(i=1..N-1): weight matrix for item,
1: Initialisation :
randomly initialize WU and WV ;
set Y ←use Equation 2 with R;
set Y + ←all none zero interactions in Y ;
set Y −←all zero interactions in Y ;
sampled ←sample neg ratio∗∥Y +∥interactions
set T ←Y + ∪Y −
8: for it from 1 to Iter do
for each interaction of User i and Item j in T do
set pi, qj ←use Equation 7 with input of Yi∗, Y∗j;
ij ←use Equation 8,13 with input of pi, qj;
set L ←use Equation 11 with input of ˆY o
use back propagation to optimize model parameters
15: end for
Training Algorithm
For cross entropy loss, because the predicted score of Yij can
be negative, we need to use Equation 13 to transform the original predictions. Let µ be a very small number, and we set
1.0e−6 in our experiments.
ij = max(µ, ˆYij)
We describe the detailed training method in Algorithm 1.
In Algorithm 1, we present the high-level training process
of DMF model. For training the parameters of weight matrix
WU and WV on each layer, we use back propagation to update the model parameters with batches. The complexity of
our algorithm is linear to the size of matrix and the layers of
Experiments
In this section, we conduct experiments to demonstrate the
effectiveness of both our proposed architecture and the re-
ﬁned loss function. We also do some extensive experiments
to compare the performance with different experimental settings, such as the negative sampling ratio, number of layers
in network, and so on.
Experimental Settings
We evaluate our models on four widely used datasets
in recommender systems:
MovieLens 100K(ML100k),
MovieLens 10M(ML1m), Amazon music(Amusic), Amazon
movies(Amovie). They are publicly accessible on the websites 1 2. For MovieLens dataset we do not process it because it was already ﬁltered, and for Amazon dataset we
ﬁltered the dataset, so that similar to the MovieLens data,
only those users with at least 20 interactions and items
with at least 5 interactions are retained [Wu et al., 2016;
He et al., 2017]. The statistics of the four datasets are given
in Table 1.
Statistics
ML100k ML1m
# of Users
# of Items
# of Ratings
1,000,209 46,468
Rating Density
Table 1: Statistics of the Four Datasets
Evaluation for Recommendation
To evaluate the performance of item recommendation, we
adopted the leave-one-out evaluation, which has been widely
used in the literatures [He et al., 2016; Kingma and Ba, 2014;
He et al., 2017].
We held-out the latest interaction as a
test item for every user and utilize the remaining dataset for
training. Since it is too time-consuming to rank all items
for every user during evaluation, following [Koren, 2008;
He et al., 2017], we randomly sample 100 items that are not
interacted by the users. Among the 100 items together with
the test item, we get the ranking according to the prediction.
We also use Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) [He et al., 2015] to evaluate the ranking performance. In our experiments, we truncated the ranked
1 
2 
Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)
Improvement of
DMF-2-nce vs. NeuMF-p
Table 2: NDCG@10 and HR@10 Comparisons of Different Methods
list at 10 for both metrics. As such, the HR intuitively measures whether the test item is present on the top-10 list, and
the NDCG measures the ranking quality which assigns higher
scores to hits at top position ranks.
Detailed Implementation
We implemented our proposed methods based on Tensor-
ﬂow3, which will be released publicly upon acceptance. To
determine hyper-parameters of DMF methods, we randomly
sampled one interaction for each user as the validation data
and tuned hyper-parameters on it. When training our models,
we sampled seven negative instances per positive instance.
For neural network, we randomly initialized model parameters with a Gaussian distribution (with a mean of 0 and standard deviation of 0.01), optimizing the model with mini-batch
Adam [Kingma and Ba, 2014]. We set the batch size to 256,
and set the learning rate to 0.0001.
Performance Comparison
In this subsection, we compare the proposed DMF with the
following methods. As our proposed methods aim to model
the relationship between users and items, we mainly compare
with user-item models. We leave out the comparison with
item-item models, such as SLIM [Ning and Karypis, 2011] ,
CDAE [Wu et al., 2016] because the performance difference
may be caused by the user models for personalization. We
also leave out the comparison with MV-DSSM [Elkahky et
al., 2015] because it uses a lot of auxiliary extra data and
evaluates on its own datasets.
It ranked the items by their popularity judged
by the number of interactions.
It is a non-personalized
method whose performance is usually used as the baseline
for personalized methods.
This is a standard item-based collaborative
ﬁltering method used by Amazon commercially [Sarwar et
al., 2001; Linden et al., 2003].
It is a state-of-the-art MF method for recommendation with square loss. It used all unobserved interactions as
negative instances and weighted them non-uniformly by the
item popularity. We tuned its hyper-parameters in the same
way as [He et al., 2016].
This is a state-of-the-art MF method for item
recommendation with cross entropy loss. It is the most related work to us. Different from our models, it only used the
3 
implicit feedback and initialized the representation of users
and items randomly. After that, it leverages a multi-layer perceptron to learn the user-item interaction function. We name
the neural matrix factorization with pretraining as NeuMFp which showed the best performance among their proposal
models. We tuned its hyper-parameters in the same way as
[He et al., 2017].
This is our proposed deep matrix factorization model, with 2 layers in the network and cross entropy
as loss function. We use the matrix including the explicit ratings and implicit feedback as the input of DMF. We name this
model as DMF-2-ce.
DMF-2-nce has the same depth of 2 layers
in the network as that in DMF-2-ce except that it uses the
normalized cross entropy loss.
The results of the comparison are summarized in Table 2.
It demonstrate the effectiveness of both our proposed architecture and the loss function. As for the proposed architecture, on almost all datasets, both of our two models achieve
the best performance in both metics of NDCG and HR, compared to other methods. Even compared to the state-of-the-art
method of NeuMF-p, DMF-2-nce obtain 2.5-7.4% (5.1% average) and 1.4-6.8% (3.8% average) relative improvements in
NDCG and HR metrics, respectively. As for the loss function,
we compared the performances of our two models. DMF-
2-nce achieves better results than DMF-2-ce, except on the
dataset of Amusic.
Impact of the Input Matrix for DMF
Table 3: Results for different input matrix. LFM-nce initialize the
input matrix randomly. DMF-1-nce use the matrix of Y as input.
They both perform 1-layer projection.
In DMF, we use the interaction matrix Y as the input. If we
Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence (IJCAI-17)
randomly initialize the representation vector of each user and
each item as the input to a one layer DMF model, the model
would be a standard Latent Factorization Model (LFM). To
test the usefulness of the input matrix of Y , we conduct experiments on two models of LFM-nce and DMF-1-nce. They
both have one layer in network and use the same loss function.
From Table 3, we can observe that, with the input
matrix, DMF-1-nce obtains a signiﬁcant improvement over
Sensitivity to Hyper-Parameters
NDCG 0.393
NDCG 0.408
NDCG 0.384
NDCG 0.521
Table 4: Results for Models with different negative sampling ratio.
Negative Sampling Ratio
In algorithm 1 as shown in Section 3.4, we need to sample
negative instances from unobserved data for training. In this
experiment, we apply different negative sampling ratio to observe the performance variance (e.g. neg-5 means we set the
negative sampling ratio as 5). From the results in Table 4, we
can ﬁnd that more negative instances seem useful to improve
performance. For these four datasets, the optimal negative
sampling ratio is around 5 which is consistent with the results
by previous work [He et al., 2017].
Depth of Layers in Network
In our proposed model, we map the users and items to
low-dimensional representations through neural network with
multiple hidden layers.
We conduct an extensive experiment on the Ml datasets to investigate our model with different number of hidden layers.
For detailed comparison,
Figure 2 shows the performance of each iteration by different layers. For space limitation, we just present the results
on ML datasets. As shown in Figure 2, on the large ML1m
dataset, our model with 2-layers illustrates the best performance. While on the relative small ML100k dataset, 2-layers
almost gets the best performance, but not stably and significantly. Deeper layers seem not useful, and 3-layers model
even decreases the performance.
Factors of the Final Latent Space
Besides the number of the hidden layers, the factors in each
layer is possibly another sensitive parameter in our model.
For simplicity, we just compare the performance with different number of factors on the top ﬁnal latent space. We
conduct the experiments to a two-layers model, and set the
number of factors on the top layer from 8 to 128. As shown
in Table 5, the ﬁnal layer with 64 factors gets the best performance except on the dataset of Amusic. On the Amusic
Figure 2: Results for models with different deep layers.
ML100k; Right: ML1m.
NDCG 0.369
NDCG 0.361
NDCG 0.357
NDCG 0.485
Table 5: Results for models with different factors of the ﬁnal latent
dataset, the best performance appears with 128 factors. The
ﬁnal representations with more factors might be more useful
when the dataset is very sparse and small.
Conclusion and Future Work
In this paper, we propose a novel matrix factorization model
with a neural network architecture. Through the neural network architecture, users and items are projected into lowdimensional vectors in a latent space. In our proposed model,
we make full use of both explicit ratings and implicit feedback in two ways. The input matrix to our proposed model includes both explicit ratings and non-preference feedback. In
another way, we also design a new loss function for training
our models in which both explicit and implicit feedback are
considered. The experiments on several benchmark datasets
demonstrate the effectiveness of our proposed model.
In the future, there are two directions to extend our work.
Pairwise objective function is another optional way for recommender system. We will verify our model with pairwise
objective function. Because of the sparseness and large missing unobserved data, many works try to incorporate auxiliary
extra data into recommender systems, such as social relation,
review text, browsing history, and so on. This give us another
interesting direction to extend our model with extra data.