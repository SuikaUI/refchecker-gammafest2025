With the expansion of data, increasing imbalanced data has
emerged. When the imbalance ratio of data is high, most existing imbalanced learning methods decline in classification
performance. To address this problem, a few highly imbalanced learning methods have been presented. However,
most of them are still sensitive to the high imbalance ratio.
This work aims to provide an effective solution for the highly imbalanced data classification problem. We conduct
highly imbalanced learning from the perspective of feature
learning. We partition the majority class into multiple
blocks with each being balanced to the minority class and
combine each block with the minority class to construct a
balanced sample set. Multiset feature learning (MFL) is performed on these sets to learn discriminant features. We thus
propose an uncorrelated cost-sensitive multiset learning
(UCML) approach. UCML provides a multiple sets construction strategy, incorporates the cost-sensitive factor into
MFL, and designs a weighted uncorrelated constraint to remove the correlation among multiset features. Experiments
on five highly imbalanced datasets indicate that: UCML
outperforms state-of-the-art imbalanced learning methods.
Introduction
Data imbalance means the case that one class severely outnumbers another. Usually, the class with more samples is
called majority class and the other one is called minority
class. When a classical classifier encounters imbalanced
data, it tends to favor the majority class samples. The imbalanced data classification problem has attracted much
interest from various communities .
Many methods have been addressed to tackle the imbalanced data classification problem , and they can be generally categorized into
Copyright Â© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
three kinds: (1) Sampling based methods. They employ
undersampling or oversampling technique to transform the
class-imbalanced dataset into a balanced one . (2) Cost-sensitive learning based
methods . This kind of methods considers the costs associated
with misclassifying samples. (3) Ensemble learning based
methods . This kind of
methods tries to improve the performance of individual
classifiers by inducing several classifiers and combining
them to obtain a new and more favorable classifier.
Generally, datasets whose imbalance ratio is higher than
10:1 can be regarded as the highly imbalanced datasets
 . Table 1 shows properties of five
highly imbalanced datasets derived from various application fields .
We can see that the majority class samples outnumber the
minority class samples severely. Ordinary imbalanced
learning methods usually decline in classification performance in highly imbalanced classification scenarios.
Recently, a few methods have been addressed to solve
the highly imbalanced data classification problem . Granular SVMs-repetitive undersampling
(GSVM-RU) , a modification to support
vector machines, can minimize the negative effect of information loss while maximizing the positive effect of data
cleaning in the undersampling process. Evolutionary undersampling boost (EUSBoost) combines boosting algorithm with evolutionary undersampling
WKDWFDQSURPRWHWKHGLYHUVLW\DPRQJFODVVLÂ¿HUV%HVLGHV
Table 1. Properties of highly imbalanced datasets.
Number of majority class samples
Number of minority class samples
ratio (IR)
Multiset Feature Learning for Highly Imbalanced Data Classification
Fei Wu,1,2,* Xiao-Yuan Jing,1,2,* Shiguang Shan,3 Wangmeng Zuo,4 Jing-Yu Yang5
1 State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China
2 College of Automation, Nanjing University of Posts and Telecommunications, China
3 Key Lab of Intelligent Information Process of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS, China
4 School of Computer, Harbin Institute of Technology, China
5 College of Computer Science and Technology, Nanjing University of Science and Technology, China
* Corresponding authors: {wufei_8888@, jingxy_2000@}126.com
Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)
 presents two sampling methods based on
the borderline synthetic minority over-sampling technique.
Motivation
Fig. 1 shows the influence of imbalance ratio (IR) to representative highly imbalanced learning methods, i.e., GSVM-
RU, EUSBoost and . Here, we take the
Abalone19 dataset as an example and observe the values of
the F1 (F-measure with the balance factor E being equal
to 1) values of these methods with increasing IR (from 1:1
to 128:1). Half of this dataset is taken as the training set
and the remainders are used as the testing set. Specifically,
we choose half of minority class samples (16 samples) and
the same number of samples from the majority class to
form the initial training set. In this case, the IR is 1:1.
Then, we increase the IR by adding more majority class
samples into the initial training set.
We can find that when the IR is increasing to 128:1, F1
of these methods is much lower than that in initial balanced data scenarios. Hence, there exists much room for
improvement in these methods. Essentially, existing highly
imbalanced learning methods can be classified into one of
three kinds of ordinary imbalanced learning methods
mentioned above, and mostly they utilize the sampling and
ensemble learning techniques. However, there exist some
shortcomings in sampling and ensemble learning techniques based methods, which will be analyzed in Related
Work Section. Therefore, highly imbalanced learning is
still a challenging task.
Contribution
The contributions of our study are summarized as following two points:
(1) We intend to address the highly imbalanced data
classification problem from the perspective of feature
learning. Multiset feature learning (MFL) technique can
jointly learn features from multiple related sample sets
effectively, such that the information of interest is fully
exploited. We are the first to introduce MFL for solving
the highly imbalanced data classification problem.
(2) We provide a multiple sets construction strategy,
which can partition the original highly imbalanced data
into multiple sets with each holding a class-balanced status. By designing cost-sensitive between-class scatter, we
incorporate the cost-sensitive factor into MFL. In addition,
we design a weighted uncorrelated constraint to remove the
correlation among features learned from different sets.
We call the proposed highly imbalanced learning approach as uncorrelated cost-sensitive multiset learning
(UCML). Experiments on five challenging datasets from
various fields demonstrate the effectiveness of UCML.
Related Work
Class-imbalanced Learning Methods
A. Sampling technique based methods. Undersampling
based methods balance the distributions between majority
class and minority class samples by reducing the majority
class samples. Oversampling based methods, however, add
the minority class samples to the imbalanced dataset. Majority
oversampling
(MWMOTE) is a synthetic minority
oversampling based method, which generates the synthetic
samples by using the weighted informative minority class
samples. These methods need to append or remove considerable samples for classifying the highly imbalanced data.
B. Cost-sensitive learning based methods. Cost-sensitive
multilayer perceptron (CSMLP) is a recently presented algorithm, which uses a single cost parameter to distinguish the importance of class
errors. For cost-sensitive learning based methods, how to
determine a cost representation is still an important and
open problem.
C. Ensemble learning based methods. Usually, the ensemble learning based methods are combined with the data
sampling technique to address the data imbalance problem
 . Undersampling based online bagging
with adaptive weight adjustment (WEOB2) can adjust the learning bias from majority to minority class effectively with adaptive weight adjustment.
 presents an ensemble system that combines feature selection algorithm, data sampling technique
and binary prediction model. These methods usually focus
on the classifier level issue. However, how to effectively
guarantee and utilize the diversity of classification ensembles is still an open problem.
The introduction and analysis of highly imbalanced
learning methods have been given in Introduction section.
Multiset Feature Learning (MFL) Methods
The idea of multiset feature learning (MFL) is to jointly
learn features from multiple related sample sets, such that
the information of interest can be fully exploited . Multiset canonical correlation analysis
(MCCA) exploits the correlation features
Figure 1. F1 of highly imbalanced learning methods on Abalone19 dataset with increasing imbalance ratio from 1:1 to 128:1.
Imbalance ratio
Jia et al.
among multiple sets. Discriminant analysis based MFL is
an important research direction in this domain, including
multi-view Fisher discriminant analysis (MFDA) and multi-view discriminant analysis (MvDA)
 . MvDA can maximize the between-class
variations and minimize the within-class variations of
samples in the learning common space from both intraview and inter-view. To our knowledge, MFL has not been
used to solve the imbalanced data classification problem.
Our Approach
Multiple Sets Construction Strategy
Fig. 2 illustrates the construction procedure of multiple
sets, which includes two steps:
Step 1: We randomly partition the majority class samples into multiple blocks, whose number of samples is the
same as that of minority class samples.
Since the number of majority class samples might not be
exactly in proportion to the number of minority class samples, some majority class samples may be left when multiple blocks have been obtained. We call these left samples
as â€œredundant samplesâ€. We delete redundant samples (if
the redundant samples are less than half of the minority
class samples) or add adequate number of samples copied
from original majority class samples (if the redundant
samples are more than half of the minority class samples)
to construct integral number of blocks. It is noted that the
added samples are all randomly copied from original majority class samples.
Step 2: We combine each block of majority class with
the minority class to form one balanced set. Then, we can
obtain multiple balanced sets.
With the designed multiple sets construction strategy, the
highly imbalanced data can be transformed to multiple
sets. And the highly imbalanced data classification problem
can be addressed by using the MFL techniques.
Cost-sensitive Multiset Feature Learning
Assume that v sets have been constructed. To boost the
misclassifying cost and improve the classification performance, we incorporate the cost-sensitive factor into MFL.
Concretely, the cost-sensitive factor is embodied in the
between-class scatter since it represents the relationship
between the majority and minority classes.
be the samples
thj set, where
k sample from the
set of the
class, c is the number of classes and
the number of samples from the
thj set of the
Samples from v sets can be projected to a common space
by using the v linear transformations
w denoted as
this common space, the between-class variation
all sets is maximized while the within-class variation
from all sets is minimized. To simplify exposition and ensure clarity, we assume the desired dimension of projected
samples equal to one, namely
w are a set of
projection vectors. We use
mean of samples in the projected space from the
in is the number of samples in the
class. Then
the within-class scatter
S is defined as:
Formally, the within-class scatter in (1) can be reformulated as follows for
the detailed derivation):
in S is defined as follows with
Assume that
denotes the punishment when a
majority class sample is misclassified as the minority class,
means the punishment when a minority-class
sample is misclassified as the majority class, as shown in
Table 2. We incorporate the cost-sensitive factors
in between-class scatter to increase the punishment when minority class samples are misclassified as
Table 2. Cost matrix for UCML.
Classified as
majority class
Classified as
minority class
Actually majority class
Actually minority class
Figure 2. Illustration of multiple sets construction strategy.
Training Data
Majority Class
Minority Class
the majority class samples. As a result, cost-sensitive MFL
makes the classification incline to classify the samples into
the minority class. Then the cost-sensitive between-class
S is defined as
where Pl is the mean of samples in the projected space
The cost-sensitive between-class scatter can be further
reformulated as follows:
D in matrix D is defined as:
is the number of samples from the
n are separately the numbers of
samples from the
set of the
is the mean sample in the
class, and
are separately the means
of samples from the
thj set of the
Weighted Uncorrelated Constraint
For constructing multiple sets, we partition majority class
samples into multiple blocks and combine each block with
the minority class to construct a set. Since the minority
class is shared by different sets and the samples other than
the minority class in different sets are all from the majority
class, there may exist correlation among multiple sets. And
the correlation in original sets will lead to the correlation in
the learned features from multiple sets. Therefore, we consider reducing the adverse correlation in the MFL process.
There already exist efforts to make features learned from
single set uncorrelated, including uncorrelated optimal
discrimination vectors (UODV) and
weighted global uncorrelated discriminant transforms
(WGUDT) . UODV and WGUDT separately make features from single set statistically uncorrelated or weighted global uncorrelated, and achieve impressive effects. Inspired by these single-set-based methods, we
design a weighted uncorrelated constraint to reduce the
statistical correlation among features from multiple sets.
The weighted correlation among multiple sets is defined as:
can be computed in the similar way. Here,
denote the
p sample of the
thj set and the
sample of the
m set, respectively.
P separately
denote the weighted mean sample corresponding to each
P can be calculated by
where V is a scalar constant. Then, our designed uncorrelated constraint is defined as:
. It is noted that since different sets contain the same number of samples, in this part,
we use N to denote the number of samples in each set.
Objective Function and Solution
By combining the multiset within-class scatter, the multiset
cost-sensitive between-class scatter and the weighted uncorrelated constraint, we define the objective function of
UCML as follows:
Like in , the solution of (9) can be obtained by solving the following eigen-equation problem:
Once the eigenvectors
associated
with d largest eigenvalues of
are obtained,
and 7 denote the testing sample set.
We can obtain the projected features of training sample set
Z and testing sample set
Z separately by
v set, we firstly use the nearest
neighbor (NN) classifier with the cosine distance to classify
Z . Then we can obtain v predicted results for
each testing sample in 7 . Next, we can adopt the majority
voting strategy to make final decision for each test sample.
Complexity Analysis
The time cost of UCML mainly includes two parts: (1)
calculating matrices S , D , and H ; (2) solving the generalized eigenvalue problem in (10). Specifically, matrices
calculation
complexity
, where dim denotes the dimensionality of samples. Solving the generalized eigenvalue
problem in (10) needs the time cost of
. Therefore,
Experiments
Competing Methods
In the experiment, we compare our UCML approach with
state-of-the-art related methods including: highly imbalanced learning methods: GSVM-RU ,
EUSBoost and ; representative
imbalanced
MWMOTE , CSMLP , WEOB2 and ; and representative multiset learning methods:
MCCA and MvDA .
Evaluation Measures and Experimental Setting
We employ three commonly used measures, including Precision, Recall, and F-measure, to evaluate the performances. Assume that A , B , C and D are the number of minority class samples that are classified as minority class,
the number of minority class samples that are classified as
majority class, the number of majority class samples that
are classified as minority class, and the number of majority
class samples that are classified as majority class, respectively, these measures can be calculated as:
. It is a tradeoff between the Precision and Recall. A greater value for
E indicates the higher importance of recall over precision.
In this paper, we use the widely used F1, that is F-measure
. In addition, we also evaluate the classimbalanced learning performance of our approach by using
F2 (F-measure with
), like in .
Obviously, an ideal method should hold high values of
Precision, Recall, F1 and F2. In experiments, we randomly
select 50% samples to construct the training set for all datasets, and use the remained samples for testing. We repeat
random selection 20 times and record the average results.
Assume that the first class is the majority class and the
second class is the minority class. Then
separately
rounded value of
N denote the numbers of majority and minority class samples.
The parameter
V in the weighted uncorrelated constraint
is set by using 5-fold cross validation on the training set.
Software Defect Prediction Application
To validate the effectiveness of UCML for software defect
prediction, we conduct experiments on the PC1 dataset
 . Each sample in this dataset has 38
features. Table 3 shows the experimental results on PC1.
We can see that UCML can achieve better results.
Document Classification Application
To validate the effectiveness of UCML for document classification, we conduct experiments on the Pageblock1 dataset. The imbalance ratio is 22.7:1 and each sample has 10
features. Table 4 shows the experimental results. We can
see that UCML obtains the best classification results.
Object Classification Application
Object classification also usually encounters the highly
imbalanced data. Thus, we conduct experiment on this type
of dataset like Glass5 . Each sample in this dataset has 9 features and the imbalance ratio is
22.8:1. Table 5 shows the experimental results. We can see
that our UCML is superior to other compared methods.
Repository,
 2009.
Table 3. Experimental results on the PC1 dataset.
0.42Â±0.03 0.48Â±0.05 0.44Â±0.04 0.47Â±0.05
0.46Â±0.06 0.51Â±0.04 0.47Â±0.04 0.50Â±0.06
MWMOTE 0.72Â±0.04 0.83Â±0.05 0.77Â±0.04 0.81Â±0.05
0.66Â±0.03 0.84Â±0.02 0.73Â±0.02 0.80Â±0.04
0.69Â±0.02 0.82Â±0.02 0.76Â±0.02 0.79Â±0.04
Dubey et al. 0.71Â±0.04 0.80Â±0.03 0.75Â±0.02 0.78Â±0.03
GSVM-RU 0.73Â±0.03 0.82Â±0.04 0.76Â±0.03 0.80Â±0.06
0.72Â±0.04 0.81Â±0.02 0.75Â±0.03 0.79Â±0.05
Jia et al.
0.72Â±0.03 0.80Â±0.03 0.76Â±0.03 0.78Â±0.05
0.74Â±0.03 0.84Â±0.04 0.78Â±0.02 0.82Â±0.03
Table 4. Experimental results on the Pageblock dataset.
0.39Â±0.06 0.45Â±0.04 0.42Â±0.05 0.44Â±0.02
0.44Â±0.04 0.47Â±0.05 0.45Â±0.05 0.46Â±0.03
MWMOTE 0.88Â±0.05 0.90Â±0.05 0.88Â±0.04 0.90Â±0.05
0.89Â±0.06 0.92Â±0.02 0.89Â±0.03 0.91Â±0.02
0.91Â±0.03 0.93Â±0.03 0.91Â±0.03 0.93Â±0.03
Dubey et al. 0.88Â±0.02 0.90Â±0.02 0.89Â±0.03 0.90Â±0.01
GSVM-RU 0.89Â±0.03 0.92Â±0.04 0.90Â±0.04 0.91Â±0.03
0.92Â±0.02 0.93Â±0.03 0.92Â±0.02 0.93Â±0.04
Jia et al.
0.89Â±0.05 0.88Â±0.04 0.88Â±0.05 0.88Â±0.02
0.93Â±0.03 0.94Â±0.03 0.94Â±0.04 0.94Â±0.02
Table 5. Experimental results on the Glass5 dataset.
0.43Â±0.12 0.76Â±0.11 0.56Â±0.10 0.66Â±0.08
0.45Â±0.09 0.75Â±0.08 0.54Â±0.08 0.66Â±0.06
MWMOTE 0.71Â±0.05 0.82Â±0.03 0.74Â±0.04 0.80Â±0.04
0.72Â±0.03 0.83Â±0.04 0.75Â±0.02 0.81Â±0.05
0.76Â±0.06 0.83Â±0.07 0.78Â±0.04 0.81Â±0.07
Dubey et al. 0.77Â±0.04 0.86Â±0.03 0.82Â±0.03 0.84Â±0.03
GSVM-RU 0.78Â±0.04 0.88Â±0.03 0.82Â±0.03 0.86Â±0.02
0.78Â±0.05 0.85Â±0.04 0.82Â±0.04 0.84Â±0.05
Jia et al.
0.74Â±0.05 0.79Â±0.05 0.77Â±0.05 0.78Â±0.04
0.81Â±0.02 0.89Â±0.04 0.86Â±0.03 0.87Â±0.06
Bio-information Prediction Application
We conduct experiments on the Yeast7 and Abalone19
datasets for bio-information prediction. Yeast7 is usually used to predict the cellular localization sites of proteins. Abalone19 is usually used to predict the age of abalone. Each sample has 8 features in these
two datasets.
Tables 6 and 7 separately show the prediction results on
Yeast7 and Abalone19. UCML obtains the best prediction
results on both datasets. We also conduct the statistical test
 to analyze the results in Tables 3-7.
The test results indicate that UCML makes a statistically
significant difference in comparison with other methods.
Table 6. Experimental results on the Yeast7 dataset.
0.51Â±0.05 0.37Â±0.02 0.44Â±0.05 0.39Â±0.03
0.54Â±0.03 0.39Â±0.02 0.45Â±0.03 0.41Â±0.05
0.67Â±0.04 0.56Â±0.03 0.60Â±0.04 0.58Â±0.07
0.71Â±0.03 0.57Â±0.02 0.63Â±0.04 0.59Â±0.08
0.70Â±0.02 0.59Â±0.03 0.65Â±0.05 0.61Â±0.05
Dubey et al. 0.73Â±0.04 0.60Â±0.02 0.63Â±0.04 0.62Â±0.07
0.73Â±0.03 0.61Â±0.04 0.67Â±0.03 0.63Â±0.04
0.77Â±0.02 0.61Â±0.02 0.68Â±0.05 0.64Â±0.06
Jia et al.
0.72Â±0.04 0.60Â±0.03 0.66Â±0.05 0.62Â±0.07
0.77Â±0.03 0.65Â±0.04 0.71Â±0.03 0.67Â±0.02
Table 7. Experimental results on the Abalone19 dataset.
0.33Â±0.04 0.52Â±0.04 0.41Â±0.03 0.47Â±0.05
0.38Â±0.03 0.53Â±0.03 0.44Â±0.02 0.49Â±0.03
0.53Â±0.02 0.58Â±0.04 0.54Â±0.03 0.57Â±0.04
0.51Â±0.01 0.55Â±0.02 0.53Â±0.02 0.54Â±0.05
0.55Â±0.02 0.60Â±0.03 0.57Â±0.03 0.59Â±0.07
Dubey et al. 0.58Â±0.05 0.67Â±0.03 0.62Â±0.04 0.65Â±0.06
0.60Â±0.03 0.70Â±0.04 0.63Â±0.03 0.68Â±0.04
0.63Â±0.02 0.69Â±0.02 0.66Â±0.03 0.68Â±0.05
Jia et al.
0.59Â±0.04 0.68Â±0.02 0.62Â±0.03 0.66Â±0.02
0.67Â±0.03 0.75Â±0.05 0.71Â±0.04 0.73Â±0.07
Table 8. F1 values of UCMLnoboth, UCMLnocost, and UCMLnowei.
UCMLnoboth UCMLnocost UCMLnowei
Effectiveness of Important Components
Multiset feature learning (main body of our approach),
cost-sensitive factor, and weighted uncorrelated constraint
are three important components of our approach. In this
subsection, we specially evaluate their effectiveness. We
perform our approach without the cost-sensitive factor and
weighted uncorrelated constraint, and we call this version
as â€œUCMLnobothâ€. In addition, we perform our approach
without the cost-sensitive factor or weighted uncorrelated
constraint, which are separately called â€œUCMLnocostâ€ and
â€œUCMLnoweiâ€. The experimental results of UCMLnoboth,
UCMLnocost, UCMLnowei and UCML are given in Table 8.
From the table, we can see that the F1 values of
UCMLnoboth are obviously lower than those of UCML, but
are still comparable to other methods. In addition,
UCMLnocost and UCMLnowei can improve the results of
UCMLnoboth. These results demonstrate the effectiveness of
the three components in our approach.
Evaluation of the Influence of IR to UCML
Fig. 3 illustrates the influence of IR to UCML on Abalone19. The experimental setting can be found in Fig. 1. We
can find that when the IR is increasing, F1 values of compared methods decline. When the IR reaches the maximized level (128:1), the F1 values are significantly lower
than those corresponding to the IR of 1:1 for all competing
methods. For UCML, its F1 experiences a relatively smaller decline, which means that UCML is relatively robust to
highly imbalance ratio as compared with related methods.
Parameter Analysis
For the parameter
in our approach, we search the parameter space
2 ,2 ,2 ,2 ,2 ,2 ,2
the mean square distance of training data. Here, we evaluate the influence of
V on the prediction result. Fig. 4
shows the F1 values of UCML versus different values of
V on PC1. We can see that the performances of our approach are stable with respect to
in the range of
Â¼ . For simplicity, we set
2V on PC1. A
similar phenomenon also exists on the other datasets.
With respect to the computational time, generally, our
approach needs comparable time as compared with
MCCA, MvDA, MWMOTE and , and
needs less time than CSMLP, WEOB2, ,
GSVM-RU and EUSBoost.
Figure 4: F1 versus
Conclusion
In this paper, we are devoted to addressing the highly imbalanced learning problem from the perspective of feature
learning. We propose a novel approach named UCML.
Figure 3. F1 vs. imbalance ratio on Abalone19.
Imbalance ratio
Dubey et al.
Jia et al.
This is the first attempt towards introducing the idea of
MFL into imbalanced learning. We conduct experiments
on five highly imbalanced datasets from various application fields. The results demonstrate that UCML outperforms state-of-the-art highly imbalanced learning methods.
The experimental results indicate that three important
components of our approach are effective. We also find
that our approach is more robust to high imbalance ratio.
Acknowledgments
The authors want to thank the editors and anonymous reviewers for their constructive comments and suggestions.
The work described in this paper was supported by the
National Nature Science Foundation of China under Project Nos. 61272273, 61671182, 61672281 and 61233011,
and the Research Project of NJUPT (XJKY14016).