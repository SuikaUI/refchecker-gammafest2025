Text Understanding from Scratch
Xiang Zhang
 
Yann LeCun
 
Computer Science Department, Courant Institute of Mathematical Sciences, New York University
This article demonstrates that we can apply deep
learning to text understanding from characterlevel inputs all the way up to abstract text
using temporal convolutional networks (ConvNets). We apply
ConvNets to various large-scale datasets, including ontology classiﬁcation, sentiment analysis,
and text categorization. We show that temporal
ConvNets can achieve astonishing performance
without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language.
Evidence shows that our models can work for both
English and Chinese.
1. Introduction
Text understanding consists in reading texts formed in natural languages, determining the explicit or implicit meaning of each elements such as words, phrases, sentences
and paragraphs, and making inferences about the implicit
or explicit properties of these texts . This
problem has been traditionally difﬁcult because of the extreme variability in language formation . To
date, most ways to handle text understanding, be it a handcrafted parsing program or a statistically learnt model, have
been resorted to the means of matching words statistics.
So far, most machine learning approaches to text understanding consist in tokenizing a string of characters into
structures such as words, phrases, sentences and paragraphs, and then apply some statistical classiﬁcation algorithm onto the statistics of such structures . These techniques work well enough when applied
to a narrowly deﬁned domain, but the prior knowledge
required is not cheap – they need to pre-deﬁne a dictio-
This technical report is superseded by a paper entitled
“Character-level Convolutional Networks for Text Classiﬁcation”,
 
and a rewritten introduction.
nary of interested words, and the structural parser needs to
handle many special variations such as word morphological changes and ambiguous chunking. These requirements
make text understanding more or less specialized to a particular language – if the language is changed, many things
must be engineered from scratch.
With the advancement of deep learning and availability
of large datasets, methods of handling text understanding using deep learning techniques have gradually become
available.
One technique which draws great interests is
word2vec . Inspired by traditional
language models, this technique constructs representation
of words into a vector of ﬁxed length trained under a
large corpus. Based on the hope that machines may make
sense of languages in a formal fashion, many researchers
have tried to train a neural network for understanding texts
based the features extracted from it or similar techniques,
to name a few, . Most of these techniques try to apply word2vec or
similar techniques with an engineered language model.
On the other hand, some researchers have also tried to train
a neural network from word level with little structural engineering . In these works, a
word level feature extractor such as lookup table or word2vec is used to
feed a temporal ConvNet . After training, ConvNets worked for both structured prediction tasks
such as part-of-speech tagging and named entity recognition, and text understanding tasks such as sentiment analysis and sentence classiﬁcation. They claim good results
for various tasks, but the datasets and models are relatively
small and there are still some engineered layers to represent
structures such as words, phrases and sentences.
In this article we show that text understanding can be handled by a deep learning system without artiﬁcially embedding knowledge about words, phrases, sentences or any
other syntactic or semantic structures associated with a language. We apply temporal ConvNets 
to various large-scale text understanding tasks, in which the
 
Text Understanding from Scratch
inputs are quantized characters and the outputs are abstract
properties of the text. Our approach is one that ‘learns from
scratch’, in the following 2 senses
1. ConvNets do not require knowledge of words – working with characters is ﬁne. This renders a word-based
feature extractor or word2vec ) unnecessary. All previous works start with words instead
of characters, which is difﬁcult to apply a convolutional layer directly due to its high dimension.
2. ConvNets do not require knowledge of syntax or semantic structures – inference directly to high-level targets is ﬁne. This also invalidates the assumption that
structured predictions and language models are necessary for high-level text understanding.
Our approach is partly inspired by ConvNet’s success in
computer vision. It has outstanding performance in various
image recognition tasks . These successful results usually involve some end-to-end ConvNet model that
learns hierarchical representation from raw pixels .
Similarly, we hypothesize that when trained from raw characters, temporal
ConvNet is able to learn the hierarchical representations of
words, phrases and sentences in order to understand text.
2. ConvNet Model Design
In this section, we introduce the design of ConvNets for
text understanding. The design is modular, where the gradients are obtained by back-propagation to perform optimization.
2.1. Key Modules
The main component in our model is the temporal convolutional module, which simply computes a 1-D convolution between input and output. Suppose we have a discrete input function g(x) ∈[1, l] →R and a discrete
kernel function f(x) ∈[1, k] →R.
The convolution
h(y) ∈[1, ⌊(l −k)/d⌋+ 1] →R between f(x) and g(x)
with stride d is deﬁned as
f(x) · g(y · d −x + c),
where c = k −d + 1 is an offset constant. Just as in traditional convolutional networks in vision, the module is parameterized by a set of such kernel functions fij(x) (i =
1, 2, . . . , m and j = 1, 2, . . . , n) which we call weights, on
a set of inputs gi(x) and outputs hj(y). We call each gi
(or hj) an input (or output) frame, and m (or n) input (or
output) frame size. The outputs hj(y) is obtained by a sum
over i of the convolutions between gi(x) and fij(x).
One key module that helped us to train deeper models
is temporal max-pooling. It is the same as spatial maxpooling module used in computer vision , except that it is in 1-D. Given a discrete input
function g(x) ∈[1, l] →R, the max-pooling function
h(y) ∈[1, ⌊(l −k)/d⌋+ 1] →R of g(x) is deﬁned as
x=1 g(y · d −x + c),
where c = k −d + 1 is an offset constant. This very pooling module enabled us to train ConvNets deeper than 6 layers, where all others fail. The analysis by might shed some light on this.
The non-linearity used in our model is the rectiﬁer or
thresholding function h(x) = max{0, x}, which makes
our convolutional layers similar to rectiﬁed linear units
(ReLUs) . We always apply this function after a convolutional or linear module, therefore we
omit its appearance in the following. The algorithm used
in training our model is stochastic gradient descent (SGD)
with a minibatch of size 128, using momentum 0.9 and initial step size 0.01
which is halved every 3 epoches for 10 times. The training
method and parameters apply to all of our models. Our implementation is done using Torch 7 .
2.2. Character quantization
Our model accepts a sequence of encoded characters as
The encoding is done by prescribing an alphabet of size m for the input language, and then quantize each character using 1-of-m encoding. Then, the sequence of characters is transformed to a sequence of such
m sized vectors with ﬁxed length l. Any character exceeding length l is ignored, and any characters that are not in
the alphabet including blank characters are quantized as
all-zero vectors. Inspired by how long-short term memory (LSTM) work, we
quantize characters in backward order. This way, the latest
reading on characters is always placed near the beginning
of the output, making it easy for fully connected layers to
associate correlations with the latest memory. The input to
our model is then just a set of frames of length l, and the
frame size is the alphabet size m.
One interesting thing about this quantization is that visually
it is quite similar to Braille(Braille, 1829) used for assisting blind reading, except that our encoding is more compact. Figure 1 depicts this fact. It seems that when trained
properly, humans can learn to read binary encoding of languages. This offers interesting insights and inspiration to
why our approach could work.
Text Understanding from Scratch
(a) Binary
(b) Braille
Figure 1. Comparison of our binary encoding and Braille on the
text “International Conference on Machine Learning”
The alphabet used in all of our models consists of 70 characters, including 26 English letters, 10 digits, new line and
33 other characters. They include:
abcdefghijklmnopqrstuvwxyz0123456789
-,;.!?:’’’/\|_@#$%ˆ&*˜‘+-=<>()[]{}
Before feeding the input to our model, no normalization is
done. This is because the input is already quite sparse by
itself, with many zeros scattered around. Our models can
learn from this simple quantization without problems.
2.3. Model Design
We designed 2 ConvNets – one large and one small. They
are both 9 layers deep with 6 convolutional layers and 3
fully-connected layers, with different number of hidden
units and frame sizes. Figure 2 gives an illustration.
Convolutions
Max-pooling
Quantization
Conv. and Pool. layers
Max-pooling
Fully-connected
Figure 2. Illustration of our model
The input have number of frames equal to 69 due to
our character quantization method, and the length of each
frame is dependent on the problem.
We also insert 2
dropout modules in between the 3
fully-connected layers to regularize. They have dropout
probability of 0.5. Table 1 lists the conﬁgurations for convolutional layers, and table 2 lists the conﬁgurations for
fully-connected (linear) layers.
Before starting training the models, we randomize the
weights using Gaussian distributions. The mean and standard deviation used for initializing the large model is
(0, 0.02), and small model (0, 0.05).
For different problems the input lengths are different, and
so are the frame lengths.
From our model design, it is
easy to know that given input length l0, the output frame
length after the last convolutional layer (but before any of
the fully-connected layers) is l6 = (l0−96)/27. This number multiplied with the frame size at layer 6 will give the
input dimension the ﬁrst fully-connected layer accepts.
Table 1. Convolutional layers used in our experiments. The convolutional layers do not use stride and pooling layers are all nonoverlapping ones, so we omit the description of their strides.
Large Frame
Small Frame
Table 2. Fully-connected layers used in our experiments.
number of output units for the last layer is determined by the problem. For example, for a 10-class classiﬁcation problem it will be
Output Units Large
Output Units Small
Depends on the problem
2.4. Data Augmentation using Thesaurus
Many researchers have found that appropriate data augmentation techniques are useful for controlling generalization error for deep learning models. These techniques usually work well when we could ﬁnd appropriate invariant
properties that the model should possess. For example, in
image recognition a model should have some controlled invariance towards changes in translating, scaling, rotating
and ﬂipping of the input image. Similarly, in speech recognition we usually augment data by adding artiﬁcial noise
background and changing the tone or speed of speech signal .
In terms of texts, it is not reasonable to augment the data
using signal transformations as done in image or speech
recognition, because the exact order of characters may form
rigorous syntactic and semantic meaning. Therefore, the
best way to do data augmentation would have been using
human rephrases of sentences, but this is unrealistic and
expensive due the large volume of samples in our datasets.
As a result, the most natural choice in data augmentation
for us is to replace words or phrases with their synonyms.
We experimented data augmentation by using an English
thesaurus, which is obtained from the mytheas component used in LibreOfﬁce1 project. That thesaurus in turn
1 
Text Understanding from Scratch
was obtained from WordNet , where every synonym to a word or phrase is ranked by the semantic
closeness to the most frequently seen meaning.
To do synonym replacement for a given text, we need to
answer 2 questions: which words in the text should be replaced, and which synonym from the thesaurus should be
used for the replacement. To decide on the ﬁrst question,
we extract all replaceable words from the given text and
randomly choose r of them to be replaced. The probability
of number r is determined by a geometric distribution with
parameter p in which P[r] ∼pr. The index s of the synonym chosen given a word is also determined by a another
geometric distribution in which P[s] ∼qs. This way, the
probability of a synonym chosen becomes smaller when it
moves distant from the most frequently seen meaning.
It is worth noting that models trained using our large-scale
datasets hardly require data augmentation, since their generalization errors are already pretty good. We will still report the results using this new data augmentation technique
with p = 0.5 and q = 0.5.
2.5. Comparison Models
Since we have constructed several large-scale datasets from
scratch, there is no previous publication for us to obtain a
comparison with other methods. Therefore, we also implemented two fairly standard models using previous methods:
the bag-of-words model, and a bag-of-centroids model via
word2vec .
The bag-of-words model is pretty straightforward. For each
dataset, we count how many times each word appears in the
training dataset, and choose 5000 most frequent ones as the
bag. Then, we use multinomial logistic regression as the
classiﬁer for this bag of features.
As for the word2vec model, we ﬁrst ran k-means on the
word vectors learnt from Google News corpus with k =
5000, and then use a bag of these centroids for multinomial
logistic regression. This model is quite similar to the bagof-words model in that the number of features is also 5000.
One difference between these two models is that the features for bag-of-words model are different for different
datasets, whereas for word2vec they are the same. This
could be one reason behind the phenomenon that bag-ofwords consistently out-performs word2vec in our experiments. It might also be the case that the hope for linear
separability of word2vec is not valid at all. That being said,
our own ConvNet models consistently out-perform both.
3. Datasets and Results
In this part we show the results obtained from various
datasets. The unfortunate fact in literature is that there is no
openly accessible dataset that is large enough or with labels
of sufﬁcient quality for us, although the research on text
understanding has been conducted for tens of years. Therefore, we propose several large-scale datasets, in hopes that
text understanding can rival the success of image recognition when large-scale datasets such as ImageNet became available.
3.1. DBpedia Ontology Classiﬁcation
DBpedia is a crowd-sourced community effort to extract
structured information from Wikipedia indicates large negative (or positive)
values, and gray indicates values near zero. It seems very
interesting that the network has learnt to care more about
the variations in letters than other characters. This phenomenon is observed in models for all of the datasets.
3.2. Amazon Review Sentiment Analysis
The purpose of sentiment analysis is to identify and extract
subjective information in different kinds of source materials. This task, when presented with the text written by
some user, could be formulated as a normal classiﬁcation
problem in which each class represents a degree indicator
for user’s subjective view. One example is the score system used from Amazon, which is a discrete score from 1 to
5 indicating user’s subjective rating of a product. The rating usually comes with a review text, which is a valuable
source for us to construct a sentiment analysis dataset.
We obtained an Amazon review dataset from the Stanford Network Analysis Project (SNAP), which spans 18
years with 34,686,770 reviews from 6,643,669 users on
2,441,053 products .
dataset contains review texts of extremely variate character
lengths from 3 to 32,788, in which the mean is around 764.
To construct a sentiment analysis dataset, we chose review
texts with character lengths between 100 and 1014. Apart
from constructing from the original 5 score labels, we also
construct a sentiment polarity dataset in which labels 1 and
2 are converted to negative and 4 and 5 positive. There are
also large number of duplicated reviews in which the title
and review text are the same. We removed these duplicates.
Table 5 lists the number of samples for each score and the
number sampled for the 2 dataset.
Table 5. Amazon review datasets.
Column “total” is the total
number of samples for each score. Column “full” and “polarity”
are number of samples chosen for full score dataset and polarity
dataset, respectively.
20,705,260
We ignored score 3 for polarity dataset because some
texts in that score are not obviously negative or positive.
Many researchers have shown that with some random text,
the inter-rater consensus on polarity is only about 60% -
80% . We believe that by picking out
score 3, the labels would have higher quality with a clearer
indication of positivity or negativity. We could have included a third “neutral” class, but that would signiﬁcantly
reduce the number of samples for each class since sample
imbalance is not desirable.
For the full score dataset, we randomly selected 600,000
samples for each score for training and 130,000 samples
for testing. The size of training set is then 3,000,000 and
testing 650,000. For the polarity dataset, we randomly selected 1,800,000 samples for each positive or negative label as training set and 200,000 samples for testing. In total, the polarity dataset has 3,600,000 training samples and
400,000 testing samples.
Because we limit the maximum length of the text to be
1014, we can safely set the input length to be 1014 and
use the same conﬁguration as the DBpedia model. Models
for Amazon review datasets took signiﬁcantly more time to
go over each epoch. The time taken for the large model per
epoch is about a 5 days, and small model 2 days, with the
Text Understanding from Scratch
Table 6. Result on Amazon review full score dataset. The numbers are accuracy.
Large ConvNet
Large ConvNet
Small ConvNet
Small ConvNet
Bag of Words
Table 7. Result on Amazon review polarity dataset. The numbers
are accuracy.
Large ConvNet
Large ConvNet
Small ConvNet
Small ConvNet
Bag of Words
polarity training taking a little bit longer. Table 6 and table
7 list the results on full score dataset and polarity dataset,
respectively.
Figure 4. Confusion matrices on full score Amazon Review prediction. White values are 1 and black 0. Vertical direction iterates
over true score from top to bottom, and horizontal direction iterates over predicted scores from left to right.
It seems that our models work much better on the polarity
dataset than the full score dataset. This is to be expected,
since full score prediction means more confusion between
nearby score labels. To demonstrate this, ﬁgure 4 shows
the training and testing confusion matrices.
3.3. Yahoo! Answers Topic Classiﬁcation
Yahoo! Answers is a web site where people post questions
and answers, all of which are public to any web user willing to browse or download them. We obtained Yahoo! Answers Comprehensive Questions and Answers version 1.0
dataset through the Yahoo! Webscope program. The data
they have collected is the Yahoo! Answers corpus as of October 25th, 2007. It includes all the questions and their corresponding answers. The corpus contains 4,483,032 questions and their answers. In addition to question and answer
text, the corpus contains a small amount of metadata, i.e.,
which answer was selected as the best answer, and the category and sub-category that was assigned to each question.
We constructed a topic classiﬁcation dataset from this corpus using 10 largest main categories. They are listed in
table 8. Each class contains 140,000 training samples and
6,000 testing samples. Therefore, the total number of training samples is 1,400,000 and testing samples 60,000 in this
dataset. From all the answers and other meta-information,
we only used the best answer content and the main category
information.
Table 8. Yahoo! Answers topic classiﬁcation dataset
Society & Culture
Science & Mathematics
Education & Reference
Computers & Internet
Business & Finance
Entertainment & Music
Family & Relationships
Politics & Government
The Yahoo! Answers dataset also contains questions and
answers of various lengths, up to 4000 characters. During
training we still set the input length to be 1014 and truncate
the rest if necessary. But before truncation, we concatenated the question title, question content and best answer
content in reverse order so that the question title and content are less likely to be truncated. It takes about 1 day for
one epoch on the large model, and about 8 hours for the
small model. Table 9 details the results on this dataset.
One interesting thing from the results on Yahoo! Answers
dataset is that both training and testing accuracy values are
quite small compared to the results we obtained from other
datasets, whereas the generalization error is pretty good.
One hypothesis for this is that there are some intrinsic confusions in determining between some classes given a pair
of question and answer.
Figure 5 shows the confusion matrix for the large model
without thesaurus augmentation.
It indicates relatively
Text Understanding from Scratch
Table 9. Results on Yahoo! Answers dataset. The numbers are
Large ConvNet
Large ConvNet
Small ConvNet
Small ConvNet
Bag of Words
Figure 5. Confusion matrices on Yahoo! Answers dataset. White
values are 1 and black 0.
Vertical direction iterates over true
classes from top to bottom, and horizontal direction iterates over
predicted classes from left to right.
large confusion for classes “Society & Culture”, “Education & Reference”, and “Business & Finance”.
3.4. News Categorization in English
News is one of the largest parts of the entire web today,
which makes it a good candidate to build text understanding models. We obtained the AG’s corpus of news article
on the web2. It contains 496,835 categorized news articles
from more than 2000 news sources. We choose 4 largest
categories from this corpus to construct our dataset, using
only the title and description ﬁelds.
Table 10. AG’s news corpus. Only categories used are listed.
2 
of_news_articles.html
Table 10 is a summary of the dataset. From each category,
we randomly chose 30,000 samples as training and 1,900
as testing. The total number of training samples is then
120,000 and testing 7,600. Compared to other datasets we
have constructed, this dataset is relatively small. Therefore
the time taken for one epoch using the large model is only
3 hours, and about 1 hour for the small model.
Table 11. Result on AG’s news corpus. The numbers are accuracy
Large ConvNet
Large ConvNet
Small ConvNet
Small ConvNet
Bag of Words
Similarly as our previous experiments, we also use an input
length of 1014 for this dataset after title and description are
concatenated. The actual resulting maximum length of all
the inputs is 9843, but the mean is only around 232.
Table 11 lists the results. It shows a sign of overﬁtting from
our models, which suggests that to achieve good text understanding results ConvNets require a large corpus in order to
learn from scratch.
3.5. News Categorization in Chinese
One immediate advantage from our dictionary-free design
is its applicability to other kinds of human languages. Our
simple approach only needs an alphabet of the target language using one-of-n encoding.
For languages such as
Chinese, Japanese and Korean where there are too many
characters, one can simply use its romanized (or latinized)
transcription and quantize them just like in English. Better
yet, the romanization or latinization is usually phonemic
or phonetic, which rivals the success of deep learning in
speech recognition . Here we investigate one example: news categorization in Chinese.
The dataset we obtained consists of the SogouCA and SogouCS news corpora , containing in total
2,909,551 news articles in various topic channels. Among
them, about 2,644,110 contain both a title and some content.
We then labeled the each piece of news using its
URL, by manually classify the their domain names. This
gives us a large corpus of news articles labeled with their
categories. There are a large number categories but most
of them contain only few articles. We choose 5 categories
– “sports”, “ﬁnance”, “entertainment”, “automobile” and
“technology”. The number of training samples selected for
Text Understanding from Scratch
each class is 90,000 and testing 12,000, as table 12 shows.
Table 12. Sogou News dataset
Entertainment
Automobile
Technology
The romanization or latinization form we have used is
Pinyin, which is a phonetic system for transcribing the
Mandarin pronunciations. During this procedure, we used
the pypinyin package combined with jieba Chinese
segmentation system. The resulting Pinyin text had each
tone appended their ﬁnals as numbers between 1 and 4.
Similar as before, we concatenate title and content to form
an input sample. The texts has a wide range of lengths from
14 to 810959. Therefore, during data acquisition procedure we constrain the length to stay between 100 and 1014
whenever possible. In the end, we also apply same models
as before to this dataset, for which the input length is 1014.
We ignored thesaurus augmentation for this dataset. Table
13 lists the results.
Table 13. Result on Sogou News corpus. The numbers are accuracy
Large ConvNet
Small ConvNet
Bag of Words
The input for a bag-of-words model is obtained by considering each Pinyin at Chinese character level as a word.
These results indicate consistently good performance from
our ConvNet models, even though it is completely a different kind of human language. This is one evidence to our
belief that ConvNets can be applied to any human language
in similar ways for text understanding tasks.
4. Outlook and Conclusion
In this article we provide a ﬁrst evidence on ConvNets’ applicability to text understanding tasks from scratch, that is,
ConvNets do not need any knowledge on the syntactic or
semantic structure of a language to give good benchmarks
text understanding. This evidence is in contrast with various previous approaches where a dictionary of words is
a necessary starting point, and usually structured parsing
is hard-wired into the model .
Deep learning models have been known to have good representations across domains or problems, in particular for
image recognition . How good the
learnt representations are for language modeling is also one
interesting question to ask in the future. Beyond that, we
can also consider how to apply unsupervised learning to
language models learnt from scratch. Previous embedding
methods have shown that predicting words or
other patterns missing from the input could be useful. We
are eager to see how to apply these transfer learning and
unsupervised learning techniques with our models.
Recent research shows that it is possible to generate text
description of images from the features learnt in a deep
image recognition model, using either fragment embedding or recurrent neural networks
such as long-short term memory (LSTM) . The models in this article show very good ability
for understanding natural languages, and we are interested
in using the features from our model to generate a response
sentence in similar ways. If this could be successful, conversational systems could have a big advancement.
It is also worth noting that natural language in its essence
is time-series in disguise. Therefore, one natural extended
application for our approach is towards time-series data,
in which a hierarchical feature extraction mechanism could
bring some improvements over the recurrent and regression
models used widely today.
In this article we only apply ConvNets to text understanding for its semantic or sentiment meaning. One other apparent extension is towards traditional NLP tasks such as
chunking, named entity recognition (NER) and part-ofspeech (POS) tagging. To do them, one would need to
adapt our models to structured outputs. This is very similar to the seminal work by Collobert and Weston , except that we probably no longer need to
construct a dictionary and start from words. Our work also
makes it easy to extend these models to other human languages.
One ﬁnal possibility from our model is learning from
symbolic systems such as mathematical equations, logic
expressions or programming languages.
Zaremba and
Sutskever have shown that it
is possible to approximate program executing using a recurrent neural network. We are also eager to see how similar
projects could work out using our ConvNet models.
With so many possibilities, we believe that ConvNet models for text understanding could go beyond from what this
Text Understanding from Scratch
article shows and bring important insights towards artiﬁcial
intelligence in the future.
Acknowledgement
We gratefully acknowledge the support of NVIDIA Corporation with the donation of 2 Tesla K40 GPUs used for this