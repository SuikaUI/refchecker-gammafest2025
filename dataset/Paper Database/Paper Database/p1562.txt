Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5100–5111,
Hong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics
LXMERT: Learning Cross-Modality Encoder Representations
from Transformers
Mohit Bansal
UNC Chapel Hill
{haotan, mbansal}@cs.unc.edu
Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two
modalities.
We thus propose the LXMERT
(Learning Cross-Modality Encoder Representations from Transformers) framework to learn
these vision-and-language connections.
LXMERT, we build a large-scale Transformer
model that consists of three encoders: an object relationship encoder, a language encoder,
and a cross-modality encoder.
Next, to endow our model with the capability of connecting vision and language semantics, we
pre-train the model with large amounts of
image-and-sentence pairs, via ﬁve diverse representative pre-training tasks:
masked language modeling, masked object prediction
(feature regression and label classiﬁcation),
cross-modality matching, and image question answering.
These tasks help in learning both intra-modality and cross-modality relationships.
After ﬁne-tuning from our pretrained parameters, our model achieves the
state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA).
We also show the generalizability of our pretrained cross-modality model by adapting it to
a challenging visual-reasoning task, NLVR2,
and improve the previous best result by 22%
absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that
both our novel model components and pretraining strategies signiﬁcantly contribute to
our strong results.1
Introduction
Vision-and-language reasoning requires the understanding of visual contents, language semantics, and cross-modal alignments and relation-
1Published at EMNLP 2019. Code and pre-trained models publicly available at: 
ships. There has been substantial past works in
separately developing backbone models with better representations for the single modalities of vision and of language. For visual-content understanding, people have developed several backbone
models and shown their effectiveness on large vision datasets . Pioneering works also show the generalizability of these pretrained (especially on ImageNet) backbone models by ﬁne-tuning them on different tasks. In terms
of language understanding, last year, we witnessed
strong progress towards building a universal backbone model with large-scale contextualized language model pre-training , which
has improved performances on various tasks to signiﬁcant levels.
Despite these inﬂuential singlemodality works, large-scale pretraining and ﬁnetuning studies for the modality-pair of vision and
language are still under-developed.
Therefore, we present one of the ﬁrst works in
building a pre-trained vision-and-language crossmodality framework and show its strong performance on several datasets. We name this framework “LXMERT: Learning Cross-Modality Encoder Representations from Transformers” (pronounced: ‘leksmert’).
This framework is modeled after recent BERT-style innovations while
further adapted to useful cross-modality scenarios.
Our new cross-modality model focuses on
learning vision-and-language interactions, especially for representations of a single image and its
descriptive sentence. It consists of three Transformer encoders: an object
relationship encoder, a language encoder, and a
cross-modality encoder. In order to better learn
the cross-modal alignments between vision and
language, we next pre-train our model with ﬁve
diverse representative tasks: (1) masked crossmodality language modeling, (2) masked object
prediction via RoI-feature regression, (3) masked
object prediction via detected-label classiﬁcation,
(4) cross-modality matching, and (5) image question answering.
Different from single-modality
pre-training (e.g., masked LM in BERT), this
multi-modality pre-training allows our model to
infer masked features either from the visible elements in the same modality, or from aligned components in the other modality. In this way, it helps
build both intra-modality and cross-modality relationships.
Empirically, we ﬁrst evaluate LXMERT on
two popular visual question-answering datasets,
VQA and GQA . Our model outperforms previous works in all question categories (e.g., Binary,
Number, Open) and achieves state-of-the-art results in terms of overall accuracy. Further, to show
the generalizability of our pre-trained model, we
ﬁne-tune LXMERT on a challenging visual reasoning task, Natural Language for Visual Reasoning for Real (NLVR2) , where
we do not use the natural images in their dataset
for our pre-training, but ﬁne-tune and evaluate
on these challenging, real-world images. In this
setup, we achieve a large improvement of 22% absolute in accuracy (54% to 76%, i.e., 48% relative
error reduction) and 30% absolute in consistency
(12% to 42%, i.e., 34% relative error reduction).
Lastly, we conduct several analysis and ablation
studies to prove the effectiveness of our model
components and diverse pre-training tasks by removing them or comparing them with their alternative options. Especially, we use several ways to
take the existing BERT model and its variants, and
show their ineffectiveness in vision-and-language
tasks, which overall proves the need of our new
cross-modality pre-training framework.
Model Architecture
We build our cross-modality model with selfattention and cross-attention layers following the
recent progress in designing natural language processing models ). As shown in Fig. 1, our model takes two
inputs: an image and its related sentence (e.g., a
caption or a question). Each image is represented
as a sequence of objects, and each sentence is represented as a sequence of words. Via careful design and combination of these self-attention and
cross-attention layers, our model is able to generate language representations, image representations, and cross-modality representations from the
inputs. Next, we describe the components of this
model in detail.
Input Embeddings
The input embedding layers in LXMERT convert the inputs (i.e., an image and a sentence)
into two sequences of features: word-level sentence embeddings and object-level image embeddings. These embedding features will be further
processed by the latter encoding layers.
Word-Level Sentence Embeddings
A sentence
is ﬁrst split into words {w1, . . . , wn} with length
of n by the same WordPiece tokenizer in Devlin et al. . Next, as shown in
Fig. 1, the word wi and its index i (wi’s absolute
position in the sentence) are projected to vectors
by embedding sub-layers, and then added to the
index-aware word embeddings:
ˆwi = WordEmbed (wi)
ˆui = IdxEmbed (i)
hi = LayerNorm ( ˆwi + ˆui)
Object-Level Image Embeddings
Instead of
using the feature map output by a convolutional
neural network, we follow Anderson et al. 
in taking the features of detected objects as the embeddings of images. Speciﬁcally, the object detector detects m objects {o1, . . . , om} from the image (denoted by bounding boxes on the image in
Fig. 1). Each object oj is represented by its position feature (i.e., bounding box coordinates) pj
and its 2048-dimensional region-of-interest (RoI)
feature fj. Instead of directly using the RoI feature
fj without considering its position pj in Anderson
et al. , we learn a position-aware embedding
vj by adding outputs of 2 fully-connected layers:
ˆfj = LayerNorm (WFfj + bF)
ˆpj = LayerNorm (WPpj + bP)
In addition to providing spatial information in visual reasoning, the inclusion of positional information is necessary for our masked object prediction pre-training task (described in Sec. 3.1.2).
riding a bike
with a dog in a
Cross-Modality Encoder
Language Encoder
Object-Relationship Encoder
Figure 1: The LXMERT model for learning vision-and-language cross-modality representations.
‘Self’ and
‘Cross’ are abbreviations for self-attention sub-layers and cross-attention sub-layers, respectively. ‘FF’ denotes
a feed-forward sub-layer.
Since the image embedding layer and the following attention layers are agnostic to the absolute indices of their inputs, the order of the object is not
speciﬁed. Lastly, in Equation 1, the layer normalization is applied to the projected features before
summation so as to balance the energy of the two
different types of features.
We build our encoders, i.e., the language encoder,
the object-relationship encoder, and the crossmodality encoder, mostly on the basis of two kinds
of attention layers: self-attention layers and crossattention layers. We ﬁrst review the deﬁnition and
notations of attention layers and then discuss how
they form our encoders.
Background: Attention Layers
Attention layers aim to
retrieve information from a set of context vectors
{yj} related to a query vector x. An attention layer
ﬁrst calculates the matching score aj between the
query vector x and each context vector yj. Scores
are then normalized by softmax:
aj = score(x, yj)
αj = exp(aj)/
The output of an attention layer is the weighted
sum of the context vectors w.r.t.
the softmaxnormalized score: AttX→Y (x, {yj}) = P
An attention layer is called self-attention when the
query vector x is in the set of context vectors {yj}.
Speciﬁcally, we use the multi-head attention following Transformer .
Single-Modality Encoders
After the embedding layers, we ﬁrst apply two transformer encoders , i.e., a language encoder and an object-relationship encoder, and
each of them only focuses on a single modality (i.e., language or vision).
Different from
BERT , which applies the
transformer encoder only to language inputs, we
apply it to vision inputs as well (and to crossmodality inputs as described later below). Each
layer (left dashed blocks in Fig. 1) in a singlemodality encoder contains a self-attention (‘Self’)
sub-layer and a feed-forward (‘FF’) sub-layer,
where the feed-forward sub-layer is further composed of two fully-connected sub-layers. We take
NL and NR layers in the language encoder and the
object-relationship encoder, respectively. We add
a residual connection and layer normalization (annotated by the ‘+’ sign in Fig. 1) after each sublayer as in Vaswani et al. .
Cross-Modality Encoder
Each cross-modality
layer (the right dashed block in Fig. 1) in the crossmodality encoder consists of two self-attention
sub-layers, one bi-directional cross-attention sublayer, and two feed-forward sub-layers. We stack
(i.e., using the output of k-th layer as the input
of (k+1)-th layer) NX these cross-modality layers in our encoder implementation. Inside the k-th
layer, the bi-directional cross-attention sub-layer
(‘Cross’) is ﬁrst applied, which contains two unidirectional cross-attention sub-layers: one from
language to vision and one from vision to language. The query and context vectors are the outputs of the (k-1)-th layer (i.e., language features
} and vision features {vk−1
i = CrossAttL→R
, . . . , vk−1
j = CrossAttR→L
, . . . , hk−1
RoI-Feature
Regression
Who is eating
the carrot?
[MASK] eat -ing
the [MASK] ?
Detected-Label
Classification
Masked Cross-
Modality LM
Answer? {RABBIT}
Match? {YES}
is eat -ing
the carrot ?
Cross-Modality
Matching & QA
Figure 2: Pre-training in LXMERT. The object RoI features and word tokens are masked. Our ﬁve pre-training
tasks learn the feature representations based on these masked inputs. Special tokens are in brackets and classiﬁcation labels are in braces.
The cross-attention sub-layer is used to exchange
the information and align the entities between
the two modalities in order to learn joint crossmodality representations. For further building internal connections, the self-attention sub-layers
(‘Self’) are then applied to the output of the crossattention sub-layer:
i = SelfAttL→L
1, . . . , ˆhk
j = SelfAttR→R
1, . . . , ˆvk
Lastly, the k-th layer output {hk
i } and {vk
produced by feed-forward sub-layers (‘FF’) on top
i } and {ˆvk
j }. We also add a residual connection and layer normalization after each sub-layer,
similar to the single-modality encoders.
Output Representations
As shown in the right-most part of Fig. 1, our
LXMERT cross-modality model has three outputs
for language, vision, and cross-modality, respectively. The language and vision outputs are the
feature sequences generated by the cross-modality
encoder. For the cross-modality output, following the practice in Devlin et al. , we append a special token [CLS] (denoted as the top
yellow block in the bottom branch of Fig. 1) before
the sentence words, and the corresponding feature
vector of this special token in language feature sequences is used as the cross-modality output.
Pre-Training Strategies
In order to learn a better initialization which understands connections between vision and language, we pre-train our model with different
modality pre-training tasks on a large aggregated
Pre-Training Tasks
Language Task: Masked
Cross-Modality LM
On the language side, we take the masked crossmodality language model (LM) task. As shown
in the bottom branch of Fig. 2, the task setup
is almost same to BERT :
words are randomly masked with a probability of 0.15 and the model is asked to predict
these masked words. In addition to BERT where
masked words are predicted from the non-masked
words in the language modality, LXMERT, with
its cross-modality model architecture, could predict masked words from the vision modality as
well, so as to resolve ambiguity. For example, as
shown in Fig. 2, it is hard to determine the masked
word ‘carrot’ from its language context but the
word choice is clear if the visual information is
considered. Hence, it helps building connections
from the vision modality to the language modality,
and we refer to this task as masked cross-modality
LM to emphasize this difference. We also show
that loading BERT parameters into LXMERT will
do harm to the pre-training procedure in Sec. 5.1
since BERT can perform relatively well in the
language modality without learning these crossmodality connections.
Vision Task: Masked Object Prediction
As shown in the top branch of Fig. 2, we pretrain the vision side by randomly masking objects (i.e., masking RoI features with zeros) with
a probability of 0.15 and asking the model to predict proprieties of these masked objects. Similar
to the language task (i.e., masked cross-modality
LM), the model can infer the masked objects either from visible objects or from the language
Inferring the objects from the vision
Image Split
Sentences (or Questions)
MS COCO - VG
MS COCO ∩VG
VG - MS COCO
Table 1: Amount of data for pre-training. Each image has multiple sentences/questions. ‘Cap’ is caption. ‘VG’ is
Visual Genome. Since MS COCO and VG share 51K images, we list it separately to ensure disjoint image splits.
side helps learn the object relationships, and inferring from the language side helps learn the crossmodality alignments. Therefore, we perform two
sub-tasks: RoI-Feature Regression regresses the
object RoI feature fj with L2 loss, and Detected-
Label Classiﬁcation learns the labels of masked
objects with cross-entropy loss. In the ‘Detected-
Label Classiﬁcation’ sub-task, although most of
our pre-training images have object-level annotations, the ground truth labels of the annotated
objects are inconsistent in different datasets (e.g.,
different number of label classes). For these reasons, we take detected labels output by Faster R-
CNN . Although detected labels
are noisy, experimental results show that these labels contribute to pre-training in Sec. 5.3.
Cross-Modality Tasks
As shown in the middle-rightmost part of Fig. 2,
to learn a strong cross-modality representation, we
pre-train the LXMERT model with 2 tasks that explicitly need both language and vision modalities.
Cross-Modality Matching
For each sentence,
with a probability of 0.5, we replace it with a mismatched2 sentence. Then, we train a classiﬁer to
predict whether an image and a sentence match
each other. This task is similar to ‘Next Sentence
Prediction’ in BERT .
Image Question Answering (QA)
In order to
enlarge the pre-training dataset (see details in
Sec. 3.2), around 1/3 sentences in the pre-training
data are questions about the images.
the model to predict the answer to these imagerelated questions when the image and the question are matched (i.e., not randomly replaced in
the cross-modality matching task). We show that
2 We take a sentence from another image as the mismatched sentence. Although the sentence and the image still
have chance to match each other, this probability is very low.
pre-training with this image QA leads to a better
cross-modality representation in Sec. 5.2.
Pre-Training Data
As shown in Table. 1, we aggregate pre-training
data from ﬁve vision-and-language datasets whose
images come from MS COCO 
or Visual Genome .
Besides the two original captioning datasets, we also
aggregate three large image question answering
(image QA) datasets: VQA v2.0 , GQA balanced version , and VG-QA . We
only collect train and dev splits in each dataset to
avoid seeing any test data in pre-training. We conduct minimal pre-processing on the ﬁve datasets to
create aligned image-and-sentence pairs. For each
image question answering dataset, we take questions as sentences from the image-and-sentence
data pairs and take answers as labels in the image QA pre-training task (described in Sec. 3.1.3).
This provides us with a large aligned vision-andlanguage dataset of 9.18M image-and-sentence
pairs on 180K distinct images. In terms of tokens,
the pre-training data contain around 100M words
and 6.5M image objects.
Pre-Training Procedure
We pre-train our LXMERT model on the large aggregated dataset (discussed in Sec. 3.2) via the pretraining tasks (Sec. 3.1). The details about the data
splits are in the Appendix. The input sentences are
split by the WordPiece tokenizer 
provided in BERT . The objects are detected by Faster R-CNN which is pre-trained on Visual Genome
 ). We do not
ﬁne-tune the Faster R-CNN detector and freeze
it as a feature extractor.
Different from detecting variable numbers of objects in Anderson et al.
 , we consistently keep 36 objects for each
Image Only
Language Only
State-of-the-Art
Table 2: Test-set results. VQA/GQA results are reported on the ‘test-standard’ splits and NLVR2 results are
reported on the unreleased test set (‘Test-U’). The highest method results are in bold. Our LXMERT framework
outperforms previous (comparable) state-of-the-art methods on all three datasets w.r.t. all metrics.
image to maximize the pre-training compute utilization by avoiding padding. For the model architecture, we set the numbers of layers NL, NX, and
NR to 9, 5, and 5 respectively.3 More layers are
used in the language encoder to balance the visual
features extracted from 101-layer Faster R-CNN.
The hidden size 768 is the same as BERTBASE. We
pre-train all parameters in encoders and embedding layers from scratch (i.e., model parameters
are randomly initialized or set to zero). We also
show results of loading pre-trained BERT parameters in Sec. 5.1. LXMERT is pre-trained with multiple pre-training tasks and hence multiple losses
are involved.
We add these losses with equal
weights. For the image QA pre-training tasks, we
create a joint answer table with 9500 answer candidates which roughly cover 90% questions in all
three image QA datasets.
We take Adam as
the optimizer with a linear-decayed learning-rate
schedule and a peak learning rate at 1e −4.
We train the model for 20
epochs (i.e., roughly 670K4 optimization steps)
with a batch size of 256. We only pre-train with
image QA task (see Sec. 3.1.3) for the last 10
epochs, because this task converges faster and empirically needs a smaller learning rate. The whole
pre-training process takes 10 days on 4 Titan Xp.
Fine-tuning
Fine-tuning is fast and robust. We
only perform necessary modiﬁcation to our model
with respect to different tasks (details in Sec. 4.2).
We use a learning rate of 1e −5 or 5e −5, a batch
size of 32, and ﬁne-tune the model from our pre-
3If we count a single modality layer as one half crossmodality layer, the equivalent number of cross-modality layers is (9 + 5)/2 + 5 = 12, which is same as the number of
layers in BERTBASE.
4For comparison, ResNet on ImageNet classiﬁcation
takes 600K steps and BERT takes 1000K steps.
trained parameters for 4 epochs.
Experimental Setup and Results
In this section, we ﬁrst introduce the datasets that
are used to evaluate our LXMERT framework and
empirically compare our single-model results with
previous best results.
Evaluated Datasets
We use three datasets for evaluating our LXMERT
framework:
VQA v2.0 dataset , GQA , and
NLVR2. See details in Appendix.
Implementation Details
On VQA and GQA, we ﬁne-tune our model from
the pre-trained snapshot without data augmentation (analysis in Sec. 5.2). When training GQA,
we only take raw questions and raw images as inputs and do not use other supervisions (e.g., functional programs and scene graphs). Since each datum in NLVR2 has two natural images img0, img1
and one language statement s, we use LXMERT
to encode the two image-statement pairs (img0, s)
and (img1, s), then train a classiﬁer based on the
concatenation of the two cross-modality outputs.
More details in Appendix.
Empirical Comparison Results
We compare our single-model results with previous best published results on VQA/GQA teststandard sets and NLVR2 public test set.
Besides previous state-of-the-art (SotA) methods, we
also show the human performance and imageonly/language-only results when available.
The SotA result is BAN+Counter in Kim
et al. , which achieves the best accuracy
among other recent works:
MFH , Pythia , DFAF , and Cycle-Consistency (Shah et al.,
LXMERT improves the SotA overall accuracy (‘Accu’ in Table 2) by 2.1% and
has 2.4% improvement on the ‘Binary’/‘Other’
question sub-categories.
Although LXMERT
does not explicitly take a counting module as in
BAN+Counter, our result on the counting-related
questions (‘Number’) is still equal or better.6
The GQA 
SotA result is taken from BAN 
on the public leaderbaord.
Our 3.2% accuracy
gain over the SotA GQA method is higher than
VQA, possibly because GQA requires more visual reasoning. Thus our framework, with novel
encoders and cross-modality pre-training, is suitable and achieves a 4.6% improvement on opendomain questions (‘Open’ in Table 2).7
NLVR2 is a challenging visual reasoning dataset where some existing approaches fail, and the SotA method is ‘MaxEnt’ in
Suhr et al. . The failure of existing methods (and our model w/o pre-training in Sec. 5.1)
indicates that the connection between vision and
language may not be end-to-end learned in a
complex vision-and-language task without largescale pre-training. However, with our novel pretraining strategies in building the cross-modality
connections, we signiﬁcantly improve the accuracy (‘Accu’ of 76.2% on unreleased test set ‘Test-
U’, in Table 2) by 22%. Another evaluation metric consistency measures the proportion of unique
sentences for which all related image pairs8 are
correctly predicted.
Our LXMERT model improves consistency (‘Cons’) to 42.1% , MUAN , and
MLI .
MCAN (VQA challenge version) uses stronger mixture of detection features and achieves
72.8% on VQA 2.0 test-standard. MUAN achieves 71.1%
(compared to our 72.5%).
6Our result on VQA v2.0 ‘test-dev’ is 72.4%.
7Our result on GQA ‘test-dev’ is 60.0%.
8Each statement in NLVR2 is related to multiple image
pairs in order to balance the dataset answer distribution.
9These are the unreleased test set (‘Test-U’) results. On
the public test set (‘Test-P’), LXMERT achieves 74.5% Accu
and 39.7% Cons.
LSTM + BUTD
BERT + BUTD
BERT + 1 CrossAtt
BERT + 2 CrossAtt
BERT + 3 CrossAtt
BERT + 4 CrossAtt
BERT + 5 CrossAtt
Train + BERT
Train + scratch
Pre-train + BERT
Pre-train + scratch
Table 3: Dev-set accuracy of using BERT.
framework by comparing it with some alternative choices or by excluding certain model
components/pre-training strategies.
BERT versus LXMERT
BERT is a pre-trained language encoder which improves several language
As shown in Table 3, we discuss several ways to incorporate a BERTBASE pre-trained
model for vision-language tasks and empirically
compare it with our LXMERT approach.
Although our full model achieves accuracy of 74.9%
on NLVR2, all results without LXMERT pretraining is around 22% absolute lower.
(BUTD) attention method
encodes questions with GRU , then attends to object RoI features {fj} to
predict the answer. We apply BERT to BUTD by
replacing its GRU language encoder with BERT.
As shown in the ﬁrst block of Table. 3, results of
BERT encoder is comparable to LSTM encoder.
BERT+CrossAtt
Since BUTD only takes the
raw RoI features {fj} without considering the object positions {pj} and object relationships, we
enhance BERT+BUTD with our novel positionaware object embedding (in Sec. 2.1) and crossmodality layers (in Sec. 2.2).
As shown in the
second block of Table 3, the result of 1 crossmodality layer is better than BUTD, while stacking more cross-modality layers further improves
However, without our cross-modality pre-
1. P20 + DA
2. P20 + FT
3. P10+QA10 + DA
4. P10+QA10 + FT
Table 4: Dev-set accuracy showing the importance
of the image-QA pre-training task.
P10 means pretraining without the image-QA loss for 10 epochs while
QA10 means pre-training with the image-QA loss. DA
and FT mean ﬁne-tuning with and without Data Augmentation, resp.
training (BERT is language-only pre-trained), results become stationary after adding 3 crossattention layers and have a 3.4% gap to our full
LXMERT framework (the last bold row in Table 3).
BERT+LXMERT
We also try loading BERT
parameters10 into LXMERT, and use it in model
training (i.e., without LXMERT pre-training) or
in pre-training. We show results in the last block
of Table. 3. Compared to the ‘from scratch’ (i.e.,
model parameters are randomly initialized) approach, BERT improves the ﬁne-tuning results but
it shows weaker results than our full model. Empirically, pre-training LXMERT initialized with
BERT parameters has lower (i.e., better) pretraining loss for the ﬁrst 3 pre-training epochs
but was then caught up by our ‘from scratch’ approach. A possible reason is that BERT is already
pre-trained with single-modality masked language
model, and thus could do well based only on the
language modality without considering the connection to the vision modality (as discussed in
Sec. 3.1.1).
Effect of the Image QA Pre-training Task
We show the importance of image QA pre-training
task (introduced in Sec. 3.1.3) by excluding it or
comparing it with its alternative: data augmentation.
Pre-training w/ or w/o Image QA
compare with our original pre-training procedure
(10 epochs w/o QA + 10 epochs w/ QA, details in
Sec. 3.3) , we pre-train LXMERT model without
image QA task for 20 epochs. As shown in Ta-
10 Since our language encoder is same as BERTBASE, except the number of layers (i.e., LXMERT has 9 layers and
BERT has 12 layers), we load the top 9 BERT-layer parameters into the LXMERT language encoder.
1. No Vision Tasks
4. Feat + Label
Dev-set accuracy of different vision pretraining tasks. ‘Feat’ is RoI-feature regression; ‘Label’
is detected-label classiﬁcation.
ble 4 rows 2 and 4, pre-training with QA loss improves the result on all three datasets. The 2.1%
improvement on NLVR2 shows the stronger representations learned with image-QA pre-training,
since all data (images and statements) in NLVR2
are not used in pre-training.
Pre-training versus Data Augmentation
augmentation (DA) is a technique which is used
in several VQA implementations .
increases the amount of training data by adding
questions from other image QA datasets.
LXMERT framework instead uses multiple QA
datasets in pre-training and is ﬁne-tuned only on
one speciﬁc dataset. Since the overall amounts of
data used in pre-training and DA are similar, we
thus can fairly compare these two strategies, and
results show that our QA pre-training approach
outperforms DA. We ﬁrst exclude the QA task in
our pre-training and show the results of DA ﬁnetuning.
As shown in Table. 4 row 1, DA ﬁnetuning decreases the results compared to non-DA
ﬁne-tuning in row 2. Next, we use DA after QApre-training (row 3) and DA also drops the results.
Effect of Vision Pre-training tasks
We analyze the effect of different vision pretraining tasks in Table 5. Without any vision tasks
in pre-training (i.e., only using the language and
cross-modality pre-training tasks), the results (row
1 of Table 5) are similar to BERT+3 CrossAtt in
Table 3. The two visual pre-training tasks (i.e.,
RoI-feature regression and detected-label classiﬁcation) could get reasonable results (row 2 and row
3) on their own, and jointly pre-training with these
two tasks achieves the highest results (row 4).
Related Work
Model Architecture: Our model is closely related
to three ideas:
bi-directional attention, Transformer, and BUTD. Lu et al. applies bi-
directional attention to the vision-and-language
tasks while its concurrent work BiDAF adds modeling layers in solving reading
comprehension.
Transformer is ﬁrst used in machine translation, we
utilize it as our single-modality encoders and
design our cross-modality encoder based on it.
BUTD embeds images
with the object RoI features, we extend it with object positional embeddings and object relationship
Pre-training: After ELMo ,
GPT , and BERT show improvements in language understanding tasks with large-scale pre-trained language model, progress has been made towards the
cross-modality pre-training.
XLM learns the joint cross-lingual representations by leveraging the monolingual data
and parallel data.
VideoBert 
takes masked LM on the concatenation of language words and visual tokens, where the visual
tokens are converted from video frames by vector quantization. However, these methods are still
based on a single transformer encoder and BERTstype token-based pre-training, thus we develop
a new model architecture and novel pre-training
tasks to satisfy the need of cross-modality tasks.
Recent works since our EMNLP submission:
This version of our paper (and all current results)
was submitted to EMNLP11 and was used to participate in the VQA and GQA challenges in May
2019. Since our EMNLP submission, a few other
useful preprints have recently been released (in
August) on similar cross-modality pre-training directions: ViLBERT and Visual-
BERT . Our LXMERT methods
differs from them in multiple ways: we use a more
detailed, multi-component design for the crossmodality model (i.e., with an object-relationship
encoder and cross-modality layers) and we employ additional, useful pre-training tasks (i.e., RoIfeature regression and image question answering).
These differences result in the current best performance (on overlapping reported tasks): a margin
of 1.5% accuracy on VQA 2.0 and a margin of
9% accuracy on NLVR2 (and 15% in consistency).
LXMERT is also the only method which ranks in
the top-3 on both the VQA and GQA challenges
11EMNLP deadline was on May 21, 2019, and the standard
ACL/EMNLP arxiv ban rule was in place till the notiﬁcation
date of August 12, 2019.
among more than 90 teams. We provide a detailed
analysis to show how these additional pre-training
tasks contribute to the ﬁne-tuning performance in
Sec. 5.2 and Sec. 5.3.
Conclusion
cross-modality
framework,
LXMERT, for learning the connections between
vision and language. We build the model based
on Transfermer encoders and our novel crossmodality encoder. This model is then pre-trained
with diverse pre-training tasks on a large-scale
dataset of image-and-sentence pairs. Empirically,
we show state-of-the-art results on two image
QA datasets (i.e., VQA and GQA) and show the
model generalizability with a 22% improvement
on the challenging visual reasoning dataset of
NLVR2. We also show the effectiveness of several
model components and training methods via
detailed analysis and ablation studies.
Acknowledgments
We thank the reviewers for their helpful comments.
This work was supported by ARO-YIP
Award #W911NF-18-1-0336, and awards from
Google, Facebook, Salesforce, and Adobe. The
views, opinions, and/or ﬁndings contained in this
article are those of the authors and should not be
interpreted as representing the ofﬁcial views or
policies, either expressed or implied, of the funding agency. We also thank Alane Suhr for evaluation on NLVR2.