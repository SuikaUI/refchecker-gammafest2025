Computer Speech and Language 12, 349–373
Article No. la980106
Evaluation in the context of natural language
generation
C. Mellish† and R. Dale‡
†Department of Artiﬁcial Intelligence, University of Edinburgh, Scotland,
‡Microsoft Research Institute, Macquarie University, Australia
What role should evaluation play in the development of natural
language generation () techniques and systems? In this paper we
describe what is involved in natural language generation, and survey
how evaluation has ﬁgured in work in this area to date. We comment
on the issues raised by this existing work and on how the problems of
evaluation are diﬀerent from the problems of evaluating work in
natural language understanding. The paper is concluded by suggesting
a way forward by looking more closely at the component problems
that are addressed in natural language generation research; a
particular text generation application is examined and the issues that
are raised in assessing its performance on a variety of dimensions are
looked at.
1998 Academic Press
1. Introduction
Over the last 10 years, the level of interest and concern expressed by the natural
language processing community with regard to evaluation has increased substantially.
Although this has been driven most notably by the evaluative focus of the sponsored and conferences, the eﬀects have been much more widespread than
this, and not just restricted to those working in those particular areas. It has now
become widely accepted that work in more generally should pay close attention to
the evaluation of results. Sparck, Jones and Galliers provide an important step
towards consolidating what we have learned so far and its consequences for the ﬁeld
of as a whole.
Natural language generation () is, in some sense at least, exactly half of the
problem of natural language processing; but the evaluation of systems is barely
mentioned by Sparck Jones and Galliers. This is not, of course, surprising, and should
not be taken as a criticism of that work: just as there is less work in as compared
to as a whole, there is similarly much less that has been written on the evaluation
of systems than has been written on the evaluation of systems or their
components. But neither have researchers in been silent on the matter: in 1990,
an workshop was held on the theme of the Evaluation of Natural Language
0885–2308/98/040349+25 $30.00/0
1998 Academic Press
C. Mellish and R. Dale
Generation Systems, and papers by Meteer and McDonald and Moore 
provide a fair expression of the community’s views on evaluation at that time. In
the intervening years, there has been a noticeable increase in empirical work. A common
request from reviewers of submissions to journals and conferences is for some
material on evaluation to be included, and researchers in the ﬁeld have tried to respond
to such requests. In the last 5 or so years several studies have appeared, which address
questions of evaluation in more directly than was previously the case.
The aim of this paper is to survey some of this work to see how much further forward
we have come since the beginning of the decade. Do we now have a clearer understanding
of how evaluation can play a role in the development of systems and techniques?
On the basis of what has been done in recent years, we try here to identify the
problematic issues evaluation raises, and suggest a way in which the evaluation of 
systems and subtasks might be considered in the future. We explore some of these ideas
by taking a simple case study of system development and exploring how evaluation
might play a role in this work.
Natural language generation is a research area whose content is often unclear to
those working outside of the area. We begin, therefore, by providing in Section 2 an
overview of what is involved in natural language generation, and elaborate upon the
relationship between and the process of natural language understanding.
In Section 3 the question of what it might mean to carry out evaluation in the context
of is addressed. We distinguish the evaluation of systems from the evaluation of
their underlying theories, and distinguish both of these from task evaluation; each of
these aspects is considered by looking at how evaluation has been carried out in the
ﬁeld so far.
On the basis of the preceding material, in Section 4 a number of questions that
anyone attempting evaluation in the context of must face are identiﬁed. In Section
5, we suggest that rather than attempting to mirror the methods of evaluation used for
some tasks, at this stage in the development of the ﬁeld it is best to reconsider the
problem of evaluation from the inside, looking at the component problems that
make up .
2. What is Natural Language Generation?
Natural Language Generation is the name we give to a body of research that is concerned
with the process of mapping from some underlying representation of information to
a presentation of that information in linguistic form, whether textual or spoken. The
underlying representation may be symbolic (for example, an expert system knowledge
base) or numeric (for example, a database containing stock market prices) but it is
generally non-linguistic.
From a theoretical perspective, the task of is to go from some communicative
goal (such as a speaker’s desire to inform the hearer of something or to persuade them
of something) to a text, written or spoken, that satisﬁes this goal. The key issues for
generation often revolve around the notion of choice. Of all the possible texts that
could be generated, which is the most appropriate to the current context?
A complete system must take a great many decisions to produce such a result.
Given the complexity of the task, individual pieces of work in often focus on what
are viewed as component parts of this overall problem; this is no diﬀerent, of course,
to the situation in natural language understanding. The most common decomposition
Evaluation in the context of natural language generation
of the task, going back at least as far as Thompson , separates decisions
about what to say from decisions about how to say it, sometimes referred to as a
distinction between and decisions. During much of the 1980s, this
distinction manifested itself within systems as an architectural decomposition into
a component and a component. However, these
terms can mean very diﬀerent things to diﬀerent researchers. When we look at the
details, we see that there is, as yet, no universally accepted architectural model for
natural language generation. This has perhaps been most evident with respect to the
question of , where the task is to determine what word or words should
be used to express some underlying concept: for some researchers this is part of text
planning, while for others it belongs ﬁrmly within linguistic realization.
Ultimately, when we look at research in the ﬁeld, it becomes apparent that there are
a number of somewhat more speciﬁc problems to be dealt with in generating language,
with the diﬀerences of view being with regard to how these are best combined into
components in a system. At the time of writing, we can identify six main categories of
problems that are discussed in the literature:
Content Determination: deciding what information should be included in the text,
and what should be omitted. Many systems operate in contexts where the
information that is to be communicated is selected from a larger body of information,
with that selection depending upon a variety of contextual factors, including the
intended purpose of the text to be generated and the particular audience at whom it
is to be directed.
Document Structuring: deciding how the text should be organized and structured.
As soon as we look at real multi-sentential texts, it becomes obvious that there is
considerable structure above the level of the sentence. This can be easily demonstrated
by taking, for example, a newspaper story and randomly re-ordering the paragraphs
and their constituent sentences: the results are invariably incoherent. This means
that, given some body of information to convey, an system has to choose an
appropriate organization for that information.
Lexicalization: choosing the particular words or phrases that are required in order
to communicate the speciﬁed information. In some cases the underlying elements of
the domain in which the system is operating, will map straightforwardly onto
speciﬁc words and phrases. However, we cannot assume that the information the
system has to work with is linguistic in nature, and so the system may have to choose
between diﬀerent ways of expressing underlying concepts in words. This is most
obviously true of systems which are intended to generate texts in more than one
language from a common underlying source, although monolingual systems may
also need to do work here: for example, it may be appropriate to vary the words
used for stylistic eﬀect.
Aggregation: deciding how information should be composed into sentence-sized
chunks. Again, we cannot assume that the underlying information is in the form of
elements that can be straightforwardly expressed as sentences: often, in the interests
of ﬂuency, it will be appropriate to fold several elements of information into one
Referring Expression Generation: determining what properties of an entity should
C. Mellish and R. Dale
be used in referring to that entity. In real natural language texts, anaphoric resources
such as pronouns and reduced deﬁnite noun phrases are used to refer to entities once
they have been introduced. This means that an system must decide how to refer
to a given entity in the most appropriate fashion; otherwise the risk is redundant
and stilted text.
Surface Realization: determining how the underlying content of a text should be
mapped into a sequence of grammatically correct sentences. A generally held view
is that the same propositional content can often be realized using diﬀerent sentential
forms (for example, a proposition may be expressed via either an active or a passive
sentence). An system has to decide which syntactic form to use, and it has to
ensure that the resulting text is syntactically and morphologically correct.
This set of categories does not necessarily exhaust the range of problems to be dealt
with in natural language generation; and it is not the case that each necessarily
corresponds to a speciﬁc task or module in an system. However, for many
researchers, each of the above categories constitutes a research area in its own right,
and serves as the focus for the development of theories and techniques.
As the authors hope to have demonstrated by cataloguing this inventory of concerns,
the process of natural language generation can be seen as the inverse of the process of
natural language understanding () as a whole. Whereas in the concern is to
map from text to some representation of meaning, is concerned with mapping
from some representation of meaning to text. It is important to appreciate that is
not simply the inverse of the parsing task in : if any component part of the overall
process of is the inverse of parsing (and this itself is questionable), then it is
linguistic realization. The overall problem of is best seen as the inverse of the
broader problem of determining the speaker’s intention that underlies a text.
As noted earlier, while it makes sense to see natural language generation and natural
language understanding as the two halves of the puzzle of natural language processing,
this symmetry has not so far been reﬂected in the balance of eﬀort. Although interest
in has increased considerably in the last 10 years, it is still the case that the bulk
of research in natural language processing is carried out in the context of . There
are a number of reasons for this, perhaps the most important of which is that from a
practical perspective we are faced with a world where there is a great deal of textual
material whose value might be leveraged by the successful achievement of partial
achievement of the goals of . Put simply, there is plenty of raw material for
researchers in to work with. It is less clear what the appropriate raw material for
is, and as a consequence less clear what the real beneﬁts of such research might
be. This point will be returned to below; for the moment, it must be stressed that just
because there is less work in does not mean that the problems to be dealt with are
any less signiﬁcant than those in ; indeed, there are some who would argue that
the eﬀort that has been expended in as a whole to date might have been better
spread across the ﬁeld more equally. Whereas a considerable body of work in is
still concerned with the parsing of single-sentence utterances to obtain representations
of literal semantic content, it is noteworthy that work in is generally more concerned
with pragmatic aspects of meaning (for example, the purposes that underlie an utterance,
and questions of how information can be structured for presentation in a text) and
with larger multi-sentential discourses. An important question here is the extent to
which this diﬀerence in emphasis may impact on methods of evaluation.
Evaluation in the context of natural language generation
3. What does it mean to evaluate NLG?
There are clear reasons why we should consider evaluation to be important for ,
as it is for . We need to be able to demonstrate progress, both to fellow researchers
and to potential funders, and the ﬁeld needs to be able to support possible users of the
technology by helping them make decisions about whether it is good enough for their
purposes. There is, however, no extant wisdom as to how work in should be
evaluated.
As Sparck Jones and Galliers and other commentators on evaluation have
pointed out, evaluation can have many objectives and can consider many diﬀerent
dimensions of a system or theory. We will suggest here that it can be fruitful to consider
the evaluation of techniques to break down into the following three main categories;
the order of presentation here reﬂects roughly the order in which they would be relevant
in the development of an system.
Evaluating Properties of the Theory: assessing the characteristics (e.g. coverage,
domain-independence) of some theory underlying an system or one of its parts.
We might want to determine whether full implementation of the theory in a domain,
or extenstion of an implementation to a new domain, will be productive. So, for
example, we might ask whether Rhetorical Structure Theory is an appropriate theory
for characterizing the structures of texts to be generated in some domain.
Evaluating Properties of the System: assessing certain characteristics (e.g. coverage,
speed, correctness) of some system or one of its parts. We might want to compare
two systems or algorithms to determine which provides the better results; or we
might want to develop some metric by means of which we can determine when the
performance of a given system has improved in some way.
Applications Potential: evaluating the potential utility of an system in some
environment. We might want to determine whether the use of provides a better
solution than some other approach (for example, the use of a sophisticated mailmerge tool, or the presentation of information via graphics, or the hiring of a human
These categories can be thought of as to a large extent cumulative, in that an evaluation
of a system in general gives some information indirectly about the theories behind its
construction and an evaluation of applications potential in general gives some information about both the properties of the system and the underlying theories. To
evaluate on applications potential, pragmatic issues that are not necessarily of primary
interest to the researcher and which arise when considering the application of any
knowledge-based system need to be considered. For instance, how cost eﬀective is the
system? What is its impact on the existing (e.g. social) environment? Is the system
maintainable? Is it fast enough, robust enough? How does it compare to its rivals (not
necessarily all other systems)? These issues are largely ignored here, as they are
concerned with what Sparck Jones and Galliers refer to as rather than ;
see Reiter & Dale for some discussion of the issues and alternatives that arise
in considering as a solution. Our intended focus in the current paper is on the
second category of evaluation, although it can often be diﬃcult to distinguish aspects
of the theory from aspects of implementation.
C. Mellish and R. Dale
In the next two sections, we summarize the main approaches that have been used
up to the present to evaluate theories as such (rather brieﬂy) and systems (more
fully). We make use of examples from neighbouring ﬁelds (for instance, Machine
Translation and Hypertext) when this seems relevant. In an area of this kind, it is
impossible to be exhaustive, but it is hoped that examples of the most signiﬁcant
evaluation methods that have been used have been covered.
3.1. Previous approaches to evaluation of NLG theories as such
Good systems are based on theories of various kinds, and there are many reasons
why an implementation may not truly represent the theory on which it is built. In
particular, constructing a complete system involves solving many practical problems
that may be unconnected to the original motivating theory, and it would be foolish to
judge a theory by the quality of the generated text if these other problems are inﬂuencing
it signiﬁcantly .
Properties of a theory (e.g. completeness, computational complexity) can sometimes
be demonstrated purely analytically. Such results have been rare in . On the other
hand, there is a tradition of evaluating theories by hand simulation in disciplines such
as Linguistics, though such evaluations are often not carried out on a large enough
scale to yield general conclusions. One might evaluate a grammar, for example, by
simulating by hand its ability to characterize a set of test sentences, extending the
grammar appropriately whenever a deﬁciency in coverage is identiﬁed. In matters
related to , the development of Rhetorical Structure Theory was supported by the fact that hand analysis of many types of text was possible
using it. The following two examples show other places where it was useful to assess
by hand how much of (relevant aspects of) a corpus of natural text could be produced
by diﬀerent algorithms.
Yeh and Mellish used hand simulation during the development stage of their
algorithms to generate referring expressions in Chinese. A human-generated corpus
was analysed and diﬀerent algorithms for referring expression generation were simulated,
with intuition being used to guess what knowledge would be available to an actual text
generator. For each algorithm, the similarity between the referring expressions in the
original corpus and those generated in the simulation gave a basis for evaluating the
algorithm. A sequence of increasingly sophisticated algorithms was motivated by this
incremental evaluation method; this is not unlike the incremental extension of the
coverage of a grammar as described above.
Robin used a human-generated corpus to evaluate his theory of revisionbased generation, which was implemented for the domain of baseball summaries.
Whereas Yeh and Mellish were concerned with ﬂuency, in this case it was coverage,
extensibility and protability between domains that were under investigation. The ﬁrst
set of evaluations estimated how much of the sub-language for the domain was captured
by the results of an analysis of one year’s and two years’ texts (by examining texts for
the succeeding year). This gave a basis for estimating how large a corpus would need
to be analysed in order for the whole of the sub-language to be covered. It was also
possible to estimate what diﬀerence it made using Robin’s revision-based model of
generation, compared to more traditional models. The second set of evaluations
concerned portability to a new ﬁnancial domain. It estimated to what extent the abstract
Evaluation in the context of natural language generation
rules of the system would do useful work in the new domain, thus giving a measure of
the domain-independence of the model.
Evaluating a theory by hand simulation can, of course, be dangerous, because the
simulator can inadvertently introduce errors or bias. This must be balanced against the
incidental problems that arise from actually trying to implement a theory. Overall,
hand simulation seems to be a useful tool to support the early stages of the development
of theories and to measure parameters that would otherwise be inaccessible to any
practical quantitative analysis. However most reported work on evaluates its
theories indirectly through the systems that implement them.
3.2. Previous approaches to evaluation of NLG systems
We now move on to consider the evaluation of systems, concentrating on approaches
which are formal and objective. All computer systems have to be evaluated throughout
their development, and inevitably much of this involves informal analysis by the system
developers, the development of examples to demonstrate theories and techniques, the
tuning of systems to deal with these examples, and so on. Informal evaluation during
system development is necessary and cannot be replaced. Here evaluation is looked at
more as a way of answering the question (which not everybody necessarily wants or
needs to ask) how good is your system?
In discussing the evaluation of systems, Meteer and McDonald echo a
distinction made elsewhere in the evaluation literature between black box and glass box
evaluation. Black box evaluation seeks to assess a system as a whole (in terms of its
inputs and outputs), whereas glass box evaluation seeks additionally to reﬂect on the
contributions of the parts of a system to its overall performance. The authors begin
below by looking at the black box evaluation techniques that have been used in ;
Section 3.2.4 then considers how these can be extended to glass box techniques.
There are a number of diﬀerent kinds of evaluation that can be pursued; these
are referred to here as accuracy evaluation, ﬂuency/intelligibility evaluation and task
evaluation. Each is described in more detail below.
3.2.1. Accuracy evaluation
By accuracy we mean the extent to which the generated text conveys the desired meaning
to the reader. This is not necessarily correlated with ﬂuency, which relates to the extent
to which the text ﬂows and is readable, although we might expect there to be some
relationship between the two: it may be very diﬃcult to determine whether a severely
disﬂuent text conveys a desired meaning.
As a place to start, we can look at work that has been carried out in Machine
Translation. Although the input to the task is quite diﬀerent to that in , both
tasks have in common the creation of natural language text as output.
Several researchers have attempted to
assess the accuracy of the texts produced by systems. This involved the following
three stages:
(1) Selecting a test suite of source texts.
(2) Running the system on this suite.
(3) Evaluating the output by comparing with expert human output for the same
task, or simply asking an expert to rate the accuracy of the generated texts.
C. Mellish and R. Dale
(492:1) * $A [possessive] $B $C *
zaochen / shengwu
dianzhong / dian / shi
Figure 1. Test Description Language example.
In the case of Jordan et al. the test suite was selected in a way intended to reﬂect
the nature of “real” data. Shiwen , on the other hand, constructed a test suite
by hand, in such a way as to test known problem areas in Chinese–English translation.
This second approach is, of course, only useful if the suite construction is undertaken
by somebody not involved in any of the systems to be evaluated.
The comparison between the output of the system and that of expert humans can
itself be carried out by humans. Jordon et al. asked subjects to paraphrase the
target sentences (hence removing uncertainty about the meaning) and then to assess
these paraphrases (compared to the source sentences) as “right”, “nearly right” and
“wrong”. They found a strong correlation between the correctness of the paraphrases
and subjects’ assessment of clarity of meaning and well-formedness (which are concepts
related to intelligibility; see Section 3.2.2. below). However, 8% of the time the evaluators
thought they understood the meaning of a sentence (i.e. it was intelligible) when they
actually did not (i.e. it was not accurate). This demonstration that accuracy and ﬂuency/
intelligibility are not always correlated provides some justiﬁcation for considering the
two factors separately.
Shiwen’s approach was to have automatic assessment of the accuracy of the texts,
with no involvement of humans. This was made possible by the fact that the test suite
was designed by hand to test particular features. For each sentence in the test suite, a
set of expressions in a special Test Description Language (TDL) indicated how the
translation was to be marked. The example in Figure 1 is for Shiwen’s test 492, with
possible results 1 or 0. ∗matches arbitrary characters, [ and ] indicate optionality and /
separates alternatives. If the expression on the right of the ﬁrst rule matches the
input, then the translation will be given the mark 1; if the second one matches, the
mark will be 0.
Accuracy evaluation requires an assessment of the relationship between input and
output. With , one has the advantages of a source text whose content can be
compared (by an expert) with that of the target. Similarly, in automatic summarization,
where the task is to select important sentences or
paragraphs from a document, an expert can
be presented with exactly the same task as the system. In order to apply these accuracy
evaluation techniques to , one would have to ﬁnd a way of presenting the input to
an impartial human evaluator in a way that they would understand. If an expert judges
the output of without understanding its input (as was done, for instance, in the
experiments discussed below) then accuracy assessments will not necessarily
reﬂect directly on the system. In this case, one is really assessing the performance
of a larger system including not just the system but also whatever provides its
input. This could be thought of as a kind of task evaluation (see below).
Of course, although with one does have an input that an expert can understand,
Evaluation in the context of natural language generation
nevertheless in the case of an system a failure in accuracy could be due to either
the process of interpreting the source text to produce the internal representation from
which the target text is generated; or it could be due to a failure in the mapping from
this internal representation to the target text. There is no obvious way to determine
which aspect of the overall process is the source of error (although intuition suggests
that the error is more likely to be due to mistakes in analysis rather than in generation).
Whereas getting an absolute characterization of a system’s accuracy may be hard,
to get a useful comparative evaluation of the accuracy of two systems it might not be
necessary for the human evaluator to have perfect understanding of the input (e.g. the
input might be paraphrased for them by a knowledge engineer).
3.2.2. Fluency/intelligibility evaluation
Fluency concerns the quality of generated text, rather than the extent to which in
conveys the desired information. This is related to the notion of “readability” and will
include notions such a syntactic correctness, stylistic appropriateness, organization and
coherence.
One might hope that there could be some way to directly measure the ﬂuency of a
text. However, although there was a ﬂurry of interest in “readability formulae” in the
1950s and some of these have made their way into modern
word-processors, it seems unlikely that these have much potential for evaluating the
output of systems. The ﬁrst problem is that it is very unclear what the relevance
is of what these formulae measure. The second problem is that, because the formulae
are very precise and simple, most systems could do well in these terms if it was so
wished (for instance, by not using particular words or syntactic constructions). There
are some interesting questions here with regard to the trade-oﬀs between diﬀerent
means for achieving the same readability values: see Dras . However, these do
not make it any easier to assess the real quality of a generated text. More recent work
on controlled languages has developed, with
some empirical support, principles for making text in certain restricted domains
“simpler” (for example, to permit easier comprehension by non-native speakers or
badly-educated users, or to permit easier ). Again, such principles are usually of a
kind that could be implemented fairly straightforwardly in an system and so might
be of dubious use for evaluation. Moreover many systems need to produce text
which is attractive as well as functional, and many researchers are interested in the
problem of producing coherent text exploiting the possibilities oﬀered by real natural
languages. So, to assess readability in general there seems to be no alternative to using
human subjects.
We have found three approaches to assessing readability:
(1) Measuring comprehension time.
(2) Measuring the time taken to post-edit the text to a state of ﬂuency.
(3) Asking human subjects directly to assess the readability of the text.
Minnis proposes the ﬁrst two as ways of evaluating systems and suggests
that there may be a correlation between them. He also suggests that errors can be
classiﬁed and that it may be possible automatically to learn the contributions that the
diﬀerent classes of errors make to post-editing time. This would enable one to assess
ﬂuency directly by counting and classifying errors.
C. Mellish and R. Dale
Eliciting direct human judgements of ﬂuency is standard practice in domains such
as text-to-speech systems . This approach was used by Acker and
Porter in the evaluation of a “view retriever” which retrieved facts from a
knowledge base in order to describe a concept from a particular perspective. The facts
were translated manually into simple English and the subjects were asked to rate the
texts from 1 (“looks like a random set of facts”) to 5 (“as coherent as a good textbook”).
To give a basis for comparison, the same experiment was run with textbook viewpoints,
degraded viewpoints and random collections of facts (Table I). A t-test showed that
there was a signiﬁcant diﬀerence between “view retriever” viewpoints and degraded or
random viewpoints. Because the actual English was hand-generated and the subjects
were unaware of the desired content, this evaluation shed light primarily on the
organization of the texts.
This work was signiﬁcantly extended by Lester and Porter into a system called
, which was a complete system generating explanations from a very
large, and independently developed, knowledge base on Biology. For this, the human
evaluation was developed into a two-panel methodology, where texts produced by the
system are assessed together with texts generated by a ﬁrst human panel, the assessment
being done by a second panel. The judges (who were unaware that some texts were
computer-generated) rated texts (using letter grades A, B, C, D and F) according to
overall quality and coherence, content, organization, writing style, and correctness. For
content and correctness (which relate to accuracy) they used their expert knowledge,
rather than having access to the information provided by the knowledge base. The
diﬀerences between the texts and those for the humans were statistically signiﬁcant
for overall coherence and writing style. The system, however, produced a better
performance overall than one of the human writers.
One can take the notion of ﬂuency beyond monologue and consider to what extent
it is displayed by a dialogue system. Cawsey used human subjects to inform the
development of a tutoring system using to teach students about electronic circuits.
The system allowed ﬂexible dialogues between system and user, and subjects in the
evaluation were asked to use the system (for as long as they needed) to ﬁnd out about
four diﬀerent circuits. The sessions were observed (and recorded) and obvious problems
with the interface were detected by inspection. The subjects were asked to ﬁll in a
questionnaire about their problems with the system and their subjective assessment of
it. This revealed problems, for instance, with the coherence of the dialogue. An additional
task evaluation might have revealed content problems that the users were not aware
In spite of its weaknesses, the approach of using subjective human judgements still
seems to be the most popular way of assessing ﬂuency/intelligibility.
3.2.2. Task evaluation
Many systems are intended to be used by humans to assist them in performing
some task or achieving some goal: for example, learning about a subject, making
decisions, writing better computer programs, or even changing their lifestyle in some
way. Although considering a system’s appropriateness for some independently-motivated
task in the real world involves taking into account many pragmatic factors of applications
potential such as cost and social impact, just considering a system’s raw performance
in support of a more abstract task can also be revealing. Task evaluation involves
Evaluation in the context of natural language generation
TI. Evaluation of “view retriever” texts
(1) Textbook viewpoints
(2) View retriever’s viewpoints
(3) Degraded viewpoints
(4) Random collections of facts
TII. IDAS: Analysis of nodes visited
Contributing
Repeat visits
directly to
to the previous
categories
observing how well a (possibly contrived) task is performed using the system. Since
the task may have its own independent criteria of success—the user either does or
doesn’t learn, he or she makes better or worse decisions, and so on—the advantage is
that one comes to an assessment of the system without directly having to consider
either its input or its output.
Task evaluation has been used in the evaluation of systems. Suppose that the
purpose of the translated texts is to inform: one can then attempt to measure the
success of the translation process by determing how well human readers can answer
questions on the basis of reading the target texts. This measures, for this task, how
well the translation has succeeded (by whatever means) in conveying the underlying
content of the source text.
Another task associated with reading documents is that of assessing and classifying
the documents according to their subject area. Jorden et al. addressed the
problem of for people scanning for texts in particular subject areas. A number of
systems were compared in terms of how well human readers could state the subject
matter of a document by reading its translation.
The user trials assessed a system that used natural
language generation techniques in the automatic generation of hypertexts. Here the
user task was to retrieve relevant information to answer speciﬁc questions, and in the
trials subjects were asked to use the system to complete a test paper, where they had
to answer factual questions from single and multiple hypertext nodes and to relate the
textual descriptions to a picture. Success was measured not just in terms of their ability
to answer the questions correctly but also in terms of how eﬀectively they could do so.
One measure of the eﬀectiveness of the system was produced by an analysis of the
hypertext nodes visited (Table II). The hypertext nodes visited were classiﬁed as
C. Mellish and R. Dale
contributing directly to the question answers, necessary nodes to move through, repeat
visits to useful nodes and other nodes. Only 42% of nodes visited were not useful.
Users got better as they progressed through a series of ﬁve questions (Q1 to Q5), and
they did not seem to get “lost”.
The advantage of task evaluation in avoiding consideration of the detailed role of
the system is, of course, also the root of its main disadvantage. If the system
was indeed successful (or not), it is hard to tell which parts were reponsible for this.
For instance, it could have been speciﬁc aspects of the user interface that had nothing
to do with . In practice, one might hope that other types of evaluation (here,
usability questionnaires completed by the subjects) would shed light on this.
3.2.4. Deriving glass box techniques
Of the evaluation methods considered so far, task evaluation appears to be the ultimate
in black box methods. We now consider ways in which glass box evaluation can be
achieved. Glass box evaluation techniques can often be derived from black box
techniques, at least in the sense that one can concentrate on evaluating one part of an
system at a time. We have observed this happening in the following forms:
(1) Task evaluation being performed using a task that highlights the performance of
a particular module.
(2) Accuracy or ﬂuency evaluation being performed when only the one module is
operating, all other functions being carried out by a human.
(3) Accuracy or ﬂuency evaluation being performed with diﬀerent implementations
of the module concerned.
We will now brieﬂy describe an example of each.
Carter needed to evaluate the results of various automatic methods of
introducing hypertext links between pieces of text (for instance, methods which linked
documents with similar distributions of word frequencies). Unfortunately, most ways
of evaluating hypertext systems treat them as black boxes. Objective
measures of hypertexts (e.g. measurable properties of the link structures such as
reachability) are not clearly related to usability, acceptability measures (e.g. via questionnaires) are subjective and can be distracted by irrelevant features of the system,
error measures are diﬃcult because it is hard to determine what is a genuine error and
learning/memory measures (e.g. via questions asked afterwards) will be aﬀected by both
text and link quality. The solution was to develop a new task for subjects, which
involved them recalling aspects of their traversed paths after performing another task
using the system.
Knight and Chander wanted to evaluate an algorithm for selecting articles
for English noun phrases. Their intention was to use this as a post-editing system for
into English from languages which do not have articles. Their approach was to take
existing (unseen) text, remove the articles and then compare the results of the system
with the original text. This method could be used for other tasks that can be thought
of as completing partial texts (as long as the task does not require any input apart
from the provided text). It could be extended to other situations where a human can
simulate some aspects of the text generation process and hence where an evaluation of
the generated text can be assumed to reﬂect directly on the modules that are operating
automatically.
Evaluation in the context of natural language generation
Yeh and Mellish evaluated diﬀerent versions of their algorithm for generating
Chinese referring expressions by running them in the context of the same automatically
generated text. Since the other parts of the system did not depend on the referring
expression generation, this gave a fair basis for comparison. The actual comparison
was done in a manner somewhat reminiscent of Knight and Chander’s approach.
Referring expressions were removed from the generated text, and human subjects were
asked to select from a list of possible expressions to be inserted at each point. The
similarity (or otherwise) between the human and the machine selections gave some
basis for preferring algorithms taking into account discourse structure and syntactic
salience from an algorithm with neither.
4. Issues and problems in evaluating natural language generation
In this section, we summarize some of the general problems that arise in evaluating
work in natural language generation. As we will see, all of these problems are to some
extent more serious for than they are for .
4.1. What should the input be?
One of the most fundamental obstacles to the evaluation of systems is that, whereas
it is fairly clear what an “input text” is for , and examples can be observed directly
in the world, there is no simple consensus as to what the input to an system should
be. Diﬀerent researchers make diﬀerent assumptions about what should be included in
the input to generation, and diﬀerent tasks require diﬀerent knowledge sources and
goals of diﬀerent kinds. As a result, it is not always easy to judge whether an 
system is actually avoiding diﬃcult questions by including in its input very direct
guidance about how to handle problems that it will have; for instance, a system whose
input is supposedly language-independent may actually anticipate the structures that
will be generated in a speciﬁc language. Natural language analysis systems at least
largely agree on the nature of their inputs, characterized via the sequential structure
and tokenization of written natural language texts. This kind of agreement makes it
possible to adopt architectures like  , where the results of processing can be expressed as annotations on
text spans. With , there is no such agreed basis.
One way to be sure of having realistic and relatively unbiased input is to take input
that is provided by another system developed for purposes unrelated to . This was
the motivation in for using the output of a hypothesized planning system
as the input to language generation; on a larger scale, the same motivation caused
Acker, Lester and Porter to adopt an
independently developed biology knowledge base as their source of content for .
A signiﬁcant problem, however, is the scarcity of independently constructed underlying
information sources that are rich enough to motivate the kinds of issues that researchers
in generation consider. There are many domains where large numerical data sets are
available, as in weather forecasting and stock market data, and these can very usefully
be adopted as sources for the generation of summaries or reports. However, experience
shows that an application that wants to use as a means of producing output often
needs to be built with this in mind if the underlying data source is to contain the
appropriate kinds of information ; presented only with tables of numbers as a source of data for generation,
inevitably the system has to be provided with other substantial sources of knowledge
to be able to utilize this data.
4.2. What should the output be?
The second major problem with evaluating is that of assessing the output. There
is no agreed objective criterion for comparing the “goodness” of texts. Other possible
notions like “appropriateness” and “correctness” depend on the task performed and
what constitutes success. Once again, a problem is that systems span a wide range
of domains and application types, and so a basis for comparison is usually lacking.
There is no “right answer” for an system, because almost always there are many
texts that will achieve the same goals, more or less. Evaluation therefore has to consider
the quality of the output somehow. Accuracy and ﬂuency are two measures of quality.
As we have seen, they are distinct but often related. Some of the work surveyed has
not made this distinction clearly, and this reﬂects a general uncertainty about what the
key dimensions are.
Task evaluation attempts to avoid directly asking the question of what the 
output (and, to some extent, also input) should be. But then its results only reﬂect
indirectly the properties of the system.
4.3. What should be measured?
It is not always obvious what, about the performance of an system, one needs to
measure, or what measurable attributes best indicate this. For instance, Carter considered
measuring “mean time per node” as an indication of how easy it was to navigate
through a hypertext system (low mean time indicating easy navigability). However, it
turned out that certain subjects, when lost, resorted to a strategy of rapid random
clicking and this, initially unexpected, phenomenon rendered the measure useless.
If possible, it seems best to choose measurements which degrade gracefully. Yeh and
Mellish measured the extent to which generated referring expressions agreed
with those selected by humans. However, as we discuss below, the humans could not
always agree. It could be that in some circumstances two diﬀerent referring expressions
really are equally good. If the subjects had been asked to rank their selections (rather
than just indicate the best), a more subtle measure of agreement (robust with respect
to certain random variations) between system and humans might have arisen.
Human subjects are a valuable resource and carrying out experiments with humans
is relatively time-consuming. One does not want to have to repeat experiments. If in
doubt, it is probably worth measuring everything that could possibly be relevant when
the subjects are there.
It is also worth bearing in mind from the start the kinds of statistical tests that one
is going to use and what they can and cannot show. For instance, standard statistical
tests can only reject an identity hypothesis, never accept one .
4.4. What controls should be used?
For many tasks we have no idea either how well it is possible to do or how easily
the task can be done, which means that absolute performance ﬁgures are not very
Evaluation in the context of natural language generation
useful. For instance, in the trials “only” 42% of nodes visited were irrelevant. Is
this good or bad? The only way to tell is to compare it to something else.
Figures obtained for humans performing the same task may give a good upper bound
for an system’s performance. Creating random or degraded texts (or hypertexts,
or links) may give a basis for establishing a lower bound [this was done by Acker &
Porter and by Carter ]. In between, it may be possible to get ﬁgures for
diﬀerent versions of the system [as was done by Yeh & Mellish ].
Sometimes upper and lower bounds can be determined by analysis or simple experimentation. Knight and Chander pointed out that an article selection algorithm
can achieve 67% correctness simply by guessing “the” every time. On the other hand,
human experts cannot achieve much better than 95% (given purely the text without
the articles). This puts Knight and Chander’s achievement of 78% into perspective.
4.5. How do we get adequate training and test data?
As Robin points out, such is the complexity of that many people have
focused on particular subtasks which have been evaluated using handcoded (and hence
few in number) inputs. Few projects have had the beneﬁt of a signiﬁcant knowledge
base of the kind used by Acker, Lester and Porter from which to generate and from
which new inputs (uninﬂuenced by the developers of the system) can be obtained
in large numbers. Thus we have rarely had the luxury of having separate training and
test sets of the kind that are traditionally required for evaluating systems.
Nevertheless, even if data is not available in large amounts, methodologically it is
necessary to separate that which is used to develop the system from that which is used
to evaluate the result. The latter needs to be put aside right from the very start, so that
it plays no role in system development. Yeh and Mellish’s approach of using a single
set of data to motivate a sequence of increasingly “better” algorithms could have
resulted in a system that was overtuned to this data set. A separate evaluation involving
comparison to humans on diﬀerent data was essential to discount this possibility.
4.6. How do we handle disagreement among human judges?
In evaluating the output of , the problem that humans will not always agree about
subjective qualities like “good style”, “coherence” and “readability” must be faced.
Some of the evaluation methods discussed here avoid consulting explicit human
judgements for this reason. Others hope to avoid problems by having suﬃcient
independent judgements that disagreements will become invisible as a result of the
averaging process.
Although they consulted 12 diﬀerent native speakers of Chinese, Yeh and Mellish
 found signiﬁcant disagreement about what the preferred referring expressions
should be. Faced with results of this kind, a natural strategy is to investigate whether
the overall disagreement is being caused by, for instance, one speaker having very
unusual intuitions or particular examples being unusually challenging. Unfortunately
even these measures did not produce really signiﬁcant agreement in the evaluations in
this case.
As we discussed above, a well-designed experiment will attempt to plan for how to
react to disagreement and will attempt to measure things in a way that ﬁnds a basis
for agreement amongst the human subjects, if only on a slightly diﬀerent version of
C. Mellish and R. Dale
the problem. Regardless of how well this succeeds, it is essential to measure the level
of agreement between the speakers, and the kappa statistic is very useful for this.
4.7. Summary
We have surveyed the various approaches to evaluation that have been attempted in
work on and related areas in recent years, and we have identiﬁed some of the
problems that arise in attempting some evaluations. In light of these problems, we
suggest that while the use of some notion of evaluation in is intuitively appealing,
we should be wary of following too closely models of evaluation that have become
common and popular in work in . It should be borne in mind that even in 
not all work is amenable to easy evaluation: -style evaluation , where a well-deﬁned target data structure can be determined for each
input text, is only applicable in very speciﬁc circumstances. Other tasks which look
at ﬁrst sight easily evaluable are not necessarily so: for example, although it might
seem that the performance of a word sense disambiguation system is easily quantiﬁable,
the results of such experiments are of dubious value unless we have independent
motivations for the sets of possible senses that can be chosen—the senses of a word
listed in a dictionary may be quite irrelevant for many real tasks where some notion
of sense disambiguation is required.
It is hardly surprising, then, that it is unclear how we might evaluate in the context
of natural language generation. Because of some of the asymmetries mentioned above,
it is not clear how one could ﬁnd the analogues of the paradigm evaluation
scenarios so often discussed in research; intuitively, there is something about
judging of the quality of a generated text that makes it a diﬀerent task from that of
determining how many database ﬁelds have been ﬁlled correctly. This basic diﬀerence
may require a somewhat diﬀerent approach to the evaluation task, and we should not
be too hasty to assume that we can ﬁnd quantitative measures similar to those that
work in certain narrowly-deﬁned tasks.
Assessing overall performance in terms of component tasks
At the end of the day, leaving aside task evaluation, any evaluation that is carried out
is focused on the output of the system: the text that is generated. At the same
time, it was suggested at the outset that a ﬁner-grained decomposition of the problems
involved in is widely accepted as useful, and in Section 2 six aspects of the overall
task of generating natural language were identiﬁed. Now that the attempts at evaluation
in found in the literature have been surveyed, this section suggests a somewhat
hybrid approach to evaluation of systems, where we consider the overall output
of the system but attempt to assess this in terms of the contributions made by speciﬁc
component tasks. This is motivated by our belief that it is only through exploring the
nature of the component tasks of that an appreciation of what it makes sense to
measure about systems as a whole can be gained. Focusing on evaluation as
applied to these components is also likely to be more productive than thinking about
as an atomic task, because these better reﬂect the immediate focus of research
activity in the ﬁeld. It could be, of course, that considering evaluation from this
perspective will lead to a reassessment of this particular way of decomposing the task
Evaluation in the context of natural language generation
The weather summary for February 1995
The month was cooler and drier than average, with the average number
of rain days. The total rain for the year so far is well below average.
There was rain on every day for eight days from the 11th to the 18th,
with mist and fog patches on the 16th and 17th. Rainfall amounts were
mostly small, with light winds.
of : as a result sub-problems in that are not encompassed by this decomposition
may be identiﬁed, or it may be decided that some of the distinctions made here are
inappropriate.
To enable exploration of this idea, in the remainder of this section we look at how
aspects of the quality of a generated text by reference to the component tasks identiﬁed
at the outset of the paper might be evaluated. We do this by examining a particular
instance of natural language generation: we explore what might be involved in the
generation of weather summaries from automatically collected meteorological data.
This scenario has the advantage we alluded to earlier that the data source used exists
for reasons quite independent of and thus helps us avoid the kinds of problems
that bespoke representations can lead to. Our scenario is not necessarily representative
of all tasks, but, we would suggest, it is characteristic enough to allow the
exploration of what can go wrong in the diﬀerent components of an system and
hence indicate what an evaluation should highlight.
5.1. The scenario
We have a collection of meteorological data that includes information about temperature
and precipitation over an extended period of time, and we want to generate monthly
summaries of the weather. This is, in fact, a generation scenario being explored by one
of the current authors; at Macquarie University, the StaﬀNewsletter each month carries
a human-authored report on the weather conditions over the preceding month. Table
III shows an example human-authored weather summary. At the same time, there are
automatic weather stations on campus that collect around two dozen meteorological
parameters—ground temperature, air temperature, wind speed, wind direction and so
on—sampled at 15 min intervals. Since the human-authored weather summaries do not
discuss weather phenomena at a granularity below that of single days, we will assume
the corresponding raw data gathered by the automatic weather stations is preprocessed
and aggregated to produce what we will call ; an example is
shown in Figure 2.
In the remainder of this discussion we look at how various aspects of the task
might play a role in generating something like the human-authored text using these
daily weather records as input.
Our concern here is with how performance of an system might be evaluated by
looking at how speciﬁc tasks within the overall process can contribute to the overall
goodness or badness of the generated text. This means that there are a number of
potential aspects of evaluation that are ignored by the authors. We list these here in
order to clarify our main concerns:
C. Mellish and R. Dale
type: dailyweatherrecord
year: 1995
unit: degrees-centigrade
number: 10.1
unit: degrees-centigrade
number: 21.3
temperature:
unit: millimetres
number: 12
Figure 2. A daily weather record.
Black box evaluation is too coarse: Given the scenario we have sketched it might
appear that we have a situation that would be very amenable to some kind of block
box evaluation. For example, since there is an archive of previously written humanauthored weather summaries, and a corresponding source of underlying meteorological data, we could simply aim to build a system that would replicate the
human-authored texts in a selected training set, and then see how the system
performed on a test set. We might evaluate the results by developing some metric to
compare the machine generated texts with the human authored texts. The point,
however, is that any such metric would be too coarse to give much in the way of
useful information unless it is considered how the component tasks in the generation
process contribute to the generated texts.
Applications potential is not relevant: We are not concerned here with the question
of whether really is the most appropriate solution to this problem: for example,
since these summaries are generated only once a month, the construction cost of an
system is likely to be far greater than the value it returns.
Unavailable data is not an issue: As Reiter and Dale point out, it is important
to establish before building an system whether it is even possible to generate
the required texts given the available data. For this scenario, we are assuming that
the requisite input data is indeed available. It turns out that some of the weather
summaries written by the human author refer to phenomena for which there is no
data available that a program could access; for example, the author in question often
mentions whether a particular creek on the University campus is dry. This is
information available to the author (perhaps because he walks past the creek each
day) that is not available to an system, and so it would be quite inappropriate
to evaluate the system on its failure to report on such phenomena.
With these issues dispensed with, we can move on to consider how we might assess the
performance of the system at a more ﬁne-grained level.
Evaluation in the context of natural language generation
5.2. Content determination
As noted earlier, content determination is concerned with the selection of the information
to be conveyed in a text. There are a number of questions that we might ask of the
content determination capabilities of a given system:
• Does it state only information which is true?
• Does it say enough?
• Does it say too much?
• Does it provide information which is inappropriate to the context?
An system might express incorrect or false information in a number of ways:
(1) There might be errors in the underlying information source. In the case of our
weather summary scenario, some problem with the automatic weather station
might result in incorrect temperature values being recorded. We would clearly
not want to say that this was a problem with the system, and so some
independent validation of the input data is required.
(2) The system might perform some analysis of the underlying information source
in order to determine higher order facts, and it might do this wrongly. It seems
reasonable to lay the blame here at the door of the content determination process.
One instance of a problem here would be where the data analysis algorithms are
incorrect. If the content determination component is tasked with analysing the
source data and identifying trends in the data, such as periods over which
temperatures have steadily increased, and it fails to identify trends that a human
expert would identify, or identiﬁed trends that are not present in the data, then
this would be a failure in content determination. The problem here is the same
as that which arises in an expert system which reaches a conclusion on the basis
of rules which do not correspond to expert knowledge.
(3) Similarly, suppose we have a system which is intended to generate diﬀerent texts
for diﬀerent user groups, perhaps varying the content expressed depending upon
the assumed level of experience or knowledge of the audience. If the system’s
model of the audience is incorrect, or if the theory that determines what content
is appropriate in speciﬁc circumstances is ﬂawed, then the system may select
content that is not appropriate, either saying too much, too little, or the wrong
As these last two examples in particular demonstrate, content selection is closest in
nature to a standard expert system task: it requires the development of code that models
“what an expert would say” in the given circumstances. This characterization of the
task of content determination means that it is not particularly linguistic in nature, and
this may lead some to consider it not to be part of the task of . Similar concerns
might be raised, of course, of any part of an understanding process that involves
reasoning and inference using world knowledge. In short, if an system says the
wrong things, then to put it simply, its theory of what to say may be incorrect.
5.3. Document structuring
Document structuring is concerned with the overall coherence of the generated text.
In current systems, there are basically two approaches that are used to address
C. Mellish and R. Dale
document or text structuring. One is to use  , which can be
thought of as scripts or text grammars that stipulate the overall structure of a text. A
schema-based approach uses a set of rules that dictate the order and content of a text;
as with a sentence grammar, arbitrarily complex structures can be produced given an
appropriate set of generating rules. The other popular approach is to use operationalizations of , these being the basic theoretical constructs in
theories such as Rhetorical Structure Theory which dictate how the component parts
of a text can be combined. It has often been noted that schemas can be viewed as
compilations of conventionalized combinations of rhetorical relations.
In each case the aim is to capture something of the coherence relations that reside
in real texts so as to avoid the generation of jumbled texts. The quality of what results
clearly depends on how well the underlying model of text structure mirrors reality. In
the case of schemas this is quite straightforward: since in general these approaches are
quite limited, with a typical text grammar only containing a relatively small number
of rules, the outcomes are quite predictable, and it is unlikely that incoherent texts would
be generated if the domain is appropriately narrow. Any such problems encountered are
easily ﬁxed, in the same way that a simple sentence grammar which over-generates can
be repaired.
In a schema-based system, a bad model of text structure might show itself in our
current scenario via inappropriate orderings of information. For example, the most
common pattern adopted in the human-authored texts is to ﬁrst talk about the overall
temperature and rainfall, and then to go on to mention speciﬁc details about rainy
spells or spells of extreme temperature if any are there to be reported. However, if a
month is particularly unusual in some regard—for example, if it is the hottest September
on record—then the human-authored text tends to stray from this pattern, instead
commencing by noting this signiﬁcant fact. If such exceptions are not catered for by
the part of the system that is responsible for structuring the text, then we might
characterize this as a case of bad .
In the case of approaches based on rhetorical relations, there is generally a considerable
increase in the variety of textual structures that can be generated; this brings with it a
lessening of the predictability of outcomes and an increase in the chances of incoherent
texts being generated. Work by Hovy in this area has suggested that the
incorporation of other aspects of coherence, using notions such as focus or theme
development, may be required in order to ensure that the generated texts remain
coherent; otherwise the possible textual structures that can be imposed over a given set
of informational elements may be quite underconstrained.
In both cases, the resulting coherence of the texts generated depends on the extent
to which the model of text structure used—either embodied in the patterns over elements
permitted by the schemas, or the combinations of elements permitted by the rhetorical
deﬁnitions—reﬂects the true nature of coherence in text.
5.4. Lexicalization
Lexicalization is concerned with choosing the right word or phrase. This can go wrong
in a number of ways. The most obvious and trivial case is where words are incorrectly
associated with underlying concepts: a system suﬀering such a fault might, for example,
state that the temperature today is warmer than it was yesterday when in fact the
temperature has decreased.
Evaluation in the context of natural language generation
More sophisticated problems in lexicalization arise where a small set of canonical
concepts are used, but these have to be mapped into a larger set of words and phrases
with the choices being determined by collocational factors. For example, the underlying
conceptual base might express a notion of increase or decrease over both temperature
and rainfall; however, although we can use terms like greater and higher to express an
increase in either case, the terms warmer and wetter are speciﬁc to one or the other.
An incorrect representation of the appropriate collocational data will lead to problems
here. We might reasonably see this as a problem with the theory of lexicalization
embodied in the system. It should be noted that such problems can only arise relative
to a chosen set of underlying concepts, and so any attempt to evaluate here must take
account of this dependence in some way. If the appropriate lexical distinctions are
already made in the underlying concept set then there is no real possibility of error at
the level of lexicalization; however, if an inappropriate conceptual structure is built
from some more basic data, then we might see this as a problem with the system’s
content determination. Once more we see that the distinction between these two aspects
of is not clear cut, and clearly depends very much on the representational
assumptions embodied within the system.
5.5. Aggregation
The lack of appropriate aggregation strategies is perhaps the most obvious source of
disﬂuent texts in simple generation systems. Although many early systems assumed
that the input elements of information to be expressed corresponded one-to-one to
sentences, this is not in general true; any such assumption easily leads to choppy texts
consisting of sequences of short sentences. So, for example, we want to generate
something like the second of the two following texts rather than the ﬁrst:
(1) The month was cooler than average.
The month was drier than average.
There were the average number of rain days.
(2) The month was cooler and drier than average, with the average number of rain
This area has received more attention in recent years as researchers move away from
hand-crafted knowledge bases to those developed from some other source where the
granularity may not be sentence-sized. There is real scope for evaluation here, in the
sense that diﬀerent aggregation strategies may be adopted: the same collections of
informational elements can be put together into sentences in diﬀerent ways. At the
same time, there are relationships between aggregation and lexicalization and referring
expression generation that confuse the issues: lexicalization (for example, in the choice
of which element to express as the head verb of a sentence, and which verb to use)
dictates the sentential complementation structure and thus may restrict aggregation
possibilities; and the construction of referring expressions itself permits aggregation by
allowing sites where information may be folded in (for example, as a post-modifying
relative clause). For these reasons, aggregation, lexicalization and referring expression
generation are sometimes seen as the domain of a distinct task;
the nature of the interactions between the three phenomena are very much ill-understood,
C. Mellish and R. Dale
and so it may be somewhat premature to attempt to evaluate approaches to any one
of the three in isolation.
5.6. Referring expression generation
We have already noted in our survey of existing approaches to evaluation a number
of cases where diﬀerent aspects of referring expression generation algorithms have
been evaluated. Some aspects of this problem lie properly in the domain of content
determination (and so here we see another hint that the elements in this catalog of 
phenomena are not necessarily distinct is seen): in particular, when an entity is introduced
into a discourse, the properties used in this initial reference need to be selected in line
with similar considerations to those that apply in the selection of content for a text as
The scope for evaluation in the generation of subsequent anaphoric references is
perhaps easier to identify. In particular, if a system is able to choose to use pronouns
to refer to entities, there is a tension between producing unambiguous text and producing
text that does not contain redundant information. Many of the problems that plague
approaches to pronoun resolution have analogues here, although evaluation is again
more diﬃcult in the case of : in evaluating a pronoun resolution algorithm, we can
always compare against a marked-up version of a text that contains co-reference
annotations, but pronoun generation can only be evaluated by comparison with how
people might use pronouns in similar circumstances, or by asking subjects if the
machine-generated usages are ambiguous: in each case, there is considerable scope for
inter-subject variability.
We have talked here simply of pronouns as the paradigm instances of anaphoric
referring expressions, but all the same issues arise for any forms of anaphoric reference,
be they proper names, reduced deﬁnite noun phrases or other forms. In the -
system , the appropriateness of anaphoric referring expressions generated by the system was tested at run-time by having the component
of the system determine whether it could disambiguate the references successfully: this
self-evaluation is of course limited by the characteristics of the component.
5.7. Surface realization
Surface realization is concerned with taking some speciﬁcation of the content of a
sentence and realizing this as a morphologically and grammatically correct sentence.
This is the one area of where there are a number of oﬀ-the-shelf components
available for the researcher to use, and so one might expect that some evaluation of
these components against each other might be possible. But this is not so, for a number
of reasons.
The most important of these, at least from a theoretical perspective, is that no two
systems share a common view as to what surface realization really involves; as a result,
each system tends to adopt a diﬀerent model of what the appropriate input to surface
realization should be. Although terms like sentence speciﬁcation, realization speciﬁcation
and sentence plan are in quite common use, they are typically taken to mean diﬀerent
things by diﬀerent people. Thus, some realization systems expect a very high level
semantic speciﬁcation as input, with the realization component performing quite
sophisticated reasoning as to how this content should be expressed; other systems
Evaluation in the context of natural language generation
expect input speciﬁed in terms of a broad grammatical structure that already dictates
what the major constituents of the sentence should be; and, in the extreme case, some
systems do no more than linearize and add appropriate morphological inﬂections to
what is to all intents and purposes a completely speciﬁed syntactic structure.
For any researcher evaluating an existing realization component, a key question is
the grammatical coverage of that system: does it generate all the required surface forms?
Again, comparison of systems in this regard is made diﬃcult by the diﬀerent expectations
of what the input should be like.
There is another possibility of failure, of course: a surface realization component
might actually generate ungrammatical sentences.
6. Conclusion
In this paper, we have tried to give a picture of what is involved in , looked at
some previous attempts to perform evaluation in the context of the systems and
techniques, and suggested that evaluation in faces diﬃculties not present in the
popular paradigms of evaluation found in some area of . There are, as yet, no clear
answers as to how systems should be evaluated; however, as a starting point we
have suggested that a breakdown into the phenomena that needs to consider
provides one way of clarifying the questions that need to be asked. We have provided
above some starting points in this regard, but as we have noted, even the catalogue of
phenomena is not without controversy.
In the interim, we will no doubt see increasing attempts to evaluate work in ;
these eﬀorts are to be praised, and will surely lead to insights that will help to shape
the debate. As we have seen, evaluating an system demands resources such as time
and human subjects, as well as good sources of data. The requirements for evaluation
need to be taken into account right from the start of a project, and arguably even in
the stages when a research project is being chosen. If one is about to embark on a
project that seeks to bring advances in the techniques of and yet no clear path to
an evaluation of the results can be seen, one may want to ask whether one should be
doing the project at all. A closer questioning and scrutiny at these stages will help to
ﬁrm up the questions that need to be asked from inside the task, so that we may
move closer to a greater understanding and a broader consensus as to what evaluation
in the context of really means.
The authors would like to thank the members of their respective research groups for
discussions which have helped develop the ideas and intuitions expressed in this paper.
Particular thanks are due to Sarah Boyd, Ce´cile Paris and Ehud Reiter for motivating
some of the questions raised in this paper.