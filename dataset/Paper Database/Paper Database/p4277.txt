Deep Learning for Time-Series Analysis
John Gamboa
University of Kaiserslautern
Kaiserslautern, Germany
Abstract. In many real-world application, e.g., speech recognition or
sleep stage classiﬁcation, data are captured over the course of time,
constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to
diﬀerent classes or predict diﬀerent behavior. This characteristic generally increases the diﬃculty of analysing them. Existing techniques often
depended on hand-crafted features that were expensive to create and required expert knowledge of the ﬁeld. With the advent of Deep Learning
new models of unsupervised learning of features for Time-series analysis
and forecast have been developed. Such new developments are the topic
of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried.
The results make it clear that Deep Learning has a lot to contribute to
Keywords: Artiﬁcial Neural Networks, Deep Learning, Time-Series
Introduction
Artiﬁcial Neural Networks (ANN), since their origin in 1943 , have been
used to solve a large range of problems as diverse as robotic processing ,
object recognition , speech and handwriting recognition , and even real
time sign-language translation . Despite the intuition that deeper architectures
would yield better results than the then more commonly used shallow ones,
empirical tests with deep networks had found similar or even worse results when
compared to networks with only one or two layers (for more details, see
 ). Additionally, training was found to be diﬃcult and often ineﬃcient .
L¨angkvist argues that this scenario started to change with the proposal of
greedy layer-wise unsupervised learning , which allowed for the fast learning
of Deep Belief Networks, while also solving the vanishing gradients problem .
Latest deep architectures use several modules that are trained separately and
stacked together so that the output of the ﬁrst one is the input of the next one.
From stock market prices to the spread of an epidemic, and from the recording of an audio signal to sleep monitoring, it is common for real world data to
be registered taking into account some notion of time. When collected together,
the measurements compose what is known as a Time-Series. For diﬀerent ﬁelds,
suitable applications vary depending on the nature and purpose of the data:
 
Deep Learning for Time-Series Analysis
while doctors can be interested in searching for anomalies in the sleep patterns
of a patient, economists may be more interested in forecasting the next prices
some stocks of interest will assume. These kinds of problems are addressed in the
literature by a range of diﬀerent approches (for a recent review of the main techniques applied to perform tasks such as Classiﬁcation, Segmentation, Anomaly
Detection and Prediction, see ).
This paper reviews some of the recently presented approaches to performing
tasks related to Time-Series using Deep Learning architectures. It is important,
therefore, to have a formal deﬁnition of Time-Series. Malhotra et al. deﬁned
Time-Series as a vector X = {x(1), x(2), . . . , x(n)}, where each element x(t) ∈Rm
pertaining to X is an array of m values such that {x(t)
2 , . . . , x(t)
m }. Each one
of the m values correspond to the input variables measured in the time-series.
The rest of this paper is structured as follows: Section 1.1 introduces basic
types of Neural Network (NN) modules that are often used to build deep neural
structures. Section 2 describes how the present paper relates to other works in
the literature. Sections 3, 4 and 5 describe some approaches using Deep Learning
to perform Modeling, Classiﬁcation and Anomaly Detection in Time-Series data,
respectively. Finally, Section 6 concludes the paper.
Artiﬁcial Neural Network
This section explains the basic concepts related to ANN. The types of networks
described here are by no means the only kinds of ANN architectures found in the
literature. The reader is referred to for a thorough description of architectural
alternatives such as Restricted Boltzmann Machines (RBM), Hopﬁeld Networks
and Auto-Encoders, as well as for a detailed explanation of the Backpropagation
algorithm. Additionally, we refer the reader to for applications of RNN as
well as more details on the implementation of a LSTM, and to for details
An ANN is basically a network of computing units linked by directed connections. Each computing unit performs some calculation and outputs a value that
is then spread through all its outgoing connections as input into other units.
Connections normally have weights that correspond to how strong two units are
linked. Typically, the computation performed by a unit is separated into two
stages: the aggregation and the activation functions. Applying the aggregation
function commonly corresponds to calculating the sum of the inputs received by
the unit through all its incoming connections. The resulting value is then fed
into the activation function. It commonly varies in diﬀerent network architectures, although popular choices are the logistic sigmoid (σ(x) =
1+e−x ) and the
hyperbolic tangent (tanh(x) =
1+e−2x −1) functions. Recently, rectiﬁed linear
units employing a ramp function (R(x) = max(0, x)) have become increasingly
The input of the network is given in a set of input computing units which
compose an input layer. Conversely, the output of the network are the values
output by the units composing the output layer. All other units are called hidden
and are often also organized in layers (see Figure 1b for an example network).
Deep Learning for Time-Series Analysis
... hidden ...
Input Gate
Output Gate
Forget Gate
Fig. 1: (a) The convolutional layer of a CNN with three groups (also called “ﬁlters”). Each group performs a 2 × 2 convolution in the image: each neuron in
the group is connected to a diﬀerent region of the image but shares the same
weights, producing a new image. In the example, the weight vector w is shared
by all neurons nj1, the vector v is shared by all neurons nj2, and z is shared
by all neurons nj3. If pooling is applied, it is applied to each one of the three
newly produced images; (b) An Artiﬁcial Neural Network with one input layer
composed of three neurons, two hidden layers composed, each one, of four neurons, and one output layer composed of three neurons. Each node of a layer is
connected to all nodes of the next layer; (c) A LSTM block (adapted from ).
Input layer
LSTM/Sigmoid
Output layer
LSTM/Sigmoid
Fig. 2: (a) The proposed “Stacked Architecture” for performing Anomaly Detection (adapted from ); (b) The architecture of a UFCNN (adapted from
Deep Learning for Time-Series Analysis
The focus of learning algorithms is frequently on deciding what weights would
cause the network to output, given some input, the expected values. A popular
learning algorithm is the Backpropagation algorithm , whereby the gradient
of an error function is calculated and the weights are iteratively set so as to
minimize the error.
Convolutional Neural Network (CNN) A network that is too big and with
layers that are fully connected can become infeasible to train. Also trained with
the Backpropagation algorithm, CNNs are common for image processing
tasks and reduce the number of parameters to be learned by limiting the number
of connections of the neurons in the hidden layer to only some of the input
neurons (i.e., a local area of the input image). A hidden layer (in this case, also
called a convolutional layer – see Figure 1a) is composed by several groups of
neurons. The weights of all neurons in a group are shared. Each group is generally
composed by as many neurons as needed to cover the entire image. This way, it
is as if each group of neurons in the hidden layer calculated a convolution of the
image with their weights, resulting in a “processed” version of the image. We
call this convolution a feature.
Commonly, pooling is applied to the resulting ﬁltered images. The tecnique
allows for achieving some translation invariance of the learned features. The
groups (containing a newly processed version of the input image) are divided
in chunks (e.g., 2 × 2) and their maximum value is taken. This results in yet
another version of the input image, now smaller than the original size (in the
example, 1/4 of the size of the group).
These steps can be repeatedly applied as many times as desired: a new convolutional layer can be applied on the pooled layer, followed by another pooling
layer, and so forth. Finally, when the layers become small enough, it is common
to have fully connected layers before the output layer.
Tiled Convolutional Neural Network The usage of shared weights in a group
allow for the translation invariance of the learned features. However, “it prevents
the pooling units from capturing more complex invariances, such as scale and
rotation invariance” . To solve this problem, Tiled CNNs allow for a group to
be divided into subgroups called tiles, each of which can have separate weights.
A parameter k deﬁnes how many tiles each group has: neurons that are exactly
k steps away from each other share the same weights.
Fully Convolutional Networks (FCN) While the pooling operation performed by
CNNs makes sense for object recognition tasks, because it has the advantage
of achieving some robustness to small shifts of the learned features, it is not
suited for tasks like Semantic Segmentation, where the goal is to segment the
pixels of the image according to the objects that they refer to. FCNs allow
for the input and output layers to have the same dimensions by introducing “a
decoder stage that is consisted of upsampling, convolution, and rectiﬁed linear
units layers, to the CNN architecture” .
Deep Learning for Time-Series Analysis
Recurrent Neural Network (RNN) When the network has loops, it is called
a RNN. It is possible to adapt the Backpropagation algorithm to train a recurrent
network, by “unfolding” the network through time and constraining some of the
connections to always hold the same weights .
Long Short-Term Memory (LSTM) One problem that arises from the unfolding
of an RNN is that the gradient of some of the weights starts to become too small
or too large if the network is unfolded for too many time steps. This is called
the vanishing gradients problem . A type of network architecture that solves
this problem is the LSTM . In a typical implementation, the hidden layer is
replaced by a complex block (see Figure 1c) of computing units composed by
gates that trap the error in the block, forming a so-called “error carrousel”.
Literature Review
Independently of Deep Learning, analysis of Time-Series data have been a popular subject of interest in other ﬁelds such as Economics, Engineering and
Medicine. Traditional techniques on manipulating such data can be found in
 , and the application of traditional ANN techniques on this kind of data is
described in .
Most work using ANN to manipulate Time-Series data focuses on modeling
and forecasting. As an early attempt on using ANN for such tasks, modelled
ﬂour prices over the range of 8 years. Still in the 90’s, delineated eight steps
on “designing a neural network forecast model using economic time series data”.
More recent approaches include usage of Elman RNNs to predict chaotic Time-
Series , employing ANN ensemble methods for forecasting Internet traﬃc
 , using simple Multilayer Perceptrons for modeling the amount of littering
in the North Sea , and implementing in FPGA a prediction algorithm using
Echo State Networks for “exploiting the inherent parallelism of these systems”
Hybrid approaches to Time-Series analysis utilizing ANN are not uncommon. presents a model for Time-Series forecasting using ANN and ARIMA
models, and applies the same kinds of models to water quality time series
prediction. In still other examples of the same ideas, compares the performance of ARIMA models and ANNs to make short-term predictions on photovoltaic power generators, while compares both models with the performance
of Multivariate Adaptive Regression Splines. performs Time-Series forecasting by using a hybrid fuzzy model: while the Fuzzy C-means method is utilized
for fuzziﬁcation, ANN are employed for defuzziﬁcation. Finally, forecasts
the speed of the wind using a hybrid of Support Vector Machines, Ensemble
Empirical Mode Decomposition and Partial Autocorrelation Function.
Despite being relatively new, the ﬁeld of Deep Learning has attracted a lot
of interest in the past few years. A very thorough review of the entire history of
developments that led the ﬁeld to its current state can be found in , while a
higher focus on the novelties from the last decade is given in . We proceed to
a review of the applications of Deep Learning to Time-Series data.
Deep Learning for Time-Series Analysis
Classiﬁcation The task of Classiﬁcation of any type of data has beneﬁted
by the advent of CNNs. Previously existing methods for classiﬁcation generally
relied on the usage of domain speciﬁc features normally crafted manually by
human experts. Finding the best features was the subject of a lot of research
and the performance of the classiﬁer was heavily dependent on their quality. The
advantage of CNNs is that they can learn such features by themselves, reducing
the need for human experts .
An example of the application of such unsupervised feature learning for the
classiﬁcation of audio signals is presented in . In , the features learned by
the CNN are used as input to a Hidden Markov Model, achieving a drop at the
error rate of over 10%. The application of CNNs in these works presuppose the
constraint that the Time-Series is composed of only one channel. An architecture
that solves this constraint is presented in .
In the performance of CNNs is compared with that of LSTM for the
classiﬁcation of Visual and Haptic Data in a robotics setting, and in the
signals produced by wearable sensors are transformed into images so that Deep
CNNs can be used for classiﬁcation.
Relevant to Tiled CNNs was the development of Independent Component
Analysis (ICA) . Several alternative methods for calculating independent
components can be found in the literature (e.g., , or ). Tiled CNNs are
normally trained with a variation of such technique that looses the assumption
that each component is statistically independent and tries to ﬁnd a topographic
order between them: the Topographic ICA .
Forecasting Several diﬀerent Deep Learning approaches can be found in the
literature for performing Forecasting tasks. For example, Deep Belief Networks
are used in the work of along with RBM. also compares the performance
of Deep Belief Networks with that of Stacked Denoising Autoencoders. This last
type of network is also employed by to predict the temperature of an indoor
environment. Another application of Time-Series forecasting can be found in
 , which uses Stacked Autoencoders to predict the ﬂow of traﬃc from a Big
Data dataset.
A popular application to the task of Time-Series prediction is on Weather
Forecasting. In , some preliminary predictions on weather data provided by
The Hong Kong Observatory are made through the usage of Stacked Autoencoders. In a follow up work, the authors use similar ideas to perform predictions
on Big Data . Instead of Autoencoders, uses Deep Belief Networks for
constructing a hybrid model in which the ANN models the joint distribution
between the weather predictors variables.
Anomaly Detection Work applying Deep Learning techniques to Anomaly
Detection detection of Time-Series data is not very abundant in the literature.
It is still diﬃcult to ﬁnd works such as , that uses Stacked Denoising Autoencoders to perform Anomaly Detection of trajectories obtained from low level
tracking algorithms.
Deep Learning for Time-Series Analysis
However, there are many similarities between Anomaly Detection and the
previous two tasks. For example, identifying an anomaly could be transformed
into a Classiﬁcation task, as was done in . Alternatively, detecting an anomaly
could be considered the same as ﬁnding regions in the Time-Series for which the
forecasted values are too diﬀerent from the actual ones.
Deep Learning for Time-Series Modeling
In this section the work presented in is reviewed. As discussed above, FCNs
are a modiﬁcation of the CNN architecture that, as required by some Time-
Series related problems, allows for the input and output signals to have the
same dimensions.
Mittelman argues that the architecture of the FCN resembles the application of a wavelet transform, and that for this reason, it can present strong
variations when the input signal is subject to small translations. To solve this
problem, and inspired by the undecimeated wavelet transform, which is translation invariant, they propose the Undecimated Fully Convolutional Neural Network (UFCNN), also translation invariant.
The only diﬀerence between an FCN and an UFCNN is that the UFCNN
removes both the upsampling and pooling operators from the FCN architecture.
Instead, the “ﬁlters at the lth resolution level are upsampled by a factor of 2l−1
along the time dimension”. See Figure 2b for a graphical representation of the
proposed architecture.
The performance of the UFCNN is tested in three diﬀerent experiments. In
the ﬁrst experiment, “2000 training sequences, 50 validation sequences and 50
testing sequences, each of length 5000 time-steps” are automatically generated by
a probabilistic algorithm. The values of the Time-Series represent the position of
a target object moving inside a bounded square. The performance of the UFCNN
in estimating the position of the object at each time-step is compared to that
of a FCN, a LSTM, and a RNN, and the UFCNN does perform better in most
In a second experiment, the “MUSE” and “NOTTINGHAM” datasets1 area
used. The goal is to predict the values of the Time-Series in the next timestep. In both cases, the UFCNN outperforms the competing networks: a RNN,
a Hessian-Free optimization-RNN , and a LSTM.
Finally, the third experiment uses a trading dataset2, where the goal is,
given only information about the past, to predict the set of actions that would
“maximize the proﬁt”. In a comparison to a RNN, the UFCNN again yielded
the best results.
1 available at boulanni/icml2012
2 available at 
Deep Learning for Time-Series Analysis
Deep Learning for Time-Series Classiﬁcation
Wang and Oates presented an approach for Time-Series Classiﬁcation using
CNN-like networks. In order to beneﬁt from the high accuracy that CNNs have
achieved in the past few years on image classiﬁcation tasks, the authors propose
the idea of transforming a Time-Series into an image.
Two approaches are presented. The ﬁrst one generates a Gramian Angular
Field (GAF), while the second generates a Markov Transition Field (MTF). In
both cases, the generation of the images increases the size of the Time-Series,
making the images potentially prohibitively large. The authors therefore propose
strategies to reduce their size without loosing too much information. Finally, the
two types of images are combined in a two-channel image that is then used to
produce better results than those achieved when using each image separately. In
the next sections, GAF and MTF are described.
In the equations below, we suppose that m = 1. The Time-Series is therefore
composed by only real-valued observations, such that referring to x(i) ∈X is the
same as referring to x(i) ∈X.
Gramian Angular Field
The ﬁrst step on generating a GAF is to rescale the entire Time-Series into
values between [−1, 1]. In the equation 1, max(X) and min(X) represent the
maximum and minimum real-values present in the Time-Series X:
˜x(i) = (x(i) −max(X)) + (x(i) −max(X))
max(X) −min(X)
The next step is to recode the newly created Time-Series ˜X into polar coordinates. The angle is encoded by x(i) and the radius is encoded by the the time
Notice that, because the values x(i) were rescaled, no information is lost by
the usage of arccos(˜x(i)) in 2.
φ = arccos(˜x(i)),
−1 ≤˜x(i) ≤1, ˜x(i) ∈˜X
Finally, the GAF is deﬁned as follows:
cos(φ1 + φ1) · · · cos(φ1 + φn)
cos(φ2 + φ1) · · · cos(φ2 + φn)
cos(φn + φ1) · · · cos(φn + φn)
Here, some information is lost by the fact that φ no more belongs to the
interval [0, π]. When trying to recover the Time-Series from the image, there
may be some errors introduced.
Deep Learning for Time-Series Analysis
Markov Transition Field
The creation of the Markov Transition Field is based on the ideas proposed in
 for the deﬁnition of the so-called Markov Transition Matrix (MTM).
For a Time-Series X, the ﬁrst step is deﬁning Q quantile bins. Each x(i) is
then assigned to the corresponding bin qj. The Markov Transition Matrix is the
matrix W composed by elements wij such that P
j wij = 1 and wij corresponds
to the normalized “frequency with which a point in the quantile qj is followed
by a point in the quantile qi.” This is a Q × Q matrix.
The MTF is the n × n matrix M. Each pixel of M contains a value from W.
The value in the pixel ij is the likelihood (as calculated when constructing W)
of going from the bin in which the pixel i is to the bin in which the pixel j is:
wij|x1∈qi,x1∈qj · · · wij|x1∈qi,xn∈qj
wij|x2∈qi,x1∈qj · · · wij|x2∈qi,xn∈qj
wij|xn∈qi,x1∈qj · · · wij|xn∈qi,xn∈qj
Performing Classiﬁcation with the Generated Images
The authors use Tiled CNNs to perform classiﬁcations using the images. In
the reported experiments, both methods are assessed separately in 12 “hard”
datasets “on which the classiﬁcation error rate is above 0.1 with the state-of-theart SAX-BoP approach” , which are 50Words, Adiac, Beef, Coﬀee, ECG200,
Face (all), Lightning-2, Lightning-7, OliveOil, OSU Leaf, Swedish Leaf and Yoga
 . The authors then suggest the usage of both methods as “colors” of the
images. The performance of the resulting classiﬁer is competitive against many
of the state-of-the-art classiﬁers, which are also reported by the authors.
Deep Learning for Time-Series Anomaly Detection
Anomaly Detection can be easily transformed into a task where the goal is to
model the Time-Series and, given this model, ﬁnd regions where the predicted
values are too diﬀerent from the actual ones (or, in other words, where the
probability of the observed region is too low). This is the idea implemented by
the paper reviewed in this section .
Through the learned model, not all m input variables need to be predicted.
The learned model predicts, at any given time-step, l vectors with d input variables, where 1 ≤d ≤m.
The modeling of the Time-Series is done through the application of a Stacked
LSTM architecture. The network has m input neurons (one for each input variable) and d × l output neurons (one neuron for each one of the d predicted
variables of the l vectors that are predicted at a time-step). The hidden layers
are composed by LSTM units that are “fully connected through recurrent connections”. Additionally, any hidden unit is fully connected to all units in the
hidden layer above it. Figure 2a sketches the proposed architecture.
Deep Learning for Time-Series Analysis
For each one of the predictions and each one of the d predicted variables,
a prediction error is calculated. The errors are then used to ﬁt a multivariate
Guassian distribution, and a probability p(t) is assigned to each observation. Any
observation whose probability p(t) < τ is treated as an anomaly.
The approach was tested in four real-world datasets. One of them, called
Multi-sensor engine data is not publicly available. The other three datasets
(Electrocardiograms (ECGs), Space Suttle Marotta valve time series, and Power
demand dataset) are available for download3. The results demonstrated a signiﬁcant improvement in capturing long-term dependencies when compared to
simpler RNN-based implementations.
Conclusion
When applying Deep Learning, one seeks to stack several independent neural
network layers that, working together, produce better results than the already
existing shallow structures. In this paper, we have reviewed some of these modules, as well the recent work that has been done by using them, found in the
literature. Additionally, we have discussed some of the main tasks normally performed when manipulating Time-Series data using deep neural network structures.
Finally, a more speciﬁc focus was given on one work performing each one
of these tasks. Employing Deep Learning to Time-Series analysis has yielded
results in these cases that are better than the previously existing techniques,
which is an evidence that this is a promising ﬁeld for improvement.
Acknowledgments. I would like to thank Ahmed Sheraz and Mohsin Munir
for their guidance and contribution to this paper.