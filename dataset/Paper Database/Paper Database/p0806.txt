High Detection-rate Cascades for Real-Time Object Detection
Hamed Masnadi-Shirazi, Nuno Vasconcelos
Department of Electrical and Computer Engineering,University of California San Diego
San Diego, CA 92093
 , 
A new strategy is proposed for the design of cascaded
object detectors of high detection-rate.
The problem of
jointly minimizing the false-positive rate and classiﬁcation
complexity of a cascade, given a constraint on its detection rate, is considered. It is shown that it reduces to the
problem of minimizing false-positive rate given detectionrate and is, therefore, an instance of the classic problem of
cost-sensitive learning. A cost-sensitive extension of boosting, denoted by asymmetric boosting, is introduced.
maintains a high detection-rate across the boosting iterations, and allows the design of cascaded detectors of high
overall detection-rate. Experimental evaluation shows that,
when compared to previous cascade design algorithms, the
cascades produced by asymmetric boosting achieve significantly higher detection-rates, at the cost of a marginal increase in computation.
1. Introduction
Signiﬁcant attention has been devoted to the problem
of real-time object detection in recent years. The seminal
contribution was the cascaded object detector of Viola and
Jones (VJ) . It achieves real time detection with performance comparable to that of previous state of the art methods. Although its success is due to a combination of innovative ideas, such as using a large set of very simple features,
efﬁcient computation through integral images, and the recourse to weak classiﬁers and boosting, one of the greatest assets of the VJ detector is the adoption of a cascaded
classiﬁer architecture . In particular, the detector is implemented as a sequence of classiﬁers of high detection and
low false positive rate. Because most image locations can
easily be identiﬁed as not containing the target object, this
allows the rejection of non-targets with a very small number of feature evaluations, enabling extreme computational
efﬁciency. In the original VJ algorithm, a standard implementation of AdaBoost is used to identify the set of discriminant features which, along with a threshold, make up
each stage of the cascaded detector. The overall goal is to
meet pre-speciﬁed detection and false-positive rates for the
whole cascade, while maximizing detection speed.
While cascades designed with the procedure of are
extremely fast, the lack of an optimal method for cascade
design is problematic in two ways. First, it makes this design dependent on manual supervision. Because the stages
of the cascade are thresholded combinations of weak learners, there is a need to search for the conﬁguration (number
of learners and threshold) of each stage. This usually requires trial and error and can be very time consuming, compromising the viability of cascaded detectors as a practical
solution for generic object detection. Second, the resulting cascades are inevitably sub-optimal. This is particularly
problematic because the objects to detect occur rarely in the
images where detection is performed, and the cost of loosing each occurrence is usually large. The problem is compounded by the fact that the missed detections of the various
stages accumulate, and it is very difﬁcult to design cascades
of high overall detection rate.
The limitations of the original VJ design have motivated
a number of enhancements in the recent years. Most of
these contributions, while improving various aspects of the
original design - e.g. by proposing improved boosting algorithms (or other feature selection procedures) , embedded cascade structures , cost-sensitive extensions
for the design of cascade stages , or threshold postprocessing - have not addressed the problem of how
to optimally design the whole cascade. This problem has
only recently started to receive some attention , and
interesting theoretical formulations have been proposed for
its solution, but their translation into practical algorithms requires assumptions or approximations that may not always
hold. Perhaps due to this, these formulations have not yet
proved effective at producing high detection-rate cascades.
In this work, we propose a new cascade design algorithm
that addresses this problem. We introduce a novel formulation for cascade design, which poses the problem as one
of jointly minimizing computational cost and false positive
rate for a given detection rate. We then show that this reduces to the problem of minimizing false positive given detection rate, i.e. the classic problem of cost-sensitive learning. We next introduce a cost-sensitive extension of boosting that maintains a high detection rate across the boosting
iterations, and allows the design of single-feature embedded cascades with high overall detection rate. Experimental
evaluation shows that, when compared to the state-of-theart, these cascades do achieve signiﬁcantly higher detection
rates, at the cost of a marginal increase in computation.
2. Relation to previous work
A number of extensions to the VJ algorithm have been
recently proposed in the literature. Some, e.g. using ﬂoating
instead of sequential feature search , optimizing thresholds after cascade designed , or using the outputs of the
cascade stages as input space to the design of a ﬁnal classi-
ﬁer , could be combined with what is discussed here. A
number of researchers proposed the idea of embedded cascades . These are cascades where each stage
is part of the subsequent one, enabling the use of computationally efﬁcient stages without compromise of detection
performance. While, like these methods, our solution produces embedded cascades, it is based on the optimal design
of the overall cascade, rather than individual stage design.
It has also been realized that one of the major problems
of the original VJ design is the reliance on the AdaBoost algorithm. Because the latter is not cost-sensitive, it forces
the manipulation of thresholds of the individual cascade
stages, which are thus forced to operate at detection and
false-positive rates for which they have not been optimized.
This has motivated various authors to adopt cost-sensitive
extensions of AdaBoost . These are, however, mostly
heuristic, and have only been applied to the design of individual stages. In result, there are no guarantees of 1) optimality, in a cost-sensitive sense, for the whole cascade, or 2)
ability to achieve a high overall detection rate. We propose a
principled cost-sensitive extension of AdaBoost, which designs the whole cascade in a single step, under an explicit
constraint on the detection rate.
Recently, some proposals have started to emerge in the
area of optimal global cascade design. Waldboost is
based on an extension of the classic work by Wald on optimal sequential decision making . Although Wald’s
underlying framework is principled, the optimal solution
is impossible to compute, and approximations are needed.
Due to this, the resulting algorithms give guarantees on the
sum of false-positive and miss rate, but not on each of these
terms individually (see, e.g., Theorem 2 of ). This compromises its ability to design high-detection rate cascades.
Furthermore, Wald’s procedure is only optimal for independent observations. Waldboost is a boosting-based extension
that addresses this limitation, but requires a number of assumptions that only hold in the asymptotic regime of inﬁnite
length cascades. It is, therefore, not clear how the intermediate decisions are related to Wald’s theory, and the difﬁculty to maintain high detection rates is compounded. The
net result is a rather non-intuitive design procedure which,
for example, relies on the speciﬁcation of a zero falsepositive rate for cascade design, when the goal is to produce
cascades with a constraint on the detection rate .
In , the globally optimal design of detector cascades is
addressed in the VJ framework, i.e. by designing each stage
individually. False positive and miss rates are treated as random variables, and a sampling based procedure is designed
to predict the best operating point for each stage, given the
operating points of the previous stages. This relies on an
assumption of repeatability, i.e. that by simple observation
of previous points it is possible to 1) predict the best operating point for the next, and 2) actually design it so as to
achieve that point. It is unclear how closely this assumption holds in practice. Overall, none of the above methods
has demonstrated the ability to produce cascades of high
detection-rate.
3. Optimal cascade design
To address this problem, we propose a new framework
for cascade design. In this section, we deﬁne optimality in
a sense close to that of VJ and show that optimal design of
embedded cascades reduces to cost-sensitive learning.
3.1. Deﬁnitions
A cascaded detector is a decision function of the form
h(x) = s1(x) ∧. . . ∧sK(x)
where si(x) are detectors, denoted as cascade stages, and
the ∧operator implements an early rejection of the examples which are classiﬁed as negatives. The false-positive
rate of the ith detector is denoted by fi, its detection rate by
di, the false-positive rate of the whole cascade by F and its
detection rate by D. An assumption, underlying all previous cascade design algorithms, is that errors committed by
successive stages are independent. While not necessarily
realistic, this assumption makes the problem signiﬁcantly
more tractable, and is also adopted in this work. It follows
and it is possible to show that the expected complexity of
the whole cascade is
and ci is the cost of evaluating the ith stage and π the probability of occurrence of the object of interest.
3.2. Optimal cascades
We formulate the problem of optimal cascade design
as the joint minimization of the false-positive rate and expected cascade complexity, given a target detection rate D∗,
h∈H F(F, E[C])
where F is some function monotonically increasing in F
and E[C], and H is the set of cascades under consideration.
Similarly to VJ, we consider the set of constant detectionrate cascades
H = {h|di = (D∗)1/K},
i.e. whose stages share a common detection rate. It is possible to show that, for any h ∈H and k ∈{1, . . . , K},
both the expected cascade complexity and its falsepositive rate, are non-decreasing functions of fk. Hence,
for any monotonically increasing F, the optimal solution
is to minimize the false-positive rate of each stage, i.e. by
choosing the cascade h∗with stage false-positive rates
dk = (D∗)1/K
for all k ∈{1, . . . , K}.
3.3. Embedded cascades
The cascade stages are binary decision rules
si(x) = sgn[gi(x)]
where gi(x) are real functions, here referred to as predictors. Usually, these are linear combinations of weak learners φi,j(x),
αi,jφi,j(x),
where φi,j(x) is a thresholded feature response, also known
as decision stump, and Ji is the number of weak learners
combined by gi(x).
Embedded cascades (also known as nested or
chained cascades) are implemented with embedded
predictors. These are predictors such that, for all i, the terms
of gi(x) are also terms of gi+1(x) . A single-feature embedded cascade is an embedded cascade where the predictor of
each stage adds one weak learner to the predictor of its predecessor,
gi(x) + αi+1φi+1(x)
with g0(x) = 0. The associated cascade is equivalent to
introducing a rejection point after each weak learner of a
decision rule produced by boosting. In what follows, we
concentrate on single feature embedded cascades, but most
results hold for embedded cascades of any conﬁguration.
3.4. Optimal embedded cascades
Consider a sequence of predictors, g∗
k(x), optimal in the
sense of (10) for k = 1, . . . , i. By deﬁnition, gi(x) is the
sum of i terms of the form of (15) such that
i (x) = sgn[g∗
achieves the smallest possible false positive rate at detection rate (D∗)1/K. The optimal design of gi+1(x) requires
selecting (αi+1, φi+1) such that
si+1(x) = sgn[g∗
i (x) + αi+1φi+1(x)]
false-positive
detection-rate (D∗)1/K.
To gain some insight on how this may be solved, it is
useful to recall the interpretation of boosting as gradient descent in the functional space S of linear combinations of
weak learners . Under this interpretation, the functions
gk(x) are elements of S, and a cost functional C(g) is de-
ﬁned in this space. Boosting can then be shown to perform
a search for the best (αi+1, φi+1), by taking a step of gradient descent: it selects the weak learner φi+1 as the negative
of the functional derivative of C, evaluated at gi(x),
φi+1 = −∇C(gi(x))
and the scaling constant αi+1 through a line search along
the descent direction φi+1 . The cost functional is the
empirical estimate of the margin
C(gi(x)) = 1
from a training set D = {(x1, y1), . . . , (xN, yN)}, also
known as the boosting loss, or exponential loss. As we will
see in the next section, this guarantees that the optimal solution converges to the minimum probability of error rule,
as the weak learners are added to the ensemble.
This process can be equally applied to the solution
of (16). There are only two differences, due to the fact that,
instead of minimizing probability of error, we are now interested in minimizing false-positive error given a constraint
on detection rate. The ﬁrst is that the cost functional should
offer guarantees of convergence to the optimal, detection
rate constrained, decision rule. These guarantees should be
identical to those offered by boosting with respect to convergence to the minimum probability of error rule. The
second is that the detection rate is met at all iterations of
the boosting process. In summary, we need an extension of
boosting that supports a constraint on detection rate. This is
usually referred as cost sensitive boosting.
4. Cost-sensitive boosting
A cost-sensitive detector s(x) is a detector that assigns
unequal importance to the two error types. An optimal costsensitive detector is designed by deﬁning a loss function of
if s(x) = y
if y = −1 and s(x) = 1
if y = 1 and s(x) = −1
where Ci > 0, and searching for the detector of minimum
risk, or expected value of this loss The optimal solution
is the well known Bayes decision rule , which declares
s(x) = 1 when
 PY |X(1|x)
PY |X(−1|x)
and s(x) = −1 otherwise. This can be summarized as
sT (x) = sgn
 PY |X(1|x)
PY |X(−1|x)
with T = log C2
The relation between boosting and minimum probability
of error classiﬁcation follows from a result of Friedman ,
who has shown that the exponential loss
E[exp(−yg(x))]
is minimized by the symmetric logistic transform of
PY |X(1|x),
2 log PY |X(1|x)
PY |X(−1|x).
Since, for a large training sample D, the empirical loss
of (17) converges to (21), it follows that the result of the
gradient descent performed by boosting is the optimal predictor g∗(x), in the minimum probability of error sense.
4.1. Why boosting is not enough
The statement that the predictor found by boosting converges to (22), and the relation between this and the optimal
cost-sensitive detector of (20) suggests a natural answer to
the design of detector cascades. Simply compute each stage
with the standard boosting algorithm, and replace the decision rule sgn[g∗(x)] by sgn[g∗(x) −T]. Given (22), this
should be equivalent to (20), producing the optimal costsensitive detector. This is the procedure proposed by VJ,
and used by all previous cascade design algorithms, including the optimal methods of . The only variations are
the approaches used to produce the thresholds.
While all these statements are correct, they are predicated on the convergence of the boosted predictor to (22)
everywhere. It should, however, be noted that this convergence does not necessarily hold for a ﬁnite number of iterations. In fact, because the minimization carried out by
boosting is performed over a set S of functions, the fact
that boosting produces a minimum probability of error rule
does not guarantee that convergence has taken place. It suf-
ﬁces that the predictor g(x) produced by boosting is equal
to g∗(x) along the classiﬁcation boundary , i.e. the set x
such that g∗(x) = 0. Away from the boundary, g(x) can
be substantially different from g∗(x), since all that matters
for the optimality of the decision rule is that both have the
same sign.
It is important to emphasize that this is not a mere theoretical curiosity. In fact, boosting is designed to emphasize convergencein the neighborhoodof the cost-insensitive
boundary. This is the role of the weight resampling mechanism, which quickly discards points located away from it.
While these points are easy to classify in the cost-insensitive
case, therefore warranting this neglect, they become the
points adjacent to the boundary when costs are considered
(note, from (20) that cost-sensitivity changes the location
of the optimal boundary). Because the convergence of the
boosted predictor to (22) does not necessarily hold in their
neighborhood, the simple modiﬁcation of its threshold is
not likely to result in the optimal cost-sensitive decision rule
The inability of boosting to produce good cost-sensitive
rules has been experimentally conﬁrmed by various authors, and a number of cost-sensitive extensions have been
proposed in the literature . Some of these
have, in fact, been applied to the problem of cascade design . The main difﬁculty is that these algorithms
are mostly heuristic, relying on somewhat arbitrary heuristics to modify boosting’s weight update rule. We have not
been able to successfully design high detection-rate cascades with any of them.
4.2. A cost-sensitive boosting algorithm
A better alternative is to generalize Friedman’s result. In
this context, we have recently shown that the asymmetric
I(y = 1)e−y.C1g(x) + I(y = −1)e−y.C2g(x)i
is minimized by the asymmetric logistic transform of
PY |X(1|x),
log PY |X(1|x)C1
PY |X(−1|x)C2
where I(y = 1) and I(y = −1) are indicator functions . Hence, (23) is an asymmetric boosting loss function that can be minimized in a manner similar to AdaBoost,
by gradient descent on the space of convex combinations of
weak learners. Deﬁning two sets
I+ = {i|yi = 1}
I−= {i|yi = −1},
this leads to a weight update rule of the form
e−C1αmφm(xi),
eC2αmφm(xi),
For a given step size α, the gradient direction is
φm(x) = arg min
(eC1α −e−C1α) · b + e−C1αT+
+(eC2α −e−C2α) · d + e−C2αT−
and the optimal step size is the solution of
2C1 · b · cosh(C1α) + 2C2 · d · cosh(C2α) =
C1 · T+ · e−C1α + C2 · T−· e−C2α
I(yi ̸= φ(xi))
I(yi ̸= φ(xi)).
Given a training set (x1, y1)....(xn, yn) where y
{+1, −1} is the class label of example x, and costs C1, C2,
the complete asymmetric boosting algorithm is as follows:
# False Positives
Detection%
No CascadeT=800
EmbeddedT=800
EmbeddedT=400
EmbeddedT=200
Viola−Jones
Boosting Chain
Figure 1. ROC of the various cascades discussed in the text, for
face detection on the MIT-CMU test set.
• Initialize weights to uniform wi
2|I−|∀i ∈I−.
• For t = 1, ...., T (Where T is the total number of weak
learners.)
1. for each j, train a weak learner/step-size pair
(φj(x), αj) using current weights wi.
2. Find the weak learner φ that minimizes the loss
of (27) with α found by solving (28).
3. update the weights according to (26).
• The ﬁnal strong classiﬁer implements the decision rule
s(x) = sign[PT
m=1 αmφm(x)].
5. Evaluation
We performed various experiments to evaluate the contributions of this paper.
Unless otherwise noted, all experiments followed the experimental protocol of , using a face database of 10K positive and 350K negative
examples, and weak learners combining decision stumps
and Haar wavelet features (selected from a feature pool of
50, 000). Because asymmetric boosting maintains a highdetection rate throughout all iterations, it enables the design
of cascades “a posteriori”. This consists of simply adding
exit points (i.e. rejecting examples classiﬁed as negatives)
at intermediate steps of the detector. In particular, all single
feature cascades discussed in what follows were produced
by running asymmetric boosting to obtain a non-cascaded
detector, and then introducing exit points after the evaluation of each weak learner.
Figure 1 presents a comparison between the performance
of a single feature embedded cascade and the cascades produced by three state-of-the-art methods: VJ , Boosting
Embedded200
Embedded400
Boosting Chain
Table 1. comparison of length and average number of features used .
chain and WaldBoost . The evaluation was conducted on the CMU-MIT face test set, and the detection
rate and number of total false positives are reported for a series of points on the ROC curves of the different cascades.
We compared the performance of three single-feature embedded cascades obtained from the same boosting run, by
considering 800, 400 and 200 features. Note that all three
achieve much higher detection rates than those produced by
the previous methods. In fact, the only method that achieves
within 3% of their detection rate is WaldBoost, and only for
substantially larger false positive rates. The previous methods produce smaller false positives in the low detectionrate regime, but we have not tried to optimize the performance in this area: all embedded cascades were obtained
with (C1 = 5,C2 = 1), other cost conﬁgurations would
have given greater preference to the low false-positive rate
It is also worth emphasizing that the design of the embedded cascade only required 350K random negative examples, extracted from images not containing faces. These
were available in our lab from previous experiments with
face detectors, but were not bootstrapped speciﬁcally for the
design of this cascade. Although bootstrapping for more examples could have been used to improve performance, it is
not a strict requirement for embedded cascades. This is in
contrast to the millions of negative image patches used in
the design of other methods. It is also interesting to note
that all three embedded cascades achieve high detectionrates. In particular, note that the detection-rate does not
seem to decrease from 200 to 800 features.
This is evidence that asymmetric boosting maintains high-detection
rates throughout the whole boosting run. It also challenges
the assumption of independent errors which, for single feature embeddings, would imply an exponential decrease in
detection rate.
On the contrary, there appears to be no
penalty for the cascading operation. To test this premise,
we have also measured the ROC curve of the non-cascaded
boosted detector. As can be seen from Figure 1, there is virtually no difference between its ROC and that of the tremendously faster single feature cascade.
Table 1 provides a comparison of the number of features
(#F) and average number of features used (Avg-Used) by
different methods. The non-cascaded detector of n features
would use n features on average. The embedded cascade
design reduces this number to about 15, a number comparable to that produced by the other methods. VJ and Wald-
Boost have slightly faster evaluation times, but signiﬁcantly
worse detection rates. Finally, the total training time was of
2 days for the 200 feature embedded cascade, and consisted
uniquely of CPU time (no manual supervision), as opposed
to the ”weeks” of trial and error reported by VJ.