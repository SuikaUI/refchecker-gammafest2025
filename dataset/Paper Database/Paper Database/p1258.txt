Proceedings of SemEval-2016, pages 85–91,
San Diego, California, June 16-17, 2016. c⃝2016 Association for Computational Linguistics
TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classiﬁcation
Georgios Balikas and Massih-Reza Amini
University of Grenoble-Alpes
 
This paper describes the participation of
the team “TwiSE” in the SemEval 2016
challenge.
Speciﬁcally,
we participated
in Task 4, namely “Sentiment Analysis in
Twitter” for which we implemented sentiment
classiﬁcation systems for subtasks A, B, C
and D. Our approach consists of two steps. In
the ﬁrst step, we generate and validate diverse
feature sets for twitter sentiment evaluation,
inspired by the work of participants of
previous editions of such challenges. In the
second step, we focus on the optimization
of the evaluation measures of the different
subtasks. To this end, we examine different
learning strategies by validating them on the
data provided by the task organisers.
our ﬁnal submissions we used an ensemble
learning approach (stacked generalization) for
Subtask A and single linear models for the
rest of the subtasks. In the ofﬁcial leaderboard
we were ranked 9/35, 8/19, 1/11 and 2/14 for
subtasks A, B, C and D respectively. The code
can be found at 
balikasg/SemEval2016-Twitter_
Sentiment_Evaluation.
Introduction
During the last decade, short-text communication
forms, such as Twitter microblogging, have been
widely adopted and have become ubiquitous. Using such forms of communication, users share a variety of information. However, information concerning one’s sentiment on the world around her has attracted a lot of research interest .
Working with such short, informal text spans
poses a number of different challenges to the Natural Language Processing (NLP) and Machine Learning (ML) communities. Those challenges arise from
the vocabulary used (slang, abbreviations, emojis)
 , the short size, and the complex
linguistic phenomena such as sarcasm that often occur.
We present, here, our participation in Task 4 of
SemEval 2016 , namely Sentiment Analysis in Twitter. Task 4 comprised ﬁve different subtasks: Subtask A: Message Polarity Classiﬁcation, Subtask B: Tweet classiﬁcation according
to a two-point scale, Subtask C: Tweet classiﬁcation
according to a ﬁve-point scale, Subtask D: Tweet
quantiﬁcation according to a two-point scale, and
Subtask E: Tweet quantiﬁcation according to a ﬁvepoint scale. We participated in the ﬁrst four subtasks
under the team name “TwiSE” (Twitter Sentiment
Evaluation). Our work consists of two steps: the
preprocessing and feature extraction step, where we
implemented and tested different feature sets proposed by participants of the previous editions of SemEval challenges , and the learning step, where we investigated and optimized the performance of different
learning strategies for the SemEval subtasks. For
Subtask A we submitted the output of a stacked generalization ensemble learning approach using the probabilistic outputs of a set of linear models as base models, whereas for the rest of
the subtasks we submitted the outputs of single models, such as Support Vector Machines and Logistic
Regression.1
The remainder of the paper is organised as follows: in Section 2 we describe the feature extraction
and the feature transformations we used, in Section
3 we present the learning strategies we employed,
in Section 4 we present a part of the in-house validation we performed to assess the models’ performance, and ﬁnally, we conclude in Section 5 with
remarks on our future work.
Feature Engineering
We present the details of the feature extraction and
transformation mechanisms we used. Our approach
is based on the traditional N-gram extraction and
on the use of sentiment lexicons describing the sentiment polarity of unigrams and/or bigrams.
the data pre-processing, cleaning and tokenization2
as well as for most of the learning steps, we used
Python’s Scikit-Learn and
NLTK .
Feature Extraction
Similar to we extracted
features based on the lexical content of each tweet
and we also used sentiment-speciﬁc lexicons. The
features extracted for each tweet include:
• N-grams with N ∈ , character grams of
dimension M ∈ ,
• # of exclamation marks, # of question marks, #
of both exclamation and question marks,
• # of capitalized words and # of elongated
• # of negated contexts; negation also affected
the N-grams features by transforming a word
w in a negated context to w NEG,
• # of positive emoticons, # of negative emoticons and a binary feature indicating if emoticons exist in a given tweet, and
1To enable replicability we make the code we used
 
SemEval2016-Twitter_Sentiment_Evaluation.
sentiment.christopherpotts.net/tokenizing.
• Part-of-speech (POS) tags 
and their occurrences partitioned regarding the
positive and negative contexts.
With regard to the sentiment lexicons, we used:
• manual sentiment lexicons: the Bing liu’s lexicon , the NRC emotion lexicon , and the
MPQA lexicon ,
• # of words in positive and negative context belonging to the word clusters provided by the
CMU Twitter NLP tool3
• positional sentiment lexicons: sentiment 140
lexicon and the Hashtag Sentiment Lexicon 
We make, here, more explixit the way we used the
sentiment lexicons, using the Bing Liu’s lexicon as
an example. We treated the rest of the lexicons similarly. For each tweet, using the Bing Liu’s lexicon we obtain a 104-dimensional vector. After tokenizing the tweet, we count how many words (i)
in positive/negative contenxts belong to the positive/negative lexicons (4 features) and we repeat the
process for the hashtags (4 features). To this point
we have 8 features. We generate those 8 features
for the lowercase words and the uppercase words.
Finally, for each of the 24 POS tags the tagger generates, we count how many
words in positive/negative contenxts belong to the
positive/negative lexicon. As a results, this generates 2 × 8 + 24 × 4 = 104 features in total for each
For each tweet we also used the distributed representations provided by using the
min, max and average composition functions on the
vector representations of the words of each tweet.
Feature Representation and
Transformation
We describe the different representations of the extracted N-grams and character-grams we compared
when optimizing our performance on each of the
classiﬁcation subtasks we participated. In the rest
of this subsection we refer to both N-grams and
3 
character-grams as words, in the general sense of
letter strings. We evaluated two ways of representing such features: (i) a bag-of-words representation,
that is for each tweet a sparse vector of dimension
|V | is generated, where |V | is the vocabulary size,
and (ii) a hashing function, that is a fast and spaceefﬁcient way of vectorizing features, i.e. turning arbitrary features into indices in a vector . We found that the performance using
hashing representations was better. Hence, we opted
for such representations and we tuned the size of the
feature space for each subtask.
Concerning the transformation of the features of
words, we compared the tf-idf weighing scheme
and the α-power transformation. The latter, transforms each vector x = (x1, x2, . . . , xd) to x′ =
2 , . . . , xα
d) . The main intuition behind the α-power transformation is that it
reduces the effect of the most common words. Note
that this is also the rationale behind the idf weighting scheme. However, we obtained better results using the α-power transformation. Hence, we tuned α
separately for each of the subtasks.
The Learning Step
Having the features extracted we experimented with
several families of classiﬁers such as linear models,
maximum-margin models, nearest neighbours approaches and trees. We evaluated their performance
using the data provided by the organisers, which
were already split in training, validation and testing
parts. Table 1 shows information about the tweets
we managed to download. From the early validation schemes, we found that the two most competitive models were Logistic Regression from the family of linear models, and Support Vector Machines
(SVMs) from the family of maximum margin models. It is to be noted that this is in line with the previous research .
Subtask A concerns a multiclass classiﬁcation problem, where the general polarity of tweets has to be
classiﬁed in one among three classes: “Positive”,
“Negative” and “Neutral”, each denoting the tweet’s
overall polarity. The evaluation measure used for the
subtask is the Macro-F1 measure, calculated only
for the Positive and Negative classes we decided to
employ an ensemble learning approach. Hence, our
goal is twofold: (i) to generate a set of models that
perform well as individual models, and (ii) to select
a subset of models of (i) that generate diverse outputs and to combine them using an ensemble learning step that would result in lower generalization error.
We trained four such models as base models.
Their details are presented in Table 2. In the stacked
generalization approach we employed, we found
that by training the second level classiﬁer on the
probabilistic outputs, instead of the predictions of
the base models, yields better results. Logistic Regression generates probabilities as its outputs. In the
case of SVMs, we transformed the conﬁdence scores
into probabilities using two methods, after adapting
them to the multiclass setting: the Platt’s method
 and the isotonic regression
 .
To solve the optimization problems of SVMs we used the Liblinear
solvers . For Logistic Regression
we used either Liblinear or LBFGS, with the latter
being a limited memory quasi Newton method for
general unconstrained optimization problems . To attack the multiclass problem, we selected among the traditional one-vs-rest approach,
the crammer-singer approach for SVMs , or the multinomial approach for
Logistic Regression (also known as MaxEnt classi-
ﬁer), where the multinomial loss is minimised across
the entire probability distribution .
For each of the four base models the tweets
are represented by the complete feature set described in Section 2.
For transforming the ngrams and character-grams, the value of α
{0.2, 0.4, 0.6, 0.8, 1}, the dimension of the space
where the hashing function projects them, as well
as the value of λ ∈{10−7, . . . , 106} that controls the importance of the regularization term in the
SVM and Logistic regression optimization problems
were selected by grid searching using 5-fold crossvalidation on the training part of the provided data.
We tuned each model independently before integrat-
Development
Subtask B & D
Subtask C & E
Table 1: Size of the data used for training and development purposes. We only relied on the SemEval 2016 datasets.
Multiclass approach
Probabilistic Outputs
crammer-singer
isotonic regression
crammer-singer
Logistic Regression
one-vs-all
Logistic Regression
multinomial loss function
Table 2: Description of the base learners used in the stacked generalization approach.
ing it in the stacked generalization.
Having the ﬁne-tuned probability estimates for
each of the instances of the test sets and for each
of the base learners, we trained a second layer classiﬁer using those ﬁne-grained outputs. For this, we
used SVMs, using the crammer-singer approach for
the multi-class problem, which yielded the best performance in our validation schemes. Also, since the
classiﬁcation problem is unbalanced in the sense that
the three classes are not equally represented in the
training data, we assigned weights to make the problem balanced. Those weights were inversely proportional to the class frequencies in the input data for
each class.
Subtask B is a binary classiﬁcation problem where
given a tweet known to be about a given topic, one
has to classify whether the tweet conveys a positive or a negative sentiment towards the topic. The
evaluation measure proposed by the organisers for
this subtask is macro-averaged recall (MaR) over the
positive and negative class.
Our participation is based on a single model. We
used SVMs with a linear kernel and the Liblinear
optimizer.
We used the full feature set described
in Section 2, after excluding the distributed embeddings because in our local validation experiments we
found that they actually hurt the performance. Similarly to subtask A and due to the unbalanced nature
of the problem, we use weights for the classes of the
problem. Note that we do not consider the topic of
the tweet and we classify the tweet’s overall polarity.
Hence, we ignore the case where the tweet consists
of more than one parts, each expressing different polarities about different parts.
Subtask C concerns an ordinal classiﬁcation problem. In the framework of this subtask, given a tweet
known to be about a given topic, one has to estimate the sentiment conveyed by the tweet towards
the topic on a ﬁve-point scale. Ordinal classiﬁcation differs from standard multiclass classiﬁcation in
that the classes are ordered and the error takes into
account this ordering so that not all mistakes weigh
equally. In the tweet classiﬁcation problem for instance, a classiﬁer that would assign the class “1” to
an instance belonging to class “2” will be less penalized compared to a classiﬁer that will assign “-1”
as the class . To this direction, the evaluation measure proposed by the organisers is the macroaveraged mean absolute error.
Similarly to Subtask B, we submitted the results
of a single model and we classiﬁed the tweets according to their overall polarity ignoring the given
topics. Instead of using one of the ordinal classiﬁcation methods proposed in the bibliography, we use
a standard multiclass approach. For that we use a
Logistic Regression that minimizes the multinomial
loss across the classes. Again, we use weights to
cope with the unbalanced nature of our data. The
distributed embeddings are excluded by the feature
We elaborate here on our choice to use a conventional multiclass classiﬁcation approach instead
of an ordinal one.
We evaluated a selection of
methods described in 
and in . In both cases, the
results achieved with the multiclass methods were
marginally better and for simplicity we opted for the
multiclass methods. We believe that this is due to
the nature of the problem: the feature sets and especially the ﬁne-grained sentiment lexicons manage
to encode the sentiment direction efﬁciently. Hence,
assigning a class of completely opposite sentiment
can only happen due to a complex linguistic phenomenon such as sarcasm. In the latter case, both
methods may fail equally.
Subtask D is a binary quantiﬁcation problem. In particular, given a set of tweets known to be about a
given topic, one has to estimate the distribution of
the tweets across the Positive and Negative classes.
For instance, having 100 tweets about the new
iPhone, one must estimate the fractions of the Positive and Negative tweets respectively. The organisers proposed a smoothed version of the Kullback-
Leibler Divergence (KLD) as the subtask’s evaluation measure.
We apply a classify and count approach for this
task , that is we
ﬁrst classify each of the tweets and we then count the
instances that belong to each class. To this end, we
compare two approaches both trained on the features
sets of Section 2 excluding the distributed representations: the standard multiclass SVM and a structure
learning SVM that directly optimizes KLD . Again, our ﬁnal submission
uses the standard SVM with weights to cope with
the imbalance problem as the model to classify the
That is because the method of although competitive was outperformed in most of our local validation schemes.
The evaluation framework
Before reporting the scores we obtained, we elaborate on our validation strategy and the steps we used
when tuning our models. in each of the subtasks we
only used the data that were realised for the 2016
edition of the challenge. Our validation had the following steps:
Table 3: The performance obtained on the “devtest” data and
the SemEval 2016 Task 4 test data.
1. Training using the released training data,
2. validation on the validation data,
3. validation again, in the union of the devtest and
trial data (when applicable), after retraining on
training and validation data.
For each parameter, we selected its value by averaging the optimal parameters with respect to the output of the above-listed steps (2) and (3). It is to be
noted, that we strictly relied on the data released as
part of the 2016 edition of SemEval; we didn’t use
past data.
We now present the performance we achieved
both in our local evaluation schemas and in the ofﬁcial results released by the challenge organisers. Table 3 presents the results we obtained in the “DevTest” part of the challenge dataset and the scores
on the test data as they were released by the organisers. In the latter, we were ranked 9/35, 8/19, 1/11
and 2/14 for subtasks A, B, C and D respectively.
Observe, that for subtasks A and B, using just the
“devtest” part of the data as validation mechanism
results in a quite accurate performance estimation.
Future Work
That was our ﬁrst contact with the task of sentiment
analysis and we achieved satisfactory results. We
relied on features proposed in the framework of previous SemEval challenges and we investigated the
performance of different classiﬁcation algorithms.
In our future work we will investigate directions
both in the feature engineering and in the algorithmic/learning part. Firstly, we aim to deal with tweets
in a ﬁner level of granularity. As discussed in Section 3, in each of the tasks we classiﬁed the overall polarity of the tweet, ignoring cases where the
tweets were referring to two or more subjects. In the
same line, we plan to improve our mechanism for
handling negation. We have used a simple mechanism where a negative context is deﬁned as the
group of words after a negative word until a punctuation symbol. However, our error analysis revealed
that punctuation is rarely used in tweets. Finally, we
plan to investigate ways to integrate more data in our
approaches, since we only used this edition’s data.
The application of an ensemble learning approach, is a promising direction towards the short
text sentiment evaluation. To this direction, we hope
that an extensive error analysis process will help us
identify better classiﬁcation systems that when integrated in our ensemble (of subtask A) will reduce
the generalization error.
Acknowledgments
We would like to thank the organisers of the Task 4
of SemEval 2016, for providing the data, the guidelines and the infrastructure. We would also like to
thank the anonymous reviewers for their insightful