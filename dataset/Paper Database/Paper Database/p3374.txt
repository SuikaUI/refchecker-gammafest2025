Charting the Right Manifold: Manifold Mixup for Few-shot Learning
Puneet Mangla∗†2
 
Mayank Singh∗1
 
Abhishek Sinha∗1
 
Nupur Kumari∗1
 
Vineeth N Balasubramanian2
 
Balaji Krishnamurthy1
 
1. Media and Data Science Research lab, Adobe
2. IIT Hyderabad, India
Few-shot learning algorithms aim to learn model parameters capable of adapting to unseen classes with the help
of only a few labeled examples.
A recent regularization
technique - Manifold Mixup focuses on learning a generalpurpose representation, robust to small changes in the data
distribution. Since the goal of few-shot learning is closely
linked to robust representation learning, we study Manifold Mixup in this problem setting. Self-supervised learning is another technique that learns semantically meaningful features, using only the inherent structure of the data.
This work investigates the role of learning relevant feature
manifold for few-shot tasks using self-supervision and regularization techniques.
We observe that regularizing the
feature manifold, enriched via self-supervised techniques,
with Manifold Mixup signiﬁcantly improves few-shot learning performance. We show that our proposed method S2M2
beats the current state-of-the-art accuracy on standard fewshot learning datasets like CIFAR-FS, CUB, mini-ImageNet
and tiered-ImageNet by 3 −8%.
Through extensive experimentation, we show that the features learned using
our approach generalize to complex few-shot evaluation
tasks, cross-domain scenarios and are robust against slight
changes to data distribution.
1. Introduction
Deep convolutional networks (CNN’s) have become a
regular ingredient for numerous contemporary computer vision tasks. They have been applied to tasks such as object recognition, semantic segmentation, object detection
 to achieve state-of-the-art performance.
However, the at par performance of deep neural networks
∗Authors contributed equally
†Work done during Adobe MDSR internship
requires huge amount of supervisory examples for training.
Generally, labeled data is scarcely available and data collection is expensive for several problem statements. Hence,
a major research effort is being dedicated to ﬁelds such as
transfer learning, domain adaptation, semi-supervised and
unsupervised learning to alleviate this requirement of enormous amount of examples for training.
A related problem which operates in the low data regime
is few-shot classiﬁcation.
In few-shot classiﬁcation, the
model is trained on a set of classes (base classes) with abundant examples in a fashion that promotes the model to classify unseen classes (novel classes) using few labeled instances. The motivation for this stems from the hypothesis
that an appropriate prior should enable the learning algorithm to solve consequent tasks more easily. Biologically
speaking, humans have a high capacity to generalize and
extend the prior knowledge to solve new tasks using only
small amount of supervision.
One of the promising approach to few-shot learning utilizes meta-learning framework to optimize for such an initialization of model parameters such that adaptation to the optimal weights of classiﬁer for novel classes can be reached with few gradient
updates . Some of the work also includes
leveraging the information of similarity between images
 and augmenting the training data by hallucinating additional examples . Another class
of algorithms learns to directly predict the weights
of the classiﬁer for novel classes.
Few-shot learning methods are evaluated using N-way
K-shot classiﬁcation framework where N classes are sampled from a set of novel classes (not seen during training)
with K examples for each class. Usually, the few-shot classiﬁcation algorithm has two separate learning phases. In
the ﬁrst phase, the training is performed on base classes to
develop robust and general-purpose representation aimed to
be useful for classifying novel classes. The second phase
of training exploits the learning from previous phase in the
 
form of a prior to perform classiﬁcation over novel classes.
The transfer learning approach serves as the baseline which
involves training a classiﬁer for base classes and then subsequently learning a linear classiﬁer on the penultimate layer
of the previous network to classify the novel classes .
Learning feature representations that generalize to novel
classes is an essential aspect of few-shot learning problem. This involves learning a feature manifold that is relevant for novel classes. Regularization techniques enables
the models to generalize to unseen test data that is disjoint from training data. It is frequently used as a supplementary technique alongside standard learning algorithms
 . In particular for classiﬁcation problems, Manifold Mixup regularization leverages interpolations in deep hidden layer to improve hidden representations and decision boundaries at multiple layers.
In Manifold Mixup , the authors show improvement
in classiﬁcation task over standard image deformations
and augmentations. Also, some work in self-supervision
 explores to predict the type of augmentation
applied and enforces feature representation to become invariant to image augmentations to learn robust visual features. Inspired by this link, we propose to unify the training
of few-shot classiﬁcation with self-supervision techniques
and Manifold Mixup . The proposed technique employs
self-supervision loss over the given labeled data unlike in
semi-supervised setting that uses additional unlabeled data
and hence our approach doesn’t require any extra data for
Many of the recent advances in few-shot learning exploit
the meta-learning framework, which simulates the training
phase as that of the evaluation phase in the few-shot setting.
However, in a recent study , it was shown that learning a
cosine classiﬁer on features extracted from deeper networks
also performs quite well on few-shot tasks.
by this observation, we focus on utilizing self-supervision
techniques augmented with Manifold Mixup in the domain
of few-shot tasks using cosine classiﬁers.
Our main contributions in this paper are the following:
• We ﬁnd that the regularization technique of Manifold
Mixup being robust to small changes in data distribution enhances the performance of few-shot tasks.
• We show that adding self-supervision loss to the training procedure, enables robust semantic feature learning that leads to a signiﬁcant improvement in few-shot
classiﬁcation. We use rotation and exemplar 
as the self-supervision tasks.
• We observe that applying Manifold Mixup regularization over the feature manifold enriched via the
self-supervision tasks further improves the performance of few-shot tasks. The proposed methodology
(S2M2) outperforms the state-of-the-art methods by
3-8% over the CIFAR-FS, CUB, mini-ImageNet and
tiered-ImageNet datasets.
• We conduct extensive ablation studies to verify the ef-
ﬁcacy of the proposed method. We ﬁnd that the improvements made by our methodology become more
pronounced with increasing N in the N-way K-shot
evaluation and also in the cross-domain evaluation.
2. Related Work
Our work is associated with various recent development
made in learning robust general-purpose visual representations, speciﬁcally few-shot learning, self-supervised learning and generalization boosting techniques.
Few-shot learning:
Few-shot learning involves building
a model using available training data of base classes that
can classify unseen novel classes using only few examples.
Few-shot learning approaches can be broadly divided into
three categories - gradient based methods, distance metric
based methods and hallucination based methods.
Some gradient based methods aim to use gradient
descent to quickly adapt the model parameters suitable for
classifying the novel task. The initialization based methods
 speciﬁcally advocate to learn a suitable initialization of the model parameters, such that adapting from
those parameters can be achieved in a few gradient steps.
Distance metric based methods leverage the information
about similarity between images to classify novel classes
with few examples. The distance metric can either be cosine similarity , euclidean distance , CNN based
distance module , ridge regression or graph neural
network . Hallucination based methods augment the limited training data for a new task by generating
or hallucinating new data points.
Recently, introduced a modiﬁcation for the simple
transfer learning approach, where they learn a cosine classi-
ﬁer instead of a linear classiﬁer on top of feature extraction layers. The authors show that this simple approach
is competitive with several proposed few-shot learning approaches if a deep backbone network is used to extract the
feature representation of input data.
Self-supervised learning:
This is a general learning
framework which aims to extract supervisory signals by
deﬁning surrogate tasks using only the structural information present in the data. In the context of images, a pretext
task is designed such that optimizing it leads to more semantic image features that can be useful for other vision
tasks. Self-supervision techniques have been successfully
applied to diverse set of domains, ranging from robotics to
computer vision . In the context of visual
Figure 1: Flowchart for our proposed approach (S2M2) for few-shot learning. The auxiliary loss is derived from Manifold Mixup regularization and
self-supervision tasks of rotation and exemplar.
data, the surrogate loss functions can be derived by leveraging the invariants in the structure of the image. In this paper,
we focus on self-supervised learning techniques to enhance
the representation and learn a relevant feature manifold for
few-shot classiﬁcation setting. We now brieﬂy describe the
recent developments in self-supervision techniques.
C. Doersch et al. took inspiration from spatial context
of a image to derive supervisory signal by deﬁning the surrogate task of relative position prediction of image patches.
Motivated by the task of context prediction, the pretext task
was extended to predict the permutation of the shufﬂed image patches . leveraged the rotation invariance of images to create the surrogate task of predicting
the rotation angle of the image. Also, the authors of 
proposed to decouple representation learning of the rotation
as pretext task from class discrimination to obtain better results. Along the lines of context-based prediction, uses
generation of the contents of image region based on context
pixel (i.e. in-painting) and in the authors propose
to use gray-scale image colorization as a pretext task.
Apart from enforcing structural constraints, uses
cluster assignments as supervisory signals for unlabeled
data and works by alternating between clustering of the image descriptors and updating the network by predicting the
cluster assignments. deﬁnes pretext task that uses lowlevel motion-based grouping cues to learn visual representation. Also, proposes to obtain supervision signal by
enforcing the additivity of visual primitives in the patches
of images and proposed to learn feature representations
by predicting the future in latent space by employing autoregressive models.
Some of the pretext tasks also work by enforcing constraints on the representation of the feature. A prominent
example is the exemplar loss from that promotes representation of image to be invariant to image augmentations.
Additionally, some research effort have also been put in to
deﬁne the pretext task as a combination of multiple pretext
task . For instance, in representation learning
is augmented with pretext tasks of jigsaw puzzle , colorization and in-painting .
Generalization:
Employing regularization techniques for
training deep neural networks to improve their generalization performances have become standard practice in the
deep learning community. Few of the commonly used regularization techniques are - dropout , cutout , Mixup
 , Manifold Mixup . Mixup is a speciﬁc case
of Manifold Mixup where the interpolation of only input data is applied. The authors in claim that Manifold
Mixup leads to smoother decision boundaries and ﬂattens
the class representations thereby leading to feature representation that improve the performance over a held-out validation dataset. We apply a few of these generalization techniques during the training of the backbone network over the
base tasks and ﬁnd that the features learned via such regularization lead to better generalization over novel tasks too.
Authors of provide a summary of popular regularization techniques used in deep learning.
3. Methodology
The few-shot learning setting is formalized by the availability of a dataset with data-label pairs D = {(xi, yi) :
i = 1, · · · , m} where x ∈Rd and yi ∈C, C being the
set of all classes. We have sufﬁcient number of labeled data
in a subset of C classes (called base classes), while very
few labeled data for the other classes in C (called novel
classes). Few-shot learning algorithms generally train in
two phases: the ﬁrst phase consists of training a network
over base class data Db = {(xi, yi), i = 1, · · · , mb} where
{yi ∈Cb ⊂C} to obtain a feature extractor, and the second
phase consists of adapting the network for novel class data
Dn = {(xi, yi), i = 1, · · · , mn} where {yi ∈Cn ⊂C}
and Cb ∪Cn = C. We assume that there are Nb base
classes (cardinality of Cb) and Nn novel classes (cardinality
of Cn). The general goal of few-shot learning algorithms is
to learn rich feature representations from the abundant labeled data of base classes Nb, such that the features can be
easily adapted for the novel classes using only few labeled
instances.
In this work, in the ﬁrst learning stage, we train a Nb-way
neural network classiﬁer:
g = cWb ◦fθ
on Db, where cWb is a cosine classiﬁer and fθ is
the convolutional feature extractor, with θ parametrizing the
neural network model. The model is trained with classiﬁcation loss and an additional auxiliary loss which we explain
soon. The second phase involves ﬁne-tuning of the backbone model, fθ, by freezing the feature extractor layers and
training a new Nn-way cosine classiﬁer cWn on data from
k randomly sampled novel classes in Dn with only classiﬁcation loss. Figure 1 provides an overview of our approach
S2M2 for few-shot learning .
Importantly, in our proposed methodology, we leverage
self-supervision and regularization techniques 
to learn general-purpose representation suitable for fewshot tasks. We hypothesize that using robust features which
describes the feature manifold well is important to obtain
better performance over the novel classes in the few-shot
In the subsequent subsections, we describe our
training procedure to use self-supervision methods (such as
rotation and exemplar ) to obtain a suitable feature manifold, following which using Manifold Mixup regularization provides a robust feature extractor backbone. We empirically show that this proposed methodology
achieves the new state-of-the-art result on standard few-shot
learning benchmark datasets.
3.1. Manifold Mixup for Few-shot Learning
Higher-layer representations in neural network classi-
ﬁers have often been visualized as lying on a meaningful
manifold, that provide the relevant geometry of data to solve
a given task . Therefore, linear interpolation of feature
vectors in that space should be relevant from the perspective of classiﬁcation. With this intuition, Manifold Mixup
 , a recent work, leverages linear interpolations in neural
network layers to help the trained model generalize better.
In particular, given input data x and x′ with corresponding
feature representations at layer l given by f l
θ(x) and f l
respectively. Assuming we use Manifold Mixup on the base
classes in our work, the loss for training Lmm is then formulated as:
Lmm = E(x,y)∈Db
θ(x′)), Mixλ(y, y′)
Mixλ(a, b) = λ · a + (1 −λ) · b
The mixing coefﬁcient λ is sampled from a β(α, α) distribution and loss L is standard cross-entropy loss. We hypothesize that using Manifold Mixup on the base classes
provides robust feature presentations that lead to state-ofthe-art results in few-shot learning benchmarks.
Training using loss Lmm encourages the model to predict less conﬁdently on linear interpolations of hidden representations. This encourages the feature manifold to have
broad regions of low-conﬁdence predictions between different classes and thereby smoother decision boundaries, as
shown in . Also, models trained using this regularizer
lead to ﬂattened hidden representations for each class with
less number of directions of high variance i.e. the representations of data from each class lie in a lower dimension subspace. The above-mentioned characteristics of the method
make it a suitable regularization technique for generalizing
to tasks with potential distribution shifts.
3.2. Charting the Right Manifold
We observed that Manifold Mixup does result in higher
accuracy on few-shot tasks, as shown in Section 4.2.3.
However, it still lags behind existing state-of-the-art performance, which begs the question: Are we charting the right
manifold? In few-shot learning, novel classes introduced
during test time can have a different data distribution when
compared to base classes. In order to counter this distributional shift, we hypothesize that it is important to capture
the right manifold when using Manifold Mixup for the base
classes. To this end, we leverage self-supervision methods.
Self-supervision techniques have been employed recently
in many domains for learning rich, generic and meaningful feature representations. We show that the simple idea of
adding auxiliary loss terms from self-supervised techniques
while training the base classes provides feature representations that signiﬁcantly outperform state-of-the-art for classifying on the novel classes.
We now describe the selfsupervised methods used in this work.
Self-Supervision: Towards the Right Manifold
In this work, we use two pretext tasks that have recently
been widely used for self-supervision to support our claim.
We describe each of these below.
Rotation :
In this self-supervised task, the input image is rotated by different angles, and the auxiliary aim of
the model is to predict the amount of rotation applied to
image. In the image classiﬁcation setting, an auxiliary loss
(based on the predicted rotation angle) is added to the standard classiﬁcation loss to learn general-purpose representations suitable for image understanding tasks. In this work,
we use a 4-way linear classiﬁer, cWr, on the penultimate
feature representation fθ(xr) where xr is the image x rotated by r degrees and r ∈CR = {0◦, 90◦, 180◦, 270◦}, to
predict one of 4 classes in CR. In other words, similar to
Eqn 1, our pretext task model is given by gr = cWr ◦fθ.
The self-supervision loss is given by:
L(cWr(fθ(xr)), r)
Lclass = E(x,y)∈Db,r∈CR
where |CR| denotes the cardinality of CR.
As the selfsupervision loss is deﬁned over the given labeled data of Db,
no additional data is required to implement this method. L
is the standard cross-entropy loss, as before.
Exemplar :
Exemplar training aims at making the
feature representation invariant to a wide range of image
transformations such as translation, scaling, rotation, contrast and color shifts. In a given mini-batch M, we create 4 copies of each image through random augmentations.
These 4 copies are the positive examples for each image and
every other image in the mini-batch is a negative example.
We then use hard batch triplet loss with soft margin on
fθ(x) on the mini-batch to bring the feature representation
of positive examples close together. Speciﬁcally, the loss is
p∈{1,··· ,4} D
p∈{1..4},i̸=j D(xi
Here, D is the Euclidean distance in the feature representation space fθ(x) and xi
k is the kth exemplar of x with class
label i (the appropriate augmentation). The ﬁrst term inside
the exp term is the maximum among distances between an
image and its positive examples which we want to reduce.
The second term is the minimum distance between the image and its negative examples which we want to maximize.
S2M2: Self-Supervised Manifold Mixup
The few-shot learning setting relies on learning robust and
generalizable features that can separate base and novel
classes. An important means to this end is the ability to
compartmentalize the representations of base classes with
generous decision boundaries, which allow the model to
generalize to novel classes. Manifold Mixup provides an effective methodology to ﬂatten representations of data from
a given class into a compact region, thereby supporting this
objective. However, while claims that Manifold Mixup
can handle minor distribution shifts, the semantic difference
between base and novel classes in the few-shot setting may
be more than what it can handle. We hence propose the
use of self-supervision as an auxiliary loss while training
the base classes, which allows the learned backbone model,
fθ, to provide feature representations with sufﬁcient decision boundaries between classes, that allow the model to
extend to the novel classes. This is evidenced in our results presented in Section 4.2.3. Our overall methodology is
summarized in the steps below, and the pseudo-code of the
proposed approach for training the backbone is presented in
Algorithm 1.
Algorithm 1 S2M2 feature backbone training
Input: {x, y} ∈Db; α; {x′, y′} ∈Dval
Output: Backbone model fθ
▷Feature extractor backbone fθ training
Initialize fθ
for epochs ∈{1, 2, ..., 400} do
Training data of size B - (X(i), Y (i)).
L(θ, X(i), Y (i)) = Lclass + Lss
θ →θ −η ∗∇L(θ, X(i), Y (i))
val acc prev = 0.0
val acc list = [ ]
▷Fine-tuning fθ with Manifold Mixup
while val acc > val acc prev do
Training data of size B - (X(i), Y (i)).
L(θ, X(i), Y (i)) = Lmm + 0.5(Lclass + Lss)
θ →θ −η ∗∇L(θ, X(i), Y (i))
val acc = Accuracyx,y∈Dval(Wn(fθ(x)), y)
Append val acc to val acc list
Update val acc prev with val acc
return ﬁne-tuned backbone fθ.
Step 1: Self-supervised training:
Train the backbone
model using self-supervision as an auxiliary loss along with
classiﬁcation loss i.e. L + Lss where Lss ∈{Le, Lrot}.
Step 2: Fine-tuning with Manifold Mixup:
the above model with Manifold Mixup loss Lmm for a few
more epochs.
After obtaining the backbone, a cosine classiﬁer is
learned over it to adapt to few-shot tasks.
S2M2E are two variants of our proposed approach which
uses Lrot and Le as auxiliary loss in Step 1 respectively.
4. Experiments and Results
In this section, we present our results of few-shot classi-
ﬁcation task on different datasets and model architectures.
We ﬁrst describe the datasets, evaluation criteria and implementation details1.
We perform experiments on four standard
datasets for few-shot image classiﬁcation benchmark, mini-
ImageNet , tiered-ImageNet , CUB and
CIFAR-FS .
mini-ImageNet consists of 100 classes
from the ImageNet which are split randomly into 64
base, 16 validation and 20 novel classes. Each class has
600 samples of size 84 × 84. tiered-ImageNet consists of
608 classes randomly picked from ImageNet which are
split randomly into 351 base, 97 validation and 160 novel
classes. In total, there are 779, 165 images of size 84 × 84.
CUB contains 200 classes with total 11, 788 images of size
84 × 84. The base, validation and novel split is 100, 50 and
1 fewshot
mini-ImageNet
tiered-ImageNet
54.69 ± 0.89
66.62 ± 0.83
51.67 ± 1.81
70.30 ± 0.08
71.29 ± 0.95
80.33 ± 0.70
58.9 ± 1.9
71.5 ± 1.0
ProtoNet 
54.16 ± 0.82
73.68±0.65
53.31 ± 0.89
72.69 ± 0.74
71.88±0.91
87.42 ± 0.48
55.5 ± 0.7
72.0 ± 0.6
RelationNet 
52.19 ± 0.83
70.20 ± 0.66
54.48 ± 0.93
71.32 ± 0.78
68.65 ± 0.91
81.12 ± 0.63
55.0 ± 1.0
69.3 ± 0.8
61.76 ± 0.08
77.59 ± 0.12
66.33 ± 0.05
81.44 ± 0.09
68.22 ± 0.22∗
78.27 ± 0.16∗
62.64 ± 0.61
78.63 ± 0.46
65.99 ± 0.72
81.56 ± 0.53
72.0 ± 0.7
84.2 ± 0.5
Baseline++
57.53 ± 0.10
72.99 ± 0.43
60.98 ± 0.21
75.93 ± 0.17
70.4 ± 0.81
82.92 ± 0.78
67.50 ± 0.64
80.08 ± 0.32
Manifold Mixup
57.16 ± 0.17
75.89 ± 0.13
68.19 ± 0.23
84.61 ± 0.16
73.47 ± 0.89
85.42 ± 0.53
69.20 ± 0.2
83.42 ± 0.15
63.9 ± 0.18
81.03 ± 0.11
73.04 ± 0.22
87.89 ± 0.14
77.61 ± 0.86
89.32 ± 0.46
70.66 ± 0.2
84.15 ± 0.14
64.93 ± 0.18
83.18 ± 0.11
73.71 ± 0.22
88.59 ± 0.14
80.68 ± 0.81
90.85 ± 0.44
74.81 ± 0.19
87.47 ± 0.13
Table 1: Comparison with prior/current state of the art methods on mini-ImageNet, tiered-ImageNet, CUB and CIFAR-FS dataset. The accuracy with ∗
denotes our implementation of LEO using their publicly released code
mini-ImageNet
Baseline++
53.56 ± 0.32
74.02 ± 0.13
54.41 ± 0.21
74.14 ± 0.19
57.53 ± 0.10
72.99 ± 0.43
Mixup (α = 1)
56.12 ± 0.17
73.42 ± 0.13
56.19 ± 0.17
73.05 ± 0.12
59.65 ± 0.34
77.52 ± 0.52
Manifold Mixup
55.77 ± 0.23
71.15 ± 0.12
55.40 ± 0.37
70.0 ± 0.11
57.16 ± 0.17
75.89 ± 0.13
58.96 ± 0.24
76.63 ± 0.12
61.13 ± 0.2
77.05 ± 0.35
63.9 ± 0.18
81.03 ± 0.11
56.39 ± 0.17
76.33 ± 0.14
56.87 ± 0.17
76.90 ± 0.17
62.2 ± 0.45
78.8 ± 0.15
56.80 ± 0.2
76.54 ± 0.14
56.92 ± 0.18
76.97 ± 0.18
62.33 ± 0.25
79.35 ± 0.16
64.06 ± 0.18
80.58 ± 0.12
63.74 ± 0.18
79.45 ± 0.12
64.93 ± 0.18
83.18 ± 0.11
Baseline++
67.68 ± 0.23
82.26 ± 0.15
68.09 ± 0.23
83.16 ± 0.3
70.4 ± 0.81
82.92 ± 0.78
Mixup (α = 1)
68.61 ± 0.64
81.29 ± 0.54
67.02 ± 0.85
84.05 ± 0.5
68.15 ± 0.11
85.30 ± 0.43
Manifold Mixup
70.57 ± 0.71
84.15 ± 0.54
72.51 ± 0.94
85.23 ± 0.51
73.47 ± 0.89
85.42 ± 0.53
72.4 ± 0.34
84.83 ± 0.32
72.74 ± 0.46
84.76 ± 0.62
77.61 ± 0.86
89.32 ± 0.46
68.12 ± 0.87
81.87 ± 0.59
69.93 ± 0.37
84.25 ± 0.56
71.58 ± 0.32
84.63 ± 0.57
71.81 ± 0.43
86.22 ± 0.53
72.67 ± 0.27
84.86 ± 0.13
74.89 ± 0.36
87.48 ± 0.49
71.43 ± 0.28
85.55 ± 0.52
72.92 ± 0.83
86.55 ± 0.51
80.68 ± 0.81
90.85 ± 0.44
Baseline++
59.67 ± 0.90
71.40 ± 0.69
60.39 ± 0.28
72.85 ± 0.65
67.5 ± 0.64
80.08 ± 0.32
Mixup (α = 1)
56.60 ± 0.11
71.49 ± 0.35
57.60 ± 0.24
71.97 ± 0.14
69.29 ± 0.22
82.44 ± 0.27
Manifold Mixup
60.58 ± 0.31
74.46 ± 0.13
58.88 ± 0.21
73.46 ± 0.14
69.20 ± 0.2
83.42 ± 0.15
59.53 ± 0.28
72.94 ± 0.19
59.32 ± 0.13
73.26 ± 0.15
70.66 ± 0.2
84.15 ± 0.14
59.69 ± 0.19
73.30 ± 0.17
61.59 ± 0.31
74.17 ± 0.37
70.05 ± 0.17
84.01 ± 0.22
61.95 ± 0.11
75.09 ± 0.16
62.48 ± 0.21
73.88 ± 0.30
72.63 ± 0.16
86.12 ± 0.26
63.66± 0.17
76.07± 0.19
62.77± 0.23
75.75± 0.13
74.81 ± 0.19
87.47 ± 0.13
Table 2: Results on mini-ImageNet, CUB and CIFAR-FS dataset over different network architecture.
50 classes. CIFAR-FS is created by randomly splitting 100
classes of CIFAR-100 into 64 base, 16 validation and
20 novel classes. The images are of size 32 × 32.
Evaluation Criteria:
We evaluate experiments on 5-way
1-shot and 5-way 5-shot classiﬁcation setting i.e using
1 and 5 labeled instances of each of the 5 classes as training
data and Q instances each from the same classes as testing data. For tiered-ImageNet, mini-ImageNet and CIFAR-
FS we report the average classiﬁcation accuracy over 10000
tasks where Q = 599 for 1-Shot and Q = 595 for 5-Shot
tasks respectively. For CUB we report average classiﬁcation
accuracy with Q = 15 over 600 tasks. We compare our approach S2M2R against the current state-of-the-art methods,
LEO and DCO in Section 4.2.3.
4.1. Implementation Details
We perform experiments on three different model architecture: ResNet-18, ResNet-34 and WRN-28-10 
which is a Wide Residual Network of 28 layers and width
factor 10. For tiered-ImageNet we only perform experiments with WRN-28-10 architecture. Average pooling is
applied at the last block of each architecture for getting
feature vectors.
ResNet-18 and ResNet-34 models have
512 dimensional output feature vector and WRN-28-10 has
640 dimensional feature vector.
For training ResNet-18
and ResNet-34 architectures, we use Adam optimizer
for mini-ImageNet and CUB whereas SGD optimizer for
CIFAR-FS. For WRN-28-10 training, we use Adam optimizer for all datasets.
4.2. Performance Evaluation over Few-shot Tasks
In this subsection, we report the result of few shot learning over our proposed methodology and its variants.
Using Manifold Mixup Regularization
All experiments using Manifold Mixup randomly sample a hidden layer (including input layer) at each step to
Figure 2: UMAP (2-dim) plot of feature vectors of images from novel classes of mini-ImageNet using Baseline++, Rotation, S2M2R (left to right).
apply mixup as described in equation 3 for the mini-batch
with mixup coefﬁcient (λ) sampled from a β(α, α) distribution with α = 2. We compare the performance of Manifold
Mixup with Baseline++ and Mixup . The results are shown in table 2. We can see that the boost in fewshot accuracy from the two aforementioned mixup strategies is signiﬁcant when model architecture is deep (WRN-
28-10). For shallower backbones (ResNet-18 and ResNet-
34), the results are not conclusive.
Using Self-supervision as Auxiliary Loss
We evaluate the contribution of rotation prediction and
exemplar training as an auxiliary task during backbone training for few-shot tasks. Backbone model is trained
with both classiﬁcation loss and auxiliary loss as explained
in section 3.2.1.
For exemplar training, we use random
cropping, random horizontal/vertical ﬂip and image jitter
randomization to produce 4 different positive variants
of each image in the mini-batch. Since exemplar training
is computationally expensive, we ﬁne-tune the baseline++
model for 50 epochs using both exemplar and classiﬁcation
The comparison of above techniques with Baseline++ is
shown in table 2. As we see, by selecting rotation and exemplar as an auxiliary loss there is a signiﬁcant improvement from Baseline++ ( 7 −8%) in most cases. Also, the
improvement is more prominent for deeper backbones like
WRN-28-10.
Our Approach: S2M2
We ﬁrst train the backbone model using self-supervision
(exemplar or rotation) as auxiliary loss and then ﬁne-tune
it with Manifold Mixup as explained in section 3.2.2. The
results are shown in table 2. We compare our approach with
current state-of-the-art and other existing few-shot
methods in Table 1. As we can observe from table,
our approach S2M2R beats the most recent state-of-the-art
results , LEO and DCO , by a signiﬁcant margin on
all four datasets. We ﬁnd that using only rotation prediction
as an auxiliary task during backbone training also outperforms the existing state-of-the-art methods on all datasets
except CIFAR-FS.
Baseline++
Table 3: Mean few-shot accuracy on mini-ImageNet as N increases in
N-way K-shot classiﬁcation.
5. Discussion and Ablation Studies
To understand the signiﬁcance of learned feature representation for few-shot tasks, we perform various experiments and analyze the ﬁndings in this section. We choose
mini-ImageNet as the primary dataset with WRN-28-10
backbone for the following experiments.
Effect of varying N in N-way classiﬁcation:
For extensive evaluation, we test our proposed methodology in complex few-shot settings. We vary N in N-way K-shot evaluation criteria from 5 to 10, 15 and 20. The corresponding results are reported in table 3. We observe that our approach S2M2R outperforms other techniques by a signiﬁcant margin. The improvement becomes more pronounced
for N > 5. Figure 2 shows the 2-dimensional UMAP 
plot of feature vectors of novel classes obtained from different methods. It shows that our approach has more segregated clusters with less variance. This supports our hypothesis that using both self supervision and Manifold Mixup
regularization helps in learning feature representations with
well separated margin between novel classes.
Cross-domain few-shot learning:
We believe that in
practical scenarios, there may be a signiﬁcant domain-shift
between the base classes and novel classes.
Therefore,
to further highlight the signiﬁcance of selecting the right
manifold for feature space, we evaluate the few-shot classiﬁcation performance over cross-domain dataset : mini-
ImageNet =⇒CUB (coarse-grained to ﬁne-grained distribution) using Baseline++, Manifold Mixup , Rotation
 and S2M2R. We train the feature backbone with the
base classes of mini-ImageNet and evaluate its performance
mini-ImageNet =⇒CUB
44.79 ± 0.75
64.98 ± 0.68
Baseline++
40.44 ± 0.75
56.64 ± 0.72
Manifold Mixup
46.21 ± 0.77
66.03 ± 0.71
48.42 ± 0.84
68.40 ± 0.75
48.24 ± 0.84
70.44 ± 0.75
Table 4: Comparison in cross-domain dataset scenario.
Base + Validation
61.76 ± 0.08
77.59 ± 0.12
64.09 ± 0.62
80.00 ± 0.45
Baseline++
61.10 ± 0.19
75.23 ± 0.12
Manifold Mixup
61.10 ± 0.27
77.69 ± 0.21
65.98 ± 0.36
81.67 ± 0.08
67.13 ± 0.13
83.6 ± 0.34
Table 5: Effect of using the union of base and validation class for training
the backbone fθ.
over the novel classes of CUB (to highlight the domainshift). We report the corresponding results in table 4.
Generalization performance of supervised learning over
base classes:
The results in table 2 and 3 empirically support the hypothesis that our approach learns a feature manifold that generalizes to novel classes and also results in
improved performance on few-shot tasks. This generalization of the learned feature representation should also hold
for base classes. To investigate this, we evaluate the performance of backbone model over the validation set of the
ImageNet dataset and the recently proposed ImageNetV2
dataset . ImageNetV2 was proposed to test the generalizability of the ImageNet trained models and consists of images having slightly different data distribution from the ImageNet. We further test the performance of backbone model
over some common visual perturbations and adversarial attack. We randomly choose 3 of the 15 different perturbation
techniques - pixelation, brightness, contrast , with 5 varying
intensity values , as mentioned in the paper . For adversarial attack, we use the FGSM with ϵ = 1.0/255.0.
All the evaluation is over the 64 classes of mini-ImageNet
used for training the backbone model. The results are shown
in table 6. It can be seen that S2M2R has the best generalization performance for the base classes also.
Effect of using the union of base and validation classes:
We test the performance of few-shot tasks after merging the
validation classes into base classes. In table 5, we see a considerable improvement over the other approaches using the
same extended data, supporting the generalizability claim
Baseline++
Validation set top-1 accuracy of different approaches over
base classes and it’s perturbed variants (I:ImageNet; I2:ImageNetv2;
P:Pixelation noise; C: Contrast noise; B: Brightness; Adv: Aversarial
Figure 3: Effect of increasing the number of self-supervised (degrees of
rotation) labels.
of the proposed method.
Different levels of self-supervision:
We conduct a separate experiment to evaluate the performance of the model
by varying the difﬁculty of self-supervision task; specifically the number of angles to predict in rotation task.
We change the number of rotated versions of each image to 1 (0◦), 2 (0◦, 180◦), 4 (0◦,90◦,180◦,270◦) and 8
(0◦,45◦,90◦,135◦,180◦,225◦,270◦,315◦) and record the performance over the novel tasks for each of the corresponding
4 variants. Figure 3 shows that the performance improves
with increasing the number of rotation variants till 4, after
which the performance starts to decline.
6. Conclusion
We observe that learning feature representation with relevant regularization and self-supervision techniques lead
to consistent improvement of few-shot learning tasks on a
diverse set of image classiﬁcation datasets. Notably, we
demonstrate that feature representation learning using both
self-supervision and classiﬁcation loss and then applying
Manifold Mixup over it, outperforms prior state-of-the-art
approaches in few-shot learning. We do extensive experiments to analyze the effect of architecture and efﬁcacy of
learned feature representations in few-shot setting.
work opens up a pathway to further explore the techniques
in self-supervision and generalization techniques to improve computer vision tasks speciﬁcally in low-data regime.
Finally, our ﬁndings highlight the merits of learning a robust representation that helps in improving the performance
of few-shot tasks.