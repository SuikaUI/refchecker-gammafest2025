Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 1–10
Hong Kong, China, November 3, 2019. c⃝2019 Association for Computational Linguistics
 
A Closer Look At Feature Space Data Augmentation For Few-Shot
Intent Classiﬁcation
Varun Kumar, Hadrien Glaude, Cyprien de Lichy, Wlliam Campbell
Amazon Alexa
Cambridge, MA, USA
{kuvrun,hglaude,cllichy,cmpw}@amazon.com
New conversation topics and functionalities
are constantly being added to conversational
AI agents like Amazon Alexa and Apple Siri.
As data collection and annotation is not scalable and is often costly, only a handful of examples for the new functionalities are available, which results in poor generalization performance. We formulate it as a Few-Shot Integration (FSI) problem where a few examples
are used to introduce a new intent. In this paper, we study six feature space data augmentation methods to improve classiﬁcation performance in FSI setting in combination with
both supervised and unsupervised representation learning methods such as BERT. Through
realistic experiments on two public conversational datasets, SNIPS, and the Facebook Dialog corpus, we show that data augmentation in
feature space provides an effective way to improve intent classiﬁcation performance in fewshot setting beyond traditional transfer learning approaches. In particular, we show that
(a) upsampling in latent space is a competitive baseline for feature space augmentation
(b) adding the difference between two examples to a new example is a simple yet effective
data augmentation method.
Introduction
Virtual artiﬁcial assistants with natural language
understanding (NLU) support a variety of functionalities.
Throughout the lifespan of the deployed NLU systems, new functionalities with
new categories, are regularly introduced. While
techniques such as active learning , semi-supervised learning are used to improve the performance of existing functionalities, performance for new functionalities suffers from the data scarcity problem.
Recently, Few-Shot Learning has been explored
to address the problem of generalizing from a few
examples per category. While it has been extensively studied for image recognition, a little attention has been paid to improve NLU performance in the low-data regime. Moreover, researchers have been mostly working on the unrealistic setting that considers tasks with few categories unseen during (pre)training, each with only
a few examples, and introduces new categories
during test time. We argue that a more realistic setting is Few-Shot Integration (FSI) where new categories with limited training data are introduced
into an existing system with mature categories.
FSI is well aligned with the goal of lifelong learning of conversational agents and measures the performance in a real-life system setting when only
a few examples of a new class are added to the
existing data from the old classes. To address the
poor generalization in data scare scenarios, several
pre-training methods such as ELMo , Generative pre-trained Transformer , BERT
 ,
have been proposed which are trained on a large
amount of unannotated text data. Such pre-trained
models can be ﬁne-tuned on a particular NLP task
and have shown to greatly improve generalization. However, in FSI setting where only a handful
of examples are provided, building accurate NLU
model is still a challenging task.
In this paper, we focus on Feature space Data
Augmentation (FDA) methods to improve the
classiﬁcation performance of the categories with
limited data. We study six widely different feature space data augmentation methods:
1) upsampling in the feature space UPSAMPLE, 2) random perturbation PERTURB, 3) extrapolation EXTRA, 4) conditional
variational auto-encoder (CVAE) CVAE, 5) delta encoder that have
been especially designed to work in the few-shot
learning setting DELTA, 6)
linear delta which is a linear version of the delta
encoder LINEAR. While UPSAMPLE, PERTURB,
EXTRA and LINEAR doesn’t require any training
beyond hyper-parameter optimization, DELTA and
CVAE are trained deep neural network generators.
We compare these six FDA techniques on
two open datasets for Intent Classiﬁcation (IC) :
SNIPS and Facebook Dialog corpus . We show that
BERT combined with LINEAR data augmentation
provides an effective method to bootstrap accurate
intent classiﬁers with limited training data. We
make the following contributions:
1. We propose the FSI evaluation, a relaxation
of the few-shot learning setting that aims to
better model the requirement of modern NLU
systems. We provide a comprehensive evaluation of FSI for text classiﬁcation and show
that UPSAMPLE and PERTURB are simple yet
efﬁcient baselines that are often neglected in
few-shot learning evaluations.
2. We provide an in-depth analysis of various
FDA methods. We show that complex methods such as DELTA and CVAE do not always
improve over simple methods like LINEAR,
and the performance heavily depends on the
feature extractor.
3. Finally, we provide guidance on when and
how to apply FDA for FSI. We show that
FDA consistently provides gains on top of
the unsupervised pre-training methods such
as BERT in FSI setting.
Related work
Few-shot learning has been studied extensively
in the computer vision domain. In particular, several metric learning based methods has been proposed for few-shot
classiﬁcation where a model ﬁrst learns an embedding space and then a simple metric is used to
classify instances of new categories via proximity to the few labeled training examples embedded
in the same space. In addition to metric-learning,
several meta-learning based approaches 
have been proposed for few-shot classiﬁcation on
unseen classes.
Recently, Few-Shot Learning on text data has
been explored using metric learning .
In ,
authors propose to learn a weighted combination
of metrics obtained from meta-training tasks for
a newly seen few-shot task. Similarly, in , authors propose to use meta-metriclearning to learn task-speciﬁc metric that can handle imbalanced datasets.
Generative models are also widely used to
improve classiﬁcation performance by data augmentation.
For example, generative models are
used for data augmentation in image classiﬁcation
 , text classiﬁcation , anomaly detection (Lim et al.,
Data augmentation through deformation
of an image has been known to be very effective for image recognition.
More advanced approaches rely on Auto-Encoders (AEs) or Generative Adversarial Networks (GANs). For example, in the authors
combine metric-learning with data augmentation
using GANs for few-shot learning. However, classical generative approaches require a signiﬁcant
amount of training data to be able to generate good
enough examples that will improve classiﬁcation
accuracy. To overcome this challenge, proposed to augment the
training data in the feature space. This both eases
the generation problem and enforces generation of
discriminative examples. In addition, the authors
propose to transfer deformations from base classes
to new classes, which allows circumventing the
data scarcity problem for new classes. Finally, in
 , authors used an Autoencoder to encode transformations between pairs of
examples of the same class and apply them to an
example of the new class.
Generative models are a good candidate for
FSI tasks, as one can just combine the generated
data for new classes with the old classes training
data . However, generating discrete sequences,
e.g. text, is known to be quite difﬁcult and requires
lots of training data. That is why, in this paper, we
focus on generative models, which augment data
utterances
(a) Learning the feature space
sentence embedding
(b) Learning with augmented data
Figure 1: Data augmentation in the feature space
in latent(feature) space to solve a few-shot integration problem for text classiﬁcation.
Data Augmentation in Feature Space
Feature space data Augmentation (FDA) is an effective method to improve classiﬁcation performance on different ML tasks . As shown on Figure 1,
FDA techniques usually work by ﬁrst learning a
data representation or feature extractor, and then
generating new data for the low resource class in
the feature space. After generating data, a classi-
ﬁer is trained with real and augmented data.
For IC, we ﬁnetune a pre-trained English BERT-
Base uncased model 1 to build our feature extractor. The BERT model has 12 layers, 768 hidden
states, and 12 heads. We use the pooled representation of the hidden state of the ﬁrst special token
([CLS]) as the sentence representation. A dropout
probability of 0.1 is applied to the sentence representation before passing it to the 1-layer Softmax classiﬁer. BERT Encoder and MLP classi-
ﬁer are ﬁne-tuned using cross-entropy loss for IC
task. Adam is used for
optimization with an initial learning rate of 5e−5.
For data augmentation, we apply six different
FDA methods, described below, to generate new
examples in the feature space. Finally, we train a
1- layer Softmax classiﬁer as in the feature learning phase.
1 
Upsampling
The simplest method to augment training data for
underrepresented categories is to duplicate the existing training data. Upsampling is a well studied
technique to handle the class imbalance problem
 . We show that for intents
with limited labeled data, upsampling the existing
data in latent space consistently improves model
performance, and thus is a good baseline method
for FDA techniques. We call this method UPSAM-
Random Perturbation
Adding random noise to the existing training data
is another simple yet effective data augmentation
technique. Random perturbation data augmentation has been previously used to improve the performance of classiﬁcation models as well as for
sequence generation models. For example, applied additive and multiplicative perturbation to improve the text generation for
data augmentation. In our experiments, we apply
both additive and multiplicative perturbation to the
existing training data. We sample noise from a
uniform distribution [-1.0, 1.0]. We use PERTURB
to refer to this method.
Conditional VAE
Conditional
Variational
Autoencoder
Variational
Autoencoder
(VAE) which can be
used to generate examples for a given category.
All components of the model are conditioned on
the category. First, we train CVAE on the sentence representations and then generate new examples by sampling from the latent distribution.
The encoder and decoder sub-networks are implemented as multi-layer perceptrons with a single
hidden layer of 2048 units, where each layer is followed by a hyperbolic tangent activation. The encoder output Z is 128-dimensional. Mean Square
Error (MSE) loss function is used for reconstruction. All models are trained with Adam optimizer
with the learning rate set to 10 −3.
Linear Delta
A simple method to generate new examples is to
ﬁrst learn the difference between a pair of examples, and then add this difference to another example. In this case, we ﬁrst compute the difference
Xi −Xj between two examples from the same
class and then add it to a third example Xk also
from the same class as shown in (1). We use LIN-
EAR to refer to this method.
ˆX = (Xi −Xj) + Xk
Extrapolation
In , authors proposed
to use extrapolation to synthesize new examples
for a given class. They demonstrated that extrapolating between samples in feature space can be
used to augment datasets. In extrapolation, a new
example, ˆX is generated according to (2). In our
experiments, we use λ = 0.5. We call this method
ˆX = (Xi −Xj) ∗λ + Xi
Delta-Encoder
Delta-Encoder extends the
idea of learning differences between two examples
using an autoencoder-based model. It ﬁrst extracts
transferable intra-class deformations (deltas) between same-class pairs of training examples, then
applies them to a few examples of a new class to
synthesize samples from that class. Authors show
that Delta-Encoder can learn transferable deformations from different source classes which can
be used to generate examples for unseen classes.
While the authors used Delta-Encoder to generate
examples for unseen classes, in our experiments,
for FSI, we also use the examples from the target class to the train both the feature extractor and
the Delta-Encoder along with all other examples.
Then we generate new examples for the target category using trained delta encoder. For data generation, we try two different approaches to select a
source sentence pair.
1. DeltaR: Sample a pair of sentences (Xi, Xj)
from a randomly selected class. DELTAR applies deltas from multiple source categories
to synthesize new examples.
2. DeltaS: Sample a pair of sentences (Xi, Xj)
from the target category.
DELTAS applies
deltas from the same target category.
The encoder and decoder sub-networks are implemented as multi-layer perceptrons with a single
hidden layer of 512 units, where each layer is followed by a leaky ReLU activation (max(x, 0.2 ∗
x)). The encoder output Z is 16-dimensional. L1
loss is used as reconstruction loss. Adam optimizer is used with a learning rate of 10 −3. A
high dropout with a 50% rate is applied to all layers, to avoid the model memorizing examples.
Experiment
We evaluate different FDA techniques on two public benchmark datasets, SNIPS , and Facebook Dialog corpus (FBDialog)
 . For SNIPS dataset, we use
train, dev and test split provided by (Goo et al.,
SNIPS dataset contains 7 intents which are collected from the Snips personal voice assistant.
The training, development and test sets contain
13, 084, 700 and 700 utterances, respectively. FB-
Dialog has utterances that are focused on navigation, events, and navigation to events. FBDialog
dataset also contains utterances with multiple intents as the root node. For our experiment, we
exclude such utterances by removing utterances
with COMBINED intent root node.
This leads
to 31, 218 training, 4, 455 development and 9, 019
testset utterances. Note that while SNIPS is a balanced dataset, FBDialog dataset is highly imbalanced with a maximum 8, 860 and a minimum of
4 training examples per intent.
Simulating Few-Shot Integration
In virtual assistants, often a new intent development starts with very limited training data.
simulate the integration of a new intent, we randomly sample k seed training examples from the
new intent, referred to as target intent, and keep
all the data from other intents. We also remove the
target intent data from the development set. We
train the feature extractor on the resulting training
data, and then generate 100, 512 examples using
different augmentation methods for the target intent. To account for random ﬂuctuations in the results, we repeat this process 10 times for a given
target intent and report the average accuracy with
the standard deviation. In all experiments, models
are evaluated on the full test set.
2 
No Augmentation
98.14 (0.42)
94.99 (0.18)
98.14 (0.47)
95.01 (0.16)
98.26 (0.40)
94.98 (0.19)
98.14 (0.45)
95.02 (0.21)
98.14 (0.45)
95.02 (0.20)
98.14 (0.45)
94.98 (0.24)
98.23 (0.46)
95.00 (0.22)
98.26 (0.42)
95.00 (0.20)
98.14 (0.47)
94.94 (0.18)
98.23 (0.41)
94.98 (0.24)
98.09 (0.50)
95.02 (0.18)
98.11 (0.49)
95.01 (0.19)
98.20 (0.42)
94.99 (0.26)
98.26 (0.42)
94.99 (0.21)
98.23 (0.42)
94.97 (0.22)
98.14 (0.45)
95.02 (0.12)
98.14 (0.44)
94.99 (0.20)
98.17 (0.43)
95.05 (0.23)
98.14 (0.45)
95.07 (0.11)
98.11 (0.44)
94.98 (0.23)
98.26 (0.40)
95.08 (0.19)
98.20 (0.46)
95.04 (0.22)
Table 1: IC accuracy on SNIPS and Facebook dataset
with all training data, reported as mean (SD).
Results and Discussion
FDA For Data-Rich Classiﬁcation
For both datasets, we generate 5%, 10%, and 20%
examples using different FDA methods. Then, we
train a classiﬁer using both generated as well as
real data. Table 1 shows that augmenting data in
feature space provides only minor improvements
in classiﬁcation accuracy. In particular, on SNIPS
dataset, PERTUB and DELTAR improve accuracy
from 98.14 to 98.26. On FBDialog dataset, DeltaR
provides a minor gain, 95.02 to 95.08 over upsample baseline.
Impact Of The Number Of Seed
To understand the impact of the number of seed
examples, we vary it to 5, 10, 15, 20, 25, and 30 for
SNIPS’s AddToPlaylist. For each experiment, we
generate 100 examples using different FDA methods. Figure 2 shows that as the number of seed examples increases, the accuracy of the model goes
up. We also observe that for a few seed examples
5 - 15, LINEAR outperforms other FSA methods.
Finally, gains are less signiﬁcant after 30 seed examples.
Number of seed examples
Intent Accuracy
Figure 2: IC accuracy on SNIPS’s AddToPlaylist intent with varying number of seed examples. 100 examples are generated using different FDA techniques.
As indicated by the accuracy trend, increasing the seed
examples leads to better performance.
Few-Shot Integration
We simulate FSI IC for all 7 intents of SNIPS
For FBDialog dataset, we run simulations on the six largest intents, viz. GetDirections,
GetDistance, GetEstimatedArrival, GetEstimated-
Duration, GetInfoTrafﬁc, and GetEvent.
BERT generalizes well with just 30 examples, to
compare the effectiveness of different FDA methods, we use 10 seed examples in FSI simulations.
For each intent, we select k = 10 seed training examples and use all training data for other intents.
Table 2 shows average accuracy for all intents’
FSI simulations. Results on individual intent’s FSI
simulations can be found in Appendix’s Table 5
and Table 6. On both datasets, all FDA methods
improve classiﬁcation accuracy over no augmentation baseline. Also, UPSAMPLE provides huge
gains over no augmentation baseline. Additionally, on both datasets, with 512 augmented examples, LINEAR and DELTAS works better than
PERTURB and UPSAMPLE.
Upsampling: Text Space vs Latent Space
In this section, we explore how upsampling in text
space impacts performances as it is supposed to
both improve the feature extractor and the linear
classiﬁer, compared to UPSAMPLE. To investigate
whether upsampling in text space helps FDA, we
upsampled the 10 seed examples to 100 and repeat
the FSI experiments on all 7 intents of the SNIPS
dataset. Table 3 shows the mean accuracy of all
7 intents FSI simulations results for different FDA
techniques. FSI simulations scores for individual
intents can be found in Appendix’s Table 7. We
No Augmentation
87.46(2.87)
81.29(0.11)
94.26(1.66)
84.34(1.84)
94.18(1.74)
84.04(1.95)
94.10(1.83)
84.10(1.94)
94.36(1.69)
84.31(1.9)
94.30(1.68)
84.13(1.83)
91.32(3.12)
81.97(0.76)
94.28(1.92)
83.50(1.92)
95.68(0.86)
89.03(0.99)
95.65(0.92)
89.02(0.99)
95.46(1.03)
88.71(1.09)
95.87(0.87)
89.30(1.03)
95.82(0.89)
89.21(0.99)
95.33(1.56)
87.28(1.46)
95.88(1.04)
89.15(1.12)
Table 2: Average IC accuracy for all intents’ FSI simulations on SNIPS and FBDialog dataset. For each simulation, k = 10 seed examples are used for target intent. Scores are reported as mean (SD). Refer to Appendix’s Table 5 and Table 6 for individual intents’ results.
observe that upsampling in text space improves the
no augmentation baseline for all intents. The mean
accuracy score improves from 87.46 to 94.38. We
also observe that different FDA techniques further
improve model accuracy.
Interestingly, upsampling in text space helps DELTAR the most. Surprisingly, upsampling in latent space provides better performance than upsampling in the text space.
In particular, without upsampling the seed examples to learn the feature extractor, the best score
is 95.88 for DELTAS, whereas with text space upsampling the best score decreases to 94.88. This
decrease in performance is only seen with BERT
and not with the Bi-LSTM feature extractor (see
We hypothesize that upsampling text
data leads to BERT overﬁtting the target category
which results in less generalized sentence representations. Overall, we found that augmentation in
the latent space seems to work better with BERT,
and is more effective than text space upsampling.
Effect Of The Pre-trained BERT Encoder
In FSI setting, Fine-Tuned BERT model provides
very good generalization performance. For example, for SNIPS’s RateBookIntent (column Book in
Table 5), it yields 96.81% accuracy. Overall for
BERT representations, LINEAR and DELTAS augmentation methods provide the best accuracy.
Overall Mean
No Augmentation
94.38(1.23)
94.53(1.12)
94.52(1.18)
94.53(1.18)
94.53(1.12)
94.53(1.13)
94.62(1.16)
94.57(1.14)
94.67(1.11)
94.68(1.14)
94.73(1.11)
94.67(1.11)
94.67(1.11)
94.88(1.12)
94.74(1.12)
Table 3: IC accuracy on SNIPS dataset in the FSI setting, reported as mean (SD). The 10 seed examples are
upsampled to 100 to train the feature extractor. Refer
to Appendix’s Table 7 for individual intents’ results.
To investigate whether these augmentation improvements can be generalized to other sentence
encoders, we experiment with a Bi-LSTM sentence encoder. For feature learning, we use a 1layer Bi-LSTM encoder followed by a single layer
softmax classiﬁer. In our experiments, we use 128
as hidden units and 300 dimension Glove embeddings. For SNIPS dataset, we use 10 examples of
AddToPlaylist intent and for FB Dialog dataset,
we use 10 examples of GetDirections intent.
Table 4 shows intent accuracy for SNIPS and
Facebook datasets.
We ﬁnd that, unlike BERT,
in the FSI setting, the Bi-LSTM encoder provides
a lower accuracy. In contrast to BERT FSI experiments, DELTAS performs worse than the UP-
SAMPLE and PERTURB baselines. The main reason is that Delta-Encoder’s performance relies on
a good feature extractor and with 10 seed examples, the Bi-LSTM encoder fails to learn good sentence representations. To improve representation
learning, we upsample 10 utterances to 100 and
then train the feature extractor.
Upsampling in
text space improves the performance of both delta
encoder methods, DELTAS, and DELTAR. Moreover, for both SNIPS’s AddToPlayList and FBDialog’s GetDirections intent, DELTAR outperforms
all other FDA methods.
SNIPS’s AddToPlaylist
FBDialog’s GetDirections
seed examples (k)
No Augmentation
80.07 (2.08)
90.17 (1.39)
87.44 (0.12)
87.94 (0.32)
88.27 (1.74)
90.61 (1.52)
88.01 (0.26)
88.17 (0.32)
88.03 (1.52)
90.86 (1.39)
88.01 (0.32)
88.25 (0.31)
88.14 (1.62)
91.06 (1.58)
88.05 (0.25)
88.26 (0.32)
88.09 (1.57)
90.74 (1.57)
88.10 (0.29)
88.20 (0.3)
88.27 (2.08)
90.90 (1.69)
88.04 (0.24)
88.17 (0.32)
82.23 (2.21)
91.46 (1.19)
87.60 (0.23)
88.75 (0.43)
84.4 (2.74)
91.07 (1.44)
88.02 (0.22)
88.57 (0.36)
91.41 (1.03)
91.61 (1.4)
88.68 (0.49)
88.40 (0.35)
91.46 (0.99)
91.73 (1.32)
88.89 (0.57)
88.56 (0.39)
91.20 (1.28)
91.41 (1.52)
88.97 (0.65)
88.47 (0.33)
91.26 (1.22)
91.57 (1.55)
88.85 (0.61)
88.48 (0.37)
91.39 (0.94)
91.44 (1.2)
89.02 (0.52)
88.48 (0.4)
87.09 (2.75)
92.97 (1.2))
88.61 (0.35)
89.70 (0.53)
89.34 (1.48)
92.00 (1.25)
89.34 (0.4)
89.09 (0.51)
Table 4: IC accuracy on SNIPS’s AddToPlaylist and
FBDialog’s GetDirections in the FSI setting, reported
as mean (SD). A 1-layer Bi-LSTM model is used as a
feature extractor. 100∗represents 10 seed examples are
upsampled to 100 to train the feature extractor.
Is Delta-Encoder Effective On Text?
While on few-shot image classiﬁcation, Delta-
Encoder provides excellent generalization performance on unseen classes,
on text classiﬁcation, its performance is heavily
dependent on the feature extractor. We observe
that in most cases, DELTAR performs worse than
DELTAS which suggests that unlike for few-shot
image classiﬁcation, Delta-Encoder fails to learn
variations which can be applied to a different category.
In addition, in FSI with BERT encoder,
DELTAS performance is close to LINEAR. This indicates that in the low-data regime, simple subtraction between BERT sentence representations is a
good proxy to learn intra-class variations. Upsampling data in text space improves Delta-Encoder
performance for both BERT and Bi-LSTM encoders. As shown in Table 3, with upsampling in
text space, DELTAR performs better than any other
FDA method.
Qualitative Evaluation
We observe signiﬁcant accuracy improvements in
all FSI experiments for all FDA methods. Since
UPSAMPLE and PERTURB also provide signiﬁcant
gains, it seems that most of the gains come from
the fact that we are adding more data. However,
in the FSI setting, LINEAR and DELTAS method
consistently perform better than both UPSAMPLE
and PERTURB, which indicates that these methods generate more relevant data than just noise,
and redundancy. Here, we focus on visualizing
generated examples from LINEAR, DELTAS and
DELTAR methods using t-SNE.
Figure 3 shows visualizations for SNIPS’s AddToPlaylist generated sentence representations using different FDA methods. We use 10 seed examples of AddToPlaylist and use BERT as sentence
encoder. While data generated by LINEAR and
EXTRA are close to the real examples, DELTAS
and DELTAR generated examples form two different clusters. Since, Delta-Encoder performance
improves when seed examples are upsampled in
text space, we plot sentence examples from upsampled data.
Figure 4 shows that when 10 seed examples
are upsampled to 100, DELTAS cluster moves
closer to the seed examples, and while most of the
DELTAR generated data forms a separate cluster, a
few of the generated examples are close to the seed
examples. Since, in experiments with upsampled
text examples, DELTAR performs better than other
FDA methods, we hypothesize that DELTAR increases the amount of variability within the dataset
by generating diverse examples which leads to a
more robust model.
Conclusion and Future Work
In this paper, we investigate six FDA methods
including UPSAMPLE, PERTURB, CVAE, Delta-
Encoder, EXTRA, and LINEAR to augment training data. We show that FDA works better when
combined with transfer learning and provides an
effective way of bootstrapping an intent classiﬁer
for new classes. As expected, all FDA methods
become less effective when the number of seed examples increases and provides minor gains in the
full-data regime. Through comparing methods on
two public datasets, our results show that LINEAR
is a competitive baseline for FDA in FSI setting,
especially when combined with transfer learning
Additionally, we provide empirical evidence
that in few-shot integration setting, feature space
augmentation combined with BERT provides better performance than widely used text space upsampling. Given that pre-trained language models provide state of the art performance on several NLP tasks, we ﬁnd this result to be in particular encouraging, as it shows potential for applying
FDA methods to other NLP tasks.
Our experiments on Delta-Encoder also shows
that unlike few-shot image classiﬁcation, Delta-
Encoder fails to learn transferable intra-class variations. This result emphasizes that methods pro-
BookRestaurant
SearchScreeningEvent
GetWeather
Linear_aug
Extrapolation_aug
DeltaR_aug
SearchCreativeWork
DeltaS_aug
AddToPlaylist
Figure 3: 10 seed examples
BookRestaurant
SearchScreeningEvent
GetWeather
Linear_aug
Extrapolation_aug
DeltaR_aug
SearchCreativeWork
DeltaS_aug
AddToPlaylist
Figure 4: 10 seed examples are upsampled to 100
Figure 5: t-SNE visualization of different data augmentation methods for AddToPlaylist intent. BERT encoder is
used to learn sentence representations.
viding improvements in computer vision domain
might not produce similar gains on NLP tasks,
thus underlining the need to develop data augmentation methods speciﬁc to NLP tasks.