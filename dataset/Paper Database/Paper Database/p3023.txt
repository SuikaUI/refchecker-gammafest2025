This is a repository copy of Data augmentation for low resource languages.
White Rose Research Online URL for this paper:
 
Version: Published Version
Proceedings Paper:
Ragni, A. orcid.org/0000-0003-0634-4456, Knill, K.M., Rath, S.P. et al. (1 more author)
 Data augmentation for low resource languages. In: INTERSPEECH 2014 : 15th
Annual Conference of the International Speech Communication Association.
INTERSPEECH 2014 : 15th Annual Conference of the International Speech
Communication Association, 14-18 Sep 2014, Singapore. International Speech
Communication Association (ISCA) , pp. 810-814.
© 2014 International Speech Communication Association (ISCA). Reproduced in
accordance with the publisher's self-archiving policy.
 
 
Items deposited in White Rose Research Online are protected by copyright, with all rights reserved unless
indicated otherwise. They may be downloaded and/or printed for private study, or other acts as permitted by
national copyright laws. The publisher or other rights holders may allow further reproduction and re-use of
the full text version. This is indicated by the licence information on the White Rose Research Online record
for the item.
If you consider content in White Rose Research Online to be in breach of UK law, please notify us by
emailing including the URL of the record and the reason for the withdrawal request.
Data augmentation for low resource languages
Anton Ragni, Kate M. Knill, Shakti P. Rath and Mark J. F. Gales
Department of Engineering, University of Cambridge
Trumpington Street, Cambridge CB2 1PZ, UK
{ar527,kmk1001,spr38,mjfg}@eng.cam.ac.uk
Recently there has been interest in the approaches for training speech recognition systems for languages with limited resources. Under the IARPA Babel program such resources have
been provided for a range of languages to support this research
area. This paper examines a particular form of approach, data
augmentation, that can be applied to these situations. Data augmentation schemes aim to increase the quantity of data available
to train the system, for example semi-supervised training, multilingual processing, acoustic data perturbation and speech synthesis. To date the majority of work has considered individual
data augmentation schemes, with few consistent performance
contrasts or examination of whether the schemes are complementary. In this work two data augmentation schemes, semisupervised training and vocal tract length perturbation, are examined and combined on the Babel limited language pack con-
ﬁguration. Here only about 10 hours of transcribed acoustic
data are available. Two languages are examined, Assamese and
Zulu, which were found to be the most challenging of the Babel languages released for the 2014 Evaluation. For both languages consistent speech recognition performance gains can be
obtained using these augmentation schemes. Furthermore the
impact of these performance gains on a down-stream keyword
spotting task are also described.
Index Terms: data augmentation, speech recognition, babel
1. Introduction
A large amount of transcribed training data is usually needed to
enable accurate speech recognition . Although for some
languages, such as English and Mandarin, these resources may
be sourced, for others, termed low resource languages, it may
not always be feasible. This has recently created lots of interest in the approaches that can be applied to these situations
 . To facilitate research in this direction, consistent packs
of limited resources for a range of languages have been provided
under the IARPA Babel program.
The goal of the program
is to provide effective search capabilities to efﬁciently process
real-world recorded speech. This is effectively a spoken term
detection task, where speech recognition systems are assessed
based on keyword search (KWS) performance rather than more
conventional transcription accuracy. Though improvements in
This work was supported by the Intelligence Advanced Research
Projects Activity (IARPA) via Department of Defense U. S. Army Research Laboratory (DoD/ARL) contract number W911NF-12-C-0012.
The U. S. Government is authorized to reproduce and distribute reprints
for Governmental purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions contained herein are
those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U. S. Government.
speech recognition performance may not necessarily translate
into improved KWS capacity , a certain positive correlation
does exist , which motivates the work on building accurate speech recognition systems.
A common issue that arises from the use of limited resources in speech recognition systems is robust parameter estimation. A range of approaches can be applied to address robustness issues. These include standard statistical approaches,
such as maximum a posteriori (MAP) estimation , and data
augmentation . The MAP estimation introduces a prior on model parameters into training objective function . However, this approach is ill-suited in situations
when there is no training data or informative prior distribution
available. The data augmentation aims to increase the quantity
of training data. This approach has an important theoretical advantage of being able to produce data when real examples are
not available . Common schemes include semi-supervised
training , multi-lingual processing , acoustic
data perturbation and speech synthesis .
The previous work with data augmentation has mostly focused on individual schemes. Not much work has been done
on contrasting and examining whether the schemes are complimentary. This paper examines and combines semi-supervised
training and acoustic data perturbation on two languages, Assamese and Zulu, found to be the most challenging of the Babel
languages released for the 2014 Evaluation.
The rest of this paper is organised as follows. Section 2
provides an overview of commonly used data augmentation
schemes including semi-supervised training and acoustic data
perturbation. Section 3 discusses options available for training speech recognition systems on augmented data.
Section 4 provides individual and combined results on using semisupervised training and acoustic data perturbation for the two
Babel program languages, Assamese and Zulu. Finally, Section 5 presents conclusions drawn from this work.
2. Data augmentation
The data augmentation refers to the schemes that aim to increase
the quantity of data available to train speech recognition systems. The schemes can be split based on the type of produced
data, such as unsupervised, synthesised and other language data.
2.1. Unsupervised data
The unsupervised data refers to data which lacks correct transcriptions. This also includes data having only rough transcriptions, such as closed captions . The unsupervised data may
be adopted by recognising it with an existing or boot-strapped
system, ﬁltering out those utterances that fail to decode/pass
conﬁdence threshold and re-training the system on su-
Copyright  2014 ISCA
14-18 September 2014, Singapore
INTERSPEECH 2014
pervised and ﬁltered unsupervised training data. This is commonly referred to as a semi-supervised training. The main advantage of unsupervised data is that it is generally possible to
collect vast amounts of such data, e.g., radio and television
news broadcasts , covering all sorts of speaker and noise
conditions. The main disadvantage of this type of data is the
lack of correct transcriptions. This limits possible gains from
the approaches particularly sensitive to the accuracy of supplied
transcriptions, such as discriminative training and speaker
adaptation based on discriminative criteria .
2.2. Synthesised data
The synthesised data may refer to existing but perturbed in a
certain way data as well as new artiﬁcially generated data. One
major advantage of synthesised data is that, similar to unsupervised case, it is possible to collect vast amounts of such data.
Furthermore, different to unsupervised case, the correctness of
associated transcriptions is usually guaranteed. A major disadvantage of this type of data could be its quality.
There are numerous options how data can be perturbed.
These include vocal tract length perturbation (VTLP) 
and stochastic feature mapping (SFM) . The VTLP scheme
attempts to alter vocal tract length during extraction of standard speech parametrisations such as Mel-frequency cepstral
(MFCC) and perceptual linear prediction (PLP) coefﬁcients.
Essentially, a single warping parameter is modiﬁed either
stochastically or deterministically . This results in a simple synthesis process yet the data is perturbed in a non-linear
way. Though the original motivation for VTLP was to learn
multi-layer perceptrons (MLP) robust to changes in vocal tract
length , the scheme could be of a wider interest, for
instance, to boot-strap systems used for recognising unsupervised data. In contrast to VTLP, the SFM is a general methodology for stochastically mapping features from one domain to
another . When applied to speakers, the SFM essentially
yields a simpliﬁed voice morphing scheme. One simple approach to map utterances of one speaker to another is to apply
global constrained maximum likelihood regression (CMLLR)
transform estimated from statistics of the other speaker
 . The issue with this approach is that a simple global transform is applied to every observation in the sequence which may
not be powerful enough to yield accurate mapping.
Rather than perturbing existing data it is possible to
artiﬁcially generate new examples using speech synthesis approaches, such as concatenative or statistical . The concatenative approach attempts to synthesise speech by concatenating
existing waveform segments into a sequence. The statistical approach usually adopts acoustic models, such as hidden Markov
models (HMM), to produce speech parameter sequences maximising likelihood . These model-based schemes may be
particularly useful since speech parameter sequences, such as
MFCC or PLP, rather than waveforms are required.
many of the waveform production issues are not relevant. Furthermore, these schemes permit model-based adaptation/compensation approaches to be used for synthesising data with target and new speaker and environment characteristics. In contrast to acoustic data perturbation
schemes, the use of speech synthesis offers ﬂexibility in generating data for arbitrary given transcription. For instance, it is
possible to generate data for targeting only particular confusions
using schemes such as acoustic code-breaking .
Hidden Layers
Bottleneck
Input Layer
Figure 1: Schematic diagram of a tandem approach
2.3. Other language data
Though for many languages there are only limited or no resources, for some languages sufﬁcient resources are available.
This has prompted lots of interest in using this type of augmenting data . Furthermore, the use of unsupervised other
language data has also been considered . Compared to synthesised data, this type of augmenting data, similar to unsupervised data, is real. However, its use may also be more complicated as it is not obvious what is the best way to exploit it .
There have been proposed several approaches. One group relies
on the use of a universal phone set to accomplish mapping of
one language to another . Another group relies on an alternative form of mapping, such as phone-to-phone or hidden layer unit-to-target as in the MLP-based work of .
Both directions have their own advantages and disadvantages
In particular, it is not obvious how to ensure sufﬁcient
coverage in the training data for approaches based on universal
phone sets, map phones between languages such as English and
Cantonese in approaches based on phone-to-phone mappings or
ensure that targets are optimally ordered in approaches that map
hidden layer units to targets.
3. Augmentation modes
Given augmenting data, an important question is how to best
exploit it. The answer to this question will ultimately depend
on the particular architecture the speech recogniser adopts and
the nature and amount of augmenting data used. There are numerous conﬁgurations possible, such as standard Gaussian mixture model (GMM) based HMM , tandem , hybrid ,
stacked versions of the tandem and hybrid architectures. This section considers several of these, putting a particular emphasis on the tandem architecture adopted in Section 4.
The tandem architecture may be illustrated by Figure 1
which shows a MLP and GMM-based HMM speech recogniser.
Three types of MLP layers are shown:
input, hidden and bottleneck.
The standard parametrisation, such as
MFCC or PLP, optionally de-correlated and transformed, is
fed into the input layer which is followed by hidden layers
where it undergoes a series of non-linear transformations until it reaches bottleneck layer where MLP-derived features are
extracted. These features, also called bottleneck (BN) features,
are then concatenated with the standard parametrisation, optionally de-correlated and transformed, and used within the standard
GMM-based HMM speech recogniser. The hybrid architecture
may also be illustrated by Figure 1, although in this case the
dashed part is not present and the bottleneck layer is replaced
with an extra hidden layer. The posterior probabilities of tar-
gets at the ﬁnal layer after proper scaling are adopted in place
of GMM likelihoods within the standard HMM speech recogniser . The stacked architectures are based on replacing
the dashed block in Figure 1 by another MLP of tandem or
hybrid conﬁguration. Though all these architectures are based
on MLPs, the ﬁnal speech recognisers often show different error behaviours. This is where system combination approaches
 may yield further gains in transcription accuracy.
Such MLP-based architectures offer ﬂexibility into the use
of augmenting data. For instance, there are options how it can
be exploited in the tandem architecture. One option is to only
re-train the GMM whilst keeping MLP parameters ﬁxed to the
estimates obtained from the supervised data. Another option is
to only re-train the MLP. The third option is to re-train both, the
GMM and MLP. For hybrid architectures it is common to retrain the whole system on the augmented data , although it
is possible to consider ﬁne-tuning on the supervised data only.
The stacked architectures offer more ﬂexibility though for simplicity they were not investigated in this paper.
In addition to architecture, the optimal approach will also
depend on the nature and amount of the particular data used. For
instance, it is not obvious which parts are best kept unilingual
and which are better to train multi-lingual in case of augmenting
data from other languages. Also, there is a clear limit to the
usefulness of schemes such as VTLP. Furthermore, some of the
augmenting data types may not combine well in practice.
4. Experiments
Experiments were conducted on two limited language packs released by IARPA Babel program: Assamese and Zulu.1 The
data is recorded in real conditions, such as conversational telephone speech in a range of acoustic conditions.
also provided phone set and phonetic lexicon, which contains
only words that appear in the supervised training data transcriptions. The amount of supervised data is 12 and 14 hours for
Assamese and Zulu respectively. The underlying transcriptions
were used to create a bigram language model (LM) for discriminative training and trigram LM for decoding. The development sets for both tasks contain approximately 10 hours of data.
The experiments were conducted using an extended version of
CUED’s HTK-3.4.1 toolkit providing GMM-based HMM
speech recognition techniques, an extended version of ICSI’s
QuickNet toolkit providing MLP techniques and IBM’s
proprietary KWS system for keyword searching.
4.1. Speech recognition system
The tandem architecture was selected for investigation. A consistent procedure was used to create tandem systems.
largely followed and consists of three stages. In the ﬁrst
stage a speaker-independent GMM-based HMM is built based
on PLP. This applies maximum likelihood (ML) training, heteroscedastic linear discriminant analysis (HLDA) and discriminative, minimum phone error (MPE) , training. The HMM
states were phonetic decision tree clustered into 1000 unique
states following the procedure in . The ﬁrst stage system
was used to produce hypotheses for adaptation by running a
Viterbi decoding with the trigram LM over the development
sets. The second stage is the MLP training. A simple MLP
topology with 3 hidden, 1 input and 1 bottleneck layer was
1The precise code identiﬁers are IARPA-babel102b-v0.5a and
IARPA-babel206b-v0.1e. These releases additionally contain full language packs, where the amount of transcribed data is roughly 70 hours.
adopted. The targets were set to 1000 unique states derived for
the ﬁrst stage system. The input to MLP was a 504-dimensional
stack of 4 past, 1 current and 4 future vectors, where each vector was a 13-dimensional PLP feature vector augmented with
pitch , its delta (∆), delta-delta (∆2) and triples (∆3). The
MLP was pre-trained layer-wise and ﬁne-tuned using a crossentropy criterion . The ﬁrst stage system was used to provide targets. This MLP was used to provide 26-dimensional
BN features for training and development data. These BN features were concatenated with 52-dimensional PLP+∆+∆2+∆3
and 3-dimensional pitch+∆+∆features. The third stage is the
tandem build. This stage (re-)estimates HLDA transform for
PLP and global semitied transform for BN. This reduces
dimensionality of tandem features from 81 to 68. The SI tandem system is then built similar to the ﬁrst stage SI system.
In addition to SI, the third stage also performs CMLLR-based
speaker adaptive training (SAT) ﬁrst using ML and then
feature-space MPE (fMPE-SAT) criterion . The CMLLR
transforms during fMPE-SAT were ﬁxed to ML estimates and
not re-estimated. This ﬁnal system was used for decoding. Prior
to this, CMLLR and MLLR transforms were estimated using
initial hypotheses produced by the ﬁrst stage system. These
transforms were then used in Viterbi decoding with the bigram
LM to produce lattices. Though these lattices could be rescored
with more advanced LMs, this was not done in this initial investigation. The accuracy of speech recogniser was assessed based
on token error rate (TER) in percentage points (%).2 The TER
performance of this system on Assamese and Zulu was 69.4 and
78.4%, as shown in the ﬁrst row of Tables 3 and 2.
4.2. Keyword search system
For the 2014 Evaluation the IARPA Babel program required
each submitted system to be assessed in keyword search capacity. The task was to ﬁnd all the exact matches of a query
in the development set. The KWS performance is measured
according to the maximum term weighted value (MTWV), a
metric that takes into account the probabilities of misses and
false alarms with larger MTWV values corresponding to better KWS performance. The supplied queries were split into invocabulary (IV) and out-of-vocabulary (OOV) parts. For the IV
queries the set of word lattices is searched to retrieve the
list of hits. For the OOV queries a different approach is used.
This operates at the phone level by converting OOV queries
into phonetic representation using a grapheme-to-phoneme converter . A soft search, which may improve recall whilst degrade precision, is then performed by expanding the obtained
phonetic OOV query representation using phone-to-phone confusion matrix . Only 100 representations with the highest
score were retained. Furthermore, the language model scores
during search were zeroed as this was found to improve KWS
performance. The same approach was also adopted with those
IV queries that produced no hits (IV-OOV). The combined list
of hits including IV, OOV and IV-OOV parts after sum-to-one
normalisation is used to compute MTWV.
4.3. Data augmentation
Two data augmentation schemes, semi-supervised (semi)
training and VTLP (vtlp), were considered. The unsupervised
2The TER is used for consistency of reporting performance for all
Babel program languages, such as Assamese and Zulu, where token is
a word, or Vietnamese, where token is a syllable or foreign word, or
Cantonese, where token is a character.
data is provided by the limited language pack release and conversational portion of the full language pack. The unsupervised
data was selected as described in Section 2.1. Here, the tandem system was used to produce lattices. These lattices were
then converted into confusion networks to yield word confusion
scores . The confusion scores were weighted by the average number of frames to yield the ﬁnal score for data selection.
The data selection process followed and retained half of
the unsupervised data. This corresponds to the threshold of 0.4
and 0.3 for Assamese and Zulu respectively as can be seen from
Figure 2. The perturbed data, sup+vtlp and semi+vtlp,
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Confidence threshold
Figure 2: Percentage of unsupervised data retained for semisupervised training at different conﬁdence threshold values
was obtained as discussed in Section 2.2. Here, the original
data, sup and semi, was perturbed 4 and 8 times for Assamese
and Zulu respectively as the former was found sensitive to the
larger amount of perturbed data. One perturbation factor was
ﬁxed to 1, which yields the original data, and the rest, 3 and
7, were randomly sampled from [0.8, 1.2] range for each side.
The perturbed semi-supervised (semi+vtlp) data was created
in the same way. This provides with an approach to increase the
amount of unsupervised data where conﬁdence in the accuracy
of the underlying transcriptions is above 0.4 and 0.3 thresholds.
The amounts of supervised (sup) and other types of augmenting data are summarised in Table 1.
Table 1: Augmenting data quantity in hours
4.4. Results
A range of experiments was conducted to assess the usefulness of augmenting data starting with the more challenging
language, Zulu. The ﬁrst experiment examined the impact of
re-training MLP only. The ﬁrst block of TER results in Ta-
Table 2: Zulu
ble 2 third column shows that the use of augmenting data yields
gains over the supervised data. In particular, the combined approach, (semi+vtlp, the fourth line) yields the largest 1.7%
absolute improvement. The next experiment assessed whether
increasing the complexity of GMM system may further improve
the results.
The tandem system was retrained on the semisupervised data starting from the ﬁrst stage. The number of
unique states was increased to 3000. This yields 76.9% TER
performance as shown on the ﬁrst line of the second block in
Table 2. The use of additional data for training MLP in this case
gives small improvement. Re-training the tandem on perturbed
semi-supervised data with 5000 unique states yields no additional improvement in TER performance (last line in Table 2).
The second series of experiments was conducted on Assamese.
The results in Table 3 show a pattern similar to that of Zulu
apart from a rather limited usefulness of perturbed compared to
unsupervised data.
Table 3: Assamese
Although the above results indicate that data augmentation
schemes may be useful for improving TER performance, the
ultimate measure of interest in the IARPA Babel program is
MTWV. The KWS results in the fourth column of Tables 2 and
3 show that consistent gains in MTWV are also possible. These
results also illustrate that improvements in TER do not necessarily translate into improvements in MTWV . For both
languages the best MTWV is obtained with the GMM trained
on supervised data and MLP trained on perturbed supervised
data. The use of standard and perturbed semi-supervised data
for training MLP yields a slightly lower MTWV. However, in
this case re-training GMM on the semi-supervised data may
hurt performance. This indicates that the approach is also sensitive to the accuracy of training transcriptions.
5. Conclusions
Providing accurate speech recognition and keyword searching
capabilities for low resource languages is a challenging task.
This paper examined an approach, data augmentation, that aims
to increase the quantity of available data. Particular schemes
discussed were semi-supervised training, acoustic data perturbation, speech synthesis and multi-lingual processing.
paper also discussed various ways to exploit this data in tandem and hybrid architectures. Two of these schemes, semisupervised training and acoustic data perturbation, individually and in combination were applied within the tandem architecture for two low resource languages, Assamese and Zulu.
Speech recognition performance gains were observed from the
use of both scheme, with the combined scheme yielding largest
gain only for Zulu. Keyword search results showed that gains
are also possible, however, the use of semi-supervised training
yielded mixed results in this case suggesting sensitivity of the
approach to the accuracy of training data transcriptions.
6. Acknowledgements
The authors are grateful to IBM for the KWS system.
7. References
 G. Evermann, H. Y. Chan, M. J. F. Gales, T. Hain, A. Liu,
D. Mrva, L. Wang, and P. C. Woodland, “Development of the 2003
CU-HTK conversational telephone speech transcription system,”
in ICASSP, vol. 1, 2004, pp. 249–252.
 M. J. F. Gales, D. Y. Kim, P. C. Woodland, H. Y. Chan, D. Mrva,
R. Sinha, and S. E. Tranter, “Progress in the CU-HTK broadcast
news transcription system,” IEEE Tran ASLP, vol. 14, no. 5, pp.
1513–1525, 2006.
 K. M. Knill, M. J. F. Gales, S. P. Rath, P. C. Woodland, C. Zhang,
and S.-X. Zhang, “Investigation of multilingual deep neural networks for spoken term detection,” in ASRU, 2013.
 R. Hsiao, T. Ng, F. Grezl, D. Karakos, S. Tsakalidis, L. Nguyen,
and R. Schwartz, “Discriminative semi-supervised training for
keyword search in low resource languages,” in ASRU, 2013, pp.
 M. Saraclar, A. Sethy, B. Ramabhadran, L. Mangu, X. Cui,
B. Kingsbury, and J. Mamou, “An empirical study of confusion modeling in keyword search for low resource languages,” in
ASRU, 2013, pp. 464–469.
 J.-L. Gauvain and C.-H. Lee, “Maximum a posteriori estimation
of multivariate gaussian mixture observations of markov chains,”
IEEE Tran SAP, vol. 2, no. 2, pp. 291–298, 1994.
 L. Lamel and J.-L. Gauvain, “Lightly supervised and unsupervised acoustic model training,” Computer speech and language,
vol. 16, pp. 115–129, 2002.
 M. J. F. Gales, A. Ragni, H. AlDamarki, and C. Gautier, “Support
vector machines for noise robust ASR,” in ASRU, 2009, pp. 205–
 N. Jaitly and G. E. Hinton, “Vocal tract length perturbation
(VTLP) improves speech recognition,” in ICML, 2013.
 N. Kanda, R. Takeda, and Y. Obuchi, “Elastic spectral distortion
for low resource speech recognition with deep neural networks,”
in ASRU, 2013, pp. 309–314.
 Y. Qian, K. Yu, and J. Liu, “Combination of data borrowing strategies for low-resource LVCSR,” in ASRU, 2013, pp. 404–409.
 X. Cui, V. Goel, and B. Kingsbury, “Data augmentation for deep
neural network acoustic modeling,” in ICASSP, 2014.
 D. Povey, M. J. F. Gales, D. Y. Kim, and P. C. Woodland, “MMI-
MAP and MPE-MAP for acoustic model adaptation,” in Eurospeech, 2003, pp. 1981–1984.
 Z. T¨uske, J. Pinto, D. Wilett, and R. Schl¨uter, “Investigation on
cross- and multilingual MLP features under matched and mismatched acoustical conditions,” in ICASSP, 2006, pp. 7349–7353.
 G. Zavaliagkos and T. Colthurst, “Utilizing untranscribed training
data to improve performance,” in BNTU, 1998, pp. 301–305.
 G. Evermann and P. C. Woodland, “Large vocabulary decoding
and conﬁdence estimation using word posterior probabilities,” in
ICASSP, vol. 3, 2000, pp. 1655–1658.
 L. Wang, M. J. F. Gales, and P. C. Woodland, “Unsupervised training for Mandarin broadcast news and conversation transcription,”
in ICASSP, vol. 4, 2007, pp. 353–356.
 L. Wang and P. C. Woodland, “Discriminative adaptive training
using the MPE criterion,” in ASRU, 2003, pp. 279–284.
 H. Zen, Y. Nankaku, and K. Tokuda, “Continuous stochastic feature mapping based on trajectory HMMs,” IEEE Tran ASLP, pp.
417–430, 2010.
 H. Ye and S. J. Young, “High quality voice morphing,” in ICASSP,
vol. 1, 2013, pp. 9–12.
 M. J. F. Gales, “Maximum likelihood linear transformations for
HMM-based speech recognition,” Computer Speech and Language, vol. 12, no. 2, pp. 75–98, 1998.
 K. Tokuda, H. Zen, and A. W. Black, “An HMM-based approach
to multilingual speech synthesis,” in Text to speech synthesis:
new paradigms and advances, S. Narayanan and A. Alwan, Eds.
Prentice Hall, 2004, ch. 7, pp. 135–153.
 H. Zen, K. Tokuda, and A. W. Black, “Statistical parametric
speech synthesis,” Speech Communication, vol. 51, pp. 1039–
1064, 2009.
 M. J. F. Gales, “Model-based approaches to handling uncertainty,”
in Robust speech recognition of uncertain or missing data. Theory and applications, D. Kolossa and R. Haeb-Umbach, Eds.
Springer, 2011, ch. 5, pp. 101–126.
 K. Ogata, M. Tachibana, J. Yamagishi, and T. Kobayashi, “Acoustic model training based on linear transformation and MAP modi-
ﬁcation for HSMM-based speech synthesis,” in Interspeech, 2006,
pp. 1328–1331.
 T. Yoshimura, K. Tokuda, T. Masuko, and T. Kobayashi, T. Kitamura, “Speaker interpolation in HMM-based speech synthesis
system,” in Eurospeech, 1997, pp. 2523–2526.
 V. Venkataramani and W. Byrne, “Lattice segmentation and support vector machines for large vocabulary continuous speech
recognition,” in ICASSP, 2005, pp. 817–820.
 T. Schultz and A. Waibel, “Fast bootstrapping of LVCSR systems
with multilingual phoneme sets,” in Eurospeech, 1997, pp. 371–
 P. Beyerlein, W. Byrne, J. M. Huerta, S. Khudanpur, B. Marthi,
J. Morgan, N. Peterek, J. Picone, and W. Wang, “Towards language independent acoustic modellings,” in ASRU, 1999.
 K. Vesely, M. Karaﬁat, F. Grezl, M. Janda, and E. Egorova,
“The language-independent bottleneck features,” in SLT, 2012,
pp. 336–341.
 S. J. Young, G. Evermann, M. J. F. Gales, T. Hain, D. Kershaw,
X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev,
and P. C. Woodland, The HTK Book (for HTK Version 3.4.1).
 University of Cambridge, 2009.
 H. Hermansky, D. P. W. Ellis, and S. Sharma, “Tandem connectionist feature extraction for conventional HMM systems,” in
ICASSP, vol. 3, 2000, pp. 1635–1638.
 G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, “Deep neural networks for acoustic modeling in speech
recognition,” Sig Proc Mag, vol. 29, pp. 82–97, 2012.
 C. Plahl, R. Schl¨uter, and H. Ney, “Hierarchical bottle neck features for LVCSR,” in Interspeech, 2010, pp. 1197–1200.
 J. Fiscus, “A post-processing system to yield reduced word error
rates: Recog- niser output voting error reduction (ROVER),” in
ASRU, 1997, pp. 347–352.
 D. Johnson, “Quicknet,” in ICSI, Berkeley, USA, 2004.
 J. Mamou, J. Cui, X. Cui, M. J. F. Gales, B. Kingsbury, K. M.
Knill, L. Mangu, D. Nolden, M. Picheny, B. Ramabhadran,
R. Schl¨uter, A. Sethy, and P. C. Woodland, “System combination
and score normalization for spoken term detection,” in ICASSP,
2013, pp. 8272–8276.
 J. Park, F. Diehl, M. J. F. Gales, M. Tomalin, and P. C. Woodland, “The efﬁcient incorporation of MLP features into automatic
speech recognition systems,” Computer speech and language,
vol. 25, pp. 519–534, 2011.
 D. Povey, “Discriminative training for large vocabulary speech
recognition,” Ph.D. dissertation, Cambridge University, 2003.
 D. Talkin, “A robust algorithm for pitch tracking (RAPT),” in
Speech Coding and Synthesis, W. B. Kleijn and K. K. Paliwal,
Elsevier Science B. V., 1995, ch. 14, pp. 495–518.
 M. J. F. Gales, “Semi-tied covariance matrices for hidden Markov
models,” IEEE Tran SAP, vol. 29, pp. 82–97, 2012.
 D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau, and
G. Zweig, “fMPE: Discriminatively trained features for speech
recognition,” in ICASSP, vol. 1, 2005, pp. 961–964.
 C. Allauzen, M. Mohri, and M. Saraclar, “General indexation of
weighted automata - application to spoken utterance retrieval,” in
IASIR, 2004, pp. 33–40.
 L. Mangu, H. Soltau, H.-K. Kuo, B. Kingsbury, and G. Saon, “Exploiting diversity for spoken term detection,” in ICASSP, 2013,
pp. 8282–8286.