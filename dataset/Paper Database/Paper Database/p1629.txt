Learning Fine-grained Image Similarity with Deep Ranking
Jiang Wang1∗
Yang Song2
Thomas Leung2
Chuck Rosenberg2
Jingbin Wang2
James Philbin2
1Northwestern University
2Google Inc.
3California Institute of Technology
jwa368, 
yangsong,leungt,chuck,jingbinw, 
 
Learning ﬁne-grained image similarity is a challenging
task. It needs to capture between-class and within-class
image differences.
This paper proposes a deep ranking
model that employs deep learning techniques to learn similarity metric directly from images. It has higher learning
capability than models based on hand-crafted features. A
novel multiscale network structure has been developed to
describe the images effectively.
An efﬁcient triplet sampling algorithm is proposed to learn the model with distributed asynchronized stochastic gradient. Extensive experiments show that the proposed algorithm outperforms
models based on hand-crafted visual features and deep
classiﬁcation models.
1. Introduction
Search-by-example, i.e. ﬁnding images that are similar
to a query image, is an indispensable function for modern
image search engines. An effective image similarity metric
is at the core of ﬁnding similar images.
similarity
category-level image similarity. For example, in ,
two images are considered similar as long as they belong
to the same category. This category-level image similarity
is not sufﬁcient for the search-by-example image search
application. Search-by-example requires the distinction of
differences between images within the same category, i.e.,
ﬁne-grained image similarity.
One way to build image similarity models is to ﬁrst extract features like Gabor ﬁlters, SIFT and HOG ,
and then learn the image similarity models on top of these
features . The performance of these methods is
largely limited by the representation power of the handcrafted features. Our extensive evaluation has veriﬁed that
being able to jointly learn the features and similarity models
∗The work was performed while Jiang Wang and Bo Chen interned at
Figure 1. Sample images from the triplet dataset. Each column
is a triplet.
The upper, middle and lower rows correspond to
query image, positive image, and negative image, where the positive image is more similar to the query image that the negative
image, according to the human raters. The data are available at
 
with supervised similarity information provides great potential for more effective ﬁne-grained image similarity models than hand-crafted features.
Deep learning models have achieved great success on
image classiﬁcation tasks .
However, similar image
ranking is different from image classiﬁcation. For image
classiﬁcation, “black car”, “white car” and “dark-gray car”
are all cars, while for similar image ranking, if a query image is a “black car”, we usually want to rank the “dark gray
car” higher than the “white car”. We postulate that image
classiﬁcation models may not ﬁt directly to task of distinguishing ﬁne-grained image similarity. This hypothesis is
veriﬁed in experiments. In this paper, we propose to learn
ﬁne-grained image similarity with a deep ranking model,
which characterizes the ﬁne-grained image similarity relationship with a set of triplets. A triplet contains a query
image, a positive image, and a negative image, where the
positive image is more similar to the query image than the
negative image (see Fig. 1 for an illustration). The image
similarity relationship is characterized by relative similarity ordering in the triplets. Deep ranking models can employ this ﬁne-grained image similarity information, which
2014 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/14 $31.00 © 2014 IEEE
DOI 10.1109/CVPR.2014.180
2014 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/14 $31.00 © 2014 IEEE
DOI 10.1109/CVPR.2014.180
is not considered in category-level image similarity models
or classiﬁcation models, to achieve better performance.
As with most machine learning problems, training data
is critical for learning ﬁne-grained image similarity. It is
challenging to collect large data sets, which is required for
training deep networks. We propose a novel bootstrapping
method (section 6.1) to generate training data, which can
virtually generate unlimited amount of training data. To
use the data efﬁciently, an online triplet sampling algorithm is proposed to generate meaningful and discriminative
triplets, and to utilize asynchronized stochastic gradient algorithm in optimizing triplet-based ranking function.
The impact of different network structures on similar image ranking is explored. Due to the intrinsic difference between image classiﬁcation and similar image ranking tasks,
a good network for image classiﬁcation ( ) may not
be optimal for distinguishing ﬁne-grained image similarity.
A novel multiscale network structure has been developed,
which contains the convolutional neural network with two
low resolution paths. It is shown that this multi-scale network structure can work effectively for similar image ranking.
The image similarity models are evaluated on a humanlabeled dataset. Since it is error-prone for human labelers
to directly label the image ranking which may consist tens
of images, we label the similarity relationship of the images
with triplets, illustrated in Fig. 1. The performance of an
image similarity model is determined by the fraction of the
triplet orderings that agrees with the ranking of the model.
To our knowledge, it is the ﬁrst high quality dataset with
similarity ranking information for images from the same
category. We compare the proposed deep ranking model
with state-of-the-art methods on this dataset. The experiments show that the deep ranking model outperforms the
hand-crafted visual feature-based approaches 
and deep classiﬁcation models by a large margin.
The main contributions of this paper includes the following. (1) A novel deep ranking model that can learn ﬁnegrained image similarity model directly from images is proposed. We also propose a new bootstrapping way to generate the training data. (2) A multi-scale network structure
has been developed. (3) A computationally efﬁcient online
triplet sampling algorithm is proposed, which is essential
for learning deep ranking models with online learning algorithms. (4) We are publishing an evaluation dataset. To
our knowledge, it is the ﬁrst public data set with similarity ranking information for images from the same category
2. Related Work
Most prior work on image similarity learning 
studies the category-level image similarity, where two images are considered similar as long as they belong to the
same category. Existing deep learning models for image
similarity also focus on learning category-level image similarity . Category-level image similarity mainly corresponds to semantic similarity. studies the relationship
between visual similarity and semantic similarity. It shows
that although visual and semantic similarities are generally consistent with each other across different categories,
there still exists considerable visual variability within a category, especially when the category’s semantic scope is
large. Thus, it is worthwhile to learn a ﬁne-grained model
that is capable of characterizing the ﬁne-grained visual similarity for the images within the same category.
The following works are close to our work in the spirit
of learning ﬁne-grained image similarity.
Relative attribute learns image attribute ranking among the images with the same attributes. OASIS and local distance learning learn ﬁne-grained image similarity ranking models on top of the hand-crafted features. These above
works are not deep learning based. employs deep learning architecture to learn ranking model, but it learns deep
network from the “hand-crafted features” rather than directly from the pixels. In this paper, we propose a Deep
Ranking model, which integrates the deep learning techniques and ﬁne-grained ranking model to learn ﬁne-grained
image similarity ranking model directly from images. The
Deep Ranking models perform much better than categorylevel image similarity models in image retrieval applications.
Pairwise ranking model is a widely used learning-to-rank
formulation. It is used to learn image ranking models in
 . Generating good triplet samples is a crucial
aspect of learning pairwise ranking model. In and ,
the triplet sampling algorithms assume that we can load the
whole dataset into memory, which is impractical for a large
dataset. We design a computationally efﬁcient online triplet
sampling algorithm that does not require loading the whole
dataset into memory, which makes it possible to learn deep
ranking models with very large amount of training data.
3. Overview
Our goal is to learn image similarity models. We deﬁne
the similarity of two images P and Q according to their
squared Euclidean distance in the image embedding space:
D(f(P), f(Q)) = ∥f(P) −f(Q)∥2
where f(.) is the image embedding function that maps an
image to a point in an Euclidean space, and D(., .) is the
squared Euclidean distance in this space. The smaller the
distance D(P, Q) is, the more similar the two images P and
Q are. This deﬁnition formulates the similar image ranking
problem as nearest neighbor search problem in Euclidean
space, which can be efﬁciently solved via approximate nearest neighbor search algorithms.
We employ the pairwise ranking model to learn image
similarity ranking models, partially motivated by .
Suppose we have a set of images P, and ri,j = r(pi, pj)
is a pairwise relevance score which states how similar the
image pi ∈P and pj ∈P are. The more similar two images
are, the higher their relevance score is. Our goal is to learn
an embedding function f(.) that assigns smaller distance to
more similar image pairs, which can be expressed as:
D(f(pi), f(p+
i )) < D(f(pi), f(p−
i such that r(pi, p+
i ) > r(pi, p−
We call ti = (pi, p+
i ) a triplet, where pi, p+
query image, positive image, and negative image, respectively. A triplet characterizes a relative similarity ranking
order for the images pi, p+
i . We can deﬁne the following hinge loss for a triplet: ti = (pi, p+
max{0, g + D(f(pi), f(p+
i )) −D(f(pi), f(p−
where g is a gap parameter that regularizes the gap between
the distance of the two image pairs: (pi, p+
i ) and (pi, p−
The hinge loss is a convex approximation to the 0-1 ranking error loss, which measures the model’s violation of the
ranking order speciﬁed in the triplet. Our objective function
ξi + λ∥W ∥2
s.t. : max{0, g + D(f(pi), f(p+
i )) −D(f(pi), f(p−
i such that r(pi, p+
i ) > r(pi, p−
where λ is a regularization parameter that controls the margin of the learned ranker to improve its generalization. W
is the parameters of the embedding function f(.). We employ λ = 0.001 in this paper. (4) can be converted to an
unconstrained optimization by replacing ξi = max{0, g +
D(f(pi), f(p+
i )) −D(f(pi), f(p−
In this model, the most crucial component is to learn an
image embedding function f(.). Traditional methods typically employ hand-crafted visual features, and learn linear
or nonlinear transformations to obtain the image embedding function. In this paper, we employ the deep learning
technique to learn image similarity models directly from
images. We will describe the network architecture of the
triple-based ranking loss function in (4) and an efﬁcient optimization algorithm to minimize this objective function in
the following sections.
4. Network Architecture
A triplet-based network architecture is proposed for the
ranking loss function (4), illustrated in Fig. 2. This net-
Triplet Sampling Layer
Ranking Layer
Figure 2. The network architecture of deep ranking model.
work takes image triplets as input. One image triplet contains a query image pi, a positive image p+
i and a negative
i , which are fed independently into three identical deep neural networks f(.) with shared architecture and
parameters. A triplet characterizes the relative similarity relationship for the three images. The deep neural network
f(.) computes the embedding of an image pi: f(pi) ∈Rd,
where d is the dimension of the feature embedding.
A ranking layer on the top evaluates the hinge loss (3)
of a triplet. The ranking layer does not have any parameter. During learning, it evaluates the model’s violation of
the ranking order, and back-propagates the gradients to the
lower layers so that the lower layers can adjust their parameters to minimize the ranking loss (3).
We design a novel multiscale deep neural network architecture that employs different levels of invariance at different scales, inspired by , shown in Fig. 3. The ConvNet
in this ﬁgure has the same architecture as the convolutional
deep neural network in . The ConvNet encodes strong
invariance and captures the image semantics. The other two
parts of the network takes down-sampled images and use
shallower network architecture. Those two parts have less
invariance and capture the visual appearance. Finally, we
normalize the embeddings from the three parts, and combine them with a linear embedding layer. In this paper, The
dimension of the embedding is 4096.
We start with a convolutional network (ConvNet) architecture for each individual network, motivated by the recent
success of ConvNet in terms of scalability and generalizability for image classiﬁcation . The ConvNet contains
stacked convolutional layers, max-pooling layer, local normalization layers and fully-connected layers. The readers
can refer to or the supplemental materials for more details.
A convolutional layer takes an image or the feature maps
of another layer as input, convolves it with a set of k learnable kernels, and puts through the activation function to
Convolution
Convolution
Max pooling
Max pooling
8 x 8 x 96
l2 Normalization
Linear Embedding
Normalization
8 x 8: 4x4
8 x 8: 4x4
3 x 3: 2x2
7 x 7: 4x4
15 x 15 x 96
4 x 4 x 96
4 x 4 x 96
l2 Normalization
Figure 3. The multiscale network structure. Ech input image goes
through three paths. The top green box (ConvNet) has the same
architecture as the deep convolutional neural network in . The
bottom parts are two low-resolution paths that extracts low resolution visual features. Finally, we normalize the features from both
parts, and use a linear embedding to combine them. The number
shown on the top of a arrow is the size of the output image or
feature. The number shown on the top of a box is the size of the
kernels for the corresponding layer.
generate k feature maps. The convolutional layer can be
considered as a set of local feature detectors.
A max pooling layer performs max pooling over a local
neighborhood around a pixel. The max pooling layer makes
the feature maps robust to small translations.
A local normalization layer normalizes the feature map
around a local neighborhood to have unit norm and zero
mean. It leads to feature maps that are robust to the differences in illumination and contrast.
The stacked convolutional layers, max-pooling layer and
local normalization layers act as translational and contrast
robust local feature detectors. A fully connected layer computes a non-linear transformation from the feature maps of
these local feature detectors.
Although ConvNet achieves very good performance for
image classiﬁcation, the strong invariance encoded in its architecture can be harmful for ﬁne-grained image similarity
tasks. The experiments show that the multiscale network architecture outperforms single scale ConvNet in ﬁne-grained
image similarity task.
5. Optimization
Training a deep neural network usually needs a large
amount of training data, which may not ﬁt into the memory of a single computer. Thus, we employ the distributed
asynchronized stochastic gradient algorithm proposed in 
with momentum algorithm .
The momentum algorithm is a stochastic variant of Nesterov’s accelerated gradient method , which converges faster than traditional
stochastic gradient methods.
Back-propagation scheme is used to compute the gradient. A deep network can be represented as the composition
of the functions of each layer.
f(.) = gn(gn−1(gn−2(· · · g1(.) · · · )))
where gl(.) is the forward transfer function of the l-th layer.
The parameters of the transfer function gl is denoted as wl.
Then the gradient ∂f(.)
∂wl can be written as:
can be efﬁciently computed in an iterative way:
∂gl+1 × ∂gl+1(.)
. Thus, we only need to compute the gradients
∂gl−1 for the function gl(.). More details of
the optimization can be found in the supplemental materials.
To avoid overﬁtting, dropout with keeping probability 0.6 is applied to all the fully connected layers. Random
pixel shift is applied to the input images for data augmentation.
5.1. Triplet Sampling
To avoid overﬁtting, it is desirable to utilize a large variety of images. However, the number of possible triplets
increases cubically with the number of images. It is computationally prohibitive and sub-optimal to use all the triplets.
For example, the training dataset in this paper contains 12
million images. The number of all possible triplets in this
dataset is approximately (1.2×107)3 = 1.728×1021. This
is an extermely large number that can not be enumerated.
If the proposed triplet sampling algorithm is employed, we
ﬁnd the optimization converges with about 24 million triplet
samples, which is a lot smaller than the number of possible
triplets in our dataset.
It is crucial to choose an effective triplet sampling strategy to select the most important triplets for rank learning.
Uniformly sampling of the triplets is sub-optimal, because
we are more interested in the top-ranked results returned by
the ranking model. In this paper, we employ an online importance sampling scheme to sample triplets.
Suppose we have a set of images P, and their pairwise
relevance scores ri,j = r(pi, pj). Each image pi belongs to
a category, denoted by ci. Let the total relevance score of
an image ri deﬁned as
j:cj=ci,j̸=i
The total relevance score of an image pi reﬂects how relevant the image is in terms of its relevance to the other images in the same category.
To sample a triplet, we ﬁrst sample a query image pi
from P according to its total relevance score. The probability of an image being chosen as query image is proportional
to its total relevance score.
Then, we sample a positive image p+
i from the images
sharing the same categories as pi. Since we are more interested in the top-ranked images, we should sample more
positive images p+
i with high relevance scores ri,i+. The
probability of choosing an image p+
i as positive image is:
i ) = min{Tp, ri,i+}
where Tp is a threshold parameter, and the normalization
constant Zi equals 
i ) for all the p+
i sharing the
the same categories with pi.
We have two types of negative image samples. The ﬁrst
type is out-of-class negative samples, which are the negative
samples that are in a different category from query image pi.
They are drawn uniformly from all the images with different categories with pi. The second type is in-class negative
samples, which are the negative samples that are in the same
category as pi but is less relevant to pi than p+
i . Since we
are more interested in the top-ranked images, we draw inclass negative samples p−
i with the same distribution as (7).
In order to ensure robust ordering between p+
a triplet ti = (pi, p+
i ), we also require that the margin
between the relevance score ri,i+ and ri,i−should be larger
than Tr, i.e.,
ri,i+ −ri,i−≥Tr, ∀ti = (pi, p+
We reject the triplets that do not satisfy this condition. If
the number of failure trails for one example exceeds a given
threshold, we simply discard this example.
Learning deep ranking models requires large amount of
data, which cannot be loaded into main memory. The sampling algorithms that require random access to all the examples in the dataset are not applicable. In this section, we
propose an efﬁcient online triplet sampling algorithm based
on reservoir sampling .
We have a set of buffers to store images. Each buffer has
a ﬁxed capacity, and it stores images from the same category. When we have one new image pj, we compute its key
kj = u(1/rj)
,where rj is its total relevance score deﬁned in
(6) and uj = uniform(0, 1) is a uniformly sampled number.
The buffer corresponding to the image pj’s can be found according to its category cj. If the buffer is not full, we insert
the image pj into the buffer with key kj. Otherwise, we ﬁnd
the image p′
j with smallest key k′
j in the buffer. If kj > k′
we replace the image p′
j with image pj in the buffer. Otherwise, the imgage example pj is discarded. If this replacing
scheme is employed, uniformly sampling from a buffer is
equivalent to drawing samples with probability proportional
to the total relevance score rj.
One image pi is uniformly sampled from all the images in the buffer of category cj as the query image. We
then uniformly generate one image p+
i from all the images
Buffers for queries
Image sample
Find buffer
of the query
Figure 4. Illustration of the online triplet sampling algorithm. The
negative image in this example is an out-of-class negative. We
have one buffer for each category. When we get a new image
sample, we insert it into the buffer of the corresponding category
with prescribed probability. The query and positive examples are
sampled from the same buffer, while the negative image is sampled
from a different buffer.
in the buffer of category cj, and accept it with probability min(1, ri,i+/ri+), which corresponds to the sampling
probability (7). Sampling is continued until one example is
accepted. This image example acts as the positive image.
Finally, we draw a negative image sample. If we are
drawing out-of-class negative image sample, we draw a image p−
i uniformly from all the images in the other buffers.
If we are drawing in-class negative image samples, we use
the positive example’s drawing method to generate a negative sample, and accept the negative sample only if it satis-
ﬁes the margin constraint (8). Whether we sample in-class
or out-of-class negative samples is controlled by a out-ofclass sample ratio parameter. An illustration of this sampling method is shown in Fig. 4 The outline of reservoir
importance sampling algorithm is shown in the supplemental materials.
6. Experiments
6.1. Training Data
We use two sets of training data to train our model. The
ﬁrst training data is ImageNet ILSVRC-2012 dataset ,
which contains roughly 1000 images in each of 1000 categories. In total, there are about 1.2 million training images,
and 50,000 validation images. This dataset is utilized to
learn image semantic information. We use it to pre-train the
“ConvNet” part of our model using soft-max cost function
as the top layer.
The second training data is relevance training data, responsible for learning ﬁne-grained visual similarity. The
data is generated in a bootstrapping fashion. It is collected
from 100,000 search queries (using Google image search),
with the top 140 image results from each query. There are
about 14 million images. We employ a golden feature to
compute the relevance ri,j for the images from the same
search query, and set ri,j = 0 to the images from different
queries. The golden feature is a weighted linear combination of twenty seven features. It includes features described
in section 6.4, with different parameter settings and distance
metrics. More importantly, it also includes features learned
through image annotation data, such as features or embeddings developed in . The linear weights are learned
through max-margin linear weight learning using human
rated data. The golden feature incorporates both visual appearance information and semantic information, and it is of
high performance in evaluation. However, it is expensive to
compute, and ”cumbersome” to develop. This training data
is employed to ﬁne-tune our network for ﬁne-grained visual
similarity.
6.2. Triplet Evaluation Data
Since we are interested in ﬁne-grained similarity, which
cannot be characterized by image labels, we collect a triplet
dataset to evaluate image similarity models 1.
We started from 1000 popular text queries and sampled
triplets (Q, A, B) from the top 50 search results for each
query from the Google image search engine. We then rate
the images in the triplets using human raters. The raters
have four choices: (1) both image A and B are similar to
query image Q; (2) both image A and B are dissimilar to
query image Q; (3) image A is more similar to Q than B; (4)
image B is more similar to Q than A. Each triplet is rated by
three raters. Only the triplets with unanimous scores from
the three rates enter the ﬁnal dataset. For our application,
we discard the triplets with rating (1) and rating (2), because
those triplets does not reﬂect any image similarity ordering.
About 14,000 triplets are used in evaluation. Those triplets
are solely used for evaluation. Fig 1 shows some triplet
6.3. Evaluation Metrics
Two evaluation metrics are used: similarity precision
and score-at-top-K for K = 30.
Similarity precision is deﬁned as the percentage of
triplets being correctly ranked.
Given a triplet ti
i ), where p+
i should be more similar to pi than
i . Given pi as query, if p+
i is ranked higher than p−
we say the triplet ti is correctly ranked.
Score-at-top-K is deﬁned as the number of correctly
ranked triplets minus the number of incorrectly ranked ones
on a subset of triplets whose ranks are higher than K. The
subset is chosen as follows. For each query image in the
test set, we retrieve 1000 images belonging to the same text
query, and rank these images using the learned similarity
metric. One triplet’s rank is higher than K if its positive
i or negative image p−
i is among the top K nearest neighbors of the query image pi. This metric is similar
to the precision-at-top-K metric, which is widely used to
1 
evaluate retrieval systems. Intuitively, score-at-top-K measures a retrieval system’s performance on the K most relevant search results. This metric can better reﬂect the performance of the similarity models in practical image retrieval
systems, because users pay most of their attentions to the
results on the ﬁrst few pages. we set K = 30 in our experiments.
6.4. Comparison with Hand-crafted Features
We ﬁrst compare the proposed deep ranking method with
hand-crafted visual features. For each hand-crafted feature,
we report its performance using its best experimental setting.
The evaluated hand-crafted visual features include
Wavelet , Color (LAB histoghram), SIFT -like features, SIFT-like Fisher vectors , HOG , and SPMK
Taxton features with max pooling . Supervised image
similarity ranking information is not used to obtain these
Two image similarity models are learned on top of the
concatenation of all the visual features described above.
• L1HashKCPA : A subset of the golden features
(with L1 distance) are chosen using max-margin linear weight learning. We call this set of features “L1
visual features”. Weighted Minhash and Kernel principal component analysis (KPCA) are applied on
the L1 visual features to learn a 1000-dimension embedding in an unsupervised fashion.
• OASIS : Based on the L1HashKCPA feature, an
transformation (OASIS transformation) is learnt with
an online image similarity learning algorithm , using the relevance training data described in Sec. 6.1.
The performance comparison is shown in Table 1. The
“DeepRanking” shown in this table is the deep ranking
model trained with 20% out-of-class negative samples. We
can see that any individual feature without learning does
not performs very well. The L1HashKCPA feature achieves
reasonably good performance with relatively low dimension, but its performance is inferior to DeepRanking model.
The OASIS algorithm can learn better features because it
exploits the image similarity ranking information in the relevance training data. By directly learning a ranking model
on images, the deep ranking method can use more information from image than two-step “feature extraction”-“model
learning” approach. Thus, it performs better both in terms
of similarity precision and score-at-top-30.
The DeepRanking model performs better in terms of
similarity precision than the golden features, which are used
to generate relevance training data.
This is because the
DeepRanking model employs the category-level information in ImageNet data and relevance training data to better
characterize the image semantics. The score-at-top-30 metric of DeepRanking is only slightly lower than the golden
Wavelet 
SIFT-like 
Fisher 
SPMKtexton1024max 
L1HashKPCA 
Golden Features
DeepRanking
Table 1. Similarity precision (Precision) and score-at-top-30
(Score-30) for different features.
6.5. Comparison of Different Architectures
We compare the proposed method with the following
architectures: (1) Deep neural network for classiﬁcation
trained on ImageNet, called ConvNet. This is exactly the
same as the model trained in . (2) Single-scale deep
neural network for ranking. It only has a single scale ConvNet in deep ranking model, but It is trained in the same
way as DeepRanking model. (3) Train an OASIS model 
on the feature output of single-scale deep neural network for
ranking. (4) Train a linear embedding on both the singlescale deep neural network and the visual features described
in the last section. The performance are shown in Table 2.
In all the experiments, the Euclidean distance of the embedding vectors of the penultimate layer before the ﬁnal softmax or ranking layer is exploited as similarity measure.
First, we ﬁnd that the ranking model greatly increases
the performance. The performance of single-scale ranking
model is much better than ConvNet. The two networks have
the same architecture except single-scale ranking model is
ﬁne-tuned with the relevance training data using ranking
layer, while ConvNet is trained solely for classiﬁcation task
using logistic regression layer.
We also ﬁnd that single-scale ranking performs very well
in terms of similarity precision, but its score-at-top-30 is
not very high. The DeepRanking model, which employs
multiscale network architecture, has both better similarity
precision and score-at-top-30.
Finally, although training
an OASIS model or linear embedding on the top increases
performance, their performance is inferior to DeepRanking
model, which uses back-propagation to ﬁne-tune the whole
An illustration of the learned ﬁlters of the multi-scale
deep ranking model is shown in Fig. 5. The ﬁlters learned in
this paper captures more color information compared with
the ﬁlter learned in .
Single-scale Ranking
OASIS on Single-scale Ranking
Single-Scale & Visual Feature
DeepRanking
Table 2. Similarity precision (Precision) and score at top 30
(Score-30) for different neural network architectures.
Figure 5. The learned ﬁlters of the ﬁrst level convolutional layers
of the multi-scale deep ranking model.
6.6. Comparison of Different Sampling Methods
We study the effect of the fraction of the out-of-class
negative samples in online triplet sampling algorithm on
the performance of the proposed method. Fig. 6 shows
the results. The results are obtained from drawing 24 million triplets samples. We ﬁnd that the score-at-top-30 metric
of DeepRanking model decreases as we have more out-ofclass negative samples. However, having a small fraction
of out-of-class samples (like 20%) increases the similarity
precision metric a lot.
We also compare the performance of the weighted sampling and uniform sampling with 0% out-of-class negative
samples. In weighted sampling, the sampling probability
of the images is proportional to its total relevance score rj
and pairwise relevance score ri,j, while uniform sampling
draws the images uniformly from all the images (but the
ranking order and margin constraints should be satisﬁed).
We ﬁnd that although the two sampling methods perform
similarly in overall precision, the weighted sampling algorithm does better in score-at-top-30. Thus, weighted sampling is employed.
6.7. Ranking Examples
A comparison of the ranking examples of ConvNet, OA-
SIS feature (L1HashKPCA features with OASIS learning)
and Deep Ranking is shown in Fig. 7. We can see that
ConvNet captures the semantic meaning of the images very
well, but it fails to take into account some global visual appearance, such as color and contrast. On the other hand,
Fraction of out−of−class negative samples
Score at 30
weighted sampling
uniform sampling
Fraction of out−of−class negative samples
Overall precision
weighted sampling
uniform sampling
Figure 6. The relationship between the performance of the proposed method and the fraction of out-of-class negative samples.
Ranking Results
Figure 7. Comparison of the ranking examples of ConvNet, Oasis
Features and Deep Ranking.
Oasis features can characterize the visual appearance well,
but fall short on the semantics. The proposed deep ranking
method incorporates both the visual appearance and image
semantics.
7. Conclusion
In this paper, we propose a novel deep ranking model to
learn ﬁne-grained image similarity models. The deep ranking model employs a triplet-based hinge loss ranking function to characterize ﬁne-grained image similarity relationships, and a multiscale neural network architecture to capture both the global visual properties and the image semantics. We also propose an efﬁcient online triplet sampling
method that enables us to learn deep ranking models from
very large amount of training data. The empirical evaluation
shows that the deep ranking model achieves much better
performance than the state-of-the-art hand-crafted featuresbased models and deep classiﬁcation models. Image similarity models can be applied to many other computer vision applications, such as exemplar-based object recognition/detection and image deduplication. We will explore
along these directions.