Online Multi-Object Tracking Using CNN-based Single Object Tracker with
Spatial-Temporal Attention Mechanism
Qi Chu1,3, Wanli Ouyang2,3, Hongsheng Li3, Xiaogang Wang3, Bin Liu1, Nenghai Yu1,∗
1University of Science and Technology of China, 2University of Sydney
3Department of Electronic Engineering, The Chinese University of Hong Kong
 , {wlouyang,hsli,xgwang}@ee.cuhk.edu.hk, {flowice,ynh}@ustc.edu.cn
In this paper, we propose a CNN-based framework for
online MOT. This framework utilizes the merits of single
object trackers in adapting appearance models and searching for target in the next frame.
Simply applying single
object tracker for MOT will encounter the problem in computational efﬁciency and drifted results caused by occlusion. Our framework achieves computational efﬁciency by
sharing features and using ROI-Pooling to obtain individual
features for each target. Some online learned target-speciﬁc
CNN layers are used for adapting the appearance model
for each target. In the framework, we introduce spatialtemporal attention mechanism (STAM) to handle the drift
caused by occlusion and interaction among targets. The
visibility map of the target is learned and used for inferring
the spatial attention map. The spatial attention map is then
applied to weight the features. Besides, the occlusion status
can be estimated from the visibility map, which controls the
online updating process via weighted loss on training samples with different occlusion statuses in different frames. It
can be considered as temporal attention mechanism. The
proposed algorithm achieves 34.3% and 46.0% in MOTA
on challenging MOT15 and MOT16 benchmark dataset respectively.
1. Introduction
Tracking objects in videos is an important problem in
computer vision which has attracted great attention.
has various applications such as video surveillance, human
computer interface and autonomous driving. The goal of
multi-object tracking (MOT) is to estimate the locations of
multiple objects in the video and maintain their identities
consistently in order to yield their individual trajectories.
MOT is still a challenging problem, especially in crowded
scenes with frequent occlusion, interaction among targets
and so on.
∗Corresponding author.
Figure 1. An example of drift caused by occlusion of other targets
when directly adopting single object trackers to MOT.
On the other hand, signiﬁcant improvement has been
achieved on single object tracking problem, sometimes
called “visual tracking” in previous work. Most state-ofthe-art single object tracking methods aim to online learn
a strong discriminative appearance model and use it to ﬁnd
the location of the target within a search area in next frame
 . Since deep convolutional neural networks
(CNNs) are shown to be effective in many computer vision applications , many works
 have explored the usage of CNNs to learn
strong discriminative appearance model in single object
tracking and demonstrated state-of-the-art performance recently. An intuitive thought is that applying the CNN based
single object tracker to MOT will make sense.
However, problems are observed when directly using
single object tracking approach for MOT.
First, single object tracker may learn from noisy samples.
In single object tracking, the training samples for
learning appearance model are collected online, where labels are based on tracking results. The appearance model
is then used for ﬁnding the target in the next frame. When
the target is occluded, the visual cue is unreliable for learning the appearance model. Consequently, the single object
tracker will gradually drift and eventually fail to track the
target. This issue becomes even more severe in MOT due to
 
more frequent occlusion caused by interaction among targets. An example is shown in Figure 1, one target is occluded by another when they are close to each other, which
makes the visual cues of the occluded target contaminated
when this target is used for training. However, the tracking
score of the occluded target is still relatively high at the beginning of occlusion. In this case, the corresponding single
object tracker updates the appearance model with the corrupted samples and gradually drifts to the occluder.
Second, since a new single object tracker needs to be
added into MOT system once a new target appears, the computational cost of applying single object trackers to MOT
may grow intolerably as the number of tracked objects increases, which limits the application of computationally intensive single object trackers in MOT such as deep learning
based methods.
In this work, we focus on handling the problems observed above. To this end, we propose a dynamic CNNbased framework with spatial-temporal attention mechanism (STAM) for online MOT. In our framework, each
object has its own individual tracker learned online.
The contributions of this paper are as follows:
First, an efﬁcient CNN-based online MOT framework. It
solves the problem in computational complexity when simply applying CNN based single object tracker for MOT by
sharing computation among multiple objects.
Second, in order to deal with the drift caused by occlusion and interactions among targets, spatial-temporal attention of the target is learned online. In our design, the visibility map of the target is learned and used for inferring the
spatial attention map. The spatial attention map is applied
to weight the features. Besides, the visibility map also indicates occlusion status of the target which is an important cue
that needs to be considered in online updating process. The
more severe a target is occluded, the less likely it should be
used for updating corresponding individual tracker. It can
be considered as temporal attention mechanism. Both the
spatial and temporal attention mechanism help to help the
tracker to be more robust to drift.
We demonstrate the effectiveness of the proposed online MOT algorithm, referred as STAM, using challenging
MOT15 and MOT16 benchmarks.
2. Related Work
Multi-object Tracking by Data Associtation. With the
development of object detection methods ,
data association has become popular for
MOT. The main idea is that a pre-deﬁned object detector
is applied to each frame, and then trajectories of objects
are obtained by associating object detection results. Most
of these works adopt an off-line way to process video sequences in which the future frames are also utilized to deal
with the problem. These off-line methods consider MOT as
a global optimization problem and focus on designing various optimization algorithm such as network ﬂow ,
continuous energy minimization , max weight independent set , k-partite graph , subgraph multi-cut
 and so on. However, ofﬂine methods are not suitable for causal applications such as autonomous driving. On
the contrary, online methods generate trajectories only using information up to the current frame which adopt probabilistic inference or deterministic optimization (e.g.
Hungarian algorithm used in ). One problem of such association based tracking methods is the heavy dependency
on the performance of the pre-deﬁned object detector. This
problem has more inﬂuence for online tracking methods,
since they are more sensitive to noisy detections. Our work
focuses on applying online single object tracking methods
to MOT. The target is tracked by searching for the best
matched location using online learned appearance model.
This helps to alleviate the limitations from imperfect detections, especially for missing detections. It is complementary to data association methods, since the tracking results
of single object trackers at current frame can be consider as
association candidates for data association.
Single Object Tracker in MOT. Some previous works
 have attempted to adopt single object
tracking methods into MOT problem.
However, single
object tracking methods are often used to tackle a small subproblem due to challenges mentioned in Sec. 1. For example, single object trackers are only used to generate initial
tracklets in . Yu et al. partitions the state space of
the target into four subspaces and only utilizes single object
trackers to track targets in tracked state. There also exists a
few works that utilize single object trackers throughout the
whole tracking process. Breitenstein et al. use targetspeciﬁc classiﬁers to compute the similarity for data association in a particle ﬁltering framework. Yan et al. keep
both the tracking results of single object trackers and the
object detections as association candidates and select the
optimal candidate using an ensemble framework. All methods mentioned above do not make use of CNN based single
object trackers, so they can not update features during tracking. Besides, they do not deal with tracking drift caused by
occlusion. Different from these methods, our work adopts
online learned CNN based single object trackers into online
multi-object tracking and focuses on handling drift caused
by occlusion and interactions among targets.
Occlusion handling in MOT. Occlusion is a wellknown problem in MOT and many approaches are proposed
for handling occlusion. Most works 
aim at utilizing better detectors for handling partial occlusion. In this work, we attempt to handle occlusion from the
perspective of feature learning, which is complementary to
these detection methods. Speciﬁcally, we focus on learning more robust appearance model for each target using the
single object tracker with the help of spatial and temporal
attention.
Target State
Features of
Candidates
Classifier
Current Positive
Training Samples
Current Negative
Training Samples
Historical Positive
Training Samples
Figure 2. Overview of the proposed algorithm STAM. Motion model provides the search area, where features of candidates are extracted
and then weighted by the spatial attention. The candidate state with the maximum classiﬁcation score is used as the estimated target state.
The positive and negative training samples at current frame are collected according to the overlap with estimated target state. The historical
positive training samples of the target are also used for online updating. Temporal attention model is used for weighting the loss of positive
training samples in current and historical frames.
3. Online MOT Algorithm
3.1. Overview
The overview of the proposed algorithm is shown in Figure 2. The following steps are used for tracking objects:
Step 1. At the current frame t, the search area of each
target is obtained using motion model. The candidates are
sampled within the search area.
Step 2. The features of candidates for each target are
extracted using ROI-Pooling and weighted by spatial attention. Then the binary classiﬁer is used to ﬁnd the best
matched candidate with the maximum score, which is used
as the estimated target state.
Step 3. The visibility map of each tracked target is inferred from the feature of corresponding estimated target
state. The visibility map of the tracked target is then used
along with the spatial conﬁgurations of the target and its
neighboring targets to infer temporal attention.
Step 4. The target-speciﬁc CNN branch of each target is
updated according to the loss of training samples in current
and historical frames weighted by temporal attention. The
motion model of each target is updated according to corresponding estimated target state.
The object management strategy determines
the initialization of new targets and the termination of untracked targets.
Step 6. If frame t is not the last frame, then go to Step 1
for the next frame t + 1 .
3.2. Dynamic CNN-based MOT Framework
We propose a dynamic CNN-based framework for online
MOT, which consists of both shared CNN layers and targetspeciﬁc CNN branches. As shown in Figure 3, the shared
CNN layers encode the whole input frame as a large feature map, from which the feature representation of each target is extracted using ROI-Pooling . For computational
efﬁciency, these shared layers are pre-trained on Imagenet
Classiﬁcation task , and not updated during tracking.
All target-speciﬁc CNN branches share the same structure,
but are separately trained to capture the appearance of different targets. They can be viewed as a set of single-object
The number of target-speciﬁc CNN branches varies with
the number of existing targets. Once a new target appears,
a new branch will be initialized and added to the model. If
a target is considered to be disappeared, its corresponding
branch will be removed from the entire model.
3.3. Online Tracking with STAM
The trajectory of an object can be represented by a
series of states denoted by {xt}t=1,2,3...,T , where xt =
[xt, yt, wt, ht]. xt and yt represent the center location of
the target at frame t. wt and ht denote the width and height
of the target, respectively. Multi-object tracking aims to obtain the estimated states of all targets at each frame.
Candidate States
For the i-th target T i to be tracked, its estimated state xi
at frame t is obtained by searching from a set of candidate
states denoted by Ci
t, which consists of two subsets:
n=1 denotes the set of candidate states that are
drawn from a Gaussian distribution N(˜xi
t), where ˜xi
is the predicted state of target T i at frame t, and Σi
t,x)2, (σi
t,y)2, (σi
t,w)2, (σi
is a diagonal covariance matrix indicating the variance of target location
and scale. ˜xi
t are estimated by the motion model
(Sec. 3.4.3). Denote by Dt = {xd
m=1 the set of all
object detections provided by an off-line trained detector at
Shared CNN
Feature Map
Estimated Target States
Input Frame
CNN Layers
Target-specific
CNN Branch 1
ROI-Pooled Features
of Candidates States
Target-specific
CNN Branch n
Visibility Map
Spatial Attention
ROI-Pooled
Conv Layer
Conv Layer
Cross-entropy Loss
Classification Score
Feature Extraction
Binary Classification
Figure 3. (a) The framework of the proposed CNN model. It contains shared CNN layers and multiple target-speciﬁc CNN branches. The
shared layers are shared by all targets to be tracked. Each target has its own corresponding target-speciﬁc CNN branch which is learned
online. The target-speciﬁc CNN branch acts as a single object tracker and can be added to or removed from the whole model according to
the entrance of new target or exit of existing target. (b) The details of the target-speciﬁc CNN branch. Each target-speciﬁc CNN branch
consists of feature extraction using visibility map and spatial attention as described in Sec. 3.3.2 and binary classiﬁcation (described in
Sec. 3.3.3). The initialization and online updating of the target-speciﬁc branch are described in Sec. 3.4.1 and Sec. 3.4.2 respectively.
frame t. Di
mi=1 ⊆Dt are selected detections
that are close to the predicted state ˜xi
t in spatial location
t,mi)k −(˜xi
t)k| < 3σi
t,k, ∀k = x, y, w, h).
Feature Extraction with Spatial Attention
The feature of candidate state is extracted from the shared
feature map using ROI-Pooling and spatial attention mechanism. The ROI-Pooling from the shared feature map ignores the fact that the tracked targets could be occluded.
In this case, the pooled features would be distorted by the
occluded parts. To handle this problem, we propose a spatial attention mechanism which pays more attention to unoccluded regions for feature extraction.
Directly using spatial attention does not work well due to
limited training samples in the online learning process. In
our work, we ﬁrst generate the visibility map which encodes
the spatial visibility of the input samples. Then the spatial
attention is derived from visibility map.
Visibility Map.
Denote the ROI-Pooled feature representation of the j-th candidate state xi
t,j) ∈RW ×H×C, the visibility map of xi
t,j is estimated as
t) = fvis(Φroi(xj
vis is the set of parameters. fvis(Φroi(xi
is modeled as two layers interleaved with ReLU layer. The
ﬁrst layer is a convolution layer which has the kernel size
of 3 × 7 and produces a feature map with 32 channels. The
second layer is a fully connected layer with the output size
of (W ∗H). Then the output is reshaped to a map with the
size of W ×H. Each element in visibility map V(xi
t,j) indicates the visibility of corresponding location in feature map
t,j). Some examples of generated visibility maps are
shown in Figure 4.
Spatial Attention. The spatial attention map Ψ(xi
RW ×H for candidate state xi
t,j is obtained from visibility
t,j) as follows:
t,j)) = fatt(V(xi
where fatt is implemented by a local connected layer followed by a spatial softmax layer and wi
att denotes the parameters. Then the spatial attention map Ψ(xi
t,j) is applied
to weight the feature map Φroi(xi
t,j) = Φroi(xi
t,j) ⋆Ψ(xi
t,j), Φroi(xi
t,j) ∈RW ×H×C
t,j) ∈RW ×H
where ⋆represents the channel-wise Hadamard product
operation, which performs Hadamard product between
t,j) and each channel of Φroi(xi
Figure 4. Examples of the generated visibility maps. The ﬁrst four
columns show examples of the target occluded by other target or
background. The last column shows the failure case when targets
are too close. Best viewed in color.
Target State Estimation Using Binary Classiﬁer
and Detection Results
Binary Classiﬁcation. Given the reﬁned feature representation Φatt(xi
t,j), the classiﬁcation score is obtained as follows:
t,j = fcls(Φatt(xi
t,j ∈ is the output of binary classiﬁer which
indicates the probability of candidate state xi
t,j belonging
to target T i. wi
cls is the parameter of the classiﬁer for target T i. In our work, fcls(Φatt(xi
cls) is modeled by
two layers interleaved with ReLU layer. The ﬁrst layer is
a convolution layer which has the kernel size of 3 × 7 and
produces a feature map with 5 channels. The second layer
is a fully connected layer with the output size of 1. Then
a sigmoid function is applied to ensure the output to be in
The primitive estimated state of target T i is obtained by
searching for the candidate state with the maximum classi-
ﬁcation score as follows:
t = arg max
fcls(Φatt(xi
State Reﬁnement. The primitive estimated state with
too low classiﬁcation score will bias the updating of the
model. To avoid model degeneration, if the score ˆyi
fcls(Φatt(ˆxi
cls) is lower than a threshold p0, the corresponding target T i is considered as “untracked” in current
frame t. Otherwise, the primitive state ˆxi
t will be further
reﬁned using the object detections states Dt = {xd
Speciﬁcally, the nearest detection state for ˆxi
t is obtained
as follows:
where IoU(ˆxi
t,m) calculates the bounding box IoU overlap ratio between ˆxi
t,m. Then the ﬁnal state of target
T i is reﬁned as
otherwise,
t = IoU(ˆxi
t ) and o0 is a pre-deﬁned threshold.
3.4. Model Initialization and Online Updating
Each target-speciﬁc CNN branch comprises of visibility
map, attention map and binary classiﬁer. The parameters
for visibility map are initialized in the ﬁrst frame when the
target appears and then all three modules are jointly learned.
Model Initialization
For the initialization of parameters in obtaining visibility
map, we synthetically generate training samples and the
corresponding ground truth based on initial target state.
Augmented Set. Denote the ROI-Pooled feature representation of initial state of target T i as Φroi(xi
RW ×H×C, a W × H matrix with all elements equal to 1 is
used as the corresponding ground truth visibility map. An
augmented set is obtained via collecting samples that have
large overlap with initial target state xi
0. For each sample in
the augmented set, the ground truth visibility map for region
not overlapping with xi
0 is set to 0.
Feature Replacement. We replace the features of the
sample with the features from another target or background
at some region and set the ground truth for replaced region
The replaced region is regarded as occluded. For
each sample in the augmented set, the feature replacement
is done using different targets/brackgrounds at different regions.
Given these training samples and ground truth visibility
maps, the model is trained using cross-entropy loss.
Online Updating Appearance Model
After initialization in the initial frame, all three modules are
jointly updated during tracking using back-propagation algorithm.
Training samples used for online updating are obtained
from current frame and historical states. For tracked target,
positive samples at current frame t are sampled around the
estimated target state xt with small displacements and scale
variations. Besides, historical states are also utilized as positive samples. If the target is considered as ”untracked” at
current frame, we only use historical states of the target as
positive samples. All negative samples are collected at current frame t. The target-speciﬁc branch needs to have the
capability of discriminating the target from other targets and
background. So both the estimated states of other tracked
targets and the samples randomly sampled from background
are treated as the negative samples.
For target T i, given the current positive samples set
j=1 , historical positive samples set {xi+
the negative samples set {xi−
j=1 , the loss function for
updating corresponding target-speciﬁc branch is deﬁned as
log[1 −fcls(Φatt(xi−
log fcls(Φatt(xi+
log fcls(Φatt(xi+
where, Li−
t , and Li+
are losses from negative samples, positive samples at current frame, and positive samples in the history, respectively. αi
t is the temporal attention
introduced below.
Temporal Attention. A crucial problem for model updating is to balance the relative importance between current
and historical visual cues. Historical samples are reliable
positive samples collected in the past frames, while samples
in current frame reﬂect appearance variations of the target.
In this work, we propose a temporal attention mechanism,
which dynamically pay attention to current and historical
samples based on occlusion status.
Temporal attention of target T i is inferred from visibility
t) and the overlap statuses with other targets
t = σ(γisi
t is the mean value of visibility map V(xi
is the maximum overlap between T i and all other targets
in current frame t. γi, βi and bi are learnable parameters.
σ(x) = 1/(1 + e−x) is the sigmoid function.
t indicates the occlusion status of target T i. If αi
is large, it means that target T i is undergoing severe occlusion at current frame t. Consequently, the weight for positive samples at current frame is small according to Eq. 9.
There, the temporal attention mechanism provides a good
balance between current and historical visual cues of the
target. Besides, if αi
t is smaller than a threshold α0, the
corresponding target state xi
t will be added to the historical
samples set of target T i.
Updating Motion Model
Most single object trackers do not consider the motion
model, while it is proved to be helpful in MOT. In our work,
a simple linear motion model with constant velocity and
Gaussian noise is applied to each target, which is used to
determine the center location and the size of search area for
tracking the target in next frame. The scale of the target is
considered as unchanged. Given the velocity vi
t at frame t,
the predicted state of target T i at frame t + 1 is deﬁned as
At frame t, the velocity of target T i is updated as
t−1 + (1 −αi
where Tgap denotes the time gap for computing velocity.
t]T is the center location of target T i at frame t.
The variance of Gaussian noise is deﬁned as
t = 0 and r > 0.75
t = 0 and r < 0.25
t−1 is the center location of target T i at
frame t predicted by motion model. ˜N i
t denotes the length
of the successive untracked frames of target T i at frame t.
r measures the prediction error of linear motion model. If
target T i is tracked at frame t, the variance σi
t is related
to the prediction error r. Otherwise, the search area will
be extended as the length of successive untracked frames
3.5. Object Management
In our work, a new target T new is initialized when a
newly detected object with high detection score is not covered by any tracked targets. To alleviate the inﬂuence of
false positive detections, the newly initialized target T new
will be discarded if it is considered as “untracked” (Sec.
3.3.3) or not detected in any of the ﬁrst Tinit frames. For
target termination, we simply terminate the target if it is
“untracked” for over Tterm successive frames. Besides, targets that exit the ﬁeld of view are also terminated.
4. Experiments
In this section, we present the experimental results and
analysis for the proposed online MOT algorithm.
4.1. Implementation details
The proposed algorithm is implemented in MATLAB
with Caffe . In our implementation, we use the ﬁrst ten
convolutional layers of the VGG-16 network trained on
Imagenet Classiﬁcation task as the shared CNN layers.
The threshold o0 is set to 0.5, which determines whether
the location found by single object tracker is covered by a
object detection. The thresholds p0 and α0 are set to 0.7
and 0.3 respectively. For online updating, we collect positive and negative samples with ≥0.7 and ≤0.3 IoU overlap ratios with the target state at current frame, respectively.
The detection scores are normalized to the range of 
and the detection score threshold in target initialization is
set to 0.25. Denote the frame rate of the video as F, we use
Tinit = 0.2F and Tterm = 2F in object management and
Tgap = 0.3F in motion model.
4.2. Datasets
We evaluate our online MOT algorithm on the public available MOT15 and MOT16 benchmarks
containing 22 (11 training, 11 test) and 14 (7 training, 7
test) video sequences in unconstrained environments respectively.
The ground truth annotations of the training
sequences are released. We use the training sequences in
MOT15 benchmark for performance analysis of the proposed method.
The ground truth annotations of test sequences in both benchmarks are not released and the tracking results are automatically evaluated by the benchmark.
So we use the test sequences in two benchmarks for comparison with various state-of-the-art MOT methods. In addition, these two benchmarks also provide object detections
generated by the ACF detector and the DPM detector
 respectively. We use these public detections in all experiments for fair comparison.
4.3. Evaluation metrics
To evaluate the performance of multi-object tracking
methods, we adopt the widely used CLEAR MOT metrics
 , including multiple object tracking precision (MOTP)
and multiple object tracking accuracy (MOTA) which combines false positives (FP), false negatives (FN) and the identity switches (IDS). Additionally, we also use the metrics
deﬁned in , which consists of the percentage of mostly
tracked targets (MT, a ground truth trajectory that are covered by a tracking hypothesis for at least 80% is regarded as
mostly tracked), the percentage of mostly lost targets (ML,
a ground truth trajectory that are covered by a tracking hypothesis for at most 20% is regarded as mostly lost), and the
number of times a trajectory is fragmented (Frag).
4.4. Tracking Speed
The overall tracking speed of the proposed method on
MOT15 test sequences is 0.5 fps using the 2.4GHz CPU
and a TITAN X GPU, while the algorithm without feature
sharing runs at 0.1 fps with the same environment.
4.5. Performance analysis
To demonstrate the effectiveness of the proposed
method, we build ﬁve algorithms for components of different aspects of our approach. The details of each algorithm
are described as follows:
Figure 5. The performance of different algorithms on training sequences of MOT15 in terms of MOTA.
p1: directly using single object trackers without the proposed spatial-temporal attention or motion model, which is
the baseline algorithm;
p2: adding the motion model based on p1;
p3: adding the spatial attention based on p2;
p4: adding the temporal attention based on p2;
p5: adding the spatial-temporal attention based on p2,
which is the whole algorithm with all proposed components.
The performance of these algorithms on the training sequences of MOT15, in terms of MOTA which is a good approximation of the overall performance, are shown in Figure 5. The better performance of the algorithm p2 compared to p1 shows the effect of the using motion model in
MOT. The advantages of the proposed spatial-temporal attention can be seen by comparing the performance of algorithm p5 and p2. Furthermore, compared to the algorithm
p2, the performance improvement of p3 and p4 shows the
effectiveness of spatial and temporal attention in improving tracking accuracy respectively. The improvement of p5
over both p3 and p4 shows that the spatial and temporal
attention are complementary to each other. Algorithm p5
with all the proposed components achieves the best performance and improves 8% in terms of MOTA compared with
the baseline algorithm p1, which demonstrates the effectiveness of our algorithm in handling the problems of using
single object trackers directly.
4.6. Comparisons with state-of-the-art methods
We compare our algorithm, denoted by STAM, with several state-of-the-art MOT tracking methods on the test sequences of MOT15 and MOT16 benchmarks. All the compared state-of-the-art methods and ours use the same public
detections provided by the benchmark for fair comparison.
Table 1 presents the quantitative comparison results 1.
MOT15 Results. Overall, STAM achieves the best performance in MOTA and IDS among all the online and of-
ﬂine methods. In terms of MOTA, which is the most impor-
1The quantitative tracking results of all these trackers are available at the website MOT 2015/ and
 
JPDA m 
SiameseCNN 
CNNTCM 
MHT DAM 
TC ODAL 
JPDA m 
MHT DAM 
Table 1. Quantitative results of our method (denoted by STAM) and several state-of-the-art MOT trackers on MOT15 and MOT16 test
sequences. Results are divided into two groups, i.e. online tracking and ofﬂine tracking. red and blue values in blod highlight the best
results of online and ofﬂine methods respectively. ’↑’ means that higher is better and ’↓’ represents that lower is better.
tant metric for MOT, STAM improves 4% compared with
MDP, the best online tracking method that is peer-reviewed
and published. Note that our method works in pure online
mode and dose not need any training data with ground truth
annotations. While MDP performs training with sequences
in the similar scenario and its ground truth annotations for
different test sequences. Besides, our method produce the
lowest IDS among all methods, which demonstrates that our
method can handle the interaction among targets well. Note
that the CNNTCM and SiameseCNN also utilize CNNs to
handle MOT problem but in ofﬂine mode. What’s more,
their methods requir abundant training data for learning
siamese CNN. The better performance compared to these
CNN-based ofﬂine methods provides strong support on the
effectiveness of our online CNN-based algorithm.
MOT16 Results.
Similarly, STAM achieves the best
performance in terms of MOTA, MT, ML, and FN among
all online methods. Besides, the performance of our algorithm in terms of MOTA is also on par with state-of-the-art
ofﬂine methods.
On the other hand, our method produces slightly more
Frag than some ofﬂine methods, which is a common defect
of online MOT methods due to long term occlusions and
severe camera motion ﬂuctuation.
5. Conclusion
In this paper, we have proposed a dynamic CNN-based
online MOT algorithm that efﬁciently utilizes the merits of
single object trackers using shared CNN features and ROI-
Pooling. In addition, to alleviate the problem of drift caused
by frequent occlusions and interactions among targets, the
spatial-temporal attention mechanism is introduced.
Besides, a simple motion model is integrated into the algorithm to utilize the motion information. Experimental results on challenging MOT benchmarks demonstrate the effectiveness of the proposed online MOT algorithm.
Acknowledgement: This work is supported by the National Natural Science Foundation of China (No.61371192),
the Key Laboratory Foundation of the Chinese Academy of
Sciences (CXJJ-17S044), the Fundamental Research Funds
for the Central Universities (WK2100330002), SenseTime
Group Limited, the General Research Fund sponsored by
the Research Grants Council of Hong Kong (Project Nos.
CUHK14213616,
CUHK14206114,
CUHK14205615,
CUHK419412, CUHK14203015, CUHK14207814, and
CUHK14239816), the Hong Kong Innovation and Technology Support Programme (No.ITS/121/15FX), and ONR
N00014-15-1-2356.