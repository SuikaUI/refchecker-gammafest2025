Supervised Reinforcement Learning with Recurrent Neural
Network for Dynamic Treatment Recommendation
School of Computer Science and Software Engineering
East China Normal University
 
Wei Zhang*
School of Computer Science and Software Engineering
East China Normal University
 
Xiaofeng He*
School of Computer Science and Software Engineering
East China Normal University
 
Hongyuan Zha
Georgia Tech
 
Dynamic treatment recommendation systems based on large-scale
electronic health records (EHRs) become a key to successfully improve practical clinical outcomes. Prior relevant studies recommend
treatments either use supervised learning (e.g. matching the indicator signal which denotes doctor prescriptions), or reinforcement
learning (e.g. maximizing evaluation signal which indicates cumulative reward from survival rates). However, none of these studies
have considered to combine the benefits of supervised learning and
reinforcement learning. In this paper, we propose Supervised Reinforcement Learning with Recurrent Neural Network (SRL-RNN),
which fuses them into a synergistic learning framework. Specifically, SRL-RNN applies an off-policy actor-critic framework to
handle complex relations among multiple medications, diseases
and individual characteristics. The “actor” in the framework is adjusted by both the indicator signal and evaluation signal to ensure
effective prescription and low mortality. RNN is further utilized to
solve the Partially-Observed Markov Decision Process (POMDP)
problem due to the lack of fully observed states in real world applications. Experiments on the publicly real-world dataset, i.e., MIMIC-3,
illustrate that our model can reduce the estimated mortality, while
providing promising accuracy in matching doctors’ prescriptions.
Supervised Reinforcement Learning; Dynamic Treatment Regime;
Deep Sequential Recommendation
*Co-corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
Conference’17, July 2017, Washington, DC, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00
 
ACM Reference Format:
Lu Wang, Wei Zhang*, Xiaofeng He*, and Hongyuan Zha. 2018. Supervised Reinforcement Learning with Recurrent Neural Network for Dynamic Treatment Recommendation. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 10 pages. 
nnnnnnn.nnnnnnn
INTRODUCTION
Treatment recommendation has been studied for a long history.
Specially, medication recommendation systems have been verified
to support doctors in making better clinical decisions. Early treatment recommendation systems match diseases with medications
via classification based on expert systems . But it heavily relies on knowledge from doctors, and is difficult to achieve
personalized medicine. With the availability of electronic health
records (EHRs) in recent years, there are enormous interests to
exploit personalized healthcare data to optimize clinical decision
making. Thus the research on treatment recommendation shifts
from knowledge-driven into data-driven.
The data-driven research on treatment recommendation involves
two main branches: supervised learning (SL) and reinforcement
learning (RL) for prescription. SL based prescription tries to minimize the difference between the recommended prescriptions and
indicator signal which denotes doctor prescriptions. Several patternbased methods generate recommendations by utilizing the similarity of patients , but they are challenging to directly
learn the relation between patients and medications. Recently,
some deep models achieve significant improvements by learning a
nonlinear mapping from multiple diseases to multiple drug categories . Unfortunately, a key concern for these SL based
models still remains unresolved, i.e, the ground truth of “good” treatment strategy being unclear in the medical literature . More
importantly, the original goal of clinical decision also considers the
outcome of patients instead of only matching the indicator signal.
The above issues can be addressed by reinforcement learning for
dynamic treatment regime (DTR) . DTR is a sequence of tailored treatments according to the dynamic states of patients, which
conforms to the clinical practice. As a real example shown in Figure
1, treatments for the patient vary dynamically over time with the
accruing observations. The optimal DTR is determined by maximizing the evaluation signal which indicates the long-term outcome of
patients, due to the delayed effect of the current treatment and the
 
Conference’17, July 2017, Washington, DC, USA
Lu Wang et al.
12:00 16:00 20:00 24:00 4:00
8:00 12:00 16:00 20:00 24:00 4:00
8:00 12:00
Dextrose 5%
Gastric Meds
Furosemide
Replete Fiber
Figure 1: The record in MIMIC-3 shows a treatment process
of a patient with acute respiratory failure, chronic kidney
disease, etc, where seven medications are randomly selected.
The x-axis represents the time-line from admission time
12:00 to the discharge time 17:00 of the third day. Each icon
represents a prescribed medication for the patient.
influence of future treatment choices . With the desired properties of dealing with delayed reward and inferring optimal policy
based on non-optimal prescription behaviors, a set of reinforcement
learning methods have been adapted to generate optimal DTR for
life-threatening diseases, such as schizophrenia, non-small cell lung
cancer, and sepsis . Recently, some studies employ deep
RL to solve the DTR problem based on large scale EHRs .
Nevertheless, these methods may recommend treatments that are
obviously different from doctors’ prescriptions due to the lack of
the supervision from doctors, which may cause high risk in
clinical practice. In addition, the existing methods are challenging
for analyzing multiple diseases and the complex medication space.
In fact, the evaluation signal and indicator signal paly complementary roles , where the indicator signal gives a basic effectiveness and the evaluation signal helps optimize policy. Imitation
learning utilizes the indicator signal to estimate
a reward function for training robots by supposing the indicator
signal is optimal, which is not in line with the clinical reality. Supervised actor-critic uses the indicator signal to pre-train
a “guardian” and then combines “actor” output and “guardian” output to send low-risk actions for robots. However, the two types of
signals are trained separately and cannot learn from each other.
Inspired by these studies, we propose a novel deep architecture to
generate recommendations for more general DTR involving multiple diseases and medications, called Supervised Reinforcement
Learning with Recurrent Neural Network (SRL-RNN). The main
novelty of SRL-RNN is to combine the evaluation signal and indicator signal at the same time to learn an integrated policy. More
specifically, SRL-RNN consists of an off-policy actor-critic framework to learn complex relations among medications, diseases, and
individual characteristics. The “actor” in the framework is not only
influenced by the evaluation signal like traditional RL but also adjusted by the doctors’ behaviors to ensure safe actions. RNN is
further adopted to capture the dependence of the longitudinal and
temporal records of patients for the POMDP problem. Note that
treatment and prescription are used interchangeably in this paper.
Our contributions can be summarized as follows:
• We propose a new deep architecture SRL-RNN for handling
a more general DTR setting involving multiple diseases and
medications. It learns the prescription policy by combing
both the indicator signal and evaluation signal to avoid unacceptable risks and infer the optimal dynamic treatment.
• SRL-RNN applies an off-policy actor-critic framework to
handle complex relations among multiple medications, diseases, and individual characteristics. The “actor” is adjusted
by both the indicator signal and evaluation signal and RNN
is further utilized to solve POMDP (see Section 4.4).
• Quantitative experiments and qualitative case studies on
MIMIC-3 demonstrate that our method can not only reduce
the estimated mortality in the hospital (see Section 5.2) by
4.4%, but also provide better medication recommendation.
The rest of this paper is organized as follows. We summarize the
related work in Section 2 and provide necessary background knowledge in Section 3 for later introduction. In what follows, our model
is specified in Section 4. Experimental results are presented in Section 5. Finally, we conclude our paper in Section 6.
RELATED WORK
Early treatment recommendation systems heuristically map diseases into medications based on expert systems . Due to
the difficulty of knowledge acquisition, it comes into data-driven
approaches with two branches: supervised learning and reinforcement learning. In this section, we overview the related studies on
data-driven treatment recommendation, and the methodologies of
combing supervised learning and reinforcement learning.
Supervised learning for prescription focuses on minimizing
the difference between recommended prescriptions and doctors’
prescriptions. Both Cheerla et al. and Rosen et al. proposed to utilize genomic information for recommending suitable
treatments for patients with different diseases. However, genomic
information is not widely spread and easy to acquire. In order to
leverage massive EHRs to improve treatment recommendation,
several pattern-based methods generate treatments by the similarity among patients . Nevertheless, these methods are
challenging to directly learn the relationship between patients and
medications. Furthermore, it is challenging to calculate the similarities between patients’ complex longitudinal records. Recently,
two deep models are proposed to learn a nonlinear mapping from
diseases to drug categories based on EHRs, and achieve significant
improvements. Bajor et al. adopted a GRU model to predict the
total medication categories given historical diagnosis records of
patients. Zhang et al. proposed a deep model to not only learn
the relations between multiple diseases and multiple medication
categories, but also capture the dependence among medication categories. In addition, Wang et al. utilized a trilinear model to
integrate multi-source patient-specific information for personalized
A major concern for these SL-based prescription is that the behaviors of doctors are prone to be imperfect. Due to the knowledge
gap and limited experiences of doctors, the ground truth of “good”
treatment strategy is unclear in the medical literature . To address this issue, we prefer to use RL that is well-suited to infer
optimal policies based on non-optimal prescriptions.
Reinforcement learning for prescription gives the treatments
by maximizing the cumulated reward, where the reward can be
assessment scores of disease or survival rates of patients. Daniel et
al. employed tabular Q-learning to recommend medications for
schizophrenia patients on real clinical data. Zhao et al. applied
SRL-RNN for Dynamic Treatment Recommendation
Conference’17, July 2017, Washington, DC, USA
fitted Q-learning to discover optimal individualized medications
for non-small cell lung cancer (NSCLC) based on simulation data,
where a support vector regression (SVR) model is used to estimate
Q-function. Nemati et al. leveraged a combination of Hidden
Markov Models and deep Q-networks to predict optimal heparin
dosing for patients in ICU under a POMDP environment. Applying
these approaches to clinical practice is challenging for their use of
relatively small amount of data. Most recently, based on large scale
available EHRs, Weng et al. combined sparse autoencoder and
policy iteration to predict personalized optimal glycemic trajectories for severely ill septic patients, which reduces 6.3% mortality.
Prasad et al. used Q-learning to predict personalized sedation
dosage and ventilator support. Raghu et al. employed dual double deep Q-learning with continuous-state spaces to recommend
optimal dosages of intravenous fluid and maximum vasopressor for
However, without knowledgeable supervisors, the system may
recommend treatments that are significantly different from doctors’, which may cause unacceptable risks . Besides, these value
based methods are hard to handle multiple diseases and complex
medication space.
Methods of combing SL and RL utilize expert behaviors to
accelerate reinforcement learning and avoid the risks of actions.
Common examples are imitation learning and
supervised actor-critic . Given the samples of expert trajectories, imitation learning requires th skill of estimating a reward
function in which the reward of expert trajectories should enjoy the
highest rewards. Recently, imitation learning combines deep learning to produce successful applications in robotics . However,
these methods assume expert behaviors are optimal, which is not
in line with clinical reality. Besides, most of them learn the policy
based on the estimated reward function, instead of directly telling
learners how to act. Supervised actor-critic uses the indicator signal
to pre-train a “guardian”, and sends a low-risk action for robots by
the weighted sum of “actor” output and “guardian” output, but each
signal cannot learn from each other in the training process. Since
this type of model requires much more expert behaviors, it has
limited applications. In this paper, we focus on combining RL and
SL by utilizing the large amount of doctors’ prescription behaviors.
Our proposed model SRL-RNN is novel in that: (1) we train “actor”
with the indicator signal and evaluation signal jointly instead of
learning “guardian” and “actor” separately; (2) SRL-RNN combines
an off-policy RL and classification based SL models, while supervised actor-critic combines an on-policy RL and regression based
SL models; and (3) SRL-RNN enables capturing the dependence of
longitudinal and temporal records of patients to solve the problem
BACKGROUND
In this section, we give a definition of the Dynamic Treatment
Regime (DTR) problem and an overview of preliminaries for our
model. Some important notations mentioned in this paper are summarized in Table 1.
Table 1: Important notations.
Description
the number of medications or medication categories
action space A ∈{0, 1}K , a ∈A, ak ∈{0, 1} indicates
whether the k-th medication is chosen for a patient
state space S, s ∈S consists of static
demographic information, lab values, vital signs, etc.
reward function, if a patient survives, r=15,
if the patient dies, r=-15 , and r=0 otherwise
γ ∈ is a discount factor to balance the
importance of immediate and future rewards
deterministic policy learned from policy gradient
stochastic policy learned from policy gradient
a greedy policy learned from Q-learning
unknown policy of a doctor
behavior policy,
generate trajectories for off-policy learning
Q µ(s, a),
Q π (s, a)
Q-values, the expected return at state s
after taking action a following policy µ or π
estimated Q function
state value, the expected total
discounted reward from state s
in , the weighted parameter to balance RL and SL
Problem Formulation
In this paper, DTR is modeled as a Markov decision process (MDP)
with finite time steps and a deterministic policy consisting of an action space A, a state space S, and a reward function r : S×A →R.
At each time step t, a doctor observes the current state st ∈S of a
patient, chooses the medication ˆat ∈{0, 1}K from candidate set A
based on an unknown policy ˆµ(s), and receives a reward rt . Given
the current observations of diseases, demographics, lab values, vital
signs, and the output event of the patient which indicate the state
st , our goal is to learn a policy µθ (s) to select an action (medication) at by maximizing the sum of discounted rewards (return)
from time step t, which is defined as Rt = ÍT
i=t γ (i−t)r(si,ai), and
simultaneously minimizes the difference from clinician decisions
There are two types of methods to learn the policy: the valuebased RL learning a greedy policy µ(s) and the policy gradient RL
maintaining a parameterized stochastic policy πθ (a|s) or a deterministic policy µθ (s) (see 3.2 for details), where θ is the parameter
of the policy.
Model Preliminaries
Q-learning is an off-policy learning scheme that finds a greedy
policy µ(s) = arдmaxaQµ(s,a), where Qµ(s,a) denotes actionvalue or Q value and is used in a small discrete action space. For
deterministic policy, the Q value can be calculated with dynamic
programming as follows:
Qµ(st,at ) = Ert,st+1∼E[r(st,at ) + γQµ(st+1, µt+1)],
where E indicates the environment. Deep Q network (DQN) 
utilizes deep learning to estimate a non-linear Q function Qw(s,a)
parameterized by w. The strategy of replay buffer is adopted to
gain independent and identical distribution of samples for training.
Conference’17, July 2017, Washington, DC, USA
Lu Wang et al.
Demographic
Supervisor of
clinician decision
𝐀𝐜𝐭𝐨𝐫𝒕𝒂𝒓𝒈𝒆𝒕
Indicator signal
Evaluation signal
𝐂𝐫𝐢𝐭𝐢𝐜𝒕𝒂𝒓𝒈𝒆𝒕
Supervisor of
clinician decision
𝐀𝐜𝐭𝐨𝐫𝒕𝒂𝒓𝒈𝒆𝒕
𝐂𝐫𝐢𝐭𝐢𝐜𝒕𝒂𝒓𝒈𝒆𝒕
Supervisor of
clinician decision
𝐀𝐜𝐭𝐨𝐫𝒕𝒂𝒓𝒈𝒆𝒕
𝐂𝐫𝐢𝐭𝐢𝐜𝒕𝒂𝒓𝒈𝒆𝒕
Demographic
Demographic
Indicator signal
Evaluation signal
Indicator signal
Evaluation signal
Evaluation
Figure 2: The general framework of supervised reinforcement learning with recurrent neural network. Solid arrows indicate
the input diagnoses, demographics, rewards, clinician decisions timeseries variables, and historical states. Dashed arrows
represent the indicator signal and evaluation signal to update the actor (actortarдet ) network and critic (critictarдet ) network.
Moreover, DQN asynchronously updates a target network Qtar
minimize the least square loss as follows:
L(w) = Ert,st ∼E[(Qw(st,at ) −yt )2],
yt = r(st,at ) + γQtar
w (st+1, µ(st+1)).
Policy gradient is employed to handle continuous or high dimensional actions. To estimate the parameter θ of πθ , we maximize the expected return from the start states Eri,si∼πθ [R1] =
Eri,si∼πθ [V πθ (s1)], which is reformulated as: Eri,si∼πθ [V πθ (s1)] =
A πθ (a|s)r(s,a)dsda, whereV πθ (s1) is the state
value of the start state. ρπθ (s′) =
t=1 γ t−1p1(s)p(s →s′,t, πθ )ds
is discounted state distribution, where p1(s1) is the initial state distribution and p(s →s′,t, πθ ) is the probability at state s′ after
transition of t time steps from state s. Policy gradient learns the
parameter θ by the gradient ∇θ J(πθ ) which is calculated using the
policy gradient theorem :
∇θ J(πθ ) =
∇θ πθ (a|s)Qπθ (s,a)dads
= Es∼ρπ θ,a∈πθ [∇θloдπθ (a|s)Qπθ (s,a)],
where the instantaneous reward r(s,a) is replaced by the long-term
value Qπθ (s,a).
Actor-critic combines the advantages of Q-learning and
policy gradient to achieve accelerated and stable learning. It consists
of two components: (1) an actor to optimize the policy πθ in the
direction of gradient ∇θ J(πθ ) using Equation 3, and (2) a critic to
estimate an action-value function Qw(s,a) with the parameter w
through Equation 2. Finally, we obtain the policy gradient denoted
as ∇θ J(πθ ) = Es∼ρπθ ,a∈πθ [∇θloдπθ (a|s)Qw(s,a)].
In an off-policy setting, actor-critic estimates the value function
of πθ (a|s) by averaging the state distribution of behavior policy
β(a|s) . Instead of considering the stochastic policy πθ (s|a), the
deterministic policy gradient (DPG) theorem proves that policy
gradient framework can be extended to find deterministic off-policy
µθ (s), which is given as follows:
∇θ Jβ(µθ ) ≈
ρβ(s)∇θ µθ (s)Qµθ (s,a)ds
= Es∼ρ β [∇θ µθ (s)∇aQµθ (s,a)|a=µθ (s)].
Deep deterministic policy gradient (DDPG) adopts deep
learning to learn the actor and critic in mini batches which store a
replay buffer with tuples (st,at,rt,st+1). To ensure the stability of
Q values, DDPG uses a similar idea as the target network of DQN to
copy the actor and critic networks as µtar
(s) and Qtar
w (s,a). Instead
of directly copying the weights, DDPG uses a “soft” target update:
θtar ←τθ+(1−τ)θtar , wtar ←τw+(1−τ)wtar ,
SRL-RNN ARCHITECTURE
This section begins with a brief overview of our approach. After
that, we introduce the components of SRL-RNN and the learning
algorithm in detail.
The goal of our task is to learn a policy µθ (s) to recommend tailored treatments given the dynamic states of patients. SL learns
the policy µθ (s) by matching the indicator signals, which guarantees a standard and safe performance. But the “good” treatment
SRL-RNN for Dynamic Treatment Recommendation
Conference’17, July 2017, Washington, DC, USA
strategy is unclear and the original goal of clinical decisions also
tries to optimize the outcome of patients. RL learns the policy µθ (s)
by maximizing evaluation signals in a sequential dynamic system
which reflects the clinical facts and can infer the optimal policy
based on non-optimal strategies. But without supervision, RL may
produce unacceptable medications which give high risk. Intuitively,
the indicator signal and evaluation signal play complementary
roles (demonstrated in Section 5.4). Thus, SRL-RNN is proposed to
combine these two signals at the same time, where the cumulated
reward rt is the evaluation signal and the prescription ˆat of doctors
from the unknown policy ˆµ(s) is the indicator signal.
Figure 2 shows our proposed supervised reinforcement learning
architecture (SRL-RNN), which consists of three core networks:
Actor (Actortarдet ), Critic (Critictarдet ), and LSTM. The actor network recommends the time-varying medications according to the
dynamic states of patients, where a supervisor of doctors’ decisions
provides the indicator signal to ensure safe actions and leverages
the knowledge of doctors to accelerate learning process. The critic
network estimates the action value associated with the actor network to encourage or discourage the recommended treatments. Due
to the lack of fully observed states in the real world, LSTM is used
to extend SRL-RNN to handle POMDP by summarizing the entire
historical observations to capture a more complete observations.
Actor Network Update
The actor network learns a policy µθ (s) parameterized by θ to predict the time-varying treatments for patients, where the input is st
and the output is the prescription at recommended by µθ (s). We
employ reinforcement learning and supervised learning to optimize the parameter θ jointly. By combining the two learning tasks
together, we maximize the following objective function:
J(θ) = (1 −ϵ)JRL(θ) + ϵ(−JSL(θ)),
where JRL(θ) is the objective function of RL task (in Equation 8)
which tries to maximize the expected return, JSL(θ) is the objective
function of SL task (in Equation 10) which tries to minimize the difference from doctors’ prescriptions, and ϵ is a weight parameter to
trade off the reinforcement learning and supervised learning tasks.
Intuitively, our objective function aims to predicting mediations
which give both high expected returns and low errors. Mathematically, the parameter θ of the learned policy is updated by gradient
ascent as follows:
θ = θ + α((1 −ϵ)∇JRL(θ) + ϵ(−∇JSL(θ))),
whereα ∈ is a positive learning rate, and ∇JRL(θ) and ∇JSL(θ)
are acquired by the RL and SL tasks, respectively.
For the reinforcement learning task, we seek to learn the policy
µθ (s) by maximizing the state value of µθ (s) averaging over the
state distribution of the behaviors of doctors:
JRL(θ) = Es∼ρ ˆµ(s)[V µθ (s)] = Es∼ρ ˆµ(s)[Qw(s, µθ (s))].
Let the parameter θ in RL task be the weights of a neural network. θ
is updated by gradient ascent, θt+1 = θt +α(1 −ϵ)∇JRL(θt ), where
∇JRL(θt ) is calculated by the deterministic off-policy gradient using
Equation 4:
∇JRL(θt ) ≈Est ∼ρ ˆµ(s)[∇θ µθ (s)|s=st ∇aQw(s,a)|s=st,a=µθ (st )].
Let at be the treatment recommended by µθ (st ). ∇aQw(s,a) is
obtained by the chain rule, which is used to tell the medications
predicted by the actor are “good” or “bad”. When the gradient
∇aQw(st,at ) is positive, the policy µθ (s) will be pushed to be closer
to at . Otherwise, µθ (s) will be pushed away from at . ∇θ µθ (s) is a
Jacobian matrix where each column is the gradient ∇θ [µθ (s)]k of
k-th medication of µθ (s) with respect to θ.
For the supervised learning task, we try to learn the policy µθ (s)
through minimizing the difference between treatments predicted by
µθ (s) and prescriptions given from the doctor’s policy ˆµ(s), using
the cross entropy loss:
JSL(θt ) = Es∼ρ ˆµ(s)[−1
ˆat,kloдµk
(1 −ˆat,k)loд(1 −µk
θ (s)]|s=st ,
where K indicates the number of medications or medication categories, and ˆat,k ∈{0, 1} denotes whether the doctor chooses
k-th medication at time step t and µk
θ (s) is the probability of the
k-th medication predicted by µθ (s). The parameter θ in SL task is
updated by the gradient descent θt+1 = θt −αϵ∇JSL(θt ) where
∇JSL(θt ) is derived by the chain rule as follows:
∇JSL(θt ) = Es∼ρ ˆµ(s)[−ϕ∇θ µθ (s|θt )|s=st ],
where ϕ = 1
ˆat,k −aSL
. Substituting Equation 9 and Equation 11 into Equation 7 gives the final actor update:
θ = θ + α[(1 −ϵ)∇aQw(s,a)|s=st,a=µθ (st ) + ϵϕ]∇θ µθ (s)|s=st .
The above formulas show that we can use ϵ to trade off the RL
and SL tasks, where ϵ is a hyper parameter in this paper and the
effect of ϵ is shown in Figure 6.
Critic Network Update
The critic network is jointly learned with the actor network, where
the inputs are the states of patients, doctor prescriptions, actor
outputs, and rewards. We can do this due to the critic network is
only needed for guiding the actor during training, while only the
actor network is required at test time. The critic network uses a
neural network to learn the action-value function Qw(s,a) which
is used to update the parameters of the actor in the direction of
performance improvement ∇aQw(s,a). The output of the critic
network is the Q value Q(st,at ) of state st after taking action at ,
where Q(st,at ) is predicted by the Q-function Qw(s,a).
The update formula for parameter w of Qw(s,a) is similar to
DQN which is specified in Equation 1 and 2:
J(w) = Ert,st ∼ρ ˆµ(s)[(Qw(st, ˆat ) −yt )2]
yt = r(st, ˆat ) + γQtar
w (st+1, µθ (st+1)),
where µt+1(st+1) are the medications recommend by the actor network. w is updated by the gradient descent:
w = w −αQ(Qw(st, ˆat ) −yt )∇wQw(st,at ),
Conference’17, July 2017, Washington, DC, USA
Lu Wang et al.
where δ = (Qw(st, ˆat ) −yt ) is called Temporal Difference (TD)
error which is used for learning the Q-function.
Recurrent SRL
In the previous section, we assume the state st of a patient is fully
observed. In fact, we are always unable to obtain the full states of
the patient. Here we reformulate the environment of SRL-RNN as
POMDP. POMDP can be described as a 4-tuple (S, A, R, O), where
O are observations. We obtain the observation o ∈O directly
which conditions on p(ot |st ), with the not fully observable state
st . LSTM has been verified to improve performance on POMDP
by summarizing entire historical observations ct = f (o1,o2, ...,ot )
when using policy gradient and Q-learning . In LSTM, the
hidden units ct is to encapsulate the entire observation history o1:t .
In order to capture the dependence of the longitudinal and temporal
records of the patient, we employ a LSTM with SRL to represent
historical observation ct for a more complete observation.
The updates of the parameters of actor network and critic network are modified as follows:
([(1 −ϵ)∇aQw(c,a)|c=ct,a=µθ (ct )
+ϵϕ]∇θ µθ (c)|c=ct ),
(Qw(ct, ˆat ) −yt )∇wQw(ct,at ).
Putting all the aforementioned components together, the learning
algorithm of SRL-RNN is provided below.
Algorithm 1 SRL-RNN
Require: observations O, actions A, reward function r, # of epochs
N, weight parameter ϵ, # of medications K;
1: Store sequences (o1,a1,r1, ...,oT ,aT ,rT ) in buffer D;
2: Initialize actor µθ (c), target actor µθ (c)tar , critic Qw(c,a), target critic Qw(c,a)tar , and TD error δ = 0;
3: for n = 0 to N do
Sample (oi
T ) i = 1, .., I from D;
t ) given by LSTM
t ←action given by µθ (ci
t ←action given by doctor ˆµ(ci
w ←w + α 1
▷update critic
wtar ←τw + (1 −τ)wtar
▷update target critic
t ) ←given by Qw(ci
θ ←θ + α 1
t [(1 −ϵ)∇aQw(ci
▷update actor
θtar ←τθ + (1 −τ)θtar
▷update target actor
16: end for
EXPERIMENTS
In this section, we present the quantitative and qualitative experiment results on MIMIC-3.
Dataset and Cohort
The experiments are conducted on a large and publicly available
dataset, namely the Multiparameter Intelligent Monitoring in Intensive Care (MIMIC-3 v1.4) database . MIMIC-3 encompasses
a very large population of patients compared to other public EHRs.
It contains hospital admissions of 43K patients in critical care units
during 2001 and 2012, involving 6,695 distinct diseases and 4,127
drugs. To ensure statistical significance, we extract the top 1,000
medications and top 2,000 diseases (represented by ICD-9 codes)
which cover 85.4% of all medication records and 95.3% of all diagnosis records, respectively. In order to experiment on different
granularity of medications, we map the 1,000 medications into the
third level of ATC1 (medication codes) using a public tool2, resulting 180 distinct ATC codes. Therefore, the action space size of the
experiments is 1,000 exact medications (K = 1000) or 180 drug
categories (K = 180).
For each patient, we extract relevant physiological parameters
with the suggestion of clinicians, which include static variables and
time-series variables. The static variables cover eight kinds of demographics: gender, age, weight, height, religion, language, marital
status, and ethnicity. The time-series variables contain lab values,
vital signs, and output events, such as diastolic blood pressure,
fraction of inspiration O2, Glascow coma scale, blood glucose, systolic blood pressure, heart rate, pH, respiratory rate, blood oxygen
saturation, body temperature, and urine output. These features correspond to the state s in MDP or the observation o in POMDP. We
impute the missing variable with k-nearest neighbors and remove
admissions with more than 10 missing variables. Each hospital admission of a patient is regarded as a treatment plan. Time-series
data in each treatment plan is divided into different units, each of
which is set to 24 hours since it is the median of the prescription
frequency in MIMIC-3. If several data points are in one unit, we
use their average values instead. Following , we remove patients less than 18 years old because of the special conditions of
minors. Finally, we obtain 22,865 hospital admissions, and randomly
divide the dataset for training, validation, and testing sets by the
proportion of 80/10/10.
Evaluation Metrics
Evaluation methodology in treatment recommendation is still a
chanllenge. Thus we try all evaluation metrics used in state-of-art
methods to judge our model.
Following , we use the estimated in-hospital mortality
rates to measure whether policies would eventually reduce the
patient mortality or not. Specifically, we discretize the learned Qvalues of each test example into different units with small intervals
shown in the x-axis of Figure 3. Given an example denoting an
admission of a patient, if the patient died in hospital, all the Qvalues belonging to this admission are associated with a value of 1
as mortality and the corresponding units add up these values. After
1 and principles/
2 
SRL-RNN for Dynamic Treatment Recommendation
Conference’17, July 2017, Washington, DC, USA
scanning all test examples, the average estimated mortality rates for
each unit are calculated, shown in y-axis of Figure 3. Based on these
results, the mortality rates corresponding to the expected Q-values
of different policies are used as the measurements to denote the
estimated in-hospital mortality (see details in ). Although
the estimated mortality does not equal the mortality in real clinical
setting, it is a universal metric currently for computational testing.
Inspired by , we further utilize mean Jaccard coefficient to
measure the degree of consistency between prescriptions generated
by different methods and those from doctors. For a patient pi in
the t-th day of ICU, let ˆU i
t be the medication set given by doctors
t be the medication set recommended from learned policies.
The mean Jaccard is defined as 1
t | , where M
is the number of patients and Ti is the number of ICU days for the
patient pi.
Comparison Methods
All the adopted baselines models in the experiments are as follows, where BL, RL, and SD3Q are the alternatives of SRL-RNN we
- Popularity-20 (POP-20): POP-20 is a patten-based method, which
chooses the top-K most co-occurring medications with the target
diseases as prescriptions. We set K = 20 for its best performance
on the validation dataset.
- Basic-LSTM (BL): BL uses LSTM to recommend the sequential
medications based on the longitudinal and temporal records of
patients. Inspired by Doctor-AI , BL fuses multi-sources of
patient-specific information and considers each admission of a
patient as a sequential treatment to satisfy the DTR setting. BL
consists of a 1-layer MLP (M-1) to model diseases, a 1-layer MLP
(M-2) to model static variables, and a 1-layer LSTM sequential
model (L-1) to capture the time-series variables. These outputs are
finally concatenated to predict prescriptions at each time-step.
- Reward-LSTM (RL): RL has the same framework as BL, except
that it considers another signal, i.e., feedback of mortality, to
learn a nontrivial policy. The model involves three steps: (1)
clustering the continuous states into discrete states, (2) learning
the Q-values using tabular Q-learning, (3) and training the model
by simultaneously mimicking medications generated by doctors
and maximizing cumulated reward of the policy.
- Dueling Double-Deep Q learning (D3Q) : D3Q is a reinforcement learning method which combines dueling Q, double
Q, and deep Q together. D3Q regards a treatment plan as DTR.
- Supervised Dueling Double-Deep Q (SD3Q): Instead of separately learning the Q-values and policy as RL, SD3Q learns them
jointly. SD3Q involves a D3Q architecture, where supervised
learning is additionally adopted to revise the value function.
- Supervised Actor Critic (SAC) : SAC uses the indicator
signal to pre-train a “guardian” and then combines “actor” output
and “guardian” output to send low-risk actions for robots. We
transform it into a deep model for a fair comparison.
- LEAP : LEAP leverages a MLP framework to train a multilabel model with the consideration of the dependence among
medications. LEAP takes multiple diseases as input and multiple
medications as output. Instead of considering each admission as
Table 2: Performance comparison on test sets for prescription prediction. l-3 ATC indicates the third level of ATC code
and Medications indicates the exact drugs.
Estimated Mortality
Granularity
Medications
Medications
Dynamic treatment setting
Static treatment setting
SRL-RNN (agg)
a sequential treatment process, LEAP regards each admission as
a static treatment setting. We aggregate the multiple prescriptions recommended by SRL-RNN as SRL-RNN (agg) for a fair
comparison.
- LG : LG takes diseases as input and adopts a 3-layer GRU
model to predict the multiple medications.
Result Analysis
Model comparisons. Table 2 depicts the mortality rates and Jaccard scores for all the adopted models on MIMIC-3. By first comparing the results of LG, BL, and RL, we find that RL outperforms
both BL and LG, showing that incorporating the evaluation signal into supervised learning can indeed improve the results. We
then compare SD3Q with its simplified version D3Q. The better
performance of SD3Q indicates the knowledgeable supervision
guarantees a standard performance of the learned policy. In the
static treatment setting, LEAP improves the basic method POP-20
by a large margin, which demonstrates that capturing the relations
between multiple diseases and multiple medications is beneficial
for better prescriptions.
Finally, our proposed model SRL-RNN performs significantly
better than all the adopted baselines, both in the dynamic treatment
setting and static treatment setting. The reasons are: 1) SRL-RNN
regards the treatment recommendation as a sequential decision
process, reflecting the clinical practice (compared with LEAP and
POP-20), and utilizes evaluation signal to infer an optimal treatment (compared with LG and BL); 2) SRL-RNN considers the prescriptions of doctors as supervision information to learn a robust
policy (compared with D3Q), and applies the off-policy actor-critic
framework to handle complex relations of medications, diseases,
and individual characteristics (compared with SD3Q); 3) SRL-RNN
integrates supervised learning and reinforcement learning in an
end-to-end learning fashion for sharing the information between
evaluation signal and indicator signal (compared with RL and SAC);
and 4) SRL-RNN adopts RNN to solve POMDP by obtaining the
representations of the entire historical observations.
Conference’17, July 2017, Washington, DC, USA
Lu Wang et al.
Expected Return
Mortality Rate
(a) all-demo-disease
Expected Return
Mortality Rate
(b) all-demo
Expected Return
Mortality Rate
Expected Return
Mortality Rate
(d) all-demo-disease
Expected Return
Mortality Rate
(e) all-demo
Expected Return
Mortality Rate
Figure 3: Mortality-expected-return curve computed by the learned policies.
Table 3: Ablation study of the features. all indicates all the
features: demographic, diseases and time-series variables.
The symbol “-” stands for “subtracting”.
Estimated Mortality
all-demo-disease
all-demo-disease
all-demo-disease
Ablation study. The different contributions of the three types of
features are reported in this part. To be specific, we progressively
add the patient-specific information, i.e., time series variables, diseases, and demographics, into the selected models. As shown in
Table 3, the Jaccard scores of the three methods monotonically
increase. In addition, the estimated mortality rates of SRL-RNN
monotonically decrease. However, the estimated mortality variations of BL and RL are not the same as that of SLR-RNN. It might
be due to the fact that with constant Q-values learned by tabular
Q-learning, learning a robust policy is a little hard.
Effectiveness and stability of policy. The relations between expected returns and mortality rates are shown in Figure 3. We observe SRL-RNN has a more clear negative correlation between
expected returns and mortality rates than BL and RL. The reason
might be that BL ignores the evaluation signal while RL discretizes
the continuous states, incurring information loss.
Difference between optimal policy and physician deision
Mortality Rate
Figure 4: Comparison of how observed mortality rates (yaxis) vary with the difference between the prescriptions generated by the optimal policy and the prescriptions administered by doctors (x-axis).
Figure 4 shows how the observed mortality changes with the
difference between the learned policies (by RL and SRL-RNN) and
doctors’ prescriptions. Let Di
t be the treatment difference for patient
pi in the t-th day, and K is the number of candidate classes of medications. We calculate the difference by Di
When the difference is minimum, we obtain the lowest mortality
rates of 0.021 and 0.016 for RL and SRL-RNN, respectively. This
phenomenon shows that SRL-RNN and RL can both learn good policies while SRL-RNN slightly outperforms RL for its lower mortality
Figure 5 presents the expected return and Jaccard scores obtained
in each learning epoch of SRL-RNN with different features. Notably,
SRL-RNN is able to combine reinforcement learning and supervised
learning to obtain the optimal policy in a stable manner. We can
SRL-RNN for Dynamic Treatment Recommendation
Conference’17, July 2017, Washington, DC, USA
see Jaccard is learned faster than Q-values, indicating that leaning
Q-values might need rich trajectories.
Ecpected return
all-demo-disease
all-demo-disease
Figure 5: Expected return and Jaccard in different learning
epochs with different features.
Case studies. Table 4 shows the prescriptions generated by different models for two patients in different ICU days. For the first
patient who is finally dead, LG recommends much more medications which seem to be non-informative. LEAP generates the same
medications for patients in different ICU days, without considering the change of patient states. An interesting observation is that
the prescriptions recommended by SRL-RNN are much different
from doctors’, especially for the stronger tranquilizers such as Acetaminophen and Morphine Sulfate. Actually, for an ICU patient
with a severe trauma, it is important to give full sedation in early
treatment. Considering the second surviving patient, the similarity
between the prescriptions of SRL-RNN and doctor becomes larger,
indicating the rationality of SRL-RNN. Particularly, all the models
except SRL-RNN recommend redundant medications in different
days, while the prescriptions of doctors are significantly different.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Figure 6: The effect of ϵ.
Effect of ϵ. Figure 6 shows the effectiveness of the weight parameter ϵ in Equation 12, which is used to balance RL and SL. It achieves
the highest Jaccard scores and lowest mortality rates when taking
the values of 0.5 and 0.6, which verifies that SRL-RNN can not only
significantly reduce the estimated mortality but also recommend
better medications as well.
CONCLUSION
In this paper, we propose the novel Supervised Reinforcement
Learning with Recurrent Neural Network (SRL-RNN) model for
DTR, which combines the indicator signal and evaluation signal
through the joint supervised and reinforcement learning. SRL-RNN
incorporates the off-policy actor-critic architecture to discover optimal dynamic treatments and further adopts RNN to solve the
POMDP problem. The comprehensive experiments on the real
world EHR dataset demonstrate SRL-RNN can reduce the estimated
mortality in hospital by up to 4.4% and provide better medication
recommendation as well.
ACKNOWLEDGEMENTS
We thank Aniruddh Raghu and Matthieu Komorowski to help us
preprocess the data. This work was partially supported by the
National Key Research and Development Program of China under
Grant No. 2016YFB1000904, NSFC (61702190), and NSFC-Zhejiang
(U1609220).