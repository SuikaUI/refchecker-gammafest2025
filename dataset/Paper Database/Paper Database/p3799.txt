Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317–1327,
Austin, Texas, November 1-5, 2016. c⃝2016 Association for Computational Linguistics
Sequence-Level Knowledge Distillation
 
Alexander M. Rush
 
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA, USA
Neural machine translation (NMT) offers a
novel alternative formulation of translation
that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying
knowledge distillation approaches that have proven
successful for reducing the size of neural models in other domains to the problem of NMT.
We demonstrate that standard knowledge distillation applied to word-level prediction can
be effective for NMT, and also introduce two
novel sequence-level versions of knowledge
distillation that further improve performance,
and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best
student model runs 10 times faster than its
state-of-the-art teacher with little loss in performance. It is also signiﬁcantly better than
a baseline model trained without knowledge
distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in
a student model that has 13× fewer parameters than the original teacher model, with a
decrease of 0.4 BLEU.
Introduction
Neural machine translation (NMT) is a deep learningbased method for translation that has recently shown
promising results as an alternative to statistical approaches. NMT systems directly model the probability of the next word in the target sentence simply by conditioning a recurrent neural network on
the source sentence and previously generated target
While both simple and surprisingly accurate,
NMT systems typically need to have very high capacity in order to perform well: Sutskever et al.
 used a 4-layer LSTM with 1000 hidden units
per layer (herein 4×1000) and Zhou et al. obtained state-of-the-art results on English →French
with a 16-layer LSTM with 512 units per layer. The
sheer size of the models requires cutting-edge hardware for training and makes using the models on
standard setups very challenging.
This issue of excessively large networks has been
observed in several other domains, with much focus on fully-connected and convolutional networks
for multi-class classiﬁcation. Researchers have particularly noted that large networks seem to be necessary for training, but learn redundant representations in the process . Therefore
compressing deep models into smaller networks has
been an active area of research. As deep learning
systems obtain better results on NLP tasks, compression also becomes an important practical issue with
applications such as running deep learning models
for speech and translation locally on cell phones.
Existing compression methods generally fall into
two categories: (1) pruning and (2) knowledge distillation. Pruning methods , zero-out weights or
entire neurons based on an importance criterion: Le-
Cun et al. use (a diagonal approximation to)
the Hessian to identify weights whose removal minimally impacts the objective function, while Han
et al. remove weights based on thresholding their absolute values. Knowledge distillation approaches learn a smaller student network
to mimic the original teacher network by minimizing the loss (typically L2 or cross-entropy) between
the student and teacher output.
In this work, we investigate knowledge distillation in the context of neural machine translation. We
note that NMT differs from previous work which has
mainly explored non-recurrent models in the multiclass prediction setting. For NMT, while the model
is trained on multi-class prediction at the word-level,
it is tasked with predicting complete sequence outputs conditioned on previous decisions. With this
difference in mind, we experiment with standard
knowledge distillation for NMT and also propose
two new versions of the approach that attempt to approximately match the sequence-level (as opposed
to word-level) distribution of the teacher network.
This sequence-level approximation leads to a simple training procedure wherein the student network
is trained on a newly generated dataset that is the
result of running beam search with the teacher network.
We run experiments to compress a large state-ofthe-art 4 × 1000 LSTM model, and ﬁnd that with
sequence-level knowledge distillation we are able to
learn a 2 × 500 LSTM that roughly matches the performance of the full system. We see similar results
compressing a 2 × 500 model down to 2 × 100 on
a smaller data set.
Furthermore, we observe that
our proposed approach has other beneﬁts, such as
not requiring any beam search at test-time. As a result we are able to perform greedy decoding on the
2 × 500 model 10 times faster than beam search on
the 4 × 1000 model with comparable performance.
Our student models can even be run efﬁciently on
a standard smartphone.1 Finally, we apply weight
pruning on top of the student network to obtain a
model that has 13× fewer parameters than the original teacher model. We have released all the code for
the models described in this paper.2
1 
2 
Background
Sequence-to-Sequence with Attention
Let s = [s1, . . . , sI] and t = [t1, . . . , tJ] be (random
variable sequences representing) the source/target
sentence, with I and J respectively being the
source/target lengths. Machine translation involves
ﬁnding the most probable target sentence given the
where T is the set of all possible sequences. NMT
models parameterize p(t | s) with an encoder neural
network which reads the source sentence and a decoder neural network which produces a distribution
over the target sentence (one word at a time) given
the source. We employ the attentional architecture
from Luong et al. , which achieved state-ofthe-art results on English →German translation.3
Knowledge Distillation
Knowledge distillation describes a class of methods
for training a smaller student network to perform
better by learning from a larger teacher network
(in addition to learning from the training data set).
We generally assume that the teacher has previously
been trained, and that we are estimating parameters for the student. Knowledge distillation suggests
training by matching the student’s predictions to the
teacher’s predictions. For classiﬁcation this usually
means matching the probabilities either via L2 on
the log scale or by crossentropy .
Concretely, assume we are learning a multi-class
classiﬁer over a data set of examples of the form
(x, y) with possible classes V. The usual training
criteria is to minimize NLL for each example from
the training data,
LNLL(θ) = −
1{y = k} log p(y = k | x; θ)
where 1{·} is the indicator function and p the
distribution from our model (parameterized by θ).
3Speciﬁcally, we use the global-general attention model
with the input-feeding approach. We refer the reader to the original paper for further details.
Figure 1: Overview of the different knowledge distillation approaches. In word-level knowledge distillation (left) cross-entropy
is minimized between the student/teacher distributions (yellow) for each word in the actual target sequence (ECD), as well as
between the student distribution and the degenerate data distribution, which has all of its probabilitiy mass on one word (black). In
sequence-level knowledge distillation (center) the student network is trained on the output from beam search of the teacher network
that had the highest score (ACF). In sequence-level interpolation (right) the student is trained on the output from beam search of
the teacher network that had the highest sim with the target sequence (ECE).
This objective can be seen as minimizing the crossentropy between the degenerate data distribution
(which has all of its probability mass on one class)
and the model distribution p(y | x; θ).
In knowledge distillation, we assume access to
a learned teacher distribution q(y | x; θT ), possibly
trained over the same data set. Instead of minimizing cross-entropy with the observed data, we instead
minimize the cross-entropy with the teacher’s probability distribution,
LKD(θ; θT ) = −
q(y = k | x; θT )×
log p(y = k | x; θ)
where θT parameterizes the teacher distribution and
remains ﬁxed. Note the cross-entropy setup is identical, but the target distribution is no longer a sparse
distribution.4 Training on q(y | x; θT ) is attractive
since it gives more information about other classes
for a given data point (e.g.
similarity between
classes) and has less variance in gradients .
4 In some cases the entropy of the teacher/student distribution is increased by annealing it with a temperature term τ > 1
˜p(y | x) ∝p(y | x)
After testing τ ∈{1, 1.5, 2} we found that τ = 1 worked best.
Since this new objective has no direct term for the
training data, it is common practice to interpolate
between the two losses,
L(θ; θT ) = (1 −α)LNLL(θ) + αLKD(θ; θT )
where α is mixture parameter combining the one-hot
distribution and the teacher distribution.
Knowledge Distillation for NMT
The large sizes of neural machine translation systems make them an ideal candidate for knowledge
distillation approaches. In this section we explore
three different ways this technique can be applied to
Word-Level Knowledge Distillation
NMT systems are trained directly to minimize word
NLL, LWORD-NLL, at each position.
Therefore if
we have a teacher model, standard knowledge distillation for multi-class cross-entropy can be applied.
We deﬁne this distillation for a sentence as,
LWORD-KD = −
q(tj = k | s, t<j) ×
log p(tj = k | s, t<j)
where V is the target vocabulary set. The student
can further be trained to optimize the mixture of
LWORD-KD and LWORD-NLL. In the context of NMT,
we refer to this approach as word-level knowledge
distillation and illustrate this in Figure 1 (left).
Sequence-Level Knowledge Distillation
Word-level knowledge distillation allows transfer of
these local word distributions. Ideally however, we
would like the student model to mimic the teacher’s
actions at the sequence-level. The sequence distribution is particularly important for NMT, because
wrong predictions can propagate forward at testtime.
First, consider the sequence-level distribution
speciﬁed by the model over all possible sequences
p(t | s) =
p(tj | s, t<j)
for any length J. The sequence-level negative loglikelihood for NMT then involves matching the onehot distribution over all complete sequences,
LSEQ-NLL = −
1{t = y} log p(t | s)
1{yj = k} log p(tj = k | s, t<j)
where y = [y1, . . . , yJ] is the observed sequence.
Of course, this just shows that from a negative
log likelihood perspective, minimizing word-level
NLL and sequence-level NLL are equivalent in this
But now consider the case of sequence-level
knowledge distillation. As before, we can simply
replace the distribution from the data with a probability distribution derived from our teacher model.
However, instead of using a single word prediction,
we use q(t | s) to represent the teacher’s sequence
distribution over the sample space of all possible sequences,
LSEQ-KD = −
q(t | s) log p(t | s)
Note that LSEQ-KD is inherently different from
LWORD-KD, as the sum is over an exponential number of terms.
Despite its intractability, we posit
that this sequence-level objective is worthwhile. It
gives the teacher the chance to assign probabilities to
complete sequences and therefore transfer a broader
range of knowledge. We thus consider an approximation of this objective.
Our simplest approximation is to replace the
teacher distribution q with its mode,
q(t | s) ∼1{t = argmax
Observing that ﬁnding the mode is itself intractable,
we use beam search to ﬁnd an approximation. The
loss is then
1{t = ˆy} log p(t | s)
−log p(t = ˆy | s)
where ˆy is now the output from running beam search
with the teacher model.
Using the mode seems like a poor approximation
for the teacher distribution q(t | s), as we are approximating an exponentially-sized distribution with
a single sample. However, previous results showing
the effectiveness of beam search decoding for NMT
lead us to belief that a large portion of q’s mass lies
in a single output sequence. In fact, in experiments
we ﬁnd that with beam of size 1, q(ˆy | s) (on average) accounts for 1.3% of the distribution for German →English, and 2.3% for Thai →English (Table 1: p(t = ˆy)).5
To summarize, sequence-level knowledge distillation suggests to: (1) train a teacher model, (2) run
beam search over the training set with this model, (3)
train the student network with cross-entropy on this
new dataset. Step (3) is identical to the word-level
NLL process except now on the newly-generated
data set. This is shown in Figure 1 (center).
5Additionally there are simple ways to better approximate
q(t | s). One way would be to consider a K-best list from beam
search and renormalizing the probabilities,
q(t | s) ∼
t∈TK q(t | s)
where TK is the K-best list from beam search. This would
increase the training set by a factor of K.
A beam of size
5 captures 2.8% of the distribution for German →English,
and 3.8% for Thai →English. Another alternative is to use a
Monte Carlo estimate and sample from the teacher model (since
LSEQ-KD = Et∼q(t | s)[ −log p(t | s) ]). However in practice we
found the (approximate) mode to work well.
Sequence-Level Interpolation
Next we consider integrating the training data back
into the process, such that we train the student
model as a mixture of our sequence-level teachergenerated data (LSEQ-KD) with the original training
data (LSEQ-NLL),
L = (1 −α)LSEQ-NLL + αLSEQ-KD
= −(1 −α) log p(y | s) −α
q(t | s) log p(t | s)
where y is the gold target sequence.
Since the second term is intractable, we could
again apply the mode approximation from the previous section,
L = −(1 −α) log p(y | s) −α log p(ˆy | s)
and train on both observed (y) and teachergenerated (ˆy) data. However, this process is nonideal for two reasons: (1) unlike for standard knowledge distribution, it doubles the size of the training
data, and (2) it requires training on both the teachergenerated sequence and the true sequence, conditioned on the same source input. The latter concern
is particularly problematic since we observe that y
and ˆy are often quite different.
As an alternative, we propose a single-sequence
approximation that is more attractive in this setting.
This approach is inspired by local updating , a method for discriminative training in statistical machine translation (although to
our knowledge not for knowledge distillation). Local updating suggests selecting a training sequence
which is close to y and has high probability under
the teacher model,
˜y = argmax
sim(t, y)q(t | s)
where sim is a function measuring closeness ).
Following local updating, we can approximate this
sequence by running beam search and choosing
˜y ≈argmax
where TK is the K-best list from beam search.
We take sim to be smoothed sentence-level BLEU
 .
We justify training on ˜y from a knowledge distillation perspective with the following generative process: suppose that there is a true target sequence
(which we do not observe) that is ﬁrst generated
from the underlying data distribution D. And further
suppose that the target sequence that we observe (y)
is a noisy version of the unobserved true sequence:
i.e. (i) t ∼D, (ii) y ∼ϵ(t), where ϵ(t) is, for example, a noise function that independently replaces
each element in t with a random element in V with
some small probability.6 In such a case, ideally the
student’s distribution should match the mixture distribution,
DSEQ-Inter ∼(1 −α)D + αq(t | s)
In this setting, due to the noise assumption, D now
has signiﬁcant probability mass around a neighborhood of y (not just at y), and therefore the argmax
of the mixture distribution is likely something other
than y (the observed sequence) or ˆy (the output from
beam search). We can see that ˜y is a natural approximation to the argmax of this mixture distribution
between D and q(t | s) for some α. We illustrate
this framework in Figure 1 (right) and visualize the
distribution over a real example in Figure 2.
Experimental Setup
To test out these approaches, we conduct two sets of
NMT experiments: high resource (English →German) and low resource (Thai →English).
The English-German data comes from WMT
2014.7 The training set has 4m sentences and we
take newstest2012/newstest2013 as the dev set and
newstest2014 as the test set. We keep the top 50k
most frequent words, and replace the rest with UNK.
The teacher model is a 4 × 1000 LSTM ) and we train two student models:
2 × 300 and 2 × 500. The Thai-English data comes
from IWSLT 2015.8 There are 90k sentences in the
6While we employ a simple (unrealistic) noise function for
illustrative purposes, the generative story is quite plausible if we
consider a more elaborate noise function which includes additional sources of noise such as phrase reordering, replacement
of words with synonyms, etc. One could view translation having two sources of variance that should be modeled separately:
variance due to the source sentence (t ∼D), and variance due
to the individual translator (y ∼ϵ(t)).
7 
8 
Figure 2: Visualization of sequence-level interpolation on an
example German →English sentence: Bis 15 Tage vor Anreise sind Zimmer-Annullationen kostenlos. We run beam
search, plot the ﬁnal hidden state of the hypotheses using t-SNE
and show the corresponding (smoothed) probabilities with contours. In the above example, the sentence that is at the top of
the beam after beam search (green) is quite far away from gold
(red), so we train the model on a sentence that is on the beam
but had the highest sim (e.g. BLEU) to gold (purple).
training set and we take 2010/2011/2012 data as the
dev set and 2012/2013 as the test set, with a vocabulary size is 25k. Size of the teacher model is 2×500
(which performed better than 4×1000, 2×750 models), and the student model is 2×100. Other training
details mirror Luong et al. .
multi-bleu.perl,
experiment
the following variations:
Word-Level Knowledge Distillation (Word-KD)
Student is trained on the original data and additionally trained to minimize the cross-entropy of the
teacher distribution at the word-level.
α ∈{0.5, 0.9} and found α = 0.5 to work better.
Sequence-Level Knowledge Distillation (Seq-KD)
Student is trained on the teacher-generated data,
which is the result of running beam search and taking the highest-scoring sequence with the teacher
model. We use beam size K = 5 (we did not see
improvements with a larger beam).
Sequence-Level Interpolation (Seq-Inter)
Student is trained on the sequence on the teacher’s beam
that had the highest BLEU (beam size K = 35). We
adopt a ﬁne-tuning approach where we begin training from a pretrained model (either on original data
or Seq-KD data) and train with a smaller learning
rate (0.1). For English-German we generate Seq-
Inter data on a smaller portion of the training set
(∼50%) for efﬁciency.
The above methods are complementary and can
be combined with each other.
For example, we
can train on teacher-generated data but still include a word-level cross-entropy term between the
teacher/student (Seq-KD + Word-KD in Table 1),
or ﬁne-tune towards Seq-Inter data starting from the
baseline model trained on original data (Baseline +
Seq-Inter in Table 1).9
Results and Discussion
Results of our experiments are shown in Table
We ﬁnd that while word-level knowledge distillation (Word-KD) does improve upon the baseline, sequence-level knowledge distillation (Seq-
KD) does better on English →German and performs similarly on Thai →English.
them (Seq-KD + Word-KD) results in further gains
for the 2 × 300 and 2 × 100 models (although not
for the 2 × 500 model), indicating that these methods provide orthogonal means of transferring knowledge from the teacher to the student: Word-KD is
transferring knowledge at the the local (i.e. word)
level while Seq-KD is transferring knowledge at the
global (i.e. sequence) level.
Sequence-level interpolation (Seq-Inter), in addition to improving models trained via Word-KD and
Seq-KD, also improves upon the original teacher
model that was trained on the actual data but ﬁnetuned towards Seq-Inter data (Baseline + Seq-Inter).
In fact, greedy decoding with this ﬁne-tuned model
has similar performance (19.6) as beam search with
the original model (19.5), allowing for faster decoding even with an identically-sized model.
We hypothesize that sequence-level knowledge
distillation is effective because it allows the student
network to only model relevant parts of the teacher
distribution (i.e. around the teacher’s mode) instead
of ‘wasting’ parameters on trying to model the entire
9For instance, ‘Seq-KD + Seq-Inter + Word-KD’ in Table
1 means that the model was trained on Seq-KD data and ﬁnetuned towards Seq-Inter data with the mixture cross-entropy
loss at the word-level.
English →German WMT 2014
Teacher Baseline 4 × 1000 (Params: 221m)
Baseline + Seq-Inter
Student Baseline 2 × 500 (Params: 84m)
Baseline + Seq-Inter
Word-KD + Seq-Inter
Seq-KD + Seq-Inter
Seq-KD + Word-KD
Seq-KD + Seq-Inter + Word-KD
Student Baseline 2 × 300 (Params: 49m)
Baseline + Seq-Inter
Word-KD + Seq-Inter
Seq-KD + Seq-Inter
Seq-KD + Word-KD
Seq-KD + Seq-Inter + Word-KD
Thai →English IWSLT 2015
Teacher Baseline 2 × 500 (Params: 47m)
Baseline + Seq-Inter
Student Baseline 2 × 100 (Params: 8m)
Baseline + Seq-Inter
Word-KD + Seq-Inter
Seq-KD + Seq-Inter
Seq-KD + Word-KD
Seq-KD + Seq-Inter + Word-KD
Table 1: Results on English-German (newstest2014) and Thai-English test sets. BLEUK=1: BLEU score with beam
size K = 1 (i.e. greedy decoding); ∆K=1: BLEU gain over the baseline model without any knowledge distillation with greedy
decoding; BLEUK=5: BLEU score with beam size K = 5; ∆K=5: BLEU gain over the baseline model without any knowledge
distillation with beam size K = 5; PPL: perplexity on the test set; p(t = ˆy): Probability of output sequence from greedy decoding
(averaged over the test set). Params: number of parameters in the model. Best results (as measured by improvement over the
baseline) within each category are highlighted in bold.
space of translations. Our results suggest that this
is indeed the case: the probability mass that Seq-
KD models assign to the approximate mode is much
higher than is the case for baseline models trained
on original data (Table 1: p(t = ˆy)). For example,
on English →German the (approximate) argmax
for the 2 × 500 Seq-KD model (on average) accounts for 16.9% of the total probability mass, while
the corresponding number is 0.9% for the baseline.
This also explains the success of greedy decoding
for Seq-KD models—since we are only modeling
around the teacher’s mode, the student’s distribution
is more peaked and therefore the argmax is much
easier to ﬁnd. Seq-Inter offers a compromise between the two, with the greedily-decoded sequence
accounting for 7.6% of the distribution.
Finally, although past work has shown that models with lower perplexity generally tend to have
Model Size
Beam = 1 (Greedy)
Table 2: Number of source words translated per second across
GPU (GeForce GTX Titan X), CPU, and smartphone (Samsung
Galaxy 6) for the various English →German models. We were
unable to open the 4 × 1000 model on the smartphone.
higher BLEU, our results indicate that this is not
necessarily the case. The perplexity of the baseline
2 × 500 English →German model is 8.2 while the
perplexity of the corresponding Seq-KD model is
22.7, despite the fact that Seq-KD model does signiﬁcantly better for both greedy (+4.2 BLEU) and
beam search (+1.4 BLEU) decoding.
Decoding Speed
Run-time complexity for beam search grows linearly
with beam size. Therefore, the fact that sequencelevel knowledge distillation allows for greedy decoding is signiﬁcant, with practical implications for
running NMT systems across various devices. To
test the speed gains, we run the teacher/student models on GPU, CPU, and smartphone, and check the
average number of source words translated per second (Table 2). We use a GeForce GTX Titan X for
GPU and a Samsung Galaxy 6 smartphone. We ﬁnd
that we can run the student model 10 times faster
with greedy decoding than the teacher model with
beam search on GPU (1051.3 vs 101.9 words/sec),
with similar performance.
Weight Pruning
Although knowledge distillation enables training
faster models, the number of parameters for the
student models is still somewhat large (Table 1:
Params), due to the word embeddings which dominate most of the parameters.10 For example, on the
10Word embeddings scale linearly while RNN parameters
scale quadratically with the dimension size.
Table 3: Performance of student models with varying % of the
weights pruned. Top two rows are models without any pruning.
Params: number of parameters in the model; Prune %: Percentage of weights pruned based on their absolute values; BLEU:
BLEU score with beam search decoding (K = 5) after retraining the pruned model; Ratio: Ratio of the number of parameters
versus the original teacher model (which has 221m parameters).
2 × 500 English →German model the word embeddings account for approximately 63% (50m out
of 84m) of the parameters. The size of word embeddings have little impact on run-time as the word
embedding layer is a simple lookup table that only
affects the ﬁrst layer of the model.
We therefore focus next on reducing the memory footprint of the student models further through
weight pruning. Weight pruning for NMT was recently investigated by See et al. , who found
that up to 80 −90% of the parameters in a large
NMT model can be pruned with little loss in performance. We take our best English →German student
model (2 × 500 Seq-KD + Seq-Inter) and prune x%
of the parameters by removing the weights with the
lowest absolute values. We then retrain the pruned
model on Seq-KD data with a learning rate of 0.2
and ﬁne-tune towards Seq-Inter data with a learning
rate of 0.1. As observed by See et al. , retraining proved to be crucial. The results are shown
in Table 3.
Our ﬁndings suggest that compression beneﬁts
achieved through weight pruning and knowledge
distillation are orthogonal.11
Pruning 80% of the
weight in the 2 × 500 student model results in a
model with 13× fewer parameters than the original
teacher model with only a decrease of 0.4 BLEU.
While pruning 90% of the weights results in a more
appreciable decrease of 1.0 BLEU, the model is
11To our knowledge combining pruning and knowledge distillation has not been investigated before.
drastically smaller with 8m parameters, which is
26× fewer than the original teacher model.
Further Observations
• For models trained with word-level knowledge
distillation, we also tried regressing the student
network’s top-most hidden layer at each time
step to the teacher network’s top-most hidden
layer as a pretraining step, noting that Romero
et al. obtained improvements with a
similar technique on feed-forward models. We
found this to give comparable results to standard knowledge distillation and hence did not
pursue this further.
• There have been promising recent results on
eliminating word embeddings completely and
obtaining word representations directly from
characters with character composition models,
which have many fewer parameters than word
embedding lookup tables .
Combining such methods with knowledge distillation/pruning to further reduce the memory
footprint of NMT systems remains an avenue
for future work.
Related Work
Compressing deep learning models is an active area
of current research. Pruning methods involve pruning weights or entire neurons/nodes based on some
criterion. LeCun et al. prune weights based
on an approximation of the Hessian, while Han et al.
 show that a simple magnitude-based pruning
works well. Prior work on removing neurons/nodes
include Srinivas and Babu and Mariet and
Sra . See et al. were the ﬁrst to apply pruning to Neural Machine Translation, observing that that different parts of the architecture (input word embeddings, LSTM matrices, etc.) admit
different levels of pruning. Knowledge distillation
approaches train a smaller student model to mimic
a larger teacher model, by minimizing the loss between the teacher/student predictions .
Romero et al. additionally regress on the intermediate hidden layers of the
student/teacher network as a pretraining step, while
Mou et al. obtain smaller word embeddings
from a teacher model via regression. There has also
been work on transferring knowledge across different network architectures: Chan et al. show
that a deep non-recurrent neural network can learn
from an RNN; Geras et al. train a CNN to
mimic an LSTM for speech recognition. Kuncoro
et al. recently investigated knowledge distillation for structured prediction by having a single
parser learn from an ensemble of parsers.
Other approaches for compression involve low
rank factorizations of weight matrices , sparsity-inducing regularizers
 , binarization of weights
 , and
weight sharing .
Finally, although we have motivated sequence-level
knowledge distillation in the context of training a
smaller model, there are other techniques that train
on a mixture of the model’s predictions and the data,
such as local updating , hope/fear
training , SEARN , DAgger , and minimum
risk training .
Conclusion
In this work we have investigated existing knowledge distillation methods for NMT (which work at
the word-level) and introduced two sequence-level
variants of knowledge distillation, which provide
improvements over standard word-level knowledge
distillation.
We have chosen to focus on translation as this
domain has generally required the largest capacity
deep learning models, but the sequence-to-sequence
framework has been successfully applied to a wide
range of tasks including parsing , summarization , dialogue
 , NER/POS-tagging ,
image captioning , video generation , and
speech recognition . We anticipate that methods described in this paper can be used
to similarly train smaller models in other domains.