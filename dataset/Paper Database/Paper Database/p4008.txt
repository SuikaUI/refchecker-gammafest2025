MIT Open Access Articles
Rethinking Few-Shot Image Classification:
a Good Embedding Is All You Need?
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Tian, Yonglong et al. “Rethinking Few-Shot Image Classification: a Good Embedding Is
All You Need?” In Proceedings of the ECCV 2020: Computer Vision – ECCV 2020, Glasgow, UK,
August 23-28, 2020, Springer, : 266-282 © 2020 The Author(s)
As Published: 
Publisher: Springer
Persistent URL: 
Version: Original manuscript: author's manuscript prior to formal peer review
Terms of use: Creative Commons Attribution-Noncommercial-Share Alike
Rethinking Few-Shot Image Classiﬁcation: a Good Embedding Is All You Need?
Yonglong Tian1*
Yue Wang1*
Dilip Krishnan2
Joshua B. Tenenbaum1
Phillip Isola1
1MIT CSAIL
2Google Research
The focus of recent meta-learning research has been on
the development of learning algorithms that can quickly
adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of
the standard benchmarks in meta-learning. In this work, we
show that a simple baseline: learning a supervised or selfsupervised representation on the meta-training set, followed
by training a linear classiﬁer on top of this representation,
outperforms state-of-the-art few-shot learning methods. An
additional boost can be achieved through the use of selfdistillation. This demonstrates that using a good learned
embedding model can be more effective than sophisticated
meta-learning algorithms. We believe that our ﬁndings motivate a rethinking of few-shot image classiﬁcation benchmarks and the associated role of meta-learning algorithms.
Code is available at: 
1. Introduction
Few-shot learning measures a model’s ability to quickly
adapt to new environments and tasks. This is a challenging problem because only limited data is available to adapt
the model. Recently, signiﬁcant advances have been made to tackle
this problem using the ideas of meta-learning or “learning
to learn”.
Meta-learning deﬁnes a family of tasks, divided into disjoint meta-training and meta-testing sets. Each task consists of limited training data, which requires fast adaptability of the learner (e.g., the deep network that is
ﬁne-tuned).
During meta-training/testing, the learner is
trained and evaluated on a task sampled from the task distribution. The performance of the learner is evaluated by
the average test accuracy across many meta-testing tasks.
Methods to tackle this problem can be cast into two main
categories: optimization-based methods and metric-based
methods. Optimization-based methods focus on designing
algorithms that can quickly adapt to each task; while metric-
*: equal contribution.
based methods aim to ﬁnd good metrics (usually kernel
functions) to side-step the need for inner-loop optimization
for each task.
Meta-learning is evaluated on a number of domains such
as few-shot classiﬁcation and meta-reinforcement learning.
Focusing on few-shot classiﬁcation tasks, a question that
has been raised in recent work is whether it is the metalearning algorithm or the learned representation that is responsible for the fast adaption to test time tasks. suggested that feature reuse is main factor for fast adaptation.
Recently, proposed transductive ﬁne-tuning as a strong
baseline for few-shot classiﬁcation; and even in a regular, inductive, few-shot setup, they showed that ﬁne-tuning
is only slightly worse than state-of-the-art algorithms. In
this setting, they ﬁne-tuned the network on the meta-testing
set and used information from the testing data. Besides,
 shows an improved ﬁne-tuning model performs slightly
worse than meta-learning algorithms.
In this paper, we propose an extremely simple baseline
that suggests that good learned representations are more
powerful for few-shot classiﬁcation tasks than the current
crop of complicated meta-learning algorithms. Our baseline
consists of a linear model learned on top of a pre-trained
embedding. Surprisingly, we ﬁnd this outperforms all other
meta-learning algorithms on few-shot classiﬁcation tasks,
often by large margins. The differences between our approach and that of are: we do not utilize information
from testing data (since we believe that inductive learning
is more generally applicable to few-shot learning); and we
use a ﬁxed neural network for feature extraction, rather than
ﬁne-tuning it on the meta-testing set. The ﬁndings in concurrent works are inline with our simple baseline.
Our model learns representations by training a neural
network on the entire meta-training set: we merge all metatraining data into a single task and a neural network is asked
to perform either ordinary classiﬁcation or self-supervised
learning, on this combined dataset. The classiﬁcation task
is equivalent to the pre-training phase of TADAM and
LEO . After training, we keep the pre-trained network
up to the penultimate layer and use it as a feature extractor.
During meta-testing, for each task, we ﬁt a linear classiﬁer
on the features extracted by the pre-trained network. In contrast to and , we do not ﬁne-tune the neural network.
 
Furthermore, we show that self-distillation on this baseline provides an additional boost.
Self-distillation is a
form of knowledge distillation , where the student and
teacher models are identical in architecture and task. We
apply self-distillation to the pre-trained network.
Contributions.
Our key contributions are:
• A surprisingly simple baseline for few-shot learning,
which achieves the state-of-the-art. This baseline suggests that many recent meta-learning algorithms are
no better than simply learning a good representation
through a proxy task, e.g., image classiﬁcation.
• Building upon the simple baseline, we use selfdistillation to further improve performance.
• Our combined method achieves an average of 3% improvement over the previous state-of-the-art on widely
used benchmarks.
On the new benchmark Meta-
Dataset , our method outperforms previous best results by more than 7% on average.
• Beyond supervised training, we show that representations learned with state-of-the-art self-supervised
methods achieve similar performance as fully supervised methods. Thus we can “learn to learn” simply
by learning a good self-supervised embedding.
2. Related works
Metric-based meta-learning.
The core idea in metricbased meta-learning is related to nearest neighbor algorithms and kernel density estimation. Metric-based methods embed input data into ﬁxed dimensional vectors and
use them to design proper kernel functions. The predicted
label of a query is the weighted sum of labels over support samples. Metric-based meta-learning aims to learn a
task-dependent metric. used Siamese network to encode image pairs and predict conﬁdence scores for each
pair. Matching Networks employed two networks for
query samples and support samples respectively and used
an LSTM with read-attention to encode a full context embedding of support samples. Prototypical Networks 
learned to encode query samples and support samples into a
shared embedding space; the metric used to classify query
samples is the distance to prototype representations of each
class. Instead of using distances of embeddings, Relation
Networks leveraged relational module to represent an
appropriate metric. TADAM proposed metric scaling
and metric task conditioning to boost the performance of
Prototypical Networks.
Optimization-based
meta-learning.
models are neither designed to train with very few examples
nor to converge very fast. To ﬁx that, optimization-based
methods intend to learn with a few examples. Meta-learner
 exploited an LSTM to satisfy two main desiderata
of few-shot learning: quick acquisition of task-dependent
knowledge and slow extraction of transferable knowledge.
MAML proposed a general optimization algorithm; it
aims to ﬁnd a set of model parameters, such that a small
number of gradient steps with a small amount of training
data from a new task will produce large improvements
on that task.
In that paper, ﬁrst-order MAML was also
proposed, which ignored the second-order derivatives of
MAML. It achieved comparable results to complete MAML
with orders of magnitude speedup.
To further simplify
MAML, Reptile removed re-initialization for each
task, making it a more natural choice in certain settings.
LEO proposed that it is beneﬁcial to decouple the
optimization-based meta-learning algorithms from highdimensional model parameters. In particular, it learned a
stochastic latent space from which the high-dimensional
parameters can be generated. MetaOptNet replaced the
linear predictor with an SVM in the MAML framework; it
incorporated a differentiable quadratic programming (QP)
solver to allow end-to-end learning. For a complete list of
recent works on meta-learning, we refer readers to .
Towards understanding MAML.
To understand why
MAML works in the ﬁrst place, many efforts have been
made either through an optimization perspective or a generalization perspective. Reptile showed a variant of
MAML works even without re-initialization for each task,
because it tends to converge towards a solution that is close
to each task’s manifold of optimal solutions. In , the authors analyzed whether the effectiveness of MAML is due
to rapid learning of each task or reusing the high quality
features. It concluded that feature reuse is the dominant
component in MAMLs efﬁcacy, which is reafﬁrmed by experiments conducted in this paper.
Meta-learning datasets.
Over the past several years,
many datasets have been proposed to test meta-learning or
few-shot learning algorithms. Omniglot was one of the
earliest few-shot learning datasets; it contains thousands of
handwritten characters from the world’s alphabets, intended
for one-shot ”visual Turing test”. In , the authors reported the 3-year progress for the Omniglot challenge, concluding that human-level one-shot learnability is still hard
for current meta-learning algorithms. introduced mini-
ImageNet, which is a subset of ImageNet . In , a
large portion of ImageNet was used for few-shot learning
Meta-dataset summarized recent datasets and
tested several representative methods in a uniform fashion.
Knowledge distillation.
The idea of knowledge distillation (KD) dates back to . The original idea was to compress the knowledge contained in an ensemble of models
into a single smaller model. In , the authors generalized
this idea and brought it into the deep learning framework.
In KD, knowledge is transferred from the teacher model to
the student model by minimizing a loss in which the target is the distribution of class probabilities induced by the
teacher model. In was shown in that KD has several
beneﬁts for optimization and knowledge transfer between
tasks. BAN introduced sequential distillation, which
also improved the performance of teacher models. In natural language processing (NLP), BAM used BAN to distill from single-task models to a multi-task model, helping
the multi-task model surpass its single-task teachers. Another two related works are which provides theoretical
analysis of self-distillation and CRD which shows distillation improves the transferability across datasets.
We establish preliminaries about the meta-learning problem and related algorithms in §3.1; then we present our
baseline in §3.2; ﬁnally, we introduce how knowledge distillation helps few-shot learning in §3.3. For ease of comparison to previous work, we use the same notation as .
3.1. Problem formulation
The collection of meta-training tasks is deﬁned as T =
i=1, termed as meta-training set. The tuple (Dtrain
) describes a training and a testing dataset
of a task, where each dataset contains a small number of
examples. Training examples Dtrain = {(xt, yt)}T
testing examples Dtest = {(xq, yq)}Q
q=1 are sampled from
the same distribution.
A base learner A, which is given by y∗= fθ(x∗) (∗denotes t or q), is trained on Dtrain and used as a predictor on
Dtest. Due to the high dimensionality of x∗, the base learner
A suffers high variance. So training examples and testing
examples are mapped into a feature space by an embedding
model Φ∗= fφ(x∗). Assume the embedding model is ﬁxed
during training the base learner on each task, then the objective of the base learner is
θ = A(Dtrain; φ)
Lbase(Dtrain; θ, φ) + R(θ),
where L is the loss function and R is the regularization
The objective of the meta-learning algorithms is to learn
a good embedding model, so that the average test error of
the base learner on a distribution of tasks is minimized. Formally,
φ = arg min
ET [Lmeta(Dtest; θ, φ)],
Figure 1. In meta-training, we train on an image classiﬁcation task
on the merged meta-training data to learn an embedding model.
This model is then re-used at meta-testing time to extract embedding for a simple linear classiﬁer.
where θ = A(Dtrain; φ).
Once meta-training is ﬁnished, the performance of the
model is evaluated on a set of held-out tasks S
j=1, called meta-testing set. The evaluation is done over the distribution of the test tasks:
ES[Lmeta(Dtest; θ, φ), where θ = A(Dtrain; φ)].
3.2. Learning embedding model through classiﬁcation
As we show in §3.1, the goal of meta-training is to learn
a transferrable embedding model fφ, which generalizes to
any new task. Rather than designing new meta-learning algorithms to learn the embedding model, we propose that a
model pre-trained on a classiﬁcation task can generate powerful embeddings for the downstream base learner. To that
end, we merge tasks from meta-training set into a single
task, which is given by
Dnew = {(xi, yi)}K
= ∪{Dtrain
, . . . , Dtrain
, . . . , Dtrain
where Dtrain
is the task from T . The embedding model is
φ = arg min
Lce(Dnew; φ),
and Lce denotes the cross-entropy loss between predictions
and ground-truth labels. We visualize the task in Figure 1.
As shown in Figure 2, for a task (Dtrain
) sampled from meta-testing distribution, we train a base learner
. The base learner is instantiated as multivariate
logistic regression. Its parameters θ = {W , b} include a
weight term W and a bias term b, given by
θ = arg min
t (W fφ(xt) + b, yt) + R(W , b). (6)
We also evaluate other base learners such as nearest neighbor classiﬁer with L-2 distance and/or cosine distance in
Figure 2. We show a meta-testing case for 5-way 1-shot task: 5 support images and 1 query image are transformed into embeddings using
the ﬁxed neural network; a linear model (logistic regression (LR) in this case) is trained on 5 support embeddings; the query image is tested
using the linear model.
Figure 3. Sequential self-distillation: a vanilla model, termed as
Generation 0, is trained with standard cross-entropy loss; then,
the k-th generation is learned with knowledge distilled from the
(k-1)-th generation.
In our method, the crucial difference between metatraining and meta-testing is the embedding model parameterized by φ is carried over from meta-training to metatesting and kept unchanged when evaluated on tasks sampled from meta-testing set. The base learner is re-initialized
for every task and trained on Dtrain of meta-testing task.
Our method is the same with the pre-training phase of methods used in . Unlike other methods , we
do not ﬁne-tune the embedding model fφ during the metatesting stage.
3.3. Sequential self-distillation
Knowledge distillation is an approach to transfer
knowledge embedded in an ensemble of models to a single model, or from a larger teacher model to a smaller student model. Instead of using the embedding model directly
for meta-testing, we distill the knowledge from the embedding model into a new model with an identical architecture,
training on the same merged meta-training set. The new
embedding model parameterized by φ′ is trained to minimize a weighted sum of the cross-entropy loss between the
predictions and ground-truth labels and the KullbackLeibler
divergence (KL) between predictions and soft targets predicted by fφ:
φ′ = arg min
(αLce(Dnew; φ′)+
βKL(f(Dnew; φ′), f(Dnew; φ))),
where usually β = 1 −α.
We exploit the Born-again strategy to apply KD sequentially to generate multiple generations, which is shown
in Figure 3. At each step, the embedding model of k-th
generation is trained with knowledge transferred from the
embedding model of (k-1)-th generation:
φk = arg min
(αLce(Dnew; φ)+
βKL(f(Dnew; φ), f(Dnew; φk−1))).
Assume we repeat the operation K times, we use φK as the
embedding model to extract features for meta-testing. We
analyze the effects of sequential self-distillation in §4.6.
4. Experiments
We conduct experiments on four widely used few-shot
image recognition benchmarks: miniImageNet , tiered-
ImageNet , CIFAR-FS , and FC100 . The ﬁrst
two are derivatives of ImageNet , while the last two are
reorganized from the standard CIFAR-100 dataset .
Additional results on Meta-Dataset is presented in §5.
4.1. Setup
Architecture. Following previous works , we use a ResNet12 as our backbone: the network consists of 4 residual blocks, where each has 3 convolutional
layers with 3×3 kernel; a 2×2 max-pooling layer is applied
after each of the ﬁrst 3 blocks; and a global average-pooling
layer is on top of the fourth block to generate the feature embedding. Similar to , we use Dropblock as a regularizer
miniImageNet 5-way
tieredImageNet 5-way
32-32-32-32
48.70 ± 1.84
63.11 ± 0.92
51.67 ± 1.81
70.30 ± 1.75
Matching Networks 
64-64-64-64
43.56 ± 0.84
55.31 ± 0.73
64-64-64-64
49.2 ± 0.7
64.7 ± 0.7
Prototypical Networks† 
64-64-64-64
49.42 ± 0.78
68.20 ± 0.66
53.31 ± 0.89
72.69 ± 0.74
64-64-64-64
51.77 ± 1.86
66.05 ± 0.85
64-64-64-64
52.22 ± n/a
66.49 ± n/a
64-64-64-64
53.21 ± 0.80
72.34 ± 0.64
KTN(Visual) 
64-64-64-64
54.61 ± 0.80
71.21 ± 0.66
64-64-64-64
55.22 ± 0.84
71.55 ± 0.66
Dynamic Few-shot 
64-64-128-128
56.20 ± 0.86
73.00 ± 0.64
Relation Networks 
64-96-128-256
50.44 ± 0.82
65.32 ± 0.70
54.48 ± 0.93
71.32 ± 0.78
96-192-384-512
51.2 ± 0.6
68.8 ± 0.1
SNAIL 
55.71 ± 0.99
68.88 ± 0.92
AdaResNet 
56.88 ± 0.62
71.94 ± 0.57
TADAM 
58.50 ± 0.30
76.70 ± 0.30
Shot-Free 
59.04 ± n/a
77.64 ± n/a
63.52 ± n/a
82.59 ± n/a
TEWAM 
60.07 ± n/a
75.90 ± n/a
61.20 ± 1.80
75.50 ± 0.80
Variational FSL 
61.23 ± 0.26
77.69 ± 0.17
MetaOptNet 
62.64 ± 0.61
78.63 ± 0.46
65.99 ± 0.72
81.56 ± 0.53
Diversity w/ Cooperation 
59.48 ± 0.65
75.62 ± 0.48
Fine-tuning 
57.73 ± 0.62
78.17 ± 0.49
66.58 ± 0.70
85.55 ± 0.48
LEO-trainval† 
61.76 ± 0.08
77.59 ± 0.12
66.33 ± 0.05
81.44 ± 0.09
Ours-simple
62.02 ± 0.63
79.64 ± 0.44
69.74 ± 0.72
84.41 ± 0.55
Ours-distill
64.82 ± 0.60
82.14 ± 0.43
71.52 ± 0.69
86.03 ± 0.49
Table 1. Comparison to prior work on miniImageNet and tieredImageNet. Average few-shot classiﬁcation accuracies (%) with 95%
conﬁdence intervals on miniImageNet and tieredImageNet meta-test splits. Results reported with input image size of 84x84. a-b-c-d
denotes a 4-layer convolutional network with a, b, c, and d ﬁlters in each layer. † results obtained by training on the union of training and
validation sets.
and change the number of ﬁlters from (64,128,256,512) to
(64,160,320,640). As a result, our ResNet12 is identical to
that used in .
Optimization setup. We use SGD optimizer with a momentum of 0.9 and a weight decay of 5e−4. Each batch
consists of 64 samples. The learning rate is initialized as
0.05 and decayed with a factor of 0.1 by three times for
all datasets, except for miniImageNet where we only decay
twice as the third decay has no effect. We train 100 epochs
for miniImageNet, 60 epochs for tieredImageNet, and 90
epochs for both CIFAR-FS and FC100. During distillation,
we use the same learning schedule and set α = β = 0.5.
Data augmentation. When training the embedding network on transformed meta-training set, we adopt random
crop, color jittering, and random horizontal ﬂip as in .
For meta-testing stage, we train an N-way logistic regression base classiﬁer. We use the implementations in scikitlearn for the base classiﬁer.
4.2. Results on ImageNet derivatives
The miniImageNet dataset is a standard benchmark for few-shot learning algorithms for recent works. It
consists of 100 classes randomly sampled from the ImageNet; each class contains 600 downsampled images of size
84x84. We follow the widely-used splitting protocol proposed in , which uses 64 classes for meta-training, 16
classes for meta-validation, and the remaining 20 classes for
meta-testing.
The tieredImageNet dataset is another subset of ImageNet but has more classes (608 classes). These classes
are ﬁrst grouped into 34 higher-level categories, which are
further divided into 20 training categories (351 classes), 6
validation categories (97 classes), and 8 testing categories
(160 classes). Such construction ensures the training set is
distinctive enough from the testing set and makes the problem more challenging.
Results. During meta-testing, we evaluate our method with
3 runs, where in each run the accuracy is the mean accuracy
CIFAR-FS 5-way
FC100 5-way
32-32-32-32
58.9 ± 1.9
71.5 ± 1.0
Prototypical Networks 
64-64-64-64
55.5 ± 0.7
72.0 ± 0.6
35.3 ± 0.6
48.6 ± 0.6
Relation Networks 
64-96-128-256
55.0 ± 1.0
69.3 ± 0.8
96-192-384-512
65.3 ± 0.2
79.4 ± 0.1
TADAM 
40.1 ± 0.4
56.1 ± 0.4
Shot-Free 
69.2 ± n/a
84.7 ± n/a
TEWAM 
70.4 ± n/a
81.3 ± n/a
Prototypical Networks 
72.2 ± 0.7
83.5 ± 0.5
37.5 ± 0.6
52.5 ± 0.6
MetaOptNet 
72.6 ± 0.7
84.3 ± 0.5
41.1 ± 0.6
55.5 ± 0.6
Ours-simple
71.5 ± 0.8
86.0 ± 0.5
42.6 ± 0.7
59.1 ± 0.6
Ours-distill
73.9 ± 0.8
86.9 ± 0.5
44.6 ± 0.7
60.9 ± 0.6
Table 2. Comparison to prior work on CIFAR-FS and FC100. Average few-shot classiﬁcation accuracies (%) with 95% conﬁdence
intervals on CIFAR-FS and FC100. a-b-c-d denotes a 4-layer convolutional network with a, b, c, and d ﬁlters in each layer.
of 1000 randomly sampled tasks. We report the median of
3 runs in Table 1. Our simple baseline with ResNet-12 is already comparable with the state-of-the-art MetaOptNet 
on miniImageNet, and outperforms all previous works by at
least 3% on tieredImageNet. The network trained with distillation further improves over the simple baseline by 2-3%.
We notice that previous works have
also leveraged the standard cross-entropy pre-training on
the meta-training set. In , a wide ResNet (WRN-28-
10) is trained to classify all classes in the meta-training set
(or combined meta-training and meta-validation set), and
then frozen during the meta-training stage. also conducts pre-training but the model is ﬁne-tuned using the support images in meta-testing set, achieving 57.73 ± 0.62.
We adopt the same architecture and gets 61.1 ± 0.86. So
ﬁne-tuning on small set of samples makes the performance
worse. Another work adopts a multi-task setting by
jointly training on the standard classiﬁcation task and fewshot classiﬁcation (5-way) task. In another work , the
ResNet-12 is pre-trained before mining hard tasks for the
meta-training stage. In this work, we show standard crossentropy pre-training is sufﬁcient to generate strong embeddings without meta-learning techniques or any ﬁne-tuning.
4.3. Results on CIFAR derivatives
The CIFAR-FS dataset is a derivative of the original CIFAR-100 dataset by randomly splitting 100 classes
into 64, 16 and 20 classes for training, validation, and testing, respectively. The FC100 dataset is also derived
from CIFAR-100 dataset in a similar way to tieredImagNnet. This results in 60 classes for training, 20 classes for
validation, and 20 classes for testing.
Results. Similar to previous experiments, we evaluate our
method with 3 runs, where in each run the accuracy is
the mean accuracy of 3000 randomly sampled tasks. TaminiImageNet 5-way
Supervised
57.56 ± 0.79
73.81 ± 0.63
54.19 ± 0.93
73.04 ± 0.61
56.10 ± 0.89
73.87 ± 0.65
Comparsions of embeddings from supervised pretraining and self-supervised pre-training (Moco and CMC). ∗the
encoder of each view is 0.5× width of a normal ResNet-50.
ble 2 summarizes the results, which shows that our simple
baseline is comparable to Prototypical Networks and
MetaOptNet on CIFAR-FS dataset, and outperforms
both of them on FC100 dataset. Our distillation version
achieves the new state-of-the-art on both datasets. This veriﬁes our hypothesis that a good embedding plays an important role in few-shot recognition.
4.4. Embeddings from self-supervised representation learning
Using unsupervised learning to improve
the generalization of the meta-learning algorithms removes the needs of data annotation. In addition to using
embeddings from supervised pre-training, we also train a
linear classiﬁer on embeddings from self-supervised representation learning. Following MoCo and CMC 
(both are inspired by InstDis ), we train a ResNet50 
(without using labels) on the merged meta-training set to
learn an embedding model.
We compare unsupervised
ResNet50 to a supervised ResNet50. From Table 3, we observe that using embeddings from self-supervised ResNet50
is only slightly worse than using embeddings from supervised ResNet50 (in 5-shot setting, the results are comminiImageNet
tieredImageNet
Table 4. Ablation study on four benchmarks with ResNet-12 as backbone network. “NN” and “LR” stand for nearest neighbour
classiﬁer and logistic regression. “L-2” means feature normalization after which feature embeddings are on the unit sphere. “Aug”
indicates that each support image is augmented into 5 samples to train the classiﬁer. “Distill” represents the use of knowledge distillation.
miniImageNet
Evaluation on different generations of distilled networks. The 0-th generation (or root generation) indicates the vanilla
network trained with only standard classiﬁcation cross-entropy loss. The k-th generation is trained by combining the standard classiﬁcation
loss and the knowledge distillation (KD) loss using the (k-1)-th generation as the teacher model. Logistic regression (LR) and nearest
neighbours (NN) are evaluated.
This observation shows the potential of selfsupervised learning in the scenario of few-shot learning.
4.5. Ablation experiments
In this section, we conduct ablation studies to analyze
how each component affects the few-shot recognition performance. We study the following ﬁve components of our
method: (a) we chose logistic regression as our base learner,
and compare it to a nearest neighbour classiﬁer with euclidean distance; (b) we ﬁnd that normalizing the feature
vectors onto the unit sphere, e.g., L-2 normalization, could
improve the classiﬁcation of the downstream base classi-
ﬁer; (c) during meta-testing, we create 5 augmented samples from each support image to alleviate the data insufﬁciency problem, and using these augmented samples to train
the linear classiﬁer; (d) we distill the embedding network on
the training set by following the sequential distillation 
Table 4 shows the results of our ablation studies on mini-
ImageNet, tieredImageNet, CIFAR-FS, and FC100. In general, logistic regression signiﬁcantly outperforms the nearest neighbour classiﬁer, especially for the 5-shot case; L-2
normalization consistently improves the 1-shot accuracy by
2% on all datasets; augmenting the support images leads to
marginal improvement; even with all these techniques, distillation can still provide 2% extra gain.
4.6. Effects of distillation
We can use sequential self-distillation to get an embedding model, similar to the one in Born-again networks .
We therefore investigate the effect of this strategy on the
performance of downstream few-shot classiﬁcation.
In addition to logistic regression and nearest-neighbour
classiﬁers, we also look into a cosine similarity classiﬁer,
which is equivalent to the nearest-neighbour classiﬁer but
with normalized features (noted as “NN+Norm.”).
miniImageNet 5-way
tieredImageNet 5-way
64-64-64-64
55.25 ± 0.58
71.56 ± 0.52
56.18 ± 0.70
72.99 ± 0.55
Ours-distill
64-64-64-64
55.88 ± 0.59
71.65 ± 0.51
56.76 ± 0.68
73.21 ± 0.54
Ours-trainval
64-64-64-64
56.32 ± 0.58
72.46 ± 0.52
56.53 ± 0.68
73.15 ± 0.58
Ours-distill-trainval
64-64-64-64
56.64 ± 0.58
72.85 ± 0.50
57.35 ± 0.70
73.98 ± 0.56
62.02 ± 0.63
79.64 ± 0.44
69.74 ± 0.72
84.41 ± 0.55
Ours-distill
64.82 ± 0.60
82.14 ± 0.43
71.52 ± 0.69
86.03 ± 0.49
Ours-trainval
63.59 ± 0.61
80.86 ± 0.47
71.12 ± 0.68
85.94 ± 0.46
Ours-distill-trainval
66.58 ± 0.65
83.22 ± 0.39
72.98 ± 0.71
87.46 ± 0.44
SEResNet-12
62.29 ± 0.60
79.94 ± 0.46
70.31 ± 0.70
85.22 ± 0.50
Ours-distill
SEResNet-12
65.96 ± 0.63
82.05 ± 0.46
71.72 ± 0.69
86.54 ± 0.49
Ours-trainval
SEResNet-12
64.07 ± 0.61
80.92 ± 0.43
71.76 ± 0.66
86.27 ± 0.45
Ours-distill-trainval
SEResNet-12
67.73 ± 0.63
83.35 ± 0.41
72.55 ± 0.69
86.72 ± 0.49
Table 5. Comparisons of different backbones on miniImageNet and tieredImageNet.
CIFAR-FS 5-way
FC100 5-way
64-64-64-64
62.7 ± 0.8
78.7 ± 0.5
39.6 ± 0.6
53.5 ± 0.5
Ours-distill
64-64-64-64
63.8 ± 0.8
79.5 ± 0.5
40.3 ± 0.6
54.1 ± 0.5
Ours-trainval
64-64-64-64
63.5 ± 0.8
79.8 ± 0.5
43.2 ± 0.6
58.5 ± 0.5
Ours-distill-trainval
64-64-64-64
64.9 ± 0.8
80.3 ± 0.5
44.6 ± 0.6
59.2 ± 0.5
71.5 ± 0.8
86.0 ± 0.5
42.6 ± 0.7
59.1 ± 0.6
Ours-distill
73.9 ± 0.8
86.9 ± 0.5
44.6 ± 0.7
60.9 ± 0.6
Ours-trainval
73.1 ± 0.8
86.7 ± 0.5
49.5 ± 0.7
66.4 ± 0.6
Ours-distill-trainval
75.4 ± 0.8
88.2 ± 0.5
51.6 ± 0.7
68.4 ± 0.6
SEResNet-12
72.0 ± 0.8
86.0 ± 0.6
43.4 ± 0.6
59.1 ± 0.6
Ours-distill
SEResNet-12
74.2 ± 0.8
87.2 ± 0.5
44.9 ± 0.6
61.4 ± 0.6
Ours-trainval
SEResNet-12
73.3 ± 0.8
86.8 ± 0.5
49.9 ± 0.7
66.8 ± 0.6
Ours-distill-trainval
SEResNet-12
75.6 ± 0.8
88.2 ± 0.5
52.0 ± 0.7
68.8 ± 0.6
Table 6. Comparisons of different backbones on CIFAR-FS and FC100.
plots of 1-shot and 5-shot results on miniImageNet and
CIFAR-FS are shown in Figure 4. The 0-th generation (or
root generation) refers to the vanilla model trained with only
standard cross-entropy loss, and the (k-1)-th generation is
distilled into k-th generation. In general, few-shot recognition performance keeps getting better in the ﬁrst two or
three generations. After certain number of generations, the
accuracy starts decreasing for logistic regression and nearest neighbour. Normalizing the features can signiﬁcantly
alleviate this problem.
In Table 1, Table 2, and Table 4, we evalute the model
of the second generation on miniImageNet, CIFAR-FS and
FC100 datasets; we use the ﬁrst generation on tieredImageNet. Model selection is done on the validation set.
4.7. Choice of base classiﬁer
One might argue in the 1-shot case, that a linear classiﬁer
should behavior similarly to a nearest-neighbour classiﬁer.
However in Table 4 and Figure 4, we ﬁnd that logistic regression is clearly better than nearest-neighbour. We argue
that this is casued by the scale of the features. After we
normalize the features by the L-2 norm, logistic regression
(“LR+Norm”) performs similarly to the nearest neighbour
classiﬁer (“NN+Norm.”), as shown in the ﬁrst row of Figure 4. However, when increasing the size of the support set
to 5, logistic regression is signiﬁcantly better than nearestneighbour even after feature normalization
4.8. Comparsions of different network backbones.
Better backbone networks generally produce better results; this is also obvious in few-shot learning and/or metalearning (as shown in Table 1).
To further verify our
assumption that the key success of few-shot learning algorithms is due to the quality of embeddings, we compare three alternatives in Table 5 and Table 6: a ConvNet with four four convolutional layers (64, 64, 64, 64);
a ResNet12 as in Table 1; a ResNet12 with sequeeze-andexcitation modules. For each model, we have four settings: training on meta-training set; training and distilling
on meta-training set; training on meta-training set and metavalidation set; training and distilling on meta-training set
and meta-validation set. The results consistently improve
with more data and better networks. This is inline with our
hypothesis: embeddings are the most critical factor to the
performance of few-shot learning/meta learning algorithms;
better embeddings will lead to better few-shot testing performance (even with a simple linear classier). In addition,
our ConvNet model also outperforms other few-shot learning and/or meta learning models using the same network.
This veriﬁes that in both small model regime (ConvNet) and
large model regime (ResNet), few-shot learning and meta
learning algorithms are no better than learning a good embedding model.
4.9. Multi-task vs multi-way classiﬁcation?
We are interested in understanding whether the efﬁcacy of our simple baseline is due to multi-task or multiway classiﬁcation. We compare to training an embedding
model through multi-task learning: a model with shared embedding network and different classiﬁcation heads is constructed, where each head is only classifying the corresponding category; then we use the embedding model to
extract features as we do with our baseline model. This
achieves 58.53 ± 0.8 on mini-ImageNet 5-way 1-shot case,
compared to our baseline model which is 62.02 ± 0.63. So
we argue that the speciality of our setting, where the fewshot classiﬁcation tasks are mutually exclusive and can be
merged together into a single multi-way classiﬁcation task,
makes the simple model effective.
5. Results on Meta-Dataset
Meta-Dataset is a new benchmark for evaluating
few-shot methods in large-scale settings.
Compared to
miniImageNet and tieredImageNet, Meta-Dataset provides
more diverse and realistic samples.
The ILSVRC (ImageNet) subset consists of
712 classes for training, 158 classes for validation, and
130 classes for testing.
We follow the setting in Meta-
Dateset where the embedding model is trained solely
on the ILSVRC training split. We use ResNet-18 as
the backbone network. The input size is 128×128. In the
pre-training stage, we use SGD optimizer with a momentum of 0.9. The learning rate is initially 0.1 and decayed
by a factor of 10 for every 30 epochs. We train the model
for 90 epochs in total. The batch size is 256. We use standard data augmentation, including randomly resized crop
and horizontal ﬂip. In the distillation stage, we set α = 0.5
and β = 1.0. We perform distillation twice and use the
model from the second generation for meta-testing. We do
not use test-time augmentation in meta-testing. In addition
to logistic regression (LR), we also provide results of linear
SVM for completeness.
We select the best results from for comparison –
for each testing subset, we pick the best accuracy over
7 methods and 3 different architectures including 4-layer
ConvNet, Wide ResNet, and ResNet-18. As shown in Table 7, our simple baselines clearly outperform the best results from on 9 out of 10 testing datasets, often by a
large margin. Our baseline method using LR outperforms
previous best results by more than 7% on average. Also,
self-distillation improves max(LR, SVM) in 7 out of the
10 testing subsets. Moreover, we notice empirically that logistic regression (LR) performs better than linear SVM.
6. Discussion
We have proposed a simple baseline for few-shot image
classiﬁcation in the meta-learning context. This approach
has been underappreciated in the literature thus far.
show with numerous experiments that uch a simple baseline
outperforms the current state-of-the-arts on four widelyused few-shot benchmarks. Combined with self-distillation,
the performance further improves by 2-3%.
meta-training labels are unavailable, it may be possible to
leverage state of the art self-supervised learning approaches
to learn very good embeddings for meta-testing tasks.
1. What is the intuition of this paper?
A: We hope this paper will shed new light on few-shot classiﬁcation.
We believe representations play an important
role. Shown by our empirical experiments, a linear model
can generalize well as long as a good representation of the
data is given.
2. Why does this simple baseline work? Is there anything
that makes few-shot classiﬁcation special?
A: Few-shot classiﬁcation is a special case of meta-learning
in terms of compositionality of tasks. Each task is an Kway classiﬁcation problem, and on current benchmarks the
classes, even between tasks, are all mutually exclusive. This
means we can merge all N of the K-way classiﬁcation tasks
into a single but harder NK-way classiﬁcation task. Our
ﬁnding is that training an embedding model on this new
Trained on ILSVRC train split
Best from 
LR-distill
SVM-distill
Quick Draw
VGG Flower
Trafﬁc Signs
Mean Accuracy
Table 7. Results on Meta-Dataset. Average accuracy (%) is reported with variable number of ways and shots, following the setup in .
We compare four variants of out method (LR, SVM, LR-distill, and SVM-distill) to the best accuracy over 7 methods in . In each
episode, 1000 tasks are sampled for evaluation.
NK-way task turns out to transfer well to meta-testing set.
On the other hand, we also ﬁnd that self-supervised embedding, which does not explicitly require this NK compositionality, achieves a similar level of performance. A concurrent work studies the representations for few-shot
learning from the theoretical point of view.
3. Does your work negate recent progress in meta-learning?
Meta-learning is much broader than just fewshot classiﬁcation. Although we show a simple baseline
outperforms other complicated meta-learning algorithms
in few-shot classiﬁcation, methods like MAML may still
be favorable in other meta-learning domains (e.g., metareinforcement learning).
4. Why does distillation work? What does it suggest?
A: The soft-labels from the teacher model depict the
fact that some classes are closer to each other than other
classes. For example, a white goat is much more similar
to a brown horse than to an airplane. But the one-hot label
does not capture this. After being regularized by soft-labels,
the network learns to capture the metric distance. From theoretical perspective, provides analysis for linear case.
Ongoing work argues distillation ampliﬁes regularization in Hilbert space.