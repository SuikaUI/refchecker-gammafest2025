For Peer Review
Crystal Structure Representations for Machine Learning
Models of Formation Energies
Journal: International Journal of Quantum Chemistry
Manuscript ID: QUA-2015-0051.R1
Wiley - Manuscript type: Full Paper
Date Submitted by the Author: 26-Mar-2015
Complete List of Authors: Faber, Felix; University of Basel, Department of Chemistry
Lindmaa, Alexander; Linköping University, Department of Physics,
Chemistry and Biology
von Lilienfeld, Anatole; University of Basel, Department of Chemistry;
Argonne National Laboratory, Argonne Leadership Computing Facility
Armiento, Rickard; Linköping University, Department of Physics, Chemistry
and Biology
Keywords: Machine Learning, Formation Energies, Representations, Crystal Structure,
Periodic systems
Note: The following files were submitted by the author for peer review, but cannot be converted to
PDF. You must view these files (e.g. movies) online.
faber_crystal_graphicaltoc.tex
John Wiley & Sons, Inc.
International Journal of Quantum Chemistry
For Peer Review
Crystal Structure Representations for Machine Learning
Models of Formation Energies
Felix Faber,1 Alexander Lindmaa,2 O. Anatole von Lilienfeld,1, 3, ∗and Rickard Armiento2, †
1Institute of Physical Chemistry and National Center for Computational Design and Discovery of Novel Materials,
Department of Chemistry, University of Basel, Switzerland.
2Department of Physics, Chemistry and Biology,
Link¨oping University, SE-581 83 Link¨oping, Sweden.
3Argonne Leadership Computing Facility, Argonne National Laboratory, 9700 S. Cass Avenue, Lemont, IL 60439, USA.
We introduce and evaluate a set of feature vector representations of crystal structures for machine
learning (ML) models of formation energies of solids. ML models of atomization energies of organic
molecules have been successful using a Coulomb matrix representation of the molecule. We consider
three ways to generalize such representations to periodic systems: (i) a matrix where each element is
related to the Ewald sum of the electrostatic interaction between two diﬀerent atoms in the unit cell
repeated over the lattice; (ii) an extended Coulomb-like matrix that takes into account a number
of neighboring unit cells; and (iii) an ansatz that mimics the periodicity and the basic features of
the elements in the Ewald sum matrix by using a sine function of the crystal coordinates of the
atoms. The representations are compared for a Laplacian kernel with Manhattan norm, trained to
reproduce formation energies using a data set of 3938 crystal structures obtained from the Materials
Project. For training sets consisting of 3000 crystals, the generalization error in predicting formation
energies of new structures corresponds to (i) 0.49, (ii) 0.64, and (iii) 0.37 eV/atom for the respective
representations.
INTRODUCTION
First-principles simulations for the prediction of properties of chemical and materials systems have become a
standard tool throughout theoretical chemistry, physics
and biology.
These simulations are based on repeatedly solving numerical approximations to the underlying many-body problem, i.e., the Schr¨odinger equation.
However, in the pursuit of ever increasing eﬃciency, there
is a growing interest in side-stepping the physics-based
formulation of the problem, and instead exploit big data
methodology, e.g., artiﬁcial intelligence, evolutionary and
machine learning (ML) schemes. If successful, such approaches can lead to an orders-of-magnitude increase
in computational eﬃciency for describing properties of
atomistic systems.
Such a change would not merely
bring incremental progress, but certainly represents a
paradigm-shift in terms of enabling the study of systems
and problems which hitherto have been completely out
Recently, one of us has presented an ML scheme
that predicts atomization energies of new (out-of-sample)
molecules with an accuracy even beyond that of the most
common ﬁrst-principles method, density functional theory . Subsequently, it was shown that this approach
also works for other properties such as molecular polarizability and frontier orbital eigenvalues , transmission
coeﬃcients of electron transport in doped nano-ribbon
models , and even densities of state within the Andersion impurity model . A critical component of any
∗ 
† 
such ML scheme is the feature vector representation of
the data set, also often called the descriptor ; i.e.,
the mapping of the atomistic description of the system
into a form suitable for matrix operations. In the cited
work , an atom by atom matrix, dubbed Coulomb matrix, was used with diagonal and oﬀ-diagonal elements
representing the potential of the free atom and the interatomic Coulomb repulsion between nuclear charges.
The aim of the present study is to consider various
ways of adapting this representation to periodic systems,
and to compare their performance in ML models of formation energies of solids. We have considered three generalizations of the Coulomb matrix idea; (i) a matrix
where each element is related to the Ewald sum of the
electrostatic interaction between two diﬀerent atoms in
the unit cell repeated over the lattice; (ii) an extended
Coulomb-like matrix that takes into account a number of
neighboring unit cells, and (iii) a simpliﬁed matrix ansatz
chosen to mimic the periodicity and basic features of the
elements in the Ewald sum matrix using a sine function
of atomic coordinates.
A few studies have already considered representations
of periodic systems suitable for ML. Schutt et al. trained
an ML model using radial distribution functions to predict the density of states at the Fermi level of solids .
Meredig et al. used a heuristics based ML model to
screen 1.6 M ternary solids for stability .
Ramprasad and co-workers, relying on chemo-structural ﬁngerprints in ML models of formation energies (and other
properties) of polymers, obtained scatter plots for
which visual inspection suggests an error on the order
of magnitude of ∼0.5 eV. Bartok et al. used ML to enhance the computer simulation of molecular materials
However, to the best of our knowledge no clear
example has been presented so far for directly applying
 
John Wiley & Sons, Inc.
International Journal of Quantum Chemistry
For Peer Review
ML to reproduce cell-, cohesive-, or formation energies
based on an atom by atom matrix based representation
of the crystal structure.
We present such an application based on the ML methods which have shown good
performance for molecules.
We ﬁnd that all representations investigated in this work yield similar accuracy
(0.4−0.6 eV/atom at 3000 crystals), which is less impressive than the accuracy found for atomization energies of
molecules (0.02 eV/atom for training sets of similar size).
However, our results indicate that if the data set used for
training can be further expanded, a machine capable of
estimating formation energies at near ﬁrst-principles accuracy is still within reach. We brieﬂy discuss the reason
for the disparity in performance between ﬁnite and periodic systems, and how this may be resolved.
This paper is organized as follows. In Sec. II we brieﬂy
present the kernel ridge regression (KRR) scheme.
Sec. III we introduce our various representations of crystal structures. In Sec. IV some technical details are given
regarding the implementation. In Sec. V we analyze the
representations and test their performance in machines
trained on a data set of formation energies of solids. In
Sec. VI we discuss our results, and ﬁnally, in Sec. VII
this study is summarized and concluded.
KERNEL RIDGE REGRESSION
We present a short summary of the ridge regression
method and KRR , deﬁning our notation. In
ridge regression one seeks to approximate an unknown
function y(x) where x is a feature-vector representation of
some input (e.g., a molecule or crystal system). We start
from a set of n data points y = (y1, y2, ..., yn)T known
for a set of feature vectors x = (x1, x2, ..., xn)T, where we
use the linear-algebra convention of distinguishing row
and column vectors, and aT is the transpose of a.
An approximation f(x, α) is constructed as
f(x, α) = k(x)α =
α = (α1, α2, ..., αn)
k(x) = (k1(x), k2(x), ..., kn(x))T,
where kj(x) is a function ansatz of x which is the same
for all j, and to be chosen freely, usually assumed to
be polynomial. The approximation f(x, α) is optimized
by seeking the α that minimizes the Euclidean norm
∥y −k(x)α∥2. The solution is given by the normal equation
α = (k(x)Tk(x))−1k(x)Ty.
If an ansatz for k(x) with a high degree of freedom is
used, there is considerable risk of overﬁtting, i.e., one arrives at a model that ﬁts the training data set well, but
still performs poorly when predicting y for out-of-sample
cases (feature vectors x that are not included in the training set x.) Several techniques are used to address this
problem, e.g., cross-validation and regularization. In the
case of the latter, Eq. (4) is modiﬁed by inserting an extra
α = (k(x)Tk(x) + λI)−1k(x)Ty,
where λ is the regularization parameter and I the identity
Ridge regression is extended into KRR by introducing
a map, ΦK, from a non-linear space to a linear space
that is known as the feature space . The mapping ΦK is usually non-trivial, but does not need to be
known explicitly since it is suﬃcient to know the inner
product between the mapped data, ⟨ΦK(xi), ΦK(xj)⟩=
l ΦK(xi)lΦK(xj)l = k(xi, xj) where k(xi, xj) is the
kernel. This procedure is also known as the kernel trick
 . The function f(x, α) now takes the form
αik(x, xi)
and the approximation is optimized by seeking the minimum of
(yi −f(xi, α))2 +
αik(xj, xi)αj,
with solution
α = (K + λI)−1y,
where K = k(xi, xj) is the kernel matrix.
choices of kernel functions are appropriate for diﬀerent
ML applications. Two of the more commonly employed
kernel functions are,
Gaussian: k(xi, xj) = e−∥xi−xj∥2
Laplacian: k(xi, xj) = e−∥xi−xj∥1/σ
deﬁned as using the Euclidean (L2) and Manhattan (L1)
norm, respectively. Here σ represents the kernel width, a
measure of locality in the employed training set. Without
any loss of generality we restrict ourselves to the Laplacian kernel for this study. This choice is motivated by the
fact that it has shown good performance in the context of
predicting molecular atomization energies . However,
the Gaussian, or other kernels, could have been used just
As a relevant benchmark of performance of the ML
model with a given representation we use the error in
predicting new data outside x, the generalization error
(GE). There are several ways to estimate the GE. We
exclude some subset of all available data x (and y) from
the solution of Eq. (8) and then evaluate the mean absolute error (MAE) for the prediction of the excluded data.
The subset of x used in Eq. (8) is called the training set
xtrain, and the remaining input the test set xtest. Hence,
MAEtest = 1
|yi −f(xi, α)|,
 
John Wiley & Sons, Inc.
International Journal of Quantum Chemistry
For Peer Review
where m is the size of the test set. The measure MAEtest
determines how well the machine predicts new data. The
number of systems included in xtest needs to be suﬃciently large for this measure to be an accurate estimate
of the GE. Hence, there is a trade-oﬀbetween using available data as part of xtest or xtrain. We alleviate this problem by a statistical validation method known as k-fold
cross-validation following the recipes in Ref. 2, combined
with random sampling. The MAEtest is calculated as the
mean of the MAEtest of several independent ML runs.
In each run xtest is constructed by selecting a speciﬁc
number of entries randomly out of the full data set.
Similarly, MAEtrain is deﬁned by substituting xtrain for
xtest in Eq. (11). This measure determines the precision
with which the machine reproduces the data it has been
trained on. A high MAEtrain suggests the machine has
too few degrees of freedom to describe the data well. A
low MAEtrain suggests high ﬂexibility of the ﬁtting function, a potential risk of overﬁtting, and that it may be
possible to reach good transferability with a suﬃciently
large data set.
REPRESENTATIONS
The feature vector representation used for the input is
of central importance for the ML scheme. Here, this is
the mapping of the positions and identities of all atoms
into a vector x. We expect a well-suited representation to
exhibit the following beneﬁcial properties (with examples
in parenthesis given for molecules)
1. Complete, non-degenerate: x should incorporate all
features in the input that are relevant for the underlying problem. (Two diﬀerent molecules should
give diﬀerent x.)
2. Compact, unique: x should have minimal features
that are redundant to the underlying problem.
(Two instances of the same molecule, but rotated
diﬀerently, should give the same x.)
3. Descriptive: instances of input that are ‘close’, giving similar y, should generally be represented by x
that are close, in the sense of a small ∥x1 −x2∥.
(Two molecules that are identical except for small
diﬀerences in the atomic positions, which have similar y, should generally have similar x.)
4. Simple: generating the representation for a given
input should require as little computational eﬀort
as possible.
(The set of eigenvalues from solution of the many-body Schr¨odinger equation of the
molecule would generally not be a useful x.)
A bijective representation is perfectly non-degenerate
and unique, i.e., it has a one-to-one correspondence between the input and x. In this discussion, the distinction made by ‘relevant for the underlying problem’ is important, because diﬀerent features of the input may be
relevant for diﬀerent applications. For example, in ML
models of atomization energies the chirality of a molecule
is not relevant, but in ML models of the optical activity
in circular dichroism it is.
A common method for analysis of the properties of
diﬀerent feature vector representations is principal component analysis (PCA) . This method reduces the dimensionality of a data set while still conserving as much
information as possible. To generate the PCA, a singular
value decomposition is performed on x,
Submatrices are created from the ﬁrst k columns extracted from the resulting matrices, Uk and Σk.
PCA data set of reduced rank k is then deﬁned as
This data set is not only useful for visualization. If one
suspects that a feature vector representation is not suf-
ﬁciently compact (in the sense of point 2 in the list of
beneﬁcial properties given above in this section), one can
try to replace x with the PCA feature vector representation, z, to extract a representative lower-dimensional
part. Note that the PCA representation should only be
constructed on the training set. The test set may not be
Molecular Coulomb matrix
The present work aims at extending the ML model for
molecular properties to properties of crystals.
We therefore brieﬂy summarize the molecular approach
in the following. The feature vector representation called
the Coulomb matrix is a symmetric atom by atom matrix
given in Hartree atomic units,
ZiZjϕ(∥ri −rj∥2)
ϕ(r) = 1/r
where ϕ(r) is the Coulomb potential, Zi and ri are
the atomic number and position of the ith atom. The
non-diagonal elements correspond thus to the pair-wise
Coulomb repulsion between the positive atomic cores in
the system, and the diagonal elements are chosen by construction as the result of an exponential ﬁt to the potential energy of a free atom.
While this representation is not bijective, it is nondegenerate in the sense that no two molecules that diﬀer
more from each other than being enatiomers will yield the
same Coulomb matrix. One molecule, however, can result in several Coulomb matrices due to the various ways
of ordering atoms. Atom index invariance can be introduced by ordering atom indices according to the norm of
each row (or column) , or by using permuted sets of
Coulomb matrices . Sorting, however, introduces the
 
John Wiley & Sons, Inc.
International Journal of Quantum Chemistry
For Peer Review
issue of diﬀerentiability—a potentially desirable property
when it comes to the modeling of atomic forces. Use of
the (diﬀerentiable) eigenvalue spectrum of the Coulomb
matrix yields an atom index invariant descriptor at the
expense of losing uniqueness . There is another
atom index invariant and diﬀerentiable representation,
the Fourier series of atomic radial distribution functions,
that also preserves uniqueness as well as spatial invariances . However, this representation has not yet been
explored in depth and has therefore not been included in
the representations discussed herewithin.
To benchmark our results for solids, we compare to the
performance for the molecular ML model that has been
trained and tested on (subsets of) the GDB-database in
previous studies . We use a subset of 7165 entries out of the GDB-13 data set. These entries are all
the organic molecules in this set with up to 7 atoms of
elements C, O, N, and S, and valencies satisﬁed by hydrogen atoms. The atomic coordinates were relaxed using
atomic force ﬁelds, and the atomization energies calculated with a higher-order ﬁrst-principles method, density
functional theory using the PBE0 hybrid functional . We call this dataset QM7 .
When considering ways of extending the Coulomb matrix to periodic systems one might be tempted to take
the Coulomb matrix for just the atoms in the primitive
unit cell of the periodic crystal, alongside with the unit
cell vectors. However, depending on the choice of primitive unit cell the set of interatomic distances will vary,
since distances to neighboring atoms from diﬀerent unit
cells are not accounted for. Hence, such a representation
is not unique in the sense that the same crystal can lead
to diﬀerent representations, and therefore is less likely to
yield well performing ML models .
Ewald Sum Matrix
We now consider a straightforward extension of the
Coulomb matrix representation that removes the most
obvious dependence on the non-unique set of interatomic
distances in the primitive unit cell. We form an atom by
atom matrix with one element for each pair of atoms in
the primitive unit cell, but now each element is deﬁned
to represent the full Coulomb interaction energy corresponding to all inﬁnite repetitions of these two atoms
in the lattice. In this way, the elements in the matrix
retain essentially the same meaning as in the Coulomb
matrix, while the complete inﬁnite repetition of the lattice is taken into account. As such, one can propose the
following expression for the matrix elements,
φ(∥rk −rl∥2)
where the sum over k is taken over the atom i in the unit
cell and its N closest equivalent atoms, and similarly for
l and j. The intention is to take N →∞to represent
the full electrostatic interaction between the inﬁnitely repeated atoms equivalent to atoms i and j in the primitive cell. However, this type of inﬁnite electrostatic sum
has well known issues with convergence that have been
discussed at length in the ﬁeld of materials science. One
resolution is given by the Ewald sum . The central
idea is to divide the problematic double sum in Eq. (16)
into two rapidly converging sums and one constant,
xij = x(r)
where x(r)
is the short range interaction calculated in
real space, x(m)
the long range interaction calculated in
the reciprocal space, and x0
ij is a constant. The division
is controlled by a screening length parameter a, which
inﬂuences how rapidly the sums converge. The ﬁrst term
is given by
erfc(a∥ri −rj + L∥2)
∥ri −rj + L∥2
where the sum is taken over all lattice vectors L inside a
sphere of a radius set by a cutoﬀLmax. The second term
cos(G · (ri −rj))
taken over all non-zero reciprocal lattice vectors G in a
sphere of radius set by a cutoﬀGmax, taken to be large
enough for the sum to converge; and V is the unit cell
volume. The last term is
√π −(Zi + Zj)2
(i ̸= j), (20)
where the ﬁrst term in Eq. (20) is the Ewald self-terms
for the i and j sites and the second term is a correction
needed as we use a charged cell, since, in analog to the
Coulomb matrix representation, the expressions describe
the interaction from the positive atomic cores. The correction makes the total energy be that of a system with
a uniform compensating background that makes the system neutral. For the diagonal terms in the matrix (i=j)
we take the Ewald sum interation energy of the lattice
of i-type atoms, which is given by the same equations
Eq. (18) and (19) but with an extra factor 1/2 and,
The value of a only aﬀects the rate of convergence in the
above sums, not the ﬁnal value of xij. There are several
suggested schemes for how to set it, in our work we take
where M is the number of atoms in the unit cell.
 
John Wiley & Sons, Inc.
International Journal of Quantum Chemistry
For Peer Review
We refer to Eqs. (18)−(20) as the Ewald sum matrix representation. Our deﬁnitions are chosen to make
each element of the matrix be the full Ewald sum of the
Coulomb interaction between the sites i and j (or i with
We note brieﬂy that the python materials genomics (pytmatgen) open source library contains a
similar matrix as an intermediate step in the calculation
of the full Ewald sum energy. However, that matrix diﬀer
from ours in that it is deﬁned to make the sum over all
elements give the total energy in a way that makes individual matrix elements depend on the speciﬁc value of
the a parameter used. This seems an undesirable feature
for using the matrix as a descriptor. (Nevertheless, for
the value a in our calculations, we use the same formula
as in pymatgen, Eq. (22).)
Extended Coulomb-like Matrix
Another way to generalize the Coulomb matrix to periodic systems is to extend the size of the representation
matrix. Let each element be the electrostatic interaction
between one of the M atoms in the unit cell and one
of the atoms in the N closest unit cells, giving an M by
N ·M matrix representation on the regular Coulomb matrix form of Eq. (14), with 1 ≤i ≤M, 1 ≤j ≤N · M.
To completely avoid the dependence on the chosen primitive unit cell, one would like to take N →∞, i.e.,
an inﬁnitely large representation matrix.
This can be
avoided if the long-range electrostatic interaction is replaced with a more rapidly decaying interaction. Here,
we chose ˜φ(r) = e−r. In this way the elements of the matrix quickly drop to zero, and the representation matrix
can be cut oﬀat a ﬁnite dimension that is taken to be
suﬃciently large for all systems in the data set. We refer
to this as the extended Coulomb matrix representation.
One beneﬁt of this representation over the Ewald sum
matrix is that it is more straightforward to evaluate (one
can simply iterate over N copies of the unit cell).
Sine Matrix
Yet another representation can be constructed by further extending the idea of reducing the computational
eﬀort by replacing the long-range electrostatic interaction by a simpler expression. We start from the M by M
Ewald sum matrix in Eq. (16), but substitute the whole
sums of the electrostatic interaction with an arbitrarily
chosen two-point potential that is intended to share the
same basic properties as such a sum, giving
ZiZj ˜Φ(ri, rj)
where rl is the position of the lth atom in the unit cell.
Consider two non-equivalent atoms in the unit cell, A
and B. The Coulomb sum contribution due to the in-
FIG. 1. An illustration of ˜Φ(r1, r2) used in the sine matrix
representation, Eq. (24), for a two-dimensional crystal lattice
in a primitive unit cell shown by arrows. The ﬁgure shows the
magnitude of our constructed ‘interaction’ between one atom
at r1 = (x, y) and another ﬁxed at the origin r2 = 0 (the
latter shown along with its inﬁnite repetitions as solid purple
dots.) The interaction is periodic across the unit cells and
grows to inﬁnity as r1 approach any repetition of the atom at
the origin.
ﬁnitely repeated grid of A and B atoms can be thought
of as a potential ﬁeld which is a function of the position of
the atom A. Three important properties of this ﬁeld are:
(i) the expression as function of each atomic coordinate
is periodic with respect to the crystal lattice; (ii) the contribution from two equivalent atoms in neighboring cells
should be the same; (iii) the potential should approach
inﬁnity when A takes the same position as B. The conclusion is that the potential needs to be symmetric with
respect to the lattice vectors.
A possible choice for ˜Φ(r1, r2) that fulﬁlls these requirements is
˜Φ(r1, r2) = ∥B ·
ˆek sin2[πˆekB−1 · (r1 −r2)]∥−1
where ˆex, ˆey, ˆez are the coordinate unit vectors and B
is the matrix formed by the basis vectors of the lattice.
The product inside the sine function thus gives the vector
between the two sites expressed in crystal lattice coordinates, which gives the right periodicity in r1 and r2. We
call Eqs. (23)−(24) the sine matrix representation. The
beneﬁt of this representation over the others suggested in
this work is that Eq. (23) is a completely straightforward
M by M matrix that only depends on the positions of
the atoms in a single unit cell. Hence, the computational
load of this representation is minimal. Figure 1 shows ˜Φ
for a two-dimensional lattice.
Note that we do not have a proof for the completeness
 
John Wiley & Sons, Inc.
International Journal of Quantum Chemistry
For Peer Review
FIG. 2. The frequency of occurrence of various elements in
the 3938 systems in the MP data set.
The distribution is
not uniform, but rather expected to reﬂect the occurrence
of elements in published materials. The four most common
elements are O, Si, Cu, and S.
or uniqueness of these representations. They are merely
constructed as sensible extensions of the Coulomb matrix
IMPLEMENTATION DETAILS AND DATA
We have implemented the KRR ML scheme using a
Laplacian kernel both for the original set of molecules
from Ref. 1 with the Coulomb matrix representation, and
for the discussed representations using a data set of cystal
structures. We use the Python programming language,
including numpy , scipy and pytmatgen .
Our data set of formation energies of periodic solids
was obtained from the Materials Project (MP) database
 . We extracted 3938 systems from the MP without
obvious order. Since the MP database is derived from
the ICSD , the distribution of elements in the extracted systems roughly match their occurrence in published materials in the literature.
This distribution is
shown in Fig. 2. While this may be interpreted as a bias
in the data set, one can also see it as representative for
the intended application of ML in predicting properties
of materials out of available material databases based on
published materials.
The machine requires values for the regularization parameter λ and kernel width σ. We follow Hansen et al.
 ; optimal values were identiﬁed by calculating MAEtest
for a training set size of 3000 for pairs of λ and σ values
on a two-dimensional logarithmic grid, using a spacing
factor of 2 for σ and 10 for λ. (This method works well
when the model has a small number of hyper parameters, but needs to be replaced with more sophisticated
TABLE I. Optimal values of parameters λ and σ for the machines using the diﬀerent feature vector representations in this
work, and for a training set consisting of 3000 crystals drawn
at random from the Materials Project.
Representation
Ewald sum matrix
Generalized Coulomb matrix
Sine matrix
Coulomb matrix for QM7 molecules
methods for ML models with a larger set of parameters
since the time it takes to ﬁnd the optimum is of O(xp)
where p is the number of hyper parameters.) The optimal values found are shown in Table I. The MAEtest for
the diﬀerent representations is not very sensitive to these
parameters, i.e., the regions around the minimums in the
generated grids were relatively ﬂat.
The performance of the representations of periodic systems studied in this work are meant to be considered
in the context of the excellent performance of the machine for molecules presented in Refs. 1 and 2. To make
this comparison clear we have reproduced the GE of
their scheme with our implementation and the QM7 data
set. The results are shown in Fig. 3. In Fig. 4 a twodimensional PCA of the QM7 set using the Coulomb
matrix representation is shown. Already with a training set size of a couple of hundred atoms, the MAE approaches the accuracy of DFT with the least computationally expensive, semi-local, functionals.
At a training set size of 500, we arrive at a MAEtest of around
0.07 eV/atom, whereas DFT with the PBE functional has
an MAE for the atomization energy of small molecules
of ca 0.15 eV/atom .
As demonstrated by Rupp
et al. , the precision of the predictions of the machine keeps improving with increased size of the training
set, and at 3000 structures one ﬁnds an GE error below
0.02 eV/atom.
We now turn to the results of applying ML to periodic crystals using the feature vector representations in
this work, shown in Fig. 3. The performance of all our
representations are similar, but their accuracy is inferior
to what we see for molecules.
All the representations
studied improve greatly with increased training set size.
The MAEtest for the representations at a training set
size of 3000 are 0.49 eV/atom for the Ewald sum matrix, 0.64 eV/atom for the extended Coulomb matrix,
and 0.37 eV/atom for the sine matrix representation,
i.e., an order of magnitude worse than we ﬁnd for the
molecules. Furthermore, we ﬁnd that MAEtrain for all
the representations are insigniﬁcant (< 5 · 10−3) even at
a training set size of 3000.
Figure 4 compares the two-dimensional PCA of the MP
 
John Wiley & Sons, Inc.
International Journal of Quantum Chemistry
For Peer Review
Training set size
MAEtest (meV/atom)
The mean absolute GE, Eq. (11), versus training
set size for the diﬀerent representations considered in this
work. Shown are MAEtest for predicting formation energies
of crystals in the MP data set: (Sine) Eqs. (23)−(24); (Ewald)
Eqs. (18)−(20); and (GCM) described in Sec. III C. For comparison we also include (QM7), MAEtest for atomization energies of molecules in the QM7 data set using a regular Coulomb
matrix, Eqs. (14)−(15).
data set to the QM7 one for the diﬀerent representations.
The QM7 PCA is localized to a set of small clusters. The
MP data for the sine and the extended Coulomb matrix
representations appear more uniformly spread out. The
Ewald sum matrix PCA has a central cluster but also a
few data points far removed from this cluster.
DISCUSSION
The results in Fig. 3 suggest that the sine matrix representation is slightly better than the other representations in this work, in that it reaches a lower GE and
has better development of the GE with training set size.
However, the performance of the diﬀerent methods are
roughly similar, indicating that the selection between the
feature vector representations presented in this work for
a given application may not have major impact on the
GE. This further strengthens the case of the sine matrix
representation, since it is the representation that requires
the least computational expense. It is interesting to note
that while the PCAs of the sine and extended Coulomb
matrix bear strong resemblance, the Ewald sum PCA is
distinctly diﬀerent with a strong clustering and a few outlier points. Furthermore, the GE of all representations
systematically decreases with increasing training set size.
These results suggest the GE could be reduced even further if only more extensive data set for training were
available.
When comparing the molecular results with the ones
PCA1 (arb. u.)
PCA2 (arb. u.)
FIG. 4. A two-dimensional PCA of the data sets used in this
work: (QM7, green) the Coulomb matrix representation of the
QM7 data set of molecules; (Sine, red) the sine matrix representation using the MP data set; (Ewald, blue) the Ewald sum
matrix representation using the MP data set; (ECM, yellow)
The extended Coulomb matrix data set using the MP data
set. The PCA of the representations of the MP data set diﬀer
substantially from the PCA of the Coulomb matrix of QM7.
The shading of the points represent the target value energy,
showing that there is no clear energy-PCA pattern.
for solids, it is important to keep in mind that the diversity and composition of the MP data set diﬀers substantially from that of QM7.
In particular, only very
few element types are present in the molecular data set
consisting of atoms with very ﬁnite size, no molecule has
more than seven atoms (not counting hydrogens). This is
illustrated by the PCA in Fig. 4, where the QM7 data is
collected in a few tight clusters. Arguably, the chemical
space of QM7 is signiﬁcantly smaller than the one we use
to evaluate the representations for solids since the MP
data set contains ten times more elements than QM7.
Hence, the central conclusion of the present work is
that there are three areas where the situation of ML
for periodic systems can be improved: (i) our methods
should be used with even larger data sets to conﬁrm that
the GE can be brought down to levels where it is useful for applications, (ii) further improved representations
may be helpful if they more eﬃciently can represent the
degrees of freedom oﬀered by periodic crystals. Such representations may reduce the need for larger training sets;
and (iii) if a way of generating a data set over a restricted
chemical space can be devised which is as compact as
QM7, we may reach more promising ML performance for
periodic systems. We are presently working in all of these
directions.
 
John Wiley & Sons, Inc.
International Journal of Quantum Chemistry
For Peer Review
SUMMARY AND CONCLUSIONS
We have investigated the performance of several crystal structure representations for ML models of formation
energies of solids. Our work is a natural generalization
of an ML scheme previously shown to be successful for
the atomization energy of molecules. We have compared
three diﬀerent representations, and found that a sine matrix which simulates the features of an inﬁnite Coulomb
sum is both most eﬃcient, and gives the smallest GE error. While the performance of all the methods may at
ﬁrst seem disappointing when compared to the small GE
conﬁrmed for molecules, we can explain this discrepancy
to be due to data sets which are too small to cover the
hugely diverse compositional and structural space with
suﬃcient density. As such, the full potential of ML for
periodic systems still has to be demonstrated. The improvement of the MAEtest with training set size suggests,
however, that the methods presented here can lead to accurate machine models if only trained on larger or more
restricted data sub sets. Hence, our results oﬀer promising indications that suﬃciently accurate and transferable
ML models of energies of periodic systems can be realized.
ACKNOWLEDGMENTS
R.A. acknowledges funds provided by the Swedish Research Council Grant No. 621-2011-4249 and the Linnaeus Environment at Link¨oping on Nanoscale Functional Materials (LiLi-NFM) funded by VR. Calculations have been performed at the Swedish National Infrastructure for Computing (SNIC). O.A.v.L. acknowledges funding from the Swiss National Science foundation
(No. PP00P2 138932). This research used resources of
the Argonne Leadership Computing Facility at Argonne
National Laboratory, which is supported by the Oﬃce
of Science of the U.S. DOE under contract DE-AC02-
06CH11357. This material is based upon work supported
by the Air Force Oﬃce of Scientiﬁc Research, Air Force
Material Command, USAF under Award No. FA9550-
15-1-0026.
 M. Rupp, A. Tkatchenko, K.-R. M¨uller, O. A. von Lilienfeld, Phys. Rev. Lett. 2012, 108, 058301.
 K. Hansen, G. Montavon, F. Biegler, S. Fazli, M. Rupp,
M. Scheﬄer, O. A. von Lilienfeld, A. Tkatchenko, K.-R.
M¨uller, J. Chem. Theory Comput. 2013, 9, 3404–3419.
 O. A. von Lilienfeld, Int. J. Quantum Chem. 2013, 113,
1676–1689.
 K. T. Sch¨utt, H. Glawe, F. Brockherde, A. Sanna, K. R.
M¨uller, E. K. U. Gross, Phys. Rev. B 2014, 89, 205118.
Mayagoitia, K. Hansen, A. Tkatchenko, K.-R. M¨uller,
O. A. v. Lilienfeld, New J. Phys. 2013, 15, 095003.
 A. Lopez-Bezanilla, O. A. von Lilienfeld, Phys. Rev. B
2014, 89, 235411.
 L.-F. Arsenault, A. Lopez-Bezanilla, O. A. von Lilienfeld,
A. J. Millis, Phys. Rev. B 2014, 90, 155136.
 O. A. von Lilienfeld, M. Rupp, A. Knoll, arXiv:1307.2918
[physics] 2013; arXiv.org/abs/1307.2918.
 A. P. Bart´ok, R. Kondor, G. Cs´anyi, Phys. Rev. B 2013,
87, 184115.
J. W. Doak, A. Thompson, K. Zhang, A. Choudhary,
C. Wolverton, Phys. Rev. B 2014, 89, 094104.
 G. Pilania, C. Wang, X. Jiang, S. Rajasekaran, R. Ramprasad, Sci. Rep. 2013, 3, 2810.
 A. P. Bart´ok, M. J. Gillan, F. R. Manby, G. Cs´anyi, Phys.
Rev. B 2013, 88, 054104.
 A. E. Hoerl, R. W. Kennard, Technometrics 1970, 12,
 V. Vovk, Empirical Inference; B. Sch¨olkopf, Z. Luo,
V. Vovk, Eds.; Springer: Berlin, Heidelberg, 2013; pp.
 T. Hastie, R. Tibshirani, J. Friedman, The Elements of
Statistical Learning: Data Mining, Inference, and Prediction, Second Edition; Springer: New York, 2011.
 K. Muller, S. Mika, G. Ratsch, K. Tsuda, B. Scholkopf,
IEEE Trans. Neural Networks 2001, 12, 181–201.
 V. N. Vapnik, The Nature of Statistical Learning Theory;
Springer-Verlag: New York, 1995.
 B. Sch¨olkopf, A. J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and
Beyond; The MIT Press: Cambridge, Mass, 2001.
 B. Sch¨olkopf, A. Smola, K.-R. M¨uller, Neural Comput.
1998, 10, 1299–1319.
 I. T. Jolliﬀe, Principal Component Analysis; Springer:
New York, 2002.
 P. G. H. Golub, D. C. Reinsch, Numer. Math. 1970, 14,
 J. E. Moussa, Phys. Rev. Lett. 2012, 109, 059801.
 M. Rupp, A. Tkatchenko, K.-R. M¨uller, O. A. von Lilienfeld, Phys. Rev. Lett. 2012, 109, 059802.
 L. C. Blum, J.-L. Reymond, J. Am. Chem. Soc. 2009,
131, 8732–8733.
 T. Fink, H. Bruggesser, J.-L. Reymond, Angew. Chem.,
Int. Ed. 2005, 44, 1504–1508.
 T. Fink, J.-L. Reymond, J. Chem. Inf. Model. 2007, 47,
 P. Hohenberg, W. Kohn, Phys. Rev. 1964, 136, B864.
 W. Kohn, L. J. Sham, Phys. Rev. 1965, 140, A1133.
 J. P. Perdew, K. Burke, M. Ernzerhof, Phys. Rev. Lett.
1996, 77, 3865.
 M. Ernzerhof, J. P. Perdew, K. Burke, Int. J. Quantum
Chem. 1997, 64, 285–295.
 M. Ernzerhof, G. E. Scuseria, J. Chem. Phys. 1999, 110,
5029–5036.
 R. Ramakrishnan, P. O. Dral, M. Rupp, O. A. von Lilienfeld, Sci. Data 2014, 1.
 P. P. Ewald, Ann. Phys. (Berlin) 1921, 369, 253–287.
 A. Y. Toukmaji, J. A. Board Jr., Comput. Phys. Commun. 1996, 95, 73–92.
 
John Wiley & Sons, Inc.
International Journal of Quantum Chemistry
For Peer Review
 R. M. Martin, Electronic Structure: Basic Theory and
Practical Methods; Cambridge University Press: Cambridge, 2008.
 S. P. Ong,
W. D. Richards,
G. Hautier,
M. Kocher, S. Cholia, D. Gunter, V. L. Chevrier, K. A.
Persson, G. Ceder, Comput. Mater. Sci. 2013, 68, 314–
 S. van der Walt, S. Colbert, G. Varoquaux, Comput. Sci.
Eng. 2011, 13, 22–30.
 E. Jones, T. Oliphant, P. Peterson, et al.SciPy: Open
source scientiﬁc tools for Python; 
 .
 A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D.
Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner,
G. Ceder, K. A. Persson, APL Mater. 20131, 011002;
 .
 G. Bergerhoﬀ, R. Hundt, R. Sievers, I. D. Brown, J.
Chem. Inf. Comput. Sci. 1983, 23, 66–69.
 A. Belsky, M. Hellenbrandt, V. L. Karen, P. Luksch, Acta
Crystallogr., Sect. B: Struct. Sci. 2002, 58, 364–369.
 
John Wiley & Sons, Inc.
International Journal of Quantum Chemistry