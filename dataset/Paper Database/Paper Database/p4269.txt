Generating Images with Perceptual Similarity Metrics based on Deep Networks
Alexey Dosovitskiy
 
University of Freiburg, Germany
Thomas Brox
 
University of Freiburg, Germany
Image-generating machine learning models are
typically trained with loss functions based on distance in the image space.
This often leads to
over-smoothed results.
We propose a class of
loss functions, which we call deep perceptual
similarity metrics (DeePSiM), that mitigate this
problem. Instead of computing distances in the
image space, we compute distances between image features extracted by deep neural networks.
This metric better reﬂects perceptually similarity
of images and thus leads to better results. We
show three applications: autoencoder training, a
modiﬁcation of a variational autoencoder, and inversion of deep convolutional networks. In all
cases, the generated images look sharp and resemble natural images.
1. Introduction
Recently there has been a surge of interest in training neural networks to generate images.
These are being used
for a wide variety of applications: unsupervised and semisupervised learning, generative models, analysis of learned
representations, analysis by synthesis, learning of 3D representations, future prediction in videos.
Nevertheless,
there is little work on studying loss functions which are
appropriate for the image generation task. Typically used
squared Euclidean distance between images often yields
blurry results, see Fig.1b. This is especially the case when
there is inherent uncertainty in the prediction. For example,
suppose we aim to reconstruct an image from its feature
representation. The precise location of all details may not
be preserved in the features. A loss in image space leads
to averaging all likely locations of details, and hence the
reconstruction looks blurry.
However, exact locations of all ﬁne details are not important for perceptual similarity of images. But the distribution
of these details plays a key role. Our main insight is that invariance to irrelevant transformations and sensitivity to local image statistics can be achieved by measuring distances
in a suitable feature space. In fact, convolutional networks
provide a feature representation with desirable properties.
They are invariant to small smooth deformations, but sensitive to perceptually important image properties, for example sharp edges and textures.
Using a distance in feature space alone, however, does not
yet yield a good loss function; see Fig. 1d. Since feature
representations are typically contractive, many images, including non-natural ones, get mapped to the same feature
vector. Hence, we must introduce a natural image prior.
To this end, we build upon adversarial training as proposed
by Goodfellow et al. . We train a discriminator network to distinguish the output of the generator from real
images. The objective of the generator is to trick the discriminator, i.e., to generate images that the discriminator
cannot distinguish from real ones. This yields a natural image prior that selects from all potential generator outputs
the most realistic one. A combination of similarity in an
appropriate feature space with adversarial training allows
to obtain the best results; see Fig. 1e.
We show three example applications: image compression
with an autoencoder, a generative model based on a variational autoencoder, and inversion of the AlexNet convolutional network. We demonstrate that an autoencoder with
Img loss Img + Adv Img + Feat
Figure 1: Reconstructions from layer FC6 of AlexNet with
different losses.
 
Deep Perceptual Similarity Metrics
DeePSiM loss can compress images while preserving information about ﬁne structures. On the generative modeling
side, we show that a version of a variational autoencoder
trained with the new loss produces images with realistic
image statistics. Finally, reconstructions obtained with our
method from high-level activations of AlexNet are dramatically better than with existing approaches. They demonstrate that even the predicted class probabilities contain rich
texture, color, and position information.
2. Related work
There is a long history of neural network based models for
image generation. A prominent class of probabilistic models of images are restricted Boltzmann machines and their deep variants .
Autoencoders have been widely used for unsupervised learning and
generative modeling, too. Recently, stochastic neural networks have become popular, and deterministic networks are being used for image generation tasks . In all these models, loss is measured
in the image space. By combining convolutions and unpooling (upsampling) layers these models can be
applied to large images.
There is a large body of work on assessing the perceptual
similarity of images. Some prominent examples are the visible differences predictor , the spatio-temporal
model for moving picture quality assessment
Branden Lambrecht & Verscheure, 1996), and the perceptual distortion metric of Winkler . The most popular
perceptual image similarity metric is the structural similarity metric (SSIM) , which compares the
local statistics of image patches. We are not aware of any
work making use of similarity metrics for machine learning, except a recent pre-print of Ridgeway et al. .
They train autoencoders by directly maximizing the SSIM
similarity of images. This resembles in spirit what we do,
but technically is very different. While psychophysical experiments go out of scope of this paper, we believe that
deep learned feature representations have better potential
than shallow hand-designed SSIM.
Generative adversarial networks (GANs) have been proposed by Goodfellow et al. . In theory, this training
procedure can lead to a generator that perfectly models the
data distribution. Practically, training GANs is difﬁcult and
often leads to oscillatory behavior, divergence, or modeling
only part of the data distribution. Recently, several modiﬁcations have been proposed that make GAN training more
stable. Denton et al. employ a multi-scale approach,
gradually generating higher resolution images.
et al. make use of a convolutional-deconvolutional
architecture and batch normalization.
GANs can be trained conditionally by feeding the conditioning variable to both the discriminator and the generator . Usually this conditioning
variable is a one-hot encoding of the object class in the input image. Such GANs learn to generate images of objects from a given class. Recently Mathieu et al. 
used GANs for predicting future frames in videos by conditioning on previous frames.
Our approach looks similar to a conditional GAN. However, in a GAN there is
no loss directly comparing the generated image to some
ground truth. We found that the feature loss introduced in
the present paper is essential to train on complicated tasks
such as feature inversion.
Most related is concurrent work of Larsen et al. . The
general idea is the same — to measure the similarity not in
the image space, but rather in a feature space. They also
use adversarial training to improve the realism of the generated images. However, Larsen et al. only apply
this approach to a variational autoencoder trained on images of faces, and measure the similarity between features
extracted from the discriminator. Our approach is much
more general, we apply it to various natural images, and
we demonstrate three different applications.
Suppose we are given a supervised learning task and a
training set of input-target pairs {xi, yi}, xi ∈RI, yi ∈
RW ×H×C . Inputs and outputs can be arbitrary vectors.
In this work, we focus on targets that are images with an
arbitrary number of channels.
The aim is to learn the parameters θ of a differentiable generator function Gθ(·): RI →RW ×H×C that optimally approximates the input-target dependency according to a loss
function L(Gθ(x), y).
Typical choices are squared Euclidean (SE) loss L2(Gθ(x), y) = ||Gθ(x) −y||2
loss L1(Gθ(x), y) = ||Gθ(x) −y||1. As we demonstrate
in this paper, these losses are suboptimal for some image
generation tasks.
We propose a new class of losses, which we call DeePSiM.
These go beyond simple distances in image space and can
capture complex and perceptually important properties of
images. These losses are weighted sums of three terms:
feature loss Lfeat, adversarial loss Ladv, and pixel space
loss Limg:
L = λfeat Lfeat + λadv Ladv + λimg Limg.
They correspond to a network architecture, an overview of
Deep Perceptual Similarity Metrics
Figure 2: Schematic of our model. Black solid lines denote the forward pass. Dashed lines with arrows on both
ends are the losses. Thin dashed lines denote the ﬂow of
gradients.
which is shown in Fig. 2. The architecture consists of three
convolutional networks: the generator G that implements
the generator function, the discriminator Dϕ that discriminates generated images from natural images, and the comparator C that computes features from images.
Loss in feature space. Given a differentiable comparator
C : RW ×H×C →RF , we deﬁne
||C(Gθ(xi)) −C(yi)||2
C may be ﬁxed or may be trained; for example, it can be a
part of the generator or the discriminator.
Lfeat alone does not provide a good loss for training. It is
known that optimizing just
for similarity in the feature space typically leads to highfrequency artifacts. This is because for each natural image
there are many non-natural images mapped to the same feature vector 1. Therefore, a natural image prior is necessary
to constrain the generated images to the manifold of natural
Adversarial loss. Instead of manually designing a prior,
as in Mahendran & Vedaldi , we learn it with
an approach similar to Generative Adversarial Networks
(GANs) of Goodfellow et al. . Namely, we introduce a discriminator Dϕ which aims to discriminate the
generated images from real ones, and which is trained concurrently with the generator Gθ. The generator is trained to
“trick” the discriminator network into classifying the generated images as real. Formally, the parameters ϕ of the
discriminator are trained by minimizing
Ldiscr = −
log(Dϕ(yi))+log(1−Dϕ(Gθ(xi))), (3)
1This is unless the feature representation is speciﬁcally designed to map natural and non-natural images far apart, such as
the one extracted from the discriminator of a GAN.
and the generator is trained to minimize
log Dϕ(Gθ(xi)).
Loss in image space. Adversarial training is known to be
unstable and sensitive to hyperparameters. We found that
adding a loss in the image space
||Gθ(xi) −yi||2
stabilizes training.
3.1. Architectures
Generators. We used several different generators in experiments. They are task-speciﬁc, so we describe these in corresponding sections below. All tested generators make use
of up-convolutional (’deconvolutional’) layers, as in Dosovitskiy et al. . An up-convolutional layer consists
of up-sampling and a subsequent convolution. In this paper
we always up-sample by a factor of 2 and a ’bed of nails’
upsampling.
In all networks we use leaky ReLU nonlinearities, that is,
LReLU(x) = max(x, 0) + α min(x, 0). We used α =
0.3 in our experiments. All generators have linear output
Comparators. We experimented with four comparators:
1. AlexNet is a network with 5
convolutional and 2 fully connected layers trained on image
classiﬁcation.
2. The network of Wang & Gupta has the same
architecture as AlexNet, but is trained using videos with
triplet loss, which enforces frames of one video to be close
in the feature space and frames from different videos to be
far apart. We refer to this network as VideoNet.
3. AlexNet with random weights.
4. Exemplar-CNN is a network with 3 convolutional layers and 1 fully connected
layer trained on a surrogate task of discriminating between
different image patches.
The exact layers used for comparison are speciﬁed in the
experiments sections.
Discriminator. The architecture of the discriminator was
nearly the same in all experiments. The version used for
the autoencoder experiments is shown in Table 1. The discriminator must ensure the local statistics of images to be
natural. Therefore after ﬁve convolutional layers with occasional stride we perform global average pooling. The result
is processed by two fully connected layers, followed by a
Deep Perceptual Similarity Metrics
Table 1: Discriminator architecture.
2-way softmax. We perform 50% dropout after the global
average pooling layer and the ﬁrst fully connected layer.
There are two modiﬁcations to this basic architecture. First,
when dealing with large ImageNet images we increase the stride in the ﬁrst layer from 2 to 4.
Second, when training networks to invert AlexNet, we additionally feed the features to the discriminator. We process
them with two fully connected layers with 1024 and 512
units, respectively. Then we concatenate the result with the
output of global average pooling.
3.2. Training details
We modiﬁed the caffe framework to train
the networks. For optimization we used Adam with momentum β1 = 0.9, β2 = 0.999 and
initial learning rate 0.0002. To prevent the discriminator
from overﬁtting during adversarial training we temporarily
stopped updating it if the ratio of Ldiscr and Ladv was below a certain threshold (0.1 in most experiments). We used
batch size 64 in all experiments. We trained for 500, 000-
1, 000, 000 mini-batch iterations.
4. Experiments
We started with a simple proof-of-concept experiment
showing how DeePSiM can be applied to training autoencoders. Then we used the proposed loss function within
the variational autoencoder (VAE) framework. Finally, we
applied the method to invert the representation learned by
AlexNet and analyzed some properties of the method.
In quantitative comparisons we report normalized Euclidean error ||a −b||2/N. The normalization coefﬁcient
N is the average of Euclidean distances between all pairs
of different samples from the test set. Therefore, the error of 100% means that the algorithm performs the same as
randomly drawing a sample from the test set.
4.1. Autoencoder
Here the target of the generator coincides with its input
(that is, y = x), and the task of the generator is to encode the input to a compressed hidden representation and
then decode back the image. The architecture is shown in
Table 2. All layers are convolutional or up-convolutional.
Table 2: Autoencoder architecture. Top: encoder, bottom:
decoder. All layers are convolutional or ’up-convolutional’.
Our-AlexNet
Table 3: Normalized Euclidean reconstruction error (in %)
of autoencoders trained with different loss functions.
The hidden representation is an 8-channel feature map 8
times smaller than the input image. We trained on the STL-
10 unlabeled dataset which contains
100, 000 images 96 × 96 pixels. To prevent overﬁtting we
augmented the data by cropping random 64 × 64 patches
during training.
We experimented with four loss functions: SE and ℓ1 in the
image space, as well as DeePSiM with AlexNet CONV3 or
Exemplar-CNN CONV3 as comparator.
Qualitative results are shown in Fig. 3, quantitative results
in Table 3. While underperforming in terms of Euclidean
loss, our approach can preserve more texture details, resulting in naturally looking non-blurry reconstructions. Interestingly, AlexNet as comparator tends to corrupt ﬁne details (petals of the ﬂower, sails of the ship), perhaps because it has stride of 4 in the ﬁrst layer. Exemplar-CNN
as comparator does not preserve the exact color because it
is explicitly trained to be invariant to color changes. We
believe that with carefully selected or speciﬁcally trained
comparators yet better results can be obtained.
We stress that lower Euclidean error does not mean better
reconstruction. For example, imagine a black-and-white
striped ”zebra” pattern. A monotonous gray image will
have twice smaller Euclidean error than the same pattern
shifted by one stripe width.
Classiﬁcation.
Reconstruction-based models are commonly used for unsupervised feature learning. We checked
Our-AlexNet
34.6 ± 0.6
35.7 ± 0.4
50.1 ± 0.5
52.3 ± 0.6
Table 4: Classiﬁcation accuracy (in %) on STL with autoencoder features learned with different loss functions.
Deep Perceptual Similarity Metrics
Figure 3: Autoencoder qualitative results. Best viewed on
if our loss functions lead to learning more meaningful representations than usual ℓ1 and SE losses.
To this end,
we trained linear SVMs on the 8-channel hidden representations extracted by autoencoders trained with different
losses. We are just interested in relative performance and,
thus, do not compare to the state of the art. We trained on
10 folds of the STL-10 training set and tested on the test
The results are shown in Table 4. As expected, the features learned with DeePSiM perform signiﬁcantly better,
indicating that they contain more semantically meaningful
information. This suggests that other losses than standard
ℓ1 and SE may be useful for unsupervised learning. Note
that the Exemplar-CNN comparator is trained in an unsupervised way.
4.2. Variational autoencoder
A standard VAE consists of an encoder Enc and a decoder
Dec. The encoder maps an input sample x to a distribution
over latent variables z ∼Enc(x) = q(z|x). Dec maps
from this latent space to a distribution over images ˜x ∼
Dec(z) = p(x|z). The loss function is
−Eq(z|xi) log p(xi|z) + DKL(q(z|xi)||p(z)),
where p(z) is a prior distribution of latent variables and
DKL is the Kullback-Leibler divergence. The ﬁrst term in
Eq. 6 is a reconstruction error. If we assume that the decoder predicts a Gaussian distribution at each pixel, then
it reduces to squared Euclidean error in the image space.
The second term pulls the distribution of latent variables
towards the prior. Both q(z|x) and p(z) are commonly assumed to be Gaussian, in which case the KL divergence
can be computed analytically. Please refer to Kingma et al.
 for details.
We use the proposed loss instead of the ﬁrst term in Eq. 6.
This is similar to Larsen et al. , but the comparator
does not have to be a part of the discriminator. Technically, there is little difference from training an autoencoder.
First, instead of predicting a single latent vector z we predict two vectors µ and σ and sample z = µ + σ ⊙ε, where
ε is standard Gaussian (zero mean, unit variance) and ⊙is
element-wise multiplication. Second, we add the KL divergence term to the loss:
2 + ||σi||2
2 −⟨log σ2
We manually set the weighting of the KL term relative to
the rest of the loss. Proper probabilistic derivation is nonstraightforward, and we leave it for future research.
We trained on 227 × 227 pixel crops of 256 × 256 pixel
ILSVRC-2012 images.
The encoder architecture is the
same as AlexNet up to layer FC6, and the decoder architecture is shown in Table 5. We initialized the encoder with
AlexNet weights, however, this is not necessary, as shown
Figure 4: Samples from VAE with the SE loss (topmost)
and the proposed DeePSiM loss (top to bottom: AlexNet
CONV5, AlexNet FC6, VideoNet CONV5).
Deep Perceptual Similarity Metrics
uconv conv uconv conv uconv uconv uconv
Table 5: Generator architecture for inverting layer FC6 of
in the appendix. We sampled from the model by sampling
the latent variables from a standard Gaussian z = ε and
generating images from that with the decoder.
Samples generated with the usual SE loss, as well as
three different comparators (AlexNet CONV5, AlexNet
FC6, VideoNet CONV5) are shown in Fig. 4. While Euclidean loss leads to very blurry samples, our method yields
images with realistic statistics. Interestingly, the samples
trained with the VideoNet comparator look qualitatively
similar to the ones with AlexNet, showing that supervised
training may not be necessary to yield a good comparator.
More results are shown in the appendix.
4.3. Inverting AlexNet
Analysis of learned representations is an important but
largely unsolved problem. One approach is to invert the
representation. This may give insights into which information is preserved in the representation and what are its
invariance properties. However, inverting a non-trivial feature representation Φ, such as the one learned by a large
convolutional network, is a difﬁcult ill-posed problem.
Our proposed approach inverts the AlexNet convolutional
network very successfully. Surprisingly rich information
about the image is preserved in deep layers of the network
and even in the predicted class probabilities. While being
an interesting result in itself, this also shows how DeeP-
SiM is an excellent loss function when dealing with very
difﬁcult image restoration tasks.
Suppose we are given a feature representation Φ, which
we aim to invert, and an image I. There are two inverse
mappings: Φ−1
R such that Φ(Φ−1
R (φ)) ≈φ, and Φ−1
L (Φ(I)) ≈I. Recently two approaches to inversion have been proposed, which correspond to these two
variants of the inverse.
Mahendran & Vedaldi , as well as Simonyan et al.
 and Yosinski et al. , apply gradient-based
Figure 6: Comparison with Dosovitskiy & Brox 
and Mahendran & Vedaldi . Our results look signiﬁcantly better, even our failure cases (second image).
optimization to ﬁnd an image eI which minimizes the loss
||Φ(I) −Φ(eI)||2
2 + P(eI),
where P is a simple natural image prior, such as total variation (TV) regularizer. This method produces images which
are roughly natural and have features similar to the input features, corresponding to Φ−1
R . However, the prior is
limited, so reconstructions from fully connected layers of
AlexNet do not look much like natural images.
Dosovitskiy & Brox train up-convolutional networks on a large training set of natural images to perform
the inversion task. They use SE distance in the image space
as loss function, which leads to approximating Φ−1
networks learn to reconstruct the color and rough positions
of objects well, but produce over-smoothed results because
they average all potential reconstructions.
Our method can be seen as combining the best of both
worlds. Loss in the feature space helps preserve perceptually important image features. Adversarial training keeps
reconstructions realistic. Note that similar to Dosovitskiy
& Brox and unlike Mahendran & Vedaldi ,
our method does not require the feature representation being inverted to be differentiable.
Technical details. The generator in this setup takes the features extracted by AlexNet and generates an image from
Deep Perceptual Similarity Metrics
Figure 5: Representative reconstructions from higher layers of AlexNet. General characteristics of images are preserved
very well. In some cases (simple objects, landscapes) reconstructions are nearly perfect even from FC8. In the leftmost
column the network generates dog images from FC7 and FC8.
them, that is, x = Φ(I), y = I.
In general we followed Dosovitskiy & Brox in designing the generators. The only modiﬁcation is that we inserted more convolutional layers, giving the network more capacity. We
reconstruct from outputs of layers CONV5 –FC8. In each
layer we also include processing steps following the layer,
that is, pooling and non-linearities. So for example CONV5
means pooled features (pool5), and FC6 means rectiﬁed
values (relu6).
Architecture used for inverting FC6 is the same as the decoder of the VAE shown in Table 5.
Architectures for
other layers are similar, except that for reconstruction from
CONV5 fully connected layers are replaced by convolutional ones. The discriminator is the same as used for VAE.
We trained on the ILSVRC-2012 training set and evaluated
on the ILSVRC-2012 validation set.
Ablation study. We tested if all components of our loss
are necessary. Results with some of these components removed are shown in Fig. 7. Clearly the full model performs
best. In the following we will give some intuition why.
Training just with loss in the image space leads to averaging all potential reconstructions, resulting in oversmoothed images.
One might imagine that adversarial
training would allow to make images sharp. This indeed
happens, but the resulting reconstructions do not correspond to actual objects originally contained in the image. The reason is that any “natural-looking” image which
roughly ﬁts the blurry prediction minimizes this loss. Without the adversarial loss predictions look very noisy. With-
Figure 7: Reconstructions from FC6 with some components of the loss removed.
out the image space loss the method works well, but one
can notice artifact on the borders of images, and training
was less stable in this case.
Sampling pre-images. Given a feature vector φ, it would
be interesting to sample multiple imageseI such that Φ(eI) =
φ. A straightforward approach would inject noise into the
generator along with the features, so that the network could
randomize its outputs. This does not yield the desired result, since nothing in the loss function forces the generator
to output multiple different reconstructions per feature vector. A major problem is that in the training data we only
have one image per feature vector, i.e., a single sample per
conditioning vector. We did not attack this problem in our
paper, but we believe it is an important research direction.
Deep Perceptual Similarity Metrics
Mahendran&Vedaldi
Dosovitskiy & Brox
Our just image loss
Our AlexNet CONV5
Our VideoNet CONV5
Figure 8: Normalized inversion error (in %) when reconstructing from different layers of AlexNet with different
methods. First in each pair – error in the image space, second – in the feature space.
Figure 9: Iteratively re-encoding images with AlexNet and
reconstructing. Iteration number shown on the left.
Best results. Representative reconstructions from higher
layers of AlexNet are shown in Fig. 5. Comparison with
existing approaches is shown in Fig. 6. Reconstructions
from CONV5 are near-perfect, combining the natural colors and sharpness of details. Reconstructions from fully
connected layers are still very good, preserving the main
features of images, colors, and positions of large objects.
Normalized Euclidean error in image space and in feature space (that is, the distance between the features of
the image and the reconstruction) are shown in Table 8.
The method of Mahendran&Vedaldi performs well in feature space, but not in image space, the method of Dosovitskiy&Brox — vice versa. The presented approach is fairly
good on both metrics.
Iterative re-encoding. We performed another experiment
illustrating how similar are the features of reconstructions
to the original image features. Given an image, we compute
its features, generate an image from those, and then iteratively compute the features of the result and generate from
those. Results are shown in Fig. 9. Interestingly, several iterations do not signiﬁcantly change the reconstruction, indicating that important perceptual features are preserved in
the generated images. More results are shown in the appendix.
Interpolation. We can morph images into each other by
linearly interpolating between their features and generating the corresponding images. Fig. 11 shows that objects
Figure 10: Reconstructions from FC6 with different comparators. The number indicates the layer from which features were taken.
shown in the images smoothly warp into each other. More
examples are shown in the appendix.
Different comparators. AlexNet network we used above
as comparator has been trained on a huge labeled dataset.
Is this supervision really necessary to learn a good comparator? We show here results with several alternatives to
CONV5 features of AlexNet: 1) FC6 features of AlexNet,
2) CONV5 of AlexNet with random weights, 3) CONV5 of
the network of Wang & Gupta which we refer to as
The results are shown in Fig. 10. While AlexNet CONV5
comparator provides best reconstructions, other networks
preserve key image features as well. We also ran preliminary experiments with CONV5 features from the discriminator serving as a comparator, but were not able to get
satisfactory results with those.
5. Conclusion
We proposed a class of loss functions applicable to image
generation that are based on distances in feature spaces.
Applying these to three tasks — image auto-encoding, random natural image generation with a VAE and feature inversion — reveals that our loss is clearly superior to the typical loss in image space. In particular, it allows reconstruction of perceptually important details even from very lowdimensional image representations. We evaluated several
feature spaces to measure distances. More research is necessary to ﬁnd optimal features to be used depending on the
task. To control the degree of realism in generated images,
an alternative to adversarial training is an approach making
use of feature statistics, similar to Gatys et al. . We
see these as interesting directions of future work.
Deep Perceptual Similarity Metrics
Image pair 1
Image pair 2
Figure 11: Interpolation between images by interpolating
between their features in FC6 and FC8.
Acknowledgements
The authors are grateful to Jost Tobias Springenberg and
Philipp Fischer for useful discussions. We acknowledge
funding by the ERC Starting Grant VideoLearn (279401).