psychonomic Bulletin & Review
1997,4(1), 79-95
Applying Occam's razor in modeling cognition:
A Bayesian approach
IN JAE MYUNG and MARK A. PITT
Ohio State University, Columbus, Ohio
In mathematical modeling of cognition, it is important to have well-justified criteria for choosing
among differing explanations (i.e., models) of observed data. This paper introduces a Bayesian model
selection approach that formalizes Occam's razor, choosing the simplest model that describes the
data well. The choice of a model is carried out by taking into account not only the traditional model
selection criteria (i.e., a model's fit to the data and the number of parameters) but also the extension
ofthe parameterspace, and, most importantly, the functional form of the model (i.e.,the way in which
the parameters are combined in the model's equation). An advantage of the approach is that it can
be applied to the comparison of non-nested models as well as nested ones. Application examples are
presented and implications of the results for evaluating models of cognition are discussed.
A goal ofresearch in psychology, as in other behavioral
sciences, is to infer the underlying process that generated
observed data. The use of sophisticated mathematical
models to describe these processes has grown considerably,
especially in cognitive psychology .
Yet, the development of equally sophisticated and welljustified methods for evaluating the adequacy ofthe models themselves has lagged behind. Jacobs and Grainger
 recently summarized a number ofcriteria for choosing among models: (I) generality (does the model generalize well across different experimental settings?); (2) explanatory adequacy (are the assumptions of the model
plausible and compatible with established findings?);
(3) descriptive adequacy (does the model fit the pattern
of data well?); and (4) complexity .
[e.g., the number of parameters] of the model simple?).
Among these, descriptive adequacy and complexity have
been used most frequently, probably because they are
easier to quantify than the other two. Togetherthey embody
the principle ofOccam's razor, which states that "entities
should not be multiplied beyond necessity" (William of
Occam, ca. 1290-1349). The goal of model selection is
to choose the simplest (i.e., least complex) model that describes the data well (i.e., descriptive adequacy).
In this paper, we introduce a new Bayesian method of
formalizing Occam's razor in model selection. It goes beyond current selection methods by taking into account
dimensions of complexity that are not captured by its
predecessors. We begin with a tutorial on model selection
methods that are currently in use. Next, fundamentals of
the Bayesian model selection approach are described,
and its desirable properties are discussed. The utility of
the Bayesian approach is then demonstrated using concrete examples with simulated data. Finally,the merits and
shortcomings of the Bayesian approach are discussed
and contrasted with traditional approaches.
MODEL SELECTION CRITERIA
Descriptive Adequacy
The goal of mathematical modeling in cognitive psychology is straightforward: Given observed data, identify
the underlying processes that generated the data. Because
a model is defined as a set of assumptions about underlying processes, the goal ofthe researcher is to determine
the viability ofthe model. There are, however, at least two
obstacles to such an endeavor. First, given the nearly infinite number of distinct models that can be defined by
combining different assumptions, the true model might
Copyright 1997 Psychonomic Society, Inc.
MYUNG AND PITT
not be one of a particular set ofmodels that is being tested.
Second, random noise in data can obscure model identification. Consequently, a realistic goal of mathematical
modeling is to choose the model that represents the closest approximation to the "true" model.
How can the closest approximation be identified when
the true model has yet to be discovered? Consider the situation in which the true model is included in the set of
models being tested and further, data are noise free. In this
ideal situation, the true model must fit the data perfectly
(e.g., as measured using a metric such as sum ofsquared
errors). Note that this is a necessary-but not sufficientcondition, for there could be more than one model that fits
the data perfectly. By extending this logic to less ideal situations (e.g., noisy data), the following model selection
rule is obtained: Choose the model that provides the best
fit to the data. Accordingly,the foremostcriterion ofmodel
selection, descriptive adequacy, is born. Examples ofdescriptive adequacy measures that are in use include the
percent variance accounted for by the model (i.e., coefficient ofdetermination), the sum ofsquared errors (SSE)
between observed and predicted outcomes, and the maximum likelihood, in which the probability ofobtaining the
observed data is maximized with respect to the model's
possible parameter values .
For a model to be considered true, it must satisfy the
minimal condition of sufficiency in fitting data well. A
failure to do so invalidates the model. An illustrative example is shown in Figure 1. Each of the four solid lines
in the figure represents a model's best fit to the same data
set (solid dots) using the least squares estimation method.
Modell is a two-parameter linear model. As can be seen,
it fits the data poorly, with only 79.5% of the variance
accounted for. Systematic deviations from the line are evident at the endpoints and in the middle of the range.
Clearly,Modell fails the test ofdescriptive adequacy and
can be dropped from further consideration. In contrast,
Model 2, a three-parameter exponential model, fits the
data fairly well, accounting for 96% ofthe variance with
no systematic deviation from the fit. Between the first two
models, Model 2 would be chosen as the preferred description ofthe data. Models 3 and 4 are discussed in the
next section.
Model Complexity
It is important to note that descriptive adequacy is a
heuristic. Selection ofthe best fitting model may be useful in identifying the true model or closest approximation,
but the rule's accuracy is not guaranteed. This is because
model fit can be improved by increasing model complexity. Complexity refers to the flexibility inherent in a
model that enables it to fit diverse patterns of data. I It
can be understood by contrasting the data-fitting capabilities of simple and complex models. A simple model
is one that assumes that a specific pattern will be found
in the data. If this pattern occurs, the model will fit the
data well. Simple models make clear and falsifiable predictions precisely because a specific pattern is assumed
to be present. In terms ofactually testing the model, what
this means is that the model's fit will be good over a sizable range ofparameter values. A complex model, on the
other hand, is more flexible than a simple model, providing good fits to a wide range of data patterns. To do so,
however, the complex model's parameters must be finely
tuned. This is because as the model's parameters change,
even slightly, the postulated data pattern also changes.
There are at least three dimensions of a model that contribute to its complexity, thereby significantly affecting
model fit: the number of parameters, the model's functional form, and the extension of the parameter space. In
Model 1 (n=2. 79.57.)
Model 3 (n=7. 98.57.)
Model 2 (n=3. 96.07.)
Model 4 (n=20. 1007.)
Figure 1. The effect of the number of parameters in a model on the model's ability
to fit data. A single data set (dots) was fitted to four models (lines) differing in the
number of parameters (n). Percentage of variance accounted for by each model is
shown in parentheses.
the following subsections, we discuss the implications of
each of these for model selection.
Number of parameters. In general, a model with
many parameters fits data better than a model with few
parameters, even ifthe latter generated the data . The effect ofexcessive parameters on model fit is
illustrated in the bottom panels in Figure 1. Model 3 was
created from Model 2 by introducing an additional cyclic
component with four new parameters. Its fit is improved
over Model 2, but only by a meager 2.5%. The extra parameters seem unnecessary, capturing what appear to be
a few idiosyncracies in the data. Model 4 is a more dramatic example ofa model with excessive parameters. It fits
the data perfectly and even tells us more than the data do.
Model 4 is an interpolation model with 20 parameters,
the same number as data points. Such a model can always be found and by design, it will fit the data perfectly.
A problem with Models 3 and 4 is that they generalize
poorly to other data because they precisely fit only one
data set. Thus it seems unlikely that these models accurately reflect the mental processes responsible for generating the data.
The preceding examples should make it clear that there
is a tradeoffbetween fit and generalizability as the number of parameters increases. The best fit should be preferred when it is not achieved at the expense ofadditional
parameters. Three
model selection methods were proposed that adjust for
variation in the number of parameters among models.
They do so by penalizing more heavily models with many
parameters as opposed to those with few parameters.
Akaike introduced the Akaike information criterion (AIC), which is defined as
AIC; = -2In(ML;) + Zn;
In this equation, ML; is the maximum likelihood for
Model i and n; is the number of free parameters in the
model. The criterion prescribes that the model that minimizes the Ale should be chosen. If the choice is between
two models with equal maximum likelihood values but
different numbers of parameters, AIC favors the model
with fewer parameters. The AIC has been employed in
time series analysis , psychometric data
analysis , and mathematical modeling ofcategorization .
Schwarz introduced another criterion called
the Bayesian information criterion (BIC), defined as
BIC; = -2In(ML;) + 2n;lns
where In(s) is the natural log of sample size (s: number
ofobservations per stimulus) ofthe data. The model that
minimizes BIC should be chosen. Note its similarity to
Ale. Only the penalty term for excessive parameters is
slightly different (2n; vs. n; In(s)). A comparison ofboth
criteria shows that AIC favors more complex models than
BIC for large sample sizes when In(s) > 2 (i.e., s > 8).
BAYESIAN MODEL SELECTION
Steiger and colleagues introduced yet another criterion, called the root
mean square error ofapproximation (RMSEA), defined as
In this equation, the function F; is a measure of the lack
offit for model i (e.g., using SSE) and N is the data size.?
RMSEA penalizes complex models by subtracting the
number of parameters (n i ) from the divisor, N. The
RMSEA closely approximates the root mean squared deviation (RMSD), which is a sample statistic often used in
mathematical modeling (see, e.g., Massaro & Friedman,
Functional form. The number of parameters is the
only dimension ofmodel complexity that AIC, BIC, and
RMSEA consider. An often unrecognized dimension of
model complexity that can significantly affect model fit
by simply capturing irrelevant patterns of data is functional form, which can be defined as the way in which
parameters are combined in the model equation.
Figure 2 illustrates the effect that functional form can
have on model fit. There is a universe (U) of possible
data, subsets ofwhich are consistent with different models. The set M, denotes a data region that Model M fits
"well" in an appropriately defined sense (e.g., maximum
likelihood). Model Mb, which has more parameters than
M, but the same functional form, accounts for a wider
range of data. Now consider the third model, Me' which
has the same number ofparameters as Model M, but assumes a functional form different from that of Ma. Not
only is the Meregion larger than that of'M, (i.e., more data
are accounted for by Me) but also, most of the data that
M, fits can also be fitted by Me' Importantly, the reverse
is not true. Thus the functional form ofModel M makes
it a more flexible model than Ma.
Another example ofthe importance offunctional form
in model behavior is a comparison of two psychophysical models. Townsend pointed out that Stevens's
law ('P(x) = k . xa) is more complex, and thus less falsifiable, than Fechner's law ('P(x) = k . In(x+ /3)), even
though both have the same number ofparameters. This is
Figure 2. Data-fitting capabilities of various models.
MYUNG AND PITT
because psychological and physical dimensions are assumed to be related by a power function in Stevens's law,
making it capable offitting data that have negative, positive, and zero curvature. Fechner's law assumes a logarithmic relationship, which can fit data patterns with a negative curvature only.
More recently, the superfluous effects of functional
form in model fitting were shown in an insightful simulation study by Cutting, Bruno, Brady, and Moore .
They compared two non-nested models ofperception, the
linear integration model and the
fuzzy logic model ofperception .3 The two models have the same number of parameters but different functional forms, a linear additive
function in LIM and a nonlinear multiplicative function
in FLMP. In the first simulation, data patterns were generated that spanned the range of the parameter space.
Both models were then fitted to all data patterns. Results
showed that FLMP was more flexible than LIM, providing a superior fit (i.e., smaller RMSD) for 80.3% of the
data patterns with exponential functions and 95.7% with
logistic functions. If the number ofparameters were the
only factor affecting model complexity, the percentage
of superior fits should have been about 50%.
A more convincing demonstration ofFLMP's superior
functional form can be found in the results from another
simulation, in which the models were fitted to a set of
random numbers (dependent variable) generated from a
uniform distribution ranging from 0 to 1. Again, FLMP
performed at better than chance level (60.8%). Further
exploring this issue, Cutting et al. also fitted the
models to noisy data whose means were generated by
LIM. Again, FLMP fitted the data better than LIM itself
about 60% of the time, and did so even more decisively
when the added noise was relatively large. Given that
LIM produced the data, the percentage ofsuperior fits by
FLMP should have been at least below the chance level
of 50%, though ideally it should have been zero.
Massaro and Cohen argued that Cutting et al.s
 simulations distorted the true data-fitting abilities
of FLMP. Massaro and Cohen stated that in the simulations in which the FLMP and LIM were fitted to a range
ofexponential and logistic functions, the functions were
ones that favored FLMp, and that a set ofplausible functions could be created that could just as easily cause the
LIM to provide a superior fit. This criticism would have
been more convincing if supporting evidence had been
provided. Massaro and Cohen also claimed that the simulations in which the models were fitted to random data provided an unfair test of the models, because both models
fit the data so poorly (very large RMSDs) that FLMP's
superior fits are not meaningful. This reasoning ignores
the consistency with which FLMP provided better fits than
LIM and places an arbitrary limit on the interpretation of
goodness-of-fit measures. Regardless ofthe quality ofthe
fit, the simulations clearly show FLMP's superior ability
at fitting data. Finally, Massaro and Cohen argued that
the simulations using noisy data were invalid because Cutting et al. had used an artificially narrow range of data
values (between 0.3 and 0.7, whereas observed values
often span from 0 to 1). Our own simulations in the present paper go a long way toward dispelling this criticism.
Data values ranged between 0.1 and 0.9, and we consistently found that FLMP provided superior fits. Furthermore, we also found that LIM never provided superior
fits to those provided by FLMP. Ifthe two models could
mimic each other equally well (i.e., if they were equally
flexible), then LIM should have fit FLMP data as well as
FLMP fit LIM data. Without assuming that FLMP is
more flexible than LIM, it is difficult to explain the simulation results, in particular the asymmetric pattern of
model mimicry.
The point ofthis discussion is not to argue against the
psychological validity ofFLMP or LIM, but rather to make
clear the importance of functional form in modelselection. Because both models have the same number ofparameters, FLMP's advantage must be due to the functional
form of the model equation. The nonlinear function of
FLMP seems more flexible than the linear function ofLIM
in fitting noisy data, thus improving model fit.
Extension of parameter space. Another dimension
ofmodel complexity that can also influence model fit is
the extension of the parameter space. To illustrate the
point, consider two one-parameter models that share the
same functional form ofy = 1/(1+e- 8x ), but assume different ranges of the parameter 8. In Modell, 8 ranges
from -R to R, where R is a constant. In Model 2, 8's
range is cut in half, spanning from 0 to R. Modell allows
the parameter to be either positive or negative, whereas
Model 2 allows it to be only positive. The parameter range
of Model 1 is twice that of Model 2. As shown in Figure 3, this difference in parameter space means that
Model 1 can fit decreasing (8 < 0) and increasing (8 >
0) patterns ofdata. Model 2 can fit only the latter. Solely
by virtue of the larger parameter space, Modell is guaranteed to fit data showing a decreasing pattern better than
To summarize, mathematical models can vary along a
number of dimensions. It is necessary to incorporate all
ofthem into the model selection procedure to maximize
its accuracy. Selection ofa model based solely on descriptive adequacy can lead to incorrect conclusions unless
appropriate adjustments are made for the three dimensions ofmodel complexity (number ofparameters, functional form, and extension ofparameter space), which can
significantly and independently affect model fit. Standard model selection criteria such as AIC and BIC consider only the first of these." Such criteria are appropriate to use when comparing nested models, which tends to
be done in psychometric research (e.g., analysis ofvariance models, regression models, latent variable models).
However, their use in comparing non-nested models,
which is virtually always the case in models ofcognition,
is not justified and can lead to erroneous conclusions, as
we demonstrate in the section Application Example of
Bayesian Model Selection.
The Bayesian method described next provides a way to
overcome the limitation ofthe standard model selection
BAYESIAN MODEL SELECTION
Figure 3. An example of how the extension of the parameter space can affect model fit. A range of possible data patterns that each of two different
models can fit is shown. The two models share the same functional form ofy =
1/(1+r 8x ) but assume different ranges ofthe parameter 8: -R < 8< R < co
for Modell; 0 < 8 < R < oo for Model 2. Values of R are listed next to the corresponding function.
criteria. The method combines all three dimensions of
model complexity, as well as descriptive adequacy, into
a single measure, thus making it particularly suitable for
testing models of cognition.'
BAYESIAN MODEL SELECTION
We begin this section by describing the basics of the
Bayesian model selection approach, followed by a consideration of the computational issues surrounding its
implementation.
Fundamentals of Bayesian Model Selection
Recently there has been a surge ofinterest in Bayesian
methods of model selection among statisticians, mathematicians, physicists, chemists, and neural network researchers . In this section, we provide a comprehensive overview of Bayesian
model selection methods, emphasizing their utility in evaluating models ofcognition and discussing their implications for mathematical modeling of psychological data
 .
Application ofthe Bayesian approach requires a statistical formulation of a model that specifies the probability density function (i.e., distribution) ofthe data. For example, a model of semantic priming may assume that
response times in the lexical decision task follow a normal probability density with a certain mean and standard
deviation. Moreover, the mean may increase or decrease
depending on theoretical predictions, whereas the standard
deviation is an unknown but fixed quantity.
Model selection is carried out by computing the posterior probability that each of the models is correct (i.e.,
true) given a particular data set. This process is formally
stated as follows:
For each model M; and the data D, compute the posterior probability, P(M;I D), from the prior probability, P(M;), and the evidence for M;, P(D IM;), using
the Bayes rule, and then choose the model that maximizes the posterior probability.
MYUNG AND PITT
For simplicity, we will consider only the two-model case
ofmodels M] and M2, although extension to multimodel
cases is straightforward. From the Bayes rule, we obtain
P(MiID) = P(M[) . P(DIM[)
The first term, P(M11 D)/P(M21D), is called the posterior
odds ratio, and represents the ratio of the model likelihoods given the data. The second term, P(M 1)/P(M2 ) , is
called the prior odds ratio; it represents the ratio of the
model likelihoods before evaluating the data. Model priors are determined independently of the data. The third
term, P(D IM])/P(DI M2 ) is called the Bayes factor or evidence ratio. The numerator and denominator each represent the probabilities ofthe data given each model. Model
selection is achieved by computing the prior odds ratio
and the Bayes factor. In other words, the equation above
can be rewritten as
posterior odds ratio = (prior odds ratio) X (Bayes factor).
Prior odds P(Mj ) . P(MJ represents the probability
that the model M, is a true description ofthe events under
study before data are collected. For real-world problems,
this probability may not exist, or not be known even if it
does. We assume equal model priors; that is, P(M])/
P(M2) = 1. This assumption is often made by researchers in Bayesian model selection , and is an unavoidable simplification ofa problem
for which there is not as yet a satisfactory solution. An implication ofthe assumption is that model selection is based
solely on the Bayes factor. The decision to ignore model
priors does not undermine use ofthe Bayesian approach.
Rather, it represents a calculated approximation to the
true Bayesian model selection criterion.v
Bayes factor P(DIM1)/P(DIM2) . The Bayes factor is
defined as a ratio oftwo marginal likelihoods, which can
best be understood by noting their relationship to likelihood functions. A likelihood function is the probability
of the given data computed for a particular value of the
parameter of a model, P(DI 8,M,'), and thus is a function
ofthe parameter 8. In essence, the likelihood function is
a goodness-of-fit measure of a model for a given parameter value. Ifthe value ofthe parameter changes, the fit
might also change. The marginal likelihood of a model,
P(DIM.), is the probability of the data as a whole, independent of parameter values. It is an average of likelihoods under a prior distribution ofthe parameter. The marginal likelihood is expressed in the following integral
P(DIMJ = JP(DI8,M;)P(8I MJd8
(i = 1,2).
where 8is a parameter vector under Model i, P(DI8,Mi)
is the likelihood function, and P(8 IMi ) is the prior density of 8 for Model i.
Under the particular assumption of a uniform prior
density function of the parameter [i.e., P( 8 IMJ = constant], the marginal likelihood is proportional to the area
Parameter 0
Figure 4. Maximum likelihood functions for three pairs of
models as a function of the parameter e. Models A, C, and E are
more complex (more peaked functions) than are models B, D,and
F. See text for details.
under the curve representing the likelihood function. To
illustrate, likelihood functions for two models, A and B,
are shown in the top panel of Figure 4. Note that the abscissa of the figure represents parameter value, not data
value. Although the maximum likelihoods (i.e., the highest values) are the same for both models, the marginal
likelihood ofModel B is larger because the area under its
function is larger. Note that the functions should be
thought ofas actual representations ofthe models. Their
peakedness is determined by model complexity, to which
we now turn.
Model Complexity: A Bayesian View
A model is selected in the Bayesian approach by maximizing the marginal likelihood. Under some simplifying
assumptions, the marginal likelihood can be expressed
in words as
goodness of fit
marginal likelihood =
mode comp exity
The numerator, goodness of fit, is the maximum likelihood ofthe data given the model, P(DI8o,Mi ) , where 80
is the parameter value that maximizes the likelihood
function. The denominator term represents a model complexity measure that embodies all three dimensions of
complexity: number ofparameters, functional form, and
extension of parameter space. A simplified mathematical function depicting their relationship is shown below.
(A technical discussion of the mathematics is presented
in Appendix A.)
model complexity = g(n;,F;(80),RJ
g(n;,F;(80 ),R;) = (JF;(80 )(Ri! ... R;n)
In the equation above, {J is a positive scaling factor, n; is
the number of parameters of Model i, and R; = (Ri!"'"
R;n) is a vector representing the range of the parameter
vector 80fModei i (e.g., R;k equals 2 ifthe parameter 8k
is defined on - I ~ 8k ~ + I). F;(80 ) is a "functionalform" factor whose value depends on how the parameters are combined mathematically in the model. Because
this factor is evaluated at 80 , which is determined by the
data, the value ofthe functional-form factor may depend
on the data.? Roughly speaking, the peakedness of the
likelihood function near 80 is positively correlated with
the value of F;(80 ) ' For example, in Figure 3, F;(80 ) is
higher for Model A than for Model B. As F,(( 0 ) , Rik , and
n; increase, so does model complexity. According to
Equation 6, a more complex model yields a smaller marginal likelihood; consequently, it is less likely to be selected as the best fitting model. Model B would be chosen over Model A using the Bayesian method if other
things were equal (i.e., same R;k and n; for both models).
The middle and lower panels ofFigure 4 are examples
of other model relationships that illustrate further how
model complexity affects model selection. For the pairs
ofmodels, assume that they have the same number ofparameters and the same extension ofthe parameter space,
and further, that the prior density of the parameter follows a uniform distribution. In the lower panel, both selection methods (Bayesian and standard) would likely
reach the same conclusion when evaluating Models E and
F. Not only does Model F provide a much poorer fit than
Model E, but also its marginal likelihood is smaller.
Whether the maximum likelihood or the marginallikelihood were used to select the most appropriate model,
Model E would be the clear winner.
In the middle panel, Model C would be chosen over
Model D using the standard method because C's maximum likelihood is larger than D's. Bayesian model selection is less straightforward in this case. Even though D's
fit is not as good, it would be chosen because its marginal
likelihood is larger than C's. Maximization of the marginal likelihood favors a model that not only fits data
well, but also does so over a wide range of parameters.
Model D does so to a far greater extent than C. Comparisons such as this, in which goodness of fit and model
complexity differ greatly between models, should benefit from application of the Bayesian approach.
In this last example, the Bayesian method might seem
to yield counterintuitive results because the model that
provides the best fit to the data is not chosen as the pre-
BAYESIAN MODEL SELECTION
ferred model. This is because model complexity is just
as important as model fit in influencing model selection.
In the Bayesian approach, the ability ofthe model to fit the
data reasonably well over a range of parameter values is
what is important because it indicates that the model
captures the structure in the data with a minimum ofcomplexity (e.g., minimal reliance on functional form, number
of parameters, etc.). Models in which parameters must
be finely tuned to achieve a good fit (Models A and C)
are valued less, precisely because the fit is achieved by
relying on the complexity ofthe model. The contribution
ofcomplexity to model fit should not be underestimated.
By finely tuning parameters, a complex model can fit a
range ofdata patterns, not because it is the "true" model,
but because it is the most flexible, easily adjusting to variations in the data. Taken to an extreme, one could imagine a highly complex model fitting almost any data set
very well.
The preceding examples should make it clear that the
difference in the pattern of data to be maximized by the
likelihood function versus the marginal likelihood can
have nontrivial implications for model selection and is at
the heart ofthe Bayesian approach. Maximization ofthe
marginal likelihood is accomplished by pitting maximization ofthe likelihood function against complexity minimization. A more complex model will be favored only if
its goodness offit is large enough tojustify the additional
complexity ofthe model. It is in this sense that Bayesian
model selection is a quantitative implementation of Occam's razor.
Computation of the Bayes Factor
Although the simplified expression in Equation 6 is
useful to illustrate basic ideas of Bayesian model selection, it is derived under some restrictive assumptions about
the shape ofthe likelihood function. Goodness offit and
model complexity are not, in general, separable from each
other, but instead, the two work together implicitly within
a single measure, which is the marginal likelihood. Computation of the Bayes factor P(DIM,)/P(DIMz) then requires evaluating the multiple integral in Equation 5.
Readers who are not interested in the mathematical details ofcomputing the Bayes factor should skip to the section, Application Example ofBayesian Model Selection.
PriorsP(8IMj ) . Toevaluate the integral, the prior density of the parameter, P and Kass and Raftery .
A simple-minded method for choosing priors is to use
a noninformative prior, which by definition assumes no
information about the parameter 8. For example, the uniform density function 7C(8) = I on 0 ~ 8 ~ 1 is a noninformative prior. Although a noninformative prior would
be an obvious choice when absolutely no information is
MYUNG AND PITT
available, often the noninformative prior is an improper
prior having infinite mass (i.e., frc(8)d8 = 00). One example is the uniform density rc(8) = c on 0 ~ 8 < 00
where c > O. Another reasonable choice of a noninformative prior for 0 ~ 8 < 00 that has the desirable scaleinvariance property [i.e., rc(8) = rc(8/a)/ a for any a>
0] is rc(8) = 1/8; this is also an improper prior. Although
the justification and interpretation of noninformative
improper priors is disputed ,
they are routinely used in Bayesian inference .
Informative priors can be used when information about
8 is available. One way to do this is to obtain an informative prior from an experimental setup. To illustrate,
consider a signal detection experiment consisting of n
conditions in which the signal-to-noise ratio is systematically varied from lowest in Condition 1to highest in Condition n. Assume that the number of correct responses
out ofa total ofs trials in Condition k (k = 1,... , n) is binomially distributed with Binis, 8k ) where 8k (0 ~ 8k ~ I)
is the binomial probability parameter. A reasonable way
to obtain the prior rc(8" ... ,en) is to assume that ek s are
the order statistics (i.e., ei ~ ~ for any i <j), resulting from
an independent random sample from the uniform distribution on . For example, given a particular random
sample of three (n = 3) observations, (.53, .29, .87), from
the uniform distribution, the desired order statistics are
obtained as e, = .29, e2 = .53, and e3 = .87. For other
similar cases, by being creative and reasonable, one can
find priors that best capture the information available in
the experimental design as well as in the data.
Numerical methods for computing the Bayes factor. A closed-form solution to the integral in Equation 5
is the preferred method for computing the Bayes factor.
Most often, however, the integral must be evaluated numerically. A simple Monte Carlo integration method
 can be used for integrals involving fewerthan 10parameters. More efficient methods such
as Markov chain Monte Carlo (MCMC) methods should
be used for integrals involving large numbers ofparameters. For comprehensive treatments of MCMC methods
and numerical integration methods in general, readers are
directed to Smith , Smith and Roberts , and
Thisted .
The logic of the MCMC method is to obtain a sufficiently large sample of random observations generated
from a probability density function in a prespecified
manner, and then to use the sample to evaluate a desired
integral. More specifically, the integral in Equation 5 is
approximated by the following time average:
In this equation, {e', e2, ... , eT } is a random sample of
T vectors drawn from the prior density function rc(e) of
Model i. Various versions of MCMC, such as the Gibbs
sampler and the
Metropolis-Hastings algorithm , havebeen
proposed. The Gibbs sampler will be described here, primariy because it is easy to apply once the set offully conditional distributions ofthe parameters (the probability distributions ofeach parameter conditional on all remaining
parameters) is identified, at least up to proportionality.
The iteration procedure for generating a random sample ofany size by the Gibbs sampler is summarized in the
following steps. A hypothetical example of the Gibbs
sampler is shown in square brackets, [ ... ], for a threeparameter case with the parameters (e1, e2, ( 3) defined
between 0 and 1:
Initialization:
Determine T;
Pick an arbitrary starting vector eo =
(e~, ..., ()~ )
[eO = (.52, .19, .34)];
Define and set a temporary vector etemp: = eo
[etemp = (.52, .19, .34)];
Letetemp{ - k} denote a set of the current values in
etemp without the kth element
[e.g., for k = 2, e temp{ -2} = (eltemp = 52,
ejemp = .34).]
Let SUM = 0;
Step 1: Take a random sample of e\ from the conditional distribution rc(ell e temp{ -I})
[e\ = .91];
Replace er' of etemp with the new
e\ [etemp = (.91, .19, .34)].
Step 2: Take a random sample of e~ from the conditional distribution rc(e2 1 e temp{ - 2})
[ei = .48];
Replace er' of etemp with the new ei
[etemp = (.91, .48, .34)].
Step n: Take a random sample of e~ from the conditional distribution rc(en Ietemp{ -n})
[e~ = .20];
e~-l of etemp with the new e~
[etemp = (.91, .48, .20)].
Evaluation:
Set et = etemp and evaluate p(Dlet, MJ at et
[e t = (.91, .48, .20)];
SUM = SUM + p(Dlet, Mi ) ;
If t < T then let t = t+ 1 and go to Step 1 or else go to
SUM = SUM /T.
The value ofthe variable SUM represents a numerical
solution to the integral in Equation 5 for the specified
number of iterations (T). The number of iterations nee-
essary to obtain an accurate approximation ofthe integral
is not easy to determine beforehand. Raftery and Lewis
 have provided useful guidelines for estimating T,
which depends on the type of problem, the model equation, and the specific priors used. Probably the most practical method is to examine the converging pattern of the
sum by plotting it against the number of iterations to
identify where the curve becomes stationary. A few runs
usually give a good estimate for a given problem.
APPLICATION EXAMPLE OF BAYESIAN
MODEL SELECTION
In this section, an implementation of the Bayesian
model selection method is illustrated through numerical
examples with simulated data.
Models of Information Integration
A long-standing question that continues to receive considerable attention in cognitive psychology is, How is information from different sources (e.g., sensory and contextual) integrated during perception? The answer to this
question has broad implications for theory development
because information integration is a common denominator among models: Models must specify when and
how independent sources of information are combined
during processing. Entire classes of models will have to
be modified if they are shown to have the wrong functional architecture.
Interest in information integration spawned a variety
ofmathematical models ofintegration . Their number
and relative simplicity provided an appropriate context
in which to test the Bayesian selection method. In a typical experiment on information integration, two or more
stimulus dimensions are factorially manipulated and presented to one or two modalities (e.g., auditory, visual).
Participants identify stimuli along one dimension. Ofinterest is the influence of the orthogonal dimension on
perception. For example, context effects in speech perception are investigated by measuring categorization of
perceptually ambiguous phonemes embedded in words
and nonwords.
For simplicity, only data from a two-factor, tworesponse-category experiment will be considered. Extension ofthe approach to more general cases is straightforward. Suppose that the first factor has nl levels and
the second has n2 levels, so that Si) (i = I, ... , nI;) =
I, ... , n2) denotes the stimulus constructed with Level i
of the first factor and Level) of the second. The stimuli
are presented to participants in a random order and equally
often over s independent trials. Participants categorize
the stimuli as one of two possibilities, A or B. Let a random variable Xi) denote the number of Category A responses participants made when Stimulus Si) was presented. Then, Xi) will follow a binomial probability
distribution with parameters Pi) (probability of Category A response) and s (number of independent observations).
BAYESIAN MODEL SELECTION
Three non-nested models of information integration
were chosen for evaluation on the basis of their similarities in implementation. They were the fuzzy logic model
ofperception (FLMP) by Oden and Massaro , the
linear integration model (LIM) by N. H. Anderson ,
and the theory of signal detection (TSD) by Green and
Swets . All three assume that the probability of
classifying a stimulus as a member of Category A is a
function ofthe extent to which the two feature dimensions
of the stimulus (i and j ) support the category response
 . Specifically, the response
probability, Pi)' is assumed to be a function of two independent parameters, ()i and Aj , each ofwhich represents
the degree of support for a Category A response given
the specific i and) feature dimensions ofa stimulus. The
three models, however, differ in how the two parameters
are combined to produce Pi)'
According to the FLMP, Pi) takes the following nonlinear form :
The LIM assumes a linear combination rule for Pi) as
Pi], LIM = -2- .
For the TSD, the response probability is given by
P'j,TSD = <l>h~1 <I>-I(Vi)2 + Vi) <I>-1(Aj)2 I], (9c)
where <1>( ) and <I>-l( ) are the cumulative and inverse
cumulative normal functions, respectively. The si) is the
sign ( :::': I) of (<I> -1(()i) +<I> - I(Aj )) and vi) is the sign (:::': I)
of (<I> -I(();) . <I>-I(Aj )).
Note that all three models possess the same number of
parameters (two) as well as the same extension ofthe parameter space (i.e., 0 < ()i' Aj < 1), but differ in their
functional form. Model selection using a standard
method, such as AIC, would be decided solely on goodness of fit. Numerical examples in the following section
demonstrate the improvement in model selection that is
achieved with the Bayesian method.
Specification of Priors
To apply the Bayesian method, a probability density
distribution for the two parameters ofthe model must be
specified. We assumed that the distributions were the
same for all three models, and further, that the parameters belonging to the first experimental variable were independent ofthose belonging to the second. In determining the characteristics of the distributions, a sensible
choice is informative priors, because prior information
about the parameters is available from the experimental
setup that the present examples are intended to simulate.
The method in which levels of the independent variable
are created frequently ensures that ordinal information
about the parameters is available, even before data are
collected. For example, in speech perception experi-
MYUNG AND PITT
ments, the levels ofone ofthe variables (e.g., steps along
a fbaf-fdaf phonetic continuum) are varied incrementally so that the probability of a Category A response at
level i is greater than or equal to that at level i' if i < i',
and vice versa. The levels ofthe other variable can be manipulated in a similar fashion. Details on how this ordinal information was incorporated in the prior probabilities are provided in Appendix e.
Simulated Data and Model Fitting
Data simulating three response patterns in a 2 X 8 factorial design (i.e., nl = 2 and n2 = 8) were created. Three
distinct parameter sets, chosen to represent a range ofresponse patterns in categorization experiments, were used
to generate the simulated data.f Each set contained 10parameter values (2+8). For each set, 16 (N = 16)binomial
response probabilities (Pi}) were then computed using one
of the three model equations in Equation 9. Each of the
nine panels in Figure 5 shows a plot of the 16 probabilities for a given model and parameter set. Note that these
probabilities represent ideal, error-free performance.
To simulate actual performance by human participants,
samples were created using each parameter set in Figure 5
according to the binomial probability distribution as follows. For each of the 16 response probabilities (Pi})' a
series of 20 (s = 20) independent binary outcomes (0
or 1) were generated in such a way that the probability
of I was Pi}' Next, the number of Is in the series was
summed and divided by s to obtain an observed proportion for the particular combination of i andj. Finally, the
above procedure was repeated for the remaining 15 values to obtain 16 observed proportions, which together
constituted a single sample. One hundred samples were
created for each of the nine panels in Figure 5.
Each of the three models was fitted to each simulated
sample separately. This was done using the standard
method and the Bayesian method to compare their abilities to recover (i.e., identify) the model that generated
the original data. For the standard method, a nonlinear
optimization routine was used to find the least squares estimates that minimized the sum ofsquared errors between
the simulated and predicted data. Because the number of
parameters was the same across models, this procedure
was equivalent to other methods, such as Ale. For the
Bayesianmethod, the integral in Equation 5 was evaluated
numerically using an extended series of simple Monte
Carlo simulations.?
Set 1: FLMP
Set 1: LIM
Set 1: TSO
Set 3: FLMP
Set 3: LIM
Set 3: TSO
Factor Level
Figure S. Binary response probabilities used to create the simulated data. The three panels in each row represent three
different response patterns generated from the fuzzy logic model of perception (FLMP), the linear integration model
(LI M), and the theory of signal detection (TSD) using Equations 9a-9c with a single set of 10 parameter values. Three
sets of parameter values, corresponding to the three rows in the figure, were used.
BAYESIAN MODEL SELECTION
Summary Fits ofthe Simulated Data for the FLMP, LIM, and TSD Models
in the Standard Method
Data Generated From:
Model fitted:
% variance
% variance
% variance
Note-For each model fitted, the first row shows the arithmetic mean root mean squared deviation
(RMSD) for the model, averaged across 100 samples, the second row shows the arithmetic mean percent
variance accounted for, and the third row shows the percent of samples in which the model fits better than
the other two models. FLMP, fuzzy logic model of perception; LIM, linear integration model; TSD, theory of signal detection.
The results are summarized in Table I for the standard
method. For the simulated data generated from the FLMp,
model recovery rate-the percent of samples in which
the model that generated the samples was correctly identified as the best fitting model-was 50%-60%, essentially uniform across the three parameter sets. When the
original FLMP model was incorrectly identified (30%-
50% of the time), it was always identified as the TSD
model, never as the LIM model. Note that even when the
FLMP was correctly identified, the winning margin was
miniscule, rarely besting the TSD by more than a 10%
difference in RMSD. The mirror image of this outcome
was obtained for the simulated data generated from the
TSD. The TSD was recovered approximately 78% ofthe
time and was incorrectly identified as the FLMP around
22% ofthe time. It was never recovered as the LIM. These
results suggest that the FLMP and TSD are close competitors. Each is flexible enough to mimic the other reasonably well. LIM is a lone loser that consistently failed to
beat the data-fitting abilities of FLMP and TSD.
The above interpretation is further supported by the
model fitting results for the LIM. What is most striking
is the failure ofLIM to beat out the competitors in fitting
its own data. Model recovery rate for the LIM was only
28% across the three parameter sets.l? The FLMP and
TSD bested the LIM in two of the three parameter sets.
Comparison ofthe RMSDs, however, reveals that the fits
for all models are fairly similar. Competitors were never
far behind the best fitting model. This is shown graphically in the three left panels of Figure 6, which are frequency distributions of RMSD ratios fitted to the LIM
data by the LIM and FLMP models. The FLMP usually
bested the LIM by less than a 10% difference in RMSD.
This asymmetry in data fitting between FLMP and LIM
is just what Cutting et al. found.
The model recovery results from application of the
Bayesian method are summarized in Table 2. Unlike
RMSD scores, which can be difficult to interpret because
they are relative values, the marginal likelihoods in Table 2 are meaningfully interpretable. For each model, they
represent the actual probabilities of observing the particular pattern ofdata for the given model. Because ofthe
extremely large number of possible data patterns, these
probabilities should be very small (10- 20 to 10- 16) .
They should not be interpreted as being insignificant.
Recall that the Bayes factor is a ratio of two marginal
likelihoods. Bayes factors greater than 20 should be con-
Summary Fits of the Simulated Data for the FLMP, LIM, and TSD Models in the Bayesian Method
Data Generated From:
Model fitted:
FLMP P(DIM)
7.0X 10- 19
1.0XIO-17 2.4XIO-17
3.4x 10- 15
2.1 X10- 15
7.7X 10- 17
2.6X 10- 20
4.9X 10- 21
1.8x 10- 16 4.3X 10- 28
1.7X 10- 27
l.l X10-17
4.8X 10- 30
7.8X 10- 19
2.2X 10- 19
2.8X 10-17
2.3X 10- 16
1.9XIO-19 1.3XIO-15
2.0X 10- 16 3.6X I0-20
Note-For each model fitted, the first row shows the geometric mean marginal likelihood, P(DIM), averaged across 100 samples, and the second row shows the percent of samples in which the model fits better than the other two models. FLMP, fuzzy logic model of perception; LIM.
linear integration method; TSD, theory of signal detection.
MYUNG AND PITT
sidered strong evidence for the favored model. Values
greater than 150 should be considered very strong evidence .
Overall, the data show a marked improvement in data
recovery over the standard method. For the FLMP and
TSD models, recovery rate between the Bayesian and
standard methods improved in four of the six cells (86
vs. 62, 98 vs. 65, 94 vs. 80,91 vs. 54). There was one tie
(75 vs. 80) and one case in which it did worse (62 vs. 74).
In addition, the winning model won more decisively with
the Bayesian method. For the four cases in which there
was an improvement, the marginal likelihood was 17 to
86 times that of the losing model.
Even more impressive are the recovery results for the
LIM model. The Bayesian method recovered the LIM
model from 89% to 100% of the time. This outcome is
in sharp contrast to the standard method, which produced recovery rates barely one third as good (17%-
36%). When the LIM did win, it was an overwhelming
victor a majority ofthe time. This is shown graphically in
the right three panels of Figure 6, which are frequency
distributions of the log ratios of marginal likelihoods
(i.e., Bayes factors) fitted to the LIM data by the LIM
and FLMP models. As can be seen across all three parameter sets, the degree of support for the LIM is at quite
a comfortable level, the Bayes factors ranging from
about 10 to 1,000.
The results in Tables 1 and 2 together clearly demonstrate the superiority of the Bayesian method in model
selection. By penalizing models for the complexity oftheir
functional form, the Bayesian method improves model
recovery rate significantly over the standard method. The
functional forms ofthe three models are indeed markedly
different from one another. The LIM assumes a linear
form with respect to the parameters, whereas both the
FLMP and TSD assume sophisticated nonlinear forms.
Unless this component of model complexity is incorporated into the selection process, a complex model will be
mistakenly declared as the best fitting model for a given
parameter set more often than a simple model, as shown
in Table 1. Note that this will occur not because the complex model is more plausible, but because the extra flexibility built into the complex model enables it to capture
more of the random, idiosyncratic variation in the data
than the simpler model, thus improving goodness of fit.
However, the Bayesian method failed to recover the
correct model some ofthe time. This occurred primarily
when the FLMP was fitted to TSD data using parameter
30r------------------.,
Bayesian Method
30r---------------------,
30r---------------------,
standard Method
oL..-_....,,-,;::----LI
30r------------------...,
oL-----,~UllJ~ilWlli;IJ!:Jd..d.l)lI,d.....I...1~-~
30r------------------,
oL...._-::-"..-.....LI
RMSDLlM/RMSDFLMP
L0910(Bayes factorLiM/FLMP)
Figure 6. Frequency distributions of root mean squared deviation (RMSD) ratios for the standard method (Ieft three
panels) and ofthe Bayes factor, P(DILIM)/P(DIFLMP), for the Bayesian method (right three panels). The LIM data were
fitted to both the linear integration model (LIM) and the fuzzy logic model of perception (FLMP). Note that the smaller
the RMSD of a model, the better the model, whereas the larger the marginal likelihood ofa model, the better the model.
sets 1 and 3 (top right corner of each 3X3 matrix). On
trials in which a model recovery error was made, the
FLMP beat the TSD by very little, with the Bayes factor
rarely exceeding 5. Note also how localized model recovery errors were across the three data sets. Fewer errors
were found with parameter set 2 or in the opposite corner ofeach matrix, where they might be expected, when
the TSD model was fitted to FLMP data (2%,9% error
Because the errors occurred with specific parameter
sets and the Bayes factor was usually small in these cases,
we attribute the high errors to FLMP providing exceptionally good fits to the regions in data space occupied by
parameter sets 1and 3 (Figure 5). That is, FLMP's goodness offit was so good in this region that it partially offset
the complexity penalty that the model is normally assessed, thereby besting the TSD by a hair. This superior
data-fitting ability does not extend to the data space occupied by parameter set 2, enabling the correct model to
be recovered most of the time. Systematic studies are
planned to test the viability of this speculative account.
DISCUSSION
In this final section, we discuss the pros and cons of
using the Bayesian and standard methods. An advantage
ofthe standard method is that it is easy to use. To choose
among models, one need obtain only two easy-to-compute
indices for each model: a model fit index, such as RMSD
or maximum likelihood, and a complexity index, a count
ofthe number offree parameters. These are combined to
yield an overall measure ofmodel fit using a simple algebraic formula, the exact form of which depends on the
particular criterion used (e.g., AIC, BIC, RMSEA). The
model that minimizes the overall fit is chosen.
The present findings demonstrate that despite the simplicity and convenience of the standard method, critical
dimensions of complexity, such as functional form, are
ignored when it is used. Influences of functional form
can be ignored only when two rather restricting assumptions are met: The sample size is sufficiently large (i.e.,
theoretically infinite) and the models being compared
are nested. Indeed, it is these assumptions from which
many ofthe selection criteria for standard models (e.g.,
AIC and BIC) were derived and their desirable properties
were shown .
Strictly speaking, whenever these assumptions are violated, use of the standard method is not justified.
This does not mean that the standard method should be
abandoned. It is useful for discriminating among nested
models that have the same functional form, thereby canceling out effects due to this dimension of complexity.
This might often be the case in psychometric data analysis in which models being compared tend to be nested
(e.g., regression models, latent variable models). However,
as we have seen, use of the standard method can lead to
erroneous decisions, especially for data ofsmall or moderate sample sizes.
BAYESIAN MODEL SELECTION
In contrast to the standard method, the Bayesian method
does not require the frequently unrealistic assumption of
a large sample size. It is safe to apply the method to comparisons among models with data of any sample size.
Nor is the Bayesian method confined to evaluating nested
models. This is because it appropriately adjusts the model
complexity measure for differences in functional form.
This latter feature makes the method particularly well
suited for comparing mathematical models ofcognition,
ofwhich many are non-nested.
Areas of inquiry in which the Bayesian method could
be fruitfully applied include categorization models, such
as decision-bound models ,prototype models , and context models ; memory models, such as search-ofassociative-memory (SAM) models , multiple trace models , and
holographic memory models ; and causal inference models, such as the
linear model , the weighted
L1P model , and the Bayesian inference model . Like the standard method, the Bayesian method is
not limited to testing models of cognition and could be
used to test quantitative models in any area ofpsychology.
The Bayesian method, however, has its drawbacks. One
is that parameter priors are required to compute the marginallikelihoods. The standard method, on the other hand,
ignores information about priors, even when available.
Although this is a rational strategy in an exploratory phase
of research (e.g., when no or little reliable information
about parameter priors can be obtained), it may not be
justifiable when building mathematical models, which
are usually developed at advanced stages of a research
program. In this case, it makes sense to capitalize on the
available information in the data or in the experimental
setup to compute priors. Wehave suggested several ways
ofdetermining priors by utilizing such information (Appendix B).
Another drawback ofthe Bayesian method is its heavy
demand for computational resources. Unlike the simple
formulas prescribed in the standard method, the Bayesian
method usually requires numerical computations ofintegral form to evaluate the posterior probability distribution.
This task has been simplified greatly by recent advances
in the area ofBayesian computation . Nevertheless, it may still
be viewed as an obstacle by those who are not familiar
with numerical computation methods. I I
Despite the apparent differences between the Bayesian
and standard methods, there are cases in which both will
perform similarly for large sample sizes. For example,
the BIC can be viewed as a crude approximation of the
Bayes factor that ignores priors (see Appendix D).
Given the advantages and disadvantages ofthe Bayesian method, it might be best to apply the method to comparisons among well-developed mathematical models.
MYUNG AND PITT
Without sufficiently detailed models, the construction of
reliable priors may not be possible, or if accomplished,
might increase the chances ofunwarranted decisions. In
addition, if a myriad of mathematical models have to be
evaluated, a nontrivial amount ofcomputing time will be
needed. We recommend that the simple, easy-to-use
standard method be applied first to eliminate grossly
unfit models in order to narrow the field to a few highly
competitive ones. Then apply the Bayesian method to further differentiate among the survivors. This technique balances computational costs with model selection costs.
There is no reason for the Bayesian method to be the single, universal model selection tool.
In conclusion, we have introduced a Bayesian model
selection method that embodies the principle ofOccam's
razor. We have tried to present a balanced view of the
method, pointing out its desirable features (i.e., sensitivity to functional form and applicability to comparison
among non-nested models and data of any sample size)
aswell as some current drawbacks (i.e., computational cost
and specification of parameter priors). When applied
with discretion, it can be a useful tool for distinguishing
sensible models from superfluous ones, thus contributing to theoretical advancement in the discipline.
One final note. Quantitative methods must not be used
as the final decision makers when comparing models.
Bayesian or non-Bayesian, quantitative model selection
methods cannot replace other selection criteria (e.g., explanatory power,plausibility, internal consistency). Mathematical modeling will best serve the discipline when
used in conjunction with these criteria. In this regard, we
concur with the word of caution offered by Browne and
Cudeck , "Fit indices should not be regarded as a measure of usefulness of a model. ... Consequently, they should not be used in a mechanical decision process for selecting a model. Model selection has
to be a subjective process involving the use ofjudgment."