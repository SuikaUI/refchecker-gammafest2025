Learning Question Classiﬁers
Department of Computer Science
University of Illinois at Urbana-Champaign
fxli1,danr 
In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining
some of the constraints the question imposes on a possible answer. These constraints may include a semantic
classiﬁcation of the sought after answer and may even
suggest using different strategies when looking for and
verifying a candidate answer.
This paper presents a machine learning approach to
question classiﬁcation. We learn a hierarchical classi-
ﬁer that is guided by a layered semantic hierarchy of answer types, and eventually classiﬁes questions into ﬁnegrained classes. We show accurate results on a large collection of free-form questions used in TREC 10.
Introduction
Open-domain question answering and story
comprehension have become important directions in natural language processing. Question answering is a retrieval task more
challenging than common search engine tasks because its purpose is to ﬁnd an accurate and concise
answer to a question rather than a relevant document. The difﬁculty is more acute in tasks such as
story comprehension in which the target text is less
likely to overlap with the text in the questions. For
this reason, advanced natural language techniques
rather than simple key term extraction are needed.
One of the important stages in this process is analyzing the question to a degree that allows determining
the “type” of the sought after answer. In the TREC
competition , participants are requested to build a system which, given a set of English questions, can automatically extract answers
(a short phrase) of no more than 50 bytes from a
5-gigabyte document library. Participants have re-
 Research supported by NSF grants IIS-9801638 and ITR IIS-
0085836 and an ONR MURI Award.
alized that locating an answer accurately hinges on
ﬁrst ﬁltering out a wide range of candidates based on some
categorization of answer types.
This work develops a machine learning approach
to question classiﬁcation (QC) . Our goal is to categorize
questions into different semantic classes that impose
constraints on potential answers, so that they can
be utilized in later stages of the question answering
process. For example, when considering the question Q: What Canadian city has the largest population?, the hope is to classify this question as having
answer type city, implying that only candidate answers that are cities need consideration.
Based on the SNoW learning architecture, we develop a hierarchical classiﬁer that is guided by a layered semantic hierarchy of answer types and is able
to classify questions into ﬁne-grained classes. We
suggest that it is useful to consider this classiﬁcation task as a multi-label classiﬁcation and ﬁnd that
it is possible to achieve good classiﬁcation results
(over 90%) despite the fact that the number of different labels used is fairly large,
50. We observe that
local features are not sufﬁcient to support this accuracy, and that inducing semantic features is crucial
for good performance.
The paper is organized as follows: Sec. 2 presents
the question classiﬁcation problem; Sec. 3 discusses
the learning issues involved in QC and presents our
learning approach; Sec. 4 describes our experimental study.
Question Classiﬁcation
We deﬁne Question Classiﬁcation(QC) here to be
the task that, given a question, maps it to one of
k classes, which provide a semantic constraint on
the sought-after answer1. The intension is that this
1We do not address questions like “Do you have a light?”,
which calls for an action, but rather only factual Wh-questions.
classiﬁcation, potentially with other constraints on
the answer, will be used by a downstream process
which selects a correct answer from among several
candidates.
A question classiﬁcation module in a question answering system has two main requirements. First, it
provides constraints on the answer types that allow
further processing to precisely locate and verify the
answer. Second, it provides information that downstream processes may use in determining answer selection strategies that may be answer type speciﬁc,
rather than uniform. For example, given the question “Who was the ﬁrst woman killed in the Vietnam
War?” we do not want to test every noun phrase
in a document to see whether it provides an answer.
At the very least, we would like to know that the
target of this question is a person, thereby reducing
the space of possible answers signiﬁcantly. The following examples, taken from the TREC 10 question
collection, exhibit several aspects of this point.
Q: What is a prism? Identifying that the target of this
question is a deﬁnition, strategies that are speciﬁc for
deﬁnitions (e.g., using predeﬁned templates) may be useful. Similarly, in:
Q: Why is the sun yellow? Identifying that this question
asks for a reason, may lead to using a speciﬁc strategy
for reasons.
The above examples indicate that, given that different answer types may be searched using different
strategies, a good classiﬁcation module may help
the question answering task. Moreover, determining the speciﬁc semantic type of the answer could
also be beneﬁcial in locating the answer and verifying it. For example, in the next two questions,
knowing that the targets are a city or country will
be more useful than just knowing that they are locations.
Q: What Canadian city has the largest population?
Q: Which country gave New York the Statue of Liberty?
However, conﬁned by the huge amount of manual work needed for constructing a classiﬁer for a
complicated taxonomy of questions, most question
answering systems can only perform a coarse classiﬁcation for no more than 20 classes. As a result,
existing approaches, as in ,
have adopted a small set of simple answer entity
types, which consisted of the classes: Person, Location, Organization, Date, Quantity, Duration, Linear Measure. The rules used in the classiﬁcation
were of the following forms:
– If a query starts with Who or Whom: type Person.
– If a query starts with Where: type Location.
– If a query contains Which or What, the head noun
phrase determines the class, as for What X questions.
While the rules used have large coverage and reasonable accuracy, they are not sufﬁcient to support
ﬁne-grained classiﬁcation.
One difﬁculty in supporting ﬁne-grained classiﬁcation is the need to extract from the questions ﬁner features that require
syntactic and semantic analysis of questions, and
possibly, many of them. The approach we adopted
is a multi-level learning approach: some of our features rely on ﬁner analysis of the questions that are
outcomes of learned classiﬁers; the QC module then
applies learning with these as input features.
Classiﬁcation Standard
Earlier works have suggested various standards of
classifying questions. Wendy Lehnert’s conceptual
taxonomy , for example, proposes
about 13 conceptual classes including causal antecedent, goal orientation, enablement, causal consequent, veriﬁcation, disjunctive, and so on. However, in the context of factual questions that are
of interest to us here, conceptual categories do not
seem to be helpful; instead, our goal is to semantically classify questions, as in earlier work on
TREC .
The key difference, though, is that we attempt to
do that with a signiﬁcantly ﬁner taxonomy of answer types; the hope is that with the semantic answer types as input, one can easily locate answer
candidates, given a reasonably accurate named entity recognizer for documents.
Question Hierarchy
We deﬁne a two-layered taxonomy, which represents a natural semantic classiﬁcation for typical
answers in the TREC task.
The hierarchy contains 6 coarse classes (ABBREVIATION, ENTITY,
DESCRIPTION, HUMAN, LOCATION and NU-
MERIC VALUE) and 50 ﬁne classes, Table 1 shows
the distribution of these classes in the 500 questions of TREC 10.
Each coarse class contains a
non-overlapping set of ﬁne classes.
The motivation behind adding a level of coarse classes is that of
compatibility with previous work’s deﬁnitions, and
comprehensibility. We also hoped that a hierarchical classiﬁer would have a performance advantage
over a multi-class classiﬁer; this point, however is
not fully supported by our experiments.
description
individual
description
instrument
DESCRIPTION
Table 1: The distribution of 500 TREC 10 questions
over the question hierarchy. Coarse classes (in bold) are
followed by their ﬁne class reﬁnements.
The Ambiguity Problem
One difﬁculty in the question classiﬁcation task is
that there is no completely clear boundary between
classes. Therefore, the classiﬁcation of a speciﬁc
question can be quite ambiguous. Consider
1. What is bipolar disorder?
2. What do bats eat?
3. What is the PH scale?
Question 1 could belong to deﬁnition or disease medicine; Question 2 could belong to food,
plant or animal; And Question 3 could be a numeric value or a deﬁnition. It is hard to categorize those questions into one single class and it is
likely that mistakes will be introduced in the downstream process if we do so. To avoid this problem,
we allow our classiﬁers to assign multiple class labels for a single question. This method is better than
only allowing one label because we can apply all the
classes in the later precessing steps without any loss.
Learning a Question Classiﬁer
Using machine learning methods for question classiﬁcation is advantageous over manual methods for
several reasons. The construction of a manual classiﬁer for questions is a tedious task that requires
the analysis of a large number of questions. Moreover, mapping questions into ﬁne classes requires
the use of lexical items (speciﬁc words) and therefore an explicit representation of the mapping may
be very large. On the other hand, in our learning
approach one can deﬁne only a small number of
“types” of features, which are then expanded in a
data-driven way to a potentially large number of features , relying on the ability of the learning process to handle it. It is hard to
imagine writing explicitly a classiﬁer that depends
on thousands or more features. Finally, a learned
classiﬁer is more ﬂexible to reconstruct than a manual one because it can be trained on a new taxonomy
in a very short time.
One way to exhibit the difﬁculty in manually constructing a classiﬁer is to consider reformulations of
a question:
What tourist attractions are there in Reims?
What are the names of the tourist attractions in Reims?
What do most tourists visit in Reims?
What attracts tourists to Reims?
What is worth seeing in Reims?
All these reformulations target the same answer
type Location. However, different words and syntactic structures make it difﬁcult for a manual classiﬁer based on a small set of rules to generalize well
and map all these to the same answer type. Good
learning methods with appropriate features, on the
other hand, may not suffer from the fact that the
number of potential features (derived from words
and syntactic structures) is so large and would generalize and classify these cases correctly.
A Hierarchical Classiﬁer
Question classiﬁcation is a multi-class classiﬁcation. A question can be mapped to one of 50 possible classes ). Our learned classiﬁer is based
on the SNoW learning architecture 2 where, in order to allow the
classiﬁer to output more than one class label, we
map the classiﬁer’s output activation into a conditional probability of the class labels and threshold
The question classiﬁer makes use of a sequence
of two simple classiﬁers , each utilizing the Winnow algorithm within
SNoW. The ﬁrst classiﬁes questions into coarse
classes (Coarse Classiﬁer) and the second into ﬁne
classes (Fine Classiﬁer). A feature extractor automatically extracts the same features for each classiﬁer. The second classiﬁer depends on the ﬁrst in
2Freely available at 
ABBR, ENTITY,DESC,HUMAN,LOC,NUM
Coarse Classifier
Fine Classifier
ind, plant
abb, animal,
food, plant…
food,plant,
ind,group…
food, plant,
city, state…
definition,
Map coarse classes
to fine classes
animal,food
all possible subsets
of C0 wih size <= 5
all possible subsets
of C2 with size <=5
Figure 1: The hierarchical classiﬁer
that its candidate labels are generated by expanding
the set of retained coarse classes from the ﬁrst into
a set of ﬁne classes; this set is then treated as the
confusion set for the second classiﬁer.
Figure 1 shows the basic structure of the hierarchical classiﬁer. During either the training or the
testing stage, a question is processed along one path
top-down to get classiﬁed.
The initial confusion set of any question is
g, the set of all the coarse classes.
The coarse classiﬁer determines a set of preferred
Then each coarse class label
1 is expanded to a ﬁxed set of ﬁne classes
determined by the class hierarchy.
That is, suppose the coarse class
i is mapped into the set
g of ﬁne classes, then
i. The ﬁne classiﬁer determines a set of
preferred labels,
3 are the ultimate outputs from the whole classiﬁer which are
used in our evaluation.
Feature Space
Each question is analyzed and represented as a list
of features to be treated as a training or test example for learning. We use several types of features
and investigate below their contribution to the QC
The primitive feature types extracted for each
question include words, pos tags, chunks (nonoverlapping phrases) , named entities,
head chunks (e.g., the ﬁrst noun chunk in a sentence) and semantically related words (words that
often occur with a speciﬁc question class).
Over these primitive features (which we call
“sensors”) we use a set of operators to compose
more complex features, such as conjunctive (ngrams) and relational features, as in . A simple script
that describes the “types” of features used, (e.g.,
conjunction of two consecutive words and their pos
tags) is written and the features themselves are extracted in a data driven way. Only “active” features
are listed in our representation so that despite the
large number of potential features, the size of each
example is small.
Among the 6 primitive feature types, pos tags,
chunks and head chunks are syntactic features while
named entities and semantically related words are
semantic features.
Pos tags are extracted using
a SNoW-based pos tagger .
The named entity classiﬁer is
also learned and makes use of the same technology developed for the chunker .
The ‘related word’ sensors were constructed semiautomatically.
Most question classes have a semantically related
word list. Features will be extracted for this class if
a word in a question belongs to the list. For example, when “away”, which belongs to a list of words
semantically related to the class distance, occurs in
the sentence, the sensor Rel(distance) will be active. We note that the features from these sensors are
different from those achieved using named entity
since they support more general “semantic categorization” and include nouns, verbs, adjectives rather
than just named entities.
For the sake of the experimental comparison, we
deﬁne six feature sets, each of which is an incremental combination of the primitive feature types.
That is, Feature set 1 (denoted by Word) contains
word features; Feature set 2 (Pos) contains features
composed of words and pos tags and so on; The ﬁnal feature set, Feature set 6 (RelWord) contains all
the feature types and is the only one that contains
the related words lists. The classiﬁers will be experimented with different feature sets to test the inﬂuence of different features. Overall, there are about
000 features in the feature space of RelWord
due to the generation of complex features over simple feature types. For each question, up to a couple
of hundreds of them are active.
Decision Model
For both the coarse and ﬁne classiﬁers, the same
decision model is used to choose class labels for
a question. Given a confusion set and a question,
SNoW outputs a density over the classes derived
from the activation of each class. After ranking the
classes in the decreasing order of density values, we
have the possible class labels
with their densities
n). As discussed earlier, for each question we output the ﬁrst
k classes (1
T is a threshold value in . If we treat
the probability that a question belongs to Class i,
the decision model yields a reasonable probabilistic
interpretation. We use
0:95 in the experiments.
Experimental Study
We designed two experiments to test the accuracy of
our classiﬁer on TREC questions. The ﬁrst experiment evaluates the contribution of different feature
types to the quality of the classiﬁcation. Our hierarchical classiﬁer is trained and tested using one
of the six feature sets deﬁned in Sect. 3.2 (we repeated the experiments on several different training
and test sets). In the second experiment, we evaluate the advantage we get from the hierarchical classiﬁer. We construct a multi-class classiﬁer only for
ﬁne classes. This ﬂat classiﬁer takes all ﬁne classes
as its initial confusion set and classiﬁes a question
into ﬁne classes directly. Its parameters and decision model are the same as those of the hierarchical
one. By comparing this ﬂat classiﬁer with our hierarchical classiﬁer in classifying ﬁne classes, we
hope to know whether the hierarchical classiﬁer has
any advantage in performance, in addition to the advantages it might have in downstream processing
and comprehensibility.
Data are collected from four sources: 4,500 English
questions published by USC ,
about 500 manually constructed questions for a few
rare classes, 894 TREC 8 and TREC 9 questions,
and also 500 questions from TREC 10 which serves
as our test set3.
These questions were manually labeled according to our question hierarchy. Although we allow
multiple labels for one question in our classiﬁers,
in our labeling, for simplicity, we assigned exactly
3The annotated data and experimental results are available
from 
one label to each question. Our annotators were requested to choose the most suitable class according to their own understanding. This methodology
might cause slight problems in training, when the
labels are ambiguous, since some questions are not
treated as positive examples for possible classes as
they should be.
In training, we divide the 5,500
questions from the ﬁrst three sources randomly into
5 training sets of 1,000, 2,000, 3,000, 4,000 and
5,500 questions. All 500 TREC 10 questions are
used as the test set.
Evaluation
In this paper, we count the number of correctly classiﬁed questions by two different precision standards
5. Suppose
i labels are output for the
ith question (k
5) and are ranked in a decreasing
order according to their density values. We deﬁne
=m where m is the total number of
test examples.
1 corresponds to the usual deﬁnition of precision which allows only one label for
each question, while
5 allows multiple labels.
5 reﬂects the accuracy of our classiﬁer with respect to later stages in a question answering system. As the results below show, although question
classes are still ambiguous, few mistakes are introduced by our classiﬁer in this step.
Experimental Results
Performance of the hierarchical classiﬁer
Table 2 shows the
5 precision of the hierarchical classiﬁer when trained on 5,500 examples and
tested on the 500 TREC 10 questions.
The results are quite encouraging; question classiﬁcation
is shown to be solved effectively using machine
learning techniques. It also shows the contribution
of the feature sets we deﬁned. Overall, we get a
98.80% precision for coarse classes with all the features and 95% for the ﬁne classes.
Table 2: Classiﬁcation results of the hierarchical classiﬁer on 500 TREC 10 questions. Training is done on
5,500 questions.
Columns show the performance for
difference feature sets and rows show the precision for
coarse and ﬁne classes, resp. All the results are evaluated using
Inspecting the data carefully, we can observe the
signiﬁcant contribution of the features constructed
based on semantically related words sensors. It is
interesting to observe that this improvement is even
more signiﬁcant for ﬁne classes.
Table 3: Classiﬁcation accuracy for coarse classes on
different training sets using the feature set RelWord. Results are evaluated using
Table 4: Classiﬁcation accuracy for ﬁne classes on different training sets using the feature set RelWord. Results are evaluated using
Tables 3 and 4 show the
5 accuracy
of the hierarchical classiﬁer on training sets of different sizes and exhibit the learning curve for this
We note that the average numbers of labels output by the coarse and ﬁne classiﬁers are 1.54 and
2.05 resp., (using the feature set RelWord and 5,500
training examples), which shows the decision model
is accurate as well as efﬁcient.
Comparison of the hierarchical and the ﬂat
The ﬂat classiﬁer consists of one classiﬁer which is
almost the same as the ﬁne classiﬁer in the hierarchical case, except that its initial confusion set is
the whole set of ﬁne classes. Our original hope was
that the hierarchical classiﬁer would have a better
performance, given that its ﬁne classiﬁer only needs
to deal with a smaller confusion set. However, it
turns out that there is a tradeoff between this factor
and the inaccuracy, albeit small, of the coarse level
prediction. As the results show, there is no performance advantage for using a level of coarse classes,
and the semantically appealing coarse classes do not
contribute to better performance.
Figure 2 give some more intuition on the ﬂat vs.
hierarchical issue. We deﬁne the tendency of Class
i to be confused with Class
j as follows:
where (when using
ij is the number of
questions in Class i that are misclassiﬁed as belong-
Table 5: Comparing accuracy of the hierarchical (h) and
ﬂat (f) classiﬁers on 500 TREC 10 question; training is
done on 5,500 questions. Results are shown for different
feature sets using
Fine Classes 1−50
Fine Classes 1−50
Figure 2: The gray–scale map of the matrix D[n,n]. The
color of the small box in position (i,j) denotes
ij is, the darker the color is. The dotted lines
separate the 6 coarse classes.
ing to Class j, and
j are the numbers of questions in Class i and j resp.
Figure 2 is a gray-scale map of the matrix D[n,n].
D[n,n] is so sparse that most parts of the graph are
We can see that there is no good clustering of ﬁne classes mistakes within a coarse class,
which explains intuitively why the hierarchical classiﬁer with an additional level coarse classes does not
work much better.
Discussion and Examples
We have shown that the overall accuracy of our classiﬁer is satisfactory. Indeed, all the reformulation
questions that we exempliﬁed in Sec. 3 have been
correctly classiﬁed. Nevertheless, it is constructive
to consider some cases in which the classiﬁer fails.
Below are some examples misclassiﬁed by the hierarchical classiﬁer.
What French ruler was defeated at the battle of Waterloo?
The correct label is individual, but the classiﬁer,
failing to relate the word “ruler” to a person, since
it was not in any semantic list, outputs event.
What is the speed hummingbirds ﬂy ?
The correct label is speed, but the classiﬁer outputs
animal. Our feature sensors fail to determine that
the focus of the question is ‘speed’. This example
illustrates the necessity of identifying the question
focus by analyzing syntactic structures.
What do you call a professional map drawer ?
The classiﬁer returns other entities instead of
equivalent term. In this case, both classes are acceptable. The ambiguity causes the classiﬁer not to
output equivalent term as the ﬁrst choice.
Conclusion
This paper presents a machine learning approach to
question classiﬁcation. We developed a hierarchical
classiﬁer that is guided by a layered semantic hierarchy of answers types, and used it to classify questions into ﬁne-grained classes. Our experimental results prove that the question classiﬁcation problem
can be solved quite accurately using a learning approach, and exhibit the beneﬁts of features based on
semantic analysis.
In future work we plan to investigate further the
application of deeper semantic analysis (including
better named entity and semantic categorization) to
feature extraction, automate the generation of the
semantic features and develop a better understanding to some of the learning issues involved in the
difference between a ﬂat and a hierarchical classi-