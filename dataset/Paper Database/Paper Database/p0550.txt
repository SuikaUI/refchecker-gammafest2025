Stochastic Approximation Approach to Stochastic
Programming
Anatoli Juditsky∗
Guanghui Lan†
Arkadi Nemirovski‡
Alexander Shapiro §
Abstract. In this paper we consider optimization problems where the objective function is
given in a form of the expectation.
A basic diﬃculty of solving such stochastic optimization
problems is that the involved multidimensional integrals (expectations) cannot be computed with
high accuracy.
The aim of this paper is to compare two computational approaches based on
Monte Carlo sampling techniques, namely, the Stochastic Approximation (SA) and the Sample
Average Approximation (SAA) methods.
Both approaches, the SA and SAA methods, have a
long history. Current opinion is that the SAA method can eﬃciently use a speciﬁc (say linear)
structure of the considered problem, while the SA approach is a crude subgradient method which
often performs poorly in practice. We intend to demonstrate that a properly modiﬁed SA approach
can be competitive and even signiﬁcantly outperform the SAA method for a certain class of convex
stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point
problems, and present (in our opinion highly encouraging) results of numerical experiments.
Key words: stochastic approximation, sample average approximation method, stochastic programming, Monte Carlo sampling, complexity, saddle point, minimax problems, mirror descent
∗LJK, Universit´e J. Fourier, B.P. 53, 38041 Grenoble Cedex 9, France, 
†Georgia Institute of Technology, Atlanta, Georgia 30332, USA, ,
research of this author was partially supported by NSF award CCF-0430644 and ONR award
N00014-05-1-0183.
‡Georgia Institute of Technology, Atlanta, Georgia 30332, USA, ,
research of this author was partly supported by the NSF award DMI-0619977.
§Georgia Institute of Technology, Atlanta, Georgia 30332, USA, ,
research of this author was partly supported by the NSF awards DMS-0510324 and DMI-0619977.
Introduction
In this paper we consider the following optimization problem
f(x) := E[F(x, ξ)]
Here X ⊂Rn is a nonempty bounded closed convex set, ξ is a random vector whose probability
distribution P is supported on set Ξ ⊂Rd and F : X × Ξ →R. We assume that for every ξ ∈Ξ
the function F(·, ξ) is convex on X, and that the expectation
E[F(x, ξ)] =
Ξ F(x, ξ)dP(ξ)
is well deﬁned and ﬁnite valued for every x ∈X. It follows that function f(·) is convex and ﬁnite
valued on X. Moreover, we assume that f(·) is continuous on X. Of course, continuity of f(·)
follows from convexity if f(·) is ﬁnite valued and convex on a neighborhood of X. With these
assumptions, (1.1) becomes a convex programming problem.
A basic diﬃculty of solving stochastic optimization problem (1.1) is that the multidimensional
integral (expectation) (1.2) cannot be computed with a high accuracy for dimension d, say, greater
The aim of this paper is to compare two computational approaches based on Monte
Carlo sampling techniques, namely, the Stochastic Approximation (SA) and the Sample Average
Approximation (SAA) methods. To this end we make the following assumptions.
(A1) It is possible to generate an iid sample ξ1, ξ2, ..., of realizations of random vector ξ.
(A2) There is a mechanism (an oracle) which for every given x ∈X and ξ ∈Ξ returns value F(x, ξ)
and a stochastic subgradient – a vector G(x, ξ) such that g(x) := E[G(x, ξ)] is well deﬁned
and is a subgradient of f(·) at x, i.e., g(x) ∈∂f(x).
Recall that if F(·, ξ), ξ ∈Ξ, is convex and f(·) is ﬁnite valued in a neighborhood of a point x,
then (cf., Strassen )
∂f(x) = E [∂xF(x, ξ)] .
In that case we can employ a measurable selection G(x, ξ) ∈∂xF(x, ξ) as a stochastic subgradient.
At this stage, however, this is not important, we shall see later other relevant ways for constructing
stochastic subgradients.
Both approaches, the SA and SAA methods, have a long history. The SA method is going
back to the pioneering paper by Robbins and Monro . Since then stochastic approximation
algorithms became widely used in stochastic optimization and, due to especially low demand for
computer memory, in signal processing (cf., and references therein). In the classical analysis of
the SA algorithm (it apparently goes back to the works and ) it is assumed that f is twice
continuously diﬀerentiable and strongly convex, and in the case when the minimizer of f belongs
to the interior of X, exhibits asymptotically optimal rate of convergence E[f(xt) −f∗] = O(1/t)
(here xt is t-th iterate and f∗is the minimal value of f(x) over x ∈X). This algorithm, however,
is very sensitive to a choice of the respective stepsizes. The diﬃcult to implement “asymptotically
optimal” stepsize policy can be very bad in the beginning, so that the algorithm often performs
poorly in practice.
An important improvement of the SA method was developed by B. Polyak , where longer
stepsizes were suggested with consequent averaging of the obtained iterates. Under the outlined
“classical” assumptions, the resulting algorithm exhibits the same optimal O(1/t) asymptotical
convergence rate, while using an easy to implement and “robust” stepsize policy. It should be
mentioned that the main ingredients of Polyak’s scheme – long steps and averaging – were, in a
diﬀerent form, proposed already in for the case of problems (1.1) with general type Lipschitz
continuous convex objectives and for convex-concave saddle point problems. The algorithms from
 exhibit, in a non-asymptotical fashion, the unimprovable in the general convex case O(1/
of convergence. For a summary of early results in this direction, see .
The SAA approach was used by many authors in various contexts under diﬀerent names. Its
basic idea is rather simple: generate a (random) sample ξ1, ..., ξN, of size N, and approximate the
“true” problem (1.1) by the sample average problem
ˆfN(x) := N−1 PN
j=1 F(x, ξj)
Note that the SAA method is not an algorithm, the obtained SAA problem (1.4) still has to be
solved by an appropriate numerical procedure.
Recent theoretical studies (cf., ) and
numerical experiments (see, e.g., ) show that the SAA method coupled with a good (deterministic) algorithm could be reasonably eﬃcient for solving certain classes of two stage stochastic
programming problems. On the other hand classical SA type numerical procedures typically performed poorly for such problems. We intend to demonstrate in this paper that a properly modiﬁed
SA approach can be competitive and even signiﬁcantly outperform the SAA method for a certain
class of stochastic problems.
The rest of this paper is organized as follows. In Section 2 we focus on the theory of SA as
applied to problem (1.1). We start with outlining the (relevant to our goals part of the) classical
“O(1/t) SA theory” (Section 2.1) along with its “O(1/
t)” modiﬁcations (Section 2.2). Well known
and simple results presented in these sections pave road to our main developments carried out in
Section 2.3. In Section 3 we extend the constructions and results of Section 2.3 to the case of
convex-concave stochastic saddle point problem. In concluding Section 4 we present results (in our
opinion, highly encouraging) of numerical experiments with the SA algorithm (Sections 2.3 and
3) applied to large-scale stochastic convex minimization and saddle point problems. Finally, some
technical proofs are given in the Appendix.
Throughout the paper, we use the following notation. By ∥x∥p we denote the ℓp norm of vector
x ∈Rn, in particular, ∥x∥2 =
xT x denotes the Euclidean norm. By ΠX we denote the metric
projection operator onto the set X, that is ΠX(x) = arg minx′∈X ∥x −x′∥2. Note that ΠX is a
contraction operator, i.e.,
∥ΠX(x′) −ΠX(x)∥2 ≤∥x′ −x∥2, ∀x′, x ∈Rn.
By O(1) we denote a generic constant independent of the data. The notation ⌊a⌋stands for the
largest integer less than or equal to a ∈R. Unless stated otherwise all relations between random
variables are supposed to hold almost surely.
Stochastic Approximation, Basic Theory
In this section we discuss theory and implementations of the stochastic approximation (SA) approach to the minimization problem (1.1).
Classical SA Algorithm
The classical SA algorithm solves problem (1.1) by mimicking the simplest subgradient descent
method. That is, for chosen x1 ∈X and a sequence γj > 0, j = 1, ..., of stepsizes, it generates the
iterates by the formula
xj+1 := ΠX
xj −γjG(xj, ξj)
Of course, the crucial question of that approach is how to choose the stepsizes γj. Let ¯x be an
optimal solution of problem (1.1). Note that since the set X is compact and f(x) is continuous,
problem (1.1) has an optimal solution. Note also that the iterate xj = xj(ξ[j−1]) is a function of
the history ξ[j−1] := (ξ1, ..., ξj−1) of the generated random process and hence is random.
Denote Aj := 1
2∥xj −¯x∥2
2 and aj := E[Aj] = 1
. By using (1.5) and since ¯x ∈X
and hence ΠX(¯x) = ¯x, we can write
xj −γjG(xj, ξj)
xj −γjG(xj, ξj)
°°xj −γjG(xj, ξj) −¯x
j ∥G(xj, ξj)∥2
2 −γj(xj −¯x)T G(xj, ξj).
We also have
(xj −¯x)T G(xj, ξj)
(xj −¯x)T G(xj, ξj)
(xj −¯x)T E
(xj −¯x)T g(xj)
Therefore, by taking expectation of both sides of (2.2) we obtain
aj+1 ≤aj −γjE
(xj −¯x)T g(xj)
∥G(x, ξ)∥2
We assume that the above constant M is ﬁnite.
Suppose, further, that the expectation function f(x) is diﬀerentiable and strongly convex on
X, i.e., there is constant c > 0 such that
f(x′) ≥f(x) + (x′ −x)T ∇f(x) + 1
2c∥x′ −x∥2
2, ∀x′, x ∈X,
or equivalently that
(x′ −x)T (∇f(x′) −∇f(x)) ≥c∥x′ −x∥2
2, ∀x′, x ∈X.
Note that strong convexity of f(x) implies that the minimizer ¯x is unique. By optimality of ¯x we
(x −¯x)T ∇f(¯x) ≥0, ∀x ∈X,
which together with (2.6) implies that
(xj −¯x)T ∇f(xj)
(xj −¯x)T (∇f(xj) −∇f(¯x))
Therefore it follows from (2.4) that
aj+1 ≤(1 −2cγj)aj + 1
Let us take stepsizes γj = θ/j for some constant θ > 1/(2c). Then by (2.7) we have
aj+1 ≤(1 −2cθ/j)aj + 1
and by induction
2θ2M2(2cθ −1)−1, a1
Suppose, further, that ¯x is an interior point of X and ∇f(x) is Lipschitz continuous, i.e., there is
constant L > 0 such that
∥∇f(x′) −∇f(x)∥2 ≤L∥x′ −x∥2, ∀x′, x ∈X.
f(x) ≤f(¯x) + 1
2L∥x −¯x∥2
f(xj) −f(¯x)
≤Laj ≤Lκ/j.
Under the speciﬁed assumptions, it follows from (2.10) and (2.11), respectively, that after t
iterations the expected error of the current solution is of order O(t−1/2) and the expected error of
the corresponding objective value is of order O(t−1), provided that θ > 1/(2c). We have arrived
at the O(t−1)-rate of convergence mentioned in the Introduction. Note, however, that the result is
highly sensitive to our a priori information on c. What would happen if the parameter c of strong
convexity is overestimated? As a simple example consider f(x) = x2/10, X = [−1, 1] ⊂R and
assume that there is no noise, i.e., F(x, ξ) ≡f(x). Suppose, further, that we take θ = 1 (i.e.,
γj = 1/j), which will be the optimal choice for c = 1, while actually here c = 0.2. Then the
iteration process becomes
xj+1 = xj −f′(xj)/j =
and hence starting with x1 = 1,
−0.25 + 0.2 ln 1.25 −1
> 0.8j−1/5.
That is, the convergence is extremely slow. For example for j = 109 the error of the iterated
solution is greater than 0.015. On the other hand for the optimal stepsize factor of γ = 1/c = 5,
the optimal solution ¯x = 0 is found in one iteration.
Robust SA Approach
The results of this section go back to and .
Let us look again at the basic estimate (2.4). By convexity of f(x) we have that for any x,
f(x) ≥f(xj) + (x −xj)T g(xj), and hence
(xj −¯x)T g(xj)
f(xj) −f(¯x)
Together with (2.4) this implies
f(xj) −f(¯x)
≤aj −aj+1 + 1
It follows that
f(xt) −f(¯x)
[at −at+1] + 1
νtf(xt) −f(¯x)
where νt :=
i=1 γi (note that Pj
t=1 νt = 1). Consider points
By convexity of f(x) we have f(˜xj) ≤Pj
t=1 νtf(xt), and since ˜xj ∈X, by optimality of ¯x we have
that f(˜xj) ≥f(¯x). Thus, by (2.13),
0 ≤E [f(˜xj) −f(¯x)] ≤a1 + 1
Let us suppose for the moment that the number of iterations of the method is ﬁxed in advance, say
equal to N. In this case one can use a constant stepsize strategy, i.e., choose γt ≡γ for t = 1, ..., N.
For this choice of γt we obtain immediately from (2.15) that the obtained approximate solution
E [f(˜xN) −f(¯x)] ≤a1
Let us denote DX := maxx∈X ∥x −x1∥2. Then a1 ≤D2
X/2, and taking
we achieve
E [f(˜xN) −f(¯x)] ≤DXM
Discussion.
We conclude that the expected error of Robust SA algorithm (2.1),(2.16), with constant stepsize strategy (2.18), after N iterations is O(N−1/2) in our setting. Of course, this is worse
than the rate O(N−1) for the classical SA algorithm when the objective function f(x) is strongly
convex. However, the error bound (2.19) is guaranteed wether the function f(x) is strongly convex
on X or not. Note also that it follows from (2.17) that the rate O(N−1/2) is guaranteed for the
constant stepsize of the form γ := θ/
N for any choice of the constant θ > 0. This explains the
adjective Robust in the name of the algorithm.
In applications it can be convenient to construct an approximate solution ˜xN which is the
average of only part of the trajectory x1, ..., xN. For instance, let for some integer ℓ∈{1, ..., N},
t=N−⌊N/ℓ⌋+1
If we sum in (2.12) between N −⌊N/ℓ⌋and N (instead of summing from 1 to N) we easily get
E [f(˜xN) −f(¯x)] ≤DXM(ℓ+ 1)
where DX := maxx′,x∈X ∥x′ −x∥2.
Of course, the constant stepsize strategy is not the only possible one. For instance, let γj =
θj−1/2, j = 1, 2, ..., with θ > 0 and let ˜xj be deﬁned as follows:
for some integer 2 ≤ℓ≤j and j ≥2. Then
E [f(˜xj) −f(¯x)] ≤O(1)ℓ[D
Note that (D
X + M2θ2)/θ attains its minimum at θ = DX/M. For that choice of θ the estimate
(2.22) becomes
E [f(˜xj) −f(¯x)] ≤O(1)ℓDXM
Mirror Descent SA Method
In this section we develop a substantial generalization of the robust SA approach (a very rudimentary form of this generalization can be found in , from where, in particular, the name “Mirror
Descent” originates). Let ∥· ∥be a (general) norm on Rn and ∥x∥∗= sup∥y∥≤1 yT x be its dual
norm. We say that a function ω : X →R is a distance generating function modulus α > 0 with
respect to ∥· ∥, if ω is convex and continuous on X, the set
x ∈X : there exists p ∈Rn such that x ∈arg minu∈X[pT u + ω(u)]
is convex (note that Xo always contains the relative interior of X), and restricted to Xo, ω is
continuously diﬀerentiable and strongly convex with parameter α with respect to ∥· ∥, i.e.,
(x′ −x)T (∇ω(x′) −∇ω(x)) ≥α∥x′ −x∥2, ∀x′, x ∈Xo.
An example of distance generating function (modulus 1 with respect to ∥· ∥2 ) is ω(x) := 1
For that choice of ω(·) we have that Xo = X,
ΠX(x −y) = arg min
+ yT (z −x)
= ω(z) −[ω(x) + ∇ω(x)T (z −x)].
Let us deﬁne function V : Xo × X →R+ as follows
V (x, z) := ω(z) −[ω(x) + ∇ω(x)T (z −x)].
In what follows we shall refer to V (·, ·) as prox-function associated with distance generating function
ω(x). Note that V (x, ·) is nonnegative and is strongly convex modulus α with respect to the norm
∥· ∥. Let us deﬁne prox mapping Px : Rn →Xo, associated with ω and a point x ∈Xo, viewed as
a parameter, as follows:
Px(y) := arg min
yT (z −x) + V (x, z)
For ω(x) = 1
2 we have that Px(y) = ΠX(x −y). Let us observe that the minimum in the right
hand side of (2.26) is attained since ω is continuous on X and X is compact, and all the minimizers
belong to Xo, whence the minimizer is unique, since V (x, ·) is strongly convex on Xo, and hence
the prox-mapping is well deﬁned. Using the deﬁnition of the prox-mapping for ω(x) = 1
iterative formula (2.1) can be written as
xj+1 = Pxj(γjG(xj, ξj)),
We discuss now the recursion (2.27) for general distance generating function ω(x).
mentioned above, if ω(x) = 1
2, then formula (2.27) coincides with (2.1). In that case we refer
to the procedure as Euclidean SA algorithm.
The following statement is a simple consequence of the optimality conditions of the right hand
side of (2.26).
Lemma 2.1 For any u ∈X, x ∈Xo and y the following inequality holds
V (Px(y), u) ≤V (x, u) + yT (u −x) + ∥y∥2
Proof of this lemma is given in the Appendix.
Using (2.28) with x = xj, y = γjG(xj, ξj) and u = ¯x, we get
γj(xj −¯x)T G(xj, ξj) ≤V (xj, ¯x) −V (xj+1, ¯x) +
2α∥G(xj, ξj)∥2
If we compare inequality (2.29) with (2.2) we see that values of the prox-function V (xj, ¯x) along
the iterations of the Mirror Descent SA satisfy exactly the same relations as values Aj = 1
2∥xj −¯x∥2
along the trajectory of the Euclidean SA. The proposed construction of the prox-function V (·, ·)
allows us to act in the general case exactly in the same way as we have done in the Euclidean
situation of the previous section. Setting
∆j := G(xj, ξj) −g(xj),
we can rewrite (2.29), with j replaced by t, as
γt(xt −¯x)T g(xt) ≤V (xt, ¯x) −V (xt+1, ¯x) −γt∆T
t (xt −¯x) + γ2
2α∥G(xt, ξt)∥2
Summing up over t = 1, ..., j, and taking into account that V (xj+1, u) ≥0, u ∈X, we get
γt(xt −¯x)T g(xt) ≤V (x1, ¯x) +
2α∥G(xt, ξt)∥2
t (xt −¯x).
Now let νt :=
i=1 γi , t = 1, ..., j, and
By convexity of f(·) we have that
t=1 γt(xt −¯x)T g(xt)
t=1 γt [f(xt) −f(¯x)] =
t=1 νtf(xt) −f(¯x)
[f(˜xj) −f(¯x)] .
Together with (2.32) this implies that
f(˜xj) −f(¯x) ≤V (x1, ¯x) + Pj
2α∥G(xt, ξt)∥2
t (xt −¯x)
Let us suppose, as in the previous section (cf., (2.5)), that there is a constant M2
∗> 0 such that
∥G(x, ξ)∥2
Taking expectations of both sides of (2.34) and noting that: (i) xt is a deterministic function of
ξ[t−1] = (ξ1, ..., ξt−1), (ii) conditional on ξ[t−1] the expectation of ∆t is 0, and (iii) the expectation
of ∥G(xt, ξt)∥2
∗does not exceed M2
∗, we obtain
E [f(˜xj) −f(¯x)] ≤h1 + (2α)−1M2
where h1 := maxu∈X V (x1, u).
To design the stepsize strategy let us start with the situation when the number j of iterations
of the method is ﬁxed in advance, say equals to N. Then the constant stepsize strategy γt ≡γ,
t = 1, ..., N, can be implemented. Let us suppose from now on that the initial point x1 is exactly
the minimizer of ω(x) on X. Then V (x1, z) ≤D2
ω,X, where
Dω,X := [maxz∈X ω(z)−minz∈X ω(z)]1/2 ,
and thus h1 ≤D2
ω,X. Then the approximate solution ˜xN satisﬁes:
E [f(˜xN) −f(¯x)] ≤
E [f(˜xN) −f(¯x)] ≤Dω,XM∗
We refer to the method (2.27), (2.33) and
(2.40) as Robust Mirror Descent SA algorithm with
constant stepsize policy.
By Markov inequality it follows from (2.41) that for any ε > 0,
Prob {f(˜xN) −f(¯x) > ε} ≤
It is possible, however, to obtain much ﬁner bounds for those probabilities when imposing more
restrictive assumptions on the distribution of G(x, ξ). Let us assume that
∥G(x, ξ)∥2
≤exp{1}, ∀x ∈X.
Note that condition (2.43) is stronger than condition (2.35). Indeed, if a random variable Y satisﬁes
E[exp{Y/a}] ≤exp{1} for some a > 0, then by Jensen inequality exp{E[Y/a]} ≤E[exp{Y/a}] ≤
exp{1}, and therefore E[Y ] ≤a.
Of course, condition (2.43) holds if ∥G(x, ξ)∥∗≤M∗for all
(x, ξ) ∈X × Ξ.
Proposition 2.1 Suppose that condition (2.43) holds. Then for the constant stepsizes (2.40) the
following inequality holds for any Ω≥1:
f(˜xN) −f(¯x) > M∗Dω,X
αN (12 + 2Ω)
≤2 exp{−Ω}.
Proof of this proposition is given in the Appendix.
Discussion.
The conﬁdence bound (2.44) can be written as
Prob {f(˜xN) −f(¯x) > ε} ≤O(1) exp
where κ :=
8M∗Dω,X and ε > 0. (Condition Ω≥1 means here that N ≥49κ−2ε−2.) It follows
that for chosen accuracy ε and conﬁdence level δ ∈(0, 1), the sample size
ω,X ln2(δ−1)
guarantees that ˜xN is an ε-optimal solution of the true problem with probability at least 1 −δ.
This can be compared with a similar estimate for an optimal solution of the SAA problem (1.4)
(cf., ). In both cases the estimated sample size N, considered as a function of the accuracy ε,
is of order O(ε−2) and depends logarithmically on the conﬁdence level δ.
We can modify Robust Mirror Descent SA algorithm so that the approximate solution ˜xN is
obtained by averaging over a part of trajectory, namely, let for some integer ℓ, 1 ≤ℓ≤N,
t=N−⌊N/ℓ⌋+1
In this case we have for the constant stepsize strategy with γ := Dω,X
E [f(˜xN) −f(¯x)] ≤Dω,XM∗(ℓ+ 1)
where the quantity
2 supx∈Xo,z∈X V (x, z)
is assumed to be ﬁnite (which deﬁnitely is the case when ω is continuously diﬀerentiable on the
entire X). Note that in the case of Euclidean SA, when ω(x) = 1
2, Dω,X coincides with the
Euclidean diameter DX of X.
A decreasing stepsize strategy with
t = 1, 2, ...,
can be also used in the Robust Mirror Descent algorithm. One can easily verify that when θ :=
Dω,X/M∗, the approximate solution ˜xj,
satisﬁes for j ≥2 and 2 ≤ℓ≤j:
[f(˜xj) −f(¯x)] ≤O(1)ℓDω,XM∗
We see that for both methods, (Euclidean) Robust SA and Robust Mirror Descent SA, the expected
value of the error of the last iterate after t steps is of order O(t−1/2). A potential beneﬁt of the
Mirror Descent over the Euclidean algorithm is that the norm ∥· ∥and the distance generating
function ω(·) can be adjusted to the geometry of the set X.
Example 2.1 Let X := {x ∈Rn : Pn
i=1 xi = 1, x ≥0} be a standard simplex. Suppose that
the initial solution is the barycenter x1 = n−1(1, 1, ..., 1) of the simplex. In that case it is not
diﬃcult to ﬁnd the exact Euclidean projection ΠX(x). The estimate (2.19) suggests an error of
order DXMN−1/2 of obtained solution for a sample of size N, with the constant M deﬁned in
(2.5). Here the (Euclidean) characteristics DX of the set X, DX = maxx∈X ∥x −x1∥2 ≤
Now let us consider the ℓ1 norm ∥x∥1 = Pn
i=1 |xi|. Its dual norm is the ℓ∞norm ∥x∥∞=
max{|x1|, ..., |xn|}. For ω(x) = 1
2 the corresponding estimate (2.41) suggests an error of order
α−1/2Dω,XM∗N−1/2, where the constants α and M∗are computed with respect to the norms ∥· ∥1
and ∥· ∥∞, respectively. We have that for any x ∈Rn,
∥x∥∞≤∥x∥2 ≤√n∥x∥∞and ∥x∥2 ≤∥x∥1 ≤√n∥x∥2,
and these inequalities are sharp. This indicates that the constant M∗might be up to √n-times
smaller than the constant M. However, the constant α, taken with respect to the ℓ1 norm is also
√n-times smaller than the corresponding constant of ω taken with respect to the ℓ2 norm. In other
words, we do not gain anything in terms of the estimate
(2.41) as compared with the estimate
(2.23). This, of course, should be not surprising since the algorithm depends on a chosen norm
only through the choice (2.40) of stepsizes.
Consider now the entropy distance generating function
Here Xo = {x ∈X : x > 0}, Dω,X =
ln n, x1 := argmin Xω = n−1(1, ..., 1)T is the barycenter of
X, and α = 1 (see the Appendix). The corresponding prox-function V (x, z) is
V (x, z) =
Note that we can easily compute the prox mapping Px(y) of (2.26) in this case:
[Px(y)]i =
k=1 xke−yk , i = 1, ..., n.
We can compare the Mirror Descent SA algorithm associated with the above choice of “entropy like”
distance generating function coupled with the ℓ1 norm and its dual ℓ∞norm, with the Euclidean
SA algorithm. The error estimate (2.41) suggests that we lose a factor ln n in the ratio
compared with D2
α . On the other hand, we have a potential gain of factor of order √n in M∗
(which is computed with respect to the norm ℓ∞) as compared with M (computed with respect to
the Euclidean norm).
Stochastic saddle point problem
We show in this section how the Mirror Descent SA algorithm can be modiﬁed to solve a convexconcave stochastic saddle point problem. Consider the following minimax (saddle point) problem
φ(x, y) := E[Φ(x, y, ξ)]
Here X ⊂Rn and Y ⊂Rm are nonempty bounded closed convex sets, ξ is a random vector whose
probability distribution P is supported on set Ξ ⊂Rd and Φ : X × Y × Ξ →R. We assume that
for every ξ ∈Ξ, function Φ(x, y, ξ) is convex in x ∈X and concave in y ∈Y , and for all x ∈X,
y ∈Y the expectation
E[Φ(x, y, ξ)] =
Φ(x, y, ξ)dP(ξ)
is well deﬁned and ﬁnite valued. It follows that φ(x, y) is convex in x ∈X and concave in y ∈Y ,
ﬁnite valued, and hence
(3.1) is a convex-concave saddle point problem.
In addition, we assume that φ(·, ·) is Lipschitz continuous on X × Y . It is well known that in the above setting
the problem
(3.1) is solvable, i.e., the corresponding “primal” and “dual” optimization problems minx∈X [maxy∈Y φ(x, y)] and maxy∈Y [minx∈X φ(x, y)], respectively, have optimal solutions
and equal optimal values, denoted φ∗, and the pairs (x∗, y∗) of optimal solutions to the respective
problems form the set of saddle points of φ(x, y) on X × Y .
As in the case of the minimization problem (1.1) we assume that neither the function φ(x, y)
nor its sub/supergradients in x and y are available explicitly. However, we make the following
assumption.
(A2′) We have at our disposal an oracle which for every given x ∈X, y ∈Y and ξ ∈Ξ returns value Φ(x, y, ξ) and a stochastic subgradient, that is, (n + m)-dimensional vector
G(x, y, ξ) =
Gx(x, y, ξ)
−Gy(x, y, ξ)
such that vector g(x, y) =
E[Gx(x, y, ξ)]
−E[Gy(x, y, ξ)]
is well deﬁned, and gx(x, y) ∈∂xφ(x, y) and −gy(x, y) ∈∂y(−φ(x, y)). For example, under
mild assumptions we can set
G(x, y, ξ) =
Gx(x, y, ξ)
−Gy(x, y, ξ)
∂xΦ(x, y, ξ)
∂y(−Φ(x, y, ξ))
Let ∥· ∥x be a norm on Rn and ∥· ∥y be a norm on Rm, and let ∥· ∥∗,x and ∥· ∥∗,y stand for the
corresponding dual norms. As in Section 2.1, the basic assumption we make about the stochastic
oracle (aside of its unbiasedness which we have already postulated) is that there exist positive
constants M2
∗,x and M2
∗,y such that
∥Gx(u, v, ξ)∥2
∥Gy(u, v, ξ)∥2
∗,y, ∀(u, v) ∈X × Y.
Mirror SA algorithm for saddle point problems
We equip X and Y with distance generating functions ωx : X →R modulus αx with respect to
∥· ∥x, and ωy : Y →R modulus αy with respect to ∥· ∥y. Let Dωx,X and Dωy,Y be the respective
constants (see deﬁnition (2.37)). We equip Rn × Rm with the norm
∥(x, y)∥:=
so that the dual norm is
∥(ζ, η)∥∗=
It follows by (3.2) that
∥G(x, y, ξ)∥2
We use notation z = (x, y) and equip Z := X × Y with the distance generating function as follows:
ω(z) := ωx(x)
It is immediately seen that ω indeed is a distance generating function for Z modulus α = 1 with
respect to the norm ∥·∥, and that Zo = Xo×Y o and Dω,Z = 1. In what follows, V (z, u) : Zo×Z →R
and Pz(ζ) : Rn+m →Zo are the prox-function and prox-mapping associated with ω and Z, see
(2.25), (2.26).
We are ready now to present the Mirror SA algorithm for saddle point problems. This is the
iterative procedure
zj+1 := Pzj(G(zj, ξj)),
where the initial point z1 ∈Z is chosen to be the minimizer of ω(z) on Z. As before (cf., (2.39)),
we deﬁne the approximate solution ˜zj of (3.1) after j iterations as
˜zj = (˜xj, ˜yj) :=
We refer to the procedure (3.6), (3.7) as Saddle Point Mirror SA algorithm.
Let us analyze the convergence properties of the algorithm. We measure quality of an approximate solution ˜z = (˜x, ˜y) by the error
y∈Y φ(˜x, y) −φ∗
x∈X φ(x, ˜y)
y∈Y φ(˜x, y) −min
x∈X φ(x, ˜y).
By convexity of φ(·, y) we have
φ(xt, yt) −φ(x, yt) ≤gx(xt, yt)T (xt −x), ∀x ∈X,
and by concavity of φ(x, ·),
φ(xt, y) −φ(xt, yt) ≤gy(xt, yt)T (y −yt), ∀y ∈Y,
so that for all z = (x, y) ∈Z,
φ(xt, y) −φ(x, yt) ≤gx(xt, yt)T (xt −x) + gy(xt, yt)T (y −yt) = g(zt)T (zt −z).
Using once again the convexity-concavity of φ we write
y∈Y φ(˜xj, y) −min
x∈X φ(x, ˜yj)
γtφ(xt, y) −min
γtφ(x, yt)
γtg(zt)T (zt −z).
To bound the right-hand side of (3.8) we use the following result.
Lemma 3.1 In the above setting, for any j ≥1 the following inequality holds
γtg(zt)T (zt −z)
Proof of this lemma is given in the Appendix.
Now to get an error bound for the solution ˜zj it suﬃces to substitute inequality (3.9) into (3.8)
E[ϵφ(˜zj)]
Let us use the constant stepsize strategy
, t = 1, ..., N.
Then ϵφ(˜zN) ≤2M∗
N , and hence (see deﬁnition (3.5) of M∗) we obtain
ϵφ(˜zN) ≤2
ωx,XM2∗,x + αxD2
ωy,Y M2∗,y
Same as in the minimization case, we can pass from constant stepsizes on a ﬁxed “time horizon”
to decreasing stepsize policy (2.47) with θ = 1/M∗and from averaging of all iterates to the “sliding
averaging”
arriving at the eﬃciency estimate
ϵ(˜zj) ≤O(1)ℓDω,ZM∗
where the quantity Dω,Z =
2 supz∈Zo,w∈Z V (z, w)
¤1/2 is assumed to be ﬁnite.
We give below a bound on the probabilities of large deviations of the error ϵφ(˜zN).
Proposition 3.1 Suppose that conditions of the bound (3.11) are veriﬁed and, further, it holds for
all (u, v) ∈Z that
∥Gx(u, v, ξ)∥2
∥Gy(x, y, ξ)∥2
Then for the stepsizes (3.10) one has for any Ω≥1 that
ϵφ(˜zN) > (8 + 2Ω)
≤2 exp{−Ω}.
Proof of this proposition is given in the Appendix.
Application to minimax stochastic problems
Consider the following minimax stochastic problem
fi(x) := E[Fi(x, ξ)]
where X ⊂Rn is a nonempty bounded closed convex set, ξ is a random vector whose probability
distribution P is supported on set Ξ ⊂Rd and Fi : X × Ξ →R, i = 1, ..., m. We assume that
for a.e. ξ the functions Fi(·, ξ) are convex and for every x ∈Rn, Fi(x, ·) are integrable, i.e., the
expectations
E[Fi(x, ξ)] =
Fi(x, ξ)dP(ξ), i = 1, ..., m,
are well deﬁned and ﬁnite valued. To ﬁnd a solution to the minimax problem (3.15) is exactly the
same as to solve the saddle point problem
φ(x, y) :=
with Y := {y ∈Rm : y ≥0, Pm
i=1 yi = 1}.
Similarly to assumptions (A1) and (A2), assume that we cannot compute fi(x) (and thus φ(x, y))
explicitly, but are able to generate independent realizations ξ1, ξ2, ... distributed according to P,
and for given x ∈X and ξ ∈Ξ we can compute Fi(x, ξ) and its stochastic subgradient Gi(x, ξ), i.e.,
such that gi(x) = E[Gi(x, ξ)] is well deﬁned and gi(x) ∈∂fi(x), x ∈X, i = 1, ..., m. In other words
we have a stochastic oracle for the problem (3.17) such that assumption (A2′) holds, with
G(x, y, ξ) :=
i=1 yiGi(x, ξ)
−F1(x, ξ), ..., −Fm(x, ξ)
g(x, y) := E[G(x, y, ξ)] =
i=1 yigi(x)
(−f1(x), ..., −fm(x))
−∂yφ(x, y)
Suppose that the set X is equipped with norm ∥· ∥x, whose dual norm is ∥· ∥∗,x, and a distance
generating function ω modulus αx with respect to ∥· ∥x, and let R2
. We equip the set Y
with norm ∥· ∥y := ∥· ∥1, so that ∥· ∥∗,y = ∥· ∥∞, and with the distance generating function
and set R2
= ln m. Next, following (3.3) we set
∥(x, y)∥:=
∥(ζ, η)∥∗=
2R2x∥ζ∥2∗,x + 2R2y∥η∥2∞.
Let us assume uniform bounds:
1≤i≤m ∥Gi(x, ξ)∥2
1≤i≤m |Fi(x, ξ)|2
i = 1, ..., m.
∥G(x, y, ξ)∥2
yiGi(x, ξ)
∥F(x, ξ)∥2
∗,y ln m =: M2
Let us now use the Saddle Point Mirror SA algorithm
(3.6), (3.7) with the constant stepsize
t = 1, 2, ..., N.
When substituting the value of M∗, we obtain from (3.11):
E [ϵφ(˜zN)] = E
y∈Y φ(ˆxN, y) −min
x∈X φ(x, ˆyN)
R2xM2∗,x + M2∗,x ln m
Discussion.
Looking at the bound
(3.22) one can make the following important observation.
The error of the Saddle Point Mirror SA algorithm in this case is “almost independent” of the
number m of constraints (it grows as O(
ln m) as m increases). The interested reader can easily
verify that if an Euclidean SA algorithm were used in the same setting (i.e., the algorithm tuned
to the norm ∥· ∥y := ∥· ∥2), the corresponding bound would grow with m much faster (in fact, our
error bound would be O(√m) in that case).
Note that properties of the Saddle Point Mirror SA can be used to reduce signiﬁcantly the
arithmetic cost of the algorithm implementation. To this end let us look at the deﬁnition (3.18)
of the stochastic oracle: in order to obtain a realization G(x, y, ξ) one has to compute m random
subgradients Gi(x, ξ), i = 1, ..., m, and then the convex combination Pm
i=1 yiGi(x, ξ).
Now let η be
an independent of ξ and uniformly distributed in random variable, and let ı(η, y) : ×Y →
{1, ..., m} equals to i when Pi−1
s=1 ys < η ≤Pi
s=1 ys. That is, random variable ˆı = ı(η, y) takes values
1, ..., m with probabilities y1, ..., ym. Consider random vector
G(x, y, (ξ, η)) :=
Gı(η,y)(x, ξ)
(−F1(x, ξ), ..., −Fm(x, ξ))
We refer to G(x, y, (ξ, η)) as a randomized oracle for problem
(3.17), the corresponding random
parameter being (ξ, η). By construction we still have E
G(x, y, (ξ, η))
= g(x, y), where g is deﬁned
(3.19), and, moreover, the same bound
(3.20) holds for E
∥G(x, y, (ξ, η))∥2
. We conclude
that the accuracy bound (3.22) holds for the error of the Saddle Point Mirror SA algorithm with
randomized oracle. On the other hand, in the latter procedure only one randomized subgradient
Gˆı(x, ξ) per iteration is to be computed. This simple idea is further developed in another interesting
application of the Saddle Point Mirror SA algorithm to bilinear matrix games which we discuss
Application to bilinear matrix games
Consider the standard matrix game problem, that is, problem (3.1) with
φ(x, y) := yT Ax + bT x + cT y,
where A ∈Rm×n, and X and Y are the standard simplices, i.e.,
x ∈Rn : x ≥0, Pn
j=1 xj = 1
y ∈Rm : y ≥0, Pm
i=1 yi = 1
In the case in question it is natural to equip X (respectively, Y ) with the usual ∥· ∥1-norm on Rn
(respectively, Rm). We choose entropies as the corresponding distance generating functions:
As we already have seen, this choice results in
= ln n and
= ln m. According to
(3.3) we set
∥(x, y)∥:=
2 ln n + ∥y∥2
∥(ζ, η)∥∗=
2∥ζ∥2∞ln n + 2∥η∥2∞ln m.
In order to compute the estimates Φ(x, y, ξ) of φ(x, y) and G(x, y, ξ) of g(x, y) = (b+AT y, −c−Ax)
to be used in the Saddle Point Mirror SA iterations (3.6), we use the randomized oracle
Φ(x, y, ξ)
cT x + bT y + Aı(ξ1,y)ı(ξ2,x),
G(x, y, ξ)
c + Aı(ξ1,y)
−b −Aı(ξ2,x)
where ξ1 and ξ2 are independent uniformly distributed on random variables, ˆj = ı(ξ1, y) and
ˆi = ı(ξ2, x) are deﬁned as in (3.23), i.e., ˆj can take values 1, ..., m with probabilities y1, ..., ym and ˆi
can take values 1, ..., n with probabilities x1, ..., xn, and Aj, [Ai]T are j-th column and i-th row in
A, respectively.
Note that g(x, y) := E
G(x, y, (ˆj,ˆi))
∂y(−φ(x, y))
. Besides this,
|G(x, y, ξ)i| ≤max
1≤j≤m ∥Aj + b∥∞, for i = 1, ..., n,
|G(x, y, ξ)i| ≤max
1≤j≤n ∥Aj + c∥∞, for i = n + 1, ..., n + m.
Hence, by the deﬁnition (3.24) of ∥· ∥∗,
E∥G(x, y, ξ)∥2
∗:= 2 ln n max
1≤j≤m ∥Aj + b∥2
∞+ 2 ln m max
1≤j≤n ∥Aj + c∥2
The bottom line is that inputs of the randomized Mirror Saddle Point SA satisfy the conditions of
validity of the bound (3.11) with M∗as above. Using the constant stepsize strategy with
t = 1, ..., N,
we obtain from (3.11):
y∈Y φ(˜xN, y) −min
x∈X φ(x, ˜yN)
We continue with the counterpart of Proposition 3.1 for the Saddle Point Mirror SA in the setting
of bilinear matrix games.
Proposition 3.2 For any Ω≥1 it holds that
ϵφ(˜zN) > 2M∗
1≤j≤m ∥Aj + b∥∞+ max
1≤j≤n ∥Aj + c∥∞.
Discussion.
Consider a bilinear matrix game with m = n and b = c = 0. Suppose that we are
interested to solve it within a ﬁxed relative accuracy ρ, that is, to ensure that a (perhaps random)
approximate solution ˜zN, we get after N iterations, satisﬁes the error bound
ϵφ(˜zN) ≤ρ max
1≤i, j≤n |Aij|
with probability at least 1−δ. According to (3.26), to this end one can use the randomized Saddle
Point Mirror SA algorithm (3.6), (3.7) with
N = O(1)ln n + ln(δ−1)
The computational cost of building ˜zN with this approach is
C(ρ) = O(1)
ln n + ln(δ−1)
arithmetic operations, where R is the arithmetic cost of extracting a column/row from A, given
the index of this column/row. The total number of rows and columns visited by the algorithm
does not exceed the sample size N, given in (3.28), so that the total number of entries in A used
in course of the entire computation does not exceed
M = O(1)n(ln n + ln(δ−1))
When ρ is ﬁxed and n is large, this is incomparably less that the total number n2 of entries of
A. Thus, the algorithm in question produces reliable solutions of prescribed quality to large-scale
matrix games by inspecting a negligible, as n →∞, part of randomly selected data. Note that
randomization here is critical. It is easily seen that a deterministic algorithm which is capable to
ﬁnd a solution with (deterministic) relative accuracy ρ ≤0.1, has to “see” in the worst case at least
O(1)n rows/columns of A.
Numerical results
In this section, we report the results of our computational experiments where we compare the
performance of the Robust Mirror Descent SA method and the SAA method applied to three
stochastic programming problems, namely: a stochastic utility problem, a stochastic max-ﬂow
problem and network planning problem with random demand. We also present a small simulation
study of performance of randomized Mirror SA algorithm for bilinear matrix games.
A stochastic utility problem
Our ﬁrst experiment was carried out with the utility model
i=1(ai + ξi)xi
Here X = {x ∈Rn : Pn
i=1 xi = 1, x ≥0}, ξi ∼N(0, 1) are independent random variables having
standard normal distribution, ai = i/n are constants, and φ(·) is a piecewise linear convex function
given by φ(t) = max{v1 + s1t, ..., vm + smt}, where vk and sk are certain constants.
Two variants of the Robust Mirror Descent SA method have been used for solving problem (4.1).
The ﬁrst variant, the Euclidean SA (E-SA), employs the Euclidean distance generating function
2, and its iterates coincide with those of the classic SA as discussed in Section 2.3.
The other distance generating function used in the following experiments is the entropy function
deﬁned in (2.49). The resulting variant of the Robust Mirror Descent SA is referred to as the
Non-Euclidean SA (N-SA) method.
These two variants of SA method are compared with the SAA approach in the following way:
ﬁxing an i.i.d. sample (of size N) for the random variable ξ, we apply the three afore-mentioned
methods to obtain approximate solutions for problem (4.1), and then the quality of the solutions
Table 1: the selection of step-sizes
[method: N-SA, N:2,000, K:10,000, instance: L1]
yielded by these algorithms is evaluated using another i.i.d. sample of size K >> N. It should be
noted that SAA itself is not an algorithm and in our experiments it is coupled with the so-called
Non-Euclidean Restricted Memory Level (NERML) deterministic algorithm (see ), for solving
the sample average problem (1.4).
In our experiment, the function φ(·) in problem (4.1) was set to be a piecewise linear function
with 10 breakpoints (m = 10) over the interval and four instances (namely: L1, L2, L3 and
L4) with diﬀerent dimensions ranging from n = 500 to 5, 000 were randomly generated. Note that
each of these instances assumes a diﬀerent function φ(·), i.e., has diﬀerent values of vk and sk for
k = 1, ..., m. All the algorithms were coded in ANSI C and the experiments were conducted on a
Intel PIV 1.6GHz machine with Microsoft Windows XP professional.
The ﬁrst step of our experimentation is to determine the step-sizes γt used by both variants
of SA method. Note that in our situation, either a constant stepsize policy (2.40) or a variable
step-size policy (2.47) can be applied. Observe however that the quantity M∗appearing in both
stepsize policies is usually unknown and requires an estimation. In our implementation, an estimate
of M∗is obtained by taking the maxima of ∥G(·, ·)∥∗over a certain number (for example, 100) of
random feasible solutions x and realizations of the random variable ξ. To account for the error
inherited by this estimation procedure, the stepsizes are set to ηγt for t = 1, ..., N, where γt are
deﬁned as in (2.40) or (2.47), and η > 0 is a user-deﬁned parameter that can be ﬁne-tuned, for
example, by a trial-and-error procedure.
Some results of our experiments for determining the step-sizes are presented in Table 1. Speciﬁcally, Table 1 compares the solution quality obtained by the Non-Euclidean SA method (N = 2,000
and K = 10,000) applied for solving the instance L1 (n = 500) with diﬀerent stepsize polices and
diﬀerent values of η. In this table, column 1 gives the name of the two policies and column 2 through
column 5 report the objective values for η = 0.1, 1, 5 and 10 respectively. The results given in Table
1 show that the constant step-size policy with a properly chosen parameter η slightly outperforms
the variable stepsize policy and the same phenomenon has also been observed for the Euclidean
SA method. Based on these observations, the constant step-size was chosen for both variants of
SA method. To set the parameter η, we run each variant of SA method in which diﬀerent values
of η ∈{0.1, 1, 5, 10} are applied, and the best selection of η in terms of the solution quality was
chosen. More speciﬁcally, the parameter η was set to 0.1 and 5, respectively, for the Euclidean SA
method and Non-Euclidean throughout our computation.
We then run each of the three afore-mentioned methods with various sample-sizes for each test
instance and the computational results are reported in Table 2, where n is the dimension of problem,
N denotes the sample-size, ‘OBJ’ and ‘DEV’ represents mean and deviation, respectively, of the
objective values of problem (4.1) as evaluated over a sample of size K = 10, 000 for the solutions
generated by the algorithms, ‘TIME’ is the CPU seconds for obtaining the solutions, and ‘ORC’
stands for the number of calls to the stochastic oracle.
Table 2: SA vs. SAA on the stochastic utility model
L1: n = 500
L2: n = 1000
L3: n = 2000
L4: n = 5000
In order to evaluate variability of these algorithms, we run each method 100 times and compute
the resulting statistics as shown in Table 3. Note that the instance L2 is chosen as a representative
and only two diﬀerent sample-sizes are applied since this test is more timeconsuming. In Table 3, column 1 and column 2 give the instance name and the sample-size used
for each run of the method. The objective value of the approximate solution yielded by each run
of the algorithm was evaluated over K = 104 sample size, and the mean and standard deviation
of these objective values over 100 runs are given in columns 3-4, columns 6-7, and columns 9-10,
respectively, for N-SA, E-SA and SAA method. The average solution time of these three methods
over 100 runs are also reported in column 5, 8, and 11 respectively.
The experiment demonstrates that the solution quality is improved for all three methods with
the increase of the sample size N. Moreover, for a given sample size, the solution time for N-SA is
signiﬁcantly smaller than that for SAA, while the solution quality for N-SA is close to that for the
latter one.
Table 3: The variability for the stochastic utility problem
Stochastic max-ﬂow problem
In the second experiment, we consider a simple two-stage stochastic linear programming, namely, a
stochastic max-ﬂow problem. The problem is to investigate the capacity expansion over a stochastic
network. Let G = (N, A) be a diagraph with a source node s and a sink node t. Each arc (i, j) ∈A
has an existing capacity pij ≥0, and a random implementing/operating level ξij. Moreover, there
is a common random degrading factor denoted by θ for all arcs in A. The goal is to determine how
much capacity to add to the arcs subject to a budget constraint, such that the expected maximum
ﬂow from s to t is maximized. Let xij denote the capacity to be added to arc (i, j). The problem
can be formulated as
f(x) := E[F(x, ξ)]
cijxij ≤b, xij ≥0,
∀(i, j) ∈A,
where cij is unit cost for the capacity to be added, b is the total available budget, and F(x, ξ)
denotes the maximum s −t ﬂow in the network when the capacity of an arc (i, j) is given by
θξij(pij + xij). Note that the above is a maximization rather than minimization problem.
For our purpose, we assume that the random variables ξij and θ are independent and uniformly
distributed over (0, 1) and (0.5, 1), respectively. Also let pij = 0 and cij = 1 for all (i, j) ∈E,
and b = 1. We randomly generated 4 network instances (referred to as F1, F2, F3 and F4) using
the network generator GRIDGEN, which is available on DIMACS challenge.
The push-relabel
algorithm (see ) was used to solve the second stage max-ﬂow problem.
The three methods, namely: N-SA, E-SA and SAA, and the same stepsize policy as discussed in
Subsection 4.1, were applied for solving these stochastic max-ﬂow instances. In the ﬁrst test, each
algorithm was run once for each test instance and the computational results are reported in Table
4, where m and n denote the number of nodes and arcs in G, respectively, N denotes the number
of samples, ‘OBJ’ and ‘DEV’ represent the mean and standard deviation, respectively, of objective
values of problem (4.2) as evaluated over K = 104 sample size at the approximated solutions yielded
by the algorithms, ‘TIME’ is CPU seconds for obtaining the approximated solution, and ‘ORC’
stands for the number of calls to the stochastic oracle. Similar to the stochastic utility problem,
we investigate the variability of these three methods by running each method for 100 times and
computing the statistical results as shown in Table 5 whose columns have exactly the same meaning
as in Table 3.
This experiment, once more, shows that for a given sample size N, the solution quality for N-SA
is close to or even in some cases is better than that for SAA, meanwhile, the solution time of N-SA
is much smaller than the latter one.
Table 4: SA vs. SAA on the stochastic max-ﬂow model
F1: m = 50, n = 500
F2: m = 100, n = 1000
F3: m = 100, n = 2000
F4: m = 250, n = 5000
A network planning problem with random demand
In the last experiment, we consider the so-called SSN problem of Sen, Doverspike, and Cosares .
This problem arises in telecommunications network design where the owner of the network sells
private-line services between pairs of nodes in the network, and the demands are treated as random
variables based on the historical demand patterns. The optimization problem is to decide where
to add capacity to the network to minimize the expected rate of unsatisﬁed demands. Since this
problem has been studied by several authors (see, e.g., ), it could be interesting to compare
the results. Another purpose of this experiment is to investigate the behavior of the SA method
when one variance reduction technique, namely, the Latin Hyperplane Sampling (LHS), is applied.
The problem has been formulated as a two-stage stochastic linear programming as follows:
f(x) := E[Q(x, ξ)]
j xj = b, xj ≥0,
where x is the vector of capacities to be added to the arcs of the network, b (the budget) is the
total amount of capacity to be added, ξ denotes the random demand, and Q(x, ξ) represents the
Table 5: The variability for the stochastic max-ﬂow problem
number of unserved requests. We have
Q(x, ˜ξ) =
r∈R(i) Airfir ≤x + c,
r∈R(i) fir + si = ˜ξi,
fir ≥0, si ≥0,
∀i, r ∈R(i).
Here, R(i) denotes a set of routes that can be used for connections associated with the node-pair
i (Note that a static network-ﬂow model is used in the formulation to simplify the problem); ˜ξ is
a realization of the random variable ξ; the vectors Air are incidence vectors whose jth component
airj is 1 if the link j belongs to the route r and is 0 otherwise; c is the vector of current capacities;
fir is the number of connections associated with pair i using route r ∈R(i); s is the vector of
unsatisﬁed demands for each request.
In the data set for SSN, there are total of 89 arcs and 86 point-to-point pairs; that is, the
dimension of x is 89 and of ξ is 86. Each component of ξ is an independent random variable with
a known discrete distribution. Speciﬁcally, there are between three and seven possible values for
each component of ξ, giving a total of approximately 1070 possible complete demand scenarios.
The three methods, namely: N-SA, E-SA and SAA, and the same stepsize policy as discussed
in Subsection 4.1, were applied for solving the SSN problem. Moreover, we compare these methods
with or without using the Latin Hyperplane Sampling (LHS) technique. In the ﬁrst test, each
algorithm was run once for each test instance and the computational results are reported in Table
6, where N denotes the number of samples, ‘OBJ’ and ‘DEV’ represent the mean and standard
deviation, respectively, of objective values of problem (4.3) as evaluated over K = 104 sample size
at the approximated solutions yielded by the algorithms, ‘TIME’ is CPU seconds for obtaining the
approximated solution, and ‘ORC’ stands for the number of calls to the stochastic oracle. Similar
to the stochastic utility problem, we investigate the variability of these three methods by running
each method for 100 times and computing the statistical results as shown in Table 7. Note that
these tests for the SSN problem were conducted on a more powerful computer: Intel Xeon 1.86GHz
with Red Hat Enterprize Linux.
This experiment shows that for a given sample size N, the solution quality for N-SA is close to
that for SAA, meanwhile, the solution time of N-SA is much smaller than the latter one. However,
for this particular instance, the improvement on the solution quality by using the Latin Hyperplane
sampling is not signiﬁcant, especially when a larger sample-size is applied. This result seems to be
consistent with the observation in .
Table 6: SA vs. SAA on the SSN problem
Without LHS
Table 7: The variability for the SSN problem
N-SA vs. E-SA
The data in Tables 3, 4, 6 demonstrate that with the same sample size N, the N-SA somehow
outperforms the E-SA in terms of both the quality of approximate solutions and the running time.
The diﬀerence, at the ﬁrst glance, seems slim, and one could think that adjusting the SA algorithm
to the “geometry” of the problem in question (in our case, to minimization over a standard simplex)
is of minor importance. We, however, do believe that such a conclusion would be wrong. In order
to get a better insight, let us come back to the stochastic utility problem. This test problem has an
important advantage – we can easily compute the value of the objective f(x) at a given candidate
solution x analytically1. Moreover, it is easy to minimize f(x) over the simplex – on a closest
inspection, this problem reduces to minimizing an easy-to-compute univariate convex function, so
that we can approximate the true optimal value f∗to high accuracy by Bisection. Thus, in the
case in question we can compare solutions x generated by various algorithms in terms of their “true
inaccuracy” f(x) −f∗, and this is the rationale behind our “Gaussian setup”. We can now exploit
the just outlined advantage of the stochastic utility problem for comparing properly N-SA and
E-SA. In Table 8, we present the true values of the objective f(x∗) at the approximate solutions x∗
generated by N-SA and E-SA as applied to the instances L1 and L4 of the stochastic utility problem
(cf. Table 3) along with the inaccuracies f(x∗) −f∗and the Monte Carlo estimates bf(x∗) of f(x∗)
1Indeed, (ξ1, ..., ξn) ∼N(0, In), so that the random variable ξx = P
i(ai + ξi)xi is normal with easily computable
mean and variance, and since φ is piecewise linear, the expectation f(x) = E[φ(ξx)] can be immediately expressed
via the error function.
Table 8: N-SA vs. E-SA
bf(x∗), f(x∗)
N-SA, N = 2, 000
L2: n = 1000
-5.9232/-5.9326
E-SA, N = 2, 000
-5.8796/-5.8864
E-SA, N = 10, 000
-5.9059/-5.9058
E-SA, N = 20, 000
-5.9151/-5.9158
N-SA, N = 2, 000
L4: n = 5000
-5.5855/-5.5867
E-SA, N = 2, 000
-5.5467/-5.5469
E-SA, N = 10, 000
-5.5810/-5.5812
E-SA, N = 20, 000
-5.5901/-5.5902
obtained via 50,000-element samples. We see that the diﬀerence in the inaccuracy f(x∗)−f∗of the
solutions produced by the algorithms is much more signiﬁcant than it is suggested by the data in
Table 3 (where the actual inaccuracy is “obscured” by the estimation error and summation with
f∗). Speciﬁcally, at the common for both algorithms sample size N = 2, 000, the inaccuracy yielded
by N-SA is 3 – 5 times less than the one for E-SA, and in order to compensate for this diﬀerence,
one should increase the sample size for E-SA (and hence the running time) by factor 5 – 10. It
should be added that in light of theoretical complexity analysis carried out in Example 2.1, the
outlined signiﬁcant diﬀerence in performances of N-SA and E-SA is not surprising; the surprising
fact is that E-SA works at all.
Bilinear matrix game
We consider here a bilinear matrix game
y∈Y yT Ax,
where both feasible sets are the standard simplices in Rn, i.e., Y = X = {x ∈Rn : Pn
i=1 xi = 1, x ≥
0}. We consider two versions of the randomized Mirror SA algorithm (3.6), (3.7) for the saddle
point problem: Euclidean Saddle Point SA (E-SA) which uses as ωx and ωy Euclidean distance
generating function ωx(x) = 1
2. The other version of the method, which is referred to as the
Non-Euclidean Saddle Point SA (N-SA), employs the entropy distance generating function deﬁned
in (2.49). To compare the two procedures we compute the corresponding approximate solutions
tzN after N iterations and compute the exact values of the error:
ϵ(˜zN) := max
y∈Y yT A˜xN −min
In our experiments we consider symmetric matrices A of two kinds. The matrices of the ﬁrst family,
parameterized by α > 0, have the elements which obey the formula
, 1 ≤i, j ≤n.
The second family of matrices, which is also parameterized by α > 0, contains the matrices with
generic element
µ|i −j| + 1
, 1 ≤i, j ≤n.
Table 9: SA for bilinear matrix games
E2(2), ϵ(˜z1) = 0.500
E2(1), ϵ(˜z1) = 0.500
E2(0.5), ϵ(˜z1) = 0.390
E1(2), ϵ(˜z1) = 0.0625
E1(1), ϵ(˜z1) = 0.125
E1(0.5), ϵ(˜z1) = 0.138
We use the notations E1(α) and E2(α) to refer to the experiences with the matrices of the ﬁrst and
second kind with parameter α. We present in Table 9 the results of experiments conducted for the
matrices A of size 104 × 104. We have done 100 simulation runs in each experiment, we present
the average error (column MEAN), standard deviation (column Dav) and the average running
time (time which is necessary to compute the error of the solution is not taken into account). For
comparison we also present the error of the initial solution ˜z1 = (x1, y1).
Our basic observation is as follows: both Non-Euclidean SA (N-SA) and Euclidean SA (E-SA)
algorithms succeed to reduce the error of solution reasonably fast. The mirror implementation
is preferable as it is more eﬃcient in terms of running time. For comparison, it takes Matlab
from 10 (for the simplest problem) to 35 seconds (for the hardest one) to compute just one answer
of the deterministic oracle.