Subspace Methods for Joint Sparse Recovery
Kiryung Lee, Yoram Bresler Fellow, IEEE, and Marius Junge
We propose robust and efﬁcient algorithms for the joint sparse recovery problem in compressed sensing, which simultaneously
recover the supports of jointly sparse signals from their multiple measurement vectors obtained through a common sensing matrix.
In a favorable situation, the unknown matrix, which consists of the jointly sparse signals, has linearly independent nonzero rows.
In this case, the MUSIC (MUltiple SIgnal Classiﬁcation) algorithm, originally proposed by Schmidt for the direction of arrival
problem in sensor array processing and later proposed and analyzed for joint sparse recovery by Feng and Bresler, provides a
guarantee with the minimum number of measurements. We focus instead on the unfavorable but practically signiﬁcant case of
rank-defect or ill-conditioning. This situation arises with limited number of measurement vectors, or with highly correlated signal
components. In this case MUSIC fails, and in practice none of the existing methods can consistently approach the fundamental
limit. We propose subspace-augmented MUSIC (SA-MUSIC), which improves on MUSIC so that the support is reliably recovered
under such unfavorable conditions. Combined with subspace-based greedy algorithms also proposed and analyzed in this paper,
SA-MUSIC provides a computationally efﬁcient algorithm with a performance guarantee. The performance guarantees are given
in terms of a version of restricted isometry property. In particular, we also present a non-asymptotic perturbation analysis of the
signal subspace estimation that has been missing in the previous study of MUSIC.
Index Terms
Compressed sensing, joint sparsity, multiple measurement vectors (MMV), subspace estimation, restricted isometry property
(RIP), sensor array processing, spectrum-blind sampling.
I. INTRODUCTION
The problem of computing a sparse approximate solution to a linear system has been studied as the subset selection problem
in matrix computations with applications in statistical regression and signal processing , . The matrix representing
the linear system and its columns are called a dictionary and atoms, respectively. The sparse recovery problem addresses the
identiﬁcation of the support, which denotes the indices of the atoms that contribute to the sparse solution, or equivalently, the
indices of the nonzero rows of the unknown matrix. Once the support is determined, the recovery of the sparse signals reduces
to standard overdetermined linear inverse problems.
The study of sparse solutions to underdetermined systems dates back to 70’s , . Relevant theories and algorithms have
been further developed in the 80’s , and in the 90’s , , . Recently, this subject became more popular in
the signal processing community with the name of compressed sensing . In particular, the elegant analysis derived with
This work was supported in part by NSF grant No. CCF 06-35234, NSF grant No. CCF 10-18660, and NSF grant No. DMS 09-01457.
K. Lee and Y. Bresler are with Coordinated Science Laboratory and Department of ECE, University of Illinois at Urbana-Champaign, IL 61801, USA,
e-mail: {klee81,ybresler}@illinois.edu
M. Junge is with Department of Mathematics, University of Illinois at Urbana-Champaign, IL 61801, USA, e-mail: 
The results in this paper have been partially presented at the 6th IEEE Sensor Array and Multichannel Signal Processing Workshop .
modern probability theory , provided performance guarantees for polynomial-time algorithms in terms of properties
of random dictionaries. These might be the most important contributions in recent years.
In some applications, there are multiple measurement vectors (righthand sides of the linear system of equations), each
corresponding to a different unknown vector, with the special property that all unknown vectors share a common support.
The sparse recovery problem with this joint structure in the sparsity pattern is called joint sparse recovery or the multiple
measurement vector (MMV) problem and is often an easier problem with better performance.
In the mid 1990’s, Bresler and Feng introduced “spectrum-blind sampling” , , . Their scheme enables sub-Nyquist
minimum-rate sampling and perfect reconstruction of multi-band signals (analog or discrete, in one or more dimensions) with
unknown but sparse spectral support. They reduced the spectrum-blind reconstruction problem to a ﬁnite-dimensional joint
sparse recovery problem. Mishali and Eldar elaborated the spectrum-blind sampling approach in , and they also proposed
“modulated wideband converter” in , which improves on spectrum-blind sampling by adding robustness against jitter. The
reconstruction in the modulated wideband converter too is reduced to a ﬁnite dimensional joint sparse recovery problem. Rao et
al. (cf. and the references therein) introduced a joint sparse recovery formulation and methods for the recovery of sparse
brain excitations. Obozinski et. al. formulated variable selection in multivariate regression as a joint sparse recovery
problem. The design matrix in regression corresponds to the linear system matrix of the joint sparse recovery and the indicator
function of the variables that mostly contribute to the given data is assumed to be sparse. Malioutov et al. posed the direction
of arrival (DOA) estimation problem as a joint sparse recovery problem . For the typically small number of sources in this
problem, the indicator function of the quantized angles is modeled to be sparse.
Algorithms that exploit the structure in the sparsity pattern have been developed for the joint sparse recovery problem.
Bresler and Feng proposed to use a version of the MUSIC algorithm from sensor array processing for the full row rank
case where the nonzero rows of the unknown matrix have full row rank , , . They also proposed methods based
on a greedy search inspired by the alternating projections algorithm in DOA estimation, later dubbed orthogonal least
squares (OLS) . Existing solutions to the sparse recovery problem for the single measurement vector (SMV) case have
been extended to the MMV case. Greedy algorithms , , extend orthogonal matching pursuit (OMP) and
convex optimization formulations with the mixed norm , , , extend the corresponding SMV solution such as
basis pursuit (BP) and LASSO . Sparse Bayesian learning (SBL) has been also extended to the MMV case ,
Owing to the similarity between the joint sparse recovery problem and DOA estimation, theories developed for DOA
estimation affected those for joint sparse recovery. For example, the fundamental limit on the performance of DOA estimation
 also applies to joint sparse recovery. Wax and Ziskind showed the condition for the unique identiﬁcation of DOA in
the sensor array processing, which has been applied to joint sparse recovery by Feng and Bresler to determine the condition
for the unique identiﬁcation of the support. The condition has been further studied in more general settings , . MUSIC
applied to joint sparse recovery was the ﬁrst method that was guaranteed with the tightest sufﬁcient condition, which also
coincides with a necessary condition required for the support identiﬁcation by any method. However, the guarantee only applies
to the case where the nonzero rows of the unknown matrix have full row rank and there is no noise in the measurement vectors.
Performance guarantees of greedy algorithms and of convex optimization formulations for joint sparse recovery have been
also studied extensively in the literature , , , , , , , . The guarantees of such methods have
not been proved to be strictly better than the guarantees for the SMV case. Moreover, unlike the guarantee of MUSIC, such
methods are not guaranteed with the minimal requirement for the full row rank case in the absence of noise.
Performance guarantees aside, the empirical performance and computational cost of any method are of key importance and
usually determines its adoption in practice. Empirically, the optimization schemes with diversity measures (e.g., the mixed
norm) perform better than greedy algorithms. In particular, the rate of exact support recovery in existing algorithms does not
improve with increasing rank of the unknown matrix beyond a certain level. Furthermore, under unfavorable settings such
as rank-defect or ill-conditioning, existing algorithms for joint sparse recovery, while not failing, are far from achieving the
guarantee of MUSIC for the full row rank case.
While the optimization scheme with diversity measures perform better empirically than the greedy algorithms, this improved
performance comes at a much higher computational cost. Convex optimization formulations , , , are usually
cast as second order cone programming (SOCP), which is more difﬁcult to solve compared to its analogues in the SMV case,
which are cast as linear programming (LP) or quadratic programming (QP). In contrast, greedy algorithms and MUSIC are
computationally efﬁcient. As a summary, none of the listed methods enjoys both good empirical performance and computational
speed at the same time.
In view of the various drawbacks of the existing algorithms for joint sparse recovery, MUSIC, when it works, is extremely
attractive. In a favorable setting, where the matrix composed of the nonzero rows of the unknown signal matrix has full
row rank, MUSIC is guaranteed to recover the support and hence provides a guarantee with minimal requirement. Moreover,
MUSIC is highly efﬁcient computationally. However, the full row rank condition is often violated in practice. For example,
if the number of measurement vectors N is smaller than the sparsity level s, then no more than N rows can be linearly
independent, and the nonzero rows do not have full row rank. In other applications, such as spectrum-blind sampling or the
DOA problem, N is large or even inﬁnite. Even in this case though, the rank might be smaller than s, or the submatrix of
nonzero rows can be ill-conditioned. For example, in the DOA problem, correlation between sources or multi-path propagation
can cause a large condition number. It is well-known that MUSIC fails in this practically important “rank-defective” case
and this has motivated numerous attempts to overcome this problem, without resorting to infeasible multi-dimensional search.
However, all these previous methods use special structure of the linear system – such as shift invariance that enables to apply
so-called spatial smoothing . Previous extension of MUSIC are therefore not applicable to the general joint sparse recovery
The main contributions of this paper are summarized as follows. First, we propose a new class of algorithms, subspaceaugmented MUSIC (SA-MUSIC) that overcome the limitations of existing algorithms and provide the best of both worlds:
good empirical performance at all rank conditions; and efﬁcient computation. In particular, SA-MUSIC algorithms improve
on MUSIC so that the support is recovered even in the case that the unknown matrix has rank-defect and/or ill-conditioning.
Compared to MUSIC , in the presence of a rank-defect, SA-MUSIC has additional steps of partial support recovery and
subspace augmentation. Combined with partial support recovery by the subspace-based greedy algorithms also introduced in
this paper, SA-MUSIC provides a computationally efﬁcient solution to joint sparse recovery with a performance guarantee.
In fact, the computational requirements of SA-MUSIC algorithms are similar to those of greedy algorithms and of MUSIC.
Secondly, we derive explicit conditions that guarantee each step of SA-MUSIC for the noisy and/or rank-defective case. The
performance is analyzed in terms of a property , which is a local version of the restricted isometry property (RIP) .
We call this property the weak-1 restricted isometry property (weak-1 RIP). Most importantly, compared to the relevant work
 with similar but independently developed ideas, the analysis in this paper is non-asymptotic and applies to wider class of
matrices including Gaussian, random Fourier, and incoherent unit-norm tight frames.
Contributions of independent interest include two new subspace-based greedy algorithms for joint sparse recovery with
performance guarantees, extension of the analysis of MUSIC for joint sparse recovery to the noisy case with imperfect
subspace estimation, and non-asymptotic analysis of subspace estimation from ﬁnitely many snapshots. The latter analysis is
different from previous analysis of subspace methods, which were based on the law of large numbers, asymptotic normality,
or low order expansion.
The remainder of this paper is organized as follows. After introducing notations in Section II, the joint sparse recovery
problem is stated in Section III. MUSIC for joint sparse recovery is reviewed in Section IV with discussion that motivates the
current work. We also propose an algorithm for signal subspace estimation in Section IV. In Section V, we propose SA-MUSIC
and subspace-based greedy algorithms. We review the notion of the weak-1 RIP in Section VI. The weak-1 RIP analysis of
various matrices that arise commonly in compressed sensing is given without any ambiguous constant, which might be of
independent interest. In Section VII, we provide non-asymptotic analysis of the algorithms of Sections IV and V by using
the weak-1 RIP. In Section VIII, we analyze subspace estimation using a random signal model. The empirical performance of
SA-MUSIC is compared to other methods in Section IX and the relation to relevant works is discussed in Section X.
II. NOTATION
Symbol K denotes a scalar ﬁeld, which is either the real ﬁeld R or the complex ﬁeld C. The vector space of d-tuples over
K is denoted by Kd for d ∈N where N is the set of natural numbers (excluding zero). Similarly, for m, n ∈N, the vector
space of m × n matrices over K is denoted by Km×n.
We will use various notations on a matrix A ∈Km×n. The range space spanned by the columns of A will be denoted by
R(A). The Hermitian transpose of A will be denoted by A∗. The j-th column of A is denoted by aj and the submatrix of
A with columns indexed by J ⊂[n] is denoted by AJ, where [ℓ] denotes the set {1, . . . , ℓ} for ℓ∈N. The k-th row of A is
denoted by ak, and the submatrix of A with rows indexed by K ⊂[m] is denoted by AK. Symbol ek will denote the k-th
standard basis vector of Kd, where d is implicitly determined for compatibility. The k-th largest singular value of A will be
denoted by σk(A). For Hermitian symmetric A, λk(A) will denote the k-the largest eigenvalue of A. The Frobenius norm and
the spectral norm of A are denoted by ∥A∥F and ∥A∥, respectively. For p, q ∈[1, ∞], the mixed ℓp,q norm of A is deﬁned by
k∈[m] ∥ak∥p
The inner product is denoted by ⟨·, ·⟩. The embedding Hilbert space, where the inner product is deﬁned, is not explicitly
mentioned when it is obvious from the context.
For a subspace S of Kd, matrices PS ∈Kd×d and P ⊥
S ∈Kd×d denote the orthogonal projectors onto S and its orthogonal
complement S⊥, respectively.
For two matrices A and B of the same dimension, A ≥B if and only if A −B is positive semideﬁnite.
Symbols P and E will denote the probability and the expectation with respect to a certain distribution. Unless otherwise
mentioned, the distribution shall be obvious from the context.
III. PROBLEM STATEMENT
A vector x ∈Kn is s-sparse if it has at most s nonzero components. The support of x is deﬁned as the set of the indices
of nonzero components. The sparsity level of x is deﬁned as the number of nonzero components of x. The sparse recovery
problem is to reconstruct an s-sparse vector x0 ∈Kn from its linear measurement vector y ∈Km through sensing matrix
A ∈Km×n. In particular, if there is no noise in y, then x0 is a solution to the linear system Ax = y. In this case, under
certain conditions on A, unknown vector x0 is recovered as the unique s-sparse solution. For example, any s-sparse x0 is
recovered if and only if any 2s columns of A are linearly independent . For x0 to be the unique s-sparse solution to
Ax = y, it is necessary that submatrix AJ0 have full column rank, where J0 is the support of x0. Otherwise, there exists
another s-sparse solution and this contradicts uniqueness. Note that once the support of x0 is determined from (y, A), then x0
is easily computed as x0 = A†
J0y where A†
J0 ∈Ks×m denotes the Moore-Penrose pseudo inverse of AJ0. Therefore, the key
step in solving the sparse recovery problem is the identiﬁcation of the support.
In practice, the measurement vector y is perturbed by noise. Usually, we assume that y is given by
y = Ax0 + w
with additive noise w ∈Km. In this case, x0 is no longer a solution to the linear system Ax = y. Instead, minimizing ∥y−Ax∥2
with the sparsity constraint that x is s-sparse provides a solution to the sparse recovery problem. Alternatively, various convex
optimization methods with sparsity inducing metrics such as the ℓ1 norm have been proposed. However, the solution provided
by such methods is not exactly s-sparse in the presence of noise. In some applications, the support has important physical
meaning and hence the identiﬁcation of the support is explicitly required. For example, in imaging applications of compressed
sensing the support corresponds to the location of the target object, and in sparse linear regression the most contributing
variables are identiﬁed by the support (cf. ). In such applications, unless the solution obtained to the sparse recovery
problem is exactly s-sparse, a step of thresholding the obtained solution to the nearest s-sparse vector is necessary. For this
reason, in this paper, the success of the sparse recovery problem is deﬁned as the exact identiﬁcation of the support of x0.
Let us now turn to the main problem of this paper, where there exist multiple sparse signal vectors {xi}N
i=1 ⊂Kn that
share the same (or similar) sparsity pattern(s) and the measurement vectors {yi}N
i=1 ⊂Km are obtained through a common
sensing matrix A ∈Km×n. We assume that the union of the supports of the xi for i = 1, . . . , N has at most s elements. Then,
X0 = [x1, . . . , xN] ∈Kn×N has at most s nonzero rows and is called row s-sparse. The row support of X0 is deﬁned as the
set of indices of nonzero rows. The joint sparse recovery problem is to ﬁnd the row support of the unknown signal matrix X0
from the matrix Y ∈Km×N with multiple measurement vectors (MMV) given by
Y = AX0 + W
with common sensing matrix A ∈Km×n and with perturbation W ∈Km×N. Let J0 denote the row support of X0. Then, AX0
is compactly rewritten as AJ0XJ0
is the matrix composed of the nonzero rows of X0, and AJ0 is the submatrix of
A with the corresponding columns. The prior knowledge that X0 is row s-sparse will be assumed 1. An important parameter
in the problem will be the rank of the unknown signal matrix X0, rank(X0) = rank(XJ0
0 ), which will be assumed unknown
as well. When matrix XJ0
has full row rank, rank(XJ0
0 ) assumes its maximum value, rank(XJ0
0 ) = s, and we will refer to
this as the full row rank case. This is the case preferred by the algorithms in this paper. Otherwise, rank(XJ0
0 ) < s, considered
as violation of the full row rank case, will be called the rank-defective case.
IV. MUSIC REVISITED
The similarity between the joint sparse recovery (or MMV) problem and the direction of arrival (DOA) estimation problem
in sensor array processing has been well studied before (e.g. ). In particular, it has been shown that the joint sparse recovery
problem can be regarded as a special case of the DOA problem with discretized angles . Through this analogy between the
two problems, the algorithms and their analysis developed for the DOA problem have been applied to the joint sparse recovery
problem . In this section, we review a subspace-based algorithm proposed by Feng and Bresler , on which our new
algorithm in Section V improves. We also elaborate the subspace-based algorithm in to work without ideal assumptions.
A. MUSIC for the Joint Sparse Recovery Problem Revisited
Inspired by the success of the MUSIC algorithm in sensor array processing, Bresler and Feng , , proposed
to use a version of MUSIC for joint sparse recovery. As in the original MUSIC algorithm in sensor array processing , the
ﬁrst step is to estimate the so-called signal subspace S deﬁned by
S ≜R(AX0) = R(AJ0XJ0
from the snapshot matrix2 Y = AJ0XJ0
+ W ∈Km×N. In sensor array processing, MUSIC estimates S using the
eigenvalue decomposition (EVD) of Y Y ∗
N . The same method is applied to the joint sparse recovery problem under the assumption
1 This is only for convenience of the analysis but is not a limitation of the proposed algorithms. See V-B for more detailed discussion.
2 We adopt the terminology from the sensor array processing literature. To emphasize the analogy between the joint sparse recovery problem and DOA
estimation, we also call each of the N columns of Y a snapshot. Then, N will denote the number of snapshots.
which is achieved with statistical assumptions on AJ0XJ0
and W under the asymptotic in N (with inﬁnitely many snapshots)
 . It is also assumed that AJ0 ∈Km×s has full column rank and XJ0
∈Ks×N has full row rank. Then, the dimension
of S coincides with rank(XJ0
0 ) = s. The assumed relation (4.1) implies that the smallest eigenvalue of Y Y ∗
has multiplicity
m −s, whence the dimension of S is exactly determined. The signal subspace S is then exactly computed as the invariant
subspace spanned by the s dominant eigenvectors of Y Y ∗
since the noise part σ2
wI in (4.1) only shifts the eigenvalues of
. The joint sparse recovery problem then reduces to the case where the subspace estimation is error-free and
hence the subsequent analysis of the support recovery in , , considered this error-free case 3.
Given the signal subspace S, MUSIC for joint sparse recovery identiﬁes the row-support J0 as the set of the indices k of
columns of A such that P ⊥
S ak = 0. In other words, MUSIC accepts the index k as an element of the support if ak ∈S and
rejects it otherwise. In the remainder of this paper, the acronym MUSIC will denote the version for joint sparse recovery 
rather than the original MUSIC algorithm for sensor array processing .
A sufﬁcient condition that guarantees the success of MUSIC for the special case where XJ0
has full row rank, is given in
terms of the Kruskal rank deﬁned below.
Deﬁnition 4.1: The Kruskal rank of a matrix A, denoted by krank(A), is the maximal number k such that any k columns
of A are linearly independent.
Proposition 4.2 ( , ): Let J0 be an arbitrary subset of [n] with s elements. Suppose that X0 ∈Kn×N is row s-sparse
with support J0 and rank(XJ0
0 ) = s. If A satisﬁes
krank(A) = s,
for S ≜R(AJ0XJ0
0 ) if and only if k ∈J0.
The following result directly follows from Proposition 4.2.
Corollary 4.3: Under the conditions on A and XJ0
in Proposition 4.2, given the exact signal subspace S, MUSIC is
guaranteed to recover J0.
Remark 4.4: In , Condition (4.2) was stated in terms of the “universality level” of A, which is in fact identical to
krank(A). A similar notion called the “spark” of A was later introduced in , which is related to the Kruskal rank by
spark(A) = krank(A) + 1.
Remark 4.5: Condition (4.2) is satisﬁed by certain A ∈Km×n with m > s. For example, it has been shown that the matrix
composed of any consecutive m rows of the n × n DFT matrix, which corresponds to the “bunched sampling pattern” in
spectrum-blind sampling, satisﬁes (4.2) when m > s .
3 Obviously, for the noiseless case, the subspace estimation is error-free without relying on the asymptotic N.
MUSIC with its performance guarantee in Proposition 4.2 is remarkable in the context of compressive sensing for the
following reasons. First, MUSIC is a polynomial-time algorithm with a performance guarantee under a condition that coincides
with the necessary condition m > s for unique recovery (by any algorithm, no matter how complex) 4. Second, MUSIC is
simple and cheap, involving little more than a single EVD of the data covariance matrix. In fact, efﬁcient methods for partial
EVD, or other rank-revealing decompositions can further reduce the cost 5.
Unfortunately, as is well-known in the sensor array processing literature , , and also demonstrated by numerical
experiments later in Section IX, MUSIC is prone to failure when XJ0
does not have full row rank, or when it is ill-conditioned
in the presence of noise. In sensor array processing, this is known as the “source coherence problem” , and (with the
exception of the case of a Vandermonde matrix A) no general solution to this problem are known. This motivates our work to
propose a new subspace-based method for joint sparse recovery that improves on MUSIC.
For the noisy case, the analysis of signal subspace estimation based on the asymptotic in N is not practical. In particular,
from a perspective of compressed sensing (with joint sparsity), recovery of the support from a ﬁnite (often small) number of
snapshots is desired. In the next subsection, we propose a subspace estimation scheme that works with ﬁnitely many snapshots,
which will be applied to both MUSIC and the new subspace-based methods in this paper. In Section IV-C, we formalize the
MUSIC algorithm for support recovery in the presence of a perturbation in the estimate of S. This will lay the ground for the
subsequent analysis of MUSIC in the noisy scenario, and for its extension in the same scenario to SA-MUSIC.
B. Signal Subspace Estimation from Finitely Many Snapshots
We study the problem of signal subspace estimation from ﬁnitely many snapshots in this subsection. For later use in other
subspace-based algorithms in Section V, we weaken the assumptions in the previous section. In particular, we assume ﬁnite N
and no longer assume that XJ0
has full row rank. We also assume that the columns of the noise matrix W are i.i.d. random
vectors with white spectrum, i.e., E WW ∗
wIm. (Otherwise, the standard pre-whitening schemes developed in sensor array
processing may be applied prior to subspace estimation.)
When AJ0XJ0
0 is ill-conditioned, the last few singular values of AJ0XJ0
0 are small, making the estimation of S ≜R(AJ0XJ0
highly sensitive to the noise in the snapshots. To improve the robustness against noise, instead of estimating the whole subspace
S, we only estimate an r-dimensional subspace of S for r < s. The dimension r is determined so that the gap between
0 ) and σr+1(AJ0XJ0
0 ) is signiﬁcant.
We propose and analyze a simple scheme that determines the dimension r by thresholding the eigenvalues of Y Y ∗
the estimated signal subspace bS spanned by the r dominant eigenvectors of Y Y ∗
is close to an r-dimensional subspace of S.
The procedure for estimating the signal subspace and its dimension is described next.
Given the snapshot matrix Y = AJ0XJ0
0 + W, we compute its sample covariance matrix ΓY deﬁned by
4 Since the mid 1990’s, when these results (for what became known later as compressive sampling) , , were published, until recently, MUSIC
was the only polynomial-time algorithm with such a guarantee. Recent results showed another (greedy) algorithm with the same guarantee.
5 The rank revealing decompositions can be computed by the Lanczos method for the truncated singular value decomposition (SVD). If the matrix is
large, randomized algorithms , can be also used to compute the truncated SVD.
Then, we compute a biased matrix bΓ by
bΓ ≜ΓY −λm(ΓY )Im.
Note that bΓ and ΓY have the same eigenvectors. Recall that our goal is to ﬁnd an r-dimensional subspace bS for some r ≤s
from bΓ such that there exists an r-dimensional subspace ¯S of the signal subspace S ≜R(AJ0XJ0
0 ) satisfying ∥Pb
S −P ¯S∥≤η
for small η. For better performance of support recovery by algorithms in Section V, larger r is preferred.
For an ideal case where (4.1) holds, bΓ reduces to ΓS deﬁned by
ΓS ≜AJ0XJ0
Since S = R(ΓS), by setting r to rank(ΓS), we compute S itself rather than a proper subspace of S.
For ﬁnite N, usually, the cross correlation terms in the sample covariance matrix ΓY between the noise term W and the
signal term AJ0XJ0
are smaller than the autocorrelation terms. Since the autocorrelation term of W is nearly removed in bΓ,
we will show that it is likely that D ≜bΓ −ΓS is small in the spectral norm. In particular, let bS be the subspace spanned
by the r dominant eigenvectors of bΓ; if ∥D∥is small compared to the gap between λr(bΓ) and λr+1(bΓ), then there exists an
r-dimensional subspace ¯S of S with small ∥Pb
S −P ¯S∥. Since ∥D∥is not available, we determine the dimension r by simple
thresholding. More speciﬁcally, given a threshold τ > 0, the dimension of bS is determined as the maximal number r satisfying
λr(bΓ) −λr+1(bΓ)
≥τ > λk(bΓ) −λk+1(bΓ)
When AJ0XJ0
(and hence ΓS) is ill-conditioned, it is likely that the gap between the consecutive eigenvalues λr0(bΓ) and
λr0+1(bΓ) where r0 = rank(ΓS) is small compared to ∥D∥, which is roughly proportional to σ2
w/λ1(bΓ). The aforementioned
increased robustness to noise is provided by choosing r < r0 so that the gap λr(bΓ) −λr+1(bΓ) is large., which will in turn
reduce the estimated subspace dimension r. More sophisticated methods for determining r are possible, but this simple method
sufﬁces for our purposes and is amenable to analysis (see Section VIII). This algorithm for estimating the signal subspace and
its dimension is summarized as Algorithm 1.
Algorithm 1 Signal Subspace Estimation
Input: Y ∈Km×N and τ > 0.
Output: r ∈N and Pb
1: ΓY ←Y Y ∗
2: bΓ ←ΓY −λm(ΓY )Im;
3: r ←m −1;
4: while λr(bΓ) −λr+1(bΓ) < τλ1(bΓ) do
6: end while
7: U ←r dominant eigenvectors of bΓ;
9: return r, Pb
C. MUSIC Applied to an Inaccurate Estimate of the Signal Subspace
In the presence of a perturbation in the estimated signal subspace bS, MUSIC ﬁnds the set bJ of s indices that satisfy
The corresponding algorithm is summarized in Algorithm 2. To provide an intuition for the selection criterion in (4.5), we use
the notion of the “angle function” .
Deﬁnition 4.6 ( ): The angle function between two subspaces S1 and S2 is deﬁned by
∢2(S1, S2) ≜sin−1  min{∥P ⊥
S1PS2∥, ∥P ⊥
Remark 4.7: The angle function ∢2(S1, S2) is different from the largest principal angle between S1 and S2 . Unlike the
largest principal angle, the angle function satisﬁes the metric properties even when S1 and S2 have different dimensions. In
particular, when dim(S1) ≥dim(S2), the expression on the right hand side of (4.6) reduces to
∢2(S1, S2) = sin−1  ∥P ⊥
By (4.7), the criterion in (4.5) is equivalent to
∢2(bS, R(ak)) <
∢2(bS, R(ak)).
That is, MUSIC ﬁnds, among all subspaces spanned by a single column of A, s subspaces nearest to S (in the angle function
Algorithm 2 MUSIC
Input: Y ∈Km×N, A ∈Km×n, s ∈N.
Output: J ⊂[n]
S ∈Rm×m, r ∈N ←estimate signal subspace from Y ;
3: for ℓ= 1, . . . , n do
Saℓ∥2/∥aℓ∥2
5: end for
6: J ←indices of the s-largest ζℓ’s;
7: return J
V. SUBSPACE-AUGMENTED MUSIC
A. MUSIC Applied to an Augmented Signal Subspace
has full row rank, the signal subspace S ≜R(AJ0XJ0
0 ) coincides with R(AJ0). In this case, given the exact S,
MUSIC is guaranteed to recover the support J0 because (i) ak ∈S for all k ∈J0; and (ii) ak ̸∈S for all k ∈[n] \ J0, which
is implied by krank(A) = s + 1 (Proposition 4.2). However, in the rank-defective case, when XJ0
does not have full row
rank, i.e., when rank(XJ0
0 ) is strictly smaller than the sparsity level s, we have dim(S) ≤rank(XJ0
0 ) < s = dim(R(AJ0).
Therefore, S is a proper subspace of R(AJ0) and it may happen that ak ̸∈S for some (or all) k ∈J0. This will cause MUSIC
to miss valid components of J0. Because, in the presence of noise (imperfect subspace estimate), MUSIC selects, by (4.8),
the s indices k ∈[n] for which R(ak) is closets (in the sense of the angle function) to S, this may result in the selection of
spurious indices into the estimate of J0. This explains the well-known fact that in the rank-defective case MUSIC is prone to
Subspace-augmented MUSIC overcomes the limitation of MUSIC to the full row rank case by capitalizing on the following
observation: MUSIC fails in the rank-defective case because S is a proper subspace of R(AJ0); however, if another subspace
T that complements S is given so that S + T = R(AJ0), then MUSIC applied to the augmented subspace S + T will be
successful.
Unfortunately, in general, ﬁnding such an oracle subspace is not feasible. The search procedure cannot even be enumerated.
However, if XJ0
0 , the matrix of nonzero rows of X0, or more generally, the subspace R(XJ0
0 ), satisﬁes a mild condition, then
the search may be restricted without loss of generality to subspaces spanned by s−r columns of A. The following proposition
states this result.
Deﬁnition 5.1: Matrix X is row-nondegenerate if
krank(X∗) = rank(X).
Remark 5.2: Condition (5.1) says that every k rows of X are linearly independent for k ≤rank(X). This is satisﬁed by X
that is generic in the set of full rank matrices of the same size. In fact, an even weaker requirement on X sufﬁces, as shown
by the next argument that reduces the requirement to R(X).
Remark 5.3: Condition (5.1) is invariant to multiplication of X by any full row rank matrix of compatible size on the right.
In particular, Condition (5.1) holds if and only if
krank(Q∗) = rank(X)
for any orthonormal basis Q of R(X). It follows that (5.1) is a property of the subspace R(X). Furthermore, (5.1) also implies
that any eQ such that R( eQ) ⊂R(X) is also row-nondegenerate (Lemma A.4).
Remark 5.4: The condition on Q in (5.2) that any subset of rows of Q up to size rank(X) are linearly independent is
purely algebraic and can be paraphrased to say that the rows of Q are in general position. This is a mild condition satisﬁed by
generic Q. For example, if the rows of Q are independently distributed with respect to any absolutely continuous probability
measure, then (5.2) is satisﬁed with probability 1.
Remark 5.5: Remarks 5.2–5.4 validate the deﬁnition of Condition 5.1 as a row-nondegeneracy condition.
Proposition 5.6 (Subspace Augmentation): Suppose that X0 ∈Kn×N is row s-sparse with support J0 ⊂[n] and XJ0
row-nondegenerate. Let ¯S be an arbitrary r-dimensional subspace of R(AJ0XJ0
0 ) where r < s. Let J1 be an arbitrary subset
of J0 with s −r elements. If AJ0 has full column rank, then
¯S + R(AJ1) = R(AJ0).
Proof: See Appendix B.
Remark 5.7: Note that dim(R(AJ1)) ≥dim(R(AJ0)) −dim( ¯S) = s −r is a necessary condition for (5.3). Therefore, J1
should be a subset of J0 with at least s −r elements for the success of the subspace augmentation.
Remark 5.8: The row-nondegeneracy condition on XJ0
is a necessary condition to guarantee (5.3) for an arbitrary subset J1
of J0 with s−r elements. Suppose that XJ0
fails to satisfy the row-nondegeneracy condition, i.e., krank((XJ0
0 )∗) < rank(XJ0
By the assumption on ¯S, there exists a row s-sparse matrix U ∈Kn×r with support J0 such that ¯S = R(AJ0U J0). Since ¯S
was an arbitrary r-dimensional subspace of S, without loss of generality, we may assume that krank((U J0)∗) < r. By the
projection update formula, it follows that PR(AJ1 )+ ¯S = PR(AJ1 )+PP ⊥
R(AJ1 ) ¯S and hence it sufﬁces to show dim(P ⊥
R(AJ1 ) ¯S) < r
for the failure of (5.3). Since krank((U J0)∗) < r, there exists J1 ⊂J0 of size s −r such that rank(U J0\J1) < r. Then,
R(AJ1 ) ¯S) = rank(P ⊥
R(AJ1 )AJ0U J0) = rank(P ⊥
R(AJ1 )AJ0\J1U J0\J1) ≤rank(U J0\J1) < r. It follows that (5.3) fails
for this speciﬁc J1. Therefore, the row-nondegeneracy condition on XJ0
is essential. Furthermore, by Remarks 5.2–5.5, the
row-nondegeneracy is a mild condition; it will be assumed to hold henceforth.
be row-nondegenerate and suppose that an error-free estimate of S is available. In this case, Proposition 5.6 implies
that, given a correct partial support J1 of size s −r, MUSIC applied to the augmented subspace S + R(AJ1) enjoys the same
guarantee as MUSIC for the full row rank case (Proposition 4.2). We will see in Section VII that a similar statement applies
even with an imperfect estimate bS.
Based on the above result, we propose a class of methods for joint sparse recovery called subspace-augmented MUSIC
(SA-MUSIC) consisting of the following steps:
1) Signal subspace estimation: compute an estimate bS of the signal subspace S ≜R(AJ0XJ0
0 ) = R(AX0).
2) Partial support recovery: compute a partial support J1 of size s−r from bS and A, where r = dim(bS) and s is the sparsity
level known a priori.
3) Augment signal subspace: compute the augmented subspace eS
eS = bS + R(AJ1).
4) Support completion: complete J1 to produce J0 ⊃J1, by adding r more support elements obtained by applying “MUSIC”
to eS, that is, ﬁnding bJ ⊂[n] satisfying
The general SA-MUSIC algorithm is summarized as Algorithm 3. An actual implementation might use orthonormal bases bQ
and eQ for the subspaces bS and eS instead of constructing the projection operators Pb
S. Step 3 could then be performed
by a QR decomposition of matrix [ bQ, AJ1].
A particular instance of the SA-MUSIC algorithm is speciﬁed by the particular methods used for the steps of signal subspace
estimation and partial support recovery. For subspace estimation, in the analysis in Sections VII and VIII, we will consider the
EVD-based scheme in Section IV-B. However, as mentioned earlier, the subspace estimation scheme is not restricted to the
Algorithm 3 Subspace Augmented MUSIC
Input: Y ∈Km×N, A ∈Km×n, s ∈N.
Output: J ⊂[n]
S ∈Rm×m, r ∈N ←estimate signal subspace from Y ;
2: J1 ←partial support recovery of size s −r;
S AJ1)(P ⊥
4: for ℓ∈[n] \ J1 do
Saℓ∥2/∥aℓ∥2
6: end for
7: J ←J1 ∪{indices of the r-largest ζℓ’s};
8: return J
given method. For example, if the noise W is also sparse, then robust principal component analysis (RPCA) will provide
a better estimate of S than the usual SVD.
The choice of method for partial support recovery is discussed in the next subsection. Here we note some special cases.
As r increases, the size of the partial support required in SA-MUSIC decreases. For small s −r, we can use an exhaustive
search over J1, the computational cost of which also decreases in r. In particular, for the special case where r = s, the step
of partial support recovery is eliminated, and SA-MUSIC reduces to MUSIC .
B. Partial Support Recovery with Practical Algorithms
When there is a “rank-defect” in XJ0
0 , i.e., rank(XJ0
0 ) < s, SA-MUSIC requires partial support recovery of size s −r. In
addition to computational efﬁciency, a key desirable property of an algorithm to accomplish this is that it solves the partial
support recovery problem more easily than solving the full support recovery problem.
From this perspective, greedy algorithms for the joint sparse recovery problem are attractive candidates. Both empirical
observations and the performance guarantees in the sequel suggest that the ﬁrst few steps of greedy algorithms are more likely
to succeed than the entire greedy algorithms. In other words, greedy algorithms take advantage of the reduction to partial
support recovery when they are combined with SA-MUSIC.
Any of the known greedy algorithms for joint sparse recovery may be used in SA-MUSIC, producing a different version of
SA-MUSIC. In particular, we may consider variations on orthogonal matching pursuit (OMP) , such as MMV orthogonal
matching pursuit (M-OMP) , simultaneous orthogonal matching pursuit (S-OMP) , or their generalization to p-SOMP
 . The p-SOMP algorithm incrementally updates the support by the following selection rule: given an index set J from the
previous steps, the algorithm adds to J the index k that satisﬁes
k = arg max
ℓ∈[n]\J ∥Y ∗P ⊥
R(AJ )aℓ∥p
where p ∈[1, ∞] is a parameter of the algorithm. M-OMP and S-OMP correspond to 2-SOMP and 1-SOMP, respectively.
In particular, we propose and analyze two different algorithms for the partial support recovery step in SA-MUSIC. The ﬁrst
is the signal subspace orthogonal matching pursuit (SS-OMP) algorithm. SS-OMP is a subspace-based variation of M-OMP
(2-SOMP) that replaces the snapshot matrix Y in (5.4) by the orthogonal projector Pb
S onto the estimated signal subspace.
(Equivalently, Y is replaced by an orthogonal basis matrix for the estimated signal subspace). Hence, given the estimated
support J from the previous steps, SS-OMP updates J by adding k selected by
k = arg max
ℓ∈[n]\J ∥Pb
R(AJ)aℓ∥2.
The complete SS-OMP is summarized as Algorithm 4.
Algorithm 4 Signal Subspace Orthogonal Matching Pursuit (SS-OMP)
Input: Y ∈Km×N, A ∈Km×n, s ∈N.
Output: J ⊂[n]
S ∈Rm×m, r ∈N ←estimate signal subspace from Y ;
3: while |J| < s do
k ←arg max
ℓ∈[n]\J ∥Pb
R(AJ)aℓ∥2;
J ←J ∪{k};
6: end while
7: return J
The second algorithm we propose for the partial support recovery step in SA-MUSIC is signal subspace orthogonal matching
subspace pursuit (SS-OMSP). SS-OMSP is a modiﬁcation of another greedy algorithm, rank-aware order recursive matching
pursuit (RA-ORMP) 6. SS-OMSP replaces the snapshot matrix Y in RA-ORMP by the orthogonal projector PS onto the
estimated signal subspace, or equivalently, by an orthogonal basis matrix for the estimated signal subspace. Given the estimated
support J from the previous steps, RA-ORMP updates J by adding k selected by
k = arg max
ℓ∈[n]\J ∥(PR(P ⊥
R(AJ )Y ))aℓ∥2/∥P ⊥
R(AJ)aℓ∥2.
Similarly, SS-OMSP updates the support by adding k selected by
k = arg max
ℓ∈[n]\J ∥(PP ⊥
R(AJ ) bS)aℓ∥2/∥P ⊥
R(AJ)aℓ∥2.
In general, P ⊥
R(AJ )Y and P ⊥
S span different subspaces, and hence the two projection operators used in (5.6) and (5.7)
are different. The two projection operators coincide only for the special case when there is no noise.
We interpret (5.7) using the angle function between two subspaces, i.e., (5.7) is equivalent to
k = arg min
ℓ∈[n]\J ∢2(P ⊥
R(AJ) bS, P ⊥
R(AJ)R(aℓ)).
Given the subspace R(AJ), which is spanned by the columns of A corresponding to support elements J determined in the
preceding steps of the algorithm, the selection rule ﬁnds the nearest subspace to P ⊥
R(AJ) bS among all subspaces, each of which
is spanned by P ⊥
R(AJ)aℓfor ℓ∈[n] \ J. The name “orthogonal matching subspace pursuit” is intended to distinguish the
matching using a subspace metric in SS-OMSP from that of OMP and its variations.
Again, we use a partial run (s −r steps) of SS-OMSP for partial support recovery and switch to MUSIC applied to the
6 The name “Rank-Aware ORMP” proposed for this algorithm appears to be a misnomer. RA-ORMP, as originally proposed , does not have any feature
to determine rank. It computes an orthonormal basis for Y . However, whereas in the ideal, noiseless case this basis will have dimension r equal to the rank
0 , with any noise present Y will have full rank equal to min{m, N}, and this will also be the dimension of the computed orthonormal basis. Hence,
the algorithm does not seem to have any built-in rank-awareness.
augmented subspace. The complete SS-OMSP is summarized as Algorithm 5.
Algorithm 5 Signal Subspace Orthogonal Matching Subspace Pursuit (SS-OMSP)
Input: Y ∈Km×N, A ∈Km×n, s ∈N.
Output: J ⊂[n]
S ∈Rm×m, r ∈N ←estimate signal subspace from Y ;
3: while |J| < s do
k ←arg max
ℓ∈[n]\J ∥(PP ⊥
S)aℓ∥2/∥P ⊥
R(AJ)aℓ∥2;
J ←J ∪{k};
6: end while
7: return J
C. Stopping Conditions for Unknown Sparsity Level
In most analyses of greedy algorithms, the sparsity level s is assumed to be known a priori. In fact, this assumption is
only for simplicity of the analysis and not a limitation of the greedy algorithms. For example, M-OMP recovers a support
of unknown size by running the steps until the residual ∥P ⊥
AJ Y ∥F falls below a certain threshold that depends on the noise
level (or vanishes, in the noiseless case). SS-OMP recovers a support of unknown size similarly but a different criterion may
be used for the stopping condition. Assume that the estimated signal subspace bS satisﬁes dim(bS) ≤dim(S). After k steps,
SS-OMP returns bJ of size k. Whether to continue to the next step is determined by the stopping condition given by
∢2(bS, R(AJ)) = ∥P ⊥
for a threshold η. For the noiseless case, η is set to 0 and, for the noisy case, η is set to an estimate of ∢2(bS, R(AJ0)). Let
us consider the solution bJNS given by the enumeration of all possible supports:
J⊂[n],|J|≥r
subject to
∢2(bS, R(AJ)) ≤η.
We assume that | bJNS| = s. Otherwise, in the noiseless case, this implies that the support is not uniquely determined. If
| bJNS| = s, then SS-OMP stops after s steps whenever it is guaranteed to recover the support with known s.
Similarly, SA-MUSIC with SS-OMP (SA-MUSIC+SS-OMP henceforth) can recover the support without knowledge of s
by applying the same stopping criterion for SS-OMP, which is summarized in Algorithm 6. The update criterion in Step 9
of Algorithm 6 determines the SA-MUSIC algorithm. For example, SA-MUSIC+SS-OMP uses the condition in (5.5) whereas
SA-MUSIC with SS-OMSP (SA-MUSIC+SS-OMSP henceforth) uses the condition in (5.7). If | bJNS| = s, then SA-MUSIC
algorithms return support of size s whenever they are guaranteed to recover the support with known s. For simplicity, the
analyses in Section VII will assume known sparsity level.
Algorithm 6 SA-MUSIC for unknown s
Input: Y ∈Km×N, A ∈Km×n, η > 0.
Output: J ⊂[n]
S ∈Rm×m, r ∈N ←estimate signal subspace from Y ;
3: for ℓ= 1, . . . , n do
Saℓ∥2/∥aℓ∥2
5: end for
6: J ←indices of the r-largest ζk’s;
8: while ∥P ⊥
Select k by an update criterion;
J1 ←J1 ∪{k};
S AJ1)(P ⊥
for ℓ∈[n] \ J1 do
Saℓ∥2/∥aℓ∥2
J ←J1 ∪{indices of the r-largest ζℓ’s};
16: end while
17: return J
VI. WEAK RESTRICTED ISOMETRY PROPERTY
A. Uniform Restricted Isometry Property
The restricted isometry property (RIP) has been proposed in the study of the reconstruction of sparse vectors by ℓ1 norm
minimization . Matrix A ∈Km×n satisﬁes the RIP of order s if there exists a constant δ ∈(0, 1) such that
(1 −δ)∥x∥2
2 ≤(1 + δ)∥x∥2
∀x, ∥x∥0 ≤s.
The smallest δ that satisﬁes (6.1) is called the restricted isometry constant (RIC) of order s and is denoted by δs(A). Note
that (6.1) is equivalent to
(1 −δ)Is ≤A∗
JAJ ≤(1 + δ)Is,
∀J ⊂[n], |J| = s
and hence δs(A) satisﬁes
δs(A) = max
The RIP of order s implies that all submatrices of A with s columns are uniformly well conditioned.
B. Weak Restricted Isometry Property
In many analyses of sparse signal recovery, the uniform RIP is unnecessarily strong and requires a demanding condition on
δs(A) that is not satisﬁed by the matrices that arise in applications. Therefore, weaker versions of RIP have been proposed,
tailored to speciﬁc analyses , .
Matrix A ∈Km×n satisﬁes the weak restricted isometry property (weak RIP) with parameter (J, s, t, δ), where s, t ∈N,
J ⊂[n] with |J| = s, and δ ∈(0, 1), if
(1 −δ)Is+t ≤A∗
KAK ≤(1 + δ)Is+t,
∀K ⊃J, |K| = s + t.
The corresponding weak restricted isometry constant is given by
s+t (A; J) =
KAK −Is+t∥.
The special case of the weak RIP with t = 1 has been previously proposed to derive an average case analysis of the
solution of the MMV problem by the mixed ℓ2,1 norm minimization, also known as MMV basis pursuit (M-BP) . This
speciﬁc case of the weak RIP with t = 1, which we call the weak-1 RIP, is useful for the analysis in this paper. Obviously,
the weak-1 RIP is satisﬁed by a less stringent condition on A. In the following, we list some matrices that satisfy the weak-1
RIP along with the required conditions 7. Importantly, in addition to Gaussian matrices, which lend themselves to relatively
easy analysis, this list includes deterministic matrices that arise in applications, and provides reasonable constants for them.
C. Gaussian Matrix
Eldar and Rauhut derived a condition for the weak-1 RIP of an i.i.d. Gaussian matrix [42, Proposition 5.3]. Their proof starts
with the concentration of the quadratic form ∥Gx∥2
2 around its expectation where G is an i.i.d. Gaussian matrix, to bound the
singular values of G, which has been originally proposed in . We provide an alternative and much tighter condition for
the weak-1 RIP of A directly using the concentration of the singular values of an i.i.d. Gaussian matrix .
Proposition 6.1: Given m, n ∈N with m ≤n, let A ∈Rm×n be an i.i.d. Gaussian matrix whose entries follow N(0, 1
Suppose that J is a subset of [n] with s elements. For any ǫ, δ ∈(0, 1), if
s+1 (A; J) ≥δ) ≤ǫ,
where the probability is with respect to A.
Proof: See Appendix C.
Remark 6.2: For large s such that the log term is negligible, Condition (6.4) reduces to m/s ≥(
1 + δ −1)−2. For δ ≈1,
the oversampling factor is approximately 6.
Remark 6.3: Using the concavity of the square root function, it follows that
1 + δ −1)2
is a sufﬁcient condition for (6.4) and hence also guarantees (6.5).
Remark 6.4: By slightly modifying the proof of Proposition 6.1, we obtain the uniform RIP of A: if
1 + δ −1)2
7 We show that, compared to the uniform RIP, the requirement on the number of measurements to satisfy the weak-1 RIP is reduced by large factors,
ranging between 200 to thousands fold.
P(δs(A) ≥δ) ≤ǫ,
where the probability is with respect to A. Compared to Condition (6.6) required for the weak-1 RIP, in Condition (6.7) for
the uniform RIP, the required oversampling factor m
s has been increased roughly by the factor 3 + 2 ln
We also consider variations of the asymmetric RIP , . Similarly to the weak-1 RIP, A satisﬁes the weak-1 asymmetric
RIP if there exist α, β > 0 such that
α ≤σs+1(AJ∪{j}) ≤σ1(AJ∪{j}) ≤β,
∀j ∈[n] \ J.
The corresponding weak-1 asymmetric RICs are deﬁned as follows:
s+1(A; J) ≜
j∈[n]\J σs+1(AJ∪{j}),
s+1 (A; J) ≜max
j∈[n]\J σ1(AJ∪{j}).
Proposition 6.5: Given m, n ∈N with m ≤n, let A ∈Rm×n be an i.i.d. Gaussian matrix whose entries follow N(0, 1
Suppose that J is a subset of [n] with s elements. For any ǫ, γ ∈(0, 1), if
s+1(A; J) ≤1 −γ] ∨[βweak
s+1 (A; J) ≥1 + γ]
where the probability is with respect to A.
Proof: See Appendix D.
Remark 6.6: For large s such that the log term is negligible, Condition (6.8) reduces to m
γ2 . For γ ≈1, the oversampling
factor is approximately 1.
Remark 6.7: Using the concavity of the square root function, it follows that
is a sufﬁcient condition for (6.8) and hence also guarantees (6.9).
Remark 6.8: Proposition 6.5 provides a sufﬁcient condition, which is not necessarily tightest, in particular, in the limit when
γ →1. Indeed, an i.i.d. Gaussian A satisﬁes αweak
s+1(A; J) > 0 with probability 1 if m > s, but Condition (6.8) does not
converge to m > s as δ approaches 1. Nevertheless, this gap vanishes if s goes to inﬁnity with n = o(es), that is, s grows
faster than ln n.
D. Uniformly Random Partial Fourier Matrix
Candes and Plan showed that a matrix A composed of randomly selected rows of a DFT matrix satisﬁes the following
local restricted isometry property under a certain mild condition: ∥A∗
JAJ −Is∥≤δ with high probability for a ﬁxed J ⊂[n]
of size s ([55, Lemma 2.1]) 8. It is not difﬁcult to derive the weak-1 RIP of such a matrix from its local RIP. We only need to
consider the union of the events corresponding to all subset of [n] that include the support J and one more element outside J.
Proposition 6.9 (Corollary to [55, Lemma 2.1]): Suppose that A is obtained by randomly selecting m rows of the n × n
DFT matrix independently, each with probability m
n , followed by normalization of each column in ℓ2 norm. Also suppose that
J is a ﬁxed subset of [n] of cardinality s. For any ǫ, δ ∈(0, 1), if
+ ln(s + 1)
s+1 (A; J) ≥δ) ≤ǫ.
where the probability is with respect to A.
Proof: See Appendix E.
Remark 6.10: The uniform RIP of a random partial Fourier matrix has been studied before , , . In particular,
Rauhut [62, Theorem 8.4] showed a sufﬁcient condition with explicit constants given by
ln(10m) ≥Cδ−2 ln2(100s) ln(4n)s,
m ≥Dδ−2 ln(ǫ−1)s,
where C ≤17, 190 and D ≤456, which is a much more demanding condition than (6.11).
Remark 6.11: As discussed in the previous subsection, the algebraic analogue of the weak-1 RIP condition is usually much
easier to satisfy. Feng and Bresler showed that a matrix A composed of m consecutive rows of the n × n DFT matrix
with m ≥s + 1 9 satisﬁes
J⊂[n],|J|=s αweak
s+1(A; J) > 0
Candes et al. showed that if n is a prime number, then the above result holds for any m rows. Note that m ≥s + 1 is a
much milder requirement than (6.11) and the property is deterministic, i.e., holds always. However, the properties mentioned
in this remark are purely algebraic and cannot be used in the analysis with noise.
E. Incoherent Unit-Norm Tight Frame with Random Support
Incoherent unit-norm tight frames also satisfy the weak-1 RIP with high probability under a mild condition if the
support J is uniformly random. Let A ∈Km×n be a unit-norm tight frame. Then, each column of A has unit ℓ2 norm and
8 In fact, the result in and hence our argument derived from their result apply to a wider class of matrices.
9The selection of the rows in this pattern was called the “bunched sampling pattern” in 
the rows of A are orthogonal . A unit-norm tight frame also satisﬁes ∥A∥= p n
m. The coherence µ of A is deﬁned by
|⟨ak, aℓ⟩|
∥ak∥2∥aℓ∥2
and is always bounded from below by the Welch bound 
For example, the rows of DFT matrix selected by the difference set method form a unit-norm tight frame that achieves
the Welch bound.
Based on the statistical RIP analysis in , we derive the following result.
Proposition 6.12: Suppose that A ∈Km×n is a ﬁxed unit-form tight frame with coherence µ satisfying
for some constant K > 0. Also suppose that J is uniformly random among all subsets of [n] with s elements. For any
ǫ, δ ∈(0, 1), if
s + 288K2 ln
s+1 (A; J) ≥δ) ≤ǫ.
where the probability is with respect to J.
Proof: See Appendix F.
Remark 6.13: If A achieves the Welch bound, then the constant K in Proposition 6.12 is no greater than 1.
Remark 6.14: By slightly modifying the proof of Proposition 6.12, we obtain the uniform RIP of A
P(δs(A) ≥δ) ≤ǫ
1 + 576K2 ln
s + 288K2 ln
Compared to the weak-1 RIP, for the uniform RIP, the oversampling factor
s has increased roughly by the factor 1 +
VII. PERFORMANCE GUARANTEES
A. MUSIC for the Full Row Rank Case
With an imperfect estimate of the signal subspace, the support recovery by MUSIC is no longer guaranteed by an algebraic
property of A. Instead, in the following proposition, we provide a new guarantee.
Theorem 7.1 (MUSIC, noisy, full row rank case): Assume that X0 ∈Kn×N is row s-sparse with support J0 ⊂[n] and XJ0
has full row rank. Let bS be an s-dimensional estimate of S ≜R(AJ0XJ0
0 ) such that ∥Pb
S −PS∥≤η for some η < 0.5. If A
satisﬁes one direction of the weak-1 asymmetric RIP
s+1(A; J0) > α
η(1 −η)∥A∗∥2,∞,
then MUSIC applied to Pb
S will identify J0.
Proof: See Appendix G.
Remark 7.2: The columns of the sensing matrix A are often normalized in the ℓ2 norm (e.g., for a partial Fourier matrix) or
their ℓ2 norms are highly concentrated around 1 (e.g., i.i.d. for Gaussian matrix). We therefore consider the quantity ∥A∗∥2,∞
to be 1 or close to 1. In particular, we assume that all columns of A are normalized in the ℓ2 norm (“A is normalized,” in
short) in all the ﬁgures and the numerical experiments in this paper.
Remark 7.3: With normalized A, we have ∥A∗∥2,∞= 1, and the weak-1 RIP δweak
s+1 (A; J0) < 1−α2 is a sufﬁcient condition
for (7.1).
Remark 7.4: When the signal subspace estimation is perfect (i.e., in the noiseless case), Conditions (7.1) and (7.2) reduce to
s+1(A; J0) > 0, which is an algebraic condition implying rank[AJ0, aj] = s+1 for all j ∈[n]\J0. This algebraic condition
is implied by a much milder condition than the weak-1 asymmetric RIP, which is an analytic condition. For example, an i.i.d.
Gaussian A with m ≥s + 1 satisﬁes this with probability 1.
Remark 7.5: Theorem 7.1 guarantees that MUSIC recovers a ﬁxed support J0. Replacing Condition (7.1) by its uniform
analog, σs+1(AJ) > α for all J ⊂[n] with |J| = s + 1, provides a uniform guarantee that MUSIC recovers an arbitrary
support of size s. With perfect subspace estimation, the uniform guarantee reduces to Proposition 4.2.
B. SA-MUSIC with Given Partial Support
SA-MUSIC ﬁnds the support by using the augmented subspace eS constructed as eS = R(AJ1) + bS, where J1 is a subset of
J0 of size s−r and bS is the estimated signal subspace of dimension r. We assume that there exists an r-dimensional subspace
¯S of the signal subspace S ≜R(AJ0XJ0
0 ) satisfying ∥Pb
S −P ¯S∥≤η. It will be shown in Section VIII that, if the number
of snapshots is large enough relative to the noise variance, then Algorithm 1 computes bS with the property that such an ¯S
exists. Recall (by Proposition 5.6) that assuming row-nondegenerate XJ0
implies R(AJ1) + ¯S = R(AJ0), which is desired by
the MUSIC step in SA-MUSIC. However, because eS is constructed using bS rather ¯S, which is not available, to show noise
robustness of support recovery, we need to bound ∥Pe
S −PR(AJ0 )∥. By the projection update formula, it follows that
S −PR(AJ0 ) = PP ⊥
R(AJ1 ) bS −PP ⊥
R(AJ1 ) ¯S.
Therefore, we need to consider the distance between the two subspaces bS and ¯S as projected onto R(AJ1)⊥. However, in
general, projecting onto another subspace can either increase or decrease the distance between subspaces arbitrarily. In our
speciﬁc case, the distance is bounded depending on the condition number of AJ0. We state the result in a formal way in the
following proposition.
Proposition 7.6: Assume that X0 ∈Kn×N is row s-sparse with support J0 ⊂[n] and XJ0
is row-nondegenerate. Let bS
be the estimated signal subspace of dimension r where r < s. Suppose that there exists an r-dimensional subspace ¯S of the
signal subspace S ≜R(AJ0XJ0
0 ) satisfying ∥Pb
S −P ¯S∥≤η. Let J be a proper subset of J0. If A satisﬁes
σ1(AJ0) > η,
R(AJ ) bS −PP ⊥
R(AJ ) ¯S∥≤
σs(AJ0) −ησ1(AJ0).
Proof: See Appendix H.
We are now ready to state one of the main results of this paper.
Theorem 7.7 (SA-MUSIC with Correct Partial Support): Assume that X0 ∈Kn×N is row s-sparse with support J0 ⊂[n]
is row-nondegenerate. Let bS be the estimated signal subspace of dimension r, where r < s. Suppose that there exists
an r-dimensional subspace ¯S of the signal subspace S ≜R(AJ0XJ0
0 ) satisfying ∥Pb
S −P ¯S∥≤η. Let J1 be an arbitrary subset
of J0 with s −r elements. If A satisﬁes the weak-1 asymmetric RIP
s+1 (A; J0) ≤βweak
s+1 (A; J0) < β
for α and β satisfying
then MUSIC applied to eS = bS + R(AJ1) using the criterion
will identify J0 \ J1.
Proof: See Appendix I.
Remark 7.8: With normalized A, Condition (7.5)+(7.6) is implied by the weak-1 RIP of A given by δweak
s+1 (A; J0) < δ for
1 + δ · 1 −
Remark 7.9: Compared to the guarantee on MUSIC (full row rank case, Theorem 7.1), the guarantee for SA-MUSIC (for the
rank defective case, Theorem 7.7) additionally requires XJ0
to be row-nondegenerate. For the noiseless case, with normalized
A, both Theorem 7.1 and Theorem 7.7 require only a mild algebraic condition αweak
s+1(A; J0) > 0. However, as shown in Fig. 1,
even with known partial support SA-MUSIC for the rank defective case suffers more from the perturbation in the subspace
estimate than does MUSIC in the full row rank case. This difference in the sensitivity to the subspace estimation error is due
to the reduced dimension r < s of the estimated signal subspace, which in turn is due to the rank defect of XJ0
Remark 7.10: Theorem 7.7 provides a performance guarantee for SA-MUSIC with a correct partial support estimate, but
noisy subspace estimate. In this scenario, SA-MUSIC provides its best performance. How realistic is this assumption? In the
next subsections, we will show that if the error in the subspace estimate is small enough, suboptimal greedy algorithms are
indeed guaranteed to recover the partial support exactly. In particular, when r/s is large, SA-MUSIC combined with partial
support recovery by greedy algorithms provides guarantees comparable to that given in Theorem 7.7 for SA-MUSIC with
correct partial support estimate.
Comparison of MUSIC for full rank case vs. SA-MUSIC with known partial support in the rank defective case: trade-off between parameter δ (for
the weak-1 RIP) and η (for subspace estimate perturbation). The region below the curve provides a guarantee.
C. SA-MUSIC with Partial Support Recovery by SS-OMP
We ﬁrst propose a new sufﬁcient condition for a guarantee of M-OMP, which is of independent interest in its own right.
Proposition 7.11 (M-OMP): Suppose that X0 ∈Kn×N is row s-sparse with support J0 ⊂[n]. Let Y = AX0 + W. Given
J ⊊J0, if A satisﬁes the weak-1 RIP
s+1 (A; J0) < δ
∥2,∞−2∥A∗W∥2,∞
then the next step of M-OMP will identify an element of J0.
Proof: See Appendix J.
Remark 7.12: Applying previous analysis [26, Theorem 5] of p-SOMP to the case p = 2 provides an alternative guarantee
for M-OMP. However, the conditions for the latter are stated in terms of the p-Babel function and its variations rather than
the weak-1 RIP as in our Proposition 7.11. The two guarantees are not directly comparable, but neither one is uniformly less
or more demanding than the other in terms of the conditions required. Because the form of the condition in Proposition 7.11
is amenable to our analysis, it is preferred in this paper.
Remark 7.13: Unlike MUSIC, M-OMP is guaranteed only if the orthogonality between two vectors is nearly preserved
through A. Therefore, unlike MUSIC, M-OMP is not guaranteed by one direction of the weak-1 asymmetric RIP.
Remark 7.14: For the noiseless case (W = 0), since
it follows that Condition (7.9)+(7.10) is implied by δweak
s+1 (A; J0) <
2√s, which is a weaker requirement than the previously
known sufﬁcient condition δs+1(A) <
Next, the guarantee of SS-OMP is obtained as a corollary to Proposition 7.11. Its main utility is as a stepping stone to the
guarantee for SA-MUSIC with partial support recovery by SS-OMP.
Corollary 7.15 (SS-OMP): Assume that X0 ∈Kn×N is row s-sparse with support J0 ⊂[n] and let bS be the estimated signal
subspace of dimension r ≤s. Suppose that there exists an r-dimensional subspace ¯S of the signal subspace S ≜R(AJ0XJ0
satisfying ∥Pb
S −P ¯S∥≤η. Let Φ ∈Ks×r satisfy Φ∗Φ = Ir, and ¯S = R(AJ0Φ). Let ρk(Φ) denote the k-th largest ℓ2 norm
of the rows of Φ. If A satisﬁes the weak-1 RIP
s+1 (A; J0) < δ
for δ satisfying
1 −δ ≥2η∥A∗∥2,∞,
then the ﬁrst k steps of SS-OMP will identify k elements of J0.
Proof: See Appendix K.
The conditions for bS in terms of ¯S and Φ appear rather technical, but we will show that they are satisﬁed by our proposed
subspace estimation scheme. Furthermore, their implications are interpreted in the following remarks.
Remark 7.16: Since ρk(Φ) corresponds to the ℓ2 norm of a row of Φ, ρk(Φ) does not change by applying a rotation to the
right of Φ. In other words, for ﬁxed ¯S, ρk(Φ) is invariant to the choice of Φ. Now, since ρk(Φ) is monotonically decreasing in
k, the sufﬁcient condition (7.11)–(7.12) of Corollary 7.15 gets more stringent (requires a smaller weak-1 RIC) as k increases
toward s. This implies that the smaller the size of the partial support that is to be recovered by SS-OMP, the less stringent the
condition for the guarantee on the recovery. Moreover, for k > s−r, the guarantee of Corollary 7.15 requires a more stringent
condition than the guarantee on the remaining step of SA-MUSIC in Theorem 7.7, where a partial support of at least size s−r
is assumed to be known. For these two reasons, switching to SA-MUSIC applied to the augmented subspace after successful
partial support recovery of size exactly k = s −r by SS-OMP is preferred rather than continuing any of the remaining steps
of SS-OMP.
Clearly, the smaller ρs−r(Φ), the more stringent the condition on δweak
s+1 (A; J0). We therefore provide a deterministic lower
bound on ρs−r(Φ). The bound is based on the Cauchy-Binet formula, and holds for r/s > 0.5. To facilitate the interpretation
of the condition, the bound of Lemma 7.17 is visualized in Fig. 3.
Lemma 7.17: For r, s ∈N where s/2 < r < s, let Φ ∈Ks×r satisfy Φ∗Φ = Ir, and let ρk(Φ) denote the k-th largest ℓ2
norm of the rows of Φ. Then,
ρs−r(Φ) ≥ρ(s, r) ≜sup
ˆρ(s, r, q)
where ˆρ(s, r, q) is deﬁned by
ˆρ(s, r, q) ≜
for q > 0.
Proof: See Appendix L.
Lower bound ˆρ(s, r, q) on ρs−r(U) by Lemma 7.17 with q = 10−3.
Combining Lemma 7.17 and Corollary 7.15 (for the success of the ﬁrst s−r steps of SS-OMP) with Theorem 7.7, we obtain
another of our main results: the following theorem provides a deterministic performance guarantee of SA-MUSIC combined
with SS-OMP. The proof is straightforward and therefore omitted.
Theorem 7.18 (SA-MUSIC+SS-OMP, rank-defective case): Assume that X0 ∈Kn×N is row s-sparse with support J0 ⊂[n]
is row-nondegenerate. Let bS be the estimated signal subspace of dimension r, where s/2 < r ≤s. Suppose that
there exists an r-dimensional subspace ¯S of the signal subspace S ≜R(AJ0XJ0
0 ) satisfying ∥Pb
S −P ¯S∥≤η. If A satisﬁes
the weak-1 RIP
s+1 (A; J0) < δ
for δ satisfying
1 −δ ≥2η∥A∗∥2,∞
where ρ(s, r) is deﬁned in Lemma 7.17, then SA-MUSIC+SS-OMP applied to bS will identify J0.
As discussed in Remark 5.4, the row-nondegeneracy condition on XJ0
is a mild condition essential to SA-MUSIC and
likewise Condition (7.15) + (7.16) is inherited from SA-MUSIC (see Theorem 7.7 and Remark 7.3). Condition (7.15) + (7.17),
on the other hand, is due to the use of SS-OMP, a suboptimal algorithm, for the partial support recovery (see Corollary 7.15).
For the noiseless case (η = 0), with normalized A, (7.16) reduces to δ ≤1, which is a necessary condition for (7.17). Therefore,
it sufﬁces to satisfy (7.15)+(7.17), where (7.17) reduces to
Required weak-1 RIC for the guarantee for SA-MUSIC+SS-OMP, rank-defective case in Theorem 7.18 (η = 0). The dot at the top right of the plot
represents the weak-1 RIP αweak
s+1(A, J0) > 0 required in the full rank case, r = s.
As shown in Fig. 3, the weak-1 RIC δ required by (7.18) increases with increasing r/s. In other words, SA-MUSIC+SS-
OMP beneﬁts from higher dimension of the signal subspace S. In particular, when r = s, SA-MUSIC reduces to MUSIC for
the full rank case and hence it sufﬁces to satisfy δweak
s+1 (A; J0) < δ = 1, which is plotted as the dot at the top right in Fig. 3.
Fig. 4 visualizes the trade-off between δ and η given by Conditions (7.16) and (7.16) for the noisy case. Given r/s, any pair
(η, δ) below the curve determined by r/s guarantees SA-MUSIC+SS-OMP. As r/s increases, the curve shifts upward and the
guarantee is given by larger δ and/or larger η. In particular, when r/s = 1, SA-MUSIC reduces to MUSIC without the step
of partial support recovery and hence, by Theorem 7.1, the trade off is given by
which is also plotted in Fig. 4.
D. SA-MUSIC with Partial Support Recovery by SS-OMSP
Proposition 7.19 (SS-OMSP, rank-defective case): Assume that X0 ∈Kn×N is row s-sparse with support J0 ⊂[n] and
is row-nondegenerate. Let bS be the estimated signal subspace of dimension r, where r ≤s. Suppose that there exists an
r-dimensional subspace ¯S of the signal subspace S ≜R(AJ0XJ0
0 ) satisfying ∥Pb
S −P ¯S∥≤η. Given J ⊊J0, if A satisﬁes
the weak-1 asymmetric RIP
s+1 (A; J0) ≤βweak
s+1 (A; J0) < β
Trade-off between parameters δ (for the weak-1 RIP) and η (for subspace estimate perturbation) for the guarantee of SA-MUSIC+SS-OMP in
Theorem 7.18. Values (η, δ) in region below the curve provide a guarantee.
for α and β satisfying
R(AJ ) ¯S)
then the next step of SS-OMSP will identify an elements of J0 \ J. For the special case when r = s, Condition (7.20) is
replaced by the weaker condition
Proof: See Appendix M
In the full row rank case, XJ0
is trivially row-nondegenerate. In the noiseless case, we have η = 0 and with normalized
A, Condition (7.21) reduces to αweak
s+1(A; J0) > 0. Therefore, for the full rank and noiseless case, SS-OMSP is guaranteed by
s+1(A; J0) > 0, which coincides with the condition for the guarantee of MUSIC in the same scenario (Proposition 4.2). In
fact, in this noiseless and full row rank case, SS-OMSP is equivalent to the corresponding data domain algorithm, RA-ORMP.
The coincidence of the guarantees of MUSIC and of RA-ORMP in this special case has been shown before . (Unlike the
analysis of RA-ORMP though, Proposition 7.19 also applies to the noisy and/or rank-defective cases.)
is row-nondegenerate, then by Proposition 5.6, dim(R(AJ)+ ¯S) = s for any J ⊂J0 with |J| = s−r, which implies
R(AJ ) ¯S) = r. Then, we also have dim(P ⊥
R(AJ) ¯S) = r for any |J| ≤s −r. Combining this result with Proposition 5.6,
Proposition 7.19, and Theorem 7.7, we obtain another main result of this paper: a guarantee for SA-MUSIC+SS-OMSP.
Theorem 7.20 (SA-MUSIC+SS-OMSP, rank-defective case): Assume that X0 ∈Kn×N is row s-sparse with support J0 ⊂[n]
is row-nondegenerate. Let bS be the estimated signal subspace of dimension r, where r ≤s. Suppose that there exists
an r-dimensional subspace ¯S of the signal subspace S ≜R(AJ0XJ0
0 ) satisfying ∥Pb
S −P ¯S∥≤η. If A satisﬁes the weak-1
asymmetric RIP
s+1 (A; J0) ≤βweak
s+1 (A; J0) < β
for α and β satisfying
then SA-MUSIC+SS-OMSP applied to bS will identify J0.
Remark 7.21: With normalized A, Condition (7.23) is implied by Condition (7.24), which means that partial support recovery
by SS-OMSP requires more stringent conditions than the subsequent MUSIC step in the guarantee of SA-MUSIC+SS-OMSP.
This results in the same guarantee for SA-MUSIC+SS-OMSP as for SS-OMSP alone. However, in the numerical experiments
in Section IX, the two algorithms exhibited substantially different performance. To interpret this, we compare SA-MUSIC and
SS-OMSP conditioned on the event that a correct partial support of size s −r has been found. By the row-nondegeneracy
condition on XJ0
0 , it follows that dim(P ⊥
R(AJ) ¯S) = s −|J| for all J ⊂J0 with |J| ≥s −r. Therefore, with known partial
support, the remaining steps of SS-OMSP are guaranteed by
which is obtained from Condition (7.20) with dim(P ⊥
R(AJ ) ¯S) = s−|J|. In contrast, as shown in Fig. 5, when a partial support
of size s −r is given, the condition in (7.8) for the guaranteed success of SA-MUSIC is substantially milder.
Comparison of SA-MUSIC and SS-OMSP when a partial support of size s −r is given: trade-off between parameter δ (for the weak-1 RIP) and η
(for subspace estimate perturbation). The region below the curve provides a guarantee.
Remark 7.22: With normalized A, Condition (7.22)+(7.23)+(7.24) is implied by the weak-1 RIP of A given by δweak
s+1 (A; J0) <
δ for δ satisfying
Furthermore, if we assume η = 0, then the guarantee for SA-MUSIC+SS-OMSP only requires
s+1 (A; J0) <
which as shown in Fig. 6 becomes less demanding as r/s increases.
Required δ (for the weak-1 RIP) for the guarantee of SA-MUSIC+SS-OMSP in Theorem 7.20 for the noiseless case (η = 0). The dot at the top
right of the plot represents the weak-1 RIP αweak
s+1(A, J0) > 0 required in the full rank case, r = s.
For the noisy case, Conditions (7.23) and (7.24) provide a trade-off between the parameters δ and η for the guarantee of
SA-MUSIC+SS-OMSP, which is visualized in Fig. 7. As r/s increases, SA-MUSIC+SS-OMSP beneﬁts from higher dimension
of the signal subspace bS. Compared to the guarantee of SA-MUSIC+SS-OMP in Fig. 4, SA-MUSIC+SS-OMSP is guaranteed
by a weaker requirement on A in terms of the weak-1 RIP.
Trade-off between parameters δ (for the weak-1 RIP) and η (for subspace estimate perturbation) for the guarantee of SA-MUSIC+SS-OMSP in
Theorem 7.20. Values (η, δ) in region below the curve provide a guarantee.
E. Implication of the Guarantees in Weak-1 RIP to the Oversampling Factor
The results in the previous subsections were stated in terms of the weak-1 RIP. Given an upper bound on δweak
s+1 (A; J0) (or
bounds on αweak
s+1(A; J0) and βweak
s+1 (A; J0)), Section VI then provides explicit conditions on the parameters n, m, s that provide
the weak-1 RIP for the matrices A discussed there.
Example 1 (i.i.d. Gaussian A, asymptotic case) In the ﬁrst example, we consider asymptotic analysis with an i.i.d. Gaussian
A. By Proposition 6.5, if n and s go to inﬁnity while satisfying n = o(es), i.e., s grows faster than ln n, and for γ ∈(0, 1)
1 −γ < αweak
s+1 (A; J0) ≤βweak
s+1 (A; J0) < 1 + γ
with probability 1. Furthermore, in this asymptotic, ∥A∗∥2,∞= 1 with probability 1. Assume that the estimated signal subspace
bS is error-free (η = 0). Then, for the full row rank case, i.e., dim(bS) = s, all SA-MUSIC algorithms reduce to MUSIC without
partial support recovery and are guaranteed by m > s, which is also a necessary condition for the support recovery. On the
other hand, if dim(bS) = r < s, then SA-MUSIC+SS-OMSP is guaranteed by
which unfortunately does not converges to m > s as r/s →1. The discontinuity at r/s = 1 is the limitation of the current
analysis in this paper. However, it provides a valid upper bound on the oversampling factor m/s in the given asymptotic.
Example 2 (structured A, non-asymptotic case) In the second example, we perform the analysis with matrices that arise
in practical applications (e.g., spectrum blind sampling or DOA estimation). We use the results in Section VI for incoherent
unit-norm tight frames and partial Fourier matrices. Fortunately, these matrices have normalized columns and we do not need to
worry about ∥A∗∥2,∞any longer. Given δ, the weak-1 RIP, δweak
s+1 (A; J0) < δ (or the weak-1 asymmetric RIP, αweak
s+1(A; J0) < α)
holds with probability 1 −ǫ if
m ≥C1(s + C2).
For an incoherent unit-norm tight frame, C1 is a constant that depends only on δ. However, for the random partial Fourier
case, C1 also depends on ln(s + 1), ln(n −s), and ǫ. We summarize the explicit formulae for C1 and C2:
• Random partial Fourier (Proposition 6.9)
C1 = 2(3 + δ)
+ ln(s + 1)
• Incoherent unit-norm tight frame with random support (Proposition 6.12)
C2 = 288K2 ln
where K is determined by the coherence of A (e.g., K = 1 if A achieves the Welch bound).
Substituting δ (for the weak-1 RIP) into these expressions determines the explicit scaling of m versus s and the other
problem parameters that will provide guaranteed recovery. As r/s increases, δ increases and hence the oversampling factor C1
in (7.26) decreases. In particular, when r = s, SA-MUSIC reduces to MUSIC without need of any partial support recovery
and hence is guaranteed by the weak-1 asymmetric RIP αweak
s+1(A; J0) > 0, which corresponds to C1 = 1 and C2 = 1 in (7.26).
In the presence of perturbation (η > 0), Figs. 4 and 7 provide the required δ for the guarantees and the oversampling factor
is computed from δ similarly. The relation between η and the number of snapshots N will be investigated in the next section.
VIII. ANALYSIS OF SIGNAL SUBSPACE ESTIMATION
Unlike the previous works in sensor array processing which relies on asymptotics, we analyze the perturbation in the
estimate of the signal subspace with ﬁnitely many observations. Combined with the results in Section VII, this analysis provides
non-asymptotic guarantees in the noisy case for the new proposed algorithms directly in terms of the measurement noise. The
results also enable us to extend the previous performance guarantees , , of MUSIC to the noisy and ﬁnite snapshot
case, which was missing before.
Assumption 1 (Noise) Given row s-sparse X0 ∈Kn×N with support J0, the snapshot matrix Y ∈Km×N is obtained with
sensing matrix A ∈Km×n as
Y = AJ0XJ0
where the columns of W are independent realizations of Gaussian vector w ∈Km with Ew = 0 and Eww∗= σ2
wIm. For the
complex ﬁeld case, we assume circular Gaussian distribution. We assume that AJ0 has full column rank.
Assumption 2 (Number of snapshots) We assume that the number of snapshots N is large but ﬁnite, more speciﬁcally, N
satisﬁes N ≥m. It is also assumed that m > s which is required for support recovery by any method.
Our assumption on the number of snapshots is motivated by the following considerations. In compressed sensing, the goal is
usually to minimize the number of expensive measurements. Now, in certain applications of joint sparse recovery, taking many
snapshots is a rather trivial task compared to acquiring many measurements in a single snapshot. For example, in spectrum-blind
sampling , the number m of measurements per snapshot m determines the sampling rate, the increase of which is usually
expensive and limited by hardware. In contrast, taking many snapshots only results in delay in the support recovery and is
usually less expensive than raising the sampling rate. Similarly, in DOA estimation and in distributed sensor networks ,
increasing m requires more sensors, which is expensive, whereas increasing the number of snapshots N corresponds to delay
in estimation, which is relatively less expensive. This motivates the setting N ≥m in the analysis of this subsection.
Assumption 3 (Signal) We assume that the nonzero rows of X0 follow the mixed multichannel model given by
where Ψ ∈Ks×M with M ≤s is a mixing matrix that has full column rank, Λ ∈RM×M is a deterministic, positive, and
diagonal matrix, and the elements of Φ ∈KM×N are independent zero mean and unit variance Gaussian random variables.
Note that rank(XJ0
0 ) = rank(Φ) = M with probability 1. We assume that Φ is independent of W. The rows of ΛΦ correspond
to realizations of M statistically independent sources, where the diagonal entries of Λ represent the magnitudes of the sources.
The columns of XJ0
in this model are independent realizations of Gaussian vector x ∈Ks with Ex = 0 and Exx∗= ΨΛ2Ψ∗.
The mixed multichannel model generalizes the multichannel model proposed for the average case analysis of various
methods for joint sparse recovery. With Ψ = Is, the mixed multichannel model reduces to the multichannel model. However,
with a rectangular mixing matrix Ψ ∈Ks×M for M < s, the mixed multichannel model can describe the “rank defect”, which
is due to the correlation between the mixed sources, i.e., between the rows of XJ0
0 . Such correlation, which often arises in
the above mentioned applications, cannot be represented by the multichannel model, in which XJ0
has full row rank with
probability one for N ≥s.
With the mixed multichannel model, the N columns of AJ0XJ0
∈Km×N are independently distributed Gaussian vectors
with zero mean and covariance matrix
Γ ≜AJ0ΨΛ2Ψ∗A∗
Assumption 4 (Covariance matrix) We assume that there exists a signiﬁcant gap between at least one pair of consecutive
eigenvalues of Γ, more speciﬁcally, the covariance matrix Γ satisﬁes the following conditions given by the parameters r ∈N
and τ, ν, θ ∈(0, 1):
(1 −θ)λr(Γ) −(1 + θ)λr+1(Γ)
≥(1 + θ)(1 + ν)τλ1(Γ)
(1 + θ)λk(Γ) −(1 −θ)λk+1(Γ)
< (1 −θ)(1 −ν)τλ1(Γ),
Condition (8.1) asserts that there exists a signiﬁcant gap between two consecutive eigenvalues λr(Γ) and λr+1(Γ). Condition
(8.2) asserts that there does not exist a signiﬁcant gap between any two consecutive eigenvalues smaller than λr(Γ). Together,
the two conditions imply that r is the maximal value that satisﬁes (8.1) (a gap can not be both big enough and small enough
at the same time).
When Γ is well conditioned, such that its condition number κ(Γ) satisﬁes
κ2(Γ) ≜λ1(Γ)
(1 + θ)(1 + ν)τ .
then r that satisﬁes (8.1)–(8.2)) will assume its maximal value of rank(Γ) = M. In this case λr+1(Γ) = 0, and (8.2) is trivially
satisﬁed. Otherwise, we consider that Γ is ill-conditioned with one or more insigniﬁcant eigenvalues and set r to the index
of the smallest eigenvalue larger than those considered insigniﬁcant. In this case, (8.2) implies that λr+1(Γ) is bounded from
λr+1(Γ) < (1 −ν)τλ1(Γ)
Proposition 8.1: Suppose that Assumptions A1–A4 hold and deﬁne
Cη,ν,θ,τ ≜(1 + θ)τ min
Let ¯S be the subspace spanned by the r dominant eigenvectors of ΓS deﬁned by
ΓS ≜AJ0XJ0
Let ǫ, η ∈(0, 1). If the number of snapshots N satisﬁes
N > 2(m + s),
m + s + ln
then with probability 1 −ǫ, Algorithm 1 with parameter τ computes an r-dimensional subspace bS such that
S −P ¯S∥≤η.
Proof: See Appendix N.
When the noise variance σ2
w is small compared to λ1(Γ), Condition (8.6) dominates the ﬁrst two and is simpliﬁed as
w/λ1(Γ))1/2[m + s + ln(8/ǫ)]
so that the number of snapshots required for the guarantee scales linearly in m. Alternatively, in the same scenario, Proposition 8.1 implies that (8.7) holds for
m + s + ln(8/ǫ)
Deﬁne the average per-sample SNR as the ratio of the powers of the measured signal and noise,
SNR ≜E∥AJ0XJ0
Then, λ1(Γ)/σ2
w is related to the SNR by
For ﬁxed Γ and SNR, η scales proportionally to N −1/2. With more snapshots, SA-MUSIC algorithms access an estimate of
signal subspace with higher accuracy (smaller η) and hence, as shown in Figs. 4 and 7, the admissible δ increases, which
results in decrease of the required oversampling factor m/s. Eventually, as N goes to inﬁnity, the performance converges to
that in the noiseless case.
IX. NUMERICAL EXPERIMENTS
We compared the performance of the two proposed SA-MUSIC algorithms: SA-MUSIC + SS-OMP and SS-MUSIC + SS-
OMSP versus MUSIC , M-BP10, and the two subspace greedy algorithms proposed in this paper for partial support recovery:
SS-OMP and SS-OMSP. As an upper bound on the performance of SA-MUSIC, we included in the comparison SA-MUSIC
10The noise variance is given to the M-BP algorithm in the experiment.
with known (“oracle”) partial support. The sensing matrix A was generated as randomly selected m rows of the n × n DFT
matrix. The snapshot matrix Y = AX0 + Z was corrupted by additive i.i.d. circular complex Gaussian noise Z.
The algorithms were tested on random XJ0
of rank less than s. Singular vectors U0 and V0 of XJ0
generated as random orthonormal columns. In order to observe the effect of the rank-defect rather than ill-conditioning, the
singular values of XJ0
are set to a common value in the ﬁrst experiment. The performance is assessed by the rate of successful
support recovery 11. As shown in Fig. 8, MUSIC fails when rank(XJ0
0 ) < s. SS-OMP is little affected by the rank-defect, but
its performance does not improve much with increasing r.
The performance of SA-MUSIC algorithms with greedy partial support recovery improves with increasing r. SA-MUSIC +
SS-OMP and SA-MUSIC + SS-OMSP, labeled as “SA-MUSIC” and “SA-MUSIC+”, respectively, in Fig. 8, performed better
than M-BP in this experiment at much lower computational cost. SA-MUSIC with known partial support of size s−r is labeled
as “SA-MUSIC with oracle” in Fig. 8 and shows perfect recovery when m > s + 1, which is nearly the necessary condition
m > s. This suggests that the success of partial support recovery is more critical than the subsequent steps and leaves room
for improving SA-MUSIC by combining with a better algorithm for partial support recovery than SS-OMP or SS-OMSP.
For the noiseless case, the performance of SA-MUSIC+SS-OMSP and SS-OMSP coincides. However, with noise, the
performance SS-OMSP severely degrades even for the full row rank case. For the full row rank case, all algorithms except
SS-OMP and SS-OMSP (noisy case) were successful in terms of nearly achieving the necessary condition m > s. Again,
SS-OMSP is sensitive to the perturbation in the estimate of signal subspace in this case.
Regarding the computation, we compared the runtime of each algorithm by increasing the size of the problem. In this
experiment, ﬁxing n = (scale factor) × 64, we set the other parameters to s = n/16, r = ⌈7s/8⌉, and m = 2s. As shown in
Fig. 9, SA-MUSIC is about a 100 times faster than M-BP 12.
In order to see the effect of ill-conditioning, in the second experiment, we tested the algorithms on a random matrix
∈Cs×N with full row rank that has geometrically decaying singular values. The k-th largest singular value of XJ0
set as σk(XJ0
0 ) = κ−(k−1)/(s−1) for k = 1, . . . , s so that the condition number of XJ0
becomes κ. The singular vectors were
generated randomly as in the ﬁrst experiment. Fig. 10 compares the performance of the algorithms for the weak noise case.
We note that M-BP is sensitive to the ill-conditioning of XJ0
0 . When XJ0
is well conditioned with κ = 10, the dimension of
the estimated signal subspace is equal to the row rank of XJ0
(= s) and hence SA-MUSIC coincides with MUSIC without
any SS-OMP step. However, when κ = 50, the estimated rank r by using (4.4) in Algorithm 1 with τ = 103 is smaller
than rank(XJ0
0 ) and hence MUSIC suffers from the rank-defect while SA-MUSIC provides consistent performance. The
performance of M-OMP is invariant to the rank-defect but is poor compared to that of SA-MUSIC.
11 M-BP does not produce an s-sparse solution in the presence of noise. In this case, the solution by M-BP has been approximated to the nearest s-sparse
vector and the support is computed as that of the s-sparse approximation.
12 For M-BP, we used an efﬁcient implementation SPGL1 , . On the other hand, the other methods were implemented as plain Matlab script.
Therefore, the speed comparison does not unfairly favor SA-MUSIC.
success rate
success rate
success rate
success rate
success rate
success rate
Test on rank-defect, n = 128, s = 8, N = 256, Left columns (noiseless): (a) rank(XJ0
0 ) = 4. (c) rank(XJ0
0 ) = 6. (e) rank(XJ0
0 ) = 8 (full row
rank). Right columns (SNR = 30 dB): (b) rank(XJ0
0 ) = 4. (d) rank(XJ0
0 ) = 6. (f) rank(XJ0
0 ) = 8 (full row rank).
X. DISCUSSION
A. Comparison to Compressive MUSIC
An algorithm similar to SA-MUSIC named “Compressive MUSIC” (CS-MUSIC) has been independently proposed by Kim
et al. . Although the main ideas in SA-MUSIC and compressive MUSIC are similar, in fact, the two papers differ in the
following ways. First, the algorithms considered are different, in particular in the step of partial support recovery. Second, the
analyses in the two papers are fundamentally different. The analysis of Kim et al. depends heavily on the assumption
scale factor
runtime (sec)
Comparison of runtime N = 256, SNR = 30 dB.
success rate
success rate
Test on large condition number, n = 128, s = 8, rank(XJ0
0 ) = s (full row rank), N = 256. (a) κ = 10, SNR = 30 dB. (b) κ = 50, SNR = 30
that A is an i.i.d. Gaussian matrix and the size of the problem goes to inﬁnity satisfying certain scaling laws. The authors
showed that, under certain conditions, the probability of failure in the support recovery converges to 0 in their “large system
model”. However, since no convergence rate is shown, the analysis provides no guarantee on any ﬁnite dimensional problem.
In contrast, the guarantees in this paper are non-asymptotic and based on the weak-1 RIP. Our guarantees provide explicit
formulae for the required m as functions of s and n, for various sensing matrices A including i.i.d. Gaussian, random partial
Fourier, and incoherent unit-norm tight frame, whereas the analysis in only applies to an i.i.d. Gaussian A.
B. Comparison to the Guarantee of M-BP with the Multichannel Model
Various practical algorithms including p-SOMP, p-thresholding, and M-BP, have been analyzed under the multichannel model
 , . Although it is restricted to the noiseless case, the average case guarantees of M-BP with the multichannel model
 has been shown to be better than the other guarantees of the same kind for other algorithms. Therefore, we compare the
guarantees of SA-MUSIC algorithms to that of M-BP.
For this comparison, we too assume that the snapshots are noise-free, i.e., Y = AJ0XJ0
0 . Nevertheless, the guarantee of
SA-MUSIC algorithms in this paper is restricted neither to the noiseless case nor to the multichannel model.
In the noiseless case, the signal subspace estimation is perfect, bS = S ≜R(AJ0XJ0
0 ), with r ≜dim(bS) = rank(XJ0
N ≥s where s is the sparsity level, then XJ0
following the multichannel model has full row rank with probability 1. In the
full row rank case, any SA-MUSIC algorithm reduces to MUSIC and provides the best possible guarantee with the minimal
requirement αweak
s+1(A; J0) > 0, which reduces to m > s for certain matrices such as i.i.d. Gaussian A. This completes the
comparison in the case N ≥s. Therefore, to compare the performance of SA-MUSIC and M-BP in the rank-defective case,
we assume that N < s. The rank of XJ0
is then determined by the number of snapshots, i.e., rank(XJ0
0 ) = N and hence
Previous work [42, Theorem 4.4] showed that M-BP is guaranteed with probability 1 −ǫ if A satisﬁes the weak-1 RIP
s+1 (A; J0) < δ
for δ satisfying
≥2 ln(n/ǫ)
SA−MUSIC+SS−OMP
SA−MUSIC+SS−OMSP
SA−MUSIC+SS−OMP
SA−MUSIC+SS−OMSP
Required weak-1 RIC for the guarantees of M-BP (the average case analysis with the multichannel model with error probability ǫ = 10−3), and
SA-MUSIC (worst case analysis) for the noiseless case (a) n = 128, s = 8. (b) n = 1024, s = 64.
SA-MUSIC+SS-OMP and SA-MUSIC+SS-OMSP are guaranteed by Theorems 7.18 and 7.20, respectively. In particular,
follows the multichannel model, it is row-nondegenerate with probability 1. Therefore, we need only compare the
weak-1 RIP conditions in Theorems 7.18 and 7.20 to the weak-1 RIP given by (10.1)+(10.2). Fig. 11 displays this comparison.
For all three algorithms, as r increases, δ required for the guarantee increases and hence the guarantee is obtained subject to
a milder condition. Fig 11 (a) shows that SA-MUSIC+SS-OMSP requires larger RIC and hence requires reduced oversampling
factor m/s compared to M-BP when the size of the problem is small (n = 128). Fig 11 (b) shows that SA-MUSIC+SS-OMSP
provides a better guarantee (larger RIC) than M-BP in the regime r/s ≥0.6 when n = 1024.
The theoretical guarantee not withstanding, in our simulations, the recovery rate of the SA-MUSIC algorithms was always
higher than that of M-BP and often substantially so.
C. Comparison to the Analysis of Group LASSO in High Dimension
The guarantee of Group LASSO by Obozinski et al. is quite tight and, in particular, achieves the optimal guarantee by
the minimal requirement (m > s) for certain scenarios. However, their guarantee is asymptotic and only applies to Gaussian
A. In contrast, although our guarantee of SA-MUSIC+SS-OMP is not as tight as that of Group LASSO , the guarantee is
non-asymptotic, i.e., valid for any ﬁnite problems, and applies to wider class of matrices that arise in practical applications,
including the partial Fourier case.
D. Comparison to Compressed Sensing with Block Sparsity
The joint sparse recovery problem can be cast as a special case of compressed sensing with block sparsity . The block
structure in the sparsity pattern in the latter problem has been exploited to improve the performance of the sparse recovery
(cf. , , , ). For example, the analysis showed improvement of the structured sparse recovery over the
unstructured original problem. However, the special case where the sensing matrix is a block diagonal with repeated blocks,
which corresponds to the joint sparse recovery problem, is not covered by the proposed theory .
ACKNOWLEDGEMENTS
We thank Wei Dai and Olgica Milenkovic for the discussion that helped improve an earlier version of the draft, and Yonina
Eldar for pointing out her paper that we ﬁnd very useful in improving the analysis. We also appreciate the effort of the
anonymous reviewers and their useful comments and suggestions.
A. Lemmata
We list a few lemmata that will be used for the proofs in the appendix.
Lemma A.1 ([72, Corollary III.1.5]): Let A1 ∈Km×n1 and A2 ∈Km×n2. Let A ∈Km×n be the concatenation of A1 and
A2, i.e., A = [A1, A2] where n = n1 + n2. Then, there is an interlacing relation between the singular values of A and A1
σk(A) ≥σk(A1) ≥σk+n2(A)
for k = 1, . . . , n1.
Lemma A.2: Let A ∈Km×n and let J, J0 ⊂[n]. Then, it follows that, for k = 1, . . . , |J0 \ J|,
J0∪JAJ0∪J)
R(AJ)AJ0\J)
≥λk+|J|(A∗
J0∪JAJ0∪J),
which is equivalent to
σk(AJ0∪J) ≥σk(P ⊥
R(AJ)AJ0\J) ≥σk+|J|(A∗
J0∪JAJ0∪J).
Proof: Note that A∗
R(AJ)AJ0\J is the Schur complement of the block A∗
JAJ of the matrix
J0∪JAJ0∪JΠ
where Π is a permutation matrix that satisﬁes
AJ0∪JΠ = [AJ0\J, AJ].
Application of the interlacing relation of the eigenvalues of the Schur complement completes the proof.
Lemma A.3: Let A ∈Km×n and B ∈Kn×p where m ≥n and p ≥n. Then,
∥AB∥≥σn−k+1(A) · σk(B)
for k = 1, . . . , n.
Proof: Let A = U1Σ1V ∗
1 and A = U2Σ2V ∗
2 denote the extended SVD of A and B, respectively, where Σ1, V1, U2, Σ2 ∈
Kn×n. Let k ∈[n]. Then,
∥AB∥= ∥U1Σ1V ∗
= ∥V1Σ1V ∗
≥λn−k+1(V1Σ1V ∗
1 ) · λk(U2Σ2U ∗
= σn−k+1(A) · σk(B)
where the inequality follows from the Gel’fand-Naimark theorem [72, Theorem III.4.5].
B. Proof of Proposition 5.6
We use the following lemma to prove Proposition 5.6.
Lemma A.4: Suppose that Φ ∈Ks×r where r ≤s satisﬁes
krank(Φ∗) = r
and that Ψ ∈Ks×k for k < r spans a k-dimensional subspace of R(Φ). Then,
krank(Ψ∗) = k.
Proof: There exists R ∈Kr×k such that R has full rank and Ψ = ΦR. Let K be a set of k indices from [s]. Since
krank(Φ∗) = r implies rank((Φ∗)K) = k, rank((Ψ∗)K) = rank(R∗(Φ∗)K) = rank((Φ∗)K) = k. Since K was arbitrary, we
have krank(Ψ∗) = k.
Proof of Proposition 5.6: By the projection update formula, we have
PR(AJ1 )+ ¯S = PR(AJ1 ) + PP ⊥
R(AJ1 ) ¯S.
Note that R(AJ1) and P ⊥
R(AJ1 ) ¯S are orthogonal to each other and both are subspaces of R(AJ0). Furthermore, by assumption,
rank(AJ0) = s and hence rank(AJ1) = |J1| = s −r. Therefore, it sufﬁces to show that
R(AJ1 ) ¯S) = r.
Let U ∈Kn×r satisfy S = R(AJ0U J0) and U [n]\J0 = 0. Then, (A.4) is equivalent to
R(AJ1 )AJ0XJ0
which holds since
R(AJ1 )AJ0U J0)
R(AJ1 )AJ0\J1U J0\J1)
R(AJ1 )AJ0\J1)σr(U J0\J1)
≥σs(AJ0)σr(U J0\J1) > 0
where (a) and (b) follow from Lemma A.3, which provides a lower bound on the minimum singular value of the product, and
Lemma A.2, respectively and the last step follows from the assumption that AJ0 has full column rank and krank((U J0)∗) = r,
which holds by Lemma A.4 because XJ0
is row-nondegenerate.
C. Proof of Proposition 6.1
The proof of Proposition 6.1 is based on the following theorem by Davidson and Szarek .
Theorem A.5 ([57, Theorem II.13]): Given m, n ∈N with m ≤n, consider the random matrix G ∈Rn×m whose entries
are i.i.d. Gaussian following N(0, 1
n). Then, for any t > 0,
σ1(G) ≥1 +
σm(G) ≤1 −
Proof of Proposition 6.1: Note that (6.4) implies
1 + δ −1 ≥2
Let j ∈[n] \ J. Theorem A.5 implies
σ1(AJ∪{j}) ≥
1 + δ −1 −
σs+1(AJ∪{j}) ≤
1 + δ −1 −
it follows that
J∪{j}AJ∪{j} −Is+1∥≥δ
1 + δ −1 −
By considering the union of the events (∥A∗
J∪{j}AJ∪{j} −Is+1∥≥δ) for all j ∈[n] \ J, we obtain
s+1 (A; J) ≥δ
≤2(n −s) exp
1 + δ −1 −
The RHS of (A.5) is bounded from above by ǫ if
1 + δ −1)√m ≥
which is implied by
1 + δ −1)√m ≥
(s + 1) + 2 ln
where we used the concavity of the square root function. Noting that (A.6) coincides with (6.4) completes the proof.
D. Proof of Proposition 6.5
Let j ∈[n] \ J. Theorem A.5 implies
 σs+1(AJ∪{j}) ≤1 −γ
By considering the union of the events (σs+1(AJ∪{j}) ≤1 −γ) for all j ∈[n] \ J, we obtain
s+1 (A; J) ≤1 −γ
≤(n −s) exp
Condition (6.8) implies that the RHS of (A.7) is bounded from above by ǫ.
E. Proof of Proposition 6.9
Let j ∈[n] \ J. Then, by [55, Lemma 2.1], AJ∪{j} satisﬁes
J∪{j}AJ∪{j} −Is+1∥≥δ)
≤2(s + 1) exp
2(1 + δ/3)
Therefore, by considering the union of the events (∥A∗
J∪{j}AJ∪{j} −Is+1∥≥δ) for all j ∈[n] \ J, we obtain
s+1 (A; J) ≥δ)
≤2(n −s)(s + 1) exp
2(1 + δ/3)
Since (6.11) implies that the RHS of (A.8) is less than ǫ, the proof is complete.
F. Proof of Proposition 6.12
Let j ∈[n]\J. First, we derive a probabilistic upper bound on ∥A∗
J∪{j}AJ∪{j} −Is+1∥where the probability is with respect
to the choice of J. To this end, we use the relevant result in . [66, Theorem 12] claims that, for s ≥3 and α ≥1,
J∪{j}AJ∪{j} −Is+1∥> δ) ≤
144µ2(s + 1) ln
α + 2(s + 1)
∥A∥2 ≤e−1/4δ.
We assumed that µ ≤
√m and ∥A∥= p n
m. To bound the RHS of (A.9) from above by
By the concavity of the square root function, Condition (A.10) is implied by
144K2(s + 1)
+ 2(s + 1)
Since ln(s+1)
≤2 for all s ≥3, (A.11) is implied by
(s + 1) + 288K2 ln
By considering the union of the events (∥A∗
J∪{j}AJ∪{j} −Is+1∥≥δ) for all j ∈[n] \ J, we obtain
s+1 (A; J) ≥δ) ≤
J∪{j}AJ∪{j} −Is+1∥≤ǫ.
G. Proof of Theorem 7.1
MUSIC ﬁnds J0 if
Since αweak
s+1(A; J0) > 0, it follows that all columns of AJ0 are linearly independent. Furthermore, since rank(XJ0
0 ) = s, we
S = R(AJ0).
By the triangle inequality,
S −PS)ak∥2
Then, for all k ∈J0,
∥PR(AJ0 )ak∥2
where (a) and (b) follow from (A.14) and (A.13), respectively. Then, we obtain a lower bound on the LHS of (A.12) given
Similarly, by (A.13) and (A.14), we have
∥PR(AJ0)ak∥2
for all k ∈[n] \ J0 where ∥PR(AJ0)ak∥2 is further bounded from above by
∥PR(AJ0)ak∥2
R(AJ0 )ak∥2
R(AJ0 )ak)
s+1(AJ0∪{k})
s+1(A; J0)
s+1(A; J0)
where (c) holds by Lemma A.2 and (d) follows by the deﬁnition of the weak-1 asymmetric RIP. Then, we obtain an upper
bound on the RHS of (A.12) given by
k∈[n]\J0 ∥Pb
s+1(A; J0)
Combining (A.15) and (A.16), we note that MUSIC is guaranteed if A satisﬁes the weak-1 asymmetric RIP given by
s+1(A; J0) > α
for α > 0 satisfying
α ≥∥A∗∥2,∞
1 −(1 −2η)2o1/2
H. Proof of Proposition 7.6
Let d ≜dim(P ⊥
R(AJ ) ¯S). By the row-nondegeneracy condition on XJ0
and Lemma A.4, it follows that d ≥1. There exists
Q ∈K(s−k)×d where k = |J| such that Q∗Q = Id and R(AJ0\JQ) is a subspace of ¯S. Then, it follows that
R(AJ )AJ0\JQ)
R(AJ )P ¯SAJ0\JQ)
R(AJ )P ¯S) · ∥AJ0\JQ∥
R(AJ )P ¯S) · ∥AJ0∥.
By the variational characterization of the singular values, (⋆) is bounded from below by
(⋆) ≥σs−k(P ⊥
R(AJ)AJ0\J) ≥σs(AJ0)
where the last step follows by Lemma A.2.
Let κ ≜σ1(AJ0)/σs(AJ0). Combining (A.17) and (A.18), we obtain
R(AJ )P ¯S) ≥κ−1.
Since dim( ¯S) = dim(bS) = r, it follows that
and hence ∥Pb
S −P ¯S∥≤η implies the followings: for all ¯x ∈¯S, there exists ˆx ∈bS such that ∥¯x −ˆx∥2 ≤η∥¯x∥2. Similarly,
for all ˆx ∈bS, there exists ¯x ∈¯S such that ∥ˆx −¯x∥2 ≤η∥ˆx∥2.
The following identity is well known (see e.g., ): given two subspace S1 and S2,
∥PS1 −PS2∥= max{∥P ⊥
S1PS2∥, ∥P ⊥
Note, by (A.21), that
R(AJ ) bS −PP ⊥
R(AJ ) ¯S∥
R(AJ ) ¯S∥
R(AJ ) ¯SPP ⊥
First, we derive an upper bound on (∗), which is equivalently rewritten as
Let z ∈P ⊥
R(AJ) ¯S satisfy ∥z∥2 = 1. Then, by (A.19), there exists ¯x ∈¯S such that P ⊥
R(AJ)¯x = z and ∥¯x∥2 ≤κ. By the
argument following (A.20), there exists ˆx ∈bS such that ∥¯x −ˆx∥2 ≤η∥¯x∥2 ≤ηκ. Then, it follows that
∥2 ≤∥¯x −ˆx∥2 ≤ηκ
∥z −y∥2 ≤ηκ.
Since z was an arbitrary unit-norm element in P ⊥
R(AJ ) ¯S, we obtain
Next, we derive an upper bound on (∗∗), which is equivalently rewritten as
R(AJ ) ¯S ∥z −y∥2.
R(AJ )P ¯S) ≤σd(P ⊥
by (A.19), we obtain a lower bound on σd(P ⊥
S) given by
S) ≥κ−1 −η = 1 −ηκ
Let z ∈P ⊥
R(AJ ) bS satisfy ∥z∥2 = 1. Then, by (A.27), there exists ˆx ∈bS such that P ⊥
R(AJ)ˆx = z and ∥ˆx∥2 ≤
1−ηκ. By the
argument following (A.20), there exists ¯x ∈¯S such that ∥ˆx −¯x∥2 ≤η∥ˆx∥2 ≤
1−ηκ. Then, it follows that
∥2 ≤∥ˆx −¯x∥2 ≤
R(AJ ) ¯S ∥z −y∥2 ≤
Since z was an arbitrary unit-norm element in P ⊥
R(AJ ) bS, we obtain
Applying (A.25) and (A.28) to (A.23) completes the proof.
I. Proof of Theorem 7.7
MUSIC applied to eS is successful if
Since (7.5) implies rank(AJ0) = s and XJ0
is row-nondegenerate, by Proposition 5.6, we obtain
¯S + R(AJ1) = R(AJ0)
and hence, by the projection update formula,
PR(AJ0 ) = PR(AJ1 ) + PP ⊥
R(AJ1 ) ¯S.
Since the augmented subspace is given by eS = bS + R(AJ1), by the projection update formula, we have
S = PR(AJ1 ) + PP ⊥
R(AJ1 ) bS.
By the triangle inequality, it follows that
PR(AJ0 )ak
S −PR(AJ0))ak
By (A.30) we continue by
(PR(AJ1 )+ b
S −PR(AJ1 )+ ¯S)ak
R(AJ1 ) ¯S)ak
R(AJ1 ) ¯S
σs(AJ0) −ησ1(AJ0)
where (a) follows from (A.31) and (A.32), and (b) follows from Proposition 7.6 because σs(AJ0) > ησ1(AJ0) is implied by
(7.5) and (7.6).
By (A.33), it holds for all k ∈J0 \ J1 that
∥PR(AJ0)ak∥2
This yields a lower bound on the LHS of (A.29) given by
Similarly, by (A.33), it holds for all k ∈[n] \ J0 that
∥PR(AJ0)ak∥2
where (∗) is further bounded from above by
R(AJ0 )ak∥2
R(AJ0 )ak)
s+1(AJ0∪{k})
≤1 −[αweak
s+1(A; J0)]2
where (c) follows by Lemma A.2. This yields an upper bound on the RHS of (A.29) given by
k∈[n]\J0 ∥Pe
σ2s(AJ0) −ησ2
Finally, by applying the bounds in (A.33) and (A.34) to (A.29), we note that (A.29) is implied by the weak-1 asymmetric
RIP given by
which is equivalent to (7.6). This completes the proof.
J. Proof of Proposition 7.11
The following lemma is used in the proof of Proposition 7.11.
Lemma A.6: Suppose that A ∈Km×n and P ∈Kn×n is an orthogonal projector in Kn. Then, for all x, y ∈Kn,
|⟨APx, APy⟩| −|⟨Px, Py⟩|
≤∥PA∗AP −P∥· ∥x∥2 · ∥y∥2.
Proof of Proposition 7.11: Given J ⊊J0, the next step of M-OMP will be also successful if
j∈J0\J ∥a∗
R(AJ)Y ∥2 >
j∈[n]\J0 ∥a∗
R(AJ )Y ∥2.
We derive a sufﬁcient condition for (A.37). First, we derive a lower bound of the LHS of (A.37).
j∈J0\J ∥a∗
j∈J0\J ∥a∗
R(AJ)AJ0XJ0
j∈J0\J ∥a∗
R(AJ )AJ0XJ0
R(AJ )∥2,∞∥W∥
j∈J0\J ∥a∗
R(AJ )AJ0XJ0
−∥A∗∥2,∞∥W∥.
Let ΠJ ∈Kn×n denote the coordinate projection that satisﬁes ΠJej = ej for j ∈J and ΠJej = 0 for j ∈[n] \ J. Then, for
all j ∈J0 \ J, we bound the term (∗) in (A.38) from below by
R(AJ)AJ0\JXJ0\J
R(AJ )AJ0\JXJ0\J
R(AJ )aj, P ⊥
R(AJ)AJ0\JXJ0\J
R(AJ )AΠJ0\Jej, P ⊥
R(AJ)AΠJ0\JX0z⟩
⟨ΠJ0\Jej, ΠJ0\JX0z⟩
−∥ΠJ0\JA∗P ⊥
R(AJ)AΠJ0\J −ΠJ0\J∥
· ∥ΠJ0\Jej∥2∥ΠJ0\JX0z∥2
⟨ΠJ0\Jej, ΠJ0\JX0z⟩
R(AJ)AJ0\J −I|J0\J|∥∥ΠJ0\JX0z∥2
⟨ΠJ0\Jej, ΠJ0\JX0z⟩
−δ∥ΠJ0\JX0z∥2
where (a) follows from Lemma A.6 and (b) holds since
R(AJ)AJ0\J −I|J0\J|∥
1 −λ|J0\J|(A∗
R(AJ )AJ0\J),
R(AJ )AJ0\J) −1
J0AJ0), λ1(A∗
J0AJ0 −Is∥
where (c) follows from Lemma A.2 and (d) is implied by (7.9).
We then continue (A.39) by using (A.40)
⟨ΠJ0\Jej, ΠJ0\JX0z⟩
δ∥ΠJ0\JX0z∥2
jΠJ0\JX0∥2 −δ∥ΠJ0\JX0∥
jΠJ0\JX0∥2 −δ∥XJ0\J
Maximizing the lower bound on (∗) in (A.41) over j ∈J0 \ J yields
j∈J0\J ∥a∗
R(AJ )AJ0XJ0
j∈J0\J ∥e∗
jΠJ0\JX0∥2 −δ∥XJ0\J
∥2,∞−δ∥XJ0\J
and hence the LHS of (A.37) is bounded from below by
j∈J0\J ∥a∗
R(AJ )Y ∥2
∥2,∞−δ∥XJ0\J
∥−∥A∗∥2,∞∥W∥.
Next, we derive an upper bound on the RHS of (A.37).
j∈[n]\J0 ∥a∗
R(AJ )Y ∥2
j∈[n]\J0 ∥a∗
R(AJ )AJ0XJ0
0 ∥2 + ∥a∗
j∈[n]\J0 ∥a∗
R(AJ)AJ0XJ0
0 ∥2 + ∥P ⊥
R(AJ)aj∥2∥W∥
j∈[n]\J0 ∥a∗
R(AJ)AJ0XJ0
+∥A∗∥2,∞∥W∥.
Similarly to the previous case, we derive an upper bound on (⋆) for all j ∈[n] \ J0.
R(AJ)AJ0XJ0
R(AJ)AX0∥2
jΠ(J0\J)∪{j}A∗P ⊥
R(AJ )AΠ(J0\J)∪{j}X0∥2
R(AJ )AΠ(J0\J)∪{j}ej,
R(AJ )AΠ(J0\J)∪{j}X0z⟩
By Lemma A.6, the last term is at most
⟨Π(J0\J)∪{j}ej, Π(J0\J)∪{j}X0z⟩
(J0\J)∪{j}A(J0\J)∪{j} −Is−|J|+1∥
· ∥Π(J0\J)∪{j}X0z∥2
|⟨ej, X0z⟩|
(J0\J)∪{j}A(J0\J)∪{j} −Is−|J|+1∥
· ∥Π(J0\J)∪{j}X0z∥2
(J0\J)∪{j}A(J0\J)∪{j} −Is−|J|+1∥
· ∥Π(J0\J)∪{j}X0∥
(J0\J)∪{j}A(J0\J)∪{j} −Is−|J|+1∥· ∥XJ0\J
J0∪{j}AJ0∪{j} −Is+1∥· ∥XJ0\J
Maximizing the upper bound on (⋆) in (A.44) yields an upper bound on the RHS of (A.37) given by
j∈[n]\J0 ∥a∗
R(AJ )Y ∥2
∥+ ∥A∗∥2,∞∥W∥.
Applying the bounds in (A.42) and (A.45) to (A.37), we conclude that, for the success of the next step, it sufﬁces to satisfy
∥2,∞−2δ∥XJ0\J
∥≥2∥A∗∥2,∞∥W∥,
which is Condition (7.10). This completes the proof.
Proof of Lemma A.6: The proof follows from the properties of an inner product:
|⟨APx, APy⟩| −|⟨Px, Py⟩|
≤|⟨APx, APy⟩−⟨Px, Py⟩|
(b)= |⟨x, PA∗APy⟩−⟨x, Py⟩|
= |⟨x, (PA∗AP −P)y⟩|
≤∥PA∗AP −P∥∥x∥2∥y∥2
where (a) follows from the triangle inequality, (b) follows since from the idempotence of P.
K. Proof of Corollary 7.15
Recall that SS-OMP is M-OMP applied to Pb
S instead of Y . Since we consider the ﬁrst k steps of SS-OMP, we assume that
J ⊊J0 satisﬁes |J| ≤k −1. Since (7.11) implies ∥A∗
J0AJ0 −Is∥< 1, all columns of AJ0 are linearly independent. Therefore,
the projection P ¯S is written as
P ¯S = AJ0Φ
Since Φ∗Φ = Ir, it follows that
= ∥Φ∗∥· ∥A∗
J0AJ0∥· ∥Φ∥
= σr(ΦΦ∗A∗
where the inequality follows from the variational characterization of the eigenvalue since ΦΦ∗is a projection onto an rdimensional subspace.
s(AJ0), ∥AJ0∥2 −1}
J0AJ0 −Is∥
s+1 (A; J0)
where (a) follows from (7.11).
−1/2. Then, by (A.47) and (A.48),
and similarly, by (A.46) and (A.48),
Let U ∈Kn×r satisfy U [n]\J0 = 0 and U J0 = Φ. Then, P ¯S is also written as
P ¯S = A(UΥV ∗)
where V is given by V = AJ0ΦΥ and satisﬁes V ∗V = Ir.
S satisﬁes
S −P ¯S∥≤η,
we can write Pb
S = A(UΥV ∗) + Z
for some Z satisfying ∥Z∥≤η.
The following two inequalities applied to Proposition 7.11 complete the proof.
∥U J0\JΥV ∗∥= ∥U J0\JΥ∥
≤∥UΥ∥≤∥U∥∥Υ∥
∥U J0\JΥV ∗∥2,∞= ∥U J0\JΥ∥2,∞
> ∥U J0\J∥2,∞
where the last step follows since |J| ≤k −1.
L. Proof of Lemma 7.17
By the Cauchy-Binet formula, it follows that
1 = det(Φ∗Φ)
J∈[s],|J|=r
J∈[s],|J|=r
The determinant of ΦJ is bounded from above by
where the ﬁrst inequality follows from Hadamard’s inequality and the second inequality follows from the inequality of arithmetic
and geometric means. Therefore,
J∈[s],|J|=r
J∈[s],|J|=r
which implies
(ρk(Φ))q ≤
(ρk(Φ))q +
≤(s −r) + (2r −s)(ρs−r(Φ))q.
Combining (A.49) and (A.50), we obtain
M. Proof of Proposition 7.19
Assume that J ⊊J0 is given from the previous steps where k = |J| < s.
for j ∈[n]. Then, ∥qj∥2 = 1 for all j ∈[n].
For the guarantee of the next step of SS-OMSP, we need to show that
j∈J0\J ∥(PP ⊥
R(AJ ) bS)qj∥>
j∈[n]\J0 ∥(PP ⊥
By the triangle inequality, it follows that
S)qj∥−∥(PP ⊥
R(AJ ) ¯S)qj∥
R(AJ ) ¯S)qj∥
R(AJ ) ¯S∥
where the last step follows from Proposition 7.6 since XJ0
is row-nondegenerate and (7.20) implies α > √ηβ.
Then, (A.53) is implied by
j∈J0\J ∥(PP ⊥
R(AJ ) ¯S)qj∥>
j∈[n]\J0 ∥(PP ⊥
R(AJ ) ¯S)qj∥+
Let Q = [q1, . . . , qn] ∈Km×n where qj for each j ∈[n] is deﬁned in (A.52). Then, QJ0\J satisﬁes
σs−k(QJ0\J) ≥
R(AJ )AJ0\J)
maxj∈J0\J ∥P ⊥
maxj∈J0\J ∥aj∥
where (a) and (b) follow by Lemma A.2 and Lemma A.1, respectively, and (c) is implied by (7.19).
First, we bound the LHS of (A.54) from below by
j∈J0\J ∥(PP ⊥
R(AJ ) ¯S)qj∥
j∈J0\J ∥q∗
R(AJ ) ¯S∥
R(AJ ) ¯S∥2,∞
R(AJ ) ¯S∥F
R(AJ ) ¯S)
s−k+1−ℓ(QJ0\J)
where (d) follows from the variational characterization of the Ky-Fan norm . For the special case when r = s, ¯S is given
by ¯S = S = R(AJ0) and hence
R(AJ ) ¯S) = rank(P ⊥
R(AJ )AJ0\J) = s −k,
which follows since σs−k(P ⊥
R(AJ )AJ0\J) ≥σs(AJ0) > 0 by Lemma A.2. In this case, the RHS of (A.56) reduces to
s−k+1−ℓ(QJ0\J)
Otherwise, if r < s, we derive a lower bound on the RHS of (A.56) by
R(AJ ) ¯S)
· σs−k(QJ0\J)
where (e) follows from (A.55).
Next, we derive an upper bound on the RHS of (A.54). Assume that j ∈[n] \ J0. Since
R(AJ) ¯S ⊂P ⊥
R(AJ )R(AJ0)
R(AJ )AJ0) = R(P ⊥
R(AJ )AJ0\J),
it follows that
R(AJ ) ¯S)qj∥2 ≤∥(PR(P ⊥
R(AJ )AJ0\J))qj∥2.
R(AJ)AJ0\J has full column rank by (A.57), we can deﬁne
R(AJ)AJ0\J
R(AJ)AJ0\JR.
Then, Φ is an orthonormal basis for R(P ⊥
R(AJ )AJ0\J).
Since AJ has full column rank, we can also deﬁne
Then, Ψ is an orthonormal basis for R(AJ).
We bound the RHS of (A.60) from above by
R(AJ )AJ0\J))qj∥2
[qj, Φ]∗[qj, Φ] −Is−k+1
[qj, Φ]∗P ⊥
R(AJ )[qj, Φ] −Is−k+1
[qj, Φ]∗P ⊥
R(AJ)[qj, Φ]
[qj, Φ]∗P ⊥
R(AJ)[qj, Φ]
[qj, Φ, Ψ]∗[qj, Φ, Ψ]
[qj, Φ, Ψ]∗[qj, Φ, Ψ]
[qj, Φ, Ψ]∗[qj, Φ, Ψ] −Is+1
where (f) and (g) follow from Lemma A.1 and Lemma A.2, respectively.
Note that [Φ, Ψ] is an orthonormal basis of R(AJ0). Therefore, the RHS of (A.61) is bounded from above by
[qj, Φ, Ψ]∗[qj, Φ, Ψ] −Is+1
∥[Φ, Ψ]∗qj∥2
∥[Φ, Ψ]∗qj∥2
= ∥[Φ, Ψ]∗qj∥2
= ∥PR(AJ0)qj∥2
R(AJ0)qj∥2
R(AJ0)aj∥2
s+1(AJ0∪{j})
where (h) follows from Lemma A.2 and (i) is implied by (7.19).
Since j was arbitrary in [n] \ J0, (A.61) implies that
j∈[n]\J ∥(PR(P ⊥
R(AJ )AJ0\J))qj∥2 <
Applying (A.59), and (A.62) to (A.54), we obtain the following sufﬁcient condition for (A.53)
R(AJ ) ¯S)
which is equivalent to (7.20). For the full row rank case, applying (A.58), and (A.62) to (A.54), we obtain Condition (7.21),
which implies (A.53).
N. Proof of Proposition 8.1
To prove Proposition 8.1, we use the following lemmata. The proof of Lemma A.8 is deferred after the proof of Proposition 8.1.
Lemma A.7 (A case of [72, Theorem VII.3.3]): For positive semi-deﬁnite matrices Γ1, Γ2 ∈Km×m and r ∈{1, . . . , m−1},
let S1 and S2 be the subspaces spanned by the r-dominant eigenvectors of Γ1 and Γ2, respectively. Then,
3∥Γ1 −Γ2∥≥∥P ⊥
S1PS2∥· [λr(Γ2) −λr+1(Γ1)] .
Lemma A.8: Given m, n ∈N with n > 2m, consider the random matrix G ∈Cn×m whose entries are i.i.d. circular Gaussian
variables with zero mean and variance 1
∥G∗G −Im∥≥3
for t > 0 satisfying
Proof of Proposition 8.1: Recall that, given the sample covariance matrix ΓY ≜Y Y ∗
of the snapshots Y , Algorithm 1
computes bΓ deﬁned by
bΓ = ΓY −λm(ΓY )Im.
Also recall that ΓS is deﬁned by
ΓS ≜AJ0XJ0
= AJ0ΨΛΦΦ∗ΛΨ∗A∗
Let D ∈Km×m denote the distortion matrix deﬁned by
D ≜bΓ −ΓS.
Then, D is decomposed as
D = Dnoise + Dcross + Dbias
Dnoise ≜WW ∗
Dcross ≜AJ0XJ0
w −λm (ΓY )
It follows that the sample covariance matrix ΓY is decomposed as ΓY = ΓS + σ2
wIm + Dcross + Dnoise. Viewing ΓY as a
perturbed version of ΓS + σ2
wIm with distortion Dnoise + Dcross, we bound the perturbation in the m-th eigenvalue by Weyl’s
Theorem as
|λm(ΓY ) −λm(ΓS + σ2
≤∥ΓY −ΓS −σ2
= ∥Dnoise + Dcross∥
≤∥Dnoise∥+ ∥Dcross∥.
Now, since rank(ΓS) ≤s and s < m, (∗) reduces to
w −λm(ΓY )
= ∥Dbias∥.
Therefore, ∥Dbias∥satisﬁes
∥Dbias∥≤∥Dnoise∥+ ∥Dcross∥.
For k ∈N where k < N, let Zk ∈Kk×N be an i.i.d. Gaussian matrix such that EZk = 0 and E Zk(Zk)∗
= Ik. Then, we
deﬁne for k = 1, . . . , N −1 a family of random variables
Using ξk, we bound the distortion terms from above by
∥Dnoise∥= σ2
∥Dcross∥≤2σw∥AJ0ΨΛ∥
Φ∗, W ∗/σw
Φ∗, W ∗/σw
∼2ξm+Mσwλ1/2
where X1 ∼X2 denotes that random variables X1 and X2 have the same distribution. Therefore, for any ﬁxed c > 0, since
P(ξk ≤c) decreases in k, it follows that
P (∥D∥≤c) ≥P
w + 2ξm+sσwλ1/2
σw + 2λ1/2
Rank estimation From the sample covariance matrix bΓ, Algorithm 1 determines the rank r as the maximal number k that
λk(bΓ) −λk+1(bΓ) ≥τλ1(bΓ).
Therefore, to guarantee that Algorithm 1 stops at the desired rank r, we need sufﬁcient conditions for
λr(bΓ) −λr+1(bΓ) ≥τλ1(bΓ),
λk(bΓ) −λk+1(bΓ) < τλ1(bΓ),
By Weyl’s perturbation theorem [72, Corollary III.2.6], it follows that
λk(bΓ) −λk(ΓS)
λr(bΓ) −λr+1(bΓ)
≥λr(ΓS) −λr+1(ΓS) −2∥D∥
λ1(ΓS) + ∥D∥
Again, by Weyl’s perturbation theorem [72, Corollary III.2.6], it follows that
for all k = 1, . . . , M and hence
By (A.67), it follows that
λk(ΓS) ≥(1 −ξM)λk(Γ)
λk(ΓS) ≤(1 + ξM)λk(Γ).
By Lemma A.8, it follows that
k + ln(4/ǫ)
Therefore, by (8.4), (8.5), and (A.70), it follows that ξM ≤θ with probability 1 −ǫ/2. Therefore, we assume ξM ≤θ in the
remaining steps of the proof.
Since ξM ≤θ, by (A.66), (A.68), and (A.69) the following condition is a sufﬁcient condition for (A.64):
(1 −θ)λr(Γ) −(1 + θ)λr+1(Γ) −2∥D∥
(1 + θ)λ1(Γ) + ∥D∥
which is equivalent to
∥D∥≤(1 −θ)λr(Γ) −(1 + θ)λr+1(Γ) −(1 + θ)τλ1(Γ)
Since Γ satisﬁes (8.1), we obtain a sufﬁcient condition for (A.71) given by
λ1(Γ) ≤(1 + θ)ντ
Similarly, since Γ satisﬁes (8.2), we verify that (A.72) is also a sufﬁcient condition for (A.65).
Recall that Algorithm 1 computes bS as the subspace spanned by the r dominant eigenvectors of bΓ. Next, we derive conditions
that guarantees that ∥Pb
S −P ¯S∥≤η where ¯S is the subspace spanned by the r dominant eigenvectors of ΓS.
We apply Γ1 = bΓ and Γ2 = ΓS to Lemma A.7 and obtain
S −P ¯S∥· [λr(ΓS) −λr+1(ΓS)] .
Since ξM ≤θ and Γ satisﬁes (8.1),
λr(ΓS) −λr+1(ΓS) ≥(1 −θ)λr(Γ) −(1 + θ)λr+1(Γ)
≥(1 + θ)(1 + ν)τλ1(Γ).
By (A.73) and (A.74), we note that ∥Pb
S −P ¯S∥≤η is implied by
λ1(Γ) ≤η(1 + θ)(1 + ν)τλ1(Γ).
Recall that Cη,ν,θ,τ is deﬁned by
Cη,ν,θ,τ ≜(1 + θ)τ min
Then, by (A.63)
2[σ2w/λ1(Γ) + 2(σ2w/λ1(Γ))1/2]
implies (A.72) and (A.75).
Finally, we note that, by (8.4), (8.5), and (A.70), it follows that (A.76) holds with probability 1 −ǫ/2.
Proof of Lemma A.8: We use the following lemma to prove Lemma A.8
Lemma A.9 ([75, Lemma 36]): Consider a matrix B ∈Kn×m (m ≤n) that satisﬁes
∥B∗B −I∥≤max(δ, δ2)
for some δ > 0. Then,
1 −δ ≤σm(B) ≤σ1(B) ≤1 + δ.
Conversely, if B satisﬁes (A.77) for some δ > 0, then ∥B∗B −I∥≤3 max(δ, δ2).
We can write G as
2(GRe + jGIm)
where GRe, GIm ∈Rn×m are mutually independent i.i.d. Gaussian matrices whose entries follow N(0, 1
where (∗) is a block matrix
whose block entries are given by
ReGRe −Is) + (G∗
ImGIm −Is)] ,
A22 = A11.
(−GRe)∗(−GRe) −Is
(−GRe)∗GIm
2 ∥[−GRe GIm]∗[−GRe GIm] −I2s∥
2 ∥[GRe GIm]∗[GRe GIm] −I2s∥.
By Theorem A.5 and Lemma A.9, we have
∥[GRe GIm]∗[GRe GIm] −I2s∥≥3
for t > 0 satisfying
By the symmetry of the Gaussian distribution, we also have
∥[−GRe GIm]∗[−GRe GIm] −I2s∥≥3
Combining (A.78)–(A.80) completes the proof.