Deep Reinforcement Active Learning for
Human-In-The-Loop Person Re-Identiﬁcation
Zimo Liu†⋆, Jingya Wang‡⋆, Shaogang Gong§, Huchuan Lu†*, Dacheng Tao‡
† Dalian University of Technology, ‡ UBTECH Sydney AI Center, The University of Sydney, § Queen Mary University of London
 , , , , 
re-identiﬁcation(Re-ID)
approaches achieve superior results based on the assumption
that a large amount of pre-labelled data is usually available
and can be put into training phrase all at once. However,
this assumption is not applicable to most real-world
deployment of the Re-ID task. In this work, we propose
an alternative reinforcement learning based human-in-theloop model which releases the restriction of pre-labelling
and keeps model upgrading with progressively collected
The goal is to minimize human annotation efforts
while maximizing Re-ID performance.
It works in an
iteratively updating framework by reﬁning the RL policy
and CNN parameters alternately.
In particular, we formulate a Deep Reinforcement Active Learning (DRAL)
method to guide an agent (a model in a reinforcement
learning process) in selecting training samples on-the-ﬂy
by a human user/annotator.
The reinforcement learning
reward is the uncertainty value of each human selected
sample. A binary feedback (positive or negative) labelled
by the human annotator is used to select the samples of
which are used to ﬁne-tune a pre-trained CNN Re-ID
model. Extensive experiments demonstrate the superiority
of our DRAL method for deep reinforcement learning
based human-in-the-loop person Re-ID when compared to
existing unsupervised and transfer learning models as well
as active learning models.
1. Introduction
Person re-identiﬁcation (Re-ID) is the problem of matching people across non-overlapping camera views distributed
at distinct locations. Most existing supervised person Re-ID
approaches employ a train-once-and-deploy scheme, that is,
pairwise training data are collected and annotated manually
* Corresponding Author
⋆Equal Contribution
Human annotator
query for label
unlabeled gallery pool
Figure 1: An illustration of Deep Reinforcement Active
learning (DRAL). For each query anchor (probe), an agent
(reinforcement active learner) will select sequential instances from gallery pool for human annotation with binary
feedback (positive/negative) in an active learning process.
for every pair of cameras before learning a model. Based
on this assumption, supervised Re-ID methods have progressed on several benchmarks in recent years [21, 56, 35,
However, in practice this assumption is not easy to adapt
due to a few reasons: Firstly, pairwise pedestrian data is
prohibitive to be collected since it is unlikely that a large
amount of pedestrian may reappear in other camera views.
Secondly, the increasing number of camera views ampliﬁes
the difﬁculties in searching the same person among multiple camera views. To address these difﬁculties, one solution is to design unsupervised learning algorithms. A few
works begin to focus on transfer learning or domain adaption technique for unsupervised Re-ID . However, unsupervised learning based Re-ID models are inherently weaker compared to supervised learning based models, compromising Re-ID effectiveness in any practical deployment.
Another possible solution is following the semisupervised learning scheme that decreases the requirement
of data annotations. Successful researches have been done
on either dictionary learning or self-paced learning 
based methods. These models are still based on a strong assumption that parts of the identities (e.g. one third of the
training set) are fully labelled for every camera view. This
remains impractical for a Re-ID task with hundreds of cameras and 24/7 operations, typical in urban applications.
To achieve effective Re-ID given a limited budget cost
on annotation, we focus on human-in-the-loop person Re-
ID with selective labelling by human feedback on-the-ﬂy
 . This approach differs from the common once-anddone model learning approach. Instead, a step-by-step sequential active learning process is adopted by exploring human selective annotations on a much smaller pool of samples for model learning. These cumulatively labelled data
by human binary veriﬁcation are used to update model training for improving Re-ID performance. Such an approach to
model learning is naturally suited for reinforcement learning together with active learning, the focus of this work.
Active learning is a technique for on-the-ﬂy human data
annotation that aims to sample actively the more informative training data for optimising model learning without exhaustive data labelling. Formally, some instance from an
unlabelled set are selected and then annotated by a human
oracle, and the label information can be employed for model
training. These operations will repeat many times until it
satisﬁes the termination criterion, e.g. the annotation budget is exhausted. The most critical in this process is the sample selection strategy. The more informative samples from
less human annotation cost can greatly beneﬁt the performance. Rather than a hand-design strategy, we propose to a
reinforcement learning-based criterion. Fig 1 illustrates our
design for a Deep Reinforcement Active Learning (DRAL)
model. Speciﬁcally, we develop a model which introduces
both active learning (AL) and reinforcement learning (RL)
in a single human-in-the-loop model learning framework.
By representing the AL part of our model as a sequence
making process, since each action affects the sample correlations among unlabelled data pool (with similarity recomputed at each step), it will inﬂuence the decision of next
step. By treating the uncertainty brought by the selected
samples as the objective goal, the RL part of our model aims
to learn a powerful sample selection strategy given human
feedback annotations. Therefore, the informative samples
selected from the RL policy can signiﬁcantly boost the performance of Re-ID which in return enhance the ability of
sample choosing strategy. The iterative training scheme will
lead to a strong Re-ID model.
The main contributions of this work are: (1) We introduce a Deep Reinforcement Active Learning (DRAL)
model, formulated to explore jointly both reinforcement
learning and active learning principles in a single CNN deep
learning framework.
(2) We design an effective DRAL
model for human-in-the-loop person Re-ID so that a deep
reinforcement active learner (agent) can facilitate humanin-the-loop active learning strategy directly on a CNN deep
network. Extensive comparative experiments show clearly
the proposed DRAL model has advantages over existing supervised and transfer learning methods on scalability and
annotation costs, over existing semi-supervised, unsupervised and active learning methods with signiﬁcant performance gain whilst using much less annotations.
2. Related Work
Person Re-ID. Person Re-ID task aims to search the same
people among multiple camera views. Recently, most person Re-ID approaches try to solve this problem under the supervised learning framework, where the training data is fully
annotated.
Despite the high performance these methods
achieved, their large annotation cost cannot be ignored. To
address the high labelling cost problem, some researchers
propose to learn the model with only a few labelled samples or without any label information. Representative algorithms include domain transfer
scheme, group association approaches, and some label estimation methods.
In addition to the above-mentioned approaches, some
researchers aim to reduce the annotation cost in a humanin-the-loop (HITL) model learning process. When there is
only a few annotated image samples, HITL model learning can be expected to improve the model performance by
directly involving human interaction in the circle of model
training, tuning or testing. With the human population correct the inaccuracies happen in machine learning predictions, the model could be efﬁciently corrected thereby leading to higher results. This circumstance sounds similar to
the situation of person Re-ID task whose pre-labelling information is hard to be obtained with the gallery candidate
size far beyond that of the query anchor.
Motivated by
this, Wang et al. formulates a Human Veriﬁcation Incremental Learning (HVIL) model which aims to optimize
the distance metric with ﬂexible human feedback continuously in real-time. The ﬂexible human feedback (true, false,
false but similar) employed in this model enables to involve
more information and boost the performance in a progressive manner.
AL and RL. Active Learning has drawn many attention
in the last few decades and been exploited in Natural Language Processing (NLP), data annotation and image classiﬁcation task . Its procedure can be thought
as human-in-the-loop setting, which allows the algorithm to
interactively query the human annotator with the instances
recognized as the most informative samples among the entire unlabelled data pool. This work is usually done by using
some heuristic selection methods with limited effectiveness.
Therefore, some researchers aim to address the shortcomings of the heuristic selection approaches by framing the active learning as a reinforcement learning problem to explicitly optimize a selection policy. In , rather than adopt-
true or false
Deep Reinforced Active Learner for sample selection
Similarity
K-reciprocal
annotated data
CNN Updating & State Initialization
Figure 2: The Deep Reinforcement Active Learning (DRAL) framework: State measures the similarity relations among all
instance. Action determines which gallery candidate will be sent for human annotator for querying. Reward is computed
with different human feedback. A CNN is adopted for state initialization and being updated by the pairwise data annotated
via a human annotator in-the-loop on-the-ﬂy when the model is deployed. This iterative process stops when it reaches the
annotation budget.
ing a ﬁxed heuristic selection strategy, Fang et al. performs
to learn a deep Q-network as an adaptive policy to select
the data instances for labelling. Woodward et al. try
to solve the one-shot classiﬁcation task by formulating an
active learning approach which incorporates meta-learning
with deep reinforcement learning. An agent learned via this
approach enables to decide how and when to request label.
Those successful applications indicate that reinforcement
learning is a natural ﬁt for active learning.
3. Methodology
3.1. Base CNN Network
We employ the Resnet-50 architecture as the base
net with ImageNet pre-train. To effectively learn the ID discriminative feature embedding, we adopt both cross entropy
loss for classiﬁcation and triplet loss for similarity learning
synchronously.
The softmax Cross Entropy loss function deﬁned as:
Lcross = −1
log(pi(y))
where nb denotes the batch size and pi(y) is the predicted
probability on the groundtruth class y of input image.
Given triplet samples xa, xp, xn, xa is an anchor point.
xp is hardest positive sample in the same class of xa , and
xn is a hardest negative sample of a different class of xa.
Finally we deﬁne the triplet loss as following:
[Dxa,xp −Dxa,xn + m]
where m is a margin parameter for the positive and negative
Finally, the total loss for can be calculated by:
Ltotal = Lcross + Ltri
3.2. A Deep Reinforced Active Learner - An Agent
The framework of the proposed DRAL is presented in
Fig 2, of which “an agent” (model) is designed to dynamically select instances that are most informative to the query
instance. As each query instance arrives, we perceive its
ns-nearest neighbors as the unlabelled gallery pool. At each
discrete time step t, the environment provides an observation state St which reveals the instances’ relationship, and
receives a response from the agent by selecting an action
At. For the action At = gk, it requests the k-th instance
among the unlabelled gallery pool being annotated by human oracle, who replies with binary feedback true or false
against the query. This operation repeats until the maximum annotation amount for each query is exhausted. When
plentiful enough pair-wise labelled data are obtained, the
CNN parameters enable to be updated via triplet loss function, which in return generates a new initial state for incoming data. Through iteratively executing the sample selection
and CNN network refreshing, the proposed algorithm could
quickly escalate. This progress terminates with all query
instances have been browsed once. More details about the
proposed active learner are revealed in the following. To
clarify on our formulation of the model, Table 1 and Algorithm 1 give the deﬁnitions of the notations and the entire
process of the approach, respectively.
Table 1: Deﬁnitions of notations.
Description
At, St, Rt
action, state and reward at time t
train set and its size
pairwise annotated data set
similarity between samples i, j
Mahalanobis distance of i, j
query, the k-th gallery candidate
binary feedback of gk at time t
positive/negative sample batch until time t
annotating sample number for each query
action size
parameter of reciprocal operation
threshold parameter
Algorithm 1 DRAL
Input: agent π, CNN weights w, τr (size n), τp= ∅
for i = 1 : n do
Sample query q and gallery pool g from τr
while t < Kmax do
St ←(Sim, R(ni, k)) via Eq. 4-8
At : gk ←π(St), requests label for pair (q, gk)
τp ←τp ∪(q, gk)
(Rt, Sim) ←(St, At) via Eq. 9
optimize π∗←arg max
E[Rt + γRt+1 + ...]
optimize w by τp after several steps
The action set deﬁnes to select an instance from the unlabelled gallery pool, hence its size is the same as the pool.
At each time step t, when encountered with the current state
St, the agent decides the action to take based on its policy π(At|St). Therefore the At instance of the unlabelled
gallery pool will be selected querying by human oracle.
Once At = gk is performed, the agent is unable to choose it
again in the subsequent steps. The termination criterion of
this process depends on a pre-deﬁned Kmax which restricts
the maximal annotation amount for each query anchor.
Graph similarity has been widely employed for data selecting in active learning framework by digging
the structural relationships among data points. Typically,
a sparse graph is adopted which only connects data point to
a few of its most similar neighbors to exploit their contextual information. In this work, we also construct a sparse
similarity graph among query and gallery samples and take
it as the state value. With a queried anchor q and its corresponding gallery candidate set g = {g1, g2, ..., gns}, there
Re-ID features could be extracted via the CNN network,
where ns is a pre-deﬁned number of the gallery candidates.
The similarity value Sim(i, j) between every two samples
i, j(i ̸= j) are then calculated as
Sim(i, j) = 1 −
i,j∈q,g dj
i is the Mahalanobis distance of i, j, else set as 0.
A k-reciprocal operation is executed to build the sparse
similarity matrix. For any node ni ∈(q, g) of the similarity matrix Sim, its top κ-nearest neighbors are deﬁned as
N(ni, κ). Then the κ-reciprocal neighbors R(ni, κ) of ni
is obtained through
R(ni, κ) = {xj|(ni ∈N(xj, κ)) ∧(xj ∈N(ni, κ))} (5)
Compared to the previous description, the κ-reciprocal
nearest neighbors are more related to the node ni, of which
the similarity value is remained otherwise be assigned with
zero. This sparse similarity matrix is then taken as the initial
state and imported into the policy network for action selection. Once the action is employed, the state value will be
adjusted accordingly to better reveal the sample relations.
To better understand the update of state value, we illustrate an example in Fig 3. For a state St at time t, the optimal action At = gk is selected via the policy network,
which indicates the gallery candidate gk will be selected for
querying by the human annotator. A binary feedback is the
given as yt
k = {1, −1}, which indicates gk to be the positive pair or negative of the query instance. Therefore the
similarity Sim(q, gk) between q and gk will be set as
Sim(q, gk) =
The similarities of the remaining gallery samples gi, i ̸= k
and query sample will also be re-computed, which aiming to
zoom in the distance among positives and push out the distance among negatives. Therefore, with positive feedback,
the similarity Sim(q, gi) is the average score between gi
with (q, gk), where
Sim(q, gi) = Sim(q, gi) + Sim(q, gk)
Otherwise, the similarity Sim(q, gi) will only be updated
when the similarity among gk and gi is larger than a threshold thred, where
Sim(q, gi) = max(Sim(q, gi) −Sim(gk, gi), 0)
The k-reciprocal operation will also be adopt afterwards,
and a renewed state St+1 is then obtained.
0.83 0.71 0.66 0.47 0.36
0.85 0.42 0.87 0.78
0.55 0.45 0.32
0.66 0.42 0.55
0.47 0.87 0.45 0.67
0.36 0.78 0.32 0.33 0.77
0.83 0.71 0.66 0.47 0.36
S((q; g&:,)
0.78 0.54 0.67 0.35
0.85 0.67 0.87 0.78
0.55 0.45 0.32
0.54 0.42 0.55
0.67 0.87 0.45 0.67
0.35 0.78 0.32 0.33 0.77
0.78 0.54 0.67 0.35
0.66 0.47 0.35
0.85 0.42 0.87 0.78
0.55 0.45 0.32
0.66 0.42 0.55
0.87 0.45 0.67
0.78 0.32 0.33 0.77
K-reciprocal(K=2)
K-reciprocal
K-reciprocal
S((g&; g0:,)
!"(/&; /0:,)
S((q; g&:,)
Figure 3: An example of state updating with different human feedback, which aims to narrow the similarities among instances
sharing high correlations with negative samples, and enlarge the similarities among instances which are highly similar to the
positive samples. The values with yellow background are the state imported into the agent.
Standard active learning methods adopt an uncertainty measurement, hypotheses disagreement or information density
as the selection function for classiﬁcation and
retrieval task . Here, we use data uncertainty as the
objective function of the reinforcement learning policy.
For data uncertainty measurement, higher uncertainty indicates that the sample is harder to be distinguished. Following the same principle of which extends a triplet
loss formulation to model heteroscedastic uncertainty in a
retrieval task, we perform a similar hard triplet loss 
to measure the uncertainty of data. Let the Xt
n indicate
the positive and negative sample batch obtained until time t,
gk be a metric function measuring Mahalanobis distances
between any two samples gk and x. Then the reward is
computed as
Rt = [m + yt
where [•]+ is the soft margin function by at least a margin m. Therefore all the future rewards(Rt+1, Rt+2, ...) discounted by a factor γ at time t can be calculated as
E[Rt + γRt+1 + γ2Rt+2 · · · |π, St, At]
Once Q∗is learned, the optimal policy π∗can be directly
inferred by selecting the action with the maximum Q value.
3.3. CNN Network Updating
For each query anchor, several samples are actively selected via the proposed DRAL agent and are manually annotated by the human oracle, and these pairwise data will be
added to a updated training data pool. The CNN network is
then updated gradually using ﬁne-tuning. We use the triplet
loss as the objective function, and when more labelled data
is involved, the model becomes more robust and smarter.
The renewed network is employed for Re-ID feature extraction, which in return helps the upgrade of the state initialization. We stop this iterative training scheme with a ﬁxed
annotation budget when each image in the training data pool
has been browsed once by our DRAL agent.
4. Experiments
4.1. Dataset and Settings
Datasets For experimental evaluations, we report results
on both large-scale and small-scale person re-identiﬁcation
benchmarks for robust analysis:
(1) The Market-1501 is widely adapt large-scale
re-id dataset that contains 1,501 identities obtained by Deformable Part Model pedestrian detector. It includes 32,668
images obtain from 6 non-overlapping camera views in the
campus with 12936 images of 751 identities used for training. In testing stage, 3368 queries are used as the query set
to search the true match among the remained candidates.
(2) CUHK01 is one of the remarkable small-scale
re-id dataset, which consists of 971 identities from two camera views, each identity has two images per camera view
and thus totally including 3884 images which are manually
cropped. The entire dataset is split into two parts: 485 identities for training and 486 for testing.
(3) DukeMTMC-ReID(Duke) is one of the most
popular large scale re-id dataset which consists 36411
pedestrian images captured from 8 different camera views.
Among them, 16522 images (702 identities) are adopted for
training, 2228 (702 identities) images are taken as query to
be retrieved from the remaining 17661 images.
Evaluation Protocols Two evaluation metrics are adopted
in this approach to evaluate the Re-ID performance. The
ﬁrst one is the Cumulated Matching Characteristics(CMC),
and the second is the mean average precision(mAP) which
taking person Re-ID task as an object retrieval problem.
Implementation Details.
We implemented the proposed
DRAL method in the Pytorch framework. We pre-train a
resnet-50 multi-class identity discrimination network with
the combination of triplet loss and cross entropy loss by
60 epochs(pre-train on Duke for Market1501 and CUHK01,
pre-train on Market1501 for Duke), at a learning rate of 5E-
4 by using the Adam optimizer. The ﬁnal FC layer output feature vector (2,048-D) is extracted as the re-id feature
vector in our model by resizing all the training images as
256×128. The policy network in the proposed method consists of three FC layers setting as 256. The proposed DRAL
model is randomly initialized and then optimized with the
learning rate at 2E-2, and (Kmax, ns, κ) are set as (10, 30,
15) by default. The balanced parameter thred and m are set
as 0.4 and 0.2, respectively. With every 25% of the training
quires have been reviewed by the human annotator, we start
to ﬁne-tune the CNN network with learning rate at 5E-6.
Comparison
Unsupervised/Transfer
Learning/Semi-Supervised Approaches
Human-in-the-loop person re-identiﬁcation does not require the pre-labelling data, but receive user feedback for
the input query little by little. It is feasible to label many
of the gallery instances, but to cut down the human annotation cost, we perform to use the active learning technique
for sample selecting. Therefore, we compare the proposed
DRAL method with some active learning based approach
and unsupervised/transfer/semi-supervised based methods,
in the table we use ’uns/trans/semi’, ’active’ to indicate the
training style. Moreover, the baseline results reported is
computed by directly employing a pre-trained CNN model,
and the upper bound result indicates that the model is ﬁnetuned on the dataset with fully supervised training data.
For unsupervised/transfer learning and semi-supervised
setting, sixteen state-of-the-arts approaches are selected
for comparing including UMDL , PUL , SP-
GAN , Tfusion , TL-AIDL , ARN ,
TAUDL , CAMEL , SSDAL , SPACO ,
One-Exampler and DML .
In table 2, 3 and 4,
we illustrate the rank-1, 5, 10 matching accuracy and
mAP(%) performance on the Market1501 , Duke 
and CUHK01 dataset, of which the results of our ap-
Table 2: Rank-1, 5, 10 accuracy and mAP (%) with some
unsupervised, semi-supervised and adaption approaches on
the Market1501 dataset.
Market1501
uns/trans/semi
SPGAN 
TFusion 
TL-AIDL 
TAUDL 
CAMEL 
SSDAL 
SPACO 
One-Exampler 
UpperBound
Table 3: Rank-1, 5, 10 accuracy and mAP (%) with some
unsupervised, semi-supervised and adaption approaches on
the Duke dataset.
uns/trans/semi
SPGAN 
TL-AIDL 
TAUDL 
CAMEL 
One-Exampler 
UpperBound
proach are in bold. The proposed method achieves 84.2%
and 66.26% at rank-1 and mAP, which outperforms the second best unsupervised/transfer/semi-supervised approaches
Table 4: Rank-1, 5, 10 accuracy and mAP (%) with some
unsupervised and adaption approaches on the CUHK01
UCDTL 
CAMEL 
TRSTP 
UpperBound
by 13.9% and 19.69% on Market1501 benchmark.
For Duke and CUHK01 datasets, DRAL also
achieves fairly good performance with rank-1 matching rate
at 74.28% and 74.07%. These results demonstrate clearly
the effectiveness of our active sample selection strategy
implemented by the DRAL method, and shows that without annotating large quantities of training data, a good reidentiﬁcation model can be built effectively by DRAL.
4.3. Comparisons with Active Learning
Beyond the approaches as mentioned above, we further compare with some active learning based approaches
which involve human-machine interaction during training.
We choose four active learning strategy as comparisons of
which the model is trained through the same framework as
our method, of which an iterative procedure of these active sample selection strategy and CNN parameter updating
is executed until the annotation budget is achieved. Here
20% of the entire training samples(around 4% pairs) are selected via the reported active learning approaches, which
indicates 388, 2588, 3304 are set as the annotation budget
for termination on the CUHK01 , Market1501 , and
Duke dataset, respectively Beside these active learning methods, we also compare the performance with another active learning approach HVIL , which runs experiments under human-in-the-loop setting. The details of
these approaches are described as follows: (1) Random, as a
baseline active learning approach, we randomly pick some
samples for querying; (2) Query Instance Uncertainty 
(QIU), QIU strategy selects the samples with the highest uncertainty for querying; (3) Query By Committee (QBC),
QBC is a very effective active learning approach which
learns an ensemble of hypotheses and queries the instances
that cause maximum disagreement among the committee;
(4) Graph Density (GD), active learning by GD is an algorithm which constructs graph structure to identify highly
connected nodes and determine the most representative data
for querying. (5) Human Veriﬁcation Incremental Learning (HVIL), HVIL is trained with the human-in-theloop setting which receives soft user feedback (true, false,
false but similar) during model training, requiring the annotator to label the top-50 candidates of each query instance.
Table 2, 3, 4 compares the rank-1, 5, 10 and mAP rate
from the active learning models against DRAL, where the
baseline model result is from directly employing the pretrained CNN model. We can observe from these results that
(1) all the active learning methods perform better than the
random picking strategy, which validates that active sample selection does beneﬁt person Re-ID performance. 2)
DRAL outperforms all the other active learning methods,
with rank-1 matching rate exceeds the second best models
QBC, HVIL and GD by 16.97%, 6.2% and 13.15% on the
CUHK01 , Market1501 and Duke dataset, respectively, with a much lower annotation cost. This suggests that DRAL is more effective than other active learning methods for person Re-ID by introducing the policy as
sample selection strategy.
4.4. Comparison at Different Annotation Cost
In this work, cost is measured via the annotation number between image pairs.
With training set size n, the
cost for the fully supervised setting will be n ∗(n −1)/2
, and 10 ∗n for the reported DRAL result.
our DRAL annotates about 0.12%(Duke ), 0.15%(Market1501 ) and 1%(CUHK01 ) pairs.
We further
compare the performance of the proposed DRAL approach
in a varying amount of labelled data (indicate by Kmax)
with fully supervised learning(UpperBound) on the three
reported datasets. With the enlarge of training data size, the
cost of annotating all data shows exponential increasement.
Among the results, the baseline is obtained by directly employing the pre-trained CNN for testing. For the fully supervised setting, with all the training data annotated, it enables to ﬁne-tune the CNN parameters with both the triplet
loss and the cross-entropy loss to looking for better performance. For DRAL method, we present the performance
with Kmax setting as 3, 5 and 10 in Table 5. As can be
observed, 1) with more annotated data, the model becomes
stronger with increasing annotation cost. With the annotation number for each query increases from 3 to 10, the rank-
1 matching rate improves 13.37%, 8.72% and 15.43% on
the Duke , Market1501 and CUHK01 benchmarks. 2) compared to the fully supervised setting, the proposed active learning approach shows only around 4% rank-
1 accuracy falling on each dataset. However, the annotation
cost of DRAL is far below the supervised one.
Table 5: Rank-1 accuracy and mAP (%) result by directly employing(Baseline), fully supervised learning(UpperBound), and
DRAL with varied Kmax on the three reported dataset, where n indicates the training instance number for each benchmark.
The annotation cost is calculated through the times of labelling behavior for every two samples.
Market1501
UpperBound
n ∗(n −1)/2
Iterations
Iterations
Iterations
(a) Market1501
(c) CUHK01
Figure 4: Rank-1 accuracy and mAP(%) improvement with respect to the iterations on the (a) Market1501, (b) Duke and (c)
CUHK01 dataset. The gray line and green bar(bold number) indicates the rank-1 accuracy and mAP respectively.
4.5. Effects with Number of Iterations
The promise of active learning is that, through iteratively
increasing the size of labelled data, the model performance
is enhanced gradually. For each input query, we only associate the label to the gallery candidates derived from the
DRAL, and adopted these pairwise labelled data for CNN
parameter updating. We set the iteration as a ﬁxed number 4 in our experiments on all the datasets. Fig 4 shows
the rank-1 accuracy and mAP improvement with respect to
the iterations on the three datasets. From these results, we
can observe that the performance of the proposed DRAL
active learner improves quickly, with rank-1 accuracy increases around 20%∼40% over the ﬁrst two iterations on
all three benchmarks, and the improvement in model performance starts to ﬂat out after ﬁve iterations. This suggests
that for person Re-ID, fully supervising may not be essential. Once the informative samples/information have been
obtained, a sufﬁciently good Re-ID model can be derived at
the cost of a much smaller annotation workload by exploring a sample selection strategy on-the-ﬂy.
5. Conclusion
In this work, we addressed the problem of how to reduce
human labelling effort in conventional data pre-labelling for
person re-identiﬁcation model training. With limited annotation cost or inaccessible large quantity of pre-labelled
training data, our model design aims to maximise the effectiveness of Re-ID model learning with a small number of selective sample labelling. The key task for the model design
becomes how to select more informative samples at a ﬁxed
annotation cost. Speciﬁcally, we formulated a Deep Reinforcement Active Learning (DRAL) method with a ﬂexible
reinforcement learning policy to select informative samples
(ranked list) for a given input query. Those samples are
then fed into a human annotator so that the model can receive binary feedback (true or false) as reinforcement learning reward for DRAL model updating. Moreover, an iterative scheme is executed for the update of DRAL and Re-ID
model. Extensive comparative evaluations were conducted
on both large-scale and small-scale Re-ID benchmarks to
demonstrate our model robustness.
Acknowledgement
This work is supported by National Natural Science
Foundation of China No.61725202, 61829102, 61751212;
Fundamental Research Funds for the Central Universities
under Grant No.DUT19GJ201; Vision Semantics Limited;
the China Scholarship Council; Alan Turing Institute; Innovate UK Industrial Challenge Project on Developing and
Commercialising Intelligent Video Analytics Solutions for
Public Safety (98111-571149); and the Australian Research
Council Projects: FL-170100117, DP-180103424.