Natural Language Engineering 15 (4): 551–582.
c⃝Cambridge University Press 2009
doi:10.1017/S1351324909990143
A machine learning approach to textual
entailment recognition
F A B I O M A S S I M O Z A N Z O T T O1,
M A R C O P E N N A C C H I O T T I2
and A L E S S A N D R O M O S C H I T T I3
1DISP, University of Rome ‘Tor Vergata’, Roma, Italy
2Computerlinguistik, Universit¨at des Saarlandes, Saarbr¨ucken, Germany
3DISI, University of Trento, Povo di Trento, Italy
e-mail: , ,
 
 
Designing models for learning textual entailment recognizers from annotated examples is not
an easy task, as it requires modeling the semantic relations and interactions involved between
two pairs of text fragments. In this paper, we approach the problem by ﬁrst introducing the
class of pair feature spaces, which allow supervised machine learning algorithms to derive
ﬁrst-order rewrite rules from annotated examples. In particular, we propose syntactic and
shallow semantic feature spaces, and compare them to standard ones. Extensive experiments
demonstrate that our proposed spaces learn ﬁrst-order derivations, while standard ones are
not expressive enough to do so.
1 Introduction
Automatically learning models from training examples is a very attractive way
to solve many complex tasks in natural language processing (NLP). Learning
algorithms generally discover important information which could be otherwise only
manually encoded in rule-based systems. In recent work, they have shown a good
level of accuracy for most natural language tasks: part-of-speech tagging, named
entity recognition, word-sense disambiguation, and semantic role labeling.
Regarding the Recognizing Textual Entailment (RTE) challenges , supervised
machine learning (ML) models have proved to be particularly successful in solving
the task, despite the fact that their application to RTE is diﬃcult, as textual
entailment is an extremely complex natural language phenomenon. Generally, NLP
tasks require a classiﬁer to assign the correct label to a target text fragment, looking
at its context. For example, in semantic role labeling , the goal is to assign the correct role to a relevant
text fragment with respect to a set of possible roles (e.g., Agent, Patient). For this
F. M. Zanzotto et al.
purpose, a model of the context of the fragment, at a speciﬁc level of linguistic
representation (e.g., bag-of-word models or syntactic interpretations) is typically
used. In contrast, textual entailment recognition requires processing two diﬀerent
texts between which complex semantic/syntactic relations hold, and the goal is to
classify such relations as true or false entailment. Typical bag-of-word models are
not useful to capture the knowledge needed by the learning algorithms.
In this paper we propose a solution to the above problem, by introducing a new
type of feature space, the pair feature space, which allows learning algorithms to
exploit the relations between a text (T) and a hypothesis (H).
To explain the novelty of our approach, we ﬁrst analyze what type of knowledge a
general RTE model needs for solving the task and explain how learning algorithms
typically learn it (Section 2). In Section 2.1 we introduce the notion of ground rewrite
rules (rules without variables) and ﬁrst-order rewrite rules (rules with variables) and
describe how they are used by rule-based systems. In Section 2.2 we show that
ML algorithms can learn some of these rules, using diﬀerent types of feature
spaces. Accordingly, we propose a classiﬁcation of feature spaces in four types: the
similarity, the entailment trigger, the content, and the pair content feature spaces. We
will demonstrate that none of these spaces oﬀers the possibility to learn ﬁrst-order
rewrite rules, which are those that more eﬀectively model the relations between T
In Section 3, we will propose our solution to learn ﬁrst-order rewrite rules or
ﬁrst-order rewrite derivations via the pair feature space. This space is based on the
notion of placeholders, which explicitly model relations between T and hypothesis
H. Pairs enriched with placeholders help ML algorithms to extract and exploit ﬁrstorder rewrite rules from training examples and to apply them to classify new ones.
In Section 4 we describe an extension of the model, integrating shallow semantic
information. Finally, in Section 5, we experiment with our models and show that the
pair feature space helps in exploiting ﬁrst-order rewriting rules implicitly deﬁned in
training examples.
2 RTE models and supervised ML
Many approaches to RTE rely on rewrite rules to detect entailment between text
and hypothesis. Such rules are built at diﬀerent linguistic levels: lexical, syntactic,
and semantic. We here aim at drawing a second important distinction, between
ground and ﬁrst-order rewrite rules. The former are rules that do not allow the use
of variables, while the latter do. A ground rewrite rule can be applied to detect
implications in a very small set of cases, e.g., ‘The sun emits UVA rays’ →‘Tanning
can expose to health risks’. On the contrary, a ﬁrst-order rewrite rule can be applied
to many entailment examples, e.g., ‘X killed Y ’ →‘Y died’.
These rules thus oﬀer an appealing level of generalization that can be exploited
while either hand-crafting or automatically learning rules for RTE. Modeling feature
spaces that allow ML algorithms to discover ﬁrst-order rewrite rules is not easy, as
we show hereafter.
A machine learning approach to textual entailment recognition
In the remainder of this section, we brieﬂy outline typical rule-based approaches
for RTE (Section 2.1) and classify existing feature spaces according to the kind
of rules they encode (Section 2.2). We show that existing feature spaces do not
fully exploit ﬁrst-order rewrite rules encoded in training examples, as this needs the
introduction of variables in the feature space. This last point is the main contribution
of our paper, and it will be described in Section 3.
2.1 Rewrite rules and rule-based systems
Ground and ﬁrst-order rewrite rules are largely used to encode knowledge in RTE
systems, operating at diﬀerent levels of interpretation: lexical, syntactic, or semantic.
Ground rewrite rules transform a text into a new text. The object of the transformation appears in the rules as a ground atom. Thus, for any transformation a
diﬀerent rule is needed. Most RTE systems apply ground rules at the lexical level
 transforming a word into a new entailed word (e.g.,
chairman →president) or a sequence of words into a new sequence. At the syntactic
level, they typically transform syntactic structures (e.g., parse-trees portions) into
new ones . At the semantic level they can transform
predicative structures. Ground rewrite rules suﬀer from the limitation that they
can encode only non-generalized knowledge. For example, the rule ‘Oswald killed
JFK’ →‘JFK died’ models a commonly agreed non-generalized piece of knowledge.
Yet, it would be more eﬀective to rely on the generalized knowledge that if ‘someone
kills someone else’, then ‘someone else dies’.
First-order rewrite rules solve the previous problem, by introducing variables. In
the above example, the generalized knowledge would be captured by the rule ‘X
killed Y ’ →‘Y died’, where the Y on one side of the rule is uniﬁed with that on the
other side of the rule. Another example is the rule modeling the predicative reading
of an apposition:
First-order rewrite rules are mostly exploited at the lexical level or at the syntactic level to represent
structured knowledge of manually built or automatically acquired lexical databases. These
rules thus oﬀer an appealing level of generalization to encode knowledge for RTE.
The design of rule-based RTE systems encoding ground or ﬁrst-order rules is
particularly expensive, for two main reasons: (i) The complete coverage of the
entailment phenomenon may require large sets of speciﬁc rules; (ii) rewrite rules are
written at a given language interpretation level: good rules applied at wrong levels
of sentences interpretation can lead to wrong decisions.
Typical approaches address these problems using a weighting schema. A weight is
assigned to each rule when rules are applied to transform the text into the hypothesis.
F. M. Zanzotto et al.
These rule weights are used to compute a score for the overall transformation,
representing the validity of the whole process. A manually derived threshold is then
applied to the score to determine the polarity of the entailment.
2.2 ML models for RTE
ML algorithms can alleviate the rule design process described in the previous section
in two ways: (1) determining the weight of known rules and the threshold of the
overall process; (2) discovering previously unknown rules. The major issue in using
ML approaches is the deﬁnition of a representation of text and hypothesis which
allows for an eﬀective learning of the entailment recognition rules. In other words,
we need to deﬁne features able to capture the knowledge enclosed in the typical
rewrite rules used by rule-based systems.
By carefully examining previous work, we note that most ML-based systems for
RTE perform one of the following functions: (i) apply similarity measures between
text (T) and hypothesis (H) ; (ii) extract content from the T and H pairs ; (iii) deﬁne rules that strongly suggest the implications (triggers) ; or (iv) extract more general features, e.g., word
or syntactic construction pairs, that describe all the possible rewrite rules encoded
According to the above classiﬁcation, we deﬁne four diﬀerent types of feature
spaces: similarity space, content, entailment trigger, and paired-content feature
2.2.1 Similarity feature space
In this space, the basic hypothesis is that if two sentences are similar, then they are
likely to be in entailment. Similarity between T and H can be captured in diﬀerent
ways and at diﬀerent levels (lexical, syntactic, and semantic) . Each feature encodes a diﬀerent similarity between T and H. Most
RTE systems use a feature space at the lexical level (hereafter called lex space).
For example, a feature could count the percentage of content words of H that are
equal to words in T or that are semantically related to words in T . Another feature could model the length of the longest common
subsequence (LCS) between T and H :
the longer the LCS, the more likely it is that the meaning of H is included in the
meaning of T. At the syntactic level, a feature could represent the percentage of
dependencies that H has in common with T or the longest common subtree
between H and T . At the semantic level, a feature
could be the percentage of semantic relations of H shared with T.
In terms of rewriting rules, these feature spaces generally exploit lexical ground
rules such as those that connect semantically related words and, basically, only one
ﬁrst-order rule, i.e., the identity rule that transforms X →X.
A machine learning approach to textual entailment recognition
Limits. The similarity feature space produces eﬀective entailment classiﬁers but
is not suﬃcient to model RTE: the fact that two texts are similar does not always
imply that they are in entailment. For example, at the lexical level, if fragments
diﬀer only by the presence of a negation, they are not in entailment, even if their
lexical similarity is very high. Similarly, at the syntactic level, two very dissimilar
fragments can be still in entailment when a syntactic alternation takes place (e.g.,
active/passive, paraphrases) or when downward and upward monotonicity is involved,
as in the following example:
‘At the end of the year, all solid companies pay dividends’
‘At the end of the year, all solid insurance companies pay dividends.’
‘At the end of the year, all solid companies pay dividends’
‘At the end of the year, all solid companies pay cash dividends.’
In the example, T2 entails H2 but it does not entail H3: in a lexical similarity feature
space, these two examples would have the same vector.
2.2.2 The content feature space
Similarity feature spaces constrain the description of entailment phenomena to
simple similarity. These spaces are then unable to describe properties which are
contained in T or H. The content feature space (hereafter cont) aims at solving
this drawback by modeling the content of T and H. This can include lexical,
syntactic, or semantic features of the two fragments. Speciﬁcally, T and H are
separately represented by two distinct and independent sets of features. The major advantage of this space is that rewrite rules can be automatically learned
from a training set and successively applied to determine the polarity of a test
For example, consider the space of syntactic subtrees F. The features are the set
of all subtrees h ∈F of H and the set of all subtrees t ∈F of T. Such space is
useful for solving complex cases that cannot be processed in other ones, like the
examples (T2, H2) and (T3, H3) reported in Section 2.2.1. Let us assume that the T
F. M. Zanzotto et al.
and the H of the two examples are represented as syntactic trees, as follows:
While in the similarity feature space (T4, H4) and (T5, H5) are identical, here they are
represented by two diﬀerent feature vectors, since H4 and H5 have diﬀerent syntactic
structures (‘all solid insurance companies’ is diﬀerent from ‘all solid companies’ and
‘dividends’ is diﬀerent from ‘cash dividends’). If we then want to classify the example
T and H structures are globally more similar (i.e., in terms of number of common
tree fragments) to (T4, H4) than to (T5, H5).
These feature spaces model independently the right-hand side and the left-hand
side of ground rewrite rules. ML algorithms can learn both new ground rules or
ground rule fragments and their associated weights.
Limits. Since the feature spaces of T and H are independent, rules that exploit the
relations between some properties of T and some properties of H cannot be derived;
i.e., most of the properties should be matched to trigger the selection of the best
representative training example. For example, the pair ‘Oswald killed JFK’ →‘JFK
died’ cannot be used to determine the polarity of ‘Oswald killed JFK by shooting
him several gun bullets’, ‘JFK died’, since there is too much diﬀerence in terms of
syntactic/semantic content in the fragments.
A machine learning approach to textual entailment recognition
2.2.3 The entailment trigger feature space
A limit of the previous space is that it does not extract joint features from a
(T, H) pair. The entailment trigger feature space (hereafter trig) overcomes such a
limitation (along with those of the similarity feature space) by modeling complex
relations between T and H. The underlying hypothesis is that entailment holds (or
does not) if speciﬁc rewrite rules (i.e., triggers) stand between T and H. Triggers
can be either positive or negative, if they respectively suggest entailment or don’t.
Each feature of the space represents a speciﬁc trigger; i.e., its value can be 0 or 1
whether the trigger is present or not in the given (T, H) pair. This approach has
been successfully explored in de Marneﬀe et al. and MacCartney et al. 
by means of the following triggers:
• Polarity features. Presence/absence of negative polarity contexts (not, no or
few, without), as in ‘Oil price surged’ ↛‘Oil prices didn’t grow’.
• Antonym features. Presence/absence of antonymous words in T and H. These
features capture cases such as ‘Oil price is surging’ ↛‘Oil prices is falling
• Adjunct features. Dropping/addition of syntactic adjunct when moving from
T to H, as in ‘all solid companies pay cash dividends’ →‘all solid companies
pay dividends’.
• Passive features. Presence/absence of a transformation from active to passive,
moving from T to H or vice versa.
These feature spaces model speciﬁc ground or ﬁrst-order rewrite rules. ML
algorithms learn the weights of the rules and the ﬁnal threshold to apply.
Limits. The trigger feature space has two limitations: (1) rules should be all
known and hand-coded in advance; (2) rule composition cannot be explicitly stated,
since features are ﬂat and independent entities—i.e., the feature space can only
model that two triggers are true, but cannot directly express the fact that one trigger
must be applied before another in order to predict a true or false entailment.
2.2.4 The paired-content feature space
Similarly to the cont space, the underlying hypothesis of the paired-content feature
space (hereafter p cont) is that evidence of entailment (rules and distances) should
emerge directly from the explicit content of T and H, instead of being manually
coded a priori. Yet, here T and H are not represented by two independent feature
sets. Instead, in this space a (T,H) pair is represented by pairs of features from T
and H so that the learner can acquire ground rewrite rules (i.e., relational properties
between the two texts). The paired content can be either a lexical, a syntactic, or a
semantic representation of the two fragments.
As an example, consider the space of syntactic subtrees F introduced in the
previous section; p cont is the set of subtree pairs ⟨t, h⟩∈F × F, where t and h
are two subtrees of T and H respectively. Some meaningful pairs may suggest the
F. M. Zanzotto et al.
syntactic properties, e.g., triggers, which T and H must have to be in an entailment
relation. This allows to overcome the limits of the cont space, since only the properties
of H and T that model a useful trigger are matched. For example, suppose that H5
contains an irrelevant part (e.g., ‘All solid companies pay cash dividends, but other
companies do not’). In cont, H6 would now result more similar to H4 than to H5
because it shares a higher percentage of subtrees with H4. The entailment prediction
of the system would then be incorrect. Instead in the p cont space, the model for
(T4, H4) contains the feature (fragment pair)
which does not depend on any additional content. This feature is also present in
the feature space of (T6, H6), but not in (T5, H5), suggesting that (T6, H6) is correctly
more similar to (T4, H4). In this case, the above feature can be then considered as a
ground rewrite rule.
The p cont space models ground rewrite rules. According to the learning examples,
ML algorithms select interesting rules, learn the associated weights, and determine
the ﬁnal threshold to apply.
Limits. The paired-content feature space allows learning only ground rewrite
rules. To learn ﬁrst-order rewrite rules we need to introduce variables in the text
representation, i.e., by making explicit the relations between elements in the texts
and elements in the hypotheses. As it is, p cont can induce incomplete or erroneous
rewrite rules. For example, consider the following entailment pair:
‘Yahoo bought Overture’
‘Yahoo owns Overture’
When p cont is built on syntactic structures, it contains (among the others) the
structure pair
A machine learning approach to textual entailment recognition
This may be an important entailment trigger. The problem is that this trigger is
contained in both the following entailment cases:
‘Wanadoo bought KStones’
‘Wanadoo owns KStones’
‘Wanadoo bought KStones’
‘KStones owns Wanadoo’
where T10 entails H10 whereas T11 does not entail H11.
This suggests that ground feature pairs (rules) are not powerful enough to
generalize diﬀerent examples. In contrast, with the use of variables, ﬁrst-order
rewrite rules solve the problem; e.g., the rule
applies (belong) to only the ﬁrst example.
3 Learning ﬁrst-order rules in a syntactic paired-content feature space
In the previous section we presented four diﬀerent feature spaces along with their
limits and properties. It is interesting to notice that in terms of expressiveness, the
similarity and the content feature spaces (lex and cont) are orthogonal and can
be both included in the trigger feature space. With the latter, we mean a space in
which features/rules are manually selected at the underlying representation layer,
e.g., syntactic parse trees or shallow semantic structures, of (T, H).
Moreover, the p cont space learns (i.e., generates) ground rules in terms of feature
pairs. In order to allow it to model ﬁrst-order rules, we need to introduce variables
in the representation layer. To do so, we use kernel methods and support vector
machines (SVMs). In this framework, we deﬁne a space and the related kernel
function which allows to extract and exploit ﬁrst-order rewrite rules from annotated
The remainder of this section is organized as follows: ﬁrst, we introduce the idea
of learning ﬁrst-order rewrite rules (Section 3.1); second, we describe how a pair
feature space including variables can be obtain from examples (Section 3.2); third,
we discuss how to obtain these feature spaces by using kernel functions (Section 3.3).
F. M. Zanzotto et al.
3.1 Learning ﬁrst-order rewrite rules
Our proposal for learning ﬁrst-order rewrite rules stems from the observation that
the trig and the p cont spaces are strictly related. Indeed, if we restrict trig to model
only ground rewrite rules, then it represents a subset of p cont. For example, the
is included in the syntactic content feature space (deﬁned in Section 2.2.4), as it
models the corresponding feature:
As shown in the previous sections, typical handcraft rules contain variables. Thus,
to obtain the same expressiveness of the entailment trigger feature space with the
paired-content feature space, we need to ﬁnd a way to include variables as content.
This allows ML algorithms to learn ﬁrst-order rules implicitly described in training
examples. In such space a pair ⟨T, H⟩is represented as follows:
P = {⟨ft, fh⟩: ft ∈G(T), fh ∈G(H)}
where G(T) and G(H) are the sets of features derivable from a structured representation of T and H. If in G(T) and G(H) variables are somehow deﬁned, each pair
⟨ft, fh⟩represents in general a ﬁrst-order derivation described in the (T, H) example.
3.2 Three syntactic pair feature spaces
We have shown that the p cont space is the most promising to encode eﬀective
knowledge for Textual Entailment (TE). However, as diﬀerent linguistic levels can
be adopted to represent T and H (lexical, syntactic, semantic), we here need to
choose the most relevant, in order to better focus our study.
For this purpose, we note that a large part of the entailment cases depend on the
syntactic structure of T and H . More speciﬁcally,
grammar rules are most useful, as they can reduce data sparseness by generalizing
word sequences expressed with the same syntax. In our case, the set P in (1) can
be generalized by using syntactic derivations (i.e., the sequence of production rules)
that in turn generate word sequences in the training examples.
We here present three feature spaces (which are subsets of the more general
paired-content feature space) that capture the above intuition: a ground syntactic
rule feature space and two ﬁrst-order syntactic rule feature spaces. The ﬁrst space is
used as the basis space to deﬁne the other two.
A machine learning approach to textual entailment recognition
3.2.1 A ground syntactic rule feature space
The syntactic paired-content feature space (synt) models entailment pairs using the
set of tree fragment pairs , similar to the syntactic content feature space. A pair ⟨T, H⟩is represented as
P τ = {⟨τt, τh⟩: τt ∈F(T), τh ∈F(H)}
where F(·) indicates the set of fragments of the sentence parse tree given as
argument. For instance, given T4 and H4 of the example in Section 2.2.4, we have
the following relational description:
⟩, . . . }
This clearly models ground rewrite derivations between T and H; e.g., the pair
⟨[VP [VBP bought] [NP]], [VP [VBP own] [NP]]⟩models the ground rewrite rule [VP [VBP bought]
[NP]] →[VP [VBP own] [NP]].
3.2.2 Two ﬁrst-order syntactic rule feature spaces
In this section, we have proposed two ﬁrst-order syntactic rule feature spaces: the
syntactic pair feature space with placeholders in the preterminal nodes (plac basic)
and the syntactic pair feature space with propagated placeholders (plac all).
plac basic. This space introduces variables in the pairs, by applying an anchoring
algorithm, which works as follows.
Before deriving the tree fragments we augment the syntactic tree with placeholders. A placeholder is a label assigned to an anchor. Anchors are nodes
from τt and τh dominating the same (or similar) information. As many other
approaches ,
our anchoring model is based on a similarity measure between words simw(wt, wh).
Speciﬁcally, we anchor the content words (verbs, nouns, adjectives, and adverbs)
in the hypothesis WH to words in the text WT, by using a two-step greedy
algorithm.
In the ﬁrst step, each word wh in WH is connected to all words wt in WT that
have the maximum similarity simw(wt, wh) with it. (More than one wt can have the
maximum similarity with wh.) As result, we have a set of anchors A ⊂WT × WH;
simw(wt, wh) is computed by means of three techniques:
(1) Two words are maximally similar if they have the same surface form, wt = wh.
(2) Otherwise, WordNet similarities 
and diﬀerent relation between words such as verb entailment and derivational
morphology are applied.
F. M. Zanzotto et al.
(3) The edit distance measure is ﬁnally used to capture the similarity between
words that are missed by the previous analysis (for misspelling errors or for
the lack of derivational forms in WordNet).
In the second step, we select the ﬁnal anchor set A′ ⊆A, such that ∀wt (or wh)
∃!⟨wt, wh⟩∈A′. The selection is based on a simple greedy algorithm. Given two pairs
⟨wt, wh⟩and ⟨w′
t, wh⟩to be selected and a pair ⟨st, sh⟩already selected, the algorithm
considers word proximity (in terms of number of words) between wt and st and
between w′
t and st, and it chooses the nearest word.
Once the set A′ is found, anchors are encoded in the syntactic trees with
placeholders. Placeholders are put on the preterminal nodes of the anchored words.
For example, the pair (T10, H10) can be augmented with placeholders as follows:
We then obtain the following richer representation based on fragment pairs:
Placeholders (or variables) X and Y specify that the NNPs labeled by the same
variables dominate similar or identical words. The ﬁrst pair of the set P τp describes
a ﬁrst-order rewriting derivation between T and H. Therefore a similar but negative
entailment example
A machine learning approach to textual entailment recognition
will have a diﬀerent P τp representation:
Placeholders are inverted, as the subject of T15 is identical to the object of H15 and
not vice versa. Although some of the components of such pairs can still be matched
with those from T14 and H14, a large part of the pairs (the actual features) are
not matched. This suggests that the learning algorithm uses very diﬀerent features
representing diﬀerent ﬁrst-order rewrite rules.
It should be noted that the pair ⟨[S [NP VP]], [S [NP VP]]⟩still belongs to both examples.
This depends on the fact that placeholders are only located on preterminal symbols,
whereas NP and VP are more internal.
plac all. In order to further diﬀerentiate relational features, in plac all placeholders
are allowed to climb toward the root, according to the following policy: The
constituent nodes in the syntactic trees take the placeholder of their semantic heads,
so that any subtree will contain relational information. For example, in the more
complex entailment pairs
placeholders are propagated toward the root, and when there is a collision between
the placeholder of a constituent, e.g., the NP containing the head, and the placeholder
of another constituent, e.g., PP, the former is preferred.
Relational information between important concepts of text and hypothesis is
described by plac basic and plac all. However, there are two computational problems
that need to be solved:
• The number of relational fragment pairs is exponential, since also the number
of fragments is exponential in the number of words in T and H. Similar
problems are usually tackled by extracting only a small subset of relevant
features. Unfortunately, in our case the phenomenon to be modeled is too
complex to allow the identiﬁcation of such a subset. We then apply a novel
F. M. Zanzotto et al.
Fig. 1. A syntactic parse tree.
approach, using kernel methods to implicitly generate such huge spaces. In
the next section, we present syntactic tree kernels , which allows the generation of all possible fragments of texts and
hypothesis.
• Placeholders used to describe a (T, H) pair may not be comparable with
placeholders used in a second pair; e.g., a pair may have more placeholders
than the other. Thus, when comparing the fragment pairs from one instance,
we need to ﬁnd the optimal correspondences with the sets of placeholders
of the second instance. Section 3.3.3 shows our approach embedded in tree
kernel functions.
3.3 Kernels for the syntactic paired-content feature spaces
The size of the above feature spaces is exponential. Kernel functions oﬀer the
possibility to deﬁne implicitly these spaces. In this section we propose a kernel
function to deﬁne the ground and ﬁrst-order spaces. We ﬁrst introduce the tree
kernel functions in Section 3.3.1. Then, we describe how we use this function to
deﬁne kernels for synt (Section 3.3.2) and for plac basic and plac all (Section 3.3.3).
3.3.1 Tree kernel functions
Tree kernels represent trees in terms of their substructures (fragments) which are
mapped into feature vector spaces, e.g., ℜn. A kernel function measures the similarity
between two trees by counting the number of their common fragments. For example,
Figure 1 shows some substructures for the parse tree of the sentence ‘book a flight’.
The main advantage of tree kernels is that to compute the substructures shared by
two trees τ1 and τ2, the whole fragment space is not used. In the following, we report
the formal deﬁnition presented in Collins and Duﬀy .
Given the set of fragments {f1, f2, . . .} = F, the indicator function Ii(n) is equal
to 1 if the target fi is rooted at node n and 0 otherwise. A tree kernel is then deﬁned
A machine learning approach to textual entailment recognition
TK(τ1, τ2) =
where Nτ1 and Nτ2 are the sets of the τ1’s and τ2’s nodes, respectively, and Δ(n1, n2) =
i=1 Ii(n1)Ii(n2). The latter is equal to the number of common fragments rooted in
the n1 and n2 nodes, and Δ can be evaluated with the following algorithm:
(1) if the productions at n1 and n2 are diﬀerent, then Δ(n1, n2) = 0;
(2) if the productions at n1 and n2 are the same, and n1 and n2 have only leaf
children (i.e., they are preterminal symbols), then Δ(n1, n2) = 1;
(3) if the productions at n1 and n2 are the same, and n1 and n2 are not preterminals, then
Δ(n1, n2) =
where nc(n1) is the number of the children of n1 and cj
n is the jth child of the node
n. Note that since the productions are the same, nc(n1) = nc(n2).
Additionally, we add the decay factor λ by modifying steps (2) and (3) as follows:1
(2) Δ(n1, n2) = λ,
(3) Δ(n1, n2) = λ
The computational complexity of (3) is O(|Nτ1|×|Nτ2|), although the average running
time tends to be linear .
The next section shows a technique to assign the same placeholders to similar text
and hypothesis pair.
3.3.2 Kernel for the ground rule space
Given the above tree kernel functions, the deﬁnition of a kernel Ks(⟨T, H⟩, ⟨T ′, H′⟩)
for a ground syntactic rule feature space (i.e., synt) is
Ks(⟨T, H⟩, ⟨T ′, H′⟩) = TK(T, T ′) × TK(H, H′)
Also, the p cont space can be simply obtained using the product . Unfortunately (and surprisingly)
when huge kernel spaces are multiplied according to the Cartesian product, the
resulting number of features is extremely high, and also a robust algorithm like
SVMs becomes subject to the curse of high dimensionality. In other words, too
many irrelevant features make those relevant ineﬀective.
The solution of this problem for TE is proposed in Zanzotto and Moschitti 
and Moschitti and Zanzotto and reported in the next section (see (6)). It is
1 To have a similarity score between 0 and 1, we also apply the normalization in the kernel
space, i.e., K′(τ1, τ2) =
TK(τ1,τ1)×TK(τ2,τ2).
F. M. Zanzotto et al.
possible to show2 that placeholders determine a link between the fragments of the
text and the hypothesis, which are produced by two distinct tree kernels and merged
by the simple kernel sum. This allows us to approximate the paired feature space.
The advantage is that only the fragments marked by placeholders (i.e., those very
interesting for the target problem) will be paired. This reduced number of feature
pairs is easily manageable by SVMs. As we want to compare the synt space with
the plac basic and plac all, we adopted the same approximation in the computation
of the kernel in (5); i.e., we use the sum instead of the product.
3.3.3 Matching placeholder-based features
Deﬁning kernel functions for plac basic and plac all is not trivial. Tree kernels
applied to two texts or two hypotheses match identical fragments. When placeholders
are added to trees as in plac basic and plac all, the labeled fragments are matched
only if the basic fragments and the assigned placeholders match. For example, let
us compare the pair (T16, H16) of Section 3.2 with the following (T10, H10):
The two pairs share many common features such as
Yet, a simple use of the tree kernel function can lead to missing these common
features. In (T16, H16) Y is 3 while in (T17, H17) it is 2 . To detect this feature with
simple tree kernel functions we need to ﬁnd a correct mapping between placeholders
in (T16, H16) and in (T17, H17). It is straightforward to note that the correspondences
1 = 1 and 3 = 2 allow more substructures (i.e., large part of the trees) to be identical.
Although, there may be several approaches to accomplish this task, we apply a
basic heuristic which is very intuitive:
Choose the placeholder assignment that maximizes the tree kernel function over all
possible correspondences.
More formally, let A and A′ be the placeholder sets of ⟨T, H⟩and ⟨T ′, H′⟩,
respectively; without loss of generality, we consider |A| ≥|A′|, and we align a subset
2 Although interesting, this aspect is beyond the purpose of this paper.
A machine learning approach to textual entailment recognition
of A with A′. The best alignment is the one that maximizes the syntactic and lexical
overlapping of the two subtrees induced by the aligned set of anchors. By calling C
the set of all bijective mappings from S ⊆A, with |S| = |A′|, to A′, an element c ∈C
is a substitution function. We deﬁne the best alignment cmax the one determined
cmax = argmaxc∈C(TK(t(T, c), t(T ′, i)) + TK(t(H, c), t(H′, i))
where (i) t(·, c) returns the syntactic tree enriched with placeholders replaced by
means of the substitution c, (ii) i is the identity substitution, and (iii) TK(τ1, τ2)
is a tree kernel function (e.g., the one speciﬁed by (3)) applied to the two trees τ1
At the same time, the desired similarity value to be used in the learning algorithm is
given by TK(t(T, cmax), t(T ′, i)) + TK(t(H, cmax), t(H′, i), i.e., by solving the following
optimization problem:
Kp(⟨T, H⟩, ⟨T ′, H′⟩) = maxc∈C(TK(t(T, c), t(T ′, i)) + TK(t(H, c), t(H′, i))
As a ﬁnal remark, it should be noted that (a) Ks(⟨T, H⟩, ⟨T ′, H′⟩) is a symmetric
function, since the set of derivation C are always computed with respect to the pair
that has the largest anchor set, and (b) it is not a valid kernel, as the max function
does not in general produce valid kernels. However, in Haasdonk , it is shown
that when kernel functions are not positive semideﬁnite like in this case, SVMs still
solve a data separation problem in pseudo-Euclidean spaces. The drawback is that
the solution may be only a local optimum. Nevertheless, such a solution can still be
valuable, as the problem is modeled with a very rich feature space.
3.4 Reﬁning cross-pair syntactic similarity
The eﬃciency of the kernel approach proposed in the previous section should be
improved to favor its applicability with SVMs. This can be done by decreasing
the computational complexity of (6) and by pruning irrelevant information in large
syntactic trees.
Controlling the computational cost. The computational cost of cross-pair similarity
between two tree pairs (6) depends on the size of C. This is combinatorial in the
size of A and A′, i.e., |C| = (|A| −|A′|)!|A′|! if |A| ≥|A′|. Thus we should keep the
sizes of A and A′ reasonably small.
To reduce the number of placeholders, we consider the notion of chunk deﬁned in
Abney , i.e., not recursive kernels of noun, verb, adjective, and adverb phrases.
When placeholders are in a single chunk in both the text and the hypothesis we
assign them the same name. The placeholder reduction procedure also gives the
possibility of resolving the ambiguity still present in the anchor set A. A way to
eliminate the ambiguous anchors is to select those that reduce the ﬁnal number of
placeholders. Finally, in Moschitti and Zanzotto , a more eﬃcient algorithm
for computing the kernel Ks is presented together with its training and testing
F. M. Zanzotto et al.
Pruning irrelevant information in large text trees. Often only a portion of the parse
trees is relevant to detect entailments. For instance, let us consider the following pair
from the RTE1 corpus:
‘Ron Gainsford, chief executive of the TSI, said: “It is a major concern to
us that parents could be unwittingly exposing their children to the risk of
sun damage, thinking they are better protected than they actually are”.’
‘Ron Gainsford is the chief executive of the TSI.’
Only the bold part of T supports the implication; the rest is useless and also
misleading: if we used it to compute the similarity it would reduce the importance
of the relevant part. Moreover, as we normalize the syntactic tree kernel with
respect to the size of the two trees, we need to focus only on the part relevant to
the implication. The anchored leaves are good indicators of relevant parts, but also
some other parts may be very relevant. For example, the function word not plays
an important role.
The reduction procedure that we apply can be formally expressed as follows:
given a syntactic tree t, the set of its nodes N(t), and a set of anchors, we build a
tree t′ with all the nodes N′ that are anchors or ancestors of any anchor. Moreover,
we add to t′ the leaf nodes of the original tree t that are direct children of the
nodes in N′. We apply such procedure only to the syntactic trees of texts before the
computation of the kernel function.
4 Toward a semantic pair feature space
For modeling RTE, plac basic and plac all are appealing spaces, as they learn
generalized rewrite rules. Unfortunately, these models suﬀer from a major problem
which limits their applicability: they can only learn rules based on syntax and on
simple lexical–semantic evidence at the leaf level, while higher levels of semantic
information are neglected. In particular, lexical–semantic knowledge is only used
to ﬁnd placeholders, by aligning two semantically similar words. Yet, the semantic
relations between words linked by placeholders are not considered in the ﬁnal
models. This limitation causes the algorithm to infer erroneous ﬁrst-order rewrite
rules. Suppose for example that the model leveraging pairs (T10, H10) has to learn
the following rule:
where the placeholder
y anchors buy and own. This rule is useful to classify examples
A machine learning approach to textual entailment recognition
‘Romans conquered Gallia’
‘Romans governed Gallia’
where the relation between the two anchored verbs conquer and govern is ‘causation’,
as for buy and own. In WordNet , own entails buy as well as govern
entails conquer. Yet, the rule will fail when used for
‘Oswald assassinated J.F. Kennedy’
‘Oswald poisoned J.F. Kennedy’
where assassinate and poison are anchored as generically similar verbs. The limitation
of the syntactic pair feature spaces is that placeholders do not convey the semantic
knowledge needed in cases such as the above, where the semantic relation between
connected verbs is essential.
In this section, we show that these models can be easily extended to include
shallow semantic information. We present the syntax-semantic pair feature space
which solves some of the above limitations by introducing the notion of typed
anchors. The idea is to enrich the syntactic trees of text and hypothesis with the
relational semantic information standing between anchored words. Operationally, we
do so by assigning a semantic tag expressing the semantic relation to placeholders. In
the example above, by making explicit the entailment relation own ←buy, we obtain
the following correct rule, where the placeholder
y is assigned the ←entailment
Of course in case there is no implication between the two verbs we would have a
diﬀerent fragment pair, since the type on
y will be diﬀerent, i.e., →.
Formally, our syntactic–semantic pair feature space is an extension of plac all,
where the trees are now enriched with semantic typed anchors:
P σ = {⟨σ(ft), σ(fh)⟩: ft ∈F(T), fh ∈F(H)}
where σ enriches fragments with typed anchors. In order to operationally implement
the model, we need to solve two issues: (i) decide what type of semantic relations we
want to represent in the typed anchors (Section 4.1); (ii) deﬁne a policy to encode
this information in the tree; i.e., decide at which level(s) of the tree the anchor type
must be encoded (Section 4.2).
F. M. Zanzotto et al.
Table 1. Ranked anchor types
Relation type
verb entailment
similarity
surface matching
4.1 Deﬁning anchor types
In the literature, many attempts to introduce semantic information in RTE systems
have failed. One of the main reasons for this failure is that any model using
semantic information deals with ambiguity. To overcome this issue, we focus on a
controlled set of relevant relation types, deﬁned in WordNet: part-of, antinomy, and
verb entailment. This controlled set has been chosen because it is relevant for a large
part of entailment cases.3
We also deﬁne two more general anchor types: similarity and surface matching.
The ﬁrst type links words which are similar according to the WordNet similarity
measure described in . This type is intended to capture
synonymy and hyponymy. The second type is activated when words or lemmas match,
capturing semantically equivalent words. The complete set of relation types used in
the experiments is given in Table 1.
4.2 Policies for augmenting placeholders with anchor types
To integrate anchor types in the syntactic tree, the main problem is to decide how the
semantic information should be encoded, i.e., where the new typed labels should be
most eﬀectively integrated. We experiment with two possible feature space models:
Typed anchor model (ta). Anchor types augment only the preterminal nodes of the
syntactic tree;
Propagated typed anchor model (tap). Anchors climb up in the syntactic tree according to some speciﬁc climbing-up rules, similar to what done for placeholders.
The ta model is easy to implement: typed anchor simply augment the preterminal
nodes of anchored words.
The tap model allows anchor types to climb up in the syntactic tree, repeating the
anchor type information in many fragments, which are compared by the tree kernel
function. This guarantees that the type information is used in the decision process.
The tap model is more complex with respect to ta, as it depends on the strategy
3 For the part-of relation, transitivity is not used: we use only connected words that are in
directly related synsets. For antinomy, inheritance is not used: we anchor words with an
antinomy relation only if these words are in directly related synsets.
A machine learning approach to textual entailment recognition
adopted for the anchor climbing-up. In particular, the strategy must account for
how anchors that climb up to the same node should interact. We implement our
strategy by using climbing-up rules as done in the case of placeholders. Yet, in our
case rules must consider the semantic information of the typed anchors. The choice
of correct climbing-up rules is critical, as an incorrect rule could completely alter the
semantics of the tree, as we show in later examples. In the case of placeholders, the
climbing-up rule states that a constituent in the syntactic tree takes the placeholder
of its semantic head. It is easy to demonstrate that in the case of typed anchors
this rule would have disastrous eﬀects. For example, consider the following false
entailment pair:
In the example, we apply the above-mentioned rule: the typed anchor = 3 climbs up
to the preterminal node NP, instead of the typed anchor ↔2 , as it is the head of the
constituent. If modeled in this way, this false entailment pair could generate, among
others, the incorrect rewrite rule
which states the following:
if two fragments have the same syntactic structure S(NP, VP(VBZ, NP)), and there
is a semantic equivalence (=) on all constituents, then entailment does not hold.
This rule is wrong, as all substructures are semantically equivalent.
The problem is that the wrong typed anchor climbed up the tree: we need the
antinomy anchor on the adjective (tall/short) to climb up, instead of the matching
anchor on the noun (boy/boy), in order to learn a correct rule. Our strategy must
then implement a climbing-up rule producing these trees:
F. M. Zanzotto et al.
In this case the pair generates correct rewrite rules, such as
The rule states the following:
if two fragments have the same syntactic structure S(NP1, VP(VBZ, NP2)), and
there is an antonym type (↔) on the S and NP2 , then entailment does not hold.
The above example shows that the anchor type that has to climb up depends on
the structure of the constituents; thus climbing-up rules depend on the structure. The
algorithm to encode such dependency can be very complex. Luckily, this intuition
can be also captured by a simpler approximation. Instead of having climbing-up
rules for each constituent type, we can rely on a ranking of the anchor types (as the
one reported in Table 1). The anchor type that climbs up is the one that has a higher
rank. In the example, this strategy produces the correct solution, as antinomy has
a higher rank than surface match. We then implement in our model the following
climbing-up rule:
If two typed anchors climb up to the same
node, give precedence to that with the highest
ranking in the ordered set of types T = (↔,
⊂, ←, ≈, =).
Our ordered set T is consistent with common-sense intuitions. In the experimental
section we will empirically demonstrate its validity by reporting experiment evidence.
5 Experimental evaluation
In the previous sections, we have deﬁned several feature spaces, and we have shown
that plac basic and plac all can encode richer and more expressive features than
simpler spaces (namely, lex, cont, p cont, and synt) in SVMs.
Our experiments aim at empirically showing the above claim, where the representation layer used to manually or automatically extract features is constituted by
automatically generated parse trees. Moreover, we show that plac basic and plac all
can be successfully extended with semantic information by creating the new spaces
ta and tap.
Our experiments are organized as follows: Section 5.2 shows that plac basic and
plac all outperforms synt. This suggests that ground syntactic rules learned from synt
are less powerful than the ﬁrst-order rules learnable from plac basic and plac all.
Unfortunately, the above outcome is less evident when the simple lex is added to
the previous models as shown in Section 5.3; the extreme eﬀectiveness of the latter
tends to make ﬂat the contribution of the other feature spaces. To support this
interpretation, in Section 5.4 we show, by means of learning curves, that plac basic
A machine learning approach to textual entailment recognition
Table 2. Feature spaces used in the experiments
Feature space
Syntactic pair
Syntactic pair with placeholders on the preterminal nodes
(plac basic)
Syntactic pair with propagated placeholders
(plac all)
Syntactic pair with typed anchors on the preterminal nodes
Syntactic pair with propagated typed anchors
Lexical similarity
Simple entailment trigger
and plac all expressing ﬁrst-order syntactic rules are able to learn from examples,
whereas lex reaches immediately a plateau.
Moreover, the ﬁrst-order-based models used in combination with the similarity
features improve the latter. As a ﬁnal analysis, experimental results in Section 5.5
show that ﬁrst-order rule feature spaces are also suited for including the semantics
of typed anchors (ta and tap).
5.1 Experimental settings
For the experiments, we used the RTE Challenge datasets: RTE1 , RTE2 , and RTE3 . These
sets contain respectively 1367, 1600, and 1600 training/testing instances, evenly split
between positive and negative examples. The RTE set is the union of the three sets.
We also used the following resources:
• the Charniak parser and the morpha lemmatizer to carry out the syntactic and morphological
• WordNet 2.0 to extract the verbs in entailment, the derivationally related words, and the antonymous words used both for ﬁnding and for
typing anchors;
• the wn::similarity package to
compute the similarity function for ﬁnding anchors between the text T and
the hypothesis H and to compute the lexical similarity (lex) in the similarity
feature space we used for comparison;
• SVM-light-TK4 which encodes the basic tree kernel function, in SVM-light .
The feature sets used in the experiments are reported in Table 2.
5.2 First-order versus ground syntactic feature spaces
In a ﬁrst set of experiments, we compare the two ﬁrst-order spaces plac basic and
plac all against the ground space, synt.
4 SVM-light-TK is available at 
F. M. Zanzotto et al.
Table 3. Mean accuracy and standard deviation within diﬀerent pair feature spaces:
n-fold cross-validations repeated m times
plac basic
2-fold × 4
55.18 (±0.92)
55.34 (±1.11)
56.12 (±1.08)
2-fold × 4
55.02 (±1.34)
58.99 (±1.56)
61.26 (±1.68)
2-fold × 4
50.12 (±1.26)
59.92 (±1.36)
62.29 (±1.51)
6-fold × 5
54.07 (±1.43)
60.31 (±1.44)
58.27 (±1.53)
We run four diﬀerent experiments by repeating m times in an n-fold crossvalidation on the RTE1, RTE2, and RTE3 and RTE datasets. The results are
reported in Table 3: the ﬁrst column shows the dataset; the second describes the
number of folds and the number of times the experiment has been carried out;
the third, the fourth, and the last column report the averaged accuracy along with
the standard deviation when using synt, plac basic, and plac all. The results show
the following: (a) The accuracy obtained with plac all is always signiﬁcantly better
than the accuracy obtained with synt, especially for the RTE2 and the RTE3 sets.
(b) In the case of RTE1 and RTE2, the accuracy produced by synt is roughly
equal to the one produced by plac basic. Indeed, plac basic diﬀers from synt only
in the leaves. (Placeholders are assigned only to the preterminal nodes.) In other
words, only few fragments contain relational information, i.e., placeholders. We can
conclude that a signiﬁcant improvement can only be observed when moving from
plac basic to plac all which better describes ﬁrst-order rules. (c) In the case of RTE3,
the assignment of placeholders to preterminal nodes already yields an important
improvement (cf. synt with plac basic).
The above results suggest that our spaces are able to model a richer set of rules,
thanks to the use of variables. We also claim that such space includes most of
the entailment trigger-based features. To show the validity of this statement, we
performed an experiment combining synt and plac all with the simple entailment
trigger feature space (trig).
For trig, we used three features representing three diﬀerent rules, similar to Hickl
et al. , Imkpen et al. , and Snow, Vanderwende and Menezes :
(1) SVO that tests if T and H share a similar subject–verb–object construct; (2)
Apposition that tests if H is a sentence headed by the verb to be and if in T there is
an apposition that states H; (3) Anaphora that tests if the SVO sentence in H has a
similar wh-sentence in T and if the wh-pronoun may be resolved in T with a word
similar to the object or the subject of H.
Results in Table 4 show that synt + trig accuracy is lower than the one of synt,
suggesting that the two feature spaces are diﬀerent, and it is complex to merge
them together. In contrast, since the ﬁrst-order syntactic rule feature space encodes
already the ﬁrst-order rules of trig the accuracy of plac all+trig is not signiﬁcantly
diﬀerent from plac all. (SVMs are very robust to redundant features.)
A machine learning approach to textual entailment recognition
Table 4. Mixing syntactic pair feature spaces with entailment trigger feature spaces
synt + trig
plac all + trig
2-fold × 4
54.80 (±1.26)
53.66 (±1.00)
59.56 (±0.84)
59.26 (±0.81)
Table 5. Experiments mixing the syntactic pair feature space and a simple distance
feature space: n-fold cross-validations repeated m times
lex + synt
lex + plac basic
lex + plac all
2-fold × 4
58.56 (±1.37)
59.58 (±1.30)
60.12 (±1.29)
60.19 (±1.54)
2-fold × 4
61.47 (±1.19)
61.80 (±1.21)
62.87 (±0.74)
63.69 (±1.23)
2-fold × 4
68.16 (±1.49)
67.77 (±1.09)
67.87 (±1.23)
68.32 (±1.00)
6-fold × 5
63.31 (±1.58)
63.36 (±1.68)
63.67 (±1.61)
64.07 (±1.45)
5.3 Combining the lexical similarity and the syntactic paired-content feature spaces
Many studies suggest that lexical overlap is a good heuristic to approximate textual
entailment predictions . This section analyzes the
interaction between the lexical similarity space (lex) and the basic plac and plac all
by combining them.
For lex, we used only one feature: the lexical overlap as described in Corley and
Mihalcea , computed by means of WordNet-based similarity between words
 along with the simple token and lemma matching.
The results, reported in Table 5 were obtained with n-fold cross-validation. They
show that the accuracy produced by lex alone is close to all mixed feature spaces:
ﬁrst-order rules seem to give no contribution, especially for the RTE3 and RTE
datasets. However, by paring the distributions of the fold accuracy generated with the
n-fold cross-validation and applying the sign test we found that on the RTE dataset,
lex + plac all is better than lex and lex + synt with 0.005 statistical signiﬁcance.5
This proves that the space using ﬁrst-order derivations is more accurate than others
when used in combination with lexical overlap heuristics.
5.4 When and why to use ﬁrst-order rule feature spaces
The kind of ﬁrst-order rules generated with our feature spaces seem to only
marginally improve lex. However, this may depend on the small size of the training
data. To conﬁrm this hypothesis, we analyzed the learning curves of the diﬀerent
models (Section 5.4.1). Moreover, to show that our models eﬀectively learn ﬁrst-order
rules, we studied them with respect to classes of examples, which can be solved by
diﬀerent classes of rules (Section 5.4.2).
5 More than 22 out of 30 times the ﬁrst space has better results than the other two.
F. M. Zanzotto et al.
Fig. 2. (a) Learning curves over RTE2. (b) Learning curves over RTE3.
Fig. 3. Learning curves of lex and lex + plac all in RTE2 and in RTE3.
5.4.1 Learning curves
We analyzed four feature spaces: synt, plac basic, plac all, and lex. The results for
the ﬁrst three spaces and the fourth space are respectively reported in Figures 2 and
3. We computed the learning curves using the oﬃcial split in development and test
sets of RTE2 and RTE3, where the development set is in turn divided in samples of
increasing size with a step of 200 training examples. Each point in the ﬁgure is the
average accuracy obtained over four runs.6
Even when all data is used, synt, plac basic and plac all do not reach a plateau,
meaning that they can improve their accuracy with further data. In contrast, the
6 For each point, four models of the classiﬁer are learned on four diﬀerent samples of the
training set.
A machine learning approach to textual entailment recognition
curves of the lex model (Figure 3) are ﬂat or, in the case of RTE3, decreasing. This is not surprising, since only one parameter has to be learnt, i.e., the
threshold; thus the number of needed examples is small. As a ﬁnal conclusion our
ﬁrst-order feature spaces can really learn from example, whereas the lex model
5.4.2 Which feature space for which pair?
In this section, we explore how the diﬀerent feature spaces behave on pairs showing
speciﬁc phenomena that can be better captured using ﬁrst-order syntactic rules. For
these pairs, plac all should outperform the other models. We also aim at studying
which rule can be learned.
For this purpose, we use the gold standard of entailment examples provided by
Vanderwende and Dolan . In their study of the RTE1 dataset the authors
discovered that 390 pairs out of the 800 of the test set can be classiﬁed using
solely syntactic cues. Most importantly, entailment examples were clustered in the
following four classes (describing the syntactic transformations that hold between
texts and hypotheses): (1) syntactic phenomena not involving alternation; (2)
syntactic phenomena involving alternation; (3) single word replacement; and (4) lack
of syntactic parallelism. Each class is further divided into subclasses, representing
speciﬁc syntactic transformations rule. For example, the Have-Possessive subclass is
a speciﬁc type of syntactic phenomenon involving ‘have’ alternation; to be correctly
classiﬁed, the examples of this category require a model able to handle the ﬁrst-order
transformation rule: X’s Y →X has Y.
Experimental results of our model over the above dataset are reported in Table 6:
the ﬁrst column reports the feature space; the ﬁrst row represents the classes of
syntactic phenomena in Vanderwende and Dolan . The second row shows the
number of cases falling in each class according to the manual gold standard (note
that examples can belong to more than one class when more than one transformation
takes place); and all the other rows illustrate the accuracy of our diﬀerent models
when classiﬁers are trained on the RTE1 development set.
The results indicate that placeholders are useful whenever ﬁrst-order transformation rules are required, i.e., for pairs in the classes syntactic phenomena
involving and not involving alternation. In these cases, plac all outperforms synt and
lex + plac all improves on lex. This is particularly true for the examples showing
syntactic phenomena not involving alternation. As expected, in the other two classes
of phenomena (single word replacement and lack of syntactic parallelism) RTE is not
improved by the use of placeholders, since ﬁrst-order transformations do not play a
relevant role.
By inspecting the above results it is also possible to determine whether or not a
speciﬁc feature space models a speciﬁc rule better than the others, by following the
principle that ‘a model which correctly classiﬁes a set of examples clearly requiring
a speciﬁc ﬁrst-order transformation most probably encodes such kind of ﬁrst-order
F. M. Zanzotto et al.
Table 6. Accuracy with diﬀerent feature spaces on speciﬁc syntactic phenomena over
a portion of the RTE1 test set
Syntactic phenomena
Not involving
Single word
Lack of syntactic
alternation
alternation
replacement
parallelism
No. of cases
plac basic
lex + synt
lex + plac basic
lex + plac all
Table 7. Experimenting with typed anchors: accuracy results on a 4-fold
cross-validation over the RTE2 dataset
Standard deviation
rule’.7 For example, if the model correctly classiﬁes active/passive alternations, it
likely encodes a rule for active/passive forms. Thus, by noting that plac all model
classiﬁes Be-Appositive, be located-Appositive, and Genitive-Location better than synt,
we argue that plac all can derive such kind of rules better than synt.
5.5 Experiments using typed anchors
In this section we check if ﬁrst-order syntactic rule feature spaces can be improved
by semantic information. Thus, we tested plac all and its extensions with semantic
information, i.e., ta and tap introduced in Section 4.
Table 7 reports the accuracy obtained in a 4-fold cross-validation over the RTE2
dataset. The small diﬀerence between ta and plac all accuracy suggests that encoding
typed anchors only at the preterminal level is again not suﬃcient for the generation
of eﬀective feature spaces. Thus, such information has to be propagated in the
7 Note that a more systematic inspection would be too diﬃcult. Indeed, determining which
rules ﬁre for a pair is complex, since SVMs make a decision over a pair using a linear
combination of the distances between the target pair and the support vectors. Detecting
which ﬁrst-order transformation rule has ﬁred, especially when a complex kernel space
(like the paired tree substructures) is currently an open problem.
A machine learning approach to textual entailment recognition
whole syntactic tree. Indeed, the results obtained with tap are signiﬁcantly higher8
than those obtained by plac all. Therefore, our way of typing anchors with the
semantics of word relations is a promising research line for RTE. In general, our
results also empirically conﬁrm the ﬁndings in Bar-Haim, Szpecktor and Glickman
 , which state that lexical and syntactic levels are complementary for RTE.
6 Conclusion
In this paper, we have proposed the pair content feature space, a novel feature
space for RTE that allows ML algorithms to derive ﬁrst-order rules based on a
syntactic–semantic representation of training examples. We have also proposed a
method to encode shallow semantic information in data representation through the
use of typed anchors. Our model employs variables (represented with placeholders)
and linguistic features, as those used in feature structures .
As a ﬁnal remark, we observe that several methods for automatically harvesting
ﬁrst-order rewrite rules from large corpora have been recently proposed in the
literature, e.g., DIRT and TE/ASE ).
These models are complementary to ours, as they are based on a completely diﬀerent
principle . While these methods can
only extract rules encoding a generic notion of similarity between two textual
patterns (e.g., X play Y ∼X win Y ), recent extensions allow the derivation of more speciﬁc
directional entailment rules, such as X play Y →X win Y. However, these models
cannot learn rewrite rules such as ‘the X VERB Y ↛X does not VERB Y ’, which
are instead learned by our model.
Although, several systems tried to leverage large repositories such as DIRT , the combined use of
the two forms of extracting ﬁrst-order rewrite rules is a very interesting research line.
Pilot experiments using verbs in entailment extracted with the method presented in
Zanzotto, Pennacchiotti and Pazienza and our model have shown promising