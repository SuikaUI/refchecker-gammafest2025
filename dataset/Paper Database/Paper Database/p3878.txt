Copyright © 2019 The Korean Society of Radiology
Design Characteristics of Studies Reporting the
Performance of Artificial Intelligence Algorithms for
Diagnostic Analysis of Medical Images: Results from
Recently Published Papers
Dong Wook Kim, MD1*, Hye Young Jang, MD2*, Kyung Won Kim, MD, PhD2, Youngbin Shin, MS2,
Seong Ho Park, MD, PhD2
1Department of Radiology, Taean-gun Health Center and County Hospital, Taean-gun, Korea; 2Department of Radiology and Research Institute of
Radiology, University of Ulsan College of Medicine, Asan Medical Center, Seoul, Korea
Objective: To evaluate the design characteristics of studies that evaluated the performance of artificial intelligence (AI)
algorithms for the diagnostic analysis of medical images.
Materials and Methods: PubMed MEDLINE and Embase databases were searched to identify original research articles
published between January 1, 2018 and August 17, 2018 that investigated the performance of AI algorithms that analyze
medical images to provide diagnostic decisions. Eligible articles were evaluated to determine 1) whether the study used
external validation rather than internal validation, and in case of external validation, whether the data for validation were
collected, 2) with diagnostic cohort design instead of diagnostic case-control design, 3) from multiple institutions, and 4)
in a prospective manner. These are fundamental methodologic features recommended for clinical validation of AI
performance in real-world practice. The studies that fulfilled the above criteria were identified. We classified the publishing
journals into medical vs. non-medical journal groups. Then, the results were compared between medical and non-medical
Results: Of 516 eligible published studies, only 6% (31 studies) performed external validation. None of the 31 studies
adopted all three design features: diagnostic cohort design, the inclusion of multiple institutions, and prospective data
collection for external validation. No significant difference was found between medical and non-medical journals.
Conclusion: Nearly all of the studies published in the study period that evaluated the performance of AI algorithms for
diagnostic analysis of medical images were designed as proof-of-concept technical feasibility studies and did not have the
design features that are recommended for robust validation of the real-world clinical performance of AI algorithms.
Keywords: Artificial intelligence; Machine learning; Deep learning; Clinical validation; Clinical trial; Accuracy; Study design;
Quality; Appropriateness; Systematic review; Meta-analysis
Received January 10, 2019; accepted after revision February 4, 2019.
This study was supported by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development
Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea (grant number: HI18C1216).
*These authors contributed equally to this work.
Corresponding author: Seong Ho Park, MD, PhD, Department of Radiology and Research Institute of Radiology, University of Ulsan
College of Medicine, Asan Medical Center, 88 Olympic-ro 43-gil, Songpa-gu, Seoul 05505, Korea.
• Tel: (822) 3010-5984 • Fax: (822) 476-4719 • E-mail: 
This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (https://
creativecommons.org/licenses/by-nc/4.0) which permits unrestricted non-commercial use, distribution, and reproduction in any medium,
provided the original work is properly cited.
Korean J Radiol 2019;20(3):405-410
eISSN 2005-8330
 
Original Article | Artificial Intelligence
Kim et al.
 
kjronline.org
INTRODUCTION
The use of artificial intelligence (AI) for medicine has
recently drawn much attention due to the advances in deep
learning technologies (1). Notably, there is a remarkable
interest in using AI for diagnostic analysis of various types
of medical images, primarily through convolutional neural
networks, a type of deep learning technology referred to
as “computer vision” (2-4). As with any other medical
devices or technologies, the importance of thorough
clinical validation of AI algorithms before their adoption
in clinical practice through adequately designed studies
to ensure patient benefit and safety while avoiding any
inadvertent harms cannot be overstated (5-10). Note
that the term “validation” is used in this study to imply
confirmation, as would be used in the medicine field, and
not algorithm tuning, which is used as technical jargon in
the field of machine learning (11, 12). Clinical validation
of AI technologies can be performed at different levels
of efficacy: diagnostic performance, effects on patient
outcome, and societal efficacy that considers cost-benefit
and cost-effectiveness (11, 13). Proper assessment of the
real-world clinical performance of high-dimensional AI
algorithms that analyze medical images using deep learning
requires appropriately designed external validation. It is
recommended for the external validation to use adequately
sized datasets that are collected either from newly
recruited patients or at institutions other than those that
provided training data in a way to adequately represent
the manifestation spectrum (i.e., all relevant variations
in patient demographics and disease states) of target
patients in real-world clinical settings where the AI will
be applied (10, 12, 14-17). Furthermore, use of data from
multiple external institutions is important for the validation
to verify the algorithm’s ability to generalize across the
expected variability in a variety of hospital systems (14,
16-18). Complex mathematical/statistical AI models such
as deep learning algorithms that analyze medical images
need a large quantity of data for algorithm training;
producing and annotating this magnitude of medical image
data is especially resource intensive and difficult (19, 20).
Therefore, individuals developing such AI algorithms might
rely on whatever data are available (methodologically
referred to as convenience case-control data), although
these may be prone to selection biases and artificial disease
prevalence and likely not represent real-world clinical
settings well (12, 19, 20). Since the performance of an AI
algorithm is strongly dependent upon its training data,
there is a genuine risk that AI algorithms may not perform
well in real-world practice and that an algorithm trained at
one institution provides inaccurate outputs when applied to
data at another institution (9, 16-19, 21, 22).
Despite the excitement around the use of AI for medicine,
the lack of appropriate clinical validation for AI algorithms
seems to be a current concern, a phenomenon referred
to as “digital exceptionalism” (16, 23, 24). For example,
computer scientists typically evaluate the performance
of AI algorithms on “test” datasets; however, these are
usually random subsamples of the original dataset, and
thus, adequate external validation of clinical performance is
not possible (10, 16, 20, 25). To our knowledge, concrete
data showing the exact extent of this perceived problem
are scarce. This study aimed to evaluate the design
characteristics of recently published studies reporting the
performance of AI algorithms that analyze medical images
and determine if the study designs were appropriate for
validating the clinical performance of AI algorithms in
real-world practice. The study design features addressed in
this study are crucial for validating the real-world clinical
performance of AI but would be excessive for proof-ofconcept technical feasibility studies (14). As not every
research study about the use of AI for medical diagnosis
is to validate the real-world clinical performance (14),
the purpose of this study was not to bluntly judge the
methodologic appropriateness of the published studies.
MATERIALS AND METHODS
This study did not require Institutional Review Board
Literature Search and Screening
PubMed MEDLINE and Embase databases were thoroughly
searched to identify original research articles that
investigated the performance of AI algorithms that analyze
medical images to provide diagnostic decisions (such as
either diagnosing or finding specific diseases or giving
information to categorize patients with a particular disease
into subgroups according to disease states, subtypes,
severity levels, stages, treatment responses, prognosis,
and risks). We used the following search query: (“artificial
intelligence” OR “machine learning” OR “deep learning”
OR “convolutional neural network”) AND (diagnosis OR
diagnostic OR diagnosing) AND . We limited
the search period to year 2018 to obtain timely results
 . Both print
publications and electronic publications ahead of print were
After removing overlaps between the two databases,
articles were screened for eligibility by two independent
reviewers. Articles with any degree of ambiguity or that
generated differences in opinion between the two reviewers
were re-evaluated at a consensus meeting, for which a
third reviewer was invited. Case reports, review articles,
editorials, letters, comments, and conference abstract/
proceedings were excluded. Our search was restricted to
human subjects and English-language studies. We defined
medical images as radiologic images and other medical
photographs (e.g., endoscopic images, pathologic photos,
and skin photos) and did not consider any line art graphs
that typically plot unidimensional data across time, for
example, electrocardiogram and A-mode ultrasound.
Studies investigating AI algorithms that combined medical
images and other types of clinical data were included.
AI algorithms that performed image-related tasks other
than direct diagnostic decision-making, for example,
image segmentation, quantitative measurements, and
augmentation of image acquisition/reconstruction, were not
considered.
Data Extraction
The full text of eligible articles was evaluated by two
reviewers for the following information: 1) whether the
study used external validation as opposed to internal
validation, and in case of external validation, whether the
data for validation were collected, 2) with diagnostic cohort
design instead of diagnostic case-control design, 3) from
multiple institutions, and 4) in a prospective manner. These
are fundamental methodologic features recommended for
clinical validation of AI performance in real-world practice
(10-12, 14). The more of these questions receive a “Yes”
answer, the more generalizable to real-world practice
the algorithm performance is. If a study validated its AI
performance in multiple ways, then the study received a
“Yes” answer for each of the above questions if at least one
analysis used the design features. We defined “external”
a bit generously and included the use of validation data
from institution(s) other than the one from which training
data were obtained, as well as cases where training and
validation data were collected from the same institution(s)
but in different time periods, even though the latter is not
considered external validation in a strict sense (10, 16, 25).
For studies in which the training and validation datasets
were collected at the same institution(s), validation data
were only considered external if the clinical settings and
patient eligibility criteria for the validation dataset were
specified separately from those of the training dataset. This
was to ensure that the validation data were not just a timesplit subsample of the original large dataset, as that results
in a type of internal validation (25). A diagnostic cohort
design was referred to as one in which the study defined the
clinical setting and patient eligibility criteria first and then
recruited patients consecutively or randomly to undergo
a particular diagnostic procedure, such as AI algorithm
application (15). In contrast, a diagnostic case-control
design would involve the collection of disease-positive and
disease-negative subjects separately (15). Diagnostic casecontrol designs are prone to spectrum bias, which can lead
to an inflated estimation of the diagnostic performance,
and unnatural prevalence, which creates uncertainty
regarding the diagnostic performance (12, 26). Additionally,
we noted the subject field (e.g., radiology, pathology, and
ophthalmology) of each article and classified the publishing
journals into either medical or non-medical journal groups.
The journals were classified primarily based on the Journal
Citation Reports (JCR) 2017 edition categories. For journals
not included in JCR databases, we referred to journal
websites and categorized them as medical if the scope/
aim of the journal included any fields of medicine or if
the editor-in-chief was a medical doctor. Articles with
any degree of ambiguity or that generated differences in
opinion between the two independent reviewers were reevaluated at a consensus meeting including a third reviewer.
Outcome Measures and Statistical Analysis
We calculated the percentage of studies that performed
external validation. For studies reporting the results of
external validation, the proportions of studies that involved
the features of diagnostic cohort designs, inclusion of
multiple institutions, and prospective data collection
for external validation were identified. The results were
compared between medical and non-medical journals using
Fisher’s exact test. A p < 0.05 was considered significant.
Of 2748 articles initially collected after removal of
Kim et al.
 
kjronline.org
overlaps between PubMed MEDLINE and Embase, 516
articles were finally eligible (Fig. 1, Table 1). The full
list of eligible articles analyzed in this study is available
as a Supplementary Material (in the online-only Data
Supplement).
Table 2 presents the proportions of the articles that had
each design feature, including breakdowns for medical vs.
non-medical journals. Only 6% (31 of 516) of the studies
performed external validation. None of the external validation
studies adopted all three design features, namely, diagnostic
cohort design, inclusion of multiple institutions, and
prospective data collection. No significant difference was
found between medical and non-medical journals (Table 2).
DISCUSSION
Our results reveal that most recently published studies
reporting the performance of AI algorithms for diagnostic
analysis of medical images did not have design features
that are recommended for robust validation of the clinical
performance of AI algorithms, confirming the worries that
premier journals have recently raised (23, 24). Our study did
not consider various detailed methodologic quality measures
for AI research studies (14), but simply evaluated major
macroscopic study design features. Therefore, the extent of
deficiency in the clinical validation of AI algorithms could
likely be even more significant.
However, it should be noted that these results do
not necessarily mean that the published studies were
inadequately designed by all means. The four criteria used
in this study–external validation and data for external
validation being obtained using a diagnostic cohort study,
from multiple institutions, and in a prospective manner–
are fundamental requirements for studies that intend to
evaluate the clinical performance of AI algorithms in realworld practice. These would be excessive for studies that
merely investigate technical feasibility (14). Readers and
Fig. 1. Flow-chart of article selection based on preferred reporting items for systematic reviews and meta-analyses guidelines.
Identification
Eligibility
Records identified through databases searching (n = 3098):
PubMed MEDLINE (n = 612), EMBASE (n = 2486)
Studies finally included in analysis (n = 516)
Records after duplicates removed
(n = 2748)
Records screened (n = 2748)
Records excluded (n = 1899):
Review articles (102)
Editorials/letters/comments (42)
Conference abstracts/proceedings (631)
Not in field of interest (1088)
Non-human subject research (36)
Records excluded (n = 333):
Review articles (14)
Editorials/letters/comments (1)
Conference abstracts/proceedings (1)
Not in field of interest (270)
Non-human subject research (24)
Unable to obtain full-text article (23)
Full-text articles assessed for eligibility
Table 1. Subject Fields of Articles Analyzed
Subject Fields*
Number of Articles (%)
Radiology (including nuclear medicine)
366 (70.9)
Ophthalmology
Dermatology
Gastroenterology
Other fields
Combined fields
Radiology and cardiology
Pathology and nuclear medicine
*Listed in descending order of article number.
Design Characteristics of Studies of AI Performance
 
kjronline.org
investigators alike should distinguish between proof-ofconcept technical feasibility studies and studies to validate
clinical performance of AI (14) and should avoid incorrectly
considering the results from studies that do not fulfill
the criteria mentioned above as sound proof of clinical
validation.
Some related methodologic guides have recently been
published (11, 12, 14). We suspect that most studies that
we analyzed in this study may have been conceived or
executed before these methodologic guides were made
available. Therefore, the design features of studies that
intend to evaluate the clinical performance of AI algorithms
for medicine may improve in the future.
Another issue that was not directly addressed in our study
but is worth mentioning is transparency regarding a priori
analysis plans and full publication of all results in studies
validating the clinical performance of AI algorithms (6, 11,
14, 27). As the performance of an AI algorithm may vary
across different institutions (16-18), some researchers or
sponsors might be inclined to selectively report favorable
results, which would result in underreporting of unfavorable
results. Prospective registration of studies, including a
priori analysis plans, similar to the registration of clinical
trials of interventions (e.g., at 
would help increase the transparency of these studies (27).
Prospective registration of diagnostic test accuracy studies,
which include studies to validate AI performance, has
already been proposed (28). The adoption of this policy by
academic journals would help enhance transparency in the
reporting of studies that validate the clinical performance
of AI algorithms.
Our current study has some limitations. First, while the
timeliness of research data is important (29), as AI is a
rapidly evolving field with numerous new studies being
published, the shelf life of our study results could be
short. Ironically, we hope to see substantial improvements
in the design of studies reporting clinical performance
of AI in medicine soon. Despite such rapid changes, our
research remains meaningful as the baseline against which
comparisons can be made to see if any improvements are
made in the future, given that most published studies that
were analyzed here likely predated the recent release of
related methodologic guides (11, 12, 14). Second, while
this study only evaluated studies reporting the diagnostic
performance of AI, clinical validation of AI extends to
evaluating the impact of AI on patient outcomes (12, 30).
However, to our knowledge, studies of how AI application
affects patient outcomes are scarce, and systematically
reviewing published studies is not feasible.
In conclusion, nearly all of the studies published in
the study period that evaluated the performance of AI
algorithms for diagnostic analysis of medical images were
designed as proof-of-concept technical feasibility studies
and did not have the design features that are recommended
for robust validation of the real-world clinical performance
of AI algorithms.
Supplementary Material
The online-only Data Supplement is available with this
article at 
Table 2. Study Design Characteristics of Articles Analyzed
Design Characteristic
All Articles
Articles Published
in Medical Journals
Articles Published
in Non-Medical Journals
External validation
485 (94.0)
410 (93.8)
In studies that used external validation
Diagnostic cohort design
Data from multiple institutions
Prospective data collection
Fulfillment of all of above three criteria
Fulfillment of at least two criteria
Fulfillment of at least one criterion
Data are expressed as number of articles with corresponding percentage enclosed in parentheses. *Comparison between medical and nonmedical journals.
Kim et al.
 
kjronline.org
Conflicts of Interest
The authors have no potential conflicts of interest to
Seong Ho Park
 
Dong Wook Kim
 
Hye Young Jang
 
Kyung Won Kim
 
Youngbin Shin