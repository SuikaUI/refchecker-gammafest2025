Less is More: CLIPBERT for Video-and-Language Learning
via Sparse Sampling
Jie Lei*1, Linjie Li*2, Luowei Zhou2, Zhe Gan2, Tamara L. Berg1, Mohit Bansal1, Jingjing Liu2
1UNC Chapel Hill
2Microsoft Dynamics 365 AI Research
{jielei, tlberg, mbansal}@cs.unc.edu
{lindesy.li, luowei.zhou, zhe.gan, jingjl}@microsoft.com
The canonical approach to video-and-language learning
(e.g., video question answering) dictates a neural model to
learn from ofﬂine-extracted dense video features from vision models and text features from language models. These
feature extractors are trained independently and usually
on tasks different from the target domains, rendering these
ﬁxed features sub-optimal for downstream tasks. Moreover,
due to the high computational overload of dense video features, it is often difﬁcult (or infeasible) to plug feature extractors directly into existing approaches for easy ﬁnetuning. To provide a remedy to this dilemma, we propose a
generic framework CLIPBERT that enables affordable endto-end learning for video-and-language tasks, by employing sparse sampling, where only a single or a few sparsely
sampled short clips from a video are used at each training step. Experiments on text-to-video retrieval and video
question answering on six datasets demonstrate that CLIP-
BERT outperforms (or is on par with) existing methods that
exploit full-length videos, suggesting that end-to-end learning with just a few sparsely sampled clips is often more
accurate than using densely extracted ofﬂine features from
full-length videos, proving the proverbial less-is-more principle. Videos in the datasets are from considerably different domains and lengths, ranging from 3-second genericdomain GIF videos to 180-second YouTube human activity
videos, showing the generalization ability of our approach.
Comprehensive ablation studies and thorough analyses are
provided to dissect what factors lead to this success. Our
code is publicly available.1
1. Introduction
Humans communicate with each other in this interactive
and dynamic visual world via languages, signs, and gestures.
The ability to jointly understand both visual and
* Equal contribution.
1 
Cross-modal
Cross-modal
Prediction
Sparse sampling
Dense sampling
Clip feature
Text feature
Existing mehods
Cross-modal
Cross-modal
Prediction
Prediction
Video-level
prediction
Video-level
prediction
Comparison between popular video-and-language
learning paradigm (top) and CLIPBERT (bottom). In contrast to
most existing methods that utilize ofﬂine (stop gradient) extracted
dense video features and text features, CLIPBERT uses sparsely
sampled clips and raw text tokens for end-to-end modeling.
textual clues is an essential ability for intelligent agents
to interpret multimodal signals in the physical world. A
wide range of tasks based on real-life videos have been
designed to test such ability, including text-to-video retrieval , video captioning , video
question answering , and video moment retrieval . The de facto paradigm to tackle these
cross-modal tasks is to ﬁrst extract dense video features
from pre-trained vision models and text features
from pre-trained language models , then apply multimodal fusion to wrangle together these ﬁxed representations in a shared embedding space (Figure 1 (top)).
Existing approaches following this
paradigm have achieved strong success, yet suffer from two
main drawbacks: (i) Disconnection in tasks/domains: of-
ﬂine feature extractors are often trained on tasks and domains different from the target task.
For example, features learned for action recognition from human activity
videos are incongruently applied to downstream video
question answering on generic-domain GIF videos .
 
(ii) Disconnection in multimodal features: learned features
from different modalities are independent of each other. For
instance, action recognition models are typically trained from pure video data without textual input, yet
are applied to video-and-language tasks. End-to-end taskspeciﬁc ﬁnetuning offers a way to mitigate these inherent
disconnections. However, extracting features from the full
sequence of video frames, as in most existing work, casts
excessive demand on memory and computation, rendering
it difﬁcult or even infeasible to directly plug feature extractors into a video+language learning framework for efﬁcient
end-to-end ﬁnetuning.
Motivated by this, we propose CLIPBERT, a generic
and efﬁcient framework for end-to-end video-and-language
learning (Figure 1 (bottom)). Two aspects distinguish CLIP-
BERT from previous work. First, in contrast to densely
extracting video features (adopted by most existing methods), CLIPBERT sparsely samples only one single or a
few short clips from the full-length videos at each training step. The hypothesis is that visual features from sparse
clips already capture key visual and semantic information
in the video, as consecutive clips usually contain similar
semantics from a continuous scene.
Thus, a handful of
clips are sufﬁcient for training, instead of using the full
video. Then, predictions from multiple densely-sampled
clips are aggregated to obtain the ﬁnal video-level prediction during inference, which is less computational demanding. This sparse-training-then-dense-inference strategy greatly reduces memory needs and computations, allowing economical end-to-end learning from raw video
frame pixels and language tokens.
The second differentiating aspect concerns the initialization of model weights (i.e., transfer through pre-training).
In recent literature, image-text pre-training (e.g., using
COCO Captions or Visual Genome Captions ) has
been applied to image-text tasks ,
and video-text pre-training (e.g., using HowTo100M )
to video-related tasks . There has been no
study to cross-examine the effect of image-text pre-training
on video-text tasks.
Intuitively, visual features learned
through pre-training from large-scale image datasets should
also help video understanding tasks that rely on visual clues
in static video frames. To investigate this, we use 2D architectures (e.g., ResNet-50 ) instead of 3D features
 as our visual backbone for video encoding, allowing us to harness the power of image-text pretraining for video-text understanding along with the advantages of low memory cost and runtime efﬁciency. Empirically, we observe that the knowledge learned in image-text
pre-training indeed helps video-text tasks; this simple strategy helps us achieve better or comparable performance to
previous state of the art on text-to-video retrieval and video
question answering tasks.
Our contributions are three-fold:
(i) We propose
CLIPBERT, a new end-to-end learning framework for
video+language tasks.
Experiments show that CLIP-
BERT achieves superior (or on par) performance than existing methods across diverse video-text tasks, where the
average video length ranges from a few seconds to three
minutes. (ii) Our work suggests “less is more”: the proposed end-to-end training strategy with a single or a few
(less) sparsely sampled clips is often more accurate than
traditional approaches that employ densely extracted video
features. (iii) We demonstrate that image-text pre-training
beneﬁts video-text tasks. We also provide comprehensive
ablation studies to reveal the key factors that lead to the success of CLIPBERT, in hope of inspiring more future work.
2. Related Work
Video and Language Understanding. Popular video-andlanguage tasks include text-to-video retrieval ,
video captioning , video question answering , and moment retrieval .
Standard approaches leverage ofﬂine extracted video and text features from action
recognition models , image recognition
models , and language models .
Aligned with the success of transformer-based language pre-training and image-text
pre-training , video-text pretraining has shown promising results
on video-and-language tasks. Beyond using ﬁxed features
and same-domain data (i.e., video-text pre-training only for
video-text tasks), our work focuses on end-to-end training
and applying image-text pre-training for video-text tasks.
Action Recognition. Modern video action recognition architectures are typically designed with deep 2D 
or 3D convolutional networks.
These backbones are often computation and memory heavy, making
it extremely difﬁcult to directly process videos of considerable length. To ease this difﬁculty, instead of training on
full-length videos, models are often trained with randomly
sampled short clips from the videos . At inference time, predictions from multiple uniformly sampled clips are aggregated together as the ﬁnal
video-level prediction. In relation to these works, we adopt
a similar strategy to perform sparse training and dense inference to reduce overhead on video processing, but focus
on video-and-language tasks with cross-modal modeling of
video and language, in contrast to pure video modeling.
3. CLIPBERT with Sparse Sampling
We propose CLIPBERT, a general framework that enables end-to-end learning on video and language data, by
learning joint representations directly from video frame pixels and raw text tokens, instead of from ofﬂine-extracted
single-modality features.
Figure 1 (bottom) gives an
overview of CLIPBERT framework. It adopts a sparse sampling strategy using only a single or a few sampled clips
at each training step, instead of full-length videos. Each
sampled clip is independently encoded with a vision backbone model, the visual features from which are then fed to a
cross-modal module that extracts relations between the clip
and its associated text representations. Independent predictions from all the sampled clips are fused together (e.g.,
through mean-pooling) to derive a consensus at the video
level. A task-speciﬁc loss is calculated based on this consensus to learn model parameters. During inference, CLIP-
BERT densely samples a sequence of clips from the video
and aggregates their predictions as the ﬁnal prediction.
Most existing work models ofﬂineextracted dense video features and text features. Formally,
we denote a video-text pair as V (for video) and S (for
text sequence). The video V is further denoted as a list
of N clips of equal duration [c1, c2, ..., cN]. This standard
paradigm can be formulated as:
p = H([FSG
(c2), ..., FSG
(cN)], FSG
are vision and language encoder,
respectively. The superscript SG denotes Stop Gradient,
meaning that gradients cannot be back-propagated through
the two encoders.
H is a cross-modal encoder and predictor, which models the relations between the encoded
video/language inputs and makes predictions.
video-level prediction. A task-speciﬁc loss function Ltask
is then applied to calculate the loss value ltask based on this
prediction and its corresponding ground-truth q:
ltask = Ltask(p, q).
Sparse Sampling for Training. Instead of using the fulllength video with N clips, CLIPBERT sparsely (and randomly) samples Ntrain clips {cτi}Ntrain
from V for training. Ntrain is typically much smaller than N. We model
a sampled clip cτi together with text input S to produce a
prediction pτi:
pτi = H(Fv(cτi), Fl(S)),
where Fv and Fl are vision/language encoders. Different
from Equation 1 that uses ofﬂine vision/language encoders,
CLIPBERT is end-to-end trainable, allowing task-speciﬁc
loss to further ﬁnetune the encoders, learning better representations. Independent predictions from all sampled clips
are aggregated to derive a consensus. The loss value ltask
is calculated based on this video-level consensus:
ltask = Ltask(G(pτ1, pτ2, ..., pτNtrain), q),
Word Embedding
Spatial Downsampling
Temporal Fusion
2D Position Embedding
Type Embedding
Position Embedding
Transformer
Prediction
people playing a
baseball game
Sparse Sampling
Text Features
Clip Features
Figure 2: Overview of CLIPBERT architecture. For simplicity,
we only show an illustration of producing prediction for a single
sampled clip. When multiple clips are used, their predictions are
fused together as the ﬁnal prediction.
where G is the prediction/score aggregation function (e.g.,
mean-pooling). When Ntrain is larger than one, this formulation can be regarded as a form of multiple instance learning (MIL) . At inference, we uniformly sample Ntest
clips of the same duration as training clips, then aggregate
predictions from all Ntest clips to form our ﬁnal prediction.
CLIPBERT’s sparse training strategy can be interpreted
as a type of data augmentation: different subsets of clips
from the same video are used at different training steps,
which improves the model’s generalization ability. In this
sense, it is analogous to random cropping commonly used in image classiﬁcation tasks. It also takes inspiration from action recognition methods ,
where a video classiﬁer is trained on sampled clips.
Model Architecture. Figure 2 gives an overview of CLIP-
BERT architecture. For the vision encoder Fv, we use a
2D CNN architecture ResNet-50 instead of 3D architectures (such as C3D or I3D ), because 2D models
typically consume less memory and run faster. Besides, 2D
CNNs have proved to work reasonably well on video understanding tasks such as action recognition . Specifically, we take the ﬁrst 5 Conv blocks of ResNet-50 
and add an extra convolution layer to reduce its output feature depth, as well as a 2×2 max-pooling layer for spatial
down-sampling, following Pixel-BERT . For each sampled clip, we uniformly sample T frames and obtain T feature maps. A temporal fusion layer M (e.g., mean-pooling)
is applied to aggregate the frame-level feature maps into
a single clip-level feature map. We then add a row-wise
and a column-wise position embedding to each feature vector based on their 2D position. These embeddings are the
same trainable position embeddings as in BERT . Collectively, these two position embeddings are indicative of
2D spatial locations of the features, which can be viewed
as a 2D position embedding. The resulting feature map is
ﬂattened into an embedding sequence to represent the clip.
We use a trainable word embedding layer as our language encoder Fl to encode language tokens and add trainable position embeddings to encode positional information
of the tokens. Next, we add different type embeddings 
to both clip and text embeddings to indicate their source
type. These two sequences are then concatenated as inputs
to a 12-layer transformer for cross-modal fusion.
Special tokens [CLS] and [SEP] are added in this process following . Given a downstream task, we add a
task-speciﬁc prediction head with the last-layer [CLS] representation as input (e.g., using a two-layer MLP with softmax to produce scores for text-to-video retrieval).
Weight Initialization and Pre-training. We initialize the
ResNet-50 layers with weights from grid-feat . It
is trained on Visual Genome for object detection and
attribute classiﬁcation, and produces effective grid features
for image VQA tasks . Input frames are resized to
have a maximum longer side of L while keeping the aspect ratios, and the shorter side is zero-padded to be L
as well . We initialize the transformer and word embedding layers from BERT-base model , pre-trained on
BookCorpus and English Wikipedia. These weights
are trained separately for their individual single-modality
tasks, thus simply combining them together in a single
framework for downstream task training may result in suboptimal performance.
Although pre-training the whole
model end-to-end with large-scale video-text datasets such
as HowTo100M are appealing, we are restricted by the
enormous computation cost.2 Luckily, as we use 2D CNN
as our vision encoder, CLIPBERT is able to directly take
image-text pairs as inputs for training. Thus, we leverage
large-scale image-text datasets (COCO Captions and Visual Genome Captions ) to perform cross-modal pretraining . Speciﬁcally, we use masked language
modeling and image-text matching objectives
to optimize the model. By default, we ﬁnetune our model
from this pre-trained weights for downstream video-text
tasks. The impact of different weight initialization strategies is examined in Section 4.3.
Implementation Details. We perform image-text pretraining on COCO Captions and Visual Genome Captions . These two datasets contain a total of 5.6M training image-text pairs on 151K images. This is the same data
2 reports that pre-training I3D with ofﬂine extracted text features on HowTo100M requires ∼3 days with 64 Cloud TPUs v3.
used in UNITER’s in-domain pre-training. We use input
image size L=768, and the resulting feature map from the
vision encoder contains 144 pixels. To improve generalization and reduce computation cost, during pre-training, we
follow Pixel-BERT to use pixel random sampling that
samples 100 pixels from the encoded feature map as the input to the transformer layers. Note that we only apply pixel
random sampling for pre-training, and always use the full
feature map for downstream tasks to avoid misalignment
in training and inference . We use WordPiece embeddings and keep at most 20 tokens from the caption. We
then randomly mask 15% of the tokens for masked language
modeling. For each image-caption pair, with a probability
of 0.5, we replace the ground-truth caption with a randomly
sampled caption from another image to form a negative pair
for image-text matching. We use AadmW to optimize
end-to-end model training, with an initial learning rate of
5e-5, β1=0.9, β2=0.98, and use learning rate warmup over
the ﬁrst 10% training steps followed by linear decay to 0.
Our model is implemented in PyTorch and transformers . It is trained for 40 epochs with mixed precision,
on 8 NVIDIA V100 GPUs with a batch size of 32 per GPU.
The whole training process takes 4 days to complete.
For downstream ﬁnetuning, we use the same training and
optimizer conﬁgurations except that the default input image
size is set to 448 (due to the typically lower resolution of
videos compared to images). Since downstream datasets
vary in scale and domain, we use task-speciﬁc learning rates
and training epochs based on validation performance.
4. Experiments
In this section, we evaluate CLIPBERT on two popular
video-and-language tasks, text-to-video retrieval and video
question answering, across six different datasets. We also
provide extensive ablation studies to analyze the key factors
that contribute to CLIPBERT’s success.
4.1. Downstream Tasks
Text-to-Video Retrieval.
(i) MSRVTT contains
10K YouTube videos with 200K descriptions.
We follow , using 7k train+val videos for training and report results on the 1K test set . We also create a local val set by sampling 1K video-caption pairs from unused
test videos for our ablation study. (ii) DiDeMo contains 10K Flickr videos annotated with 40K sentences. (iii)
ActivityNet Captions contains 20K YouTube videos
annotated with 100K sentences. The training set contains
10K videos, and we use val1 set with 4.9K videos to report results. For MSRVTT, we evaluate standard sentenceto-video retrieval. For DiDeMo and ActivityNet Captions,
we follow to evaluate paragraph-to-video retrieval,
where all sentence descriptions for a video are concatenated
3.1 14.8 29.3
ActivityNet Captions
Figure 3: Average video length in different datasets.
MSRVTT Retrieval
Table 1: Impact of input image size L.
to form a paragraph for retrieval. We use average recall at K
(R@K) and median rank (MdR) to measure performance.
Video Question Answering. (i) TGIF-QA contains
165K QA pairs on 72K GIF videos. We experiment with 3
TGIF-QA tasks: Repeating Action and State Transition for
multiple-choice QA, and Frame QA for open-ended QA.
We leave the Count task as future work as it requires directly modeling full-length videos. (ii) MSRVTT-QA 
is created based on videos and captions in MSRVTT, containing 10K videos and 243K open-ended questions. (iii)
MSRVTT multiple-choice test is a multiple-choice
task with videos as questions, and captions as answers.
Each video contains 5 captions, with only one positive
match. For all the QA tasks, we use standard train/val/test
splits and report accuracy to measure performance.
Figure 3 shows a comparison of average video length of
evaluated datasets. Videos across datasets demonstrate considerable difference in domains and lengths, ranging from
3-second generic-domain GIF videos in TGIF-QA to 180second human activity videos in ActivityNet Captions.
4.2. Analysis of Sparse Sampling
We conduct comprehensive ablation studies concerning
various aspects of CLIPBERT’s design in this section and
Section 4.3. If not otherwise stated, we randomly sample a
single frame (Ntrain=1 and T=1) from full-length videos
for training, and use the middle frame (Ntest=1) for inference, with input image size L=448. All ablation results are
on MSRVTT retrieval local val and MSRVTT-QA val sets.
Do we need larger input image size? We compare models
with different input image sizes L ∈{224, 448, 768, 1000},
results shown in Table 1.
Compared to the model with
L=224, larger input resolution improves performance on
the retrieval task, while maintaining a similar performance
on the QA task. The best performance is achieved at around
Further increasing the resolution does not provide signiﬁcant performance boost. shows that increasing input image size from 448 to 1333 always improves
image VQA performance with no sign of saturation,
while we observe the performance converges at around 448
MSRVTT Retrieval
Mean Pooling
Conv(2+1)D
Table 2: Impact of #frames (T) and temporal fusion function
(M). We use a 1-second clip for all experiments.
for MSRVTT retrieval and QA. This is potentially because
VQA images are typically of higher raw resolution than
MSRVTT videos (we are only able to obtain videos at a
maximum height of 240 pixels). We expect higher resolution videos could further improve model performance.
Do we need densely sampled frames? A common practice for video understanding and video+language understanding is to model densely sampled frames from the original video (e.g., sample frames at 25 frames per
second). To understand the impact of using densely sampled frames, we conduct a set of controlled experiments.
Speciﬁcally, we randomly sample a ﬁxed-length 1-second
clip from the video, then evenly sample T={1, 2, 4, 8, 16}
frames within this clip for training. For inference, we use
the middle clip of the video. When multiple frames are used
(i.e., T>1), we use mean pooling for temporal fusion.
We also experiment with variants using additional 3D
convolutions before mean pooling: (i) Conv3D: a standard 3D convolution layer with kernel size 3, stride 1; (ii)
Conv(2+1)D: a spatial and temporal separable 3D convolution . Adding 3D convolutions to 2D convolutions
essentially leads to a design similar to Top-Heavy S3D architecture , which shows better performance than full
3D convolutions on video action recognition and runs faster.
Results are shown in Table 2. Overall, models that use
3D convolutions perform worse than models that adopt a
simple mean pooling. For mean pooling, we observe that
using two frames provides a notable improvement over using a single frame. However, models that use more than
two frames perform similarly compared to the one using
two frames, suggesting that two frames already represent
enough local temporal information for the tasks.
Do more clips at inference help? At inference, we aggregate prediction scores from multiple densely sampled clips
as the ﬁnal score. To show how this strategy affects performance, we evenly sample Ntest ∈{1, 2, 4, 8, 16} clips from
a video and average their individual predictions at inference.
For this experiment, we provide two models trained with
#clips at inference (Ntest)
MSRVTT retrieval R10
One frame per clip
Two frames per clip
#clips at inference (Ntest)
MSRVTT-QA Accuracy
Figure 4: Impact of #inference clips (Ntest).
MSRVTT Retrieval
Mean Pooling
Max Pooling
Table 3: Impact of #training clips (Ntrain) and score aggregation function (G). All models use Ntest=16 clips for inference.
Sampling Method Ntrain
MSRVTT Retrieval
Dense Uniform
Sparse Random
Table 4: Sparse random sampling vs. dense uniform sampling.
All models use Ntest=16 clips for inference.
different numbers of training frames per clip: one with a
single frame and the other with two frames. Both models
use a single clip for training. Results are shown in Figure 4.
Adding more clips generally improves performance, especially the ﬁrst few additions, but after a certain point performance saturates. For example, in Figure 4 (left), MSRVTT
retrieval performance increases substantially as we use two
and four clips, compared to using a single clip; then the performance gain gradually becomes marginal.
Do more clips in training help? We randomly sample
Ntrain clips and aggregate scores from the clips with aggregation function G as the ﬁnal score to calculate the training
loss. When multiple clips are used, information from these
samples is aggregated through multiple instance learning
to maximize the utility of these clips. To understand how
this strategy affects model performance, we evaluate model
variants that use Ntrain ∈{1, 2, 4, 8, 16} at training. We
also consider 3 different commonly used score aggregation
functions for G: mean pooling, max pooling, and LogSum-
Exp . In mean pooling and max pooling, the cross-clip
pooling is performed over logits, followed by a softmax operator. In LogSumExp, logits from each clip are ﬁrst fed
#clips at training (Ntrain)
Max. allowed bacth size
#clips at training (Ntrain)
Single pass time (s)
#frames per clip at training (T)
Max. allowed bacth size
#frames per clip at training (T)
Single pass time (s)
MSRVTT retrieval R10
MSRVTT retrieval R10
Figure 5: Memory and computation cost comparison w.r.t. different numbers of clips (Ntrain) or frames (T) at training. (a):
Maximum allowed batch size with ﬁxed T=1. (b): Time cost for
a single forward and backward pass with ﬁxed T=1, batch size 8.
(c): Maximum allowed batch size with ﬁxed Ntrain=1. (d): Time
cost for a single forward and backward pass with ﬁxed Ntrain=1,
batch size 8. All experiments are conducted on a single NVIDIA
V100 GPU with 32GB memory. MSRVTT retrieval performance
is also added in (b, d) for reference. Best viewed in color.
through an element-wise exponential operator, followed by
a cross-clip mean pooling. The aggregated output is further
normalized by its own sum to make it eligible as a probability distribution. For simplicity, we always use the same
aggregation function for training and inference. For a fair
comparison, all models use a single frame per clip for training and 16 clips for inference, i.e., T=1 and Ntest=16.
Results are shown in Table 3. In general, adding more
clips helps, and the second added clip gives the most performance gain.
For example, for models with LogSum-
Exp, Ntrain=2 improves retrieval R1 score of Ntrain=1
by 2.8%, while Ntrain=16 improves only 1.9% upon
Ntrain=2, even though it adds much more clips. As for
score aggregation function G, LogSumExp works the best.
Sparse Random Sampling vs. Dense Uniform Sampling.
At each training step, CLIPBERT randomly samples only
a single or a few short clips from a full-length video. Intuitively, this sparse random sampling strategy can be interpreted as a type of data augmentation where different
subsets of clips for a video are used to calculate the loss
at different training steps.
To show the effectiveness of
this approach, we compare CLIPBERT with a variant that
uses uniformly sampled dense clips. Speciﬁcally, we use
the same CLIPBERT architecture as before but always uses
16 uniformly sampled clips for training. Table 4 shows the
comparison. Sparse random sampling with only 4 clips outperforms dense uniformly sampling with 16 clips across all
Weight Initialization
MSRVTT Retrieval
transformer
image-text pre-training
Table 5: Impact of weight initialization strategy.
metrics in both retrieval and QA tasks. Meanwhile, using 4
clips is much more memory and computation efﬁcient than
using 16 clips, as we show in the next paragraph. In addition to these two sampling approaches, it is also possible
to choose clips using content-based methods such as .
However, this requires an extra non-trivial selection step,
and may also remove some of the data augmentation effect
brought by random sampling.
Memory and Computation Cost. Figure 5 shows a comparison of memory and computation cost w.r.t. different
numbers of clips (Ntrain) or frames (T) at training. We
observe that using more clips or more frames at training considerably increases memory demand and computational cost. For example, in Figure 5 (a), we see that the
maximum allowed batch size for a single NVIDIA V100
GPU is 190 when Ntrain=2, compared to that of 16 when
Ntrain=16. Meanwhile, in Figure 5 (b), we see that the
time cost increases almost linearly with Ntrain, while the
performance improvement on MSRVTT retrieval is less signiﬁcant. These comparisons demonstrate the efﬁciency and
effectiveness of the proposed sparse training strategy.
4.3. Analysis of Pre-training/End-to-end Training
Impact of Image-text Pre-training. Our model is initialized with image-text pre-training on COCO and Visual
Genome Captions, to obtain better-aligned visual and textual representations.
To validate the effectiveness of using image-text pre-training for weight initialization, we also
evaluate variants that use other weight initialization strategies. Speciﬁcally, for CNN, we use weights from random
initialization, image classiﬁcation model pre-trained on ImageNet , frame-wise action recognition model TSN pre-trained on Kinetics-700 , or detection model
grid-feat pre-trained on Visual Genome . For transformer and word embedding layers, we use weights from
random initialization or pre-trained BERTBASE model .
For random initialization, we use the default setup in Py-
Torch and Transformer libraries for CNN and
transformer layers, respectively. Results are summarized in
Table 5. We notice that randomly initializing CNN leads to
massive performance degradation or even training failure,
we hypothesize that it is mostly because of the difﬁculty
of training large models on relatively small datasets (e.g.,
Parameters Trainable?
MSRVTT Retrieval
Table 6: Impact of end-to-end training.
MSRVTT retrieval train split: 7K videos). The best performance is achieved using image-text pre-trained weights,
clearly indicating the beneﬁt of utilizing image-text pretraining for video-text tasks.
Impact of End-to-End Training. The standard paradigm
for video-and-language understanding is to train models
with ofﬂine extracted features, in which the task objective
does not affect the video and text encoding process. In this
work, we train our model in an end-to-end manner, allowing
the model to ﬁnetune feature representations by leveraging
task supervision. In Table 6, we compare our model with
variants that freeze portions of the parameters. Overall, the
best performance is achieved by our model, showing the importance of end-to-end training. Note that all the models in
Table 6 are ﬁnetuned from our end-to-end image-text pretrained model, which partly resolves the multimodal feature
disconnection issue in Section 1. Thus, we expect smaller
improvement from further end-to-end ﬁnetuning.
Main Conclusions from the analyses in Section 4.2 and
Section 4.3 are summarized as follows: (i) Larger input image size helps improve model performance, but the gain diminishes when image size is larger than 448; (ii) Sparsely
sampling 2 frames from each clip performs on par with
dense sampling 16 frames, showing that just one or two
frames are sufﬁcient for effective video-and-language training; mean-pooling is more effective than 3D Conv when
fusing information from different frames; (iii) More clips
at inference helps improve model performance; aggregation strategy of predictions across clips affects ﬁnal performance; (iv) Sparse (random) sampling is more effective than dense uniform sampling while being more memory
and computation efﬁcient; (v) Image-text pre-training bene-
ﬁts video-text tasks; and (vi) End-to-end training improves
model performance.
4.4. Comparison to State-of-the-art
For evaluation, we compare CLIPBERT with state-ofthe-art methods on text-to-video retrieval and video question answering tasks.
We denote models using different
sampling methods at training as CLIPBERT Ntrain×T,
(randomly sample Ntrain 1-second clips for training, each
contains T uniformly sampled frames of size L=448). We
use LogSumExp to aggregate scores from multiple clips. At
inference time, if not otherwise stated, we aggregate scores
from Ntest=16 uniformly sampled clips.
R5 R10 MdR
HERO ASR, PT 20.5 47.6 60.9
JSFusion 
10.2 31.2 43.2
HT PT
14.9 40.2 52.8
ActBERT PT
16.3 42.8 56.9
HERO PT
16.8 43.4 57.7
CLIPBERT 4×1
19.8 45.1 57.5
CLIPBERT 8×2
22.0 46.8 59.9
(a) MSRVTT 1K test set.
R5 R10 MdR
CLIPBERT 4×1 19.9 44.5 56.7
CLIPBERT 8×2 20.4 48.0 60.8
(b) DiDeMo test set.
R5 R10 MdR
22.7 54.2 93.2
MMT PT
28.7 61.4 94.5
Dense 
CLIPBERT 4×2∗
20.9 48.6 62.8
CLIPBERT 4×2∗(Ntest=20) 21.3 49.0 63.5
(c) ActivityNet Captions val1 set.
Table 7: Comparison with state-of-the-art methods on text-to-video retrieval. CLIPBERT models with different training input sampling methods are denoted by Ntrain×T. We use Ntest=16 if not otherwise stated. We gray out models that used features other than
appearance and motion for a fair comparison, e.g., CE used appearance, scene, motion, face, audio, OCR, ASR features from 11 different
models. PT indicates the model is pre-trained on HowTo100M. * denotes models use 2-second clips instead of the default 1-second clips.
Transition FrameQA
ST-VQA 
Co-Memory 
Heterogeneous Memory 
QueST 
CLIPBERT 1×1 (Ntest=1)
CLIPBERT 1×1
(a) TGIF-QA test set.
ST-VQA (by )
Co-Memory (by )
Heterogeneous Memory 
CLIPBERT 4×1
CLIPBERT 8×2
(b) MRSVTT-QA test set.
SNUVL (by )
ST-VQA (by )
CT-SAN (by )
MLB (by )
JSFusion 
ActBERT PT
CLIPBERT 4×1
CLIPBERT 8×2
(c) MRSVTT multiple-choice test.
Table 8: Comparison with state-of-the-art methods on video question answering.
Text-to-Video Retrieval. Table 7 summarizes results on
text-to-video retrieval. In Table 7a, CLIPBERT achieves
signiﬁcant performance gain over existing methods on
MSRVTT retrieval, including HT , ActBERT , and
HERO , which are pre-trained on 136M clip-caption
pairs from HowTo100M .
Under a fair comparison, CLIPBERT 4×1 outperforms HERO by 3.0%
Note that HERO uses SlowFast features
extracted from full-length videos at a very dense frame
rate of 21 FPS (i.e., on average 310 frames per video
for MSRVTT), while CLIPBERT 4×1 uses only 4 randomly sampled frames. When more frames are used, CLIP-
BERT 8×2 achieves even higher performance, surpassing HERO by 5.2%. Compared to the HERO ASR model
that uses extra input from Automatic Speech Recognition
(ASR), CLIPBERT still obtains 1.5% higher R1 score.
Similarly,
ActivityNet
paragraph-to-video retrieval tasks (Table 7b and Table 7c),
we notice our best CLIPBERT models outputform CE 
by 4.3% and 3.1% on R1, respectively, despite CE’s use of
appearance, scene, motion, face, audio, OCR, ASR features
densely extracted from 11 different models. ActivityNet
Caption videos are on average 180-second long. In Table 7c
we show CLIPBERT performs competitively with existing
methods that model long-range relations in this dataset. Especially, CLIPBERT obtains 0.8% higher R1 than HSE 
and is competitive compared to MMT that uses extra
audio features3, even though CLIPBERT 4×2∗(Ntest=20)
samples only 8-second clips from 180-second videos at
each training step, and uses only 40-second content for inference. We expect CLIPBERT’s performance to be further
improved by sampling more clips during training and inference. Meanwhile, we also encourage future work to explore
combining extra input signals, such as audio, into the CLIP-
BERT framework for better performance.
Video Question Answering. Table 8 shows evaluation results on video question answering. Across all three tasks,
CLIPBERT achieves signiﬁcant performance gain. In Table 8a, CLIPBERT 1×1 outperforms prior state-of-the-art
QueST by 6.9%, 6.8%, and 0.6% on TGIF-QA Action, Transition, and FrameQA tasks, respectively.
is especially surprising considering CLIPBERT 1×1 uses
only a single randomly sampled frame from the videos at
each training step, while QueST uses 10 uniformly sampled
frames. Moreover, when using only a single frame (the middle frames of the videos) for inference, CLIPBERT 1×1
(Ntest=1) already far outperforms QueST on Action and
Transition tasks, and is on par with QueST on FrameQA.
3 shows that adding audio features greatly improves performance.
In Table 8b, CLIPBERT 4×1 outperforms HCRN by
1.4% on MSRVTT-QA. Note that HCRN adopts a sophisticated hierarchical relation modeling network over the entire
video sequence of 24 clips at training time, while we use
only four randomly sampled frames. Using more frames
further increases this performance gap to 1.8%. Table 8c
shows CLIPBERT 8×2 improves ActBERT model
pre-trained on HowTo100M by 2.5%, on MSRVTT multiple choice test task.
5. Conclusion
We present CLIPBERT, a generic framework for end-toend video-and-language learning, which adopts sparse sampling to use only a few sampled short clips from the videos
at each training step. Experiments across diverse tasks show
that CLIPBERT outperforms (or is on par with) state-ofthe-art methods with densely sampled ofﬂine features, suggesting that the “less is more” principle is highly effective in
practice. Comprehensive ablation studies reveal several key
factors that lead to this success, including sparse sampling,
end-to-end training, and image-text pre-training.
Acknowledgements:
This research was partially done
when Jie was an intern with Microsoft. He was later supported at UNC by NSF Award #1562098, DARPA KAIROS
Grant #FA8750-19-2-1004, ARO-YIP Award #W911NF-
18-1-0336, and Microsoft Investigator Fellowship.
views contained in this article are those of the authors and
not of the funding agency.
A. Additional Experiments
Visual Question Answering. As CLIPBERT is designed
based on 2D CNN, and is pre-trained on image-text corpus, it is also directly applicable to image-text downstream
tasks, such as image-based question answering. We show
CLIPBERT’s performance on VQA 2.0 dataset in Table 9.
The model is ﬁnetuned from the image-text pretrained weights on 8GPUs for 13 epochs, with batch size
32 and learning rate 5e-5.
CLIPBERT shows a reasonable performance compared to the strong pre-training baselines. Note that CLIPBERT uses grid features instead of the commonly used region features, which is much
more computation efﬁcient, e.g., extracting grid features is
around 80× faster than extracting region features according
to the computation time reported in .
B. Downstream Task Adaptation
Our CLIPBERT is quite generic, once trained, it can
be easily adopted and transferred for various downstream
tasks. In particular, in this work, we focus on text-to-video
retrieval and video question answering.
feature test-dev test-std
grid-feat 
ViLBERT 
VL-BERT 
Pixel-BERT 
LXMERT 
UNITER 
Oscar 
CLIPBERT 1×1
Table 9: Comparison with state-of-the-art methods on VQA.
G stands for grid features, R stands for region features.
Text-to-video Retrieval. We use a two-layer MLP with the
last layer [CLS] token hidden state for a two way (i.e.,
matched or not matched) classiﬁcation for retrieval. We use
LogSumExp loss for training. Denote the two-way classiﬁcation logit output for clip τi from the video associated with
the j-th example as g(j)
τi ∈R2, where i = 1, . . . , Ntrain for
training (i = 1, . . . , Ntest for inference; see Section 3 of the
main paper). The LogSumExp prediction p(j) ∈R2 is de-
sum(PNtrain
We then use a negative log likelihood loss for training:
logp(j)[yj],
where D is the dataset, yj is the index of the ground-truth
answer for the j-th example.
We conduct experiments on three popular text-to-video
retrieval datasets, MSRVTT , DiDeMo , and ActivityNet Captions . Table 10 shows the training details for
models on each of the datasets.
Bsz ×Grad-Accu ×#GPUs
ActivityNet Captions
Table 10: Training details for text-to-video retrieval tasks. Bsz
is short for batch size. Grad-Accu stands for gradient accumulation steps. LR means initial learning rate.
Video Question Answering. Similar to text-to-video retrieval task, we take the last layer [CLS] token hidden state
through a two-layer MLP for classiﬁcation. We use Log-
SumExp to aggregate prediction from multiple clips to calculate loss. The formulation of LogSumExp loss is simlar
to Equation 5 except that the dimension of gτi equals to the
number of answer candidates.
We conduct experiments on three video QA datasets,
TGIF-QA , MSRVTT-QA , and MSRVTT MC
Test .
For TGIF-QA, we evaluate three sub-tasks,
i.e., Action, Transition, and FrameQA. We train a separate model for each of the evaluated TGIF-QA tasks. For
MSRVTT MC Test, as it uses the same training set as
the MSRVTT retrieval task, we directly use the trained retrieval model to rank the ﬁve candidate answers. Table 10
shows the training details for models on TGIF-QA tasks and
MSRVTT-QA.
Bsz×Grad-Accu ×#GPUs
TGIF-QA Action
TGIF-QA Transition
TGIF-QA FrameQA
Table 11: Training details for video question answering tasks.
Bsz is short for batch size. Grad-Accu stands for gradient accumulation steps. LR means initial learning rate.