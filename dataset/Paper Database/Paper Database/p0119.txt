Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 551–561,
Austin, Texas, November 1-5, 2016. c⃝2016 Association for Computational Linguistics
Long Short-Term Memory-Networks for Machine Reading
Jianpeng Cheng, Li Dong and Mirella Lapata
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
{jianpeng.cheng,li.dong}@ed.ac.uk, 
In this paper we address the question of how
to render sequence-level networks better at
handling structured input. We propose a machine reading simulator which processes text
incrementally from left to right and performs
shallow reasoning with memory and attention. The reader extends the Long Short-Term
Memory architecture with a memory network
in place of a single memory cell.
This enables adaptive memory usage during recurrence with neural attention, offering a way to
weakly induce relations among tokens. The
system is initially designed to process a single
sequence but we also demonstrate how to integrate it with an encoder-decoder architecture.
Experiments on language modeling, sentiment
analysis, and natural language inference show
that our model matches or outperforms the
state of the art.
Introduction
How can a sequence-level network induce relations
which are presumed latent during text processing?
How can a recurrent network attentively memorize
longer sequences in a way that humans do? In this
paper we design a machine reader that automatically
learns to understand text. The term machine reading is related to a wide range of tasks from answering reading comprehension questions , to fact and relation extraction , ontology learning , and textual entailment . Rather than focusing on a speciﬁc
task, we develop a general-purpose reading simulator, drawing inspiration from human language processing and the fact language comprehension is incremental with readers continuously extracting the
meaning of utterances on a word-by-word basis.
In order to understand texts, our machine reader
should provide facilities for extracting and representing meaning from natural language text, storing
meanings internally, and working with stored meanings to derive further consequences. Ideally, such
a system should be robust, open-domain, and degrade gracefully in the presence of semantic representations which may be incomplete, inaccurate,
or incomprehensible. It would also be desirable to
simulate the behavior of English speakers who process text sequentially, from left to right, ﬁxating
nearly every word while they read 
and creating partial representations for sentence pre-
ﬁxes .
Language modeling tools such as recurrent neural
networks (RNN) bode well with human reading behavior . RNNs treat each sentence as a sequence of words and recursively compose each word with its previous memory, until the
meaning of the whole sentence has been derived. In
practice, however, sequence-level networks are met
with at least three challenges. The ﬁrst one concerns
model training problems associated with vanishing
and exploding gradients , which can be partially ameliorated with
gated activation functions, such as the Long Short-
Term Memory (LSTM) , and gradient clipping . The second issue relates to memory compression problems. As the input sequence gets compressed and blended into a single dense vector, suf-
The FBI is chasing a criminal on the run .
The FBI is chasing a criminal on the run .
FBI is chasing a criminal on the run .
isis chasing a criminal on the run .
chasing a criminal on the run .
a criminal on the run .
criminal on the run .
on the run .
Figure 1: Illustration of our model while reading the
sentence The FBI is chasing a criminal on the run.
Color red represents the current word being ﬁxated,
blue represents memories. Shading indicates the degree of memory activation.
ﬁciently large memory capacity is required to store
past information. As a result, the network generalizes poorly to long sequences while wasting memory
on shorter ones. Finally, it should be acknowledged
that sequence-level networks lack a mechanism for
handling the structure of the input. This imposes
an inductive bias which is at odds with the fact that
language has inherent structure. In this paper, we
develop a text processing system which addresses
these limitations while maintaining the incremental,
generative property of a recurrent language model.
Recent attempts to render neural networks more
structure aware have seen the incorporation of external memories in the context of recurrent neural networks . The idea is to use multiple
memory slots outside the recurrence to piece-wise
store representations of the input; read and write
operations for each slot can be modeled as an attention mechanism with a recurrent controller. We
also leverage memory and attention to empower a
recurrent network with stronger memorization capability and more importantly the ability to discover
relations among tokens. This is realized by inserting a memory network module in the update of a recurrent network together with attention for memory
addressing. The attention acts as a weak inductive
module discovering relations between input tokens,
and is trained without direct supervision. As a point
of departure from previous work, the memory network we employ is internal to the recurrence, thus
strengthening the interaction of the two and leading to a representation learner which is able to reason over shallow structures. The resulting model,
which we term Long Short-Term Memory-Network
(LSTMN), is a reading simulator that can be used
for sequence processing tasks.
Figure 1 illustrates the reading behavior of the
LSTMN. The model processes text incrementally
while learning which past tokens in the memory and
to what extent they relate to the current token being
processed. As a result, the model induces undirected
relations among tokens as an intermediate step of
learning representations.
We validate the performance of the LSTMN in language modeling, sentiment analysis, and natural language inference. In
all cases, we train LSTMN models end-to-end with
task-speciﬁc supervision signals, achieving performance comparable or better to state-of-the-art models and superior to vanilla LSTMs.
Related Work
Our machine reader is a recurrent neural network exhibiting two important properties: it is incremental,
simulating human behavior, and performs shallow
structure reasoning over input streams.
Recurrent neural network (RNNs) have been successfully applied to various sequence modeling and
sequence-to-sequence transduction tasks. The latter
have assumed several guises in the literature such
as machine translation , sentence compression , and reading
comprehension . A key contributing factor to their success has been the ability to handle well-known problems with exploding
or vanishing gradients , leading
to models with gated activation functions , and
more advanced architectures that enhance the information ﬂow within the network .
A remaining practical bottleneck for RNNs is
memory compression : since
the inputs are recursively combined into a single
memory representation which is typically too small
in terms of parameters, it becomes difﬁcult to accurately memorize sequences . In our model, memory and attention
are added within a sequence encoder allowing the
network to uncover lexical relations between tokens.
The idea of introducing a structural bias to neural models is by no means new. For example, it is
reﬂected in the work of Socher et al. who
apply recursive neural networks for learning natural
language representations. In the context of recurrent neural networks, efforts to build modular, structured neural models date back to Das et al. 
who connect a recurrent neural network with an external memory stack for learning context free grammars. Recently, Weston et al. propose Memory Networks to explicitly segregate memory storage from the computation of neural networks in general. Their model is trained end-to-end with a memory addressing mechanism closely related to soft attention and has been applied to machine translation .
Grefenstette et al. deﬁne a set of differentiable data structures (stacks, queues, and dequeues)
as memories controlled by a recurrent neural network. Tran et al. combine the LSTM with an
external memory block component which interacts
with its hidden state. Kumar et al. employ
a structured neural network with episodic memory
modules for natural language and also visual question answering .
Similar to the above work, we leverage memory
and attention in a recurrent neural network for inducing relations between tokens as a module in a larger
network responsible for representation learning. As
a property of soft attention, all intermediate relations we aim to capture are soft and differentiable.
This is in contrast to shift-reduce type neural models where
the intermediate decisions are hard and induction is
more difﬁcult. Finally, note that our model captures
undirected lexical relations and is thus distinct from
work on dependency grammar induction where the learned head-modiﬁer relations are directed.
The Machine Reader
In this section we present our machine reader which
is designed to process structured input while retaining the incrementality of a recurrent neural network.
The core of our model is a Long Short-Term Memory (LSTM) unit with an extended memory tape that
explicitly simulates the human memory span. The
model performs implicit relation analysis between
tokens with an attention-based memory addressing
mechanism at every time step. In the following, we
ﬁrst review the standard Long Short-Term Memory
and then describe our model.
Long Short-Term Memory
A Long Short-Term Memory (LSTM) recurrent neural network processes a variable-length sequence
x = (x1,x2,··· ,xn) by incrementally adding new
content into a single memory slot, with gates controlling the extent to which new content should be
memorized, old content should be erased, and current content should be exposed. At time step t, the
memory ct and the hidden state ht are updated with
the following equations:
775W ·[ht−1, xt]
ct = ft ⊙ct−1 +it ⊙ˆct
ht = ot ⊙tanh(ct)
where i, f, and o are gate activations. Compared
to the standard RNN, the LSTM uses additive memory updates and it separates the memory c from the
hidden state h, which interacts with the environment
when making predictions.
Long Short-Term Memory-Network
The ﬁrst question that arises with LSTMs is the extent to which they are able to memorize sequences
under recursive compression. LSTMs can produce
a list of state representations during composition,
however, the next state is always computed from the
current state. That is to say, given the current state
ht, the next state ht+1 is conditionally independent of
states h1 ···ht−1 and tokens x1 ···xt. While the recursive state update is performed in a Markov manner, it
is assumed that LSTMs maintain unbounded memory (i.e., the current state alone summarizes well the
tokens it has seen so far). This assumption may fail
in practice, for example when the sequence is long
Long Short-Term Memory-Network.
Color indicates degree of memory activation.
or when the memory size is not large enough. Another undesired property of LSTMs concerns modeling structured input. An LSTM aggregates information on a token-by-token basis in sequential order,
but there is no explicit mechanism for reasoning over
structure and modeling relations between tokens.
Our model aims to address both limitations. Our
solution is to modify the standard LSTM structure
by replacing the memory cell with a memory network .
The resulting Long
Short-Term Memory-Network (LSTMN) stores the
contextual representation of each input token with
a unique memory slot and the size of the memory
grows with time until an upper bound of the memory
span is reached. This design enables the LSTM to
reason about relations between tokens with a neural
attention layer and then perform non-Markov state
updates. Although it is feasible to apply both write
and read operations to the memories with attention,
we concentrate on the latter. We conceptualize the
read operation as attentively linking the current token to previous memories and selecting useful content when processing it. Although not the focus of
this work, the signiﬁcance of the write operation
can be analogously justiﬁed as a way of incrementally updating previous memories, e.g., to correct
wrong interpretations when processing garden path
sentences .
The architecture of the LSTMN is shown in Figure 2 and the formal deﬁnition is provided as follows.
The model maintains two sets of vectors
stored in a hidden state tape used to interact with the
environment (e.g., computing attention), and a memory tape used to represent what is actually stored in
memory.1 Therefore, each token is associated with
a hidden vector and a memory vector. Let xt denote the current input; Ct−1 = (c1,··· ,ct−1) denotes
the current memory tape, and Ht−1 = (h1,··· ,ht−1)
the previous hidden tape. At time step t, the model
computes the relation between xt and x1 ···xt−1
through h1 ···ht−1 with an attention layer:
i = vT tanh(Whhi +Wxxt +W˜h˜ht−1)
i = softmax(at
This yields a probability distribution over the hidden
state vectors of previous tokens. We can then compute an adaptive summary vector for the previous
hidden tape and memory tape denoted by ˜ct and ˜ht,
respectively:
and use them for computing the values of ct and ht
in the recurrent update as:
775W ·[˜ht, xt]
ct = ft ⊙˜ct +it ⊙ˆct
ht = ot ⊙tanh(ct)
where v, Wh, Wx and W˜h are the new weight terms of
the network.
A key idea behind the LSTMN is to use attention
for inducing relations between tokens. These relations are soft and differentiable, and components of
a larger representation learning network. Although
it is appealing to provide direct supervision for the
attention layer, e.g., with evidence collected from
a dependency treebank, we treat it as a submodule being optimized within the larger network in a
downstream task. It is also possible to have a more
structured relational reasoning module by stacking
multiple memory and hidden layers in an alternating fashion, resembling a stacked LSTM have a
set of key vectors and a set of value vectors.
2013) or a multi-hop memory network . This can be achieved by feeding the
t of the lower layer k as input to the upper
layer (k + 1). The attention at the (k + 1)th layer is
computed as:
i,k+1 = vT tanh can be applied to
feed xt to upper layers as well.
Modeling Two Sequences with LSTMN
Natural language processing tasks such as machine
translation and textual entailment are concerned
with modeling two sequences rather than a single
one. A standard tool for modeling two sequences
with recurrent networks is the encoder-decoder architecture where the second sequence (also known
as the target) is being processed conditioned on the
ﬁrst one (also known as the source). In this section
we explain how to combine the LSTMN which applies attention for intra-relation reasoning, with the
encoder-decoder network whose attention module
learns the inter-alignment between two sequences.
Figures 3a and 3b illustrate two types of combination. We describe the models more formally below.
Shallow Attention Fusion
Shallow fusion simply
treats the LSTMN as a separate module that can
be readily used in an encoder-decoder architecture,
in lieu of a standard RNN or LSTM. As shown in
Figure 3a, both encoder and decoder are modeled
as LSTMNs with intra-attention. Meanwhile, interattention is triggered when the decoder reads a target token, similar to the inter-attention introduced in
Bahdanau et al. .
Deep Attention Fusion
Deep fusion combines
inter- and intra-attention (initiated by the decoder)
when computing state updates. We use different notation to represent the two sets of attention. Following Section 3.2, C and H denote the target memory
tape and hidden tape, which store representations of
the target symbols that have been processed so far.
The computation of intra-attention follows Equations (4)–(9). Additionally, we use A = [a1,··· ,am]
and Y = [g1,··· ,gm] to represent the source memory tape and hidden tape, with m being the length of
the source sequence conditioned upon. We compute
inter-attention between the input at time step t and
tokens in the entire source sequence as follows:
j = uT tanh(Wggj +Wxxt +W˜g˜gt−1)
j = softmax(bt
After that we compute the adaptive representation of
the source memory tape ˜at and hidden tape ˜gt as:
We can then transfer the adaptive source representation ˜at to the target memory with another gating
operation rt, analogous to the gates in Equation (7).
rt = s(Wr ·[˜gt,xt])
The new target memory includes inter-alignment
rt ⊙˜at, intra-relation ft ⊙˜ct, and the new input information it ⊙ˆct:
ct = rt ⊙˜at + ft ⊙˜ct +it ⊙ˆct
ht = ot ⊙tanh(ct)
As shown in the equations above and Figure 3b, the
major change of deep fusion lies in the recurrent
storage of the inter-alignment vector in the target
memory network, as a way to help the target network review source information.
Experiments
In this section we present our experiments for evaluating the performance of the LSTMN machine
We start with language modeling as it
is a natural testbed for our model.
We then assess the model’s ability to extract meaning representations for generic sentence classiﬁcation tasks
such as sentiment analysis.
Finally, we examine
whether the LSTMN can recognize the semantic
relationship between two sentences by applying it
to a natural language inference task.
is available at 
SNLI-attention.
(a) Decoder with shallow attention fusion.
(b) Decoder with deep attention fusion.
Figure 3: LSTMNs for sequence-to-sequence modeling. The encoder uses intra-attention, while the decoder
incorporates both intra- and inter-attention. The two ﬁgures present two ways to combine the intra- and
inter-attention in the decoder.
Perplexity
Table 1: Language model perplexity on the Penn
Treebank. The size of memory is 300 for all models.
Language Modeling
Our language modeling experiments were conducted on the English Penn Treebank dataset. Following common practice , we
trained on sections 0–20 (1M words), used sections 21–22 for validation (80K words), and sections 23–24 (90K words for testing). The dataset
contains approximately 1 million tokens and a vocabulary size of 10K. The average sentence length
is 21. We use perplexity as our evaluation metric:
PPL = exp(NLL/T), where NLL denotes the negative log likelihood of the entire test set and T the
corresponding number of tokens. We used stochastic gradient descent for optimization with an initial learning rate of 0.65, which decays by a factor
of 0.85 per epoch if no signiﬁcant improvement has
been observed on the validation set. We renormalize the gradient if its norm is greater than 5. The
mini-batch size was set to 40. The dimensions of
the word embeddings were set to 150 for all models.
In this suite of experiments we compared the
LSTMN against a variety of baselines.
one is a Kneser-Ney 5-gram language model (KN5)
which generally serves as a non-neural baseline for
the language modeling task. We also present perplexity results for the standard RNN and LSTM
We also implemented more sophisticated LSTM architectures, such as a stacked LSTM
(sLSTM), a gated-feedback LSTM ) and a depth-gated LSTM ). The gated-feedback LSTM has
feedback gates connecting the hidden states across
multiple time steps as an adaptive control of the information ﬂow. The depth-gated LSTM uses a depth
gate to connect memory cells of vertically adjacent
In general, both gLSTM and dLSTM are
able to capture long-term dependencies to some degree, but they do not explicitly keep past memories.
We set the number of layers to 3 in this experiment,
mainly to agree with the language modeling experiments of Chung et al. . Also note that that
there are no single-layer variants for gLSTM and
dLSTM; they have to be implemented as multi-layer
systems. The hidden unit size of the LSTMN and all
comparison models (except KN5) was set to 300.
The results of the language modeling task are
shown in Table 1. Perplexity results for KN5 and
RNN are taken from Mikolov et al. . As can
be seen, the single-layer LSTMN outperforms these
Examples of intra-attention (language
modeling).
Bold lines indicate higher attention
scores. Arrows denote which word is being focused
when attention is computed, but not the direction of
the relation.
two baselines and the LSTM by a signiﬁcant margin. Amongst all deep architectures, the three-layer
LSTMN also performs best. We can study the memory activation mechanism of the machine reader by
visualizing the attention scores.
Figure 4 shows
four sentences sampled from the Penn Treebank validation set. Although we explicitly encourage the
reader to attend to any memory slot, much attention
focuses on recent memories. This agrees with the
linguistic intuition that long-term dependencies are
relatively rare. As illustrated in Figure 4 the model
captures some valid lexical relations (e.g., the dependency between sits and at, sits and plays, everyone and is, is and watching). Note that arcs here
are undirected and are different from the directed
arcs denoting head-modiﬁer relations in dependency
Sentiment Analysis
Our second task concerns the prediction of sentiment labels of sentences. We used the Stanford Sentiment Treebank , which contains ﬁne-grained sentiment labels (very positive,
positive, neutral, negative, very negative) for 11,855
sentences. Following previous work on this dataset,
Fine-grained Binary
RAE 
RNTN 
DRNN 
DCNN 
CNN-MC 
T-CNN 
PV 
CT-LSTM 
LSTM 
2-layer LSTM 
2-layer LSTMN
Table 2: Model accuracy (%) on the Sentiment Treebank (test set). The memory size of LSTMN models
is set to 168 to be compatible with previously published LSTM variants .
we used 8,544 sentences for training, 1,101 for validation, and 2,210 for testing. The average sentence
length is 19.1. In addition, we also performed a binary classiﬁcation task (positive, negative) after removing the neutral label. This resulted in 6,920 sentences for training, 872 for validation and 1,821 for
testing. Table 2 reports results on both ﬁne-grained
and binary classiﬁcation tasks.
We experimented with 1- and 2-layer LSTMNs.
For the latter model, we predict the sentiment label of the sentence based on the averaged hidden
vector passed to a 2-layer neural network classiﬁer
with ReLU as the activation function. The memory size for both LSTMN models was set to 168 to
be compatible with previous LSTM models applied to the same task. We used pretrained 300-D Glove 840B vectors to initialize the word embeddings. The
gradient for words with Glove embeddings, was
scaled by 0.35 in the ﬁrst epoch after which all word
embeddings were updated normally.
We used Adam for optimization with the two momentum parameters set
to 0.9 and 0.999 respectively. The initial learning
rate was set to 2E-3. The regularization constant was
1E-4 and the mini-batch size was 5. A dropout rate
of 0.5 was applied to the neural network classiﬁer.
We compared our model with a wide range of topperforming systems. Most of these models (including ours) are LSTM variants (third block in Table 2),
recursive neural networks (ﬁrst block), or convolu-
tional neural networks (CNNs; second block). Recursive models assume the input sentences are represented as parse trees and can take advantage of
annotations at the phrase level. LSTM-type models
and CNNs are trained on sequential input, with the
exception of CT-LSTM which operates over tree-structured network topologies such
as constituent trees. For comparison, we also report
the performance of the paragraph vector model ; see Table 2, second block)
which neither operates on trees nor sequences but
learns distributed document representations parameterized directly.
The results in Table 2 show that both 1- and
2-layer LSTMNs outperform the LSTM baselines
while achieving numbers comparable to state of the
art. The number of layers for our models was set to
be comparable to previously published results. On
the ﬁne-grained and binary classiﬁcation tasks our
2-layer LSTMN performs close to the best system
T-CNN . Figure 5 shows examples
of intra-attention for sentiment words. Interestingly,
the network learns to associate sentiment important
words such as though and fantastic or not and good.
Natural Language Inference
The ability to reason about the semantic relationship between two sentences is an integral part of
text understanding. We therefore evaluate our model
on recognizing textual entailment, i.e., whether two
premise-hypothesis pairs are entailing, contradictory, or neutral.
For this task we used the Stanford Natural Language Inference (SNLI) dataset
 , which contains premisehypothesis pairs and target labels indicating their
relation. After removing sentences with unknown
labels, we end up with 549,367 pairs for training,
9,842 for development and 9,824 for testing. The
vocabulary size is 36,809 and the average sentence
length is 22. We performed lower-casing and tokenization for the entire dataset.
Recent approaches use two sequential LSTMs to
encode the premise and the hypothesis respectively,
and apply neural attention to reason about their logical relationship . Furthermore, Rockt¨aschel et al. 
show that a non-standard encoder-decoder architecture which processes the hypothesis conditioned on
Examples of intra-attention (sentiment
analysis).
Bold lines (red) indicate attention between sentiment important words.
the premise results signiﬁcantly boosts performance.
We use a similar approach to tackle this task with
LSTMNs. Speciﬁcally, we use two LSTMNs to read
the premise and hypothesis, and then match them
by comparing their hidden state tapes. We perform
average pooling for the hidden state tape of each
LSTMN, and concatenate the two averages to form
the input to a 2-layer neural network classiﬁer with
ReLU as the activation function.
We used pre-trained 300-D Glove 840B vectors
 to initialize the word embeddings.
Out-of-vocabulary (OOV) words were
initialized randomly with Gaussian samples for optimization with the two momentum parameters set to 0.9 and 0.999 respectively, and the
initial learning rate set to 1E-3. The mini-batch size
was set to 16 or 32. For a fair comparison against
previous work, we report results with different hidden/memory dimensions (i.e., 100, 300, and 450).
We compared variants of our model against different types of LSTMs (see the second block in Table 3). Speciﬁcally, these include a model which
encodes the premise and hypothesis independently
with two LSTMs , a shared
LSTM , a word-by-word
attention model , and a
matching LSTM ).
This model sequentially processes the hypothesis,
and at each position tries to match the current word
with an attention-weighted representation of the
premise (rather than basing its predictions on whole
sentence embeddings). We also compared our mod-
BOW concatenation
LSTM 
LSTM-att 
mLSTM 
LSTMN shallow fusion
LSTMN deep fusion
LSTMN shallow fusion
LSTMN deep fusion
LSTMN shallow fusion
LSTMN deep fusion
Parameter counts |q|M, size of hidden
unit h, and model accuracy (%) on the natural language inference task.
els with a bag-of-words baseline which averages the
pre-trained embeddings for the words in each sentence and concatenates them to create features for a
logistic regression classiﬁer (ﬁrst block in Table 3).
LSTMNs achieve better performance compared
to LSTMs (with and without attention; 2nd block
in Table 3).
We also observe that fusion is generally beneﬁcial, and that deep fusion slightly improves over shallow fusion. One explanation is that
with deep fusion the inter-attention vectors are recurrently memorized by the decoder with a gating
operation, which also improves the information ﬂow
of the network. With standard training, our deep fusion yields the state-of-the-art performance in this
task. Although encouraging, this result should be interpreted with caution since our model has substantially more parameters compared to related systems.
We could compare different models using the same
number of total parameters. However, this would inevitably introduce other biases, e.g., the number of
hyper-parameters would become different.
Conclusions
In this paper we proposed a machine reading simulator to address the limitations of recurrent neural networks when processing inherently structured input.
Our model is based on a Long Short-Term Memory architecture embedded with a memory network,
explicitly storing contextual representations of input tokens without recursively compressing them.
More importantly, an intra-attention mechanism is
employed for memory addressing, as a way to induce undirected relations among tokens.
The attention layer is not optimized with a direct supervision signal but with the entire network in downstream tasks. Experimental results across three tasks
show that our model yields performance comparable
or superior to state of the art.
Although our experiments focused on LSTMs, the
idea of building more structure aware neural models
is general and can be applied to other types of networks. When direct supervision is provided, similar architectures can be adapted to tasks such as dependency parsing and relation extraction. In the future, we hope to develop more linguistically plausible neural architectures able to reason over nested
structures and neural models that learn to discover
compositionality with weak or indirect supervision.
Acknowledgments
We thank members of the ILCC at the School of
Informatics and the anonymous reviewers for helpful comments.
The support of the European Research Council under award number 681760 “Translating Multiple Modalities into Text” is gratefully
acknowledged.