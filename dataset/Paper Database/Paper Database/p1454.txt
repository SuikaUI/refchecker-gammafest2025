FSIM: A Feature Similarity Index for Image Quality Assessment
Lin Zhanga, Student Member, IEEE, Lei Zhanga1, Member, IEEE
Xuanqin Moub, Member, IEEE, and Daivd Zhanga, Fellow, IEEE
aDepartment of Computing, Hong Kong Polytechnic University, Hong Kong
bInstitute of Image Processing and Pattern Recognition, Xi'an Jiaotong University, China
Abstract: Image quality assessment (IQA) aims to use computational models to measure the image quality
consistently with subjective evaluations. The well-known structural-similarity (SSIM) index brings IQA from
the pixel based stage to the structure based stage. In this paper, a novel feature-similarity (FSIM) index for
IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according
to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the
significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast
invariant while the contrast information does affect HVS’ perception of image quality, the image gradient
magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in
characterizing the image contents. Experimental results on benchmark databases show that FSIM can
achieve much higher consistency with the subjective evaluations than all the state-of-the-art IQA metrics
used in comparison.
Index Terms: Image quality assessment, phase congruency, gradient, low-level feature
I. INTRODUCTION
With the rapid proliferation of digital imaging and communication technologies, image quality assessment
(IQA) has been becoming an important issue in numerous applications such as image acquisition,
transmission, compression, restoration and enhancement, etc. Since the subjective IQA methods cannot be
readily and routinely used for many scenarios, e.g. real-time and automated systems, it is necessary to
develop objective IQA metrics to automatically and robustly predict the perceived image quality. Meanwhile,
it is anticipated that the evaluation results should be statistically consistent with those of the human
observers. To this end, the scientific community has developed various IQA methods over the past decades.
1 Corresponding author. Email: . This project is supported by the Hong Kong RGC General Research
Fund (PolyU 5330/07E) and the Ho Tung Fund (5-ZH25).
According to the availability of a reference image, objective IQA metrics can be classified as full reference
(FR), no-reference (NR) and reduced-reference (RR) methods . In this paper, the discussion is confined to
FR methods, where the original “distortion free” image is known as the reference image.
The conventional metrics such as the peak signal-to-noise ratio (PSNR) and the mean squared error
(MSE) operate directly on the intensity of the image and they do not correlate well with the subjective
fidelity ratings. Thus many efforts have been made on designing human visual system (HVS) based IQA
metrics. Such kinds of models emphasize the importance of HVS’s sensitivity to different visual signals,
such as the luminance, the contrast, the frequency content, and the interaction between different signal
components . The noise quality measure (NQM) and the visual signal-to-noise ratio (VSNR) are
two representatives. Instead of building on the assumptions about HVS models, methods such as the
structural similarity (SSIM) index are motivated by the need to capture the loss of structure in the image.
SSIM is based on the hypothesis that HVS is highly adapted to extract the structural information from the
visual scene; therefore, a measurement of structural similarity should provide a good approximation of
perceived image quality. The multi-scale extension of SSIM, called MS-SSIM , produces better results
than its single-scale counterpart. In , Sheikh et al. introduced the information theory into image fidelity
measurement, and proposed the information fidelity criterion (IFC) for IQA by quantifying the information
shared between the distorted and the reference images. IFC was later extended to the visual information
fidelity (VIF) metric in . In , Sampat et al. made use of the steerable complex wavelet transform to
measure the structural similarity of the two images and proposed the CW-SSIM index.
The great success of SSIM and its extensions owes to the fact that HVS is adapted to the structural
information in images. The visual information in an image, however, is often very redundant, while the HVS
understands an image mainly based on its low-level features, such as edges and zero-crossings . In
other words, the salient low-level features convey crucial information for the HVS to interpret the scene.
Accordingly, perceptible image degradations will lead to perceptible changes in image low-level features,
and hence a good IQA metric could be devised by comparing the low-level feature sets between the
reference image and the distorted image. Based on the above analysis, in this paper we propose a novel
low-level feature similarity induced IQA metric, namely FSIM (Feature SIMilarity).
One key issue is then what kinds of features could be used in designing FSIM? Based on the
physiological and psychophysical evidence, it is found that visually discernable features coincide with those
points where the Fourier waves at different frequencies have congruent phases . That is, at points of
high phase congruency (PC) we can extract highly informative features. Therefore, PC is used as the primary
feature in computing FSIM. Meanwhile, considering that PC is contrast invariant but image local contrast
does affect HVS’s perception on the image quality, the image gradient magnitude (GM) is computed as the
secondary feature to encode the contrast information. PC and GM are complementary and they reflect
different aspects of the HVS in assessing the input image. Although FSIM is designed for grayscale image
(or the luminance component of a color image), the chrominance information can be easily incorporated by
means of a simple extension of FSIM, and we call this extension FSIMC. FSIM and FSIMC are evaluated on
six benchmark IQA databases in comparison with seven state-of-the-art IQA methods. The extensive results
show that FSIM and FSIMC can achieve very high consistency with the human subjective evaluations,
outperforming all the other competitors. Particularly, FSIM or FSIMC works consistently well across all the
databases, while other methods may work well only on some specific databases.
The remainder of this paper is organized as follows. Section II discusses the extraction of PC and GM.
Section III presents in detail the computation of the FSIM and FSIMC indices. Section IV reports the
experimental results. Finally, Section V concludes the paper.
II. EXTRACTION OF PHASE CONGRUENCY AND GRADIENT MAGNITUDE
A. Phase congruency (PC)
Rather than define features directly as points with sharp changes in intensity, the PC model postulates that
features are perceived at points where the Fourier components are maximal in phase. Based on the
physiological and psychophysical evidences, the PC model is a simple but biologically plausible model of
how mammalian visual systems could detect and identify features in an image . PC can be considered
as a dimensionless measure for the significance of a local structure.
Under the definition of PC in , there can be different implementations to compute the PC map of a
given image. In this paper we adopt the method developed by Kovesi in , which is widely used in
literature. We start from the 1D signal g(x). Denote by M
n the even-symmetric and odd-symmetric
filters at scale n and they form a quadrature pair. Responses of each quadrature pair to the signal will form a
response vector at position x and scale n: [en(x), on(x)] = [g(x)* M
n, g(x)* M
n], and the local amplitude at
scale n is
. Let F(x)=∑nen(x) and H(x)=∑non(x). The 1D PC can be computed as
and ε is a small positive constant.
With respect to the quadrature pair of filters, i.e. M
n, we adopt the log-Gabor filter because its
transfer function has an extended tail at the high frequency end, which makes it more capable to encode
natural images and it is consistent with measurements on mammalian visual systems . The log-Gabor
filter has a transfer function in the frequency domain of the form G(ω)=exp(-(log(ω/ω0))2/2σ
r ), where ω0 is
the filter’s center frequency and σr controls the filter’s bandwidth.
To compute the PC of 2D grayscale images, we can apply the 1D analysis over several orientations and
then combine the results using some rule. The 1D log-Gabor filters described above can be extended to 2D
ones by simply applying some spreading function across the filter perpendicular to its orientation. By using
Gaussian as the spreading function, the 2D log-Gabor function has the following transfer function
where θj = jπ / J, j = {0,1,…, J-1} is the orientation angle of the filter, J is the number of orientations and σθ
determines the filter’s angular bandwidth. An example of the 2D log-Gabor filter in the frequency domain,
with ω0 = 1/6, θj = 0, σr = 0.3, and σθ = 0.4, is shown in Fig. 1.
By modulating ω0 and θj and convolving G2 with the 2D image, we get a set of responses at each point x
. The local amplitude at scale n, orientation θj is
the local energy along orientation θj is
x . The 2D PC at x is defined as
It should be noted that PC2D(x) is a real number within 0 ~ 1. Examples of the PC maps of 2D images can be
found in Fig. 2.
Fig. 1: An example of the log-Gabor filter in the frequency domain, with ω0 = 1/6, θj = 0, σr = 0.3, and σθ = 0.4.
(a) The radial component of the filter. (b) The angular component of the filter. (c) The log-Gabor filter, which is
the product of the radial component and the angular component.
B. Gradient Magnitude (GM)
Image gradient computation is a traditional topic in image processing. Gradient operators can be expressed
by convolution masks. Some commonly used gradient operators are Robert operator, Laplace operator,
Prewitt operator, Sobel operator, etc. In this paper, we simply use the Sobel operator to compute the gradient
of an image. The partial derivatives of image f(x) along horizontal and vertical directions are
Then, the gradient magnitude (GM) of f(x) can be defined as
III. THE FEATURE SIMILARITY (FSIM) INDEX FOR IQA
With the extracted PC and GM features, in this section we present a novel Feature SIMilarity (FSIM) index
for IQA. Suppose that we are going to calculate the similarity between images f1 and f2. Denote by PC1 and
PC2 the PC maps extracted from f1 and f2, and G1 and G2 the GM maps extracted from them. It should be
noted that for color images, PC and GM features are extracted from their luminance channels. FSIM will be
defined and computed based on PC1, PC2, G1 and G2. Furthermore, by incorporating the image chrominance
information into FSIM, an IQA index for color images, denoted by FSIMC, can be obtained.
A. The FSIM Index
We separate the feature similarity measurement between f1(x) and f2(x) into two components, each for PC or
GM. First, the similarity measure for PC values PC1(x) and PC2(x) is defined as
where the constant T1 is introduced to increase the stability of SPC (such a consideration was also included in
SSIM ). In practice, the determination of T1 depends on the dynamic range of PC values. Similarly, the
GM values G1(x) and G2(x) are compared and the similarity measure is defined as
where T2 is a constant depending on the dynamic range of GM values. In the experiments, both T1 and T2 will
be fixed to all databases so that the proposed FSIM can be conveniently used. SPC(x) and SG(x) are then
combined as follows to get the similarity measure SL(x) of f1(x) and f2(x):
Having obtained the similarity SL(x) at each location x, the overall similarity between f1 and f2 can be
obtained. However, different locations will have different contributions to HVS’s perception of the image.
For example, edge locations convey more crucial visual information than the locations within a smooth area.
Since PC is a dimensionless metric to measure the significance of a local structure , the PC value at a
location can reflect how likely it is a perceptibly significant structure point. Intuitively, for a given location x,
if anyone of f1(x) and f2(x) has a significant PC value, it implies that this position x will have a high impact
on HVS in evaluating the similarity between f1 and f2. Therefore, we use PCm(x) = max(PC1(x), PC2(x)) to
weight the importance of SL(x) in the overall similarity between f1 and f2, and accordingly the FSIM index
between f1 and f2 is defined as
where Ω means the whole image spatial domain.
B. Extension to Color Image Quality Assessment
FSIM index is designed for grayscale images or the luminance components of color images. Since the
chrominance information will also affect the HVS in understanding the images, better performance can be
expected if the chrominance information is incorporated in FSIM for color IQA. Such a goal can be achieved
by applying a straightforward extension to the FSIM framework.
YIQ decomposition
Fig. 2: Illustration for the FSIM/FSIMC index computation. f1 is the reference image and f2 is a distorted version of f1.
At first, the original RGB color images are converted to another color space, where the luminance can be
separated from the chrominance. To this end, we adopt the widely used YIQ color space , in which Y
represents the luminance information and I and Q convey the chrominance information. The transform from
the RGB space to the YIQ space can be accomplished via :
Let I1 (I2) and Q1 (Q2) be the I and Q chromatic channels of the image f1 (f2), respectively. Similar to the
definitions of SPC(x) and SG(x), we define the similarity between chromatic features as
where T3 and T4 are constants. Since I and Q components have nearly the same dynamic range, in this paper
we set T3 = T4 for simplicity. SI(x) and SQ(x) can then be combined as follows to get the chrominance
similarity measure, denoted by SC(x), of f1(x) and f2(x):
Then, the FSIM index can be extended to FSIMC by incorporating the chromatic information in a
straightforward manner
where λ > 0 is the parameter used to adjust the importance of the chromatic components. The procedures to
calculate the FSIMC index is illustrated in Fig. 2. If the chromatic information is ignored in Fig. 2, the FSIMC
index is reduced to the FSIM index.
IV. EXPERIMENTAL RESULTS AND DISCUSSIONS
In order to validate the efficacy of the proposed FSIM and FSIMC indices, we evaluated their performance in
comparison with seven IQA metrics, including six state-of-the-arts (SSIM , MS-SSIM , VIF , VSNR
 , IFC , NQM ) and the classical PSNR, on six publicly available test databases: TID2008 , CSIQ
( LIVE ( IVC ( 
ec-nantes.fr/ivcdb/), MICT ( and A57 ( 
ece.cornell.edu/dmc27/vsnr/vsnr.html). The characteristics of the six databases are summarized in Table I.
We used the public software MeTriX MuX ( for the
implementation of the competing IQA metrics except for SSIM, whose implementation is available at .
TABLE I. BENCHMARK TEST DATABASES FOR IQA
Source Images
Distorted Images
Distortion Types
Image Type
The parameters required in the proposed methods were experimentally tuned and then fixed to all the six
databases: n = 4, J = 4, σr = 0.5978, σθ = 0.6545, T1 = 0.85, T2 = 180, T3 = T4 = 200, and λ = 0.03. Besides, the
center frequencies of the log-Gabor filters at four scales were set as: 1/6, 1/12, 1/24 and 1/48. It should be
noted that the FSIM/FSIMC index will be most effective if used at the appropriate scale. The precisely
“right” scale depends on both the image resolution and the viewing distance and hence is difficult to be
obtained. In practice, we used the following empirical steps proposed by Wang to determine the scale
for images viewed from a typical distance. 1) Let F=max(1, round(N / 256)), where N is the number of pixels
in image height or width; 2) average local F × F pixels and then down-sample the image by a factor of F.
TABLE II: PERFORMANCE COMPARISON OF IQA MODELS ON 6 BENCHMARK DATABASES
In order to evaluate the IQA models, four commonly used performance metrics are employed. The first
two are the Spearman rank-order correlation coefficient (SROCC) and the Kendall rank-order correlation
coefficient (KROCC), which can measure the prediction monotonicity of an IQA model. These two metrics
operate only on the rank of the data points and ignore the relative distance between data points. To compute
the other two metrics we need to apply a regression analysis, as suggested by the video quality experts group
(VQEG) , to provide a nonlinear mapping between the objective scores and the subjective mean opinion
scores (MOS). The third metric is the Pearson linear correlation coefficient (CC) between MOS and the
objective scores after nonlinear regression. The fourth metric is the root mean squared error (RMSE)
between MOS and the objective scores after nonlinear regression. For the nonlinear regression, we used the
following mapping function :
where βi, i =1, 2, …, 5, are the parameters to be fitted.
TABLE III: RANKING OF THE IQA MODELS EVALUATED (EXCEPT FOR FSIMC) ON THE 6 DATABASES
Table II gives the performance comparison of the 9 IQA models on the 6 databases. The three models
producing the greatest SROCC values for each database are highlighted in bold. It should be noted that
except for FSIMC, all the other eight IQA indices are based only on the luminance of the image. From Table
II, we can see that the proposed feature-similarity based IQA model FSIM or FSIMC performs consistently
well across all the databases. In order to demonstrate this consistency more clearly, in Table III we list the
performance ranking of all the IQA models evaluated according to their SROCC values. For fairness, the
Objective score by MS-SSIM
Images in TID2008
Curve fitted
Objective score by SSIM
Images in TID2008
Curve fitted with logistic function
Objective score by VIF
Images in TID2008
Curve fitted
Objective score by VSNR
Images in TID2008
Curve fitted
Objective score by IFC
Images in TID2008
Curve fitted
Objective score by NQM
Images in TID2008
Curve fitted
Objective score by PSNR
Images in TID2008
Curve fitted with logistic function
Objective score by FSIM
Images in TID2008
Curve fitted with logistic function
Objective score by FSIMc
Images in TID2008
Curve fitted with logistic function
Fig. 3: Scatter plots of subjective MOS versus scores obtained by model prediction on the TID 2008 database. (a)
MS-SSIM; (b) SSIM; (c) VIF; (d) VSNR; (e) IFC; (f) NQM; (g) PSNR; (h) FSIM and (i) FSIMC.
FSIMC index, which also exploits the chrominance besides luminance, is excluded in Table III.
From the experimental results summarized in Table II and Table III, we can see that our methods get the
best results on almost all the databases, except for MCIT and A57. However, even on these two databases,
the proposed FSIM (or FSIMC) is only slightly worse than the best results. FSIM and FSIMC can get the most
consistent and stable performance across all the 6 databases. By contrast, for the other methods, they may
work well on a specific database but fail to provide good results on other databases. For example, although
VIF can get rather pleasing results on LIVE, it performs very poorly on TID2008 and A57. Experimental
results also demonstrate that the chromatic information of an image does affect its perceptible quality since
FSIMC has better performance than FSIM on all color images databases.
Although 6 publicly available databases were used in evaluating the IQA models, they do not reflect
equally well the performance of an IQA model because the 6 databases are very different in various aspects,
including the number of reference images, the number of distorted images, the number of distortion types,
the image type, and the number of observers (refer to Table I please). In addition, the experimental
configuration and the methodology to collect subjective scores also vary. Taking these factors into account,
we think that TID 2008 may be the most comprehensive database currently available for IQA model
evaluation and thereby the evaluation results obtained on this database are the most convincing. Our
proposed methods FSIM and FSIMC perform much better than the other IQA models evaluated on this
database. Fig. 3 shows the scatter distributions of subjective MOS versus the predicted scores by the 9 IQA
indices on the TID 2008 database. The curves shown in Fig. 3 were obtained by a nonlinear fitting according
to Eq. (13). It can be seen that the objective scores predicted by FSIM and FSIMC correlate much more
consistently with the subjective evaluations than the other methods.
V. CONCLUSIONS
In this paper, we proposed a novel human visual system (HVS) driven image quality assessment (IQA)
metric, namely Feature-SIMilarity (FSIM) index. The underlying principle of FSIM is that HVS perceives an
image mainly based on its salient low-level features. Two kinds of features, the phase congruency (PC) and
the gradient magnitude (GM), are used in FSIM, and they represent complementary aspects of the image
visual quality. The PC value is also used to weight the contribution of each point to the overall similarity of
two images. FSIM was then extended to FSIMC by incorporating the image chromatic features into
consideration. The FSIM/FSIMC index reflects the similarity of two images at the feature level, which
conforms well to the HVS. The FSIM/FSIMC index was compared with seven representative and prominent
IQA models on six benchmark databases. The experimental results clearly demonstrated that FSIM and
FSIMC outperform all the other state-of-the-art IQA models used in comparison. Particularly, they perform
consistently well across all the test databases, validating that they are very robust IQA models.