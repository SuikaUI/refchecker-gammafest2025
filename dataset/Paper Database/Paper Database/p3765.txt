Social Network Analytics for Churn Prediction in Telco: Model Building,
Evaluation and Network Architecture
María Óskarsdóttira,∗, Cristián Bravob, Wouter Verbekee, Carlos Sarrautef, Bart Baesensc,d, Jan
Vanthienenc
aDepartment of Computer Science, Reykjavik University, Reykjavik, Iceland.
bDepartment of Statistical and Actuarial Sciences, The University of Western Ontario, London, Canada.
cDepartment of Decision Sciences and Information Management, KU Leuven, Leuven, Belgium.
dDepartment of Decision Analytics and Risk, University of Southampton, Southampton, UK.
eFaculty of Economic and Social Sciences and Solvay Business School, Vrije Universiteit Brussel, Brussels, Belgium.
fGrandata Labs, Buenos Aires, Argentina.
Social network analytics methods are being used in the telecommunication industry to predict customer churn with great success. In particular it has been shown that relational learners adapted to this
speciﬁc problem enhance the performance of predictive models. In the current study we benchmark
different strategies for constructing a relational learner by applying them to a total of eight distinct
call-detail record datasets, originating from telecommunication organizations across the world. We
statistically evaluate the effect of relational classiﬁers and collective inference methods on the predictive power of relational learners, as well as the performance of models where relational learners are
combined with traditional methods of predicting customer churn in the telecommunication industry.
Finally we investigate the effect of network construction on model performance; our ﬁndings imply
that the deﬁnition of edges and weights in the network does have an impact on the results of the predictive models. As a result of the study, the best conﬁguration is a non-relational learner enriched with
network variables, without collective inference, using binary weights and undirected networks. In
addition, we provide guidelines on how to apply social networks analytics for churn prediction in the
telecommunication industry in an optimal way, ranging from network architecture to model building
and evaluation.
Keywords: Social Networks Analytics, Churn Prediction, Relational Learning, Collective Inference,
∗Corresponding author
Email addresses: (María Óskarsdóttir), (Cristián Bravo),
 (Wouter Verbeke), (Carlos Sarraute),
 (Bart Baesens), (Jan Vanthienen)
 
 
Telecommunication Industry, Network Construction
1. Introduction
Customer churn prediction in telecommunication companies (telcos) has become an increasingly
popular research topic in the literature in recent years . The competitive landscape of these companies, in which customers
have many providers to choose from and can easily switch providers should they become unhappy,
creates a ﬁerce environment that requires a high level of sophistication to thrive. Apart from that,
studies have shown that customer attrition can be much more expensive for companies than customer
retention is . A widely used strategy is therefore to
identify customers with the highest propensity to churn, and offer them incentives to persuade them
to stay. Long-term customers are also more proﬁtable for the company, since they are more likely
to buy additional products and spread the word of their satisfaction, thus indirectly attracting more
customers . Finally, telcos gather an abundance of data about their customers,
such as demographics, ﬁnancials, usage behavior and call records which presents the opportunity to
make these data actionable by using analytics techniques.
In classical customer churn prediction (CCP) modeling, a binary classiﬁer is applied to available customer data at the company to build a predictive model which assigns each customer a score
representing their propensity of churning. Social network analytics (SNA) has become a substantial
addition to this ﬁeld, as studies show that, when the customer datasets contain network features in
addition to customer attributes, the performance of CCP models is enhanced . The network features are extracted from call networks and
encapsulate both calling behavior and interactions between customers. As such they carry valuable
information that can be used to generate more accurate CCP models. In addition, a group of methods called relational – or network – learners have been successfully used to exploit the information
ﬂow between connected customers in a call network to predict churn . In an exploratory study, Verbeke et al. adapted and applied
the network learning framework and toolkit, NetKit, as proposed by Macskassy & Provost , to
classify customers in a call network. Relational learners, which are made up of relational classiﬁers
and collective inference methods, simulate how customers who have already churned affect others and
Figure 1: The ﬁgures show an example of an application of a relational learner. The ﬁgure on the left
displays a graph with eight customers, of which two have churned (black) and six have not churned
(white). The ﬁgure on the right shows the same network after the RL has been applied. Each customer
now has a score or probability of churning.
how ‘churn inﬂuence’ spreads through the network. Figure 1 shows an example of a network before
and after application of a relational learner. The result is a score for each customer that can be used as
a churn label or an additional feature in the dataset. Verbeke et al. explored both possibilities
and showed that some of the relational learners had great potential when it comes to predicting churn.
When applying SNA, one of the biggest challenges is deﬁning the social network using the available relational data. For telco, these are usually call-detail records (CDR) that need to be ﬁltered and
aggregated in an intelligent way, for which many possibilities exist. These include, but are not limited
to, whether the network should be uni- or bidirectional, or whether the edges should be represented by
binary weights or by weights deﬁned by taking into account the length of the call, or the number of
calls between two customers. Differentiation can be made between calls taking place on certain days
or at certain times of the day and also between types of customers (retail vs. corporate). Some studies
ﬁlter out connections that are non-reciprocal , while others
disregard phone calls that last less than a given threshold, since these possibly represent unintentional
phone calls . These and many other factors need to be taken into consideration
when the network is built. To the best of our knowledge, very few studies investigate the impact of
how the network is deﬁned on the techniques which are applied to them, and the resulting ﬁndings
 . This could be a consequence of the fact that CDR datasets often
contain millions and even billions of records which makes pre-processing and transforming them into
networks both difﬁcult and time consuming.
Table 1: Research Questions. Performance is measured using four measures: lift at 0.5% and 5%,
AUC, and EMP as described in subsection 3.4.
Effect of Relational
RQ1 Which relational learners perform statistically different from the rest when predicting customer churn?
RQ2 Do some relational classiﬁers perform statistically better than the others?
RQ3 Is the performance of some collective inference methods statistically better than the others?
When predicting customer churn, do collective inference methods improve the predictive performance of relational classiﬁers?
Combination of
RL and NRC
Which non-relational classiﬁer model performs best when predicting churn? A model built using network
features only, a model with relational learners scores only, or a model which is built with a combination of
RQ6 Which model type performs better when predicting churn? Relational learners or non-relational classiﬁers?
The objective of this study is to explore and evaluate ways of using SNA for churn prediction in
telco, from network deﬁnition, to model building, and model evaluation and to discuss the implications our results have for academics and practitioners alike. To achieve this, we compare classiﬁcation
methods that incorporate SNA, and investigate how the network architecture affects the performance
of these models. The study is composed of two main parts. Firstly, based on Verbeke et al. ,
our goal is to rank a selection of relational learners with respect to performance and to see whether
combining them with classical non-relational classiﬁers improves churn prediction in telco. Although
Verbeke et al. observed differences in performance between the methods, they could not draw
conclusions regarding the statistical signiﬁcance of these differences. Therefore, we have gathered
eight distinct CDR datasets from around the world, which allows us to apply statistical tests for evaluating signiﬁcance of the results and provide conclusive answers. For this part of the analysis we
pose six research questions as can be seen in Table 1. The ﬁrst four questions are targeted towards
the relational learners with the aim of ranking them by performance when predicting churn (RQ1), as
well as comparing the performance of the two components, relational classiﬁers (RQ2) and collective
inference methods (RQ3), separately. In addition, because collective inference methods have been
shown to improve the performance of relational classiﬁers in other ﬁelds , we
investigate whether this is also the case when predicting churn in telco (RQ4). Thus, we provide a detailed analysis of the various dimensions of the relational learners in terms of performance. Research
question RQ5 examines whether combining relational learners with non-relational classiﬁers improves
performance and ﬁnally, with research question RQ6 we study how relational learners perform in com-
Figure 2: The model building process
parison to non-relational classiﬁers, and thus, whether they are good enough to be used on their own.
The research questions have practical implications for practitioners in the telecommunications industry who can use them as a guideline on how to optimally apply SNA in churn prediction modelling.
The consequences of the research questions are also scientiﬁc, due to the comprehensive analysis of
the relational learners using a unique number of CDR datasets. Figure 2 shows how the churn prediction models were set up, with regards to interaction of relational learners and non-relational classiﬁers.
For this part of the study, we enforce certain restrictions on the network building and non-relational
classiﬁers because it would be infeasible to test every possible approach. These decisions are made
based on existing literature as detailed later. Since CDR are the only available source of data, we could
not make use of alternative customer attributes, such as socio-demographic data, and marketing and
ﬁnancial information, which are commonly used in the literature . Again, to the best of our knowledge, no benchmarking study of this scope, using
this many CDR datasets, has been conducted. The second part of this study explores the effect of network construction on model performance. Using the best performing relational learner on networks
deﬁned with increasing complexity using a variation in deﬁnition of edges and weights, we study the
difference in performance of the resulting classiﬁer. To the best of our knowledge, this study is the
ﬁrst to explore the impact of network composition on the predictive capability of a SNA-based model.
In summary, our study builds on and extends the previous exploratory study by Verbeke et al. 
by exploring the impact of the network architecture or deﬁnition on model performance by studying
different time frames and representation of edges and weights. Additionally, we expand the number
of data sources involved, allowing us to statistically test and evaluate the results that are found and, as
such, to draw conclusive answers to the above-positioned research questions.
We make two main contributions to the ﬁeld. Firstly, we provide answers to the six research questions in Table 1 by empirically evaluating the difference in performance of a set of relational learners
and their combination with non-relational classiﬁers using the eight distinct CDR datasets. Thus, we
provide a ranking of the relational learners, determine whether relational classiﬁers perform better
when used with collective inference methods, and provide conclusive answers on how to combine relational and non-relational methods to achieve optimum performance. Secondly, our follow-up study
shows that the performance of CCP models does in fact depend on how the network is deﬁned. Our
results imply that the complexity of the phenomenon is not in the complexity of the weights, but in
the network structure and the relationship between this structure and the other variables in the dataset.
The rest of this paper is arranged as follows. In the next section, we discuss the related literature on
churn prediction in telco and network construction. Next, we describe the methodology of the current
research in detail followed by a section about the experimental design that we apply. Thereafter, we
present the results of the experiments together with a discussion about those results. In section 6, we
demonstrate how the model is affected by the network, and subsequently we discuss the managerial
and academic insights of our results. The paper concludes with a summary of our main ﬁndings and
suggestions for future research.
2. Related Work
As a research domain, CCP is already well established. This classiﬁcation problem has been studied intensively in various sectors where maintaining relationships with current customers is considered
important. It has been applied in the banking sector , by insurance companies
 , internet service providers , online social
networks , and in the telecommunication industry , which
is the case we discuss here. We refer to Verbeke et al. for an overview of commonly used
classiﬁcation techniques for CCP in telco and a benchmarking study of those techniques.
Recent publications where social network analysis has been applied to predict customer churn in
telco can be seen in the top part of Table 2. These studies show how the performance of CCP models
is enhanced when network effects are taken into account by means of SNA. The table also lists how
edges and weights are deﬁned in each case, together with the number of nodes in the networks and the
duration of the CDR data that are used in each study.
Table 2: Literature overview of the application of social network analytics for customer churn in telco.
Network Details
Authors (Year)
# Datasets
Methods/Application
Phadke et al. 
Prediction of subscriber churn using social network analysis
Churn prediction, inﬂuence propagation
undirected
duration and number of calls, neighborhood overlap overlap
Richter et al. 
Predicting customer churn in mobile networks through analysis
of social groups
Group ﬁrst churn prediction
Dasgupta et al. 
Social Ties and their Relevance to Churn in Mobile Telecom
Spreading Activation
call duration
Kim et al. 
Improved churn prediction in telecommunication industry by
analyzng a large network
Combination of local features and SPA
with logistic regression
undirected
duration of calls
Dierkes et al. 
Estimating the effect of word of mouth on churn and crossbuying in the mobile phone market with Markov logic networks
Logistic regression ,Markov logic with
local and network features
undirected
number and duration of phonecalls
Zhang et al. 
Predicting customer churn through interpersonal inﬂuence
RL(propagation) with LogReg,
Modani et al. 
CDR Analysis Based Telco Churn Prediction and Customer Behavior Insights: A Case Study
Network featurization, CHAID and LogReg
undirected
number and duration of calls
Kusuma et al. 
Combining customer attribute and social network mining for
prepaid mobile churn prediction
Combinations of SPA, network and local variables together with CHAID and
undirected
number of calls and texts with decay
Verbeke et al. 
Social Network analysis for customer churn predictions
Combinations of RL and NRC
1.2/1.4·106
undirected
duration of calls
Backiel et al. 
Mining Telecommunication Networks to Enhance Customer
Lifetime Predictions
network featurization, LogReg
undirected
duration of phonecalls
Baras et al. 
The effect of social afﬁnity and predictive horizon on churn prediction using diffusion modelling
RL:SPA, time until churn
Number of calls
Backiel et al. 
Combining Local and Social Network Classiﬁers to Improve
Churn Prediction
Combination of SPA, network variables, non-relational classifers
duration of calls
Haenlein 
Social interactions in customer churn decisions: The impact of
relationship directionality
Churn Prediction
incoming/outgoing
duration of calls
Rehman & Raza Ali
Customer churn prediction, segmentation and fraud detection in
telecommunication industry
Churn prediction
undirected
duration of calls
6/12 months
Nanavati et al. 
Analyzing the structure and evolution of massive telecom
binary, with calls and texts
Miritello et al. 
Time as a limited resource: Communication strategy in mobile
phone networks
Social behavoir
undirected
duration of calls
Raeder et al. 
Predictors of short-term decay of cell phone contacts in a large
scale communication network
edge prediction
Tomar et al. 
Social network analysis of the short message service
Zhu et al. 
Role deﬁning using behavior-based clustering in telecommunication network
Behavior Clustering
As Table 2 explains, the most widely used relational learner is the spreading activation method
 . It is used on its own and to
produce scores that are then used as variables in non-relational classiﬁers, such as logistic regression
and decision trees . Other works enrich their datasets with
network measures for non-relational classiﬁers, instead of exploiting relational learners . Verbeke et al. compare
the performance of multiple relational learners to the performance of non-relational classiﬁers, with
and without network variables. The best model is obtained when predictions from both relational
learners and non-relational classiﬁers with network variables were combined. Additionally, Backiel
et al. combine these types of models in various ways, with the result that the combined model,
a binary classiﬁcation built using the scores resulting from non-relational classiﬁers and relational
learners (in this case SPA) as variables, gives the best result. Contrary to the current study, almost
none of the discussed papers include a comparison of relational learners in terms of performance or
apply their techniques to multiple CDR datasets.
For most of the papers in Table 2, SNA relies on a single network which is deﬁned and constructed
only once for the situation at hand. In most cases, length of phone calls between customers are used
as weights, but the directionality of the edges varies. All studies rely on in-network customers only,
since information about customers of other telcos is rarely available.
Only a handful of studies compare the effects of networks deﬁned in different ways or test the
difference in results obtained from different networks as we do here. For example, Haenlein 
studies the dynamics of social interactions for customer churn within a directed network. He compares customers’ propensity to churn using call networks with incoming calls, outgoing calls and both
together, producing an undirected network. According to his study, a customer is more likely to churn
if they have been in contact with a person who already churned, but only if the relationship is outgoing, meaning that the customer calls the churned person, and not the other way around. In addition,
Zhu et al. apply behavior-based clustering to deﬁne roles of the customers in their call network
using both incoming and outgoing edges. The people they identify as potential churners make many
phone calls but have low betweenness and closeness. In addition, the length of incoming and outgoing
calls is higher than for other customers.
So far we have only discussed papers that exploit CDR data for churn prediction in telco, whereas
numerous other applications exist. An extensive survey by Naboulsi et al. on the fast-growing
ﬁeld of mobile trafﬁc analysis and how CDR data are increasingly used in data-mining applications
classiﬁes the analyses in the existing literature as social, mobility, and network analyses. Some of
the research that falls into the ﬁrst of these classes, which deals with human dynamics and social
interactions, pays special attention to network construction. The results of these studies might possibly
have interesting implications for problem-setting in our context. The lower part of Table 2 contains
a brief overview of papers where CDR data have been used to build networks for SNA in telco with
various objectives.
Miritello et al. examine how callers distribute their time across their social network in
relation to their network size and intensity of mobile use. According to their results, people with more
connections spend more time communicating than people with fewer connections and the average
time spent on each connection increases as the number of connections increases, up to a threshold of
10 to 40 connections. In addition, independent of the size of their network, people distribute their
time unevenly, thus dedicating a small amount of time to many people and a great deal of time to a
few people. In a study about the decay of edges in a call network, Raeder et al. use weighted
data to determine the importance of edge weight for the persistence and decay of connecting between
people in a call network. Using machine learning techniques, their results imply that directed edge
weight, reciprocated edge weight and recency of connections are important features when predicting
edge decay.
Park et al. propose a relational learner, similar to the spreading activation algorithm, in
order to to validate self-reported demographic data of customers. In their network construction they
use out-degree, number of outgoing calls and duration of outgoing calls and the results imply that the
best performing models are those that use networks with out-degree.
Featurizing networks offers the possibility to compare the importance of various network deﬁnitions. This was the case in Sarraute et al. who extract features based on different edge and
weight deﬁnitions, such as incoming and outgoing calls, length and number of calls, and number of
text messages in addition to taking time of day and the day of week into account. In a subsequent
PCA analysis, total number of calls, total duration of calls, and total number of text messages are
shown to explain most of the variance. This indicates that the activity of users is a good candidate to
characterize users’ social behavior.
3. Customer Churn Prediction Using Social Networks Analytics
3.1. Networks
In this particular predictive analytics framework, SNA-based methods and binary classiﬁers are
applied to assign each customer of a telco to one of two classes: churner or non-churner. The starting
point of conducting social network analysis is the network itself . A network is
composed of nodes and edges, which in this case are the customers of a telecommunications operator
and the correspondence between them, respectively. Formally, the node component V of the network
consists of a set of nodes V = {v1,...,vn} and a label vector L = {l1,...,ln} where each li ∈C =
{c1,...,cm} is the class label –in this case churner or non-churner– of node vi. We denote by V K and
V U the sets of nodes for which class labels are known and unknown, respectively. The known labels
are used to infer labels for the unknown ones.
The edge component E of the network consists of two parts, edges and weights. The edges, E, are
a set of the two-subsets of V , where the edge ei j ∈E represents an existing connection from vi to vj,
i.e. if the two customers have shared a phonecall. If an edge exists between nodes i and j we say they
are connected, and if
∀i, j ∈1,...,n : ei j ∈E ⇐⇒eji ∈E
we say that the network is undirected and directed otherwise. Incoming and outgoing edges of a node
vi are the sets of nodes of the form eji and ei j, respectively
A non-negative weight, wij, can be associated to each edge to denote the strength of the connection. There are various ways to deﬁne the weights. They can be binary, indicating whether two nodes
are connected or not, or be assigned some other value based on additional information. In addition,
weights can vary in time by conferring more recent connections higher importance than older connections. To model this in the network, the weights at time t, wi j,t can be exponentially weighted in time
(wi j)t = e−γtwi j,t
where γ is the decay constant. We obtain the ﬁnal weights by aggregating all (wi j)t for the whole
time period. Weighted networks have been successfully used in credit card and social security fraud
detection .
Finally, the ﬁrst order neighborhood N 1
i of a node vi ∈V is the set of nodes that are connected to
i = {vi}∪{v j|ei,j ∈E, j = 1,...,n}.
3.2. Enriching Non-Relational Classiﬁers with Network Features
A network can be used to extract network features in a process called featurization . Information from the neighborhood of each node, such as the labels of connected nodes and
weights of the edges between them, is used to compute numerical attributes to characterize and describe the nodes. Examples of such features include the number of neighbors a node has (degree), the
number of fully connected subgraphs of three nodes (triangles), relational neighbor score, and probabilistic relational neighbor score. Various features describing the position and connectivity of a node
within a network also exist .
Link-based features are a speciﬁc group of network features, introduced by Lu & Getoor .
These features are constructed using class labels, in addition to nodes and edges. They deﬁne three
types of features: mode-link, count-link and binary-link. The ﬁrst one, mode-link, is a single feature
deﬁned as the most frequently occurring class label amongst the nodes in the neighborhood. The
count-link statistic counts the number of times each different class label appears in the neighborhood,
and the last statistic, the binary-link, documents for each class in C whether or not it appears as a class
label for a node in the neighborhood.
Featurization has been successfully used to improve the performance of churn prediction models in
telco; see Table 2. Typically, the extracted network features are added to the customer dataset, before
binary classiﬁers, such as decision trees, logistic regression, support vector machines and artiﬁcial
neural networks are used to train and test CCP models using the enriched dataset. We limit the number
of binary classiﬁers, since our goal is not to compare them. Instead, we choose three that have been
shown to perform well when predicting churn in telco . We choose the classiﬁers
logistic regression (Log) , random forests
(RF) , and artiﬁcial neural networks (NN) because they
represent different aspects of the trade-off between complexity and predictive capability . Popular in the industry, logistic regression is simple to understand, whereas NN and
RF are more powerful but harder to interpret since they are black-box models. In the model building
we refer to these classiﬁers as non-relational classiﬁers (NRC). Alternative binary classiﬁers were
implemented as well, for example Adaboost.M1 which has been shown to outperform other methods
for CCP in telco . However, its performance did not vary signiﬁcantly from the
classiﬁers mentioned above and it was therefore not included in our experiments.
3.3. Relational Learning in Social Networks
Contrary to featurization for non-relational classiﬁers, churn probabilities can be inferred from
the network directly by exploiting the information ﬂow between the interlinked entities. Relational
learning (RL) for CCP in telco is particularly interesting because it simulates how ‘churn-inﬂuence’
spreads through the network and, as a result, how churners might affect non-churners. In general,
relational learners are composed of two parts: relational classiﬁers and collective inference methods.
Relational Classiﬁers (RC) are the methods which infer class labels for each node in a network
based on the weight of links to other nodes and the labels of those nodes. They perform a single,
local operation going from node to node until all have been classiﬁed. The four relational classiﬁers
applied here are the weighted vote relational classiﬁer (WVRN), class distribution relational classiﬁer
(CDRN), network-only link-based classiﬁer (NLB), and the spreading activation relational classiﬁer
(SPA RC). A detailed description of each RC is provided in Appendix A as our goal is not to focus on
the methods themselves, but to rank them.
When going through the network in this manner, classiﬁcations may not be very stable. Once the
ﬁrst node has been classiﬁed, its label is used to infer the label for the second node, which in turn
might change, which could again have an effect on the ﬁrst node. To capture this behavior, collective
inference methods are applied.
Collective Inference methods (CI) are procedures which infer class labels for the nodes in a network while taking into account how the inferred labels affect each other. They decide in which order
the nodes are labeled and how a ﬁnal label is determined. They have been shown to improve the
performance of relational classiﬁers in genomes and bibliographic networks . In general, CIs perform two operations iteratively until a terminating condition is met.
First, a relational classiﬁer is applied to each node in the network and then the resulting scores are
used to update the labels of the nodes. The methods used here are Gibbs sampling (Gibbs), iterative
classiﬁcation (IC), relaxation labeling (RL), relaxation labeling with simulated annealing (RLSA), and
spreading activation collective inference method (SPA CI). We refer to Appendix A for descriptions
and pseudo codes for the CI methods.
Number of iterations
Relational Learner
gibbs−wvrn
gibbs−cdrn
gibbs−sparc
Number of iterations
Relational Learner
Number of iterations
Relational Learner
Figure 3: The ﬁgures show the results of the sensitivity analysis of some of the relational learners.
When CI methods iteratively classify the nodes in a network, a smoothing effect of the churn
inﬂuence may occur. Before benchmarking the methods along all available datasets, we investigate
this effect for each of the relational learners using one of the datasets. Figure 3 shows the variance of
the predicted churn scores as a function of the number of iterations in the collective inference method
for various relational learners. This sensitivity analysis shows that the variation decreases very rapidly
as the number of iterations increases. This means that, with the default settings of each CI method,
the variation of the ﬁnal scores is very low and consequently there is very little distinction between
churners and non-churners. In particular, for the iterative classiﬁcation CI, the scores stabilize after
only a few iterations. It can be seen from the ﬁgure, that for some relational classiﬁers, such as WVRN,
this decline in variance happens faster than for the others. We note that the CI spreading activation
was not included, as it already contains an early stopping mechanism to compensate for this kind of
smoothing effect. Based on the sensitivity analysis, it was decided to implement the same kind of
mechanism for early stopping in the other CIs.
Overall, we consider four relational classiﬁers and ﬁve collective inference methods, which, together with the option of not applying a CI, results in a total of two dozen unique combinations of
methods, or 24 relational learners.
3.4. Performance Measures
We next discuss two commonly used measures – lift and AUC – in addition to two more recent
ones – the Maximum Proﬁt and the Expected Maximum Proﬁt measures. The ﬁrst two measures
are applicable in classiﬁcation problems, whereas the last two are speciﬁcally developed for churn
prediction models.
Lift is a commonly used performance measure for customer churn . It compares the ratio of churners in a fraction of customers with the
highest predicted probabilities to the ratio of churners in the actual customer base. Thereby, it represents how much better a prediction model is at identifying churners, compared to a random sample
of customers. The top decile lift, considering the top 10% of the customer base, is typically used.
However, this choice of the top fraction is arbitrary and does not necessarily reﬂect the needs of the
company, especially if the customer base is large, as is the case for some of the datasets in this study,
see table 3. Including too many customers in a retention campaign does not only increase the cost of
the campaign but also increases the risk of offering promotions to customers who have no intention
of churning. Therefore, it is more meaningful to consider smaller fractions to identify only those who
are most likely to churn. To compensate for the variation in number of observations in our datasets we
choose to use lift at 0.5% and 5%. As a result, the lift measure proves meaningful for both large and
small organizations.
Another widely used and well established performance measure is the receiver operating characteristic curve (ROC) and its corresponding AUC value . AUC is typically a number
between 0.5 and 1 and encapsulates the trade-off between the true and false positive rates. It can be
interpreted as the probability that a randomly chosen churner is ranked higher than a randomly chosen
non-churner. Recent studies have suggested Hand’s H-measure as a coherent alternative to the AUC
measure but because of evidence of correlation between these two , we will only include the widely used AUC.
In a business setting, CCP modeling is usually conducted to decide which customers to target
in a retention campaign. As such, the speciﬁc requirements of the campaign should be taken into
account when evaluating the models, to accurately measure the anticipated proﬁt. The maximum proﬁt
measure was developed with this objective. Based on the total proﬁt of a
retention campaign, as proposed by Neslin et al. , the average classiﬁcation proﬁt for customer
churn is deﬁned as
Pccp(t;γ,CLV,δ,φ) = CLV(γ(1−δ)−φ)·π0F0(t)−CLV(δ+φ)π1F1(t)
with η the fraction of customers that are targeted, γ the probability that a customer accepts the incentive, CLV the average customer lifetime value, δ the cost of the incentive, π0 the base churn rate and
π1 the base non-churn rate, F0(t) and F1(t) the cumulative density functions for churn and non-churn,
respectively, given the cut-off t, λ the percentage of churners within the targeted fraction, and φ the
cost of contacting the customer. The maximum proﬁt (MP) measure is then deﬁned as
MPccp = max
∀t Pccp(t;η,γ,CLV,δ,φ)
and represents the fraction of customers that should be targeted for a campaign to achieve maximum
proﬁt. As many of the parameters are not always known, a corresponding expected maximum proﬁt
(EMP) measure is proposed. It is given by
γ PccpT(γ);γ,CLV,δ,φ)·h(γ)dγ
where T is the optimal cut-off value for a given γ and h(γ) is the probability density function for γ,
chosen as the beta distribution as in the H-measure.
The maximum proﬁt measure has advantages over both the lift measure and AUC. Firstly, the
fractions chosen for the lift measure are arbitrary and take neither costs nor beneﬁts into account when
measuring the performance. While both lift and AUC are universal and applicable to any classiﬁcation
problem, MP and EMP are the only measures that are speciﬁcally designed to take into account the
requirements of a CCP model, which is the campaign itself, its costs, and its expected return. Here,
we choose to use the EMP measure because it is more robust in terms of the inherent stochastic nature
of costs and beneﬁts in churn management.
4. Experimental Design
4.1. Data Description and Preparation
Following collection, eight distinct CDR datasets were analyzed in order to provide answers to the
research questions in Table 1. Table 3 summarizes the main features of the datasets. Most of the data
come from telcos in Europe, while some have their origins in North America.
Table 3: Dataset Description
Churn Rate
North America
North America
All datasets contain six consecutive months of cell phone usage data for the customers of the
respective company, with information about the time, date and length of phone calls, and in some
cases text messages and data roaming. The oldest records are from 2008 and the most recent ones are
from 2016. Before building networks and churn labels, the datasets are all pre-processed in the same
way. Only in-network phone calls are considered, and – based on data exploration – those lasting less
than four seconds are disregarded. Filtering the CDR data this way is a common approach in this type
of research , ﬁrstly because call records from competing telcos are usually not
available and, as a result, neither is information about customers outside the network and secondly,
shorter phone calls are regarded as unintentional.
As Table 3 shows, the datasets vary greatly in number of customers, which ranges from less than
a hundred thousand to over four million. Actual churn dates are not available for all the datasets so,
for the sake of consistency, we use the following deﬁnitions for churners and churn dates, based on
literature :
Churner: A customer with no perceivable activity for 30 consecutive days.
Churndate: The ﬁrst day of the 30 consecutive inactive days.
These deﬁnitions are appropriate for our purposes, because reduced activity is not only undesirable,
but tracing and detecting churn rapidly, in order to offer incentives to remain loyal before it is too late,
is highly important. Because of these deﬁnitions the last month is needed to build the churn labels and
therefore excluded from the analyses. To ensure comparability of the different types of models, the
ﬁfth month is always used as the test and prediction month. As a result, the ﬁrst four months of each
dataset are used to build the models. The fraction of churners in the customer base, or the churn rates,
are shown in Table 3 for the prediction month.
A ﬁnal observation about the datasets concerns the sparsity of the networks. Sparsity is a typical
characteristic of social networks, since the number of people is often very high but each person is
only connected to a limited fraction of the network. We can deﬁne sparsity as the fraction of non-zero
elements in a matrix, which means that the lower it is, the fewer edges there are in the network. As a
result, lower sparsity means that the customers are less connected within the network. Table 3 displays
the sparsity of each dataset for the month used as train data.
4.2. Deﬁnition of Timeline and Networks
For each of the eight datasets, the call-detail records span six months which we refer to chronologically as M1, M2, M3, M4, M5, and M6. This partition of the data into consecutive months is
the foundation for the timeline of our experimental setup. As mentioned above, the last month M6 is
needed to create the churn labels and is therefore left out of the analyses, and M5 is used for prediction
and validation, which leaves M1, M2, M3 and M4 for model building.
The CDR data is aggregated to construct the networks for the analyses, where several decisions
have to be made about the deﬁnitions of edges and weights. Some are based on literature and others
on speciﬁc factors that we intend to empirically estimate. To measure effects both within the network
when the call was made, and in the network over a longer period, we look at two time periods, shortterm and long-term, deﬁned as one and three months, respectively. Thus, we build separate networks
by aggregating information from CDRs over either one or three months. The weights of the edges
are computed in two different ways: using the total number of phone calls between two customers, or
the aggregated length –measured in seconds– of all phone calls between them, in a given time period.
Both of these deﬁnitions are common in the literature .
Due to insights about the recency of edges in Raeder et al. and Kusuma et al. , greater
importance is ascribed to more recent connections. We use Equation 1 to apply decay to the weights,
with time measured in weeks and the decay constant set to give links three months in the past some
effect, but links that were made over a year before churning no effect. This selection is made based
on exploration of the data and expert knowledge. Because the factors ‘time span’ and ‘weights’ have
two levels each, there are four different combinations of the two factors.
The relational learners and the non-relational classiﬁers require different setups for conducting the
(a) Relational Learners
(b) Non-Relational Classiﬁers
Figure 4: The ﬁgures show how the datasets are split up by month to build the networks for pretraining, training and predicting in the short- and long-term settings for the relational learners and for
out-of-time testing for the non-relational classiﬁers.
experiments, as we describe below.
4.2.1. Setup for Relational Learners
In case of relational learners, our objective is to rank all two dozen of them, as well as evaluate the
RC and CI separately. We implement the time aspect of CCP as in Verbeke et al. , that is, we
train the methods at a speciﬁc time, t, where all labels are known, and use the resulting scores as the
estimated labels at time t +1, which in this case equals M5. This means that we assume that V K = /0
at time t +1 before the analyses are undertaken.
As described in Appendix A, two of the relational learners, CDRN and NLB, need a pre-training
period, deﬁned as having the same length as the corresponding training period, but starting one month
prior as in Verbeke et al. . In summary, eight networks are built for the analysis of each dataset:
for each weight representation, there are two short-term networks, M3 and M4, and two long-term
networks, M1 to M3 and M2 to M4. This is depicted in Figure 4a.
Overall, each RL will produce four sets of scores for each of the eight datasets, or a total of 32
As is evident from Table 3 there is severe imbalance in the distribution of the two classes, churners
and non-churners. Because of the networked nature of the data, commonly used sampling techniques,
such as over- and under-sampling, are not applicable when using relational learners. Since the goal of
the learning is to rank the customers and not assign churn probabilities to them, we do not address this
problem further.
4.2.2. Setup for Non-Relational Classiﬁers
The second part of the experimental setup is to build churn prediction models using non-relational
classiﬁers with network features and scores from relational learners. Thus, we try three combinations
of features: network features only, RL scores only, and network features together with RL scores.
Common network features1 and various RFM features, such as number of days since last phone call,
number of phone calls and total length of phone calls in the last 30, 60 and 90 days, are extracted from
the four network structures described above, in addition to RL scores. For robustness, an Out-of-Time
(OoT) experimental setup is used for this part, as is depicted in Figure 4b. As before, churn labels of
month M5 are used for validation, but when training the models, churn labels of month M4 are used.
Variables for the training dataset originate from months M1, M2, and M3. Thus, models to predict
churn in M4 are trained on data from months M1, M2, and M3 and subsequently the models are applied
to the same variables from months M2, M3, and M4, to predict churn in month M5. As mentioned
above, the classiﬁers we use are logistic regression, artiﬁcial neural networks and random forests. The
random forest models are trained using 500 trees and one hidden layer is used in the neural networks
since this is sufﬁcient for the neural network to be a universal approximator. Other parameters such
as the number of hidden units were tuned experimentally on a separate validation dataset. Because
of the class imbalance, we applied oversampling to the training datasets before building classiﬁcation
5. Relational and Non-Relational Learner Effects
In this section, we present and discuss the results of the experiments. We follow the guidelines of
 to statistically compare classiﬁers and methods. More precisely, to measure a differ-
1Network features include degree, full, churn and non-churn; triangles, full, churn and non-churn; transitivity and linkbased measures, described in subsection 3.2.
ence in performance of various methods, a Friedman test is applied to the average rank in performance
of the methods. A low p-value means that the null hypothesis of no signiﬁcant difference between the
methods can be rejected. If the null hypothesis is rejected, a post-hoc Nemenyi test is applied to
explore which methods diverge from the rest . In other cases, we turn to the
non-parametric Kruskal-Wallis test, to investigate differences in performance, where a low p-value
means that the null hypothesis of the samples originating from the same distribution can be rejected.
We use a 95% conﬁdence level when reporting signiﬁcance of results.
Appendix B has a table where all abbreviations are explained.
5.1. Relational Learners
The ﬁrst four research questions in Table 1 involve comparing and ranking all relational learners
in addition to the ﬁve collective inference methods and four relational classiﬁers.
Friedman tests applied to the performance of the relational learners results in p-values of less
than 0.01 for all performance measures, which means there is a signiﬁcant difference in performance
between at least some of them. A post-hoc Nemenyi test established the results in Figure 5, where
the methods in the gray boxes are those that do not differ signiﬁcantly from the best performing one
according to the four measures, 0.5% lift, 5% lift, AUC and EMP. When performance is measured
by 0.5% lift, 14 relational learners perform worse than the best performing one, 16 when measured
by 5% lift, nine when measured by AUC, and 15 according to EMP. Moreover, four of the relational
learners consistently perform the best according to all four performance measures, and nine relational
learners perform worse than the best performing method for all measures. From the ﬁgure, it is evident
that there is a signiﬁcant difference between some of the methods; and it is also evident that the same
methods tend to consistently outperform some of the others. Noticeably, relational learners which
have the relational classiﬁer network-only link-based classiﬁer (NLB) tend to perform better, except
when combined with the collective inference method iterative classiﬁcation (IC). Clearly, there is a
signiﬁcant difference between good and bad methods.
We further investigate this difference by applying a Friedman test to the four relational classiﬁers.
The p-values for all performance measures are less than 0.05 and therefore we continue to explore the
pairwise differences between the classiﬁers. The results can be seen in Figure 6a which shows the
difference in performance between all RC, measured by 0.5% lift. The gray boxes, which represent
signiﬁcant differences, show that the differences NLB-CDRN, spaRC-NLB and WVRN-NLB are all
The ﬁgure shows the relative ranks, from best, at the top, to worst, at the bottom, as
measured by the four performance measures. The gray boxes represent the methods which are not signiﬁcantly different at the 95% conﬁdence level. Lines for the methods which performed signiﬁcantly
worse by all measures were not drawn.
signiﬁcant. This implies that NLB performs signiﬁcantly better than all the other relational classiﬁers,
and it is the only one that differs from the rest. Similar ﬁgures for the other performance measures
show the same results. The superiority of NLB can be explained by how comprehensive it is, as it
considers the whole network before inferring labels. In a pre-training step, this classiﬁer builds a
logistic regression model for the nodes in the network, using their link-based measures. This model is
then used in the inferencing process, and because it already has internalized the information from the
whole network it is able to make more accurate predictions. In contrast, the other relational classiﬁers
calculate a score based on each node’s neighborhood, without any pre-training and without taking into
account the rest of the network.
The collective inference methods are tested in the same way. Again the Friedman test results in
low enough p-values and the results of a post-hoc analysis of the differences can be seen in Figure
6b. The results in the ﬁgure are based on the 0.5% lift, but performance according to the other
measures shows the same behavior. As the ﬁgure shows, IC always performs signiﬁcantly worse than
the other methods, and not applying CI performs signiﬁcantly better than applying any of the CIs. A
probable explanation for why IC performs worse than the rest is that, in each step of the iteration, it
GGGGGGGGGGG
nlb − cdrn
spaRC − cdrn
wvrn − cdrn
spaRC − nlb
wvrn − nlb
wvrn − spaRC
(a) Differences of Relational Classiﬁers
ic − gibbs
no − gibbs
rl − gibbs
rlsa − gibbs
spaCI − gibbs
spaCI − ic
spaCI − no
spaCI − rl
spaCI − rlsa
(b) Differences of Collective Inference Methods
The ﬁgures show pairwise differences in performance measured by 0.5% lift of the four
relational classiﬁers (left) and the six collective inference methods (right). Gray- and white- colored
boxes represent differences that are and are not signiﬁcant at the 95% conﬁdence level, respectively.
assigns churn labels and not probabilities or scores like the other methods. By making such absolute
decisions, a signiﬁcant amount of information is lost, and there is less ﬂexibility when inferencing in
subsequent steps; therefore the prediction-making process becomes less accurate on the whole. The
sensitivity analysis in subsection 3.3 even reveals that the assigned labels do not change after the ﬁrst
few iterations.
Finally, we test whether there is a difference in performance between methods with CI and without
CI using the parametric Kruskal-Wallis test. The p-value for each performance measure was less than
0.001, meaning that there is strong statistical evidence of the difference between methods that apply
CI and methods that do not. Further investigation shows that methods without CI performed better.
Although this may be a contradictory result to previous ﬁndings on the performance of collective
inference methods , there are various possible explanations for
this behavior. In fact, as we have already shown, the variation in churn scores decreases very rapidly
when CIs are applied, thus decreasing the predictive performance. The call networks in this speciﬁc
problem setting are very large and sparse, and with few churn signals. As a result, when the CIs start
spreading the ‘churn inﬂuence’ it gets too diluted and in the end the signals are not strong enough or
too similar to the non-signals to be meaningful. This also seems to suggest that ‘churn inﬂuence’ does
not spread much beyond a person’s ﬁrst neighborhood, meaning that only the friends of a churner are
affected by it and the friends of friends much less so.
To conclude, we have successfully answered the ﬁrst four research questions. Figure 5 provides
a ranking of the performance of the relational learners (RQ1). Our results show that NLB is the best
performing relational classiﬁer (RQ2) and that there is no best performing collective inference method
although IC performs the worst (RQ3). Finally, we can conclude that collective inference methods do
not improve the performance of relational classiﬁers (RQ4).
5.2. Non-Relational Classiﬁers
The second part of the analysis aims to answer research question RQ5 about non-relational classi-
A Friedman test is applied to the results of churn prediction models built with the binary classiﬁers
logistic regression, random forests and neural networks; using as attributes, network features only, RL
scores only, and both of these combined. The p-value for all performance measures was less than
0.01 except for lift at 0.5%, where the p-value equalled 0.17. A comparison of the performance of the
models can be seen in Figure 7 which shows the relative ranking of each combination of classiﬁer and
feature set for each of the eight datasets. In the ﬁgure, a lighter gray color means a worse performance
and a darker gray signiﬁes a better performance. In addition, the measurement values are shown for
the two lift measures and AUC but the actual ranking for EMP, since these values were too varied to
ﬁt adequately in the ﬁgure. Subsequent analysis reveals that the models with both types of variables
perform signiﬁcantly better than models with only one of them. This is evident by the fact that for
each of the three triples in every line of the four ﬁgures, the column on the right, which represents
models built using both sets of features, tends to be darker than the other two columns. Although that
is not the case for some of the datasets, it is the overall effect.
Finally, no signiﬁcant difference is found between the performance of models built with either RL
scores or network variables; only models with a combination of the two features performed signiﬁcantly better. This illustrates that adding features increases the model performance and that relational
learner scores do possess valuable knowledge that is not captured by network features alone. This is
further conﬁrmed by the fact that the scores were almost never correlated with the network features,
although there were often correlations within each set of features. However, scores produced when
0.12 1.49 1.69 1.69
1.95 1.81 1.27
4.45 2.89 5.04 5.98 1.06 2.81 6.22 2.73
0.88 1.71 3.94 1.75 1.84 1.68
1.15 1.46 1.33 3.06 0.89 1.26 1.04 1.38
0.98 4.65 6.44 0.83 1.85 0.81 0.79
0.98 5.81 6.19 1.13 4.92 1.31 1.08 0.75 1.27
4.31 2.44 3.09 4.35 2.59 3.14 3.16 0.49 2.77
6.25 4.57 4.76 4.66 5.04
0.47 1.49 1.64 1.65 1.02 1.79 1.65 1.15
3.71 2.84 4.02 4.94
1.91 5.17 2.81 5.16
1.55 1.77 2.81
0.68 1.59 1.16 2.23 1.04 1.22 0.67 1.06 1.16
0.78 4.32 5.91 0.99 1.67 0.99 0.71 0.87 0.75
0.77 4.52 6.16 1.08 3.94 1.12 0.59 0.78 0.69
3.52 0.86 3.14
4.26 5.32 4.99 4.84 5.18 5.69 3.73 2.32
0.66 0.67 0.58 0.66 0.64 0.55 0.65
0.84 0.74 0.84 0.86 0.51 0.68 0.83 0.74 0.81
0.66 0.56 0.66 0.71 0.57 0.69 0.74 0.57 0.75
0.56 0.57 0.61 0.56 0.53 0.67 0.55 0.59
0.55 0.82 0.88 0.61
0.61 0.52 0.56 0.56
0.58 0.82 0.89 0.66 0.81 0.66 0.56 0.55
0.78 0.74 0.79
0.74 0.81 0.78
0.85 0.81 0.86 0.87 0.81 0.89 0.85 0.64 0.85
The ﬁgures compare the performance of each of the three non-relational classiﬁers with
the three subsets of variables for the eight datasets and the four performance measures. Each row
represents one dataset. In the ﬁrst three columns on the left are the logistic regression models (Log),
in the three columns in the middle are the random forests models (RF) and in the last three columns on
the right are the neural networks models (NN). Each classiﬁer is used to build a model using network
variables only (NO), RL scores only (RL), and network variables together with RL scores (All). The
ﬁgures show the the measurement values –or the actual rank in the case of EMP– of each combination
and a color coded ranking of the nine models for each dataset, with a darker color representing a better
performance.
Table 4: P-values from the Kruskal-Wallis test when comparing the difference in performance between
each relational learner listed in the table and all non-relational classiﬁers.
Performance Measure
Relational Learner
applying the CI Gibbs sampling were never correlated with any other features and not even amongst
themselves, which was a common behavior for the other RL scores.
We can conclude, based on our results, that models with a combination of network features and
relational learner scores perform best (RQ5).
5.3. Comparison of RL and NRC
Regarding research question RQ6, when comparing the predictive performance of relational learners and non-relational classiﬁers, we apply the Kruskal-Wallis test which results in p-values of less
than 0.01 for all performance measures. Further exploration reveals that the non-relational classi-
ﬁers, with any set of features, perform signiﬁcantly better than the relational learners. This result is
understandable, because relational learners take into account fewer network properties than the nonrelational classiﬁers do. The former approach, which simulates the effect people have on others further
away in the network and takes into account how churners in the network inﬂuence each other, is therefore a valuable addition to the latter approach. It also shows that, for the purpose of accurate churn
predictions, relational learners are better when used in combination with non-relational classiﬁers.
We note that these results hold for the cumulative performance of the relational learners. When
considered individually, it is evident that not all of the best performing relational learners are outperformed by the non-relational classiﬁers. This is veriﬁed using the Kruskal-Wallis test to compare the
performance of each relational learner, that appears in the gray box for lift at 5% in Figure 5, to the
overall performance of the three non-relational classiﬁers. The resulting p-values for the four performance measures can be seen in Table 4. The table shows that the ﬁrst four relational learners are only
Normalized Value
(a) Relational Learners
Normalized Value
(b) Non-Relational Classiﬁers
Figure 8: The ﬁgures show, for the relational learners (left) and for the non-relational classiﬁers (right),
the distribution of the four performance measures (boxplots) and the correlations between them (dotted
outperformed by the non-relational classiﬁers when performance is measured using AUC, but not using any of the other three performance measures. The last four relational learners are all outperformed
by the non-relational classiﬁers, except when measured using EMP.
We conclude our discussion of relational learners and non-relational classiﬁers by looking at their
distribution and correlations, see Figure 8. For the two approaches, we normalize the values of each
performance measure in order to make them comparable and present their distributions with boxplots. Subsequently, measurements of the same model are connected, as shown by the dotted lines.
The ﬁgure thus illustrates a combination of box- and scatter- plots with correlations. The distributions
of the measures vary, as the box plots show, and the values are also not very correlated, as illustrated
by how frequently the dotted lines connecting the measurements intersect. Since all the measures give
consistent results, this is a good indication of the robustness of our ﬁndings. Note that in Figure 8b,
most of the values of the EMP measure are very small, whereas one of the datasets resulted in values
that were much higher (by a factor of 106). This explains why there is a cluster around 0 in the ﬁgure.
To answer the ﬁnal research question, we assert that overall, non-relational classiﬁers perform
better than relational learners, although a few of them are just as good at predicting churn as the
non-relational methods.
6. Effects of Network Construction
As mentioned, research on the exact impact of network construction on models and ﬁndings in
the telecommunication industry is scarce. In most studies, a network is built in a speciﬁc way, often
without detailed supporting arguments or explanations . A construction issue
which is overlooked in such studies is that the ﬁndings could depend on how the network is deﬁned,
and might therefore change if the network is constructed differently by whoever is conducting the
experiment. Clearly, such a situation would result in potentially incorrect conclusions, as the predictive
capability of the model would not be affected by the sheer explanatory power of the data but by the
decisions of the modeler.
Here, in a follow-up study, we explore this potential effect, by building more than 500 networks
with varying deﬁnitions of edges and weights and aggregating them in numerous ways. The detailed
setup to arrive at the 500 networks will be explained in subsection 6.1. The number of networks can increase very rapidly when taking all possibilities into account, and as a result, it becomes computationally infeasible to compare all model combinations. Instead, we exploit the result of the benchmarking
study in the previous section for ‘algorithm selection’. Since we have shown that CIs do not improve
the performance of RC, those RLs can be left out, and based on Table 4 some RLs are just as good
at predicting churn as NRC is. Therefore, we have chosen one relational learners (NLB) as a proxy
method to predict churn in the different types of networks to compare the performance of the resulting
models. As a result, we hope to guide further studies on how to optimize network architecture when
predicting churn in telco.
6.1. Deﬁning the Network Architecture
For this follow-up study, we select the BC2 dataset as it represents an ‘average’ among all the
datasets, both in size and sparsity.
As before, predictions are made for month M5 and, to reduce complexity, the relational learner
was applied on one time frame – the short-term time frame which spans one month. Since NLB
needs a pre-training period as well, months M3 and M4 are used for the pre-training and training,
respectively. As in section 4.1, phone calls lasting less than four seconds are removed. The edges of
Table 5: Segmentation of Networks
Part-of-Week
Time-of-Day
Comb. Part-of-Week
Comb. Time-of-Day
Working days(WD)
2Day+Evening
Weekend(WE)
Evening(16-24)
Night(0-8)
3Day+Evening
the network are deﬁned in three ways: incoming, representing all phone calls made to a customer;
outgoing, the phone calls made by a customer; and undirectional, when the previous distinction is
not made. The weights of the edges are deﬁned in four ways. These are the aggregated length of all
phone calls, the total number of all phone calls, the normalized average of these two, and ﬁnally an
indicator of whether a phone call was made or not. We refer to these as length, count, average and
binary. Section 3.1 explicates how weights can be weighted in time with decay using equation 1 to
confer more importance to more recent phone calls. This technique is also included in our network
construction together with aggregating the weights normally. We call these two levels ‘with decay’
and ‘simple weights’. For each network the combination of weights, directions and decay leads to
24 different combinations. The CDR data hold information about the date and time of each phone
call. We use this information to segment the networks, thus building separate networks for phone calls
made on each day of the week, during working days (WD) and weekends (WE) and during the day,
evening and night. Finally, we combine times of the day and parts of the week in various ways to build
more networks. Table 5 shows a summary of all the networks that were built in this way.
On a social level, a relationship between two people only exists when both call the other. As a
result, some studies remove connections that were not reciprocal before applying SNA. To include this possibility in our exploration, we repeat the whole setup
with non-reciprocal calls removed.
In total over 500 networks were built for the one dataset, which gives an indication of the scalability problem associated with these experiments and why more datasets and more relational learners are
not included. The proxy classiﬁer is subsequently applied to all the networks, and the performance of
the resulting models compared.
Table 6: Results of whole network measured in AUC and (lift at 0.5%)
0.748(3.21)
0.748(3.21)
0.504(1.16)
0.748(3.21)
0.755(3.21)
0.748(3.21)
0.743(3.05)
0.748(3.21)
Undirected
Time-of-Day comb
0.748(3.19)
0.749(3.19)
0.744(2.87)
0.777 (3.19)
0.765(3.19)
0.748(3.19)
0.713 (3.20)
0.777(3.19)
Part-of-Week comb
0.557(1.30)
0.557(1.30)
0.519(1.12)
0.557(1.30)
0.748(3.21)
0.748(3.21)
0.698(2.05)
0.748(3.21)
0.741(2.54)
0.741(2.54)
0.502(1.14)
0.741 (2.54)
0.741(2.54)
0.741(2.54)
0.721 (2.36)
0.741(2.54)
Time-of-Day comb
0.740(2.56)
0.740(2.56)
0.688 (1.93)
0.740(2.56)
0.743(2.56)
0.741(2.56)
0.643(1.69)
0.740(2.56)
Part-of-Week comb
0.535(1.26)
0.535 (1.26)
0.510(1.12)
0.535(1.26)
0.741(2.54)
0.741(2.54)
0.623(1.51)
0.741(2.54)
0.717(2.28)
0.717(2.26)
0.703(2.22)
0.717(2.28)
0.717(2.28)
0.717(2.26)
0.704(2.22)
0.717(2.28)
Time-of-Day comb
0.728(2.26)
0.716(2.26)
0.680(1.87)
0.716(2.26)
0.721(2.26)
0.718(2.26)
0.679(1.87)
0.716(2.26)
Part-of-Week comb
0.729(2.28)
0.717(2.26)
0.666(1.83)
0.717(2.28)
0.723 (2.28)
0.717(2.26)
0.666(1.83)
0.717(2.28)
Simple weights
With decay
6.2. Results of Network Construction
We ﬁrst discuss the results of the whole network where non-reciprocal edges have not been removed.
Table 6 shows the performance of the relational learner when predicting churn measured in AUC
and lift at 0.5%.
For comprehensibility, we have chosen to show only three segmentations of the networks, namely
the whole network, the best combination of part-of-week and the best combination of time-of-day. The
best combination of part-of-week is the fourth one in Table 5 or WD+ 1
3WE and the best combination
of time-of-day is the ﬁrst one in Table 5 which is 1
2Day + Evening. From Table 6 it is clear that the
performance on the part-of-week combinations is lower than that on the other networks. In addition,
performance on the whole network tends to be slightly better than on the time-of-day combination
network. This result is the ﬁrst interesting behavior that we can now extract. In some cases there is an
increase in predictive capability by segmenting the network by part-of-week and also by time-of-day,
with weekdays more important than weekends, and evenings more important than daytime. These
results are consistent with the conclusion from our previous section: calling the close circle of the
user (family and close friends) tends to be the most relevant factor when spreading churn inﬂuence.
There is a greater chance that these calls occur in the evenings (after work) or during the weekdays,
as, during the weekend, it is more likely that the users will be in the company of their close circle.
Given that such a weighting scheme can be used to improve predictive capability, and that this
is closely related to the nature of the problem, then a theoretical approach of ﬁrst propagating the
network inﬂuence over the whole network (the second best approach), later analyzing extracting on
whom the inﬂuence is exerted, and ﬁnally weighting the networks to reﬂect this group might bring an
improvement in predictive capabilities of a few percentage points.
When comparing the deﬁnition of the edges – that is, the difference between performance on
undirected, outgoing, and incoming networks – it is clear the undirected outperforms the other two.
However, it is not as clear whether the outgoing or the incoming network is better. This result contradicts the ﬁndings of Haenlein , where the outgoing edges are shown to have higher correlation
with churn. Given that our study uses a far larger number of mobile phone users (representing all
calls in a given time frame) instead of a selected sample – Haenlein uses 3431 users – we can
venture that the directionality effect might occur only on some selected cliques, but in general it is
only the presence of connectivity that is the main factor for allowing the spread of inﬂuence.
In addition, there seems to be a correlation between the edge deﬁnition and segmentation of the
network, in that the performance of part-of-week network is higher for the incoming network and
lower for the outgoing network, while the opposite holds for the time-of-day network.
Regarding the weights of the edges, the performance on the network with binary weights is almost
consistently better or at least as good as on the networks with length and count, which have very
similar performance. This indicates that it does not matter whether length or counts are used for the
weights and that the simplest variant, binary weights, will result in models that are just as efﬁcient.
On the average network, performance is always the worst. This behavior hints at the importance of
the existence of connectivity rather than the intensity of the communication between two users. As
mentioned several times, it is those in the close circle of the user who are more at risk of churn when
the user churns, so the number, and particularly the length of the call would be a poor proxy of the
intensity of the relationship (a couple that lives together would probably not make very long calls
to each other consistently, for example) and, as such, binary weights that reﬂect whether there is a
connection or not would be sufﬁcient to represent this relationship.
Finally, we compare performance on networks with and without decay on the weights. The networks with decay often slightly outperform the variant without decay, at least when performance is
measured in AUC. This effect is not as clear in the table for lift at 0.5%. This result, now consistent
with Haenlein , indicates that the inﬂuence of a churner is spread over the users that are in
recent contact with the churner. This is quite obvious: the close circle is likely to be in permanent (so
also recent) contact with the user, and users not in recent contact with the user would not have that
Table 7: Results of reciprocal network measured in AUC and (lift at 0.5%).
0.681(1.83)
0.681(1.83)
0.676(1.81)
0.681(1.83)
0.681(1.83)
0.681(1.83)
0.676(1.81)
0.681(1.83)
Undirected
Time-of-Day comb
0.681(1.85)
0.681(1.85)
0.661 (1.71)
0.681(1.85)
0.681(1.85)
0.681(1.85)
0.661 (1.71)
0.681(1.85)
Part-of-Week comb
0.681(1.83)
0.681(1.83)
0.651(1.67)
0.681(1.83)
0.682(1.83)
0.681(1.83)
0.651(1.67)
0.681(1.83)
0.681(1.83)
0.681(1.83)
0.664(1.73)
0.681(1.83)
0.683(1.83)
0.681(1.83)
0.664(1.73)
0.681 (1.83)
Time-of-Day comb
0.681(1.85)
0.680(1.85)
0.622(1.53)
0.682(1.85)
0.680 (1.85)
0.680(1.85)
0.622(1.53)
0.682(1.85)
Part-of-Week comb
0.681(1.83)
0.681(1.83)
0.606 (1.48)
0.681 (1.83)
0.682(1.83)
0.681(1.83)
0.606 (1.48)
0.681 (1.83)
0.681(1.83)
0.681(1.83)
0.664(1.73)
0.681(1.87)
0.681(1.83)
0.681(1.83)
0.664(1.73)
0.681(1.83)
Time-of-Day comb
0.681(1.87)
0.680(1.87)
0.639 (1.61)
0.681(1.83)
0.680(1.87)
0.680(1.87)
0.639(1.61)
0.681(1.83)
Part-of-Week comb
0.681(1.83)
0.681(1.83)
0.626(1.55)
0.681(1.83)
0.683(1.83)
0.681(1.83)
0.626(1.55)
0.681(1.83)
Simple weights
With Decay
effect. Another interesting conclusion that can be extracted from this result is that the time frame for
inﬂuencing is short, as our networks cover six months of calls at most.
We now turn to the results for networks with reciprocal edges only, which can be seen, measured
in AUC and lift at 0.5%, in Table 7. Overall, the difference in performance between the types of
edges, types of segmentation, types of weights and with and without decay is much less evident than
above, and in most cases there is no difference. It is possible to detect slightly better performance in
the undirected network. However, the clear difference between these results and the results in Table 6
is that, by removing the non-reciprocal edges, the performance has decreased signiﬁcantly.
To conclude, the use of the best performing network – that is, one which is constructed with binary
weights, undirected and weighted in time – brings about a signiﬁcant increase in performance, so the
modeler has a large responsibility to correctly deﬁne the best network for the problem that is being
tackled, as failure to do so will result in less predictive capability and therefore less potential gains.
7. Scientiﬁc and Managerial Insights
Applying SNA for CCP in telco is a complex process where multiple factors need to be taken
into consideration. The results of the previous sections, however, can be used as guidelines on how
to conduct this process in a practical way to achieve best performance. In addition to being relevant
for business users, the results also have important implications for academics because experiments as
extensive as these ones, for SNA-based CCP in telco, do not have precedence in the literature.
Firstly, the follow-up study on the effect of network construction on model performance suggests
Table 8: Research Questions with Answers. Performance is measured using four measures: lift at
0.5% and 5%, AUC, and EMP as described in subsection 3.4.
Effect of Relational
RQ1 Which relational learners perform statistically different from the rest when predicting customer churn?
See Figure 5
RQ2 Do some relational classiﬁers perform statistically better than the others?
RQ3 Is the performance of some collective inference methods statistically better than the others?
When predicting customer churn, do collective inference methods improve the predictive performance of relational classiﬁers?
Combination of
RL and NRC
Which non-relational classiﬁer model performs best when predicting churn? A model built using network
features only, a model with relational learners scores only, or a model which is built with a combination of
RQ6 Which model type performs better when predicting churn? Relational learners or non-relational classiﬁers?
that how the network is deﬁned does indeed make a difference to the performance of the models that
are applied to them. Networks with undirected edges and binary weights show the highest performance. In addition, more recent connections seem to matter more than older ones do. This is a very
meaningful result for the day-to-day business user since it implies that simpler networks are just as
good as more complex ones. Therefore, less effort needs to be made when ﬁltering data and building
networks, thus simplifying the process. Furthermore, as more recent connections are more important,
less data is needed. Because of the lack of research on the effect of network construction on model
performance, the consequences of our study are also important in an academic setting. They could
furthermore help in understanding human behavior when it comes to social ties and churn.
Secondly, our extensive empirical comparison of the performance of relational learners and their
combination with binary classiﬁers can be summarized with the six research questions and answers in
Table 8. We compare all relational learners collectively, as well as their two components – relational
classiﬁers and collective inference methods. Our tests reveal a small group of learners that perform
better overall, and which are signiﬁcantly better at predicting churn than the rest. The classiﬁer,
network-only link-based classiﬁer proposed by Lu & Getoor is the only one that signiﬁcantly
differs from the others, and it also proved to be the best performing one. This classiﬁer uses the
whole network to build a logistic regression model with link-based features from the network. Not
only does it take into account information from a node’s neighborhood when inferring a label, like
the other classiﬁers, but also considers the joint information from the whole network. In fact, it is
very similar to the logistic NRC model. Two of the collective inference methods show a signiﬁcant
difference from the rest. On the one hand, iterative classiﬁcation is signiﬁcantly worse than the other.
It is most likely because it infers labels and not scores, which gives it less ﬂexibility. On the other
hand, not combining RC with any CI proves more efﬁcient than the alternative. This is an interesting
result since CIs have been shown to improve the performance of RC in other ﬁelds. In the case
of telcos, however, the churn signals might not be strong, common and interconnected enough for
them to survive being spread through the network in multiple iterations, and the resulting scores are
simply too diluted for the models to be of any use for making accurate predictions. We also test the
effect of building churn prediction models using non-relational classiﬁers, with and without scores
from relational learners. We show that the NRC performs better than the RL by themselves, but
also that including the scores in the classiﬁers has a signiﬁcant increase in their performance. Thus
the scores do help, when combined with other features in NRC. However, when the performances
of best performing relational learners are compared separately to those of the NRC models, there
is not always a clear advantage of using the NRC models and, in this case, RLs do perform just
as well, which indicates that they can be used on their own as churn prediction models. Important
consequences of the research questions can be identiﬁed for both business and scientiﬁc communities.
A business user, such as a retention manager in a telco, can use the results when creating a data driven
campaign. By using the result of this paper, a model with a non-relational classiﬁer will ensure optimal
results. Because we have shown that the scores of relational learners improve the model performance,
enriching the dataset with scores from a relational classiﬁer such as NLB is recommended. On the
other hand, in terms of scientiﬁc contributions, our result that collective inference methods do not
improve the performance of relational classiﬁers, as is the case in other ﬁelds , is important. Furthermore, the conclusion that RL scores make a difference in the NRC
models, suggests that "churn inﬂuence" is not only bound to a customer’s ﬁrst order neighborhood,
but reaches further in the network. Finally, since many relational learners exist, our results provide a
quantitative evaluation of these methods and how to apply them; which is an important contribution
to the research domain of churn prediction in the telecommunication industry.
The performance of churn prediction models needs to be evaluated in order to compare different
models and select the best one . The choice of the performance measure should
depend on the purpose of the model and take into account how it is to be used. In CCP, this means
accurately identifying the customers with the highest propensity to churn, since the loss of these customers will result in the highest loss for the provider. The commonly used AUC and lift measures do
not take into account the proﬁt and costs which inevitably accompany a retention campaign, in contrast to the Maximum Proﬁt Measure . This proﬁt-centric
measure evaluates models by optimizing the proﬁt of the model and the fraction of the customers that
should be offered promotions in order to get the highest proﬁt.
8. Conclusion
8.1. Main Contributions
Social network analysis has been shown to make a difference when applied to customer churn
prediction in telco. From the deﬁnition of the network itself, to extracting insightful features, and
building and evaluating the predictive models, multiple decisions need to be made at every step of the
process. In this study, we performed tests on some of these possibilities, investigated the impact of
the construction of networks and the applied techniques on model performance and offered guidelines
on how this can be done optimally. Overall our study offers two main contributions to the existing
literature.
Our ﬁrst main contribution is an empirical comparison of the performance of relational learners
and their combination with binary classiﬁers when predicting customer churn in telco.The results were
evaluated on a large number of CDR datasets, which allowed us to apply statistical tests to assess the
signiﬁcance of our results. The CDR datasets originated from a number of telcos across the world
and varied in both size and churn rate. The performance of the resulting models was evaluated with
four measures, which gave consistent and robust results. A benchmarking of SNA methods for churn
prediction in telco on this scale does not have precedence in the literature to the best of our knowledge.
An overview of our ﬁndings can be seen in Table 8.
The second main contribution of this paper is an exploratory study of various network architectures
and how they effect model performance. As far as we know, this has not been studied explicitly in the
context of customer churn before. Our results imply that network deﬁnition does matter for performance, with undirected, binary network resulting in the highest performance and recent connections
having greater importance than older ones do. Furthermore, the inclusion of contextual information
that leads to an appropriate deﬁnition of the network is key in assuring maximum predictive capabilities.
8.2. Future Work
Although we have been able to provide answers to the research questions, many more still remain
unanswered and can be viewed as objectives for future studies.
First of all, a vital component of each dataset was missing in our analyses: local variables. These
represent information about the customers as isolated entities without relations to other customers,
such as demographic information, pricing plans, and ﬁnancials, among others. Previous studies have
shown that models built with these variables alone perform worse than when network features are
included as well . We would like to see how this effect presents itself together
with relational learners and include it in the experimental design of our study.
Secondly, only information about phone calls was used to build networks throughout the study.
It would be interesting to see what effect including networks created from text messages and even
roaming information would have.
Thirdly, we have provided an insight into how network construction might effect model performance. This was only done by the means of one relational learner and one dataset. A more comprehensive study with more datasets and other types of models would offer the possibility to evaluate this
effect statistically and, as a result, provide guidelines on how to construct the network to achieve the
highest performance. In addition, since we only used one classiﬁer, we were unable to verify whether
there is correlation between network construction and classiﬁer when it comes to model performance.
Finally, an in-depth study would allow us to determine the optimal way of constructing networks,
with regard to edges and weights, and to determine the actual timespan of CDR data that are needed
to achieve best performance.