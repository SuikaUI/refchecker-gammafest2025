Leveraging Financial News for Stock Trend Prediction
with Attention-Based Recurrent Neural Network
Huicheng Liu
Department of Electrical and Computer Engineering
Queen’s University, Canada
Kingston, ON, Canada K7L 2N8
Stock market prediction is one of the most attractive research topic since the
successful prediction on the market’s future movement leads to signiﬁcant
proﬁt. Traditional short term stock market predictions are usually based on
the analysis of historical market data, such as stock prices, moving averages
or daily returns. However, ﬁnancial news also contains useful information on
public companies and the market.
Existing methods in ﬁnance literature exploit sentiment signal features,
which are limited by not considering factors such as events and the news context. We address this issue by leveraging deep neural models to extract rich
semantic features from news text. In particular, a Bidirectional-LSTM are
used to encode the news text and capture the context information, self attention mechanism are applied to distribute attention on most relative words,
news and days. In terms of predicting directional changes in both Standard
& Poor’s 500 index and individual companies stock price, we show that this
technique is competitive with other state-of-the-art approaches, demonstrating the eﬀectiveness of recent NLP technology advances for computational
Recurrent Neural Network, Stock Prediction, Attention
Mechanism, S&P 500
1. Introduction
Stock market prediction is the act of trying to determine the future value
of a company stock . Apparently, the successful prediction of a stock’s future price can yield signiﬁcant proﬁt, making the prediction problem an area
 
of strong appeal for both academic researchers and industry practitioners.
However, Stock market prediction is usually considered as one of the most
challenging issues among time series predictions due to its noise and volatile
features . During the past decades, machine learning models, such as Support Vector Regression (SVR) and Support Vector Machines(SVM) ,
have been widely used to predict ﬁnancial time series and gain high predictive
accuracy .
How to accurately predict stock movement is still an open question with
respect to the economic and social organization of modern society.
well-known eﬃcient-market hypothesis (EMH) suggests that stock prices
reﬂect all currently available information and any price changes based on
the newly revealed relevant information. However, due to the implicit correlations between daily events and their eﬀect on the stock prices, ﬁnding
relevant information that contribute to the change of price for each individual stock are diﬃcult. Besides, the inﬂuences of events to stock prices can
occur in indirect ways and act in chain reactions, which sets obstacles for
precise market prediction. There are three approaches related to the information required to make a prediction. The ﬁrst approach, technical analysis,
is based on the premise that the future behavior of a ﬁnancial time series
is conditioned to its own past.
Secondly, fundamental analysis, is based
on external information as political and economic factors. A major source
of information are text from the internet, these information are taken from
unstructured data as news articles, ﬁnancial reports or even microblogs. Nofsinger et al shows that in some cases, investors tend to buy after positive
news resulting in a stress of buying and higher stock prices, they sell after
negative news resulting in a decrease of prices. Finally the third approach
considers as all relevant information coming from both, ﬁnancial time series
and textual data.
In this work, our goal is to leverage public released ﬁnancial news and
train a model named Attention-based LSTM (At-LSTM) to make prediction on directional changes for both Standard & Poor’s 500 index and individual companies stock price. Our model consists a Recurrent Neural network(RNN) to encode the news text and capture the context information,
self attention mechanism is applied to distribute attention on most relative
words, news and days . The model input are ﬁnancial news titles extracted
from Reuters and Bloomberg. Our model take advantages from the rapid
development of deep neural networks and we show that our model is competitive with other state-of-the-art approaches, demonstrating the eﬀectiveness
of recent NLP technology advances for computational ﬁnance.
The rest of the paper are organized as follows. We introduce related work
in Section 2. Followed by, we present some background and the methodologies
for our proposed prediction model in Section 3.
Experimental setup and
results are demonstrated in Section 4. The paper ﬁnally concludes and point
out future directions in Section 5.
2. Related work
Stock market prediction is an intriguing time-series learning problem in
ﬁnance and economics, which has attracted a considerable amount of research. Eﬀorts on predicting stock market have been carried out based on
diﬀerent resources and approaches.
For example, one of the most widely
studied approach relies on analyzing recent prices and volumes on the market .
Analyzing stock market using relevant text is complicated but intriguing . For instance, a model with the
name Enalyst was introduced in Lavrenko et al. Their goal is to predict
stock intraday price trends by analyzing news articles published in the homepage of YAHOO ﬁnance. Mittermayer and Knolmayer implemented several
prototypes for predicting the short-term market reaction to news based on
text mining techniques. Their model forecast 1-day trend of the ﬁve major
companies indices. Wu et al. predicted stock trends by selecting a representative set of bursty features (keywords) that have impact on individual
stocks. Vivek Sehgal et al. introduced a method to predict stock market using sentiment. Similarly, Micha l Skuza et al. used sentiment from
postings on twitter to predict future stock prices. However, these methods
have many limitations including unveiling the rules that may govern the dynamics of the market which makes the prediction model incapable to catch
the impact of recent trends.
More recently, neural networks have been leveraged to further improve
the accuracy of prediction.
In general, neural networks are able to learn
dense representations of text, which have been shown to be eﬀective on a
wide range of NLP problems, given enough training sample. This is the case
in stock market prediction where the prices of stocks are available together
with a great collection of relevant text data. This provides a good setting
for exploring deep learning-based models for stock price prediction. More
speciﬁcally, dense representations can represent related sentiment, events,
and factual information eﬀectively, which can then be extremely challenging
to represent using sparse indicator features.
Advance of deep learning models has inspired increasing eﬀorts on stock
market prediction by analyzing stock-related text.
For example, Ding et
al. showed that deep learning representation of event structures yields
better accuracy compared to discrete event features. They further augmented
their approach
 to incorporating an outside knowledge graph into the
learning process for event embedding. As another example, Chang et al. 
used neural networks to directly learn representations for news abstracts,
showing that it is eﬀective for predicting the cumulative abnormal returns
of public companies. Other work,e.g., , has proposed diﬀerent
models of neural network that improve the prediction accuracy.
3. Methodology
In this section, we ﬁrst introduce some relative background on our prediction models. Followed by, we introduce the design of our proposed model
to predict the directional movements of Standard & Poor’s 500 index and
individual companies stock price using ﬁnancial news titles. The model is
named as Attention-based LSTM (At-LSTM) and shown in Fig 1.
3.1. Background
3.1.1. Bag of Words Model and Word Embedding
The Bag-of-Words model is a simplifying representation often used in
Natural Language Processing and Information Retrieval . Also known
as the vector space model.
In this model, a text (such as a sentence or
a document) is represented as a bag of its words, disregarding grammar
and even word order but keeping multiplicity. The bag-of-words model is
commonly used in document classiﬁcation where the occurrence of each word
represents a feature for training a classiﬁer.
Word Embedding is the collective name for a set of language modeling and feature learning techniques in Natural Language Processing (NLP)
where words or phrases from the vocabulary are mapped to vectors of real
numbers . Conceptually it involves a mathematical embedding from a
space with one dimension per word to a continuous vector space with much
lower dimension. Neural networks can be used to generate this mapping and
the most common algorithm used are continuous bag of words(CBOW) and
skip-gram algorithm .
Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as
syntactic parsing and sentiment analysis.
3.1.2. Convolutional Neural Network
As shown in ﬁgure 1, Convolutional Neural Network (CNN) is a class
of deep, feed-forward artiﬁcial neural networks that has successfully been
applied to analyzing visual imagery and NLP related tasks. Same as neural
network, CNN are made up of neurons that have learnable weights and biases.
Each neuron receives some inputs, performs a dot product and optionally
follows it with a non-linearity. A Convolutional layer is composed by four
consecutive operations: Convolution, subsampling(pooling), activation and
dropout. The convolution layer can help select features so that CNN requires
minimal pre-processing compared to other deep learning models. The core
part of the Convolutional neural network is the convolution ﬁlter.
Convolution ﬁlters leverage two important ideas that can help improve a
machine learning system: sparse interaction and parameter sharing. Sparse
interaction contrasts with traditional neural networks where each output is
interactive with each input. As a result, the output is only interactive with
a narrow window of the input. Parameter sharing refers to reusing the ﬁlter
parameters in the convolution operations, while the element in the weight
matrix of traditional neural networks are used only once to calculate the
3.1.3. Long-Short Term Memory
Long Short Term Memory (LSTM) networks are a special kind of RNN,
proposed by Hochreiter in 1997 . LSTM units are a building unit for layers
of a Recurrent Neural Network(RNN) . A RNN composed of LSTM units
are often called an LSTM network. LSTM are explicitly designed to avoid the
long-term dependency problem. A common LSTM unit is composed of a cell,
an input gate, an output gate and a forget gate. The cell is responsible for
”remembering” values over arbitrary time intervals; hence LSTM is capable
of learning long-term dependencies.
input image
layer l = 0
convolutional layer
with non-linearities
layer l = 1
subsampling layer
layer l = 3
convolutional layer
with non-linearities
layer l = 4
subsampling layer
layer l = 6
fully connected layer
layer l = 7
fully connected layer
output layer l = 8
Figure 1: The architecture of the original convolutional neural network, as introduced by
LeCun et al. , alternates between convolutional layers including hyperbolic tangent
non-linearities and subsampling layers. In this illustration, the convolutional layers already
include non-linearities and, thus, a convolutional layer actually represents two layers. The
feature maps of the ﬁnal subsampling layer are then fed into the actual classiﬁer consisting
of an arbitrary number of fully connected layers. The output layer usually uses softmax
activation functions.
Forget Gate
Input Gate
Output Gate
Figure 2: The architecture of the original Long short term memory neural network, as
introduced by Hochreiter et al.
 , Here, ft is the forget gate’s activation vector, it is
the input gate’s activation vector, ot is the output gate’s activation vector, xt is the input
vector to LSTM unit, ht is the output vector of the LSTM unit and Ct is the cell state
The expression long-short term refers to the fact that LSTM is a model
for the short-term memory which can last for a long period of time. An
LSTM is well-suited to classify, process and predict time series given time
lags of unknown size and duration between important events. Besides, LSTM
shows the promising result in sentence encoding in many NLP applications
 . The LSTM archticture are shown in ﬁgure 2, the computations of
LSTM cells are:
ft = σ(Wf[ht−1, xt] + bf)
it = σ(Wi[ht−1, xt] + bi)
˜Ct = tanh(WC[ht−1, xt] + bC)
Ct = ft ⊗Ct−1 + it ⊗˜Ct
ot = σ(Wo[ht−1, xt] + bo)
ht = ot ⊗tanh(Ct)
The forget gate is described in Equation (1) and is used to decide whether
a value should be reserved. The Input gate in Equation (2) is used to control
which values should be updated through a certain time step. Here, a new
value ˜Ct is created as per Equation (3) using the tanh function. Next, with
Equation (4), the cell state will be updated from Ct−1 to Ct, ft and it are
used here to decide whether an information should be discarded. in Equation
(5), an output gate is used to ﬁlter out redundant values. The ﬁnal output is
calculated by Equation (6). W is the weight matrix and b refers to the bias.
3.1.4. Optimization Algorithm and Loss Function
In this section, we will brieﬂy introduce the Optimization algorithm and
loss function we used in our prediction model.
• Adadelta Optimization Algorithm: Apart from Stochastic Gra-
dient Descent, Adaptive Gradient Algorithm or the famous Adam algorithm , we chose Adadelta as our optimization algorithm.
Adadelta is an optimization algorithm that can be used to update
network weights iterative based on training data. Adadelta combines
the advantages of two other extensions of stochastic gradient descent,
Adaptive Gradient Algorithm and Root Mean Square Propagation Algorithm. The method dynamically adapts over time using only ﬁrst
order information and has minimal computational overhead beyond
vanilla Stochastic Gradient Descent . The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, diﬀerent model architecture choices, various data modalities
and selection of hyperparameters.
• Cross Entropy Loss: In information theory, the cross entropy between two probability distributions p and q over the same underlying
set of events measures the average number of bits needed to identify
an event drawn from the set . Cross entropy is the most suitable
loss function since we want to measure the output from our prediction
model with it’s true output. A binary cross entropy loss formula are
shown in Equation (7). In which, J(w) refers to the loss, N refers to the
number of examples, yn is the expected output and ˜yn is true output.
We use this formula as our loss function in the model.
N H(pn, qn) = −1
[yn ∗log ˜yn + (1 −yn) ∗log(1 −˜yn)]
3.2. Model design
In this subsection, we introduce the design of our Attention-based LSTM
model (At-LSTM). The design has four stages: Input and Embedding layer,
news-level Bi-LSTM and self-attention layer, day-level Bi-LSTM and selfattention layer and the ﬁnal output and prediction layer. These stages will
be described below and shown in ﬁgure 3.
Figure 3: Model Structure
3.2.1. Input and Embedding Layer
Model Input: The same world event can be described in diﬀerent expression ways. This variety makes the representation space for events very
sparse. Several approaches represent the event as tuple < S; V ; O >
to gain generalization. However, we argue that this representation is oversimpliﬁed and might lose lots of valuable information. Instead, we use the
entire news title content as our models input and use LSTM-based encoder to
encode it to a distributional representation to tackle with sparsity problem.
Word and Character Embedding: For each input news, we remove the
punctuation and then use a word embedding layer to calculate the embedded
vectors for each word. The embedding layer takes a sequence of sentences as
input, this sequence corresponds to a set of titles of news articles. These embedding are unique vectors of continuous values with length w = (w1, ..., wl)
and wi ∈Rm for each word in the training corpus, m is the word level
embedding dimension.
Existing pre-trained word embedding such as Glove and Word2Vec 
typically comes from general domains( Google News or Wikipedia, etc). How-
ever, these word embeddings often fail to capture rich domain speciﬁc vocabularies. We therefore train our own word embedding with ﬁnancial domain
news text consisting of news articles from Reuters an Bloomberg. The data
are further described in Section 4.
In addition, we leverage character composition from Chen et al. and
concatenate the character level composition with the original word embedding to gain rich represent for each word. The character composition feeds all
characters of each word into a Convolutional Neural Network (CNN) with
max-pooling to obtain representations c = (c1, ..., cl) and cn ∈Rn for
each word in the training corpus, n is the character composition dimension.
Finally, each word is represented as a concatenation of word-level embedding
and character-composition vector ei = [wi; ci]. A matrices es ∈Rk∗(m+n) can
be used to represent a news after the embedding layer, where k is the length
of the news.
3.2.2. News Level Bi-LSTM and Self-Attention Layer
Bi-LSTM Encoding: After the embedding layer, we fed the words and
their context in the news title into a Bi-LSTM based sentence encoder to
perform distributional representation. Bidirectional LSTM (Bi-LSTM) is a
variant of LSTM which shows better result then uni-direction LSTM in recent
NLP tasks as they can understand context better. A bidirectional LSTM runs
a forward and backward LSTM on a sequence starting from the left and the
right end, respectively . In this case, Bi-LSTM can not only preserve
information from the past, but also catch the information from the future.
We obtain the hidden vectors(−→
hi shown in Equation (8) and (9)) from
the sentence encoders and concatenate them to Ht = [−→
h1, ...., −→
i ∈R2u represents the ith news title after encoding in date t and m refers
to the sequence number.
hn = −−−−→
hn = ←−−−−
Word level Self-attention layer: Instead of taking the average of
the hidden vector after the sentence encoding, we leverage multi-hop selfattention mechanism on top of the Bi-LSTM layer. The attention mech-
anism takes the whole LSTM hidden states Ht
i as input, and outputs a vector
of weights A:
A = softmax(W2tanh(W1Ht⊺
Shown in ﬁgure 4, here W1 is a weight matrix with a shape of W1 ∈Rda∗2u
which u refers to the hidden unit of the news level Bi-LSTM. and W2 is a
vector of parameters with size W2 ∈Rr∗da, da and r are hyper parameter
that can be set arbitrarily. Ht
i are sized Ht
i ∈Rn∗2u, and the annotation
vector A will have a size A ∈Rr∗n, the softmax() ensures all the computed
weights sum up to 1. Then we sum up the LSTM hidden states Ht
i according
to the weight provided by A to get a vector representation N t
i for the input
sentence. We can deem Equation (10) as a 2-layer MLP without bias, whose
hidden units numbers are da and parameters are W1, W2. We compute the
r weighted sums by multiplying the annotation matrix A and LSTM hidden
Figure 4: Self attention mechanism
Eventually, the sentence encoding vector Ht
i then becomes a matrix N t
Rr∗2u and we use N t
i to represent the ith news title in date t after encoding.
By using multi-hop attention mechanism, the vector representation usually
focuses on a speciﬁc component of the sentence, like a special set of related
words or phrases. Therefore, it is expected to reﬂect an aspect, or component
of the semantics in a sentence instead of adding attention on a speciﬁc word.
Shown in equation (12), we apply another MLP layer on top of our selfattention layer to learn which attention group should be rewarded with the
highest assign value. We name this as attention-over-attention, the weight
matrix have a shape of W3 ∈Rr, and the ﬁnal representation of the sentence
encoding vector N t
i are shaped as N t
i ∈R2u. At last, we use N t
i to represent
a news title encoded from the input.
i = tanh(W3N t
News level Self-attention layer: Not all news contributes equally to
predicting the stock trend. Hence, in order to reward the news that oﬀers
critical information, we apply the same structure multi-hop self-attention on
top of the encoding layer to aggregate the news weighted by an assigned
attention value. Speciﬁcally:
A = softmax(W5tanh(W4N t⊺))
Dt = tanh(W6Dt + b2)
Here, N t = (N t
1, ...., N t
m) and N t ∈Rm∗2u, in which m refers to the
number of news in date t.
Note that the weights [W4, W5, W6, b2] in the
news level attention layer are diﬀerent from the word level attention layer
weight [W1, W2, W3, b1]. A vector Dt represents the temporal sequence for
all the news proposed in the date t.
The merit of using multi-hop selfattention mechanism is that it learns and assign diﬀerent groups of attention
value to the news encoding. Formally, the ﬁrst group of attention reward the
news that contains positive sentiments to the stock market(”raise”, ”growth”
or ”decrease”, ”down” etc). Whereas the second group of attention assign
there reward to the news that mentions the major companies in the S&P 500
(”Microsoft”,”Google” instead of a small company outside of the S&P 500).
Obviously, the attention layer can be trained end-to-end and thus gradually learn to assign more attention to the reliable and informative news based
on its content.
3.2.3. Day level Bi-LSTM and self-attention layer
hi = −−−−→
hi = ←−−−−
We adopt day level Bi-LSTM to encode the temporal sequence of corpus
vectors Di, t ∈[1, N]. Shown in Equation (16) and (17), We obtain the hidden vectors(−→
hi) from the day-level Bi-LSTM and concatenate them
to Hi = [−→
h1, ...., −→
hN], Hi represents a vector that encodes the temporal
sequence Dt where N refers to the sequence number. Since the news published at diﬀerent dates contribute to the stock trend unequally, we adopt
self-attention mechanism again to reward the dates that contribute most to
the stock trend prediction, Shown in Equation below:
A = softmax(W8tanh(W7H⊺
V = tanh(W9V + b3)
In the formula, D = (D1, ...., Dt) and D ∈RN∗2v , V ∈R2v represents
the ﬁnal vector for all news proposed before the prediction date t + 1 in
a delay window with size N, where v is the hidden unit number in the
day level Bi-LSTM. Note that the weight matrix [W7, W8, W9, b3] in the day
level attention layer are diﬀerent from the weight matrices mentioned in the
previous section.
3.2.4. Output and Prediction Layer
The last stage of our At-LSTM model is a traditional fully connected
layer with softmax as activation function whose output is the probability
distribution over labels. In this work, the objective is to forecast the direction
of daily price movements of the stock price, this direction are used to create
a binary class label where a label represents that the stock price will
increase and label represents that the stock price will decrease.
4. Experiments
4.1. Experimental Setup
4.1.1. Data
We evaluated our model on a data set of ﬁnancial news collected from
Reuters and Bloomberg over the time period from October 2006 to November
2013. This data set was made publicly available by Ding et al. and shown
in table 1. We further collected data from Reuters for 473 companies listed
in the Standard & Poor’s 500 over the time period started from November
2013 to march 2018. Meanwhile, the historical stock price data from October
2006 to March 2018 for all individual shares in Standard & Poor’s 500 are
collected from Yahoo Finance.
The second part of the data are used for
individual stock price prediction and shown in table
2. Due to the page
limit, We only show the major companies listed in the S&P 500, this will
also be applied in the result section.
Data for S&P 500 index prediction
Development
Time interval
20/10/2006-
27/06/2012
28/06/2012-
13/03/2013
14/03/2013-
20/11/2013
Table 1: Data for S&P 500 index prediction
Following Ding et al. we focus on the news headlines instead of the
full content of the news articles for prediction since they found it produced
better results. We use news articles on each day and predict if the S&P 500
index closing price movement (increase or decrease) in the day t+1 compared
with the closing price on day t.
4.1.2. Implementation Details
As mentioned in the previous section, We pre-trained 100 dimentional
word embedding with skip-gram algorithm on the data set shown in
table 1, the size of the trained vocabulary is 153,214.
In addition, ﬁrm
names and an UNK token to represent any words out of the vocabulary are
added to the vocabulary set, having an initial embedding initialized randomly
with Gaussian samples. The word embedding are ﬁne-tuned during model
training. The character embedding has 15 dimensions, and CNN ﬁlters length
Data for individual stock prediction
Training news
Development
Testing news
Table 2: Data for individual stock prediction: The time interval of the news crawled from
Reuters various between diﬀerent companies. Hence, here we only present the number of
articles in the training, development and testing set respectively.
are respectively, each of those are 32 dimensions. The news level Bi-
LSTM and day-level Bi-LSTM both have 300 hidden units, the day level
LSTM window size N has been set to 7. Hyper-parameters da and r in the
self attention layer are set to 600 and 10, respectively. Mentioned in section
3, we use Adadelta for our optimization algorithm, the initial learning rate
has been set to 0.04. Our model are trained for 200 Epoch.
4.2. Base Lines and Proposed Model
In this subsection, we propose a few baselines to compare with our proposed model. For the sake of simplicity, the following notation identiﬁes each
• SVM : Luss and d’Aspremont et al. propose using Bags-of-Words
to represent news documents, and constructed the prediction model
using Support Vector Machines (SVMs).
• Bag-At-LSTM : At-LSTM without sentence encoder. We take the
average of the word embedding inputs instead of using Bi-LSTM to
encode the news title.
• WEB-At-LSTM : Same as our proposed model but without the character level composition.
• Ab-At-LSTM : Instead of the news title, we use the news abstract as
input for our model, the model structure remains the same.
• Doc-At-LSTM : We further leverage a Hierarchical Attention Networks proposed by Yang et al. to represent the entire news Document, this model adds a sentence level attention layer into our proposed
model to diﬀerentiate more and less important content when constructing the document representation.
• Tech-At-LSTM : We concatenate seven Technical indicator leveraged
from Zhai et al. shown in ﬁgure 5 with the vector V after the day
level LSTM layer and fed together into the prediction layer.
Figure 5: Self attention mechanism
• CNN-LSTM : We use CNN instead of the news level self-attention
layer. Note that the word and day level self-attention layer still remains the same. We want to see how well the self-attention layer works
compared to CNN which is good at capturing local and semantic information from texts.
• E-NN : Ding et al. reported a system that uses structure event
tuples input and standard neural network prediction model.
• EB-CNN : Ding et al. proposed a model using event embedding
input and Convolutional Neural Network as prediction model.
• KGEB-CNN : Ding et al. further incorporated an outside knowledge graph into the learning process for event embedding. The model
structure is the same as EB-CNN.
4.3. Result and Discussion
Shown in table 3, the results on the comparison between the models
SVM and the rest of the models indicates that deep neural network model
achieves better performance than the SVM model. Comparison between Bag-
At-LSTM and At-LSTM demonstrates that sentence encoding with LSTM
have slightly better result than Bag-of-words model. Furthermore, WEB-
At-LSTM and At-LSTM indicates that the character level composition helps
improved the models accuracy. The technical Indicator leveraged from Zhai
et al. doesn’t show any performance improvement to our model.
contrast, it results in a decline on the accuracy and it might be caused by
adding noise to the dense representation vector V before the output and
prediction layer. The comparison between CNN-LSTM and At-LSTM shows
that the news level self-attention layer can help capture more relevant news
titles and their temporal features. As Ding et al. concluded in , the news
titles might contain more useful information where as the abstract or article
might cause some negative eﬀect to the model. The Ab-At-LSTM and Doc-
At-LSTM conﬁrmed this viewpoint since their accuracy are lower than the
proposed model that only used the information from the title.
Our proposed model has a 65.53% max accuracy and a average accuracy
of 63.06% which is lower than the KGEB-CNN proposed by Ding et al., this is
likely due to Knowledge graph event embedding (KGEB) is a more powerful
method for model the content in news titles than the sequence embedding
shown in this work.
S&P 500 index prediction Experimental Results
Max Accuracy
Bag-At-LSTM
WEB-At-LSTM
Ab-At-LSTM
Doc-At-LSTM
Tech-At-LSTM
Table 3: Experimental Results: We don’t know the max accuracy in , so here
we assume the accuracy presented in those papers are the average accuracy and use – to
represent the max accuracy.
We use the At-LSTM model for individual stock price prediction after
conﬁrmed that it out performs other approaches. The results are shown in
table 4, each stock shown in the table has more than 66% accuracy and the
company WALMART has an average accuracy of 70.36% and max accuracy
of 72.06%.
Apparently, predicting individual stock price leads to higher
accuracy than predicting S&P 500 index, and this is mainly due to the news
article we used for input. In terms of the individual stock prediction, the
news article we used are more relative to it’s corresponding company. In
contrast, we used the full corpus as input for S&P 500 index prediction and
certainly this adds noise to our model and hence eﬀects the accuracy.
5. Conclusion
This paper has been motivated by the successes of Deep learning methods
in Natural Language Processing task. we proposed a Attention-based LSTM
model(At-LSTM) to predict the directional movements of Standard & Poor’s
500 index and individual companies stock price using ﬁnancial news titles.
Experimental results suggests that our model is promising and competitive
with the state-of-the-art model which incorporate knowledge graph into the
learning process of event embeddings .
Individual stock prediction Experimental Results
Max Accuracy
Table 4: Experimental Results for individual stock prediction: We only list the major
companies that are in the S&P 500.
There are some directions in our future work. While previous work and
our result has found that including the body text of the news performs worse
than just the headline, there may be useful information to extract from the
body text, other directions include looking at predicting price movements at a
range of time horizons, in order to gauge empirically how quickly information
are absorbed in the market, and relate this to the ﬁnance literature on the
topic. The ﬁnancial time series are known by its volatility, in many cases
small changes in the series that can be interpreted as noise. Moreover, the
elimination of small variations makes the model focus only on news with
signiﬁcant variation on prices which might lead to accuracy increase.