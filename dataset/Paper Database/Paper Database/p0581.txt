BREGMAN MONOTONE OPTIMIZATION ALGORITHMS∗
HEINZ H. BAUSCHKE†, JONATHAN M. BORWEIN‡, AND PATRICK L. COMBETTES§
SIAM J. CONTROL OPTIM.
⃝2003 Society for Industrial and Applied Mathematics
Vol. 42, No. 2, pp. 596–636
A broad class of optimization algorithms based on Bregman distances in Banach
spaces is uniﬁed around the notion of Bregman monotonicity.
A systematic investigation of this
notion leads to a simpliﬁed analysis of numerous algorithms and to the development of a new class
of parallel block-iterative surrogate Bregman projection schemes. Another key contribution is the
introduction of a class of operators that is shown to be intrinsically tied to the notion of Bregman
monotonicity and to include the operators commonly found in Bregman optimization methods. Special emphasis is placed on the viability of the algorithms and the importance of Legendre functions
in this regard. Various applications are discussed.
Key words.
Banach space, block-iterative method, Bregman distance, Bregman monotone,
Bregman projection, B-class operator, convex feasibility problem, essentially smooth function, essentially strict convex function, Fej´er monotone, Legendre function, monotone operator, proximal
mapping, proximal point algorithm, resolvent, subgradient projection
AMS subject classiﬁcations. 90C25, 90C48, 47H05
PII. S0363012902407120
1. Introduction. A sequence (xn)n∈N in a Banach space X is Fej´er monotone
with respect to a set S ⊂X if
(∀x ∈S)(∀n ∈N)
∥xn+1 −x∥≤∥xn −x∥.
In Hilbert spaces, this notion has proven to be remarkably useful and successful in
attempts to unify and harmonize the convergence proofs of a large number of optimization algorithms; see, e.g., . A classical example is the
method of cyclic projections for ﬁnding a point in the intersection S ̸= Ø of a ﬁnite
family of closed convex sets (Si)1≤i≤m. In 1965, Bregman [14, Thm. 1] showed that
for every initial point x0 ∈X the sequence (xn)n∈N generated by the cyclic projections
xn+1 = Pn (mod m)+1xn,
where Pi denotes the metric projector onto Si and where the mod m function takes
values in {0, . . . , m−1}, is Fej´er monotone with respect to S and converges weakly to
a point in that set. Two years later , the same author investigated the convergence
of this method in a general topological vector space X. To this end, he introduced
a distance-like function D: E × E →R, where E is a convex subset of X such that
i=1 Si ̸= Ø. The conditions deﬁning D require, in particular, that for
∗Received by the editors May 6, 2002; accepted for publication (in revised form) January 8, 2003;
published electronically May 29, 2003.
 
†Department of Mathematics and Statistics, University of Guelph, Guelph, Ontario N1G 2W1,
Canada ( ). This author’s research was supported by the Natural Sciences and
Engineering Research Council of Canada.
‡Centre for Experimental & Constructive Mathematics, Simon Fraser University, Burnaby, British
Columbia V5A 1S6, Canada ( ). This author’s research was supported by the
Natural Sciences and Engineering Research Council of Canada and the Canada Research Chair
Programme.
§Laboratoire Jacques-Louis Lions, Universit´e Pierre et Marie Curie – Paris 6, 75005 Paris, France
( ).
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
every i ∈{1, . . . , m} and every y ∈E, there exists a point Piy ∈E ∩Si such that
D(Piy, y) = min D(E∩Si, y). In this broader context, Bregman showed that for every
initial point x0 ∈E the cyclic projections algorithm (1.2) produces a sequence that
satisﬁes the monotonicity property
(∀x ∈S)(∀n ∈N)
D(x, xn+1) ≤D(x, xn)
and whose cluster points are in S [15, eq. (1.2) and Thm. 1]. If X is a Hilbert space,
an example of a D-function satisfying the required conditions relative to the weak
topology is D: X 2 →R: (x, y) →∥x −y∥2/2. In this case, we recover the previous
convergence result [15, Example 1] and observe that (1.3) reduces to (1.1). If X is
the Euclidean space RN, another example of a suitable D-function is
D: E × E →R: (x, y) →f(x) −f(y) −⟨x −y, ∇f(y)⟩,
where f : E ⊂RN →R is a convex function which is diﬀerentiable on E and satisﬁes a
set of auxiliary properties [15, Example 2]. Due to its importance in applications, this
particular type of D-function was further studied in and has since been known as
a Bregman distance (see for an historical account). In RN, various investigations
have focused on the use of Bregman distances in projection, proximal point, and ﬁxed
point algorithms; see . (See also , where extensions of
(1.4) to nondiﬀerentiable functions were studied.) Extensions to Hilbert 
and Banach spaces have also been considered
more recently. In the present paper, we adopt the following deﬁnition for Bregman
distances.
Definition 1.1. Let X be a real Banach space and let f : X →]−∞, +∞] be a
lower semicontinuous convex function which is Gˆateaux-diﬀerentiable on int dom f ̸=
Ø. The Bregman distance (for brevity D-distance) associated with f is the function
D: X × X →[0, +∞],
f(x) −f(y) −⟨x −y, ∇f(y)⟩
if y ∈int dom f,
otherwise.
In addition, the Bregman distance to a set C ⊂X is the function
DC : X →[0, +∞],
y →inf D(C, y).
In Hilbert spaces, one recovers D: (x, y) →∥x −y∥2/2 by setting f = ∥· ∥2/2.
This observation suggests that the following natural variant of the notion of Fej´er
monotonicity suits the environment described in Deﬁnition 1.1.
Definition 1.2. A sequence (xn)n∈N in X is Bregman monotone (for brevity
D-monotone) with respect to a set S ⊂X if the following conditions hold:
(i) S ∩dom f ̸= Ø.
(ii) (xn)n∈N lies in int dom f.
(iii) (∀x ∈S ∩dom f)(∀n ∈N) D(x, xn+1) ≤D(x, xn).
Let us note that item (ii) is stated only for the sake of clarity and that it could be
replaced by x0 ∈int dom f since, in view of (1.5), (iii) then forces the whole sequence
(xn)n∈N to lie in int dom f.
The importance of the notion of Bregman monotonicity is implicit in . In the
Euclidean space setting of (see also [33, page 55]), Bregman monotone sequences
were called “Df Fej´er monotone” by analogy with (1.1).
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
The goal of this paper is to provide a broad framework for the design and the analysis of algorithms based on Bregman distances around the notion of D-monotonicity.
This framework not only will lead to a uniﬁed convergence analysis for existing algorithms, but also will serve as a basis for the development of a new class of parallel,
block-iterative, surrogate Bregman projection methods for solving convex feasibility problems involving variational inequalities, convex inequalities, equilibrium constraints, and ﬁxed point constraints. The tools developed in this paper also provide
the main building blocks for the algorithms proposed in to ﬁnd best Bregman
approximations from intersections of closed convex sets in reﬂexive Banach spaces.
Guide to the paper. We proceed towards our goal of constructing a broad
framework for Bregman distance-based algorithms in several steps.
We collect assumptions, notation, and basic results in section 2. The standing
assumptions on the underlying space X and the function f that generates the Bregman
distance are stated in section 2.1. In sections 2.2–2.6, we introduce basic notation and
terminology, including D-viable operators and Legendre functions. Useful identities
for the Bregman distance are provided in section 2.7.
A general and powerful class of operators based on Bregman distances is introduced and analyzed in section 3. This so-called B-class includes types of operators
fundamental in Bregman optimization such as D-ﬁrm operators, D-resolvents, D-prox
operators, and (subgradient) D-projections, which correspond to their classical counterparts when X is a Hilbert space and f = ∥· ∥2/2. For example, it is shown that if
X is reﬂexive and f is Legendre, then D-prox operators belong to B (Corollary 3.25).
This result underscores the importance of Legendreness. Moreover, B-class operators
are stable under a certain type of parallel combination, which will be crucial in the
formulation of a new block-iterative algorithmic framework in section 5.
Section 4 is devoted to D-monotonicity.
This is a central notion in the analysis of Bregman optimization methods because it describes the behavior of a wide
class of algorithms based on Bregman distances. Assumptions are given under which
simple characterizations can be established for the weak and strong convergence of
D-monotone sequences. In conjunction with the results of section 3, D-monotonicity
provides a global framework for the development and analysis of algorithms. Indeed,
we show that D-monotone sequences can be generated systematically via the iterative
x0 ∈int dom f and (∀n ∈N) xn+1 ∈Tnxn, where Tn ∈B.
A detailed convergence analysis of this unifying model is carried out which, in turn,
covers and extends known convergence results.
Finally, in section 5, we are in a position to construct a new block-iterative algorithmic framework. Results obtained in sections 3 and 4 are combined to construct
and investigate new classes of parallel, block-iterative methods for solving convex feasibility problems. The main result, Theorem 5.7, provides conditions suﬃcient for the
weak and strong convergence of sequences generated by the new algorithm. Section 5.4
presents several scenarios in which these suﬃcient conditions are satisﬁed, including
the frequently encountered situation when f is a separable Legendre function on RN
such that dom f ∗is open (Example 5.14). The concluding sections, sections 5.5 and
5.6, discuss how the main result can be applied to speciﬁc optimization problems such
as solving convex inequalities, ﬁnding common zeros of maximal monotone operators,
ﬁnding common minimizers of convex function, and ﬁnding common ﬁxed points of
D-ﬁrm operators.
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
2. Notation, assumptions, and basic facts.
2.1. Standing assumptions. We assume throughout the paper that X is a real
Banach space and that f : X →]−∞, +∞] is a lower semicontinuous convex function
which is Gˆateaux-diﬀerentiable on int dom f ̸= Ø.
2.2. Basic notation. Throughout, N is the set of nonnegative integers. The
norm of X and that of its topological dual X ∗is denoted by ∥·∥, the associated metric
distance by d, and the canonical bilinear form on X × X ∗by ⟨·, ·⟩. (If X is a Hilbert
space, ⟨·, ·⟩denotes also its scalar (or inner) product.) The metric distance function
to a set C ⊂X is dC : X →[0, +∞] : y →infx∈C ∥x −y∥where, by convention,
inf Ø = +∞. For every y ∈int dom f, we set fy = f −∇f(y). The symbols ⇀,
∗⇀, and →denote, respectively, weak, weak∗, and strong convergence. S(xn)n∈N and
W(xn)n∈N are, respectively, the sets of strong and weak cluster points of a sequence
(xn)n∈N in X. bdry C denotes the boundary of a set C ⊂X, int C its interior, and
C its closure. The closed ball of center x and radius ρ is denoted by B(x; ρ). The
normalized duality mapping J of X is deﬁned by
x∗∈X ∗| ∥x∥2 = ⟨x, x∗⟩= ∥x∗∥2
RN is the standard N-dimensional Euclidean space.
2.3. Set-valued operators. Let Y be a Banach space and 2Y the family of
all subsets of Y. A set-valued operator from X to Y is an operator A: X →2Y.
It is characterized by its graph gr A = {(x, u) ∈X × Y | u ∈Ax}; its domain is
dom A = {x ∈X | Ax ̸= Ø} (with closure dom A); its range is ran A = 
x∈X Ax (with
closure ran A); and, if Y = X, its ﬁxed point set is Fix A = {x ∈X | x ∈Ax} (with
closure Fix A). The graph of the inverse A−1 of A is {(u, x) ∈Y × X | (x, u) ∈gr A}.
If B : X →2Y and α ∈R, then gr(αA + B) = {(x, αu + v) ∈X × Y | (x, u) ∈
gr A, (x, v) ∈gr B}. As is customary, if x ∈dom A and A is single-valued on dom A,
we shall denote the unique element in Ax by Ax. Finally, A is locally bounded at
x ∈X if there exists ρ ∈]0, +∞[ such that A
is bounded. (We adopt the
same deﬁnition as in [79, section 17]; it diﬀers slightly from Phelps’ deﬁnition [71,
Chap. 2] which requires x ∈dom A.)
2.4. Orbits and suborbits of algorithms. In section 4 and subsequent sections, we shall discuss various algorithms. Sequences generated by algorithms are
called orbits, and their subsequences are referred to as suborbits.
2.5. Functions. The domain of a function g: X →]−∞, +∞] is dom g = {x ∈
X | g(x) < +∞} (with closure dom g), and g is proper if dom g ̸= Ø. Moreover, g is
subdiﬀerentiable at x ∈dom g if its subdiﬀerential at this point,
x∗∈X ∗| (∀y ∈X) ⟨y −x, x∗⟩+ g(x) ≤g(y)
is not empty; a subgradient of g at x is an element of ∂g(x). The domain of continuity
x ∈X | |g(x)| < +∞and g is continuous at x
and its lower level set at height η ∈R is lev≤η g = {x ∈X | g(x) ≤η}. Recall that
the value of g∗, the conjugate of g, at point x∗∈X ∗is deﬁned by
g∗(x∗) = sup
⟨x, x∗⟩−g(x);
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
g is coﬁnite if dom g∗= X ∗. Furthermore, g is coercive if lim∥x∥→+∞g(x) = +∞,
supercoercive if lim∥x∥→+∞g(x)/∥x∥= +∞, (weak) lower semicontinuous if its lower
level sets
η∈R are (weakly) closed, and (weak) inf-compact if they are (weakly)
compact. If X is reﬂexive, the notions of weak inf-compactness and coercivity coincide
for weak lower semicontinuous functions. The set of minimizing sequences of g is
denoted by
(xn)n∈N in dom g | g(xn) →inf g(X)
and the set of global minimizers of g by Argmin g. (If it is a singleton, its unique
element is denoted by argmin g.) The inf-convolution of two functions g1, g2 : X →
]−∞, +∞] is g1 □g2 : X →[−∞, +∞] : x →infy∈X g1(y) + g2(x −y).
The indicator function of a set C ⊂X is the function ιC : X →{0, +∞} that
takes value 0 on C and +∞on its complement, and its normal cone is
NC = ∂ιC : X →2X ∗: x →
x∗∈X ∗| (∀y ∈C) ⟨y −x, x∗⟩≤0
otherwise.
2.6. D-viability and Legendre functions. Operators based on Bregman distances are not deﬁned outside of int dom f. Thus, using the terminology of , for an
algorithm such as (1.7) to be viable in the sense that its iterates remain in int dom f,
the operators involved must satisfy the following viability condition.
Definition 2.1.
An operator T : X →2X is D-viable if ran T ⊂dom T =
int dom f.
It was shown in that a suﬃcient condition for Bregman projection operators
onto closed convex sets in Euclidean spaces to be D-viable is that f be a Legendre
function. (In this context, “D-viability” was called “zone consistency” after .)
The classical ﬁnite-dimensional deﬁnition of a Legendre function, as introduced by
Rockafellar in [77, section 26], is of limited use in general Banach spaces since the
resulting class of functions loses some of its remarkable ﬁnite-dimensional properties.
In the context of Banach spaces, we introduced in the following notion a Legendre
function. It not only generalizes Rockafellar’s classical deﬁnition but also preserves its
salient properties in reﬂexive spaces. (For results on Legendre functions in nonreﬂexive
spaces, see .)
Definition 2.2 ([8, Def. 5.2]). The function f is
(i) essentially smooth if ∂f is both locally bounded and single-valued on its domain;
(ii) essentially strictly convex if (∂f)−1 is locally bounded on its domain and f is
strictly convex on every convex subset of dom ∂f;
(iii) Legendre if it is both essentially smooth and essentially strictly convex.
Such functions will be of prime importance in our analysis as they will be shown to
provide a simple and convenient suﬃcient condition for the D-viability of the operators
commonly encountered in Bregman optimization methods in Banach spaces.
2.7. Basic properties of Bregman distances. The following properties follow
directly from (1.5).
Proposition 2.3. Let {x, y} ⊂X and {u, v} ⊂int dom f. Then
(i) D(u, v) + D(v, u) = ⟨u −v, ∇f(u) −∇f(v)⟩;
(ii) D(x, u) = D(x, v) + D(v, u) + ⟨x −v, ∇f(v) −∇f(u)⟩;
(iii) D(x, v) + D(y, u) = D(x, u) + D(y, v) + ⟨x −y, ∇f(u) −∇f(v)⟩.
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
Fig. 1. If T ∈B, x ∈int dom f, and u ∈Tx, the half-space H(x, u) contains Fix T.
3. Operators associated with Bregman distances. In Hilbert spaces, various nonlinear operators are involved in the design of algorithms, including projection
operators, proximal operators, resolvents, subgradient projection operators, ﬁrmly
nonexpansive operators, and combinations of these. Such operators arise in convex
feasibility problems, in equilibrium theory, in systems of convex inequalities, in variational inequalities, as well as in numerous ﬁxed point problems . Intrinsically tied to the very deﬁnition of these operators is the use of the
standard notion of metric distance to measure the proximity between two points. In
the context of Bregman distances, it is therefore natural to attempt to deﬁne variants
of these operators. This eﬀort has been undertaken by several authors at various levels
of generality. In this section, we systematically study nonlinear operators associated
with Bregman distances in order to bring together and extend a collection of results
disseminated in the literature. Speciﬁcally, we investigate when D-ﬁrm operators,
D-resolvents, D-prox operators, D-projectors, and subgradient D-projectors belong
to class B. (For relationships among these operators in the classical case, i.e., when X
is a Hilbert space and f = ∥·∥2/2, see [9, Prop. 2.3].) Moreover, the class B is shown
to be closed under a certain type of relaxed parallel combination. The discussion is
not limited to convex problems as nonconvex extensions of standard algorithms have
been found to be quite useful in a number of applications; see .
3.1. The class B. Ultimately, our goal is to deﬁne a class of operators for
which (1.7) systematically generates D-monotone sequences. In this perspective, the
operators employed in (1.7) must be D-viable (see Deﬁnition 2.1) and induce a certain
monotonicity property (see Deﬁnition 1.2). These requirements lead to the following
class of operators (see Figure 1).
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
Definition 3.1. For every x and u in int dom f, set
y ∈X | ⟨y −u, ∇f(x) −∇f(u)⟩≤0
T : X →2X | ran T ⊂dom T = int dom f, (∀(x, u) ∈gr T) Fix T ⊂H(x, u)
If X is Hilbertian, f = ∥· ∥2/2, and only single-valued operators are considered,
then B reverts to the class T of operators introduced in and further investigated
in this context in . In these studies, T was shown to play a central role in the
analysis of Fej´er monotone algorithms. Because of Proposition 3.3(i) below, there is
some overlap between the “paracontractions” introduced in (see also )
and operators in B.
Furthermore, if f satisﬁes certain conditions and T ∈B is
single-valued with Fix T ̸= Ø, then T is “totally nonexpansive” in the sense of .
Lemma 3.2. Let C1 and C2 be two convex subsets of X such that C1 is closed
and C1 ∩int C2 ̸= Ø. Then C1 ∩int C2 = C1 ∩C2.
Proof. Since C2 is convex with nonempty interior, C1 ∩int C2 ⊂C1 ∩int C2 =
To show the reverse inclusion, ﬁx x0 ∈C1 ∩int C2 and x1 ∈C1 ∩C2.
By convexity, [x0, x1] ⊂C1 and [x0, x1[ ⊂int C2.
Therefore, (∀α ∈[0, 1[) xα =
(1 −α)x0 + αx1 ∈C1 ∩int C2. Consequently x1 = limα↑1−xα ∈C1 ∩int C2, and we
conclude C1 ∩C2 ⊂C1 ∩int C2.
Proposition 3.3. Let T be an operator in B and let F = 
(x,u)∈gr T H(x, u).
(i) (∀(x, u) ∈gr T)(∀y ∈Fix T) D(y, u) ≤D(y, x) −D(u, x);
(ii) (∀(x, u) ∈gr T) D(u, x) ≤DFix T (x);
(iii) (∀(x, u) ∈gr T)(∀y ∈Fix T) D(x, u) + D(u, x) ≤⟨y −x, ∇f(u) −∇f(x)⟩.
Now suppose that f|int dom f is strictly convex; then
(iv) Fix T = F ∩int dom f;
(v) Fix T is convex;
(vi) T is single-valued on Fix T.
If, in addition, Fix T ̸= Ø, then
(vii) Fix T = F ∩dom f;
(viii) (∀(x, u) ∈gr T)(∀y ∈Fix T) D(y, u) ≤D(y, x) −D(u, x).
Proof. (i) Take (x, u) ∈gr T and y ∈Fix T. Then Proposition 2.3(ii) and the
inclusion y ∈H(x, u) yield D(y, u) = D(y, x) −D(u, x) + ⟨y −u, ∇f(x) −∇f(u)⟩≤
D(y, x) −D(u, x).
(ii) By (i), (∀(x, u) ∈gr T)(∀y ∈Fix T) D(u, x) ≤D(y, x).
(iii) Take (x, u) ∈gr T and y ∈Fix T, and suppose yn →y for some sequence (yn)n∈N
in Fix T. Then it follows from Proposition 2.3(i) that
(∀n ∈N) D(x, u) + D(u, x) = ⟨x −u, ∇f(x) −∇f(u)⟩
= ⟨x −yn, ∇f(x) −∇f(u)⟩+ ⟨yn −u, ∇f(x) −∇f(u)⟩
≤⟨x −yn, ∇f(x) −∇f(u)⟩.
Since ⟨x −yn, ∇f(x) −∇f(u)⟩→⟨x −y, ∇f(x) −∇f(u)⟩, the proof is complete.
(iv) Take y ∈F ∩int dom f. Then y ∈
u∈T y H(y, u) and, in turn,
(∀u ∈Ty) ⟨y −u, ∇f(y) −∇f(u)⟩≤0.
However, {y} ∪Ty ⊂int dom f and, since f|int dom f is strictly convex, ∇f is strictly
monotone on int dom f. Therefore Ty = {y} and y ∈Fix T. Thus, F ∩int dom f ⊂
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
Since T ∈B, the reverse inclusion is clear.
(iv) ⇒(v) Since the sets
(x,u)∈gr T and int dom f are convex, so is their intersection Fix T.
was proved in the proof of (iv). (iv) ⇒(vii) Observe that F is closed and apply
Lemma 3.2. (viii) Take (x, u) ∈gr T, y0 ∈Fix T, and y ∈Fix T. By (iv) and (vii),
Fix T = F ∩int dom f and Fix T = F ∩dom f.
Since F and dom f are convex,
[y0, y] ⊂F and [y0, y[ ⊂int dom f. Therefore,
(∀α ∈[0, 1[) yα = (1 −α)y0 + αy ∈Fix T.
Invoking the lower semicontinuity and convexity of f, we get
f(y) ≤lim α↑1−f(yα) ≤lim α↑1−f(yα) ≤lim α↑1−(1 −α)f(y0) + αf(y) = f(y).
Hence limα↑1−f(yα) = f(y) and, in turn,
(∀z ∈int dom f)
α↑1−D(yα, z) = D(y, z).
On the other hand, since u ∈Tx and T ∈B, (3.4) and (i) yield
(∀α ∈[0, 1[) D(yα, u) ≤D(yα, x) −D(u, x).
Consequently, D(y, u) ≤D(y, x) −D(u, x).
3.2. D-ﬁrm operators. An operator T : X →X is said to be ﬁrmly nonexpansive if for all x and y in dom T one has 
(∀α ∈]0, +∞[) ∥Tx −Ty∥≤∥α(x −y) + (1 −α)(Tx −Ty)∥.
For the sake of notational simplicity, let us now suppose that X is smooth. Then its
normalized duality map J is single-valued and, upon invoking the equivalence (∀α ∈
]0, +∞[) ∥u∥≤∥u + αv∥⇔0 ≤⟨v, Ju⟩ , we observe that (3.8) is equivalent to
⟨Tx −Ty, J(Tx −Ty)⟩≤⟨x −y, J(Tx −Ty)⟩.
If X is not a Hilbert space, then J is not linear and this type of inequality may be
diﬃcult to manipulate. In Hilbert spaces, J = Id = ∇f for f = ∥· ∥2/2, and (3.9)
can therefore be written
⟨Tx −Ty, ∇f(Tx) −∇f(Ty)⟩≤⟨Tx −Ty, ∇f(x) −∇f(y)⟩.
In the framework of Bregman distances, this inequality suggests the following deﬁnition.
Definition 3.4. An operator T : X →2X with dom T ∪ran T ⊂int dom f is
(∀(x, u) ∈gr T)(∀(y, v) ∈gr T) ⟨u −v, ∇f(u) −∇f(v)⟩≤⟨u −v, ∇f(x) −∇f(y)⟩.
Proposition 3.5. Let T : X →2X be a D-ﬁrm operator. Then
(i) (∀(x, u) ∈gr T) Fix T ⊂H(x, u);
(ii) T ∈B if int dom f = dom T;
(iii) T is single-valued on its domain if f|int dom f is strictly convex;
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
(iv) (∀(x, u) ∈gr T)(∀(y, v) ∈gr T)
D(u, v) + D(v, u) ≤D(u, y) + D(v, x) −
D(u, x) −D(v, y).
Proof. (i) Suppose y ∈Ty. Then (3.11) implies that
(∀(x, u) ∈gr T) ⟨y −u, ∇f(x) −∇f(u)⟩≤0.
(i) ⇒(ii) is clear. (iii) Fix x ∈dom T and {u, v} ⊂Tx. Then (3.11) implies that
⟨u −v, ∇f(u) −∇f(v)⟩≤0.
Since ∇f is strictly monotone on int dom f ⊃{u, v}, we obtain u = v. (iv) follows
from Proposition 2.3(i), (3.11), and Proposition 2.3(iii).
Remark 3.6. For single-valued operators in Hilbert spaces and f strongly convex
(i.e., f −β∥· ∥2/2 is convex for some β ∈]0, +∞[), item (iv) above was used to deﬁne
D-ﬁrmness in .
3.3. D-resolvents. The resolvent of an operator A: X →2X is (Id +A)−1. It
is known that an operator T : X →X is ﬁrmly nonexpansive if and only if it is the
resolvent of an accretive operator A: X →2X .
Now let A: X →2X ∗be a nontrivial operator, i.e., gr A ̸= Ø.
Then, in the
context of Bregman distances, it is reasonable to introduce the following variant of
the notion of a resolvent to obtain an operator from X to X (this deﬁnition appears
to have ﬁrst been proposed in RN in ).
Definition 3.7. The D-resolvent associated with A: X →2X ∗is the operator
RA = (∇f + A)−1 ◦∇f : X →2X .
An a posteriori motivation for (3.14) is that it preserves the usual ﬁxed point
characterization of the zeros of A, namely,
(∀x ∈X)(∀γ ∈]0, +∞[)
x ∈Fix RγA,
as 0 ∈Ax ⇔∇f(x) ∈∇f(x)+γA(x) = (∇f+γA)(x) ⇔x ∈(∇f+γA)−1 
is also consistent with previous attempts to deﬁne resolvents for monotone operators:
• Let X be smooth and set f = ∥·∥2/2. Then ∇f = J and RA = (J +A)−1 ◦J.
This type of resolvent was used in .
• If X is Hilbertian and f : x →∥Πx∥2/2, where Π is the metric projector onto
a closed vector subspace of X, then ∇f = Π and RA = (Π + A)−1 ◦Π. This
generalized resolvent was used in .
Proposition 3.8. RA satisﬁes the following properties:
(i) dom RA ⊂int dom f.
(ii) ran RA ⊂int dom f.
(iii) Fix RA = (int dom f) ∩A−10.
(iv) Suppose A is monotone. Then the following conditions hold:
(a) RA is D-ﬁrm.
(b) RA is single-valued on its domain if f|int dom f is strictly convex.
(c) Suppose ran ∇f ⊂ran(∇f + A).
Then RA ∈B.
If, in addition,
f|int dom f is strictly convex, then Fix RA is convex.
Proof. (i) is clear. (ii) We have
ran RA ⊂ran(∇f + A)−1 = dom(∇f + A) = dom ∇f ∩dom A ⊂dom ∇f
= int dom f.
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
(iii) Fix RA ⊂int dom f by (i) and (∀x ∈int dom f) 0 ∈Ax ⇔x ∈RAx by (3.15).
Hence, A−10 ∩int dom f = Fix RA ∩int dom f = Fix RA. (iv) Suppose that A is
monotone. (a) In view of (i) and (ii), let us show that (3.11) is satisﬁed. Fix (x, u) and
(y, v) in gr RA. Then ∇f(x) −∇f(u) ∈Au and ∇f(y) −∇f(v) ∈Av. Consequently,
since A is monotone, we get ⟨u−v, ∇f(x)−∇f(u)−(∇f(y)−∇f(v))⟩≥0. (b) follows
from (a) and Proposition 3.5(iii). (c) ran ∇f ⊂ran(∇f + A) ⇔ran ∇f ⊂dom(∇f +
A)−1 ⇔dom RA = dom ∇f = int dom f.
In view of (a) and Proposition 3.5(ii),
RA ∈B. Proposition 3.3(v) implies the convexity of Fix RA.
Definition 3.9 (see [86, sections 32.14 and 32.21]). A is
(i) weakly coercive if lim∥x∥→+∞inf ∥Ax∥= +∞;
(ii) strongly coercive if
(∀x ∈dom A)
∥y∥→+∞inf ⟨y −x, Ax⟩
(iii) 3-monotone if
(x, x∗), (y, y∗), (z, z∗)
⟨x −y, x∗⟩+ ⟨y −z, y∗⟩+ ⟨z −x, z∗⟩≥0;
(iv) 3∗-monotone if it is monotone and
(∀(x, x∗) ∈dom A × ran A) sup
⟨x −y, y∗−x∗⟩| (y, y∗) ∈gr A
Lemma 3.10 (see [86, section 32.21], ). Suppose that X is reﬂexive and that
A is monotone and satisﬁes one of the following properties:
(i) A is 3-monotone.
(ii) A is strongly coercive.
(iii) ran A is bounded.
(iv) A = ∂ϕ, where ϕ: X →]−∞, +∞] is a proper function.
Then A is 3∗-monotone.
The following lemma is Reich’s extension to a reﬂexive Banach space setting of
the Br´ezis–Haraux theorem on the range of the sum of two monotone operators.
Lemma 3.11 (see [74, Thm. 2.2]). Suppose that X is reﬂexive and let A1, A2 : X →
2X ∗be two monotone operators such that A1 + A2 is maximal monotone and A1 is
3∗-monotone. In addition, suppose that dom A2 ⊂dom A1 or A2 is 3∗-monotone.
Then int ran(A1 + A2) = int(ran A1 + ran A2) and ran (A1 + A2) = ran A1 + ran A2.
Proposition 3.12. Let γ ∈]0, +∞[. Suppose that X is reﬂexive and that A is
maximal monotone with (int dom f) ∩dom A = dom ∂f ∩dom A ̸= Ø. Then ∇f + γA
is maximal monotone. Moreover, the inclusions
int(ran ∇f + γ ran A) ⊂ran(∇f + γA)
ran ∇f + γ ran A ⊂ran (∇f + γA)
are satisﬁed if one of the following conditions holds:
(i) dom A ⊂int dom f.
(ii) A is 3∗-monotone.
Proof. Since f is proper, lower semicontinuous, and convex, ∂f is maximal monotone [79, Thm. 30.3] and int dom f = cont f ⊂dom ∂f ⊂dom f [48, Chap. I]. Since
(int dom f) ∩dom A = dom ∂f ∩dom A ̸= Ø, we have (int dom ∂f) ∩dom γA =
(int dom f) ∩dom A ̸= Ø, and it follows from Rockafellar’s sum theorem [79, section 23] that ∂f + γA is maximal monotone. However, the above assumption implies
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
that dom(∇f + γA) = dom(∂f + γA) and, in turn, that ∇f + γA = ∂f + γA
since {∇f} = ∂f|int dom f. Thus, ∇f + γA is maximal monotone. The second assertion is an application of Lemma 3.11 with A1 = ∇f and A2 = γA.
dom ∇f = int dom f and, by Lemma 3.10(iv), ∂f is 3∗-monotone and so is, therefore,
∇f since gr ∇f ⊂gr ∂f.
Theorem 3.13. Let γ ∈]0, +∞[. Suppose that X is reﬂexive, that A is maximal
monotone with (int dom f) ∩dom A = dom ∂f ∩dom A ̸= Ø, and that one of the
following conditions holds:
(i) X is smooth and f = ∥· ∥2/2.
(ii) (∇f + γA)−1 is locally bounded at every point in X ∗.
(iii) ∇f + γA is weakly coercive.
(iv) dom A ⊂int dom f or A is 3∗-monotone, and one of the following conditions
(a) ran ∇f + γ ran A = X ∗.
(b) f is Legendre and coﬁnite.
(c) ran(∇f + γA) is closed and 0 ∈ran A.
(d) ran ∇f is open and 0 ∈ran A.
Then RγA ∈B.
In view of Proposition 3.8(iv)(c), it suﬃces to show that ran ∇f ⊂
ran(∇f + γA). (i) Since X is smooth, ∇f = J [34, Corollary I.4.5] and Rockafellar’s surjectivity theorem [79, Thm. 10.7] yields ran(∇f + γA) = X ∗. (ii) Proposition 3.12 asserts that ∇f + γA is maximal monotone.
It therefore follows from
the Br´ezis–Browder surjectivity theorem (see [34, Thm. V.3.8] or [86, Thm. 32.G])
that ran(∇f + γA) = X ∗. (iii) ⇒(ii) follows from [86, Cor. 32.35] since ∇f + γA
is maximal monotone. (iv) By Proposition 3.12, (3.17) holds. (a) By (3.17), X ∗=
int(ran ∇f + γ ran A) ⊂ran(∇f + γA). (b) ⇒(a) By [8, Thm. 5.10], Legendreness
guarantees ran ∇f = int dom f ∗while coﬁniteness gives int dom f ∗= X ∗. Consequently, ran ∇f + γ ran A = X ∗. (c) By (3.17), ran ∇f = ran ∇f + {0} ⊂ran ∇f +
γ ran A ⊂ran (∇f+γA) = ran(∇f+γA). (d) By (3.17), ran ∇f = int(ran ∇f+{0}) ⊂
int(ran ∇f + γ ran A) ⊂ran(∇f + γA).
In connection with the problem of ﬁnding zeros of maximal monotone operators,
the following corollary is particularly useful.
Corollary 3.14.
Let γ ∈]0, +∞[.
Suppose that X is reﬂexive, that A is
maximal monotone with 0 ∈ran A, and that one of the following conditions holds:
(i) ran ∇f is open and dom A ⊂int dom f.
(ii) f is Legendre and dom A ⊂int dom f.
(iii) f is Legendre, A is 3∗-monotone, and dom A ∩int dom f ̸= Ø.
Then RγA ∈B.
Proof. The assertions follow from Theorem 3.13(iv)(d). Indeed, in (i), dom A ⊂
int dom f = cont f ⊂dom ∂f ⇒(int dom f) ∩dom A = dom ∂f ∩dom A = dom A ̸=
Ø. On the other hand, in (ii) and (iii), ran ∇f is open since Legendreness yields
ran ∇f = int dom f ∗[8, Thm. 5.10]. Consequently, if dom A ⊂int dom f, then (ii) is a
consequence of (i). Otherwise, if A is 3∗-monotone and (int dom f)∩dom A ̸= Ø, then
it suﬃces to note that essential smoothness yields dom ∂f = int dom f [8, Thm. 5.6],
whence (int dom f) ∩dom A = dom ∂f ∩dom A ̸= Ø.
Remark 3.15. In RN, Corollary 3.14(i) corresponds to [46, Thm. 4].
3.4. D-prox operators. The classical notion of a proximal operator was introduced by Moreau in Hilbert spaces. The proximal operator associated
with a function ϕ: X →]−∞, +∞] is proxϕ : y →argmin ϕ + ∥· −y∥2/2. Outside of
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
Hilbert spaces, this notion is of less interest since Fermat’s rule for the minimization
of ϕ + ∥· −y∥2/2 becomes a nonseparable inclusion, namely, 0 ∈∂ϕ(x) + J(x −y).
In RN, the idea of deﬁning proximal operators based on D-distance—rather than
quadratic—penalizations was introduced in . In our setting, they will be deﬁned
as follows.
Definition 3.16. Let ϕ: X →]−∞, +∞]. The D-prox operator of index γ ∈
]0, +∞[ associated with ϕ is the operator
γ : X →2X ,
x ∈dom f ∩dom ϕ | ϕ(x) + 1
γ D(x, y) = min
It follows from this deﬁnition that
γ ⊂int dom f
γ ⊂dom f ∩dom ϕ.
Recall (see section 2.5) that a function is weak inf-compact if all its lower level
sets are weakly compact.
Lemma 3.17. Suppose that g1 : X →]−∞, +∞] is weak lower semicontinuous
and bounded from below and that g2 : X →]−∞, +∞] is weak inf-compact. Then
g1 + g2 is weak inf-compact.
Proof. Set β = inf g1(X) and let η ∈R. Since g1 and g2 are weak lower semicontinuous, so is their sum, and therefore lev≤η (g1 + g2) is weakly closed. On the other
hand, lev≤η (g1 +g2) is contained in the weakly compact set lev≤η−β g2. We conclude
that lev≤η (g1 + g2) is weakly compact.
The following result concerns the domain requirement for the D-viability of Dprox operators. Recall (see sections 2.5 and 2.2) that M denotes the set of minimizing
sequences of a function and that W is the set of weak cluster points of a sequence.
Theorem 3.18. Let γ ∈]0, +∞[, let ϕ: X →]−∞, +∞] be such that dom f ∩
dom ϕ ̸= Ø, and assume that one of the following conditions holds:
(i) (∀y ∈int dom f)(∃(xn)n∈N ∈M(fy + γϕ))(∃x ∈W(xn)n∈N) f + γϕ is weak
lower semicontinuous at x.
(ii) (∀y ∈int dom f) fy + γϕ is weak inf-compact.
(iii) ϕ is weak lower semicontinuous and bounded from below, and, for every y ∈
int dom f, fy is weak inf-compact.
(iv) ϕ is weak inf-compact.
Then dom proxϕ
γ = int dom f.
Proof. Fix y ∈int dom f and set g = fy + γϕ. (i) Pick (xn)n∈N ∈M(g) such that
xkn ⇀x and g is weak lower semicontinuous at x. It follows that g(x) ≤lim g(xkn) =
inf g(X) and hence g(x) = inf g(X). Therefore, g achieves its inﬁmum and the result
holds since proxϕ
γ y = Argmin(fy+γϕ) = Argmin(g). (ii) ⇒(i) Take (xn)n∈N ∈M(g).
Then it follows from weak inf-compactness of g that (xn)n∈N lies in a weakly compact
set and therefore that W(xn)n∈N ̸= Ø. On the other hand, as g is weak inf-compact,
it is weak lower semicontinuous and so is f + γϕ = fy + γϕ + ∇f(y) = g + ∇f(y).
(iii) ⇒(ii) follows from Lemma 3.17. (iv) ⇒(ii) It is clear that fy is weak lower
semicontinuous.
On the other hand, it follows from the convexity of f that, for
every x ∈X, ⟨x −y, ∇f(y)⟩+ f(y) ≤f(x) and, therefore, fy(x) ≥fy(y). Hence
inf fy(X) ≥fy(y) > −∞and, by Lemma 3.17, g is weak inf-compact.
The following fundamental result is due to Moreau and Rockafellar .
Lemma 3.19. Let y∗∈X ∗. Then f−y∗is coercive if and only if y∗∈int dom f ∗.
Lemma 3.20. Let g1, g2 : X →]−∞, +∞] be two convex functions. Then
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
(i) if g1 and g2 are lower semicontinuous and 0 ∈int
dom g1 −dom g2
(g1 + g2)∗= g∗
(ii) if cont g1 ∩dom g2 ̸= Ø, then ∂(g1 + g2) = ∂g1 + ∂g2 [79, Thm. 28.2].
Proposition 3.21. Let ϕ: X →]−∞, +∞] be a lower semicontinuous convex
function such that dom f ∩dom ϕ ̸= Ø and let γ ∈]0, +∞[. Suppose that X is reﬂexive
and that one of the following conditions holds:
(i) (∀y ∈int dom f)(∃(xn)n∈N ∈M(fy + γϕ)) supn∈N ∥xn∥< +∞.
(ii) (∀y ∈int dom f) fy + γϕ is coercive.
(iii) ran ∇f ⊂int dom (f + γϕ)∗.
(iv) f + γϕ is coﬁnite.
(v) 0 ∈int(dom f −dom ϕ) and dom f ∗+ γ dom ϕ∗= X ∗.
(vi) ϕ is bounded from below and f is essentially strictly convex.
(vii) f + γϕ is supercoercive.
(viii) ϕ is bounded from below and f is supercoercive.
(ix) ϕ is coercive.
Then dom proxϕ
γ = int dom f.
Proof. Let y be an arbitrary point in int dom f. Note that, since ϕ is weak lower
semicontinuous, so are f + γϕ and fy + γϕ and that, since X is reﬂexive, coercive
weak lower semicontinuous functions are weak inf-compact. (i) is a consequence of
Theorem 3.18(i). Indeed, take a bounded sequence (xn)n∈N ∈M(fy + γϕ). Then
it follows from the reﬂexivity of X that W(xn)n∈N ̸= Ø. (ii) follows at once from
Theorem 3.18(ii). (iii) ⇔(ii) ∇f(y) ∈int dom (f +γϕ)∗⇔f +γϕ−∇f(y) is coercive
by Lemma 3.19. (iv) ⇒(iii) is clear. (v) ⇒(iv) Lemma 3.20(i) yields
dom f ∗+ γ dom ϕ∗= dom f ∗+ dom γϕ∗(·/γ) = dom f ∗+ dom(γϕ)∗
f ∗□(γϕ)∗
0 ∈int(dom f −dom ϕ)
f ∗□(γϕ)∗= (f + γϕ)∗.
Hence dom f ∗+ γ dom ϕ∗= X ∗⇒dom(f + γϕ)∗= X ∗. (vi) is a consequence of
Theorem 3.18(iii): indeed, by [8, Thm. 5.9(ii)], ∇f(y) ∈int dom f ∗and fy is therefore
coercive by Lemma 3.19. (vii) ⇒(iv) See [8, Thm. 3.4]. (viii) ⇒(vii) is clear. (ix) is
a consequence of Theorem 3.18(iv).
The next result gathers some facts concerning D-prox operators for convex functions.
Proposition 3.22. Let ϕ: X →]−∞, +∞] be convex and let γ ∈]0, +∞[. Then
the following hold:
(ii) If, in addition, ran proxϕ
γ ⊂int dom f, then
(b) Fix proxϕ
γ = (int dom f) ∩Argmin ϕ;
γ is D-ﬁrm;
γ is single-valued on its domain if f|int dom f is strictly convex.
Proof. Fix y ∈int dom f. (i) By (3.18), ran proxϕ
γ ⊂dom f ∩dom ϕ. If dom f ∩
dom ϕ = Ø, both sides of the desired identity reduce to the trivial operator z →
Ø. If not, take x ∈dom f ∩dom ϕ. Since cont ∇f(y) = X, Lemma 3.20(ii) yields
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
∂(fy + γϕ)(x) = ∂(f + γϕ)(x) −∇f(y). Consequently,
γ y ⇔0 ∈∂(fy + γϕ)(x)
⇔∇f(y) ∈∂(f + γϕ)(x)
(ii) Suppose ran proxϕ
γ ⊂int dom f. (a) On the one hand, it follows from (3.18) that
γ ⊂(int dom f) ∩dom ϕ. On the other hand, ran Rγ∂ϕ ⊂dom(∇f + γ∂ϕ) ⊂
(int dom f) ∩dom ϕ. Therefore, if (int dom f) ∩dom ϕ = Ø, both sides of the desired
identity reduce to the trivial operator z →Ø. If not, take x ∈(int dom f) ∩dom ϕ =
cont f ∩dom ϕ. Lemma 3.20(ii) now yields ∂(f +γϕ)(x) = ∇f(x)+γ∂ϕ(x) and (3.21)
γ y ⇔∇f(y) ∈∇f(x) + γ∂ϕ(x) ⇔x ∈Rγ∂ϕy.
(a) ⇒(b) follows from Proposition 3.8(iii). (a) ⇒(c) Since ∂ϕ is monotone, Rγ∂ϕ is
D-ﬁrm by Proposition 3.8(iv)(a). (a) ⇒(d) follows from Proposition 3.8(iv)(b).
We now turn our attention to the range requirement for the D-viability of D-prox
operators.
Proposition 3.23. Let ϕ: X →]−∞, +∞] be convex such that dom f ∩dom ϕ ̸=
Ø, and let γ ∈]0, +∞[. Assume that one of the following conditions holds:
(i) dom ∂(f + γϕ) ⊂int dom f.
(ii) dom f ∩dom ϕ ⊂int dom f.
(iii) dom f is open.
(iv) dom ϕ ⊂int dom f.
(v) (int dom f) ∩dom ϕ ̸= Ø and one of the following conditions holds:
(a) dom ∂f ∩dom ∂ϕ ⊂int dom f.
(b) f is essentially smooth.
(c) dom ∂ϕ ⊂int dom f.
Then ran proxϕ
γ ⊂int dom f.
Proof. (i) By Proposition 3.22(i),
−1 = dom ∂(f + γϕ) ⊂int dom f.
(ii) ⇒(i) dom ∂(f + γϕ) ⊂dom(f + γϕ) = dom f ∩dom ϕ ⊂int dom f. (iii) ⇒(ii)
and (iv) ⇒(ii) are clear. (v) ⇒(i) It results from Lemma 3.20(ii) that ∂(f + γϕ) =
∂f +γ∂ϕ. Whence, (a) ⇒(i). (b) ⇒(a) Essential smoothness ⇒dom ∂f = int dom f
[8, Thm. 5.6(iii)]. (c) ⇒(a) is clear.
Upon combining Propositions 3.23, 3.22(ii)(c), 3.21, and 3.5(ii), we obtain the
following theorem.
Theorem 3.24. Let ϕ: X →]−∞, +∞] be a lower semicontinuous convex function such that dom f ∩dom ϕ ̸= Ø, and let γ ∈]0, +∞[. Suppose that X is reﬂexive
and that one of conditions (i)–(ix) in Proposition 3.21 holds together with one of
conditions (i)–(v) in Proposition 3.23. Then proxϕ
The following special case underscores the importance of the notion of Legendreness.
Corollary 3.25.
Let ϕ: X →]−∞, +∞] be a lower semicontinuous convex
function such that (int dom f) ∩dom ϕ ̸= Ø, and let γ ∈]0, +∞[. Suppose that X is
reﬂexive, that f is Legendre, and that ϕ is bounded below. Then
γ is single-valued on its domain and proxϕ
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
(ii) for every x and y in int dom f,
γ y ⇔(∀z ∈dom ϕ) ⟨z −x, ∇f(y) −∇f(x)⟩/γ + ϕ(x) ≤ϕ(z).
Proof. (i) Combine Propositions 3.23(v)(b), 3.22(ii)(c) and (d), 3.21(vi), and 3.5(ii).
(ii) By (3.22), x = proxϕ
γ y ⇔∇f(y) −∇f(x) ∈γ∂ϕ(x).
Remark 3.26. A special case of Theorem 3.18(iii) in RN can be found in [32,
Prop. 3.1].
In RN, assertions (iv) and (v)(b) of Proposition 3.23 appear in [58,
Lemma 3.3]. In the case when X is Hilbertian and f = ∥· ∥2/2, the characterization
supplied by Corollary 3.25(ii) is well known; see, e.g., [48, section II.2].
3.5. D-projections. The following concept goes back to Bregman’s original
paper .
Definition 3.27. The D-projector onto a set C ⊂X is the operator
PC : X →2X ,
x ∈C ∩dom f | D(x, y) = DC(y) < +∞
It is clear that, for any γ ∈]0, +∞[, PC = proxιC
γ . Hence, the results of section 3.4
will automatically yield results on D-projections when specialized to ϕ = ιC. Before
we proceed in this direction, let us introduce a couple of deﬁnitions, which are natural
adaptations of standard ones in metric approximation theory .
Definition 3.28.
A set C ⊂X is D-proximinal if dom PC = int dom f and
D-semi-Chebyshev if PC is single-valued on its domain. C is D-Chebyshev if it is
D-proximinal and D-semi-Chebyshev.
Definition 3.29. A set C ⊂X is D-approximately weakly compact if
(∀y ∈int dom f)(∀(xn)n∈N in C ∩dom f) D(xn, y) →DC(y) ⇒W(xn)n∈N ∩C ̸= Ø.
Theorem 3.30. Let C be a subset of X such that C ∩dom f ̸= Ø and assume
that one of the following conditions holds:
(i) C is D-approximately weakly compact.
(ii) (∀y ∈int dom f)(∃η ∈R) C ∩lev≤η fy is nonempty and weakly compact.
(iii) C is weakly closed and, for every y ∈int dom f, fy is weak inf-compact.
(iv) C is weakly compact.
Then C is D-proximinal.
Proof. (i) Since f is weak lower semicontinuous, f + ιC is weak lower semicontinuous at every point in C.
Now ﬁx y ∈int dom f and (xn)n∈N ∈M(fy + ιC).
Then D(xn, y) →DC(y) and Deﬁnition 3.29 yields W(xn)n∈N ∩C ̸= Ø. Now take
x ∈W(xn)n∈N ∩C. Since f + ιC is weak lower semicontinuous at x, the claims follow
from Theorem 3.18(i) with ϕ = ιC. (ii) Fix y ∈int dom f. As minimizing D(·, y) over
C is equivalent to minimizing the weak lower semicontinuous function fy over the
weakly compact set C ∩lev≤η fy, the result follows. Assertions (iii) and (iv) follow,
respectively, from assertions (iii) and (iv) in Theorem 3.18 with ϕ = ιC.
Upon setting ϕ = ιC, Proposition 3.21 becomes the following.
Proposition 3.31.
Let C be a closed and convex subset of X such that C ∩
dom f ̸= Ø.
Suppose that X is reﬂexive and that one of the following conditions
(i) (∀y ∈int dom f)(∀(xn)n∈N ∈M(fy + ιC)) supn∈N ∥xn∥< +∞.
(ii) (∀y ∈int dom f) fy + ιC is coercive.
(iii) ran ∇f ⊂int dom (f + ιC)∗.
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
(iv) f + ιC is coﬁnite.
(v) 0 ∈int(dom f −C) and dom f ∗+ dom ι∗
(vi) f is essentially strictly convex.
(vii) f + ιC is supercoercive.
(viii) f is supercoercive.
(ix) C is bounded.
Then C is D-proximinal.
Likewise, Proposition 3.22 with ϕ = ιC yields the following.
Proposition 3.32. Let C be a convex subset of X. Then the following hold:
(ii) If, in addition, ran PC ⊂int dom f, then
(a) PC = RNC.
(b) Fix PC = C ∩int dom f.
(c) PC is D-ﬁrm.
(d) C is D-semi-Chebyshev if f|int dom f is strictly convex.
The D-viability requirements for the range of PC are obtained by setting ϕ = ιC
in Proposition 3.23.
Proposition 3.33. Let C ⊂X be convex such that C ∩dom f ̸= Ø. Assume
that one of the following conditions holds:
(i) dom ∂(f + ιC) ⊂int dom f.
(ii) C ∩dom f ⊂int dom f.
(iii) dom f is open.
(iv) C ⊂int dom f.
(v) C ∩int dom f ̸= Ø and one of the following conditions holds:
(a) C ∩dom ∂f ⊂int dom f;
(b) f is essentially smooth.
Then ran PC ⊂int dom f.
Theorem 3.34. Let C ⊂X be a closed convex set such that C ∩dom f ̸= Ø.
Suppose that X is reﬂexive and that one of conditions (i)–(ix) in Proposition 3.31
holds together with one of conditions (i)–(v) in Proposition 3.33. Then PC ∈B.
Since Proposition 3.31 parallels Proposition 3.21 and Proposition 3.33
parallels Proposition 3.23, it suﬃces to set ϕ = ιC in Theorem 3.24.
We conclude this section with the following result.
Corollary 3.35. Suppose that X is reﬂexive, that f is Legendre, and that C is
a closed convex subset of X such that C ∩int dom f ̸= Ø. Then
(i) C is D-Chebyshev and PC ∈B;
(ii) for every x and y in int dom f,
C ⊂H(y, x).
Proof. Take ϕ = ιC in Corollary 3.25.
Remark 3.36. Proposition 3.31(vii)–(ix) can be found in [1, Prop. 2.1]. Corollary 3.35(i) covers [8, Cor. 7.9] (see also [7, section 3] in the special case of Euclidean spaces), which was obtained via diﬀerent arguments. If X is Hilbertian and
f = ∥· ∥2/2, Corollary 3.35(ii) reduces to the classical characterization of metric
projections onto closed convex sets.
3.6. Subgradient D-projections. The D-projection onto a closed convex set
may be hard to compute. If the set is speciﬁed as a lower level set, it can be approximated by the D-projection onto a separating hyperplane, which is much easier
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
to compute. In the traditional case when X is Hilbertian and f = ∥· ∥2/2, this is
a standard approach which goes back to (see also ). In the context
of Bregman distances, we shall deﬁne subgradient D-projections as follows (see also
 for special instances).
Definition 3.37. Suppose that
X is reﬂexive and f is Legendre,
g: X →]−∞, +∞] is lower semicontinuous and convex,
lev≤0 g ∩int dom f ̸= Ø and dom f ⊂dom g.
For every x ∈int dom f and x∗∈∂g(x), set
G(x, x∗) =
y ∈X | ⟨x −y, x∗⟩≥g(x)
The operator
Qg : int dom f →X : x →
PG(x,x∗)x | x∗∈∂g(x)
is the subgradient D-projector onto lev≤0 g.
Note that G(x, x∗) is a proper closed half-space if x∗̸= 0 and the whole space X
otherwise; the latter may occur only when x ∈Argmin g.
Proposition 3.38. Suppose that (3.26) is in force and let Qg be the subgradient
D-projector onto lev≤0 g. Then
(i) Fix Qg = lev≤0 g ∩int dom f;
(ii) Qg ∈B.
Proof. Fix x ∈int dom f and x∗∈∂g(x). Since int dom f ⊂int dom g ⊂dom ∂g,
∂g(x) ̸= Ø and the closed convex set G(x, x∗) is well deﬁned. Moreover, (2.2) yields
(∀y ∈lev≤0 g) ⟨y −x, x∗⟩≤g(y) −g(x) ≤−g(x).
Therefore, lev≤0 g ⊂G(x, x∗) and, in turn, G(x, x∗) ∩int dom f ̸= Ø. Hence, Corollary 3.35(i) asserts that PG(x,x∗) is single-valued with ran PG(x,x∗) ⊂int dom f =
dom PG(x,x∗), whence ran Qg ⊂int dom f = dom Qg. (i) Take y ∈X. Then it follows
from Proposition 3.32(ii)(b) that
y ∈Fix Qg ⇔(∃y∗∈∂g(y)) y = PG(y,y∗)y
⇔(∃y∗∈∂g(y)) y ∈G(y, y∗) ∩int dom f
⇔(∃y∗∈∂g(y)) 0 = ⟨y −y, y∗⟩≥g(y) and y ∈int dom f
⇔y ∈lev≤0 g ∩int dom f.
Thus, Fix Qg = lev≤0 g ∩int dom f. (ii) To show that Qg ∈B observe that Corollary 3.35(ii) implies that G(x, x∗) ⊂H(x, PG(x,x∗)x). Consequently, Fix Qg ⊂lev≤0g ⊂
G(x, x∗) ⊂H(x, PG(x,x∗)x), where (x, PG(x,x∗)x) is an arbitrary point in gr Qg. Altogether, Qg ∈B.
3.7. Relaxed parallel combination of B-class operators. The following
proposition describes a scheme to aggregate B-class operators in order to create a
new B-class operator.
Proposition 3.39.
Suppose that X is reﬂexive and that f is Legendre.
(Ti)i∈I be a ﬁnite family of operators in B such that 
i∈I Fix Ti ̸= Ø, let (ωi)i∈I be
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
weights in ]0, 1] such that 
i∈I ωi = 1, and let λ be a relaxation parameter in ]0, 1].
For every x ∈int dom f, select (ui)i∈I ∈×i∈ITix, put
y ∈X | ⟨y, x∗⟩≤η(x)
x∗= ∇f(x) −
i∈I ωi∇f(ui),
i∈I ωi ⟨x + λ(ui −x), ∇f(x) −∇f(ui)⟩,
and deﬁne T : int dom f →X : x →PH(x)x. Then the following hold:
(i) T is single-valued on dom T = int dom f ⊃ran T.
(ii) For every x ∈int dom f, the following statements are equivalent:
i∈I Fix Ti.
(b) x∗= 0.
(c) H(x) = X.
(d) x ∈H(x).
(e) x ∈Fix T.
(iii) Fix T = 
i∈I Fix Ti.
(iv) Fix T = 
i∈I Fix Ti.
(v) (∀x ∈int dom f) H(x) = H(x, Tx).
(vi) T ∈B.
Proof. Fix x ∈int dom f. (i) We ﬁrst observe that the operator T is well deﬁned.
Indeed, since (Ti)i∈I lies in B, x∗and η(x) are well deﬁned and we have
⊂(int dom f) ∩
⊂(int dom f) ∩
y ∈X | ⟨y −ui, ∇f(x) −∇f(ui)⟩
≤(1 −λ) ⟨x −ui, ∇f(x) −∇f(ui)⟩
⊂(int dom f) ∩
ωi ⟨y −ui, ∇f(x) −∇f(ui)⟩
ωi ⟨x −ui, ∇f(x) −∇f(ui)⟩
= (int dom f) ∩H(x),
where the second inclusion follows from the inequality λ ≤1 and the monotonicity
of ∇f. Whence, (int dom f) ∩H(x) ̸= Ø, and it follows from Corollary 3.35(i) that
PH(x)x is a well-deﬁned point in int dom f. (ii) Since f is essentially strictly convex,
it is strictly convex on int dom f and it follows from Proposition 3.3(vi) that (a) ⇒
(∀i ∈I) ui = x ⇒(b). (b) ⇒(c) Suppose x∗= 0 and ﬁx y ∈
i∈I Fix Ti. Then,
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
since (Ti)i∈I lies in B,
ωi ⟨ui −y, ∇f(x) −∇f(ui)⟩
= η(x) −⟨y, x∗⟩−(1 −λ)
ωi ⟨x −ui, ∇f(x) −∇f(ui)⟩
Accordingly, H(x) = X. The implications (c) ⇒(d) ⇒x = PH(x)x ⇒(e) are clear
in view of Proposition 3.32(ii)(b). (e) ⇒(a) We have
x ∈Fix T ⇔x = PH(x)x
⇔⟨x, x∗⟩≤η(x)
ωi ⟨x −ui, ∇f(x) −∇f(ui)⟩≤0
⇔(∀i ∈I) x = ui ∈Tix
where the next to last equivalence follows from the strict monotonicity of ∇f on
int dom f (f is strictly convex on int dom f) and the inequalities λ > 0 and mini∈I ωi >
0. (iii) (i) and (ii) yield Fix T = (int dom f) ∩Fix T = 
i∈I(Fix Ti ∩int dom f) =
i∈I Fix Ti.
(iv) Set (∀i ∈I) Fi = 
(x,u)∈gr Ti H(x, u).
Then (iii) and Proposition 3.3(iv) yield Fix T = (int dom f)∩
i∈I Fi. Therefore, by Lemma 3.2 and Proposition 3.3(vii),
Fix T = dom f ∩
(Fi ∩dom f) =
(v) By Corollary 3.35(ii), we always have H(x) ⊂H(x, PH(x)x) = H(x, Tx). Now
suppose x ∈H(x). Then (ii) yields H(x) = X = H(x, x) = H(x, PH(x)x) = H(x, Tx).
Next, suppose x /∈H(x). Then (ii) yields x∗̸= 0 and H(x) is therefore a proper
closed half-space in X.
On the other hand, x ̸= PH(x)x = Tx and, since ∇f is
injective [8, Thm. 5.10], ∇f(x) ̸= ∇f(Tx). Consequently, H(x, Tx) is also a proper
closed half-space in X. Since Tx ∈H(x) ∩bdry H(x, Tx) and H(x) ⊂H(x, Tx), we
conclude H(x) = H(x, Tx). (vi) It follows successively from (iii), (3.32), and (v) that
i∈I Fix Ti ⊂H(x) = H(x, Tx). In view of (i), the proof is complete.
4. Bregman monotonicity.
4.1. Properties. D-monotonicity was introduced in Deﬁnition 1.2.
collect some elementary properties.
Proposition 4.1. Let (xn)n∈N be a sequence in X which is D-monotone with
respect to a set S ⊂X. Then the following hold:
(i) (∀x ∈S ∩dom f)
n∈N converges.
(ii) (∀n ∈N) DS(xn+1) ≤DS(xn).
n∈N converges.
(iv) (∀(x, x′) ∈(S ∩dom f)2)
⟨x −x′, ∇f(xn)⟩
n∈N converges.
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
(v) (xn)n∈N is bounded if, for some z ∈S ∩dom f, the set lev≤D(z,x0) D(z, ·) is
bounded. This is true in particular if S ∩int dom f ̸= Ø, X is reﬂexive, and
one of the following properties is satisﬁed:
(a) f is supercoercive;
(b) dim X < +∞and dom f ∗is open.
Proof. (i) and (ii) are immediate consequences of Deﬁnition 1.2, and (iii) follows from (ii). (iv) Take x and x′ in S ∩dom f. By (i), the sequences
⟨x −xn, ∇f(xn)⟩
f(xn)+⟨x′ −xn, ∇f(xn)⟩
n∈N converge and so does their
⟨x −x′, ∇f(xn)⟩
n∈N. (v) By deﬁnition, for every x ∈S ∩dom f, (xn)n∈N
lies in lev≤D(z,x0) D(z, ·).
The second assertion follows from [8, Lemma 7.3(viii)
and (ix)], which asserts that D(z, ·) is coercive under the stated assumptions if
z ∈int dom f.
The following example shows that the conclusion of Proposition 4.1(v) may hold
even though the properties (a) and (b) are not satisﬁed.
Example 4.2. Let X = ℓ2(N) and deﬁne
f : X →]−∞, +∞] : x = (ξk)k∈N →
k∈N ξk −ln(1 + ξk)
if (∀k ∈N) ξk > −1,
otherwise.
Then f is Legendre and dom f is open. Moreover, lev≤η D(0, ·) is bounded for η > 0
suﬃciently small.
Proof. We only sketch the arguments, as the example is not utilized elsewhere.
Observe that f is separable: (∀x ∈X) f(x) = 
k∈N h(ξk), where
(∀ξ ∈R) h(ξ) =
ξ −ln(1 + ξ)
if ξ > −1,
otherwise.
Using calculus, one veriﬁes that dom f =
x ∈X | (∀k ∈N)
, which is
open. Also, f is Gˆateaux-diﬀerentiable on its domain with ∇f(x) =
Hence f is essentially smooth. Now (∀x ∈X) f ∗(x) = f(−x). Thus f ∗is essentially
smooth as well.
By [8, Thm. 5.4], f is essentially strictly convex.
Altogether, f
is Legendre. Let α = ln(2) −1/2. A careful analysis of the Bregman distance Dh
associated with h reveals that Dh(0, ξ) < α ⇒|ξ| < 1 ⇒Dh(0, ξ) ≥α|ξ|2.
passing, we point out that Dh(0, ·) is convex precisely on ]−1, +1[.) Fix η ∈[0, α[ and
x ∈X such that D(0, x) ≤η. Then (∀k ∈N) Dh(0, ξk) ≥α|ξk|2. Summing yields
η ≥D(0, x) ≥α∥x∥2, whence x ∈B(0;
The next two assumptions will be quite helpful in the analysis of the convergence
of D-monotone sequences.
Condition 4.3. Given S ⊂X, for every bounded sequence (xn)n∈N in int dom f,
x ∈W(xn)n∈N ∩S,
x′ ∈W(xn)n∈N ∩S,
(xn)n∈N is D-monotone with respect to S
Condition 4.4. For all bounded sequences (xn)n∈N and (yn)n∈N in int dom f,
D(xn, yn) →0
xn −yn →0.
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
These two assumptions cover familiar situations, as the following examples show.
Example 4.5. Suppose that S is a subset of X such that S ∩dom f is a singleton.
Then Condition 4.3 is satisﬁed.
Take (xn)n∈N in int dom f.
Then W(xn)n∈N ⊂dom f and, therefore,
W(xn)n∈N ∩S is at most a singleton.
Example 4.6. Suppose that S ⊂int dom f is convex, f|S is strictly convex, and
∇f is sequentially weak-to-weak∗continuous at every point in S. Then Condition 4.3
is satisﬁed.
Proof. Let (xn)n∈N be a bounded sequence which is D-monotone with respect to
S. Then xkn ⇀x ∈S and xln ⇀x′ ∈S imply ∇f(xkn)
∗⇀∇f(x) and ∇f(xln)
∇f(x′). Proposition 4.1(iv) therefore forces ⟨x −x′, ∇f(x)⟩= ⟨x −x′, ∇f(x′)⟩; hence
⟨x −x′, ∇f(x) −∇f(x′)⟩= 0.
Since ∇f is strictly monotone on S, we get x =
Our next example requires the following lemma.
Lemma 4.7. Suppose that ε ∈]0, +∞[, x ∈dom f, and y ∈int dom f. Then
there exists z ∈int dom f such that ∥x −z∥≤ε and |D(x, y) −D(z, y)| ≤ε.
Proof. Put (∀α ∈[0, 1[) xα = (1 −α)y + αx. Then (xα)α∈[0,1[ lies in int dom f,
limα↑1−xα = x and, by (3.6), limα↑1−D(xα, y) = D(x, y). Thus, for α suﬃciently
close to 1, we can take z = xα.
We now recall the notion of a Bregman/Legendre function in RN, which covers
numerous functions of importance in convex optimization . This notion will allow
us to describe a ﬁnite-dimensional setting in which Condition 4.3 holds.
Definition 4.8. Suppose that X = RN and f is Legendre. Then f is Bregman/
Legendre, if each of the following conditions is satisﬁed:
(i) dom f ∗is open.
(ii) (∀x ∈dom f ∖int dom f) D(x, ·) is coercive.
x ∈dom f ∖int dom f,
(yn)n∈N in int dom f,
yn →y ∈bdry dom f,
n∈N bounded
D(y, yn) →0.
(xn)n∈N in int dom f,
(yn)n∈N in int dom f,
xn →x ∈dom f ∖int dom f,
yn →y ∈dom f ∖int dom f,
D(xn, yn) →0
Example 4.9. Suppose that X = RN, f is Bregman/Legendre, and S is a subset
of X such that S ∩dom f ̸= Ø. Then Condition 4.3 is satisﬁed.
Proof. Let us start with two useful facts, namely
(yn)n∈N in int dom f,
n∈N bounded
D(y, yn) →0,
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
(yn)n∈N in int dom f,
yn →y ∈dom f,
D(x, yn) →0
If x ∈int dom f, (4.5) follows from [7, Thm. 3.8(ii)].
On the other hand, if x ∈
dom f ∖int dom f, (4.5) follows from [7, Prop. 3.3] if y ∈int dom f and from [7,
Def. 5.2.BL2] if y ∈bdry dom f. We now turn to (4.6). If x or y belongs to int dom f,
it suﬃces to apply [7, Thm. 3.9(iii)].
Otherwise, {x, y} ⊂dom f ∖int dom f and
Lemma 4.7 ensures that, for every n ≥1, we can ﬁnd a point xn ∈int dom f such
that ∥x −xn∥≤1/n and |D(x, yn) −D(xn, yn)| ≤1/n. Therefore, xn →x and, since
D(x, yn) →0 by assumption, D(xn, yn) →0. It then follows from [7, Def. 5.2.BL3]
that x = y.
Now let (xn)n∈N be a bounded sequence which is D-monotone with
respect to S and let z ∈S ∩dom f. Suppose xkn →x ∈S and xln →x′ ∈S. Since by
D-monotonicity the sequences
n∈N are bounded, (4.5)
yields D(x, xkn) →0, D(x′, xln) →0, and {x, x′} ⊂S ∩dom f. However, it follows
from Proposition 4.1(i) that D(x, xkn) →0 ⇒D(x, xn) →0 ⇒D(x, xln) →0. In
view of (4.6), we conclude x = x′, as required.
Following , we say that f is uniformly convex on bounded sets if, for every
bounded set B ⊂X, one has
(∀t ∈]0, +∞[) inf µ
B ∩dom f, t
µ: dom f × [0, +∞[ →[0, +∞] : (x, t) →
f(x) + f(y)
Examples of such functions are given in .
The next result gives suﬃcient conditions for Condition 4.4 to hold. (See also 
and for item (ii).)
Example 4.10. Condition 4.4 is satisﬁed whenever one of the following is true:
(i) f is uniformly convex on bounded sets.
(ii) X = RN, dom f is closed, and f|dom f is strictly convex and continuous.
(iii) X = R and f|dom f is strictly convex.
Proof. (i) is a direct consequence of [25, Prop. 4.2]. (ii) and (iii) are special cases
of (i) by [85, Prop. 3.6.6(i)].
In passing, we note that it follows from [85, Thm. 3.5.13] that item (i) of Example 4.10 forces the underlying space X to be reﬂexive.
The above assumptions lead to remarkably simple weak and strong convergence
criteria for D-monotone sequences. In the case when X is Hilbertian and f = ∥·∥2/2,
Conditions 4.3 and 4.4 are satisﬁed and these criteria can essentially be found in 
(see also and ). Recall (see section 2) that S denotes the set of strong cluster
points of a sequence.
Theorem 4.11. Let (xn)n∈N be a bounded sequence in X which is D-monotone
with respect to a set S ⊂X. Suppose that X is reﬂexive and Condition 4.3 is satisﬁed.
(i) (xn)n∈N converges weakly to a point in S∩dom f if and only if W(xn)n∈N ⊂S;
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
(ii) supposing that xn ⇀x ∈S ∩int dom f and Condition 4.4 is satisﬁed, then
xn →x if and only if S(xn)n∈N ̸= Ø.
Proof. (i) Necessity is clear. To prove suﬃciency, suppose that W(xn)n∈N ⊂S
and take x and x′ in W(xn)n∈N, say xkn ⇀x and xln ⇀x′. Then x and x′ lie in
S and (4.3) forces x = x′. Since X is reﬂexive and (xn)n∈N is bounded, we conclude
xn ⇀x. Furthermore, since dom f ∋xn ⇀x and dom f is weakly closed, x ∈dom f.
(ii) Necessity is clear. To prove suﬃciency, suppose that Condition 4.4 is satisﬁed,
x ∈S ∩int dom f, and S(xn)n∈N ̸= Ø, i.e., some subsequence (xkn)n∈N converges
strongly. Since xn ⇀x, we must have xkn →x. In turn, [8, Lemma 7.3(x)] yields
D(x, xkn) →0 and it follows from Proposition 4.1(i) that D(x, xn) →0. In view of
(4.4), we conclude xn →x.
4.2. Construction.
Algorithm 4.12. Starting with x0 ∈int dom f, at every iteration n ∈N, select
ﬁrst Tn ∈B and then xn+1 ∈Tnxn.
Proposition 4.13. Let (xn)n∈N be an arbitrary orbit of Algorithm 4.12. Suppose
Fix Tn ̸= Ø, S ⊂
Fix Tn, and
S ∩dom f ̸= Ø.
(i) if f|int dom f is strictly convex, (xn)n∈N is D-monotone with respect to S;
n∈N D(xn+1, xn) < +∞.
(i) Proposition 3.3(viii) yields (∀n ∈N)(∀y ∈Fix Tn) D(y, xn+1) ≤
(ii) Fix y ∈
n∈N Fix Tn.
Then Proposition 3.3(i) yields the stronger
(∀n ∈N) D(y, xn+1) ≤D(y, xn) −D(xn+1, xn).
Therefore 
n∈N D(xn+1, xn) ≤D(y, x0).
Theorem 4.14. Let (xn)n∈N be an arbitrary bounded orbit of Algorithm 4.12.
Suppose that X is reﬂexive, that f|int dom f is strictly convex, and that (4.9) is satisﬁed.
Suppose in addition that Condition 4.3 is satisﬁed and that
D(xn+1, xn) < +∞
W(xn)n∈N ⊂S.
(i) (xn)n∈N converges weakly to a point x ∈S;
(ii) the convergence is strong in (i) if x ∈int dom f, Condition 4.4 is satisﬁed,
D(xn+1, xn) < +∞
S(xn)n∈N ̸= Ø.
Proof. Combine Theorem 4.11 and Proposition 4.13.
5. Parallel block-iterative D-monotone algorithm.
5.1. Objective. For the remainder of this paper, we assume that
X is reﬂexive and f is Legendre,
(Si)i∈I is a countable family of closed convex subsets of X,
(int dom f) ∩
i∈I Si ̸= Ø,
S = dom f ∩
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
The purpose of this section is to develop a relaxed, parallel, block-iterative algorithm
to solve the convex feasibility problem
Find x ∈S.
5.2. Algorithm.
Algorithm 5.1. Starting with x0 ∈int dom f, take at every iteration n
➀a nonempty ﬁnite index set In ⊂I,
➁operators (Ti,n)i∈In in B such that (∀i ∈In) Si ∩int dom f ⊂Fix Ti,n,
➂points (ui,n)i∈In ∈×i∈InTi,nxn,
➃weights (ωi,n)i∈In in such that 
i∈In ωi,n = 1,
➄a relaxation parameter λn ∈]0, 1]
n = ∇f(xn) −
i∈In ωi,n∇f(ui,n),
xn, ∇f(xn) −
ωi,n∇f(ui,n)
ωi,n ⟨ui,n −xn, ∇f(ui,n) −∇f(xn)⟩,
y ∈X | ⟨y, x∗
Then set xn+1 = PHnxn.
We now motivate this algorithm geometrically. At iteration n, xn is given and
a ﬁnite block of indices In is retained. Set I+
i ∈In | ωi,n > 0
. Then, using
Lemma 3.2 for the ﬁrst and last equality, step ➁for the third inclusion, and (3.32)
for the fourth inclusion,
S = (int dom f) ∩
Si ⊂(int dom f) ∩
Si ⊂(int dom f) ∩
Fix Ti,n ⊂(int dom f) ∩Hn = dom f ∩Hn ⊂Hn.
Thus, Hn acts as an outer approximation to the intersection of the block of constraint
sets (dom f ∩Si)i∈In and, therefore, to S. More precisely, the block constraint y ∈
i∈In Si is replaced by the surrogate aﬃne constraint ⟨y, x∗
n⟩≤ηn. The
update xn+1 is then the D-projection of xn onto Hn, i.e., the D-closest point to xn
which satisﬁes the surrogate constraint. (xn+1 is well deﬁned by virtue of (5.1) and
Corollary 3.35(i).) Naturally, such a point is considerably simpler to ﬁnd than a point
in dom f ∩
i∈In Si. In spirit, this type of surrogate constraint construction can be
found—explicitly or implicitly—in several places in the literature, although not in the
context of Bregman distances. (See, for instance, and the references therein.)
The parallel nature of the algorithm stems from the fact that the points (ui,n)i∈In
at step ➂can be computed independently on concurrent processors. In addition, the
algorithm has the ability to process variable blocks of constraints, which makes it
possible to match closely the computational load of each iteration to the parallel
processing architecture at hand. A discussion on the importance of block-processing
for task scheduling on parallel architectures can be found in .
To shed more light on Algorithm 5.1, we ﬁrst consider the case when X is Hilbertian and f = ∥· ∥2/2. Then, steps ➅and ➆become
i∈In ωi,nui,n,
i∈In ωi,nui,n
i∈In ωi,n∥ui,n −xn∥2.
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
Furthermore, the updating step is explicitly given as
xn+1 = PHnxn = xn + ηn −⟨xn, x∗
n = xn + λnLn
ωi,n(ui,n −xn)
i∈In ωi,n∥ui,n −xn∥2
i∈In ωi,n(ui,n −xn)∥2
otherwise.
This is essentially the algorithm proposed in [41, section 6] (in this setting, the range
of λn can be extended to ]0, 2[), which itself contains those of 
as special cases. In particular, if I is ﬁnite, In ≡I, ωi,n = ωi, and ui,n = Pixn,
where Pi is the metric projector onto Si, then (5.5)–(5.6) reduces to Pierra’s classic
extrapolated parallel projection method , which in turn can be traced back to Merzlyakov’s method for solving systems of linear inequalities in RN. Since Ln ≥1
in (5.6), large extrapolations are possible in this algorithm by selecting λn ≈1. It is
known that these extrapolations yield signiﬁcantly accelerated convergence in numerical experiments in comparison with purely averaged iterations, i.e.,
which can be derived from (5.5) by setting λn = 1/Ln.
Returning to the standing assumptions, let us now consider the parallel blockiterative update rule
∇f(xn+1) =
ωi,n∇f(ui,n).
This alternative method for solving (5.2) was recently proposed by Censor and Herman
in (see also ) for the special case when X = RN, I is ﬁnite, and ui,n is the
D-projection of xn onto Si. If we assume that X is a Hilbert space and f = ∥· ∥2/2,
then (5.8) reduces to (5.7) which, as noted above, is itself a special case of (5.5)–(5.6),
hence of Algorithm 5.1. In general, however, we do not know whether (5.8) is always
a particularization of Algorithm 5.1.
We now turn to Butnariu and Iusem’s algorithmic framework for solving
(5.2). (In fact, they study the so-called stochastic convex feasibility problem, which is
similar to (5.2) but allows for an uncountable index set I. Their framework requires
measure theory for a precise formulation and their assumptions on the underlying
function f are diﬀerent from the ones made here. The reader is referred to for
further details.) Let (Ri)i∈I be a family of totally nonexpansive operators in the sense
of . (See also the paragraph following Deﬁnition 3.1.) Specialized to the case when
I is ﬁnite, the update step in this algorithm is
This resembles (5.8), except for notably absent gradients on both sides of the equation and for weights that do not depend on n. (If the Ri’s are D-projectors, then
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
(5.9) can also be interpreted as a sequential algorithm in the product space XI; see
 .) Note that if X is a Hilbert space and f = ∥· ∥2/2, then (5.9) once again corresponds to a parallel Cimmino-type algorithm, which is genuinely more restrictive
than Algorithm 5.1 for this set-up.
While a detailed numerical and theoretical comparison of these algorithms lies
beyond the scope of this paper, we remark that preliminary experiments suggest
that Algorithm 5.1 is more ﬂexible and faster than the one given by (5.8) and that
Algorithm 5.1 is genuinely diﬀerent from the method given by (5.9).
5.3. Convergence. The following notions were introduced in [6, Def. 3.7] and
[41, Def. 6.5], respectively, to study the asymptotic behavior of Fej´er monotone algorithms in Hilbert spaces. The former can be interpreted as an extension of the
notion of demiclosedness at 0 and the latter as an extension of the notion of
demicompactness at 0 .
Definition 5.2. Algorithm 5.1 is
• focusing if for every bounded suborbit (xkn)n∈N it generates and every index
ui,kn −xkn →0
• demicompactly regular if there exists i ∈I, called an index of demicompact
regularity, such that for every bounded suborbit (xkn)n∈N it generates,
ui,kn −xkn →0
S(xkn)n∈N ̸= Ø.
We now describe the context in which the convergence of Algorithm 5.1 will be
investigated.
Condition 5.3.
(i) For some z ∈dom f ∩
i∈I Si, C = lev≤D(z,x0) D(z, ·) is bounded.
(ii) For all sequences (un)n∈N and (vn)n∈N in C such that (∀n ∈N) un ̸= vn, one
⟨un −vn, ∇f(un) −∇f(vn)⟩
∥∇f(un) −∇f(vn)∥
∇f(un) −∇f(vn) →0.
Condition 5.4.
(i) (∃δ1 ∈]0, 1[)(∀n ∈N)(∃j ∈In)
∥∇f(uj,n) −∇f(xn)∥= max
i∈In ∥∇f(ui,n) −∇f(xn)∥and ωj,n ≥δ1.
(ii) (∃δ2 ∈]0, 1[)(∀n ∈N) λn ≥δ2.
(iii) (∀i ∈I)(∃Mi ∈N ∖{0})(∀n ∈N) i ∈n+Mi−1
As will be seen subsequently, the above set of assumptions deﬁnes a broad framework which covers numerous practical situations. Note that, by virtue of (5.1), the
quotient in (5.12) is well deﬁned since ∇f is injective on int dom f [8, Thm. 5.10].
Situations in which Condition 5.3(ii) is satisﬁed are detailed below. Note also that
Condition 5.4(iii) imposes that every index i be activated at least once within any
Mi consecutive iterations. This control rule, which has already been used in metric
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
projection algorithms in Hilbert spaces , provides great ﬂexibility in
the management of the constraints and the implementation of the algorithm. Condition 5.4(i) provides added ﬂexibility by oﬀering the possibility of setting ωi,n = 0
if the corresponding step size ∥∇f(ui,n) −∇f(xn)∥is not maximal. It is thereby
possible to meet the control condition Condition 5.4(iii) without actually using the
ith constraint in the construction of xn+1.
Recall that an operator T from a Banach space Y to its dual Y∗is said to be
uniformly monotone on U ⊂dom T with modulus c if [86, section 25.3]
(∀x ∈U)(∀y ∈U) ⟨x −y, Tx −Ty⟩≥∥x −y∥· c(∥x −y∥),
where c: [0, +∞[ →[0, +∞[ is a strictly increasing function such that c(0) = 0. In
particular, T is said to be strongly monotone on U with constant α ∈]0, +∞[ if it is
uniformly monotone on U with modulus c: t →αt.
Proposition 5.5. Let z and C be as in Condition 5.3(i). Then Condition 5.3(ii)
is satisﬁed in each of the following cases:
(i) ∇f ∗is uniformly monotone on ∇f(C).
(ii) ∇f is Lipschitz-continuous on dom f = X.
(iii) X = RN and C ⊂int dom f.
(iv) X = RN and z ∈int dom f.
Let (un)n∈N and (vn)n∈N be two sequences in C such that (∀n ∈N)
(i) Let c be the modulus of uniform monotonicity of ∇f ∗on ∇f(C).
Since ∇f is a bijection from int dom f to int dom f ∗with inverse ∇f ∗[8, Thm. 5.10]
and since C ⊂int dom f, we have (∀u ∈C)(∀v ∈C) ⟨u −v, ∇f(u) −∇f(v)⟩≥
∥∇f(u) −∇f(v)∥· c (∥∇f(u) −∇f(v)∥). Hence, since c is strictly increasing and
⟨un −vn, ∇f(un) −∇f(vn)⟩
∥∇f(un) −∇f(vn)∥
∥∇f(un) −∇f(vn)∥
⇒∇f(un) −∇f(vn) →0.
(ii) ⇒(i) If ∇f is κ-Lipschitz-continuous on X, then it follows from the Baillon–
Haddad theorem [4, Cor. 10] that (∀x ∈X)(∀y ∈X) ⟨x −y, ∇f(x) −∇f(y)⟩≥
∥∇f(x)−∇f(y)∥2/κ, i.e., ∇f ∗is strongly monotone with constant 1/κ. Consequently,
∇f ∗is uniformly monotone on ∇f(C). (iii) Suppose
⟨un −vn, ∇f(un) −∇f(vn)⟩
∥∇f(un) −∇f(vn)∥
∇f(un) −∇f(vn) ̸→0.
Then there exists a strictly increasing sequence (kn)n∈N in N and ε ∈]0, +∞[ such
that infn∈N ∥∇f(ukn) −∇f(vkn)∥≥ε. Since (ukn)n∈N lies in C, it is bounded and
therefore possesses a convergent subsequence, say ukln →u. As (vkln)n∈N is also
bounded, we can assume (passing to a subsequence if necessary) that it converges,
say vkln →v. Since {u, v} ⊂C ⊂int dom f and ∇f is continuous at every point in
int dom f by [77, Thm. 25.5], taking the limit yields ∥∇f(u) −∇f(v)∥≥ε and, by
injectivity of ∇f on int dom f [8, Thm. 5.10], u ̸= v. On the other hand, (5.15) yields
ukln −vkln, ∇f(ukln) −∇f(vkln)
∥∇f(ukln) −∇f(vkln)∥
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
and, since ∥∇f(u) −∇f(v)∦= 0, taking the limit yields ⟨u −v, ∇f(u) −∇f(v)⟩=
0. However, f|int dom f is strictly convex and therefore ∇f is strictly monotone on
int dom f ⊃{u, v}. This forces u = v and we reach a contradiction. (iv) In view of
(iii), it is enough to show that C ⊂int dom f. If the inclusion does not hold, then we
can ﬁnd y ∈bdry dom f and (yn)n∈N in C such that yn →y. Thus supn∈N D(z, yn) ≤
D(z, x0) < +∞, and, at the same time, since f is essentially smooth, [7, Thm. 3.8(i)]
yields D(z, yn) →+∞, which is absurd.
Remark 5.6. A careful analysis of [85, Corollary 3.4.4“(iii)⇔(iv)”], [85, Proposition 3.5.1], and [85, Proposition 3.6.2] shows that Proposition 5.5(i) holds as soon as
∇f is Lipschitz on bounded sets. In turn, this condition is satisﬁed in Lp spaces for
p, where {p, s} ⊂[2, +∞[. (The proof relies on the case when s = 2; see also
Example 5.11 below.)
Examples of Legendre functions f which satisfy Conditions 4.3, 4.4, and 5.3(i)–
(ii) will be supplied in section 5.4. Our main convergence result can now be stated
and proved.
Theorem 5.7. Suppose that Conditions 4.3, 4.4, 5.3, and 5.4 are satisﬁed, and
let (xn)n∈N be an arbitrary orbit of Algorithm 5.1. Then, for every n ∈N, xn and
(ui,n)i∈In lie in the bounded set C. If, in addition, Algorithm 5.1 is focusing, then the
following statements hold true:
(i) (xn)n∈N converges weakly to a point x ∈S.
(ii) If the weak limit x from (i) belongs to int dom f and the algorithm is demicompactly regular, then (xn)n∈N converges strongly.
Proof . For every n ∈N, set Tn = PHn and I+
i ∈In | ωi,n > 0
x0 ∈int dom f and, by Proposition 3.39(vi), Tn ∈B, we recognize that
Algorithm 5.1 is a special case of Algorithm 4.12.
Our goal is to apply Theorem 4.14 and we must start by verifying (4.9).
considering (5.1), Algorithm 5.1➁, and Proposition 3.39(iii), we obtain
(∀n ∈N) Ø ̸= (int dom f) ∩
(Si ∩int dom f) ⊂
Fix Ti,n = Fix Tn.
n∈N Fix Tn ̸= Ø. In addition, (5.1), Lemma 3.2, and (5.18) yield
(∀n ∈N) S = dom f ∩
Si ⊂Fix Tn.
Consequently, S ⊂
n∈N Fix Tn. Next, we derive from (5.1) that
Ø ̸= (int dom f) ∩
Si ⊂dom f ∩dom f ∩
Si = dom f ∩S.
Thus, (4.9) holds. Now, let z and C be as in Condition 5.3(i). It follows from (5.17)
and Proposition 4.13(i) that the sequences (xn)n∈N and (Tnxn)n∈N are contained in C,
which is bounded. In order to verify (4.11), some key facts must be established. Let us
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
temporarily ﬁx n ∈N. The ﬁrst fact is supplied by the inclusion xn+1 = PHnxn ∈Hn,
which yields
∥xn+1 −xn∥≥dHn(xn).
Next, it follows from Condition 5.3(i), (5.1), Lemma 3.2, and Algorithm 5.1➁that
(∀i ∈In) z ∈Si ∩dom f = Si ∩int dom f ⊂Fix Ti,n.
Hence, for every i ∈In, Algorithm 5.1➂and Proposition 3.3(viii) yield D(z, ui,n) ≤
D(z, xn) −D(ui,n, xn) ≤D(z, xn). Therefore,
(∀i ∈In) ui,n ∈C.
Now, per Condition 5.4(ii), pick jn ∈In such that
∥∇f(ujn,n) −∇f(xn)∥= max
i∈In ∥∇f(ui,n) −∇f(xn)∥
ωjn,n ≥δ1.
We claim that
n Fix Ti,n
ujn,n = xn
∥∇f(ujn,n) −∇f(xn)∥= 0,
n Fix Ti,n
dHn(xn) ≥δ1δ2
⟨ujn,n −xn, ∇f(ujn,n) −∇f(xn)⟩
∥∇f(ujn,n) −∇f(xn)∥
On the one hand, using Proposition 3.3(vi) and the injectivity of ∇f on int dom f
[8, Thm. 5.10], since (5.24) forces jn ∈I+
n , we get xn ∈
n Fix Ti,n ⇔(∀i ∈I+
ui,n = xn ⇒ujn,n = xn ⇒∥∇f(ujn,n) −∇f(xn)∥= 0 ⇒(∀i ∈In) ∥∇f(ui,n) −
∇f(xn)∥= 0 ⇔(∀i ∈In) ui,n = xn ⇒(∀i ∈I+
n ) ui,n = xn. On the other hand, if
n Fix Ti,n, then Proposition 3.39(ii) asserts that xn /∈Hn and x∗
n ̸= 0, so
dHn(xn) = ⟨xn, x∗
i∈In ωi,n ⟨ui,n −xn, ∇f(ui,n) −∇f(xn)⟩
∇f(ui,n) −∇f(xn)
i∈In ωi,n ⟨ui,n −xn, ∇f(ui,n) −∇f(xn)⟩
i∈In ωi,n∥∇f(ui,n) −∇f(xn)∥
⟨ujn,n −xn, ∇f(ujn,n) −∇f(xn)⟩
∥∇f(ujn,n) −∇f(xn)∥
where (5.26) follows from [80, Lemma I.1.2] and (5.27) from Condition 5.4(ii). Altogether, (5.25) is veriﬁed. The third key fact is derived from (5.23) and Proposition 2.3(i) as follows:
(∀i ∈In) diam(C)∥∇f(ui,n) −∇f(xn)∥≥⟨ui,n −xn, ∇f(ui,n) −∇f(xn)⟩
= D(ui,n, xn) + D(xn, ui,n)
≥D(ui,n, xn).
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
Let us now verify (4.11). To this end, let us ﬁx i ∈I and x ∈W(xn)n∈N, say xkn ⇀x.
Because x ∈dom f, it is suﬃcient to show
D(xn+1, xn) →0
Let Mi be as in Condition 5.4(iii). After passing to a subsequence of (xkn)n∈N if
necessary, we assume that, for every n ∈N, kn+1 ≥kn + Mi. This guarantees the
existence of a sequence (pn)n∈N in N such that
(∀n ∈N) kn ≤pn ≤kn + Mi −1 < kn+1 ≤pn+1
Now consider the subsequence (xpn)n∈N of (xn)n∈N. The triangle inequality yields
(∀n ∈N) ∥xpn −xkn∥≤
∥xl+1 −xl∥≤(Mi −1)
kn≤l≤kn+Mi−2 ∥xl+1 −xl∥.
Now suppose D(xn+1, xn) →0. Then (4.4) yields
xn+1 −xn →0
and it follows from (5.21) that dHn(xn) →0. Consequently, we derive from (5.25),
(5.23), and Condition 5.3(ii) that maxj∈In ∥∇f(uj,n) −∇f(xn)∥→0. In turn, (5.29)
implies that D(ui,pn, xpn) →0 and, invoking (4.4) again, we obtain
ui,pn −xpn →0.
We also derive from (5.32) and (5.33) that xpn −xkn →0, whence xpn ⇀x. However, since the algorithm is focusing, (5.10) yields x ∈Si. Thus (5.30) holds and,
consequently, the following conclusions can be drawn:
(i) Theorem 4.14(i) asserts that (xn)n∈N converges weakly to x ∈S.
(ii) Suppose that x ∈int dom f, i ∈I is an index of demicompact regularity, and
D(xn+1, xn) →0. Then it results from (5.34) and (5.11) that (4.12) holds.
In view of Condition 4.4, the strong convergence claim therefore follows from
Theorem 4.14(ii).
5.4. When all the assumptions hold. In this subsection, we describe scenarios in which all the assumptions required in Theorem 5.7 on f and on the constraint
sets (Si)i∈I are satisﬁed.
As a preamble to our ﬁrst example, recall that if X is a Hilbert space, the Moreau–
Yosida regularization of a proper lower semicontinuous convex function ϕ: X →
]−∞, +∞] with parameter γ ∈]0, +∞[ is the ﬁnite continuous convex function
∥· ∥2/(2γ)
Moreover, Moreau’s classic proximal operator associated
with ϕ and γ is given by Deﬁnition 3.16 for f = ∥· ∥2/2 and will be denoted by
γ . It follows from Proposition 3.21(v) that Proxϕ
γ is deﬁned everywhere and,
from Proposition 3.22(ii)(d) and (c), that it is single-valued and ﬁrmly nonexpansive.
Moreover [67, Prop. 7.d],
∇γϕ = Id −Proxϕ
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
Example 5.8 (Moreau–Yosida regularization).
Let X be a Hilbert space, set
w = ∥· ∥2/2, and deﬁne f : X →R by
f = (1 + γ)w −1ϕ,
where ϕ: X →]−∞, +∞] is a proper lower semicontinuous convex function and γ ∈
]0, +∞[. Then
D: (x, y) →γw(x −y) + w(x −Proxϕ
1 y) + ϕ(Proxϕ
w(x −Proxϕ
1 x) + ϕ(Proxϕ
and Conditions 4.4 and 5.3 are satisﬁed. If Proxϕ
1+γ is aﬃne or S is a singleton, then
Condition 4.3 is also satisﬁed.
Proof. The expression (5.37) is derived from (1.5) by simple algebra. Now set
ψ = w −1ϕ. Then
x∈dom ϕ ϕ(x) + w(· −x) =
⟨x, ·⟩−ϕ(x) −w(x) = (ϕ + w)∗.
Hence, ψ is a proper lower semicontinuous convex function as the conjugate of one
such function. Since ψ is convex, f = ψ+γw is strongly (hence uniformly) convex and,
in view of Example 4.10(i), Condition 4.4 is therefore satisﬁed. On the other hand,
(5.35) yields dom ∇f = X and ∇f = Proxϕ
1 +γ Id. Hence f is essentially smooth
by [8, Thm. 5.6]. Furthermore, since Proxϕ
1 is ﬁrmly nonexpansive, it is 1-Lipschitz
and therefore ∇f is (1 + γ)-Lipschitz. Accordingly, Proposition 5.5(ii) asserts that
Condition 5.3(ii) is satisﬁed. Next, using standard Hilbertian convex calculus, we
∗= ψ∗□(w/γ) = (ϕ + w) □(w/γ) = γ
· /(1 + γ)
+ w/(1 + γ).
It therefore follows from (5.35) that
dom ∇f ∗= X
· /(1 + γ)
Consequently, f ∗is also essentially smooth and it follows from [8, Thm. 5.4] that f
is Legendre. Moreover, since X is a Hilbert space, it is reﬂexive. We also derive from
(5.40) that, since Id −Proxϕ
γ/(1+γ) is (ﬁrmly) nonexpansive, ∇f ∗is 1/γ-Lipschitz and,
thereby, maps bounded sets to bounded sets. It then follows from [8, Thm. 3.3] that
f is supercoercive, and Proposition 4.1(v)(a) asserts that Condition 5.3(i) is satisﬁed.
Finally, since ∇f is continuous, it will be weakly continuous when it is aﬃne, i.e.,
when Proxϕ
γ/(1+γ) is. In turn, Example 4.6 implies that Condition 4.3 is satisﬁed. On
the other hand, if S is a singleton, the claim follows from Example 4.5.
If we let ϕ be the indicator function of a nonempty closed convex set in (5.36),
then we obtain the Legendre function studied in [8, Example 7.2]. Specializing even
further, we obtain the following examples.
Example 5.9 (distance).
In the previous example, set ϕ = ιM, where M is
a closed aﬃne subspace of X, and let PM be the metric projector onto M. Then
Conditions 4.3, 4.4, and 5.3 are satisﬁed, f = (1 + γ)w −d2
2, and D: (x, y) →
γw(x −y) + w(x −PMy) −w(x −PMx).
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
Example 5.10 (energy).
In the previous example, set M = {0} and γ = 1.
Then f = ∥· ∥2/2, ∇f = Id, D: (x, y) →∥x −y∥2/2, and we recover the usual Fej´er
monotonicity framework.
The next example shows that the function f = ∥· ∥2/2 can also be used outside
Hilbert spaces.
Example 5.11 (Lp spaces). Let (Ω, F, µ) be a positive measure space and let p ∈
[2, +∞[. Let X = Lp(Ω, F, µ), equipped with its canonical norm, and set f = ∥· ∥2/2.
Then Conditions 4.4 and 5.3 are satisﬁed. If S is a singleton, then Condition 4.3 is
also satisﬁed.
Proof. By [8, Example 6.5], f is Legendre and uniformly convex on closed balls.
Hence Condition 4.4 holds by Example 4.10(i).
Since f is supercoercive, Condition 5.3(i) follows from [8, Lemma 7.3(viii)]. We now establish Condition 5.3(ii). As
p ∈[2, +∞[, [45, Corollary V.1.2] implies that ρ∥·∥, the modulus of smoothness of X,
is of power type 2. We thus obtain κ ∈]0, +∞[ so that (see [45, section IV.4])
(∀t ∈[0, +∞[)
ρ∥·∥(t) ≤κt2.
Recall that ∇f = J and deﬁne j(x) = J(x)/∥x∥= ∇∥x∥for all nonzero x ∈X. Now
(5.41) and [45, Lemma IV.5.1] yield
(∀u ∈SX )(∀v ∈SX )
∥j(u) −j(v)∥≤κ∥u −v∥.
Fix two nonzero points x and y in X and assume, without loss of generality, that
∥x∥≥∥y∥. Then, using the triangle inequality,
+ ∥y∥· y −∥x∥· y
∥x∥∥x −y∥.
∥j(x) −j(y)∥=
∥x∥∥x −y∥,
where we have used the deﬁnition of j for the equality, (5.42) for the ﬁrst inequality,
and (5.43) for the second. Furthermore,
∥J(x) −J(y)∥=
∥x∥· j(x) −∥y∥· j(y)
∥x∥· j(x) −∥x∥· j(y)
∥x∥· j(y) −∥y∥· j(y)
≤∥x∥· ∥j(x) −j(y)∥+ ∥j(y)∥·
≤(2κ + 1) · ∥x −y∥,
where the last inequality follows from (5.44) and the fact that ∥j(y)∥= 1. Now (5.45)
implies that J = ∇f is Lipschitz-continuous on dom f = X, with constant 2κ + 1
(for x = 0 or y = 0, argue directly). We apply Proposition 5.5(ii) and conclude that
Condition 5.3(ii) is satisﬁed. Finally, if S is a singleton, we employ Example 4.5.
Guaranteeing Condition 4.3 requires some care.
Remark 5.12. As already discussed in Remark 5.6, Proposition 5.5(i) holds as
soon as ∇f is Lipschitz on bounded sets.
Thus, the assertions of Example 5.11
remain true for f = ∥· ∥s/s, where s ∈[2, +∞[. The case when s = p is particularly
interesting because then ∇f becomes Jϕ, the duality mapping corresponding to the
weight ϕ: t →tp−1 (see ). If we specialize this further to the space ℓp(N), then
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
Jϕ is known to be sequentially weakly continuous (see [34, Prop. II.4.14]) and thus
Example 4.6 is applicable. To sum up,
let X = ℓp(N) and f = ∥· ∥p/p, for p ∈[2, +∞[ ;
then Conditions 4.3, 4.4, and 5.3 are satisﬁed.
Additional examples can be generated in suitable product spaces such as ℓp1(N) ×
ℓp2(N), equipped with the Euclidean product norm and with {p1, p2} ⊂[2, +∞[, or in
certain spaces of power type 2. (See for further information about such spaces.)
Example 5.13 (closed domain Bregman/Legendre functions). Let X = RN and
let f be a Bregman/Legendre function with closed domain. Then Conditions 4.3, 4.4,
and 5.3 are satisﬁed.
Proof. Example 4.9 implies that Condition 4.3 holds. Condition 4.4 follows from
[7, Def. 5.2.BL3 and Thm. 3.9(iii)]. It remains to check items (i) and (ii) in Condition 5.3: since D(z, ·) is coercive for every z ∈dom f [7, Remark 5.3], (i) holds,
whereas (ii) follows from Proposition 5.5(iv).
The class of Bregman/Legendre functions (see Deﬁnition 4.8) is large enough to
contain many functions important in convex optimization and it is related to the
Bregman functions of , which require closed domains. We refer the reader to
 for further information. The following example gives conditions that are easy to
verify in practice.
Example 5.14 (separable Bregman/Legendre functions). Let (ϕk)1≤k≤N : R →
]−∞, +∞] be a family of Legendre functions such that (dom ϕ∗
k)1≤k≤N are open. Let
X = RN, and let f : (ξk)1≤k≤N →N
k=1 ϕk(ξk). Then Conditions 4.3, 4.4, and 5.3
are satisﬁed.
Proof. By [7, Corollary 5.13], f is Bregman/Legendre. Mimicking the proof of
the previous example, we note that it remains to check Condition 4.4.
k ∈{1, . . . , m}, since ϕk|int dom ϕk is strictly convex by Legendreness and ϕk|dom ϕk
is continuous by (3.5), ϕk|dom ϕk is strictly convex. Hence, it follows from Example 4.10(iii) that the Bregman distance Dk induced by ϕk on R satisﬁes Condition 4.4
and, in turn, so does D:
(ξk)1≤k≤N, (χk)1≤k≤N
k=1 Dk(ξk, χk).
Unlike the previous examples, the following example does not require that X be
ﬁnite-dimensional or that f have full domain.
Example 5.15. Let X be the Hilbert space ℓ2(N) × R and deﬁne
f : X →]−∞, +∞] : (x, ξ) →
2∥x∥2 + ξ ln(ξ) −ξ
Let (∀i ∈I) Si = S = ℓ2(N) × [1, +∞[.
Fix (z, ζ) ∈S, η > 0, and set C =
. Then Conditions 4.3, 4.4, and 5.3 are satisﬁed.
Proof. Let g = f(·, 0) and h = f(0, ·). Hence, (∀(x, ξ) ∈X) f(x, ξ) = g(x) + h(ξ).
Note that g and h are Legendre, and so is f, with dom f = ℓ2(N) × [0, +∞[. Now,
let Dg and Dh be the Bregman distances induced by g on ℓ2(N) and h on R, respectively.
Take (y, χ) ∈X with D
(z, ζ), (y, χ)
= Dg(z, y) + Dh(ζ, χ) ≤η.
particular, Dg(z, y) ≤η and Dh(ζ, χ) ≤η. Since Dg(z, ·) and Dh(ζ, ·) are coercive
by Proposition 4.1(v)(a) and (b), C is bounded. Condition 4.3 is a consequence of
Example 4.6. Since D: (x, ξ) →Dg(x) + Dh(ξ), Condition 4.4 is immediate by Examples 5.10 and 5.13. Applying [7, Thm. 3.8.(i)] to h and ζ ∈int dom h, we obtain
ε ∈]0, +∞[ such that (∀(y, χ) ∈C) χ ≥ε. A straightforward computation shows
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
that ∇f ∗is strongly monotone with constant min{1, ε}. Therefore, using Proposition 5.5(iv), Condition 5.3(ii) holds as well and the proof is complete.
5.5. Applications. A broad class of problems in convex optimization and nonlinear analysis are captured by the mixed convex feasibility problem
Find x ∈dom f such that
(∀i ∈I(1))
(∀i ∈I(2))
(∀i ∈I(3))
ϕi(x) = inf ϕi(X),
(∀i ∈I(4))
where (gi)i∈I(1) and (ϕi)i∈I(3) are families of proper lower semicontinuous convex
functions from X into ]−∞, +∞], (Ai)i∈I(2) is a family of maximal monotone operators
from X into 2X ∗, and (Ti)i∈I(4) is a family of D-ﬁrm operators from X into X. Here,
I(1), I(2), I(3), and I(4) are pairwise disjoint, possibly empty, countable index sets
such that I = 4
k=1 I(k) ̸= Ø. Now let us deﬁne
(∀i ∈I) Si =
if i ∈I(1),
if i ∈I(2),
if i ∈I(3),
if i ∈I(4).
Throughout this section, the following set of assumptions will be made.
Condition 5.16.
(i) Conditions 4.3, 4.4, 5.3, and 5.4 are satisﬁed.
(ii) For every i ∈I(1), ∂gi(C) is bounded and dom f ⊂dom gi.
(iii) For every i ∈I(2), one of the following conditions holds:
(a) dom Ai ⊂int dom f,
(b) Ai is 3∗-monotone.
(iv) For every i ∈I(4), dom Ti = int dom f and Ti −Id is demiclosed at 0 in the
sense that for every sequence (yn)n∈N in dom Ti
(∀n ∈N) un ∈Tiyn,
y ∈Fix Ti.
Let us observe that the sets (Si)i∈I are closed and convex. For i ∈I(1)∪I(2)∪I(3),
this follows from well-known facts; for i ∈I(4), this follows from Condition 5.16(iv),
Propositions 3.5(ii), the essential strict convexity of f, and Proposition 3.3(v). Accordingly, (5.47) is a special case of the convex feasibility problem (5.2) and it can
therefore be solved by Algorithm 5.1.
Algorithm 5.17 (speciﬁc implementation of Algorithm 5.1). Fix (εi)i∈I(2) and
(εi)i∈I(3) in ]0, +∞[. Implement Algorithm 5.1➁by choosing for every i ∈In
if i ∈I(1) (see Deﬁnition 3.37),
Rγi,nAi, where γi,n ∈[εi, +∞[
if i ∈I(2) (see Deﬁnition 3.7),
γi,n, where γi,n ∈[εi, +∞[
if i ∈I(3) (see Deﬁnition 3.16),
if i ∈I(4) (see Deﬁnition 3.4).
Thanks to Condition 5.16, (5.50) meets the requirements of Algorithm 5.1➁since
in each case we have the following:
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
• Ti,n ∈B.
This follows from Proposition 3.38(ii) if i ∈I(1), from Corollary 3.14(ii) and (iii) if i ∈I(2) (since A−1
i 0 ∩int dom f ̸= Ø, dom Ai ∩
int dom f ̸= Ø), from Corollary 3.25(i) if i ∈I(3) (since ϕi is proper and
Argmin ϕi∩int dom f ̸= Ø, ϕi is bounded below and dom ϕi∩int dom f ̸= Ø),
and from Proposition 3.5(ii) if i ∈I(4).
• Si∩int dom f ⊂Fix Ti,n. (See Proposition 3.38(i), Proposition 3.8(iii), Proposition 3.22(ii)(b), and Proposition 3.3(iv) and (vii), respectively.)
Theorem 5.18. Suppose that Condition 5.16 is in force and let (xn)n∈N be an
arbitrary orbit of Algorithm 5.17. Then (xn)n∈N converges weakly to a point x ∈S.
The convergence is strong if x ∈int dom f and any of the following assumptions is
(i) For some i ∈I(1) and some η ∈]0, +∞[, C ∩lev≤η gi is relatively compact.
(ii) For some i ∈I(2), C ∩dom Ai is relatively compact.
(iii) For some i ∈I(3), C ∩dom ∂ϕi is relatively compact.
(iv) For some i ∈I(4), Ti is demicompact at 0 in the sense that for every sequence
(yn)n∈N in dom Ti
(yn)n∈N bounded,
(∀n ∈N) un ∈Tiyn,
S(yn)n∈N ̸= Ø.
Proof. As seen above, (5.47) is a special case of (5.2), whereas Algorithm 5.17
is a special case of Algorithm 5.1. Invoking Theorem 5.7, we shall prove that Algorithm 5.17 is focusing to establish the weak convergence claim and then that it is
demicompactly regular to establish the strong convergence claim. It is recalled that
Theorem 5.7 asserts that (xn)n∈N and
(ui,n)i∈In
n∈N lie in the bounded set C.
To show that Algorithm 5.17 is focusing, let us ﬁx i ∈I and a suborbit (xkn)n∈N
such that i ∈
n∈N Ikn, xkn ⇀x, and ui,kn −xkn →0. According to (5.10), we must
show x ∈Si. Four cases will be considered:
(1) i ∈I(1). We must show gi(x) ≤0. In view of (5.50), for every n ∈N, ui,kn is
the D-projection of xn onto Gi(xkn, x∗
y ∈X | ⟨xkn −y, x∗
n⟩≥gi(xkn)
for some x∗
n ∈∂gi(xkn). Since ui,kn ∈Gi(xkn, x∗
n), we have
∥ui,kn −xkn∥≥dGi(xkn,x∗
i (xkn)/∥x∗
otherwise,
i = max{0, gi} and the last equality follows from [80, Lemma I.1.2].
Since (xkn)n∈N lies in C, (x∗
n)n∈N is bounded by Condition 5.16(ii). Therefore, ui,kn −xkn →0 implies g+
i (xkn) →0. However, as g+
is convex and
lower semicontinuous, it is weak lower semicontinuous and thus g+
i (xkn) = 0. We conclude gi(x) ≤0.
(2) i ∈I(2).
We must show (x, 0) ∈gr Ai.
For every n ∈N, (5.50) yields
ui,kn ∈(∇f + γi,knAi)−1
and we deﬁne
n = ∇f(xkn) −∇f(ui,kn)
(ui,kn, u∗
n∈N lies in gr Ai and ui,kn −xkn →0 ⇒ui,kn ⇀x. If
for all n suﬃciently large we have xkn = ui,kn, then by Proposition 3.8(iii) the
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
tail of (xkn)n∈N is in the weakly closed set A−1
i 0 and therefore (x, 0) ∈gr Ai.
Otherwise, we can extract a subsequence (xkln)n∈N such that, for all n ∈N,
xkln ̸= ui,kln. Since, on the one hand, (xkln)n∈N and (ui,kln)n∈N lie in C and,
on the other hand,
(∀n ∈N) ∥ui,kln −xkln∥≥
ui,kln −xkln, ∇f(ui,kln) −∇f(xkln)
∥∇f(ui,kln) −∇f(xkln)∥
it follows from Condition 5.3(ii), (5.53), and the inequality infn∈N γi,kln ≥εi
that ui,kln −xkln →0 ⇒∇f(ui,kln) −∇f(xkln) →0 ⇒u∗
ln →0. Finally,
since Ai is maximal monotone, gr Ai is sequentially closed in the weak ×
strong topology of X × X ∗and we conclude that (x, 0) ∈gr Ai, as required.
(3) i ∈I(3).
We must show ϕi(x) = inf ϕi(X), i.e., (x, 0) ∈gr ∂ϕi.
ϕ is a proper lower semicontinuous convex function, Ai = ∂ϕi is maximal
monotone [79, section 29] and 3∗-monotone by Lemma 3.10(iv), and, in view
of Propositions 3.22(ii)(a) and 3.23(v)(b), the claim follows from case (2).
(4) i ∈I(4). We must show x ∈Fix Ti. This follows at once from (5.49).
It remains to show that in each instance (i)–(iv), i is an index of demicompact regularity. Henceforth, (xkn)n∈N is a suborbit such that i ∈
n∈N Ikn and ui,kn −xkn →0.
By (5.11), we must show S(xkn)n∈N ̸= Ø.
(i) Arguing as in case (1), we obtain
lim gi(xkn) ≤0. Therefore, the tail of (xkn)n∈N lies in the compact set C ∩lev≤η gi,
whence S(xkn)n∈N ̸= Ø. (ii) It follows from (3.16) that for every n ∈N
ui,kn ∈C ⊂int dom f,
ui,kn ∈ran(∇f + γi,knAi)−1 ◦∇f ⊂dom ∇f ∩dom Ai = int dom f ∩dom Ai.
Therefore, (ui,kn)n∈N lies in the compact set C ∩dom Ai, whence S(ui,kn)n∈N ̸= Ø.
Since ui,kn −xkn →0, we conclude S(xkn)n∈N ̸= Ø. (iii) As in case (3), this is a
special case of (ii). (iv) This is clear from (5.51).
Theorem 5.18 produces convergence results for various new block-iterative parallel schemes for solving problems, including solving convex inequalities (I(2) = I(3) =
I(4) = Ø), ﬁnding common zeros (I(1) = I(3) = I(4) = Ø), solving systems of variational inequalities (I(1) = I(2) = I(4) = Ø), ﬁnding common ﬁxed points (I(1) =
I(2) = I(3) = Ø), and combinations of these. Note that D-projection methods are
also captured by Theorem 5.18 since, in view of Proposition 3.32(ii)(c), one can take,
for instance, Ti to be the D-projector onto Si if i ∈I(4) in (5.50).
Naturally, our framework also encompasses relaxed sequential algorithms, which
are obtained by taking (In)n∈N to be a sequence of singletons, as in the following
Example 5.19. Suppose X = RN, (Si)1≤i≤m is a (ﬁnite) family of half-spaces
with D-projectors (Pi)1≤i≤m, and, for every n ∈N, In = {n (mod m) + 1} and
Ti,n = Pi. Then Algorithm 5.1 reduces to the relaxed D-projection method of .
In the case of unrelaxed sequential algorithms, our working assumptions can be
loosened. This is discussed next.
5.6. Unrelaxed sequential algorithms. Algorithm 5.1 can be specialized to
an unrelaxed sequential algorithm for solving the convex feasibility problem (5.2).
Indeed, suppose that at each iteration n only one index, say i(n), is retained and
λn = 1. Then Algorithm 5.1➇becomes
y ∈X | ⟨y −un, ∇f(xn) −∇f(un)⟩≤0
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
H. H. BAUSCHKE, J. M. BORWEIN, AND P. L. COMBETTES
where un ∈Tnxn for some Tn ∈B such that Si(n)∩int dom f ⊂Fix Tn. Consequently,
since by Corollary 3.35(ii) PHnxn = un, Algorithm 5.1 can be rewritten as follows.
Algorithm 5.20. Starting with x0 ∈int dom f, take at every iteration n
➀an index i(n) ∈I,
➁an operator Tn in B such that Si(n) ∩int dom f ⊂Fix Tn.
Then select xn+1 ∈Tnxn.
In this context, Deﬁnition 5.2 takes the following form.
Definition 5.21. Algorithm 5.20 is
• focusing if for every bounded suborbit (xkn)n∈N it generates and every index
(∀n ∈N) i = i(kn),
xkn+1 −xkn →0
• demicompactly regular if there exists i ∈I, called an index of demicompact
regularity, such that for every bounded suborbit (xkn)n∈N it generates,
(∀n ∈N) i = i(kn),
xkn+1 −xkn →0
S(xkn)n∈N ̸= Ø.
Removing item (ii) from Condition 5.3 yields the following set of assumptions for
the unrelaxed sequential case.
Condition 5.22.
For some z ∈dom f ∩
i∈I Si, C = lev≤D(z,x0) D(z, ·) is
Condition 5.23. (∀i ∈I)(∃Mi ∈N∖{0})(∀n ∈N) i ∈
i(n), . . . , i(n+Mi−1)
We now show that Algorithm 5.20 converges under this reduced set of assumptions.
Theorem 5.24. Suppose that Conditions 4.3, 4.4, 5.22, and 5.23 are satisﬁed
and that Algorithm 5.20 is focusing. Then the following statements hold true for every
orbit (xn)n∈N generated by Algorithm 5.20:
(i) (xn)n∈N converges weakly to a point x ∈S.
(ii) If the weak limit x from (i) belongs to int dom f and the algorithm is demicompactly regular, then (xn)n∈N converges strongly.
Proof. In the proof of Theorem 5.7, note that Condition 5.3(ii) is used only to
obtain (5.34), i.e., in the present context, xpn+1 −xpn →0. However, this property
follows directly from(5.33).
As an example, we revisit Bregman’s original cyclic projection method (1.2). (See
[1, Thm. 3.1] for a special case.)
Corollary 5.25. Suppose that Conditions 4.3 and 4.4 are satisﬁed, that I =
{1, . . . , m}, and that C = lev≤D(z,x0) D(z, ·) is bounded for some z ∈dom f ∩
Let (Pi)1≤i≤m be the D-projectors of (Si)1≤i≤m.
Then the following
statements hold true for every orbit (xn)n∈N generated by (1.2):
(i) (xn)n∈N converges weakly to a point x ∈dom f ∩
(ii) If the weak limit x from (i) belongs to int dom f and C ∩Si is relatively compact (e.g., Si is boundedly compact) for some i ∈{1, . . . , m}, then (xn)n∈N
converges strongly.
Proof. In view of Corollary 3.35(i), (1.2) is a special realization of Algorithm 5.20
with (∀n ∈N) Tn = Pn (mod m)+1 (single-valued) and λn = 1. In addition, the index
Downloaded 06/03/13 to 134.148.10.13. Redistribution subject to SIAM license or copyright; see 
BREGMAN MONOTONE OPTIMIZATION ALGORITHMS
control rule i: n →n (mod m) + 1 complies with Condition 5.23. On the other hand,
algorithm (1.2) is focusing, as a direct consequence of the weak closedness of the sets
(Sj)1≤j≤m. Finally, i is an index of demicompact regularity since (xnm+i)n∈N lies in
C ∩Si. The announced results therefore follow from Theorem 5.24.
Remark 5.26. Throughout section 5, Legendreness has been imposed on f. This
property has been shown to provide a rich and convenient framework in which our
results could be derived in a uniﬁed manner.
Further results can nonetheless be
obtained from the analysis of sections 3 and 4 for functions which are not Legendre
at the expense of more technical assumptions.
Acknowledgments. We wish to thank Dan Butnariu and Yair Censor for sending us , Constantin Z˘alinescu for sending us , and especially Jon
Vanderwerﬀfor his help in the derivation of Example 5.11. Two anonymous referees
made several helpful comments and suggestions, which led to improvements over the
originally submitted version.