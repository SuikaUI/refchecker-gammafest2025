Journal of Statistical Planning and Inference 137 669–683
www.elsevier.com/locate/jspi
On bagging and nonlinear estimation
Jerome H. Friedmana,∗, Peter Hallb
aStatistics Department, Stanford University, Stanford, CA 94305, USA
bCSIRO Mathematical Sciences and Centre for Mathematics and its Application, Australian National University, Canberra ACT 0200, Australia
Available online 4 August 2006
We propose an elementary model for the way in which stochastic perturbations of a statistical objective function, such as a
negative log-likelihood, produce excessive nonlinear variation of the resulting estimator. Theory for the model is transparently
simple, and is used to provide new insight into the main factors that affect performance of bagging. In particular, it is shown that
if the perturbations are sufﬁciently symmetric then bagging will not signiﬁcantly increase bias; and if the perturbations also offer
opportunities for cancellation then bagging will reduce variance. For the ﬁrst property it is sufﬁcient that the third derivative of
a perturbation vanish locally, and for the second, that second and fourth derivatives have opposite signs. Functions that satisfy
these conditions resemble sinusoids. Therefore, our results imply that bagging will reduce the nonlinear variation, as measured by
either variance or mean-squared error, produced in an estimator by sinusoid-like, stochastic perturbations of the objective function.
Analysis of our simple model also suggests relationships between the results obtained using different with-replacement and withoutreplacement bagging schemes. We simulate regression trees in settings that are far more complex than those explicitly addressed by
the model, and ﬁnd that these relationships are generally borne out.
© 2006 Published by Elsevier B.V.
MSC: Primary 62G09; secondary 62E20
Keywords: Bias; Bootstrap; Half-sampling; Regression tree; Variance reduction; With-replacement sampling; Without-replacement sampling
1. Introduction
Bagging was introduced by Breiman as a means for improving the accuracy of estimators of functions (x)
of data x = {x1, . . . , xN},
ˆ(x) = arg min
(x)∈ L((x)).
Here,  denotes a function class representable by the estimator, such as neural networks or decision trees. The objective
function L((x)) is a data-based estimate of the expected value of some functional such as negative log-likelihood or
other loss function. ‘Bagging’ involves repeatedly drawing random resamples xb of the data, and either optimizing the
value of L((xb)) averaged over the resamples, or averaging the resample values of ˆ. That is, we deﬁne the bagged
∗Corresponding author.
E-mail addresses: (J.H. Friedman), (P. Hall).
0378-3758/$ - see front matter © 2006 Published by Elsevier B.V.
doi:10.1016/j.jspi.2006.06.002
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
estimator to be either
ˆbagg(x) = arg min
ˆbagg(x) = 1
where ˆ(xb) is the version of ˆ(x) computed from the bth resample. Since its introduction, considerable evidence has
been accumulated that clearly demonstrates the effectiveness of bagging some estimators, such as decision trees and
neural networks; it is now routinely used. The underlying reasons for its success have been less clear.
Bagging the objective function L() often has the effect of reducing stochastic ‘bumpiness’from which the function
tends to suffer. As a result the bagged objective function is better approximated locally, in the neighbourhood of
the true parameter value, by a paraboloid whose optimum occurs relatively close to the linear component of the
statistic. Likewise, the operation of bagging the estimator itself often reduces variability by averaging out stochastic
ﬂuctuations caused by bumps; see for example Breiman . These empirical results raise the question of whether the
performance of bagging can be understood relatively simply by theoretically modelling, in an elementary and intuitive
but nevertheless mathematically rigorous way, the effects of stochastic bumps on an essentially quadratic objective
function. In the present paper we suggest such a model, describe the insights that its analysis provides into bagging, and
relate that to the results of numerical experiments in the more complex, and less readily accessible, setting of regression
In particular, in Section 2 we give an elementary argument that points to why bagging works in contexts that are
substantially more complex than that of the model on which the argument is based. Of course, speciﬁc problems can be
addressed in greater detail.That is the route taken, to good effect, by Bühlmann andYu in their more sophisticated
account, based on cube-root asymptotics, of decision trees and related topics. But we argue that the general principles
behind the performance of bagging are less elaborate, and may be understood more readily. To make this point we
construct a naive model for a quadratic surface with randomly located bumps, and discuss the performance of both
types of bagging (see (1.1)) in reducing the effects that bumpiness has on performance of the estimator. From this
viewpoint, two main requirements emerge as the key to reducing variance and mean-squared error by bagging.
First, bagging reduces variance if the shapes of the bumps provide opportunities for cancellation. Sinusoidal bumps
are a good example—shifting the location of a sinusoid (by computing either the objective function or the estimator
for different resamples), and forming the average over the different results, affords considerable scope for reducing
variability. On the other hand, if the stochastic ‘contamination’ that causes the objective function to depart from a
quadratic is in the form of a non-oscillatory function, then surprisingly, averaging over it will primarily increase the
variance, not the bias, of the bagged estimator.
Second, if the contaminating bumps are not symmetric in shape then bagging will introduce a new bias term, and
may increase variance as well. Intuitively, this property is to be expected. Bagging works by producing additional noise
(conditional on the data), and then averaging over it; asymmetries in the bumps result in the added noise being averaged
in a biased way, giving rise to the new bias term.
The remarks just above call to mind the known result that bagging can substantially increase the bias of nonlinear
estimators, not least because it adds an extra quadratic term. In particular, if we take the square of the sample mean ¯X
to be an estimator of the square of the population mean , then its conventional bagged form (i.e. the expected value
of the squared bootstrap mean ( ¯X∗)2, conditional on the data) has virtually twice the bias of ¯X2, since
E{( ¯X∗)2} −2 = (2 −N−1){E( ¯X2) −2},
where N denotes sample size. If the noisy perturbations that exacerbate variability of an estimator have a signiﬁcant
quadratic component (deriving from a cubic term in the corresponding contaminations of the objective function) then
the perturbations will make themselves felt as inﬂated bias and possibly increased variance in the bagged estimator,
potentially with dire consequences. Remarks made in the previous paragraph, about the need for the high-variability
contaminating perturbations to be reasonably symmetric, reﬂect this result. Indeed, our mathematical arguments in
Section 4 show that the requisite condition for symmetry is precisely that local cubic terms in contamination of the
objective function vanish, or equivalently, the contaminants add no new quadratic terms to the estimator.
The latter result links our work to recent research of Buja and Steutzle , who studied properties of bagged
quadratics and related functions. One of their results, that bagging can increase mean-squared error in part by increasing
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
bias, is effectively equivalent to our observation that asymmetric departures from linearity at the level of quadratic terms
in the estimator, or cubic terms in the objective function, can result in poor performance of the bagged estimator.
One way of presenting these results is in the form of Taylor expansions of estimators, using them to elucidate the
properties discussed above. However, the properties follow much more transparently from analysis of the estimating
equation in the presence of stochastic contamination of the objective function. There, the variance-reducing effect of
bagging when it works, or variance-inﬂating effect when bagging fails, appear as factors that multiply the term in the
equation that adds nonlinear variability to the estimator. Provided the contamination adds no extra quadratic terms
to the estimating equation (i.e. adds no cubic terms to the objective function), the factor is less than 1 when bagging
works. And it is greater than 1 when bagging fails. We shall, however, also develop theory in more general settings; see
Section 4. There we shall use a detailed Taylor-expansion approach to conﬁrm results that are intuitively clear from
the estimating function viewpoint.
The simple theoretical model on which our results are based is introduced in Section 2.1, and our main conclusions
are drawn there.They foreshadow results about relative performances of different approaches to bagging, based on withreplacement and without-replacement resampling, respectively. These techniques are discussed in Section 2.2, where
they are linked to the conclusions in Section 2.1. It is argued, on the basis of on our theory, that certain different bagging
approaches (for example, n-out-of-n with-replacement bagging, and 1
2n-out-of-n without-replacement bagging) can be
expected to perform similarly.
Section 3 takes this matter up in a more complex setting, by treating substantially more sophisticated problems than
can be addressed explicitly at the level of Section 2. It is shown there that some of the main conclusions of our theoretical
analysis are borne out by numerical work in the case of regression trees. Our theoretical model for the effects of bagging
on estimator performance is admittedly a toy one, and does not correspond explicitly to more complex settings were
bagging is generally used. But there is nevertheless signiﬁcant connection between the levels of performance that our
model suggests, and those observed numerically.
With-replacement resampling is of course in the spirit of the contemporary bootstrap. Without-replacement methods
were employed in early approaches to resampling, for example those of Mahalanobis and Hartigan . McCarthy was an early proponent of without-replacement resampling using half-samples. Efron 
discussed related issues, including the estimation of variance by half-sampling.
2. Interpretations of bagging
2.1. Noisy perturbations of objective functions
The theoretical arguments given here and in Section 4 require only Taylor expansion, and so there are no more than
notational differences between one- and arbitrary-dimensional cases. The numerical study in Section 3 will explore
high-dimensional settings. In the present section, to make our technical manipulations more transparent, we shall
conﬁne attention to one dimension.
In an ideal, no-noise case the objective function L will be a simple quadratic having its minimum at the true
parameter value. Without loss of generality the true value is 0, and L()= 1
22. The ‘estimator’of , ˆ=arg min L(),
is then of course 0. Suppose, however, that the simple quadratic is contaminated by a noisy, ‘bumpy’ function :
22 +(). If the bumps in  occur consistently in the same place then the effect of the contamination is often
more systematic than stochastic, and bagging cannot be expected to be of much help.
We shall instead propose a simple model for  in which the bump locations vary randomly:
() = ( + Z),
where Z, a function of the data X, is a random variable. We have included stochastic variability in the contamination
term , and not in the idealized quadratic term 1
22, because we are primarily modelling the case where stochastic
variability of the nonlinear component of the estimator ˆ tends to swamp that of the linear component.
The conventional estimator of  is thus
ˆ = arg min
22 + ( + Z)},
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
i.e. ˆ is deﬁned by solving the estimating equation
 + ′( + Z) = 0.
One form of the bagged estimator ˆbagg is obtained by minimizing, with respect to , the function
22 + ( + Z∗)|X} = 1
22 + E{( + Z∗)|X},
where Z∗is the version of Z computed from a resample and X denotes the data. (Here, we have effectively taken the
number of resamples B, in our discussion in Section 1, to be inﬁnite.) That is,  = ˆbagg solves
 + E{′( + Z∗)|X} = 0.
The other approach to bagging takes ˆ∗to be the solution of  + ′( + Z∗) = 0, and then puts ˆbagg = E(ˆ∗|X).
The approaches have virtually identical properties, but theory for the ﬁrst is more transparent, so we devote more of
our attention to it. Results for the second approach will be outlined in this Section, with mathematical details given in
Section 4.2.
Assuming the perturbation Z has relatively small variance, we may Taylor-expand the estimating equation (2.2):
ˆbagg solves
 + ′( + Z) + 1
2 E{(∗)2|X} ′′′( + Z) + 1
6 E{(∗)3|X} (4)( + Z) + · · · ,
where ∗= Z∗−Z and we have taken E(∗|X) to be 0. Suppose initially that the function  is sinusoidal, so that
equally spaced bumps—with randomly varying locations—are added to the quadratic. The tendency of bagging to
‘cancel out’ bumps can be seen particularly clearly in this case, since now ′ = −′′′ and so (2.3) reduces to:
2 ˆ2) ′( + Z) + 1
6 E{(∗)3|X} (4)( + Z) + · · · ,
where ˆ2 = E{(∗)2|X}.
Assuming that the noise variable Z is something like a mean of n variables, perhaps each with relatively large variance,
the third conditional moment E{(∗)3|X} will be an order of magnitude smaller than ˆ2. (It will be of order n−2 if Z
is the aforesaid mean, compared with order n−1 for ˆ2.) Therefore, the cubic term in (2.4) will be negligible, and so
too will be the quartic, etc. (In Section 4.1 we shall make this mathematically explicit.) Thus, to a ﬁrst approximation
the estimating equation has changed from (2.1) to
2 ˆ2) ′( + Z) = 0.
The critical feature of (2.5) is of course the way in which bagging has reduced the main effect of the bumpy function,
from ′(+Z) in the unbagged estimating equation (2.1) to (1−1
2 ˆ2) ′(+Z) after bagging. This results in improved
performance, by reducing both variance and mean-squared error relative to those of the unbagged estimator ˆ. See
Section 4.1 for a proof.
Eq. (2.5) also neatly quantiﬁes the way in which the main effects of bagging may be predicted in terms of the size of
the empirical variance term, ˆ2. In particular, if we bag in two quite different ways, for example, using with-replacement
sampling on the one hand and without-replacement sampling on the other, with different resample sizes in the two
respective cases; but if the value of ˆ2 is similar under both regimes; then the extents to which bagging reduces the
effects of noisy bumps in the objective function may be predicted to be similar too, provided the stochastic variability
associated with the bumpy function is not too great.
We make the latter caveat only because (2.4) may not be explicitly valid if Z is too highly variable. However, for bump
functions such as the sinusoid, the variability of ˆ will still be reduced, as may be seen from numerical experiments.
Moreover, even our more general results (discussed later in this section) do not require the variance of Z to converge to
0 in order to be valid; they hold if the variance of Z is sufﬁciently small, but ﬁxed. It is therefore incorrect to describe
these effects as second-order ones. Recall that we are modelling problems where nonlinear perturbations of estimators
are of ﬁrst order. In particular, we have added the ‘contaminant’(+Z), which produces those perturbations, directly
to a pure quadratic, which produces a noiseless linear component.
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
The arguments above are of course founded on the assumption that ′ = −′′′. If ′′′ resembles ′, rather than the
negative of the latter, then the reverse conclusion will obtain: bagging will impair rather than enhance performance.
Lying at the heart of this analysis is the question of whether the bump function  tends to self-correct for large
perturbations from the origin, or whether large perturbations are actually reinforced by . Note that bagging adds extra
noise to the argument of ′, since it replaces ′( + Z) by ′( + Z + ∗). If the structure of  is such that, after
taking conditional expectation, the effects of added noise tend to cancel those of existing noise, then bagging will tend
to improve performance. Otherwise the impact of bagging may be deleterious, and both variance and mean-squared
error can be increased.
To better appreciate what is going on here, let us assume for simplicity that  is an even function, so that it may be
represented by
() = (0) + 1
6 4 + · · ·
in the neighbourhood of the origin, where  = ′′(0) and = (4)(0). To simplify discussion, suppose  > −1 and
 ̸= 0. (Indeed, =0 implies that even the unbagged estimator is superefﬁcient, while  <−1 implies that the noiseless
objective function 1
22 + () is concave downward, rather than upward, near the origin. In such cases, and often too
for  = −1, we can no longer regard  = 0 as the true parameter value.) If  is a sinusoid then  and are of opposite
signs, and this means that when () strays too far from (0) the quartic term in (2.6) tugs it back, tending to reduce
the value of () relative to what it would be if  and were of the same sign.
For general bump functions , this ‘self-correcting’ property turns out to be fundamental: if the signs of  and
are different then adding extra noise to the bump function tends to smooth out existing perturbations, and as a result
bagging reduces both variance and mean-squared error; but if the signs are the same then, for the analogous reason,
bagging makes things worse. Moreover, this is true for bagging either the estimating equation or the estimator itself.
Provided the signs of  and are different, the extent to which bagging reduces variance is virtually proportional to
the (expected) value of ˆ2, just as is suggested by (2.5) in the simplest case where  is a sinusoid.Again this is true both
for bagging the estimating equation and for bagging the estimator. See Section 4 for a detailed argument. Therefore, ˆ2
is quite generally the key to the amount by which bagging reduces variance. Numerical work in Section 3 will use this
property to illustrate the link between our toy theory and more complex settings to which bagging is generally applied.
Matters are somewhat more complex if the function  is asymmetric, in particular, if ′′′(0) ̸= 0. There the operation
of adding extra noise (in the form of ∗) and averaging over it tends to introduce bias, in addition to the impact it has on
variability. As noted in Section 1, this result is to be expected. Once again, it holds for bagging the estimating equation
and for bagging the estimator. It will be discussed in mathematical detail in Section 4.
More generally, these results apply to more complex models for bumps on the objective function, for example,
the model L() = 1
i i( + Zi) for potentially correlated random variables Zi (provided the bootstrap step
correctly captures the main effects of the dependence structure). To appreciate why, note that if this were the model
then, following the argument leading from (2.1) down to (2.5), we would replace (2.5) by
i( + Zi) = 0,
where the factor 1 −1
i reduces the main effect of the bumpy function from ′
i( + Zi) before bagging to (1 −
i( + Zi) after. The keys to obtaining improved performance using bagging are: (a) the bumps should have the
self-correcting property, so as to reduce variance, and (b) they should be reasonably symmetric, so as not to increase
2.2. Bagging with or without replacement
The method of m-out-of-n with-replacement bagging involves drawing a resample X∗, of size m⩽n, by sampling
with replacement from a data set X. In without-replacement bagging a resample X†, in this case a subsample, of size
m⩽n −1 is drawn by sampling without replacement from X.
To appreciate the effects these different methods have on variability of the bagging step, take Z = n−1 
i Xi where
X={X1, . . . , Xn}isacollectionofindependentandidenticallydistributedrandomvariables.Let ˆ
i (Xi−¯X)2,
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
where ¯X = n−1 
i Xi, denote the variance of X. Write ¯X∗and ¯X† for the means of the resamples X∗and X†,
respectively. Put
m = n/m⩾1. We shall show shortly that if
as n →∞, where 1⩽
> 1 in the
case of without-replacement bagging, then
E{( ¯X∗−¯X)2|X} = n−1
E{( ¯X† −¯X)2|X} = n−1 (
2 + · · · ,
where the ‘remainder’ denoted by ‘. . .’ represents higher-order terms.
The relevance of these results is that if we are using m-out-of-n with-replacement bagging then the value of ˆ2,
introduced in Section 2.1, equals the left-hand side of the ﬁrst formula at (2.7); and if we are using m-out-of-n withoutreplacement bagging then it equals the left-hand side of the second formula. It therefore follows from (2.7), and the
fact that the extent of variance reduction is virtually proportional to ˆ2, that if
m2 −1 for two resample sizes
m1 and m2, then the variability of bagging algorithms using m1-out-of-n with-replacement sampling, and m2-out-of-n
without-replacement sampling, may be expected to be similar. In particular, taking m ∼1
2n in without-replacement
bagging will tend to produce an estimator where the effect of the quadratic term is virtually identical to that in n-out-of-n
with-replacement bagging. Numerical illustrations of this relationship will be provided in Section 3, in settings well
beyond the simpliﬁed one of the model being considered here.
There is another, more intuitive interpretation of why n-out-of-n with-replacement bagging, and 1
2n-out-of-n withoutreplacement bagging, perform similarly. Note that the effective size of an n-out-of-n with-replacement bootstrap resample X∗, in terms of the amount of information it contains, is given by the ratio
where Ni denotes the number of times the ith data value Xi is repeated in X∗. In particular, the variance of the mean
of Ni copies of independent and identically distributed random variables Yi, for 1⩽i ⩽n, is very nearly equal to twice
the variance of the mean of the n independent random variables themselves, for large n.
Finally, we derive the second part of (2.7); the ﬁrst part is straightforward. Deﬁne
m = (m −1)/(n −1). Provided
2⩽m⩽n −1 we obtain, by expressing ( ¯X† −¯X)2 as a double series,
E{( ¯X† −¯X)2 |X} = m−2 [m(m −1) E{(X†
1 −¯X) (X†
2 −¯X)|X} + m E{(X†
1 −¯X)2|X}]
(Xi1 −¯X) (Xi2 −¯X) + m
= m−1 (1 −
The exact form of the second result at (2.7) has
m −1 replaced by
m −(1 −m−1)(1 −n−1)−1, in which the ‘. . .’
terms in the second formula at (2.7) may be dropped. Since n ∼
m then the difference between the exact form and
the stated one is of smaller order than the ﬁrst term on the right-hand side in the second formula.
3. Numerical experiments
In order to gain further insights we present the results of several simulation experiments. All involve estimating a
function of a multivariate argument in noisy settings using regression trees. Regression trees 
represent a very nonlinear estimation method. Data {yi, xi}n
1 were generated according to the model
yi = f (xi) + 
with each xi independently generated from a 10-dimensional uniform distribution, xi∽U10 . Each
i was randomly
drawn from a standard normal distribution. Three ‘target’ functions f (x) were considered:
• constant: f (x) = 0,  = 1,
• piecewise-constant: f (x) =
j=11(xj ⩾0.13),  = 0.5,
• linear: f (x) = 5
j=1j · xj,  = 3.
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
Without Rep.
Constant: N=5000, nt=50
Fig. 1.Average variance as a function of sampling fraction m/n without (left frame) and with (right frame) replacement, for n=5000, with a constant
target. The variance of the unbagged estimate is 0.0856.
The truncation point 0.13 for the piecewise-constant function is chosen since (1 −0.13)5 = 1
2, to two signiﬁcant
ﬁgures. This ensures that the piecewise-constant target equals 1 for half its volume, and equals zero for the other half.
Note that the last two targets are functions of only ﬁve of the 10 predictor variables, so that the others represent
irrelevant ‘noise’ variables. Each generated data set was used to induce a 50-terminal node regression tree, producing
a corresponding function estimate ˆf (x). Average bias-squared
B2 = Ex{f (x) −Eyx ˆf (x)}2
and average variance
V = Eyx{ ˆf (x) −Eyx ˆf (x)}2
were computed by averaging over 100 independent trials for each experiment. In all experiments B = 50 bagging
iterations were employed.
3.1. Constant target
In this setting we study the effect of various forms of bagging on variance alone, since here regression trees are
unbiased. Fig. 1 shows the average variance (3.3) of the bagged regression tree estimator ˆf (x) as a function of the
sampling fraction m/n, without (left frame) and with (right frame) replacement. Training samples of n = 5000 were
used to induce the trees. The variance of the original ‘unbagged’ estimate was 0.0856. Thus, for all sampling fractions
shown, both types of bagging dramatically reduce variance. The optimal sampling fraction is approximately 0.6 for
without replacement sampling and 1.0 for sampling with replacement. However, the curves are fairly ﬂat near their
optima, so that a choice is not critical. Note that smaller fractions require less computation.
These results reﬂect the theoretical conclusions reached in Section 2, in that (a) the variance of without-replacement
bagging is approximately a U-shaped function of the sampling fraction, (b) variance in the with-replacement case is
a decreasing function of the sampling fraction, and (c) a sampling fraction of 1
2 in the case of without-replacement
bagging produces almost the same variance as a sampling fraction of 1 in the with-replacement case.
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
bias-squared
Piecewise-constant: N =5000, nt = 50, WO/R
Fig. 2. Average bias-squared (left frame) and variance (right frame) as a function of sampling fraction m/n, for without-replacement sampling and
n = 5000, with a piecewise-constant target. Unbagged trees have a bias-squared of 0.0025 and variance 0.0863.
3.2. Piecewise-constant target
This represents a situation in which regression trees have the potential to be unbiased because their approximations
ˆf (x) are piecewise-constant . The target f (x) has the value 1 in the upper corner of a ﬁve-dimensional hyper-cube. The
responses {yi}n
1 are roughly evenly divided between those that have Eyi = 1, and those with Eyi = 0. A six terminal
node regression tree with ﬁve optimally placed splits can exactly represent the target.
Fig. 2 shows the bias-squared (3.2) (left frame) and variance (3.3) (right frame) of the without-replacement bagged
estimate ˆf (x) as a function of the sampling fraction. These values are reported in units of the global target variance
Ex{f (x) −Exf (x)}2. Training samples of n = 5000 were used. Although the target lies within the space of the
approximating functions, one sees a small bias-squared that decreases with increasing sampling fraction. Unbagged
trees have a bias-squared of 0.0025 and variance 0.0863. This bias is due to the fact that the smaller training samples limit the size of the induced trees. Fig. 3 shows the corresponding results for n = 500. Here, unbagged trees
had a bias-squared of 0.0325 and variance 0.2440. For the bagged trees, the bias-squared is seen to be comparable to the variance, and the latter increases monotonically with the sampling fraction. This monotonic increase of
variance with sampling fraction also occurs for the constant target with small training samples (not shown). This
effect is also evident with the larger n = 5000 sample (Fig. 2) in that the variance is minimized for smaller fractions than with the constant target (Fig. 1, left frame). Thus, with bagging there can be a bias-variance trade-off in
choosing the sampling fraction m/n. As with any ‘meta’-parameter that controls bias-variance trade-off, an optimal
value can be estimated by minimizing an estimate of prediction error through cross-validation or a left out ‘test’
For completeness Figs. 4 and 5 show the corresponding results for n = 5000 and 500, respectively, sampling with
replacement. One sees results similar to those for sampling without replacement in the interval m/n ∈[0.2, 0.5] of the
The variance plots in Figs. 4 and 5 again lend support to the theoretical conclusions reached in Section 2. In
particular, properties (a)–(c) noted in the last paragraph of Section 3.1 apply here, too, except that in the case of
with-replacement bagging, variance is not a decreasing function of sampling fraction when the latter is large. Note,
however, that in the case n = 500, shown in Fig. 5, variance is an increasing function of sampling fraction, but that by
increasing sample size to the ‘more asymptotic’ value n = 5000 (Fig. 4) the function has almost, but not quite, turned
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
bias-squared
Piecewise-constant: N=500, nt =50, WO/R
Fig. 3. Average bias-squared (left frame) and variance (right frame) as a function of sampling fraction m/n, for without-replacement sampling and
n = 500, with a piecewise-constant target. Unbagged trees have a bias-squared of 0.0325 and variance 0.2440.
bias-squared
Piecewise-constant: N=5000, nt=50, W/R
Fig. 4. Average bias-squared (left frame) and variance (right frame) as a function of sampling fraction m/n, for with-replacement sampling and
n = 5000, with a piecewise-constant target. Unbagged trees have a bias-squared of 0.0025 and variance 0.0863.
3.3. Linear target
A linear function represents one of the most difﬁcult targets for approximation by regression trees. It does not lie
within the space of piecewise-constant functions and its contours are everywhere oblique to the coordinate axes. Fig. 6
shows the bias-squared (3.2) (left frame) and variance (3.3) (right frame) of the without-replacement bagged estimate
ˆf (x) as a function of the sampling fraction for n=5000, again reported in units of the global target variance. Unbagged
trees have a bias-squared of 0.0402 and variance 0.2494. Here one sees the dramatic super-linear increase of variance
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
bias-squared
Piecewise constant: N=500, nt=50, W/R
Fig. 5. Average bias-squared (left frame) and variance (right frame) as a function of sampling fraction m/n, for with-replacement sampling and
n = 500, with a piecewise-constant target. Unbagged trees have a bias-squared of 0.0325 and variance 0.2440.
bias-squared
Linear: N=5000, nt=50, WO/R
Fig. 6. Average bias-squared (left frame) and variance (right frame) as a function of sampling fraction m/n, for without-replacement sampling and
n = 5000, with a linear target. Unbagged trees have a bias-squared of 0.0402 and variance 0.2494.
with sampling fraction characteristic of the smaller (n = 500) training samples above. Also, the bias-squared increases
with sampling fraction. Here, bagging is reducing bias-squared as well as variance, with smaller sampling fractions
producing the most improvement in both. Fig. 7 shows the corresponding plot for sampling with replacement. Again
the results are similar to the left half (fraction m/n ⩽0.5) of the without replacement results.
Fig. 8 shows without-replacement results for much larger training samples n=50 000. Here, unbagged trees have an
average bias-squared of 0.0775 and average variance of 0.0821. Comparing to the corresponding (unbagged) n = 5000
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
bias-squared
Linear: N=5000, nt=50, W/R
Fig. 7. Average bias-squared (left frame) and variance (right frame) as a function of sampling fraction m/n, for with-replacement sampling and
n = 5000, with a linear target. Unbagged trees have a bias-squared of 0.0402 and variance 0.2494.
bias-squared
Linear: N=50000, nt=50, WO/R
Fig. 8. Average bias-squared (left frame) and variance (right frame) as a function of sampling fraction m/n, for without-replacement sampling and
n = 50 000, with a linear target. Unbagged trees have an average bias-squared of 0.0775 and average variance of 0.0821.
results above, one sees that using the larger training sample reduces variance, but by a factor of about 1/
10. However,
the bias-squared has increased by almost a factor of two. The dependence of bias-squared and variance on sampling
fraction is similar to that for n = 5000 shown in Fig. 6; they both decrease with decreasing sampling fraction m/n.
However, here the bias-squared dominates mean-squared error of the bagged trees.
Although perhaps counter intuitive, the increase in bias-squared with larger samples for ﬁxed-sized regression trees
is easy to understand. Fig. 9 illustrates the concept in an idealized setting. Shown is a hypothetical asymptotic (n = ∞)
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
likelihood
Fig. 9. Hypothetical asymptotic likelihood and an estimator that can only realize a discrete set of values.
likelihood as a function of a parameter  with maximum at 0 = 0. Suppose an estimator ˆ that can realize a discrete
set of values ˆ ∈{ˆi} = {−0.8, −0.3, 0.2, 0.7} (hash marks), none of which is equal to the population parameter value
0 = 0. In the asymptotic limit the value of the estimate is ˆ∞= 0.2, with a bias-squared of 0.04. However, with ﬁnite
samples of size n, the expected value of ˆ is
with pi(n) being the probability that a likelihood based on a sample of size n is maximized at ˆi. The dispersion of
the pi(n)-values will tend to increase with decreasing n. This enlarges the set of values that can be realized by (3.4),
enabling it to achieve values closer to 0, thereby potentially decreasing the bias.
An L-terminal node regression tree ˆfL(x) cannot get arbitrarily close to a continuous function f (x) such as the linear
target used here. Thus, there is an asymptotic average bias-squared characteristic of the closest possible L-terminal node
tree. As the sample size is reduced, the distance to the target of the expected approximation ¯fL(x) = En ˆfL(x) becomes
smaller, reducing bias-squared. Of course, this expected approximation ¯fL(x) is itself not realizable as a (ﬁnite sized)
regression tree. Bagging reduces bias-squared in such situations simply by reducing the sample size; each bagged tree
is trained on a subset of the complete training set (2.8). The averaging aspect of bagging has no effect on bias-squared,
but sharply reduces the nonlinear component of variance, thereby producing the win–win situation observed here.
However, this argument does not explain why variance increases monotonically with increasing sampling fraction. It
may be that in this situation the linear component of the estimator is so small that it does not play a signiﬁcant role. In
such cases the theory of Section 2 cannot provide much insight.
3.4. Bootstrap versus half-sampling
One of the results of the theory is that half-sampling (m=n/2) without replacement should produce similar results to
full (m=n) bootstrap sampling with replacement. In the case of variance, conﬁrmation of this property can be deduced
directly from the ﬁgures. Table 1 shows that the relationship also extends to bias-squared, and thus to root-meansquared error. The ﬁrst column identiﬁes each example by the ﬁgure(s) in which it was presented. The second and third
columns give the corresponding root-mean-squared estimation error (√bias-squared + variance) for half-sampling
without replacement and full sampling with replacement, respectively. The results are seen to verify the theory in this
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
Root-mean-squared estimation errors in the case of half-sampling without replacement (see the column headed n/2: W/O) or full sampling with
replacement (column headed n: W)
Half-sampling of course has a computational advantage, especially if the implementation of the estimator does not
support observation weights. More generally, one can see by comparing the relevant ﬁgures that m-out-of-n with, and
2m-out-of-n without, replacement sampling give fairly similar results.
4. Technical properties
4.1. Theory for solutions of bagged estimating equations
Here, we demonstrate theoretically the properties of bagging discussed heuristically in Section 2.1. The following
notation is used:  = ′′(0), j = (j+2)(0)/(j + 1)!, = (4)(0), j = (j+4)(0)/(j + 1)! and  = ′′′(0).
We take Z to be the mean of n independent and identically distributed random variables with zero mean, and consider
theory as n increases. Further we assume  is smooth in a neighbourhood of the origin, so that Taylor expansion may be
conducted there, and that ′(0) = 0 and −1 < ′′(0) ̸= 0. The constraint on ′(0) serves only to ensure that bumpiness
of the objective function L does not add a systematic error to the estimator ˆ. That assumption is not essential to our
analysis, but it simpliﬁes the algebra.
The conditions on ′′(0) are more substantive. If ′′(0) = 0 then the unbagged estimator becomes superefﬁcient,
in the sense that its convergence rate is faster than Op(n−1/2). (For example, the rate is Op(n−1) if ′′(0) = 0 and
′′′(0) ̸= 0.) And if ′′(0) = −1 then the objective function is no longer quadratic near the origin, since the component
22 cancels completely with the quadratic term in (). As a result the convergence rate is again different; depending
on high-order derivatives of , the estimator ˆ may now be inconsistent. If ′′(0) < −1 then  = 0 produces a local
maximum, rather than a local minimum, of the noiseless objective function 1
22 + (). In this case the cup shape of
the objective function has been completely overwhelmed, and reversed, by the added bumpy function.
The ﬁrst step in our analysis is to derive properties of the bagged estimator. Put ˆ2 = E{(∗)2|X} and  = 1
2 ˆ2, and
note that since E(∗|X) = 0,
E{′( + Z∗)|X} = ′( + Z) + 1
2 E{(∗)2|X} ′′′( + Z) + 1
6 E{(∗)3|X} (4)( + Z) + · · ·
= ′( + Z) + ′′′( + Z) + Op(n−2).
Therefore, the bagged estimating equation (2.2) may be expanded as
 +  + ( + ) ( + Z) + (1 + 1) ( + Z)2 + (2 + 2) ( + Z)3 + · · · + Op(n−2) = 0.
Taking  = (1 +  + )−1{ −( + ) Z −} in (4.1) we deduce that
 + Z =  −( + ) Z − + (1 +  + ) Z
=  + Z −
1 +  +  .
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
Substituting this formula for  + Z in (4.1) we ﬁnd that (4.2), we deduce that  satisﬁes
 + (1 + 1)
 + Z −
+ (2 + 2)
 + Z −
+ · · · + Op(n−2) = 0.
Since  = Op(n−1) and Z = Op(n−1/2), then (4.2) implies that  = Op(n−1). Substituting the results  = Op(n−1),
Z = Op(n−1/2) and  = Op(n−1) into all but the ﬁrst term (i.e. the ) on the left-hand side of (4.2), we deduce that 
+ Op(n−3/2) = 0 +
(1 + )2 Z + Op(n−2),
where 0 is the value assumed by  in the unbagged case, i.e. where  = (1 + )−1 (0 −Z) solves (2.1). Therefore,
the solution ˆbagg of (4.1) satisﬁes
0 −( + ) Z +
(1 + )2 Z −
(1 + )2 Z −1
 + Op(n−2),
where  = ˆ, the unbagged estimator, solves (2.1).
Note too that
1 +  + Op(n−1)
2 + Z1) + Op(n−2),
2 is the variance of each of the data of which Z is the mean, and Z1 is the average of the sum of the squares of
those data, centered at its expected value. Assume for the time being that  = 0. Then (4.3) simpliﬁes signiﬁcantly, and
in company with (4.4) it implies that
+ O(n−3) =
(1 + ) n
E(ˆ2) + O(n−3).
It may also be proved that when  = 0,
E(ˆ) = O(n−2)
E(ˆbagg) = O(n−2).
From (4.5) we deduce that, provided and  have different signs, E(ˆ2
bagg) < E(ˆ2) for all sufﬁciently large n. Moreover, to ﬁrst order the amount by which E(ˆ2) exceeds E(ˆ2
bagg) increases in direct proportion to
2 limn→∞nE(ˆ2).
The ﬁrst of these results conﬁrms the potential reductions in mean-squared error offered by bagging, and the second
argues that if the variance of the bumpy perturbations is not too large then the improvement tends to be in proportion
to the extent to which bagging reduces the variability of locations of bumps.
On the other hand, if and  have the same sign then E(ˆ2
bagg) > E(ˆ2), and so bagging increases, rather than
reduces, mean-squared error. Again this effect is in proportion to the variance of the location of the bumps, to ﬁrst
order. Eqs. (4.5) and (4.6) also imply that the results in this paragraph and the previous one remain true if we replace
‘mean-squared error’ by ‘variance’ throughout.
The case where ′′′(0) ̸= 0 is more complex. There we can see from (4.3) that the bagged estimator has acquired
an extra bias term, equal to −(1 + )−1 E() + O(n−2) and of size n−1. This can increase mean-squared error of the
bagged estimator, at the same level (i.e. n−2) as any improvements offered by bagging. Also, contributions to variance
from the term in Z at (4.3) will affect performance at the level n−2. The net inﬂuence of these contributions is that
overall mean-squared error performance of the bagged estimator, relative to its unbagged counterpart, can no longer
be explained solely in terms of the sign of / . Bagging can either improve or degrade performance, depending on the
relationship among , and .
The difﬁculty when ′′′(0) ̸= 0 is caused by the fact that  is signiﬁcantly asymmetric, and of course bagging
the estimating equation compounds asymmetry. That is the source of the bias term, and indirectly of the component
involving Z.
J.H. Friedman, P. Hall / Journal of Statistical Planning and Inference 137 669–683
4.2. Theory for bagged solutions of estimating equations
For brevity we treat only the case  = 0, noting that the main effect of the level of asymmetry implied by  ̸= 0 is
exactly that reported in Section 4.1: a bias component of size n−1 is added to the bagged estimator, and variance of the
estimator alters a little, at the level n−2. Now, the unbagged estimator is deﬁned by ˆ = (1 + )−1 (0 −Z), where 0
is obtained by solving (2.1) for  = ˆ. This leads to the formula
6 (1 + )3 Z3
+ Op(n−2).
ReplacingZbyZ∗=Z+∗inthisexpression,takingexpectationconditionalonX,putting= 1
2 E{(∗)2|X}=Op(n−1),
and noting that E{(∗)j|X} = Op(n−2) for j ⩾3, we deduce that the bagged solution of Eq. (2.1) satisﬁes
ˆbagg = −
6 (1 + )3 Z3 +
ˆ + Op(n−2).
Moreover, the analogue of (4.6) holds: when  = 0, E(ˆ) = O(n−2) and E(ˆbagg) = O(n−2).
Eq. (4.7) is a direct analogue of (4.3), provided we take  = 0 in the latter. It produces the corresponding direct
analogue of (4.5):
2(1 + )3 n
E(ˆ2) + O(n−3),
2 is the variance of each of the data of which Z is the mean. This implies the main properties noted in the
case of bagged estimating equations. In particular, bagging improves performance (by reducing both variance and
mean-squared error), provided  and have different signs (and −1 <  ̸= 0). The extent of the improvement is once
again proportional to
2, to ﬁrst order. And bagging degrades performance if  and have different signs.
If  > 0, meaning that the perturbation ′(+Z) of the estimating equation enhances the equation’s local convexity,
then it can be seen by comparing (4.5) and (4.8) that bagging the estimating equation has a greater effect on performance
than bagging the estimator itself.
Acknowledgements
The work of Jerome H. Friedman was partially supported by CSIRO Mathematical and Information Sciences,
Australia, the Department of Energy under contract DE-AC03-76SF00515, and by Grant DMS9764431 of the National
Science Foundation. We are grateful to Rob Tibshirani for the helpful comments.