Department of Statistics Papers
Object Perception as Bayesian Inference
 
Daniel Kersten
Pascal Mamassian
Alan Yuille
Publication Date
2011-10-25
eScholarship.org
Powered by the California Digital Library
University of California
Object Perception as Bayesian Inference
Object Perception as Bayesian Inference
Daniel Kersten
Department of Psychology, University of Minnesota
Pascal Mamassian
Department of Psychology, University of Glasgow
Alan Yuille
Departments of Statistics and Psychology, University of California, Los Angeles
KEYWORDS: shape, material, depth, perception, vision, neural, psychophysics, fMRI, computer vision
We perceive the shapes and material properties of objects quickly and reliably despite the
complexity and objective ambiguities of natural images.
Typical images are highly complex
because they consist of many objects embedded in background clutter. Moreover, the image
features of an object are extremely variable and ambiguous due to the eﬀects of projection,
occlusion, background clutter, and illumination. The very success of everyday vision implies
neural mechanisms, yet to be understood, that discount irrelevant information and organize
ambiguous or “noisy” local image features into objects and surfaces. Recent work in Bayesian
theories of visual perception has shown how complexity may be managed and ambiguity resolved
through the task-dependent, probabilistic integration of prior object knowledge with image
OBJECT PERCEPTION: GEOMETRY AND MATERIAL . . . . . . . . . . . . .
INTRODUCTION TO BAYES
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
How to resolve ambiguity in object perception?
. . . . . . . . . . . . . . . . . . . . . .
How does vision deal with the complexity of images?
. . . . . . . . . . . . . . . . . . .
PSYCHOPHYSICS
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Ideal observers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Basic Bayes: The trade-oﬀbetween feature reliability and priors
. . . . . . . . . . . .
Discounting & task-dependence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Integration of image measurements & cues
. . . . . . . . . . . . . . . . . . . . . . . .
Perceptual “explaining away” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
THEORETICAL AND COMPUTATIONAL ADVANCES
. . . . . . . . . . . . .
Annu. Rev. Psych. 200X 1
1056-8700/97/0610-00
Bayes Decision Theory and Machine Learning.
. . . . . . . . . . . . . . . . . . . . . .
Learning probability distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Visual inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
NEURAL IMPLICATIONS
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Network models with lateral connections . . . . . . . . . . . . . . . . . . . . . . . . . .
Combining bottom-up and top-down processing
. . . . . . . . . . . . . . . . . . . . . .
Implementation of the decision rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
CONCLUSIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ACKNOWLEDGMENTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
LITERATURE CITED
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
OBJECT PERCEPTION: GEOMETRY AND MATERIAL
Object perception is important for the everyday activities of recognition, planning, and motor action. These tasks require the visual system to obtain geometrical information about the shapes of objects, their spatial layout, and their
material properties.
The human visual system is extraordinarily competent at extracting necessary
geometrical information. Navigation, judgments of collision, and reaching rely on
knowledge of spatial relationships between objects, and between the viewer and
object surfaces. Shape-based recognition and actions such as grasping require
information about the internal shapes and boundaries of objects.
Extracting information about the material–the “stuﬀ” that things are made
of–is also important for daily visual function. Image features such as color, texture, and shading depend on the material reﬂectivity and roughness of a surface.
Distinguishing diﬀerent materials is useful for object detection (e.g. texture differences are cues for separating ﬁgure from ground), as well as for judging aﬀordances such as edibility (e.g. ripe fruit or not) and graspability (e.g. slippery or
Understanding how the brain translates retinal image intensities to useful information about objects is a tough problem on theoretical grounds alone. The
diﬃculty of object perception arises because natural images are both complex
and objectively ambiguous. The images received by the eye are complex highdimensional functions of scene information (Figure 1). The complexity of the
problem is evident in the primate visual system in which ten million retinal measurements or so are sent to the brain each second, where they are processed by
some billion cortical neurons. The ambiguity of image intensities also poses a
major computational challenge to extracting object information. Similar 3D geometrical shapes can result in diﬀerent images, and diﬀerent shapes can result in
very similar images, Figure 1A & B. Similarly, the same material (e.g. polished
silver) can give rise to drastically diﬀerent images, depending on the environment,
and the same image can be due to quite diﬀerent materials, Figure 1C & D.
This paper treats object perception as a visual inference problem (Helmholtz
Object Perception as Bayesian Inference
1867) and, more speciﬁcally, as statistical inference . This approach is particularly attractive because it has
been used in computer vision to develop theories and algorithms to extract information from natural images useful for recognition and robot actions. Computer
vision has shown how the problems of complexity and ambiguity can be handled
using Bayesian inference, which provides a common framework for modeling artiﬁcial and biological vision. In addition, studies of natural images have shown
statistical regularities that can be used for designing theories of Bayesian inference. The goal of understanding biological vision also requires using the tools of
psychophysics and neurophysiology to investigate how the visual pathways of the
brain transform image information into percepts and actions.
In the next section, we provide an overview of object perception as Bayesian
inference. In subsequent sections, we review psychophysical (Section 3), computational, (Section 4) and neurophysiological (Section 5) results on the nature of
the computations and mechanisms that support visual inference. Psychophysically, a major challenge for vision research is to obtain quantitative models of
human perceptual performance given natural image input for the various functional tasks of vision. These models should be extensible in the sense that one
should be able to build on simple models, rather than having a diﬀerent model
for each set of psychophysical results. Meeting this challenge will require further
theoretical advances and Section 4 highlights recent progress in learning classi-
ﬁers, probability distributions, and in visual inference. Psychophysics constrains
neural models, but can only go so far and neural experiments are required to
further determine theories of object perception. Section 5 describes some of the
neural implications of Bayesian theories.
INTRODUCTION TO BAYES
How to resolve ambiguity in object perception?
The Bayesian framework has its origins in Helmholtz’s idea of perception as
“unconscious inference”. Helmholtz realized that retinal images are ambiguous
and that prior knowledge was required to account for perception. For example,
diﬀerently curved cylinders can produce exactly the same retinal image if viewed
from the appropriate angles, and the same cylinder can produce very diﬀerent
images if viewed at diﬀerent angles. Thus, the ambiguous stimulus in Figure 2A
could be interpreted as a highly curved cylinder from a high angle of view, a
ﬂatter cylinder from a low angle of view, or as concave cylinder from yet another
viewpoint. Helmholtz proposed that the visual system resolves ambiguity through
built-in knowledge of the scene and how retinal images are formed, and uses this
knowledge to automatically and unconsciously infer the properties of objects.
Bayesian statistical decision theory formalizes Helmholtz’s idea of perception
as inference1. Theoretical observers that use Bayesian inference to make opti-
1Recent reviews include Knill et al. , Yuille and B¨ulthoﬀ , Kersten ,
Maloney , Pizlo , and Mamassian et al. .
Kersten et al.
mal interpretations are called ideal observers. Let’s ﬁrst consider one type of
ideal observer that computes the most probable interpretation given the retinal
image stimulus. Technically, this observer is called a maximum a posteriori or
“MAP” observer. The ideal observer bases its decision on the posterior probability distribution–the probability of each possible true state of the scene given the
retinal stimulus. According to Bayes’ theorem, the posterior probability is proportional to the product of the prior probability–the probability of each possible
state of the scene prior to receiving the stimulus, and the likelihood–the probability of the stimulus given each possible state of the scene. In many applications,
prior probability distributions represent knowledge of the regularities of possible
object shapes, materials, and illumination, and likelihood distributions represent
knowledge of how images are formed through projection onto the retina. Figure 2
illustrates how a symmetric likelihood (a function of the stimulus representing
the curvature of the 2D line) can lead to an asymmetric posterior due to a prior
towards convex objects. The ideal (MAP) observer then picks the most probable
interpretation for that stimulus–i.e. the state of the scene (3D surface curvature
and viewpoint slant) for which the posterior distribution peaks in panel D of
Figure 2. An ideal observer does not necessarily get the right answer for each
input stimulus, but it does make the best guesses so it gets the best performance
averaged over all the stimuli. In this sense an ideal observer may “see” illusions.
Let’s take a closer look at three key aspects to Bayesian modeling: the generative model, the task speciﬁcation, and the inference solution.
The generative model. The generative model, S →I, speciﬁes how an image
description I (e.g. the image intensity values or features extracted from them)
is determined by the scene description S (e.g. vector with variables representing
surface shape, material reﬂectivity, illumination direction, and viewpoint). The
likelihood of the image features, p(I|S), and the prior probability of the scene
description, p(S), determine an external generative model. As illustrated later
in Figure 5, a strong generative model allows one to draw image samples–the
high-dimensional equivalent to throwing a loaded die. The product of the likelihood and prior speciﬁes an ensemble distribution p(S, I) = p(I|S)p(S) which
gives a distribution over problem instances, (Section 4). In Bayesian networks,
a generative model speciﬁes the causal relationships between random variables
(e.g. objects, lighting, and viewpoint) that determine the observable data (e.g.
image intensities)2. Below, we describe how Bayesian networks can be used to
cope with the complexity of problems of visual inference.
The task speciﬁcation.
There is a limitation with the MAP ideal observer
described above. Finding the most probable interpretation of the scene does not
allow for the fact that some tasks may require more accurate estimates of some
aspects of the scene than other aspects. For example, it may be critical to get
2When not qualiﬁed, we use the term “generative model” to mean an external model that
describes the causal relationship in terms of variables in the scene. Models of inference may
also use an “internal generative model” to test perceptual hypotheses against the incoming data
(e.g. image features). We use the term “strong generative model” to mean one that produces
consistent image samples in terms of intensities.
Object Perception as Bayesian Inference
the exact object shape, but not the exact viewpoint (represented in the utility
function in Figure 2E). The task speciﬁes the costs and beneﬁts associated with
the diﬀerent possible errors in the perceptual decision. Generally, an optimal
perceptual decision is a function of the task as well as the posterior.
Often we can simplify the task requirements by splitting S into components
(S1, S2) that specify which scene properties are important to estimate (S1, e.g.
surface shape), and which confound the measurements and are not worth estimating at all (S2, e.g. viewpoint, illumination).
The inference solution. Putting things together, Bayesian perception is an inverse solution, I →S1, to the generative model, that estimates the variables S1
given the image I and discounts the confounding variables. Decisions are based on
the posterior distribution p(S|I) which, by Bayes, is speciﬁed by p(I|S)p(S)/p(I).
The decision may be designed, for example, to choose the scene description for
which the posterior is biggest (MAP). But we’ve noted that other tasks are possible such as only estimating components of S.
In general, an ideal observer
convolves the posterior distribution with a utility function (or negative loss function)3. The result is the expected utility (or the negative of the expected loss)
associated with each possible interpretation of the stimulus.
The Bayes ideal
observer picks the interpretation that has the maximum expected utility (Figure 2F).
How does vision deal with the complexity of images?
Recent work has shown how specifying generative models in terms of inﬂuence graphs
 , together with a description of visual tasks,
allow us to break problems down into categories (see the example in Figure 3A).
The idea is to decompose the description of a scene S into n components S1, . . .,
Sn, the image into m features I1, . . ., Im, and express the ensemble distribution
as p(S1, . . . , Sn; I1, . . . , Im). We represent this distribution by a graph where the
nodes correspond to the variables S1, . . ., Sn and I1, . . ., Im and links are drawn
between nodes which directly inﬂuence each other. There is a direct correspondence between graphical structure and the factorization (and thus simpliﬁcation)
of the joint probability. In the most complex case, every random variable inﬂuences every other one, and the joint probability cannot be factored into simpler
parts. In order to build models for inference, it is useful to ﬁrst build quantitative models of image formation–external generative models based on real-world
statistics.
As we have noted above, the requirements of the task split S into
variables that are important to estimate accurately for the task (disks) and those
which are not (hexagons) (Figure 3B). The consequences of task speciﬁcation are
described in more detail in subsection 3.3.
In the next Section 3 we review psychophysical results supporting the Bayesian
approach to object perception. The discussion is organized around the four simple
inﬂuence graphs of Figure 4.
3Optimists maximize the utility or gain while pessimists minimize their loss.
Kersten et al.
PSYCHOPHYSICS
Psychophysical experiments test Bayesian theories of human object perception at
several levels. Ideal observers (subsection 3.1) provide the strongest tests, because
they optimally discount and integrate information to achieve a well-deﬁned goal.
But even without an ideal observer model, one can assess the quality of vision’s
built-in knowledge of regularities in the world.
In subsection (3.2) on Basic Bayes, we review the perceptual consequences of
knowledge speciﬁed by the prior p(S), the likelihood p(I|S), and the trade-oﬀ
between them. Psychophysical experiments can test theories regarding knowledge speciﬁed by p(S). For example, the perception of whether a shape is convex
or concave is biased by the assumption that the light source (part of the scene
description S) is above the object.
Psychophysics can test theories regarding
knowledge speciﬁed by the likelihood, p(I|S). The likelihood characterizes regularities in the image given object properties S, which include eﬀects of projecting
a 3D object onto a 2D image. For example, straight lines in 3D project to straight
lines in the image. Additional image regularities can be obtained by summing
over S to get p(I) = P
S p(I|S)p(S).
These regularities are expected to hold
independently of the speciﬁc objects in the scene. Image regularities can be divided into geometric (e.g.
bounding contours in the image of an object) and
photometric descriptions (e.g. image texture in a region).
As illustrated in the examples of Figure 3, vision problems have more structure than Basic Bayes. In subsequent subsections (3.3, 3.4, and 3.5), we review
psychophysical results pertaining to three additional graph categories, each of
which is important for resolving ambiguity: discounting variations to achieve object constancy, integration of image measurements for increased reliability, and
perceptual explaining away given competing perceptual hypotheses.
Ideal observers
Ideal observers provide the strongest psychophysical tests because they are complete models of visual performance based on both the posterior and the task.
A Bayesian ideal observer is designed to maximize a performance measure for
a visual task (e.g. proportion of correct decisions) and as a result serves as a
benchmark with which to compare human performance for the same task . Characterizing the visual information for a task can be critical for proper
interpretation of psychophysical results , as well as for the
analysis of neural information transmission . Liu and Kersten show that human
thresholds for discriminating 3D asymmetric objects is less than for symmetric
objects (the image projections were not symmetric); however, when one compares
Object Perception as Bayesian Inference
human performance to the ideal observer for the task, which takes into account
the redundancy in symmetric objects, human discrimination is more eﬃcient for
symmetric objects.
Because human vision is limited by both the nature of the computations and its
physiological hardware, we might expect signiﬁcant departures from optimality.
Nevertheless, Bayesian ideal observer models provide a ﬁrst approximation to
human performance that has been surprisingly eﬀective .
A major theoretical challenge for ideal observer analysis of human vision is the
requirement for strong generative models so that human observers can be tested
with image samples I drawn from p(I|S). Shortly, we will discuss results from
statistical models of natural image features that constrain, but are not suﬃcient
to specify the likelihood distribution. Then in Section 4, we discuss relevant work
on machine learning of probability distributions. But let’s ﬁrst get a preview of
one aspect of the problem.
The need for strong generative models is an extensibility requirement that rules
out classes of models for which the samples are image features. The distinction is
sometimes subtle. The key point is that images features may either be insuﬃcient
to uniquely determine an image or they may sometimes overconstrain it. For example, suppose that a system has learned probability models for airplane parts.
Then sampling from these models is highly unlikely to produce an airplane– the
samples will be images, but additional constraints are needed to make sure they
correspond to airplanes, see Figure 5A. Escher’s pictures and other “impossible
ﬁgures” such as the impossible trident give examples of images which are not
globally consistent. In addition, local feature responses can sometimes overconstrain the image locally. For example, consider the binary image in Figure 5B
and suppose our features ∆L are deﬁned to be the diﬀerence in image intensity
L at neighboring pixels. The nature of binary images puts restrictions on the
values that ∆L can take at neighboring pixels. It is impossible, for example,
that neighboring ∆Ls can both take the value +1. So sampling from these features will not give a consistent image unless we impose additional constraints.
Additional consistency conditions are also needed in two-dimensional images, see
Figure 5C, where the local image diﬀerences in the horizontal, ∆L1, ∆L3, and vertical, ∆L2, ∆L4, directions must satisfy the constraint ∆L2 +∆L3 = ∆L1 +∆L4
(this is related to the surface integrability condition which must be imposed on
the surface normals of an object to ensure that the surface is consistent).
But it is important to realize that strong generative models can be learned from
measurements of the statistics of feature responses (discussed more in Section 4).
Work by Zhu, Wu, and Mumford shows how statistics
on image, or shape, features can be used to obtain strong generative models.
Samples from these models are shown in Figure 5D & E.
Basic Bayes: The trade-oﬀbetween feature reliability and priors
The objective ambiguity of images arises if several diﬀerent objects could have
produced the same image description or image features. In this case the visual
Kersten et al.
system is forced to guess, but it can make intelligent guesses by biasing its guesses
toward typical objects or interpretations (see Figure 4A). Bayes formula implies that these
guesses, and hence perception, are a trade-oﬀbetween image feature reliability,
as embodied by the likelihood p(I|S), and the prior probability p(S).
perceptions may be more prior driven, and others more data-driven. The less
reliable the image features (e.g. the more ambiguous), the more the perception is
inﬂuenced by the prior. This trade-oﬀhas been illustrated for a variety of visual
phenomena .
In particular, Weiss et al addressed the “aperture problem” of motion
perception: how to combine locally ambiguous motion measurements into a single
global velocity for an object. The authors constructed a Bayesian model whose
prior is that motions tend to be slow, and which integrates local measurements
according to their reliabilities . With this model (using a single free parameter),
the authors showed that a wide range of motion results in human perception could
be accounted for in terms of the trade-oﬀbetween the prior and the likelihood.
The Bayesian models give a simple uniﬁed explanation for phenomena that had
previously been used to argue for a “bag of tricks” theory requiring many diﬀerent
mechanisms .
Prior regularities p(S): Object shape, material, and lighting
Psychophysics can test hypotheses regarding built-in visual knowledge of the prior
p(S) independent of projection and rendering. This can be done at a qualitative
or quantitative level.
Geometry & shape. Some prior regularities refer to the geometry of objects that
humans interact with. For instance, the perception of solid shape is consistently
biased towards convexity rather than concavity .
This convexity prior is robust over a range of object shapes, sizes and tasks.
More speciﬁc tests and ideal observer models will necessitate developing probability models for the high-dimensional spaces of realistic objects. Some of the
most highly developed work is on human facial surfaces . Relating image intensity to such measured surface depth statistics has
yielded computer vision solutions for face recognition and has
provided objective prior models for face recognition experiments suggesting that
human vision may represent facial characteristics along principal component dimensions in an opponent fashion .
Material. The classical problems of color and lightness constancy are directly
tied to the nature of the materials of objects and environmental lighting. However, most past work has implicitly or explicitly assumed a special case, that
surfaces are Lambertian (matte). Here, the computer graphics communities have
been instrumental in going beyond the Lambertian model by measuring and characterizing the reﬂectivities of natural smooth homogeneous surfaces in terms of
the Bidirectional Reﬂectance Distribution Function (BRDF) , with important extensions to more complicated textured surfaces including human skin . Real images are of course intimately tied to the structure of illumination, and below we review psychophysical
results on realistic material perception.
Lighting. Studies of human object perception (as well as computer vision) have
traditionally assumed simple lighting models, such as a single point light source
with a directionally non-speciﬁc ambient term. One of the best known examples
of a prior is the assumption that light is coming from above. This assumption
is particularly useful to disambiguate convex from concave shapes from shading
information. The light from above prior is natural when one considers that the
sun and most artiﬁcial light sources are located above our heads. However, two
diﬀerent studies have now shown that humans prefer the light source to be located
above-left rather than straight above . A convincing explanation for this leftward bias remains to be advanced.
Apparent motion in depth of an object is strongly inﬂuenced by the movement
of its cast shadow . This result can
be interpreted in terms of a “stationary light source” prior–the visual system is
more likely to interpret change in the image as being due to a movement of the
object or a change in viewpoint, rather than a movement of the light source.
Do we need more complex lighting models? The answer is surely “yes”, especially in the context of recent results on the perception of specular surfaces , and color given indirect lighting , both discussed
in subsection (3.3). Dror et al have shown that spatial maps of natural
illumination show statistical regularities similar to those found
in natural images .
Image regularities p(I) = P
S p(I|S)p(S)
Image regularities arise from the similarity between natural scenes. They cover
geometric properties such as the statistics of edges and photometric properties
such as the distribution of contrast as a function of spatial frequency in the image.
Geometric regularities. Geisler et al. used spatial ﬁltering to extract local edge fragments from natural images. They measured statistics on the distance
between the element centers, the orientation diﬀerence between the elements, and
the direction of the second element relative to the orientation of ﬁrst (reference)
element. A model derived from the measured statistics and from a simple rule
that integrated local contours together could quantitatively predict human contour detection performance. More detailed rules to perceptually organize a chain
of dot elements into a subjective curve with a corner or not, or to be split into one
vs. two groups, have also been given a Bayesian interpretation .
Elder & Goldberg measured statistics of contours from images handsegmented into sets of local tangents. These statistics were used to put probability distributions on three Gestalt principles of perceptual organization: proximity, continuation, and luminance similarity. The authors found that these three
grouping cues were independent and that the proximity cue was by far the most
powerful. Moreover, the contour likelihood distribution (the probability of a gap
Kersten et al.
length between two tangents of a contour) follows a power law with an exponent
very close to that determined psychophysically on dot lattice experiments.
The work of Geisler et al deals with distributions on image features, namely
edge pairs. To devise a distribution p(I) from which one can draw true contour
samples, one needs to also take into account the consistency condition that edge
pairs have to lie in an image (see Figure 5A for an airplane analogy). Zhu 
learns a distribution on the image itself and thus samples drawn from it produce
contours, see Figure 5E. See also Elder and Goldberg .
Photometric regularities. It is now well-established that natural scenes have
a particular signature in their contrast spatial frequency spectrum in which low
spatial frequencies are over-represented relative to high spatial frequencies. This
regularity is characterized by a 1/f β spectrum, where f is spatial frequency, and
β is a parameter that can be ﬁt from image measurements . Human observers are better at discriminating changes in β
for Gaussian and natural images when the values of β are near 
or at the value measured from natural images , suggesting
that the visual system is in some sense “tuned” to the second-order statistics (i.e.
spatial frequency spectrum) of natural images.
The histograms of spatial ﬁlter responses (e.g. the simplest ﬁlter being diﬀerences between neighboring pixel intensities) of natural images also show consistent regularities . The histograms are non-gaussian having a high kurtosis (see ﬁrst panel in Figure 5C). Zhu
& Mumford have derived a strong generative model they call a “generic
image prior” based on ﬁlter histogram statistics.
Image likelihood p(I|S)
The likelihood characterizes how image regularities result from projection and
rendering of objects as a function of view and lighting, and is related to what is
sometimes called the forward optics or “computer graphics” problem.
Geometrical descriptions. Object perception shows biases consistent with preferred views. Mamassian & Landy show how the interpretation of a simple
line drawing ﬁgure changed with rotations of the ﬁgure about the line of sight.
They modeled their result with a simple Bayesian model that had a prior probability for surface normals pointing upward (Figure 2). This prior can be interpreted
as a preference for a viewpoint situated above the scene, which is reasonable when
we consider that most of the objects we interact with are below the line of sight.
Why does a vertical line appear longer than the same line when horizontal?
Although there have been a number of recent studies of the statistics of image
intensities and contours, Howe and Purves go the next step to directly
compare statistics on the separation between two points in the image with measurements of the causes in the originating 3D scene. The authors found that the
average ratio of the 3D physical interval to the projected image interval from
real-world measurements shows the same pattern as perceived length.
Photometric descriptions. The global spatial frequency amplitude spectrum
is an inadequate statistic to capture the structure of speciﬁc texture classes.
Object Perception as Bayesian Inference
More recent work has shown that wavelet representations provide richer models
for category-speciﬁc modeling of spatially homogeneous texture regions, such as
“text”, “fur”, “grass”, etc. . As with
the contour case, a future challenge is to develop strong generative models from
which one can draw samples of images from p(I|S) (Section 4). Promising work
along these lines is illustrated in Figure 5D .
Where do the priors come from?
Without direct input, how does image-independent knowledge of the world get
built into the visual system? One pat answer is that the priors are in the genes.
Observations of stereotyped periods in the development of human depth perception do in fact suggest a genetic component . In another domain, the
strikingly rapid development of object concepts in children is still a major mystery that suggests predispositions to certain kinds of grouping rules. Adults too
are quick to accurately generalize from a relatively small set of positive examples
(in many domains, including objects) to a whole category. Tenenbaum provides a Bayesian synthesis of two theories of generalization (similaritylike and rule-like), and provides a computational framework that helps to explain
rapid category generalization.
The accurate segmentation of objects such as the kayaks in Figure 1 likely
requires high-level prior knowledge regarding the nature of the forms of possible
object classes. Certain kinds of priors, such as learning the shapes of speciﬁc
objects may develop through what Brady and Kersten have called “opportunistic learning” and “bootstrapped learning”. In opportunistic learning, the
visual system seizes the opportunity to learn object structure during those relatively rare occasions when an object is seen under conditions of low ambiguity,
such as when motion breaks camouﬂage of an object in “plain view”.
Bootstrapped learning operates under low or intermediate conditions of ambiguity
(e.g. in which none of the training images provide a clear view of an object’s
boundaries). Then later, the visual system can apply the prior knowledge gained
to high (objective) ambiguity situations more typical of everyday vision. The
mechanisms of bootstrapped learning are not well-understood although there has
been some computer vision work , see Section 4.
General purpose cortical learning mechanisms have been proposed ; however, it is not clear whether these are
workable with complex natural image input. We discuss computer vision methods
for learning priors in Section 4.
Discounting & task-dependence
How does the visual system enable us to infer the same object despite considerable
image variation due to confounding variables such as viewpoint, illumination,
occlusion, and background changes? This is the well-known problem of invariance
or object constancy.
Objective ambiguity results when variations in a scene
property change the image description of an object. As we’ve seen, both viewpoint
Kersten et al.
and 3D shape inﬂuence the 2D image.
Suppose the visual task is to determine object shape, but the presence of unknown variables (e.g. viewpoint) confounds the inference. Confounding variables
are analogous to “noise” in classical signal detection theory but they are more
complicated to model and they aﬀect image formation in a highly non-linear
manner. For example, a “standard” noise model has I = S + n where n is Gaussian noise. Realistic “vision noise” is better captured by p(I|S) as illustrated in
Figure 6B. Here the problem is making a good guess independent of (or invariant
to) the true value of the confounding variable. The task itself can serve to reduce ambiguity by discounting the confounding variable . Discounting irrelevant
scene variations (or accounting for object invariance) has received recent attention in the fMRI literature where certain cortical areas show more or less degrees
of invariance to size, translation, rotation in depth, or illumination .
From the Bayesian perspective, we can model problems with confounding variables by the graph in Figure 4B. We deﬁne an ensemble distribution p(S1, S2, I),
where S1 is the target (e.g. keys), S2 is the confounding variable (e.g. pose), and
I is the image. Then we discount the confounding variables by integrating them
out (or summing over them):
p(S1, I) =
p(S1, S2, I)
This is equivalent to spreading out the loss function completely in one of the
directions (e.g. extending the utility function vertically in Figure 2E). As noted
above, the choice of which variables to discount will depend on the task. Viewpoint is a confounding variable that can be discounted if the task is to discriminate
objects independent of viewpoint. Illumination is a confounding variable if the
task is to recognize or determine the depth of an object but it is not if the task is
to determine the light source direction. This can be related to notions of utility4,
discounting a variable is equivalent to treating it as having such low utility that
it is not worth estimating (Yuille & B¨ulthoﬀ1996), see Section 4.
4Bayesian decision theory, see Section 4, provides a precise language to model the costs of
errors determined by the choice of visual task (Yuille & B¨ulthoﬀ1996). The cost or risk R(α; I)
of guessing α when the image measurement is I is deﬁned as the expected loss (or negative
L(α, S)p(S | I),
with respect to the posterior probability, p(S|I). The best interpretation of the image can then
be made by ﬁnding the α which minimizes the risk function. The loss function L(α, S) speciﬁes
the cost of guessing α when the scene variable is S. One possible loss function is −δ(α −S).
In this case the risk becomes R(α; I) = −p(α | I), and then the best strategy is to pick the
most likely interpretation. This is maximum a posteriori (MAP) estimation and is the optimal
strategy for the task requiring an observer to maximize the proportion of correct decisions. A
second kind of loss function assumes that costs are constant over all guesses of a variable. This
is equivalent to “integrating out”, “summing over”, or “marginalization” of the posterior with
respect to that variable.
Object Perception as Bayesian Inference
Viewpoint variation. When interpreting 2D projections of 3D shapes, the human visual system favors interpretations that assume that the object is being
viewed from a general (or generic) rather than accidental viewpoint . Freeman showed that a Bayesian observer
that integrates out viewpoint can account for the generic view assumption.
How does human vision recognize 3D objects as the same despite changes in
viewpoint? Shape-based object recognition models range between two extremes–
those that predict a strong dependence of recognition performance on viewpoint
as a function of familiarity with particular views), and those which as
a result of using view-invariant features together with “structural descriptions”
of objects do not
 . By
comparing human performance to several types of ideal observers which integrate
out viewpoint variations, Liu et al. showed that models that allow rigid
rotations in the image plane of independent 2D templates could not account for
human performance in discriminating novel object views. More recent work by
Liu & Kersten showed that the performance of human observers relative
to Bayesian aﬃne models (which allow stretching, translations, rotations and
shears in the image) is better for the novel views than for the template views,
suggesting that humans have a better means to generalize to novel views than
could be accomplished with aﬃne warps. Other related work describes the role
of structural knowledge , the importance of view-frequency , and shape constancy under perspective .
Bayesian task dependence may account for studies where diﬀerent operationalisations for measuring perceived depth lead to inconsistent, though related, estimates. For instance, the information provided by a single image of an object
is insuﬃcient for an observer to infer a unique shape (see Figure 1B). Not surprisingly, these ambiguities will lead diﬀerent observers to report diﬀerent depth
percepts for the same picture, and the same observer to report diﬀerent percepts when using diﬀerent depth probes. Koenderink et al. show that
most of this variability could be accounted for by aﬃne transformations of the
perceived depth, in particular scalings and shears. These aﬃne transformations
correspond to looking at the picture from a diﬀerent viewpoint. The authors call
the perceived depth of a surface once the viewpoint has been discounted “pictorial
Illumination variation. Discounting illumination by integrating it out can reduce ambiguity regarding the shape or depth of an object . Integrating out illumination level or direction has also
been used to account for apparent surface color .
Work so far has been for extremely simple lighting, such as a single point light
source. One of the future challenges will be to understand how the visual system
discounts the large spatial variations in natural illumination that are particularly
apparent in many surfaces typically encountered such as metal, plastic, and paper (see Figure 1C). Fleming et al. showed that human observers could
Kersten et al.
match surface reﬂectance properties more reliably and accurately for more realistic patterns of illumination. They manipulated pixel and wavelet properties
of illumination patterns to show that the visual system’s built-in assumptions
about illumination are of intermediate complexity (e.g. presence of edges and
bright light sources) and rather than depending on high-level knowledge such as
recognizable objects.
Integration of image measurements & cues
The visual system likely achieves much of its inferential power through sophisticated integration of ambiguous local features.
This can be modeled from a
Bayesian perspective . For example, a depth discontinuity
typically causes more than one type of cue (stereo disparity and motion parallax) and visual inference can exploit this to improve the reliability and accuracy
of the depth estimate. Given two conﬂicting cues to depth, the visual system
might get by with a simple averaging of each estimate, even though inaccurate.
Or in cases of substantial conﬂict, it may determine that one measurement is
an outlier, and should not be integrated with the other measurement . Ruling out outliers is possible within a single
modality (e.g. when integrating disparity and texture gradients in vision) but
may not be possible between modalities (e.g. between vision and touch) since in
the latter case, single cue information appears to be preserved .
The visual system is often more sophisticated and combines image measurements
weighted according to their reliability which we
discuss next.
Figure 4C illustrates the inﬂuence graph for cue integration with an example
of illumination direction estimation.
From a Bayes net perspective, the two
cues are conditionally independent given the shared explanation. An important
case is when the nodes represent Gaussian variables that are independent when
conditioned on the common cause and we have estimates for each cue alone (i.e.
ˆSi is the best estimate of Si from p(S|Ii) ). Then optimal integration (i.e. the
most probable value) of the two estimates takes into account the uncertainty due
to measurement noise (the variance) and is given by the weighted average:
where ri, the reliability, is the reciprocal of the variance. This model has been
used to study whether the human visual system combines cues optimally .
For instance, visual and haptic information about object size are combined,
weighted according to the reliability of the source . Object size can be perceived both visually and by touch. When
the information from vision and touch disagree, vision usually dominates. The
authors showed that when one takes into account the reliability of the sensory
measurements, information from vision and touch are integrated optimally. Vi-
Object Perception as Bayesian Inference
sual dominance occurs when the reliability of the visual estimation is greater than
that of the haptic one.
Integration is also important for grouping local image elements likely to belong to the same surface. The human visual system combines spatial frequency
and orientation information optimally when detecting the boundary between two
regions .
Human observers also behave like an optimal observer when integrating information from skew-symmetry and disparity
in perceiving the orientation of a planar object . The
projection of a symmetric ﬂat object has a distorted or skewed symmetry that
provides partial information about the object’s orientation in depth. Saunders
and Knill show that human observers integrate symmetry information with stereo
information weighted according to the reliability of the source.
Prior probabilities can also combine like weighted cues. We saw above that
human observers interpret the shape of an object assuming that both the viewpoint and the light source are above the object . Mamassian and Landy manipulated the reliability of
each of the two constraints by changing the contrast of diﬀerent parts of the
stimuli. For instance, increasing the shading contrast increased the reliability of
the light source prior and biased the observers’ percept towards the shape most
consistent with the light source prior alone. Their interpretation of the results
was that observers modulated the strength of their priors based on the stimulus
contrast. As a consequence, prior constraints behaved just like depth cue integration: cues with more reliable information have higher weight attributed to their
corresponding prior constraint.
Not all kinds of cue integrating are consistent with the simple graph of Figure 4C. Yuille et al. have argued that strong coupling of visual cues is required to model a range of visual phenomena . The graph for “explaining away” (Figure 4D) provides one useful
simple extension to the graphs discussed so far and we discuss this next.
Perceptual “explaining away”
The key idea behind explaining away is that two alternative perceptual hypotheses can compete to explain the image. From a Bayesian perspective, the competition results from the two (otherwise independent) hypotheses becoming conditionally dependent given the image.
Pearl was one of the ﬁrst to emphasize that, in contrast to traditional
artiﬁcial intelligence expert systems, human reasoning is particularly good at
explaining away the eﬀects of alternative hypotheses. In a now classic example,
imagine that you emerge from your house in the morning and notice that the
grass is wet. Prior probabilities might tell you that it was unlikely due to rain
(e.g. you live in Los Angeles), and thus you’ve probably left the sprinkler on
overnight. But then you check the neighbor’s lawn, and notice that it too is wet.
This auxiliary piece of evidence now undercuts the earlier explanation, and lowers
the probability that you left the sprinkler on. The more probable explanation is
that it rained last night, explaining why both lawns are wet. Both the sprinkler
Kersten et al.
and the rain hypothesis directly inﬂuence the observation “your lawn is wet”, but
only the rain hypothesis aﬀects both lawns’ being wet.
Human perception is also good at explaining away but automatically and without conscious thought 5. In perception, the essential idea is that if two (or more)
hypotheses about an object property can explain the same image measurements,
then lowering the probability of one of the explanations increases the probability
of the other. One can observe explaining away in both material (e.g. lightness
and color) and shape perception.
Material. In the Land & McCann version of the classic Craik-O’Brien Cornsweet illusion, two abutting regions that have the same gradual change of luminance appear to have diﬀerent reﬂectances. Knill and Kersten found that
the illusion is weakened when a curved occluding contour (auxiliary evidence)
bounding the regions above and below suggests that the variation of luminance
is due to a change a surface orientation, Figure 4D. The lightness gradients are
explained away by the gradual surface curvature. Buckley et al. extended
this result when binocular disparities were used to suggest a three-dimensional
surface. These results indicate that some scene attributes (in this case surface curvature) inﬂuence the inference of the major scene attributes (material reﬂectance)
that are set by the task.
Another study asked whether human vision could discount the eﬀects of the
color of indirect lighting . Imagine a concave folded card consisting of a red half facing a white half. With white direct illumination, pinkish
light radiates from the white card because of the indirect additional illumination
from the red card. Does vision use shape knowledge to discount the red illuminant in order to perceive the true material color, white? A change in retinal
disparities (an auxiliary measurement) can cause the concave card to appear as
convex, without any change in the chromatic content of the stimulus. When the
card appears convex, the white card appears more pinkish, as if perception has
lost its original explanation for the pinkish tinge in the image and now attributes
it to reddish pigment rather than reddish light.
Geometry & shape.
“Explaining away” occurs in discounting the eﬀects of
occlusion, and when simple high-level object descriptions overide more complex
interpretations of line arrangements or moving dots . We describe this in more detail in
Section 5.
Explaining away is closely related to previous work on competitive models,
where two alternative models compete to explain the same data. It has been
argued that this accounts for a range of visual phenomena (Yuille & B¨ulthoﬀ1996)
including the estimation of material properties (Blake & B¨ulthoﬀ1990). This
approach has been used successfully in computer vision systems by Tu and Zhu
 , including recent work in which a whole class of generative
models including faces, text, generic shading, and texture models compete and
cooperate to explain the entire image (Section 4).
In particular, the generic
5Some related perceptual phenomena Rock described as “perceptual interactions” (Rock
Object Perception as Bayesian Inference
shading models help detect faces by “explaining away” shadows and glasses.
Failures to explain away. Visual perception can also unexpectedly fail to explain away. In one simple demonstration, an ambiguous “Mach” folded card can
be interpreted as a concave horizontal edge or a convex vertical edge. A shadow
cast over the edge by an object (e.g. pencil) placed in front provides enough
information to disambiguate the percept, and yet humans fail to use this information . There has yet to be a good explanation for this
THEORETICAL AND COMPUTATIONAL ADVANCES
Psychophysical and neurophysiological studies of vision necessarily rely on simpliﬁcations of both stimuli and tasks.
This simpliﬁcation, however, must be
extensible to the visual input experienced during natural perceptual functioning.
In the previous sections, several psychophysical studies used models of natural image statistics, as well as models of prior object structure, such as shape.
Future advances in understanding perception will increasingly depend on the
eﬃcient characterization (and simulation) of realistic images to identify informative image statistics, models of scene properties, and a theoretical understanding
of inference for natural perceptual functions. In this section, we discuss relating Bayesian decision theory to current theories of machine learning, learning
the probability distributions relevant for vision, and determining algorithms for
Bayesian inference.
Bayes Decision Theory and Machine Learning.
The Bayesian approach seems completely diﬀerent from the type of feedforward
models required to recognize objects in 150 milli-seconds . How does the Bayesian approach relate to alternative models based on
neural networks, radial basis functions, or other techniques?
This subsection shows the relationships using concepts from Bayes decision
theory and machine learning .
This relationship also gives a
justiﬁcation for Bayes rule and the intuitions behind it.
We ﬁrst introduce additional concepts from decision theory: (i) a decision
rule S∗= α(I), and (ii) a loss function (or negative utility) L(α(I), S) which is
the penalty for making the decision α(I) when the true state is S (e.g. a ﬁxed
penalty for a misclassiﬁcation). Suppose we have a set of examples {Ii, Si : i =
1, ..., N}, then the empirical risk (e.g. the
proportions of misclassiﬁcations) of the rule α(I) is deﬁned to be:
Remp(α) = (1/N)
L(α(Ii), Si).
The best decision rule α∗(.) is selected to minimize Remp(α). For example,
the decision rule is chosen to minimize the number of misclassiﬁcations. Neural
Kersten et al.
networks and machine learning models select rules to minimize Remp(α) .
Now suppose that the samples {Si, Ii} come from a distribution p(S, I) over
the set of problem instances. Then, if we have a suﬃcient number of samples 6,
we can replace the empirical risk by the true risk:
L(α(I), S)p(S, I).
Minimizing R(α) leads to a decision rule that depends on the posterior distribution p(S|I) obtained by Bayes rule p(S|I) = p(I|S)p(S)/p(I). To see this,
we rewrite equation (2) as R(α) = P
S p(S|I)L(α(I), S)} where we have
expressed p(S, I) = p(S|I)p(I). So the best decision α(I) for a specify image I is
α∗(I) = arg min
p(S|I)L(α(I), S),
and depends on the posterior distribution p(S|I). Hence Bayes arises naturally
when you start from the risk function speciﬁed by equation (2).
There are two points to be made here. Firstly, the use of the Bayes posterior
p(S|I) follows logically from trying to minimize the number of misclassiﬁcations
in the empirical risk (provided there are a suﬃcient number of samples). Secondly, it is possible to have an algorithm, or a network, that computes α(.) and
minimizes the Bayes risk but which does not explicitly represent the probability
distributions p(I|S) and p(S). For example, Liu and Kersten compared
the performance of ideal observers for object recognition with networks using
radial basis functions . It is possible that the radial basis
networks, given suﬃcient examples and having suﬃcient degrees of freedom, are
eﬀectively doing Bayesian inference.
Learning probability distributions
Recent studies show considerable statistical regularities in natural images and
scene properties that help tame the problems of complexity and ambiguity in ways
that can be exploited by biological, and artiﬁcial, visual systems. The theoretical
diﬃculties of the Bayesian approach reduce to two issues. Firstly, can we learn
the probability distributions p(I|S) and p(S) from real data? Secondly, can we
ﬁnd algorithms that can compute the best estimators α∗(.)? We brieﬂy review
the advances in learning distributions p(I|S), p(S), p(I), p(S|I) and decision rules
The Minimax Entropy theory gives a method for learning probability models for textures p(I|S) where S labels the texture (e.g. cheetah fur).
These models are realistic in the sense that stochastic samples from the models
appear visually similar to the texture examples that the models were trained
on, see Figure 5D. Learning these models is, in principle, similar to determining
6The number of samples required is a complicated issue . The input to Minimax Entropy learning are the same histograms of ﬁlter
responses that other authors showed were useful for describing textures . The distribution learned by Minimax Entropy are typically
non-Gaussian but lie in a more general class of distributions called exponential
models. More advanced models of this type can learn distributions with parameters representing hidden states .
For certain problems it is also possible to learn the posterior distribution p(S|I)
directly which relates to directly learning a classiﬁer α(I).
For example, the
AdaBoost learning algorithm has been applied very
successfully to build a decision rule α(I) for classﬁying between faces (seen from
front-on) and non-faces . But the AdaBoost theory shows
that the algorithm can also learn the posterior distributions p(face|I) and p(non-
−face|I) . Other workers have learned posterior
probabilities p(edge|φ(I)) and p(not −edge|φ(I)) where φ(I) are local image
features .
Similarly Oliva and colleagues have learned a
decision rule α(I) to determine the type of scene (urban, mountain, etc.) from
feature measurements . Fine and
Macleod used the statistics of the spatio-chromatic structure of natural
scenes to segment natural images into regions likely to be part of the same surface.
They computed the probability of whether or not two points within an image fall
on the same surface given measurements of luminance and color diﬀerences.
Visual inference
It is necessary to have algorithms to perform Bayesian inference after the probability distributions have been learned. The complexity of vision makes it very
unlikely that we can directly learn a classiﬁer α(I) to solve all visual tasks. (The
brain may be able to do this but we don’t know how to). Recently, however,
there has been some promising new algorithms for Bayesian inference. Particle
ﬁlters have been shown to be very useful for tracking objects over time . Message passing algorithms, such as belief propagation, have had
some success . Zhu and Tu have developed a general
purpose algorithm for Bayesian inference known as DDMCMC. This algorithm
has been very successful at segmenting images when evaluated on datasets with
speciﬁed ground truth. It works, loosely speaking, by using low level cues to propose high-level models (scene descriptions) which are validated, or rejected, by
generative models. It therefore combines bottom-up and top-down processing in
a way suggestive of the feedforward and feedback pathways in the brain described
in the next section. The algorithm has been extended to combine segmentation
with the detection and recognition of faces and text .
Kersten et al.
NEURAL IMPLICATIONS
What are the neural implications of Bayesian models? The graphical structure
of these models often makes it straightforward to map them onto networks and
suggests neural implementations. The notion of incorporating prior probabilities
in visual inference is frequently associated with top-down biases on decisions.
However, some prior probabilities are likely built into the feedforward processes
through lateral connections. More dramatically, some types of inverse inference
(e.g. to deal with ambiguities of occlusion, rotation in depth, or background clutter) may require an internal generative process that in some sense mirrors aspects
of the external generative model used for inference or for learning . There is evidence that descending pathways in cortex may be involved in computations that implement model-based
inference using bottom-up and top-down interactions of the sort suggested by
the phenomena of perceptual “explaining away”. In the next two sections, we
discuss Bayesian computations in networks with lateral connections and in larger
scale cortical models that combine bottom-up with top-down information.
Network models with lateral connections
One class of Bayesian models can be implemented by parallel networks with local
interactions. These include a temporal motion model which
was designed to be consistent with neural mechanisms. In this model, the priors
and likelihood functions are implemented by synaptic weights. Another promising
approach is to model the lateral connections within area MT/V5 to account for
the integration and segmentation of image motion .
Anatomical constraints will sometimes bias the processing of information in a
way that can be interpreted in terms of prior constraints. For instance, the speciﬁc
connections of binocular sensitive cells in primary visual cortex will dictate the
way the visual system solves the correspondence problem for stereopsis. Some
recent psychophysical work suggests that the connections between simple and
complex binocular cells implement a preference for small disparities .
There are also proposed neural mechanisms for representing uncertainty in
neural populations and thereby give a mechanism for weighted cue combination.
The most plausible candidate is population encoding .
Combining bottom-up and top-down processing
There is a long history to theories of perception and cognition involving topdown feedback or “analysis by synthesis” . The generative aspect
of Bayesian models is suggestive of the ascending and descending pathways that
connect the visual areas in primates . The key Bayesian
aspect is model-based ﬁtting, in which models need to compare their predictions
Object Perception as Bayesian Inference
to the image information represented earlier, such as in V1.
A possible role for higher-level visual areas may be to represent hypotheses
regarding object properties, represented for example in the lateral occipital complex , that could be used to resolve
ambiguities in the incoming retinal image measurements represented in V1. These
hypotheses could predict incoming data through feedback and be tested by computing a diﬀerence signal or residual at the earlier level . Thus, low activity at an early level would mean a “good ﬁt” or
explanation of the image measurements. Experimental support for this possibility comes from fMRI data . Earlier
fMRI work by a number of groups has shown that the human lateral occipital
complex (LOC) has increased activity during object perception.
The authors
use fMRI to show that when local visual information is perceptually organized
into whole objects, activity in human primary visual cortex (V1) decreases over
the same period that activity in higher, lateral occipital areas (LOC) increases.
The authors interpret the activity changes in terms of high-level hypotheses that
compete to explain away the incoming sensory data.
There are two alternative theoretical possibilities for why early visual activity
is reduced. High-level areas may explain away the image and cause the early
areas to be completely suppressed–high-level areas tell lower levels to “shut up”.
Such a mechanism would be consistent with the high metabolic cost of neuronal spikes .
Alternatively, high level areas might sharpen the
responses of the early areas by reducing activity that is inconsistent with the
high level interpretation– high level areas tell lower levels to “stop gossiping”.
The second possibility seems more consistent with some single unit recording
experiments . Lee et al. have shown that cells in V1 and V2 of
macaque monkeys respond to the apparently high-level task of detecting stimuli that pop-out due to shape-from-shading. These responses changed with the
animal’s behavioral adaptation to contingencies suggesting dependence on experience and utility.
Lee and Mumford review a number of neurophysiological studies consistent with a model of the interactions between cortical areas based on particle ﬁlter
methods, which are non-Gaussian extensions of Kalman ﬁlters that use Monte
Carlo methods. Their model is consistent with the “stop gossiping” idea. In
other work, Yu & Dayan raise the intriguing possibility that acetylcholine
levels may be associated with the certainty of top-down information in visual
inference.
Implementation of the decision rule
One critical component of the Bayesian model is the consideration of the utility
(gain or negative loss in decision theory terminology) associated with each decision. Where and how is this utility encoded? Platt & Glimcher systematically varied the expectation and utility (juice reward) linked to an eye-movement
performed by a monkey. The activity of cells in one area of the parietal cortex was
modulated by the expected reward and the probability that the eye-movement
Kersten et al.
will result in a reward.
It will be interesting to see whether similar activity
modulations occur within the ventral stream for object recognition decisions.
Gold and Shadlen propose neural computations that
can account for categorical decisions about sensory stimuli (e.g. whether a ﬁeld
of random dots is moving one way or the other) by accumulating information over
time represented by a single quantity representing the logarithm of the likelihood
ratio favoring one alternative over another.
CONCLUSIONS
The Bayesian perspective yields a uniform framework for studying object perception. We have reviewed work that highlights several advantages of this perspective. First, Bayesian theories explicitly model uncertainty. This is important in
accounting for how the visual system combines large amounts of objectively ambiguous information to yield percepts that are rarely ambiguous. Second, in the
context of speciﬁc experiments, Bayesian theories are optimal, and thus deﬁne
ideal observers. Ideal observers characterize visual information for a task and
can thus be critical for interpreting psychophysical and neural results. Third,
Bayesian methods allow the development of quantitative theories at the information processing level, avoiding premature commitment to speciﬁc neural mechanisms.
This is closely related to the importance of extensibility in theories.
Bayesian models provide for extensions to more complicated problems involving
natural images and functional tasks as illustrated in recent advances in computer
vision. Fourth, Bayesian theories emphasize the role of the generative model, and
thus tie naturally to the growing body of work on graphical models and Bayesian
networks in other areas such as language, speech, concepts and reasoning. The
generative models also suggest top-down feedback models of information processing in the cortex.
ACKNOWLEDGMENTS
Supported by NIH RO1 EY11507-001, EY02587, EY12691 and, EY013875-01A1,
NSF SBR-9631682, 0240148, HFSP RG00109/1999-B and EPSRC GR/R57157/01.
We thank Zili Liu for helpful comments.
LITERATURE CITED