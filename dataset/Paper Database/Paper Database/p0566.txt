Petuum: A New Platform for Distributed Machine Learning on Big Data
Eric P. Xing1, Qirong Ho2, Wei Dai1, Jin Kyu Kim1, Jinliang Wei1, Seunghak Lee1, Xun Zheng1,
Pengtao Xie1, Abhimanu Kumar1, and Yaoliang Yu1
1School of Computer Science, Carnegie Mellon University
2Institute for Infocomm Research, A*STAR, Singapore
{epxing,wdai,jinkyuk,jinlianw,seunghak,xunzheng,pengtaox,yaoliang}@cs.cmu.edu
{hoqirong,abhimanyu.kumar}@gmail.com
May 18, 2015
What is a systematic way to eﬃciently apply a wide
industrial
scale problems, using Big Models (up to 100s of billions of parameters) on Big Data (up to terabytes or
petabytes)?
Modern parallelization strategies employ
ﬁne-grained operations and scheduling beyond the classic
bulk-synchronous processing paradigm popularized by
MapReduce, or even specialized graph-based execution
that relies on graph representations of ML programs.
The variety of approaches tends to pull systems and
algorithms design in diﬀerent directions, and it remains
diﬃcult to ﬁnd a universal platform applicable to a
wide range of ML programs at scale.
We propose a
general-purpose framework that systematically addresses
data- and model-parallel challenges in large-scale ML,
by observing that many ML programs are fundamentally
optimization-centric
error-tolerant,
iterative-convergent algorithmic solutions. This presents
unique opportunities for an integrative system design,
such as bounded-error network synchronization and
dynamic scheduling based on ML program structure. We
demonstrate the eﬃcacy of these system designs versus
well-known implementations of modern ML algorithms,
allowing ML programs to run in much less time and at
considerably larger model sizes, even on modestly-sized
compute clusters.
Introduction
Machine learning (ML) is becoming a primary mechanism
for extracting information from data. However, the surging volume of Big Data from Internet activities and sensory advancements, and the increasing needs for Big Models for ultra high-dimensional problems have put tremendous pressure on ML methods to scale beyond a single
machine, due to both space and time bottlenecks. For
example, the Clueweb 2012 web crawl1 contains over 700
million web pages as 27TB of text data, while photosharing sites such as Flickr, Instagram and Facebook are
anecdotally known to possess 10s of billions of images,
again taking up TBs of storage. It is highly ineﬃcient,
if possible, to use such big data sequentially in a batch
or scholastic fashion in a typical iterative ML algorithm.
On the other hand, state-of-the-art image recognition systems have now embraced large-scale deep learning models
with billions of parameters ; topic models with up to
106 topics can cover long-tail semantic word sets for substantially improved online advertising ; and veryhigh-rank matrix factorization yields improved prediction
on collaborative ﬁltering problems . Training such big
models with a single machine can be prohibitively slow,
if possible.
Despite the recent rapid development of many new
ML models and algorithms aiming at scalable application , adoption of these technologies
remains generally unseen in the wider data mining, NLP,
vision, and other application communities for big problems, especially those built on advanced probabilistic or
optimization programs. We suggest that, from the scalable execution point of view, what prevents many stateof-the-art ML models and algorithms from being more
widely applied at Big-Learning scales is the diﬃcult migration from an academic implementation, often specialized for a small, well-controlled computer platform such
as desktop PCs and small lab-clusters, to a big, less predictable platform such as a corporate cluster or the cloud,
where correct execution of the original programs require
careful control and mastery of low-level details of the distributed environment and resources through highly nontrivial distributed programming.
1 
 
Figure 1: The scale of Big ML eﬀorts in recent literature. A
key goal of Petuum is to enable larger ML models to be run
on fewer resources, even relative to highly-specialized implementations.
Many platforms have provided partial solutions to
bridge this research-to-production gap: while Hadoop 
is a popular and easy to program platform, the simplicity of its MapReduce abstraction makes it diﬃcult to exploit ML properties such as error tolerance (at least, not
without considerable engineering eﬀort to bypass MapReduce limitations), and its performance on many ML programs has been surpassed by alternatives . One
such alternative is Spark , which generalizes MapReduce and scales well on data while oﬀering an accessible programming interface; yet, Spark does not oﬀer
ﬁne-grained scheduling of computation and communication, which has been shown to be hugely advantageous,
if not outright necessary, for fast and correct execution
of advanced ML algorithms . Graph-centric platforms
such as GraphLab and Pregel eﬃciently partition
graph-based models with built-in scheduling and consistency mechanisms; but ML programs such as topic modeling and regression either do not admit obvious graph
representations, or a graph representation may not be
the most eﬃcient choice; moreover, due to limited theoretical work, it is unclear whether asynchronous graphbased consistency models and scheduling will always yield
correct execution of such ML programs. Other systems
provide low-level programming interfaces , that,
while powerful and versatile, do not yet oﬀer higher-level
general-purpose building blocks such as scheduling, model
partitioning strategies, and managed communication that
are key to simplifying the adoption of a wide range of ML
methods. In summary, existing systems supporting distributed ML each manifest a unique tradeoﬀon eﬃciency,
correctness, programmability, and generality.
In this paper, we explore the problem of building a
distributed machine learning framework with a new angle toward the eﬃciency, correctness, programmability,
and generality tradeoﬀ. We observe that, a hallmark of
most (if not all) ML programs is that they are deﬁned by
an explicit objective function over data (e.g., likelihood,
error-loss, graph cut), and the goal is to attain optimality of this function, in the space deﬁned by the model
parameters and other intermediate variables. Moreover,
these algorithms all bear a common style, in that they resort to an iterative-convergent procedure (see Eq. 1). It is
noteworthy that iterative-convergent computing tasks are
vastly diﬀerent from conventional programmatic computing tasks (such as database queries and keyword extraction), which reach correct solutions only if every deterministic operation is correctly executed, and strong consistency is guaranteed on the intermediate program state
— thus, operational objectives such as fault tolerance and
strong consistency are absolutely necessary. However, an
ML program’s true goal is fast, eﬃcient convergence to
an optimal solution, and we argue that ﬁne-grained fault
tolerance and strong consistency are but one vehicle to
achieve this goal, and might not even be the most eﬃcient one.
We present a new distributed ML framework, Petuum,
built on an ML-centric optimization-theoretic principle,
as opposed to various operational objectives explored earlier. We begin by formalizing ML algorithms as iterativeconvergent programs, which encompass a large space of
modern ML such as stochastic gradient descent, MCMC
for determining point estimates in latent variable models , coordinate descent, variational methods for graphical models , proximal optimization for structured
sparsity problems , among others. To our knowledge,
no existing ML platform has considered such a wide spectrum of ML algorithms, which exhibit diverse representation abstractions, model and data access patterns, and
synchronization and scheduling requirements.
are the shared properties across such a “zoo of ML algorithms”? We believe that the key lies in the recognition of
a clear dichotomy between data (which is conditionally independent and persistent throughout the algorithm) and
model (which is internally coupled, and is transient before converging to an optimum). This inspires a simple
yet statistically-rooted bimodal approach to parallelism:
data parallel and model parallel distribution and execution of a big ML program over a cluster of machines.
This data parallel, model parallel approach keenly exploits
the unique statistical nature of ML algorithms, particularly the following three properties: (1) Error tolerance —
iterative-convergent algorithms are often robust against
limited errors in intermediate calculations; (2) Dynamic
structural dependency — during execution, the changing correlation strengths between model parameters are
critical to eﬃcient parallelization; (3) Non-uniform convergence — the number of steps required for a parameter
to converge can be highly skewed across parameters. The
core goal of Petuum is to execute these iterative updates
in a manner that quickly converges to an optimum of
the ML program’s objective function, by exploiting these
three statistical properties of ML, which we argue are
fundamental to eﬃcient large-scale ML in cluster environments.
This design principle contrasts that of several existing
frameworks discussed earlier. For example, central to the
Spark framework is the principle of perfect fault tolerance and recovery, supported by a persistent memory architecture (Resilient Distributed Datasets); whereas central to the GraphLab framework is the principle of local and global consistency, supported by a vertex programming model (the Gather-Apply-Scatter abstraction).
While these design principles reﬂect important aspects of
correct ML algorithm execution — e.g., atomic recoverability of each computing step (Spark), or consistency
satisfaction for all subsets of model variables (GraphLab)
— some other important aspects, such as the three statistical properties discussed above, or perhaps ones that
could be more fundamental and general, and which could
open more room for eﬃcient system designs, remain unexplored.
To exploit these properties, Petuum introduces three
novel system objectives grounded in the aforementioned
key properties of ML programs, in order to accelerate
their convergence at scale: (1) Petuum synchronizes the
parameter states with a bounded staleness guarantee,
which achieves provably correct outcomes due to the
error-tolerant nature of ML, but at a much cheaper communication cost than conventional per-iteration bulk synchronization; (2) Petuum oﬀers dynamic scheduling policies that take into account the changing structural dependencies between model parameters, so as to minimize parallelization error and synchronization costs; and (3) Since
parameters in ML programs exhibit non-uniform convergence costs (i.e. diﬀerent numbers of updates required),
Petuum prioritizes computation towards non-converged
model parameters, so as to achieve faster convergence.
To demonstrate this approach, we show how a dataparallel and a model-parallel algorithm can be implemented on Petuum, allowing them to scale to large model
sizes with improved algorithm convergence times. This is
illustrated in Figure 1, where Petuum is able to solve a
range of ML problems at reasonably large model scales,
even on relatively modest clusters (10-100 machines) that
are within reach of most ML practitioners. The experiments section provides more detailed benchmarks on a
range of ML programs: topic modeling, matrix factorization, deep learning, Lasso regression, and distance metric
learning. These algorithms are only a subset of the full
open-source Petuum ML library2, which includes more
algorithms not explored in this paper: random forests, Kmeans, sparse coding, MedLDA, SVM, multi-class logistic
regression, with many others being actively developed for
future releases.
Preliminaries: On Data and
Model Parallelism
We begin with a principled formulation of iterativeconvergent ML programs, which exposes a dichotomy of
data and model, that inspires the parallel system architecture (§3), algorithm design (§4), and theoretical analysis (§5) of Petuum. Consider the following programmatic
view of ML as iterative-convergent programs, driven by
an objective function:
Iterative-Convergent ML Algorithm:
Given data
D and model L (i.e., a ﬁtness function such as RMS
loss, likelihood, margin), a typical ML problem can be
grounded as executing the following update equation iteratively, until the model state (i.e., parameters and/or
latent variables) A reaches some stopping criteria:
A(t) = F(A(t−1), ∆L(A(t−1), D))
where superscript (t) denotes iteration. The update function ∆L() (which improves the loss L) performs computation on data D and model state A, and outputs intermediate results to be aggregated by F(). For simplicity,
in the rest of the paper we omit L in the subscript with
the understanding that all ML programs of our interest
here bear an explicit loss function that can be used to
monitor the quality of convergence and solution, as oppose to heuristics or procedures not associated such a loss
In large-scale ML, both data D and model A can be
very large.
Data-parallelism, in which data is divided
across machines, is a common strategy for solving Big
Data problems, while model-parallelism, which divides the
ML model, is common for Big Models. Below, we discuss
the (diﬀerent) mathematical implications of each parallelism (see Fig. 2).
Data Parallelism
In data-parallel ML, the data D is partitioned and assigned to computational workers (indexed by p = 1..P);
we denote the p-th data partition by Dp.
that the function ∆() can be applied to each of these
data subsets independently, yielding a data-parallel update equation:
A(t) = F(A(t−1), PP
p=1 ∆(A(t−1), Dp)).
2Petuum is available as open source at 
Data Parallel
Model Parallel
~✓t+1 = ~✓t + ∆f~✓(D)
D ⌘{D1, D2, . . . , Dn}
2 , . . . , ~✓T
Di?Dj | ✓, 8i 6= j
~✓i 6? ~✓j | D, 9(i, j)
Figure 2: The diﬀerence between data and model parallelism:
data samples are always conditionally independent given the
model, but there are some model parameters that are not independent of each other.
In this deﬁnition, we assume that the ∆() outputs are
aggregated via summation, which is commonly seen in
stochastic gradient descent or sampling-based algorithms.
For example, in distance metric learning problem which
is optimized with stochastic gradient descent (SGD), the
data pairs are partitioned over diﬀerent workers, and the
intermediate results (subgradients) are computed on each
partition and are summed before applied to update the
model parameters.
Other algorithms can also be expressed in this form, such as variational EM algorithms
p=1 ∆(A(t−1), Dp).
Importantly, this additive
updates property allows the updates ∆() to be aggregated
at each local worker before transmission over the network,
which is crucial because CPUs can produce updates ∆()
much faster than they can be (individually) transmitted
over the network. Additive updates are the foundation
for a host of techniques to speed up data-parallel execution, such as minibatch, asynchronous and boundedasynchronous execution, and parameter servers. Key to
the validity of additivity of updates from diﬀerent workers is the notion of independent and identically distributed
(iid) data, which is assumed for many ML programs, and
implies that each parallel worker contributes “equally”
(in a statistical sense) to the ML algorithm’s progress via
∆(), no matter which data subset Dp it uses.
Model Parallelism
In model-parallel ML, the model A is partitioned and
assigned to workers p = 1..P and updated therein in
parallel, running update functions ∆().
Unlike dataparallelism,
each update function ∆() also takes a
scheduling function S(t−1)
(), which restricts ∆() to operate on a subset of the model parameters A:
A(t−1), {∆(A(t−1), S(t−1)
(A(t−1)))}P
where we have omitted the data D for brevity and clarity.
() outputs a set of indices {j1, j2, . . . , }, so that ∆()
only performs updates on Aj1, Aj2, . . . — we refer to such
selection of model parameters as scheduling.
Unlike data-parallelism which enjoys iid data properties, the model parameters Aj are not, in general, independent of each other (Figure 2), and it has been established that model-parallel algorithms can only be eﬀective
if the parallel updates are restricted to independent (or
weakly-correlated) parameters . Hence, our
deﬁnition of model-parallelism includes a global scheduling mechanism that can select carefully-chosen parameters for parallel updating.
The scheduling function S() opens up a large design
space, such as ﬁxed, randomized, or even dynamicallychanging scheduling on the whole space, or a subset of,
the model parameters. S() not only can provide safety
and correctness (e.g., by selecting independent parameters and thus minimize parallelization error), but can offer substantial speed-up (e.g., by prioritizing computation
onto non-converged parameters). In the Lasso example,
Petuum uses S() to select coeﬃcients that are weakly correlated (thus preventing divergence), while at the same
time prioritizing coeﬃcients far from zero (which are more
likely to be non-converged).
Implementing Dataand Model-Parallel Programs
Data- and model-parallel programs are stateful, in that
they continually update shared model parameters A.
Thus, an ML platform needs to synchronize A across
all running threads and processes, and this should be
done in a high-performance non-blocking manner that
still guarantees convergence. Ideally, the platform should
also oﬀer easy, global-variable-like access to A (as opposed to cumbersome message-passing, or non-stateful
MapReduce-like functional interfaces). If the program is
model-parallel, it may require ﬁne control over parameter scheduling to avoid non-convergence; such capability
is not available in Hadoop, Spark nor GraphLab without
code modiﬁcation. Hence, there is an opportunity to address these considerations via a platform tailored to dataand model-parallel ML.
a Platform for Distributed ML
A core goal of Petuum is to allow practitioners to easily implement data-parallel and model-parallel ML algorithms.
Petuum provides APIs to key systems that
make data- and model-parallel programming easier: (1)
a parameter server system, which allows programmers
to access global model state A from any machine via
a convenient distributed shared-memory interface that
resembles single-machine programming, and adopts a
ML App Code
ML App Code
Consistency
Controller
Consistency
Controller
Dependency/
Priority Mgr.
Network Layer
parameter exchange channel
scheduling control channel
Scheduling
Petuum system:
scheduler, workers, parameter
bounded-asychronous consistency model that preserves
data-parallel convergence guarantees, thus freeing users
from explicit network synchronization; (2) a scheduler,
which allows ﬁne-grained control over the parallel ordering of model-parallel updates ∆() — in essence, the scheduler allows users to deﬁne their own ML application consistency rules.
Petuum System Design
ML algorithms exhibit several principles that can be exploited to speed up distributed ML programs: dependency structures between parameters, non-uniform convergence of parameters, and a limited degree of error tolerance . Petuum allows practitioners
to write data-parallel and model-parallel ML programs
that exploit these principles, and can be scaled to Big
Data and Big Model applications. The Petuum system
comprises three components (Fig. 3): scheduler, workers, and parameter server, and Petuum ML programs are
written in C++ (with Java support coming in the near
Scheduler:
The scheduler system enables modelparallelism, by allowing users to control which model
parameters are updated by worker machines.
performed through a user-deﬁned scheduling function
schedule() (corresponding to S(t−1)
()), which outputs
a set of parameters for each worker — for example, a
simple schedule might pick a random parameter for every
worker, while a more complex scheduler (as we will show)
may pick parameters according to multiple criteria, such
as pair-wise independence or distance from convergence.
The scheduler sends the identities of these parameters
to workers via the scheduling control channel (Fig. 3),
while the actual parameter values are delivered through
a parameter server system that we will soon explain; the
scheduler is responsible only for deciding which parameters to update. Later, we will discuss some of the theoretical guarantees enjoyed by model-parallel schedules.
Several common patterns for schedule design are worth
highlighting:
the simplest option is a ﬁxed-schedule
(schedule fix()), which dispatches model parameters
A in a pre-determined order (as is common in existing ML algorithm implementations). Static, round-robin
schedules (e.g. repeatedly loop over all parameters) ﬁt
the schedule fix() model.
Another type of schedule is dependency-aware (schedule dep()) scheduling, which allows re-ordering of variable/parameter updates to accelerate model-parallel ML algorithms such as
Lasso regression. This type of schedule analyzes the dependency structure over model parameters A, in order
to determine their best parallel execution order. Finally,
prioritized scheduling (schedule pri()) exploits uneven convergence in ML, by prioritizing subsets of variables U sub ⊂A according to algorithm-speciﬁc criteria,
such as the magnitude of each parameter, or boundary
conditions such as KKT.
Because scheduling functions schedule() may be
compute-intensive, Petuum uses pipelining to overlap
scheduling computations schedule() with worker execution, so workers are always doing useful computation. In
addition, the scheduler is responsible for central aggregation via the pull() function (corresponding to F()), if it
is needed.
Each worker p receives parameters to be updated from the scheduler function schedule(), and then
runs parallel update functions push() (corresponding to
∆()) on data D.
Petuum intentionally does not specify a data abstraction, so that any data storage system
may be used — workers may read from data loaded into
memory, or from disk, or over a distributed ﬁle system or
database such as HDFS. Furthermore, workers may touch
the data in any order desired by the programmer: in dataparallel stochastic algorithms, workers might sample one
data point at a time, while in batch algorithms, workers
might instead pass through all data points in one iteration. While push() is being executed, the model state A
is automatically synchronized with the parameter server
via the parameter exchange channel, using a distributed
shared memory programming interface that conveniently
resembles single-machine programming. After the workers ﬁnish push(), the scheduler may use the new model
state to generate future scheduling decisions.
Parameter Server:
The parameter server (PS) provides global access to model parameters A, via a convenient distributed shared memory API that is similar
to table-based or key-value stores. To take advantage of
ML-algorithmic principles, the PS implements the Stale
Synchronous Parallel (SSP) consistency model ,
which reduces network synchronization and communication costs, while maintaining bounded-staleness convergence guarantees implied by SSP. We will discuss these
guarantees in more detail later.
Programming Interface
Figure 4 shows a basic Petuum program, consisting of
a central scheduler function schedule(), a parallel up-
// Petuum Program Structure
schedule() {
// This is the (optional) scheduling function
// It is executed on the scheduler machines
A_local = PS.get(A)
// Parameter server read
PS.inc(A,change)
// Can write to PS here if needed
// Choose variables for push() and return
svars = my_scheduling(DATA,A_local)
return svars
push(p = worker_id(), svars = schedule()) {
// This is the parallel update function
// It is executed on each of P worker machines
A_local = PS.get(A)
// Parameter server read
// Perform computation and send return values to pull()
// Or just write directly to PS
change1 = my_update1(DATA,p,A_local)
change2 = my_update2(DATA,p,A_local)
PS.inc(A,change1)
// Parameter server increment
return change2
pull(svars = schedule(), updates = (push(1), ..., push(P)) ) {
// This is the (optional) aggregation function
// It is executed on the scheduler machines
A_local = PS.get(A)
// Parameter server read
// Aggregate updates from push(1..P) and write to PS
my_aggregate(A_local,updates)
PS.put(A,change)
// Parameter server overwrite
Figure 4: Petuum Program Structure.
date function push(), and a central aggregation function
pull(). The model variables A are held in the parameter
server, which can be accessed at any time from any function via the PS object. The PS object can be accessed from
any function, and has 3 functions: PS.get() to read a parameter, PS.inc() to add to a parameter, and PS.put()
to overwrite a parameter.
With just these operations,
the SSP consistency model automatically ensures parameter consistency between all Petuum components; no additional user programming is necessary. Finally, we use
DATA to represent the data D; as noted earlier, this can
be any 3rd-party data structure, database, or distributed
ﬁle system.
Petuum Parallel Algorithms
Now we turn to development of parallel algorithms for
large-scale distributed ML problems, in light of the data
and model parallel principles underlying Petuum. We focus on a new data-parallel Distance Metric Learning algorithm, and a new model-parallel Lasso algorithm, but
our strategies apply to a broad spectrum of other ML
problems as brieﬂy discussed at the end of this section.
We show that with the Petuum system framework, we
can easily realize these algorithms on distributed clusters
without dwelling on low level system programming, or
non-trivial recasting of our ML problems into representations such as RDDs or vertex programs. Instead our
ML problems can be coded at a high level, more akin to
Matlab or R.
Data-Parallel Distance Metric Learning
Let us ﬁrst consider a large-scale Distance Metric Learning (DML) problem. DML improves the performance of
other ML programs such as clustering, by allowing domain experts to incorporate prior knowledge of the form
“data points x, y are similar (or dissimilar)” — for
example, we could enforce that “books about science are
diﬀerent from books about art”. The output is a distance
function d(x, y) that captures the aforementioned prior
knowledge. Learning a proper distance metric is essential for many distance based data mining and machine
learning algorithms, such as retrieval, k-means clustering
and k-nearest neighbor (k-NN) classiﬁcation. DML has
not received much attention in the Big Data setting, and
we are not aware of any distributed implementations of
The most popular version of DML tries to learn a Mahalanobis distance matrix M (symmetric and positivesemideﬁnite), which can then be used to measure the distance between two samples D(x, y) = (x −y)TM(x −y).
Given a set of “similar” sample pairs S = {(xi, yi)}|S|
and a set of “dissimilar” pairs D = {(xi, yi)}|D|
learns the Mahalanobis distance by optimizing
(x −y)TM(x −y)
(x −y)TM(x −y) ≥1, ∀(x, y) ∈D
where M ⪰0 denotes that M is required to be positive
semideﬁnite. This optimization problem tries to minimize
the Mahalanobis distances between all pairs labeled as
similar while separating dissimilar pairs with a margin of
In its original form, this optimization problem is diﬃcult to parallelize due to the constraint set. To create a
data-parallel optimization algorithm and implement it on
Petuum, we shall relax the constraints via slack variables
(similar to SVMs). First, we replace M with LTL, and
introduce slack variables ξ to relax the hard constraint in
Eq.(4), yielding
∥L(x −y)∥2 + λ
∥L(x −y)∥2 ≥1 −ξx,y, ξx,y ≥0, ∀(x, y) ∈D
Using hinge loss, the constraint in Eq.(5) can be eliminated, yielding an unconstrained optimization problem:
∥L(x −y)∥2
max(0, 1 −∥L(x −y)∥2)
Unlike the original constrained DML problem, this relaxation is fully data-parallel, because it now treats the dis-
// Data-Parallel Distance Metric Learning
schedule() { // Empty, do nothing }
L_local = PS.get(L) // Bounded-async read from param server
change = 0
for c=1..C
// Minibatch size C
(x,y) = draw_similar_pair(DATA)
(a,b) = draw_dissimilar_pair(DATA)
change += DeltaL(L_local,x,y,a,b)
// SGD from Eq 7
PS.inc(L,change/C)
// Add gradient to param server
pull() { // Empty, do nothing }
Figure 5: Petuum DML data-parallel pseudocode.
similar pairs as iid data to the loss function (just like the
similar pairs); hence, it can be solved via data-parallel
Stochastic Gradient Descent (SGD). SGD can be naturally parallelized over data, and we partition the data
pairs onto P machines.
Every iteration, each machine
p randomly samples a minibatch of similar pairs Sp and
dissimilar pairs Dp from its data shard, and computes the
following update to L:
(x,y)∈Sp 2L(x −y)(x −y)T
(a,b)∈Dp 2L(a −b)(a −b)T · I(∥L(a −b)∥2 ≤1)
where I(·) is the indicator function.
Figure 5 shows pseudocode for Petuum DML, which is
simple to implement because the parameter server system PS abstracts away complex networking code under a
simple get()/read() API. Moreover, the PS automatically ensures high-throughput execution, via a boundedasynchronous consistency model (Stale Synchronous Parallel) that can provide workers with stale local copies of
the parameters L, instead of forcing workers to wait for
network communication. Later, we will review the strong
consistency and convergence guarantees provided by the
SSP model.
Since DML is a data-parallel algorithm, only the parallel update push() needs to be implemented (Figure 5).
The scheduling function schedule() is empty (because
every worker touches every model parameter L), and we
do not need aggregation push() for this SGD algorithm.
In our next example, we will show how schedule() and
push() can be used to implement model-parallel execution.
Model-Parallel Lasso
Lasso is a widely used model to select features in highdimensional problems, such as gene-disease association
studies, or in online advertising via ℓ1-penalized regression . Lasso takes the form of an optimization problem:
β ℓ(X, y, β) + λ
// Model-Parallel Lasso
schedule() {
for j=1..J
// Update priorities for all coeffs beta_j
c_j = square(beta_j) + eta // Magnitude prioritization
(s_1, ..., s_L’) = random_draw(distribution(c_1, ..., c_J))
// Choose L<L’ pairwise-independent beta_j
(j_1, ..., j_L) = correlation_check(s_1, ..., s_L’)
return (j_1, ..., j_L)
push(p = worker_id(), (j_1, ..., j_L) = schedule() ) {
// Partial computation for L chosen beta_j; calls PS.get(beta)
(z_p[j_1], ..., z_p[j_L]) = partial(DATA[p], j_1, ..., j_L)
return z_p
pull((j_1, ..., j_L) = schedule(),
(z_1, ..., z_P) = (push(1), ..., push(P)) ) {
for a=1..L
// Aggregate partial computation from P workers
newval = sum_threshold(z_1[j_a], ..., z_P[j_a])
PS.put(beta[j_a], newval)
// Overwrite to parameter server
Figure 6: Petuum Lasso model-parallel pseudocode.
where λ denotes a regularization parameter that determines the sparsity of β, and ℓ(·) is a non-negative convex
loss function such as squared-loss or logistic-loss; we assume that X and y are standardized and consider (8)
without an intercept. For simplicity but without loss of
generality, we let ℓ(X, y, β) =
2 ∥y −Xβ∥2
2; other loss
functions (e.g. logistic) are straightforward and can be
solved using the same approach . We shall solve this
via a coordinate descent (CD) model-parallel approach,
similar but not identical to .
The simplest parallel CD Lasso , shotgun , selects a
random subset of parameters to be updated in parallel.
We now present a scheduled model-parallel Lasso that
improves upon shotgun: the Petuum scheduler chooses
parameters that are nearly independent with each other,
thus guaranteeing convergence of the Lasso objective. In
addition, it prioritizes these parameters based on their
distance to convergence, thus speeding up optimization.
Why is it important to choose independent parameters
via scheduling? Parameter dependencies aﬀect the CD
update equation in the following manner: by taking the
gradient of (8), we obtain the CD update for βj:
j xkβ(t−1)
where S(·, λ) is a soft-thresholding operator, deﬁned by
S(βj, λ) ≡sign(β) (|β| −λ).
In (9), if xT
j xk ̸= 0 (i.e.,
nonzero correlation) and β(t−1)
̸= 0 and β(t−1)
̸= 0, then
a coupling eﬀect is created between the two features βj
and βk. Hence, they are no longer conditionally independent given the data: βj ̸⊥βk|X, y. If the j-th and the
k-th coeﬃcients are updated concurrently, parallelization
error may occur, causing the Lasso problem to converge
slowly (or even diverge outright).
Petuum’s schedule(), push() and pull() interface is
readily suited to implementing scheduled model-parallel
We use schedule() to choose parameters with
low dependency, and to prioritize non-converged parameters.
Petuum pipelines schedule() and push();
thus schedule() does not slow down workers running
push(). Furthermore, by separating the scheduling code
schedule() from the core optimization code push() and
pull(), Petuum makes it easy to experiment with complex scheduling policies that involve prioritization and
dependency checking, thus facilitating the implementation of new model-parallel algorithms — for example,
one could use schedule() to prioritize according to
KKT conditions in a constrained optimization problem,
or to perform graph-based dependency checking like in
Graphlab . Later, we will show that the above Lasso
schedule schedule() is guaranteed to converge, and gives
us near optimal solutions by controlling errors from parallel execution. The pseudocode for scheduled model parallel Lasso under Petuum is shown in Figure 6.
Other Algorithms
We have implemented other data- and model-parallel algorithms on Petuum as well. Here, we brieﬂy mention a
few, while noting that many others are included in the
Petuum open-source library.
Topic Model (LDA): For LDA, the key parameter
is the “word-topic” table, that needs to be updated by
all worker machines. We adopt a simultaneous data-andmodel-parallel approach to LDA, and use a ﬁxed schedule
function schedule fix() to cycle disjoint subsets of the
word-topic table and data across machines for updating
(via push() and pull()), without violating structural
dependencies in LDA.
Matrix Factorization (MF): High-rank decompositions of large matrices for improved accuracy can be
solved by a model-parallel approach, and we implement it
via a ﬁxed schedule function schedule fix(), where each
worker machine only performs the model update push()
on a disjoint, unchanging subset of factor matrix rows.
Deep Learning (DL): We implemented two types on
Petuum: a general-purpose fully-connected Deep Neural
Network (DNN) using the cross-entropy loss, and a Convolutional Neural Network (CNN) for image classiﬁcation
based oﬀthe open-source Caﬀe project. We adopt a dataparallel strategy schedule fix(), where each worker uses
its data subset to perform updates push() to the full
While this data-parallel strategy could be
amenable to MapReduce, Spark and GraphLab, we are
not aware of DL implementations on those platforms.
Principles and Theory
Our iterative-convergent formulation of ML programs,
and the explicit notion of data and model parallelism,
make it convenient to explore three key properties of
Figure 7: Key properties of ML algorithms: (a) Non-uniform
convergence; (b) Error-tolerant convergence; (c) Dependency
structures amongst variables.
ML programs — error-tolerant convergence, non-uniform
convergence, dependency structures (Fig. 7) — and
to analyze how Petuum exploits these properties in a
theoretically-sound manner to speed up ML program
completion at Big Learning scales.
Some of these properties have previously been successfully exploited by a number of bespoke, large-scale implementations of popular ML algorithms: e.g. topic models , matrix factorization , and deep learning . It is notable that MapReduce-style systems (such
as Hadoop and Spark ) often do not fare competitively against these custom-built ML implementations,
and one of the reasons is that these key ML properties are
diﬃcult to exploit under a MapReduce-like abstraction.
Other abstractions may oﬀer a limited degree of opportunity — for example, vertex programming permits
graph dependencies to inﬂuence model-parallel execution.
Error tolerant convergence
Data-parallel ML algorithms are often robust against minor errors in intermediate calculations; as a consequence,
they still execute correctly even when their model parameters A experience synchronization delays (i.e.
P workers only see old or stale parameters), provided
those delays are strictly bounded .
Petuum exploits this error-tolerance to reduce network
communication/synchronization overheads substantially,
by implementing the Stale Synchronous Parallel (SSP)
consistency model on top of the parameter server
system, which provides all machines with access to the
parameters A.
The SSP consistency model guarantees that if a
worker reads from parameter server at iteration c, it
is guaranteed to receive all updates from all workers
computed at and before iteration c −s −1, where s is the
staleness threshold.
If this is impossible because some
straggling worker is more than s iterations behind, the
reader will stop until the straggler catches up and sends
its updates. For stochastic gradient descent algorithms
(such as the DML program), SSP has very attractive
theoretical properties , which we partially re-state here:
Theorem 1 (adapted from ) SGD under SSP,
convergence in probability:
Let f(x) = PT
be a convex function, where the ft are also convex. We
search for a minimizer x∗via stochastic gradient descent
on each component ∇ft under SSP, with staleness parameter s and P workers. Let ut := −ηt∇tft(˜xt) with
t. Under suitable conditions (ft are L-Lipschitz
and bounded divergence D(x||x′) ≤F 2), we have
η + 2ηL2µγ
2¯ηT σγ + 2
3ηL2(2s + 1)Pτ
t=1 ft(˜xt) −f(x∗),
η2L4(ln T +1)
This means that R[X]
converges to O(T −1/2) in probability with an exponential tail-bound; convergence is faster
when the observed staleness average µγ and variance σγ
are smaller (and SSP ensures both µγ, σγ are as small as
possible). Dai et al. also showed that the variance of x
can be bounded, ensuring reliability and stability near an
optimum .
Dependency structures
Naive parallelization of model-parallel algorithms (e.g.
coordinate descent) may lead to uncontrolled parallelization error and non-convergence, caused by interparameter dependencies in the model. Such dependencies have been thoroughly analyzed under ﬁxed execution schedules (where each worker updates the same set
of parameters every iteration) , but there has
been little research on dynamic schedules that can react to
changing model dependencies or model state A. Petuum’s
scheduler allows users to write dynamic scheduling functions S(t)
p (A(t)) — whose output is a set of model indices
{j1, j20, . . . }, telling worker p to update Aj1, Aj2, . . . —
as per their application’s needs. This enables ML programs to analyze dependencies at run time (implemented
via schedule()), and select subsets of independent (or
nearly-independent) parameters for parallel updates.
To motivate this, we consider a generic optimization
problem, which many regularized regression problems —
including the Petuum Lasso example — ﬁt into:
w∈Rd f(w) + r(w),
where r(w) = P
i r(wi) is separable and f has β-Lipschitz
continuous gradient in the following sense:
f(w + z) ≤f(w) + z⊤∇f(w) + β
where X = [x1, . . . , xd] are d feature vectors. W.l.o.g.,
we assume that each feature vector xi is normalized, i.e.,
∥xi∥2 = 1, i = 1, . . . , d. Therefore |x⊤
i xj| ≤1 for all i, j.
In the regression setting, f(w) represents a leastsquares loss, r(w) represents a separable regularizer (e.g.
ℓ1 penalty), and xi represents the i-th feature column of
the design (data) matrix, each element in xi is a separate
data sample. In particular, |x⊤
i xj| is the correlation between the i-th and j-th feature columns. The parameters
w are simply the regression coeﬃcients.
In the context of the model-parallel equation (3), we
can map the model A = w, the data D = X, and the
update equation ∆(A, Sp(A)) to
jp ←arg min
2 [z −(wjp −1
β gjp)]2 + r(z),
where S(t)
p (A) has selected a single coordinate jp to be
updated by worker p — thus, P coordinates are updated
in every iteration. The aggregation function F() simply
allows each update wjp to pass through without change.
The eﬀectiveness of parallel coordinate descent depends
on how the schedule S(t)
p () selects the coordinates jp. In
particular, naive random selection can lead to poor convergence rate or even divergence, with error proportional
to the correlation |x⊤
jaxjb| between the randomly-selected
coordinates ja, jb .
An eﬀective and cheaplycomputable schedule S(t)
RRP,p() involves randomly proposing a small set of Q > P features {j1, . . . , jQ}, and then
ﬁnding P features in this set such that |x⊤
jaxjb| ≤θ for
some threshold θ, where ja, jb are any two features in
the set of P. This requires at most O(B2) evaluations of
jaxjb| ≤θ (if we cannot ﬁnd P features that meet the
criteria, we simply reduce the degree of parallelism). We
have the following convergence theorem:
Theorem 2 SRRP () convergence:
Let k = 1 and ϵ :=
d(P −1)(ρ−1)
≈(P −1)(ρ−1)
< 1, for constants d, B. After t
iterations,
E[F(w(t)) −F(w⋆)] ≤
where F(w) := f(w)+r(w) and w⋆is a minimizer of F.
For reference, the Petuum Lasso scheduler uses SRRP (),
augmented with a prioritizer we will describe soon.
In addition to asymptotic convergence, we show that
SRRP ’s trajectory is close to ideal parallel execution:
Theorem 3 SRRP () is close to ideal execution:
Sideal() be an oracle schedule that always proposes P random features with zero correlation. Let w(t)
ideal be its parameter trajectory, and let w(t)
RRP be the parameter trajectory of SRRP (). Then,
ideal −w(t)
(T + 1)2 ˆP
for constants C, m, L, ˆP.
The proofs for both theorems can be found in the online
supplement3.
SRRP () is diﬀerent from Scherrer et al. , who precluster all M features before starting coordinate descent,
in order to ﬁnd “blocks” of nearly-independent parameters. In the Big Data and especially Big Model setting,
feature clustering can be prohibitive — fundamentally, it
requires O(M 2) evaluations of |x⊤
i xj| for all M 2 feature
combinations (i, j), and although greedy clustering algorithms can mitigate this to some extent, feature clustering
is still impractical when M is very large, as seen in some
regression problems . The proposed SRRP () only needs
to evaluate a small number of |x⊤
i xj| every iteration, and
we explain next, the random selection can be replaced
with prioritization to exploit non-uniform convergence in
ML problems.
Non-uniform convergence
In model-parallel ML programs, it has been empirically
observed that some parameters Aj can converge in much
fewer/more updates than other parameters .
instance, this happens in Lasso regression because the
model enforces sparsity, so most parameters remain at
zero throughout the algorithm, with low probability of becoming non-zero again. Prioritizing Lasso parameters according to their magnitude greatly improves convergence
per iteration, by avoiding frequent (and wasteful) updates
to zero parameters .
We call this non-uniform ML convergence, which can
be exploited via a dynamic scheduling function S(t)
whose output changes according to the iteration t — for
instance, we can write a scheduler Smag() that proposes
parameters with probability proportional to their current
magnitude (A(t)
j )2. This Smag() can be combined with
the earlier dependency structure checking, leading to a
dependency-aware, prioritizing scheduler. Unlike the dependency structure issue, prioritization has not received
as much attention in the ML literature, though it has
been used to speed up the PageRank algorithm, which is
iterative-convergent .
The prioritizing schedule Smag() can be analyzed in
the context of the Lasso problem.
First, we rewrite
it by duplicating original J
features with opposite
sign: F(β) := minβ 1
2 ∥y −Xβ∥2
j=1 βj. Here, X
contains 2J features and βj ≥0, for all j = 1, . . . , 2J.
Theorem 4 (Adapted from ) Optimality of
Lasso priority scheduler:
Suppose B is the set of
indices of coeﬃcients updated in parallel at the t-th iteration, and ρ is suﬃciently small constant such that
≈0, for all j ̸= k ∈B. Then, the sampling
3 
Figure 8: Left: Petuum DML convergence curve with diﬀerent number of machines from 1 to 4. Right: Lasso convergence curve by Petumm Lasso and Shotgun.
distribution p(j) ∝(δβ(t)
j )2 approximately maximizes a
lower bound on EB[F(β(t)) −F(β(t) + ∆β(t))].
This theorem shows that a prioritizing scheduler speeds
up Lasso convergence by decreasing the objective as much
as possible every iteration. The pipelined Petuum scheduler system approximates p(j) ∝(δβ(t)
j )2 with p′(j) ∝
)2 + η, because δβ(t)
is unavailable until all computations on β(t)
have ﬁnished (and we want schedule
before that happens, so that workers are fully occupied).
Since we are approximating, we add a constant η to ensure all βj’s have a non-zero probability of being updated.
Performance
Petuum’s ML-centric system design supports a variety of
ML programs, and improves their performance on Big
Data in the following senses: (1) Petuum implementations of DML and Lasso achieve signiﬁcantly faster convergence rate than baselines (i.e., DML implemented on
single machine, and Shotgun ); (2) Petuum ML implementations can run faster than other platforms (e.g.
Spark, GraphLab4), because Petuum can exploit model
dependencies, uneven convergence and error tolerance;
(3) Petuum ML implementations can reach larger model
sizes than other platforms, because Petuum stores ML
program variables in a lightweight fashion (on the parameter server and scheduler); (4) for ML programs without
distributed implementations, we can implement them on
Petuum and show good scaling with an increasing number of machines. We emphasize that Petuum is, for the
moment, primarily about allowing ML practitioners to
implement and experiment with new data/model-parallel
ML algorithms on small-to-medium clusters; Petuum currently lacks features that are necessary for clusters with
≥1000 machines, such as automatic recovery from machine failure. Our experiments are therefore focused on
clusters with 10-100 machines, in accordance with our
target users.
Performance of Distance Metric Learning and
4We omit Hadoop, as it is well-established that Spark and
GraphLab signiﬁcantly outperform it .
Matrix Fact.
Relative Speedup
Platforms vs Petuum
GraphLab (out of memory)
GraphLab (out of memory)
Deep Learning: CNN
Distance Metric Learning
Relative Speedup
Single-machine vs Petuum
Ideal Linear Speedup
Petuum Caffe 4 mach
Original Caffe 1 mach
Petuum DML 4 mach
Xing2002 1 mach
Figure 9: Left: Petuum performance: relative speedup vs
popular platforms (larger is better).
Across ML programs,
Petuum is at least 2-10 times faster than popular implementations. Right: Petuum is a good platform for writing cluster versions of existing single-machine algorithms, achieving
near-linear speedup with increasing number of machines (Caﬀe
CNN and DML).
We ﬁrst demonstrate the performance of DML and
lasso, implemented under Petuum. In Figure 8, we showcase the convergence of Petuum and baselines using a
ﬁxed model size (we used a 21504 × 1000 distance matrix for DML; 100M features for Lasso). For DML, increasing the number of machines consistently increases
the convergence speed. Petuum DML achieves 3.8 times
speedup with 4 machines and 1.9 times speedup with 2
machines, demonstrating that Petuum DML has the potential to scale very well with more machines. For Lasso,
given the same number of machines, Petuum achieved a
signiﬁcantly faster convergence rate than Shotgun (which
randomly selects a subset of parameters to be updated).
In the initial stage, Petuum lasso and Shotgun show similar convergence rates because Petuum updates every parameter in the ﬁrst iteration to “bootstrap” the scheduler
(at least one iteration is required to initialize all parameters). After this initial stage, Petuum dramatically decreases the Lasso objective compared to Shotgun, by taking advantage of dependency structures and non-uniform
convergence via the scheduler.
Comparison
Petuum to popular ML platforms (Spark and GraphLab)
and well-known cluster implementations (YahooLDA).
For two common ML programs of LDA and MF, we
show the relative speedup of Petuum over the other platforms’ implementations. In general, Petuum is between
2-6 times faster than other platforms; the diﬀerences help
to illustrate the various data/model-parallel features in
Petuum. For MF, Petuum uses the same model-parallel
approach as Spark and GraphLab, but it performs twice
as fast as Spark, while GraphLab ran out of memory.
On the other hand, Petuum LDA is nearly 6 times faster
than YahooLDA; the speedup mostly comes from scheduling S(), which enables correct, dependency-aware modelparallel execution.
Scaling to Larger Models
Here, we show that Petuum supports larger ML models
for the same amount of cluster memory. Figure 10 shows
Figure 10:
Left: LDA convergence time: Petuum vs YahooLDA (lower is better). Petuum’s data-and-model-parallel
LDA converges faster than YahooLDA’s data-parallel-only implementation, and scales to more LDA parameters (larger
vocab size, number of topics). Right panels: Matrix Factorization convergence time: Petuum vs GraphLab vs Spark.
Petuum is fastest and the most memory-eﬃcient, and is the
only platform that could handle Big MF models with rank
K ≥1000 on the given hardware budget.
ML program running time versus model size, given a ﬁxed
number of machines — the left panel compares Petuum
LDA and YahooLDA; PetuumLDA converges faster and
supports LDA models that are > 10 times larger5, allowing long-tail topics to be captured.
The right panels compare Petuum MF versus Spark and GraphLab;
again Petuum is faster and supports much larger MF
models (higher rank) than either baseline.
model scalability is the result of two factors: (1) modelparallelism, which divides the model across machines; (2)
a lightweight parameter server system with minimal storage overhead.
Fast Cluster Implementations of New ML Programs
We show that Petuum facilitates the development of
new ML programs without existing cluster implementations. In Figure 9 (right), we present two instances: ﬁrst,
a cluster version of the open-source Caﬀe CNN toolkit,
created by adding ∼600 lines of Petuum code. The basic
data-parallel strategy was left unchanged, so the Petuum
port directly tests Petuum’s eﬃciency.
Compared to
the original single-machine Caﬀe with no network communication, Petuum achieves approaching-linear speedup
(3.1-times speedup on 4 machines) due to the parameter
server’s SSP consistency for managing network communication. Second, we compare the Petuum DML program
against the original DML algorithm proposed in (denoted by Xing2002), which is optimized with SGD on a
single-machine (with parallelization over matrix operations). The intent is to show that a fairly simple dataparallel SGD implementation of DML (the Petuum program) can greatly speed up execution over a cluster. The
Petuum implementation converges 3.8 times faster than
Xing2002 on 4 machines — this provides evidence that
Petuum enables data/model-parallel algorithms to be ef-
ﬁciently implemented over clusters.
Experimental settings
5LDA model size is equal to vocab size times number of topics.
We used 3 clusters with varying speciﬁcations, demonstrating Petuum’s adaptability to diﬀerent hardware:
“Cluster-1” has machines with 2 AMD cores, 8GB RAM,
1Gbps Ethernet; “Cluster-2” has machines with 64 AMD
cores, 128GB RAM, 40Gbps Inﬁniband; “Cluster-3” has
machines with 16 Intel cores, 128GB RAM, 10Gbps Ethernet.
LDA was run on 128 Cluster-1 nodes, using 3.9m English Wikipedia abstracts with unigram (V = 2.5m) and
bigram (V = 21.8m) vocabularies. MF and Lasso were
run on 10 Cluster-2 nodes, respectively using the Net-
ﬂix data and a synthetic Lasso dataset with N = 50k
samples and 100m features/parameters.
CNN was run
on 4 Cluster-3 nodes, using a 250k subset of Imagenet
with 200 classes, and 1.3m model parameters. The DML
experiment was run on 4 Cluster-2 nodes, using the 1million-sample Imagenet dataset with 1000 classes
(220m model parameters), and 200m similar/dissimilar
statements.