A Class of End-to-End Congestion Control Algorithms for the Internet
S. Jamaloddin Golestani
Bell Laboratories
Murray Hill NJ 07974
 
Supratik Bhattacharyya
Dept. of Computer Science
University of Massachusetts Amherst
Amherst MA 01002 USA
 
We formulate end-to-end congestion control as a global
optimization problem. Based on this formulation, a class
of minimum cost Ô¨Çow control (MCFC) algorithms for adjusting session rates or window sizes are proposed. Significantly, we show that these algorithms can be implemented
at the transport layer of an IP network and can provide certain fairness properties and user priority options without
requiring non-FIFO switches. Two algorithm versions are
discussed. A coarse version is geared towards implementation in the current Internet, relying on the end-to-end packet
loss observations as indication of congestion. A more complete version anticipates an Internet where sessions can solicit explicit congestion information through a concise probing mechanism. We show that TCP congestion control, after
some modiÔ¨Åcation, may be treated as a special case of the
MCFC algorithms.
1. Introduction
Congestion control in packet networks has proven to be a
difÔ¨Åcult problem, in general. However, this problem is particularly challenging in the Internet, due to very limited degrees of network observability and controllability. In order
to accommodate rapid growth and proliferation, the design
of the IP protocol and the requirements placed on individual
subnetworks have been kept at a minimum. Consequently,
the main form of congestion control possible in the current
Internet is end-to-end control of user trafÔ¨Åc at the transport
layer. As exempliÔ¨Åed by TCP [Jac88], this control must be
exerted using only the limited network observation that sessions can locally make, based on their own performance.
The prevalent form of service discipline in the Internet is
FIFO queuing, and control approaches based on more sophisticated service disciplines are not easily applicable.
Although the current TCP congestion control has been
relatively successful, its ability is exceedingly stretched
by the rapid growth of the Internet and the proliferation
of both real-time and multicast services.
Over the past
several years, considerable effort has been directed at improving the existing techniques of congestion control in
the Internet and at introducing new approaches to accommodate the requirements of new services and applications
[RJ88, CJ89, Flo91, Kes91, Mit92, MS90, MS93, ZDE
Flo94, FJ93, FF96, Bra97].
In this paper, we formulate the end-to-end control of
user trafÔ¨Åc in IP networks as a global optimization problem.
The optimization framework enables us to bring out, in a
comprehensive and concrete manner, the tradeoff between
avoiding congestion and satisfying users and the issues of
fairness and priority among different users. Using this theoretical approach, we come up with a class of congestion
control algorithms which have well-deÔ¨Åned fairness properties and allow for the possibility of providing user priorities through the proper setting of certain design parameters
associated with a particular user or application type.
Although methods of enforcing fairness or user priorities have been extensively studied in recent years, they are
usually based on non-FIFO service scheduling at network
switches where trafÔ¨Åc streams meet and competitions arise
[Zha91, Gol94]. Remarkably, in the class of algorithms presented here, we are able to achieve certain fairness properties and user priority options without requiring non-FIFO
switches. The key to this success is the underlying optimization framework which strikes a balance between the
satisfaction of various users and the congestion cost associated with various links.
Performing global optimization involving users and links
scattered throughout the Internet is a seemingly infeasible
task. Although it is well-known that network optimization
problems can be solved using distributed computations, algorithms proposed for this purpose [Gal77, Gol79, GG80]
have relied on the presence of sophisticated network layer
protocols, a luxury not available in the Internet for end-toend congestion control. A signiÔ¨Åcant accomplishment of
this paper is in showing how such optimization is indeed
feasible in the Internet. We even show that the current TCP
congestion control, after some modiÔ¨Åcation, belongs to the
class of optimal algorithms that we describe. We refer to
these optimal algorithms, either collectively or individually,
as the minimum cost Ô¨Çow control (MCFC) algorithm.
Two versions of the MCFC algorithm, referred to as the
coarse realization and the exact realization, are explored in
this paper. The coarse realization is geared towards implementation in the current Internet. This version of the algorithm, like TCP, relies on the end-to-end packet loss observations made by each session as indication of network
congestion. The exact realization anticipates an Internet
where sessions can solicit explicit congestion information
from the network switches through a concise probing mechanism: some of the data packets of each session include a
short congestion Ô¨Åeld which is modiÔ¨Åed by each switch that
the packet visits.
Our systematic formulation of congestion control in the
Internet provides a concrete framework to address several
topics of increasing importance. One of these topics is congestion control for multicast communications. The theoretical framework developed here extends very naturally to
multicast communications and allows for a systematic exploration of what is fair and how a particular notion of fairness impacts scalability. This subject is presented in a companion paper.
Another research topic of long standing is the comparison of algorithms that are based on explicit and implicit
congestion notiÔ¨Åcation.
By implicit congestion notiÔ¨Åcation we refer to the packet delay and loss observations that
each session can locally make. The class of MCFC algorithms, by including both the coarse and the exact versions,
provides a coherent setting to compare congestion control
based on implicit and explicit notiÔ¨Åcation.
The MCFC algorithm can be applied to both rate-based
and window-based congestion control. Although both of
these methods have been extensively studied, the real distinction between them and their dynamics are not always
well-understood. Our approach to rate-based and windowbased congestion control is founded on a distinction between quasi-static and dynamic Ô¨Çuctuations in network traf-
Ô¨Åc. Therefore, before we embark on developing the optimization framework, we lay out in Section 2 our viewpoint on different time scales of congestion control and the
corresponding roles that may be played by rate-based and
window-based mechanisms.
2. Multiple Scales of Congestion Control
The dynamics of a network congestion control strategy
can span multiple time scales. On the fastest time scale,
congestion control should provide protection against sudden surges of trafÔ¨Åc by quick reaction to buffer overloads.
The reaction time in this type of control is, at best, in the
order of one round-trip time since that is how fast news of
congestion can reach a source node and the response to it
propagate back to the trouble spot. We refer to this type
of congestion control as dynamic and to the corresponding
time scale as short term. On a slower time scale, congestion
control could mean gradual but more steady reaction to the
build-up of congestion, as perceived over a period involving tens or hundreds of round-trip times. It is on this time
scale that notions such as the average transmission rate of
a session, rate allocation, and fairness become meaningful.
We shall use the terms quasi-static and medium term to refer to this type of congestion control and the corresponding
time scale, respectively. Still, on slower time scales, congestion control can include longer term activities such as
service scheduling on network switches and network recon-
Ô¨Åguration, where possible. Our discussion in this paper is
limited to the Ô¨Årst two parts, i.e., dynamic and quasi-static
congestion control.
Now, consider a window scheme for end-to-end congestion control. A window scheme keeps the amount of outstanding data for a given session limited to a maximum,
called the window size. The window size may be changed
in response to changing network conditions. But, let us Ô¨Årst
see what type of congestion control can be accomplished by
window scheme if the window size is held constant. Consider a session
  with a window size
 . Assume an inÔ¨Ånite
source for the session, so that the window is always fully
utilized. Denote by
 the round-trip time and the
transmission rate of this session, averaged over some short
term interval. According to Little‚Äôs formula,
Even with a Ô¨Åxed
 , as the trafÔ¨Åc increases and network
links approach congestion, the round-trip time
 will increase, resulting in a proportionate reduction in the rate
Notice that the reaction to increased queuing delay
takes place within one round-trip time.
Therefore, window scheme provides a form of dynamic congestion control even if the window size is not adjusted according to
network conditions. If we now take the viewpoint of modifying the window size in response to quasi-static network
conditions, then the window scheme combines dynamic and
quasi-static congestion control.
The rest of this paper is concerned with developing algorithms for adaptive allocation of average session rates in the
Internet, based on quasi-static network conditions. These
allocated rates, once computed (e.g., by the sessions themselves), may be applied in two ways: directly by controlling
the instantaneous transmission rate, or indirectly by a window scheme with the window size set in accordance with
the allocated rate. Let
 denote the average rate allocated
to session
  , based on algorithms to be discussed later. In
the direct, i.e., rate-based approach, in order to apply
minimum spacing between packet transmission times may
be either set to
 , or more generally, determined through
a leaky bucket mechanism. Alternatively, to apply
means of window scheme, the window size
 should be

 is the average round-trip time of session
The above approach combines the fast dynamics of window scheme with the quasi-static rate adjustments, provided
 in (2) is the medium term average round-trip time of
  . To see why the medium term qualiÔ¨Åcation in
is necessary, let us denote by
 the round-trip time
and the transmission rate of session
  , averaged over some
short term interval. It follows that:
clearly showing that with the increase of short term roundtrip time
 , the short term rate
 decreases, providing the
quick reaction to congestion that was discussed. If we instead allow
 to be dominated by short term Ô¨Çuctuations of
round-trip time, then we get
 , in effect
neutralizing the dynamic component of window congestion
Consider using an exponentially weighted running average algorithm to update the estimation of
 , upon observing round-trip time
of each new packet
is in the order of
 , then the averaging interval in
(4) is roughly one round-trip time (i.e., the time it takes to
 packets), which is not what we want. For averaging
to take place over medium term,
in (4) should be at least
one or two orders of magnitude smaller than
In Sections 3 the quasi-static allocation of session rates
is formulated as a global optimization problem. While the
solution algorithms are initially expressed in terms of the
session rates
 , we later use (2) to convert them to algorithms for directly updating the window sizes
3. A Global Optimization Framework
In this section, we formulate the allocation of session
rates in a packet network as a global optimization problem.
Formulation of network congestion control as a convex optimization problem was Ô¨Årst studied in [Gol79, GG80]. However, in that study, routing and congestion control are treated
as a uniÔ¨Åed problem. Consequently, congestion control parameters are determined based on and in conjunction with
routing parameters, making the approach inapplicable to IP
Consider a packet network, such as the Internet and denote the communication links (including both point-to-point
and multiple access links) by integers
  and the
network sessions by integers
 . By a session, we
refer to a one way Ô¨Çow of trafÔ¨Åc between a given source and
a given destination. Let
 , denote the average rate of trafÔ¨Åc
of session
 , denote the average rate of trafÔ¨Åc of link
 , all in bits/sec. The averaging intervals used in evaluating
 are medium term, i.e. they consist of tens or
hundreds of round-trip times. We refer to
 , more
brieÔ¨Çy, as rate of session
  , and Ô¨Çow of link
 , respectively.
Denote the vector of session rates by
and the vector of link Ô¨Çows by
(') .
We would like to come up with a quasi-static congestion control algorithm which determines and allocates the
session rates
 , based on the existing user demand and
some reasonable notion of fairness. This goal can be accomplished by formulating network congestion control as
a global optimization problem. We take the viewpoint of a
supplier of network resources who can lose revenue either
by failing to meet user demands or by causing long delays
and lost messages due to congestion. Thus for each session
  using the network we create a cost function
is a decreasing function of the rate
 allocated to
  (Figure 1). As
 is decreased, the cost in user dissatisfaction
clearly increases. Similarly, for each communication link
of the network, we create a cost function
&, (Figure 2),
which is an increasing function of the Ô¨Çow
( on that link.
As the Ô¨Çow approaches the capacity of the link, the average
queue length of messages waiting to traverse the link increases and the danger of congestion goes up. Thus,
should represent the cost of this congestion danger.
Figure 1. Cost of
limiting the rate of
Figure 2. Congestion cost of the Ô¨Çow
562 is the link
transmission speed.
The implications of different forms of the link and session cost functions will be discussed later. For the time being, we simply restrict them to be twice differentiable and
convex, i.e., have nonnegative second derivative. We also
assume that
9, are decreasing and increasing functions, respectively.
We distinguish between two types of possible routing in
the network, single-path and multi-path routing. In singlepath routing (such as the routing in the current Internet), the
same path is applied to the trafÔ¨Åc of each session, until the
routing tables are changed. In multi-path routing, that we
consider here for the sake of generality, a session‚Äôs trafÔ¨Åc
may be divided over different paths leading to its destination. Of course, in both single-path and multi-path routing,
the routing tables could be updated over the time. Unless
otherwise stated, the discussions and conclusions of this paper apply to both single-path and multi-path routing.
 denote the fraction of trafÔ¨Åc of
  that is carried
 . Obviously, in single-path routing,
 is one for
 along the path of
  and zero, otherwise. In contrast, in multi-path routing,
 may assume any value between zero and one. ¬øFrom the above deÔ¨Ånitions it follows
which simply says that the Ô¨Çow of each link is the sum of
the rate of all sessions carried over it. Notice that if
, were taken to be short term averages, then (5) would not
be quite correct, and in fact the right side minus the left
side would represent the rate of buffer build up at link
Also, notice that in (5), packets lost at
 are assumed to be a
negligible part of the total link trafÔ¨Åc.
While, the routing parameters
 show up in our formulation of congestion control and subsequent derivations,
it turns out that the resulting end-to-end congestion control algorithm is realizable without requiring explicit knowledge about them. However, an assumption regarding the
time scale of routing updates in the network is necessary
for our formulation to be valid: the routing updates should
take place over a time scale sufÔ¨Åciently longer than the time
scale of congestion control updates so that the routing parameters can be assumed to be relatively constant.
denote the average transmission rate desired by
  . The rate
 to be allocated to each session
the congestion control algorithm should satisfy:
As we shall see, an explicit knowledge of the desired rates
 is not required by the rate allocation algorithms discussed
in this paper. These parameters only appear in the interim
mathematical steps leading to those algorithms.
We formulate network congestion control based on the
following optimization problem expressed in terms of the
rate vector
to be allocated to network sessions:
subject to constraint (6) and the expression of link Ô¨Çows
In order to state the optimality conditions for this problem, we introduce two new functions. First, we deÔ¨Åne the
incremental reward function (or reward function for short)
of each session
 is the incremental decrease in the cost of session
 (due to user dissatisfaction) for a unit of increase in the allocated rate
 , hence the name incremental reward function.
 is by deÔ¨Ånition a decreasing convex function,
 is positive and decreasing.
Next we deÔ¨Åne the congestion measure of a session
the incremental cost of network congestion due to a unit of
increase in
 is denoted as a function of
 to emphasize its dependence
on the link Ô¨Çows in the network. Since by assumption, link
cost functions are increasing and their derivatives are positive, a session‚Äôs congestion measure is always a positive
quantity. Clearly, with single-path routing, (9) reduces to
 denotes the path of session
  . We observe that
with single-path routing (e.g., in the Internet), the congestion measure of each session
  equals sum of the incremental costs of the links along its path.
Applying Kuhn-Tucker theory [Lue89] to the optimization problem (7), we are led to the following optimality conditions:
Theorem 1 Assume that
 have Ô¨Årst and second derivatives satisfying
  . The following is a set of necessary and sufÔ¨Åcient
conditions for the session rate vector
 to minimize (7) subject to the constraint (5) and (6):
('8 is the
link Ô¨Çow vector corresponding to
" , as given by (5).
The interpretation of the above optimality condition is
straight forward: at the optimal point
" , as long as constraint (6) is not active for a session
  , the session‚Äôs incremental reward function should be equal to the incremental
cost of congestion, i.e., the session‚Äôs congestion measure.
 ), that is if
cannot be decreased
(increased) anymore, then the session‚Äôs incremental reward
function can be smaller (larger) than the session‚Äôs congestion measure.
3.1. Iterative Distributed Algorithms for the Solution
The constrained convex optimization problem (7) can be
solved by means of a gradient projection algorithm with the
following iterations:
This algorithm converges to the optimal point of (7), provided that the step size
is properly chosen [Lue89]. We
shall refer to this algorithm or its variants as the minimum
cost Ô¨Çow control (MCFC) algorithm.
Distributed execution of Iterations (12) by various sessions in the network is possible if, prior to each iteration, the
current values of congestion measures
 are available. In
Section 4, we will show how these congestion measures can
be locally evaluated by the corresponding sessions, without
involvement of the IP layer.
A priori knowledge of the desired session rates
not actually necessary for the execution of the MCFC algorithm. When updating session rates, we can simply disregard the upper bounds
 , and let the course of action determine whether or not a session
  fully utilizes the allocated
rate. In other words, we can replace Iteration (12) by
 is the rate allocated to session
 which may or
may not be fully utilized, and
 is the average rate actually
attained by
  during the past iteration interval. In the rest
of this paper, we simplify the presentation by ignoring the
distinction between
 , which reduces (13) to:
This simpliÔ¨Åcation is equivalent to assuming that sessions
are always greedy, i.e., they utilize whatever rate is allocated
to them. The extension of subsequent results in the paper to
the more general scenario is straight forward.
The speed of convergenceof the MCFC algorithm can be
signiÔ¨Åcantly improved by incorporating the second derivatives of the cost function in Iteration (7) [Lue89, BG92], as
In single-path routing, (16) reduces to
which facilitates the end-to-end evaluation of
( , as discussed later.
We defer a discussion of other considerations, such as
asynchronous implementation of the MCFC algorithm and
the impact of quasi-static trafÔ¨Åc Ô¨Çuctuations to Section 4.
In the next two subsections, we study the role of the session
and link cost functions more closely, and identify some appropriate forms for them.
3.2. Fairness, Priorities, and the Impact of Session
Reward Functions
In order to study the fairness properties of the session
rate allocations at the optimal point, let us consider two
 , with identical cost functions and (therefore) identical reward functions, i.e.,
 . Assume that the congestion measures seen
by these two sessions at the optimal point are the same, i.e.,
 . This means that sessions
the same increase in the total cost of network congestion for
a unit of increase in their rate. This condition would apply
if, for example,
share the same (set of) routes in
the network. According to (11), as long as the desired rates
permit, from
" , it follows that
 ; hence the following corollary:
Corollary 1 To the extent permitted by session desired
rates, at the optimal point of the MCFC algorithm, equal
rates are allocated to sessions experiencing the same degree of network congestion (i.e., having the same congestion measures), unless the corresponding reward functions
are different.
In order to see how the rate allocated to a session may be
inÔ¨Çuenced by the form of its reward function, we consider
the class of reward functions:
for some positive
 in Fig. 3). We
can see from (18) that if the desired rate
 is large enough,
Comparison of different
forms of a session‚Äôs
reward function :
correspond to
respectively.
illustrates
the reward function
shown to be associated with TCP.
Figure 4. Comparison
choices for the incremental congestion cost
of a link :
 correspond to (23) with
respectively.
2 , used in
the coarse realization.
562 is the link
transmission speed.
It follows that,
A few observation can be made from these results. First,
we notice that the allocated rate is proportional to
Therefore, a session with a large trafÔ¨Åc volume, may be accommodated by assigning to it a large
 . Next, we see in
(19) that as congestion builds up in the network and
 increases, the allocated session rate decreases and the change
is inversely proportional to
 , a doubling
of the congestion measure
 , reduces the allocated rate by
 becomes larger, the sensitivity of the allocated
rate to the congestion measure goes down. For example, for
/. , a doubling of the congestion measure cuts the allocated rate only by 16%. The role of
 is best illustrated by
(20) which relates the changes in
 , in percentages
We conclude that
 can be used as a priority assignment to sessions. Sessions with larger
 , would be cut
less severely in response to network congestion. Similarly,
 makes sessions less sensitive to the number of
hops they must traverse in the network. We should realize,
however, that this advantage is only relative. If all sessions
are assigned a large
 , the congestion measures
 will increase enough until every body is cut back to the proper
usage level, as discussed in Section 3.3.
As we will see later, realization of the MCFC algorithm
in the current Internet is best accommodated by using session reward functions that are conÔ¨Åned to an upper bound
 . Keeping in mind that the reward functions must
be positive and decreasing, we are led to consider the following class of reward functions, as another example:
for some positive
7 in Fig. 3). The priority implication of the index
 , earlier discussed in connection with (18), also applies to (21) as long as
where the functional form of (21) converges to that of (18).
We will later show that the reward function (21), with
 , is actually associated with the TCPreno congestion control algorithm.
Notice that the fairness property stated in Corollary 1 applies regardless of the form of the reward function, provided
that it is the same for the sessions in comparison.
3.3. Congestion Avoidance and the Impact of Link
Cost Functions
The purpose of including link cost functions
the optimization problem (7) is to inhibit the algorithm from
driving the links into congestion by accepting too much traf-
Ô¨Åc from the sessions. By proper choice of these cost functions, it is possible to keep the packet loss probability of
each link below some desired maximum, as will be shown
next. Rather than specifying the form of the cost functions
&,- , we carry out the discussion directly in terms of the
incremental costs
 , which are the actual quantities
needed for the evaluation of congestion measures
keep in mind that for
9, to be increasing and convex,
&,- must be positive and increasing.
Consider a link
 and denote the probability of packet
losses due to buffer overÔ¨Çow over this link by
 and the
desired cap on this loss probability by
 . With a given
buffer size and given trafÔ¨Åc statistics,
, is a monotonically
increasing function of the link Ô¨Çow
 . Denote by
link Ô¨Çow at which the maximum permissible loss probability is reached, i.e.,
Theorem 2 Consider a link
 be an arbitrary
positive increasing function of
 . It follows that, at the optimal point of (7),
Therefore, once the algorithm converges to its optimal
point, the loss probability of link
 is guaranteed to be below the desired cap
 . An analytically simple form for the
incremental cost function of link
is decreased,
 becomes steeper
(Fig. 4, curves
 ), which on the one hand, increases
the link utilization at the optimal point and, on the other
hand, reduces the speed of convergence.
So far, the incremental congestion cost of a link is speci-
Ô¨Åed as an explicit function of the link Ô¨Çow. Therefore, in the
actual running of the algorithm, the link Ô¨Çow must be measured and plugged into the function
 , to evaluate the
incremental congestion cost. Alternatively, it is possible to
use the average queue length of a link as the measurement
parameter based on which the incremental congestion cost
is speciÔ¨Åed. Let
 denote the average queue length of link
 . Denote by
 , the average queue length corresponding to
 , i.e., the average queue length at which
Consider specifying
 as an increasing function of
which approaches inÔ¨Ånity at
 . For example, let:
It is easy to verify that
9, , as implicitly deÔ¨Åned in the
above, satisÔ¨Åes the properties required by Theorem 2.
It must be noted that the strong congestion avoidance
property stated in Theorem 2, hinges on the ability to specify the threshold parameters
 in (23) or
 in (24), based
on the desired loss probability cap
 . Obviously, the relationship between these parameters depends on the statistics
of the trafÔ¨Åc passing through the link, which is not easily
predictable. Therefore, the threshold parameter of choice,
 , must be speciÔ¨Åed in anticipation of likely
changes in trafÔ¨Åc statistics, such as burstiness. A main distinction between deÔ¨Åning the incremental congestion cost
directly in terms of
 , or implicitly in terms of
) , is in the
sensitivity of the corresponding threshold parameter to the
trafÔ¨Åc statistics. Intuitively, it seems that
 should be less
sensitive than
 to changes in trafÔ¨Åc statistics, suggesting
that the incremental congestion cost should be speciÔ¨Åed in
terms of the average queue length. More research is needed
to conclusively determine the best measurement parameter(s) to be used for specifying the incremental congestion
cost of a link.
4. Realization of the MCFC algorithm in IP
The main difÔ¨Åculty facing the realization of the MCFC
algorithm (14) is the distributed computation of the congestion measures
 . In a network with a highly developed network layer, the task of computing congestion measures and
distributing them to the corresponding sessions (or access
points) can be performed by a specially designed network
layer protocol, in possible cooperation with the routing protocol. In the Internet or other IP networks, realization of
the MCFC algorithm is more challenging since it should be
done without explicit knowledge of the routing parameters
and without expecting cooperation from the IP layer.
In this section, we introduce two possible realization for
the MCFC algorithm at the transport layer of an IP network:
an exact realization requiring modest cooperation by network switches, and a coarse realization with no such requirement, which is therefore applicable to the current Internet. We have also come up with a hybrid realization of
the algorithm in a network consisting of both cooperative
and non-cooperative switches, facilitating transition from
the coarse to the exact realization when the necessary protocol and switch enhancements are gradually introduced. Due
to space limitation, this hybrid realization is not discussed
4.1. Exact Realization with Switch Cooperation
Distributed execution of the MCFC algorithm (14) by
various network sessions is possible if the sessions have a
way of evaluating the corresponding congestion measures.
There are two basic requirements for the evaluation of congestion measures
 by each session
  . First, at each link
 , there must be a local capability to evaluate the incremental congestion cost
 , on an ongoing basis. Second,
there must be a way of communicating this information to
the sessions traversing link
 . The method we employ to
relay congestion information to the sessions is both simple
and concise. But more signiÔ¨Åcantly, it relaxes the need for
explicit knowledge about routing parameters, thereby enabling a realization of the algorithm in the Internet.
Consider a packet network with the following capabilities:
1. Each switch (or router) in the network has the capability of estimating
 , for each link originating from
it. This estimation is performed on an on-going basis.
Here, the term switch refers to any multiplexing point
in the network.
2. Some of the data packets traversing the network are
marked by the source (or the access point) as probe
packets. Each probe packet, in addition to user data,
includes a short congestion Ô¨Åeld to carry congestion
information. This Ô¨Åeld is initially set to zero, at the
3. Each switch in the network, before forwarding a probe
packet over a link
 , increments its congestion Ô¨Åeld
by the current estimate of the link‚Äôs incremental cost
Theorem 3 Consider a session
  and a probe packet
belonging to this session. Let
denote the value of the
congestion Ô¨Åeld of
 upon arrival to the destination. Then,
For single-path routing, (25) reduces to
Here, we present an intuitive explanation about this theorem. Consider a short period of time during which the
statistics of network trafÔ¨Åc and the routing parameters do
not change. Let
probe packets from a session
  be transmitted during this interval. Since
 is the fraction of packets of
  which use
is sufÔ¨Åciently large, about
these probe packets traverse
 , each of which will have its
congestion Ô¨Åeld incremented by
9, . The total increase
of the congestion Ô¨Åeld of probe packets which traverse
&,- . Accordingly, the total value of the
congestion Ô¨Åeld of all of the
probe packets upon arrival
to the destination will be
Therefore,
 equals the average of the congestion Ô¨Åeld of
the probe packets. Notice that with single-path routing, the
path traveled by
and the value of
are deterministic,
simplifying (25) to (26).
As Theorem 3 shows, with single-path routing, the value
of a session‚Äôs congestion measure at any given time can be
obtained from a single probe packet. It is interesting to
see whether the second order congestion parameters
 required to implement the second order algorithm (15) can
also be determined in a similar manner. For single-path
routing, in view of the similarity of (10) and (17),
be determined based on an identical approach; it sufÔ¨Åces to
designate a new Ô¨Åeld in each probe packet to second order
information and have this Ô¨Åeld incremented by each visited
switch, in a similar fashion. For multi-path routing, however, it can be shown that there is no way to implement
the second order algorithm (15), short of full cooperation
by the network layer. In the Internet, since the routing is
single-path, both the Ô¨Årst and the second order algorithms
are realizable. For the rest of this paper, we limit our attention to the Ô¨Årst order algorithm (14).
We now consider the important issue of interactions between the quasi-static changes in network trafÔ¨Åc and the
algorithmic iterations in (14). Ideally, one would like to
see the network trafÔ¨Åc remain stationary until the algorithm
converges to its optimal point. In real network operation,
however, due to quasi-static trafÔ¨Åc changes, the optimal
point is not stationary and may be viewed as a moving target that the algorithm tries to reach. Although this target
may not be reached exactly, with a sufÔ¨Åcient speed of convergence, the algorithm should be able to keep up with the
pace of network changes and follow the optimal point relatively closely. Since the network trafÔ¨Åc is an aggregation of
trafÔ¨Åc from many sources, its changes are typically slower
than the dynamics of individual sessions [BG92].
In general, a distributed algorithm such as (14) may be
executed either synchronously, or asynchronously [BT89].
In a loosely connected network such as the Internet, synchronous execution of (14) by various sessions is not feasible. Moreover, the potential beneÔ¨Åt of synchronous execution in terms of providing faster convergence is either
minimized or totally removed by the quasi-static trafÔ¨Åc variations.
In an asynchronous implementation of (14), each session
updates its input rate without timing coordination with other
sessions. To increase the speed of convergence, the session
congestion measures should be updated regularly, based on
regular transmission of probe packets. Similarly, each link
should update its incremental congestion cost on a regular
basis. Evaluation of session congestion measures and link
incremental costs should involve a limited memory span, so
that the information regarding past network status is slowly
forgotten and replaced by the more recent network conditions. This goal may be accomplished by updating session
congestion measures and link average queue lengths, using
the following exponentially weighted running averages:
is the congestion Ô¨Åeld of the received probe
packet, and
 is the queue length at the time of update.
The time constant in the above averaging algorithms (i.e.
the memory span, measured in seconds) is equal to
 ) times the corresponding updating interval. The
choice of this time constant involves a trade-off between
accurately measuring trafÔ¨Åc conditions in the network and
quickly responding to it. Conceptually, it seems desirable
to apply the same time constant to the evaluation of link incremental costs, throughout the network. However, due to
the wide range of link and session transmission rates in a diverse network such as the Internet, it may prove inevitable
to apply different time constants to various parts of the network.
Once a session‚Äôs congestion measure is evaluated, its rate
can be updated through
is a small rate initially allocated to each new
 to enable transmission of probe packets needed
for the initial evaluation of congestion measure. Notice that
a session need not execute (27) and (29) with the same frequency. The congestion measure is updated each time a new
probe packet is received, while the rate may be updated at
the same time, or less frequently.
An alternative to explicitly updating the congestion measure through (27) and using it for rate updates, is to update
the rate directly based on the congestion Ô¨Åeld of the received
probe packets
One can easily verify that the statistical average of the rate
change in (30) is identical to the rate change according to
(29), provided that the right step size
  is used. Although in
this approach, the congestion measure is not explicitly determined, updating the rate through (30) amounts to maintaining an implicit estimation of the congestion measure.
A session‚Äôs rate or congestion measure may be updated
by the source or receiver (or by a policing entity, where such
an entity exists). Obviously, each approach has different implications on the design of transport protocols, the control
information which must be exchanged between the source
and receiver, and the interaction between error control and
congestion control. These issues fall beyond the scope of
the present paper.
4.2. Coarse Realization in the Current Internet
In this section, we develop a realization for the MCFC algorithm without using probe packets and requiring explicit
congestion information from network switches.
In the absence of explicit congestion notiÔ¨Åcation, the
only observation a session can have about the network is
through its own performance, i.e., the loss and delay of its
own packets. We try to choose a form for the cost functions
 so that the resulting congestion measures
best estimated through the available loss and delay information.
Let us denote the end-to-end loss probability and the average delay of packets of session
 , respectively. Similarly, we denote the average delay of each link
Theorem 4 The loss probability and the average delay of
each session
  can be expressed as,
where (32) is valid assuming that all sessions sharing a link
 encounter the same loss probability at that link. The approximation in (32) is good for
By comparing (31) and (32) with (9), we get the following corollary:
Corollary 2 Consider the following incremental congestion costs for the links of the network:
The congestion measure of each session
  can be stated as:
In principle, a session can estimate the average delay
and loss probability associated with its own transmissions.
Therefore, Corollary 2 suggests that if incremental cost
functions of the form (33) are a suitable representation for
the level of congestion on individual links, then the associated congestion measures can be estimated locally by the
sessions, without receiving explicit congestion notiÔ¨Åcation
from the switches. The cost function speciÔ¨Åed in (33) meets
the convexity requirement since
&,- are both
increasing functions of
( . To see how well it can indicate
congestion, we consider the delay and loss terms in (33),
separately. While there is a positive correlation between the
average delay and the level of congestion on a link, average
delay is not indicative of congestion, in itself. Other information, such as the propagation delay and the available
buffer space (or the acceptable range of queuing delays) are
essential to infer the level of congestion associated with a
given average delay. In contrast, the loss probability provides a more conclusive indication of the severity of congestion, suggesting that we should use the second term in
(33), and set
which gives rise to the following congestion measure:
Note however that if a large fraction of losses are due to
transmission error, as could be the case in wireless communications, link loss probability cannot be trusted as a good
indicator of congestion, either.
The strong congestion avoidance property stated in Theorem 2 was based on link cost functions that approach in-
Ô¨Ånity as the link Ô¨Çow exceeds a critical threshold, and does
not apply with link cost functions chosen as in (35). In fact,
it is easy to see that if link cost functions (35) are used in
conjunction with unbounded session reward functions such
as (18), the MCFC algorithm could drive the network into
heavy congestion. If, on the other hand, the reward functions are appropriately bounded, small loss probabilities can
still be guaranteed at the optimal point of the algorithm, as
established by the following theorem:
Theorem 5 Consider a network with single-path routing
and let the following bound be satisÔ¨Åed by all session reward functions:
 . It follows that, at the optimal point of
the MCFC algorithm, for all sessions
  and links
4 , provided that the initial session rates
are sufÔ¨Åciently small and, by themselves, do not lead
to excessive loss.
What is ignored by the above theorem, is the difÔ¨Åculties
and inaccuracies involved in the estimation of congestion
 , an issue that we now explore. To make
an analogy with the estimation of congestion measures using probe packets, we might associate a parameter
each packet
 , and assume that
 , if the packet is
  , otherwise. With this convention, we
notice that,
which parallels (25) in Theorem 3. For the asynchronous
implementation of the MCFC algorithm, we may again estimate
 using the exponentially weighted running average
algorithm (27). In the present case, (27) may be restated as
the following iteration, executed every time a new loss or
successful transmission is observed:
successful transmission,
packet loss.
The algorithmic similarities between estimating
coarse and exact realizations, should not obscure a fundamental difference between the two cases regardingthe range
of statistical Ô¨Çuctuations in
and the accuracy of estimations. We notice from Theorem 3 that in the exact realization in a network with single-path routing, one probe
packet is enough to determine the congestion measure. In
the coarse realization, on the other hand, the analogous parameter
associated with each packet
 , is either one or
zero, with an average typically in the order of few percent or
less. Here, due to the random nature of
, a much larger
number of observations are necessary before algorithm (39)
converges to a reasonable estimation of the end-to-end loss
probability. As a numerical example, if
  , typically
one out of every 100 packets are lost, implying that at least
several hundred observations are needed for a meaningful
estimation of
 . This sharp difference with the exact realization is the result of restricting information about network
status to the packet losses locally observed.
We should emphasize that the choice of link cost functions in (33) or (35) was dictated by the requirement of coming up with congestion measures
 that sessions can locally
evaluate, using their own loss and delay observations. It is
possible to show that, in a network of arbitrary topology,
no other form of link cost functions can satisfy this requirement.
One way to run the coarse MCFC algorithm is to update the rate via (29), based on explicit estimation of
 obtained in (39). An alternative approach, like in the exact realization, is to directly update the rate, upon observing each
new loss or successful transmission, by way of (30). In the
coarse realization, due to the wide random Ô¨Çuctuations of
, (30) constitutes a stochastic gradient algorithm. The
choice of the step size
  in this case involves hard tradeoffs,
as will be illustrated in the simulation section. A small
prolongs the time necessary for the rate of new sessions to
reach the Ô¨Ånal value. A large
  , on the other hand, gives rise
to large oscillations in the session rates, induced by the random Ô¨Çuctuations of
. This difÔ¨Åculty can be overcome
by adopting a variable step size in (30), i.e. adjusting
a function of iteration number, session rate, or some other
parameter.
For the coarse realization, we restate (30) as:
successful trans.
packet loss
In the above equations, we have denoted
  as a function
 , in order to emphasize the possibility of changing the
step size during the course of the algorithm, based on the
value attained by
 , (or some other criteria). According to
(40), a session‚Äôs rate must be increased by
 , each time
a packet is successfully transmitted, and reduced by
each time a packet loss is observed.
As discussed in Section 2, the allocated rate
enforced by means of the window scheme, with the added
beneÔ¨Åt of combining fast dynamics of the window scheme
with the quasi-static control that the MCFC algorithm provides. Using (2) and (40), we get the following iteration for
directly updating the window size:
  
successful trans.
packet loss
Obviously, the smallest feasible value for the initial window
is the size of one packet.
The window version of the MCFC algorithm in (43), reveals signiÔ¨Åcant similarity to TCP congestion control which
also decreases the window size in reaction to packet losses
and increases it when packets are successfully transmitted.
In Section 5.1, we further explore the relationship between
TCP congestion control and the MCFC algorithm.
5. Comparison with Alternative Schemes
In this section, we provide a comparison between
the MCFC algorithm and some of the congestion control schemes previously proposed for the Internet. These
schemes are the TCP congestion control [Jac88] currently
used in the Internet, the Binary Feedback Scheme [RJ88],
the Random Early Detection Gateways [FJ93], and the Dynamic Adaptive Windows [Mit92, MS90, MS93]. While
the global optimization framework is a foundation unique
to the MCFC algorithm, there are important commonalities between the above schemes and the MCFC algorithm,
regarding the underlying ideas or methods of execution.
These common features allow us to apply some of the insight gained from the design and analysis of the MCFC algorithm to other schemes and develop a clearer understanding of the merits and drawbacks of each approach.
5.1. TCP Congestion Control
In order to compare the MCFC algorithm with TCP congestion control, we consider the window-based coarse realization of the algorithm with
 selected
which, in view of (41) and (42), correspond to the reward
which is a special case of (21) for
7 in Fig. 3). Notice that
 has no effect
on the form of the reward function and merely determines
the step size of the algorithm. In the window-based implementation, in view of (44) and (45), window sizes may be
updated using iteration (43) with:
We will refer to the above coarse MCFC algorithm as the
modiÔ¨Åed TCP algorithm. To see the reason behind this naming, consider the function
which differs from
 in the missing term
easy to see that the adjustment of window sizes in TCPreno, after the slow start phase, can be expressed by (43),
 in place of
 and applying coefÔ¨Åcients
 . It turns out that the essential difference between the special case of the MCFC algorithm, here
called modiÔ¨Åed TCP, and TCP-reno is the extra term
In order to determine the impact of the extra term
the modiÔ¨Åed TCP algorithm, let us compute the statistical
average of the window size change during one iteration of
 packet loss
 successful transmission
Assuming that the algorithm reaches the optimal point
 , the statistical average of change at this
point should be zero. It follows that
For the modiÔ¨Åed TCP algorithm, we conclude from (50) and
Notice that the LHS of (55) is equal to
 , reafÔ¨Årming the optimality condition in (11). For TCP-reno, on the
other hand, by substituting
 in (54) and
applying (51) and (52), we get
The impact of the term
in the modiÔ¨Åed TCP algorithm
is now clearly explained by comparing (55) and (56). In
the modiÔ¨Åed TCP algorithm, at the equilibrium point, equal
rates are allocated to sessions with the same end-to-end loss
probability. In contrast, in TCP-reno, equal window sizes
would be allocated to sessions experiencing identical loss
probabilities, should the point of equilibrium be reached.
Another difference between TCP-reno and the MCFC algorithm is in the choice of step size
  . The value of
used in TCP-reno, gives rise to large window size oscillations and prevents convergence to an equilibrium point,
while accelerating reaction to changing trafÔ¨Åc conditions.
In Section 6, using simulation in a simple network, we will
 must be substantially smaller, in order to ensure convergence of the coarse MCFC algorithm. We will
also show how multiple values of
  may be used to combine
rapid increase of the window size after a session‚Äôs initiation,
with ultimate convergence. As discussed earlier, the exact
MCFC algorithm converges much faster than the coarse algorithm.
Both of the above differences with TCP have a positive
impact on the scalability of congestion control algorithms
for multicast communications. In a forthcoming paper, we
show that multicast communications is more scalable (in
terms of the number of receivers) when the applied notion of
fairness is based on transmission rates, rather than window
sizes, and when the window sizes (or rates) undergo slow
adjustments, instead of abrupt changes.
We should also point out that a packet loss refers to different events, when considered in the context of TCP and
the coarse MCFC algorithm. In TCP, a packet loss is registered whenever a packet is lost or whenever several consecutive acknowledgments are lost, while in the coarse MCFC
algorithm a loss exclusively refers to a packet loss in the
forward path. It must be clear that congestion on a sourcereceiver path is best indicated by losses in the forward direction rather than round trip losses, and that the TCP interpretation of packet losses is an implementation necessity
rather than a conceptual preference. In this paper, in order
to focus on the concepts, we have not addressed the details
of protocols needed to implement our algorithms. However,
we mention in passing that updating the rate or window size
based on forward path loss probability is made possible by
executing the update algorithms at the receiver site. This approach has the additional beneÔ¨Åt of improving scalability in
multicast communications by pushing some of the required
processing to the receiver sites.
The above differences notwithstanding, the similarities
between the coarse MCFC algorithm and TCP congestion
control are signiÔ¨Åcant and allow us to extend our earlier observations to TCP. We have noticed that in the coarse MCFC
algorithm, the process of updating rates or window sizes
via (40) or (43), is a substitute for estimating the session
loss probabilities via (39). In other words, updating a session‚Äôs window size via (43) in the coarse MCFC algorithm,
as well as TCP, amounts to an implicit estimation of the loss
probability. In both cases, the end-to-end loss probability
summarizes the observations which are used for congestion
Controlling congestion based on the end-to-end loss
probability has three drawbacks.
First, in cases of high
packet error rates, e.g. where wireless links are encountered, loss probability is not necessarily indicative of congestion, making congestion control on this basis excessively
difÔ¨Åcult. Second, the number of packet transmissions required for a reasonable estimation of loss probability is at
least an order of magnitude larger than the inverse of the
loss probability, itself. This amounts to at least two minutes
of observation for estimating a loss probability of 1%, at a
session rate of 10 packets/sec. Finally, the loss probability
cannot be made too small, for it makes the required observation time even longer. Ironically, in the current Internet,
some packet losses are needed in order to practice congestion control and prevent more losses. The only way to avoid
this irony and the earlier drawbacks is to provide better endto-end observations about network congestion status. In the
next subsections, other proposals for enhancing congestion
observations in the Internet are discussed and compared to
the exact MCFC algorithm.
Some of the observations in this section regarding TCP
congestion control have been previously discussed in the
literature. The impact of making window size increments
proportional to
on the relationship between a session‚Äôs
throughput and round-trip time has been studied by Sally
Floyd [Flo91], through simulation and analysis of a cascade
of congested links. Our results in (55) and (56), which are
applicable to an arbitrary topology, essentially agree with
those in [Flo91]. Lakshman and Madhow [LM97] provide a
detailed analysis and simulation of the performance of TCP
congestion control, in which they approximately show that
the average throughput of each session
  is inversely proportional to
  , where
? . In comparison, we
have found in (56) that the throughput at the equilibrium
point is inversely proportional to
 . In regard to TCP performance, the results in [LM97] should be more accurate
than our conclusion, for two reasons. First, window sizes in
TCP never converge to an equilibrium point whereas (56)
speciÔ¨Åes the window size (and throughput) at the equilibrium point. Second, in TCP, the link loss probability before
and after sessions react to a packet loss is grossly different,
due to substantial drop in the volume of trafÔ¨Åc. For this
reason, there is a considerable time correlation among the
dropping of window sizes on sessions sharing a bottleneck
link, an issue taken into account in [LM97]. Such correlations are negligible near the equilibrium point of converging
algorithms. The equilibrium window size speciÔ¨Åed in (56)
would closely match the average window size of TCP connections, if a step size
 , is used.
5.2. Binary Feedback for Congestion Avoidance
One of the early proposals for explicit notiÔ¨Åcation of
congestion is the Binary Feedback Scheme introduced by
Ramakrishnan and Jain [RJ88, CJ89]. In this scheme, users
are notiÔ¨Åed about network status through a binary feedback
mechanism, i.e., a congestion bit which is set in the packets traversing some congested link. Although our global
optimization framework and the MCFC algorithm are different from the methodology and the algorithms in [RJ88],
the probing mechanism that we use to collect congestion
measures can be viewed as a generalization of the binary
feedback. To see the relationship, let the congestion Ô¨Åeld in
probe packets be only one bit. In this case, the incremental
congestion cost of the links must also be quantized to two
levels; 0 and 1. It follows that once a packet‚Äôs congestion
Ô¨Åeld is incremented to 1 at some link, it remains 1 regardless
of the status of subsequent links, an arrangement identical
to the binary feedback in [RJ88].
5.3. Random Early Detection Gateways
The Random Early Detection (RED) scheme [FJ93],
proposed by Floyd and Jacobson, enhances the conventional
TCP congestion control in two major ways. First, it provides the means of detecting link congestion gradually, by
averaging the buffer occupancy over the long run, rather
than waiting for buffer overÔ¨Çow and inevitable packet losses
to signal congestion abruptly. In this regard, there is a fundamental similarity between the RED scheme and the evaluation of link incremental cost functions in the MCFC algorithm. Second, in the RED scheme, explicit notiÔ¨Åcation of
congestion through binary feedback is permitted, thereby
detaching congestion notiÔ¨Åcation from packet losses. As
discussed in Section 5.1, detaching congestion notiÔ¨Åcation
from packet losses resolves several drawbacks inherent to
implicit congestion notiÔ¨Åcation via packet losses. Unfortunately, in the proposals [Bra97] that follow the original
paper on the RED gateways, this latter aspect of the RED
scheme is not promoted.
Although the RED scheme constitutes an important step
in the right direction for the improvement of Internet congestion control, we believe that further steps in this direction are needed. Once congestion at the link level is detected gradually and with a quasi-static point of view, it is
equally important to convey this gradual change with suf-
Ô¨Åcient granularity to the users, in order to facilitate gradual and smooth reaction to congestion. In other words, a
binary feedback stating that the path is either congested or
un-congestedinvites abrupt reaction to such notiÔ¨Åcation and
is not commensurate with a smooth and stable control approach.
We believe that timely and efÔ¨Åcient congestion control in
the Internet would be substantially facilitated by accommodating the use of probe packets, preferably with more than
one bit to carry congestion information. Of course, only a
small fraction of user packets need to belong to the probing
category. In this paper, besides providing a concrete framework for congestion control, we have demonstrated that in
order to probe the network congestion status, probing packets need not carry a separate congestion Ô¨Åeld for each link
they traverse. No useful information is missed by providing
a single congestion Ô¨Åeld in the probing packets and adding
the incremental congestion cost of the traversed links onto
5.4. Dynamic Adaptive Windows
Mitra and Seery, using an elegant analysis of closed
queuing networks, have come up with the Dynamic Adaptive Windows (DAW), a distributed algorithm for end-to-end
calculation of session window sizes [Mit92, MS90, MS93].
Unlike TCP congestion control and the coarse realization
of the MCFC algorithm devised in this paper, the DAW algorithm updates the session window sizes based on packet
delay measurements, rather than loss observations. In view
of the foregoing discussions, the ability to control congestion without relying on packet losses is an attractive feature.
Earlier in Section 4.2, in search of a realization of the
MCFC algorithm in the current Internet, we noted that the
end-to-end packet delays could be used to detect congestion
if the propagation component of the delay was known and if
some idea regarding the acceptable range of queuing delays
existed. In the DAW scheme, these requirements are satis-
Ô¨Åed Ô¨Årst, by assuming that the round trip propagationtime is
exactly known for each session and second, by conÔ¨Åning the
study to small network topologies with speciÔ¨Åc cross trafÔ¨Åc
statistics and packet length distributions to enable characterization of queuing delays in a desirable regime of operation,
referred to as moderate usage. We think that the elaborate
study and design in [Mit92, MS90, MS93], when contrasted
with its limited application, reinforces the necessity of some
form of explicit congestion notiÔ¨Åcation in the increasingly
complex Internet.
6. A Brief Simulation Study of Coarse MCFC
In this section, we discuss a limited set of simulations for
the coarse realization of the MCFC algorithm. The goal of
these simulations is to gain some understanding of the behavior of the algorithm, rather than to provide a comprehensive study. In particular, no simulation results are provided
for the exact realization of the algorithm.
We consider the window-based implementation of
MCFC in (43) with
 8 given by equations
(50) and (51). The round-trip time
 is estimated using (4)
with a coefÔ¨Åcient
    . A receiver notiÔ¨Åes its source
of successful packet delivery by means of an acknowledgment (Ack) packet. We assume that packets are never reordered, and Ack‚Äôs are never reordered or lost.
1000 1200 1400 1600 1800 2000
Window size (packets)
Time (seconds)
zeta = 0.01
1000 1200 1400 1600 1800 2000
Time (seconds)
zeta = 0.05
1000 1200 1400 1600 1800 2000
Time (seconds)
zeta = 0.25
Figure 5. Experiment
 ‚Äì Evolution of the window of session
 for different step sizes
  . Sessions 4‚Äì10 are active, but not
packet losses are detected via gaps in the sequence number of successive Ack‚Äôs. We enforce some minimum packet
spacing for the window-based implementation of MCFC in
order to prevent the phenomenon of packet batching, which
has been reported in earlier simulation studies [SZC90].
We have studied a simple network topology consisting
of a single link with a capacity of
   #  packets/sec and a
buffer space of
   packets. Packets are served according to
a FIFO scheduling discipline and are dropped from the tail
of the link queue in case of overÔ¨Çow.
The single link is shared by
  sessions,
Every session always has data to send after it is activated
and its rate is limited by the source congestion algorithm
alone. The reverse path delays for Ack‚Äôs are different for
different sources. The Ô¨Åxed and random components of
these delays are chosen such that the round-trip times of
the sources are as follows :
    msec for
    msec for
 #  msec for
     msec for
In the Ô¨Årst experiment, sessions 4‚Äì10 are started and the
network is allowed to reach a stable operating point, then
 is activated at time
 #    sec. Using
 , we observe the effect of
the step size
 in (50) and (51) on the stability and speed
of convergence of the algorithm. We observe from Figure
  increases, session
 ‚Äôs window reaches its steady
state value faster but the size of oscillations in the steady
state increases.
In the second experiment, we have tried to combine the
beneÔ¨Åts of a large
  (fast rise to steady state) and a small
 (small oscillations). Hence we have used
a session is Ô¨Årst activated and have switched to
at a later stage. The criterion that we have applied for this
switching to takes place is the number of losses experienced
by a session. A threshold of 12 losses has been used in this
simulation. The threshold value has to be chosen in a way
such that the session‚Äôs window reaches a given neighborhood of the steady state value before the switching takes
place. It can be shown that, with
 8 chosen as in (50) and (51), the threshold, on the average, depends only on the initial value of
 and is independent of
the Ô¨Ånal value of the window, the link capacity, or the buffer
For Figures 6.A and 6.B, sessions
reached their steady states at
   # sec. Sessions
 are activated at
      sec,
      sec, and
 #    sec, respectively. In order to study the algorithm
behavior under severe congestion, an uncontrolled source
with a rate of
     packets/sec is activated at time
 #   Figure 6. Experiment
  . Rates of sessions
 (averaged
second intervals) for MCFC.
 . The loss probability of
the link (averaged over
 second intervals) for MCFC.
of sessions
 , averaged over 1000 second intervals, for TCPreno. The rate of session
 (averaged over
 second intervals) is
also shown.
$ The loss probability of the link (averaged over
second intervals) for TCP-reno.
sec, and stopped at
 #   sec. The rate of sessions
 , averaged over
 second intervals is illustrated in Figure 6.A. We observe that at every point of time, all the active
sessions attain the same rate, in spite of differences in their
round-trip times. We also observe that every incoming session is allowed its fair share of the link bandwidth and it
attains this share quite rapidly. Moreover, all sessions react
rapidly and uniformly to changes in the quasi-static state of
the network, as exempliÔ¨Åed by the activation of sessions 1,
2, and 3, and by the activation and the termination of the uncontrolled source. Figure 6.B illustrates the loss probability
at the link. The most signiÔ¨Åcant observation in this Ô¨Ågure
is the sudden jump in the loss probability when the uncontrolled source is activated. However, small loss probability
at the queue is restored within a few seconds.
In Figures 6.C and 6.D, we present the results for the
same network activity and conÔ¨Åguration, but with sessions
running the TCP-reno algorithm, modiÔ¨Åed to Ô¨Åt our simulation model.
Here we observe that link bandwidth is
not shared fairly among the sessions ‚Äì sessions with larger
round-trip times attain lower average rates. The behavior of
the link loss probability in Figure 6.D is similar to that in
6.B, though we do not observe the sharp jump at
      sec, indicating that TCP reacts faster to the build up of congestion.
In conclusion, we add that the speed of the coarse MCFC
algorithm may be further improved by incorporating the
TCP slow start phase into it, or by using a function
 with some
 , instead of the
 used in (47).
We assert once again that the convergence and the reaction
speed of the exact MCFC algorithm is inherently faster than
the coarse algorithm studied in this simulation.
7. Conclusion
We have developed a class of optimal algorithms for endto-end congestion control at the transport layer of IP networks. The global optimization framework used for this
purpose, allowed us to systematically address issues of fairness and user priority. Although the proposed algorithms
do not require non-FIFO switches, we have shown that they
can provide fair services to the users or help enforce certain
priority options among them. These algorithms are realizable in both a coarse and an exact fashion, using implicit
or explicit congestion information. Therefore, they facilitate an objective evaluation of the performance improvement that explicit congestion notiÔ¨Åcation can bring to the
As a signiÔ¨Åcant result, we noticed that TCP-reno algorithm, once modiÔ¨Åed to make its session throughput independent of round trip times, belongs to the class of MCFC
algorithms. We provided a mathematical characterization of
session throughputs or window sizes in terms of loss probabilities, for both TCP and its modiÔ¨Åed version. Although
these results are not precise because of the large step sizes
in TCP-reno algorithm, they are applicable to arbitrary network topologies.
In a forthcoming paper, we use the methodology developed in this paper to study congestion control for multicast
communications in the Internet and the associated problems
of fairness and scalability.