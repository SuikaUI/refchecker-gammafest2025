Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103–111,
October 25, 2014, Doha, Qatar. c⃝2014 Association for Computational Linguistics
On the Properties of Neural Machine Translation: Encoder–Decoder
Approaches
Kyunghyun Cho
Bart van Merri¨enboer
Universit´e de Montr´eal
Dzmitry Bahdanau∗
Jacobs University Bremen, Germany
Yoshua Bengio
Universit´e de Montr´eal, CIFAR Senior Fellow
Neural machine translation is a relatively
new approach to statistical machine translation based purely on neural networks.
The neural machine translation models often consist of an encoder and a decoder.
The encoder extracts a ﬁxed-length representation from a variable-length input sentence, and the decoder generates a correct
translation from this representation. In this
paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder
and a newly proposed gated recursive convolutional neural network. We show that
the neural machine translation performs
relatively well on short sentences without
unknown words, but its performance degrades rapidly as the length of the sentence
and the number of unknown words increase. Furthermore, we ﬁnd that the proposed gated recursive convolutional network learns a grammatical structure of a
sentence automatically.
Introduction
A new approach for statistical machine translation based purely on neural networks has recently
been proposed . This new approach, which
we refer to as neural machine translation, is inspired by the recent trend of deep representational
learning. All the neural network models used in
 consist of
an encoder and a decoder. The encoder extracts a
ﬁxed-length vector representation from a variablelength input sentence, and from this representation
the decoder generates a correct, variable-length
target translation.
∗Research done while visiting Universit´e de Montr´eal
The emergence of the neural machine translation is highly signiﬁcant, both practically and theoretically. Neural machine translation models require only a fraction of the memory needed by
traditional statistical machine translation (SMT)
The models we trained for this paper
require only 500MB of memory in total.
stands in stark contrast with existing SMT systems, which often require tens of gigabytes of
memory. This makes the neural machine translation appealing in practice.
Furthermore, unlike conventional translation systems, each and every component of the neural translation model is
trained jointly to maximize the translation performance.
As this approach is relatively new, there has not
been much work on analyzing the properties and
behavior of these models.
For instance: What
are the properties of sentences on which this approach performs better? How does the choice of
source/target vocabulary affect the performance?
In which cases does the neural machine translation
It is crucial to understand the properties and behavior of this new neural machine translation approach in order to determine future research directions. Also, understanding the weaknesses and
strengths of neural machine translation might lead
to better ways of integrating SMT and neural machine translation systems.
In this paper, we analyze two neural machine
translation models.
One of them is the RNN
Encoder–Decoder that was proposed recently in
 . The other model replaces the
encoder in the RNN Encoder–Decoder model with
a novel neural network, which we call a gated
recursive convolutional neural network (grConv).
We evaluate these two models on the task of translation from French to English.
Our analysis shows that the performance of
the neural machine translation model degrades
quickly as the length of a source sentence increases. Furthermore, we ﬁnd that the vocabulary
size has a high impact on the translation performance. Nonetheless, qualitatively we ﬁnd that the
both models are able to generate correct translations most of the time. Furthermore, the newly
proposed grConv model is able to learn, without
supervision, a kind of syntactic structure over the
source language.
Neural Networks for Variable-Length
In this section, we describe two types of neural
networks that are able to process variable-length
sequences.
These are the recurrent neural network and the proposed gated recursive convolutional neural network.
Recurrent Neural Network with Gated
Hidden Neurons
Figure 1: The graphical illustration of (a) the recurrent neural network and (b) the hidden unit that
adaptively forgets and remembers.
A recurrent neural network (RNN, Fig. 1 (a))
works on a variable-length sequence x
(x1, x2, · · · , xT ) by maintaining a hidden state h
over time. At each timestep t, the hidden state h(t)
is updated by
h(t−1), xt
where f is an activation function. Often f is as
simple as performing a linear transformation on
the input vectors, summing them, and applying an
element-wise logistic sigmoid function.
An RNN can be used effectively to learn a distribution over a variable-length sequence by learning the distribution over the next input p(xt+1 |
xt, · · · , x1).
For instance, in the case of a sequence of 1-of-K vectors, the distribution can be
learned by an RNN which has as an output
p(xt,j = 1 | xt−1, . . . , x1) =
for all possible symbols j = 1, . . . , K, where wj
are the rows of a weight matrix W. This results in
the joint distribution
p(xt | xt−1, . . . , x1).
Recently, in a new activation
function for RNNs was proposed. The new activation function augments the usual logistic sigmoid
activation function with two gating units called reset, r, and update, z, gates. Each gate depends on
the previous hidden state h(t−1), and the current
input xt controls the ﬂow of information. This is
reminiscent of long short-term memory (LSTM)
units .
details about this unit, we refer the reader to and Fig. 1 (b). For the remainder of
this paper, we always use this new activation function.
Gated Recursive Convolutional Neural
Besides RNNs, another natural approach to dealing with variable-length sequences is to use a recursive convolutional neural network where the
parameters at each level are shared through the
whole network (see Fig. 2 (a)). In this section, we
introduce a binary convolutional neural network
whose weights are recursively applied to the input
sequence until it outputs a single ﬁxed-length vector. In addition to a usual convolutional architecture, we propose to use the previously mentioned
gating mechanism, which allows the recursive network to learn the structure of the source sentences
on the ﬂy.
Let x = (x1, x2, · · · , xT ) be an input sequence,
where xt ∈Rd. The proposed gated recursive
convolutional neural network (grConv) consists of
four weight matrices Wl, Wr, Gl and Gr. At
each recursion level t ∈[1, T −1], the activation
of the j-th hidden unit h(t)
is computed by
+ ωlh(t−1)
j−1 + ωrh(t−1)
where ωc, ωl and ωr are the values of a gater that
sum to 1. The hidden unit is initialized as
where U projects the input into a hidden space.
Figure 2: The graphical illustration of (a) the recursive convolutional neural network and (b) the proposed
gated unit for the recursive convolutional neural network. (c–d) The example structures that may be
learned with the proposed gated unit.
The new activation ˜h(t)
is computed as usual:
j−1 + Wrh(t)
where φ is an element-wise nonlinearity.
The gating coefﬁcients ω’s are computed by
j−1 + Grh(t)
where Gl, Gr ∈R3×d and
j−1 + Grh(t)
According to this activation, one can think of
the activation of a single node at recursion level t
as a choice between either a new activation computed from both left and right children, the activation from the left child, or the activation from
the right child.
This choice allows the overall
structure of the recursive convolution to change
adaptively with respect to an input sample. See
Fig. 2 (b) for an illustration.
In this respect, we may even consider the proposed grConv as doing a kind of unsupervised
parsing. If we consider the case where the gating unit makes a hard decision, i.e., ω follows an
1-of-K coding, it is easy to see that the network
adapts to the input and forms a tree-like structure
(See Fig. 2 (c–d)). However, we leave the further
investigation of the structure learned by this model
for future research.
Purely Neural Machine Translation
Encoder–Decoder Approach
The task of translation can be understood from the
perspective of machine learning as learning the
Economic growth has slowed down in recent years .
La croissance économique a ralenti ces dernières années .
[z ,z , ... ,z ]
Figure 3: The encoder–decoder architecture
conditional distribution p(f | e) of a target sentence (translation) f given a source sentence e.
Once the conditional distribution is learned by a
model, one can use the model to directly sample
a target sentence given a source sentence, either
by actual sampling or by using a (approximate)
search algorithm to ﬁnd the maximum of the distribution.
A number of recent papers have proposed to
use neural networks to directly learn the conditional distribution from a bilingual, parallel corpus . For instance, the authors of proposed an approach involving a convolutional ngram model to extract a vector of a source sentence which is decoded with an inverse convolutional n-gram model augmented with an RNN. In
 , an RNN with LSTM units
was used to encode a source sentence and starting
from the last hidden state, to decode a target sentence. Similarly, the authors of 
proposed to use an RNN to encode and decode a
pair of source and target phrases.
At the core of all these recent works lies an
encoder–decoder architecture (see Fig. 3).
encoder processes a variable-length input (source
sentence) and builds a ﬁxed-length vector representation (denoted as z in Fig. 3). Conditioned on
the encoded representation, the decoder generates
a variable-length sequence (target sentence).
Before this encoder–
decoder approach was used mainly as a part of the
existing statistical machine translation (SMT) system. This approach was used to re-rank the n-best
list generated by the SMT system in , and the authors of used this approach to provide an additional score for the existing phrase table.
In this paper, we concentrate on analyzing the
direct translation performance, as in , with two model conﬁgurations. In both
models, we use an RNN with the gated hidden
unit , as this is one of the only
options that does not require a non-trivial way to
determine the target length. The ﬁrst model will
use the same RNN with the gated hidden unit as
an encoder, as in , and the second
one will use the proposed gated recursive convolutional neural network (grConv). We aim to understand the inductive bias of the encoder–decoder
approach on the translation performance measured
Experiment Settings
We evaluate the encoder–decoder models on the
task of English-to-French translation. We use the
bilingual, parallel corpus which is a set of 348M
selected by the method in 
from a combination of Europarl (61M words),
news commentary (5.5M), UN (421M) and two
crawled corpora of 90M and 780M words respectively.1 We did not use separate monolingual data.
The performance of the neural machien translation models was measured on the news-test2012,
news-test2013 and news-test2014 sets ( 3000 lines
each). When comparing to the SMT system, we
use news-test2012 and news-test2013 as our development set for tuning the SMT system, and
news-test2014 as our test set.
Among all the sentence pairs in the prepared
parallel corpus, for reasons of computational ef-
ﬁciency we only use the pairs where both English
and French sentences are at most 30 words long to
train neural networks. Furthermore, we use only
the 30,000 most frequent words for both English
and French. All the other rare words are consid-
downloaded
//www-lium.univ-lemans.fr/˜schwenk/cslm_
joint_paper/.
ered unknown and are mapped to a special token
We train two models:
The RNN Encoder–
Decoder (RNNenc) and the
newly proposed gated recursive convolutional
neural network (grConv). Note that both models
use an RNN with gated hidden units as a decoder
(see Sec. 2.1).
We use minibatch stochastic gradient descent
with AdaDelta to train our two models. We initialize the square weight matrix (transition matrix) as an orthogonal matrix with its spectral radius set to 1 in the case of the RNNenc and
0.4 in the case of the grConv. tanh and a rectiﬁer
(max(0, x)) are used as the element-wise nonlinear functions for the RNNenc and grConv respectively.
The grConv has 2000 hidden neurons, whereas
the RNNenc has 1000 hidden neurons. The word
embeddings are 620-dimensional in both cases.2
Both models were trained for approximately 110
hours, which is equivalent to 296,144 updates and
846,322 updates for the grConv and RNNenc, respectively.
Translation using Beam-Search
We use a basic form of beam-search to ﬁnd a translation that maximizes the conditional probability
given by a speciﬁc model (in this case, either the
RNNenc or the grConv).
At each time step of
the decoder, we keep the s translation candidates
with the highest log-probability, where s = 10
is the beam-width. During the beam-search, we
exclude any hypothesis that includes an unknown
word. For each end-of-sequence symbol that is selected among the highest scoring candidates the
beam-width is reduced by one, until the beamwidth reaches zero.
The beam-search to (approximately) ﬁnd a sequence of maximum log-probability under RNN
was proposed and used successfully in and .
Recently, the authors of 
found this approach to be effective in purely neural machine translation based on LSTM units.
2In all cases, we train the whole network including the
word embedding matrix. The embedding dimensionality was
chosen to be quite large, as the preliminary experiments
with 155-dimensional embeddings showed rather poor performance.
Development
Moses+RNNenc⋆
Moses+LSTM◦
Development
(a) All Lengths
(b) 10–20 Words
Table 1: BLEU scores computed on the development and test sets. The top three rows show the scores on
all the sentences, and the bottom three rows on the sentences having no unknown words. (⋆) The result
reported in where the RNNenc was used to score phrase pairs in the phrase table. (◦)
The result reported in where an encoder–decoder with LSTM units was used to
re-rank the n-best list generated by Moses.
When we use the beam-search to ﬁnd the k best
translations, we do not use a usual log-probability
but one normalized with respect to the length of
the translation. This prevents the RNN decoder
from favoring shorter translations, behavior which
was observed earlier in, e.g., .
Results and Analysis
Quantitative Analysis
In this paper, we are interested in the properties
of the neural machine translation models. Specifically, the translation quality with respect to the
length of source and/or target sentences and with
respect to the number of words unknown to the
model in each source/target sentence.
First, we look at how the BLEU score, reﬂecting the translation performance, changes with respect to the length of the sentences (see Fig. 4 (a)–
(b)). Clearly, both models perform relatively well
on short sentences, but suffer signiﬁcantly as the
length of the sentences increases.
We observe a similar trend with the number of
unknown words, in Fig. 4 (c). As expected, the
performance degrades rapidly as the number of
unknown words increases. This suggests that it
will be an important challenge to increase the size
of vocabularies used by the neural machine translation system in the future.
Although we only
present the result with the RNNenc, we observed
similar behavior for the grConv as well.
In Table 1 (a), we present the translation performances obtained using the two models along with
the baseline phrase-based SMT system.3 Clearly
the phrase-based SMT system still shows the superior performance over the proposed purely neural machine translation system, but we can see that
under certain conditions (no unknown words in
both source and reference sentences), the difference diminishes quite signiﬁcantly. Furthermore,
if we consider only short sentences (10–20 words
per sentence), the difference further decreases (see
Table 1 (b).
Furthermore, it is possible to use the neural machine translation models together with the existing
phrase-based system, which was found recently in
 to improve the overall translation performance (see Table 1 (a)).
This analysis suggests that that the current neural translation approach has its weakness in handling long sentences. The most obvious explanatory hypothesis is that the ﬁxed-length vector representation does not have enough capacity to encode a long sentence with complicated structure
and meaning. In order to encode a variable-length
sequence, a neural network may “sacriﬁce” some
of the important topics in the input sentence in order to remember others.
This is in stark contrast to the conventional
phrase-based machine translation system .
As we can see from Fig. 5, the
conventional system trained on the same dataset
(with additional monolingual data for the language
model) tends to get a higher BLEU score on longer
3We used Moses as a baseline, trained with additional
monolingual data for a 4-gram language model.
She explained her new position of foreign affairs and security policy representative as a reply to a
question: ”Who is the European Union? Which phone number should I call?”; i.e. as an important step
to uniﬁcation and better clarity of Union’s policy towards countries such as China or India.
Elle a expliqu´e le nouveau poste de la Haute repr´esentante pour les affaires ´etrang`eres et la politique de
d´efense dans le cadre d’une r´eponse `a la question: ”Qui est qui `a l’Union europ´eenne?” ”A quel num´ero
de t´el´ephone dois-je appeler?”, donc comme un pas important vers l’unicit´e et une plus grande lisibilit´e
de la politique de l’Union face aux ´etats, comme est la Chine ou bien l’Inde.
Elle a d´ecrit sa position en mati`ere de politique ´etrang`ere et de s´ecurit´e ainsi que la politique de l’Union
europ´eenne en mati`ere de gouvernance et de d´emocratie .
Elle a expliqu´e sa nouvelle politique ´etrang`ere et de s´ecurit´e en r´eponse `a un certain nombre de questions
: ”Qu’est-ce que l’Union europ´eenne ? ” .
Elle a expliqu´e son nouveau poste des affaires ´etrang`eres et la politique de s´ecurit´e repr´esentant en
r´eponse `a une question: ”Qui est l’Union europ´eenne? Quel num´ero de t´el´ephone dois-je appeler?”;
c’est comme une ´etape importante de l’uniﬁcation et une meilleure lisibilit´e de la politique de l’Union
`a des pays comme la Chine ou l’Inde .
The investigation should be complete by the end of the year when the ﬁndings will be presented to
Deutsche Bank’s board of managing directors - with recommendations for action.
L’examen doit ˆetre termin´e d’ici la ﬁn de l’ann´ee, ensuite les r´esultats du conseil d’administration de la
Deutsche Bank doivent ˆetre pr´esent´es - avec recommandation, d’ habitude.
L’´etude devrait ˆetre termin´ee `a la ﬁn de l’ ann´ee, lorsque les conclusions seront pr´esent´ees au conseil
d’administration de la Deutsche Bank, conseil d’association avec des mesures.
L’enquˆete devrait ˆetre termin´ee `a la ﬁn de l’ann´ee o`u les conclusions seront pr´esent´ees par le conseil
d’administration de la BCE `a la direction des recommandations.
L’enquˆete devrait ˆetre termin´e d’ici la ﬁn de l’ann´ee lorsque les r´esultats seront pr´esent´es `a la Deutsche
Bank conseil des directeurs g´en´eraux - avec des recommandations .
And there are thorny mechanical questions that must be resolved during that time, like how to balance the state’s mandate of ”adequate access” to licensed marijuana with its prohibitions on cannabis
businesses within 1,000 feet of a school, park, playground or child care center.
Pendant ce temps, des questions pratiques restent en suspens: comment ´equilibrer le mandat de l’´etat
qui garantit un acc`es appropri´e `a la marijuana agr´e´ee et interdit l’installation de commerces de vente de
cannabis dans un rayon de 30 km autour d’une ´ecole, d’un parc, d’un terrain de jeu ou d’une cr`eche.
Il y a des questions pr´eventives qui se posent quant `a l’´equilibre des droits de l’enfant dans les limites
d’une entreprise de collecte de sang.
De fac¸on g´en´erale, il y a des raisons de s´ecurit´e pour que les entreprises aient acc`es `a des milliers de
centres de pˆeche, d’eau ou de recherche.
Et il y a des probl`emes m´ecaniques complexes qui doivent ˆetre r´esolues au cours de cette p´eriode,
comme la mani`ere d’´equilibrer le mandat de ”l’acc`es ad´equat” permis de marijuana avec l’interdiction
du cannabis aux entreprises de 1000 pieds d’une ´ecole de jeu ou de parc, le service de garde.
(a) Long Sentences
There is still no agreement as to which election rules to follow.
Toutefois il n’existe toujours pas d’accord selon quel r`eglement de vote il faut proc´eder.
Il n’y a pas encore d’accord sur les r`egles ´electorales.
Il n’y a pas encore d’accord sur la question des ´elections `a suivre.
Il y a toujours pas d’accord sur l’´election des r`egles `a suivre.
Many of these ideas may have been creative, but they didn’t necessarily work.
Beaucoup de ces id´ees ´etaient cr´eatives mais elles n’ont pas forc´ement fonctionn´e.
Bon nombre de ces id´ees ont peut-ˆetre ´et´e cr´eatrices, mais elles ne s’appliquaient pas n´ecessairement.
Beaucoup de ces id´ees peuvent ˆetre cr´eatives, mais elles n’ont pas fonctionn´e.
Beaucoup de ces id´ees ont pu ˆetre cr´eatif, mais ils n’ont pas n´ecessairement.
There is a lot of consensus between the Left and the Right on this subject.
C’est qu’il y a sur ce sujet un assez large consensus entre gauche et droite.
Il existe beaucoup de consensus entre la gauche et le droit `a la question.
Il y a un consensus entre la gauche et le droit sur cette question.
Il y a beaucoup de consensus entre la gauche et la droite sur ce sujet.
According to them, one can ﬁnd any weapon at a low price right now.
Selon eux, on peut trouver aujourd’hui `a Moscou n’importe quelle arme pour un prix raisonnable.
Selon eux, on peut se trouver de l’arme `a un prix trop bas.
En tout cas, ils peuvent trouver une arme `a un prix tr`es bas `a la fois.
Selon eux, on trouve une arme `a bas prix pour l’instant.
(b) Short Sentences
Table 2: The sample translations along with the source sentences and the reference translations.
Sentence length
BLEU score
Source text
Reference text
(a) RNNenc
Sentence length
BLEU score
Source text
Reference text
(b) grConv
Max. number of unknown words
BLEU score
Source text
Reference text
(c) RNNenc
Figure 4: The BLEU scores achieved by (a) the RNNenc and (b) the grConv for sentences of a given
length. The plot is smoothed by taking a window of size 10. (c) The BLEU scores achieved by the RNN
model for sentences with less than a given number of unknown words.
sentences.
In fact, if we limit the lengths of both the source
sentence and the reference translation to be between 10 and 20 words and use only the sentences
with no unknown words, the BLEU scores on the
test set are 27.81 and 33.08 for the RNNenc and
Moses, respectively.
Note that we observed a similar trend even
when we used sentences of up to 50 words to train
these models.
Qualitative Analysis
Although BLEU score is used as a de-facto standard metric for evaluating the performance of a
machine translation system, it is not the perfect
metric ). Hence, here we present some of the actual translations generated from the two models,
RNNenc and grConv.
In Table. 2 (a)–(b), we show the translations of
some randomly selected sentences from the development and test sets. We chose the ones that
have no unknown words. (a) lists long sentences
(longer than 30 words), and (b) short sentences
(shorter than 10 words). We can see that, despite
the difference in the BLEU scores, all three models (RNNenc, grConv and Moses) do a decent job
at translating, especially, short sentences. When
the source sentences are long, however, we notice the performance degradation of the neural machine translation models.
Additionally, we present here what type of
structure the proposed gated recursive convolutional network learns to represent. With a sample
sentence “Obama is the President of the United
States”, we present the parsing structure learned
by the grConv encoder and the generated translations, in Fig. 6. The ﬁgure suggests that the gr-
Sentence length
BLEU score
Source text
Reference text
Figure 5: The BLEU scores achieved by an SMT
system for sentences of a given length. The plot
is smoothed by taking a window of size 10. We
use the solid, dotted and dashed lines to show the
effect of different lengths of source, reference or
both of them, respectively.
Conv extracts the vector representation of the sentence by ﬁrst merging “of the United States” together with “is the President of” and ﬁnally combining this with “Obama is” and “.”, which is
well correlated with our intuition. Note, however,
that the structure learned by the grConv is different from existing parsing approaches in the sense
that it returns soft parsing.
Despite the lower performance the grConv
showed compared to the RNN Encoder–Decoder,4
we ﬁnd this property of the grConv learning a
grammar structure automatically interesting and
believe further investigation is needed.
4However, it should be noted that the number of gradient
updates used to train the grConv was a third of that used to
train the RNNenc. Longer training may change the result,
but for a fair comparison we chose to compare models which
were trained for an equal amount of time. Neither model was
trained to convergence.
the President
United States
Translations
Obama est le Pr´esident des ´Etats-Unis . (2.06)
Obama est le pr´esident des ´Etats-Unis . (2.09)
Obama est le pr´esident des Etats-Unis . (2.61)
Obama est le Pr´esident des Etats-Unis . (3.33)
Barack Obama est le pr´esident des ´Etats-Unis . (4.41)
Barack Obama est le Pr´esident des ´Etats-Unis . (4.48)
Barack Obama est le pr´esident des Etats-Unis . (4.54)
L’Obama est le Pr´esident des ´Etats-Unis . (4.59)
L’Obama est le pr´esident des ´Etats-Unis . (4.67)
Obama est pr´esident du Congr`es des ´Etats-Unis .(5.09)
Figure 6: (a) The visualization of the grConv structure when the input is “Obama is the President of
the United States.”. Only edges with gating coefﬁcient ω higher than 0.1 are shown. (b) The top-10
translations generated by the grConv. The numbers in parentheses are the negative log-probability.
Conclusion and Discussion
In this paper, we have investigated the property
of a recently introduced family of machine translation system based purely on neural networks.
We focused on evaluating an encoder–decoder approach, proposed recently in , on the task of sentence-to-sentence translation.
Among many possible encoder–decoder
models we speciﬁcally chose two models that differ in the choice of the encoder; (1) RNN with
gated hidden units and (2) the newly proposed
gated recursive convolutional neural network.
After training those two models on pairs of
English and French sentences, we analyzed their
performance using BLEU scores with respect to
the lengths of sentences and the existence of unknown/rare words in sentences. Our analysis revealed that the performance of the neural machine
translation suffers signiﬁcantly from the length of
sentences. However, qualitatively, we found that
the both models are able to generate correct translations very well.
These analyses suggest a number of future research directions in machine translation purely
based on neural networks.
Firstly, it is important to ﬁnd a way to scale up
training a neural network both in terms of computation and memory so that much larger vocabularies for both source and target languages can be
used. Especially, when it comes to languages with
rich morphology, we may be required to come up
with a radically different approach in dealing with
Secondly, more research is needed to prevent
the neural machine translation system from underperforming with long sentences. Lastly, we need
to explore different neural architectures, especially
for the decoder. Despite the radical difference in
the architecture between RNN and grConv which
were used as an encoder, both models suffer from
the curse of sentence length. This suggests that it
may be due to the lack of representational power
in the decoder. Further investigation and research
are required.
In addition to the property of a general neural
machine translation system, we observed one interesting property of the proposed gated recursive
convolutional neural network (grConv). The gr-
Conv was found to mimic the grammatical structure of an input sentence without any supervision
on syntactic structure of language. We believe this
property makes it appropriate for natural language
processing applications other than machine translation.
Acknowledgments
The authors would like to acknowledge the support of the following agencies for research funding
and computing support: NSERC, Calcul Qu´ebec,
Compute Canada, the Canada Research Chairs
and CIFAR.