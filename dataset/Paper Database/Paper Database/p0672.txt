Selective Kernel Networks
Xiang Li∗1,2, Wenhai Wang†3,2, Xiaolin Hu‡4 and Jian Yang§1
1PCALab, Nanjing University of Science and Technology
3Nanjing University
4Tsinghua University
In standard Convolutional Neural Networks (CNNs), the
receptive ﬁelds of artiﬁcial neurons in each layer are designed to share the same size. It is well-known in the neuroscience community that the receptive ﬁeld size of visual
cortical neurons are modulated by the stimulus, which has
been rarely considered in constructing CNNs. We propose
a dynamic selection mechanism in CNNs that allows each
neuron to adaptively adjust its receptive ﬁeld size based
on multiple scales of input information. A building block
called Selective Kernel (SK) unit is designed, in which multiple branches with different kernel sizes are fused using
softmax attention that is guided by the information in these
branches. Different attentions on these branches yield different sizes of the effective receptive ﬁelds of neurons in
the fusion layer. Multiple SK units are stacked to a deep
network termed Selective Kernel Networks (SKNets). On
the ImageNet and CIFAR benchmarks, we empirically show
that SKNet outperforms the existing state-of-the-art architectures with lower model complexity. Detailed analyses
show that the neurons in SKNet can capture target objects
with different scales, which veriﬁes the capability of neurons for adaptively adjusting their receptive ﬁeld sizes according to the input. The code and models are available at
 
∗Xiang Li and Jian Yang are with PCA Lab, Key Lab of Intelligent
Perception and Systems for High-Dimensional Information of Ministry of
Education, and Jiangsu Key Lab of Image and Video Understanding for
Social Security, School of Computer Science and Engineering, Nanjing
University of Science and Technology, China. Xiang Li is also a visiting
scholar at Momenta. Email: 
†Wenhai Wang is with National Key Lab for Novel Software Technology, Nanjing University. He was an research intern at Momenta.
‡Xiaolin Hu is with the Tsinghua National Laboratory for Information
Science and Technology (TNList) Department of Computer Science and
Technology, Tsinghua University, China.
§Corresponding author.
1. Introduction
The local receptive ﬁelds (RFs) of neurons in the primary
visual cortex (V1) of cats have inspired the construction of Convolutional Neural Networks (CNNs) in the
last century, and it continues to inspire mordern CNN structure construction. For instance, it is well-known that in the
visual cortex, the RF sizes of neurons in the same area (e.g.,
V1 region) are different, which enables the neurons to collect multi-scale spatial information in the same processing
stage. This mechanism has been widely adopted in recent
Convolutional Neural Networks (CNNs). A typical example is InceptionNets , in which a simple concatenation is designed to aggregate multi-scale information
from, e.g., 3×3, 5×5, 7×7 convolutional kernels inside the
“inception” building block.
However, some other RF properties of cortical neurons
have not been emphasized in designing CNNs, and one such
property is the adaptive changing of RF size. Numerous experimental evidences have suggested that the RF sizes of
neurons in the visual cortex are not ﬁxed, but modulated by
the stimulus. The Classical RFs (CRFs) of neurons in the
V1 region was discovered by Hubel and Wiesel , as determined by single oriented bars. Later, many studies (e.g.,
 ) found that the stimuli outside the CRF will also affect the responses of neurons. The neurons are said to have
non-classical RFs (nCRFs). In addition, the size of nCRF
is related to the contrast of the stimulus: the smaller the
contrast, the larger the effective nCRF size . Surprisingly, by stimulating nCRF for a period of time, the CRF
of the neuron is also enlarged after removing these stimuli . All of these experiments suggest that the RF sizes
of neurons are not ﬁxed but modulated by stimulus .
Unfortunately, this property does not receive much attention in constructing deep learning models. Those models
with multi-scale information in the same layer such as InceptionNets have an inherent mechanism to adjust the RF
size of neurons in the next convolutional layer according
to the contents of the input, because the next convolutional
 
layer linearly aggregates multi-scale information from different branches. But that linear aggregation approach may
be insufﬁcient to provide neurons powerful adaptation ability.
In the paper, we present a nonlinear approach to aggregate information from multiple kernels to realize the adaptive RF sizes of neurons. We introduce a “Selective Kernel”
(SK) convolution, which consists of a triplet of operators:
Split, Fuse and Select. The Split operator generates multiple paths with various kernel sizes which correspond to
different RF sizes of neurons. The Fuse operator combines
and aggregates the information from multiple paths to obtain a global and comprehensive representation for selection
weights. The Select operator aggregates the feature maps of
differently sized kernels according to the selection weights.
The SK convolutions can be computationally lightweight
and impose only a slight increase in parameter and computational cost. We show that on the ImageNet 2012 dataset
 SKNets are superior to the previous state-of-the-art
models with similar model complexity. Based on SKNet-
50, we ﬁnd the best settings for SK convolution and show
the contribution of each component. To demonstrate their
general applicability, we also provide compelling results on
smaller datasets, CIFAR-10 and 100 , and successfully
embed SK into small models (e.g., ShufﬂeNetV2 ).
To verify the proposed model does have the ability to
adjust neurons’ RF sizes, we simulate the stimulus by enlarging the target object in natural images and shrinking the
background to keep the image size unchanged. It is found
that most neurons collect information more and more from
the larger kernel path when the target object becomes larger
and larger. These results suggest that the neurons in the proposed SKNet have adaptive RF sizes, which may underlie
the model’s superior performance in object recognition.
2. Related Work
Multi-branch convolutional networks.
Highway networks introduces the bypassing paths along with gating units. The two-branch architecture eases the difﬁculty
to training networks with hundreds of layers.
is also used in ResNet , but the bypassing path
is the pure identity mapping.
Besides the identity mapping, the shake-shake networks and multi-residual networks extend the major transformation with more identical paths. The deep neural decision forests form the
tree-structural multi-branch principle with learned splitting
functions.
FractalNets and Multilevel ResNets 
are designed in such a way that the multiple paths can
be expanded fractally and recursively. The InceptionNets
 carefully conﬁgure each branch with customized kernel ﬁlters, in order to aggregate more informative and multifarious features. Please note that the proposed
SKNets follow the idea of InceptionNets with various ﬁlters
for multiple branches, but differ in at least two important
aspects: 1) the schemes of SKNets are much simpler without heavy customized design and 2) an adaptive selection
mechanism for these multiple branches is utilized to realize
adaptive RF sizes of neurons.
Grouped/depthwise/dilated convolutions. Grouped convolutions are becoming popular due to their low computational cost. Denote the group size by G, then both the
number of parameters and the computational cost will be
divided by G, compared to the ordinary convolution. They
are ﬁrst adopted in AlexNet with a purpose of distributing the model over more GPU resources. Surprisingly, using grouped convolutions, ResNeXts can also improve
accuracy. This G is called “cardinality”, which characterize
the model together with depth and width.
Many compact models such as IGCV1 , IGCV2 
and IGCV3 are developed, based on the interleaved
grouped convolutions. A special case of grouped convolutions is depthwise convolution, where the number of groups
is equal to the number of channels. Xception and MobileNetV1 introduce the depthwise separable convolution which decomposes ordinary convolutions into depthwise convolution and pointwise convolution.
The effectiveness of depthwise convolutions is validated in the subsequent works such as MobileNetV2 and ShufﬂeNet
 . Beyond grouped/depthwise convolutions, dilated
convolutions support exponential expansion of the
RF without loss of coverage. For example, a 3×3 convolution with dilation 2 can approximately cover the RF of
a 5×5 ﬁlter, whilst consuming less than half of the computation and memory. In SK convolutions, the kernels of
larger sizes (e.g., >1) are designed to be integrated with the
grouped/depthwise/dilated convolutions, in order to avoid
the heavy overheads.
Attention mechanisms. Recently, the beneﬁts of attention
mechanism have been shown across a range of tasks, from
neural machine translation in natural language processing to image captioning in image understanding. It biases the allocation of the most informative feature expressions and simultaneously suppresses the
less useful ones. Attention has been widely used in recent
applications such as person re-ID , image recovery ,
text abstraction and lip reading . To boost the performance of image classiﬁcation, Wang et al. propose
a trunk-and-mask attention between intermediate stages of
a CNN. An hourglass module is introduced to achieve the
global emphasis across both spatial and channel dimension.
Furthermore, SENet brings an effective, lightweight
gating mechanism to self-recalibrate the feature map via
channel-wise importances. Beyond channel, BAM and
CBAM introduce spatial attention in a similar way. In
contrast, our proposed SKNets are the ﬁrst to explicitly focus on the adaptive RF size of neurons by introducing the
Kernel 3x3
Kernel 5x5
element-wise summation
element-wise product
Figure 1. Selective Kernel Convolution.
attention mechanisms.
Dynamic convolutions. Spatial Transform Networks 
learns a parametric transformation to warp the feature map,
which is considered difﬁcult to be trained. Dynamic Filter can only adaptively modify the parameters of ﬁlters, without the adjustment of kernel size. Active Convolution augments the sampling locations in the convolution with offsets. These offsets are learned end-to-end but
become static after training, while in SKNet the RF sizes
of neurons can adaptively change during inference.
Deformable Convolutional Networks further make the location offsets dynamic, but it does not aggregate multi-scale
information in the same way as SKNet does.
3. Methods
3.1. Selective Kernel Convolution
To enable the neurons to adaptively adjust their RF sizes,
we propose an automatic selection operation, “Selective
Kernel” (SK) convolution, among multiple kernels with different kernel sizes. Speciﬁcally, we implement the SK convolution via three operators – Split, Fuse and Select, as illustrated in Fig. 1, where a two-branch case is shown. Therefore in this example, there are only two kernels with different kernel sizes, but it is easy to extend to multiple branches
Split: For any given feature map X ∈RH′×W ′×C′, by
default we ﬁrst conduct two transformations eF : X →eU ∈
RH×W ×C and bF : X →bU ∈RH×W ×C with kernel sizes
3 and 5, respectively. Note that both eF and bF are composed of efﬁcient grouped/depthwise convolutions, Batch
Normalization and ReLU function in sequence.
For further efﬁciency, the conventional convolution with a
5×5 kernel is replaced with the dilated convolution with a
3×3 kernel and dilation size 2.
Fuse: As stated in Introduction, our goal is to enable
neurons to adaptively adjust their RF sizes according to the
stimulus content. The basic idea is to use gates to control
the information ﬂows from multiple branches carrying different scales of information into neurons in the next layer.
To achieve this goal, the gates need to integrate information
from all branches. We ﬁrst fuse results from multiple (two
in Fig. 1) branches via an element-wise summation:
U = eU + bU,
then we embed the global information by simply using
global average pooling to generate channel-wise statistics
as s ∈RC. Speciﬁcally, the c-th element of s is calculated
by shrinking U through spatial dimensions H × W:
sc = Fgp(Uc) =
Further, a compact feature z ∈Rd×1 is created to enable
the guidance for the precise and adaptive selections. This
is achieved by a simple fully connected (fc) layer, with the
reduction of dimensionality for better efﬁciency:
z = Ffc(s) = δ(B(Ws)),
where δ is the ReLU function , B denotes the Batch
Normalization , W ∈Rd×C. To study the impact of d
on the efﬁciency of the model, we use a reduction ratio r to
control its value:
d = max(C/r, L),
where L denotes the minimal value of d (L = 32 is a typical
setting in our experiments).
Select: A soft attention across channels is used to adaptively select different spatial scales of information, which is
guided by the compact feature descriptor z. Speciﬁcally, a
softmax operator is applied on the channel-wise digits:
eAcz + eBcz , bc =
eAcz + eBcz ,
where A, B ∈RC×d and a, b denote the soft attention vector for eU and bU, respectively. Note that Ac ∈R1×d is
the c-th row of A and ac is the c-th element of a, likewise
ResNeXt-50 (32×4d)
7 × 7, 64, stride 2
3 × 3 max pool, stride 2
1 × 1, 128
3 × 3, 128, G = 32
1 × 1, 256
1 × 1, 128
3 × 3, 128, G = 32
1 × 1, 256
fc, 
1 × 1, 128
SK[M = 2, G = 32, r = 16], 128
1 × 1, 256
1 × 1, 256
3 × 3, 256, G = 32
1 × 1, 512
1 × 1, 256
3 × 3, 256, G = 32
1 × 1, 512
fc, 
1 × 1, 256
SK[M = 2, G = 32, r = 16], 256
1 × 1, 512
1 × 1, 512
3 × 3, 512, G = 32
1 × 1, 1024
1 × 1, 512
3 × 3, 512, G = 32
1 × 1, 1024
fc, 
1 × 1, 512
SK[M = 2, G = 32, r = 16], 512
1 × 1, 1024
1 × 1, 1024
3 × 3, 1024, G = 32
1 × 1, 2048
1 × 1, 1024
3 × 3, 1024, G = 32
1 × 1, 2048
fc, 
1 × 1, 1024
SK[M = 2, G = 32, r = 16], 1024
1 × 1, 2048
7 × 7 global average pool, 1000-d fc, softmax
Table 1. The three columns refer to ResNeXt-50 with a 32×4d template, SENet-50 based on the ResNeXt-50 backbone and the corresponding SKNet-50, respectively. Inside the brackets are the general shape of a residual block, including ﬁlter sizes and feature dimensionalities.
The number of stacked blocks on each stage is presented outside the brackets. “G = 32” suggests the grouped convolution. The inner
brackets following by fc indicates the output dimension of the two fully connected layers in an SE module. #P denotes the number of
parameter and the deﬁnition of FLOPs follow , i.e., the number of multiply-adds.
Bc and bc. In the case of two branches, the matrix B is redundant because ac + bc = 1. The ﬁnal feature map V is
obtained through the attention weights on various kernels:
Vc = ac · eUc + bc · bUc,
ac + bc = 1,
where V = [V1, V2, ..., VC], Vc ∈RH×W . Note that
here we provide a formula for the two-branch case and one
can easily deduce situations with more branches by extending Eqs. (1) (5) (6).
3.2. Network Architecture
Using the SK convolutions, the overall SKNet architecture is listed in Table 1. We start from ResNeXt for
two reasons: 1) it has low computational cost with extensive
use of grouped convolution, and 2) it is one of the state-ofthe-art network architectures with high performance on object recognition. Similar to the ResNeXt , the proposed
SKNet is mainly composed of a stack of repeated bottleneck blocks, which are termed “SK units”. Each SK unit
consists of a sequence of 1×1 convolution, SK convolution and 1×1 convolution. In general, all the large kernel
convolutions in the original bottleneck blocks in ResNeXt
are replaced by the proposed SK convolutions, enabling the
network to choose appropriate RF sizes in an adaptive manner. As the SK convolutions are very efﬁcient in our design,
SKNet-50 only leads to 10% increase in the number of parameters and 5% increase in computational cost, compared
with ResNeXt-50.
In SK units, there are three important hyper-parameters
which determine the ﬁnal settings of SK convolutions: the
number of paths M that determines the number of choices
of different kernels to be aggregated, the group number G
that controls the cardinality of each path, and the reduction
ratio r that controls the number of parameters in the fuse
operator (see Eq. (4)). In Table 1, we denote one typical
setting of SK convolutions SK[M, G, r] to be SK .
The choices and effects of these parameters are discussed in
Table 1 shows the structure of a 50-layer SKNet which
has four stages with {3,4,6,3} SK units, respectively. By
varying the number of SK units in each stage, one can obtain different architectures. In this study, we have experimented with other two architectures, SKNet-26, which has
{2,2,2,2} SK units, and SKNet-101, which has {3,4,23,3}
SK units, in their respective four stages.
Note that the proposed SK convolutions can be applied to
other lightweight networks, e.g., MobileNet , Shuf-
ﬂeNet , in which 3×3 depthwise convolutions are
extensively used. By replacing these convolutions with the
SK convolutions, we can also achieve very appealing results
in the compact architectures (see Sec. 4.1).
4. Experiments
4.1. ImageNet Classiﬁcation
The ImageNet 2012 dataset comprises 1.28 million training images and 50K validation images from 1,000
classes. We train networks on the training set and report the
top-1 errors on the validation set. For data augmentation,
we follow the standard practice and perform the random-
top-1 err (%)
ResNeXt-50
AttentionNeXt-56 
InceptionV3 
ResNeXt-50 + BAM 
ResNeXt-50 + CBAM 
SENet-50 
SKNet-50 (ours)
ResNeXt-101
Attention-92 
DPN-92 
DPN-98 
InceptionV4 
Inception-ResNetV2 
ResNeXt-101 + BAM 
ResNeXt-101 + CBAM 
SENet-101 
SKNet-101 (ours)
Table 2. Comparisons to the state-of-the-arts under roughly identical complexity. 224× denotes the single 224×224 crop for evaluation, and likewise 320×. Note that SENets/SKNets are all based
on the corresponding ResNeXt backbones.
size cropping to 224 ×224 and random horizontal ﬂipping
 . The practical mean channel subtraction is adpoted to
normalize the input images for both training and testing.
Label-smoothing regularization is used during training. For training large models, we use synchronous SGD
with momentum 0.9, a mini-batch size of 256 and a weight
decay of 1e-4. The initial learning rate is set to 0.1 and decreased by a factor of 10 every 30 epochs. All models are
trained for 100 epochs from scratch on 8 GPUs, using the
weight initialization strategy in . For training lightweight
models, we set the weight decay to 4e-5 instead of 1e-4,
and we also use slightly less aggressive scale augmentation
for data preprocessing. Similar modiﬁcations can as well
be referenced in since such small networks usually
suffer from underﬁtting rather than overﬁtting. To benchmark, we apply a centre crop on the validation set, where
224×224 or 320×320 pixels are cropped for evaluating the
classiﬁcation accuracy. The results reported on ImageNet
are the averages of 3 runs by default.
Comparisons with state-of-the-art models. We ﬁrst compare SKNet-50 and SKNet-101 to the public competitive
models with similar model complexity. The results show
that SKNets consistently improve performance over the
state-of-the-art attention-based CNNs under similar budgets.
Remarkably, SKNet-50 outperforms ResNeXt-101
by above absolute 0.32%, although ResNeXt-101 is 60%
larger in parameter and 80% larger in computation. With
comparable or less complexity than InceptionNets, SKNets
achieve above absolute 1.5% gain of performance, which
top-1 err. (%)
ResNeXt-50 (32×4d)
SKNet-50 (ours)
20.79 (1.44)
ResNeXt-50, wider
22.13 (0.10)
ResNeXt-56, deeper
22.04 (0.19)
ResNeXt-50 (36×4d)
22.00 (0.23)
Table 3. Comparisons on ImageNet validation set when the computational cost of model with more depth/width/cardinality is increased to match that of SKNet. The numbers in brackets denote
the gains of performance.
demonstrates the superiority of adaptive aggregation for
multiple kernels. We also note that using slightly less parameters, SKNets can obtain 0.3∼0.4% gains to SENet
counterparts in both 224×224 and 320×320 evaluations.
Selective Kernel vs.
Depth/Width/Cardinality.
Compared with ResNeXt (using the setting of 32×4d), SKNets
inevitably introduce a slightly increase in parameter and
computation due to the additional paths of differnet kernels
and the selection process. For fair comparison, we increase
the complexity of ResNeXt by changing its depth, width
and cardinality, to match the complexity of SKNets. Table
3 shows that increased complexity does lead to better prediction accuracy.
However, the improvement is marginal when going
deeper (0.19% from ResNeXt-50 to ResNeXt-53) or
wider (0.1% from ResNeXt-50 to ResNeXt-50 wider), or
with slightly more cardinality (0.23% from ResNeXt-50
(32×4d) to ResNeXt-50 (36×4d)).
In contrast, SKNet-50 obtains 1.44% absolute improvement over the baseline ResNeXt-50, which indicates that
SK convolution is very efﬁcient.
Performance with respect to the number of parameters.
We plot the top-1 error rate of the proposed SKNet with
respect to the number of parameters in it (Fig. 2). Three architectures, SK-26, SKNet-50 and SKNet-101 (see Section
3.2 for details), are shown in the ﬁgure. For comparison,
we plot the results of some state-of-the-art models including ResNets , ResNeXts , DenseNets , DPNs 
and SENets in the ﬁgure. Each model has multiple
variants. The details of the compared architectures are provided in the Supplementary Materials. All Top-1 errors are
reported in the references. It is seen that SKNets utilizes parameters more efﬁciently than these models. For instance,
achieving ∼20.2 top-1 error, SKNet-101 needs 22% fewer
parameters than DPN-98.
Lightweight models. Finally, we choose the representative compact architecture – ShufﬂeNetV2 , which is one
of the strongest light models, to evaluate the generalization
ability of SK convolutions. By exploring different scales of
models in Table 4, we can observe that SK convolutions not
only boost the accuracy of baselines signiﬁcantly but also
The number of parameters (M)
Top-1 error (%), single crop 224×224
Figure 2. Relationship between the performance of SKNet and the
number of parameters in it, compared with the state-of-the-arts.
ShufﬂeNetV2
top-1 err.(%)
0.5× (our impl.)
0.5× + SE 
1.0× (our impl.)
1.0× + SE 
Table 4. Single 224×224 crop top-1 error rates (%) by variants of
lightweight models on ImageNet validation set.
perform better than SE (achieving around absolute 1%
gain). This indicates the great potential of the SK convolutions in applications on low-end devices.
4.2. CIFAR Classiﬁcation
To evaluate the performance of SKNets on smaller
datasets, we conduct more experiments on CIFAR-10 and
100 . The two CIFAR datasets consist of colored
natural scence images, with 32×32 pixel each. The train
and test sets contain 50k images and 10k images respectively. CIFAR-10 has 10 classes and CIFAR-100 has 100.
We take the architectures as in for reference: our networks have a single 3×3 convolutional layer, followed by 3
stages each having 3 residual blocks with SK convolution.
We also apply SE blocks on the same backbone (ResNeXt-
29, 16×32d) for better comparisons. More architectural and
training details are provided in the supplemantary materials. Notably, SKNet-29 achieves better or comparable performance than ResNeXt-29, 16×64d with 60% fewer parameters and it consistently outperforms SENet-29 on both
CIFAR-10 and 100 with 22% fewer parameters.
4.3. Ablation Studies
In this section, we report ablation studies on the ImageNet dataset to investigate the effectiveness of SKNet.
ResNeXt-29, 16×32d
ResNeXt-29, 8×64d
ResNeXt-29, 16×64d
SENet-29 
SKNet-29 (ours)
Table 5. Top-1 errors (%, average of 10 runs) on CIFAR. SENet-29
and SKNet-29 are all based on ResNeXt-29, 16×32d.
Table 6. Results of SKNet-50 with different settings in the second
branch, while the setting of the ﬁrst kernel is ﬁxed. “Resulted
kernel” in the last column means the approximate kernel size with
dilated convolution.
Table 7. Results of SKNet-50 with different combinations of multiple kernels. Single 224×224 crop is utilized for evaluation.
The dilation D and group number G. The dilation D and
group number G are two crucial elements to control the RF
size. To study their effects, we start from the two-branch
case and ﬁx the setting 3×3 ﬁlter with dilation D = 1 and
group G = 32 in the ﬁrst kernel branch of SKNet-50.
Under the constraint of similar overall complexity, there
are two ways to enlarge the RF of the second kernel branch:
1) increase the dilation D whilst ﬁxing the group number G,
and 2) simultaneously increase the ﬁlter size and the group
Table 6 shows that the optimal settings for the other
branch are those with kernel size 5×5 (the last column),
which is larger than the ﬁrst ﬁxed kernel with size 3×3. It
is proved beneﬁcial to use different kernel sizes, and we
attribute the reason to the aggregation of multi-scale information.
There are two optimal conﬁgurations: kernel size 5×5
with D = 1 and kernel size 3×3 with D = 2, where the
latter has slightly lower model complexity. In general, we
empirically ﬁnd that the series of 3×3 kernels with various
dilations is moderately superior to the corresponding counterparts with the same RF (large kernels without dilations)
in both performance and complexity.
Combination of different kernels. Next we investigate the
effect of combination of different kernels. Some kernels
may have size larger than 3×3, and there may be more than
two kernels. To limit the search space, we only use three
different kernels, called “K3” (standard 3×3 convolutional
kernel), “K5” (3×3 convolution with dilation 2 to approximate 5×5 kernel size), and “K7” (3×3 with dilation 3 to
approximate 7×7 kernel size). Note that we only consider
the dilated versions of large kernels (5×5 and 7×7) as Table 6 has suggested. G is ﬁxed to 32. If “SK” in Table 7
is ticked, it means that we use the SK attention across the
corresponding kernels ticked in the same row (the output of
each SK unit is V in Fig. 1), otherwise we simply sum up
the results with these kernels (then the output of each SK
unit is U in Fig. 1) as a naive baseline model.
The results in Table 7 indicate that excellent performance
of SKNets can be attributed to the use of multiple kernels
and the adaptive selection mechanism among them. From
Table 7, we have the following observations: (1) When the
number of paths M increases, in general the recognition
error decreases. The top-1 errors in the ﬁrst block of the
table (M = 1) are generally higher than those in the second block (M = 2), and the errors in the second block are
generally higher than the third block (M = 3). (2) No matter M = 2 or 3, SK attention-based aggregation of multiple paths always achieves lower top-1 error than the simple
aggregation method (naive baseline model). (3) Using SK
attention, the performance gain of the model from M = 2 to
M = 3 is marginal (the top-1 error decreases from 20.79%
to 20.76%). For better trade-off between performance and
efﬁciency, M = 2 is preferred.
4.4. Analysis and Interpretation
To understand how adaptive kernel selection works, we
analyze the attention weights by inputting same target object but in different scales. We take all the image instances
from the ImageNet validation set, and progressively enlarge
the central object from 1.0× to 2.0× via a central cropping
and subsequent resizing (see top left in Fig. 3a,b).
First, we calculate the attention values for the large kernel (5×5) in each channel in each SK unit. Fig. 3a,b (bottom left) show the attention values in all channels for two
randomly samples in SK 3 4, and Fig.
3c (bottom left)
shows the averaged attention values in all channels across
all validation images. It is seen that in most channels, when
the target object enlarges, the attention weight for the large
kernel (5×5) increases, which suggests that the RF sizes of
1.0x 1.5x 2.0x
mean attention difference (kernel 5x5 - 3x3)
96 128 160 192 224
channel index
activation
attention for 5x5 kernel in SK_3_4
1.0x 1.5x 2.0x
mean attention difference (kernel 5x5 - 3x3)
96 128 160 192 224
channel index
activation
attention for 5x5 kernel in SK_3_4
mean attention difference (kernel 5x5 - 3x3)
96 128 160 192 224
channel index
activation
attention for 5x5 kernel in SK_3_4
Figure 3. (a) and (b): Attention results for two randomly sampled
images with three differently sized targets (1.0x, 1.5x and 2.0x).
Top left: sample images. Bottom left: the attention values for the
5×5 kernel across channels in SK 3 4. The plotted results are the
averages of 16 successive channels for the ease of view. Right: the
attention value of the kernel 5×5 minus that of the kernel 3×3 in
different SK units. (c): Average results over all image instances in
the ImageNet validation set. Standard deviation is also plotted.
the neurons are adaptively getting larger, which agrees with
our expectation.
We then calculate the difference between the the mean
attention weights associated with the two kernels (larger minus smaller) over all channels in each SK unit. Fig. 3a,b
(right) show the results for two random samples at different
SK units, and Fig. 3c (right) show the results averaged over
all validation images. We ﬁnd one surprising pattern about
the role of adaptive selection across depth: The larger the
target object is, the more attention will be assigned to larger
class index
mean attention difference (kernel 5x5 - 3x3) on SK_2_3 over various classes
class index
mean attention difference (kernel 5x5 - 3x3) on SK_3_4 over various classes
class index
mean attention difference (kernel 5x5 - 3x3) on SK_5_3 over various classes
Figure 4. Average mean attention difference (mean attention value of kernel 5×5 minus that of kernel 3×3) on SK units of SKNet-50, for
each of 1,000 categories using all validation samples on ImageNet. On low or middle level SK units (e.g., SK 2 3, SK 3 4), 5×5 kernels
are clearly imposed with more emphasis if the target object becomes larger (1.0x →1.5x).
kernels by the Selective Kernel mechanism in low and middle level stages (e.g., SK 2 3, SK 3 4). However, at much
higher layers (e.g., SK 5 3), all scale information is getting
lost and such a pattern disappears.
Further, we look deep into the selection distributions
from the perspective of classes. For each category, we draw
the average mean attention differences on the representative
SK units for 1.0× and 1.5× objects over all the 50 images
which belong to that category. We present the statistics of
1,000 classes in Fig. 4. We observe the previous pattern
holds true for all 1,000 categories, as illustrated in Fig. 4,
where the importance of kernel 5×5 consistently and simultaneously increases when the scale of targets grows. This
suggests that in the early parts of networks, the appropriate kernel sizes can be selected according to the semantic
awareness of objects’ sizes, thus it efﬁciently adjusts the
RF sizes of these neurons. However, such pattern is not
existed in the very high layers like SK 5 3, since for the
high-level representation, “scale” is partially encoded in the
feature vector, and the kernel size matters less compared to
the situation in lower layers.
5. Conclusion
Inspired by the adaptive receptive ﬁeld (RF) sizes of neurons in visual cortex, we propose Selective Kernel Networks
(SKNets) with a novel Selective Kernel (SK) convolution,
to improve the efﬁciency and effectiveness of object recognition by adaptive kernel selection in a soft-attention manner. SKNets demonstrate state-of-the-art performances on
various benchmarks, and from large models to tiny models.
In addition, we also discover several meaningful behaviors
of kernel selection across channel, depth and category, and
empirically validate the effective adaption of RF sizes for
SKNets, which leads to a better understanding of its mechanism. We hope it may inspire the study of architectural
design and search in the future.
Acknowledgments The authors would like to thank the
editor and the anonymous reviewers for their critical and
constructive comments and suggestions.
This work was
supported by the National Science Fund of China under
Grant No. U1713208, Program for Changjiang Scholars
and National Natural Science Foundation of China, Grant
no. 61836014.