Accelerating Very Deep Convolutional
Networks for ClassiÔ¨Åcation and Detection
Xiangyu Zhang, Jianhua Zou, Kaiming He‚Ä†, and Jian Sun
Abstract‚ÄîThis paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very
deep CNNs that have substantially impacted the computer vision community. Unlike previous methods that are designed
for approximating linear Ô¨Ålters or linear responses, our method takes the nonlinear units into account. We develop an effective
solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly,
while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction
that reduces the rapidly accumulated error when multiple (e.g., ‚â•10) layers are approximated. For the widely used very deep
VGG-16 model , our method achieves a whole-model speedup of 4√ó with merely a 0.3% increase of top-5 error in ImageNet
classiÔ¨Åcation. Our 4√ó accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged
into the Fast R-CNN detector .
Index Terms‚ÄîConvolutional Neural Networks, Acceleration, Image ClassiÔ¨Åcation, Object Detection
INTRODUCTION
convolutional
(CNNs) , has been continuously improving ,
 , , , , but the computational cost of these
networks also increases signiÔ¨Åcantly. For example, the
very deep VGG models , which have witnessed
great success in a wide range of recognition tasks ,
 , , , , , , are substantially slower
than earlier models , . Real-world systems may
suffer from the low speed of these networks. For
example, a cloud service needs to process thousands
of new requests per seconds; portable devices such as
phones and tablets may not afford slow models; some
recognition tasks like object detection , , ,
 and semantic segmentation , , need
to apply these models on higher-resolution images. It
is thus of practical importance to accelerate test-time
performance of CNNs.
There have been a series of studies on accelerating
deep CNNs , , , . A common focus
of these methods is on the decomposition of one or
a few layers. These methods have shown promising
speedup ratios and accuracy on one or two layers and
whole (but shallower) models. However, few results
are available for accelerating very deep models (e.g.,
‚â•10 layers). Experiments on complex datasets such
as ImageNet are also limited - e.g., the results in
 , , are about accelerating a single layer of
the shallower AlexNet . Moreover, performance of
Correspondence author.
‚Ä¢ X. Zhang and J. Zou are with Xi‚Äôan Jiaotong University, Xi‚Äôan, China.
This work was done when X. Zhang was an intern at Microsoft
‚Ä¢ K. He and J. Sun are with Microsoft Research, Beijing, China. E-mail:
{kahe,jiansun}@microsoft.com
the accelerated networks as generic feature extractors
for other recognition tasks , remain unclear.
It is nontrivial to speed up whole, very deep models
for complex tasks like ImageNet classiÔ¨Åcation. Acceleration algorithms involve not only the decomposition
of layers, but also the optimization solutions to the
decomposition. Data (response) reconstruction solvers
 based on stochastic gradient descent (SGD) and
backpropagation work well for simpler tasks such
as character classiÔ¨Åcation , but are less effective
for complex ImageNet models (as we will discussed
in Sec. 4). These SGD-based solvers are sensitive to
initialization and learning rates, and might be trapped
into poorer local optima for regressing responses.
Moreover, even when a solver manages to accelerate
a single layer, the accumulated error of approximating
multiple layers grow rapidly, especially for very deep
models. Besides, the layers of a very deep model
may exhibit a great diversity in Ô¨Ålter numbers, feature
map sizes, sparsity, and redundancy. It may not be
beneÔ¨Åcial to uniformly accelerate all layers.
In this paper, we present an accelerating method
that is effective for very deep models. We Ô¨Årst propose a response reconstruction method that takes
into account the nonlinear neurons and a low-rank
constraint. A solution based on Generalized Singular
Value Decomposition (GSVD) is developed for this
nonlinear problem, without the need of SGD. Our
explicit treatment of the nonlinearity better models
a nonlinear layer, and more importantly, enables an
asymmetric reconstruction that accounts for the error
from previous approximated layers. This method effectively reduces the accumulated error when multiple layers are approximated sequentially. We also
present a rank selection method for adaptively determining the acceleration of each layer for a whole
 
model, based on their redundancy.
In experiments, we demonstrate the effects of the
nonlinear solution, asymmetric reconstruction, and
whole-model acceleration by controlled experiments
of a 10-layer model on ImageNet classiÔ¨Åcation .
Furthermore, we apply our method on the publicly available VGG-16 model , and achieve a 4√ó
speedup with merely a 0.3% increase of top-5 centerview error.
The impact of the ImageNet dataset is not
merely on the speciÔ¨Åc 1000-class classiÔ¨Åcation task;
deep models pre-trained on ImageNet have been actively used to replace hand-engineered features, and
have showcased excellent accuracy for challenging
tasks such as object detection , , , and
semantic segmentation , , . We exploit our
method to accelerate the very deep VGG-16 model for
Fast R-CNN object detection. With a 4√ó speedup
of all convolutions, our method has a graceful degradation of 0.8% mAP (from 66.9% to 66.1%) on the
PASCAL VOC 2007 detection benchmark .
A preliminary version of this manuscript has been
presented in a conference . This manuscript extends the initial version from several aspects to
strengthen our method. (1) We demonstrate compelling acceleration results on very deep VGG models,
and are among the Ô¨Årst few works accelerating very
deep models. (2) We investigate the accelerated models for transfer-learning-based object detection ,
 , which is one of the most important applications
of ImageNet pre-trained networks. (3) We provide
evidence showing that a model trained from scratch
and sharing the same structure as the accelerated
model is inferior. This discovery suggests that a very
deep model can be accelerated not simply because the
decomposed network architecture is more powerful,
but because the acceleration optimization algorithm is
able to digest information.
RELATED WORK
Methods , , , for accelerating testtime computation of CNNs in general have two
components: (i) a layer decomposition design that
reduces time complexity, and (ii) an optimization
scheme for the decomposition design. Although the
former (‚Äúdecomposition‚Äù) attracts more attention because it directly addresses the time complexity, the
latter (‚Äúoptimization‚Äù) is also essential because not all
decompositions are similarly easy to Ô¨Åne good local
The method of Denton et al. is one of the
Ô¨Årst to exploit low-rank decompositions of Ô¨Ålters.
Several decomposition designs along different dimensions have been investigated. This method does not
explicitly minimize the error of the activations after
the nonlinearity, which is inÔ¨Çuential to the accuracy
as we will show. This method presents experiments
of accelerating a single layer of an OverFeat network
 , but no whole-model results are available.
Jaderberg et al. present efÔ¨Åcient decompositions
by separating k √ó k Ô¨Ålters into k √ó 1 and 1 √ó k Ô¨Ålters,
which was earlier developed for accelerating generic
image Ô¨Ålters . Channel-wise dimension reduction
is also considered. Two optimization schemes are proposed: (i) ‚ÄúÔ¨Ålter reconstruction‚Äù that minimizes the error of Ô¨Ålter weights, and (ii) ‚Äúdata reconstruction‚Äù that
minimizes the error of responses. In , conjugate
gradient descent is used to solve Ô¨Ålter reconstruction,
and SGD with backpropagation is used to solve data
reconstruction. Data reconstruction in demonstrates excellent performance on a character classiÔ¨Åcation task using a 4-layer network. For ImageNet
classiÔ¨Åcation, their paper evaluates a single layer of an
OverFeat network by ‚ÄúÔ¨Ålter reconstruction‚Äù. But the
performance of whole, very deep models in ImageNet
remains unclear.
Concurrent with our work, Lebedev et al. adopt
‚ÄúCP-decomposition‚Äù to decompose a layer into Ô¨Åve
layers of lower complexity. For ImageNet classiÔ¨Åcation, only a single-layer acceleration of AlexNet is
reported in . Moreover, Lebedev et al. report that
they ‚Äúfailed to Ô¨Ånd a good SGD learning rate‚Äù in their
Ô¨Åne-tuning, suggesting that it is nontrivial to optimize
the factorization for even a single layer in ImageNet
Despite some promising preliminary results that
have been obtained in the above works , , ,
the whole-model acceleration of very deep networks for
ImageNet is still an open problem.
Besides the research on decomposing layers, there
have been other streams on improving train/test-time
performance of CNNs. FFT-based algorithms , 
are applicable for both training and testing, and are
particularly effective for large spatial kernels. On the
other hand, it is also proposed to train ‚Äúthin‚Äù and
deep networks , for good trade-off between
speed and accuracy. Besides reducing running time, a
related issue involving memory conservation has
also attracted attention.
APPROACHES
Our method exploits a low-rank assumption for decomposition, following the stream of , . We
show that this decomposition has a closed-form solution (SVD) for linear neurons, and a slightly more
complicated solution (GSVD , , ) for nonlinear neurons. The simplicity of our solver enables
an asymmetric reconstruction method for reducing
accumulated error of very deep models.
Low-rank Approximation of Responses
Our assumption is that the Ô¨Ålter response at a pixel of
a layer approximately lies on a low-rank subspace. A
ùëë‚Ä≤ channels
Figure 1: Illustration of the decomposition. (a) An
complexity
approximated
complexity
O(d‚Ä≤k2c) + O(dd‚Ä≤).
resulting low-rank decomposition reduces time complexity. To Ô¨Ånd the approximate low-rank subspace,
we minimize the reconstruction error of the responses.
More formally, we consider a convolutional layer
with a Ô¨Ålter size of k√ók√óc, where k is the spatial size
of the Ô¨Ålter and c is the number of input channels of
this layer. To compute a response, this Ô¨Ålter is applied
on a k √ó k √ó c volume of the layer input. We use x ‚àà
Rk2c+1 to denote a vector that reshapes this volume,
where we append one as the last entry for the sake of
the bias. A response y ‚ààRd at a position of a layer is
computed as:
where W is a d-by-(k2c+1) matrix, and d is the number
of Ô¨Ålters. Each row of W denotes the reshaped form
of a k √ó k √ó c Ô¨Ålter with the bias appended.
Under the assumption that the vector y is on a lowrank subspace, we can write y = M(y ‚àí¬Øy)+ ¬Øy, where
M is a d-by-d matrix of a rank d‚Ä≤ < d and ¬Øy is the
mean vector of responses. Expanding this equation,
we can compute a response by:
y = MWx + b,
where b = ¬Øy‚àíM¬Øy is a new bias. The rank-d‚Ä≤ matrix M
can be decomposed into two d-by-d‚Ä≤ matrices P and
Q such that M = PQ‚ä§. We denote W‚Ä≤ = Q‚ä§W as a
d‚Ä≤-by-(k2c+1) matrix, which is essentially a new set of
d‚Ä≤ Ô¨Ålters. Then we can compute (2) by:
y = PW‚Ä≤x + b.
The complexity of using Eqn.(3) is O(d‚Ä≤k2c) + O(dd‚Ä≤),
while the complexity of using Eqn.(1) is O(dk2c).
For many typical models/layers, we usually have
O(dd‚Ä≤) ‚â™O(d‚Ä≤k2c), so the computation in Eqn.(3) will
reduce the complexity to about d‚Ä≤/d.
Fig. 1 illustrates how to use Eqn.(3) in a network.
We replace the original layer (given by W) by two
layers (given by W‚Ä≤ and P). The matrix W‚Ä≤ is actually
d‚Ä≤ Ô¨Ålters whose sizes are k √ó k √ó c. These Ô¨Ålters
produce a d‚Ä≤-dimensional feature map. On this feature
map, the d-by-d‚Ä≤ matrix P can be implemented as d
Ô¨Ålters whose sizes are 1 √ó 1 √ó d‚Ä≤. So P corresponds
to a convolutional layer with a 1√ó1 spatial support,
which maps the d‚Ä≤-dimensional feature map to a ddimensional one.
Note that the decomposition of M = PQ‚ä§can be
arbitrary. It does not impact the value of y computed
in Eqn.(3). A simple decomposition is the Singular
Value Decomposition (SVD) : M = Ud‚Ä≤Sd‚Ä≤Vd‚Ä≤‚ä§,
where Ud‚Ä≤ and Vd‚Ä≤ are d-by-d‚Ä≤ column-orthogonal
matrices and Sd‚Ä≤ is a d‚Ä≤-by-d‚Ä≤ diagonal matrix. Then
we can obtain P = Ud‚Ä≤S1/2
and Q = Vd‚Ä≤S1/2
In practice the low-rank assumption does not
strictly hold, and the computation in Eqn.(3) is approximate. To Ô¨Ånd an approximate low-rank subspace, we optimize the following problem:
‚à•(yi ‚àí¬Øy) ‚àíM(yi ‚àí¬Øy)‚à•2
rank(M) ‚â§d‚Ä≤.
Here yi is a response sampled from the feature maps
in the training set. This problem can be solved by
SVD or actually Principal Component Analysis
(PCA): let Y be the d-by-n matrix concatenating n
responses with the mean subtracted, compute the
eigen-decomposition of the covariance matrix YY‚ä§=
USU‚ä§where U is an orthogonal matrix and S is
diagonal, and M = Ud‚Ä≤Ud‚Ä≤‚ä§where Ud‚Ä≤ are the Ô¨Årst
d‚Ä≤ eigenvectors. With the matrix M computed, we can
Ô¨Ånd P = Q = Ud‚Ä≤.
How good is the low-rank assumption? We sample
the responses from a CNN model (with 7 convolutional layers, detailed in Sec. 4) trained on ImageNet.
For the responses of each layer, we compute the
eigenvalues of their covariance matrix and then plot
the sum of the largest eigenvalues (Fig. 2). We see
that substantial energy is in a small portion of the
largest eigenvectors. For example, in the Conv2 layer
(d = 256) the Ô¨Årst 128 eigenvectors contribute over
99.9% energy; in the Conv7 layer (d = 512), the Ô¨Årst
256 eigenvectors contribute over 95% energy. This
indicates that we can use a fraction of the Ô¨Ålters to
precisely approximate the original Ô¨Ålters.
The low-rank behavior of the responses y is because
of the low-rank behaviors of the Ô¨Ålter weights W
and the inputs x. Although the low-rank assumptions
about Ô¨Ålter weights W have been adopted in recent
work , , we further adopt the low-rank assumptions about the Ô¨Ålter inputs x, which are local
volumes and have correlations. The responses y will
have lower rank than W and x, so the approximation
can be more precise. In our optimization (4), we
directly address the low-rank subspace of y.
PCA Accumulative Energy (%)
Figure 2: PCA accumulative energy of the responses in each layer, presented as the sum of largest d‚Ä≤ eigenvalues
(relative to the total energy when d‚Ä≤ = d). Here the Ô¨Ålter number d is 96 for Conv1, 256 for Conv2, and 512
for Conv3-7 (detailed in Table 1). These Ô¨Ågures are obtained from 3,000 randomly sampled training images.
Nonlinear Case
Next we investigate the case of using nonlinear units.
We use r(¬∑) to denote the nonlinear operator. In this
paper we focus on the RectiÔ¨Åed Linear Unit (ReLU)
 : r(¬∑) = max(¬∑, 0).
Driven by Eqn.(4), we minimize the reconstruction
error of the nonlinear responses:
‚à•r(yi) ‚àír(Myi + b)‚à•2
rank(M) ‚â§d‚Ä≤.
Here b is a new bias to be optimized, and r(My+b) =
r(MWx + b) is the nonlinear response computed by
the approximated Ô¨Ålters.
The above optimization problem is challenging due
to the nonlinearity and the low-rank constraint. To
Ô¨Ånd a feasible solution, we relax it as:
‚à•r(yi) ‚àír(zi)‚à•2
2 + Œª‚à•zi ‚àí(Myi + b)‚à•2
rank(M) ‚â§d‚Ä≤.
Here {zi} is a set of auxiliary variables of the same
size as {yi}. Œª is a penalty parameter. If Œª ‚Üí‚àû, the
solution to (6) will converge to the solution to (5) .
We adopt an alternating solver, Ô¨Åxing {zi} and solving
for M, b and vice versa.
(i) The subproblem of M, b. In this case, {zi} are
Ô¨Åxed. It is easy to show that b is solved by b = ¬Øz ‚àí
M¬Øy where ¬Øz is the mean vector of {zi}. Substituting
b into the objective function, we obtain the problem
involving M:
‚à•(zi ‚àí¬Øz) ‚àíM(yi ‚àí¬Øy)‚à•2
rank(M) ‚â§d‚Ä≤.
This problem appears similar to Eqn.(4) except that
there are two sets of responses.
This optimization problem also has a closed-form
solution by Generalized SVD (GSVD) , , .
Let Z be the d-by-n matrix concatenating the vectors
of {zi ‚àí¬Øz}. We rewrite the above problem as:
M ‚à•Z ‚àíMY‚à•2
rank(M) ‚â§d‚Ä≤.
Here ‚à•¬∑ ‚à•F is the Frobenius norm. A problem in
this form is known as Reduced Rank Regression
 , , . This problem belongs to a broader
category of procrustes problems that have been
adopted for various data reconstruction problems
 , , . The solution is as follows (see ).
Let ÀÜM = ZY‚ä§(YY‚ä§)‚àí1. GSVD is applied on
ÀÜM: ÀÜM = USV‚ä§, such that U is a d-by-d orthogonal
matrix satisfying U‚ä§U = Id where Id is a d-by-d
identity matrix, and V is a d-by-d matrix satisfying
V‚ä§YY‚ä§V = Id (called generalized orthogonality). Then
the solution M to (8) is given by M = Ud‚Ä≤Sd‚Ä≤Vd‚Ä≤‚ä§
where Ud‚Ä≤ and Vd‚Ä≤ are the Ô¨Årst d‚Ä≤ columns of U and
V and Sd‚Ä≤ are the largest d‚Ä≤ singular values. One can
show that if Z = Y (so the problem in (7) becomes
(4)), this GSVD solution becomes SVD, i.e., eigendecomposition of YY‚ä§.
(ii) The subproblem of {zi}. In this case, M and b
are Ô¨Åxed. Then in this subproblem each element zij of
each vector zi is independent of any other. So we solve
a 1-dimensional optimization problem as follows:
zij (r(yij) ‚àír(zij))2 + Œª(zij ‚àíy‚Ä≤
ij is the j-th entry of Myi + b. By separately
considering zij ‚â•0 and zij < 0, we obtain the solution
as follows: let
z0 = min(0, y‚Ä≤
z1 = max(0, Œª ¬∑ y‚Ä≤
ij + r(yij)
then zij = arg minz0,z1(r(yij) ‚àír(zij))2 + Œª(zij ‚àíy‚Ä≤
Our method is also applicable for other types of nonlinearities. The subproblem in (9) is a 1-dimensional
nonlinear least squares problem, so can be solved by
gradient descent for other r(¬∑).
We alternatively solve (i) and (ii). The initialization
is given by the solution to the linear case (4). We warm
up the solver by setting the penalty parameter Œª =
0.01 and run 25 iterations. Then we increase the value
of Œª. In theory, Œª should be gradually increased to
inÔ¨Ånity . But we Ô¨Ånd that it is difÔ¨Åcult for the
iterative solver to make progress if Œª is too large. So
we increase Œª to 1, run 25 more iterations, and use
the resulting M as our solution. As before, we obtain
P and Q by SVD on M.
In experiments, we Ô¨Ånd that it is sufÔ¨Åcient to randomly sample 3,000 images to solve Eqn.(5). It only
takes our method 2-5 minutes in MATLAB solving a
layer. This is much faster than SGD-based solvers.
Asymmetric Reconstruction for Multi-Layer
When each layer is approximated independently, the
error of shallower layers will be rapidly accumulated
and affect deeper layers. We propose an asymmetric
reconstruction method to alleviate this problem.
We apply our method sequentially on each layer,
from the shallower layers to the deeper ones. Let
us consider a layer whose input feature map is not
precise due to the approximation of the previous
layer/layers. We denote the approximate input to the
current layer as ÀÜx. For the training data, we can still
compute its non-approximate responses as y = Wx.
So we can optimize an ‚Äúasymmetric‚Äù version of (5):
‚à•r(Wxi) ‚àír(MWÀÜxi + b)‚à•2
rank(M) ‚â§d‚Ä≤.
In the Ô¨Årst term r(Wx) = r(y) is the non-approximate
output of this layer. In the second term, ÀÜxi is the
approximated input to this layer, and r(MWÀÜxi + b)
is the approximated output of this layer. In contrast
to using x (or ÀÜx) for both terms, this asymmetric
formulation faithfully incorporates the two actual
terms before/after the approximation of this layer.
The optimization problem in (12) can be solved using
the same algorithm as for (5).
Rank Selection for Whole-Model Acceleration
In the above, the optimization is based on a target d‚Ä≤
of each layer. d‚Ä≤ is the only parameter that determines
the complexity of an accelerated layer. But given a
desired speedup ratio of the whole model, we need to
determine the proper rank d‚Ä≤ used for each layer. One
may adopt a uniform speedup ratio for each layer. But
this is not an optimal solution, because the layers are
not equally redundant.
PCA Accumulative Energy (%)
‚àÜ Accuracy (%)
Figure 3: PCA accumulative energy and the accuracy
rates (top-5). Here the accuracy is evaluated using the
linear solution (the nonlinear solution has a similar
trend). Each layer is evaluated independently, with
other layers not approximated. The accuracy is shown
as the difference to no approximation.
We empirically observe that the PCA energy after
approximations is roughly related to the classiÔ¨Åcation
accuracy. To verify this observation, in Fig. 3 we
show the classiÔ¨Åcation accuracy (represented as the
difference to no approximation) vs. the PCA energy.
Each point in this Ô¨Ågure is empirically evaluated
using a reduced rank d‚Ä≤. 100% energy means no approximation and thus no degradation of classiÔ¨Åcation
accuracy. Fig. 3 shows that the classiÔ¨Åcation accuracy
is roughly linear on the PCA energy.
To simultaneously determine the reduced ranks of
all layers, we further assume that the whole-model
classiÔ¨Åcation accuracy is roughly related to the product of the PCA energy of all layers. More formally, we
consider this objective function:
Here œÉl,a is the a-th largest eigenvalue of the layer
l, and Pd‚Ä≤
a=1 œÉl,a is the PCA energy of the largest d‚Ä≤
eigenvalues in the layer l. The product Q
l is over all
layers to be approximated. The objective E is assumed
to be related to the accuracy of the approximated
whole network. Then we optimize this problem:
Here dl is the original number of Ô¨Ålters in the layer l,
and Cl is the original time complexity of the layer l.
dl Cl is the complexity after the approximation. C
is the total complexity after the approximation, which
Ô¨Ålter size
# channels
output size
complexity (%)
# of zeros
Table 1: The architecture of the SPP-10 model . It has 7 conv layers and 3 fc layers. Each layer (except the
last fc) is followed by ReLU. The Ô¨Ånal conv layer is followed by a spatial pyramid pooling layer that have
4 levels ({6 √ó 6, 3 √ó 3, 2 √ó 2, 1 √ó 1}, totally 50 bins). The resulting 50 √ó 512-d is fed into the 4096-d fc layer
(fc6), followed by another 4096-d fc layer (fc7) and a 1000-way softmax layer. The column ‚Äúcomplexity‚Äù is the
theoretical time complexity, shown as relative numbers to the total convolutional complexity. The column ‚Äú#
of zeros‚Äù is the relative portion of zero responses, which shows the ‚Äúsparsity‚Äù of the layer.
is given by the desired speedup ratio. This optimization problem means that we want to maximize the
accumulated energy subject to the time complexity
constraint.
The problem in (14) is a combinatorial problem
 . So we adopt a greedy strategy to solve it. We
initialize d‚Ä≤
l as dl, and consider the set {œÉl,a}. In each
step we remove an eigenvalue œÉl,d‚Ä≤
l from this set,
chosen from a certain layer l. The relative reduction
of the objective is ‚ñ≥E/E = œÉl,d‚Ä≤/Pd‚Ä≤
a=1 œÉl,a, and the
reduction of complexity is ‚ñ≥C =
dl Cl. Then we
deÔ¨Åne a measure as ‚ñ≥E/E
‚ñ≥C . The eigenvalue œÉl,d‚Ä≤
has the smallest value of this measure is removed.
Intuitively, this measure favors a small reduction of
‚ñ≥E/E and a large reduction of complexity ‚ñ≥C. This
step is greedily iterated, until the constraint of the
total complexity is achieved.
Higher-Dimensional Decomposition
In our formulation, we focus on reducing the channels
(from d to d‚Ä≤). There are algorithmic advantages of operating on the channel dimension. Firstly, this dimension can be easily controlled by the rank constraint
rank(M) ‚â§d‚Ä≤. This constraint enables closed-form
solutions, e.g., SVD or GSVD. Secondly, the optimized
low-rank projection M can be exactly decomposed
into low-dimensional Ô¨Ålters (P and Q). These simple
and closed-form solutions can produce good results
using a very small subset of training images (3,000
out of one million).
On the other hand, compared with decomposition
methods that operate on multiple dimensions (spatial
and channel) , our method has to use a smaller
d‚Ä≤ to approach a given speedup ratio, which might
limit the accuracy of our method. To avoid d‚Ä≤ being
too small, we further propose to combine our solver
with Jaderberg et al.‚Äôs spatial decomposition. Thanks
to our asymmetric reconstruction, our method can
effectively alleviate the accumulated error for the
multi-decomposition.
To determined the decomposed architecture (but
not yet the weights), we Ô¨Årst use our method to
decompose all conv layers of a model. This involves
the rank selection of d‚Ä≤ for all layers. Then we apply
Jaderberg et al.‚Äôs method to further decompose the
resulting k√ók layers (k > 1) into k√ó1 and 1√ók Ô¨Ålters.
The Ô¨Årst k√ó1 layer has d‚Ä≤‚Ä≤ output channels depending
on the speedup ratio. In this way, an original layer of
(k √ó k, d) is decomposed into three layers of (k √ó 1,
d‚Ä≤‚Ä≤), (1 √ó k, d‚Ä≤), and (1 √ó 1, d). For a speedup ratio r,
we let each method contribute a speedup of ‚àör.
With the decomposed architecture determined, we
solve for the weights of the decomposed layers. Given
their order as above, we Ô¨Årst optimize the (k √ó 1,
d‚Ä≤‚Ä≤) and (1 √ó k, d) layers using ‚ÄúÔ¨Ålter reconstruction‚Äù
 (we will discuss ‚Äúdata reconstruction‚Äù later). Then
we adopt our solution on the (1 √ó k, d) layer and
optimize for the (1 √ó k, d‚Ä≤) and (1 √ó 1, d) layers. We
use our asymmetric reconstruction in Eqn.(12). In the
r(MWÀÜx+b) term, ÀÜx is the approximated input to this
1 √ó k layer, and the r(Wx) = r(y) term is still the
true response of the original k √ó k layer without any
decomposition. The approximation error of the spatial
decomposition will also be addressed by our asymmetric reconstruction, which is important to alleviate
accumulated error. We term this as ‚Äúasymmetric (3d)‚Äù
in the following.
Fine-tuning
With any approximated whole model, we may ‚ÄúÔ¨Ånetune‚Äù this model end-to-end in the ImageNet training
data. This process is similar to training a classiÔ¨Åcation
network with the approximated model as the initialization.
Conv1 Speedup
Increase in Error (%)
Conv2 Speedup
Conv3 Speedup
Conv4 Speedup
Conv5 Speedup
Increase in Error (%)
Conv6 Speedup
Conv7 Speedup
Figure 4: Linear vs. Nonlinear for SPP-10: single-layer performance of accelerating Conv1 to Conv7. The
speedup ratios are computed by the theoretical complexity of that layer. The error rates are top-5 single-view,
and shown as the increase of error rates compared with no approximation (smaller is better).
However, we empirically Ô¨Ånd that Ô¨Åne-tuning is
very sensitive to the initialization (given by the approximated model) and the learning rate. If the initialization is poor and the learning rate is small, the Ô¨Ånetuning is easily trapped in a poor local optimum and
makes little progress. If the learning rate is large, the
Ô¨Åne-tuning process behaves very similar to training
the decomposed architecture ‚Äúfrom scratch‚Äù (as we
will discuss later). A large learning rate may jump out
of the initialized local optimum, and the initialization
appears to be ‚Äúforgotten‚Äù.
Fortunately, our method has achieved very good
accuracy even without Ô¨Åne-tuning as we will show
by experiments. With our approximated model as the
initialization, the Ô¨Åne-tuning with a sufÔ¨Åciently small
learning rate is able to further improve the results. In
our experiments, we use a learning rate of 1e-5 and a
mini-batch size of 128, and Ô¨Åne-tune the models for 5
epochs in the ImageNet training data.
We note that in the following the results are without
Ô¨Åne-tuning unless speciÔ¨Åed.
EXPERIMENTS
We comprehensively evaluate our method on two
models. The Ô¨Årst model is a 10-layer model of ‚ÄúSPPnet
(OverFeat-7)‚Äù in , which we denote as ‚ÄúSPP-10‚Äù.
This model (detailed in Table 1) has a similar architecture to the OverFeat model but is deeper. It has
7 conv layers and 3 fc layers. The second model is
the publicly available VGG-16 model 1 that has 13
1. www.robots.ox.ac.uk/‚àºvgg/research/very deep/
conv layers and 3 fc layers. SPP-10 won the 3-rd place
and VGG-16 won the 2-nd place in ILSVRC 2014 .
We evaluate the ‚Äútop-5 error‚Äù using single-view
testing. The view is the center 224√ó224 region cropped
from the resized image whose shorter side is 256.
The single-view error rate of SPP-10 is 12.51% on the
ImageNet validation set, and VGG-16 is 10.09% in our
testing (which is consistent with the number reported
by 2). These numbers serve as the references for the
increased error rates of our approximated models.
Experiments with SPP-10
We Ô¨Årst evaluate the effect of our each step on the
SPP-10 model by a series of controlled experiments.
Unless speciÔ¨Åed, we do not use the 3-d decomposition.
Single-Layer: Linear vs. Nonlinear
In this subsection we evaluate the single-layer performance. When evaluating a single approximated
layer, the remaining layers are unchanged and not approximated. The speedup ratio (involving that single
layer only) is shown as the theoretical ratio computed
by the complexity.
In Fig. 4 we compare the performance of our linear
solution (4) and nonlinear solution (6). The performance is displayed as increase of error rates (decrease
of accuracy) vs. the speedup ratio of that layer. Fig. 4
shows that the nonlinear solution consistently performs better than the linear solution. In Table 1, we
2. 
(b) 3‚àílayer (Conv2, 3 and 4)
(a) 2‚àílayer (Conv6 and 7)
Increase of Error (%)
(c) 3‚àílayer (Conv5, 6 and 7)
Asymmetric
Asymmetric
Asymmetric
Figure 5: Symmetric vs. Asymmetric for SPP-10: the cases of 2-layer and 3-layer approximation. The speedup is
computed by the complexity of the layers approximated. (a) Approximation of Conv6 & 7. (b) Approximation
of Conv2, 3 & 4. (c) Approximation of Conv5, 6 & 7.
show the sparsity (the portion of zero activations
after ReLU) of each layer. A zero activation is due
to the truncation of ReLU. The sparsity is over 60%
for Conv2-7, indicating that the ReLU takes effect
on a substantial portion of activations. This explains
the discrepancy between the linear and nonlinear
solutions. Especially, the Conv7 layer has a sparsity
of 95%, so the advantage of the nonlinear solution is
more obvious.
Fig. 4 also shows that when accelerating only a
single layer by 2√ó, the increased error rates of our
solutions are rather marginal or negligible. For the
Conv2 layer, the error rate is increased by < 0.1%;
for the Conv3-7 layers, the error rate is increased by
We also notice that for Conv1, the degradation is
negligible near 2√ó speedup (1.8√ó corresponds to d‚Ä≤ =
32). This can be explained by Fig. 2(a): the PCA energy
has little loss when d‚Ä≤ ‚â•32. But the degradation can
grow quickly for larger speedup ratios, because in this
layer the channel number c = 3 is small and d‚Ä≤ needs
to be reduced drastically to achieve the speedup ratio.
So in the following whole-model experiments of SPP-
10, we will use d‚Ä≤ = 32 for Conv1.
Multi-Layer: Symmetric vs. Asymmetric
Next we evaluate the performance of asymmetric
reconstruction as in the problem (12). We demonstrate
approximating 2 layers or 3 layers. In the case of 2
layers, we show the results of approximating Conv6
and 7; and in the case of 3 layers, we show the
results of approximating Conv5-7 or Conv2-4. The
comparisons are consistently observed for other cases
of multi-layer.
We sequentially approximate the layers involved,
from a shallower one to a deeper one. In the asymmetric version (12), ÀÜx is from the output of the previous approximated layer (if any), and x is from the
output of the previous non-approximate layer. In the
symmetric version (5), we use x for both terms. We
have also tried another symmetric version of using ÀÜx
for both terms, and found this symmetric version is
even worse.
Fig. 5 shows the comparisons between the symmetric and asymmetric versions. The asymmetric solution has signiÔ¨Åcant improvement over the symmetric solution. For example, when only 3 layers are
approximated simultaneously (like Fig. 5 (c)), the
improvement is over 1.0% when the speedup is 4√ó.
This indicates that the accumulative error rate due to
multi-layer approximation can be effectively reduced
by the asymmetric version.
When more and all layers are approximated simultaneously (as below), if without the asymmetric
solution, the error rates will increase more drastically.
Whole-Model: with/without Rank Selection
In Table 2 we show the results of whole-model
acceleration. The solver is the asymmetric version.
For Conv1, we Ô¨Åx d‚Ä≤ = 32. For other layers, when
the rank selection is not used, we adopt the same
speedup ratio on each layer and determine its desired
rank d‚Ä≤ accordingly. When the rank selection is used,
we apply it to select d‚Ä≤ for Conv2-7. Table 2 shows
that the rank selection consistently outperforms the
counterpart without rank selection. The advantage of
rank selection is observed in both linear and nonlinear
solutions.
In Table 2 we notice that the rank selection often
chooses a higher rank d‚Ä≤ (than the no rank selection)
in Conv5-7. For example, when the speedup is 3√ó,
the rank selection assigns d‚Ä≤ = 167 to Conv7, while
this layer only requires d‚Ä≤ = 153 to achieve 3√ó singlelayer speedup of itself. This can be explained by
Fig. 2(c). The energy of Conv5-7 is less concentrated,
so these layers require higher ranks to achieve good
approximations.
As we will show, the rank selection is more promi-
Table 2: Whole-model acceleration with/without rank selection for SPP-10. The solver is the asymmetric
version. The speedup ratios shown here involve all convolutional layers (Conv1-Conv7). We Ô¨Åx d‚Ä≤ = 32 in
Conv1. In the case of no rank selection, the speedup ratio of each other layer is the same. Each column of
Conv1-7 shows the rank d‚Ä≤ used, which is the number of Ô¨Ålters after approximation. The error rates are top-5
single-view, and shown as the increase of error rates compared with no approximation.
nent for VGG-16 because of its diversity of layers.
Comparisons with Jaderberg et al.‚Äôs method 
We compare with Jaderberg et al.‚Äôs method ,
which is a recent state-of-the-art solution to efÔ¨Åcient
evaluation. Although our decomposition shares some
high-level motivations as , we point out that our
optimization strategy is different with and is important for accuracy, especially for very deep models
that previous acceleration methods rarely addressed.
Jaderberg et al.‚Äôs method decomposes a k √ó k
spatial support into a cascade of k √ó 1 and 1 √ó k
spatial supports. A channel-dimension reduction is
Speedup Ratio
Increase of Error (%)
Jaderberg et al. (our impl.)
Our asymmetric
Our asymmetric (3d)
Figure 6: Comparisons with Jaderberg et al.‚Äôs spatial
decomposition method for SPP-10. The speedup
ratios are theoretical speedups of the whole model.
The error rates are top-5 single-view, and shown as
the increase of error rates compared with no approximation (smaller is better).
also considered. Their optimization method focuses
on the linear reconstruction error. In the paper of ,
their method is only evaluated on a single layer of an
OverFeat network for ImageNet.
Our comparisons are based on our implementation
of . We use the Scheme 2 decomposition in 
and its ‚ÄúÔ¨Ålter reconstruction‚Äù version (as we explain
below), which is used for ImageNet as in . Our
reproduction of the Ô¨Ålter reconstruction in gives
a 2√ó single-layer speedup on Conv2 of SPP-10 with
0.2% increase of error. As a reference, in it reports 0.5% increase of error on Conv2 under a 2√ó
single-layer speedup, evaluated on another OverFeat
network similar to SPP-10.
It is worth discussing our implementation of Jaderberg et al.‚Äôs ‚Äúdata reconstruction‚Äù scheme, which
was suggested to use SGD and backpropagation for
optimization. In our reproduction, we Ô¨Ånd that data
reconstruction works well for the character classiÔ¨Åcation
task as studied in . However, we Ô¨Ånd it nontrivial
to make data reconstruction work for large models
trained for ImageNet. We observe that the learning
rate needs to be carefully chosen for the SGD-based
data reconstruction to converge (as also reported
independently in for another decomposition),
and when the training starts to converge, the results
are still sensitive to the initialization (for which we
have tried Gaussian distributions of a wide range
of variances). We conjecture that this is because the
ImageNet dataset and models are more complicated,
and using SGD to regress a single layer may be
sensitive to multiple local optima. In fact, Jaderberg
et al.‚Äôs only report ‚ÄúÔ¨Ålter reconstruction‚Äù results
of a single layer on ImageNet. For these reasons,
our implementation of Jaderberg et al.‚Äôs method on
ImageNet models is based on Ô¨Ålter reconstruction.
We believe that these issues have not be settled and
top-5 err.
SPP-10 
SPP-10 (4√ó)
Jaderberg et al. (our impl.)
278 (3.3√ó)
2.41 (3.2√ó)
271 (3.4√ó)
2.62 (2.9√ó)
our asym. (3d)
267 (3.5√ó)
2.32 (3.3√ó)
our asym. (3d) FT
267 (3.5√ó)
2.32 (3.3√ó)
AlexNet 
Table 3: Comparisons of absolute performance of SPP-10. The top-5 error is the absolute value. The running
time is a single view on a CPU (single thread, with SSE) or a GPU. The accelerated models are those of 4√ó
theoretical speedup (Fig. 6). On the brackets are the actual speedup ratios.
need to be investigated further, and accelerating deep
networks does not just involve decomposition but also
the way of optimization.
In Fig. 6 we compare our method with Jaderberg et
al.‚Äôs for whole-model speedup. For whole-model
speedup of , we implement their method sequentially on Conv2-7 using the same speedup ratio.3 The
speedup ratios are the theoretical complexity ratios
involving all convolutional layers. Our method is the
asymmetric version and with rank selection. Fig. 6
shows that when the speedup ratios are large (4√ó
and 5√ó), our method outperforms Jaderberg et al.‚Äôs
method signiÔ¨Åcantly. For example, when the speedup
ratio is 4√ó, the increased error rate of our method
is 4.2%, while Jaderberg et al.‚Äôs is 6.0%. Jaderberg
et al.‚Äôs result degrades quickly when the speedup
ratio is getting large, while ours degrades slowly.
This suggests the effects of our method for reducing
accumulative error.
We further compare with our asymmetric version
using 3d decomposition (Sec. 3.5). In Fig. 6 we show
the results ‚Äúasymmetric (3d)‚Äù. Fig. 6 shows that this
strategy leads to signiÔ¨Åcantly smaller increase of error.
For example, when the speedup is 5√ó, the error
is increased by only 2.5%. Our asymmetric solver
effectively controls the accumulative error even if
the multiple layers are decomposed extensively, and
the 3d decomposition is easier to achieve a certain
speedup ratio.
For completeness, we also evaluate our approximation method on the character classiÔ¨Åcation model released by . Our asymmetric (3d) solution achieves
4.5√ó speedup with only a drop of 0.7% in classiÔ¨Åcation accuracy, which is better than the 1% drop for the
same speedup reported by .
Comparisons with Training from Scratch
The architecture of the approximated model can
3. We do not apply Jaderberg et al.‚Äôs method on Conv1,
because this layer has a small number of input channels (3), and
the Ô¨Årst k√ó1 decomposed layer can only have a very small number
of Ô¨Ålters (e.g., 5) to approach a speedup ratio (e.g., 4√ó). Also note
that the speedup ratio is about all conv layers, and because Conv1
is not accelerated, other layers will have a slightly larger speedup.
also be trained ‚Äúfrom scratch‚Äù on the ImageNet dataset.
One hypothesis is that the underlying architecture is
sufÔ¨Åciently powerful, and the acceleration algorithm
might be not necessary. We show that this hypothesis
is premature.
We directly train the model of the same architecture
as the decomposed model. The decomposed model is
much deeper than the original model (each layer replaced by three layers), so we adopt the initialization
method in otherwise it is not easy to converge. We
train the model for 100 epochs. We follow the common
practice in , of training ImageNet models.
The comparisons are in Table 4. The accuracy of the
model trained from scratch is worse than that of our
accelerated model by a considerable margin (2.8%).
These results indicate that the accelerating algorithms
can effectively digest information from the trained
models. They also suggest that the models trained
from scratch have much redundancy.
top-5 err.
increased err.
SPP-10 
our asym. 3d (4√ó)
from scratch
Table 4: Comparisons with the same decomposed
architecture trained from scratch.
Comparisons of Absolute Performance
Table 3 shows the comparisons of the absolute
performance of the accelerated models. We also evaluate the AlexNet which is similarly fast as our
accelerated 4√ó models. The comparison is based on
our re-implementation of AlexNet. Our AlexNet is the
same as in except that the GPU splitting is ignored.
Our re-implementation of this model has top-5 singleview error rate as 18.8% (10-view top-5 16.0% and top-
1 37.6%). This is better than the one reported in 4.
The models accelerated by our asymmetric (3d)
version have 14.1% and 13.8% top-5 error, without
4. In the 10-view error is top-5 18.2% and top-1 40.7%.
Ô¨Ålter size
# channels
output size
complexity (%)
# of zeros
Table 5: The architecture of the VGG-16 model . It has 13 conv layers and 3 fc layers. The column
‚Äúcomplexity‚Äù is the theoretical time complexity, shown as relative numbers to the total convolutional
complexity. The column ‚Äú# of zeros‚Äù is the relative portion of zero responses, which shows the ‚Äúsparsity‚Äù
of the layer.
Table 6: Whole-model acceleration with/without rank selection for VGG-16. The solver is the asymmetric
version. The speedup ratios shown here involve all convolutional layers. We do not accelerate Conv11. In the
case of no rank selection, the speedup ratio of each other layer is the same. Each column of C12-C53 shows
the rank d‚Ä≤ used, which is the number of Ô¨Ålters after approximation. The error rates are top-5 single-view, and
shown as the increase of error rates compared with no approximation.
and with Ô¨Åne-tuning. This means that the accelerated
model has 5.0% lower error than AlexNet, while its
speed is nearly the same as AlexNet.
Table 3 also shows the actual running time per view,
on a C++ implementation and Intel i7 CPU (2.9GHz)
or Nvidia K40 GPU. In our CPU version, our method
has actual speedup ratios (3.5√ó) close to theoretical
speedup ratios (4.0√ó). This overhead mainly comes
from the fc and other layers. In our GPU version, the
actual speedup ratio is about 3.3√ó. An accelerated
model is less easy for parallelism in a GPU, so the
actual ratio is lower.
Experiments with VGG-16
The very deep VGG models have substantially
improved a wide range of visual recognition tasks,
including object detection , , , , semantic
segmentation , , , , , image captioning , , , video/action recognition ,
image question answering , texture recognition
 , etc. Considering the big impact yet slow speed
of this model, we believe it is of practical signiÔ¨Åcance
to accelerate this model.
Accelerating VGG-16 for ImageNet ClassiÔ¨Åcation
Firstly we discover that our whole-model rank selection is particularly important for accelerating VGG-
16. In Table 6 we show the results without/with
rank selection. No 3d decomposition is used in this
comparison. For a 4√ó speedup, the rank selection
reduces the increased error from 6.38% to 3.84%. This
is because of the greater diversity of layers in VGG-
increase of top-5 error (1-view)
speedup ratio
Jaderberg et al. (our impl.)
our asym. (3d)
our asym. (3d) FT
Table 7: Accelerating the VGG-16 model using a speedup ratio of 3√ó, 4√ó, or 5√ó. The top-5 error rate
(1-view) of the VGG-16 model is 10.1%. This table shows the increase of error on this baseline.
top-5 error
VGG-16 
VGG-16 (4√ó)
Jaderberg et al. (our impl.)
875 (3.8√ó)
6.40 (2.9√ó)
875 (3.8√ó)
7.97 (2.3√ó)
our asym. (3d)
860 (3.8√ó)
6.30 (3.0√ó)
our asym. (3d) FT
858 (3.8√ó)
6.39 (2.9√ó)
Table 8: Absolute performance of accelerating the VGG-16 model . The top-5 error is the absolute value.
The running time is a single view on a CPU (single thread, with SSE) or a GPU. The accelerated models are
those of 4√ó theoretical speedup (Table 7). On the brackets are the actual speedup ratios.
16 (Table 5). Unlike SPP-10 (or other shallower models
 , ) that repeatedly applies 3√ó3 Ô¨Ålters on the
same feature map size, the VGG-16 model applies
them more evenly on Ô¨Åve feature map sizes (224,
112, 56, 28, and 14). Besides, as the Ô¨Ålter numbers in
Conv51-53 are not increased, the time complexity of
Conv51-53 is smaller than others. The selected ranks
d‚Ä≤ in Table 6 show their adaptivity - e.g., the layers
Conv51 to Conv53 keep more Ô¨Ålters, because they
have small time complexity and it is not a good tradeoff to compactly reduce them. The whole-model rank
selection is a key to maintain a high accuracy for
accelerating VGG-16.
In Table 7 we evaluate our method on VGG-16
for ImageNet classiÔ¨Åcation. Here we evaluate our
asymmetric 3d version (without or with Ô¨Åne-tuning).
We evaluate challenging speedup ratios of 3√ó, 4√ó and
5√ó. The ratios are those of the theoretical speedups of
all 13 conv layers.
Somewhat surprisingly, our method has demonstrated compelling results for this very deep model,
even without Ô¨Åne-tuning. Our no-Ô¨Åne-tuning model
has a 0.9% increase of 1-view top-5 error for a speedup
ratio of 4√ó. On the contrary, the previous method 
suffers greatly from the increased depth because of the
rapidly accumulated error of multiple approximated
layers. After Ô¨Åne-tuning, our model has a 0.3% increase of 1-view top-5 error for a 4√ó speedup. This
degradation is even lower than that of the shallower
model of SPP-10. This suggests that the information
in the very deep VGG-16 model is highly redundant,
and our method is able to effectively digest it.
Fig. 7 shows the actual vs. theoretical speedup ratios
of VGG-16 using CPU and GPU implementations. The
CPU speedup ratios are very close to the theoretical
ratios. The GPU implementation, which is based on
the standard Caffe library , exhibits a gap between
actual vs. theoretical ratios (as is also witnessed in
 ). GPU speedup ratios are more sensitive to specialized implementation, and the generic Caffe kernels
are not optimized for some layers (e.g., 1√ó1, 1√ó3, and
3√ó1 convolutions). We believe that a more specially
engineered implementation will increase the actual
GPU speedup ratio.
Figurnov et al.‚Äôs work is one of few existing
works that present results of accelerating the whole
model of VGG-16. They report increased top-5 1view error rates of 3.4% and 7.1% for actual CPU
speedups of 3√ó and 4√ó (for 4√ó theoretical speedup
they report a 3.8√ó actual CPU speedup). Thus our
Theoretical Speedup
Actual Speedup
Figure 7: Actual vs. theoretical speedup ratios of VGG-
16 using CPU and GPU implementations.
conv speedup
Table 9: Object detection mAP on the PASCAL VOC
2007 test set. The detector is Fast R-CNN using the
pre-trained VGG-16 model.
method is substantially more accurate than theirs.
Note that results in are after Ô¨Åne-tuning. This
suggests that Ô¨Åne-tuning is not sufÔ¨Åcient for wholemodel acceleration; a good optimization solver for the
decomposition is needed.
Accelerating VGG-16 for Object Detection
Current state-of-the-art object detection results ,
 , , mostly rely on the VGG-16 model. We
evaluate our accelerated VGG-16 models for object
detection. Our method is based on the recent Fast R-
We evaluate on the PASCAL VOC 2007 object detection benchmark . This dataset contains 5k trainval
images and 5k test images. We follow the default
setting of Fast R-CNN using the publicly released
code5. We train Fast R-CNN on the trainval set and
evaluate on the test set. The accuracy is evaluated by
mean Average Precision (mAP).
In our experiments, we Ô¨Årst approximate the VGG-
16 model on the ImageNet classiÔ¨Åcation task. Then we
use the approximated model as the pre-trained model
for Fast R-CNN. We use our asymmetric 3d version
with Ô¨Åne-tuning. Note that unlike image classiÔ¨Åcation
where the conv layers dominate running time, for Fast
R-CNN detection the conv layers consume about 70%
actual running time . The reported speedup ratios
are the theoretical speedups about the conv layers
Table 9 shows the results of the accelerated models
in PASCAL VOC 2007 detection. Our method with a
4√ó convolution speedup has a graceful degradation
of 0.8% in mAP. We believe this trade-off between
accuracy and speed is of practical importance, because
even with the recent advance of fast object detection
 , , the feature extraction running time is still
considerable.
CONCLUSION
We have presented an acceleration method for very
deep networks. Our method is evaluated under
whole-model speedup ratios. It can effectively reduce
the accumulated error of multiple layers thanks to
the nonlinear asymmetric reconstruction. Competitive
5. 
speedups and accuracy are demonstrated in the complex ImageNet classiÔ¨Åcation task and PASCAL VOC
object detection task.