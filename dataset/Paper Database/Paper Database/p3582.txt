Recurrent Neural Networks for Noise Reduction in Robust ASR
Andrew L. Maas1, Quoc V. Le1, Tyler M. O’Neil1, Oriol Vinyals2, Patrick Nguyen3, Andrew Y. Ng1
1Computer Science Department, Stanford University, CA, USA
2EECS Department, University of California - Berkeley, Berkeley, CA, USA
3Google, Inc., Mountain View, CA, USA
[amaas,quocle,toneil]@cs.stanford.edu, ,
 , 
Recent work on deep neural networks as acoustic models for automatic speech recognition (ASR) have demonstrated substantial performance improvements. We introduce a model which uses a deep recurrent auto encoder
neural network to denoise input features for robust ASR.
The model is trained on stereo (noisy and clean) audio
features to predict clean features given noisy input. The
model makes no assumptions about how noise affects the
signal, nor the existence of distinct noise environments.
Instead, the model can learn to model any type of distortion or additive noise given sufﬁcient training data. We
demonstrate the model is competitive with existing feature denoising approaches on the Aurora2 task, and outperforms a tandem approach where deep networks are
used to predict phoneme posteriors directly.
Index Terms: neural networks, robust ASR, deep learning
1. Introduction
Robust automatic speech recognition (ASR), that with
background noise and channel distortion, is a fundamental problem as ASR increasingly moves to mobile devices. Existing state-of-the-art methods for robust ASR
use specialized domain knowledge to denoise the speech
signal or train a word-segment discriminative model
robust to noise . The performance of such systems
is highly dependent upon the methods’ designers imbuing domain knowledge like temporal derivative features
or signal ﬁlters and transforms.
Neural network models have also been applied to
learn a function to map a noisy utterance x to a clean
version of the utterance y . Such function approximators encode less domain-speciﬁc knowledge and by increasing hidden layer size can approximate increasingly
nonlinear functions. Previous work also introduced neural network models which capture the temporal nature of
speech using recurrent connections, and directly estimate
word identity for isolated digit recognition . However,
previous work has focused on fairly small neural network
models and experimented with simple noise conditions –
often a single background noise environment.
Our novel approach applies large neural network
models with several hidden layers and temporally recurrent connections. Such deep models should better capture
the complex relationships between noisy and clean utterances in data with many noise environments and noise
levels. In this way, our work shares underlying principles with deep neural net acoustic models, which recently
yielded substantial improvements in ASR . We experiment with a reasonably large set of background noise
environments and demonstrate the importance of models
with many hidden layers when learning a denoising function.
Training noise reduction models using stereo (clean
and noisy) data has been used successfully with the
SPLICE algorithm . SPLICE attempts to model the
joint distribution between clean and noisy data, p(x, y).
Separate SPLICE models are trained for each noise condition and noise level. SPLICE is similar to a linear neural network with ﬁxed non-linear basis functions. Our
model attempts to learn the basis functions with multiple hidden layers as opposed to choosing nonlinearities
which may be important a priori.
We demonstrate our model’s ability to produce
cleaned utterances for the Aurora2 robust ASR task. The
method outperforms the SPLICE denoising algorithm, as
well as the hand-engineered ETSI2 advanced front end
(AFE) denoising system. We additionally perform control experiments to assess our model’s ability to generalize to an unseen noise environments, and quantify the
importance of deep and temporally recurrent neural network architecture elements.
For robust ASR we would like a function f(x) →y
which maps the noisy utterance x to a clean utterance
y with background noise and distortion removed. For
example we might hand engineer a function f(x) using
band pass ﬁlters to suppress the signal at frequencies we
suspect contain background noise. However engineering such functions is difﬁcult and often doesn’t capture
the rich complexity present in noisy utterances. Our approach instead learns the function f(x) using a broad
class of nonlinear function approximators – neural networks. Such models adapt to model the nonlinear relationships between noisy and clean data present in given
training data. Furthermore, we can construct various network architectures to model the temporal structure of
speech, as well as enhance the nonlinear capacity of the
model by building deep multilayer function approximators.
Given the noisy utterance x, the neural network outputs a prediction ˆy =f(x) of the clean utterance y. The
error of the output is measured via squared error,
||ˆy −y||2,
where || · || is the ℓ2 norm between two vectors. The
neural network learns parameters to minimize this error
on a given training set. Such noise removal functions are
often studied in the context of speciﬁc feature types, such
as cepstra. Our model is instead agnostic as to the type of
features present in y and can thus be applied to any sort
of speech feature without modiﬁcation.
2.1. Single Layer Denoising Autoencoder
A neural network which attempts to reconstruct a clean
version of its own noisy input is known in the literature
as a denoising autoencoder (DAE) . A single hidden
layer DAE outputs its prediction ˆy using a linear reconstruction layer and single hidden layer of the form,
ˆy = V h(1)(x) + c
h(1)(x) = σ(W (1)x + b(1))
The weight matrices V and W (1) along with the bias vectors c and b(1) parameters of the model. The hidden layer
representation h(1)(x) is a nonlinear function of the input
vector x because σ(˙) is a point-wise nonlinearity. We use
the logistic function σ(z) =
1+ez in our work.
Because an utterance x is variable-length and training an autoencoder with high-dimensional input is expensive, it is typical to train a DAE using a small temporal
context window. This increases computational efﬁciency
and saves the model from needing to re-learn the same
denoising function at each point in time. Furthermore,
this technique allows the model to handle large variation
in utterance durations without the need to zero pad inputs
to some maximum length. Ultimately the entire clean utterance prediction ˆy is created by applying the DAE at
each time sample of the input utterance – much in the
same way as a convolutional ﬁlter.
2.2. Recurrent Denoising Autoencoder
The conventional DAE assumes that only short context
regions are needed to reconstruct a clean signal, and thus
[x1 x2 x3]
[x(N-1) xN 0]
Figure 1: Deep Recurrent Denoising Autoencoder.
model with 3 hidden layers that takes 3 frames of noisy
input features and predicts a clean version of the center
considers small temporal windows of the utterance independently. Intuitively this seems a bad assumption since
speech and noise signals are highly correlated at both
long and short timescales. To address this issue we add
temporally recurrent connections to the model, yielding
a recurrent denoising autoencoder (RDAE). A recurrent
network computes hidden activations using both the input
features for the current time step xt and the hidden representation from the previous time step h(1)(xt−1). The
full equation for the hidden activation at time t is thus,
h(1)(xt) = σ(W (1)xt + b(1) + Uh(1)(xt−1)),
which builds upon the DAE (Equation 3) by adding a
weight matrix U which connects hidden units for the current time step to hidden unit activations in the previous
time step. The RDAE thus does not assume independence
of each input window x but instead models temporal dependence which we expect to exist in noisy speech utterances.
2.3. Deep Architectures
A single layer RDAE is a nonlinear function, but perhaps
not a sufﬁciently expressive model to capture the complexities of noise environments and channel distortions.
We thus make the model more nonlinear and add free
parameters by adding additional hidden layers. Indeed,
much of the recent success of neural network acoustic
models is driven by deep neural networks – those with
more than one hidden layer. Our models naturally extend to using multiple hidden layers, yielding the deep
denoising autoencoder (DDAE) and the deep recurrent
denoising autoencoder (DRDAE). Figure 1 shows a DR-
DAE with 3 hidden layers. Note that recurrent connections are only used in the middle hidden layer in DRDAE
architectures.
With multiple hidden layers we denote the ith hidden
layer’s activation in response to input as h(i)(xt). Deep
hidden layers, those with i > 1 compute their activation
h(i)(xt) = σ(W (i)h(i−1)(xt) + b(i)).
Each hidden layer h(i) has a corresponding weight matrix W (i) and bias vector b(i). For recurrent models, the
middle hidden layer has temporal connections as in Equation 4.
3. Experiments
We perform robust ASR experiments using the Aurora2
corpus .
Noisy utterances in Aurora2 were created
synthetically, so it provides noisy and clean versions of
the same utterance which is required to train our denoising models. However, the training set provides only four
noise environments and we do not expect our model to
learn a general denoising function given such a limited
view of possible clean utterance corruptions. Our model
attempts to predict clean MFCC features given MFCC
features of the corrupted utterance.
3.1. Training
We train our model using the standard multi-condition
training set which includes 8,001 utterances corrupted by
4 noise types at 5 different noise levels. Model gradients
are computed via backpropagation – unrolling the model
through time with no truncation or approximation for recurrent models. The L-BFGS optimization algorithm is
used to train all models from random initialization. We
ﬁnd this batch optimization technique to perform as well
as the pre-training and stochastic gradient ﬁne-tuning approach used in other deep learning work . We train a
DRDAE with 3 hidden layers of 500 hidden units and use
an input context of 3 frames to create xt.
3.2. Robust ASR
We evaluate the model using the “clean” testing condition – where the standard HMM system is trained on
only clean data and evaluated on noisy data. This condition evaluates whether the model can properly transform
noisy features into their corresponding clean versions as
expected by the HMM acoustic models.
Table 1 shows error rates for each noise condition
averaged across the 4 noise types present in test set A.
For comparison we include the error rates when using the
original MFCC features, as well as the features produced
by the ETSI2 advanced front end (AFE) denoising system
 . Overall, our model outperforms the MFCC baseline
as well as the AFE denoising system. At lower SNRs
our model reconstructs predicts clean MFCCs much better than the AFE leading to substantial performance improvements. When averaged across SNR conditions and
ignoring the clean and -5dB settings as per the ETSI stan-
Table 1: Word error rates (WER) on Aurora2 test set A.
Performance is averaged across the four noise types in
the test set. These noise environments are the same four
present in the DRDAE training set.
dard, our model gives a WER of 10.85%. This is better than the 12.26% of the AFE, and outperforms the
SPLICE denoising algorithm which yields an 11.67%
WER for this setting.
We also compare to a tandem approach, where a neural network is trained to output phoneme posteriors .
In the tandem setup, as in our own, network output is used
as observations for the HMM Gaussian mixture models.
We ﬁnd our model to perform substantially better than
a recurrent neural network used in a tandem setup. We
trained the tandem network on exactly the same clean and
noisy data used to train our denoising model – the Aurora2 clean and multi-condition training sets combined.
Table 1 shows the tandem result. The tandem network
trained on clean and noisy data outperforms a tandem
approach trained on clean data only as reported in .
However, our feature denoising outperforms the tandem
approach, which suggests that for robust ASR it is better
to predict clean features as opposed to phoneme posteriors.
3.3. ASR on Unseen Noise Environments
Test set A uses the same four noise environments as the
training set used to train our DRDAE model. Because the
model is discriminatively trained to remove only the four
noise environments given, there is a danger it will generalize poorly to new noise environments. We thus evaluate
the DRDAE model on test set B which contains four noise
environments not seen during denoising training, Table 2
shows the result.
Our model signiﬁcantly outperforms the MFCC baseline, suggesting on unseen noise it still removes a substantial amount of noise from utterances. However, the
DRDAE performs worse than it did on test set A, which
is to be expected as it trained on only four noise environments. For test sets A and B, the AFE performs similarly
because its processing pipeline encodes no special knowledge of the noise environments. This consistent performance across noise environments for the AFE leads it to
outperform the DRDAE in this test condition. We note
Table 2: Word error rates (WER) on Aurora2 test set B.
Performance is averaged across the four noise environments in the test set. The test noise environments do not
match the four noise environments used to train the DR-
that the reported WER average of 12.25% for the SPLICE
algorithm outperforms the DRDAE average of 17.47%.
We hypothesize SPLICE generalizes better because separate models are trained for different noise environments
and at test time the noise is matched to the most similar model. Our model makes fewer assumptions and is
thus more dependent upon the training data to provide a
reasonable sample of noise environments that could be
encountered at test time.
3.4. Denoising Model Comparison
Because of the many possible neural network architectures for denoising, we wish to explore what aspects
of the architecture are important for good performance.
This also serves to help understand whether architecture
choice impacts how well the model generalizes to unseen
noise conditions. We thus train versions of the model
which are shallow as opposed to deep, and non-recurrent.
Because the WER performance metric additionally depends upon HMM training, we compare models with
a mean-squared error (MSE) metric to directly measure
how well they predict clean data y from noisy input x.
We are interested in both how well the models ﬁt
the training data, and generalize to a type of noise unseen during training. For this, we train the models using three of the four noise types present in the multicondition training set, and measure performance on the
fourth noise type as a development set. Clean utterances
of the Aurora2 test sets are not readily available so we do
not evaluate MSE on the test sets.
We train single hidden layer recurrent (RDAE) and
non-recurrent (DAE) denoising models with 1000 hidden
units each. We also train the same 3 layer with 500 hidden units each DRDAE model as used in the ASR experiment. Additionally, we train a non-recurrent version of
this model (DDAE). All models are trained with the same
input window size and on the same training set. Table 3
shows the training and development set MSE results, as
Table 3: Average mean squared error (MSE) of denoised
input with respect to the true clean features. One noise
type from the Aurora 2 multi-condition training set (T)
was used as a development set (D) to assess how well
models generalize to an unseen noise type.
two layer models were trained with and without recurrent connections for comparison. The MSE of the noisy
input serves as a reference for the error metric.
well as the MSE of the corrupted input for each noise condition. We see that both temporally recurrent connections
and multiple hidden layers are essential in both ﬁtting the
training set well and generalizing to unseen noise types.
4. Acknowledgments
The authors thank Mike Seltzer, Jasha Droppo, and three
anonymous reviewers for helpful feedback on this work.
AM was supported as an NSF IGERT Traineeship Recipient under Award 0801700. OV was supported by a
Microsoft Research Fellowship.
5. References
 ETSI, “Advanced front-end feature extraction algorithm,” Technical Report. ETSI ES 202 050, 2007.
 A. Ragni and M. Gales, “Derivative Kernels for Noise Robust
ASR,” in ASRU, 2011.
 S. Tamura and A. Waibel, “Noise reduction using connectionist
models,” in ICASSP, 1988, pp. 553–556.
 S. Parveen and P. Green, “Speech recognition with missing data
using recurrent neural nets,” in NIPS, 2001.
 G. Dahl, D. Yu, and L. Deng, “Large vocabulary continuous speech recognition with context-dependent DBN-HMMs,” in
Proc. ICASSP, 2011.
 L. Deng, A. Acero, L. Jiang, J. Droppo, and X. Huang, “Highperformance robust speech recognition using stereo training data,”
in ICASSP, 2001.
 P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol, “Extracting and composing robust features with denoising autoencoders,”
ACM, 2008, pp. 1096–1103.
 D. Pearce and H. Hirsch, “The Aurora experimental framework
for the performance evaluation of speech recognition systems under noisy conditions,” in ICSLP, 2000.
 Q. V. Le, A. Coates, B. Prochnow, and A. Y. Ng, “On Optimization Methods for Deep Learning,” in ICML, 2011.
 O. Vinyals, S. Ravuri, and D. Povey, “Revisiting Recurrent Neural
Networks for Robust ASR,” in ICASSP, 2012.