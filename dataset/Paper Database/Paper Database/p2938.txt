Deep Convolutional Neural Networks for Multi-Modality
Isointense Infant Brain Image Segmentation
Wenlu Zhanga, Rongjian Lia, Houtao Dengb, Li Wangc, Weili Lind, Shuiwang Jia,*, and
Dinggang Shenc,*
aDepartment of Computer Science, Old Dominion University, Norfolk, VA 23529
bInstacart, San Francisco, CA 94107
cIDEA Lab, Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC
dMRI Lab, Department of Radiology and BRIC, University of North Carolina at Chapel Hill, NC
The segmentation of infant brain tissue images into white matter (WM), gray matter (GM), and
cerebrospinal fluid (CSF) plays an important role in studying early brain development in health
and disease. In the isointense stage (approximately 6–8 months of age), WM and GM exhibit
similar levels of intensity in both T1 and T2 MR images, making the tissue segmentation very
challenging. Only a small number of existing methods have been designed for tissue segmentation
in this isointense stage; however, they only used a single T1 or T2 images, or the combination of
T1 and T2 images. In this paper, we propose to use deep convolutional neural networks (CNNs)
for segmenting isointense stage brain tissues using multi-modality MR images. CNNs are a type of
deep models in which trainable filters and local neighborhood pooling operations are applied
alternatingly on the raw input images, resulting in a hierarchy of increasingly complex features.
Specifically, we used multimodality information from T1, T2, and fractional anisotropy (FA)
images as inputs and then generated the segmentation maps as outputs. The multiple intermediate
layers applied convolution, pooling, normalization, and other operations to capture the highly
nonlinear mappings between inputs and outputs. We compared the performance of our approach
with that of the commonly used segmentation methods on a set of manually segmented isointense
stage brain images. Results showed that our proposed model significantly outperformed prior
methods on infant brain tissue segmentation. In addition, our results indicated that integration of
multi-modality images led to significant performance improvement.
© 2014 Elsevier Inc. All rights reserved.
*Joint corresponding author: (Shuiwang Ji), (Dinggang Shen).
Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our
customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of
the resulting proof before it is published in its final citable form. Please note that during the production process errors may be
discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.
NIH Public Access
Author Manuscript
Neuroimage. Author manuscript; available in PMC 2016 March 01.
 
Neuroimage. 2015 March ; 108: 214–224. doi:10.1016/j.neuroimage.2014.12.061.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
Image segmentation; multi-modality data; infant brain image; convolutional neural networks; deep
1. Introduction
During the first year of postnatal human brain development, the brain tissues grow quickly,
and the cognitive and motor functions undergo a wide range of development . The segmentation of infant brain tissues into white
matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) is of great importance for
studying early brain development in health and disease .
It is widely accepted that the segmentation of infant brains is more difficult than that of the
adult brains. This is mainly due to the lower tissue contrast in early-stage brains . There are three distinct WM/GM contrast patterns in chronological
order, which are infantile (birth), isointense, and adult-like (10 months and onward) . In this work, we focused on the isointense stage that corresponds to the infant age
of approximately 6–8 months. In this stage, WM and GM exhibit almost the same level of
intensity in both T1 and T2 MR images. This property makes the tissue segmentation
problem very challenging .
Currently, most of prior methods for infant brain MR image segmentation have focused on
the infantile or adult-like stages . They assumed that each tissue class can be modeled by a single Gaussian distribution
or the mixture of Gaussian distributions . This assumption may not be valid for the isointense stage,
since the distributions of WM and GM largely overlap due to early maturation and
myelination. In addition, many previous methods segmented the tissues using a single T1 or
T2 images or the combination of T1 and T2 images . It has been shown that the fractional
anisotropy (FA) images from diffusion tensor imaging provide rich information of major
fiber bundles , especially in the middle of the first year (around 6–8 months
of age). The studies in Wang et al. demonstrated that the complementary
information from multiple image modalities was beneficial to deal with the insufficient
tissue contrast.
To overcome the above-mentioned difficulties, we considered the deep convolutional neural
networks (CNNs) in this work. CNNs are a
type of multi-layer, fully trainable models that can capture highly nonlinear mappings
between inputs and outputs. These models were originally motivated from computer vision
problems and thus are intrinsically suitable for image-related applications. In this work, we
proposed to employ CNNs for segmenting infant tissue images in the isointense stage. One
appealing property of CNNs is that it can naturally integrate and combine multi-modality
brain images in determining the segmentation. Our CNNs took complementary and
multimodality information from T1, T2, and FA images as inputs and then generated the
Zhang et al.
Neuroimage. Author manuscript; available in PMC 2016 March 01.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
segmentation maps as outputs. The multiple intermediate layers applied convolution,
pooling, normalization, and other operations to transform the input to the output. The
networks contain millions of trainable parameters that were adjusted on a set of manually
segmented data. Specifically, the networks took patches centered at a pixel as inputs and
produced the tissue class of the center pixel as the output. This enabled the segmentation
results of a pixel to be determined by all pixels in the neighborhood. In addition, due to the
convolution operations applied at intermediate layers, nearby pixels contribute more to the
segmentation results than those that are far away. We compared the performance of our
approach with that of the commonly used segmentation methods. Results showed that our
proposed model significantly outperformed prior methods on infant brain tissue
segmentation. In addition, our results indicated that the integration of multi-modality images
led to significant performance improvement. Furthermore, we showed that our CNN-based
approach outperformed other methods at increasingly large margin when the size of patch
increased. This is consistent with the fact that CNNs weight pixels differently based on their
distance to the center pixel.
2. Material and methods
2.1. Data acquisition and image preprocessing
The experiments were performed with the approval of Institutional Review Board (IRB). All
the experiments on infants were approved by their parents with written forms. We acquired
T1, T2, and diffusion-weighted MR images of 10 healthy infants using a Siemens 3T headonly MR scanner. These infants were asleep, unsedated, fitted with ear protection, and their
heads were secured in a vacuum-fixation device during the scan. T1 images having 144
sagittal slices were acquired with TR/TE as 1900/4.38 ms and a flip angle of 7° using a
resolution of 1 × 1 × 1 mm3. T2 images having 64 axial slices were acquired with TR/TE as
7380/119 ms and a flip angle of 150° using a resolution of 1.25 ×1.25 ×1.95 mm3.
Diffusion-weighted images (DWI) having 60 axial slices were acquired with TR/TE as
7680/82 ms using a resolution of 2 × 2 × 2 mm3 and 42 non-collinear diffusion gradients
with a diffusion weight of 1000s/mm2.
T2 images and fractional anisotropy (FA) images, derived from distortion-corrected DWI,
were first rigidly aligned with the T1 image and further up-sampled into an isotropic grid
with a resolution of 1 × 1 × 1 mm3. A rescanning was executed when the data was
accompanied with moderate or severe motion artifacts . We then
applied intensity inhomogeneity correction on both T1 and aligned T2
images (but not for FA image since it is not needed). After that, we applied the skull
stripping and removal of cerebellum and brain stem on the T1 image by
using in-house tools. In this way, we obtained a brain mask without the skull, cerebellum
and brain stem. With this brain mask, we finally removed the skull, cerebellum and brain
stem also from the aligned T2 and FA images.
To generate manual segmentation, an initial segmentation was obtained with publicly
available infant brain segmentation software, IBEAT . Then, manual
editing was carefully performed by an experienced rater according to the T1, T2 and FA
images for correcting possible segmentation errors. ITK-SNAP 
Zhang et al.
Neuroimage. Author manuscript; available in PMC 2016 March 01.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
(www.itksnap.org) was particularly used for interactive manual editing. For each infant
brain image, there are generally 100 axial slices; we randomly selected slices from the
middle regions (40th – 60th slices) for manual segmentation. This work only used these
manually segmented slices. Since we were not able to obtain the FA images of 2 subjects,
we only used the remaining 8 subjects in this work. Note that pixels are treated as samples in
segmentation tasks. For each subject, we generated more than 10,000 patches centered at
each pixel from T1, T2, and FA images. These patches were considered as training and
testing samples in our study.
2.2. Deep CNN for multi-modality brain image segmentation
Deep learning models are a class of machines that can learn a hierarchy of features by
building high-level features from low-level ones. The convolutional neural networks
(CNNs) are a type of deep models, in which
trainable filters and local neighborhood pooling operations are applied alternatingly on the
raw input images, resulting in a hierarchy of increasingly complex features. One property of
CNN is its capability to capture highly nonlinear mappings between inputs and outputs
 . When trained with appropriate regularization, CNNs can achieve
superior performance on visual object recognition and image classification tasks . In addition, CNN has also been used in a few other
applications. In Jain et al. ; Jain and Seung ; Turaga et al. ; Helmstaedter
et al. , CNNs were applied to restore and segment the volumetric electron microscopy
images. Ciresan et al. applied deep CNNs to detect mitosis in breast histology
images by using pixel classifiers based on patches.
In this work, we proposed to use CNN for segmenting the infant brain tissues by combining
multi-modality T1, T2, and FA images. Although CNN has been used for similar tasks in
prior studies, none of them has focused on integrating and combining multi-modality image
data. Our CNN contained multiple input feature maps corresponding to different data
modalities, thus providing a natural formalism for combining multi-modality data. Since
different modalities might contain complementary information, our experimental results
showed that combining multi-modality data with CNN led to improved segmentation
performance. Figure 1 showed a CNN architecture we developed for segmenting infant brain
images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF).
2.3. Deep CNN architectures
In this study, we designed four CNN architectures to segment infant brain tissues based on
multi-modality MR images. In the following, we provided details on one of the CNN
architectures with input patch size of 13 × 13 to explain the techniques used in this work.
The detailed architecture was shown in Figure 1. This CNN architecture contained three
input feature maps corresponding to T1, T2, and FA image patches of 13 × 13. It then
applied three convolutional layers and one fully connected layer. This network also applied
local response normalization and softmax layers.
The first convolutional layer contained 64 feature maps. Each of the feature maps was
connected to all of the three input feature maps through filters of size 5 × 5. We used a stride
Zhang et al.
Neuroimage. Author manuscript; available in PMC 2016 March 01.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
size of one pixel. This generated feature maps of size 9 × 9 in this layer. The second
convolutional layer took the output of the first convolutional layer as input and contained
256 feature maps. Each of the feature maps was connected to all of the feature maps in the
previous layer through filters of size 5 × 5. We again used a stride size of one pixel. The
third convolutional layer contained 768 feature maps of size 1 × 1. They were connected to
all feature maps in the previous layer through 5 × 5 filters. We also used a stride size of one
pixel in this layer. The rectified linear unit (ReLU) function was
applied after the convolution operation in all of the convolutional layers. It has been shown
 that the use of ReLU can expedite the training of CNN.
In addition to the convolutional layers, a few other layer types have been used in the CNN.
Specifically, the local response normalization scheme was applied after the third
convolutional layer to enforce competitions between features at the same spatial location
across different feature maps. The fully-connected layer following the normalization layer
had 3 outputs that correspond to the three tissue classes. A 3-way softmax layer was used to
generate a distribution over the 3 class labels after the output of the fully-connected layer.
Our network minimized the cross entropy loss between the predicted label and ground truth
label. In addition, we used dropout to learn more robust features and
reduce overfitting. This technique set the output of each neuron to zero with probability 0.5.
The dropout was applied before the fully-connected layers in the CNN architecture of Figure
1. In total, the number of trainable parameters for this architecture is 5,332,995.
We also considered three other CNN architectures with input patch sizes of 9×9, 17×17, and
22 × 22. These CNN architectures consisted of different numbers of convolutional layers
and feature maps. Both local response normalization and softmax layers have been applied
on these architectures. We also used max-pooling layer for the architecture with input patch
size of 22 × 22 after the first convolutional layer. The pooling size was set to 2 × 2 and a
stride size of 2 × 2 was used. The complete details of these architectures were given in Table
1. The numbers of trainable parameters for these architectures are 6,577,155, 5,947,523, and
5,332,995, respectively.
2.4. Model training and calibration
We trained the networks using data consisting of patches extracted from the MR images and
the corresponding manual segmentation ground truth images. In this work, we did not
consider the segmentation of background as this is clear from the T1 images. Instead, we
focused on segmenting the three tissue types (GM, WM, and CSF) from the foreground. For
each foreground pixel, we extracted three patches centered at this pixel from T1, T2, and FA
images, respectively. The three patches were used as input feature maps of CNNs. The
corresponding output was a binary vector of length 3 indicating the tissue class to which the
pixel belonged. This procedure generated more than 10, 000 instances, each corresponding
to three patches, from each subject. We used leave-one-subject-out cross validation
procedure to evaluate the segmentation performance. Specifically, we used seven out of the
eight subjects to train the network and used the remaining subject to evaluate the
performance. The average performance across folds was reported. All the patches from each
training subject are stored in a batch file separately, leading to seven batch files in total. We
Zhang et al.
Neuroimage. Author manuscript; available in PMC 2016 March 01.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
used patches in these seven batches as the input of CNN consecutively for training. Note
that patches in each batch file were presented to the training algorithm in random orders as
was commonly used.
The weights in the networks were initialized randomly with Gaussian distribution N(0,
1×10−4) . During training, the weights were updated by stochastic
gradient descent algorithm with a momentum of 0.9 and a weight decay of 4 × 10−4. The
biases in convolutional layers and fully-connected layer were initialized to 1. The number of
epochs was tuned on a validation set consisting of patches from one randomly selected
subject in the training set. The learning rate was set to 1 × 10−4 initially.
Following Krizhevsky et al. , we first used the validation set to obtain a coarse
approximation of the optimal epoch by minimizing the validation error. This epoch number
was used to train a model on the training and validation sets consisting of seven subjects.
Then the learning rate was reduced by a factor of 10 twice successively, and the model was
trained for about 10 epochs each time. By following this procedure, the network with a patch
size of 13 × 13 was trained for about 370 epochs. The training took less than one day on a
Tesla K20c GPU with 2496 cores. The networks with other patch sizes were trained in a
similar way. One advantage of using CNN for image segmentation is that, at test time, the
entire image can be used as an input to the network to produce the segmentation map, and
patch-level prediction is not needed . This leads to very
efficient segmentation at test time. For example, our CNN models took about 50–100
seconds for segmenting an image of size 256×256.
3. Results and discussion
3.1. Experimental setup
In the experiments, we focused on evaluating our CNN architectures for segmenting the
three types of infant brain tissues. We formulated the prediction of brain tissue classes as a
three-class classification task. For comparison purposes, we also implemented two other
commonly used classification methods, namely the support vector machine (SVM) and the
random forest (RF) methods. The linear SVM was used in our experiments,
as other kernels yielded lower performance empirically. The performance of SVM was
generated by tuning the regularization parameters using cross validation. An RF is a treebased ensemble model in which a set of randomized trees are built and the final decision is
made using majority voting by all trees. This method has been used in image-related
applications , including medical image segmentation . In this work, we used RFs containing 100 trees, and
each tree was grown fully and unpruned. The number of features at each node randomly
selected to compete for the best split was set to the square root of the total number of
features. We used the “randomForest” R package in the
experiments. We reshaped the raw training patches into vectors whose elements were
considered as the input features of SVM and RF. We also compared our methods with two
common image segmentation methods, namely the coupled level set (CLS) and the majority voting (MV) methods. Note that the method based on local
dictionaries of patches proposed in Wang et al. requires the images of different
Zhang et al.
Neuroimage. Author manuscript; available in PMC 2016 March 01.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
subjects to be registered, since a local dictionary was constructed by using patches extracted
from the corresponding locations on the training images. We thus did not compare our
methods with the one in Wang et al. .
To evaluate the segmentation performance, we used the Dice ratio (DR) to quantitatively
measure the segmentation accuracy. Specifically, let A and B denote the binary segmentation
labels generated manually and computationally, respectively, about one tissue class on
pixels for certain subject. The Dice ratio is defined as
where |A| denotes the number of positive elements in the binary segmentation A, and |A∩B|
is the number of shared positive elements by A and B. The Dice ratio lies in , and a
larger value indicates a higher segmentation accuracy. We also used another measure known
as the modified Hausdorff distance (MHD). Supposing that C and D are two sets of positive
pixels identified manually and computationally, respectively, about one tissue class for a
certain subject, the MHD is defined as
where d(C, D) = maxc∈Cd(c, D), and the distance between a point c and a set of points D is
defined as d(c, D) = mind∈D||c − d||. A smaller value indicates a higher proximity of two
point sets, thus implying a higher segmentation accuracy.
3.2. Comparison of different CNN architectures
The nonlinear relationship between inputs and outputs of a CNN is represented by its multilayer architecture using convolution, pooling and normalization. We first studied the impact
of different CNN architectures on segmentation accuracy. We devised four different
architectures, and the detailed configuration have been described in Table 1. The
classification performance of these architectures was reported in Figure 2 using box plots. It
can be observed from the results that the predictive performance is generally higher for the
architectures with input patch sizes of 13 × 13 and 17 × 17. This result is consistent with the
fact that networks with more convolutional layers and feature maps tend to have a deeper
hierarchical structure and more trainable parameters. Thus, these networks are capable of
capturing the complex relationship between input and output. We can also observe that the
architecture with input patch size of 22 × 22 did not generate substantially higher predictive
performance, suggesting that the pooling operation might not be suitable for the data we
used. In the following, we focused on evaluating the performance of CNN with input patch
size of 13 × 13. To examine the patterns captured by the CNN models, we visualized the 64
filters in the first convolutional layer for the model with an input patch size of 13 × 13 in
Figure 3. Similar to the observation in Zeiler and Fergus , these filters capture
primitive image features such as edges and corners.
Zhang et al.
Neuroimage. Author manuscript; available in PMC 2016 March 01.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
3.3. Effectiveness of integrating multi-modality data
To demonstrate the effectiveness of integrating multi-modality data, we considered the
performance achieved by each single image modality. Specifically, the T1, T2, and FA
images of each subject were separately used as the input of the architecture with a patch size
of 13 × 13 in Table 1. The segmentation performance achieved using different modalities
was presented in Tables 2 and 3. It can be observed that the combination of different image
modalities invariably yielded higher performance than any of the single image modality. We
can also see that the T1 images produced the highest performance among the three
modalities. This suggests that the T1 images are most informative in discriminating the three
tissue types. Another interesting observation is that the FA images are very informative in
distinguishing GM and WM, but they achieved low performance on CSF. This might be
because the anisotropic diffusion is hardly detectable using FA for liquids such as
cerebrospinal fluid (CSF) in brain. In contrast, T2 images are more powerful for capturing
CSF instead of GM and WM. These results demonstrated that certain modality is more
informative in distinguishing certain tissue types, and combination of all modalities leads to
improved segmentation performance.
3.4. Comparison with other methods
In order to provide a comprehensive and quantitative evaluation of the proposed method, we
reported the segmentation performance on all 8 subjects using leave-one-subject-out cross
validation. The performance of CNN, RF, SVM, CLS, and MV was reported in Tables 4 and
5 using the Dice ratio and MHD, respectively. We can observe from these two tables that
CNN outperformed other methods for segmenting all three types of brain tissues in most
cases. Specifically, CNN could achieve Dice ratios as 83.55%±0.94% (CSF), 85.18%
±2.45% (GM), and 86.37% ± 2.34% (WM) on average over 8 subjects, yielding an overall
value of 85.03% ± 2.27%. In contrast, RF, SVM, CLS, and MV achieved overall Dice ratios
of 83.15% ± 2.52%, 76.95% ± 3.55%, 82.62% ± 2.76%, and 77.64% ± 8.28%, respectively.
Meanwhile, CNN also outperformed other methods in terms of MHD. Specifically, CNN
could achieve MHDs as 0.4354 ±0.0979 (CSF), 0.2482 ±0.0871 (GM), and 0.2894 ±0.0710
(WM), yielding an overall value of 0.3243 ± 0.1161. In contrast, RF, SVM, CLS, and MV
achieved overall MHDs of 0.4593 ± 0.2506, 0.6424 ± 0.2665, 0.4839 ± 0.1597, and 0.7076
± 0.5721, respectively.
To assess the statistical significance of the performance differences, we performed one-sided
Wilcoxon signed rank tests on both Dice ratio and MHD produced by the 8 subjects, and the
p-values were reported in Table 6. When considering the Dice ratio, we chose the left-sided
test with the alternative hypothesis that the averaged performance of CNN is higher than that
of either RF, SVM, CLS or MV. The right-sided test was considered for MHD. We can see
that the proposed CNN method significantly outperformed SVM, RF, CLS and MV in most
cases. These results demonstrated that CNN is effective in segmenting the infant brain
tissues as compared to other methods.
In addition to quantitatively demonstrating the advantage of the proposed CNN method, we
visually examined the segmentation results of different tissues for two subjects in Figures 4
and 5. The original T1, T2, and FA images were shown in the first row and the following
Zhang et al.
Neuroimage. Author manuscript; available in PMC 2016 March 01.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
three rows presented the segmentation results of human experts, CNN, and RF, respectively.
It can be seen that, the segmentation patterns of CNN are quite similar to the ground truth
data generated by human experts. In contrast, RF generated more defects and fuzzy
boundaries for different tissues. These results further showed that the proposed CNN method
was more effective than other methods.
In order to further compare results by different methods, the label difference maps that
compare the ground-truth segmentation with the predicted segmentation were also
presented. In Figures 6 and 7, the original T1, T2, FA images and the ground-truth
segmentations for two subjects were shown in the first rows. The false positives and false
negatives of CNN and RF were given in the second and third rows, respectively. We also
showed the segmentation results in these two figures. We can see that the CNN
outperformed RF in both the number of false pixels and the performance of tissue boundary
detection. For example, RF generated more false positives around the surface of brain, and
also more false negatives around hippocampus for white matters on Subject 2. We can also
observe that most of the mis-classified pixels are located in the areas having large tissue
contrast, such as cortices consisting of gyri and sulci. This might be explained by the fact
that our segmentation methods are patch-based, and patches centered at boundary pixels
contain pixels of multiple tissue types.
To compare the performance between CNNs and RF when the patch size varies, we reported
the performance differences between CNNs and RF averaged over 8 subjects for different
input patch sizes in Figure 8. We can observe that the performance gains of CNNs over RF
are generally amplified for an increased input patch size. This difference is even more
significant for the results of CSF and WM, which have more restricted distributions than
GM. This is because of the fact that RF treated each pixel independently, and therefore, did
not leverage the spatial relationships between pixels. In comparison, CNNs weighted pixels
differently based on their spatial distance to the center pixel, enabling the retaining of spatial
information. The impact of this essential difference between CNNs and RF is expected to be
more significant with a larger patch size, since more spatial information is ignored by RF.
This difference probably also explains why CNNs could segment the boundary pixels with a
higher accuracy, which was shown in Figures 4 and 5.
4. Conclusion and future work
In this study, we aimed at segmenting infant brain tissue images in the isointense stage. This
was achieved by employing CNNs with multiple intermediate layers to integrate and
combine multi-modality brain images. The CNNs used the complementary and
multimodality information from T1, T2, and FA images as input feature maps and generated
the segmentation labels as output feature maps. We compared the performance of our
approach with that of the commonly used segmentation methods. Results showed that our
proposed model significantly outperformed prior methods on infant brain tissue
segmentation. Overall, our experiments demonstrated that CNNs could produce more
quantitative and accurate computational modeling and results on infant tissue image
segmentation.
Zhang et al.
Neuroimage. Author manuscript; available in PMC 2016 March 01.
NIH-PA Author Manuscript
NIH-PA Author Manuscript
NIH-PA Author Manuscript
In this work, the tissue segmentation problem was formulated as a patch classification task,
where the relationship among patches was ignored. Some prior work has incorporated
geometric constraints into segmentation models . We will improve our
CNN models to include similar constraints in the future. In the current experiments, we
employed CNNs with a few hidden layers. Recent studies showed that CNNs with many
hidden layers yielded very promising performance on visual recognition tasks when
appropriate regularization was applied . We will explore CNNs
with many hidden layers in the future as more data become available. In the current study,
we used all the patches extracted from each subject for training the convolutional neural
network. The number of patches from each tissue type is not balanced. The imbalanced data
might affect the prediction performance. For example, we might use sampling and ensemble
learning for combating this imbalance problem, although this will further increase the
training time. The current work used 2D CNN for image segmentation, because only
selected slices have been manually segmented in the current data set. In principle, CNN
could be used to segment 3D images when labeled data are available. In this case, it is more
natural to apply 3D CNN as such models have been developed for
processing 3D video data. The computational costs for training and testing 3D CNNs might
be higher than those for training 2D CNNs, as 3D convolutions are involved in these
networks. We will explore these high-order models in the future.
Acknowledgments
This work was supported by the National Science Foundation grants DBI-1147134 and DBI-1350258, and the
National Institutes of Health grants EB006733, EB008374, EB009634, AG041721, MH100217, and AG042599.