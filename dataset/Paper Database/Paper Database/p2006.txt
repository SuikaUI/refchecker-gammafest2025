Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5108–5120
July 5 - 10, 2020. c⃝2020 Association for Computational Linguistics
Optimizing the Factual Correctness of a Summary:
A Study of Summarizing Radiology Reports
Yuhao Zhang1, Derek Merck2, Emily Bao Tsai1,
Christopher D. Manning1, Curtis P. Langlotz1
1Stanford University
2University of Florida
{yuhaozhang, ebtsai, manning, langlotz}@stanford.edu
 
Neural abstractive summarization models are
able to generate summaries which have high
overlap with human references. However, existing models are not optimized for factual
correctness, a critical metric in real-world applications.
In this work, we develop a general framework where we evaluate the factual
correctness of a generated summary by factchecking it automatically against its reference
using an information extraction module. We
further propose a training strategy which optimizes a neural summarization model with a
factual correctness reward via reinforcement
learning. We apply the proposed method to the
summarization of radiology reports, where factual correctness is a key requirement. On two
separate datasets collected from hospitals, we
show via both automatic and human evaluation
that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system, producing radiology summaries that approach the quality of humanauthored ones.
Introduction
Neural abstractive summarization systems aim at
generating sentences which compress a document
while preserving the key facts in it .
These systems are potentially useful in many realworld applications.
For example, Zhang et al.
 have shown that customized neural abstractive summarization models are able to generate
radiology summary statements with high quality
by summarizing textual ﬁndings written by radiologists. This task has signiﬁcant clinical value
because of its potential to accelerate the radiology workﬂow, reduce repetitive human labor, and
improve clinical communications (Kahn Jr et al.,
Background: radiographic examination of the chest.
clinical history: 80 years of age, male ...
Findings: frontal radiograph of the chest demonstrates
repositioning of the right atrial lead possibly into the ivc.
... a right apical pneumothorax can be seen from the
image. moderate right and small left pleural effusions
continue. no pulmonary edema is observed. heart size
is upper limits of normal.
Human Summary: pneumothorax is seen. bilateral
pleural effusions continue.
Summary A (ROUGE-L = 0.77):
no pneumothorax is observed. bilateral pleural effusions continue.
Summary B (ROUGE-L = 0.44):
pneumothorax is observed on radiograph. bilateral pleural effusions continue to be seen.
A (truncated) radiology report and summaries with their ROUGE-L scores. Compared to the
human summary, Summary A has high textual overlap
(i.e., ROUGE-L) but makes a factual error; Summary
B has a lower ROUGE-L score but is factually correct.
However, while existing abstractive summarization models are optimized to generate summaries
that highly overlap with human references , this does not guarantee factually correct summaries, as shown in Figure 1. Therefore,
maintaining factual correctness of the generated
summaries remains a critical yet unsolved problem.
For example, Zhang et al. found that about
30% of the outputs from a radiology summarization model contain factual errors or inconsistencies.
This has made such a system unusable in practice,
as factual correctness is critically important in this
domain to prevent medical errors.
Existing attempts at improving the factual correctness of abstractive summarization models have
seen very limited success. For example, Cao et al.
 augmented the attention mechanism of neural models with factual triples extracted with open
information extraction systems; Falke et al. 
studied using natural language inference systems to
rerank generated summaries based on their factual
consistencies; Kry´sci´nski et al. proposed to
verify factual consistency of generated summaries
with a weakly-supervised model. Despite these
efforts, none of the existing work has focused explicitly on optimizing an abstractive summarization
system with a correctness objective. As a result,
even state-of-the-art systems trained with ample
data still produce summaries with a substantial
number of factual errors .
In this work we aim to optimize the factual correctness of existing neural summarization systems,
with a focus on summarizing radiology reports.
This task has several key properties that make it
ideal for studying factual correctness in summarization models. First, the clinical facts or observations
present in radiology reports have less ambiguity
compared to open-domain text, which allows objective comparison of facts. Second, radiology reports
involve a relatively limited space of facts, which
makes automatic measurement of factual correctness in the generated text approachable. Lastly, as
factual correctness is a crucial metric in this domain, improving factual correctness will directly
lead to an ability to use the system.
To this end, we design a framework where an
external information extraction system is used to
extract information in the generated summary and
produce a factual accuracy score by comparing it
against the human reference summary. We further
develop a training strategy where we combine a
factual correctness objective, a textual overlap objective and a language model objective, and jointly
optimize them via reinforcement learning (RL).
On two datasets of radiology reports collected
from different hospitals, we show that our training
strategy substantially improves the factual correctness of the summaries generated by a competitive
neural summarization system. Moreover, we observe for the ﬁrst time that, even in the absence of
a factual correctness objective, optimizing a textual overlap-based metric substantially improves
the factual correctness of the resulting system compared to maximum likelihood training. We further
show via human evaluation and analysis that our
training strategy leads to summaries with higher
overall quality and correctness and which are closer
to the human-written ones.
Our main contributions are: (i) we propose a
general framework and a training strategy for improving the factual correctness of summarization
models by optimizing a multi-part objective via RL;
(ii) we apply the proposed strategy to radiology reports, and empirically show that it improves the factual correctness of the generated summaries; and
(iii) we demonstrate via radiologist evaluation that
our system is able to generate summaries with clinical validity close to human-written ones. To our
knowledge, our work represents the ﬁrst attempt at
directly optimizing a neural summarization system
with a factual correctness objective via RL.
Related Work
Neural Summarization Systems.
Neural models for text summarization can be broadly divided
into extractive approaches and abstractive approaches .
While existing models are often trained in an endto-end manner by maximizing the likelihood of the
reference summaries, RL has been shown useful in
recent work . Speciﬁcally, Paulus et al. found that
directly optimizing an abstractive summarization
model on the ROUGE metric via RL can improve
the summary ROUGE scores. Our work extends
the rewards used in existing work with a factual correctness reward to further improve the correctness
of the generated summaries.
Factual Correctness in Summarization.
work is closely related to recent work that studies factual correctness in summarization. Cao et al.
 proposed to improve summarization models
by attending to fact triples extracted using open
information extraction systems. Goodrich et al.
 compared different information extraction
systems to evaluate the factual accuracy of generated text. Falke et al. explored using natural language inference systems to evaluate the correctness of generated summaries, and found models trained on existing datasets to be inadequate.
Kry´sci´nski et al. proposed to evaluate factual consistencies in the generated summaries using
a weakly-supervised fact veriﬁcation model. Despite these efforts, none of this work has shown success in directly optimizing a summarization system
for factual correctness, and to our knowledge our
work represents the ﬁrst attempt in this direction.
While our work is focused on improving neural
summarization models, we note that the idea of
using information extraction systems to evaluate
the ﬁdelity of generated text has also been explored
for data-to-text generation .
Summarization of Radiology Reports.
et al. ﬁrst studied the problem of automatic
generation of radiology impressions by summarizing textual radiology ﬁndings, and showed that
an augmented pointer-generator model achieves
high overlap with human references. MacAvaney
et al. extended this model with an ontologyaware pointer-generator and showed improved summarization quality. Li et al. and Liu et al.
 studied generating textual descriptions of
radiology ﬁndings from medical images, and proposed RL-based approaches to tackle this problem.
While Zhang et al. found that about 30%
of the radiology summaries generated from neural
models contain factual errors, improving factual
correctness in radiology summarization remains
unstudied.
Task & Baseline Pointer-Generator
We start by brieﬂy introducing the task of summarizing radiology ﬁndings. Given a passage of
radiology ﬁndings represented as a sequence of
tokens x = {x1, x2, . . . , xN}, with N being the
length of the ﬁndings, the task involves ﬁnding a
sequence of tokens y = {y1, y2, . . . , yL} that best
summarizes the salient and clinically signiﬁcant
ﬁndings in x. In routine radiology workﬂow, an
output sequence y is produced by the radiologist,
which we treat as a reference summary sequence.1
To model the summarization process, we use
the background-augmented pointer-generator network as the backbone of our
method. This abstractive summarization model extends a pointer-generator with a
separate background section encoder and is shown
to be effective in summarizing radiology notes with
multiple sections. We brieﬂy describe this model
and refer readers to the original papers for details.
At a high level, this model ﬁrst encodes the input
sequence x into hidden states with a Bi-directional
Long Short-Term Memory (Bi-LSTM) network,
and then generates an output sequence y with a
separate LSTM decoder. To make the input information available at decoding time, an attention
1While the name “impression” is often used in clinical settings, we use “summary” and “impression” interchangeably.
mechanism over the input
hidden states is also added to the decoder.
The baseline pointer-generator model by Zhang
et al. adds two augmentations to this attentional encoder-decoder model to make it suitable
for summarizing radiology ﬁndings:
Copy Mechanism.
To enable the model to copy
words from the input, a copy mechanism is added to calculate
a generation probability at each step of decoding.
This generation probability is then used to blend
the original output vocabulary distribution and a
copy distribution to generate the next word.
Background-guided Decoding.
As shown in
Figure 1, radiology reports often consist of a background section which documents the crucial study
background information (e.g., purpose of the study,
patient conditions), and a ﬁndings section which
documents clinical observations. While words can
be copied from the ﬁndings section to form the summary, Zhang et al. found it worked better
to separately encode the background section, and
inject the representation into the decoding process
by concatenating it with the input.
Fact Checking in Summarization
Summarization models such as the one described
in Section 3 are commonly trained with the teacherforcing algorithm by
maximizing the likelihood of the reference, humanwritten summaries. However, this training strategy
results in a signiﬁcant discrepancy between what
the model sees during training and test time, often
referred to as the exposure bias issue , leading to degenerate output at test time.
An alternative training strategy is to directly optimize standard metrics such as ROUGE scores with RL and this was shown to improve
summarization quality . Nevertheless, this method still provides no guarantee that
the generated summary is factually accurate and
complete, since the ROUGE scores merely measure
the superﬁcial text overlap between two sequences
and do not account for the factual alignment between them. To illustrate this, a reference sentence
pneumonia is seen and a generated sentence pneumonia is not seen have substantial text overlap and
thus the generated sentence would achieve a high
ROUGE score, however the generated sentence
conveys an entirely opposite fact. In this section
Summarization Model
Fact Extractor
cardiomegaly
LNLL + λ1LR + λ2LC
Severe cardiomegaly is seen.
nsubj:pass
Background:
patient with chest pain …
persistent low lung volumes
with enlarged heart.
Radiographs show
severe cardiomegaly
with plural effusions.
Severe cardiomegaly
v = (0, 1, 1, 0)
ˆv = (0, 1, 0, 0)
Figure 2: Our proposed training strategy. Compared to existing work which relies only on a ROUGE reward rR, we
add a factual correctness reward rC which is enabled by a fact extractor. The summarization model is updated via
RL, using a combination of the NLL loss, a ROUGE-based loss and a factual correctness-based loss. For simplicity
we only show a subset of the clinical variables in the fact vectors v and ˆv.
we ﬁrst introduce a method to verify the factual
correctness of the generated summary against the
reference summary, and then describe a training
strategy to directly optimize a factual correctness
objective to improve summary quality.
Evaluating Factual Correctness via Fact
Extraction
A convenient way to explicitly measure the factual
correctness of a generated summary against the
reference is to ﬁrst extract and represent the facts
in a structured format. To this end, we deﬁne a
fact extractor to be an information extraction (IE)
module, denoted as f, which takes in a summary
sequence y and returns a structured fact vector v:
v = f(y) = (v1, ..., vm)
where vi is a categorical variable that we want to
measure via fact checking and m the total number of such variables. For example, in the case
of summarizing radiology reports, vi can be a binary variable that describes whether an event or a
disease such as pneumonia is present or not in a
radiology study.
Given a fact vector v output by f from a reference summary and ˆv from a generated summary,
we further deﬁne a factual accuracy score s to be
the ratio of variables in ˆv which equal the corresponding variables in v, namely:
s(ˆv, v) =
i=1 1[vi = ˆvi]
where s ∈ . Note that this method requires a
summary to be both precise and complete in order
to achieve a high s score: missing out a positive
variable or falsely claiming a negative variable will
be equally penalized.
Our general deﬁnition of the fact extractor module f allows it to have different realizations for
different domains. For our task of summarizing radiology ﬁndings, we make use of the open-source
CheXpert radiology report labeler .2 At its core, the CheXpert labeler parses
the input sentences into dependency structures and
runs a series of surface and syntactic rules to extract the presence status of 14 clinical observations
seen in chest radiology reports.3 It was evaluated to
have over 95% overall F1 when compared against
oracle annotations from multiple radiologists on a
large-scale radiology report dataset.
Improving Factual Correctness via Policy
The fact extractor module introduced above not
only enables us to measure the factual accuracy of
a generated summary, but also provides us with
an opportunity to directly optimize the factual accuracy as an objective. This can be achieved by
viewing our summarization model as an agent, the
actions of which are to generate a sequence of
words to form the summary ˆy, conditioned on the
input x.4 The agent then receives rewards r(ˆy)
for its actions, where the rewards can be designed
to measure the quality of the generated summary.
Our goal is to learn an optimal policy Pθ(y|x) for
the summarization model, parameterized by the
network parameters θ, which achieves the highest
expected reward under the training data.
Formally, we minimize loss L, the negative ex-
2 
chexpert-labeler
3For this study we used a subset of these variables and
discuss the reasons in Appendix A.
4For clarity, we drop the bold symbol and use x and y to
represent the input and output sequences, respectively.
pectation of the reward r(ˆy) over the training data:
L(θ) = −Eˆy∼Pθ(y|x)[r(ˆy)].
The gradient can be calculated as :
∇θL(θ) = −Eˆy∼Pθ(y|x)[∇θ log Pθ(ˆy|x)r(ˆy)].
In practice, we approximate this gradient over a
training example with a single Monte Carlo sample
and deduct a baseline reward to reduce the variance
of the gradient estimation:
∇θL(θ) ≈−∇θ log Pθ(ˆys|x)(r(ˆys) −¯r),
where ˆys is a sampled sequence from the model and
¯r a baseline reward. Here we adopt the self-critical
training strategy , where we
obtain the baseline reward ¯r by applying the same
reward function r to a greedily decoded sequence
ˆyg, i.e., ¯r = r(ˆyg). We empirically ﬁnd that using
this self-critical baseline reward helps stabilize the
training of our summarization model.
Reward Function
The learning strategy in Equation (5) provides us
with the ﬂexibility to optimize arbitrary reward
functions. Here we decompose our reward function
into two parts:
r = λ1rR + λ2rC,
where rR ∈ is a ROUGE reward, namely
the ROUGE-L score of the predicted
sequence ˆy against the reference y; rC ∈ is
a correctness reward, namely the factual accuracy
s of the predicted sequence against the reference
sequence, as in Equation (2); λ1, λ2 ∈ are
scalar weights that control the balance between the
two. To measure the similarity between the reference and the generation, we also experimented
with more recent metrics that rely on neural representations of text, such as the BERTScore . However, we found that these metrics,
mostly trained on web and newswire data, generalize poorly to our domain of text.
Paulus et al. found that directly optimizing a reward function without the original negative
log-likelihood (NLL) objective as used in teacherforcing can hurt the readability of the generated
summaries, and proposed to alleviate this problem
by combining the NLL objective with the RL loss.
Number of Examples
89,992 (68.8%)
84,194 (60.3%)
22,031 (16.8%)
25,966 (18.6%)
18,827 (14.4%)
29,494 (21.1%)
Table 1: Statistics of the Stanford and RIH datasets.
Here we adopt the same strategy, and our ﬁnal loss
during training is:
L = λ1LR + λ2LC + λ3LNLL,
where λ3 ∈ is an additional scalar that controls the weight of the NLL loss.
Our overall training strategy is illustrated in Figure 2. Our ﬁnal loss jointly optimizes three aspects
of the summaries: LNLL serves as a conditional
language model that optimizes the ﬂuency and relevance of the generated summary, LR controls the
brevity of the summary and encourages summaries
which have high overlap with human references,
and LC encourages summaries that are factually accurate when compared against human references.
Experiments
We collected two real-world radiology report
datasets and describe our experiments using them
as our main training and evaluation corpora.
Data Collection
We collected anonymized chest radiographic reports within a certain period of time from two collaborating hospitals: the Stanford University Hospital and the Rhode Island Hospital (RIH).5
For both datasets, we ran simple preprocessing
following Zhang et al. . To test the generalizability of the models, instead of using random
stratiﬁcation, we stratiﬁed each dataset over time
into training, dev and test splits. We include statistics of both datasets in Table 1 and preprocessing
and stratiﬁcation details in Appendix B.
As we use the augmented pointer-generator network described in Section 3 as the backbone of
our method, we mainly compare against it as the
5Our retrospective study has been approved by the corresponding institutional review boards with waiver of consent.
Factual F1
Factual F1
LexRank 
BanditSum 
PG Baseline 
PG + RLR+C
52.0 41.0 49.3
Table 2: Main results on the two datasets. R-1, R-2, R-L represent the ROUGE scores. PG Baseline represents our
baseline augmented pointer-generator; RLR, RLC and RLR+C represent RL training with the ROUGE reward alone,
with the factual correctness reward alone and with both. All the ROUGE scores have a 95% conﬁdence interval of
at most ±0.6. F1 scores for extractive models were not evaluated for the reason discussed in Section 5.3.
baseline model (PG Baseline), and use the open
implementation by Zhang et al. .
For the proposed RL-based training, we compare three variants: training with only the ROUGE
reward (RLR), with only the factual correctness
reward (RLC), or with both (RLR+C). All three
variants have the NLL component in the training
loss as in Equation (7). For all variants, we initialize the model with the best baseline model trained
with standard teacher-forcing, and then ﬁnetune it
on the training data with the corresponding RL loss,
until it reaches the best validation score.
To understand the difﬁculty of the task and evaluate the necessity of using abstractive summarization models, we additionally evaluate two extractive summarization methods: (1) LexRank , a widely-used non-neural extractive summarization algorithm; and (2) BanditSum , a state-of-the-art RLbased neural extractive summarization model. For
both methods we use their open implementations.
We include other model implementation and training details in Appendix C.
Evaluation
We use two sets of metrics to evaluate model performance at the corpus level. First, we use the standard ROUGE scores , and report the
F1 scores for ROUGE-1, ROUGE-2 and ROUGE-
L, which compare the word-level unigram, bigram
and longest common sequence overlap with the
reference summary, respectively.
For factual correctness evaluation, we use a Factual F1 score. While the factual accuracy score s
that we use in the reward function evaluates how
factually accurate a speciﬁc summary is, comparing it at the corpus level can be misleading, for the
same reason that accuracy is a misleading measure
in information retrieval . To
understand this, imagine the case where a clinical
variable v has rare presence in the corpus. A model
which always generates a negative summary for it
(i.e., v = 0; the disease is not present) can have
high accuracy, but is useless in practice. Instead,
for each variable, we obtain a model’s predictions
over all test examples and calculate its F1 score.
We then macro-average the F1 of all variables to
obtain the overall factual F1 score of the model.
Note that the CheXpert labeler that we use is
speciﬁcally designed to run on radiology summaries, which usually have a different style of language compared to the radiology ﬁndings section
of the reports (see further analysis in Section 7).
As a result, we found the labeler to be less accurate when applied to the ﬁndings section. For this
reason, we were not able to estimate the factual
F1 scores on the summaries generated by the two
extractive summarization models.
We ﬁrst present our automatic evaluation results
on the two collected datasets. We then present a
human evaluation with board-certiﬁed radiologists
where we compare the summaries generated by
humans, the baseline and our proposed model.
Automatic Evaluation
Our main results on both datasets are shown in
Table 2. We ﬁrst notice that while the neural extractive model, BanditSum, outperforms the non-neural
extractive method on ROUGE scores, our PG baseline model substantially outperforms both of them,
PG Baseline RLR+C
No Finding
Cardiomegaly
Airspace Opacity
Consolidation
Atelectasis
Pneumothorax
Pleural Effusion
Macro Avg.
Table 3: Test set factual F1 scores for each variable on
the Stanford dataset. ∗marks statistically signiﬁcant
improvements with p < .01 under a bootstrap test.
suggesting that on both datasets abstractive summarization is necessary to generate summaries comparable to human-written ones. We further show
that this difference is likely due to the different
styles of language (see Section 7): while radiologists tend to use more compressed language when
writing the summaries, extractive methods produce
more verbose summaries that fail to capture this
difference.
On the Stanford dataset, training the pointergenerator model with ROUGE reward alone (RLR)
leads to improvements on all ROUGE scores, with
a gain of 2.9 ROUGE-L scores. Training with the
factual correctness reward alone (RLC) leads to
the best overall factual F1 with a substantial gain
of 10% absolute, however with consistent decline
in the ROUGE scores compared to RLR training.
Combining the ROUGE and the factual correctness
rewards (RLR+C) achieves a balance between the
two, leading to an overall improvement of 2.7 on
ROUGE-L and 8.6% on factual F1 compared to
the baseline. This indicates that RLR+C training
leads to both higher overlap with references and
improved factual correctness.
Most surprisingly, while ROUGE has been criticized for its poor correlation with human judgment
of quality and insufﬁciency for evaluating correctness of the generated text ,
we ﬁnd that optimizing ROUGE reward jointly with
NLL leads to substantially more factually correct
summaries than the baseline, shown by the notable
gain of 7.3% factual F1 from the RLR training.
All of our ﬁndings are consistent on the RIH
dataset, with RLR+C achieving an overall improve-
Stanford Dataset
Background: radiographic examination of the chest ...
Findings: continuous rhythm monitoring device again seen projecting
over the left heart.
persistent low lung volumes with unchanged cardiomegaly. again seen is a diffuse reticular pattern with interstitial prominence demonstrated represent underlying emphysematous changes with
superimposed increasing moderate pulmonary edema.
small bilateral
pleural effusions.
persistent bibasilar opacities left greater than right
which may represent infection versus atelectasis.
increased moderate pulmonary edema with small bilateral
pleural effusions. left greater than right basilar opacities which may represent infection versus atelectasis.
PG Baseline (s = 0.33): no signiﬁcant interval change.
RLR+C (s = 1.00): increasing moderate pulmonary edema. small bilateral pleural effusions. persistent bibasilar opacities left greater than right
which may represent infection versus atelectasis.
RIH Dataset
Background: history: lobar pneumonia, unspeciﬁed organism ...
Findings: lines/tubes: none. lungs: :::
airspace:::::
seen on prior radiographs from <date> and <date> is::
longer:::::
bilateral lungs appear clear. pleura: there is no pleural effusion or pneumothorax. heart and mediastinum: no cardiomegaly. thoracic aorta appears calciﬁed and mildly tortuous. bones: ...
Human: no acute cardiopulmonary abnormality.
PG Baseline (s = 0.75): :::
right ::::
lobe :::::
airspace ::::
disease could represent atelectasis, aspiration or pneumonia.
RLR+C (s = 1.00): no acute cardiopulmonary abnormality.
Figure 3: Truncated examples from the test sets along
with human, PG baseline and RLR+C outputs. Factual
accuracy scores (s) are also shown for the model outputs. For the Stanford example, clinical observations
in the summaries are marked for clarity; for RIH, :a
wrongly :::::
copied::::::::::
observation is marked.
ment of 2.5 ROUGE-L and 5.5% factual F1 scores.
Fine-grained Correctness.
To understand how
improvements in individual variables contribute to
the overall improvement, we show the ﬁne-grained
factual F1 scores for all variables on the Stanford dataset in Table 3 and include results on the
RIH dataset in Appendix D. We ﬁnd that on both
datasets, improvements in RLR+C can be observed
on all variables tested. We further ﬁnd that, as we
change the initialization across different training
runs, while the overall improvement on factual F1
stays approximately unchanged, the distribution of
the improvement on different variables can vary
substantially. Developing a training strategy for
ﬁne-grained control over different variables is an
interesting direction for future work.
Qualitative Results.
In Figure 3 we present two
example reports along with the human references,
the PG baseline outputs and RLR+C outputs. In the
ﬁrst example, while baseline output seems generic
and does not include any meaningful observation,
the summary from the RLR+C model aligns well
with the reference, and therefore achieves a higher
Our Model vs. PG Baseline
Factual Correctness
Overall Quality
Our Model vs. Human Reference
Factual Correctness
Overall Quality
Table 4: Results of the radiologist evaluation. The top
three rows present results when comparing our RLR+C
model output versus the baseline model output; the
bottom three rows present results when comparing our
model output versus the human-written summaries.
factual accuracy score. In the second example,
the baseline model wrongly copied an observation
from the ﬁndings although the actual context is no
longer evident, while the RLR+C model correctly
recognizes this and produces a better summary.
Human Evaluation
To study whether the improvements in the factual
correctness scores lead to improvement in summarization quality under expert judgment, we run
a comparative human evaluation following previous work . We sampled 50 test
examples from the Stanford dataset, and for each
example we presented to two board-certiﬁed radiologists the full radiology ﬁndings along with
blinded summaries from (1) the human reference,
(2) the PG baseline and (3) our RLR+C model. We
shufﬂed the three summaries such that the correspondence cannot be guessed, and asked the radiologists to compare them based on the following
three metrics: (1) ﬂuency, (2) factual correctness
and completeness, and (3) overall quality. For
each metric we asked the radiologists to rank the
three summaries, with ties allowed. After the evaluation, we converted each ranking into two binary
comparisons: (1) our model versus the baseline
model, and (2) our model versus human reference.
The results are shown in Table 4. Comparing
our model against the baseline model, we ﬁnd that:
(1) in terms of ﬂuency our model is less preferred,
although a majority of the results (60%) are ties;
(2) our model wins more on factual correctness
and overall quality. Comparing our model against
Stanford pplx.
PG Baseline
PG + RLR+C
Table 5: Perplexity scores as evaluated by the trained
radiology impression LM on the test set human references and model predictions.
human references, we ﬁnd that: (1) human wins
more on ﬂuency; (2) factual correctness results are
close, with 72% of our model outputs being at least
as good as human; (3) surprisingly, in terms of
overall quality our model was slightly preferred
by the radiologists compared to human references.
Lastly, when comparing the baseline model against
human references, we ﬁnd that outputs from the
baseline model are much less correct and lowerquality than human summaries.
Analysis & Discussion
Fluency and Style of Summaries.
evaluation results in Section 6.2 suggest that in
terms of ﬂuency our model output is less preferred
than human reference and baseline output. To further understand the ﬂuency and style of summaries
from different models at a larger scale, we trained
a neural language model (LM) for radiology summaries following previous work .
Intuitively, radiology summaries which are more
ﬂuent and consistent with humans in style should
be able to achieve a lower perplexity under this
in-domain LM, and vice versa. To this end, we
collected all human-written summaries from the
training and dev split of both datasets, which in
total gives us about 222,000 summaries. We then
trained a strong Mixture of Softmaxes LM on this corpus, and evaluated the perplexity of test set outputs for all models.
The results are shown in Table 5. We ﬁnd that
while extractive models can achieve non-trivial
overlap with references, their perplexity scores tend
to be much higher than humans. We conjecture that
this is because radiologists are trained to write the
summaries with more compressed language than
when they are writing the ﬁndings, therefore sentences directly extracted from the ﬁndings tend to
be more verbose than needed.
Top 10 trigrams (most frequent on the left)
Ratio in outputs (%)
PG Baseline
Figure 4: Distributions of the top 10 most frequent trigrams from model outputs on the Stanford test set.
We further observe that the baseline model
achieves even lower perplexity than humans, and
our proposed method leads to a perplexity score
much closer to human references. We hypothesize
that this is because models trained with teacherforcing are prone to generic generations which are
ﬂuent and relevant but may not be factually correct. Training with the proposed rewards alleviates
this issue, leading to summaries more consistent
with humans in style. For example, we ﬁnd that no
signiﬁcant interval change is a very frequent generation from the baseline, regardless of the actual
input. This sentence occurs in 34% of the baseline
outputs on the Stanford dev set, while the number
for RLR+C and human are only 24% and 17%. This
hypothesis is further conﬁrmed when we plot the
distribution of the top 10 most frequent trigrams
from different models in Figure 4: while the baseline heavily reuses the few most frequent trigrams,
our model RLR+C tends to have more diverse summaries which are closer to human references. The
same trends are observed for 4-grams and 5-grams.
Limitations.
While we showed the success of
our proposed method on improving the factual
correctness of a radiology summarization model,
we also recognize several limitations of our work.
First, our proposed training strategy crucially depends on the availability of an external IE module.
While this IE module is relatively easy to implement for a domain with a limited space of facts,
how to generalize this method to open-domain summarization remains unsolved. Second, our study
was based on a rule-based IE system, and the use of
a more robust statistical IE model can potentially
improve the results. Third, we mainly focus on key
factual errors which result in a ﬂip of the binary
outcome of an event (e.g., presence of disease),
whereas factual errors in generated summaries can
occur in other forms such as wrong adjectives or
coreference errors . We
leave the study of these problems to future work.
Conclusion
In this work we presented a general framework and
a training strategy to improve the factual correctness of neural abstractive summarization models.
We applied this approach to the summarization of
radiology reports, and showed its success via both
automatic and human evaluation on two separate
datasets collected from hospitals.
Our general takeaways include: (1) in a domain
with a limited space of facts such as radiology
reports, a carefully implemented IE system can
be used to improve the factual correctness of neural summarization models via RL; (2) even in the
absence of a reliable IE system, optimizing the
ROUGE metrics via RL can substantially improve
the factual correctness of the generated summaries.
We hope that our work draws the community’s
attention to the factual correctness issue of abstractive summarization models and inspires future
work in this direction.
Acknowledgments
The authors would like to thank the anonymous
reviewers, Peng Qi and Urvashi Khandelwal for
their helpful comments, and Dr. Jonathan Movson
for his help with obtaining the RIH data used in
this study.