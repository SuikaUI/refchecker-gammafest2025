Who are you with and Where are you going?
Kota Yamaguchi
Alexander C. Berg
Luis E. Ortiz
Tamara L. Berg
Stony Brook University
Stony Brook University, NY 11794, USA
{kyamagu, aberg, leortiz, tlberg}@cs.stonybrook.edu
We propose an agent-based behavioral model of pedestrians to improve tracking performance in realistic scenarios. In this model, we view pedestrians as decision-making
agents who consider a plethora of personal, social, and environmental factors to decide where to go next. We formulate prediction of pedestrian behavior as an energy minimization on this model. Two of our main contributions are
simple, yet effective estimates of pedestrian destination and
social relationships (groups). Our ﬁnal contribution is to
incorporate these hidden properties into an energy formulation that results in accurate behavioral prediction. We
evaluate both our estimates of destination and grouping,
as well as our accuracy at prediction and tracking against
state of the art behavioral model and show improvements,
especially in the challenging observational situation of infrequent appearance observations – something that might
occur in thousands of webcams available on the Internet.
1. Introduction
Despite many recent advances in tracking algorithms,
effective tracking in realistic scenarios is still quite challenging. One common, yet less well studied scenario, is
surveillance of scenes with infrequent appearance observations – such as the sporadic frames one would get from the
thousands of webcams streaming pictures from around the
globe. In this case, the video stream consists of images that
are low resolution, low frame rate (sometimes every few
seconds), and display uncontrolled lighting conditions. Additional confusion can result from occlusion between multiple targets due to crowding. Having a strong prior on what
we observe will be essential for successful tracking in these
challenging situations. In this paper, we look at low frame
rate and crowded tracking scenarios with a focus on the behavioral model of pedestrians. This focus helps us both predict where people will go, and who they are with, and leads
to improved tracking results.
Pedestrians exhibit complex behavior from various social and environmental factors. For instance, a pedestrian
has his or her own destination in mind a comfortable walking speed, and plans a motion path that avoids other pedestrians and scene obstacles. Our goal in this paper is to build
a behavioral model that takes into account these higher level
decisions and which can easily be “plugged” into existing
appearance-based algorithms. With this in mind, we model
individual pedestrians as agents who make decisions about
velocity in the next time step, given factors from the scene
(e.g. other pedestrians to avoid or walk with, or obstacles).
We frame this decision process as minimization of an energy function that encodes physical condition, personal motivation, and social interaction.
One aspect of our approach is that we explicitly address
the problem of estimating hidden factors that might effect
a pedestrian’s decision making. One factor is the desired
grouping behavior – who a pedestrian is trying to walk with.
Another is the pedestrian’s desired destination in the scene.
Neither of these factors is usually known in the surveillance
setting. We estimate these hidden personal properties by
viewing them as a classiﬁcation problem, predictable from
the trajectory of a pedestrian in the scene. In a surveillance
scenario, it is reasonable to assume that there is a set of
a few destinations in the scene, such as the entrance of a
building. This naturally limits the pattern of trajectories in
the scene. Also, people undergoing social interactions tend
to show a unique behavioral pattern compared with individuals moving alone. We deﬁne a feature representation of
trajectories on top of our velocity observations, and predict
both of these hidden personal variables using efﬁcient classiﬁcation approaches.
The contributions of this paper are: 1) producing an
explicit energy function based behavioral model that encodes personal, social, and environmental decision factors,
2) data-driven estimation of hidden personal properties that
affect the behavior of pedestrians, and 3) use of our proposed behavioral model for improved tracking performance
in low frame rate scenarios. We emphasize that our energy
function considers social interactions (grouping of pedestrians as they walk, talk and interact), a factor which has only
recently been explored in . Our approach to social group
estimation is simpler, and more computationally efﬁcient,
while remaining effective.
This paper is organized as follows. Section 2 describes
related work. Section 3 describes our comprehensive behavioral model, followed by parameter learning in Section
4. Section 5 details the estimation method of hidden personal properties using trajectory features.
Section 6 describes the quantitative evaluation of our behavioral model
and property estimation with application in tracking, and
Section 7 concludes this paper.
2. Related work
The pedestrian behavior model has been extensively
studied in the ﬁelds where simulation plays an important
role, such as graphics , and civil engineering ,
or where accurate prediction is required, such as robotics
 . In most crowd simulation contexts, the base model
dates back to the classic social force model , in which
the behavioral factors are assumed to give an equation that
drives pedestrians in analogy to physics. In computer vision, the attempt to detect abnormal events with the social
force model is reported in .
 , several social factors are known to affect a person’s
behavior. Antonini’s work is one of the earliest in computer vision to take advantage of the rich behavioral information in a tracking application. The discrete choice model
in their work assumes that individual pedestrians make a
choice from a discrete set of velocity options at each time
step based on social and environmental factors in the scene.
The assumption of discretized choice allows efﬁcient prediction with analytical solutions despite the large number of
factors considered in the model . However, due to the
nature of the discretization, the behavioral prediction tends
to show artifacts when metric is continuous. In contrast,
continuous models have been recently proposed by .
An advantage of continuous model is the ﬂexibility of constructing complex models, however, previous work focuses
on individual motivation of the behavior , and the
only social context is collision avoidance.
of social interaction in behavioral model. Social interaction in the pedestrian group began to be studied only recently in computer vision . A tracking application is
included in . There the problem is formulated as a simultaneous discrete assignment of hypothetical tracks and estimation of social relationships based on observations over a
short time frame using a CRF. The CRF formulation indirectly encodes a behavioral model. Our focus is to build an
explicit behavioral model which can exploit the rich behavioral context in social interactions, yet remain straightforward and efﬁcient enough to be plugged into other tracking
approaches as a module.
3. Behavioral model
3.1. An energy function for pedestrian navigation
Our behavioral model is based on an energy function for
each pedestrian that expresses the desirability of possible
directions of motion for the pedestrian. The energy function combines terms for the various factors that inﬂuence
the pedestrian’s choice. These are explained in this section.
We optimize the parameters of this model so that choosing the minimum energy direction accurately predicts the
behaviors of pedestrians in labeled training data, and then
evaluate the performance of the model on previously unseen
test data. The ﬁtting procedure is described in Section 3.2.
At each time step t, pedestrian i is represented by a state
variable s(t)
i ), where p(t)
are the position, velocity, preferred speed and
chosen destination, respectively, of pedestrian i at time t,
while Ai is the set of pedestrians in the same social group
as pedestrian i, including himself.
Note that u(t)
and Ai are not observable and usually assumed static, i.e.,
= ui, z(t)
= zi and A(t)
= Ai are time-invariant1.
As in , our model assumes that each pedestrian makes a
decision on the velocity v(t+∆t)
based on various environmental and social factors in the scene, and we model this
decision-making process as the minimization of an energy
{λ0, λ1, λ2, λ3, λ4, σd, σw, β} denotes a set of parameters, is as follows and consists of a linear combination2 of
six components:
EΘ(v; si, s−i) ≡λ0 Edamping(v; si)+
λ1 Espeed(v; si)+
λ2 Edirection(v; si)+
λ3 Eattraction(v; si, sAi)+
λ4 Egroup(v; si, sAi)+
Ecollision(v; si, s−i | σd, σw, β),
where we deﬁne sAi to be a set of state variables of the
pedestrians in i’s social group Ai, and s−i to be the set of
states of other pedestrians except i. From now on, the time
step t is dropped from each variable for notational simplicity.
The following paragraphs provide a description of each
of the six components of the energy function EΘ.
The damping term penalizes sudden changes
in the choice of velocity, relative to the current state:
Edamping(v; si) ≡|v −vi|2 .
1Sec. 4 shows how we automatically estimate these.
2The coefﬁcients are relative, so we ﬁx the collision coefﬁcient to 1.
Pedestrians have their own preferred speed depending on physical state, culture or scene environment.
The speed term penalizes choosing a speed that deviates
from the (hidden) preferred speed ui of the pedestrian i:
Espeed(v; si) ≡(ui −|v|)2 .
Direction.
The direction term concerns the choice of the
correct direction towards the goal. We model this by using
the negative cosine between the velocity choice v and the
direction to the destination zi from the current location pi:
Edirection(v; si) ≡−zi −pi
|zi −pi| · v
Attraction.
People in the same group tend to stay close
to each other while moving together. To capture this effect,
we deﬁne the attraction term as
Eattraction(v; si, sAi) ≡
where ∆pij = pi −pj. The second factor penalizes choosing a forward direction that is far from another pedestrian
j ∈Ai −{i} in the group Ai of pedestrian i. The ﬁrst factor is a weight that ﬂips this attraction effect if person j is
moving in a direction opposite to i.
People in the same group tend to walk at similar speeds and directions. The grouping term penalizes velocity choices that are different from the average velocity of
the group:
Egroup(v; si, sAi) ≡|v −¯vAi|2
where ¯vAi ≡
Note that the social group Ai always includes pedestrian i.
If Ai is a singleton, the grouping term has the same effect
as the damping term.
Collision.
Pedestrians try to avoid collisions with obstacles or other pedestrians. We use the model described in 
to capture this effect:
Ecollision(v; si, s−i | σd, σw, β) ≡
j̸=i w(si, sj) exp
−d2(v,si,sj)
Note that this term requires three parameters σd, σw, and
β. The factor w(si, sj) is a weight coefﬁcient, while the
function d(v, si, sj) in the exponent is the expected minimum distance between pedestrian i and j under a constantvelocity assumption :
w(si, sj) ≡exp
|∆pij| · vi
d(v, si, sj) ≡
∆pij −∆pij · (v −vj)
The ﬁrst term in (9) assigns less inﬂuence to distant pedestrians, while the second term in (9) assigns less weight to
pedestrians outside the view of pedestrian i.
3.2. Dynamical model
We now describe how to ﬁt the parameters of the model.
Recall our assumption that the personal properties of individual pedestrians are static, i.e., u(t)
≈Ai. With this assumption, and our energy function encoding pedestrian velocity preferences (deﬁned in
the previous subsection), the state transition of pedestrian
i from time t to t + ∆t is deﬁned by the following dynamical system:
EΘ(v; s(t)
We use a gradient descent algorithm to solve for the minima
of the energy function.
parameters
{λ0, λ1, λ2, λ3, λ4, σd, σw, β} required
our energy
function from previously annotated data.
In order to make behavioral predictions with our model,
we need to deal with the fact that personal properties ui, zi
and Ai are unobservable and thus unavailable at prediction
time. To deal with this problem, we estimate the personal
properties from the past history of states, as described in
Section 5.
We learn optimal parameters Θ∗by ﬁtting the energy
function to fully observed trajectories in the labeled training data. During training, while predicting the behavior of
an individual pedestrian, we ﬁx the states of the other pedestrians to the ground truth. Let us denote the ground truth
data by ˜si. We deﬁne the learning problem as computing
This is a complex nonlinear optimization problem and
computing a global optimum is hard. We use a variant of
simplex algorithm, with restarts, to solve for a local minima.
We use eth and hotel sequences from as training
data. The dataset includes a total of 750 pedestrians with
15,452 observations of positions under 2.5 frames per second. To estimate the personal properties, we assume the
preferred speed of a pedestrian is the average speed over
that person’s trajectories. The destination is set to be one of
4-5 manually labeled positions outside the scene according
to the direction and position at the end of the trajectories.
The social group is also manually labeled. We model scene
obstacles as virtual pedestrians with zero-velocity, and manually set these positions along the actual scene obstacles.
We sub-sampled at most 12 consecutive tracks (4.8 seconds)
every 4 frames (1.6 seconds) for each pedestrian track from
each dataset. Then we use these tracks in (13) to learn the
parameters.
4. Estimation of personal properties
Our model requires knowledge of the hidden personal
properties, preferred speed, ui, destination, zi, and social
grouping, Ai, for behavioral prediction. As described in
more detail in this section, we estimate these variables using
the trajectory’s history information available at prediction
4.1. Preferred speed
We assume a mean speed of past Npast steps as the preferred speed of the person.
A simple, but ineffective alternative is to assume a single
global speed for all pedestrians. According to pedestrian
speed statistics in , a typical person walks around 1.3
m/s. However, this approach ignores individual differences
and seems too rough in complex scenes (e.g., sometimes a
person slows down to look around, or walks together with
4.2. Destination
The key observation here is that a scene contains only a
few types of trajectories. For example, if a scene is a street
laying from left to right, we observe persons either passing
from left to right or right to left. In this case, it is easy
to see that a person walking toward the right side also has
his destination in the right side of the scene. This simple
assumption might not work if the scene is more complex
and has more potential destinations. But looking at certain
previous steps in someone’s past motion gives us a strong
cue as to where his destination is in the scene.
We generalize this observation to the destination prediction problem. Given a past trajectory r(t)
want to predict a destination zi ∈{Z1, Z2, Z3, ..., ZK} of
the pedestrian i.
We introduce a trajectory feature function fdest(r(t)
train a K-class classiﬁer Cdest to predict the destination:
= Cdest(fdest(r(t)
The feature representation of the trajectory is a concatenation of the normalized histograms of position pi, speed
|vi| and direction arctan(vi). In our experiments, position,
speed and direction histograms are discretized into 7-by-7,
7 and 9 bins, respectively. All the histograms have equally
spaced bins.
We adopt linear support vector machine (SVM) as a classiﬁer for this task. It is generally preferred to use as little trajectory history information as possible to estimate the
destination, especially when using behavioral prediction in
real time applications. We evaluate the estimation performance with respect to number of past step used to compute
features in the next section.
4.3. Social groups
Pedestrians walking in groups tend to behave differently
from pedestrian walking alone. Pedestrian in groups tend
to walk at the same speed while keeping certain distance
between each other. As attempted in , we also try to
estimate social groups in a scene, but using a simple yet
more efﬁcient approach.
The task is to decide whether a pedestrian i and another
pedestrian j are in the same group. More precisely, given
a pair of past trajectories (r(t)
j ), we want to assign a
binary label yij ∈{+1, −1} that indicates whether they are
in the same group (+1) or not (−1). This is a binary classiﬁcation problem over pairwise trajectories. By deﬁning a
feature function fgroup(r(t)
j ), we can train a classiﬁer
Cgroup from training data:
ˆyij = Cgroup(fgroup(r(t)
The predicted social group is then given by
ˆAi = {j|ˆyij = +1, j ̸= i} ∪{i} .
We use the following quantities as features:
1. normalized histogram of distance |pi −pj|,
2. normalized histogram of absolute difference in speed
||vi| −|vj||,
3. normalized histogram of absolute difference in direction | arctan(vi) −arctan(vj)|,
4. normalized histogram of absolute difference in velocity direction and relative position | arctan(pj −pi) −
arctan(vi)|, and
5. time-overlap ratio |T (t)
j |/|T (t)
j |, where
= {t′|t′ ≤t, s(t′)
̸= ∅}, i.e., a set of past time
steps in which pedestrian i appears.
As with destination estimation, we use an SVM classi-
ﬁer. In the next section, we show the accuracy of prediction
as a function of the number of past steps used to produce
the feature values.
5. Evaluation
5.1. Datasets
For evaluation, we used the eth and hotel sequences
from , and the zara01, zara02 and stu03 sequences
Table 1. Total number of annotations in datasets
Pedestrians
Observations
Destinations
from . These sequences have annotated positions. We
manually added the annotations of scene obstacles, destinations and social groups. Table 1 summarizes the number of
annotations in the different datasets. All the sequences have
25 fps, and annotations are given every 0.4 seconds.
We use all the sequences to evaluate our personalproperty estimator. For the experiments on behavioral prediction and tracking, we use eth and hotel to learn parameters, and zara01, zara02 and stu03 to evaluate.
5.2. Personal-property estimation
To evaluate the performance of destination and group
estimation,
we ran a 3-fold cross-validation on prediction accuracy.
In this experiment, we do this by
subsampling tracks {s(t′)
}t−Npast∆t≤t′≤t for Npast
{0, 1, 2, 4, 8, ∞}, 3 every 4 frames for each person i. The
sampled trajectories are then uniformly split into the 3 sets
used for the 3-fold cross-validation. Table 2 shows the average accuracy of destination prediction while Table 3 shows
the average precision and recall of social group prediction,
both as a function of the number of past steps used to compute trajectory features.
The experimental results in Table 2 suggest that the difﬁculty of destination estimation depends strongly on the type
of scene. Typically, confusion occurs when trajectories having different destinations share a sub-trajectory. In fact, our
estimation is worse in the eth and hotel datasets than in
the zara01 and zara02, because the initial part of trajectories in the former datasets look very similar, even if
those trajectories later diverge as pedestrians move. The estimation in stu03 dataset is worst because, in that dataset,
many people standing at the same location, which confuses
our predictor. Note that in the annotation we automatically
assigned the closest destination located outside the scene to
pedestrian temporarily standing. Also, we can observe that
the number of past steps used to compute trajectory features
has almost no inﬂuence on prediction accuracy. Rather, it is
remarkable that the estimation using only the current state
already gives reasonable performance. This indicates
that the location and velocity of the pedestrian in the current
3By Npast = ∞we mean all past steps available.
Table 2. Destination prediction accuracy
Table 3. Social group prediction precision and recall
scene already provides enough information to guess where
that person will move in the future.
Table 3 shows that the social group can be estimated with
reasonably well, regardless of scene environment. Also, in
this case, having more past information does indeed improve estimation performance.
Note that in this experiment we predict a label of a directional link label between two persons but do not consider
the symmetric and transitive properties of social relations in
groups. Imposing these properties via additional constraints
might further improve estimation performance.
5.3. Behavioral prediction
To evaluate the performance of behavioral prediction, we
calculated the average displacement of the predicted position of a single pedestrian from the ground truth position.
As in parameter learning, in this experiment, we also ﬁx the
states of other pedestrians to the ground truth. We evaluated the error in the zara01, zara02 and stu03 datasets
using the parameters learned from the eth and hotel
datasets. Because the destination estimator requires scene
speciﬁc data, we performed 2-fold cross-validation by splitting each dataset into a ﬁrst and a second half (corresponding to the initial and last period of time in the video).
Similarly to parameter learning, we subsampled at most
12 consecutive tracks (4.8 seconds) every 4 frames (1.6 seconds) for each pedestrian track, and computed prediction
Table 4. Error in behavioral prediction (m)
error using the average of the objective function given in
(13). Tracks in the training set are then used to build personal property estimators. We allow estimators to use at
most 5 past steps of information to compute features.
In this evaluation, we compared the constant speed
model (LIN); the collision avoidance model of with
ground truth destination (LTA) and with predicted destination (LTA+D); and our model with ground truth (ATTR),
predicted destination (ATTR+D), predicted social groups
(ATTR+G) and predicted destination and social groups
combined (ATTR+DG). The preferred speed is set to
ground truth in all cases. Table 4 summarizes the average
displacement (in meters) from ground truth at each prediction. The result is the average of a 2-fold cross-validation.
Both LTA and our model perform better than LIN in all
cases, with or without ground truth. We can also see that using predicted destination and social groups does not degrade
the error signiﬁcantly, and in fact, their combination with
our behavioral model produces better results in the zara02
and the students03 datasets. This may seem to contradict our intuition, because the model may be using incorrectly predicted destinations. However, those datasets have
many crowded scenes in which often pedestrians stop walking to chat or look around. In that case, predicting a destination outside the scene apparently is unreasonable. We
believe our predictions dynamically provided more reasonable destinations for such tricky pedestrians and thus better
describe the actual situation.
Figure 1 shows an example of the prediction over 12
steps in the zara01 dataset. A subject is moving towards
the rightside of the scene with another person, and is about
to pass by another group moving in the opposite direction.
The LIN model loses its track from the ground truth. Both
LTA and our model track the ground-truth pedestrian path
more closely. However, LTA predicts a straight path towards the goal while our model also predicts ﬂuctuations
as a consequence of the social interaction.
5.4. Tracking
We evaluated the effect of our behavioral model in a
tracking application. Having in mind a video surveillance
Figure 1. Example of behavioral prediction
scenario using a low frame rate webcam, we compare the
number of successful tracks achieved by the different models under an observation frequency of every 0.4 seconds (2.5
fps) and 1.6 seconds (0.625 fps), in the zara01, zara02
and students03 datasets, keeping behavioral prediction
running every 0.4 seconds in both cases.
To illustrate the positive effect of our behavioral prior in
this setting, we use a simple pixel correlation for the appearance model. Our tracker is a simple combination of
appearance and behavioral model:
P(p) = Pappearance(p) · Pbehavior(p)
−(1 −NCC(p))2
where NCC is the normalized cross correlation of pixels,
is the prediction from the behavioral model, and σa and
σb are the variance parameter for each model. The predicted
position at time t is thus given by
= argmaxp P(p).
Under the less frequent image observation, we treat the behavioral prediction as the combined prediction when we do
not have an appearance term.
The video sequences in the datasets have relatively noisy
background. We ﬁrst apply background subtraction as an
image preprocessing step before we compute pixel correlations. We use a running average as background of the scene,
and regarded a region having small absolute difference from
the model as a background. We start to accumulate background 5 steps before we start tracking in each sequence.
We experiment with tracking in a subsequence of videos.
As in the case of the previous section, we split the dataset
into a ﬁrst half and a second half, train a personal-property
estimator in one fold, and test in the other. Since our social
group estimator is compatible across datasets, in this experiment we use a single group estimator trained using all
three datasets. We start tracking every 16 frames and keep
them running for at most 24 subsequent frames as long as
the ground truth data exist for the scene. The experimental
data contains 55, 64 and 34 subsequences for the zara01,
zara02 and students03 datasets, respectively, in total for the 2-fold experiments. The tracker starts from a
ground-truth state, with at most 5 past steps available for
personal-property prediction and background subtraction.
Once the tracker starts, no future or ground truth information is available.
We compare the tracking performance between a linear model with full appearance observation under 2.5
fps (LIN+FULL), with less frequent appearance observation under 0.625 fps (LIN+LESS), LTA model with
full or less frequent appearance observation (LTA+FULL,
LTA+LESS), our model with full or less frequent appearance observation (ATTR+FULL or ATTR+LESS, respectively), and for reference, a simple correlation tracker without any behavioral prior under full image observation.
In this experiment, we use predicted personal properties for LTA and our model. To comply with the tracking
method in , the LTA model uses nearest-neighbor decisions to predict destination, using current direction of velocity and direction to a set of destinations. Our model uses
the prediction method of section 5.
The evaluation consists of calculating how many trackers stay within 0.5 meter from the ground-truth annotation
of the same person at the N = {8, 16, 24}-th step since
the initial state. A track that is more than 0.5 meter away
from its corresponding track in the ground-truth data is regarded as lost, while a track that is within 0.5 meter from
ground-truth but whose closest track in the ground-truth is
of different person is considered ID switch.
Figure 2 compares the number of successful tracks between tracking methods.
The performance of a tracker
under full appearance observation (2.5 fps) does not vary
among behavioral models, and full appearance observation
always results in performance improvement. However, under less frequent observation, our method outperforms in
zara01 and zara02 dataset.
Table 5 summarizes the
number of successful, ID-switched, or lost tracks under less
frequent appearance observation. The stu03 result shows
that the linear model is the best among others. This is likely
the result of irregular type of pedestrians in the dataset: in
those scenes, there are many people who walk around unpurposefully and stop to chat. Both LTA and our model
assume that a pedestrian is always walking towards the goal
and cannot correctly deal with a standing person. This resulted in better performance for the linear model.
Figure 3. Tracker example. The line indicates the displacement
from the previous frame. Under 0.625 fps, it is hard to ﬁnd a
correspondence between frames without prior.
Figure 3 shows an example of a tracker from the
zara01 dataset. Our behavioral model gives stronger preference to keeping the distance between pedestrians in the
same social group constant.
6. Conclusion and Future Work
We propose an agent-based formulation of pedestrian behavior and a method to estimate hidden personal properties. Our evaluation of destination and social group estimation, together with that of the behavioral prediction error, suggests that it is possible to get a reasonable estimate
of unobservable personal information from purely behavioral and environmental data only. Our tracking experiment
shows that, for usual scenes where pedestrians do not exhibit sudden irregular motions, our behavioral model further improves performance over simpler behavioral models
under low frame rates.
It would be interesting to extend our behavioral model by
using an explicit model of pedestrian behavior that accounts
for more that just a walking state. Also, in our future work,
we will take into account the interaction between pedestrian
behavior and scene events or objects.