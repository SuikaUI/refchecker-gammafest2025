This is the accepted manuscript made available via CHORUS. The article has been
published as:
Fast and Accurate Modeling of Molecular Atomization
Energies with Machine Learning
Matthias Rupp, Alexandre Tkatchenko, Klaus-Robert Müller, and O. Anatole von Lilienfeld
Phys. Rev. Lett. 108, 058301 — Published 31 January 2012
DOI: 10.1103/PhysRevLett.108.058301
REVIEW COPY
NOT FOR DISTRIBUTION
Fast and Accurate Modeling of Molecular Atomization Energies with Machine
Matthias Rupp,1, 2 Alexandre Tkatchenko,3, 2 Klaus-Robert M¨uller,1, 2 and O. Anatole von Lilienfeld4, 2, ∗
1Machine Learning Group, Technical University of Berlin, Franklinstr 28/29, 10587 Berlin, Germany
2Institute of Pure and Applied Mathematics, University of California Los Angeles, Los Angeles, CA 90095, USA
3Fritz-Haber-Institut der Max-Planck-Gesellschaft, 14195 Berlin, Germany
4Argonne Leadership Computing Facility, Argonne National Laboratory, Argonne, Illinois 60439, USA
We introduce a machine learning model to predict atomization energies of a diverse set of organic
molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schr¨odinger equation is mapped onto a non-linear statistical regression problem of reduced
complexity. Regression models are trained on and compared to atomization energies computed with
hybrid density-functional theory. Cross-validation over more than seven thousand organic molecules
yields a mean absolute error of ∼10 kcal/mol. Applicability is demonstrated for the prediction of
molecular atomization potential energy curves.
Solving the Schr¨odinger equation (SE), HΨ = EΨ, for
assemblies of atoms is a fundamental problem in quantum
mechanics. Alas, solutions that are exact up to numerical
precision are intractable for all but the smallest systems
with very few atoms. Hierarchies of approximations have
evolved, usually trading accuracy for computational eﬃciency . Conventionally, the external potential, deﬁned
by a set of nuclear charges {ZI} and atomic positions
{RI}, uniquely determines the Hamiltonian H of any
system, and thereby the ground state’s potential energy
by optimizing Ψ, H({ZI, RI})
7−→E. For a diverse
set of organic molecules, we show that one can use machine learning (ML) instead, {ZI, RI}
7−→E. Thus, we
circumvent the task of explicitly solving the SE by training once a machine on a ﬁnite subset of known solutions.
Since many interesting questions in physics require to
repeatedly solve the SE, the highly competitive performance of our ML approach may pave the way to large
scale exploration of molecular energies in chemical compound space .
ML techniques have recently been used with success to
map the problem of solving complex physical diﬀerential
equations to statistical models. Successful attempts include solving Fokker-Planck stochastic diﬀerential equations , parameterizing interatomic force ﬁelds for ﬁxed
chemical composition , and the discovery of novel
ternary oxides for batteries . Motivated by these, and
other related eﬀorts , we develop a non-linear regression ML model for computing molecular atomization
energies in chemical compound space .
is based on a measure of distance in compound space
that accounts for both stoichiometry and conﬁgurational
variation. After training, energies are predicted for new
(out-of-sample) molecular systems, diﬀering in composition and geometry, at negligible computational cost, i.e.
milli seconds instead of hours on a conventional CPU.
While the model is trained and tested using atomization
energies calculated at the hybrid density-functional theory (DFT) level , any other training set or level
of theory could be used as a starting point for subsequent
ML training. Cross-validation on 7165 molecules yields
a mean absolute error of 9.9 kcal/mol, which is an order of magnitude more accurate than counting bonds or
semi-empirical quantum chemistry.
We use the GDB data base, a library of nearly one
billion organic molecules that are stable and synthetically accessible according to organic chemistry rules .
While potentially applicable to any stoichiometry, as a
proof of principle we restrict ourselves to small organic
molecules. Speciﬁcally, we deﬁne a controlled test-bed
consisting of all 7165 organic molecules from the GDB
data base with up to seven “heavy” atoms that
contain C, N, O, or S, being saturated with hydrogen atoms. Atomization energies range from -800 to -
2000 kcal/mol. Structural features include a rich variety of chemistry such as double, and triple-bonds; (hetero)cycles, carboxy, cyanide, amide, alcohol, and epoxygroups.
For each of the many stoichiometries, many
constitutional (diﬀering chemical bonds) but no conformational isomers are part of this data base.
on the string representation of molecules in the data
base, we generated Cartesian geometries with OpenBabel . Thereafter, the PBE0 approximation to hybrid
DFT in converged numerical basis, as implemented in the FHI-aims code (tight settings/tier2
basis set), was used to compute reference atomization
energies for training.
Our choice of the PBE0 hybrid
functional is motivated by small errors (< 5 kcal/mol)
for thermo-chemistry data that includes molecular atomization energies .
One of the most important ingredients for ML is the
choice of an appropriate data representation that reﬂects
prior knowledge of the application domain, i.e. a model
of the underlying physics.
A variety of such “descriptors” are used by statistical methods for chem- and bioinformatics applications . For modeling atomization energies, we use the same molecular information that
enters the Hamiltonian for an electronic structure calcu-
lation, namely the set of Cartesian coordinates, {RI},
and nuclear charges, {ZI}. Our representation consists
of atomic energies, and the inter-nuclear Coulomb repulsion operator, Speciﬁcally, we represent any molecule by
a “Coulomb” matrix M,
Here, oﬀ-diagonal elements correspond to the Coulomb
repulsion between atoms I and J, while diagonal elements encode a polynomial ﬁt of atomic energies to nuclear charge.
Using ML we attempt to construct a non-linear map
between molecular characteristics and atomization energies. This requires a measure of molecular (dis)similarity
that is invariant with respect to translations, rotations,
and the index ordering of atoms. To this end, we measure the distance between two molecules by the Euclidean norm of their diagonalized Coulomb matrices:
d(M, M′) = d(ǫ, ǫ′) =
I|2, where ǫ are the
eigenvalues of M in order of decreasing absolute value.
For matrices that diﬀer in dimensionality, ǫ of the smaller
system is extended by zeros.
Note that by representing chemical compound space in this way, (i) any system is uniquely encoded because stoichiometry as well
as atomic conﬁguration are explicitly accounted for, (ii)
symmetrically equivalent atoms contribute equally, (iii)
the diagonalized M is invariant with respect to atomic
permutations, translations, and rotations, and (iv) the
distance is continuous with respect to small variations in
inter-atomic distances or nuclear charges. As discussed
in Ref. , these are all crucial criteria for representing
atomistic systems within statistical models.
In Fig. 1, relative atomization energies, as a function
of d(M, M′), and a histogram of distances are shown for
all pairs of molecules in our data set. The inset exempli-
ﬁes the distances between three molecular species, pyrrol,
thiophene, and ethanol: Within our measure of similarity the nitrogen containing aromatic heterocycle pyrrol
is ∼10 times farther away from its sulfur containing analogue, thiophene, than from ethanol. This is due to the
large diﬀerence in nuclear charges between atoms from
diﬀerent rows in the periodic table.
Within our ML model , the energy of a molecule
M is a sum over weighted Gaussians,
2σ2 d(M, Mi)2
where i runs over all molecules Mi in the training set.
Regression coeﬃcients {αi} and length-scale parameter
σ are obtained from training on {Mi, Eref
}. Note that
each training molecule i contributes to the energy not
only according to its distance, but also according to its
speciﬁc weight αi. The {Eref
} were computed at PBE0
DFT level of theory.
(Color online) Top:
Distribution of distances,
d(M, M′), for all molecular pairs occurring in the ﬁrst
7165 small organic molecules from the GDB data base .
The inset exempliﬁes two distances,
pyrrol/ethanol and
pyrrol/thiophene (N: Blue, O: Red, S: Yellow, C: Black, H:
White). Bottom: Absolute diﬀerences in atomization energies
between M and M′ as a function of d(M, M′).
To determine {αi}, we used kernel ridge regression .
This regularized model limits the norm of regression coef-
ﬁcients, {αi}, thereby ensuring the transferability of the
model to new compounds. For given length-scale σ and
regularization parameter λ, the explicit solution to the
minimization problem,
 Eest(Mi) −Eref
(K + λI)−1Eref,
exp[−d(Mi, Mj)2/(2σ2)] being the kernel matrix of all
training molecules, and I denoting the identity matrix.
We used stratiﬁed ﬁve-fold cross-validation 
for model selection and to estimate performance. Parameters λ and σ were determined in an inner loop of ﬁvefold cross-validation using a logarithmically scaling grid.
This procedure is routinely applied in machine learning
and statistics to avoid over-ﬁtting and overly optimistic
error estimates.
The dependence of the cross-validated ML performance on the number of molecules in training set, N, is
illustrated in Fig. 2 (top). When increasing N from 500
to 7000, the mean absolute error (MAE) falls oﬀfrom
more than 17 kcal/mol to less than 10 kcal/mol. Furthermore, the width σ of the Gaussian kernel decreases
from 460 to 25 on the distance scale of Fig. 1. Because
of the discrete nature of chemical space (nuclear charges
can only assume integer values), however, we do not expect continuous coverage for N →∞, implying that σ
will converge to a small but ﬁnite value. The regularization hyperparameter λ remains small throughout, consistent with the fact that we model noise-free numerical
solutions of the approximated Schr¨odinger equation. An
asymptotic ﬁt of the form ∼1/
N, based on statistical
theory suggests that the MAE can be lowered to
∼7.6 kcal/mol for N →∞. It is remarkable that already
for the here presented, relatively small training set sizes,
ML achieves errors of roughly one percent on the relevant
scale of energies, clearly outperforming bond counting or
semi-empirical quantum chemistry methods. The crossvalidated performance for a training set size of N = 1000
is displayed in Fig. 2 (bottom). There is good correlation with the DFT data. For comparison, corresponding
correlations are shown for bond counting , and semiempirical quantum chemistry (PM6 ) computed with
MOPAC . While the latter two methods exhibit a systematic shift in slope, the inset highlights that the ML
correlation accurately reproduces clustering, and slope of
Eq. (2) implies that the energy of a query molecule M
can be seen as an expansion in reference molecules {Mi}.
The regression weights {αi} are scaled by the similarity
between query and reference compound as measured by
a Gaussian of the distance.
Hence, αi assigns a positive or negative weight for the energy contribution of
the i-th reference molecule. Within the compound space
used as reference, molecules could therefore in principle
be ranked according to |α|. However, since {αi} are regression coeﬃcients in a non-linear model, i.e. after a
non-linear transformation of the training data, the resulting energy contributions are speciﬁc to the employed
training set without general implications for other properties or regions of compound space. The locality of the
model is measured by σ, enabling us to deﬁne a range outside of which reference molecules Mi can be neglected in
their contribution to the energy. For larger number N of
training samples, a smaller σ is obtained and the model
becomes more local in chemical space (see supplement
for quantiﬁcation).
In order to assess transferability and applicability of
our model to chemical compound space, we use a ML
model trained on N = 1000 molecules (model 1k). The
training set of model 1k contains all small molecules with
3 to 5 heavy atoms, and a randomized stratiﬁed selection
of larger compounds covering the entire energy range.
The thousand Coulomb matrices corresponding to the
OpenBabel conﬁgurations were included as well as four
FIG. 2: (Color online) Top: Cross-validated ML errors as
a function of number of molecules in training set, N. Bottom: For N = 1000, correlation of DFT-PBE0 results
(Eref) with ML (cross validated) based estimates (Eest) of
atomization energies.
Correlations for bond counting 
and semi-empirical quantum chemistry (PM6 ) are also
shown. Corresponding RMSE (root mean square error)/MAE
(mean absolute error) for bond counting, PM6, and ML are
75.0/71.0, 75.1/73.1, 30.1/14.9 kcal/mol, respectively.
additional Coulomb matrices per molecule.
These additional matrices were scaled in order to represent the
repulsive wall, the dissociative limit, and the energy
minimum at f = 1 . All predictions are made
for molecules that were not used during training of the
For testing the transferability, we applied the 1k model
to the remaining 6k molecules. The calculations yield errors that hardly change from the estimated performance
in the training with a MAE of 15.2 kcal/mol. For the
selected molecular subset of the seven thousand smallest
molecules in the GDB database , we therefore conclude that training on
15% of the molecules permits
predictions of atomization energies for the remaining 85%
with an accuracy of roughly 15 kcal/mol.
For probing its applicability, we investigated whether
the 1k model can also be useful beyond the equilibrium
geometries. Speciﬁcally, we calculated the functional dependence of atomization energies on scaling Cartesian geometries by a factor, f. From the 6k molecules (not used
for training) we picked four chemically diverse species.
Speciﬁcally, these molecules contain single bonds and
branching only (C7H16), a double bond (C6H12), triple
bonds including nitrogen (C6NH5), and a sulfur containing cycle with a hydroxy group (C4SH3OH). The
resulting ML atomization energy curves (Fig. 3) correctly distinguish between the molecules, closely reproduce the DFT energy at f = 1, and appear continuous and diﬀerentiable throughout relevant bonding distances.
For comparison, corresponding DFT potential
curves are also displayed.
While their well-depth and
position can be compared to the ML curve, their repulsive wall and dissociative limit was not used for training. Since DFT is a single-determinant theory, it does
not reproduce the molecular dissociation limit properly,
and does not converge to the sum of atomic energies for
large f. On the other hand, our ML model converges to
the right dissociation limit by construction. Albeit signiﬁcantly overestimating the position of the well depth
in the case of C4SH3OH and C6NH5, the ML model
is in very good agreement with the DFT data for the
larger molecules.
This might be due to the fact that
in the total set larger molecules are more frequent than
smaller molecules.
Overall, the ML model is in good
agreement with the correct physics (single and diﬀerentiable well depth of reasonable magnitude and position)
as represented by the DFT potential curves. The four
ML curves deviate from their dissociative and repulsive
limit (E(f = 2/3) = E(f = 3) = 0) used during training
at most by 20 kcal/mol at f = 2/3, and by 8 kcal/mol
at f = 3. We reiterate that while the DFT curves had
to be calculated explicitly for these four molecules, the
ML curves correspond to analytical predictions based on
a training set with one thousand other molecules.
We have developed a ML approach for modeling atomization energies across molecular compound space. For
larger training sets, N ≥1000, the accuracy of the ML
model becomes competitive with mean-ﬁeld electronic
structure theory—at a fraction of the computational cost.
We ﬁnd good performance when making predictions for
unseen organic molecules (transferability), and when
predicting atomization energies for distorted equilibrium
geometries. Our representation of molecules as Coulomb
matrices is inspired by the nuclear repulsion term in the
molecular Hamiltonian, and free atom energies. Future
extensions of our approach might be used for geometry
relaxation, chemical reactions , molecular dynamics
in various ensembles , or rational compound design
applications . Finally, our results suggest that the
Coulomb matrix, or improvements thereof, could be of
interest as a descriptor beyond the presented application.
(Color online) Energy of atomization curves of
four molecules containing single bonds and branching only
(C7H16), a double bond (C6H12), triple bonds including nitrogen (C6NH5), and a sulfur containing cycle with a hydroxy
group (C4SH3OH). (bottom to top in insets; black: Carbon;
blue: Nitrogen; yellow: Sulfur; red: Oxygen; white: Hydrogen) (DFT-PBE0 and ML model 1k).
We are thankful for helpful discussions with K. Burke,
M. Cuendet, K. Hansen, J-L. Reymond, B. C. Rinderspacher, M. Rozgic, M. Scheﬄer, A. P. Thompson,
M. E. Tuckerman, S. Varma. All authors acknowledge
support from the long program “Navigating Chemical
Compound Space for Materials and Bio Design”, IPAM,
UCLA. This research used resources of the Argonne
Leadership Computing Facility at Argonne National Laboratory, which is supported by the Oﬃce of Science
of the U.S. DOE under contract DE-AC02-06CH11357.
M. R. and K.-R. M. acknowledge partial support by DFG
(MU 987/4-2) and EU (PASCAL2).
∗Electronic address: 
J. Gasteiger,
P. Kollman,
H.F. Schaefer
P. Schreiner, eds., Encyclopedia of Computational Chemistry .
 P. Hohenberg and W. Kohn, Phys. Rev. 136, B864
 P. Kirkpatrick and C. Ellis, Nature 432, 823 .
 O. A. von Lilienfeld and M. E. Tuckerman, J. Chem.
Phys. 125, 154104 .
 R. R. Coifman, I. G. Kevrekidis, S. Lafon, M. Maggioni,
and B. Nadler, Multiscale Mod. Sim. 7, 842 .
 A. P. Bart´ok, M. C. Payne, R. Kondor, and G. Cs´anyi,
Phys. Rev. Lett. 104, 136403 .
 C. M. Handley and P. L. A. Popelier, J. Chem. Theory
Comput. 5, 1474 .
 G. Hautier, C. C. Fischer, A. Jain, T. Mueller, and
G. Ceder, Chem. Mater. 22, 3762 .
 A. Brown, B. J. Braams, K. Christoﬀel, Z. Jin, and J. M.
Bowman, J. Chem. Phys. 119, 8790 .
 S. Lorenz, A. Gross, and M. Scheﬄer, Chem. Phys. Lett.
395, 210 .
 J. Behler and M. Parrinello, Phys. Rev. Lett. 98, 146401
 J. Behler, R. Martonak, D. Donadio, and M. Parrinello,
Phys. Rev. Lett. 100, 185501 .
 W. Kohn and L. J. Sham, Phys. Rev. 140, A1133 .
 A. D. Becke, J. Chem. Phys. 98, 5648 .
 L. C. Blum and J.-L. Reymond, J. Am. Chem. Soc. 131,
8732 .
 T. Fink, H. Bruggesser, and J.-L. Reymond, Angew.
Chem. Int. Ed. 44, 1504 .
 T. Fink and J.-L. Reymond, J. Chem. Inf. Model. 47,
342 .
 R. Guha, M. T. Howard, G. R. Hutchison, P. Murray-
Rust, H. Rzepa, C. Steinbeck, J. K. Wegner, and E. Willighagen, J. Chem. Inf. Model. 46, 991 .
 J. P. Perdew, M. Ernzerhof, and K. Burke, J. Chem.
Phys. 105, 9982 .
 M. Ernzerhof and G. E. Scuseria, J. Chem. Phys. 110,
5029 .
 V. Blum, R. Gehrke, F. Hanke, P. Havu, V. Havu,
X. Ren, K. Reuter, and M. Scheﬄer, Comput. Phys.
Comm. 180, 2175 .
 B. J. Lynch and D. G. Truhlar, J. Phys. Chem. A 107,
3898 .
 G. Schneider, Nature Reviews 9, 273 .
 O. Ivanciuc, J. Chem. Inf. Comp. Sci. 40, 1412 .
 J.-L. Faulon, D. P. Visco, Jr., and R. S. Pophale, J.
Chem. Inf. Comp. Sci. 43, 707 .
 J. Behler, J. Chem. Phys. 134, 074106 .
 B. Sch¨olkopf and A. J. Smola, Learning with Kernels
 .
 T. Hastie, R. Tibshirani, and J. Friedman, The Elements
of Statistical Learning. Data Mining, Inference, and Prediction , 2nd ed.
 K.-R. M¨uller,
S. Mika, G. R¨atsch, K. Tsuda, and
B. Sch¨olkopf, IEEE Transactions on Neural Networks 12,
181 .
 Stratiﬁcation was done by sorting energies of the training data, grouping corresponding sorted compounds into
blocks of ﬁve compounds each, and, for each such
block, randomly assigning one compound to one crossvalidation fold. This procedure ensures that each fold
covers the whole energy range.
 K. R. M¨uller, M. Finke, N. Murata, K. Schulten, and
S. Amari, Neural Comp. 8, 1085 .
 As summarized on 
chemistry/data/bond energies lengths.html:
pps. A-21 to A-34; T.L. Cottrell, ”The Strengths of
Chemical Bonds,” 2nd ed., Butterworths, London, 1958;
B. deB. Darwent, ”National Standard Reference Data Series,” National Bureau of Standards, No. 31, Washington,
DC, 1970; S.W. Benson, J. Chem. Educ., 42, 502 .
 J. J. P. Stewart, J. Mol. Modeling 13, 1173 .
 MOPAC2009,
James J. P. Stewart,
Computational Chemistry,
Colorado Springs,
HTTP://OpenMOPAC.net .
 T.-C. Lim, Mol. Phys. 108, 1589 .
 For the repulsive wall, atomization energies for Coulomb
matrices were set to zero at typical roots for covalent
bonds, f = 2/3. For the minimum atomization energies, the ﬁnite diﬀerence derivative, dE/df = 0, was set
to zero at f = 1, using df = 0.005. For the dissociative
tail we assume zero atomization energies at f = 3.
 G. Cs´anyi, T. Albaret, M. C. Payne, and A. D. Vita,
Phys. Rev. Lett. 93, 175503 .
 M. E. Tuckerman, Statistical mechanics:
Theory and
molecular simulation .
 O. A. von Lilienfeld, R. Lins, and U. Rothlisberger, Phys.
Rev. Lett. 95, 153002 .
 O. A. von Lilienfeld, J. Chem. Phys. 131, 164102 .
 D. Sheppard, G. Henkelman, and O. A. von Lilienfeld, J.
Chem. Phys. 133, 084104 .