warwick.ac.uk/lib-publications
Original citation:
Larsen, Ask, Mortensen, Jens, Blomqvist, Jakob, Castelli, Ivano, Christensen, Rune, Dulak,
Marcin, Friis, Jesper, Groves, Michael, Hammer, Bjork, Hargus, Cory et al.. The atomic
simulation environment — a python library for working with atoms. Journal of Physics:
Condensed Matter. 
Permanent WRAP URL:
 
Copyright and reuse:
The Warwick Research Archive Portal (WRAP) makes this work of researchers of the
University of Warwick available open access under the following conditions.
This article is made available under the Attribution-NonCommercial-NoDerivatives 4.0 (CC
BY-NC-ND 4.0) license and may be reused according to the conditions of the license. For
more details see: 
A note on versions:
The version presented in WRAP is the published version, or, version of record, and may be
cited as it appears here.
For more information, please contact the WRAP Team at: 
Collections
Contact us
My IOPscience
The Atomic Simulation Environment — A Python library for working with atoms
This content has been downloaded from IOPscience. Please scroll down to see the full text.
Download details:
IP Address: 217.112.157.113
This content was downloaded on 22/03/2017 at 10:27
Manuscript version: Accepted Manuscript
Larsen et al
To cite this article before publication: Larsen et al, 2017, J. Phys.: Condens. Matter, at press:
 
This Accepted Manuscript is: © 2017 IOP Publishing Ltd
During the embargo period (the 12 month period from the publication of the Version of Record of this
article), the Accepted Manuscript is fully protected by copyright and cannot be reused or reposted
elsewhere.
As the Version of Record of this article is going to be / has been published on a subscription basis,
this Accepted Manuscript is available for reuse under a CC BY-NC-ND 3.0 licence after a 12 month embargo
After the embargo period, everyone is permitted to use all or part of the original content in this
article for non-commercial purposes, provided that they adhere to all the terms of the licence
 
Although reasonable endeavours have been taken to obtain all necessary permissions from third parties to
include their copyrighted content within this article, their full citation and copyright line may not be
present in this Accepted Manuscript version. Before using any content from this article, please refer to
the Version of Record on IOPscience once published for full citation and copyright details, as
permissions will likely be required. All third party content is fully copyright protected, unless
specifically stated otherwise in the figure caption in the Version of Record.
When available, you can view the Version of Record for this article at:
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
19Freiburg Centre for Interactive Materials and Bioinspired Technologies, University of Freiburg,
The Atomic Simulation Environment (ASE) is a software package written in the Python
programming language with the aim of setting up, steering, and analyzing atomistic simulations. In ASE, tasks are fully scripted in Python. The powerful syntax of Python combined
with the NumPy array library make it possible to perform very complex simulation tasks. For
example, a sequence of calculations may be performed with the use of a simple “for-loop”
construction.
Calculations of energy, forces, stresses and other quantities are performed
through interfaces to many external electronic structure codes or force ﬁelds using a uniform
interface. On top of this calculator interface, ASE provides modules for performing many
standard simulation tasks such as structure optimization, molecular dynamics, handling of
constraints and performing nudged elastic band calculations.
Introduction
The understanding of behaviour and properties of materials at the nanoscale has developed
immensely in the last decades. Experimental techniques like scanning probe microscopy and
electron microscopy have been reﬁned to provide information at the sub-nanometer scale. At
the same time, theoretical and computational methods for describing materials at the electronic
level have advanced and these methods now constitute valuable tools to obtain reliable atomicscale information .
The Atomic Simulation Environment (ASE) is a collection of Python modules intended to set
up, control, visualise, and analyse simulations at the atomic and electronic scales. ASE provides
Python classes like “Atoms” which store information about the properties and positions of
individual atoms. In this way, ASE works as a front-end for atomistic simulations where atomic
structures and parameters controlling simulations can be easily deﬁned. At the same time, the
full power of the Python language is available so that the user can control several interrelated
simulations interactively and in detail.
The execution of many atomic-scale simulations requires information about energies and forces
of atoms, and these can be calculated by several methods. One of the most popular approaches
is density functional theory (DFT) which is implemented in diﬀerent ways in dozens of freely
available codes . DFT codes calculate atomic energies and forces by solving a set of eigenvalue
equations describing the system of electrons. A simpler but also more approximate approach
is to use interatomic potentials (or so-called force ﬁelds) to calculate the forces directly from
the atomic positions . ASE can use DFT and interatomic potential codes as backends called
“Calculators” within ASE. By writing a simple Python interface between ASE and, for example,
a DFT code, the code is made available as an ASE calculator to the users of ASE. At the same
time, researchers working with this particular code can beneﬁt from the powerful setup and
simulation facilities available in ASE. Furthermore, the uniform interface to diﬀerent calculators
in ASE makes it easy to compare or combine calculations with diﬀerent codes. At the moment,
ASE has interfaces to about 30 diﬀerent atomic-scale codes as described in more detail later.
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
A few historical remarks: In the 1990s, object-oriented programming was widespread in many
ﬁelds but not used much in computational physics. Most physics codes had a monolithic character written in compiled languages like Fortran or C using static input/output ﬁles to control
the execution. However, the idea that physics codes should be “wrapped” in object-oriented
scripting languages was put forward . The idea was that the object-oriented approach would
allow the user of the program to operate with more understandable “physics” objects instead
of technical details, and that the scripting would encourage more interactive development and
testing of the program to quickly investigate new ideas. One of the tasks was therefore also
to split up the Fortran or C code to make relevant parts of the code available individually to
the scripting language. Also in the mid-nineties, the book on Design Patterns was published
discussing how to program eﬃciently using speciﬁc object-oriented patterns for diﬀerent programming challenges. These patterns encourage better structuring of the code, for example by
keeping diﬀerent sub-modules of the code as independent as possible, which improves readability
and simpliﬁes further development.
Inspired by these ideas, the ﬁrst version of ASE was developed around the turn of the
century to wrap the DACAPO DFT code at the Center of Atomic-scale Materials Physics at
the Technical University of Denmark. DACAPO is written in Fortran and controlled by a text
input ﬁle. It was decided to use Python both because of the general gain in popularity at the
time – although mostly in the computer science community – and because the development of
numerical tools like Numeric and NumArray, the predecessors of NumPy , were under way.
Gradually, more and more features, like atomic dynamics, were moved from DACAPO into ASE
to provide more control at the ﬂexible object-oriented level.
A major rewrite of ASE took place with the release of both versions 2 and 3. In the ﬁrst version
of the code, the “objectiﬁcation” was enthusiastically applied, so that for example the position
of an atom was an object. This meant that the user applying the “get position” method to
an Atom object would receive such a Position object.
One could then query this object to
get the coordinates in diﬀerent frames of reference. Over time, it turned out that too much
“objectiﬁcation” made ASE more diﬃcult to use, in particular for new users who experienced a
fairly steep learning curve to become familiar with the diﬀerent objects. It was therefore decided
to lower the degree of abstraction so that for example positions would be described by simply
the three coordinates in a default frame of reference. However, the general idea of creating code
consisting of independent modules by applying appropriate design patterns has remained. One
example is the application of the “observer-pattern” , which allows for development of a small
module of code (the “Observer”) to be called regularly during a simulation. By just attaching
the Observer to the “Dynamics” object, which is in control of the simulation, the Observer
calculations will automatically be performed as requested.
ASE has now developed into a full-ﬂedged international open-source project with developers in
several countries. Many modules have been added to ASE to perform diﬀerent tasks, for example
the identiﬁcation of transition states using the nudged elastic band method . Recently,
a database module which allows for convenient storage and retrieval of calculations including a
web-interface has also been developed. More calculators are added regularly as backends, and
new open-source projects like Amp (Atomistic Machine-learning Package) build on ASE
as a ﬂexible interface to the atomic calculators. The reﬁnement of libraries like NumPy allows
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
for more and more tasks to be eﬃciently performed at the Python level without the need for
compiled languages. This also opens up new possibilities for both inclusion of more modules in
ASE and for eﬃcient use of ASE in other projects.
In the following we provide a brief overview of the main features of ASE.
A distinguishing feature of ASE is that most tasks are accomplished by writing and running
Python scripts. Python is a dynamically typed programming language with a clear and expressive syntax. It can be used for writing everything from small scripts to large programs or
libraries like ASE itself. Python has gained popularity for scientiﬁc applications , thanks
particularly to the free and open-source numerical libraries of the SciPy community.
Consider the classical approach of many computational codes, where a compiled binary runs on a
specially formatted input ﬁle. A single run can perform only those actions that are implemented
in the code, and any change would require modifying the source code and recompiling. With
ASE, the scripting environment makes it trivial to combine several tasks in any way desired, to
attach observers that run custom code as callbacks during longer simulations, or to customize
calculation outputs.
Here is a simple example showing an interactive session in the Python interpreter:
>>> from ase import
>>> from ase.optimize
>>> from ase.calculators.nwchem
>>> from ase.io import
>>> h2 = Atoms(’H2’,
positions =[ ,
[0, 0, 0.7]])
>>> h2.calc = NWChem(xc=’PBE’)
>>> opt = BFGS(h2)
>>> opt.run(fmax =0.02)
-31.435229
-31.490773
-31.492791
-31.492848
>>> write(’H2.xyz’, h2)
>>> h2. get_potential_energy ()
-31.492847800329216
This example deﬁnes an ASE Atoms object representing a hydrogen molecule with an approximate 0.7 ˚A bond length.
ASE uses eV and ˚A as units.
The molecule is equipped with a
calculator, NWChem, which is the ASE interface to the NWChem code. It is instructed to
use the PBE functional for exchange and correlation eﬀects. Next, a structure optimization
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
is performed using the BFGS algorithm as implemented within ASE. The following lines of
output text show energy and maximum force for each iteration until it converges.
Note that due to dynamic typing, it is not necessary to declare the types of variables, and
due to automatic memory management, there are no explicit allocations or deallocations. This
makes the language clear and concise. However, Python itself is not designed for heavy numerical computations. High-performance computational codes would need to be written in a
language that gives more control over memory, such as C or Fortran. Several Python libraries
are available which provide eﬃcient implementations of numerical algorithms and other scientiﬁc
functionality. ASE relies on three external libraries:
• NumPy provides a multidimensional array class with eﬃcient implementations of basic
arithmetic and other common mathematical operations for ordinary dense arrays, such as
matrix multiplication, eigenvalue computation, and fast Fourier transforms.
• SciPy works on top of NumPy and provides algorithms for more specialized numerical
operations such as integration, optimization, special mathematical functions, sparse arrays,
and spline interpolation.
• matplotlib is a plotting library which can produce high-quality plots of many types.
Together, the three libraries provide an environment reminiscent of applications such as Octave
or Matlab.
It is also possible to write extensions in C that can be called from Python, or to link to compiled libraries written in another language. The DFT code GPAW , which is designed
speciﬁcally to work with ASE, consists of about 85–90 % Python with the remainder written
in C. Almost all logically complex tasks are written in Python, whereas only computationally
demanding parts, typically tight loops of ﬂoating point operations, are written in C. Like most
DFT codes, GPAW also relies on external libraries such as BLAS and LAPACK for high performance. ASE itself, however, does not perform extremely performance-critical functions and
is written entirely in Python.
Atoms and calculators
At the center of ASE is the Atoms object. It represents a collection of atoms of any chemical
species with given Cartesian positions. Depending on the type of simulation, the atoms may
have more properties such as velocities, masses, or magnetic moments. They may also have a
simulation cell given by three vectors and can represent crystals, surfaces, chains, or the gas
phase, by prescribing periodic or non-periodic boundary conditions along the directions of each
cell vector. Atoms objects behave similarly to Python lists:
from ase import
a = Atoms ()
a.extend(Atoms(’Xe10 ’))
# append 10 xenon
a.append(’H’)
print(a )
print(a )
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
del a[-3:]
Properties of the Atoms object are backed by NumPy arrays to retain good performance even
with thousands or millions of atoms.
ASE provides modules to generate many kinds of structures such as bulk crystals, surfaces,
or nanoparticles, and can read and write a large number of diﬀerent ﬁle formats. Structures
can further be manipulated by many operations such as rotations, translations, repetition as a
supercell, or simply by modifying the values of the positions array. Complex systems can be
formed by combining (adding) several atoms objects. Section 3 gives a detailed description.
Atoms can be equipped with calculators.
A calculator is a black box that can take atomic
numbers and positions from the atoms and calculate the energy and forces, and possibly other
properties such as the stress tensor. For example, calling get_potential_energy () on the
Atoms object will trigger a call to the calculator which evaluates and retrieves the energy.
What exactly happens behind the scenes depends on the calculator’s implementation. Many
calculators are interfaces to established electronic structure codes, and in this case, the call will
generally result in a self-consistent DFT calculation according to the parameters which were
passed when creating the calculator.
There are ASE calculators for many diﬀerent electronic structure, (semi-)empirical, tight-binding
and classical (reactive) interatomic potential codes. Some calculator interfaces are maintained
and distributed as a part of ASE, while others are included with the external codes themselves,
and a few are distributed by third parties. A few calculators are not just interfaces, but are
implemented fully in Python and are included with ASE. This is summarized in Table 1.
In addition to the listed calculators, there are two calculators which wrap ordinary calculators and add corrections to the energies and forces: One for the van der Waals corrections by
Tkatchenko and Scheﬄer , and one for the Grimme D3 dispersion correction .
The most common way to communicate with the codes is by reading and writing ﬁles, but some
have more eﬃcient interfaces that use sockets or pipes, or simply run within the same physical
process. This is discussed in Section 5.
Atomistic algorithms in ASE
On top of the atoms–calculator interface, ASE provides algorithms for diﬀerent tasks in atomistic
simulations. These algorithms typically rely on energies and forces evaluated by the calculators,
but interact only with the Atoms objects, and know nothing about the underlying implementation of the calculator.
• Molecular dynamics with diﬀerent controls such as thermostats, Section 6.1.
• Structure optimization using the atomic forces, Section 6.2.
• Saddle-point searches on the potential energy surface, such as determination of minimumenergy paths for a reaction using the nudged elastic band method (Section 6.4) or the
dimer method (Section 6.5).
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
Communication
 
 
Atomisticaa
 
 
 
Interprocess
 
 
Part of ASE
 
Part of ASE
 
 
 
 
 
 
 
 
Interprocess
 
 
Lennard–Jones
Part of ASE
 
 
Part of ASE
 
 
 
Quantum Espressoc
 
Interprocess
 
 
Part of ASE
 
 
aDistributed as part of the code instead of with ASE
bDistributed by third party: 
cDistributed by third party: 
Table 1: Summary of ASE calculators.
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
• Global structure optimization using basin hopping (Section 7.1) or minima hopping (Section 7.2) algorithms.
• Genetic algorithms for optimization of structure or chemical composition, Section 7.3.
• Analysis of molecular vibrational modes or phonon modes for solids, Section 8.
These features and more will be discussed in the following sections.
Generating structures
The ﬁrst problem in atomistic simulations is to set up an atomic structure. Using the built-in
GUI of ASE, a structure can be built by adding the desired atoms to the system and moving
them to the desired location manually. More general structures can be constructed by scripting.
This also allows for the speciﬁcation of other properties such as constraints, magnetic moments
and charges of the individual atoms.
Generic structures
ASE has modules to deﬁne a wide range of diﬀerent structures; nanotubes, bulk lattices, surfaces,
and nanoparticles are a few such examples. The simplest predeﬁned structures involve gas phase
species and small organic molecules. ASE includes the G2 test set of 148 molecules , which
are useful as predeﬁned adsorbates for slab calculations. The example below shows the manual
deﬁnition of H2 and how to retrieve H2O from the G2 collection:
from ase import
h2 = Atoms(’H2’, [(0, 0, 0), (0, 0, 0.74)])
from ase.build
water = molecule(’H2O’)
Bulk crystals can be constructed manually like this:
cu = Atoms(’Cu’, [(0, 0, 0)],
cell =[(0, a / 2, a / 2),
(a / 2, 0, a / 2),
(a / 2, a / 2, 0)],
pbc=[True , True , True ])
or equivalently with the shortcut:
from ase.build
cu = bulk(’Cu’, ’fcc’, a=3.6)
Space group
In three dimensions, the set of all symmetry operations of a crystalline structure is the space
group for this structure. All symmetry operations of the in total 230 unique space groups are
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
Figure 1: (a) unit cell of beryl, (b) same cell repeated twice and seen along .
listed in the ﬁle spacegroup.dat in the ase.spacegroup package. The space group numbers
and nomenclature follow the deﬁnitions in International Tables .
The spacegroup package can create, and to some extent manipulate, crystalline structures. For
users of ASE, the most important function in this package is crystal () which returns a unit
cell of a crystalline structure given information about the space group, lattice parameters and
scaled coordinates of the non-equivalent atoms. The example below shows how to create a unit
cell of beryl§ from crystallographic information typically provided in publications:
from ase.spacegroup
# Adamo et al. Mineralogical
Magazine 72 
beryl = crystal(
symbols =[’Al’, ’Be’, ’Si’, ’O’, ’O’],
basis =[(2. / 3., 1. / 3.,
spacegroup=’P 6/m c c’,
cellpar =[9.25 , 9.25, 9.22, 90, 90, 120])
The resulting structure is shown in Figure 1. The Spacegroup object can be used to investigate
symmetry-related properties of the structure, like whether beryl is centrosymmetric;
>>> from ase.spacegroup
Spacegroup
>>> sg = Spacegroup (192)
>>> sg.centrosymmetric
or to ﬁnd its non-equivalent scaled atomic positions:
>>> sg.unique_sites(beryl. get_scaled_positions ())
array ([[ 0.33333333 ,
0.66666667 ,
§Beryl is a naturally occurring mineral with chemical composition Be3Al2(SiO3)6 and a hexagonal crystal
structure with 58 atoms in the unit cell.
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
(a) Platinum surface with 2 N2 adsorbed at top sites , (b) carbon nanoribbon with
H-saturated edge, and (c) cuboctahedral gold nanoparticle constructed using various functions
Surfaces and interfaces
Surfaces are generated by cleaving a bulk material along one of its crystallographic planes. The
functions ase.build.surface () and ase.build.cut() can create arbitrary surface slabs.
ASE also has specialized functions to generate many of the common metal surfaces, such as
FCC(111) and HCP(0001). Slabs of diﬀerent sizes and thicknesses can be deﬁned using this
tool. For periodic slab models, the vacuum between the slab and the periodic images can also
be deﬁned. Molecules can be placed as adsorbates in predeﬁned binding sites, such as top, bridge
and hollow, as shown in Figure 2(a) where an N2 molecule is adsorbed on a Pt(111) surface:
# FCC (111)
surface of platinum
absorbed N2
from ase.build
import fcc111 , add_adsorbate , molecule
slab = fcc111(’Pt’, size =(4, 4, 4), a=4.0, vacuum =6.0)
add_adsorbate (slab , molecule(’N2’), height =3.0, position=’ontop ’)
add_adsorbate (slab , molecule(’N2’), height =3.0, offset =(2, 2),
position=’ontop ’)
Both utilizing the GUI and via scripting, single metal nanoparticles can be constructed using
the Wulﬀconstruction method.
# Graphene
nanoribbon
from ase.build
graphene_nanoribbon
ribbon = graphene_nanoribbon (2, 2, type=’armchair ’, saturated=True)
cuboctahedral
nanoparticle
from ase.cluster
Octahedron
cluster = Octahedron(’Au’, 5, cutoff =2)
More complicated surfaces and interfaces can be made by combining existing structures or by
combining the structure generators with other ASE functions. Here, an FeO ﬁlm on a Pt(111)
surface is constructed by combining two slabs (Figure 3(a)):
# FeO film on Pt (111)
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
ﬁle can contain one or more Atoms objects, but all of them must have the same number and
kind of atoms in the same order. The json ﬁle format can also contain more that one Atoms
object, but there are no restrictions on what it can contain (see also Section 4.1). Reading and
writing is done with the read () and write () functions from the ase.io module. The latter
can also be used to create images (png, svg, eps and pdf). It can also use POVRAY to
render the output, like the core/shell nanoparticle on Figure 3(b).
ASE has a database module (ase.db) that can be used for storing and retrieving atoms and
associated data in a compact and convenient way. A database for atomic conﬁgurations is ideal
for keeping systematic track of many related calculations. This will for example be the situation
in computational screening studies or when working with genetic search algorithms. Every row
in the database contains all the information stored in the atoms object and in addition key–value
pairs for storing extra quantities and for searching in the database.
Imagine a screening study looking for materials with a large density of states at the Fermi level.
Storing the results in a database could then look like this:
from ase.db import
connection = connect(’dos.db’)
for atoms in ...:
# Do calculation
dos = get_dos_at_fermi_level (...)
connection.write(atoms , dos=dos)
Here we have added a special dos column to our database, and we can now use the dos.db ﬁle
for analysis with either the ase.db Python module (connection.select(’dos >0.3 ’)) or
the ase-db command-line tool:
$ ase -db dos.db "dos >0.3"
The ase-db tool can also start a local web server so that one can query the database using a
web browser (see example in Figure 4). By clicking on a row of the table shown in the web
browser, one can see all the details for that row and also download the structure and data for
that row. There are currently three database backends:
JSON Simple human-readable text ﬁle.
SQLite3 Self-contained, serverless, zero-conﬁguration database.
SQLite3 is built into the
Python interpreter and the data is stored in a single ﬁle.
PostgreSQL Server-based database.
The JSON and SQLite3 backends work “out of the box”, whereas PostgreSQL requires a server.
Checkpointing
ASE includes a checkpointing module (ase.calculators.checkpoint) that adds powerful
generic restart and rollback capabilities to scripts. It stores the current state of the simulation
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
Figure 4: Showing the ﬁrst 10 rows of the query xc=PBE,O=0 (xc key must be PBE and no
oxygen atoms) sorted after formation energy, hform.
and its history using the ASE database discussed in Section 4.1.
This replaces the manual
checking for previously written output ﬁles common to many scripts. The Checkpoint class
takes care of storing and retrieving information from the database. This information always
includes an Atoms object, and it can also include attached information on the internal state
of the calculation.
Code blocks can be wrapped into checkpointed regions using try/except
statements, with the code block in the except statement being executed only if it was not
completed in a previous run of the script. This allows one to quickly replay the script from
cached snapshots up to the point where the script terminated in a previous run. The module
also provides a CheckpointCalculator class which provides a shorthand for wrapping every
single energy/force evaluation in a checkpointed region by wrapping the actual calculator so
that calls to compute the potential energy or forces only carry out the calculation the ﬁrst time
the script is invoked. This is useful when each energy evaluation is slow (e.g. DFT), particularly
when the overall runtime of the script is likely to exceed wall times typically available from the
queueing system. Checkpointing capabilities therefore enable complex monolithic and reusable
scripts whose execution spans a few or many runs on a high-performance computing system. In
combination with a job management framework, this opens the possibility to encode and deploy
robust automatic simulation workﬂows in ASE Python scripts, e.g. for combinatorial screening
of material properties.
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
Calculators
The calculator constitutes a central object in ASE and allows one to calculate various physical
quantities from an Atoms object. The properties that can be extracted from a given Atoms
object depend crucially on the nature of the calculator attached to the atoms. For example, a
DFT calculator may return properties such as the electronic density and Kohn–Sham eigenvalues,
which are inaccessible with calculators based on classical interatomic potentials.
Energy and forces
An important method common to every ASE calculator is get_potential_energy (), which
returns the potential energy of a given atomic conﬁguration. In a quantum mechanical treatment
of the electrons, this is the adiabatic ground state energy of the electronic system. Applying the
method to two diﬀerent atomic conﬁgurations will thus give the diﬀerence in energy between
the two conﬁgurations.
A useful application of the method is illustrated by the equation of state module exempliﬁed by
the script below. The potential energy of fcc Al is calculated at various cell volumes and ﬁtted
using the stabilized jellium model . The ﬁt is shown in Figure 5. This method provides a
convenient way of obtaining lattice constants for solids.
from ase.eos import
EquationOfState
from ase.build
from ase.calculators.emt import EMT
al = bulk(’Al’, crystalstructure =’fcc’, a=4.0)
calc = GPAW(mode=’pw’, kpts =(4, 4
al.calc = calc
cell = al.get_cell ()
volumes = []
energies = []
for x in [0.9, 0.95, 1.0, 1.05, 1.1]:
al.set_cell(x * cell)
volumes.append(al.get_volume ())
energies.append(al. get_potential_energy ())
eos = EquationOfState (volumes , energies)
v0 , e0 , B = eos.fit()
eos.plot(’eos_Al.pdf’)
Another universal method carried by all calculators is get_forces (), which returns the forces
on all atoms. The method is applied extensively when performing dynamics or structure optimization as described in Section 6.
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
The main advantage of this scheme is its simplicity. It interacts with the simulation code in the
same way as a user would. Hence, it does not require any changes to the simulation code itself.
The big disadvantage of this scheme is the high I/O costs. When there are many consecutive
invocations, a restart wave-function or electron density might have to be loaded from a ﬁle. If
the simulation is MPI-parallelized, then the binary has to be accessed by each compute node
before execution. Just creating the MPI session can already take several seconds .
Some ﬁle-based calculators like Quantum Espresso or Jacapo mitigate the start-up costs by
keeping the simulation process alive across multiple invocations. The next calculation is triggered
by writing a new input ﬁle, which the code automatically runs.
A way to avoid ﬁle I/O completely is to communicate via pipes. Such a scheme was recently
implemented by the CP2K calculator . For this, the CP2K distribution comes with a
small helper program called CP2K-shell. It provides a simple interactive command line interface
with a well deﬁned, parseable syntax. When invoked, the CP2K calculator calls popen to
launch the CP2K-shell as a sub-process and attaches pipes to its stdin and stdout ﬁle handles.
This even works together with MPI, because the majority of MPI-implementations forward the
stdin/stdout of the mpiexec process to the rank-0 process by default. The CP2K calculator
also allows for multiple CP2K instances to be controlled from within the same Python process
by instantiating multiple calculator objects simultaneously.
Parallel computing
Scientiﬁc computing is today usually done on computers with some kind of parallelism, either
multiple CPU cores in a single computer, or clusters of computers often with multiple cores
each. In the typical atomic-scale simulation performed with ASE, the performance bottleneck
is almost always the calculation of forces and energies by the calculator. For this reason, ASE
supports three diﬀerent modes of calculator parallelization.
In the simplest case, a single process on a single core runs ASE, but whenever control is passed to
the calculator, the calculation runs in parallel. This is the natural model whenever the interface
to the calculator is ﬁle based: ASE writes the input ﬁles, starts the parallel calculation, and
harvests the result.
Another model, for example used by the GPAW calculator, is to have ASE running on each
CPU core with identical data. In this case Python itself is started in parallel e.g. by the mpiexec
tool. This is only used with calculators having a native Python interface written for ASE. One
has to be careful that all Python processes remain synchronized and with identical data. In this
way, the data from ASE is already present in the Python process on all cores, and any necessary
communication during the calculation is done by the calculator. Some care must be taken in
the user’s script when this model is used. First, data associated with the atoms must remain
identical on all processes. This is particularly an issue if random numbers are used, for example
in Monte Carlo simulations or Molecular Dynamics with the Langevin algorithm, where the
random numbers must be generated either by a deterministic pseudorandom number generator,
or on a single core and distributed to the rest. In most ASE modules using random numbers,
this is done automatically. Second, care must be taken when writing output ﬁles. If more than
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
one process writes to the same ﬁle, corruption is likely, in particular on network ﬁle systems.
Printing from just one process may be dangerous, since asking the atoms for any quantity
involving the calculator must be done on all processes to avoid a deadlock. ASE solves some of
these issues transparently by providing helper functions such as ase.parallel.paropen. This
function opens a ﬁle as normal on the master process, whereas data written by other processes
is discarded. Since the ASE data is not distributed, this is suﬃcient for any normal output and
does not require the user to think about parallelism.
For very large molecular dynamics simulations (millions of atoms), ASE is able to run in a
distributed atoms mode. Currently, only the Asap calculator is able to run in this mode, and
it needs to extend some modules normally provided by ASE. In this mode, the atoms are
distributed among the processes according to their position in space (domain decomposition),
each Python process thus only sees a fraction of the total number of atoms. If atoms move,
they need to be transferred between processes; for performance reasons this is the responsibility
of the calculator.
When atoms thus migrate between processes, the number of atoms and
their identities change in each Python process. Any module that stores data about the atoms
internally, for example energy optimizers and molecular dynamics objects, will have their internal
data invalidated when this happens. For that reason, Asap needs to provide specialized versions
of such objects that delegate storage of internal data to the Atoms object. In the Atoms object,
all data is stored in a single dictionary, and the calculator then migrates all data from this
dictionary when atoms are transferred between processors.
When atomic conﬁgurations are written from a massively parallel molecular dynamics simulation, all information is normally gathered in the master process before being written to disk
using one of ASE’s supported ﬁle formats. In the most extreme simulations, gathering all data
on the master process may be problematic (e.g. due to memory constraints). ASE supports a
special ﬁle format for handling this case: the BundleTrajectory. The BundleTrajectory
is not a ﬁle but a folder, with each atomic conﬁguration in its own subfolder, and each quantity
in one or more ﬁles. Normally, data would be written by a single process, and each quantity is
written as an array into a single ﬁle, but in massively parallel simulations it is possible to have
each process write its own data into separate ﬁles. ASE then assembles the arrays when the
data is read again.
Dynamics and optimization
One is usually not only interested in static atomic structures, but also wants to study their
movement under internal and external inﬂuences. ASE provides multiple algorithms for structure
manipulation that can be used together with the calculator interfaces as was shown in the code
example in Section 2.1. The features supported by ASE and discussed in the following sections
are: molecular dynamics with diﬀerent thermodynamic controls, searching for local and global
energy minima, or minimum-energy paths or transition states of chemical reactions. ASE further
allows these types of structure manipulations to be restricted by complex constraints.
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
Molecular dynamics
The general idea of molecular dynamics (MD) is to numerically solve Newton’s second law for
all the atoms, thus generating a time-series from an initial conﬁguration. The purpose of the
molecular dynamics simulation may be to investigate the dynamics of a speciﬁc process, or to
generate an ensemble of conﬁgurations in order to calculate a thermodynamic property. Many
MD algorithms have been developed for related but slightly diﬀerent purposes (see e.g. Ref. ).
This is reﬂected in the ASE code which supports a number of the more popular algorithms.
As Newton’s second law preserves the total energy of an isolated system, so will any algorithm
that integrates this equation of motion without modiﬁcation: the simulation will produce a
microcanonical (or NV E) ensemble with well-deﬁned particle number, volume and total energy.
One of the most popular such algorithms is velocity Verlet. In ASE this is implemented as a
dynamics object:
import ase.units
from ase.md.verlet
VelocityVerlet
dyn = VelocityVerlet (atoms , timestep =5 * ase.units.fs)
dyn.run (1000)
# Run 1000 time
A dynamics object shares many of the properties of an optimization object; it is possible, for
example, to attach functions that are called at each time step, or after each N time steps.
Useful objects to attach this way include Trajectories for storing the atomic conﬁguration and
MDLogger, which writes a log ﬁle with energies and temperatures.
Temperature control
Often, a constant-energy simulation is not what is desired, as the real system being modelled by
the simulation is thermally coupled to its surroundings, and thus has a well-deﬁned temperature.
It is therefore necessary to be able to do simulations in the NV T ensemble without having to
describe the coupling to the surroundings in details. ASE implements three diﬀerent algorithms
for constant-temperature MD: Berendsen dynamics, Langevin dynamics and Nos´e–Hoover dynamics.
Berendsen dynamics is conceptually the simplest:
at each time step the velocities
are rescaled such that the kinetic energy approaches the desired value over a characteristic
time chosen by the user.
While simple, this algorithm suﬀers from the problem that the
magnitude of thermal ﬂuctuations in the kinetic energy is not reproduced correctly, although
this is mainly a problem for very small systems.
Berendsen dynamics can be found in the
ase.md.nvtberendsen module.
Langevin dynamics adds a small friction and a ﬂuctuating force to Newtons second law.
While originally invented to simulate Brownian motion, it can be argued to be a physically
reasonable approximation to the interactions with the electron gas in a metal. The main advantages of Langevin dynamics are that it is very stable and that the thermostat is local: if kinetic
energy is produced in one part of the system, there is no risk that other parts cool down to
compensate, as can be the case with other thermostats. A possible drawback is that Langevin
dynamics is stochastic in nature, thus restarting a simulation will result in a slightly diﬀerent
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
trajectory. Langevin dynamics is implemented in the ase.md.langevin module.
Nos´e–Hoover dynamics adds a single degree of freedom representing the heat bath;
this degree of freedom is coupled to the velocities of the atoms through a rescaling factor. This
method is very popular in the statistical mechanics community because it can be rigorously
derived from a Hamiltonian. One major drawback of this method is that with only a single
degree of freedom to describe the heat bath, oscillations may appear in this variable and thus in
the temperature. While Nos´e–Hoover dynamics is good at maintaining prescribed temperature,
it is therefore less suitable to establish a speciﬁc temperature in the simulation. This problem can
be addressed by introducing more auxiliary variables, the so-called Nos´e–Hoover chain, but this
is not implemented in ASE. Nos´e–Hoover dynamics is implemented together with Parrinello–
Rahman dynamics in the ase.md.npt module.
Pressure control
In addition to keeping temperature constant, it is often desirable to keep pressure (or the stress
for solids) constant, leading to the isothermal-isobaric (NpT) ensemble.
ASE provides two
algorithms for NpT dynamics: Berendsen and Nos´e–Hoover–Parrinello–Rahman.
Berendsen dynamics allows for rescaling the simulation volume in addition to the kinetic
energy, leading to the conceptually simplest implementation of NpT dynamics. This algorithm
is implemented in the ase.md.nptberendsen module.
Nos´e–Hoover–Parrinello–Rahman dynamics is a combination of Nos´e–Hoover temperature control and Parrinello–Rahman pressure/stress control . ASE implements the algorithm set forth by Melchionna . As is the case for Nos´e–Hoover dynamics, there is
the possibility of oscillations in the auxiliary variables controlling both the temperature and the
pressure, and the algorithm is more suitable for maintaining a given temperature and pressure
than for approaching it. The ASE implementation allows for varying only the volume of the
simulation box (suitable for constant-pressure simulations of e.g. liquids), and for varying both
shape and volume of the box, possibly constraining the simulation box to remain orthogonal. In
addition, constant strain rate simulations are possible where a dimension of the computational
box is kept unaﬀected by the dynamics, but is assigned a constant rate of change.
implemented in the ase.md.npt module.
Local structure optimizations
Local structure optimization algorithms start from an initial guess for the atomic positions and
(mostly) use the forces acting on the atoms to ﬁnd structures of lower energy in an iterative
procedure until a given convergence criterion is reached. The methods available in ASE, in
ase.optimize, are described below.
BFGS (Broyden–Fletcher–Goldfarb–Shanno) . This algorithm chooses each step from
the current atomic forces and an approximation of the Hessian matrix, i.e. the matrix of second
derivatives of the energy with respect to the atomic positions (see Section 8.1). The Hessian is
established from an initial guess which is gradually improved as more forces are evaluated.
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
L-BFGS is a low-memory version of the BFGS algorithm . The full Hessian matrix
has O(N2) elements, making BFGS very expensive for force ﬁeld calculations with millions of
L-BFGS represents it implicitly as a series of up to n evaluated force vectors for a
linear-scaling memory requirement of O(nN).
MDMin is an energy minimization algorithm based on a molecular dynamics simulation. From
the initial positions, the atoms accelerate according to the forces acting on them. The algorithm
monitors the scalar product F·p of the force and momentum vectors. Whenever it is negative, the
atoms have started moving in an unfavourable direction, and the momentum is set to zero. The
simulation continues with whatever energy remains in the system. An advantage of MDMin
is that it is inspired by an intuitive physical process, but the algorithm does not converge
quadratically like those based on Newton’s method, and is therefore not eﬃcient when close to
the minimum.
FIRE (fast inertial relaxation engine ) likewise formulates an optimization through molecular dynamics. An artiﬁcial force term is added which “steers” the atoms gradually towards the
direction of steepest descent. FIRE uses a dynamic time step and other parameters to control
the simulation. Again, if at some point the atoms would move against the forces, the velocities
are set to zero and the dynamic parameters are reset. The FIRE algorithm often requires more
iterations than BFGS as implemented in ASE, but the atoms move in smaller steps which can
decrease the cost of a single self-consistent iteration.
SciOpt. ASE can use the optimization algorithms provided with SciPy for structure optimizations as well. However most of these general optimization algorithms are not as eﬃcient as those
designed speciﬁcally for atomistic problems.
Preconditioners can speed up optimization approaches by incorporating information about
the local bonding topology into a redeﬁned metric through a coordinate transformation. Preconditioners are problem dependent, but the general-purpose implementation in ASE provides a
basis that can be adapted to achieve optimized performance for speciﬁc applications . While
the approach is general, the implementation is speciﬁc to a given optimizer: currently L-BFGS
and FIRE can be preconditioned.
Tests with a variety of solid-state systems using both DFT and classical interatomic potentials
driven though ASE calculators show speedup factors of up to an order of magnitude for preconditioned L-BFGS over standard L-BFGS, and the gain grows with system size. Precomputations
are performed to automatically estimate all parameters required. A line search based on enforcing only the ﬁrst Wolﬀcondition (i.e. the Armijo suﬃcient descent condition) is also provided;
this typically leads to a further speed up when used in conjunction with the preconditioner.
The preconditioned L-BFGS method implemented in ASE does not require external dependencies, but the scipy.sparse module can be used for eﬃcient sparse linear algebra, and the
matscipy package is used for fast computation of neighbour lists if available. PyAMG can be used
to eﬃciently invert the preconditioner using an adaptive multigrid method.
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
Constraints
When performing optimization or molecular dynamics simulations one often wants to constrain
the movement of the atoms. For example, it is common to ﬁx the lower layers of a “slab”-type
adsorbate–surface model to the bulk lattice coordinates. This can be achieved by attaching the
FixAtoms constraint to the atoms.
A number of built-in constraints are available in ASE. The user can easily combine these constraints or — if required — build their own constraints. The built-in constraints include:
• Fix atoms. Fixes the Cartesian positions of speciﬁed atoms.
• Fix bond length. Fixes a bond length between two atoms, while allowing the atoms to
otherwise move freely.
• Fixed-line, -plane, and -mode movement. An atom can be constrained to move only along
a speciﬁed line or within a speciﬁed plane; or, in ﬁxed-mode, a system can be constrained
to move along a speciﬁed mode only. An example of ﬁxed-mode could be a vibrational
• Preserving molecular identity.
This constraint applies a restoring force if the distance
between two atoms exceeds a certain threshold. In this way molecules can be prevented
from dissociating. This class can also apply a restoring force to prevent an atom from
moving too far away from a speciﬁed point or plane. The constraint was designed to work
with techniques such as minima hopping in order to explore the potential energy surface
while enforcing molecular identity .
• Constraining internal coordinates.
Any number of bond lengths, angles, and dihedral
angles can be constrained to ﬁx the internal structure of a system.
An alternative to constraints is to use a “ﬁlter”, which works as a wrapper around the Atoms
object when it is used with a dynamics method (optimization, molecular dynamics etc.). In
other words, the dynamics sees only the degrees of freedom that the ﬁlter provides and not all
the atomic coordinates. The ﬁlter can thus hide certain degrees of freedom or make combinations
of them. A number of ﬁlters are built into ASE, and again the user is free to build their own.
The built-in methods include the following:
• Masking atoms. One can use a basic ﬁlter to ﬁx the positions of speciﬁed atoms; this
works by presenting only the positions, forces, momenta, etc., on the free atoms when the
corresponding attributes are accessed. In particular for large-scale simulations, this can
have the advantage of reducing the size of the Hessian matrix.
• Optimizing unit cell vectors. A ﬁlter can present the stresses on the unit cell along with
the forces; this can be used to optimize the unit cell lattice vectors either simultaneously
or independently from the atomic positions. These ﬁlters also present the strain of the
unit cell as part of the positions attribute.
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
Transition states from the nudged elastic band method
Locating saddle points in a complex energy landscape is a common task in atomic simulations.
For example, ﬁnding the saddle point is required to determine the energy barrier for diﬀusion
of an adsorbate across a surface for a chemical reaction event (bond breaking or formation).
To locate saddle points within the harmonic approximation, ASE oﬀers the nudged elastic band
method and the dimer method.
The nudged elastic band (NEB) method as formulated by Henkelman and J´onsson 
works by creating a series of Atoms objects between two local minima. These Atoms objects,
images, are then relaxed in order to determine the lowest-energy pathway. So-called “springs”
are inserted between adjacent images in order to apply a restoring force on each image which
prevents them from relaxing into each other and the starting or ending minima. At the same
time the component of the force from the energy landscape that is parallel to the band is
removed resulting in the nudged elastic band. The force then have two components; one from
the energy landscape perpendicular to the band converging the band towards a minimum energy
path (MEP) and the spring force that secures the images are equally spaced.
The NEB method is accessed by importing NEB from the ase.neb module.
NEB accepts as
input a series of initial images with attached calculators. The initial images can be acquired e.g.
from interpolation of an initial and a ﬁnal state between which the pathway is desired, or from
a previous pathway relaxed with another energy descriptor. After the NEB object is created,
it is handed to the chosen optimizer and the relaxation of the pathway is initialized. The end
result is a series of images describing the lowest-energy pathway.
In ASE, the NEB method is implemented on top of the a normal relaxation scheme. For each
image, the assigned optimizer determines the forces on each atom, and these calculated forces
are then modiﬁed by the NEB method. Thus, before the atoms are moved, the restoring forces
are applied between each image to maintain the pathway.
The following is an example of a gold atom diﬀusing on top of an aluminium (001) surface. The
upper panel of Figure 6 shows the atom conﬁguration. First, the script initializes the initial and
ﬁnal images of the gold atom placed into two neighbouring hollow sites. It then relaxes these
two images, which will serve as end-points for the NEB path. Next, the intermediate images
are initialized so that they linearly interpolate the initial and the ﬁnal state. This is done by
creating several copies of the atoms and passing them to neb.interpolate ().
from ase.calculators.emt import EMT
from ase.neb import NEB
from ase.optimize
from ase.io import
from ase.build
import fcc100 , add_adsorbate
from ase.constraints
# 2x2 -Al (001)
with 3 layers and an
adsorbed in a hollow
initial = fcc100(’Al’, size =(2, 2, 3))
add_adsorbate (initial , ’Au’, 1.7, ’hollow ’)
initial.center(axis=2, vacuum =4.0)
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
write(’output.traj ’, images)
The above script produces an output ﬁle containing the relaxed pathway used to produce Figure 6.
Finding the true saddle point for more complex pathways is not trivial. The best initial guess
for the reaction path may not be a linear interpolation between initial and ﬁnal image, but
instead be related by rotations of subgroups of atoms. An improved preliminary guess for the
minimum energy path can be generated using images on the image dependent pair potential
(IDPP) surface . Optimization of all atom pair distances toward an interpolation of all atom
pair distances for all intermediate images along the path results in an initial path much closer
to the MEP.
Once a good approximate reaction pathway has been determined, the climbing-image extension
of the NEB module can be invoked to converge the highest-energy image to the nearest saddle
point. The method works by omitting spring forces on the highest energy image and inverting
the force it experiences parallel to the path. The climbing image is still allowed to relax down the
energy landscape perpendicularly to the lowest-energy path like all other intermediate images.
Because the additional freedom of the climbing image makes this calculation computationally
more expensive, it is advised that this is only done when a good guess of the saddle point is
available.
Additional NEB extensions are available in the module. For instance, the full spring force is
omitted by default and only the spring force parallel to the local tangent is used together with
the true force (as evaluated by the calculator) perpendicular to the local tangent. A full list of
capabilities is available on the ASE website.
The standard NEB algorithm distributes the assigned computational resources equally to all the
images along the designated path. This implementation results in an inﬂexible and potentially
ineﬃcient allocation of resources, given that each image has a diﬀerent level of importance
towards ﬁnding the saddle point. A dynamic resource allocation approach is possible through
the AutoNEB method in ase.autoneb. AutoNEB uses a simple strategy to add images
dynamically during the optimization.
AutoNEB ﬁrst converges a rough reaction path with a few images using standard NEB. Once
converged, an image is added either where the gap between the current images is largest, or
where the energy slope is greatest. The reaction path is reﬁned by iteratively adding images
and reconverging the pathway.
The virtue of the AutoNEB method is that it is possible to deﬁne a total number of internal
images which is greater than the number of images to simultaneously participate in the optimisation. Some images will then be moving while others are frozen. Whenever an image is added,
the moving images will be those closest to the most recently added one. This feature allows
for computational resources to always be focused on the most important region of the pathway.
The method has been utilized for a number of examples with benchmarking cases with
50–70% reduced computational cost relative to the standard algorithm .
For systems with no ﬁxed atom positions and/or periodic boundary conditions, overall rota-
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
tion and translation are external changes with no internal structural changes. For normal NEB
calculations involving nanoparticles, external structural changes can pose problems for the convergence to the minimum energy path. The system will seek to avoid high energy areas and
hence rotate and/or translate away from these, which is not possible to the same extent for
a constrained system. The NEB module supports the NEB-TR method, which speeds up
convergence for such systems by removing rotation and translation movement.
Transition states from the dimer method
Like the NEB method above, the dimer method is used to ﬁnd saddle points in multidimensional
potential energy landscapes. Unlike NEB, the dimer method only requires a starting basin and
is useful for ﬁnding most or all relevant pathways out of that basin.
The dimer method is a minimum mode following method which estimates the eigenmode corresponding to the lowest eigenvalue of the Hessian (minimum mode) and traverses the potential
energy surface (PES) uphill in that direction. Eventually, it reaches a ﬁrst order saddle point
with exactly one negative eigenvalue.
The dimer method can be split into three independent phases.
1. Estimating the minimum mode.
2. Inverting the gradient along the minimum mode to make ﬁrst order saddle points appear
as minima.
3. Move the system according to the inverted gradient.
Only the ﬁrst of these phases is unique to the dimer algorithm. Other methods estimate the
minimum mode diﬀerently. The dimer method is implemented in ASE in such a way that it
should be straightforward to include other minimum mode estimators.
To ﬁnd a saddle point, the system is initially located at an energy minimum and randomly
displaced . The displacement achieves two things: ﬁrst, it moves the system away from a
zero gradient location (the minimum), and secondly, it can be used as the seed to sample as
many saddle points as possible.
The dimer method identiﬁes the minimum mode by making an estimate of the curvature of
the PES along a given unit vector, ˆs, and then iteratively rotates ˆs until it reaches an energy
minimum. This energy minimum represents the lowest curvature.
The name of the dimer method is derived from the way that ˆs is deﬁned. Two images are chosen:
one with the input system coordinates and the other displaced along ˆs by a distance of ∆D.
The gradients at each image are then used to estimate the 2nd derivative of the PES using ﬁnite
diﬀerence. The force components perpendicular to ˆs are used to determine the torque which
rotates the dimer to obtain lower energy, iteratively reaching the estimate of the minimum mode.
The dimer method is implemented in ASE through the DimerAtoms class, which extends the
Atoms class with information relevant to the dimer method, such as the minimum mode estimate
and the curvature. The system can initially be displaced from the energy minimum conﬁguration
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
by a predeﬁned vector, by selecting certain atoms to be randomly displaced or by deﬁning a
sphere in which all atoms are randomly displaced.
A default dimer calculation can be set up by creating a DimerAtoms object from an Atoms object with a calculator attached. The parameters controlling the calculation can either be passed
directly to the DimerAtoms object when created or can be speciﬁed using a DimerControl
Multiple control parameters are deﬁned in the DimerControl but the most important ones
have to do with ∆D and the amount of rotations allowed before performing an optimization
• Rotational force thresholds to deﬁne the conditions under which no more rotations will be
performed before performing an optimization step.
• The maximum number of rotations to be performed before making an optimization step.
• ∆D, the separation of the dimer’s images. In order for an accurate ﬁnite diﬀerence estimate
of the 2nd derivative, this should be kept small, but still large enough to avoid eﬀects of
noise in the force calculations.
Global optimization
Finding the atomic conﬁguration with the lowest possible energy is much more challenging than
ﬁnding just a local minimum. The number of local minima grows at least exponentially with the
number of atoms (the existence of about 106 local minima for a 147-atom Lennard-Jones cluster
has been suggested ), and ﬁnding the global minimum is therefore a daunting task. One of
the main challenges for global optimization is that diﬀerent local minima might be separated by
high energy barriers which much be overcome in order to move between local minima.
One common approach to this problem is simulated annealing, in which the atomic system is
initially equilibrated at a certain temperature using, for example, molecular dynamics. After
this, the temperature is decreased slowly to identify low energy conﬁgurations. However, this
method is not always eﬃcient and a number of alternative global optimization techniques have
been developed.
ASE provides three methods for global optimization: basin hopping, minima hopping and genetic
algorithms.
Basin hopping
In the basin hopping method , the atoms perform a random walk. Every structure visited
is relaxed with a local optimization method, and the unrelaxed structure is then assigned the
energy of the obtained minimum. Thus all structures within the same basin are considered to
have the same energy. In this way the barriers between close-lying local minima are eﬀectively
removed, and the PES becomes a step-function with the energies deﬁned by the local minima
(see the illustrative Figure 2 in Ref. ). The modiﬁed PES is then explored by Monte Carlo at
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
an adjustable temperature: Going from one basin to another is accepted at random depending
on how favourable its energy is.
Minima hopping
A more automated approach is minima hopping , in which one uses alternating MD and
local minimization steps in combination with self-adjusting parameters to explore the potential
energy surface. Brieﬂy, a MD step at a speciﬁed initial temperature is used to randomly “shoot”
the structure out of a local minimum region in the PES; after the MD trajectory encounters a
speciﬁed number of path minima, the structure is optimized to the next local minimum. If the
minimum found is identical to any previously-found minimum, the MD temperature is increased
and the step is repeated. Otherwise the algorithm ﬁrst lowers the search temperature and then
decides to accept the step if the new local minimum is no higher than a speciﬁed energy diﬀerence
above the previous step. If the found local minimum is accepted, it is added to a list of found
minima and the energy diﬀerence threshold is decreased. If it is rejected, the energy diﬀerence
threshold is increased. In this way, a list of local minima is generated while two parameters—
the search temperature and the acceptance criterion—are automatically adjusted in response
to the success of the algorithm. The ASE implementation of minima hopping allows the user
to easily customize the key features of the algorithm, such as the local optimizer employed or
the molecular dynamics scheme. It is also possible to perform parallel searches which share the
list of found minima. The minima hopping scheme can also be combined with constraints, for
example to prevent molecules from dissociating .
Genetic Algorithms
In addition to the global optimization schemes described in Sections 7.1 and 7.2, ASE also
contains ase.ga, a genetic algorithm (GA) module. A GA takes a Darwinistic approach
to optimization by maintaining a population of solution candidates to a problem (e.g. what is the
most stable four component alloy ?). The population is evolved to obtain better solutions
by mating and mutating selected candidates and putting the ﬁttest oﬀspring in the population.
The ﬁtness of a candidate is a function which, for example, measures the stability or performance
of a candidate. Natural selection is used to keep a constant population size by removing the
least ﬁt candidates. Mating or crossover combine candidates to create oﬀspring with parts from
more candidates present, when favorable traits are combined and passed on the population is
evolved. Only performing crossover operations risks stagnating the evolution due to a lack of
diversity – performing crossover on very similar candidates is unlikely to create progress when
performed repeatedly. Mutation induces diversity in the population and thus prevents premature
convergence.
GAs are generally applicable to areas where traditional optimization by derivative methods are
unsuited and a brute force approach is computationally infeasible. Furthermore, the output
of a GA run will be a selection of optimized candidates, which often will be preferred over
only getting the best candidate, especially taking into account the potential inaccuracy of the
employed methods. Thus a GA ﬁnds many applications within atomic simulations, and will
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
often be one of the best methods for searching large phase spaces. We will present a couple of
usage cases in Section 7.3.1.
Usage examples of the GA implementation in ASE
The ase.ga implementation has been used to determine the most stable atomic distribution
in alloy nanoparticle catalysts . For this purpose, speciﬁc permutation operators were
implemented promoting the search for diﬀerent possible atomic distributions within particles,
i.e. core/shell, mixed or segregated. For example the operator promoting core/shell particles
permutes two atoms in the central and surface region respectively, a mixed (segregated) particle
is promoted by permuting two atoms each in local environments with a high (low) density of
identical atoms. These operators, if used dynamically, greatly improved the convergence speed
of the algorithm.
The most stable Au6–12 clusters on a TiO2 support were also determined using ase.ga . The
approach, inspired by Deaven and Ho , implemented the cut-and-splice operator and utilized
the ﬂexibility of ASE to perform local optimization with increasing levels of theory. This led to
the discovery of a new global minimum structure. The GA implementation was benchmarked
for small clusters and described in greater detail in .
Bulk systems are also readily treated in ase.ga; for example, ase.ga was used in a search
for ammonia storage materials with high storage capacities and optimal ammonia release characteristics. The best candidates, some with a record high accessible hydrogen capacity, were
subsequently experimentally veriﬁed . Operators utilizing chemical trends were implemented, e.g. mutating an atom to a neighboring element in the periodic table (up, down, right
or left). This approach is readily transferable to other screening studies where the phase space
is too great for a full computational screening.
GA implementation in ASE
The implementation requires some insight from the user on how the algorithm should treat the
problem, e.g. which operators are appropriate or when is convergence achieved. The only way to
get the algorithm running optimally is by testing the use of diﬀerent functions and parameters
for the given search objective.
However, even without optimization of parameters, the GA
can perform reasonably well. In the following we shall discuss the diﬀerent components in a
GA calculation in ASE (also shown in the ﬂow chart Figure 7), and provide an example for
optimization of a small cluster on a gold surface. Other GA examples are available on the ASE
web-pages.
Start The initial population must be generated to start. A helper function, StartGenerator,
is supplied for atomic clusters. The initial population can be random (unbiased) or comprise seeded suggestions, e.g. candidates the user thinks will be good (biased). The subsequent steps are then repeated until some convergence criterion is reached.
Store Unevaluated candidates (initial or oﬀspring) are saved in the database.
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
StartGenerator
set_raw_score
Population
Population
and/or Mutation
get_two_candidates
Convergence
Figure 7: Flow of the genetic algorithm. Examples of functions or classes at each step are shown
Evaluate Candidates are typically evaluated in a separate script, allowing the search to progress
in parallel through a queueing system. The evaluation will typically involve a local geometry optimization. The main objective of the evaluation script is to calculate the raw score,
i.e. the ﬁtness of the candidate without taking the rest of the population into account.
Evaluated candidates and their raw scores are added to the database.
Population The population step determines how to calculate the ﬁtness from the raw score.
It is useful to compare candidates collectively and only keep the ﬁttest one of similar
candidates. This can also be achieved by penalizing the ﬁtness of similar candidates.
Stop The GA run is normally considered converged when no evolution of the ﬁttest candidates
takes place for a certain amount of time. It is also possible to set a hard limit for the
number of candidates that should be generated and conclude the search once this criterion
has been met.
Selection This step performs the selection among the current population, e.g. by the roulette
wheel method. This can function in two ways:
1. Generational where the current population is used as parents to form a new generation
of candidate oﬀspring. The new generation and current population are then combined
to form the new population. However, since the algorithm halts until the evaluation
of a full generation is ﬁnished, this does not parallelize optimally.
2. The population can be treated as a pool from which candidates are used as parents
and to which new candidates are added continually. This keeps the computational
load constant.
Operation The types of operators to use in a search are determined by the speciﬁc problem.
It is always desirable though to use both crossover and mutation, however not necessarily
in equal amounts – it is possible to give a certain weight to any operator, that can be
changed dynamically . It should be noted that, for a problem well suited to a GA, it
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
will be beneﬁcial to set a greater likelihood of crossover compared to mutation. The newly
created oﬀspring is added to the database and the whole cycle repeats.
If the user has a new idea, the easiest way to implement it is to modify a copy of one of the
existing classes.
Once the GA script runs, no action is required from the user, but it is beneﬁcial to check the
evolution by investigating the db ﬁle as described in section 4.1.
It is typical to run a GA
several times with identical or slightly diﬀerent initial conditions to verify the global minimum.
If allowed by the nature of the problem, it is common to utilize a central database to check
during the evaluation step if the candidate has been calculated in a previous run or in another
instance of the GA running in parallel.
Running a genetic algorithm
In the following we outline a complete GA search for maximally stable conﬁgurations of a
gold/silver overlayer on top of a 2×4×1 Au(111) slab The complete code for the example can
be found on the ASE web page.
First, the gold slab is created. A random initial population is then produced on top of the
slab (as displayed in Figure 8a) and added to the database using the two ase.ga objects
StartGenerator and PrepareDB. The start population is subsequently relaxed before the
GA run is initiated. Examples from an unrelaxed start population can be found in Figures 8b,
The next step is to set up the population, pairing scheme, and types of mutation before the
population is evolved. This is facilitated by helper classes for cut-and-splice pairing and permutation mutations. In this case, 50 new candidates are initialized either by mutation (30 %
permutation mutation in this case) or by a crossover pairing of two candidates from the current
population. New candidates are relaxed in turn while keeping the population updated with
the ﬁttest 20 unique candidates after each new child is added to the database. Setting up the
population and running the main loop can be done as shown in the code example below.
# Create the
population
n_to_optimize = len(atom_numbers)
comp = InteratomicDistanceComparator (n_top=n_to_optimize )
population = Population( data_connection =da , population_size =20,
comparator=comp)
pairing = CutAndSplicePairing (slab , n_to_optimize , blmin)
mutation = PermutationMutation (n_to_optimize )
# Test 50 new
candidates
for i in range (50):
parent1 , parent2 = population. get_two_candidates ()
# Check if we want to do a mutation
(30 % chance) or pair
candidates
if random () < 0.3:
child , desc = mutation. get_new_individual ([ parent1 ])
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
Molecular vibrations
We begin by expanding the potential energy function with respect to atomic positions, ui:
(ui −ui0) + 1
(ui −ui0)(uj −uj0) + · · ·
Here, indices i and j run over the three coordinate axes of N atoms and the subscript 0 indicates
the reference geometry. The ﬁrst term in the expansion accounts for an arbitrary shift of the
total potential energy, so we are free to set it equal to zero. Assuming the expansion is about
a stationary point on the potential energy surface (usually energy minima and occasionally
saddle points are of interest), the derivative in the linear term of the expansion is zero. Ignoring
higher-order terms, we may rewrite the expansion as
∆uiHij∆uj = 1
H is called the Hessian matrix and is deﬁned as
In the above expression, Fj denotes the force along the atomic coordinate axis j. H is constructed
from ﬁnite-diﬀerence ﬁrst derivatives of the forces obtained from an attached ASE calculator.
Each atom is displaced forwards and backwards along each coordinate axis, requiring 6N+1 total
force evaluations. Note that, by construction, H is symmetric. It follows that the eigenvalues of
H are real and its eigenvectors form a complete orthogonal basis. Writing Newton’s equations
of motion in terms of the Hessian yields
−H∆u = Md2∆u
which is solved by uk(t) = akexp(−iωkt). Plugging in this solution, the equation of motion
where M is a diagonal matrix of the atomic masses. This is then solved as a standard eigenvalue problem. The eigenvalues of the mass-weighted Hessian M−1/2HM−1/2 are the squared
vibrational frequencies and the eigenvectors are the mass-weighted normal modes M1/2uk.
Many calculators represent quantities such as the electron density on a real-space grid. This
gives rise to the so-called egg-box eﬀect: The forces on each atom vary artiﬁcially under translational movement of the grid relative to the atom, leading to inaccuracies in the Hessian. This
dilemma may be resolved for isolated systems by imposing momentum conservation such
i Hij = 0 for all j. In ASE, this condition is enforced by adjusting the diagonal elements
through Hjj = −P
i̸=j Hij, which preserves the symmetry of H.
The eﬀect is to replace the force on an atom when displaced from its equilibrium position with
minus the sum of the forces on all other atoms under the same displacement, averaging the
egg-box noise over the whole system.
In practice, the vibrations of a molecule described by an Atoms object is found by creating a
Vibrations object and running it:
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
from ase.vibrations
Vibrations
vib = Vibrations(atoms)
The Vibrations object can then be queried for information about the calculated vibrational
Spectral properties
Post-processing methods are available in ase.vibrations for calculating spectral properties
related to vibrations.
These methods use quantities that are calculated via ﬁnite diﬀerence
displacements. Infrared spectra can be obtained from changes in molecular dipoles that
are available from electronic structure calculators.
Raman spectra are more involved, as they require excited state properties that are not as
commonly provided by calculators as ground state properties. The ASE modules are intended
to be usable with diﬀerent caculators, but were tested so far with GPAW only. These modules
are a work in progress, but special topics like the evaluation of approximate Raman spectra 
and the calculation of Franck–Condon overlaps depending on Huang–Rhys factors derived from
excited state forces already exist.
Treatment of lattice dynamics in terms of phonons is essential for describing many properties of
crystalline solids including thermodynamic functions (see ase. thermochemistry), superconductivity, infrared and Raman absorption, phase transitions and sound propagation . The
ase.phonons module is used to calculate phonon frequencies, which can be used to construct
band structures and densities of states and visualize the time-dependent motion of the phonon
The calculation of phonon modes may be thought of as an extension of the vibrational analysis
described in Section 8.1 to a periodic lattice. The phonon modes are characterized by a wave
vector k, which speciﬁes the direction and spatial frequency of propagation.
Two well-established ab initio methodologies for performing phonon analyses are the ﬁnite displacement method, also called the direct method , and the linear response method .
ase.phonons uses the former, in which the primitive cell is repeated across its periodic boundary conditions to form a supercell. Ideally, the supercell should be large enough that no atom
interacts with any of its periodic images in neighboring cells. In ordinary metals, interatomic
forces decay rapidly. In polar materials, the dipole–dipole forces are longer in range. For reliable
results one should therefore always check any phonon-related observable for convergence with
respect to supercell size.
The matrix of force constants, H, is calculated by displacing the atom or atoms in one primitive
cell within the supercell along each Cartesian axis and calculating the force response on all other
atoms in the supercell. It is only necessary to perform the displacement on the atoms in this one
dynamic primitive cell because all other primitive cells in the crystal are related by translation of
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
The Phonons object can then be queried for information about the diﬀerent phonon modes.
Thermochemistry
A common task in atomistic simulations is to convert the output of electronic structure calculations, which represent calculations of single conﬁgurations on the potential energy surface
(PES), into thermodynamic properties, which are statistical functions of the energetics over an
ensemble of states. For example, optimization routines within ASE can ﬁnd a local minimum
on the PES; however, to convert this potential energy Epot into a thermodynamic quantity such
as the free energy, a statistical-mechanics treatment that utilizes the shape of the PES near
the local minimum is necessary, along with an appropriate model. Three standard cases are
implemented in ASE (within ase. thermochemistry) to convert PES information into thermodynamic quantities (ignoring electronic excitation contributions to the heat capacity); given
the modular nature of ASE, the user can readily expand upon these basic methods with customized methods as needed, for example to deal with anharmonicity or electronic contributions.
The three standard approaches are listed below.
• The harmonic limit.
In the simplest case, all 3N atomic degrees of freedom (DOFs)
are treated as separable and harmonic within the accessible temperature range; the vibrational energy levels ǫi corresponding to these motions can be produced from a normal-mode
analysis as described in Section 8.1. This is the same treatment as in the ideal-gas limit,
described below, but without translational or vibrational DOFs. A common example of
this limit is the examination of the thermodynamics of an adsorbed species on a catalytic
surface ; the 3N DOFs of the adsorbate are then assumed to be harmonic and independent of the modes in the surface. This allows the calculation of properties such as
internal energy U and entropy S at a speciﬁed temperature T with the Helmholtz energy
F = U −TS. These methods are available via the HarmonicThermo class. The internal
energy is taken to be
U(T) = Epot + ∆EZPE +
eǫi/kBT −1,
where the zero-point energy ∆EZPE has its usual deﬁnition of 1
i ǫi and kB is the Boltzmann constant. The entropy is found from
 eǫi/kBT −1
1 −e−ǫi/kBT #
• The ideal-gas limit. In the limit of an ideal gas, one assumes that the 3N atomic DOFs
can be treated independently and separated into translational, rotational, and vibrational
components . Three-dimensional gases have three translational DOFs. General
polyatomic gases have an additional three rotational DOFs; although linear molecules
have only two rotational DOFs and monotonic gases have none. The remaining DOFs are
vibrations which are treated harmonically in this limit. This allows for the calculation
of such properties as the enthalpy H, entropy S, and Gibbs free energy G. Aside from
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
ideal gases, this method can be used to estimate the properties of condensed species via
thermodynamic arguments. For example, the direct calculation of the free energy of liquid
water at 298 K from ﬁrst principles is daunting. However, one can calculate the free energy
of water vapor in equilibrium with liquid water (which, at a vapor pressure of 3.2 kPa is
well described by the ideal-gas approximation), and equate this to the free energy of liquid
water via the thermodynamic criterion for equilibrium. These methods are available in
the IdealGasThermo class. The enthalpy is calculated as
H(T) = Epot + ∆EZPE +
where the constant-pressure heat capacity is
CP = kB + CV ,trans + CV ,rot + CV ,vib.
The translational heat capacity term is 3
2kB, while the rotational term is 1
2kB per rotational
DOF. The integral of the vibrational heat capacity takes the same form as in (9), with the
sum taken over the vibrational DOFs. The entropy is found from
S(T, P) = S◦
trans + Srot + Svib + Selec −kB ln P
where the translational term is calculated at a reference pressure P ◦. The vibrational
component is the same as (10) whereas the remaining components are given below.
Strans = kB
if monatomic,
if linear,
 √πIAIBIC
if nonlinear,
Selec = kB ln [2 × (spin multiplicity) + 1] .
In the above, IA, IB, and IC are the principal moments of inertia of the molecule, which are
calculated in ASE from the atomic coordinates and masses and are available to the user
with the atoms. get_moments_of_inertia method. In the case of a linear molecule
there are two degenerate moments, I. σ is the molecule’s symmetry number and h is the
Planck constant. Finally, the Gibbs free energy is reported as G = H −TS.
• Crystalline solids. The vibrational characteristics of a periodic system are often found
by treating it as a system of harmonic oscillators (centered around each nucleus), which
can be analyzed via the ase.phonons module. In this periodic limit, a continuum of
vibrational states (described as a phonon density of states) is produced, which can be
transformed to a partition function by straightforward means . The CrystalThermo
class has methods to calculate the internal energy U and entropy S as below, with the
Helmholtz energy also available as U −TS:
U(T) = Epot + ∆EZPE +
eǫ/kBT −1σ(ǫ)dǫ,
 eǫ/kBT −1
1 −e−ǫ/kBT #
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
In this case, the zero-point energy ∆EZPE is evaluated in integral form,
The integration of methods from the thermochemistry module with other ASE methods is
straightforward. Typically, one begins by performing a search for a stationary point on the
potential energy surface—such as a local minimum or a saddle point—then performs a normalmode or phonon analysis about this point. The output can be directly fed to the appropriate class
in the thermochemistry module. However, the user must be careful to assess the validity of
the approximate model employed. For example, to calculate the Gibbs free energy of adsorption
of a molecule from the gas phase onto a catalytic surface, one might use the approximation:
∆Gadsorption = Gmol + Gsurf −Gmol+surf ≈Gideal gas
−F adsorbed, harmonic
Here G indicates Gibbs free energy while F indicates Helmholtz free energy. Subscripts “mol”
and “surf” refer to the molecule in the gas phase and the bare catalytic surface, respectively.
“mol+surf” refers to the system composed of the molecule adsorbed on the surface. Two approximations have been employed: ﬁrst, that the free energy of the surface is unchanged by adsorption
of the molecule; second, that the adsorbed molecule has no translational DOFs so the pV term in
G = U −TS+pV may be taken to be zero such that Gadsorbed, harmonic
= F adsorbed, harmonic
which may be calculated using the HarmonicThermo class.
Below, we show a simple example of computing the ideal gas free energy for an existing Atoms
opt = QuasiNewton(atoms)
vib = Vibrations(atoms)
thermo = IdealGasThermo (vib.get_energies (),
geometry=’nonlinear ’,
potentialenergy =atoms. get_potential_energy (),
symmetrynumber =2,
atoms=atoms)
G = thermo. get_gibbs_energy (temperature =298.0 ,
pressure =101325.0)
Phase and Pourbaix diagrams
The ASE module ase.phasediagram allows to investigate the stability of compounds by means
of phase and Pourbaix diagrams.
Phase diagram.
A phase stability diagram is obtained by comparing the energy of a particular material with the energies of all possible competing solid structures. For example, the
competing phases of a hypothetical compound KTaO3 are K, Ta, and all the possible stoichiometries KxTayOz (K2O, KO3, Ta2O5, TaO3, K3TaO8, and so on). The calculated energies
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
the considered range of pH and potential.
So far, the chemical potentials of the involved chemicals are kept ﬁxed at values corresponding
to their standard states. Future releases of the module could include the possibility of tuning
the chemical potentials of the elements.
Electronic structure
ASE has several tools that support electronic structure calculations and facilitate post processing
of calculations. DFT calculators can deﬁne common methods which return various properties
of the Kohn–Sham states, like eigenvalues, occupation numbers, and the Fermi level. These
methods allow for generic calculations of density of states, band gaps, and related quantities
using methods in ase.dft following any DFT calculation. For calculators that can retrieve the
pseudo wavefunction it is also possible to simulate a simple Tersoﬀ–Hamann STM image
with the ase.dft.stm module.
Calculations with periodic systems usually require Brillouin zone sampling and need a set of
k-points on which the Bloch Hamiltonian is diagonalized.
The module ase.dft.kpoints
contains functions that return lists of k-points in a format that is compatible with the electronic structure calculators supported by ASE. For example, one can ask for a Monkhorst–Pack
grid or a set of k-points along a path in reciprocal space speciﬁed by a set of high symmetry
points in the Brillouin zone. The module also contains a dictionary of “standard paths” 
in the Brillouin zone for the most common crystal structures. These facilitate systematic calculations of band structures.
Below is an example of a script that calculates and plots the
Kohn–Sham band structure of bulk Si using ASE’s BandStructure object.
The result is
shown in Figure 12.
from ase.build
si = bulk(’Si’, ’diamond ’, a=5.4)
si.calc = GPAW(mode=’pw’, kpts =(4, 4, 4))
si. get_potential_energy ()
si.calc.set(fixdensity=True ,
kpts ={’path ’: ’WLGXWK ’, ’npoints ’: 100},
symmetry=’off’)
si. get_potential_energy ()
bs = si.calc.band_structure ()
bs.plot(emax =5.0, filename=’bands_Si.pdf’)
Estimation of exchange–correlation errors
The major approximation within DFT is the exchange–correlation functional. The BEEFEnsemble
class in ASE provides tools for estimating errors due to the choice of exchange-correlation functional. The most eﬃcient method is tightly linked with the BEEF functionals . The
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
Central region
Figure 14: Schematic illustration of the transport setup and the matrices h1, h and h2 deﬁning
the left lead, central region and right lead, respectively.
the zero-bias conductance is obtained from the expression G = 2e2
h T(EF ), where e, h and EF
are electronic charge, Planck’s constant and the Fermi energy, respectively.
The eﬀect of a
semi-inﬁnite lead α on the central region is described by the self-energy
Σα(E) = (zSCα −HCα)g0
α(z) is a surface Green’s function that is calculated using a decimation technique .
The spectral broadening matrices in (22) are given by Γα = i(Σr
In the transport module, the C, L and R regions (Figure 14) are speciﬁed by three Hamiltonian
matrices h, h1 and h2 (in case of non-orthogonal basis the three overlap matrices s, s1 and s2
are also needed). h1 and h2 should contain two principal layers; a principal layer (PL) being
a periodic part of the Hamiltonian such that there is only coupling between nearest-neighbour
principal layers. By default, the transport module assumes that the coupling between the central
region and lead α, hCα, is the same as the coupling between principal layers. In this case, h
should contain at least one principal layer of lead 1 in its upper left corner, and at least one
principal layer of lead 2 in the lower right corner. See Figure 14.
With the transport module, a number of standard transport properties can be calulated, such
• transmission function
• eigenchannel-resolved transmission functions
• eigenchannel wavefunctions
• projected density of states
and there are various tools for analysis of the transport mechanism:
• rotation to eigenbasis of a subspace, e.g. basis functions on the molecule
• cutting out a subspace, e.g. to see which molecular states are involved in the transport
To illustrate how to use the transport module we consider a simple example consisting of a
junction between graphene leads with zig-zag edges and a benzene molecule linked via alkyne
groups; see Figure 15a. For simplicity we describe the π-system only using a H¨uckel model
Hamiltonian with all nearest-neighbour hopping elements set to −3 eV and on-site energies set
to zero, i.e. bond length alternation is not taken into account. In this model, the principal layers
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
the transmission of the pristine system. This indicates that the charge carriers are not scattered
at all at these energies.
To understand the origin of the broad transmission peaks around ±1.6 eV, indicated by the
black vertical lines, we can use the analysis functions provided by the transport calculator. we
have diagonalized the subspace of the molecule including linkers. This yields the eigenenergies
of the subsystem, and from this we ﬁnd that the transmission peak position coincides with the
HOMO and LUMO energy.
To further investigate the origin of the transmission peaks we can calculate the eigenchannel
scattering functions.The eigenchannels are calculated using the method of Ref. . Figures 15c
and d visualize the eigenchannel states using coloured circles on the atomic sites; the size of the
circle indicates the absolute weight and the color indicates the phase.
When considering the eigenchannel calculated at the HOMO and LUMO energy found above,
see Figures 15c and d, it resembles the isolated molecule HOMO and LUMO (not shown) to
a large extent. This indicates resonance transmission through the HOMO and LUMO. The
eigenchannel state in Figure 15e has considerable weight on the edges of the graphene leads,
which indicates a transport mechanism involving edge states. It is well known that graphene
zigzag edges have localized edge states at the Dirac point .
Miscellaneous
Command-line tools
In addition to the Python library that can be used in Python scripts and programs, an ASE
installation will also include four command-line tools for work on the command line:
• ase-gui: Graphical user interface
• ase-db: Manipulate ASE-databases
• ase-build: Build molecules or crystals
• ase-run: Run simple calculation
The GUI can be used to visualize, manipulate and analyze trajectories. Among the analysis
tools are the possiblity to create x–y plots from the structure and energetics.
The ase-db command-line tool is for querying and manipulating databases as described in detail
in Section 4.1.
Here is an example showing how to use the ase-build and ase-run tools to optimize the
structure of the H2 molucule with the PBE functional:
$ ase -build H2 | ase -run nwchem -p xc=PBE -f 0.02
Running: H2
-31.487747
-31.492828
-31.492848
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
The ase-build and ase-run tools are very useful for doing quick test calculations, but to get
the most use out of ASE you will need to write scripts that can take advantage of the ﬂexibility
of the Python language.
Projects using ASE
A number of the calculators listed in Table 1 are developed externally to ASE. For examples,
Atomistica is a library of classical and tight-binding interatomic potentials that can be
compiled as a Python extension module and directly interfaced with ASE. It provides implementations of empirical bond-order potentials (Abell–Tersoﬀ–Brenner , REBO2 ,
etc.), their screened counterparts , and embedded atom method potentials . The
non-orthogonal tight-binding module supports charge-selfconsistency and can read a variety of parameter databases.
The quippy package is also fully interoperable with ASE. It
provides a calculator for the interatomic potentials implemented in the QUIP Fortran code 
such as the Gaussian Approximation Potential as well as support for multiscale QM/MM
calculations .
The matscipy package , developed by Lars Pastewka and James Kermode, builds on top of
ASE and adds a number of additional general purpose tools for computational materials science,
including a fast neighbour list implemented in C, tools for structure analysis, post processing
and visualisation, a calculator capable of communicating with remote codes over POSIX sockets,
as well as a number of specialist modules targeting speciﬁc applications including fracture and
contact mechanics.
Technical information
ASE is free software licensed under GNU LGPLv2.1 or newer. The source code and documentation are available from the ASE web page 
ASE requires Python 2 or 3 and the NumPy package. SciPy and MatPlotLib is recommended. The graphical user interface requires PyGTK .
Install ASE by running pip install
ase (or pip3
ase for Python 3 users) or see
the web page for alternative methods. The ase-users mailing list or #ase IRC channel on
Freenode.net are available for support.
Everybody is invited to participate in using and developing the code. Coordination and discussions take place on the mailing list whereas the development takes place on GitLab, at
 Please send us bug-reports, patches, code, and ideas.
Conclusions
The Atomic Simulation Environment is currently in use by many researchers working on diverse
applications within condensed matter physics, chemistry, and other ﬁelds. There are also quite
a few developers adding new features to the environment. So what can be expected from ASE
in the future?
 
AUTHOR SUBMITTED MANUSCRIPT - JPCM-108490
 
At the present stage the core structure of ASE dealing with the basic properties of atoms
and calculators is well developed, and it is therefore straightforward to add new functionality.
For example, new interfaces to the ONETEP and Amber codes are currently being
implemented. In the near future we hope to see further development in several areas. To mention
some of them: 1) New screening or search methods (like the currently implemented genetic
algorithm) using machine learning could be implemented.
2) The analysis of the electronic
information obtained by DFT calculations could be signiﬁcantly improved.
Recently a way
of calculating band structures was introduced but there is a general need for more detailed
analysis based on electronic spectra, densities, or wave functions. 3) Crystal symmetry analysis is
currently done mostly by the DFT codes. This task could be done directly by ASE. 4) Currently
some of the external codes like CP2K and GPAW are “kept alive” when atoms are moved to
avoid computational overhead when restarting a calculation. This could be implemented in a
more generic way in ASE so that other calculators easily could obtain this feature. 5) The fairly
new database module allows for storage and retrieval of DFT calculations. The database can
be used to keep track of the status of many similar calculations performed for example in a
computational screening study. However, there are currently no utility functions or classes to
perform this task in an easy way.
The users of ASE beneﬁt from the large number of functions available for setting up, controlling,
and analyzing simulations based on many diﬀerent calculators, and from the large ﬂexibility
in the Python language itself to construct loops and to allow for interplay between diﬀerent
simulations. As the number of available calculators increases and new functionality is added,
ASE will hopefully become an even more attractive toolbox contributing to eﬃcient development
and utilization of electronic structure theory and molecular dynamics simulations. We hope that
ASE will also encourage and contribute to further collaborative eﬀorts with open exchange of
not only data and results but also eﬃcient scripting to the beneﬁt of the research community.
Acknowledgements
The authors acknowledge funding from: The European Union’s Horizon 2020 research and
innovation program under grant agreement no. 676580 with The Novel Materials Discovery
(NOMAD) Laboratory, a European Center of Excellence; research grant 9455 from VILLUM
FONDEN; Deutsche Forschungsgemeinschaft (grant PA2023/2); the UK Engineering and Physical Sciences Research Council (grants EP/L014742/1, EP/L027682/1 and EP/P002188/1).