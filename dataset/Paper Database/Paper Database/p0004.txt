Gaussian Process Dynamical Models
for Human Motion
Jack M. Wang, David J. Fleet, Senior Member, IEEE, and Aaron Hertzmann, Member, IEEE
Abstract—We introduce Gaussian process dynamical models (GPDMs) for nonlinear time series analysis, with applications to learning
models of human pose and motion from high-dimensional motion capture data. A GPDM is a latent variable model. It comprises a lowdimensional latent space with associated dynamics, as well as a map from the latent space to an observation space. We marginalize out
the model parameters in closed form by using Gaussian process priors for both the dynamical and the observation mappings. This
results in a nonparametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach and
compare four learning algorithms on human motion capture data, in which each pose is 50-dimensional. Despite the use of small data
sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces.
Index Terms—Machine learning, motion, tracking, animation, stochastic processes, time series analysis.
INTRODUCTION
OOD statistical models for human motion are important
for many applications in vision and graphics, notably
visual tracking, activity recognition, and computer animation. It is well known in computer vision that the estimation
of 3D human motion from a monocular video sequence is
highly ambiguous. Many recently reported approaches
have relied strongly on training prior models to constrain
inference to plausible poses and motions , , , .
Specific activities could also be classified and recognized by
evaluating the likelihood of the observation, given models
for multiple activities . In computer animation, instead of
having animators specify all degrees of freedom (DOF) in a
humanlike character, the task of animating characters can
be simplified by finding the most likely motion, given
sparse constraints , .
Onecommonapproachistolearnaprobabilitydistribution
over the space of possible poses and motions, parameterized
bythejointanglesofthebody,aswellasitsglobalpositionand
orientation. Such a density function provides a natural
measure of plausibility, assigning higher probabilities to
motions that are similar to the training data. The task is
challenging due to the high dimensionality of human pose
data and to the complexity of the motion. However, poses
fromspecificactivitiesoftenlienearanonlinearmanifoldwith
much lower dimensionality than the number of joint angles.
Motivated by this property, a common approach to define the
generative model is to decouple the modeling of pose and
motion. The motion is modeled by a dynamical process
definedonalower-dimensionallatentspaceandtheposesare
generated by an observation process from the latent space.
The current literature offers a number of generative
models where the dynamics is not directly observed. Simple
models such as hidden Markov model (HMM) and linear
dynamical systems (LDS) are efficient and easily learned
but limited in their expressiveness for complex motions.
More expressive models such as switching linear dynamical
systems (SLDS) and nonlinear dynamical systems (NLDS),
are more difficult to learn, requiring many parameters that
need to be hand tuned and large amounts of training data.
In this paper, we investigate a Bayesian approach to
learningNLDS,averagingovermodelparametersratherthan
estimating them. Inspired by the fact that averaging over
nonlinear regression models leads to a Gaussian process (GP)
model, we show that integrating over NLDS parameters can
alsobeperformedinclosedform.TheresultingGPdynamical
model (GPDM) is fully defined by a set of low-dimensional
representations of the training data, with both observation
and dynamical processes learned from GP regression. As a
natural consequence of the GP regression, the GPDM
removes the need to select many parameters associated with
function approximators while retaining the power of nonlinear dynamics and observation.
Our approach is directly inspired by the GP latent
variable model (GPLVM) . The GPLVM models the joint
distribution of the observed data and their corresponding
representation in a low-dimensional latent space. It is not,
however, a dynamical model; rather, it assumes that data
are generated independently, ignoring temporal structure
of the input. Here, we augment the GPLVM with a latent
dynamical model, which gives a closed-form expression for
the joint distribution of the observed sequences and their
latent space representations. The incorporation of dynamics
not only enables predictions to be made about future data
but also helps regularize the latent space for modeling
temporal data in general (for example, see ).
The unknowns in the GPDM consist of latent trajectories
and hyperparameters. Generally, if the dynamical process
defined by the latent trajectories is smooth, then the models
tend to make good predictions. We first introduce a
maximum a posteriori (MAP) algorithm for estimating all
unknowns and discuss cases where it fails to learn smooth
trajectories. Generally, if the dynamics process defined by the
latent trajectories is smooth, then the models tend to make
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
FEBRUARY 2008
. The authors are with the Department of Computer Science, University of
Toronto, 40 St. George Street, Toronto, Ontario M5S 2E4 Canada.
E-mail: {jmwang, hertzman}@dgp.toronto.edu, .
Manuscript received 31 Oct. 2006; revised 10 Apr. 2007; accepted 16 Apr.
2007; published online 2 May 2007.
Recommended for acceptance by S. Sclaroff.
For information on obtaining reprints of this article, please send e-mail to:
 , and reference IEEECS Log Number TPAMI-0771-1006.
Digital Object Identifier no. 10.1109/TPAMI.2007.1167.
0162-8828/08/$25.00  2008 IEEE
Published by the IEEE Computer Society
good predictions. To learn smoother models, we also
consider three alternative learning algorithms, namely, the
balanced GPDM(B-GPDM) , manuallyspecifying the
hyperparameters , and a two-stage MAP approach. These
algorithms present trade-offs in efficiency, synthesis quality,
and ability to generalize. We compare learned models based
on the visual quality of generated motion, the learned latent
space configuration, and their performance in predicting
missing frames of test data.
RELATED WORK
Dynamical modeling and dimensionality reduction are two
essential tools for the modeling of high-dimensional time
series data. The latter is often necessary before one can
approach density estimation, whereas the former captures
the temporal dependence in the data.
Dimensionality Reduction
Many tasks in statistics and machine learning suffer from
the “curse of dimensionality.” More specifically, the
number of samples required to adequately cover a
hypervolume increases exponentially with its dimension.
Performance in various algorithms, both in terms of speed
and accuracy, is often improved by first obtaining a lowdimensional representation of the data.
2.1.1 Linear Methods
A natural way to achieve dimensionality reduction is to
represent the data in a linear subspace of the observation
space. Probabilistic principal components analysis (PPCA)
 , and factor analysis provide both a basis for the
subspace and a probability distribution in the observation
space. They are straightforward to implement and efficient,
and are often effective as a simple preprocessing step before
the application of more complex modeling techniques ,
 , . For purposes of density estimation, however, PCA
is often unsuitable since many data sets are not well
modeled by a Gaussian distribution. For instance, images of
objects taken over the surface of the viewsphere usually
occupy a nonlinear manifold , as does human motion
capture data (for example, see Fig. 3a).
2.1.2 Geometrically Motivated Manifold Learning
Nonlinear dimensionality reduction techniques allow one to
represent data points based on their proximity to each other
on nonlinear manifolds. Locally linear embedding (LLE) 
and the Laplacian eigenmap algorithm obtain the
embedding by observing that all smooth manifolds are
locallylinearwithrespecttosufficientlysmallneighborhoods
on the manifold. The Isomap algorithm and its variants
C-Isomap, L-Isomap , and ST-Isomap extend multidimensional scaling by ensuring that the “dissimilarity”
measure between pairs of data correspond to approximate
geodesics on the manifold.
Inapplicationssuchasdatavisualizationandanalysis ,
it is often sufficient to recover a low-dimensional latent
representation of the data without closed-form mappings
between the latent space and observation space. Although
manifold learning methods can be augmented with such
mappings as a postprocess, they do not provide a probability
distribution over data. Techniques such as mixtures of
Gaussians or the Parzen window method can be used to
learn a density model in the lower-dimensional space, but as
observed in , with human pose data, estimation of mixture
models is prone to overfitting and requires tuning a large
number of parameters in practice. For LLE and Isomap, an
additional problem is that they assume that the observed data
are densely sampled on the manifold, which is typically not
true for human motion data.
2.1.3 Nonlinear Latent Variable Models (NLVMs)
NLVMs are capable of modeling data generated from a
nonlinear manifold. NLVM methods treat the latent
coordinates and the nonlinear mapping to observations as
parameters in a generative model, which are typically
learned using optimization or the Monte Carlo simulation
when needed. Compared to linear models such as PPCA, a
lower number of dimensions can be used in the latent space
without compromising reconstruction fidelity.
The GPLVM is a generalization of the PPCA that allows
for a nonlinear mapping from the latent space to the
observation space. The model estimates the joint density of
the data points and their latent coordinates. The estimates of
the latent coordinates are used to represent a learned model
and can be directly used for data visualization. The GPLVM
has the attractive property of generalizing reasonably well
from small data sets in high-dimensional observation spaces
 , and fast approximation algorithms for sparse GP
regression can be used for learning , .
Except for ST-Isomap, neither manifold learning nor such
NLVM methods are designed to model time series data. For
applications in vision and graphics, the training data are
typically video and motion capture sequences, where the
frame-to-frame dependencies are important. Temporal models can also provide a predictive distribution over future data,
which is important for tracking applications.
Dynamical Systems
The modeling of time series data by using dynamical systems
is of interest to fields ranging from control engineering to
economics. Given a probabilistic interpretation, state-space
dynamical systems corresponding to the graphical model in
Fig. 1a provide a natural framework for incorporating
dynamics into latent variable models. In Fig. 1a, xt represents
the hidden state of the system at time t, whereas yt represents
the observed output of the system at time t. A dynamical
function, which is parameterized by A, and additive process
noise govern the evolution of xt. An observation function,
which is parameterized by B, and measurement noise
generate yt. The noise is assumed to be Gaussian, and the
dynamical process is assumed to be Markov. Note that
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
FEBRUARY 2008
Fig. 1. Time series graphical models. (a) Nonlinear latent-variable model
for time series (hyperparameters , , and W are not shown). (b) GPDM
model. Because the mapping parameters A and B have been marginalized over, all latent coordinates X ¼ ½x1; . . . ; xNT are jointly correlated,
as are all poses Y ¼ ½y1; . . . ; yNT.
dynamical systems can also have input signals ut, which are
useful for modeling control systems. We focus on the fully
unsupervised case with no system inputs.
Learning such models typically involves estimating the
parameters A and B and the noise covariances, and is often
referred to as system identification. In a maximum likelihood
(ML) framework, the parameters ðÞ are chosen to maximize
pðy1...N j Þ ¼
pðy1;...;N; x1;...;N j Þdx1;...;N
as the states x1...N are unobserved. The optimization can often
be done using the expectation-maximization (EM) algorithm
 . Once a system is identified, a probability distribution
over sequences in the observation space is defined.
2.2.1 Linear Dynamical Systems
The simplest and most studied type of dynamical model is
the discrete-time LDS, where the dynamical and observation functions are linear. The ML parameters can be
computed iteratively using the EM algorithm , . As
part of the E-step, a Kalman smoother is used to infer the
expected values of the hidden states. LDS parameters can
also be estimated outside a probabilistic framework, and
subspace state space system identification (4SID) methods
 identify the system in closed form but are suboptimal
with respect to ML estimation .
Although computations in LDS are efficient and are
relatively easy to analyze, the model is not suitable for
modeling complex systems such as human motion . By
definition, nonlinear variations in the state space are treated
as noise in an LDS model, resulting in overly smoothed
motion during simulation. The linear observation function
suffers from the same shortcomings as linear latent variable
models, as discussed in Section 2.1.1.
2.2.2 Nonlinear Dynamical Systems
A naturalwayof increasing theexpressivenessof themodelis
to turn to nonlinear functions. For example, SLDSs augment
LDS with switching states to introduce nonlinearity and
appear to be better models for human motion , , .
Nevertheless,determiningtheappropriatenumberofswitchingstatesischallenging,andsuchmethodsoftenrequirelarge
amounts of training data, as they contain many parameters.
Ijspeert et al. propose an approach for modeling
dynamics by observing that in robotic control applications,
the task of motion synthesis is often to make progress
toward a goal state. Since this behavior is naturally
exhibited by differential equations with well-defined
attractor states or limit cycles, faster learning and more
robust dynamics can be achieved by simply parameterizing
the dynamical model as a differential equation.
The dynamics and observation functions can be modeled
directly using nonlinear basis functions. Roweis and
Ghahramani use radial basis functions (RBFs) to model
the nonlinear functions and identify the system by using an
approximate EM algorithm. The distribution over hidden
states cannot be estimated exactly due to the nonlinearity of
the system. Instead, extended Kalman filtering, which
approximates the system by using locally linear mappings
around the current state, is used in the E-step.
In general, a central difficulty in modeling time series data
is in determining a model that can capture the nonlinearities
of the data without overfitting. Linear autoregressive models
require relatively few parameters and allow closed-form
analysis but can only model a limited range of systems. In
contrast, existing nonlinear models can model complex
dynamics but usually require many training data points to
accurately learn models.
Applications
Our work is motivated by human motion modeling for
video-based people tracking and data-driven animation.
People tracking requires dynamical models in the form of
transition densities in order to specify predictive distributions over new poses at each time instant. Similarly, datadriven computer animation can benefit from prior distributions over poses and motion.
2.3.1 Monocular Human Tracking
Despite the difficulties with linear subspace models mentioned above, PCA has been applied to video-based people
tracking of humans and other vision applications , ,
 , . To this end, the typical data representation is the
concatenation of the entire trajectory of poses to form a single
vector in observation space. The lower-dimensional PCA
subspace is then used as the state space. In place of explicit
dynamics, a phase parameter, which propagates forward in
time, can serve as an index to the prior distribution of poses.
Nonlinear dimensionality reduction techniques such as
LLE have also been used in the context of human pose
analysis. Elgammal and Lee use LLE to learn activitybased manifolds from silhouette data. They then use nonlinear regression methods to learn mappings from manifolds
back to the silhouette space and to the pose space. Jenkins and
Mataric use ST-Isomap to learn embeddings of multiactivity human motion data and robot teleoperation data.
Sminchisescu and Jepson used spectral embedding
techniques to learn an embedding of human motion capture
data. They also learn a mapping back to pose space
separately. None of the above approaches learns a dynamical
function explicitly and no density model is learned in . In
general, learning the embedding, the mappings, and the
density function separately is undesirable.
2.3.2 Computer Animation
The applications of probabilistic models for animation
revolve around motion synthesis, subject to sparse user
constraints. Brand and Hertzmann augment an HMM
with stylistic parameters for style-content separation. Li et al.
 model human motion by using a two-level statistical
model, combining linear dynamics and Markov switching
dynamics. A GPLVM is applied to inverse kinematics by
Grochow et al. , where ML is used to determine pose, given
kinematics constraints.
Nonparametric methods have also been used for motion
prediction and animation , , . For example, in
animation with motion graphs, each frame of motion is
treated as a node in the graph. A similarity measure is
assigned to edges in the graph and can be viewed as
transition probabilities in a first-order Markov process.
Motion graphs are designed to be used with large motion
capture databases, and the synthesis of new motions
typically amounts to reordering the poses already in the
database. An important strength of motion graphs is the
ability to synthesis high-quality motions, but the need for a
large amount of data is undesirable.
WANG ET AL.: GAUSSIAN PROCESS DYNAMICAL MODELS FOR HUMAN MOTION
Motion interpolation techniques are designed to create
natural-looking motions relatively far from input examples.
Typically, a set of interpolation parameters must be either
well-defined (that is, the location of the right hand) or
specified byhand(that is,anumberrepresentingemotion) for
each example. A mapping from the parameter space to the
pose or motion space is then learned using nonlinear
regression , . Linear interpolation between motion
segments by using the spatial-temporal morphable models is
possible , , provided that correspondences can be
established between the available segments. More closely
related to our work, Mukai and Kuriyama employ a form
of GP regression to learn the mapping from interpolation
parameters to pose and motion. In particular, one can view
the GPLVM and the GPDM introduced below as interpolation methods with learned interpolation parameters.
GAUSSIAN PROCESS DYNAMICS
The GPDM is a latent variable model. It comprises a
generative mapping from a latent space x to the observation
space y and a dynamical model in the latent space (Fig. 1).
Thesemappingsare,ingeneral,nonlinear. Forhumanmotion
modeling, avectoryintheobservationspacecorrespondstoa
pose configuration, and a sequence of poses defines a motion
trajectory. The latent dynamical model accounts for the
temporal dependence between poses. The GPDM is obtained
bymarginalizingouttheparametersofthetwomappingsand
optimizing the latent coordinates of training data.
Moreprecisely, ourgoalis tomodeltheprobabilitydensity
of a sequence of vector-valued states y1; . . . ; yt; . . . ; yN, with
discrete-time index t and yt 2 IRD. As a basic model, consider
a latent variable mapping (3) with first-order Markov
dynamics (2)
xt ¼ fðxt1; AÞ þ nx;t;
yt ¼ gðxt; BÞ þ ny;t:
Here, xt 2 IRd denotes the d-dimensional latent coordinates
at time t, f and g are mappings parameterized by A and B,
and nx;t and ny;t are zero-mean, isotropic, white Gaussian
noise processes. Fig. 1a depicts the graphical model.
Although linear mappings have been used extensively in
autoregressive models, here we consider the more general
nonlinear case, for which f and g are linear combinations of
(nonlinear) basis functions:
for basis functions i and j, with weights A  ½a1; a2; . . .T
and B  ½b1; b2; . . .T. To fit this model to the training data,
one must select an appropriate number of basis functions,
and one must ensure that there is enough data to constrain
the shape of each basis function. After the basis functions
are chosen, one might estimate the model parameters A and
B, usually with an approximate form of EM . From a
Bayesian perspective, however, the uncertainty in the
model parameters is significant, and because the specific
forms of f and g are incidental, the parameters should be
marginalized out if possible. Indeed, in contrast with
previous NLDS models, the general approach that we take
in the GPDM is to estimate the latent coordinates while
marginalizing over the model parameters.
Each dimension of the latent mapping g in (5) is a linear
function of the columns of B. Therefore, with an isotropic
Gaussian prior on the columns of B and the Gaussian noise
assumption above, one can show that marginalizing over g
can be done in closed form , . In doing so, we obtain
a Gaussian density over the observations Y  ½y1; . . . ; yNT,
which can be expressed as a product of GPs (one for each of
the D data dimensions)
pðY j X; ; WÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞNDjKY jD
where KY is a kernel matrix with hyperparameters  that
are shared by all observation space dimensions, as well as
hyperparameters W. The elements of the kernel matrix KY
are defined by a kernel function ðKY Þij  kY ðxi; xjÞ. For the
mapping g, we use the RBF kernel
kY ðx; x0Þ ¼ exp  1
2 kx  x0k2
The width of the RBF kernel function is controlled by 1
is the variance of the isotropic additive noise in (3).
The ratio of the standard deviation of the data and the
additive noise also provides a signal-to-noise ratio (SNR)
 , and here, SNRð Þ ¼
Following , we include D scale parameters W 
diagðw1; . . . ; wDÞ, which model the variance in each observationdimension.1Thisisimportantinmanydatasetsforwhich
different dimensions do not share the same length scales or
differ significantly in their variability over time. The use of W
in (6) is equivalent to a GP with kernel function kY ðx; x0Þ=w2
for dimension m. That is, the hyperparameters fwmgD
account for the overall scale of the GPs in each data
dimension. In effect, this assumes that each dimension of
the input data should exert the same influence on the shared
kernel hyperparameters 1 and 2.
The dynamic mapping on the latent coordinates X 
½x1; . . . ; xNT is conceptually similar but more subtle.2 As
above, one can form the joint density over the latent
coordinates and the dynamics weights A in (4). Then, one
can marginalize over the weights A to obtain
pðX j Þ ¼
pðX j A; Þ pðA j Þ dA;
where  is a vector of kernel hyperparameters. Incorporating the Markov property (2) gives
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
FEBRUARY 2008
1. With the addition of the scale parameters W, the latent variable
mapping (3) becomes yt ¼ W1ðgðxt; BÞ þ ny;tÞ.
2. Conceptually, we would like to model each pair ðxt; xtþ1Þ as a training
pair for regression with g. However, we cannot simply substitute them
directly into the GP model of (6), as this leads to the nonsensical expression
pðx2; . . . ; xN j x1; . . . ; xN1Þ.
pðX j Þ ¼ pðx1Þ
pðxt j xt1; A; ÞpðA j Þ dA:
Finally, with an isotropic Gaussian prior on the columns of
A, one can show that (9) reduces to
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞðN1ÞdjKXjd
where X2:N ¼ x2; . . . ; xN
T, and KX is the ðN  1Þ  ðN 
1Þ kernel matrix constructed from X1:N1 ¼ x1; . . . ; xN1
Next, we also assume that x1 also has a Gaussian prior.
The dynamic kernel matrix has elements defined by a
kernel function ðKXÞij  kXðxi; xjÞ, for which a linear
kernel is a natural choice, that is,
kXðx; x0Þ ¼ 1xTx0 þ 1
In this case, (10) is the distribution over the state trajectories
of length N, drawn from a distribution of autoregressive
models with a preference for stability . Although a
substantial portion of human motion (as well as many other
systems) can be well modeled by linear dynamical models,
ground contacts introduce nonlinearity . We found that
the linear kernel alone is unable to synthesize good walking
motions (for example, see Figs. 3h and 3i). Therefore, we
typically use a “linear þ RBF” kernel
kXðx; x0Þ ¼ 1 exp  2
2 kx  x0k2
þ 3xTx0 þ 1
The additional RBF term enables the GPDM to model
nonlinear dynamics, whereas the linear term allows the
system to regress to linear dynamics when predictions are
made far from the existing data. Hyperparameters 1 and
2 represent the output scale and the inverse width of the
RBF terms, and 3 represents the output scale of the linear
term. Together, they control the relative weighting between
the terms, whereas 1
represents the variance of the noise
term nx;t. The SNR of the dynamical process is given by
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð1 þ 3Þ4
It should be noted that, due to the marginalization over
A, the joint distribution of the latent coordinates is not
Gaussian. One can see this in (10), where the latent variables
occur both inside the kernel matrix and outside it, that is,
the log likelihood is not quadratic in xt. Moreover, the
distribution over state trajectories in the nonlinear dynamical system is, in general, non-Gaussian.
Following , we place uninformative priors on the
kernel hyperparameters ðpðÞ / Q
and pð Þ / Q
Such priors represent a preference for a small output scale
(that is, small 1 and 3), a large width for the RBFs (that is,
small 2 and 1), and large noise variances (that is, small 4
and 2). We also introduce a prior on the variances wm that
comprise the elements of W. In particular, we use a broad
half-normal prior on W, that is,
where wm > 0, and  is set to 103 in the experiments below.
Such a prior reflects our belief that every data dimension
has a nonzero variance. This prior avoids singularities in
the estimation of the parameters wj (see Algorithm 1) and
prevents any one data dimension with an anomalously
small variance from dominating the estimation of the
remaining kernel parameters.
Taken together, the priors, the latent mapping, and the
dynamics define a generative model for time series
observations (Fig. 1b)
pðX; Y; ; ; WÞ
¼ pðY j X; ; WÞ pðX j Þ pðÞ pð Þ pðWÞ:
Multiple Sequences
This model extends naturally to multiple sequences
Yð1Þ; . . . ; YðPÞ, with lengths N1; . . . ; NP. Each sequence
has associated latent coordinates Xð1Þ; . . . ; XðPÞ within a
shared latent space. To form the joint likelihood, concatenate all sequences and proceed as above with (6). A
similar concatenation applies for the latent dynamical
model (10) but accounting for the assumption that the first
pose of sequence i is independent of the last pose of
sequence i  1. That is, let
X2:N ¼ Xð1Þ
T; . . . ; XðPÞ
X1:N1 ¼ Xð1Þ
T; . . . ; XðPÞ
The kernel matrix KX is constructed with rows of X1:N1 as in
(10) and is of size ðN  PÞ  ðN  PÞ. Finally, we place an
isotropic Gaussian prior on the first pose of each sequence.
Higher-Order Features
The GPDM can be extended to model higher-order Markov
chains and to model velocity and acceleration in inputs and
outputs. For example, a second-order dynamical model,
xt ¼ fðxt1; xt2; AÞ þ nx;t
can be used to explicitly model the dependence on two past
frames (or on velocity). Accordingly, the kernel function
will depend on the current and previous latent positions,
kXð ½xt;xt1; ½x ; x 1 Þ
¼ 1 exp  2
2 kxt  x k2  3
2 kxt1  x 1k2
t x þ 5 xT
t1x 1 þ 1
Similarly, the dynamics can be formulated to predict the
velocity in the latent space,
vt1 ¼ fðxt1; AÞ þ nx;t:
Velocity prediction may be more appropriate for modeling
smooth motion trajectories. Using a first-order Taylor series
approximation of position as a function of time, in the
neighborhood of t  1, with time step t, we have
xt ¼ xt1 þ vt1t. The dynamics likelihood pðXjÞ can
then be written by redefining X2:N ¼ ½x2  x1; . . . ; xN 
xN1T=t in (10). For a fixed time step of t ¼ 1, the
WANG ET AL.: GAUSSIAN PROCESS DYNAMICAL MODELS FOR HUMAN MOTION
velocity prediction is analogous to using xt1 as a “mean
function” for predicting xt. Higher-order features have
previously been used in GP regression as a way to reduce
the prediction variance , .
Conditional GPDM
Thus far, we have defined the generative model and formed
the posterior distribution (14). Leaving the discussion of
learning algorithms to Section 4, we recall here that the
main motivation for the GPDM is to use it as a prior model
of motion. A prior model needs to evaluate or predict
whether a new observed motion is likely.
Given the learned, model  ¼
Y; X; ; ; W
, the distribution over a new sequence YðÞ and its associated latent
trajectory XðÞ is given by
pðYðÞ; XðÞ j Þ ¼ pðYðÞ j XðÞ; ÞpðXðÞ j Þ;
¼ pðY; YðÞ j X; XðÞ; ; WÞ
pðY j X; ; WÞ
pðX; XðÞ j Þ
/ pðY; YðÞ j X; XðÞ; ; WÞpðX; XðÞ j Þ;
where YðÞ and XðÞ are M  D and M  d matrices,
respectively. Here, (20) factors the conditional density into
a density over latent trajectories and a density over poses
conditioned on latent trajectories, which we refer to as the
reconstruction and dynamic predictive distributions.
For sampling and optimization applications, we only
need to evaluate (20) up to a constant. In particular, we can
form the joint distribution over both new and observed
sequences (22) by following the discussion in Section 3.1.
The most expensive operation in evaluating (22) is the
inversion of kernel matrices of size ðN þ MÞ  ðN þ MÞ.3
When the number of training data is large, the computation
cost can be reduced by evaluating (20) in terms of
precomputed block entries to the kernel matrices in (22).
Since the joint distribution over fYðÞ; Yg in (22) is
Gaussian, it follows that YðÞ j Y is also Gaussian. More
specifically, suppose the reconstruction kernel matrix in (22)
is given by
KY ;Y ðÞ ¼
where ðAÞij ¼ kY ðxi; xðÞ
j Þ and ðBÞij ¼ kY ðxðÞ
j Þ are elements of N  M and M  M kernel matrices, respectively.
pðYðÞ j XðÞ; Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞMDjKY ðÞjD
Y ðÞZY W2ZT
where ZY ¼ YðÞ  ATK1
Y Y and KY ðÞ ¼ B  ATK1
Here, KY only needs to be inverted once by using the
learned model. To evaluate (24) for new sequences, only
KY ðÞ must be inverted, which has size M  M, and is not
dependent on the size of the training data.
The distribution pðXðÞ j Þ ¼ pðX;XðÞ j Þ
is not Gaussian,
but by simplifying the quotient on the right-hand side, an
expression similar to (24) can be obtained. As above,
suppose the dynamics kernel matrix in (22) is given by
where ðCÞij ¼ kXðxi; xðÞ
j Þ and ðDÞij ¼ kXðxðÞ
j Þ are elements of ðNPÞðM 1Þ and ðM  1Þ  ðM  1Þ kernel
matrices, respectively. Then,
pðXðÞ j Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2ÞðM1ÞdjKXðÞ
jd exp  1
where ZX ¼XðÞ
2:N CTK1
X X2:N and KXðÞ ¼ D  CTK1
The matrices X2:N and XðÞ
2:N are described in Section 3.1. As
with KY above, KX only needs to be inverted once. Also
similar to KY ðÞ, the complexity of inverting KXðÞ does not
depend on the size of the training data.
GPDM LEARNING
Learning the GPDM from measured data Y entails using
numerical optimization to estimate some or all of the
unknowns in the model fX; ; ; Wg. A model gives rise to
a distribution over new poses and their latent coordinates
(20). We expect modes in this distribution to correspond to
motions similar to the training data and their latent
coordinates. In the following sections, we evaluate the
models based on examining random samples drawn from
the models, as well as the models’ performance in filling in
missing frames. We find that models with visually smooth
latent trajectories X not only better match our intuitions but
also achieve better quantitative results. However, care must
be taken in designing the optimization method, including the
objective function itself. We discuss four options: MAP,
B-GPDM , hand tuning  , and two-stage MAP in this
The data used for all the experiments are human motion
capture data from the Carnegie Mellon University motion
capture (CMU mocap) database. As shown in Fig. 2, we use a
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
FEBRUARY 2008
3. The dimension of the dynamics kernel is only smaller by a constant.
Fig. 2. The skeleton used in our experiments is a simplified version of
the default skeleton in the CMU mocap database. The numbers in
parentheses indicate the number of DOFs for the joint directly above the
labeled body node in the kinematic tree.
simplified skeleton, where each pose is defined by 44 Euler
angles for joints, three global (torso) pose angles, and three
global (torso) translational velocities.4 The data are mean
subtracted, but otherwise, we do not apply preprocessing
such as time synchronization or time warping.
MAP Estimation
A natural learning algorithm for the GPDM is to minimize the
joint negative log-posterior of the unknowns  ln pðX; ;
; W j YÞ that is given, up to an additive constant, by
L ¼ LY þ LX þ
22 trðW2Þ þ
2 ln jKY j þ 1
 N ln jWj;
2 ln jKXj þ 1
As described in Algorithm 1, we alternate between minimizing L with respect to W in closed form5 and with respect to
fX; ; g by using scaled conjugate gradient (SCG). The
latent coordinates are initialized using a subspace projection
onto the first d principal directions given by PCA applied to
mean-subtracted data Y. In our experiments, we fix the
number of outer loop iterations as I ¼ 100 and the number of
SCG iterations per outer loop as J ¼ 10.
Algorithm 1. MAP estimation of fX; ; ; Wg.
Require: Data Y. Integers fd; I; Jg:
Initialize X with PCA on Y with d dimensions.
Initialize  ( ð0:9; 1; 0:1; eÞ,  ( ð1; 1; eÞ, fwkg ( 1.
for i ¼ 1 to I do
for j ¼ 1 to D do
ðYÞ1j; . . . ; ðYÞNj
j ( N dTK1
fX; ; g ( optimize (27) with respect to fX; ; g
using SCG for J iterations.
Fig. 3 shows a GPDM on a 3D latent space, learned using
MAP estimation. The training data comprised two gait cycles
of a person walking. The initial coordinates provided by PCA
are shown in Fig. 3a. Fig. 3c shows the MAP latent space. Note
that the GPDM is significantly smoother than a 3D GPLVM
(that is, without dynamics), as shown in Fig. 3b.
Fig. 5b shows a GPDM latent space learned from the
walking data of four different walkers. In contrast to the
model learned with a single walker in Fig. 3, the latent
trajectories here are not smooth. There are small clusters of
latent positions separated by large jumps in the latent space.
Although such models produce good reconstructions from
latent positions close to the training data, they often produce
poor dynamical predictions. For example, neither the sample
trajectories shown in Fig. 5d nor the reconstructed poses in
Fig. 10a resemble the training data particularly well.
Balanced GPDM
Since the LX term in the MAP estimation penalizes
unsmooth trajectories, one way to encourage smoothness is
to increase the weight on LX during optimization. Urtasun
et al. suggest replacing LX in (27) with D
d LX, thereby
“balancing” the objective function based on the ratio
between dimensions of data and latent spaces ðD
dÞ. Learned
from the same data as that in Fig. 5b, Fig. 6a shows a model
learned using the balanced GPDM (B-GPDM). It is clear that
the latent model is now much smoother. Furthermore,
random samples drawn from the model yield better walking
simulations, and it has proven to be successful as a prior for
3D people tracking , . Though simple and effective,
the weighting constant in the B-GPDM does not have a valid
probabilistic interpretation; however, similar variations
have been used successfully in time series analysis for
speech recognition with HMMs , .
Manually Specified Hyperparameters
The B-GPDM manipulates the objective function to favor
smooth latent trajectories. A more principled way of
achieving this is by ensuring that pðX j Þ represents a strong
preference for smooth trajectories, which can be achieved by
selecting  by hand instead of optimizing for it. One way to
select a suitable  is to examine samples from pðX j Þ . If
a sufficiently strong prior is selected, then models with
smooth trajectories can be learned. Fig. 7a shows a fourwalker model learned with such a smoothness prior. We
set  ¼ ½0:009; 0:2; 0:001; 1e6T, inspired by observations
from .6 It is conceivable that a better choice of  could
give a very different set of latent trajectories and better
results in our experiments.
Two-Stage Map Estimation
Both the B-GPDM and hand tuning  are practical ways to
encourage smoothness. However, MAP learning is still
prone to overfitting in high-dimensional spaces.7 When we
seek a MAP estimate, we are looking to approximate the
posterior distribution with a delta function. Here, as there
are clearly a multiplicity of posterior modes, the estimate
may not represent a significant proportion of the posterior
probability mass . To avoid this problem, we could aim
to find a mode of the posterior that effectively represents a
significant proportion of the local probability mass. In
effect, this amounts to minimizing the expected loss with
respect to the different loss functions (cf. ).
Toward this end, we consider a two-stage algorithm for
estimating unknowns in the model: First, estimate the
hyperparameters  ¼ f; ; Wg with respect to an unknown
distribution of latent trajectories X, and then, estimate X
while holding  fixed. Because X comprises the vast majority
of the unknown model parameters, by marginalizing over X
WANG ET AL.: GAUSSIAN PROCESS DYNAMICAL MODELS FOR HUMAN MOTION
4. For each frame, the global velocity is set to the difference between the
next and the current frames. The velocity for the last frame is copied from
the second to last frames.
5. The update for wk shown in Algorithm 1 is a MAP estimate, given the
current values of fX; ; g. It is bounded by 
, which is due to our
choice of prior on W (13). Note that a prior of pðwkÞ / w1
regularize the estimation of wk, since its MAP estimate then becomes
undefined when dT K1
6. Note that the model in used velocity prediction (cf. Section 3.2)
and an RBF kernel (rather than linear þ RBF).
7. We are optimizing in a space with a dimension over N  d since there
is one latent point for every training pose.
and, therefore, taking its uncertainty into account while
estimating , we are finding a solution that is more
representative of the posterior distribution on the average.
This is also motivated by the fact that the parameter
estimation algorithms for NLDS typically account for
uncertainty in the latent space . Thus, in the first step,
we find an estimate of  that maximizes pðY j Þ ¼
R pðY; X j ÞdX. The optimization is approximated using a
variant of EM , called Monte Carlo EM (MCEM) .
In the E-step of the ith iteration, we compute the
expected complete negative log likelihood8  ln pðY; X j Þ
under pðXjY; iÞ, which is the posterior, given the current
estimate of hyperparameters
pðX j Y; iÞ ln pðY; X j ÞdX:
In the M-step, we seek a set of hyperparameters iþ1, that
minimizes LE. In MCEM, we numerically approximate (30)
by sampling from pðXjY; iÞ using the HMC 9:
LEðÞ   1
ln pðY; XðrÞ j Þ;
where fXðrÞgR
r¼1  pðXjY; iÞ. The derivative with respect
to the hyperparameters is given by
@ ln pðY; XðrÞ j Þ:
The approximations are simply the sums of the derivatives of the complete log likelihood, which we used for
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
FEBRUARY 2008
Fig.3.Modelslearnedfroma walking sequencecomprisingtwogaitcycles.(a)ThePCAinitializationsandthelatentcoordinateslearnedwith(b)GPLVM
and (c) GPDM are shown in blue. Vectors depict the temporal sequence. (d)  ln variance for reconstruction shows positions in latent space that are
reconstructed with high confidence. (e) Random trajectories drawn from the dynamic predictive distribution by using hybrid Monte Carlo (HMC) are
green, whereas the red trajectory is the mean prediction sample. (f) Longer random trajectories drawn from the dynamic predictive distribution. (g), (h),
and (i)  ln variance for reconstruction, randomtrajectories, and longer randomtrajectoriescreatedin the same fashion as (d), (e), and (f), using a model
learned with the linear dynamics kernel. Note that the samples do not follow the training data closely, and longer trajectories are attracted to the origin.
8. In practice, we compute the expected value of the log of (14), which is
regularized by the priors on the hyperparameters.
9. We initialize the sampler by using SCG to find a mode in pðXjY; iÞ,
and 50 samples, in total, are returned to compute the expectation. We use
10 burn-in samples and take 100 steps per trajectory, and the step size is
adjusted so that an acceptance rate of 0.6 to 0.95 is achieved on the first
25 samples.
optimizing (14). Algorithm 2 describes the estimation in
pseudocode. We set R ¼ 50, I ¼ 10, J ¼ 10, and K ¼ 10 in
our experiments.
Algorithm 2. MAP estimation of f; ; Wg using MCEM.
Require: Data matrix Y. Integers fd; R; I; J; Kg.
Initialize  ( ð0:9; 1; 0:1; eÞ,  ( ð1; 1; eÞ, fwkg ( 1.
for i ¼ 1 to I do
Generate fXðrÞgR
r¼1  pðX j Y; ; ; WÞ using HMC
Construct fKðrÞ
r¼1 from fXðrÞgR
for j ¼ 1 to J do
for k ¼ 1 to D do
d ( ½ðYÞ1k; . . . ; ðYÞNkT
f; g ( minimize (31) with respect to f; g using
SCG for K iterations.
In the second stage, we maximize ln pðX;  j YÞ with
respect to X by using SCG. The resulting trajectories
estimated by the two-stage MAP on the walking data are
shown in Fig. 8a. In contrast with previous methods, data
from the four walking subjects are placed in separate parts of
the latent space. On the golf swings data set (Fig. 9a),
smoother trajectories are learned as compared to the MAP
model in Fig. 4a.
EVALUATION OF LEARNED MODELS
The computational bottleneck for the learning algorithms
above is the inversion of the kernel matrices, which is
necessary to evaluate the likelihood function and its gradient.
Learning by using the MAP estimation, the B-GPDM, and
fixed hyperparameters  requires approximately 6,000 inversions of the kernel matrices, given our specified number of
iterations. These algorithms take approximately 500 seconds
for a data set of 289 frames. The two-stage MAP algorithm is
more expensive to run, as both the generation of samples in
the E-step and the averaging of samples in the M-step require
evaluation of the likelihood function. The experiments below
used approximately 400,000 inversions, taking about 9 hours
for the same data set of 289 frames. Note that our
implementation is written in Matlab, with no attempts made
to optimize performance, nor is sparsification exploited (for
example, see ).
In the rest of this section, we discuss visualizations and
comparisons of the GPDMs. We first consider the visualization methods on a single-walker model and golf swing
models learned using MAP and two-stage MAP, and then,
we discuss the failure of MAP in learning a four-walker
model. Finally, we compare the four-walker models learned
using the different methods above. The comparison is based
on visually examining samples from the distribution over
new motions, as well as errors in the task of filling in
missing frames of data.
Single-Walker Model
Fig. 3 shows the 3D latent models learned from data
comprising two walk cycles from a single subject.10 In all
WANG ET AL.: GAUSSIAN PROCESS DYNAMICAL MODELS FOR HUMAN MOTION
Fig. 4. Models learned from four golf swings from the same golfer. The latent coordinates learned with (a) GPLVM and (b) GPDM are shown in blue.
Vectors depict the temporal sequence. (c)  ln variance for reconstruction shows positions in latent space that are reconstructed with high confidence.
(d) Random trajectories drawn from the dynamic predictive distribution using HMC are green, whereas the red trajectory is the mean of the samples.
Fig. 5. Models learned from walking sequences from four different subjects. The latent coordinates learned with (a) GPLVM and (b) GPDM are
shown in blue. (c)  ln variance plot shows clumpy high-confidence regions. (d) Samples from the dynamic predictive distribution are shown in
green, whereas the mean prediction sample is shown in red. The samples do not stay close to the training data.
10. CMU database file 07_01.amc, frames 1 to 260, downsampled by a
factor of 2.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
FEBRUARY 2008
Fig. 7. Models learned with fixed  from three different walking subjects. (a) The learned latent coordinates are shown in blue. (b)  ln variance plot
shows smooth high-confidence regions, but the variance near the data is larger than in Fig. 5c, similar to the B-GPDM. (c) Typical samples from the
dynamic predictive distribution are shown in green, whereas the mean prediction sample is shown in red.
Fig. 8. Models learned with the two-stage MAP from four different walking subjects. (a) The learned latent coordinates are shown in blue. Note that
the walkers are separated into distinct portions of the latent space. (b)  ln variance plot shows smooth high-confidence regions, and the variance
near the data is similar to Fig. 5c. (c) Typical samples from the dynamic predictive distribution are shown in green, whereas the mean prediction
sample is shown in red.
Fig. 9. Models learned with the two-stage MAP from four golf swings from the same golfer. (a) The learned latent coordinates are shown in blue.
(b)  ln variance for reconstruction shows positions in latent space that are reconstructed with high confidence. (c) Random trajectories drawn from
the dynamic predictive distribution by using HMC are green, whereas the red trajectory is the mean prediction sample. The distribution is conditioned
on starting from the beginning of a golf swing.
Fig. 6. B-GPDMs learned from the walking sequences from three different subjects. (a) The learned latent coordinates are shown in blue. (b)  ln
variance plot shows smooth high-confidence regions, but the variance near the data is larger than in Fig. 5c. (c) Samples from the dynamic predictive
distribution are shown in green, whereas the mean prediction sample is shown in red.
the experiments here, we use a 3D latent space. Learning with
more than three latent dimensions significantly increases the
number of latent coordinates to be estimated. Conversely, in
two dimensions, the latent trajectories often intersect, which
makes learning difficult. In particular, GPs are function
mappings, providing one prediction for each latent position.
Accordingly, learned 2D GPDMs often contain large “jumps”
in latent trajectories, as the optimization breaks the trajectory
to avoid nearby positions requiring inconsistent temporal
predictions.
Fig. 3b shows a 3D GPLVM (that is, without dynamics)
learned from the walking data. Note that without the
dynamical model, the latent trajectories are not smooth:
There are several locations where consecutive poses in the
walking sequence are relatively far apart in the latent space.
In contrast, Fig. 3c shows that the GPDM produces a much
smoother configuration of latent positions. Here, the GPDM
arranges the latent positions roughly in the shape of a saddle.
Fig. 3d shows a volume visualization of the value
ln p xðÞ; yðÞ ¼
Y ðxðÞÞ j 
Y ðxðÞÞ is the mean of
the GP for pose reconstruction as a function of the
latent space position xðÞ, that is,
Y ðxÞ ¼ YTK1
Y ðxÞ ¼ kY ðx; xÞ  kY ðxÞTK1
The prediction variance is
Y ðxÞ. The color in the figure
depicts the variance of the reconstructions, that is, it is
proportional to  ln
Y ðxÞ. This plot depicts the confidence
with which the model reconstructs a pose as a function of
latent position x. The GPDM reconstructs the pose with high
confidence in a “tube” around the region occupied by the
training data.
To further illustrate the dynamical process, we can draw
samples from the dynamic predictive distribution. As noted
above, because we marginalize over the dynamic weights A,
the resulting density over latent trajectories is non-Gaussian.
In particular, it cannot be factored into a sequence of loworder Markov transitions (Fig. 1a). Hence, one cannot
properly draw samples from the model in a causal fashion
one state at a time from a transition density pðxðÞ
Instead, we draw fair samples of entire trajectories by
using a Markov chain Monte Carlo sampler. The Markov
chain was initialized with what we call a mean prediction
sequence, generated from xðÞ
by simulating the dynamical
process one frame at a time. That is, the density over xðÞ
conditioned on xðÞ
t1 is Gaussian:
XðxÞ ¼ kXðx; xÞ  kXðxÞTK1
where kXðxÞ is a vector containing kXðx; xiÞ in the ith entry,
and xi is the ith training vector. At each step of mean
prediction, we set the latent position to be the mean latent
position conditioned on the previous step xðÞ
Given an initial mean prediction sequence, a Markov
chain with several hundred samples is generated using
HMC.11 Fig. 3e shows 23 fair samples from the latent
dynamics of the GPDM. All samples are conditioned on the
same initial state xðÞ
1 , and each has a length of 62 time steps
(that is, drawn from pðXðÞ
2:62 j xðÞ
1 ; ÞÞ. The length was
chosen to be just less than a full gait cycle for ease of
visualization. The resulting trajectories are smooth and
roughly follow the trajectories of the training sequences.
The variance in latent position tends to grow larger when
the latent trajectories corresponding to the training data are
farther apart and toward the end of the simulated trajectory.
It is also of interest to see samples generated that are
much longer than a gait cycle. Fig. 3f shows one sample
from an HMC sampler that is approximately four cycles in
length. Notice that longer trajectories are also smooth,
generating what look much like limit cycles in this case. To
see why this process generates motions that look smooth
and consistent, note that the variance of pose xðÞ
determined in part by
t Þ. This variance will be lower
is nearer to other samples in the training data or
the new sequence. As a consequence, the likelihood of xðÞ
can be increased by moving xðÞ
closer to the latent
positions of other poses in the model.
Figs. 3g, 3h, and 3i show a GPDM with only a linear term
in the dynamics kernel (12). Here, the dynamical model is
not as expressive, and there is more process noise. Hence,
random samples from the dynamics do not follow the
training data closely (Fig. 3h). The longer trajectories in
Fig. 3i are attracted toward the origin.
Golf Swing Model
TheGPDMcanbeappliedtobothcyclicmotions(likewalking
above) and acyclic motions. Fig. 4 shows a GPDM learned
from four swings of a golf club, all by the same subject.12
Figs.4aand4bshowa3DGPLVManda3DGPDMonthesame
WANG ET AL.: GAUSSIAN PROCESS DYNAMICAL MODELS FOR HUMAN MOTION
11. We allow for 40 burn-in samples and set the HMC parameters to
obtain a rejection rate of about 20 percent.
12. CMU database files: 64_01.amc (frames 120 to 400), 64_02.amc
(frames 170 to 420), 64_03.amc (frames 100 to 350), and 64_04.amc (frames 80
to 315). All are downsampled by a factor of 4.
Fig. 10. Walks synthesized by taking the mean of the predictive distribution, conditioned on a starting point in latent space. (a) The walk produced by
the MAP model is unrealistic and does not resemble the training data. (b) High-quality walk produced by a model learned using two-stage MAP.
golf data. The swings all contain periods of high acceleration;
consequently, the spacing between points in latent space are
more varied compared to the single-walker data. Although
the GPLVM latent space contains an abrupt jump near the
bottom of the figure, the GPDM is much smoother. Fig. 4c
shows the volume visualization, and Fig. 4d shows samples
drawn from the dynamic predictive distribution.
Although the GPDM learned with the MAP estimation is
better behaved than the GPLVM, an even smoother model
can be learned using the two-stage MAP. For example,
Figs. 9a and 9b show the GPDM learned with the two-stage
MAP. Random samples from its predictive dynamical
model, as shown in Fig. 9c, nicely follow the training data
and produce animations that are of visually higher quality
than samples from the MAP model in Fig. 4d.
Four-Walker Models
The MAP learning algorithm produces good models for the
single-walker and the golf swings data. However, as
discussed above, this is not the case with the model learned
with four walkers (Fig. 5b).13 In contrast to the GPDM learned
for the single-walker data (Fig. 3), the latent positions for the
training poses in the four-walker GPDM consist of small
clumps of points connected by large jumps. The regions with
high reconstruction certainty are similarly clumped (Fig. 5c),
and only in the vicinity of these clumps is pose reconstructed
reliably. Also, note that the latent positions estimated for the
GPDM are very similar to those estimated by the GPLVM on
the same data set (Fig. 5a). This suggests that the dynamical
term in the objective function (27) is overwhelmed by the data
reconstruction term during learning and therefore has a
negligible impact on the resulting model.
To better understand this GPDM, it is instructive to
examine the estimated kernel hyperparameters. Following
 , Table 1 shows the SNR and the characteristic length scale
(CLS) of thekernels.The SNRdependslargely on thevariance
of the additive process noise. The CLS is defined as the square
root of the inverse RBF width, that is, CLSð Þ ¼ 0:5
CLSðÞ ¼ 0:5
. The CLS is directly related tothe smoothness
of the mapping , . Table 1 reveals that dynamical
hyperparameters  for the four-walker GPDM has both a low
SNR and a low CLS. Not surprisingly, random trajectories of
the dynamical model (see Fig. 5d) show a larger variability
than any of the three other models shown. The trajectories do
not stay close to regions of high reconstruction certainty and,
therefore, yield poor pose reconstructions and unrealistic
walking motions. In particular, note the feet locations in the
fourth pose from the left in Fig. 10a.
Fig. 6 shows the B-GPDM learned from the four-walker
data. Note that the learned trajectories are smooth, and poses
are not clumped. Sample trajectories from the model
dynamics stay close to the training data (Fig. 6c). On the
other hand, Fig. 6b shows that the B-GPDM exhibits higher
reconstruction uncertainty near the training data (as compared to Fig. 5c). The emphasis on smoothness when learning
the B-GPDM yields hyperparameters that give small variance
to dynamical predictions but large variance in pose reconstruction predictions. For example, in Table 1, note that the
dynamics kernel  has a high SNR of 940, whereas the SNR of
reconstruction kernel  is only 5.23. Because of the high
reconstruction variance, fair pose samples from the B-GPDM
(20) are noisy and do not resemble realistic walks (see
Fig.11a).Nevertheless,unliketheMAPmodel,meanmotions
produced from the B-GPDM from different starting states
usually correspond to high-quality walking motions.
Fig. 7 shows how a model learned with fixed hyperparameters
 (Section 4.3) also produces smooth learned
trajectories. The samples from the dynamic predictive
distribution (see Fig. 7c) have low variance, staying close
to the training data. Like the B-GPDM, the pose reconstructions have high variance (Fig. 7b). One can also infer this
from the low SNR of 9.32 for the reconstruction kernel .
Hence, like the B-GPDM, the sample poses from random
trajectories are noisy.
Fig. 8 shows a model learned using the two-stage MAP
(Section 4.4). The latent trajectories are smooth, but these are
not as smooth as those in Figs. 6 and 7. Notably, the walk
cycles from the four subjects are separated in the latent space.
Random samples from the latent dynamical model tend to
stay near the training data (Fig. 8c), like the other smooth
In contrast to other smooth models, the hyperparameters 
for the two-stage MAP model have a higher SNR of 45.0. One
can see this with the reconstruction uncertainty near the
training data in Fig. 8b. On the other hand, the SNR for the
dynamics kernel parameters  is 18.0, which is lower than
those for the B-GPDM and the model learned with fixed  but
higher than that for the MAP model. Random samples
generated from this model (for example, Fig. 11b) have
smaller variance and produce realistic walking motions.
We should stress here that the placement of the latent
trajectories is not strictly a property of the learning algorithm.
The B-GPDM, for example, sometimes produces separate
walk cycles when applied to other data sets . We have
observed the same behavior when  is fixed at various
settings to encourage smoothness. Conversely, the two-stage
MAP model learned on the golf swing data does not separate
the swings in the latent space (Fig. 9). One conclusion that can
be made about the algorithms is on the smoothness of the
individual trajectories. For thesame data sets,models learned
from MAP tend to have the least smooth trajectories, whereas
models learned from the B-GPDM and fixed  tend to
produce the smoothest trajectories. Models learned from the
two-stage MAP are somewhere in between.
Missing Data
To further examine the models, we consider the task of filling
in missing frames of new data by using the four-walker
models. We take 50 frames of new data, remove 31 frames in
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
FEBRUARY 2008
13. CMU database files: 35_02.amc (frames 55 to 338), 10_04.amc
(frames 222 to 499), 12_01.amc (frames 22 to 328), and 16_15.amc (frames 62
to 342). All are downsampled by a factor of 4.
Kernel Properties for Four-Walker Models
the middle, and attempt to recover the missing frames by
optimizing (22).14 We set yðÞ ¼
for the missing
Table 2 compares the RMS error per frame of each of the
learned models,theresultofoptimizingaK-nearestneighbor
(KNN) least squares objective function on eight test sets,15
and direct cubic spline interpolation in the pose space. Each
erroristheaverageof12experimentsondifferentwindowsof
missing frames (that is, missing frames 5-35, 6-36, . . . , 16-46).
None of the test data was used for training; however, the first
four rows in Table 2 are test motions from the same subjects,
as used to learn the models. The last four rows in Table 2 are
for the test motions of new subjects.
Other than spline interpolation, the unsmooth model
learned from MAP performed the worst on the average. As
expected, the test results on subjects whose motions were
used to learn the models show significantly smaller errors
than for the test motions from subjects not seen in the training
set. None of the models consistently performs well in the
latter case.
The B-GPDM achieved the best average error with
relatively small variability across sequences. One possible
explanation is that models with high variance in the
reconstruction process such as the B-GPDM do a better
job at constraining poses far from the training data. This is
consistent with results from related work that it can be used
as a prior for human tracking , where observations are
typically distinct from the training data.
Nevertheless, the visual quality of the animations do not
necessarily correspond to the RMS error values. The twostage MAP model tends to fill in missing data by pulling the
corresponding latent coordinates close to one of the training
walk cycles. Consequently, the resulting animation contains
noticeable“jumps”whentransitioningfromtheobservedtest
frames to missing test frames, especially for subjects not seen
in thetrainingset. Both theB-GPDM andmodelslearned with
fixed  rely more on the smoothness of the latent trajectories
to fill in the missing frames in latent space. As a result, the
transitions from the observed to missing frames tend to be
smoother in the animation. For subjects not seen in the
training set, the models with hand-specified  tend to place
all of the test data (both observed and missing) far from them
training data in latent space. This amounts to filling in
missing frames only using newly observed frames, which
does not have enough information to produce high-quality
walks. Severe footskate is observed, even in cases with small
RMS errors (such as on data set 07-01).
DISCUSSION AND EXTENSIONS
We have presented the GPDM, a nonparametric model for
high-dimensional dynamical systems that account for uncertainty in the model parameters. The model is applied to 50dimensional motion capture data, and four learning algorithms are investigated. We showed that high-quality
motions can be synthesized from the model without postprocessing, as long as the learned latent trajectories are
reasonablysmooth.Themodeldefinesadensityfunctionover
new motions, which can be used to predict missing frames.
The smoothness of the latent trajectories and the
corresponding inverse variance plots tell us a lot about
the quality of the learned models. With the single-walker
data set, for example, if the learned latent coordinates
define a low-variance tube around the data, then new poses
along the walk cycle (in phases not in the training data) can
be reconstructed. This is not true if the latent space contains
clumps of low-variance regions associated with an unsmooth trajectory. One of the main contributions of the
GPDM is the ability to incorporate a soft smoothness
constraint on the latent space for the family of GPLVMs.
In addition to smoothness, the placement of latent
trajectories is also informative. When trajectories are placed
far apart in the latent space with no low-variance region
between them, little or no structure between the trajectories is
WANG ET AL.: GAUSSIAN PROCESS DYNAMICAL MODELS FOR HUMAN MOTION
Fig. 11. Six walks synthesized by sampling from the predictive distribution, conditioned on a starting point in latent space. (a) Walks generated from
the B-GPDM. (b) Walks generated from the two-stage MAP model. Note the difference in variance.
14. The observed data roughly correspond to 1.5 cycles, of which nearly
one cycle was missing.
15. We tried K ¼ ½3; 6; 9; 15; 20, with 15 giving the lowest average error.
Missing Frames RMS Errors
learned. However, that is not unreasonable, and as observed
in related work , the intratrajectory distance between
poses is often much smaller than the intertrajectory distance.
That is, it may well better reflect the data. The GPDM does not
explicitly constrain the placement of individual trajectories,
and incorporating prior knowledge to enable the modeling of
the intertrajectory structure is an interesting area of future
work. One potential approach is adapting a mixture of GPs
 to the latent variable model framework.
Performance is a major issue in applying GP methods to
larger data sets. Previous approaches prune uninformative
vectors from the training data . This is not straightforward
when learning a GPDM, however, because each time step is
highly correlated with the steps before and after it. For
example, if we hold xt fixed during optimization, then it is
unlikelythattheoptimizerwillmakemuchadjustmenttoxtþ1
or xt1. The use of higher-order features provides a possible
solution to this problem. Specifically, consider a dynamical
model of the form vt ¼ fðxt1; vt1Þ. Since adjacent time steps
are related only by the velocity vt  ðxt  xt1Þ=t, we can
handle irregularly sampled data points by adjusting the time
step t, possibly by using a different t at each step. Another
intriguing approach for speeding up the GPDM learning is
through the use of pseudoinputs , , .
A number of further extensions to the GPDM are
possible. It would be straightforward to include an input
signal ut in the dynamics fðxt; utÞ, which could potentially
be incorporated into the existing frameworks for GPs in
reinforcement learning as a tool for model identification of
system dynamics . The use of a latent space in the
GPDM may be particularly relevant for continuous problems with high-dimensional state-action spaces.
It would also be interesting to improve the MCEM
algorithm used for the two-stage MAP. The algorithm
currently used is only a crude approximation and does not
utilize samples efficiently. Methods such as ascent-based
MCEM can potentially be used to speed up the two-stage
learning algorithm.
For applications in animation, animator constraints
could be specified in pose space to synthesize entire motion
sequences by constrained optimization. Such a system
would be a generalization of the interactive posing
application presented by Grochow et al. . However, the
speed of the optimization would certainly be an issue due to
the dimensionality of the state space.
A more general direction of future work is the learning
and inference of motion models from long highly variable
motion sequences such as a dance score. A latent variable
representation of such sequences must contain a variety of
loops and branches, which the current GPDM cannot learn,
regardless of performance issues. Modeling branching in
latent space requires taking non-Gaussian process noise into
account in the dynamics. Alternatively, one could imagine
building a hierarchical model, where a GPDM is learned on
segment(s) of motion and connected through a higher level
Markov model .
ACKNOWLEDGMENTS
An earlier version of this work appeared in . The authors
would like to thank Neil Lawrence and Raquel Urtsasun for
their comments on the manuscript, and Ryan Schmidt for
assisting in producing the supplemental video, which can be
found at The
volume rendering figures were generated using Joe Conti’s
code on www.mathworks.com. This project is funded in part
by the Alfred P. Sloan Foundation, Canadian Institute for
Advanced Research, Canada Foundation for Innovation,
Microsoft Research, Natural Sciences and Engineering
Research Council (NSERC) Canada, and Ontario Ministry
of Research and Innovation. The data used in this project was
obtained from www.mocap.cs.cmu.edu. The database was
created with funding from the US National Science Foundation Grant EIA-0196217.