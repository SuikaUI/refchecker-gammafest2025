Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
Transformer-XL: Attentive Language Models
Beyond a Fixed-Length Context
Zihang Dai⇤12, Zhilin Yang⇤12, Yiming Yang1, Jaime Carbonell1,
Quoc V. Le2, Ruslan Salakhutdinov1
1Carnegie Mellon University, 2Google Brain
{dzihang,zhiliny,yiming,jgc,rsalakhu}@cs.cmu.edu, 
Transformers have a potential of learning
longer-term dependency, but are limited by a
ﬁxed-length context in the setting of language
We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a ﬁxed length without disrupting temporal coherence.
It consists of a segment-level recurrence mechanism
and a novel positional encoding scheme. Our
method not only enables capturing longer-term
dependency, but also resolves the context fragmentation problem. As a result, Transformer-
XL learns dependency that is 80% longer than
RNNs and 450% longer than vanilla Transformers, achieves better performance on both
short and long sequences, and is up to 1,800+
times faster than vanilla Transformers during
evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103,
21.8 on One Billion Word, and 54.5 on Penn
Treebank (without ﬁnetuning). When trained
only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel
text articles with thousands of tokens.
code, pretrained models, and hyperparameters
are available in both Tensorﬂow and PyTorch1.
Introduction
Language modeling is among the important problems that require modeling long-term dependency,
with successful applications such as unsupervised
pretraining . However, it has been a challenge to equip neural
networks with the capability to model long-term
dependency in sequential data.
Recurrent neural networks (RNNs), in particular Long Short-
⇤Equal contribution. Order determined by swapping the
one in Yang et al. .
1 
transformer-xl
Term Memory (LSTM) networks , have been a standard solution to language modeling and obtained strong
results on multiple benchmarks.
Despite the
wide adaption, RNNs are difﬁcult to optimize
due to gradient vanishing and explosion , and the introduction of gating in LSTMs and the gradient clipping technique might not be sufﬁcient to
fully address this issue.
Empirically, previous
work has found that LSTM language models use
200 context words on average , indicating room for further improvement.
On the other hand, the direct connections between long-distance word pairs baked in attention mechanisms might ease optimization and enable the learning of long-term dependency .
Recently, Al-Rfou et al. designed a set of auxiliary losses to train deep Transformer networks
for character-level language modeling, which outperform LSTMs by a large margin. Despite the
success, the LM training in Al-Rfou et al. 
is performed on separated ﬁxed-length segments
of a few hundred characters, without any information ﬂow across segments. As a consequence of
the ﬁxed context length, the model cannot capture
any longer-term dependency beyond the prede-
ﬁned context length. In addition, the ﬁxed-length
segments are created by selecting a consecutive
chunk of symbols without respecting the sentence
or any other semantic boundary. Hence, the model
lacks necessary contextual information needed to
well predict the ﬁrst few symbols, leading to inef-
ﬁcient optimization and inferior performance. We
refer to this problem as context fragmentation.
To address the aforementioned limitations of
ﬁxed-length contexts, we propose a new architecture called Transformer-XL (meaning extra long).
We introduce the notion of recurrence into our
deep self-attention network. In particular, instead
of computing the hidden states from scratch for
each new segment, we reuse the hidden states obtained in previous segments. The reused hidden
states serve as memory for the current segment,
which builds up a recurrent connection between
the segments. As a result, modeling very longterm dependency becomes possible because information can be propagated through the recurrent connections.
Meanwhile, passing information from the previous segment can also resolve
the problem of context fragmentation. More importantly, we show the necessity of using relative
positional encodings rather than absolute ones, in
order to enable state reuse without causing temporal confusion. Hence, as an additional technical contribution, we introduce a simple but more
effective relative positional encoding formulation
that generalizes to attention lengths longer than the
one observed during training.
Transformer-XL obtained strong results on ﬁve
datasets, varying from word-level to characterlevel language modeling. Transformer-XL is also
able to generate relatively coherent long text articles with thousands of tokens (see Appendix E),
trained on only 100M tokens.
Our main technical contributions include introducing the notion of recurrence in a purely selfattentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone
does not address the issue of ﬁxed-length contexts.
Transformer-XL is the ﬁrst self-attention
model that achieves substantially better results
than RNNs on both character-level and word-level
language modeling.
Related Work
In the last few years, the ﬁeld of language modeling has witnessed many signiﬁcant advances,
including but not limited to devising novel architectures to better encode the context , improving regularization and optimization algorithms , speeding up the Softmax computation , and enriching the output
distribution family .
To capture the long-range context in language
modeling, a line of work directly feeds a representation of the wider context into the network
as an additional input.
Existing works range
from ones where context representations are manually deﬁned to others that rely on
document-level topics learned from data .
More broadly, in generic sequence modeling,
how to capture long-term dependency has been a
long-standing research problem. From this perspective, since the ubiquitous adaption of LSTM,
many efforts have been spent on relieving the
vanishing gradient problem, including better initialization , additional loss signal , augmented memory structure and others that modify the internal architecture of RNNs to ease the optimization . Different
from them, our work is based on the Transformer
architecture and shows that language modeling as
a real-world task beneﬁts from the ability to learn
longer-term dependency.
Given a corpus of tokens x = (x1, . . . , xT ), the
task of language modeling is to estimate the joint
probability P(x), which is often auto-regressively
factorized as P(x) = Q
t P(xt | x<t). With the
factorization, the problem reduces to estimating
each conditional factor. In this work, we stick to
the standard neural approach to modeling the conditional probability. Speciﬁcally, a trainable neural network is used to encode the context x<t into
a ﬁxed size hidden state, which is multiplied with
the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding
a categorical probability distribution over the next
Vanilla Transformer Language Models
In order to apply Transformer or self-attention to
language modeling, the central problem is how to
train a Transformer to effectively encode an arbitrarily long context into a ﬁxed size representation.
Given inﬁnite memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer
decoder, similar to a feed-forward neural network.
However, this is usually infeasible with the limited
resource in practice.
One feasible but crude approximation is to split
the entire corpus into shorter segments of man-
(a) Train phase.
Limited Context
Limited Context
Limited Context
(b) Evaluation phase.
Figure 1: Illustration of the vanilla model with a segment length 4.
ageable sizes, and only train the model within
each segment, ignoring all contextual information
from previous segments. This is the idea adopted
by Al-Rfou et al. . We call it the vanilla
model and visualize it in Fig.
Under this
training paradigm, information never ﬂows across
segments in either the forward or backward pass.
There are two critical limitations of using a ﬁxedlength context. First, the largest possible dependency length is upper bounded by the segment
length, which is a few hundred on character-level
language modeling . Therefore, although the self-attention mechanism is less
affected by the vanishing gradient problem compared to RNNs, the vanilla model is not able to
fully exploit this optimization advantage. Second,
though it is possible to use padding to respect the
sentence or other semantic boundaries, in practice
it has been standard practice to simply chunk long
text into ﬁxed-length segments due to improved
efﬁciency . However, simply chunking
a sequence into ﬁxed-length segments will lead to
the context fragmentation problem as discussed in
Section 1.
During evaluation, at each step, the vanilla
model also consumes a segment of the same length
as in training, but only makes one prediction at the
last position. Then, at the next step, the segment
is shifted to the right by only one position, and the
new segment has to be processed all from scratch.
As shown in Fig. 1b, this procedure ensures that
each prediction utilizes the longest possible context exposed during training, and also relieves context fragmentation issue encountered in training.
However, this evaluation procedure is extremely
expensive. We will show that our proposed architecture is able to substantially improve the evaluation speed.
Segment-Level Recurrence with State
To address the limitations of using a ﬁxed-length
context, we propose to introduce a recurrence
mechanism to the Transformer architecture. During training, the hidden state sequence computed
for the previous segment is ﬁxed and cached to
be reused as an extended context when the model
processes the next new segment, as shown in Fig.
2a. Although the gradient still remains within a
segment, this additional input allows the network
to exploit information in the history, leading to an
ability of modeling longer-term dependency and
avoiding context fragmentation. Formally, let the
two consecutive segments of length L be s⌧=
[x⌧,1, · · · , x⌧,L] and s⌧+1 = [x⌧+1,1, · · · , x⌧+1,L]
respectively. Denoting the n-th layer hidden state
sequence produced for the ⌧-th segment s⌧by
⌧2 RL⇥d, where d is the hidden dimension.
Then, the n-th layer hidden state for segment s⌧+1
is produced (schematically) as follows,
⌧+1 = hn−1
⌧+1 = Transformer-Layer (qn
where the function SG(·) stands for stop-gradient,
the notation [hu ◦hv] indicates the concatenation
of two hidden sequences along the length dimension, and W· denotes model parameters. Compared to the standard Transformer, the critical difference lies in that the key kn
⌧+1 and value vn
are conditioned on the extended context ehn−1
hence hn−1
cached from the previous segment.
We emphasize this particular design by the green
paths in Fig. 2a.
With this recurrence mechanism applied to every two consecutive segments of a corpus, it essentially creates a segment-level recurrence in the
hidden states. As a result, the effective context being utilized can go way beyond just two segments.
However, notice that the recurrent dependency between hn
⌧+1 and hn−1
shifts one layer downwards
New Segment
Fixed (No Grad)
Fixed (No Grad)
New Segment
(a) Training phase.
Extended Context
(b) Evaluation phase.
Figure 2: Illustration of the Transformer-XL model with a segment length 4.
per-segment, which differs from the same-layer
recurrence in conventional RNN-LMs.
Consequently, the largest possible dependency length
grows linearly w.r.t. the number of layers as well
as the segment length, i.e., O(N ⇥L), as visualized by the shaded area in Fig.
is analogous to truncated BPTT , a technique developed for training RNN-
LMs. However, different from truncated BPTT,
our method caches a sequence of hidden states instead of the last one, and should be applied together with the relative positional encoding technique described in Section 3.3.
Besides achieving extra long context and resolving fragmentation, another beneﬁt that comes
with the recurrence scheme is signiﬁcantly faster
evaluation.
Speciﬁcally, during evaluation, the
representations from the previous segments can
be reused instead of being computed from scratch
as in the case of the vanilla model.
In our experiments on enwiki8, Transformer-XL is up to
1,800+ times faster than the vanilla model during
evaluation (see Section 4).
Finally, notice that the recurrence scheme does
not need to be restricted to only the previous segment. In theory, we can cache as many previous
segments as the GPU memory allows, and reuse
all of them as the extra context when processing
the current segment. Thus, we can cache a prede-
ﬁned length-M old hidden states spanning (possibly) multiple segments, and refer to them as the
⌧2 RM⇥d, due to a clear connection to
the memory augmented neural networks . In our experiments, we set M equal to the segment length during training, and increase it by multiple times during evaluation.
Relative Positional Encodings
While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven’t solved in order to reuse the hidden states. That is, how can
we keep the positional information coherent when
we reuse the states? Recall that, in the standard
Transformer, the information of sequence order is
provided by a set of positional encodings, denoted
as U 2 RLmax⇥d, where the i-th row Ui corresponds to the i-th absolute position within a segment and Lmax prescribes the maximum possible
length to be modeled. Then, the actual input to the
Transformer is the element-wise addition of the
word embeddings and the positional encodings. If
we simply adapt this positional encoding to our
recurrence mechanism, the hidden state sequence
would be computed schematically by
h⌧+1 = f(h⌧, Es⌧+1 + U1:L)
h⌧= f(h⌧−1, Es⌧+ U1:L),
where Es⌧2 RL⇥d is the word embedding sequence of s⌧, and f represents a transformation
function. Notice that, both Es⌧and Es⌧+1 are associated with the same positional encoding U1:L.
As a result, the model has no information to distinguish the positional difference between x⌧,j and
x⌧+1,j for any j = 1, . . . , L, resulting in a sheer
performance loss.
In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a
temporal clue or “bias” about how information
should be gathered, i.e., where to attend. For the
same purpose, instead of incorporating bias statically into the initial embedding, one can inject the
same information into the attention score of each
layer. More importantly, it is more intuitive and
generalizable to deﬁne the temporal bias in a relative manner. For instance, when a query vector q⌧,i
attends on the key vectors k⌧,i, it does not need
to know the absolute position of each key vector
to identify the temporal order of the segment. Instead, it sufﬁces to know the relative distance between each key vector k⌧,j and itself q⌧,i, i.e. i−j.
Practically, one can create a set of relative posi-
tional encodings R 2 RLmax⇥d, where the i-th row
Ri indicates a relative distance of i between two
positions. By injecting the relative distance dynamically into the attention score, the query vector
can easily distinguish the representations of x⌧,j
and x⌧+1,j from their different distances, making
the state reuse mechanism feasible. Meanwhile,
we won’t lose any temporal information, as the absolute position can be recovered recursively from
relative distances.
Previously, the idea of relative positional encodings has been explored in the context of machine
translation and music generation . Here, we offer a different derivation, arriving at a new form of relative positional encodings, which not only has a
one-to-one correspondence to its absolute counterpart but also enjoys much better generalization
empirically (see Section 4). Firstly, in the standard
Transformer , the attention
score between query qi and key vector kj within
the same segment can be decomposed as
Following the idea of only relying on relative positional information, we propose to reparameterize the four terms as follows
q Wk,RRi−j
+ u>Wk,EExj
+ v>Wk,RRi−j
• The ﬁrst change we make is to replace all appearances of the absolute positional embedding
Uj for computing key vectors in term (b) and
(d) with its relative counterpart Ri−j. This essentially reﬂects the prior that only the relative
distance matters for where to attend. Note that
R is a sinusoid encoding matrix without learnable parameters.
• Secondly, we introduce a trainable parameter
u 2 Rd to replace the query U>
(c). In this case, since the query vector is the
same for all query positions, it suggests that the
attentive bias towards different words should remain the same regardless of the query position.
With a similar reasoning, a trainable parameter
v 2 Rd is added to substitute U>
• Finally, we deliberately separate the two weight
matrices Wk,E and Wk,R for producing the
content-based key vectors and location-based
key vectors respectively.
Under the new parameterization, each term has
an intuitive meaning: term (a) represents contentbased addressing, term (b) captures a contentdependent positional bias, term (c) governs a
global content bias, and (d) encodes a global positional bias.
In comparison, the formulation in Shaw et al.
 only has terms (a) and (b), dropping the
two bias terms (c) and (d). Moreover, Shaw et al.
 merge the multiplication WkR into a single trainable matrix ˆR, which abandons the inductive bias built into the original sinusoid positional
encoding . In contrast, our
relative positional embedding R adapts the sinusoid formulation.
As a beneﬁt of the inductive
bias, a model trained on a memory of some certain
length can automatically generalize to a memory
several times longer during evaluation.
Equipping the recurrence mechanism with our
proposed relative positional embedding, we ﬁnally
arrive at the Transformer-XL architecture.
completeness, we summarize the computational
procedure for a N-layer Transformer-XL with a
single attention head here. For n = 1, . . . , N:
⌧,i,j = qn
+ u>k⌧,j + v>Wn
⌧= Masked-Softmax(An
⌧= LayerNorm(Linear(an
⌧= Positionwise-Feed-Forward(on
:= Es⌧deﬁned as the word embedding sequence. In addition, it is worth mentioning that a naive way to compute A requires computing Wn
k,RRi−j for all pairs (i, j), whose cost
is quadratic w.r.t.
the sequence length.
However, noticing that the value of i −j only ranges
from zero to the sequence length, we show a simple computation procedure in Appendix B, which
reduces the cost to be linear w.r.t. the sequence
Experiments
Main Results
We apply Transformer-XL to a variety of datasets
on both word-level and character-level language
#Param PPL
Grave et al. - LSTM
Bai et al. - TCN
Dauphin et al. - GCNN-8
Grave et al. - Neural cache
Dauphin et al. - GCNN-14
Merity et al. - QRNN
Rae et al. - Hebbian + Cache
Ours - Transformer-XL Standard
Baevski and Auli - Adaptive Input⇧
Ours - Transformer-XL Large
Table 1: Comparison with state-of-the-art results on
WikiText-103. ⇧indicates contemporary work.
#Param bpc
Ha et al. - LN HyperNetworks
Chung et al. - LN HM-LSTM
Zilly et al. - RHN
Mujika et al. - FS-LSTM-4
Krause et al. - Large mLSTM
Knol - cmix v13
Al-Rfou et al. - 12L Transformer
Ours - 12L Transformer-XL
Al-Rfou et al. - 64L Transformer
Ours - 18L Transformer-XL
Ours - 24L Transformer-XL
Table 2: Comparison with state-of-the-art results on enwik8.
modeling to have a comparison with state-of-theart systems, including WikiText-103 , enwik8 , text8 ,
One Billion Word , and Penn
Treebank .
WikiText-103 is the largest available word-level
language modeling benchmark with long-term dependency. It contains 103M training tokens from
28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling.
the attention length to 384 during training and
1600 during evaluation. We adopted adaptive softmax and input representations . As shown in Table 1,
Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which
demonstrates the superiority of the Transformer-
XL architecture.
The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text.
We compare our architecture with the previous results in Table 2.
Under the model size constraint, the 12-layer
Transformer-XL achieves a new SoTA result, out-
#Param bpc
Cooijmans et al. - BN-LSTM
Chung et al. - LN HM-LSTM
Zilly et al. - RHN
Krause et al. - Large mLSTM
Al-Rfou et al. - 12L Transformer
Al-Rfou et al. - 64L Transformer
Ours - 24L Transformer-XL
Table 3: Comparison with state-of-the-art results on
#Param PPL
Shazeer et al. - Sparse Non-Negative
Chelba et al. - RNN-1024 + 9 Gram
Kuchaiev and Ginsburg - G-LSTM-2
Dauphin et al. - GCNN-14 bottleneck
Jozefowicz et al. - LSTM
Jozefowicz et al. - LSTM + CNN
Shazeer et al. - Low-Budget MoE
Shazeer et al. - High-Budget MoE
Shazeer et al. - Mesh Tensorﬂow
Baevski and Auli - Adaptive Input⇧
Baevski and Auli - Adaptive Input⇧
Ours - Transformer-XL Base
Ours - Transformer-XL Large
Table 4: Comparison with state-of-the-art results on One
Billion Word. ⇧indicates contemporary work.
performing the 12-layer vanilla Transformer from
Al-Rfou et al. by 0.05, while both Transformer variants have a large margin over conventional RNN-based models. Notably, our 12-layer
architecture achieves the same result as the 64layer network from Al-Rfou et al. , using
only 17% of the parameter budget. In order to see
whether better performances can be obtained by
increasing the model size, we train 18-layer and
24-layer Transformer-XLs with increased model
sizes. With the attention length 784 during training and 3,800 during evaluation, we obtained a
new SoTA result and our method is the ﬁrst to
break through 1.0 on widely-studied characterlevel benchmarks. Different from Al-Rfou et al.
 , Transformer-XL does not need any auxiliary losses, and thus all beneﬁts are credited to a
better architecture.
Similar to but different from enwik8, text8 contains 100M processed Wikipedia characters created by lowering case the text and removing any
character other than the 26 letters a through z, and
space. Due to the similarity, we simply adapt the
best model and the same hyper-parameters on enwik8 to text8 without further tuning. The compari-
#Param PPL
Inan et al. - Tied Variational LSTM
Zilly et al. - Variational RHN
Zoph and Le - NAS Cell
Merity et al. - AWD-LSTM
Pham et al. - Efﬁcient NAS
Liu et al. - Differentiable NAS
Yang et al. - AWD-LSTM-MoS
Melis et al. - Dropout tuning
Ours - Transformer-XL
Merity et al. - AWD-LSTM+Finetune†
Yang et al. - MoS+Finetune†
Table 5: Comparison with state-of-the-art results on
Penn Treebank. † indicates using two-step ﬁnetuning.
son with previous methods is summarized in Table
3. Again, Transformer-XL achieves the new SoTA
result with a clear margin.
One Billion Word does not preserve any longterm dependency because sentences have been
shufﬂed. Consequently, this dataset mainly tests
the ability of modeling only short-term dependency. The comparison between Transformer-XL
and the other methods is shown in Table 4. Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7
Speciﬁcally, Transformer-XL signiﬁcantly outperforms a contemporary method using
vanilla Transformers ,
suggesting the advantage of Transformer-XL is
generalizable to modeling short sequences.
We also report the results on word-level Penn
Treebank in Table 5.
Similar to AWD-LSTM
 , we apply variational dropout
and weight average to Transformer-XL. With
proper regularization, Transformer-XL achieves a
new SoTA result among models without two-step
ﬁnetuning. Penn Treebank has only 1M training
tokens, which implies that Transformer-XL also
generalizes well even on small datasets.
Ablation Study
We conduct two sets of ablation studies to examine the effects of two proposed techniques used in
Transformer-XL: the recurrence mechanism and
the new positional encoding scheme.
The ﬁrst study is performed on WikiText-103,
which requires modeling long-term dependency.
The results are reported in Table 6. Among the
compared encoding schemes, Shaw et al. is
relative, while Vaswani et al. and Al-Rfou
et al. are absolute. “Full” and “half” losses
refer to applying a cross entropy loss to all or the
recent half positions in the segment. We found
that absolute encodings only work well with half
losses because half losses exclude positions with
very short attention lengths during training for better generalization.
Table 6 shows that both the
recurrence mechanism and our encoding scheme
are necessary to achieve the best performance, as
well as generalizing to longer attention sequences
during evaluation time. Although the backpropagation length during training is only 128, with
the two techniques the attention length can be increased to 640 at test time. In the standard setting
with 151M parameters, the perplexity decreases as
the attention length increases.
Since the recurrence mechanism costs additional memory, we also compare Transformer-XL
with baselines under the same GPU memory constraints. As shown in Table 10 in Appendix A,
despite using a shorter backpropagation length,
Transformer-XL remains superior to the baselines.
The second study targets at isolating the effects of resolving the context fragmentation problem from the beneﬁt of capturing longer context
length. In order to achieve this goal, we deliberately choose a dataset that does not require longterm dependency, so that any improvement from
establishing the recurrence can be attributed to
solving the context fragmentation.
Speciﬁcally,
we perform this controlled experiment on the One
Billion Word dataset, which can only beneﬁt from
removing the context fragmentation.
a 20-layer Transformer-XL with ⇠0.3B parameters for 400K steps. As shown in Table 7, using
segment-level recurrence substantially improves
performance even when long-term dependency is
not needed, which is consistent with our previous
discussion that the recurrence mechanism resolves
the context fragmentation problem. Moreover, our
relative positional encodings is also superior to
Shaw et al. on short sequences.
Relative Effective Context Length
Khandelwal et al. proposed a method to
evaluate the Effective Context Length (ECL) of a
sequence model.
ECL is the longest length to
which increasing the context span would lead to
a gain more than a threshold. However, ECL ignores the fact that it is harder to get improvement when a model already achieves a lower per-
Recurrence
Transformer-XL (128M)
Shaw et al. 
Shaw et al. 
Shaw et al. 
Vaswani et al. 
Transformer (128M)†
Al-Rfou et al. 
Transformer-XL (151M)
Table 6: Ablation study on WikiText-103. For the ﬁrst two blocks, we use a slightly smaller model (128M parameters). † indicates that the corresponding row is reduced to the same setting as the Transformer network in , except that two auxiliary losses are not implemented in our experiments. “PPL init” refers to using
the same length as training. “PPL best” indicates the perplexity obtained by using the optimal length. “Attn Len”
is the shortest possible attention length during evaluation to achieve the corresponding result (PPL best). Increasing the attention length during evaluation improves performance only when our positional encoding is used. The
“Transformer-XL (151M)” setting uses a standard parameter budget as previous work , where
we observe a similar effect when increasing the attention length during evaluation.
With Shaw et al. encodings
Without recurrence
Table 7: Ablation study on One Billion Word, a dataset
without long-term dependency.
r = 0.1 r = 0.5 r = 1.0
Transformer-XL 151M
Transformer-XL 128M
- use Shaw et al. encoding
- remove recurrence
Transformer
Table 8: Relative effective context length (RECL) comparison. See text for the deﬁnition of RECL and r. The
ﬁrst three models and the last four models are compared as two model groups when we calculate RECL
(RECL is computed on a model group rather than a single model). Each group has the same parameter budget.
plexity using only a shorter context, and thus it
is not suitable for fair comparison among multiple models. We instead propose a new metric
called Relative Effective Context Length (RECL).
RECL is deﬁned on a model group instead of a
single model, and the gain of a long context is
measure by the relative improvement over the best
short context model. As such, the model group
shares the same baseline to enable fair comparison. RECL also has a parameter r, which means
constraining the comparison on top-r hard examples. See Appedix C for more details about RECL.
As shown in Table 8, Transformer-XL manages
to model dependency of 900 words long on average with r = 0.1. The RECL of Transformer-
XL is 80% and 450% longer than recurrent networks and Transformer respectively. Both the recurrence mechanism and our positional encodings
contribute to a longer RECL. This further substantiates our argument that Transformer-XL is able to
model longer-term dependency.
Generated Text
Trained only on WikiText-103 which is mediumsized, Transformer-XL is already able to generate
relatively coherent articles with thousands of tokens without manual cherry picking, despite minor ﬂaws. Please refer to Appendix E for samples.
Evaluation Speed
Finally, we compare the evaluation speed of our
model with the vanilla Transformer model . As shown in Table 9, due to
the state reuse scheme, Transformer-XL achieves
an up to 1,874 times speedup during evaluation.
Conclusions
Transformer-XL obtains strong perplexity results,
models longer-term dependency than RNNs and
Transformer, achieves substantial speedup during
How much Al-Rfou et al. is slower
Table 9: Slowdown in terms of running time during
evaluation. Evaluation is based on per-token time on
evaluation, and is able to generate coherent text
articles. We envision interesting applications of
Transformer-XL in the ﬁelds of text generation,
unsupervised feature learning, image and speech
Acknowledgments
ZD and YY were supported in part by National
Science Foundation (NSF) under the grant IIS-
1546329 and by the DOE-Ofﬁce of Science under the grant ASCR #KJ040201.
were supported in part by the Ofﬁce of Naval
Research grant N000141812861, the NSF grant
IIS1763562, the Nvidia fellowship, and the Siebel
scholarship.