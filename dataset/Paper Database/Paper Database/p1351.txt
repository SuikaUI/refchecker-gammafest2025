Data Engineering for Fraud Detection
Bart Baesensa,b, Sebastiaan H¨oppnerc, Tim Verdonckd,c,∗
aKU Leuven, Faculty of Economics and Business, Naamsestraat 69, Leuven 3000, Belgium
bUniversity of Southampton, School of Management, Highﬁeld Southampton, SO17 1BJ, United Kingdom
cKU Leuven, Department of Mathematics, Celestijnenlaan 200B, Leuven 3001, Belgium
dUniversity of Antwerp, Department of Mathematics, Middelheimlaan 1, Antwerp 2020, Belgium
Financial institutions increasingly rely upon data-driven methods for developing fraud detection
systems, which are able to automatically detect and block fraudulent transactions. From a machine
learning perspective, the task of detecting suspicious transactions is a binary classiﬁcation problem
and therefore many techniques can be applied. Interpretability is however of utmost importance
for the management to have conﬁdence in the model and for designing fraud prevention strategies.
Moreover, models that enable the fraud experts to understand the underlying reasons why a case
is ﬂagged as suspicious will greatly facilitate their job of investigating the suspicious transactions.
Therefore, we propose several data engineering techniques to improve the performance of an analytical
model while retaining the interpretability property. Our data engineering process is decomposed into
several feature and instance engineering steps. We illustrate the improvement in performance of
these data engineering steps for popular analytical models on a real payment transactions data set.
Decision analysis, Payment transactions fraud, Instance engineering, Feature
engineering, Cost-based model evaluation.
1. Introduction
The association of certiﬁed fraud examiners (ACFE) estimates that a typical organization loses
5% of its revenues to fraud each year. The ﬁfth oversight report on card fraud analyses developments
in fraud related to card payment schemes (CPSs) in the Single Euro Payments Area (SEPA), issued
in September 2018 by the European Central Bank and covering almost the entire card market,
indicates that the total value of fraudulent transactions conducted using cards issued within SEPA
and acquired worldwide amounted to 1.8 billion Euros in 2016, which in relative terms, i.e.
∗Corresponding author: Tim Verdonck
Email addresses: (Bart Baesens), (Sebastiaan
H¨oppner), (Tim Verdonck)
 
November 13, 2020
a share of the total value of transactions, amounted to 0.041% in 2016 . These are just a few numbers to indicate the severity of the payment transactions
fraud problem.
It is also seen that losses due to fraudulent activities keep increasing each year
and aﬀect card holders worldwide. Therefore, fraud detection and prevention are more important
than ever before and developing powerful fraud detection systems is of crucial importance to many
organizations and ﬁrms in order to reduce losses by timely blocking, containing and preventing
fraudulent transactions.
The Oxford Dictionary deﬁnes fraud as follows: the crime of cheating somebody in order to get
money or goods illegally. This deﬁnition captures the essence of fraud and covers the many diﬀerent
forms and types of fraud. On the other hand, it does not very precisely describe the nature and
characteristics of fraud and as such does not provide much direction for discussing the requirements
of a fraud detection system.
A more thorough and detailed characterization of the multifaceted
phenomenon of fraud is provided by Van Vlasselaer et al. : Fraud is an uncommon, wellconsidered, imperceptibly concealed, time-evolving and often carefully organized crime which appears
in many types of forms. This deﬁnition highlights ﬁve characteristics that are associated with particular challenges related to developing a fraud detection system.
The ﬁrst emphasized characteristic and associated challenge concerns the fact that fraud is uncommon. Independent of the exact setting or application, only a small minority of the involved
population of cases typically concerns fraud, of which furthermore only a limited number will be
known to be fraudulent. This makes it diﬃcult to both detect fraud, since the fraudulent cases are
covered by the non-fraudulent ones, as well as to learn from historical cases to build a powerful fraud
detection system since only few examples are available. This will make it hard for machine learning
techniques to extract meaningful patterns from the data.
Fraud is also imperceptibly concealed since fraudsters exactly try to blend into their environments
to remain unnoticed. This relates to the subtlety of fraud since fraudsters try to imitate normal behavior. Moreover, fraud is well-considered and intentional and complex fraud structures are carefully
planned upfront. Fraudsters can also adapt or reﬁne their tactics whenever needed, for example, due
to changing fraud detection mechanisms. Therefore, fraud detection systems need to improve and
learn by example.
The traditional approach to fraud detection is expert-driven, which builds on the experience,
intuition, and business or domain knowledge of one or more fraud investigators. Such expert-based
rule base or engine is typically hard to build and maintain.
A shift is occurring toward data-
driven or machine learning based fraud detection methodologies.
This shift is triggered by the
digitization of almost every aspect of society and daily life, which leads to an abundance of available
data. Financial institutions increasingly rely upon data-driven methods for developing powerful fraud
detection systems, which are able to automatically detect and block fraudulent transactions. In other
words, we need adaptive analytical models to complement experience-based approaches for ﬁghting
fraud. A stream of literature has reported upon the adoption of data-driven aproaches for developing
fraud detection systems . These methods signiﬁcantly improve
the eﬃciency of fraud detection systems and are easier to maintain and more objective. From a
machine learning perspective, the task of detecting fraudulent transactions is a binary classiﬁcation
A natural ﬁrst step to move from expert-based approaches to data driven techniques (while still
taking into account the experience of the fraud experts) is to consider logistic regression and/or
decision trees. These simple analytical models can then be replaced by complex techniques such as
random forests and boosting methods, support vector machines, neural networks and deep learning
to increase the detection power. Although the latter are deﬁnitely powerful analytical techniques,
they suﬀer a very important drawback which is not desirable from a fraud prevention perspective:
they are black box models which means that they are very complex to interpret. We would also like
to note that these complex models not always signiﬁcantly outperform simple analytical models such
as logistic regression and we strongly believe that you
should always start with implementing these simple techniques. Many benchmarking studies have
illustrated that complex analytical techniques only provide marginal performance gains on structured,
tabular data sets as frequently encountered in common classiﬁcation tasks such as fraud detection,
credit scoring and marketing analytics . It is our ﬁrm
belief that in order to improve the performance of any analytical model, we should focus more on
the data itself rather than developing new, complex predictive analytical techniques. This is exactly
the aim of data engineering. It can be deﬁned as the clever engineering of data hereby exploiting the
bias of the analytical technique to our beneﬁt, both in terms of accuracy and interpretability at the
same time. Often times it will be applied in combination with simple analytical techniques such as
linear or logistic regression so as to maintain the interpretability property which is so often needed in
analytical modeling. In our context of fraud analytics, interpretability is of key importance to design
smart fraud prevention mechanisms. Data engineering can be decomposed into feature engineering
and instance engineering. Feature engineering aims at designing smart features in one of two possible
ways: [either by transforming existing features using smart transformations or by designing better,
new features which will allow a simple analytical technique such as linear or logistic regression to
boost its performance, or by designing new features (a process often called featurization) to basically
achieve the same aim.] Instance engineering entails the careful selection of instances or observations
again with the aim to improve predictive modeling performance. Put diﬀerently, it aims at selecting
those observations which positively contribute to the learning of the analytical technique and remove
those that have a detrimental impact on it. Obviously, this is not a trivial exercise and many instance
engineering techniques have been developed which we will carefully study and experiment with in this
paper. In this paper the focus will be on successful data engineering steps to improve the performance
of a fraud detection model. More concretely, we will describe the lessons that we have learnt when
complementing expert-based approaches with machine learning or data-driven techniques to combat
payment transactions fraud for a large European bank.
This paper is organized as follows. We start with presenting our data engineering process: Section
2 presents feature engineering steps whereas instance engineering is explained in Section 3. In Section
4 popular performance measures in an (imbalanced) classiﬁcation setting are described. In Section 5,
more information about payment transaction fraud and the observed data set is given. This section
also illustrates the beneﬁts of the various data engineering steps by showing increased performance
on our real data set. Finally, concluding remarks and potential directions for future research are
provided in Section 6.
2. Feature engineering
The main objective of machine learning is to extract patterns to turn data into knowledge. Since
the beginning of this century, technological advances have drastically changed the size of data sets
as well as the speed with which these data must be analyzed. Modern data sets may have a huge
number of instances, a very large number of features, or both.
In most applications, data sets
are compiled by combining data from diﬀerent sources and databases (containing both structured
and unstructured data) where each source of information has its strengths and weaknesses. Before
applying any machine learning algorithm, it is therefore necessary to transform these raw data sources
into interesting features that better help the predictive models. This essential step, which is often
denoted feature engineering, is of utmost importance in the machine learning process. We believe
that data scientists should be well aware of the power of feature engineering and that they should
share good practices.
An important set of interesting features can be created based on the famous Recency, Frequency,
Monetary (RFM) principle. Recency measures how long ago a certain event took place, whereas
frequency counts the number speciﬁc events per unit of time.
Besides recency features, we also
present several other time-related features. Features related to monetary value measure the intensity
of a transaction, typically expressed in a currency such as Euros or USD. We also introduce features
based on unsupervised anomaly detection and brieﬂy discuss some other advanced feature engineering
techniques.
2.1. Frequency features
We explain the idea behind the RFM principle by ﬁrst deriving frequency features using a transaction aggregation strategy in order to capture a customer’s spending behavior. This methodology
was ﬁrst proposed by Whitrow et al. and has been used by a number of studies . Frequency calculates
how many transactions were made during a sliding time window that satisﬁes predeﬁned conditions,
as illustrated in Figure 1. The ﬁrst step in creating frequency features consists in aggregating the
transactions made during the last given time period (e.g. last 3 months), ﬁrst by card or account
number, then by payment channel, authentication method, beneﬁciary country or other, followed
by counting the number of transactions. It is important to choose an appropriate time period over
which to aggregate a customer’s transactions. When time passes, the spending patterns of a customer are not expected to remain constant over the years. For transactions made with debit cards,
we propose to use a ﬁxed time frame of 90, 120 or 180 days (∼3, 4 or 6 months). Let D denote a set
of N transactions where each transaction is represented by the pair (xi, yi) for i = 1, 2, . . . , N. Here
yi ∈{0, 1} describes the true class of transfer i and xi =
i , . . . , xp
represents the p associated
features of transfer i. Bahnsen et al. describe the process of creating frequency features as
selecting those transactions that were made in the previous tp days, for each transaction i in the data
sliding time window of !! days
timestamp of a
transaction
Figure 1: Timeline of transactions of a customer using, for example, a particular payment channel.
tp,i = AGGfreq (D, i, tp)
where AGG(·) is a function that aggregates transactions of D into a subset associated with a transaction i with respect to the time frame tp; xtime
is the timestamp of transaction i; xamt
is the amount
of transaction i; xid
is the customer or card identiﬁcation number of transaction i; and days(t1, t2)
is a function that calculates the number of days between the times t1 and t2. Finally, the frequency
feature is calculated as
where |·| is the cardinality of a set. This aggregation strategy, however, does not take the combination
of diﬀerent features into account. For example, we can aggregate transactions according to certain
criteria, such as: transactions made in the last tp days using the same authentication method (e.g.
pin code or ﬁngerprint) and the same payment channel (e.g. online banking or mobile app). For
calculating such features, Bahnsen et al. expand (1) as follows
=AGGfreq (D, i, tp, cond1, cond2)
where cond1 and cond2 could be one of the features of a transaction (e.g. authentication method,
payment channel, beneﬁciary country, etc.). Similarly, the frequency feature is then calculated as
One could also deﬁne new features as the ratio of frequency features. For example,
which is always between 0 and 1. Since xratio
is the fraction of of transfers for which conditions cond1
and cond2 hold over all transactions in the past tp days, this feature represents the probability that
both conditions cond1 and cond2 are met by the customer.
We show an example to further clarify how the frequency features are calculated. Consider a set
of transactions made by a customer between 01/07/2019 and 03/07/2019, as shown in Table 1. Then
we estimate the frequency features xfreq
and xfreq2
by setting tp = 1 day (∼24 hours) for ease of
calculation.
The frequency features give us speciﬁc details about the spending behavior of the customer.
For example, if a customer frequently used a particular payment channel in the past tp days, its
frequency is obviously large. However, a zero frequency for a particular payment channel implies
that the customer has not used that payment channel in the past tp days which indicates anomalous
behavior and perhaps fraud. The total number of frequency features can grow quite quickly, as tp can
have several values, and the combination of criteria can be quite large as well. For the experiments we
set the diﬀerent values of tp to 90, 120 and 180 days. Then we calculate the frequency features using
(2) and (4) as well as (5) with the aggregation criteria including payment channel, authentication
method, beneﬁciary country, type of communication, and others.
Initial features
Frequency features
TransId CustId
Authentication
01/07/2019 16:51
01/07/2019 19:04
01/07/2019 19:36
ﬁngerprint
01/07/2019 23:31
02/07/2019 17:48
ﬁngerprint
02/07/2019 22:12
ﬁngerprint
02/07/2019 23:34
ﬁngerprint
03/07/2019 01:40
Table 1: Example calculation of frequency features: xfreq
is the number of transactions in the last 24 hours, and
is the number of transactions with the same authentication method and payment channel in the last 24 hours.
2.2. Recency features
Although frequency features are powerful in describing a customer’s spending behavior, they do
not take the aspect of time into account. Recency features are a way to capture this information.
Recency measures the time passed since the previous transaction that satisfy predeﬁned conditions.
To explain how recency features are deﬁned we show an example where we create a recency feature
derived from the authentication method used by the customer as illustrated in Figure 2.
a customer makes a transfer xi, she chooses a method xAU
to authenticate herself. Examples of
authentication methods are passwords, pin codes, ﬁngerprints, itsme1, iris scans and hardware tokens.
1This is a popular app in Belgium that allows you to safely, easily and reliably conﬁrm your (digital) identity and
approve transactions.
Figure 2: Example of a recency feature derived from the authentication method used by a customer. When the customer
makes a transaction, she chooses one of ﬁve possible authentication methods which are labeled as AU01, AU02, ...,
AU05. If the time between the same two successive authentication methods is long, the recency is close to zero, while
if that time is short, the recency is close to 1. If an authentication method is used for the ﬁrst time, its recency is
deﬁned as zero.
For each transaction i in the data set D, we deﬁne the recency of the transaction’s authentication
xAU,recency
= exp (−γ · ∆ti) where
Here ∆ti is the time interval, typically in days, between two consecutive transfers made by the same
customer with identiﬁcation number xid
i using the same authentication method xAU
. The parameter
γ can be chosen such that, for example, the recency is small (e.g. 0.01) when ∆t = 180 days (∼6
months) in which case γ = −log(0.01)/180 = 0.026. Notice that recency is always a number between
0 and 1. When the time period ∆t between two consecutive transfers with the same authentication
method is small (large), we say that the authentication method has (not) recently been used. In
that case the recency for this authentication method is close to one (zero). When an authentication
method is used for the ﬁrst time, we deﬁne its recency to be zero. A zero or small recency shows
atypical behavior and might indicate fraud. Figure 3 shows that recency indeed decreases when the
time interval becomes larger. The parameter γ determines how fast the recency decreases. For larger
values of γ, recency will decrease quicker with time and vice versa.
Figure 3: Recency versus time (in days) for diﬀerent values of γ.
2.3. Other time-related features
It is well-known that time is an important aspect in fraud detection. Besides recency features other
time-related features can be created based on the assumption that certain events, like a customer
who makes transactions, occur at similar moments in time. Having a transaction at 22:00 might
be very regular for one person, but very suspicious for another person. Since, for every customer,
we know the timestamps of all their transactions in the past, we can use this information to decide
whether a new transaction at 22:00 is atypical for a particular customer. For the set of timestamps
of transactions made by a each customer we can construct a circular histogram, as shown in Figure
4 (left). Since 00:00 is the same as 24:00, we have to model the time of a transaction as a periodic
variable by ﬁtting an appropriate statistical distribution . A popular choice is
the von Mises distribution, also known as the periodic normal distribution because it represents a
normal distribution wrapped around a circle . The von Mises distribution of a set of
timestamps Dtime = {t1, t2, . . . , tN} is deﬁned as
Dtime ∼von Mises (µ, κ)
where parameters µ and 1/κ represent the periodic mean and the periodic standard deviation, respectively. These parameters can easily be estimated by most statistical software. We use the function
mle.vonmises from the R package circular to compute the maximum likelihood estimates for the
parameters of a von Mises distribution.
For each customer we construct a conﬁdence interval for the time of a transaction. First, we
select the set of transactions made by the same customer in the last tp days,
tp,i = AGGtime (D, i, tp)
Based on this set of selected timestamps, the estimated parameters ˆµ and ˆκ are calculated. Next, a
von Mises distribution is ﬁtted on the set of timestamps using these estimates:
∼von Mises
Once the von Mises distribution is ﬁtted on the timestamps of the customer’s transactions we can
construct a conﬁdence interval with probability α, e.g. 80%, 90%, 95%. An example is presented in
Figure 4 (right). Using the conﬁdence interval, a binary feature is created: a transaction is ﬂagged as
normal or suspicious depending on whether or not the time of the transaction is within the conﬁdence
interval. Table 2 shows an example of a binary feature that takes the value of one if the current
time of the transaction is within the conﬁdence interval of the time of the previous transactions with
a conﬁdence of α = 0.9. Of course, multiple of these binary features can be extracted for diﬀerent
values of α and time period tp. The new feature also helps to get a better understanding of when
a customer is expected to make transactions. Note that this feature (just as many others) solely
indicates atypical behavior for a customer, which might give an indication for fraud. If a certain
Figure 4: (Left) Circular histogram of timestamps of transactions. The dashed line is the estimated periodic mean of
the von Mises distribution. (Right) Circular histogram including the 90% conﬁdence interval (orange area).
transaction is ﬂagged as potentially fraudulent due to this feature, then it is important that this
information is also given to the fraud investigators. If they see that the customer is abroad, then
that could be the reason for the atypical value of this feature.
Instead of looking at the timestamp of a transaction within a day, we can of course create
similar features indicating how atypical it is for a customer to have a payment on a certain day
or above a certain amount.
Some customers, for example, may only do transactions during the
weekend. Adding such features based on customer spending history may bring signiﬁcant increase in
model performance. Most predictive models let you also easily evaluate which features increased the
performance of your model and which are not signiﬁcant for discriminating frauds from non-frauds.
Periodic mean Conﬁdence interval Binary feature
01/07/2019 16:51
01/07/2019 19:04
01/07/2019 19:36
16:07 - 19:48
01/07/2019 23:31
16:32 - 20:29
02/07/2019 17:48
15:39 - 23:40
02/07/2019 22:12
15:27 - 23:01
02/07/2019 23:34
15:52 - 23:42
03/07/2019 01:40
16:05 - 00:38
Table 2: Example calculation of a binary feature that informs whenever a transaction is being made within the
conﬁdence interval (with α = 0.9) of the time of the previous transactions.
2.4. Monetary value related features
The last pillar of the RFM principle involves monetary value related features which focus on the
amount that is transferred. Monetary features calculate various statistics such as the total value,
the average, and the standard deviation of the transferred amounts that were pursued during the
sliding time window of !! days
timestamp of a
transaction
Figure 5: Timeline of amounts transferred by a customer using, for example, a particular payment channel.
sliding time window that satisfy predeﬁned conditions (Figure 5). The ﬁrst step in creating monetary
features is the same as with frequency features: select those transactions that were made in the last
tp days, as in (1). Next, we can calculate the total amount spent on those transactions,
where I(·) is the indicator function.
Of course, we can also aggregate transactions according to
certain criteria, as in (3), followed by calculating their sum,
Transferring 500 Euros may be little for one person, but a lot for another person. A monetary feature
that calculates the so-called z-score of an amount can indicate whether the amount is atypical for a
particular customer. For a set of amounts Dfreq
tp,i , the standardized values or z-scores are deﬁned as
where ˆµD and ˆσD are the sample mean and sample standard deviation, respectively,
ˆµD = Mean
and ˆσD = Stdev
As a rule of thumb, an amount is ﬂagged as an outlier if its z-score is larger than 3, |zi| > 3. Now
consider the transactions made by a customer, as shown in Figure 6. The last amount of 500 Euros
is clearly an outlier compared to the previous amounts. However, when using the sample mean and
sample standard deviation, the z-score of the atypically high amount is only 2.66 and is therefore not
regarded as abnormal.
Instead of computing the z-score using traditional estimates such as sample mean and sample
standard deviation, we propose using robust alternatives such as the median and the median absolute
deviation (MAD),
D = Median
MAD ({x1, x2, . . . , xn}) = 1.4826 · Median
xi −Median({xj}n
The constant scale factor 1.4826 ensures that the MAD is a consistent estimator for the estimation
of the standard deviation σ, i.e. E [MAD ({X1, X2, . . . , Xn})] = σ for Xj distributed as N
and large n. Using the robust estimates, the z-score of the last amount in Figure 6 is 5.79, which
clearly indicates that the 500 Euros is atypical for this customer.
Remark: transferred amounts are often right-skewed as shown in Figure 7 (left).
of thumb, i.e. |zi| > 3, implicitly assumes that the z-scores are distributed as N
standardizing the amounts, a transformation is often applied to them that changes their distribution to one that resembles a normal distribution, or at least a symmetric distribution. One such
transformation is the natural logarithm, as shown in Figure 7 (right).
Figure 6: An example of transferred amounts. The last amount of 500 Euros is clearly an outlier compared to the
previous amounts. The atypical high amount is not indicated when using traditional estimates such as sample mean
and sample standard deviation. Instead, we have to use robust estimates such as the median and the median absolute
deviation (MAD).
Figure 7: Histogram and kernel density estimate of amounts (left) and natural logarithm of those amounts (right).
A popular alternative for computing (robust) z-scores is the boxplot, which is a very popular
graphical tool to analyze a univariate data set . The boxplot marks all observations
outside the interval [Q11.5IQR; Q3 + 1.5IQR] as potential outliers, where Q1, Q2 and Q3 denote respectively the ﬁrst, second (or median) and third quartile and IQR = Q3−Q1 equals the interquartile
range. It is known that the boxplot typically ﬂags too many points as outlying when the data are
skewed and therefore Hubert and Vandervieren have modiﬁed the boxplot interval so that the
skewness is suﬃciently taken into account.
In practice one often tries to detect outliers using diagnostics starting from a classical or traditional ﬁtting method. Unfortunately, these traditional techniques can be aﬀected by outliers so
strongly that the resulting ﬁtted model may not allow to detect the deviating observations. This is
called the masking eﬀect ). Additionally, some good data points
might even appear to be outliers, which is known as swamping . To avoid
these eﬀects, the goal of robust statistics is to ﬁnd a ﬁt which is close to the ﬁt we would have found
without the outliers. We can then automatically identify the outliers by their large ‘deviation’ (e.g.,
their distance or residual) from that robust ﬁt. It is not our aim to replace traditional techniques by
a robust alternative, but we have illustrated that robust methods can give you extra insights in the
data and may improve the reliability and accuracy of your analysis.
2.5. Features based on (unsupervised) anomaly detection techniques
In this section we focus on unsupervised techniques that do not use the target variable (fraudulent
or not). Anomaly detection techniques ﬂag anomalies or outliers, which are observations that deviate
from the pattern of the majority of the data. These ﬂagged observations indicate atypical behavior
and hence may contain crucial information for fraud detection and should be investigated by the fraud
expert. As an alternative, we propose to use the outlyingness score or metric of several anomaly
detection techniques as features that we add to our data set.
Anomalies in a single dimension (i.e. univariate outliers) can be detected by computing (robust)
z-scores (and see which observations are in absolute value larger than 3) or by constructing the
(adjusted) boxplot (and see which observations are outside the boxplot interval or fence). Another
tool for univariate anomaly detection that is also popular in fraud detection is Newcomb-Benford law,
which makes predictions about the distribution of the ﬁrst leading digit of all numbers . These techniques can then be applied on each feature in the data set. However,
in this way it is only possible to detect anomalies that are atypical in (at least) one dimension or
feature of our data set. Since fraudsters succeed very well in blending in with legitimate customers,
they are typically not detected by checking each feature separately. It is important to ﬂag those
observations that deviate in several dimensions from the main data structure but are not atypical in
one of the features. Such multivariate outliers can only be detected in the multidimensional space
and require the use of advanced models.
A ﬁrst tool for this purpose is robust statistics, which ﬁrst ﬁts the majority of the data and
then ﬂags the observations that deviate from this robust ﬁt . For
a multivariate n × p data set X , one can calculate the robust Mahalanobis distance (or robust
generalized distance) for each observation xi:
MD(xi, ˆµ, ˆΣ) =
(x −ˆµ)T ˆΣ
−1(x −ˆµ).
An observation is then ﬂagged as anomaly if its distance exceeds the cut-oﬀvalue
p,0.975, which
is the 0.975 quantile of the chi-squared distribution with p degrees of freedom.
It is of utmost
importance that robust estimates of multivariate location and scatter are used in the computation of
the distances (to avoid masking and swamping eﬀects). A popular method yielding such estimates
is the Mininimum Covariance Determinant (MCD) method of Rousseeuw and Driessen or the
Minimum Regularized Covariance Determinant (MRCD) estimator of Boudt et al. in case of
high-dimensional data. Note that also various robust alternatives for popular predictive models are
proposed in literature. These robust supervised techniques automatically ﬂag anomalies (typically
with a convenient graphical tool to visualize the anomalies). Therefore it is interesting to also apply
robust versions of the predictive models on the data and carefully examine the anomalies ﬂagged
with these techniques ; Heritier et al. ;
Atkinson and Riani ). Recently, Rousseeuw et al. also used robust statistics to detect
potential fraud cases in time series of imports into the European Union.
Besides robust statistics, many other unsupervised anomaly detection tools from various research
ﬁelds have been proposed . We brieﬂy introduce and illustrate three
popular techniques: k-nearest neighbors distance ,
local outlier factor (LOF) and isolation forests .
The knearest neighbors distance for an observation is the average distance to each of its k closest neighbors.
This distance measures how isolated an observation is from its neighbors and hence a large distance
typically indicates an anomaly. The LOF score is the average density around the k nearest neighbors
divided by the density around the observation itself and anomalies typically have a score above
one. Isolation forest is obtained by taking an ensemble of isolation trees which try to isolate each
observation as quickly as possible. The ﬁnal score is the average of the standardized path length (i.e.
number of splits to isolate the observation) over all trees. Hence for all the methods above it holds:
the higher the score or metric, the more suspicious is the observation.
2.6. Other feature engineering techniques
In this paper, we only study a few feature engineering techniques to illustrate their importance as
a key data engineering mechanism. Other powerful feature engineering techniques are the Box-Cox
and Yeo-Johnson transformation which both univariately transform data variables so as to boost
the performance of the predictive analytical model. Note that these transformation techniques are
sensitive to outliers and will try to move outliers inward at the expense of the normality of the
central part of the data. Therefore various robust transformation procedures have been proposed
in literature ; Riani ; Marazzi et al. ; Raymaekers
and Rousseeuw ). Feature engineering techniques have also been designed for unstructured
data such as text, network data, and multimedia data (e.g., images, audio, videos). For text data,
one commonly uses Singular Value Decomposition (SVD) or Natural Language Processing (NLP) as
feature engineering techniques. For network data, node2vec and GraphSage have proven to be very valuable techniques. Deep learning has been used
to learn complex features for multimedia data. As an example, convolutional neural networks can
learn key features to describe objects in images. However, an important caveat is that many of these
features are black box in nature and thus hard to interpret for business decision makers. Finally,
tailored feature engineering techniques have been designed for speciﬁc domains, e.g., Item2Vec in
Recommender Systems .
3. Instance engineering
A major challenge in fraud analytics is the imbalance or skewness of the data, meaning that
typically there are plenty of historical examples of non-fraudulent cases, but only a limited number of
fraudulent cases. For example, in a credit card fraud setting, typically less than 0.5% of transactions
are fraudulent. Such a problem is commonly referred to as the needle in a haystack problem, and
might cause an analytical technique to experience diﬃculties in learning to create an accurate model.
Every classiﬁer faced with a skewed data set typically tends to favor the majority class. In other
words, the classiﬁer tends to label all transactions as non-fraudulent since it then already achieves
a classiﬁcation accuracy of more than 99%. Classiﬁers typically learn better from a more balanced
distribution. Two popular ways to accomplish this is by undersampling, whereby non-fraudulent
transactions in the training set are removed, or oversampling, whereby fraudulent transactions in the
training set are replicated.
A practical question concerns the optimal, non-fraud/fraud odds, which should be the goal by
doing under- or oversampling. This of course depends on the data characteristics and quality and
type of classiﬁer. Although train and error is commonly adopted to determine this optimal odds,
the ratio 90% non-fraudsters versus 10% fraudsters is usually already suﬃcient for most business
applications.
The Synthetic Minority Oversampling technique, or SMOTE, is another interesting approach
to deal with skewed class distributions .
In SMOTE, the minority class is
oversampled by adding synthetic observations.
The creation of these artiﬁcal fraudsters goes as
follows. In Step 1 of SMOTE, for each minority class observation, the k nearest neighbors (of same
class) are determined. Step 2 then randomly selects one of the neighbors and generates synthetic
observations as follows: 1) take the diﬀerence between the features of the current minority sample
and those of its nearest neighbor. 2) multiply this diﬀerence with a random number between 0 and
1 and 3) add the obtained result as new observation to the sample, hereby increasing the frequency
of the minority class.
The key idea of these undersampling and oversampling techniques is to adjust the class priors to
enable the analytical technique to create a meaningful model that discriminates the fraudsters from
the non-fraudsters. By doing so, the class posteriors become biased. This is not a problem if the
fraud analyst is interested in ranking the observations in terms of their fraud risks. However, if wellcalibrated fraud probabilities are needed, then the posterior probabilities can be adjusted .
Since its introduction in 2002, many variants of SMOTE have been proposed in literature and Kov´acs for an overview). In this paper, we visually show the diﬀerences
between ADASYN , MWMOTE and ROSE and show their performance on our data set. We refer to their papers for details. It is clear
that there is not one oversampling technique that always yield the best result .
4. Measuring performance
The aim of detecting transfer fraud is to identify transactions with a high probability of being
fraudulent. From the perspective of machine learning, the task of predicting the fraudulent nature of
transactions can be presented as a binary classiﬁcation problem where observations (i.e. transactions,
customers, etc.) belong either to class 0 or to class 1. We follow the convention that the fraudulent
observations belong to class 1, whereas the legitimate observations correspond to class 0. We often
speak of positive (class 1) and negative (class 0) observations.
Consider again our set D = {(xi, yi)}N
i=1 of N transactions. In general, a classiﬁcation algorithm
provides a continuous score si := s(xi) ∈ for each transaction i. This score si is a function
of the observed features xi of transaction i and represents the fraud propensity of that transaction.
Here we assume that legitimate transfers (class 0) have a lower score than fraudulent ones (class 1).
The score si is then converted to a predicted class ˆyi ∈{0, 1} by comparing it with a classiﬁcation
threshold t ∈ . If a transfer’s probability of being fraudulent as estimated by the classiﬁcation
model lies above this threshold value, then the transfer is predicted as fraud (si > t ⇒ˆyi = 1), and
otherwise it is classiﬁed as legitimate (si ≤t ⇒ˆyi = 0).
A classiﬁcation exercise typically leads to a confusion matrix as shown in Table 3. Based on
the confusion matrix, we can compute several performance measure such as Precision, Recall (also
Original data
Figure 8: Illustration of SMOTE, ADASYN, MWMOTE and ROSE. The blue circles represent the legitimate cases,
the black squares are the original fraud cases, and the red dots are the synthetic fraud cases.
Actual legitimate
Actual fraudulent
(negative) y = 0
(positive) y = 1
Predicted as legitimate
True negative
False negative
(negative) ˆy = 0
Predicted as fraudulent
False positive
True positive
(positive) ˆy = 1
Table 3: Confusion matrix of a binary classiﬁcation task.
called True Positive Rate, Sensitivity or Hit Rate), False Positive Rate, and F1-measure. Each of
these measures are calculated for a given confusion matrix that is based on a certain threshold value
t ∈ .
The receiver operating characteristic (ROC) curve, as shown on the left plot in Figure 9, is
probably the most popular method to analyze the eﬀectiveness of a classiﬁer. The ROC curve is
obtained by plotting for each possible threshold value the false positive rate (FPR) on the X-axis
and the true positive rate (TPR) on the Y -axis. As a graphical tool the ROC curve visualizes the
tradeoﬀbetween achieving a high recall (TPR) while maintaining a low false positive rate (FPR),
and is often used to ﬁnd an appropriate decision threshold. Provost et al. argue that ROC
curves, as an alternative to accuracy estimation for comparing classiﬁers, would enable stronger and
more general conclusions. For more information about ROC curves we refer to Krzanowski and Hand
 and Swets .
Comparing classiﬁers based solely on their ROC curves can be challenging. Therefore, the ROC
area = 0.963
False Positive Rate
True Positive Rate
area = 0.756
Figure 9: (Left) example of a ROC curve. (Right) example of a Precision-Recall curve. Both curves are based on the
same classiﬁer validated on the same data set.
curve is often summarized in a single score, namely the Area Under the ROC Curve (AUC) which
varies between 0 and 1 . In the context of fraud detection, the
AUC of a classiﬁer can be interpreted as being the probability that a randomly chosen fraud case is
predicted a higher score than a randomly chosen legitimate case. Therefore, a higher AUC indicates
superior classiﬁcation performance. A perfect classiﬁer would achieve an AUC of 1 while a random
model (i.e. no prediction power) would yield an AUC of 0.5.
When dealing with highly imbalanced data as is the case with fraud detection, AUC (and ROC
curves) may be too optimistic and the Area under the Precision-Recall Curve (AUPRC) gives a more
informative picture of a classiﬁer’s performance . As the name suggest, the Precision-Recall curve (right plot in Figure
9) plots the precision (Y-axis) against the recall (X-axis) or each possible threshold. The AUPRC is
therefore also a value between 0 and 1. Both ROC and PR curves use the recall, but the ROC curve
also plots the FPR whereas PR curves focus on precision. In the denominator of FPR, one sums
the number of true negatives and false positives. In highly imbalanced data, the number of negatives
(legitimate observations) is much larger than the number of positives (fraudulent observations) and
hence the number of true negatives is typically very high compared to the number of false positives.
Therefore, a large increase or decrease in the number of false positives will have almost no impact on
FPR in the ROC curves. Precision, on the other hand, compares the number of true positives to the
number of false positives and hence copes better with the imbalance between positive and negative
observations. Since precision is more sensitive to class imbalance, the area under the Precision-Recall
curve (AUPRC) is better to highlight diﬀerences between models for highly imbalanced data sets.
Despite the many ways to evaluate a classiﬁcation model’s performance we argue that the true
business objective of a fraud detection system is to minimize the ﬁnancial losses due to fraud. How-
Actual legitimate Actual fraudulent
(negative) yi = 0
(positive) yi = 1
Predicted as legitimate
True negative
False negative
(negative) ˆyi = 0
[Ci(0|0) = 0]
[Ci(0|1) = Ai]
Predicted as fraudulent
False positive
True positive
(positive) ˆyi = 1
[Ci(1|0) = cf]
[Ci(1|1) = cf]
Table 4: Cost matrix where, between square brackets, the related instance-dependent classiﬁcation costs for transfer
fraud are given.
ever, the performance measures mentioned so far do not incorporate any costs related to incorrect
predictions such as not detecting a fraudulent transaction. Therefore, they may not be the most
appropriate evaluation criteria when evaluating fraud detection models. In fact, the previous performance measures tacitly assume that misclassiﬁcation errors carry the same cost, similarly with the
correctly classiﬁed transactions. This assumption clearly does not hold in practice because wrongly
predicting a fraudulent transaction as legitimate carries a signiﬁcantly diﬀerent ﬁnancial cost than
the inverse case. To better align the assessment of data-driven fraud detection systems with the
actual objective of decreasing losses due to fraud, we extend the confusion matrix in Table 3 by
incorporating costs as proposed in . Let Ci(ˆy|y) be the cost of predicting class
ˆy for a transfer i when the true class is y. If ˆy = y then the prediction is correct, while if ˆy ̸= y the
prediction is incorrect. In general, the costs can be diﬀerent for each of the four cells in the confusion matrix and can even be instance-dependent, in other words, speciﬁc to each transaction i as
indicated in Table 4. Hand et al. proposed a cost matrix, where in the case of a false positive
(i.e. incorrectly predicting a transaction as fraudulent) the associated cost is the administrative cost
Ci(1|0) = cf. This ﬁxed cost cf has to do with investigating the transaction and contacting the card
holder. When detecting a fraudulent transfer, the same cost Ci(1|1) is allocated to a true positive,
because in this situation, the card owner will still need to be contacted. In other words, the action
undertaken by the company towards an individual transaction i comes at a ﬁxed cost cf ≥0, regardless of the nature of the transaction. However, in the case of a false negative, in which a fraudulent
transfer is not detected, the cost is deﬁned as the amount Ci(0|1) = Ai of the transaction i. The
instance-dependent costs are summarized in Table 4. We argue that the cost matrix in Table 4 is a
reasonable assumption. However, one could alter the cost matrix, for example, by using a variable
cost for false positives that reﬂects the level of friction that the card holder experiences.
Using the instance-dependent cost matrix in Table 4, Bahnsen et al. deﬁne the cost of
using a classiﬁer s (·) on the transactions in D as
Cost (s (D)) =
ˆyiCi(1|1) + (1 −ˆyi)Ci(0|1)
ˆyiCi(1|0) + (1 −ˆyi)Ci(0|0)
yi(1 −ˆyi)Ai + ˆyicf.
In other words, the total cost is the sum of the amounts of the undetected fraudulent transactions
(yi = 1, ˆyi = 0) plus the administrative cost incurred. The total cost may not always be easy to
interpret because there is no reference to which the cost is compared . So
Bahnsen et al. proposed the cost savings of a classiﬁcation algorithm as the cost of using the
algorithm compared to using no algorithm at all. The cost of using no algorithm is
Costl(D) = min{Cost(s0(D)), Cost (s1(D))}
where s0 refers to a classiﬁer that predicts all the transactions in D as belonging to class 0 (legitimate)
and similarly s1 refers to a classiﬁer that predicts all the transfers in D as belonging to class 1 (fraud).
The cost savings is then expressed as the cost improvement of using an algorithm as compared with
Costl (D),
Savings (s(D)) = Costl (D) −Cost (s (D))
In the case of transaction fraud, the cost of not using an algorithm is equal to the sum of amounts
of the fraudulent transactions, Costl (D) = PN
i=1 yiAi. The savings are then calculated as
Savings (s (D)) =
i=1 yiˆyiAi −ˆyicf
In other words, the costs that can be saved by using an algorithm are the sum of amounts of detected
fraudulent transactions minus the administrative cost incurred in detecting them, divided by the sum
of amounts of the fraudulent transactions.
Besides obtaining the best statistical accuracy or the highest cost savings, there are many other
reasons why one model might be preferred above another, such as interpretability, operational eﬃciency and economical cost.
Interpretability refers to the intelligibility or readability of the analytical model. Models that
enable the user to understand the underlying reasons why the model signals a case to be suspicious
are called white-box models.
Complex incomprehensible mathematical models are often referred
to as black-box models. It might well be, in a fraud detection setting, that black-box models are
acceptable, although in most settings, some level of understanding and in-fact validation, which
is facilitated by interpretability, is required for the management to have conﬁdence and allow the
eﬀective implementation of the model. In most situations, the aim of the fraud detection system
is to select out of millions of payments the transactions that are most suspicious. These top, say
100, most suspicious transactions are then given to the fraud investigators for further examination.
When using white box models, it is straightforward to also give information about why a certain
transaction is ﬂagged as being suspicious. This of course facilitates the job of the fraud investigators
leading to more suspicious transactions that can be examined for example in one day. The need of
interpretability on the operator side, which advocates for relatively simple models and methods, has
also the advantage to simplify for the end-user (a bank) the implementation, maintainability and
possibility to update/enrich the system over time.
Operational eﬃciency refers to the response time or the time that is required to evaluate the
model, or in other words, the time required to evaluate whether a case is suspicious. It also entails
the eﬀorts needed to collect and preprocess the data, evaluate the model, monitor and back-test the
model, and re-estimate it when necessary. Operational eﬃciency can be a key requirement, meaning
that the fraud detection system might have only a limited amount of time available to reach a decision
and let a transaction pass or not. In others words, huge volumes of data need to be processed in
a short time span. For example, in a credit card fraud detection setting, the decision time must
typically be less than eight seconds. Such a requirement clearly impacts the design of the operational
IT systems, but also the design of the analytical model.
The economical cost refers to the total cost of ownership and return on investment of the analytical
fraud model. Although the former can be approximated reasonably well, the latter is more diﬃcult to
determine. Fraud analytical models should also be in line and comply with all applicable regulation
and legislation with respect to, for example, privacy or the use of cookies in a web browser.
5. Experimental assessment
In this Section 5.1 we ﬁrst describe the observed data set for the experiments. In Section 5.2 we
present the experimental design and in Section 5.3 we show the results of the experiments.
5.1. Information about the real data set
We illustrate the proposed techniques on a data set that has been provided to our research group
by a large European bank. The data set consists of fraudulent and legitimate transactions made
with debit cards between September 2018 and July 2019. Note that the magnitude of the data set
illustrated here is much smaller than data sets typically used in fraud prediction and its incidence
of fraudulent transactions is also much higher. This is because a kind of white-listing (based on
experience-driven business rules) was ﬁrst applied to the data by the bank to ﬁlter out deﬁnitely safe
transactions. The total data set contains 31,763 individual transactions, each with 14 attributes and
a fraud label that indicates when a transaction is conﬁrmed as fraudulent. This label was created
internally in the bank by fraud investigators, and can be considered as highly accurate. Only 506
transactions in the data set were labeled as fraud, resulting in a fraud ratio of 1.6%.
The initial set of features include information regarding individual transactions, such as amount,
timestamp, payment channel and beneﬁciary country. Table 5 contains examples of such typical
attributes that are available for transactions.
Feature name
Description
Transaction ID
Transaction identiﬁcation number
Date and time of the transaction
Originator’s account number
Identiﬁcation number of the originator’s bank account
Beneﬁciary’s account number
Identiﬁcation number of the beneﬁciary’s bank account
Beneﬁciary’s name
Name of the beneﬁciary
Card number
Identiﬁcation of the debit card
Payment channel
Electronic channel (e.g. online banking, mobile app, ...)
Authentication method
e.g. pin code, ﬁngerprint, itsme, ...
Original currency (e.g. Euros, USD, ...)
Amount of the transaction in Euros
Originator country
Country from which the money is send
Beneﬁciary country
Country to which the money is send
Communication
Message provided with the transfer
Gender of the customer
Age of the customer
Customer’s country of residence
Customer’s preferred language
Table 5: Examples of typical features of transactions.
5.2. Experimental design
In order to test the performance of machine learning models that only use these 14 initial features,
we split the data into a training and testing set. Each one contains 70% and 30% of the transactions,
respectively, stratiﬁed according to the fraud label to obtain similar fraud distributions as observed
in the original data set. Table 6 summarizes the diﬀerent data sets.
For the experiments we use the following popular classiﬁcation methods: logistic regression (LR),
Transactions
Table 6: Summary of the data sets.
decision tree (DT), using the CART algorithm , and gradient boosted trees
(GBT), using the XGBoost algorithm . Logistic regression is often used
in the industry because it is fast to compute, easy to understand and interpret. Moreover, logistic
regression is often used as a benchmark model to which other classiﬁcation algorithms are compared.
Commonly used decision tree algorithms include CART and C4.5 . The tree-like structure of a decision tree makes it particularly easy to gain insight in its
decision process. This is especially useful in a fraud detection setting to understand how fraud is
committed and work out corresponding fraud prevention strategies. XGBoost is short for eXtreme
Gradient Boosting . It is an eﬃcient and scalable implementation of the
gradient boosting framework by Friedman et al. and Friedman , but it uses a more
regularized model formalization to control over-ﬁtting, which gives it better performance. The name
XGBoost refers to the engineering goal to push the limit of computational resources for boosted
tree algorithms. The XGBoost algorithm is widely used by data scientists to achieve state-of-the-art
results on many machine learning challenges and has been used by a series of competition winning
solutions . Note that recent model explaining techniques, such as SHapley Additive exPlanation ) and Local Interpretable Model-agnostic
Explanations ) make it possible to provide model interpretability for
such black box methods. These perturbation-based methods estimate the contribution of individual
features towards a speciﬁc prediction. The purpose of this paper is to illustrate the beneﬁt of the
proposed data engineering techniques to the performance of fraud detection models regardless of the
chosen model structure. Therefore, all three classiﬁers (LR, DT and GBT) are trained on the training
set using their default parameters as suggested by their respective authors. The performance of the
three classiﬁers is evaluated on the testing set using Precision, Recall (i.e. hit rate), F1 measure, false
positive rate (FPR, i.e. false alarm rate), Area Under Precision Recall Curve (AUPRC), Savings,
and the fraction of fraudulent amounts that are detected. Hereby a decision threshold of t = 50% is
used. For the calculation of the Savings measure, we choose a ﬁxed cost of cf = 5 Euros.
5.3. Results
Table 7 contains the performance of logistic regression (LR), decision tree (DT) and gradient
boosted trees (GBT) on the testing set using the 14 original features (top). When we include RFM
features and time features using the von Mises distribution, the performance of all three models
improves signiﬁcantly (middle of Table 7). In particular the Savings, F1 and AUPRC values of the
three models have clearly increased. Their overall performance is further enhanced when we add
the features that are based on the anomaly detection techniques (bottom of Table 7). Using the
original features, the three models are only able to detect around 50% of the fraudulent amounts.
By including the features that are created by the various feature engineering methods, the improved
models can block more than 70% of the stolen money and thus saving more than 67% of the costs
compared to not using any fraud detection system.
Original features
Precision Recall
AUPRC Savings % of fraud amount detected
0.3810 0.4706 0.0025
0.1905 0.3200 0.0000
0.3333 0.4667 0.0010
Including RFM and other time-related features
Precision Recall
AUPRC Savings % of fraud amount detected
0.4286 0.4865 0.0035
0.3810 0.5161 0.0010
0.4286 0.5294 0.0020
Including features based on anomaly detection techniques
Precision Recall
AUPRC Savings % of fraud amount detected
0.6190 0.6842 0.0020
0.6190 0.7027 0.0015
0.6667 0.7568 0.0010
Table 7: Performance of logistic regression (LR), decision tree (DT) and gradient boosted trees (GBT) on the testing
set using (top) the 14 original features, (middle) the RFM and other time-related features, (bottom) and the features
based on anomaly detection techniques.
While the data set is now extended with new features, the imbalance between the fraudulent and
legitimate transactions remains. To address this issue we apply the following over-sampling methods
on the extended training set: SMOTE, ADASYN, MWMOTE and ROSE, each with their default
parameters as suggested by their respective authors. We use these over-sampling techniques such that
the new, re-balanced training set contains a ratio of 90% legitimate cases versus 10% fraud cases. In
Table 8 we present the results for all three classiﬁers with each of the over-sampling methods. Notice
how the performance varies depending on the chosen over-sampling method. The Savings value of
the logistic regression model is mostly improved with MWMOTE as well as SMOTE and ROSE. The
Savings value of the decision tree, however, only increases with ADASYN and SMOTE. While logistic
regression and decision tree may beneﬁt from over-sampling methods, the overall performance of the
gradient boosted trees is decreasing. This may be due to the boosting algorithm which could be over-
ﬁtting the classiﬁer on the over-sampled training set resulting in a lesser performance on the testing
set. Depending on the chosen classiﬁcation method, there is deﬁnitely potential in over-sampling the
training set with synthetic fraud cases, although there is not one over-sampling technique that will
always yield the best result.
Logistic regression (LR)
Precision Recall
AUPRC Savings % of fraud amount detected
0.6190 0.6842 0.0020
0.7619 0.5333 0.0116
0.7143 0.5263 0.0106
0.7619 0.5818 0.0091
0.7619 0.5517 0.0106
Decision tree (DT)
Precision Recall
AUPRC Savings % of fraud amount detected
0.6190 0.7027 0.0015
0.7619 0.6038 0.0081
0.8095 0.6667 0.0066
0.7143 0.5556 0.0091
0.6190 0.6190 0.0040
Gradient boosted trees (GBT)
Precision Recall
AUPRC Savings % of fraud amount detected
0.6667 0.7568 0.0010
0.6190 0.6500 0.0030
0.5238 0.6471 0.0010
0.5714 0.6486 0.0020
0.0952 0.1667 0.0005
Table 8: Performance of logistic regression (top), decision tree (middle) and gradient boosted trees (bottom) on the
testing set using diﬀerent over-sampling methods: SMOTE, ADASYN, MWMOTE and ROSE.
6. Conclusions and future research
In this paper, we extensively researched data engineering in a fraud detection setting.
speciﬁcally, we decomposed data engineering into feature engineering and instance engineering. Our
motivation for doing so is that, based upon past extensive research, it it is our ﬁrm belief that the best
way to boost the performance of any analytical technique is to smartly engineer the data instead of
overly focusing on the development of new, often times highly complex, analytical techniques giving
us analytical models which are often only poorly benchmarked and give us no interpretability at all.
We used a payment transactions data set from a large European Bank to illustrate the substantial
impact of data engineering on the performance of a fraud detection mode. We empirically showed that
both the feature engineering and instance engineering steps signiﬁcantly improved the performance of
popular analytical models. Moreover, we have illustrated that by clever engineering of the data simple
analytical techniques as logistic regression and classiﬁcation trees yield very good results.Although
the focus in this paper is on payment transactions fraud, the discussed techniques are also useful or
could be extended to other types of fraud, e.g. in healthcare, insurance or e-commerce.