Distribution Category:
Mathematics and Computers
ANL-80 -10 6
LABORATORY
9700 South Cass Avenue
Argonne, Illinois 60439
NEWTON'S METHOD
MODEL TRUST-REGION
MODIFICATION
C. Sorensen
Applied Mathematics Division
September 1980
TABLE OF CONTENTS
ABSTRACT ............. .
. . . . . . . . . . ...*.... 5
Introduction ..........
Constrained Quadratic Minimization ...........................
A Modified Newton Iteration .................................
Convergence of the Modified Newton Iteration................. 16
Implementation .........
....................................
Conclusions ................................................. 32
Acknowledgement ............................................. 33
References .................................................. 34
NEWTON'S METHOD
MODEL TRUST-REGION MODIFICATION
D. C. Sorensen
Newton method for unconstrained minimization is
presented and analyzed.
The modification is based upon the model
trust region approach.
This report contains a thorough analysis
of the locally constrained quadratic minimizations that arise as
subproblems in the modified Newton iteration.
Several promising
alternatives are presented for solving these subproblems in ways
that overcome certain theoretical difficulties exposed by this
Very strong convergence results are presented concerning the minimization algorithm.
In particular the explicit use of
second order information is justified by demonstrating that the
iterates converge to a point which satisfies the second order
necessary conditions for minimization.
With the exception of very
pathological cases this occurs whenever the algorithm is applied
to problems with continuous second partial derivatives.
Introduction
The problem of minimizing a real valued function f
of several real
variables is generally attacked by some variant of Newton's method for finding
a zero of the gradient of f.
The term variant here is meant to include any
method based upon maintaining an approximation to the Hessian matrix of mixed
second order
derivatives of
computable, then Newton's method is probably the method of choice for the
minimization problem.
As we shall point out in Section 3 there are several things to consider
when attempting to provide a practical implementation of Newton's method for
general use.
Not the least of these is the problem of forcing convergence
the method when good initial guess at the solution is not available.
purpose of this report is to describe and analyze a technique for the solution
of this problem.
The approach we shall present is well known.
It is apprpriately called a model trust region approach in that the step to a new
iterate is obtained by minimizing a local quadratic model to the objective
function over a restricted ellipsoidal region centered about the current
The diameter of
is expanded and
contracted in a
controlled way based upon how well the local model predicts behavior of the
objective function.
It is possible to control the iteration in this way so
that convergence is forced from any starting value assuming reasonable conditions on the objective function.
In fact, we shall prove some very strong
convergence properties for this method in Section 4.
There it is shown that
one can expect (but not ensure) that the iteration will converge to a point
which satisfies the second order necessary conditions for a minimum.
The origin of this method properly lies with the work of Levenberg 
and Marquardt for nonlinear least squares calculations.
The method was
first discussed in connection with general minimization by Goldfeld, Quandt,
and Trotter .
Powell applied the modification in a more general
situation of a quasi-Newton iteration.
computational observations.
This paper is most heavily influenced by the work
of More for the nonlinear least squares case.
from several
to obtain a
practical implementation of a Modified Newton method that takes full advantage
second order information.
Several of the more recent works have
attempted to explicitly use directions of negative curvature to accomplish
various tasks such as escape from saddle points , search along more
paths , obtain convergence to points that satisfy
second order necessary conditions etc.
We observe along with
Gay that the method proposed nere will accomplish these things in a very
elegant and intuitively appealing way.
is hoped that
this report will present a succinct but thorough
analysis of this method.
In particular we feel
it is important to clearly
describe the theoretical nature of the locally constrained quadratic minimization in Section 2.
The analysis given in Section 4 is made sufficiently
general to apply to several possible implementations.
These possibilities are
described in Section 5 where particular attention is paid to overcoming a
practical problem of implementation exposed by the theoretical discussion in
Section 2.
We make an effort to offer several alternatives to implementation
shall make no recommendations until
numerical evidence
Constrained Quadratic Minimization
unconstrained
minimization
presented in Section 3 will be concerned with the solution of the following
4(w) = f + gTw +!wTBw. Find p E 1 n such that
P(p) = minIj*(w):
In (2.1) B = BT F gnXn;
wg E $n; f,A E I with A >
-II throughout is
the 2-norm.
There are some important subtleties to this problem.
The purpose
of this section is to give a complete discussion of the theoretical aspects of
problem (2.1) and to expose the nature of the computational difficulties that
may be present.
Several authors have considered problem (2.1) or related problems.
problem appears implicitly as a subsidiary calculation in Levenberg-Marquardt
type algorithms for nonlinear least squares .
The computational aspect
of this calculation was fully discussed by More in .
A relatively early
paper by Forsythe and Golub considers a closely related problem concerning
minimization of the Farm
minj(x-b)TA(x-b): Dxi j
While their work gives an extensive study of problem (2.2), it
is not fully
applicable to problem (2.1) since g r
range(B) may not hold, and the interior
is not considered.
Problem (2.1) first appeared as a subsidiary calculation
in unconstrained minimization in the work of GoLdfeld, Quandt, and Trotter
Hebden made an important contribution concerning the practical
computation of a solution to
More recently the problem has been
discussed by Gay .
If the method of Lagrange is applied to the equivalent problem
s.t. w w<A
is a straightforward conclusion of the first order necessary conditions
that p solves (2.3) and hence (2.1) only i&
p satisfies an equation of the
form (B+AI)p = -g with A
> 0 the Lagrange multiplier associated with the
constraint ww < A2
Lemma (2.4):
If p is a nonzero solution to (2.1) then p is a solution to an
equation of the form
(B+AI)p = -g
with A > 0 and B+AI positive semidefinite.
It has been noted that p must solve an equation of the form of
It remains to show that B+AI is positive semidefinite.
Since p solves
it also solves minbl'(w):
It follows that $(w) > J(p) for
all w such that iiwi
This inequality together with equation (2.5) gives
f pT(B+AI)w + 2 w Bw > f - p (B+AI)p + - p Bp
Rearranging terms in (2.6) gives
2 (w-p) (B+AI)(w-p) >2 (w w-p p) = 0
for all w such that fwl = Mpi.
it follows readily from (2.7)
that B+AI is positive semidefinite.
Lemma (2.4) establishes necessary conditions concerning the pair A,p when
p solves (2.1).
Our next result establishes sufficient conditions that will
ensure p is a solution to (2.1).
These results are essentially given in
However, we wish to present a statement and proof of these results that
is more complete and better suited to this presentation.
Lemma (2.8):
, p e In satisfy
(B+AI)p " -g with B+AI positive semidefinite.
0 and Hpi <
then p solves (2.1).
If IpU = A then p solves
iJ(p) = min{(J(w): liwi = A}
If A > 0 and
p = A then p solves (2.1).
If, in fact, B+XI
is positive definite then p is unique in each of the cases
(i), (ii), (iii).
If A,p satisfy (2.9) then
f + gTw +2wT(B+AI)w
> f + g p + 2pT(B+XI)p
holds for any w E
It follows that
Statements
immediate consequences
uniqueness statement follows from (2.10) because the inequality is strict when
B+AI is positive definite and w # p.
The solution of problem (2.1) is closely related to solving the nonlinear
where $(a) =(B+aI)-1gI.
Using the eigensystem of the symmetric matrix B together with the invariance
under orthogonal
transformations
is easy to show if g # 0 that
is a rational function with second order poles all belonging to a subset
of the eigenvalues of -B.
0 it follows that (2.12) has a
solution whenever A > 0 and g 0 0.
We can construct a solution to problem (2.1) using a particular solution
of (2.12).
be the smallest eigenvalue of B;
Bq = X1 qJ; let a be the largest root of (2.12) when g 0 0 and a *-0
If there is any q c S1 such that gTq f 0 then a > -A1
must hold.
is not a pole of $. Thus $(-A 1) is well defined when g C Si
and this is the only possibility for & < -A1
A = max{0,-A 1,a
0, = 0 otherwise.
We now construct a solution p to problem (2.1) by the formula
p = -(B+AI) g + 6q
where q c Sl,
IqIl = 1, and (t) denotes pseudo-inverse .
Note B+AI must be
semidefinite with
this choice of X.
qT(B+AI) = 0 when
is easily checked that p is a solution to (2.9) and satisfies
Thus p solves (2.1) and
ipI = A whenever Al <
The solution given by (2.13) shows that p is not
unique whenever g e S and $(-a ) < A due to the arbitrary choice of sign in
defining 6.
This discussion of the theoretical subtleties of solving (2.1) indicates
numerical difficulties may arise when a solution to problem (2.1) is sought.
The case g r SA
= -al in (2.13) will give rise to a very sensitive numerical problem.
Any computational technique for solving (2.9) will introduce
roundoff error.
However, in this sensitive case small perturbations in the
quantities B,g,A can lead to large perturbations of the solution p due to the
fact that B+AI will be nearly singular.
Apparently the true nature of the
difficulty here is the non-uniqueness of the solution p given by (2.14).
illustrate this point with a simple example.
gT = (1,0) ,
with n < 0
The solutions to (2.1) are of the form
Cln)' +0l -A
whenever 1/(1-n)2 ( A2 .
The perturbation g_
- (1,e) gives a solution
there is a perturbation C such that
In case Ti < 0 we must have lip11 = A to solve (2.1) and
we can be led to extremely different solutions as a result of error introduced
by roundoff.
The convergence analysis to be given in Section 4 will depend heavily
upon the following technical result concerning the amount of decrease in the
local quadratic model.
A geometric interpretation of the result is that, for
a quadratic function, any solution p to (2.1) produces a decrease f-P(p) that
is at least as much as the decrease a search along the steepest descent direction -g would provide.
Lemma (2.14):
Let p be a solution to (2.1).
I gllmin(IPI, IB
A proof of this result may be found in .
In fact, the inequality in Lemma (2.14) is obtained by Powell's "dog-leg"
step .
This inequality is the main ingredient used to show the sequence
of gradients tend to zero for the Modified Newton's method we are about to
The reason for solving (2.1) rather than using the dog-leg step is
that second order information is used to greater advantage.
This will become
evident as we present some very strong convergence results in Section 4.
A particular method for obtaining numerical solutions to (2.1) will be
suggested in Section 5.
For the moment we assume that a numerical solution p
to problem (2.1) can be obtained which satisfies
(B+XI)p = -g+6g
with B+AI positive semidefinite
IUpI-AI < e2A
(when A > 0)
NO < (1+e )A
for some fixed 0 < e1(<e2 in (0,1) that are consistent with the finite
precision arithmetic.
The results of Lemma (2.8) imply that such a p solves
the modified problem.
min{f + gT w +
(1-e2 )L < A< (1+e2 )t and
g = g+6g with 116g1I
< e 1 1g II and i
In our analysis we shall assume E
A trivial but tedious
modification of the analysis would apply to a computed step p which satisfies
the above criteria.
This is primarily because the crutial inequality of Lemma
(2.14) will become
It is straightforward to see that the inequality of (2.16) is sufficient
for purposes of the ensuing analysis, but we wish to refrain from including
such complicated expressions at each stage of the analysis.
A Modified Newton Iteration
A well known method for solving the unconstrained minimization problem is
Newton's method applied to finding a zero of the gradient of the objective
However, this iteration is clearly not suitable as a general algorithm without modification.
The basic iteration is
xk+l = xk -
k=0,1,2,...
where an initial iterate x0 must be specified, Vf(xk) is the gradient of f,
V2 f(xk) is
the nxn (symmetric) Hessian matrix of mixed second partial
derivatives of f.
The algorithm we shall discuss will require that f is twice
differentiable at any point x in the domain of f, and that these derivatives
can be evaluated explicitly.
fundamental
reasons why this
basic method must
First, the initial iterate may have to be very "close" to a local
minimizer in order to be assured that the iteration will converge.
even if the iteration converges to a stationary value x* (Vf(x )=0) there is
no guarantee that x will be a local minimizer.
Third, the iterate xk+l may
not be well defined by (3.1) if the Hessian Gk is singular or it may not be a
sensible move if Gk is indefinite.
Our purpose here is to discuss certain
theoretical properties of a modification of the basic iteration (3.1).
approach is not a new one, however we feel that the theoretical and numerical
properties of the proposed method should be fully treated and that is the main
goal of this discussion.
The method we shall consider is called the model
region method.
already mentioned
The main concern here is the implementation of this type of algorithm.
Therefore, this discussion is intended to apply to several possible
implementations.
Specific implementations are presented in Section 5.
Before the iteration is defined let us set out some of the properties
desired of a modified-Newton iteration:
For a sufficiently general class
should be well defined and convergent given any initial iterate
When the iteration converges to a point x*, this point should
satisfy as many necessary conditions for a minimizer as possible.
The modification should not detract from the local quadratic rate
of convergence enjoyed by Newton's method.
The method should be invariant under linear affine scalings of
the variables.
That is, if we replace f(x) by f(w) f(Jw+z)
where J c 1 nxn is nonsingular and w,z c 1n, then applying the
iteration to f with initial guess w0 satisfying x0 = Jw0 +z should
produce a sequence lwkl related
the sequence {xk} by xk
Jwk+z, where Ixk} is produced by applying the algorithm to f with
initial guess x0
The algorithm we are about to define will be shown to meet criteria a,b,c
for all practical purposes.
The last criterion (d)
will be discussed in
Section 6.
To begin we introduce a factorization of the Hessian matrix.
each k we let
be a factorization of Gk with Bk = B
1 nxn and Jk nonsingular.
It follows
that Bk has the same inertia (see [17,
p. 377]) as Gk.
For each k we put
$k(w) = f(xk+Jkw).
This function 4 k(w) may be regarded as a locally scaled
objective function.
The first three terms of the Taylor series of $k about
w=0 will define a local quadratic model
fk + gkw + 2 wTBkw
f(xk) Tk and fk = f(xk) = k(0).
Along with
the local quadratic
shall maintain a control parameter Ak > 0 which defines a local
of trust 1w: Ilwl < Ak} where the model
considered valid.
parameter Ok will be revised during tht. iteration according to specific rules
which are designed to force convergence of the iterates {xk}.
are potentially
considering
any symmetric
factorization
matrix Gk,
but certain requirements should be kept
= Vf(xk)TJk
explicitly or by solving
Vf(xk )T = gkJj.
Also, it will be an advantage if the eigensystem of Bk is
relatively
inexpensive
eigenvalue
corresponding eigenvector(s) are easy to obtain.
The reason for this is that
the solution to problem (2.1) will play an important role in this iteration
and as we have seen the eigensystem information may be required.
especially true at points xk where Gk ij
indefinite or singular.
Now we are ready to define the iteration.
Algorithm (3.3):
let 0 < n1 < n2
prespecified
constants;
Let x1 EIn, A1 > 0 be given;
If "convergence" then STOP;
Evaluate fk : f(xk); Gk .=V0 2(fxk);
FactorBk :
J Gk Jk; Evaluate gk:
Jk Vf(xk);
Compute wk
argmin14k(w): IwI(Ak}
k ' gkw + 7 wTBkw;
Put ared :
k(O)k(wk); pred :=
k(O)- k(wk);
k(w) = f(xk+Jkw);
then begin Ak
oto 5; end;
Ifn < armed
xk+1 :=xk + Jkwk
I 2) if ared >n
pred>2 then
k; k := k+1;
There are ways to update the value of A at step 7 and step 8.2 which make
better use of the
information available at the current iterate xk.
example, the cubic polynomial that fits
*k( wk) by interpolating 0(0),
V'(0), 0"(0) and @(1) will have a minimum a in (0,1) when the test at step 7
is passed.
The region is contracted by setting Y1 = E if a is not "too close"
to 0 or 1.
Details of this type of idea appear in .
Similar ideas
may be applied at (8.2) to obtain an expansion factor Y2 > 1 that depends upon
available information.
Other variations involving step 7 include accepting
the predicted minimizer it 0 <o
armed< 1 but reducing the trust region.
The analysis we shall perform on Algorithm (3.3) can be adapted to cover these
possibilities in a fairly straightforward way.
However, the gain in generality will result in a substantial loss in clarity of exposition in the analysis
so we shall analyze the simple choices set forth in Algorithm (3.3).
Finally it should be pointed out
that this iteration is well defined
because step 7 will produce a sufficiently small ok
to obtain red > nl
a finite number of steps since the quadratic function 1k(w) is defined by the
three terms
of the Taylor series for $k(w).
Our statement of
strategy is slightly different than the usual description in that xk+1 is
always different
By doing this we avoid having to distinguish
between "successful" and "unsuccessful" iterates in the analysis.
except ion the statement of the algorithm and the ensuing -nalysis are in the
spirit of the paper presented by Powell .
Numerical schemes for producing
constrained
minimization
step 5 will
Section 5.
Convergence of the Modified Newton Iteration
In this section we shall establish that some very strong convergence
properties are possessed by Algorithm (3.3).
The first result is a slight
modification of Powell's result in .
Our proof is much simpler due to the
fact that here second order information is explicitly available.
Theorem (4.1):
f: In + I be bounded below and let G(x) =
V2 f(x) be
continuous and satisfy NG(x)U (
< for all x c .1(x0). Let lxkI C n be the
sequence produced by Algorithm (3.3) applied to f given starting value x0
= 0,1,2,... for some a > 1. Then there is no
constant c > 0 such that *Vf(xk)I > C for all k.
Assume there is an e > 0 such that iVf(xk ) >e for all k.
kf(xk)II/iI
1 > c/a = Y > 0
From step (7) of Algorithm (3.3) and from Lemma (2.14) we have
J(w k))J >
where UBkU =
k=0,1,2,...
we have fk~ k+l + 0.
Since IIgkII/UBkUi >Y/20
Ak + 0 from (4.2).
is obtained from the inequality
H(Bk+a(k)I)wk
(Owkil(UBk ii+(k))
where X(k)
the multiplier associated with the solution to step 5 of
Algorithm (3.3).
Thus Inequality (4.3) shows a(k) + +.
Now, from Taylor's
theorem and Lemma (2.8) it readily follows (for k sufficiently large) that
(Bk(6)-Bk)(1-O)d~wk)
w (Bk+X(k)IJwk + A(kwrW
sup MBk(0)-Bk
Jk[G(xk+6sk
Since A(k) + +
pred(k) + 1 and thus the test at step (7) of Algorithm (3.3) is passed the
first time through for all k sufficiently large.
This implies the existence
of a K > 0 such that
K > A for all k >
K. Therefore, the
assumption
> C for all k has led to a contradiction.
We remark that the continuity of G(x) is only used to obtain the numerator on
the right hand side of (4.4) and that the Theorem can also be established
without this assumption.
See Powell for example.
This result has shown that at least one subsequence of {xk} converges to
a critical point of f.
The next result which is due to Thomas will
establish the much stronger fact that every accumulation point of the sequence
{xkl is a critical point of f.
hypotheses
of Theorem
limIlf(xk)II = 0.
subsequence
IIVf(xkj)II >e > 0,
j=1,2,... .
Theorem (4.1) this implies
due to Theorem (4.1), we may select an integer Lj
corresponding to each j such that
= min{R > kj: IIg If > Y/2a2 }
and without loss of generality kj <
j=1,2,... .
From inequality
(4.2) we obtain that
will hold for all kj <
< Lj, j=1,2,... .From
(4.7) it follows that
>402min(: A ,
From inequality (4.8) it follows that
and the right hand side of (4.9) is forced to zero due to (4.8).
The uniform
bound on G(*) implies the uniform continuity of Vf(x) on f(x0 ) and it follows
IIVf(xk )-Vf(xR
for all j sufficiently large.
0 < aIIVf(x )II
( a(IIVf(xkj )-f(xj+)
+ IIVf(xj+1I)
for all j sifficiently large.
The assumption that UVf(xkj)N > e > 0 has led
to a contradiction and we must conclude that limfVf(xk)II = 0.
established
point of the sequence
{xkj satisfies the first o:der necessary conditions for a minimum.
shall establish results which give added justification to the use of second
order information when it is available.
Several authors have
proposed modified Newton methods which guarantee convergence to a critical
point x* with the additional feature that the Hessian G(x*) be positive semidefinite.
Thus second order necessary conditions for a minimum are satisfied
The following series of results show that Algorithm (3.3) shares this
Lemma (4.10):
Let the hypotheses of Theorem (4.1) be satisfied.
If G(x) is
uniformly continuous on
(xk0) then there is no positive number A > 0 such
that a(k) > A
for k > k0
If A(k) > 0 then NwkI
to Lemma 2.8.
We conclude from
inequality (4.4) that
< 1 sup UBk()-Bku
where Bk(6) = J(Gk (xk+Osk))Jk.
Since pred(k) > A(k)
it follows that
Ak + 0 because pred(k) + 0.
Now the boundedness of UJkI, UJkN0
together with
the uniform continuity of G(x) on
(,k() gives
ared(k) + 1
We must conclude as
the proof of Theorem (4.1) that Ak>.AK for some
This contradiction establishes the result.
Since -a(k) <
(k) which is the smallest eigenvalue of Bk, the next theorem
follows easily from the boundedness of NJIkIlik1I together with Lemma (4.10).
Theorem (4.11):
the hypotheses of Lemma (4.10) be satisfied.
sequence {xk}
is convergent to a limit x* say, then Vf(x*) = 0 and G(x*) is
positive semidefinite.
At this point we should remark that failure of this iteration to converge
will require an extremely pathological situation.
A moments reflection will
convince the reader that every limit point of the sequence {xk} must be a
critical point of f,
and f must have the same value at each of these critical
least one of these critical points has a positive
semidefinite Hessian.
The next result- shows that if any one of the limit
points of the sequence, x* say, satisfies G(x*) is positive definite then the
entire sequence must converge to x*.
Lemma (4.12):
hypothesis
{xkj }C{lxkl be a subsequence which converges to a critical point x*.
G(x*) is positive definite and G(x) is continuous in a neighborhood of x* then
the entire sequence must converge to x*.
Due to the continuity of G we must have HG(x) II <_
for all x in some
neighborhood .7? of x*.
Thus for any E > 0 there is a 6 > 0 and a corresponding
ball 96 = {x: ix-x*i <
61 c f such that
NG(x)-Vf(x)II
This follows from the continuity of Vf and G,
as well as the fact that
Vf(x*) = 0.
The assumption that G(x*) is positive definite implies that x* is
an isolated local minimum.
Therefore, there is a 6 > 0 such that f(x*) <
G(x) is positive definite, Vf(x)
0 for every XE
= lz: Uz-x*II <61
that is different from x*.
Let 0 < 62 <1/461 be chosen so that
IG(x) 1 Vf(x)II
all x E 92
z: Iz-x*U < 62}.
sufficiently large, the iterates xk lie in 92.
Since xkj + x* and f(x*) < f(x) for all x e 9,
x # x* there is a j0 such
) < infjf(x): 6 ( IIx-x*II < 6 }
and xkj c }2 for all j
j >j0 be fixed and suppose that
Since k+1 > kj 0 we have f(x+
1) < f(xj 0) and thus x+ 1
due to (4.13).
follows that
I xt+l~'" H
*il> 61-62 >
This implies that At
However, this is a contradiction since x e Q2 implies
UG(x ) Vf(x)U <
so the Newton iterate z = x -
G(x )'Vf(x ) satisifes
*JR, (z-x )H < aUz-xR, U
This argument shows that xk e .02 for all k > kj0 .
Since x* is the only
critical point of f in -2
since every limit point of the sequence {xkI
is a critical point of f we must conclude that the entire sequence converges
It would be more desirable to obtain a result that would ensure convergence of the sequence Ixk} without assuming a subsequence converges to a
strong local minimum.
However, just extending this argument to the case of an
isolated local minimum with singular Hessian would be difficult since one can
no longer rely on the Newton step.
Our final result will show, in conjunction
with Lemma (4.12),
that if there is a subsequence which converges to a strong
local minimum then the entire sequence converges and ultimately the rate of
convergence is quadratic.
Theorem (4.14);
Let the hypotheses of Theorem (4.1) be satisfied.
further that xk + x* with G(x*) positive definite and
IG(x)-G(x*)II
< LIIx-x*II
for all x in some neighborhood of x*.
Then there is a constant x > 0 such
IIXk+1 -x*H (tXKIIxk-*1
for all k sufficiently large.
positive definite
continuity
constants 13
sufficiently large.
Thus Ipred(k)I|> s G(xk)sk > Usk 2 /UG(xk)'INH > 1
Iared(k)-pred(k)I (
< sk2 J AGk(0)-Gkn(1-e)d
where Gk(6) -
G(xk+Osk) with sk - xk+lxk.
It follows easily from (4.15) that
ared(k) _ 11+ +0
and we must conclude that there is some K > 0 such that Ak
AK for all k > K.
Again, since xk + x* with Vf(x*) - 0 it follows that UG(xk )
so the Newton step is accepted for all k sufficiently large.
Hence the tail
of the sequence {xkl
is the unmodified Newton iterat:ion which is quadratically
convergent to x* since G(x*) is positive definite [.'1, p. 421].
While these results hold little computational meaning in the presence of
roundoff error, it is satisfying to have established such strong results about
the iteration.
This is especially true since the method has such an intuitive
Our aim in this section has been to establish these theoretical
results in a framework that
is general enough to encompass many possible
implementations.
We shall consider some of these implementations in the next
Implementation
Numerical performance
algorithm described
Section 3 and
in Section 4 is obviously going to depend upon a careful implementation of the locally constrained minimization of the quadratic model.
Section 2 we pointed out several theoretical facts that indicate great care
should be exercised in this computation.
In this section we shall put forth
several possible implementations.
Each of these will have certain advantages
and disadvantages depending upon the nature of the optimization problem at
The convergence
theory provided
in Section 4 was purposely made
sufficiently general to apply to all of the alternative implementations to be
presented here.
Our main concern is to provide an efficient and stable method for the
solution of problem (2.1).
To this end we consider factorizations
of the symmetric nxn matrix G.
We are assuming that IIJIUJ 1U <( o where a > 1
is some fixed number that is independent of G.
Recall that the matrix B is
also symmetric and must have the same inertia as G.
Some specific examples
(a) J orthogonal and B diagonal; (b) J orthogonal and B tridiagonal;
(c) JT = LI1 P where L is unit lower triangular, P is a permutation matrix, and
B is either tridiagonal or block diagonal with lxl or 2x2 diagonal blocks
We shall also consider the case when J is just a diagonal nonsingular
If the eigensystem of B is easily obtained (i.e. in case a or case c when
B is block diagonal) then we are able to solve problem (2.1) directly by
constructing a solution to
(2.1) using formula (2.13).
This method of
solution has the particular advantage that the case when g e S
is explicitly
A disadvantage of using factorization (a)
relatively
expensive to compute.
One of the reasons for introducing generality into the
model trust region calculation was to allow use of the Bunch-Parlett factorization
This factorization
is very efficient
due to the
symmetry is exploited.
The matrix B for this factorization has an eigensystem
easily computed.
the matrices J satisfy the criteria
< a so in theory all of the results of Section 4 apply.
be some cause for concern regarding the effect of the transformation J on the
descent direction, because the triangular coordinate system may be very skewed
even though the matrix J is well conditioned.
Nevertheless, our main concern with either of these factorizations is the
efficient and reliable solution to an equation of the form
for the largest root A.
The left hand side of (5.1) is precisely the form of
$(a) = I(B+aI)
regardless of whether or not B is diagonal.
Several authors
 discuss the solution of equations that closely
resemble (5.1).
The key observation is that Newton's method which is based on
a local linear approxir tion to 4(a) is not likely to be the best method for
solving (5.1) because the rational structure of +2(a) is ignored.
Instead, an
iteration for solving (5.1) can be derived based upon a local rational approximation to $. The iteration is obtained by requiring 4(a) -
-a to satisfy
where we regard a as the current approximation to the root A.
This approximation is
then improved by solving for an a that satisfies $a) = A.
resulting iteration is
4 (a ) F - 4 (a )1
form of $(a)
known explicitly
straightforward
safeguard (5.1).
The local rate of convergence of thi' iteration is quadratic
feature of
that usually
the number of
iterations required to produce an acceptable approximation to A
is very small
because the iteration is based upon the rational structure of $2
Iteration (5.2)
can be implemented without explicit knowledge of the
eigensystem of B.
This important observation which is due to Hebden 
makes it possible to implement (5.2) merely by solving linear systems with
as the coefficient matrix.
is easy to see since $(a) = "lpall, and
pa where (B+aI)p = -g.
Hebden suggests a way to
obtain a > -A1 during the process of attempting to compute the Cholesky factorizatin of B+aI.
This is discussed in more detail by Gay in where the
difficult case g
E Si is addressed.
Within this context we could allow J to
be taken as a nonsingular diagonal matrix for solving purposes.
More has used
this idea in his adaptation of Hebden's work to the nonlinear least squares
problem .
The result of More's work is a very elegant robust algorithm
for nonlinear least squares.
 careful attention is paid to safeguarding the step calculation.
The safeguarding
difficult in the present setting due to the fact that B may have negative
eigenvalues.
The essential difficulty seems to stem from the
without explicit knowledge of the eigensystem it
is difficult to detect the
case g e S1.
Moreover, it seems to be necessary to have an estimate of the
smallest eigenvalue and a corresponding eigenvector in order to obtain a
solution to (2.1) in case g c Sl
(see formula 2.13).
This was recognized by
Hebden but he did not provide a suitable solution.
Gay suggests obtaining
an eigenvector using inverse iteration if the case g E S is detected because
a factorization of the (nearly) singular matrix B+AI will be available.
Here we suggest an alternative to the methods which have been proposed
previously.
following we
considering J
a diagonal
nonsingular matrix.
Let us return to the derivation of iteration (5.2).
Another way to obtain this
iteration is
to apply Newton's method to the
From this observation we can see that iteration (5.2) is closely related to
Newton's method applied to the problem
where we use the notation r(p,a) = Bap+g with Ba = B+aI.
There is a serious
disadvantage to this iteration when g r
S or nearly so.
This is because the
Jacobian of (5.4) is
this matrix is singular at a solution A,p of (2.1) in the sensitive case
< A, where X = -A1.
convergence.
Moreover, as the iteration converges to such a solution the method requires
solving linear systems which have increasingly ill-conditioned coefficient
As an alternative we suggest removing the explicit dependence of 4(a) on
the variable a in (5.4).
Instead of (5.4) we shall apply Newton's method to
Due to Lemma (2.8) a solution a = A, p = p, to (5.6) provides a solution to
problem (2.1) whenever BX is positive (semi) definite and A > 0.
The Jacobian
of (5.6) is
and this matrix is nonsingular at a solution to (2.1) in the cases that are
most likely to occur.
This is important since it follows that Newton's method
applied to
(5.6) will usually enjoy a quadratic rate of convergence.
precise statement of when (5.7) is nonsingular at a solution is given in the
following lemma.
Lemma (5.7):
Let a = A > 0, p = p
0 be a solution to problem (5.6) with B
semidefinite.
positive definite
if IIB gII
dim(S1 ) = 1 then the Jacobian matrix (5.7) is nonsingular.
Let p = pX, a = A.
It is sufficient to show
is nonsingular.
Suppose that
Bz + pS = 0
These two equations imply zTBXz = 0.
Therefore either z = 0 or B is singular
and z E S1.
these possibilities
- 0 since p f 0,
Thus, when BA is positive definite the only solution to (5.9) is
z = 0, r = 0 so (5.8) is nonsingular.
If on the other hand B
is singular and
IIB gII < 0 then p = -B
g+Oq with q
e S1, liqil = 1, and 0 # 0.
Since dim(S1 )
follows that z = Yq and thus 0 = zTp
= 9Y which shows Y = 0.
conclude (5.9) only has the trivial solution so (5.8) is nonsingular.
The basic iteration (without safeguards) for solving (5.6) will be given
The details of various suggested implementations will follow.
Algorithm (5.10):
an initial
guess p0 and a0 such
for k = 0,1,2,...
rk = Bkpk+g;
Bk+l = Bk+a;
We must address several computational questions concerning this iteration.
These include what
initial guess should be used, how to solve the linear
systems at step 2.2, how to safeguard the basic iteration, and finally how to
stop the iteration.
First of all we shall discuss some methods for solving the linear system
at step 2.2.
For matrices B that are of moderate size and those which have no
particular structure we recommend the following.
Compute an orthogonal matrix
Q through a product of Householder transformations such that
where T is tridiagonal and eT = (O,...,,l).
Initially this factorization is
more expensive than some alternatives (such as the Bunch-Kaufman factorization).
However, it presents several advantages as we shall see.
all, since T = QBQT has the same eigenvalues as B we can easily compute a
Sturm-sequence for T to tell very reliably whether or not T is positive
definite .
Moreover, since good upper and lower bounds for the smallest
eigenvalues of T are available, a good safeguarding scheme can be obtained.
After applying transformation (5.11)
to the linear system at step 2.2 of
(5.10) a solution can be obtained using ordinary Gaussian-elimination with
partial pivoting.
It is preferable to ignore symmetry in this case for the
same reason it is preferable in the case of inverse iteration for the computation of an eigenvector.
See Wilkinson for more detail.
A more important
observation to make here is that iteration (5.10) is invariant under transformation (5.11).
Once the correction 6p = QT6p is obtained we have the updated
The form of the matrix on the right hand side of (5.12) is
where the x's denote nonzero elements.
Gaussian elimination with partial
pivoting preserves this structure if the pivots are taken from the tridiagonal
part until the very last elimination step.
The result of this strategy
applied to (5.13) will be of the form
denote multipliers which have overwritten the original matrix
elements, and the +'s denote possible fill-in due to pivoting.
With this scheme only one expensive factorization is required.
of the iteration is performed under the transformation (5.11) and only after
convergence to a vector p is obtained do we transform back to get p = Qp as a
solution to problem (2.1).
Since the factorization given in equation (5.11) is roughly four times as
expensive as a Cholesky factorization we might wish to consider the following
alternate scheme.
The system at step 2.2 of Algorithm (5.10) is equivalent
(via symmetric permutation) to one of the form
Use a single Householder transformation Q1
the exact factorization
1 (n-1)x(n-1).
------ ------
The matrix on the right of (5.16) has
The eigenvalues of 9 separate the eigenvalues of B so B is positive definite
when B is.
Moreover, B is better conditioned than B whenever the separation
is strict.
(For a proof of separation see Wilkinson [27, pp. 95-104].)
solution to
is now possible using a Cholesky factorization of B
together with factorization (5.17).
The purpose of arranging the calculation
this way is to avoid "pivoting" on the matrix B which is the essential result
of factoring forward at step (2.2) of Algorithm (5.10).
This second scheme is much better suited to the problem of obtaining an
initial guess a0 ,p0 at step 1 of Algorithm (5.10).
If B is positive definite
compute p = -B~'g and check to see if UpI ( A.
then take p0 =
Thus it will be advantageous to attempt the computation
of the Cholesky factorization of B.
If B is not positive definite then we
should compute a0 so that Ba0 is positive definite and then take
as an initial guess.
Various schemes for computing a0 are possible.
 for example.
Safeguarding this iteration is possible.
At present several schemes are
being considered but none of these are elegant.
Therefore we shall postpone
discussion of safeguarding at this time.
The decision to stop the iteration should be based upon the following
Require ak+l'pk+1 to satisfy
:s B+ak+lI positive semidefinite
where 6a * ak+1%, 6p pk+1 k'
|Ipk+1 1AlI
Note (from step 2.2 of Algorithm 5.10) that (B+ak+lI)pk+1 = -g+6
Therefore, if g
0 then conditions (a) and (b) together with Lemma (2.8) imply
that pk+1 solves
min{f + g w +
IlwI' < Al
where (1-e2)A K
< < (1+e2)t and g = g+6g with I16gU < e UIghI and lipk+111 = A. On
the other hand, if g = 0 then Pk+l will be an approximate eigenvector for B
which satisfies
lIBpk+1+ak+l
with ak+l on approximation to -al.
Thus el > 0 should be taken quite small
and E2 > 0 moderately small.
When these stopping rules
are in effect
the remarks at
the end of
Section 2 will apply.
Therefore, the analysis of Section 4 will apply to the
modified Newton iteration when the step is computed in the way described here.
Conclusions
The main purpose of this work has been to discuss the theory of the model
trust region modification of Newton's method with an aim towards understanding
the best way to implement it.
Because of this goal we introduced sufficient
generality into the analysis so that it would apply to many possible implementations based upon various factorizations of the Hessian matrix.
similar to the second order properties given in Section 4 have been stated
without proof by Gay in .
We feel that it is important to give proofs of
these facts.
This is of particular interest because no proof has been given
that ensures convergence of the entire sequence (unless we make the assumption
of a nonsingular Hessian at any critical point).
This is despite the fact
that the situation would have to be extremely pathological even in theory for
convergence not to occur.
The basic ideas
for possible
implementations we have set
Section 5 are new alternatives which have been directed towards overcoming the
theoretical difficulties of the locally constrained quadratic minimization
discussed in Section 2.
In particular we considered using the Bunch-Parlett
factorization and we also considered basing our method of solution on a more
properly posed problem.
It will be interesting to examine the behavior of
these implementations in practice.
Finally, we have not overcome the problem of invariance under linear
affine scalings of the variables.
There is sufficient generality in the
method to introduce uniformly bounded diagonal scalings of the variables.
Ways to choose these scalings has been discussed by Fletcher , Gay , and
is most appropriate to note here that the reason is that our
method of proof of convergence
is essentially based upon not doing worse than
steepest descent at any step and this introduces a term that makes calculation
of the step scale dependent.
Nevertheless, we expect good performance on
practical problems especially in the case that the variables can be well
Acknowledgement
Much of this work was done originally in 1977 at the Applied Mathematics
Division of Argonne National Laboratory.
Since that time it has undergone
several revisions due to the encouragement of Jorge More.
It is a pleasure to
take this opportunity to thank him for the numerous discussions we have had
over this work and to acknowledge
what a valuable contribution his advice has
I would also like to thank Roger Fletcher for several penetrating
discussions
concerning
the material
Section 5.
particular,
factorization (5.17) came directly from one of nhese discussions.