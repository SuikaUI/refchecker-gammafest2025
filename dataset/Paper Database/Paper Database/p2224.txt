GAUSSIAN ERROR LINEAR UNITS (GELUS)
Dan Hendrycks∗
University of California, Berkeley
 
Kevin Gimpel
Toyota Technological Institute at Chicago
 
We propose the Gaussian Error Linear Unit (GELU), a high-performing neural
network activation function. The GELU activation function is xΦ(x), where Φ(x)
the standard Gaussian cumulative distribution function. The GELU nonlinearity
weights inputs by their value, rather than gates inputs by their sign as in ReLUs
(x1x>0). We perform an empirical evaluation of the GELU nonlinearity against
the ReLU and ELU activations and find performance improvements across all
considered computer vision, natural language processing, and speech tasks.
INTRODUCTION
Early artificial neurons utilized binary threshold units .
These hard binary decisions are smoothed with sigmoid activations, enabling a neuron to have a “firing rate” interpretation and to train with backpropagation. But as networks became deeper, training
with sigmoid activations proved less effective than the non-smooth, less-probabilistic ReLU which makes hard gating decisions based upon an input’s sign. Despite having less of
a statistical motivation, the ReLU remains a competitive engineering solution which often enables
faster and better convergence than sigmoids. Building on the successes of ReLUs, a recent modification called ELUs allows a ReLU-like nonlinearity to output negative values
which sometimes increases training speed. In all, the activation choice has remained a necessary
architecture decision for neural networks lest the network be a deep linear classifier.
Deep nonlinear classifiers can fit their data so well that network designers are often faced with the
choice of including stochastic regularizer like adding noise to hidden layers or applying dropout , and this choice remains separate from the activation function. Some stochastic
regularizers can make the network behave like an ensemble of networks, a pseudoensemble , and can lead to marked accuracy increases. For example, the stochastic regularizer dropout creates a pseudoensemble by randomly altering some activation decisions through zero
multiplication. Nonlinearities and dropout thus determine a neuron’s output together, yet the two
innovations have remained distinct. More, neither subsumed the other because popular stochastic
regularizers act irrespectively of the input and nonlinearities are aided by such regularizers.
In this work, we introduce a new nonlinearity, the Gaussian Error Linear Unit (GELU). It relates
to stochastic regularizers in that it is the expectation of a modification to Adaptive Dropout . This suggests a more probabilistic view of a neuron’s output. We find that this novel
nonlinearity matches or exceeds models with ReLUs or ELUs across tasks from computer vision,
natural language processing, and automatic speech recognition.
GELU FORMULATION
We motivate our activation function by combining properties from dropout, zoneout, and ReLUs.
First note that a ReLU and dropout both yield a neuron’s output with the ReLU deterministically multiplying the input by zero or one and dropout stochastically multiplying by zero. Also,
a new RNN regularizer called zoneout stochastically multiplies inputs by one . We merge this functionality by multiplying the input by zero or one, but the values of
this zero-one mask are stochastically determined while also dependent upon the input. Specifically, we can multiply the neuron input x by m ∼Bernoulli(Φ(x)), where Φ(x) = P(X ≤
∗Work done while the author was at TTIC. Code available at github.com/hendrycks/GELUs
 
x), X ∼N(0, 1) is the cumulative distribution function of the standard normal distribution.
We choose this distribution since neuron inputs tend to follow a normal distribution, especially
with Batch Normalization. In this setting, inputs have a higher probability of being “dropped”
as x decreases, so the transformation applied to x is stochastic yet depends upon the input.
Figure 1: The GELU (µ = 0, σ = 1), ReLU, and ELU
Masking inputs in this fashion retains non-determinism but maintains
dependency upon the input value.
stochastically chosen mask amounts to
a stochastic zero or identity transformation of the input.
This is much like
Adaptive Dropout ,
but adaptive dropout is used in tandem
with nonlinearities and uses a logistic
not standard normal distribution.
found that it is possible to train competitive MNIST and TIMIT networks
solely with this stochastic regularizer,
all without using any nonlinearity.
We often want a deterministic decision
from a neural network, and this gives
rise to our new nonlinearity. The nonlinearity is the expected transformation
of the stochastic regularizer on an input x, which is Φ(x) × Ix + (1 −Φ(x)) × 0x = xΦ(x).
Loosely, this expression states that we scale x by how much greater it is than other inputs. Since the
cumulative distribution function of a Gaussian is often computed with the error function, we define
the Gaussian Error Linear Unit (GELU) as
GELU(x) = xP(X ≤x) = xΦ(x) = x · 1
1 + erf(x/
We can approximate the GELU with
0.5x(1 + tanh[
2/π(x + 0.044715x3)])
xσ(1.702x),
if greater feedforward speed is worth the cost of exactness.
We could use different CDFs. For example we could use Logistic Distribution CDF σ(x) to get
what we call the Sigmoid Linear Unit (SiLU) xσ(x). We could use the CDF of N(µ, σ2) and have
µ and σ be learnable hyperparameters, but throughout this work we simply let µ = 0 and σ = 1.
Consequently, we do not introduce any new hyperparameters in the following experiments. In the
next section, we show that the GELU exceeds ReLUs and ELUs across numerous tasks.
GELU EXPERIMENTS
We evaluate the GELU, ELU, and ReLU on MNIST classification (grayscale images with 10 classes,
60k training examples and 10k test examples), MNIST autoencoding, Tweet part-of-speech tagging
(1000 training, 327 validation, and 500 testing tweets), TIMIT frame recognition (3696 training,
1152 validation, and 192 test audio sentences), and CIFAR-10/100 classification (color images with
10/100 classes, 50k training and 10k test examples). We do not evaluate nonlinearities like the
LReLU because of its similarity to ReLUs for a description of LReLUs).
MNIST CLASSIFICATION
Let us verify that this nonlinearity competes with previous activation functions by replicating an
experiment from Clevert et al. . To this end, we train a fully connected neural network with
GELUs (µ = 0, σ = 1), ReLUs, and ELUs (α = 1). Each 8-layer, 128 neuron wide neural
network is trained for 50 epochs with a batch size of 128. This experiment differs from those of
Log Loss (no dropout)
Log Loss (dropout keep rate = 0.5)
Figure 2: MNIST Classification Results. Left are the loss curves without dropout, and right are
curves with a dropout rate of 0.5. Each curve is the the median of five runs. Training set log losses
are the darker, lower curves, and the fainter, upper curves are the validation set log loss curves.
Noise Strength
Test Set Accuracy
Noise Strength
Test Set Log Loss
Figure 3: MNIST Robustness Results. Using different nonlinearities, we record the test set accuracy
decline and log loss increase as inputs are noised. The MNIST classifier trained without dropout
received inputs with uniform noise Unif[−a, a] added to each example at different levels a, where
a = 3 is the greatest noise strength. Here GELUs display robustness matching or exceeding ELUs
and ReLUs.
Clevert et al. in that we use the Adam optimizer rather than stochastic gradient descent without momentum, and we also show how well nonlinearities cope with dropout.
Weights are initialized with unit norm rows, as this has positive impact on each nonlinearity’s performance . Note that we
tune over the learning rates {10−3, 10−4, 10−5} with 5k validation examples from the training set
and take the median results for five runs. Using these classifiers, we demonstrate in Figure 3 that
classifiers using a GELU can be more robust to noised inputs. Figure 2 shows that the GELU tends
to have the lowest median training log loss with and without dropout. Consequently, although the
GELU is inspired by a different stochastic process, it comports well with dropout.
MNIST AUTOENCODER
We now consider a self-supervised setting and train a deep autoencoder on MNIST . To accomplish this, we use a network with layers of width 1000, 500, 250, 30, 250,
500, 1000, in that order. We again use the Adam optimizer and a batch size of 64. Our loss is
the mean squared loss. We vary the learning rate from 10−3 to 10−4. We also tried a learning
rate of 0.01 but ELUs diverged, and GELUs and RELUs converged poorly. The results in Figure 4
indicate the GELU accommodates different learning rates and significantly outperforms the other
nonlinearities.
Reconstruction Error (lr = 1e-3)
Reconstruction Error (lr = 1e-4)
Figure 4: MNIST Autoencoding Results. Each curve is the median of three runs. Left are loss
curves for a learning rate of 10−3, and the right figure is for a 10−4 learning rate. Light, thin curves
correspond to test set log losses.
Figure 5: TIMIT Frame Classification. Learning curves show training set convergence, and the
lighter curves show the validation set convergence.
TWITTER POS TAGGING
Many datasets in natural language processing are relatively small, so it is important that an activation
generalize well from few examples. To meet this challenge we compare the nonlinearities on POSannotated tweets which contain 25 tags. The tweet
tagger is simply a two-layer network with pretrained word vectors trained on a corpus of 56 million
tweets . The input is the concatenation of the vector of the word to be tagged
and those of its left and right neighboring words. Each layer has 256 neurons, a dropout keep
probability of 0.8, and the network is optimized with Adam while tuning over the learning rates
{10−3, 10−4, 10−5}. We train each network five times per learning rate, and the median test set
error is 12.57% for the GELU, 12.67% for the ReLU, and 12.91% for the ELU.
TIMIT FRAME CLASSIFICATION
Our next challenge is phone recognition with the TIMIT dataset which has recordings of 680
speakers in a noiseless environment. The system is a five-layer, 2048-neuron wide classifier as
in with 39 output phone labels and a dropout rate of 0.5 as in .
This network takes as input 11 frames and must predict the phone of the center
Classification Error (%)
Figure 6: CIFAR-10 Results. Each curve is the median of three runs. Learning curves show training
set error rates, and the lighter curves show the test set error rates.
frame using 26 MFCC, energy, and derivative features per frame. We tune over the learning rates
{10−3, 10−4, 10−5} and optimize with Adam. After five runs per setting, we obtain the median
curves in Figure 5, and median test error chosen at the lowest validation error is 29.3% for the
GELU, 29.5% for the ReLU, and 29.6% for the ELU.
CIFAR-10/100 CLASSIFICATION
Next, we demonstrate that for more intricate architectures the GELU nonlinearity again outperforms
other nonlinearities. We evaluate this activation function using CIFAR-10 and CIFAR-100 datasets
 on shallow and deep convolutional neural networks, respectively.
Our shallower convolutional neural network is a 9-layer network with the architecture and training
procedure from Salimans & Kingma while using batch normalization to speed up training.
The architecture is described in appendix A and recently obtained state of the art on CIFAR-10
without data augmentation. No data augmentation was used to train this network. We tune over
the learning initial rates {10−3, 10−4, 10−5} with 5k validation examples then train on the whole
training set again based upon the learning rate from cross validation. The network is optimized with
Adam for 200 epochs, and at the 100th epoch the learning rate linearly decays to zero. Results are
shown in Figure 6, and each curve is a median of three runs. Ultimately, the GELU obtains a median
error rate of 7.89%, the ReLU obtains 8.16%, and the ELU obtains 8.41%.
Next we consider a wide residual network on CIFAR-100 with 40 layers and a widening factor of 4
 . We train for 50 epochs with the learning rate schedule described
in (T0 = 50, η = 0.1) with Nesterov momentum, and with a dropout
keep probability of 0.7. Some have noted that ELUs have an exploding gradient with residual
networks , and this is alleviated with batch normalization at the end of a residual
block. Consequently, we use a Conv-Activation-Conv-Activation-BatchNorm block architecture
to be charitable to ELUs. Over three runs we obtain the median convergence curves in Figure 7.
Meanwhile, the GELU achieves a median error of 20.74%, the ReLU obtains 21.77% ), and the ELU obtains 22.98%.
DISCUSSION
Across several experiments, the GELU outperformed previous nonlinearities, but it bears semblance
to the ReLU and ELU in other respects. For example, as σ →0 and if µ = 0, the GELU becomes
a ReLU. More, the ReLU and GELU are equal asymptotically. In fact, the GELU can be viewed
as a way to smooth a ReLU. To see this, recall that ReLU = max(x, 0) = x1(x > 0) (where
Figure 7: CIFAR-100 Wide Residual Network Results. Learning curves show training set convergence with dropout on, and the lighter curves show the test set convergence with dropout off.
1 is the indicator function), while the GELU is xΦ(x) if µ = 0, σ = 1. Then the CDF is a
smooth approximation to the binary function the ReLU uses, like how the sigmoid smoothed binary
threshold activations. Unlike the ReLU, the GELU and ELU can be both negative and positive. In
fact, if we used the cumulative distribution function of the standard Cauchy distribution, then the
ELU (when α = 1/π) is asymptotically equal to xP(C ≤x), C ∼Cauchy(0, 1) for negative
values and for positive values is xP(C ≤x) if we shift the line down by 1/π. These are some
fundamental relations to previous nonlinearities.
However, the GELU has several notable differences. This non-convex, non-monotonic function is
not linear in the positive domain and exhibits curvature at all points. Meanwhile ReLUs and ELUs,
which are convex and monotonic activations, are linear in the positive domain and thereby can lack
curvature. As such, increased curvature and non-monotonicity may allow GELUs to more easily
approximate complicated functions than can ReLUs or ELUs. Also, since ReLU(x) = x1(x > 0)
and GELU(x) = xΦ(x) if µ = 0, σ = 1, we can see that the ReLU gates the input depending
upon its sign, while the GELU weights its input depending upon how much greater it is than other
inputs. In addition and significantly, the GELU has a probabilistic interpretation given that it is the
expectation of a stochastic regularizer.
We also have two practical tips for using the GELU. First we advise using an optimizer with momentum when training with a GELU, as is standard for deep neural networks. Second, using a
close approximation to the cumulative distribution function of a Gaussian distribution is important.
A sigmoid function σ(x) = 1/(1 + e−x) is an approximation of a cumulative distribution function of a normal distribution. However, we found that a Sigmoid Linear Unit (SiLU)
xσ(x) performs worse than GELUs but usually better than ReLUs and ELUs, so our SiLU is
also a reasonable nonlinearity choice. Instead of using a xσ(x) to approximate Φ(x), we used
0.5x(1 + tanh[
2/π(x + 0.044715x3)]) 1 or xσ(1.702x). Both are sufficiently
fast, easy-to-implement approximations, and we used the former in every experiment in this paper.
CONCLUSION
For the numerous datasets evaluated in this paper, the GELU exceeded the accuracy of the ELU and
ReLU consistently, making it a viable alternative to previous nonlinearities.
1Thank you to Dmytro Mishkin for bringing an approximation like this to our attention.
ACKNOWLEDGMENT
We would like to thank NVIDIA Corporation for donating several TITAN X GPUs used in this