ADVANCES IN OPTIMIZING RECURRENT NETWORKS
Yoshua Bengio, Nicolas Boulanger-Lewandowski and Razvan Pascanu
U. Montreal
After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress
both in understanding and in technical solutions towards more efﬁcient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep
learning. Although recurrent networks are extremely powerful in
what they can in principle represent in terms of modeling sequences,
their training is plagued by two aspects of the same issue regarding
the learning of long-term dependencies. Experiments reported here
evaluate the use of clipping gradients, spanning longer time ranges
with leaky integration, advanced momentum techniques, using more
powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the
combined effects of these techniques in generally improving both
training and test error.
Index Terms— Recurrent networks, deep learning, representation learning, long-term dependencies
1. INTRODUCTION
Machine learning algorithms for capturing statistical structure in sequential data face a fundamental problem , called the difﬁculty
of learning long-term dependencies.
If the operations performed
when forming a ﬁxed-size summary of relevant past observations
(for the purpose of predicting some future observations) are linear,
this summary must exponentially forget past events that are further
away, to maintain stability. On the other hand, if they are non-linear,
then this non-linearity is composed many times, yielding a highly
non-linear relationship between past events and future events. Learning such non-linear relationships turns out to be difﬁcult, for reasons
that are discussed here, along with recent proposals for reducing this
difﬁculty.
Recurrent neural networks can represent such non-linear
maps (F, below) that iteratively build a relevant summary of past
observations.
In their simplest form, recurrent neural networks
(RNNs) form a deterministic state variable ht as a function of the
present input observation xt and the past value(s) of the state variable, e.g., ht = Fθ(ht−1, xt), where θ are tunable parameters that
control what will be remembered about the past sequence and what
will be discarded. Depending on the type of problem at hand, a loss
function L(ht, yt) is deﬁned, with yt an observed random variable
at time t and Ct = L(ht, yt) the cost at time t. The generalization
objective is to minimize the expected future cost, and the training
objective involves the average of Ct over observed sequences. In
principle, RNNs can be trained by gradient-based optimization procedures (using the back-propagation algorithm to compute a
gradient), but it was observed early on that capturing dependencies that span a long interval was difﬁcult, making the task of
optimizing θ to minimize the average of Ct’s almost impossible for
some tasks when the span of the dependencies of interest increases
sufﬁciently. More precisely, using a local numerical optimization
such as stochastic gradient descent or second order methods (which
gradually improve the solution), the proportion of trials (differing
only from their random initialization) falling into the basin of attraction of a good enough solution quickly becomes very small as
the temporal span of dependencies is increased (beyond tens or
hundreds of steps, depending of the task).
These difﬁculties are probably responsible for the major reduction in research efforts in the area of RNNs in the 90’s and 2000’s.
However, a revival of interest in these learning algorithms is taking
place, in particular thanks to and . This paper studies the issues giving rise to these difﬁculties and discusses, reviews, and combines several techniques that have been proposed in order to improve
training of RNNs, following up on a recent thesis devoted to the subject . We ﬁnd that these techniques generally help generalization
performance as well as training performance, which suggest they
help to improve the optimization of the training criterion. We also
ﬁnd that although these techniques can be applied in the online setting, i.e., as add-ons to stochastic gradient descent (SGD), they allow
to compete with batch (or large minibatch) second-order methods
such as Hessian-Free optimization, recently found to greatly help
training of RNNs .
2. LEARNING LONG-TERM DEPENDENCIES AND THE
OPTIMIZATION DIFFICULTY WITH DEEP LEARNING
There has been several breakthroughs in recent years in the algorithms and results obtained with so-called deep learning algorithms
(see and for reviews). Deep learning algorithms discover
multiple levels of representation, typically as deep neural networks
or graphical models organized with many levels of representationcarrying latent variables.
Very little work on deep architectures
occurred before the major advances of 2006 , probably
because of optimization difﬁculties due to the high level of nonlinearity in deeper networks (whose output is the composition of
the non-linearity at each layer). Some experiments showed the
presence of an extremely large number of apparent local minima of
the training criterion, with no two different initializations going to
the same function (i.e. eliminating the effect of permutations and
other symmetries of parametrization giving rise to the same function). Furthermore, qualitatively different initialization (e.g., using
unsupervised learning) could yield models in completely different
regions of function space. An unresolved question is whether these
difﬁculties are actually due to local minima or to ill-conditioning
(which makes gradient descent converge so slowly as to appear
stuck in a local minimum). Some ill-conditioning has clearly been
shown to be involved, especially for the difﬁcult problem of training
deep auto-encoders, through comparisons of stochastic gradient
descent and Hessian-free optimization (a second order optimization method).
These optimization questions become particularly
important when trying to train very large networks on very large
datasets , where one realizes that a major challenge for deep
learning is the underﬁtting issue. Of course one can trivially overﬁt
by increasing capacity in the wrong places (e.g. in the output layer),
but what we are trying to achieve is learning of more powerful
representations in order to also get good generalization.
The same questions can be asked for RNNs. When the computations performed by a RNN are unfolded through time, one clearly
sees a deep neural network with shared weights (across the ’layers’,
each corresponding to a different time step), and with a cost function
that may depends on the output of intermediate layers. Hessian-free
optimization has been successfully used to considerably extend the
span of temporal dependencies that a RNN can learn , suggesting that ill-conditioning effects are also at play in the difﬁculties of
training RNN.
An important aspect of these difﬁculties is that the gradient can
be decomposed into terms that involve products of Jacobians
∂ht−1 over subsequences linking an event at time t1 and one at time
∂ht1 = Qt2
∂hτ−1 . As t2 −t1 increases, the products of
t2 −t1 of these Jacobian matrices tend to either vanish (when the
leading eigenvalues of
∂ht−1 are less than 1) or explode (when the
leading eigenvalues of
∂ht−1 are greater than 11). This is problematic because the total gradient due to a loss Ct2 at time t2 is a sum
whose terms correspond to the effects at different time spans, which
are weighted by
∂ht1 for different t1’s:
∂θ(t1) is the derivative of ht1 with respect to the instantiation of the parameters θ at step t1, i.e., that directly come into the
computation of ht1 in F. When the
∂ht1 tend to vanish for increasing t2 −t1, the long-term term effects become exponentially smaller
in magnitude than the shorter-term ones, making it very difﬁcult to
capture them. On the other hand, when
∂ht1 “explode” (becomes
large), gradient descent updates can be destructive (move to poor
conﬁguration of parameters). It is not that the gradient is wrong, it is
that gradient descent makes small but ﬁnite steps ∆θ yielding a ∆C,
whereas the gradient measures the effect of ∆C when ∆θ →0. A
much deeper discussion of this issue can be found in , along with
a point of view inspired by dynamical systems theory and by the geometrical aspect of the problem, having to do with the shape of the
training criterion as a function of θ near those regions of exploding
gradient. In particular, it is argued that the strong non-linearity occurring where gradients explode is shaped like a cliff where not just
the ﬁrst but also the second derivative becomes large in the direction orthogonal to the cliff. Similarly, ﬂatness of the cost function
occurs simultaneously on the ﬁrst and second derivatives. Hence dividing the gradient by the second derivative in each direction (i.e.,
pre-multiplying by the inverse of some proxy for the Hessian matrix) could in principle reduce the exploding and vanishing gradient
effects, as argued in .
1 Note that this is not a sufﬁcient condition, but a necessary one. Further
more one usually wants to operate in the regime where the leading eigenvalue
is larger than 1 but the gradients do not explode.
3. ADVANCES IN TRAINING RECURRENT NETWORKS
3.1. Clipped Gradient
To address the exploding gradient effect, recently proposed
to clip gradients above a given threshold. Under the hypothesis that
the explosion occurs in very small regions (the cliffs in cost function mentioned above), most of the time this will have no effect, but
it will avoid aberrant parameter changes in those cliff regions, while
guaranteeing that the resulting updates are still in a descent direction.
The speciﬁc form of clipping used here was proposed in and is
discussed there at much greater length: when the norm of the gradient vector g for a given sequence is above a threshold, the update
is done in the direction threshold
||g||. As argued in , this very
simple method implements a very simple form of second order optimization in the sense that the second derivative is also proportionally
large in those exploding gradient regions.
3.2. Spanning Longer Time Ranges with Leaky Integration
An old idea to reduce the effect of vanishing gradients is to introduce shorter paths between t1 and t2, either via connections with
longer time delays or inertia (slow-changing units) in some of
the hidden units , or both . Long-Short-Term Memory (LSTM) networks , which were shown to be able to handle much longer range dependencies, also beneﬁt from a linearly
self-connected memory unit with a near 1 self-weight which allows
signals (and gradients) to propagate over long time spans.
A different interpretation to this slow-changing units is that they
behave like low-pass ﬁlter and hence they can be used to focus certain units on different frequency regions of the data. The analogy
can be brought one step further by introducing band-pass ﬁlter units
 or by using domain speciﬁc knowledge to decide on what frequency bands different units should focus. shows that adding
low frequency information as an additional input to a recurrent network helps improving the performance of the model.
In the experiments performed here, a subset of the units were
forced to change slowly by using the following “leaky integration”
state-to-state map: ht,i = αiht−1,i + (1 −αi)Fi(ht−1, xt). The
standard RNN corresponds to αi = 0, while here different values
of αi were randomly sampled from (0.02, 0.2), allowing some units
to react quickly while others are forced to change slowly, but also
propagate signals and gradients further in time. Note that because
α < 1, the vanishing effect is still present (and gradients can still
explode via F), but the time-scale of the vanishing effect can be
3.3. Combining Recurrent Nets with a Powerful Output Probability Model
One way to reduce the underﬁtting of RNNs is to introduce multiplicative interactions in the parametrization of F, as was done successfully in . When the output predictions are multivariate, another approach is to capture the high-order dependencies between
the output variables using a powerful output probability model such
as a Restricted Boltzmann Machine (RBM) or a deterministic variant of it called NADE . In the experiments performed
here, we have experimented with a NADE output model for the music data.
3.4. Sparser Gradients via Sparse Output Regularization and
Rectiﬁed Outputs
 hypothesized that one reason for the difﬁculty in optimizing
deep networks is that in ordinary neural networks gradients diffuse
through the layers, diffusing credit and blame through many units,
maybe making it difﬁcult for hidden units to specialize.
the gradient on hidden units is more sparse, one could imagine
that symmetries would be broken more easily and credit or blame
assigned less uniformly. This is what was advocated in , exploiting the idea of rectiﬁer non-linearities introduced earlier in
 , i.e., the neuron non-linearity is out = max(0, in) instead
of out = tanh(in) or out = sigmoid(in). This approach was
very successful in recent work on deep learning for object recognition , beating by far the state-of-the-art on ImageNet (1000
classes). Here, we apply this deep learning idea to RNNs, using
an L1 penalty on outputs of hidden units to promote sparsity of
activations. The underlying hypothesis is that if the gradient is concentrated in a few paths (in the unfolded computation graph of the
RNN), it will reduce the vanishing gradients effect.
3.5. Simpliﬁed Nesterov Momentum
Nesterov accelerated gradient (NAG) is a ﬁrst-order optimization method to improve stability and convergence of regular gradient
descent. Recently, showed that NAG could be computed by the
following update rules:
vt = µt−1vt−1 −ǫt−1∇f(θt−1 + µt−1vt−1)
θt = θt−1 + vt
where θt are the model parameters, vt the velocity, µt ∈ the
momentum (decay) coefﬁcient and ǫt > 0 the learning rate at iteration t, f(θ) is the objective function and ∇f(θ′) is a shorthand
notation for the gradient ∂f(θ)
∂θ |θ=θ′. These equations have a form
similar to standard momentum updates:
vt = µt−1vt−1 −ǫt−1∇f(θt−1)
θt = θt−1 + vt
= θt−1 + µt−1vt−1 −ǫt−1∇f(θt−1)
and differ only in the evaluation point of the gradient at each iteration. This important difference, thought to counterbalance too high
velocities by “peeking ahead” actual objective values in the candidate search direction, results in signiﬁcantly improved RNN performance on a number of tasks.
In this section, we derive a new formulation of Nesterov momentum differing from (3) and (5) only in the linear combination
coefﬁcients of the velocity and gradient contributions at each iteration, and we offer an alternative interpretation of the method. The
key departure from (1) and (2) resides in committing to the “peekedahead” parameters Θt−1 ≡θt−1 + µt−1vt−1 and backtracking by
the same amount before each update. Our new parameters Θt updates become:
vt = µt−1vt−1 −ǫt−1∇f(Θt−1)
Θt = Θt−1 −µt−1vt−1 + µtvt + vt
= Θt−1 + µtµt−1vt−1 −(1 + µt)ǫt−1∇f(Θt−1)
Assuming a zero initial velocity v1 = 0 and velocity at convergence
of optimization vT ≃0, the parameters Θ are a completely equivalent replacement of θ.
Note that equation (7) is identical to regular momentum (5)
with different linear combination coefﬁcients. More precisely, for an
equivalent velocity update (6), the velocity contribution to the new
parameters µtµt−1 < µt is reduced relatively to the gradient contribution (1 + µt)ǫt−1 > ǫt−1. This allows storing past velocities
for a longer time with a higher µ, while actually using those velocities more conservatively during the updates. We suspect this mechanism is a crucial ingredient for good empirical performance. While
the “peeking ahead” point of view suggests that a similar strategy
could be adapted for regular gradient descent (misleadingly, because
it would amount to a reduced learning rate ǫt), our derivation shows
why it is important to choose search directions aligned with the current velocity to yield substantial improvement. The general case is
also simpler to implement.
4. EXPERIMENTS
In the experimental section we compare vanilla SGD versus SGD
plus some of the enhancements discussed above. Speciﬁcally we
use the letter ‘C‘ to indicate that gradient clipping is used, ‘L‘ for
leaky-integration units, ‘R‘ if we use rectiﬁer units with L1 penalty
and ‘M‘ for Nesterov momentum.
4.1. Music Data
We evaluate our models on the four polyphonic music datasets of
varying complexity used in : classical piano music (Pianomidi.de), folk tunes with chords instantiated from ABC notation (Nottingham), orchestral music (MuseData) and the four-part
chorales by J.S. Bach (JSB chorales). The symbolic sequences contain high-level pitch and timing information in the form of a binary
matrix, or piano-roll, specifying precisely which notes occur at each
time-step. They form interesting benchmarks for RNNs because of
their high dimensionality and the complex temporal dependencies
involved at different time scales. Each dataset contains at least 7
hours of polyphonic music with an average polyphony (number of
simultaneous notes) of 3.9.
Piano-rolls were prepared by aligning each time-step (88 pitch
labels that cover the whole range of piano) on an integer fraction
of the beat (quarter note) and transposing each sequence in a common tonality (C major/minor) to facilitate learning. Source ﬁles and
preprocessed piano-rolls split in train, validation and test sets are
available on the authors’ website2.
4.1.1. Setup and Results
We select hyperparameters, such as the number of hidden units nh,
regularization coefﬁcients λL1, the choice of non-linearity function,
or the momentum schedule µt, learning rate ǫt, number of leaky
units nleaky or leaky factors α according to log-likelihood on a validation set and we report the ﬁnal performance on the test set for the
best choice in each category. We do so by using random search 
on the following intervals:
nh ∈ 
ǫt ∈[10−4, 10−1]
µt ∈[10−3, 0.95]
λL1 ∈[10−6, 10−3]
nleaky ∈{0%, 25%, 50%}
α ∈[0.02, 2]
The cutoff threshold for gradient clipping is set based on the
average norm of the gradient over one pass on the data, and we used
15 in this case for all music datasets. The data is split into sequences
2www-etud.iro.umontreal.ca/˜boulanni/icml2012
Table 1. Log-likelihood and expected accuracy for various RNN models in the symbolic music prediction task. The double line separates
sigmoid recognition layers (above) to structured output probability models (below).
Piano-midi.de
Nottingham
JSB chorales
RNN (SGD+C)
RNN (SGD+CL)
RNN (SGD+CLR)
RNN (SGD+CRM)
RNN-NADE (SGD)
RNN-NADE (SGD+CR)
RNN-NADE (SGD+CRM)
RNN-NADE (HF)
Table 2. Entropy (bits per character) and perplexity for various RNN models on next character and next word prediction task.
Penn Treebank Corpus
Penn Treebank Corpus
word level
character level
perplexity
perplexity
RNN (SGD+C)
RNN (SGD+CL)
RNN (SGD+CLR)
of 100 steps over which we compute the gradient. The hidden state
is carried over from one sequence to another if they belong to the
same song, otherwise is set to 0.
Table 1 presents log-likelihood (LL) and expected frame-level
accuracy for various RNNs in the symbolic music prediction task.
Results clearly show that these enhancements allow to improve
on regular SGD in almost all cases; they also make SGD competitive
with HF for the sigmoid recognition layers RNNs.
4.2. Text Data
We use the Penn Treebank Corpus to explore both word and character prediction tasks. The data is split by using sections 0-20 as
training data (5017k characters), sections 21-22 as validation (393k
characters) and sections 23-24 as test data (442k characters).
For the word level prediction, we ﬁx the dictionary to 10000
words, which we divide into 30 classes according to their frequency
in text (each class holding approximately 3.3% of the total number
of tokens in the training set). Such a factorization allows for faster
implementation, as we are not required to evaluate the whole output
layer (10000 units) which is the computational bottleneck, but only
the output of the corresponding class .
4.2.1. Setup and Results
In the case of next word prediction, we compute gradients over sequences of 40 steps, where we carry the hidden state from one sequence to another. We use a small grid-search around the parameters
used to get state of the art results for this number of classes , i.e.,
with a network of 200 hidden units yielding a perplexity of 134. We
explore learning rate of 0.1, 0.01, 0.001, rectiﬁer units versus sigmoid units, cutoff threshold for the gradients of 30, 50 or none, and
no leaky units versus 50 of the units being sampled from 0.2 and
For the character level model we compute gradients over sequences of 150 steps, as we assume that longer dependencies are
more crucial in this case. We use 500 hidden units and explore learning rates of 0.5, 0.1 and 0.01.
In table 2 we have entropy (bits per character) or perplexity for
varous RNNs on the word and character prediction tasks. Again, we
observe substantial improvements in both training and test perplexity, suggesting that these techniques make optimization easier.
5. CONCLUSIONS
Through our experiments we provide evidence that part of the issue
of training RNN is due to the rough error surface which can not be
easily handled by SGD. We follow an incremental set of improvements to SGD, and show that in most cases they improve both the
training and test error, and allow this enhanced SGD to compete or
even improve on a second-order method which was found to work
particularly well for RNNs, i.e., Hessian-Free optimization.
6. REFERENCES
 S. Hochreiter, “ Untersuchungen zu dynamischen neuronalen
Netzen. Diploma thesis, T.U. M¨unich,” 1991.
 Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies with gradient descent is difﬁcult,” IEEE T. Neural
Nets, 1994.
 D.E. Rumelhart, G.E. Hinton, and R.J. Williams, “Learning
representations by back-propagating errors,” Nature, vol. 323,
pp. 533–536, 1986.
 J. Martens and I. Sutskever, “Learning recurrent neural networks with Hessian-free optimization,” in ICML’2011, 2011.
 T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur,
“Extensions of recurrent neural network language
model,” in ICASSP 2011, 2011.
 I. Sutskever, Training Recurrent Neural Networks, Ph.D. thesis, CS Dept., U. Toronto, 2012.
 Yoshua Bengio, Learning deep architectures for AI, Now Publishers, 2009.
 Y. Bengio, A. Courville, and P. Vincent, “Unsupervised feature
learning and deep learning: A review and new perspectives,”
Tech. Rep., arXiv:1206.5538, 2012.
 G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning
algorithm for deep belief nets,” Neural Computation, vol. 18,
pp. 1527–1554, 2006.
 Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle,
“Greedy layer-wise training of deep networks,” in NIPS’2006,
 M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun,
“Efﬁcient learning of sparse representations with an energy-based
model,” in NIPS’2006, 2007.
 D. Erhan, Y. Bengio, A. Courville, P. Manzagol, P. Vincent,
and S. Bengio, “Why does unsupervised pre-training help deep
learning?,” J. Machine Learning Res., (11) 2010.
 J. Martens, “Deep learning via Hessian-free optimization,” in
ICML’2010, 2010, pp. 735–742.
 Q. Le, M. Ranzato, R. Monga, M. Devin, G. Corrado, K. Chen,
J. Dean, and A. Ng, “Building high-level features using large
scale unsupervised learning,” in ICML’2012, 2012.
 Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio, “Understanding the exploding gradient problem,” Tech. Rep., Universit´e De Montr´eal, 2012, arXiv:arXiv:1211.5063.
 Tomas Mikolov, Statistical Language Models based on Neural
Networks, Ph.D. thesis, Brno University of Technology, 2012.
 T. Lin, B. G. Horne, P. Tino, and C. L. Giles, “Learning longterm dependencies is not as difﬁcult with NARX recurrent neural networks,” Tech. Rep. UMICAS-TR-95-78, U. Mariland,
 S. ElHihi and Y. Bengio, “Hierarchical recurrent neural networks for long-term dependencies,” in NIPS’1995, 1996.
 Herbert Jaeger, Mantas Lukosevicius, Dan Popovici, and Udo
Siewert, “Optimization and applications of echo state networks
with leaky- integrator neurons,” Neural Networks, vol. 20, no.
3, pp. 335–352, 2007.
 I. Sutskever and G. Hinton, “Temporal kernel recurrent neural
networks,” Neural Networks, vol. 23, no. 2, (23) 2, 2010.
 S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.
 Udo Siewert and Welf Wustlich, “Echo-state networks with
band-pass neurons: Towards generic time-scale-independent
reservoir structures,” Preliminary Report, October 2007.
 Tomas Mikolov and Geoffrey Zweig, “Context dependent reucrrent neural network language model,” Workshop on Spoken
Language Technology, 2012.
 I. Sutskever, G. Hinton, and G. Taylor, “The recurrent temporal
restricted Boltzmann machine,” in NIPS’2008. 2009.
 N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent, “Modeling temporal dependencies in high-dimensional sequences:
Application to polyphonic music generation and transcription,”
in ICML’2012, 2012.
 H. Larochelle and I. Murray, “The Neural Autoregressive Distribution Estimator,” in AISTATS’2011, 2011.
 X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectiﬁer
neural networks,” in AISTATS’2011, 2011.
 V. Nair and G.E. Hinton, “Rectiﬁed linear units improve restricted Boltzmann machines,” in ICML’2010, 2010.
 A. Krizhevsky, I. Sutskever, and G. Hinton,
classiﬁcation with deep convolutional neural networks,”
NIPS’2012. 2012.
 Yu Nesterov, “A method for unconstrained convex minimization problem with the rate of convergence o(1/k2),” Doklady
AN SSSR (translated as Soviet. Math. Docl.), vol. 269, pp. 543–
547, 1983.
 James Bergstra and Yoshua Bengio,
“Random search for
hyper-parameter optimization,” J. Machine Learning Res., vol.
13, pp. 281–305, 2012.
 Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur, “Extensions of recurrent neural network language model,”
in Proc. 2011 IEEE international conference on acoustics, speech and signal processing
 , 2011.