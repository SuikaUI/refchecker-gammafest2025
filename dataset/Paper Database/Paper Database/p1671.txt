Ensemble deep learning: A review
M.A. Ganaiea, Minghui Hub, A.K. Malika, M. Tanveera,∗, P.N. Suganthanb,c,∗
aDepartment of Mathematics, Indian Institute of Technology Indore, Simrol, Indore, 453552, India
bSchool of Electrical & Electronic Engineering, Nanyang Technological University, Singapore
cKINDI Center for Computing Research College of Engineering, Qatar University, Qatar
Ensemble learning combines several individual models to obtain better generalization performance. Currently, deep learning architectures are showing better performance compared to the
shallow or traditional models. Deep ensemble learning models combine the advantages of both
the deep learning models as well as the ensemble learning such that the ﬁnal model has better generalization performance. This paper reviews the state-of-art deep ensemble models and
hence serves as an extensive summary for the researchers. The ensemble models are broadly
categorised into bagging, boosting, stacking, negative correlation based deep ensemble models,
explicit/implicit ensembles, homogeneous/heterogeneous ensemble, decision fusion strategies
based deep ensemble models. Applications of deep ensemble models in diﬀerent domains are
also brieﬂy discussed. Finally, we conclude this paper with some potential future research directions.
Keywords: Ensemble Learning, Deep Learning.
1. Introduction
Deep learning architectures have been successfully employed across a wide range of applications from image/video classiﬁcation to the health care. The success of these models is attributed
to the better feature representation via multi layer processing architectures. The deep learning
models have been mainly used for classiﬁcation, regression and clustering problems. Classiﬁcation problem is deﬁned as the categorization of the new observations based on the hypothesis
∗Corresponding authors
Email addresses: (M.A. Ganaie), (Minghui Hu),
 (A.K. Malik), (M. Tanveer), (P.N.
Suganthan)
 
August 9, 2022
 
h learned from the set of training data. The hypothesis h represents a mapping of input data
features to the appropriate target labels/classes. The main objective, while learning the hypothesis h, is that it should approximate the true unknown function as close as possible to reduce the
generalization error. There exist several applications of these classiﬁcation algorithms ranging
from medical diagnosis to remote sensing. Mathematically,
Oc = h(x, θc), Oc ∈Z,
where x is the input feature vector, Oc is the category of the sample x, θc is the set of learning
parameters of the hypothesis h and Z is the set of class labels.
Regression problems deal with the continuous decisions, instead of discrete categories. Mathematically,
Or = h(x, θr), Or ∈R,
where x is the observation vector, Or is the output, and θr is the set of learning parameters of the
hypothesis h.
Broadly speaking, there are diﬀerent approaches of classiﬁcation like supervised, unsupervised classiﬁcation, few-shot, one-shot and so on. Here, we only discuss supervised and unsupervised classiﬁcation problems. In supervised learning, the building of hypothesis h is supervised
based on the known output labels provided in the training data samples, while as in unsupervised
learning hypothesis h is generated without any supervision as no known output values are available with the training data. This approach, also known as clustering, generates the hypothesis h
based on the similarities and dissimilarities present in the training data.
Generally speaking, the goal of generating the hypothesis h in Machine learning area is that it
should perform better when applied to unknown data. The performance of the model is measured
with respect to the area in which the model is applied. Combining the predictions from several
models has proven to be an elegant approach for increasing the performance of the models.
Combination of several diﬀerent predictions from diﬀerent models to make the ﬁnal prediction is
known as ensemble learning or ensemble model. The ensemble learning involves multiple models combined in some fashion like averaging, voting such that the ensemble model is better than
any of the individual models. To prove that average voting in an ensemble is better than individual model, Marquis de Condorcet proposed a theorem wherein he proved that if the probability
of each voter being correct is above 0.5 and the voters are independent, then addition of more
voters increases the probability of majority vote being correct until it approaches 1 . Although
Marquis de Condorcet proposed this theorem in the ﬁeld of political science and had no idea of
the ﬁeld of Machine learning, but it is the similar mechanism that leads to better performance of
the ensemble models. Assumptions of Marquis de Condorcet theorem also holds true for ensembles . The reasons for the success of ensemble learning include: statistical, computational and
representation learning , bias-variance decomposition and strength-correlation .
In this era of machine learning, deep learning automates the extraction of high-level features
via hierarchical feature learning mechanism wherein the upper layer of features are generated
on the previous set of layer/layers. Deep learning has been successfully applied across diﬀerent ﬁelds since the ImageNet Large Scale Recognition Challenge (ILSVRC) competitions 
and has achieved state-of-art performance. It has obtained promising results in object detection,
semantic segmentation, edge detection and number of other domains. However, given the computational cost, the training of deep ensemble models is an uphill task. Diﬀerent views have been
provided to understand how the deep learning models learn the features like learning through hierarchy of concepts via many levels of representation . Given the advantages of deep
learning models from deep architectures, there are several bottlenecks like vanishing/exploding
gradients and degradation problem which prevent to reach this goal. Recently,
training deep network’s has become feasible through the Highway networks and Residual
networks . Both these networks enabled to train very deep networks. The ensemble learning
has been recently known to be strong reason for enhancing the performance of deep learning
models . Thus, the objective of deep ensemble models is to obtain a model that has best of
both the ensemble and deep models.
There exist multiple surveys in the literature which mainly focus on the review of ensemble
learning like learning of ensemble models in classiﬁcation problems , regression problems and clustering . Review of both the classiﬁcation and regression
models was given in . Comprehensive review of the ensemble methods and the challenges
were given in . Though provided some insight about the deep ensemble models but
couldn’t give the comprehensive review of the deep ensemble learning while as reviewed
the ensemble deep models in the context of bioinformatics. The past decade has successively
evolved diﬀerent deep learning strategies which have lead to the exploration and innovation of
these models in multiple areas like health care, speech, image classiﬁcation, forecasting and other
applications. Broadly speaking, ensemble learning approaches have followed classical methods,
general methods and diﬀerent fusion strategies for improving the performance of the models.
Since deep learning models are computation and data extensive, hence, ensemble deep learning models need special attention while exploring the complementary information of multiple
algorithms into a uniform framework. Ensemble deep learning models need to handle multiple
questions like how to induce diversity among the baseline models, how to keep the training time
as well the models complexity lower for the practical applications, how to fuse the predictions of
the complementary algorithms. Multiple studies have handled these problems diﬀerently. In this
review paper, we comprehensively review the diﬀerent approaches used to handle the aforementioned problems. In this paper, we give a comprehensive review of deep ensemble models. To
the best of our knowledge, this is the ﬁrst comprehensive review paper on deep ensemble
The rest of this paper is organised as follows: Section-3 discusses the theoretical aspects
of deep ensemble learning, Section-4 discusses the diﬀerent approaches used in deep ensemble
strategies, applications of deep ensemble methods are given in Section-5 and ﬁnally conclusions
and future directions are given in Section-6.
Ensemble Deep Learning
Bias-Variance
Decomposition
Statistical Computational
and Representational
Ensemble Strategy
Explicit \ Implicit
Negative correlation
Homogeneous /
Heterogeneous
Fusion Strategy
Majority Voting
Unweighted Model
Bayes Optimal
Classifier
Generalization
Super Learner
Query-By-Committee
Applications
Health Care
Image Classification
Forecasting
Conclusion and
Future Work
Figure 1: Layout of the paper.
2. Research Methodology
The studies in this review are searched from the Google Scholar and Scopus search engines.
The papers are the result of ensemble learning, ensemble deep learning, deep ensemble learning,
deep ensembles keywords. The articles where screened based on the title and abstract, followed
by the screening of full-text version. The articles are elaborated based on the ensemble learning
and deep learning approaches.
The various reasons which have been justiﬁed for the success of ensemble learning can be
discussed under the following subheadings:
3.1. Bias-Variance Decomposition
Initially, the success of ensemble methods was theoretically investigated for regression problems. Krogh and Vedelsby , Brown et al. proved via ambiguity decomposition that the
proper ensemble classiﬁer guarantees a smaller squared error as compared to the individual predictors of the classiﬁer. Ambiguity decomposition was given for single dataset based ensemble
methods, later on, multiple dataset bias-variance-covariance decomposition was introduced in
 and is given as:
E[o −t]2 = bias2 + 1
M var + (1 −1
(E[oi] −t),
E[oi −E[oi]]2,
E[oi −E[oi]][o j −E[oj]],
where t is target, oi is the output of ith model and M is the ensemble size. Here, bias term
measures the average diﬀerence between the base learner and the model output, var indicates
their average variance, and covar is the covariance term measuring the pairwise diﬀerence of the
base learners.
Ensemble methods have been supported by several theories like bias-variance , strength
correlation , stochastic discrimination , and margin theory . These theories provide
the equivalent of bias-variance-covariance decomposition .
The above given equations of decomposition error can’t be directly applied to the datasets
with discrete class labels due to their categorical nature. However, alternate ways to decompose
the error in classiﬁcation problems are given in .
Multiple approaches like bagging, boosting have been proposed for generating the ensemble
methods. Bagging reduces the variance among the base classiﬁers while as boosting based
ensembles lead to the bias and variance reduction .
3.2. Statistical, Computational and Representational Aspects
Dietterich provided Statistical, Computational and Representational reasons for success
of ensemble models. The learning model is viewed as the search of the optimal hypothesis h
among the several hypothesis in the search space. When the amount of data available for the
training is smaller compared to the size of the hypothesis space, the statistical problem arises.
Due to this statistical problem, the learning algorithm identiﬁes the diﬀerent hypothesis which
gives same performance on the training samples. Ensembling of these hypothesis results in an
algorithm which reduces the risk of being a wrong classiﬁer. The second reason is computational
wherein a learning algorithm stucks in a local optima due to some form of local search. Ensemble model overcomes this issue by performing some form of local search via diﬀerent starting
points which leads to better approximation of the true unknown function. Another reason is representational wherein none of the hypotheses among the set of hypothesis is able to represent the
true unknown function. Hence, ensembling of these hypothesis via some weighting technique
results into the hypothesis which expands the representable function space.
3.3. Diversity
One of the main reasons behind the success of ensemble methods is increasing the diversity
among the base classiﬁers and the same thing was highlighted in . Diﬀerent approaches
have been followed to generate diverse classiﬁers. Diﬀerent methods like bootstrap aggregation
(bagging) , Adaptive Boosting (AdaBoost) , random subspace , and random forest
 approaches are followed for generating the multiple datasets from the original dataset to train
the diﬀerent predictors such that the outputs of predictors are diverse. Attempts have been made
to increase diversity in the output data wherein multiple outputs are created instead of multiple
datasets for the supervision of the base learners. ‘Output smearing’ is one of this kind which
induces random noise to introduce diversity in the output space.
4. Ensemble Strategies:
The diﬀerent ensemble strategies have evolved over a period of time which results in better generalization of the learning models. The ensemble strategies are broadly categorised as
4.1. Bagging
Bagging , also known as bootstrap aggregating, is one of the standard techniques for
generating the ensemble-based algorithms. Bagging is applied to enhance the performance of an
ensemble classiﬁer. The main idea in bagging is to generate a series of independent observations
with the same size, and distribution as that of the original data. Given the series of observations, generate an ensemble predictor which is better than the single predictor generated on the
original data. Bagging increases two steps in the original models: First, generating the bagging
samples and passing each bag of samples to the base models and second, strategy for combining
the predictions of the multiple predictors. Bagging samples may be generated with or without
replacement. Combining the output of the base predictors may vary as mostly majority voting is
used for classiﬁcation problems while the averaging strategy is used in regression problems for
generating the ensemble output. Figure 2 shows the diagram of the bagging technique. Here, Di
represents the bagged datasets, Ci represents the algorithms and Fens calculates the ﬁnal outcome.
Random Forest is an improved version of the decision trees that uses the bagging strategy
for improving the predictions of the base classiﬁer which is a decision tree. The fundamental
diﬀerence between these two methods is that at each tree split in Random Forest, only a subset
of features is randomly selected and considered for splitting. The purpose of this method is to
decorrelate the trees and prevent over-ﬁtting. Breiman showed heuristically that the variance
of the bagged predictor is smaller than the original predictor and proposed that bagging is better in higher dimensional data. However, the analysis of the smoothing eﬀect of bagging 
revealed that bagging doesn’t depend on the data dimensionality.
Figure 2: Bagging
Figure 3: Boosting
Figure 4: Stacking
B¨uhlmann and Yu gave theoretical explanation of how bagging gives smooth hard decisions, small variance, and mean squared error. Since bagging is computationally expensive,
hence subbagging and half subbagging were introduced. Half subbagging, being computationally eﬃcient, is as accurate as the bagging.
Several attempts tried to combine bagging with other machine learning algorithms. Kim et al.
 used bagging method to generate multiple bags of the dataset and multiple support vector
machines were trained independently with each bag as the input. The output of the models is
combined via majority voting, least squares estimation weighting and double layer hierarchical
approach. In the double layer hierarchical approach, another support vector machines (SVM) is
used to combine the outcomes of the multiple SVM’s eﬃciently. Tao et al. used asymmetric
bagging strategy to generate the ensemble model to handle the class imbalance problems. A
case study of bagging, boosting and basic ensembles revealed that at higher rejection rates
of samples boosting is better as compared to bagging and basic ensembles. However, as the
rejection rate increases the diﬀerence disappears among the boosting, bagging and basic ensembles. Bagging based multilayer perceptron combined bagging to train multiple perceptrons
with the corresponding bag and showed that bagging based ensemble models perform better as
compared to individual multilayer perceptron. In , the analysis of the bagging approach and
other regularisation techniques revealed that bagging regularized the neural networks and hence
provide better generalization. In , bagged neural networks (BNNs) was proposed wherein
each neural network was trained over diﬀerent dataset sampled randomly with replacement from
original dataset and was implemented for the short term load forecasting. Unlike Random forest
 which uses majority voting for aggregating the ensemble of decision trees, bagging based
survival trees used Kaplan–Meier curve to predict the ensemble output for breast cancer
and lymphoma patients. In , ensembles of stacked denoising autoencoders for classiﬁcation
showed that the bagging and switching technique in a general deep machine results in improved
diversity.
Bagging has also been applied to solve the problem of imbalanced data. Roughly Balanced
Bagging tries to equalize each class’s sampling probability in binary class problems wherein
the negative class samples are sampled via negative binomial distribution, instead of keeping the
sample size of each class the same number. Neighbourhood Balanced Bagging incorporated the neighbourhood information for generating the bagging samples for the class imbalance
Contribution
Breiman 
Proposed the idea of Bagging
Case study of bagging, boosting and basic ensembles
Buja and Stuetzle 
Theoretical analysis of bagging
Breiman 
Bagging with random subspace Decision trees and ensembling outputs via majority voting
Genc¸ay and Qi 
Study of Bayesian regularization, early stopping and Bagging
Kim et al. 
Bagging with SVM’s and ensembling outputs via SVM’s, majority voting and least squares estimation
B¨uhlmann and Yu 
Theoretical justiﬁcation of Bagging, proposed subbagging and half subagging
Hothorn et al. 
Bagging with decision trees and ensembling outputs via Kaplan–Meier curve
Theoretical and experimental analysis of online bagging and boosting
Tao et al. 
Proposed assymmetric bagging with SVM’s and ensembling outputs SVM’s
Hido et al. 
Roughly balanced bagging on decision trees and ensembling outputs via majority voting
2005, 2015
Ha et al. , Khwaja et al. 
Bagging with Neural networks and ensembling outputs via majority voting
Błaszczy´nski and Stefanowski 
Neighbourhood balanced bagging ensembling outputs via majority voting
Table 1: Bagging based ensemble models
problems. Błaszczy´nski and Stefanowski concluded that applying conventional diversiﬁcation is more eﬀective when applied at the last classiﬁcation methods. Both roughly balanced
Bagging and Neighbourhood Balanced Bagging have not been explored in deep learning architectures. Thus, these approaches can be exploited to handle the class imbalance problems via
deep ensemble models.
The theoretical and experimental analysis of online bagging and boosting showed that
the online bagging algorithm can achieve similar accuracy as the batch bagging algorithm with
only a little more training time. However, online bagging is an option when all training samples
can’t be loaded into the memory due to memory issues.
Although ensembling may lead to increase in the computational complexity, but bagging
possesses the property that it can be paralleled and can lead to eﬀective reduction in the training
time subject to the availability of hardware for running the parallel models. Since deep learning
models have high training time, hence optimization of multiple deep models on diﬀerent training
bags is not a feasible option.
4.2. Boosting
Boosting technique is used in ensemble models for converting a weak learning model into a
learning model with better generalization. Figure 3 shows the diagram of the boosting technique.
The techniques such as majority voting in case of classiﬁcation problems or a linear combination of weak learners in the regression problems results in better prediction as compared to the
single weak learner. Boosting methods like AdaBoost and Gradient Boosting have
been used across diﬀerent domains. Adaboost uses a greedy technique for minimizing a convex
surrogate function upper bounded by misclassiﬁcation loss via augmentation, at each iteration,
the current model with the appropriately weighted predictor. AdaBoost learns an eﬀective ensemble classiﬁer as it leverages the incorrectly classiﬁed sample at each stage of the learning.
AdaBoost minimizes the exponential loss function while as the Gradient boosting generalized
this framework to the arbitrary diﬀerential loss function.
Boosting, also known as forward stagewise additive modelling, was originally proposed to
improve the performance of the classiﬁcation trees. It has been recently incorporated in the deep
learning models to further improve their performance.
Boosted deep belief network (DBN) for facial expression recognition uniﬁed the boosting technique and multiple DBN’s via objective function which results in a strong classiﬁer. The
model learns complex feature representation to build a strong classiﬁer in an iterative manner.
Deep boosting is an ensemble model that uses the deep decision trees. It can also be used in
combination with any other rich family classiﬁer to improve the generalization performance. In
each stage of the deep boosting, the decisions of which classiﬁer to add and what weights should
be chosen depends on the (data-dependent) complexity of the classiﬁer to which it belongs. The
interpretation of the deep boosting classiﬁer is given via structural risk minimization principle at
each stage of the learning. Multiclass Deep boosting extended the Deep boosting algorithm to theoretical, algorithmic, and empirical results to the multiclass problems. Due to the
limitation of the training data in each mini batch, Boosting CNN may overﬁt the data. To avoid
overﬁtting, incremental Boosting CNN (IBCNN) accumulated the information of multiple
batches of the training data samples. IBCNN uses decision stumps on the top of single neurons
as the weak learners and learns weights via AdaBoost method in each mini batch. Unlike DBN
 which uses image patch for learning the weak classiﬁers, IBCNN trains the weak classiﬁers
from the fully connected layer i.e. the whole image is used for learning the weak classiﬁers. To
make the IBCNN model more eﬃcient, the weak learners loss functions are combined with the
global loss function.
Boosted CNN used boosting for training the deep CNN. Instead of averaging, least
squares objective function was used to incorporate the boosting weights into CNN. Moghimi
et al. also showed that CNN can be replaced by network structure within their boosting
framework for improving the performance of the base classiﬁer. Boosting increases the complexity of training the networks, hence the concept of dense connections was introduced in a
deep boosting framework to overcome the problem of vanishing gradient problem for image denoising . Deep boosting framework was extended to image restoration in wherein the
dilated dense fusion network was used to boost the performance.
The convolutional channel features generated the high level features via CNN and then
used boosted forest for ﬁnal classiﬁcation. Since CNN has high number of hyperparameters
than the boosted forest, hence the model proved to be eﬃcient than end-to-end training of CNN
models both in terms of performance and time. Yang et al. showed its application in edge
detection, object proposal generation, pedestrian and face detection. A stagewise boosting deep
CNN trains several models of the CNNs within the oﬄine paradigm boosting framework.
To extend the concept of boosting in online scenario’s wherein only a chunk of data is available
at given time, Boosting Independent Embeddings Robustly (BIER) was proposed to cope
up the online scenario’s. In BIER, a single CNN model is trained end-to-end with an online
boosting technique. The training set in the BIER is reweighed via the negative gradient of the
loss function to project the input spaces (images) into a collection of independent output spaces.
To make BIER more robust, Hierarchical Boosted deep metric learning incorporated the
hierarchical label information into the embedding ensemble which improves the performance of
the model on the large scale image retrieval application. Using deep boosting results in higher
training time, to reduce the warm-up phase of training which trains the classiﬁer from scratch
deep incremental boosting used transfer learning approach. This approach leveraged the
initial warm-up phase of each incremental base model of the ensemble during the training of
the network. To reduce the training time of boosting based ensembles, snapshot boosting 
combined the merits of snapshot ensembling and boosting to improve the generalization without
increasing the cost of training. Snapshot boosting trains each base network and combines the
outputs via meta learner to combine the output of base learners more eﬃciently.
Literature shows that the boosting concept is the backbone behind well-known architectures
like Deep Residual networks , AdaNet . The theoretical background for the success
of the Deep Residual networks (DeepResNet) was explained in the context of boosting
theory . The authors proposed multi-channel telescoping sum boosting learning framework,
known as BoostResNet, wherein each channel is a scalar value updated during rounds of boosting
Contribution
Liu et al. 
Boosted deep belief network (DBN) as base classiﬁers for facial expression recognition.
Cortes et al. 
Decision trees as base classiﬁers for binary class classiﬁcation problems.
Kuznetsov et al. 
Decision trees as base classiﬁers for multiclass classiﬁcation problems.
Yang et al. 
Ensemble of CNN and boosted forest for edge detection, object proposal generation, pedestrian and face detection.
Moghimi et al. 
Boosted CNN
Walach and Wolf 
CNN Boosting applied to bacterila cell images and crowd counting.
Opitz et al. 
Boosted deep independent embedding model for online scenarios.
Mosca and Magoulas 
Transfer learning based deep incremental boosting.
Han et al. 
Boosting based CNN with incremental approach for facial action unit recognition.
Chen et al. 
Deep boosting for image denoising with dense connections.
Chen et al. 
Deep boosting for image restoration and image denoising.
Waltner et al. 
Hierarchical boosted deep metric learning with hierarchical label embedding.
Zhang et al. 
Snapshot boosting.
Table 2: Boosting based ensemble models
to minimize the multi-class error rate. The fundamental diﬀerence between the AdaNet and
BoostResnet is that the former maps the feature vectors to classiﬁer space and boosts weak
classiﬁers while the latter used multi-channel representation boosting. Moreover, BoostResNet
is more eﬃcient than DeepResnet in terms of computational time.
The theory of boosting was extended to online boosting in and provided theoretical
convergence guarantees. Online boosting shows improved convergence guarantees for batch
boosting algorithms.
The ensembles of bagging and boosting have been evaluated in . The study evaluated the
diﬀerent algorithms based on the concept of bagging and boosting along with the availability of
software tools. The study highlighted the practical issues and opportunities of their feasibility in
ensemble modeling.
4.3. Stacking
Ensembling can be done either by combining outputs of multiple base models in some fashion
or using some method to choose the “best” base model. Figure 4 shows the stacking technique.
Stacking is one of the integration techniques wherein the meta-learning model is used to integrate
the output of base models. If the ﬁnal decision part is a linear model, the staking is often referred
to as “model blending” or simply “blending”. The concept of stacking or stacked regression was
initially given by . In this technique, the dataset is randomly split into J equal parts. For the
jth-fold cross-validation one set is used for testing and the rest are used for training. With these
training testing pair subsets, we obtain the predictions of diﬀerent learning models which are
used as the meta-data to build the meta-model. Meta-model makes the ﬁnal prediction, which is
also called the winner-takes-all strategy.
Stacking is a bias reducing technique . Following , Deep convex net (DCN) 
was proposed which is a deep learning architecture composed of a variable number of modules
stacked together to form the deep architecture. Each learning module in DCN is convex. DCN is
a stack of several modules consisting of linear input units, hidden layer non-linear units, and the
second linear layer with the number of units as that of target classiﬁcation classes. The modules
are connected layerwise as the output of the lower module is given as input to the adjacent higher
module in addition to the original input data. The deep stacking network (DSN) enabling parallel
training on very large scale datasets was proposed in , the network was named stacking
based as it shared the concept of “stacked generalization” . The kernelized version of DCN,
known as kernel deep convex networks (K-DCN), was given in , here the number of hidden
layer approach inﬁnity via kernel trick. Deng et al. showed that K-DCN performs better as
compared to the DCN. However, due to kernel trick the memory requirements increase and hence
may not be scalable to large scale datasets. Also, we need to optimize the hyperparameters like
the number of levels in the stacked network, the kernel parameters to get the optimal performance
of the network. To leverage the memory requirements, random Fourier feature-based kernel deep
convex network approximated the Gaussian kernel which reduces the training time and helps
in the evaluation of K-DCN over large scale datasets. A framework for parameter estimation and
model selection in kernel deep stacking networks is based on the combination of modelbased optimization and hill-climbing approaches. Welchowski and Schmid used data-driven
framework for parameter estimation, hyperparameter tuning and model selection in kernel deep
stacking networks. Another improvement over DSN was Tensor Deep Stacking Network (T-
DSN) , here in each block of the stacked network, large single hidden layer was split into
two smaller ones and then mapped bilinearly to capture the higher-order interactions among
the features. Comprehensive evaluation, the more detailed analysis of the learning algorithm
and T-DSN implementation is given in . Sparse coding is another popular method that is
used in the deep learning area. The advantage of sparse representation is numerous, including
robust to noise, eﬀective for learning useful features, etc. Sparse Deep Stacking Network (S-
DSN) is proposed for image classiﬁcation and abnormal detection . Li et al. , Sun
et al. stacked many sparse simpliﬁed neural network modules (SNNM) with mixed-norm
regularization, in which weights are solved by using the convex optimization and the gradient
descent algorithm. In order to make sparse SNNM learning the local dependencies between
hidden units, Li et al. split the hidden units or representations into diﬀerent groups, which is
termed as group sparse DSN (GS-DSN). The DSN idea is also utilized in the Deep Reinforcement
Learning ﬁeld. Zhang et al. employed DSN method to integrate the observations from the
formal network: Grasp network and Stacking network based on Q-learning algorithm to make
an integrated robotic arm system do grasp and place actions. Wang et al. stacked blocks
multiple times to increase the performance of the neural architecture search task. Zhang et al.
 presents a deep hierarchical multi-patch network for image deblurring via stacking approach.
Since there is no temporal representation of the data in DSNs, they are less eﬀective to the
problems where temporal dependencies exist in the input data. To embed the temporal information in DSNs, Recurrent Deep Stacking Networks (R-DSNs) combined the advantages of
DSNs and Recurrent neural networks (RNN). Unlike RNN which uses Back Propagation through
time for training the network, R-DSNs use Echo State Network (ESN) to initialize the weights
and then ﬁne-tuning them via batch-mode gradient descent. A stacked extreme learning machine
was proposed in . Here, at each level of the network ELM with the reduced number of hidden nodes was used to solve the large scale problems. The number of hidden nodes was reduced
via the principal component analysis (PCA) reduction technique. Keeping in view the eﬃciency
of stacked models, the number of stacked models based on support vector machine have been
proposed . Traditional models like Random Forests have also been extended to deep
architecture, known as deep forests , via stacking concept.
In addition to DSNs, there are some novel network architectures proposed based on the stacking method, Low et al. contributed a stacking-based deep neural network (S-DNN) which
is trained without a backpropagation algorithm. Kang et al. presented a model by stacking
conditionally restricted Boltzmann machine and deep neural network, which achieved signiﬁcant
superior performance with fewer parameters and fewer training samples.
4.4. Negative Correlation Based Deep Ensemble Methods
Negative correlation learning (NCL) is an important technique for training the learning
algorithms. The main concept behind the NCL is to encourage diversity among the individual
models of the ensemble to learn the diverse aspects of the training data. NCL minimizes the empirical risk function of the ensemble model via minimization of error functions of the individual
networks. NCL was evaluated for regression as well as classiﬁcation tasks. The evaluation
used diﬀerent measures like simple averaging and winner-takes-all measures on classiﬁcation
tasks and simple average combination methods for regression problems. The authors ﬁgured out
that winner-takes-all is better as compared to simple averaging in NCL ensemble models.
Shi et al. proposed deep negative correlation learning architecture for crowd counting
known as D-ConvNet i.e. decorrelated convolutional networks. Here, counting is done based on
regression-based ensemble learning from a pool of convolutional feature mapped weak regressors. The main idea behind this is to introduce the NCL concept in deep architectures. Robust
regression via deep NCL is an extension of in which theoretical insights about the
Rademacher complexity are given and extended to more regression-based problems.
Buschj¨ager et al. formulated a generalized bias-variance decomposition method to control the diversity and smoothly interpolates. They present the Generalized Negative Correlation Learning (GNCL) algorithm, which can encapsulate many existing works in literature and
achieve superior performance.
The NCL can also be employed for incremental learning tasks.
Muhlbaier and Polikar
 employed a dynamically modiﬁed weighted majority voting strategy to combine the subclassiﬁers. Tang et al. proposed a negative correlation learning (NCL) based approach for
ensemble incremental learning.
4.5. Explicit / Implicit Ensembles
Ensembling of deep neural networks doesn’t seem to be an easy option as it may lead to increase in computational cost heavily due to the training of multiple neural networks. High performance hardware’s with GPU acceleration may take weeks of weeks to train the deep networks.
Implicit/Explicit ensembles obtain the contradictory goal wherein a single model is trained in
such a manner that it behaves like ensemble of training multiple neural networks without incurring additional cost or to keep the additional cost as minimum as possible. Here, the training time
of an ensemble is same as the training time of a single model. In implicit ensembles, the model
parameters are shared and the single unthinned network at test times approximates the model
averaging of the ensemble models. However, in explicit ensembles model parameters are not
shared and the ensemble output is taken as the combination of the predictions of the ensemble
models via diﬀerent approaches like majority voting, averaging and so on.
Dropout creates an ensemble network by randomly dropping out hidden nodes from
the network during the training of the network. During the time of testing, all nodes are active.
Dropout provides regularization of the network to avoid overﬁtting and introduces sparsity in
the output vectors. Overﬁtting is reduced as it trains exponential number of models with shared
weights and provides an implicit ensemble of networks during testing. Dropping the units randomly avoids coadaptation of the units by making the presence of a particular unit unreliable.
The network with dropout takes 2 −3 times more time for training as compared to a standard
neural network. Hence, a balance is to be set appropriately between the training time of the
network and the overﬁtting. Generalization of DropOut was given in DropConnect . Unlike DropOut which drops each output unit, DropConnect randomly drops each connection and
hence, introduces sparsity in the weight parameters of the model. Similar to DropOut, Drop-
Connect creates an implicit ensemble during test time by dropping out the connections (setting
weights to zero) during training. Both DropOut and DropConnect suﬀer from high training time.
To alleviate this problem, deep networks with Stochastic depth aimed to reduce the network depth during training while keeping it unchanged during testing of the network. Stochastic
depth is an improvement on ResNet wherein residual blocks are randomly dropped during
training and bypassing these transformation blocks connections via skip connections. Swapout
 is a generalization of DropOut and Stochastic depth. Swapout involves dropping of individual units or to skip the blocks randomly. Embarking on a distinctive approach of reducing
the test time, distilling the knowledge in a network transferred the “knowledge” from ensembles to a single model. Gradual DropIn or regularised DropIn of layers starts from a
shallow network wherein the layers are added gradually. DropIN trains the exponential number
of thinner networks, similar to DropOut, and also shallower networks.
All the aforementioned methods provided an ensemble of networks by sharing the weights.
There have been attempts to explore explicit ensembles in which models do not share the weights.
Snapshot ensembling develops an explicit ensemble without sharing the weights. The authors exploited good and bad local minima and let the stochastic gradient descent (SGD) converge M-times to local minima along the optimization path and take the snapshots only when the
model reaches the minimum. These snapshots are then ensembled by averaging at multiple local
Contribution
Wan et al. 
Introduced DropConnect (Random skipping of connections)
Srivastava et al. 
Introduced Dropout (Random skipping of units)
Huang et al. 
Deep networks with Stochastic depth (Random skipping of blocks)
Singh et al. 
Introduced Swapout (Hybrid of Dropout and Stochastic depth approach)
Table 3: Implicit / Explicit ensembles
minima for object recognition. The training time of the ensemble is the same as that of the single
model. The ensemble out is taken as the average of the output of the snapshot outputs at multiple
local minimas. Random vector functional link network has also been explored for
creating the explicit ensembles where diﬀerent random initialization of the hidden layer
weights in a hierarchy diversiﬁes the ensemble predictions.
Explicit/implicit produce ensembles out of a single network at the expense of base model
diversity as the lower level features across the models are likely to be the same. To alleviate
this issue, branching based deep models branch the network to induce more diversity.
Motivated by diﬀerent initializations of the neural networks leads to diﬀerent local minima, Xue
et al. proposed deep ensemble model wherein ensemble of fully convolution neural network
over multiloss module with coarse ﬁne compensation module resulted in better segmentation of
central serous chorioretinopathy lesion. Multiple neural networks with diﬀerent initializations,
multiple loss functions resulted in better diversity in an ensemble.
4.6. Homogeneous & Heterogeneous ensembles
Homogeneous ensemble (HOE) and heterogeneous ensemble (HEE) involve training a group
of base learners either from the same family or diﬀerent families, as shown in Fig. 5 and Fig. 6,
respectively. Hence, each model of an ensemble must be as diverse as possible, and each base
model must perform better than the random guess. The base learner can be a decision tree, neural
network, or any other learning model.
In homogeneous ensembles, the same base learner is used multiple times to generate the family of base classiﬁers. However, the key issue is to train each base model such that the ensemble
model is as diverse as possible, i.e. no two models are making the same error on a particular
data sample. The two most common ways of inducing randomness in a homogeneous ensemble
Figure 5: Homogeneous ensemble (HOE) has models based on the same algorithm, but each
individual model are fed with distinct datasets.
Figure 6: The components in heterogeneous ensemble (HEE) share the same dataset but consists
of various algorithms.
are either sampling of the training set multiple times, thereby training each model on a diﬀerent
bootstrapped sample of the training data or sampling the feature space of the training data and
train each model on a diﬀerent feature subset of the training data. In some ensemble models
like Random forest used both these techniques for introducing diversity in the ensemble of
decision trees. In neural networks, training models independently with diﬀerent initialization
of the models also induces diversity. However, deep learning models have high training costs
and hence, training of multiple deep learning models is not a feasible option. Some attempts,
like horizontal vertical voting of deep ensembles have been made to obtain ensembles of
deep models without independent training. Temporal ensemble trains multiple models with
diﬀerent input augmentation, diﬀerent regularisation and diﬀerent training epochs. Training of
multiple deep neural networks for image classiﬁcation and for disease prediction 
showed that better performance is achieved via an ensemble of multiple networks and averaging
the outputs. Despite these models, training multiple deep learning models for ensemble is an
uphill task as millions or billions of parameters need to be optimized. Hence, some studies have
used deep learning in combination with traditional models to build heterogeneous ensemble models, enjoying the beneﬁts of lower computation and higher diversity. Heterogeneous ensemble
for default prediction is an ensemble of the extreme gradient boosting, deep neural network
and logistic regression. Heterogeneous ensemble for text classiﬁcation is an ensemble of
multivariate Bernoulli na¨ıve Bayes (MVNB), multinomial na¨ıve Bayes (MNB), support vector
machine (SVM), random forest (RF), and convolutional neural network (CNN) learning algorithms. Using diﬀerent perspectives of data, model and decision fusion, heterogeneous deep
network fusion showed that complex heterogeneous fusion architectures are more diverse
and hence, show better generalization performance. Furthermore, Seijo-Pardo et al. employed both homogeneous and heterogeneous ensembles for feature selection. Zhao et al. 
suggested that the heterogeneous bagging based ensemble strategy performs better than boosting based Learn++ algorithms and some other NCL methods. Other examples that employed
homogeneous ensemble methods were used to deal with the presence of incremental tasks, such
as concept drift , power load forecasting , myoelectric prosthetic hands surface
electromyogram characteristics , etc. Das et al. proposed an ensemble incremental learning with pseudo-outer-product fuzzy neural network for traﬃc ﬂow prediction, real-life
stock price, and volatility predictions, etc.
4.7. Decision Fusion Strategies
Ensemble learning trains several base learners and aggregates the outputs of base learners
using some rules. The rule used to combine the outputs determines the eﬀective performance
of an ensemble. Most of the ensemble models focus on the ensemble architecture followed by
their naive averaging to predict the ensemble output. However, naive averaging of the models,
followed in most of the ensemble models, is not data adaptive and leads to less optimal perfor-
mance as it is sensitive to the performance of the biased learners. As there are billions of
hyperparameters in deep learning architecture, the issue of overﬁtting may lead to the failure of
some base learners. Hence, to overcome these issues, approaches like Bayes optimal classiﬁer
and super learner have been followed .
The diﬀerent approaches followed in the literature for combining the outputs of the ensemble
models are:
4.7.1. Unweighted Model Averaging
Unweighted averaging of the outputs of the base learners in an ensemble is the most followed
approach for fusing the decisions in the literature. Here, the outcomes of the base learners are
averaged to get the ﬁnal prediction of the ensemble model. Deep learning architectures have high
variance and low bias, thus, simple averaging of the ensemble models improve the generalization
performance due to the reduction of the variance among the models.
The averaging of the base learners is performed either on the outputs of the base learners
directly or on the predicted probabilities of the classes via softmax function:
i = softmax j(Oi) =
k=1 exp(Oj
i is the probability outcome of the ith unit on the jth base learner, Oj
i is the output of the
ith unite of the jth base learner and K is the number of the classes.
Unweighted averaging is a reasonable choice when the performance of the base learners is
comparable, as suggested in . However, when the ensemble contains heterogeneous
base learners naive unweighted averaging may result in suboptimal performance as it is aﬀected
by the performance of the weak learners and the overconﬁdent learners . The adaptive
metalearner should be good enough to adaptively combine the strengths of the base learners as
some learners may have lower overall performance but maybe good at the classiﬁcation of certain
subclasses and hence, leading to better overall performance.
4.7.2. Majority Voting
Similar to unweighted averaging, majority voting combines the outputs of the base learners.
However, instead of taking the average of the probability outcomes, majority voting counts the
votes of the base learners and predicts the ﬁnal labels as the label with the majority of votes.
In comparison to unweighted averaging, majority voting is less biased towards the outcome of
a particular base learner as the eﬀect is mitigated by majority vote count. However, favouring
of a particular event by most of the similar base learners or dependent base learners leads to the
dominance of the event in the ensemble model. In majority voting, the analysis by Kuncheva
et al. showed that the pairwise dependence among the base learners plays an important
role and for the classiﬁcation of images, the prediction of shallow networks is more diverse as
compared to the deeper networks . Hence, Ju et al. hypothesised that the performance
of the majority voting based shallows ensemble models is better as compared to the majority
based deep ensemble models.
Voting methods have also started to be integrated with semi-supervised deep learning. Li
et al. proposed an ensemble semi-supervised deep acoustic models for in automatic speech
recognition. Wang et al. explored an ensemble self-learning method to enhance semisupervised performance and extract adverse drug events from social media in . In the semisupervised classiﬁcation area, the author proposed a deep coupled ensemble learning method
which is combined with complementary consistency regularization and gets the state of the art
performance in . Some results have also been achieved with semi-supervised ensemble
learning on some datasets where the annotation is costly. Pio et al. employed an ensemble
method to improve the reliability of miRNA:miRNA predicted interactions.
Furthermore, the multi-label classiﬁcation problem is also a major point addressed by
the voting method, a typical application is the RAndom k-labELsets (RAKEL) algorithm .
The author trained several single-label classiﬁers using small random subsets of actual labels.
Then the ﬁnal output is carried out by a voting scheme based on the predictions of these single
classiﬁers. There are also many variants of RAKEL proposed in recent years .
Shi et al. proposed a solution for multi-label ensemble learning problem, which construct
several accurate and diverse multi-label based basic classiﬁers and employ two objective functions to evaluate the accuracy and diversity of multi-label base learners. Another work 
proposed an ensemble multi-label classiﬁcation framework based on variable pairwise constraint
projection. Xia et al. proposed a weighted stacked ensemble scheme that employs the
sparsity regularization to facilitate classiﬁer selection and ensemble construction. Besides, there
are many applications of ensemble multi-label methods. Some publications employ multi-label
ensemble classiﬁers to explore the protein, such as protein subcellular localization , protein
function prediction , etc. The Muli-label classiﬁer is also utilized in predicting the drug side
eﬀects , predicting the gene prediction , etc. Moreover, there is another critical ensemble multi-label algorithm called ensemble classiﬁer chains (ECC) . This method involves
binary classiﬁers linked along a chain. The ﬁrst classiﬁer is trained using only the input data,
and then each subsequent classiﬁer is trained on the input space and all previous classiﬁers in the
chain. The ﬁnal prediction is obtained by the integration of the predictions and selection above a
manually set threshold. Chen et al. propose an ensemble application of convolutional and
recurrent neural networks to capture both the global and local textual semantics and to model
high-order label correlations.
4.7.3. Bayes Optimal Classiﬁer
In Bayesian method, hypothesis hj of each base learner with the conditional distribution of
target label t given x. Let hj be the hypothesis generated on the training data D evaluated on test
data (x, t), mathematically, hj(t|x) = P[y|x, hj, D]. With Bayes rule, we have
P(t|x, D) ∝
P[t|h j, x, D]P[D|hj]P[hj]
and the Bayesian Optimal classiﬁer is given as:
P[t|h j, x, D]P[D|hj]P[hj],
where P[D|h j] = Π(t,x)∈Dhj(t|x) is the likelihood of the data under h j. However, due to overﬁtting
issues this might be not a good measure. Hence, training data is divided into two sets-one for
training the model and the other for evaluating the model. Usually validation set is used to tune
the hyperparameters of the model.
Choosing prior probabilities in Bayes optimal classiﬁer is diﬃcult and hence, usually set
to uniform distribution for simplicity. With a large sample size, one hypothesis tends to give
larger posterior probabilities than others and hence the weight vector is dominated by a single
base learner and hence Bayes optimal classiﬁer would behave as the discrete superlearner with a
negative likelihood loss function.
4.7.4. Stacked Generalization
Stacked generalization works by deducing the biases of the generalizer(s) with respect
to a provided learning set. To obtain the good linear combination of the base learners in regression, cross-validation data and least squares under non-negativity constraints was used to get the
optimal weights of combination . Consider the linear combination of the predictions of the
base learners f1, f2, · · · , fm given as:
fstacking(x) =
where w is the optimal weight vector learned by the meta learner.
4.7.5. Super Learner
Inspired by the cross validation for choosing the optimal classiﬁer, Van der Laan et al. 
proposed super learner which is weighted combination of the predictions of the base learner.
Unlike the stacking approach, it uses cross validation approach to select the optimal weights for
combining the predictions of the base learners.
With smaller datasets, cross validation approach can be used to optimize the weights. However, with the increase in the size of the data and the number of base learners in the model, it
may not be a feasible option. Instead of optimizing the V-fold cross validation, single split cross
validation can also be used for optimizing the weights for optimal combination . In deep
learning models, usually, a validation set is used to evaluate the performance instead of using the
cross validation.
Another application ﬁeld for super learner is in Reinforcement Learning With the development of Deep learning, some researchers have implemented deep reinforcement learning, which
combines deep learning with a Q-learning algorithm . Ensemble methods in deep Q learning have decent performance. Chen et al. proposed an ensemble network architecture
for deep reinforcement learning. The integrated network includes Temporal Ensemble and Target Values Ensemble. Develop a human-like chat robot is a challenging job, by incorporating deep reinforcement learning and ensemble method, Cuay´ahuitl et al. integrated 100
deep reinforcement learning agents, the agents are trained based on clustered dialogues. They
also demonstrate the ensemble of DRL agents has better performance than the single variant or
Seq2Seq model. Stock trading is another topic where ensemble deep reinforcement learning has
achieved a promising result. Carta et al. found the single supervised classiﬁer is inadequate to deal with the complex and volatile stock market. They employed hundreds of neural
networks to pre-process the data, then they combined several reward-based meta learners as a
trading agency. Moreover, Yang et al. trained an ensemble trading agency based on three
diﬀerent metrics: Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), and Deep
Deterministic Policy Gradient (DDPG). The ensemble strategy combines the advantages of the
three diﬀerent algorithms. Besides, some researchers try to use ensemble strategy to solve the
disease-prediction problem. The proposed model in consists of several sub-models which
are in response to diﬀerent anatomical parts.
4.7.6. Consensus
Figure 7: The process of consensus clustering. An ensemble of diﬀerent clustering results can
be combined by a consensus approach.
Unsupervised learning is another group of machine learning techniques. The fundamental
diﬀerence between it and supervised learning is that unsupervised learning usually handles training samples without corresponding labels. Therefore, the primary usage of unsupervised learning
is to do clustering. The reason why ensemble methods are employed is to combine some weak
clusters into strong one. To create diverse clusters, several approaches can be applied: using
diﬀerent sampling data, using diﬀerent subsets of the original features, and employing diﬀerent clustering methods . Sometimes, even some random noise can be added to these base
models to increase randomness, which is good for ensemble methods according to . After
receiving all the outputs from each cluster, various consensus functions can be chosen to obtain
the ﬁnal output based on the user’s requirement . The ensemble clustering is also known as
consensus clustering Fig. 7.
Zhou and Tang explored ensemble methods for unsupervised learning and developed
four diﬀerent approaches to combine the outputs of these clusters. In recent years, some new ensemble clustering methods have been proposed that illustrated the priority of ensemble learning
 . Most of the clustering ensemble method is based on the co-association matrix
solution, which can be regarded as a graph partition problem. Besides, there is some research
focus on integrating the deep structure and ensemble clustering method. Liu et al. 
ﬁrstly showed that ensemble unsupervised representation learning with deep structure can be applied in large scale data. Then the author combined the method with auto-encoder and extends
it to the vision ﬁeld. Shaham et al. ﬁrst demonstrated that some crowdsourcing algorithms
can be replaced by a Restricted Boltzmann Machine with a single hidden neuron, then propose
an RBM-based Deep Neural Net (DNN) used for unsupervised ensemble learning. The unsupervised ensemble method also makes some contribution to the ﬁeld of Natural Language Processing. Alami et al. demonstrated that the ensemble of unsupervised deep neural network
models that use Sentence2Vec representation as the input has the best performance according to
the experiments. Hassan et al. proposed a module that includes four semantic similarity
measures, which improves the performance on the semantic textual similarity (STS) task. The
unsupervised ensemble method is also widely used for tasks that lack annotation, such as the
medical image. Ahn et al. proposed an unsupervised feature learning method integrated
ensemble approach with a traditional convolutional neural network. Lahiri et al. employed
unsupervised hierarchical feature learning with ensemble sparsely autoencoder on retinal blood
vessels segmentation task, meanwhile, Liu et al. also propose an unsupervised ensemble
architecture to automatically segment retinal vessel. Besides, there are also some ensemble deep
methods working on localization predicting for long non-coding RNAs . Hu and Suganthan
 extended the ensemble random vector functional link to unsupervised tasks. The authors
employ manifold regularization to re-represent the original features, and then use the Kuhn-
Munkre algorithm with consensus clustering to ensemble the clustering results from multiple
hidden layers.
4.7.7. Query-By-Committee
Active Learning is another popular topic in the deep learning area, which is also often used
in conjunction with semi-supervised learning and ensemble learning. The key sight of this is
to make the algorithm learning from less annotated data. Some conventional active learning
algorithms, such as Query-By-Committee (as shown in Fig 8), have already adopted the idea of
ensemble learning. Melville and Mooney explored an ensemble method that builds
a diverse committee. Beluch et al. discussed the power of ensembles for active learning
is signiﬁcantly better than Monte-Carlo Dropout and geometric approaches. Sharma and Rani
 show some applications in drug-target interaction prediction. Ensemble active learning is
Figure 8: Query-by-committee in Active Learning. Sampling with replacement is used to partition the labeled training data set into training splits. The committee determines whether to label
the data based on the output of several algorithms.
also available to conquer the concept drift and class imbalance problem .
5. Applications
In this section, we brieﬂy present the applications of deep ensemble models across diﬀerent
domains in a tabular form. Ensemble deep models have been implemented in several domains
and therefore, in broad sense, we have classiﬁed the application domains into ﬁve categories,
i.e., health care, speech, image classiﬁcation, forecasting and the rest models are listed in others
category. Table 4 gives the information about the ensemble deep models that have been implemented in health care domain. Here, several papers are based on heterogeneous ensemble
technique. It reveals that using diﬀerent family’s models into a single frame perform better in
health care domain. Recently, ensemble deep techniques have been successful and have shown
good performance in health care domain. The models which have been implemented for speech
task have been given in Table 5 and most of the ensemble approaches are based on stacking
technique. Table 6 contains the ensemble deep models that have been implemented in speech
areas. Models that have been implemented in forecasting and other domains have been given
in Table 7 and Table 8, respectively. Fig. 9 shows the percentages of the application domains.
Other Applications
Forecasting
Health Care
Image Classification
Figure 9: Ensemble-based approach in diﬀerent areas. Data from Tables 4 to 8.
The statistics reveals that diﬀerent ensemble deep techniques have been used in diﬀerent areas.
A larger number of models, i.e. 27% of the ensemble deep models, have been implemented in
health care domain and 5.6% percent of the models for speech application and 22.5% of the
models for image classiﬁcation task. Moreover, 9% models have been used in forecasting and
36% in other applications areas, i.e., information retrieval, emotion recognition, text categorization and so on. Fig. 10 shows the ensemble strategies in percentage. In ensemble learning,
there are several ways to integrate the outcomes of the models in an ensemble. In the literature,
researchers have proposed diﬀerent techniques of decision fusion according to diﬀerent areas
of application. Bagging, boosting and stacking are the classical ensemble techniques. Based
on these three techniques, researchers have developed several other techniques also. Boosting
(18.2%), stacking (12.5%) and bagging (4.5%) techniques have been implemented in ensemble
deep framework. Heterogeneous and implicit ensemble are also popular for making an eﬃcient
ensemble model and their contribution are as follows 11.4% and 10.2%, respectively. The rest
ensemble techniques are, i.e. unsupervised (3.4%), NCL (3.4%), reinforcement (1.1%), active
learning (1.1%), explicit ensemble (1.1%) and homogeneous ensemble (3.4%).
6. Conclusions and future works
In this paper, we reviewed the recent developments of ensemble deep learning models. The
theoretical background of ensemble learning has been elaborated to understand the success of
ensemble learning. The various approaches ranging from traditional ones like bagging, boosting
to the recent novel approaches like implicit/explicit ensembles, heterogeneous ensembles, have
Zheng et al. 
HIBAG—HLA genotype imputation with attribute bagging
Genotype Imputation
Cortes et al. 
Deep Boosting
Classiﬁcation
Zhang et al. 
Predicting drug side eﬀects by multi-label learning and ensemble
Decision Fusion
Predict the drug side eﬀects
Guo et al. 
Human protein subcellular localization with integrated source and
multi-label ensemble classiﬁer.
Decision Fusion
Protein subcellular localization prediction
Lahiri et al. 
Deep neural ensemble for retinal vessel segmentation in fundus images towards achieving label-free angiography
Decision Fusion
Medical image segmentation
Cabria and Gondra 
MRI segmentation fusion for brain tumor detection
Heterogeneous ensemble
MRI segmentation
Grassmann et al. 
A deep learning algorithm for prediction of age-related eye disease
study severity scale for age-related macular degeneration from color
fundus photography
Homogeneous ensemble
Disease prediction
Cao et al. 
The lnclocator: a subcellular localization predictor for long noncoding RNAs based on a stacked ensemble classiﬁer
Decision Fusion
Subcellular localization predictor
Sharma and Rani 
Be-dti’: Ensemble framework for drug target interaction prediction
using dimensionality reduction and active learning
Active learning
Drug target interaction prediction
Ahn et al. 
Unsupervised feature learning with k-means and an ensemble of
deep convolutional neural networks for medical image classiﬁcation
Decision Fusion
Medical image classiﬁcation
Liu et al. 
Unsupervised ensemble strategy for retinal vessel segmentation.
Unsupervised
Medical image classiﬁcation
Shalbaf and Vafaeezadeh 
Automated detection of COVID-19 using ensemble of transfer
learning with deep convolutional neural network based on CT scans
Heterogeneous Ensemble
Detection of COVID-19
Ali et al. 
A smart healthcare monitoring system for heart disease prediction
based on ensemble deep learning and feature fusion
Heart disease prediction
Zhou et al. 
The ensemble deep learning model for novel COVID-19 on CT images
Heterogeneous Ensemble
Detection of COVID-19
Li et al. 
Intelligent Fault Diagnosis by Fusing Domain Adversarial Training
and Maximum Mean Discrepancy via Ensemble Learning
Heterogeneous Ensemble
Fault diagnosis
Das et al. 
Automatic COVID-19 detection from X-ray images using ensemble
learning with convolutional neural network
Heterogeneous Ensemble
Detection of COVID-19
Sukegawa et al. 
Identiﬁcation of osteoporosis using ensemble deep learning model
with panoramic radiographs and clinical covariates
Decision Fusion
Identiﬁcation of osteoporosis
Gao et al. 
Vessel segmentation for X-ray coronary angiography using ensemble methods with deep learning and ﬁlter-based features
Vessel segmentation
Rath et al. 
Improved heart disease detection from ECG signal using deep learning based ensemble model
Heterogeneous Ensemble
Heart disease detection
Tanveer et al. 
Classiﬁcation of Alzheimer’s Disease Using Ensemble of Deep
Neural Networks Trained Through Transfer Learning
Heterogeneous Ensemble
Classiﬁcation of Alzheimer’s Disease
Rai and Chatterjee 
Hybrid CNN-LSTM deep learning model and ensemble technique
for automatic detection of myocardial infarction using big ECG data
Heterogeneous Ensemble
Detection of myocardial infarction
Ganaie and Tanveer 
Ensemble deep random vector functional link network using privileged information for Alzheimer’s disease diagnosis
Implicit ensemble
Diagnosis of Alzheimer’s disease
Table 4: Applications in health care
Tur et al. 
Towards deeper understanding: Deep convex networks for semantic
utterance classiﬁcation
Semantic Utterance Classiﬁcation
Deng et al. 
Use of kernel deep convex networks and end-to-end learning for
spoken language understanding
Spoken Language Understanding
Deng and Platt 
Ensemble deep learning for speech recognition
Speech Recognition
Palangi et al. 
Recurrent Deep-Stacking Networks for sequence classiﬁcation
Sequence classiﬁcation
Li et al. 
Semi-supervised ensemble DNN acoustic model training
Decision Fusion
Speech Recognition
Table 5: Applications in speech
Ciregan et al. 
Multi-column deep neural networks for image classiﬁcation
Homogeneous ensemble
Classiﬁcation
Wan et al. 
Regularization of Neural Networks using DropConnect
Implicit ensemble
Image recognition
Srivastava et al. 
Dropout: a simple way to prevent neural networks from overﬁtting
Implicit Ensemble
Computer vision, speech recognition
document classiﬁcation and computational biology
Liu et al. 
Facial expression recognition via a boosted deep belief network
Facial expression recognition
Li et al. 
Sparse deep stacking network for image classiﬁcation
Image Classiﬁcation
Yang et al. 
Convolutional channel features
Pedestrian detection, face detection,
edge detection and object proposal generation
Moghimi et al. 
Boosted Convolutional Neural Networks
Classiﬁcation
Huang et al. 
Deep networks with stochastic depth
Implicit ensemble
Classiﬁcation
He et al. 
Deep residual learning for image recognition
Implicit ensemble
classiﬁcation, and object detection
Singh et al. 
Swapout: Learning an ensemble of deep architectures
Implicit ensemble
classiﬁcation
Smith et al. 
Gradual dropin of layers to train very deep neural networks
Implicit ensemble
Classiﬁcation
Laine and Aila 
Temporal ensembling for semi-supervised learning
Homogeneous ensemble
Classiﬁcation
Tang et al. 
Inquire and diagnose: Neural symptom checking ensemble using
deep reinforcement learning
Decision Fusion
Inquire symptoms and diagnose diseases
Huang et al. 
Snapshot ensembles: train 1, get M for free
Explicit ensemble
Classiﬁcation
Mosca and Magoulas 
Deep incremental boosting
Classiﬁcation
Beluch et al. 
The power of ensembles for active learning in image classiﬁcation
Decision Fusion
Image classiﬁcation
Amin-Naji et al. 
Ensemble of CNN for multi-focus image fusion
Decision Fusion
Image Classiﬁcation
Li et al. 
Semi-supervised deep coupled ensemble learning with classiﬁcation landmark exploration.
Decision Fusion
Image classiﬁcation
Wang et al. 
Particle swarm optimisation for evolving deep neural networks for
image classiﬁcation by evolving and stacking transferable blocks.
Image Classiﬁcation
Table 6: Applications in image classiﬁcation
Qiu et al. 
Ensemble deep learning for regression and time series forecasting
Decision Fusion
Regression and Time Series Forecasting
Grmanov´a et al. 
Incremental ensemble learning for electricity load forecasting
Decision Fusion
Electricity load forecasting
Qiu et al. 
Empirical Mode Decomposition based ensemble deep learning for
load demand time series forecasting
Decision Fusion
Load demand forecasting
Liu et al. 
A Flood Forecasting Model Based on Deep Learning Algorithm via
Integrating Stacked Autoencoders with BP Neural Network
Flood Forecasting
Qiu et al. 
Ensemble incremental learning random vector functional link network for short-term electric load forecasting.
Decision Fusion
Electric load forecasting
Carta et al. 
A multi-layer and multi-ensemble stock trader using deep learning
and deep reinforcement learning
Implicit ensemble
Stock trader
Yang et al. 
Deep reinforcement learning for automated stock trading: An ensemble strategy
Decision Fusion
Stock trading agency
Bhusal et al. 
Deep ensemble learning-based approach to real-time power system
state estimation
Electric Power
Singla et al. 
An ensemble method to forecast 24-h ahead solar irradiance using
wavelet decomposition and BiLSTM deep learning network
Decision Fusion
Forecasting
Table 7: Applications in forecasting
Deng et al. 
Deep stacking networks for information retrieval
Information Retrieval
Kuznetsov et al. 
Multi-class deep boosting
Classiﬁcation
Wang et al. 
Sentiment classiﬁcation The contribution of ensemble learning
Bagging, Boosting
Sentiment classiﬁcation
Zareapoor and Shamsolmoali 
Application of Credit Card Fraud Detection: Based on Bagging Ensemble Classiﬁer
Credit Card Fraud Detection
Yin et al. 
Recognition of emotions using multimodal physiological signals
and an ensemble deep learning model
Decision Fusion
Emotions Recognition
Liu et al. 
A deep learning approach to unsupervised ensemble learning
Decision Fusion
Clustering
Walach and Wolf 
Learning to count with CNN boosting
Object counting in images
Han et al. 
Incremental boosting convolutional neural network for facial action
unit recognition
Facial action unit recognition
Chen et al. 
Ensemble application of convolutional and recurrent neural networks for multi-label text categorization
Decision Fusion
Text Categorization.
Opitz et al. 
Bier-boosting independent embeddings robustly
Image retrieval
Shi et al. 
Crowd Counting with Deep Negative Correlation Learning
Negative correlation learning
Crowd Counting
Kazemi et al. 
Novel genetic-based negative correlation learning for estimating
soil temperature
Negative correlation learning
Soil Temperature Estimation
Randhawa et al. 
Credit Card Fraud Detection Using AdaBoost and Majority Voting
Credit Card Fraud Detection
Sun et al. 
Sparse Deep Stacking Network for Fault Diagnosis of Motor
Fault Diagnosis
Chen et al. 
Deep boosting for image denoising
Image denoising
Li et al. 
Heterogeneous ensemble for default prediction of peer-to-peer lending in China
Heterogeneous ensemble
Default prediction
Kilimci and Akyokus 
Deep learning-and word embedding-based heterogeneous classiﬁer
ensembles for text classiﬁcation
Heterogeneous ensemble
Classiﬁcation
Liu et al. 
SSEL-ADE: a semi-supervised ensemble learning framework for
extracting adverse drug events from social media
Decision Fusion
Extracting adverse drug events
Mart´ın et al. 
Android malware detection through hybrid features fusion and
ensemble classiﬁers: The AndroPyTool framework and the OmniDroid dataset
Decision Fusion
Android malware detection
Cuay´ahuitl et al. 
Ensemble-based deep reinforcement learning for chatbots
Reinforcement
Chat robot
Chen et al. 
Real-world image denoising with deep boosting
Image denoising
Waltner et al. 
HiBsteR: Hierarchical Boosted Deep Metric Learning for Image
Image Retrieval
Wang et al. 
Adaboost-based security level classiﬁcation of mobile intelligent
Security Level Classiﬁcation
Chen et al. 
Novel Hybrid Integration Approach of Bagging-Based Fisher’s Linear Discriminant Function for Groundwater Potential Analysis
Groundwater Potential Analysis
Shi et al. 
Random vector functional link neural network based ensemble deep
Implicit ensemble
Classiﬁcation
Zhang et al. 
Deep stacked hierarchical multi-patch network for image deblurring
Deblurring Image
Alami et al. 
Enhancing unsupervised neural networks based text summarization
with word embedding and ensemble learning
Decision Fusion
Text summarization
Hassan et al. 
An unsupervised ensemble semantic textual similarity
Decision Fusion
Semantic textual similarity
Zhang et al. 
Grasp for stacking via deep reinforcement learning
Robotic arm control
Zhang et al. 
Snapshot boosting: a fast ensemble framework for deep neural networks
Computer vision (CV) and the natural language processing (NLP) tasks
Tsogbaatar et al. 
DeL-IoT: A deep ensemble learning approach to uncover anomalies
Decision Fusion
Wen et al. 
A new ensemble convolutional neural network with diversity regularization for fault diagnosis
Snapshot ensemble learning
Fault diagnosis
Hu and Suganthan 
Representation Learning Using Deep Random Vector Functional
Link Networks for Clustering
Decision Fusion
Clustering
Table 8: Other applications
Homogeneous Ensemble
Explicit Ensemble
Active Learning
Reinforcement
Negative Correlation Learning
Unsupervised
Implicit Ensemble
Heterogeneous Ensemble
Decision Fusion
Figure 10: Analysis of the applications of various ensemble methods. Data from Tables 4 to 8.
led to better performance of deep ensemble models. We also reviewed the applications of the
deep ensemble models in diﬀerent domains.
Although deep ensemble models have been applied across diﬀerent domains, there are several
open problems which can be explored in the future to ﬁll the gap. Big data is still a
challenging problem, one can explore the beneﬁts of deep ensemble models for learning the
patterns using the techniques like implicit deep ensemble to maximize the performance in both
time and generalization aspects.
Deep learning models are diﬃcult to train than shallow models as large number of weights
corresponding to diﬀerent layers need to be tuned. Creating deep ensemble models may further
complicate the problem. Hence, randomized models can be explored to overcome the training
cost. Bagging based deep ensemble may incur heavy training time for optimizing the ensemble
models. Hence, one can investigate the alternate ways of inducing diversity in the base models with lesser training cost. Randomized learning modules like random vector functional link
network are best suited for creating the ensemble models as randomized models lead to a
signiﬁcant variance reduction. Also, the hidden layers are randomly initialized, hence, can be
used to create deep ensembles without incurring any additional cost of training . Random-
ized modules can be further explored using diﬀerent techniques like implicit / explicit ensembles
 , stacking based ensembles . However, there are still open directions which can be
worked upon like negative correlation learning, heterogeneous ensembles and so on.
Implicit/explicit ensembles are faster compared to training of multiple deep models. However, creating diversity within a single model is a big challenge. One can explore the methods
to induce more diversity among the learners within these ensembles like branching based deep
models . Investigate the extension of explicit/implicit ensembles to traditional models.
Following the stacking based approach, Deep convex net (DCN) , traditional methods
like random forest , support vector machines have been extended to deep
learning architectures which resulted in improved performance. One can investigate these traditional models for creating the deep ensemble models.
Another big challenge of ensemble deep learning lies in model selection for building the
ensemble architecture, homogeneous and heterogeneous ensembles represent two diﬀerent ways
for choosing the model. However, to answer how many diﬀerent algorithms, and how many base
learners in the ensemble architecture, are still problem-dependent. Finding a criterion for model
selection in ensemble deep learning should be an important target for researchers in the next few
years. Since most of the models focus on developing the architectures with little attention towards
how to combine the base learners prediction is still unanswered. Hence, one can investigate the
eﬀect of diﬀerent fusion strategies on the prediction of an ensemble output.
For unsupervised ensemble learning or consensus clustering, the ensemble approaches include but are not limited to: Hyper-graph partitioning, Voting approach, Mutual information, etc.
Consensus clustering is a powerful tool and it can improve performance in most cases. However,
there are many concerns remain to be tackled, it is exquisitely sensitive, which might assert as an
apparent structure without obvious demarcation or declared cluster stable without cluster resistance. Besides, current method cannot handle some complex but possible scenarios, such as the
boundary samples are assigned to the single cluster, clusters do not intersect and the methods are
not able to represent outliers. These are the possible research directions for future work.
The problem of semi-supervised ensemble domains has not been extensively studied yet, and
most of the literature shows that semi-supervised ensemble methods are mainly used in cases
where there is insuﬃcient labeling data. Also, combining the semi-supervision with some other
machine learning methods, such as active learning, is a direction for future research.
Reinforcement learning is another popular topic recently. The idea of integrating modelbased reinforcement learning with ensemble learning has been used with promising results in
many applications, but there is little integration of planning & learning-based reinforcement
learning with ensemble learning methods.
Acknowledgment
The funding for this work is provided by the National Supercomputing Mission under DST
and Miety, Govt. of India under Grant No. DST/NSM/ R&D HPC Appl/2021/03.29, as well
as the Department of Science and Technology under Interdisciplinary Cyber Physical Systems
(ICPS) Scheme grant no. DST/ICPS/CPS-Individual/2018/276. Mr. Ashwani Kumar Malik
acknowledges the ﬁnancial support (File no - 09/1022 (0075)/2019-EMR-I) given as scholarship
by Council of Scientiﬁc and Industrial Research (CSIR), New Delhi, India. We are grateful to
IIT Indore for the facilities and support being provided.