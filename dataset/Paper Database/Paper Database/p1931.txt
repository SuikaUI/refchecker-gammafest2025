MIT Open Access Articles
SUN database: Large-scale scene recognition from abbey to zoo
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Jianxiong Xiao et al. ‚ÄúSUN database: Large-scale scene recognition from abbey to zoo.‚Äù
Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. 2010. 3485-3492.
As Published: 
Publisher: Institute of Electrical and Electronics Engineers
Persistent URL: 
Version: Author's final manuscript: final author's manuscript post peer review, without
publisher's formatting or copy editing
Terms of use: Attribution-Noncommercial-Share Alike 3.0 Unported
SUN Database: Large-scale Scene Recognition from Abbey to Zoo
Jianxiong Xiao
James Hays‚Ä†
Krista A. Ehinger
Aude Oliva
Antonio Torralba
 
 
 
 
 
Massachusetts Institute of Technology
‚Ä†Brown University
Scene categorization is a fundamental problem in computer vision. However, scene understanding research has
been constrained by the limited scope of currently-used
databases which do not capture the full variety of scene
categories. Whereas standard databases for object categorization contain hundreds of different classes of objects,
the largest available dataset of scene categories contains
only 15 classes.
In this paper we propose the extensive
Scene UNderstanding (SUN) database that contains 899
categories and 130,519 images. We use 397 well-sampled
categories to evaluate numerous state-of-the-art algorithms
for scene recognition and establish new bounds of performance.
We measure human scene classiÔ¨Åcation performance on the SUN database and compare this with computational methods. Additionally, we study a Ô¨Åner-grained
scene representation to detect scenes embedded inside of
larger scenes.
1. Introduction
Whereas the Ô¨Åelds of computer vision and cognitive science have developed several databases to organize knowledge about object categories , a comprehensive
database of real world scenes does not currently exist (the
largest available dataset of scene categories contains only
15 classes). By ‚Äúscene‚Äù we mean a place in which a human
can act within, or a place to which a human being could navigate. How many kinds of scenes are there? How can the
knowledge about environmental scenes be organized? How
do the current state-of-art scene models perform on more
realistic and ill-controlled environments, and how does this
compare to human performance?
To date, computational work on scene and place recognition has classiÔ¨Åed natural images within a limited number of semantic categories, representing typical indoor and
outdoor settings . However, any restricted set of categories fails to capture the richness and
diversity of environments that make up our daily experience. Like objects, scenes are associated with speciÔ¨Åc funcairlock
anechoic chamber
bookbindery
boat deck house
departure lounge
hunting lodge
jewelleryshop
pilothouse
police office
skating rink
sports stadium
access road
fire escape
floating bridge
fly bridge
loading dock
lookout station
plantation
signal box
skyscraper
apple orchard
archipelago
rice paddy
rock outcrop
sunken garden
Figure 1. Examples of scene categories in our dataset.
tions and behaviors, such as eating in a restaurant, drinking
in a pub, reading in a library, and sleeping in a bedroom.
Scenes, and their associated functions, are closely related to
the visual features that structure the space. The function of
environments can be deÔ¨Åned by their shape and size (a narrow corridor is for walking, an expansive arena is for public
events), by their constituent materials (snow, grass, water,
wood), or by embedded objects (table and chairs, displays
of jewelry, laboratory equipment).
The spatial layout and structure of a place often constrain
human activities: for instance, a long open corridor affords
walking or running, a classroom affords seating. Previous
attempts to characterize environmental scenes have capitalized on uncovering a manageable set of dimensions, features or objects that are correlated with the semantic of purpose of a space .
This paper has the following four objectives. First, we
seek to quasi-exhaustively determine the number of different scene categories with different functionalities. Rather
than collect all scenes that humans experience ‚Äì many of
which are accidental views such as the corner of an ofÔ¨Åce
or edge of a door ‚Äì we identify all the scenes and places that
are important enough to have unique identities in discourse,
and build the most complete dataset of scene image categories to date. Second, we measure how accurately humans
can classify scenes into hundreds of categories. Third, we
evaluate the scene classiÔ¨Åcation performance of state of the
art algorithms and establish new bounds for performance on
the SUN database and the 15 scene database using a kernel
combination of many features. Finally, we study the possibility of detecting scenes embedded inside larger scenes.
2. A Large Database for Scene Recognition
In order to get a quasi-exhaustive list of environmental
categories, we selected from the 70,000 terms of WordNet
 available on the tiny Images dataset all the terms
that described scenes, places, and environments (any concrete noun which could reasonably complete the phrase I
am in a place, or Let‚Äôs go to the place). Most of the terms
referred to basic and entry level places with
different semantic descriptions. We did not include speciÔ¨Åc
place names (like Grand Canyon or New York) or terms
which did not seem to evoke a speciÔ¨Åc visual identity (territory, workplace, outdoors). Non-navigable scenes (such as
a desktop) were not included, nor were vehicles (except for
views of the inside of vehicles) or scenes with mature content. We included speciÔ¨Åc types of buildings (skyscraper,
house, hangar), because, although these can be seen as objects, they are known to activate scene-processing-related
areas in the human brain. . We also maintained a high tolerance for vocabulary terms that may convey signiÔ¨Åcance to
experts in particular domains (e.g. a baseball Ô¨Åeld contains
specialized subregions such the pitcher‚Äôs mound, dugout,
and bullpen; a wooded area could be identiÔ¨Åed as a pine
forest, rainforest, orchard, or arboretum, depending upon its
layout and the particular types of plants it contains). To the
WordNet collection we added a few categories that seemed
like plausible scenes but were missing from WordNet, such
as jewelry store and mission1.
This gave about 2500 initial terms of space and scene,
and after bundling together synonyms (provided by Word-
Net, and separating scenes with different visual identities
such as indoor and outdoor views of churches), the Ô¨Ånal dataset reaches 899 categories and 130,519 images.
We refer to this dataset as ‚ÄúSUN‚Äù (Scene UNderstanding)
database2.
1An alternate strategy to obtain a list of relevant scene categories is to
record the visual experience of an observer and to count the number of
different scene categories viewed. This procedure is unlikely to produce
a complete list of all scene categories, as many scene categories are only
viewed in rare occasions (e.g., a cloister, a corn Ô¨Åeld, etc.). However, we
use this to validate the completeness of the list provided by WordNet. A
set of 7 participants where asked to write down every half an hour the
name of the scene category in which they were. They reported scenes for
a total period of 284 hours. In that period, they reported 52 different scene
categories, all of them within the set of scenes covered by our dataset.
deÔ¨Ånitions
 













Figure 2. SUN categories with the highest human recognition rate.
For each scene category, images were retrieved using
WordNet terminology from various search engines on the
web . Only color images of 200 √ó 200 pixels or larger
were kept. Each image was examined to conÔ¨Årm whether
or not it Ô¨Åt a detailed, verbal deÔ¨Ånition for its category.
For similar scene categories (e.g. ‚Äúabbey‚Äù, ‚Äúchurch‚Äù, and
‚Äúcathedral‚Äù) explicit rules were formed to avoid overlapping deÔ¨Ånitions. Degenerate or unusual images (black and
white, distorted colors, very blurry or noisy, incorrectly rotated, aerial views, noticeable borders) were removed. All
duplicate images, within and between categories, were removed.
For many of the 899 SUN categories the Internet search
returns relatively few unique photographs3. For all experiments in this paper we use only the 397 categories for which
there are at least 100 unique photographs.
3. Human Scene ClassiÔ¨Åcation
In the previous section we provide a rough estimate on
the number of common scene types that exist in the visual world and built the extensive SUN database to cover as
many of those scenes as possible. In this section, we measure human scene classiÔ¨Åcation performance on the SUN
database. We have two goals: 1) to show that our database is
constructed consistently and with minimal overlap between
categories 2) to give an intuition about the difÔ¨Åculty of 397way scene classiÔ¨Åcation and to provide a point of comparison for computational experiments (Section 4.2).
Measuring human classiÔ¨Åcation accuracy with nearly
400 categories is difÔ¨Åcult.
We do not want to penalize humans for being unfamiliar with our speciÔ¨Åc scene
taxonomy, nor do we want to require extensive training
concerning the deÔ¨Ånitions and boundaries between scenes
(however, such training was given to those who built the
database). Ideally our scene categories would be clear and
distinct enough such that an untrained participant can unambiguously assign a categorical label to each scene in our
database given a list of possible scene types. To help par-
3Examples of under-sampled categories include ‚Äúairlock‚Äù, ‚Äúediting
room‚Äù, ‚Äúgrotto‚Äù, ‚Äúlaunchpad‚Äù, ‚Äúlava Ô¨Çow‚Äù, ‚Äúnaval base‚Äù, ‚Äúoasis‚Äù, ‚Äúossuary‚Äù, ‚Äúsalt plain‚Äù, ‚Äúsignal box‚Äù, ‚Äúsinkhole‚Äù, ‚Äúsunken garden‚Äù, ‚Äúwinners


 
 
 


 !

 
 !
 !
 !

 
Figure 3. Top row: SUN categories with the lowest human recognition rate. Below each of these categories, in the remaining three
rows, are the most confusing classes for that category.
ticipants know which labels are available, we group the 397
scene categories in a 3-level tree, and the participants navigate through an overcomplete three-level hierarchy to arrive at a speciÔ¨Åc scene type (e.g. ‚Äúbedroom‚Äù) by making
relatively easy choices (e.g. ‚Äúindoor‚Äù versus ‚Äúoutdoor natural‚Äù versus ‚Äúoutdoor man-made‚Äù at the Ô¨Årst level). Many
categories such as ‚ÄúhayÔ¨Åeld‚Äù are duplicated in the hierarchy
because there might be confusion over whether such a category belongs in the natural or man-made sub-hierarchies.
This hierarchy is used strictly as a human organizational
tool, and plays no roll in our experimental evaluations. For
each leaf-level SUN category the interface shows a prototypical photograph of that category.
We measure human scene classiÔ¨Åcation accuracy using
Amazon‚Äôs Mechanical Turk (AMT). For each SUN category we measure human accuracy on 20 distinct test scenes,
for a total of 397√ó20 = 7940 experiments or HITs (Human
Intelligence Tasks in AMT parlance). We restricted these
HITs to participants in the U.S. to help avoid vocabulary
confusion.
On average, workers took 61 seconds per HIT and
achieved 58.6% accuracy at the leaf level. This is quite
high considering that chance is 0.25% and numerous categories are closely related (e.g. ‚Äúdining room‚Äù, ‚Äúdining car‚Äù,
‚Äúhome dinette‚Äù, and ‚Äúvehicle dinette‚Äù). However, a significant number of workers have 0% accuracy ‚Äì they do not
appear to have performed the experiment rigorously. If we
instead focus on the ‚Äúgood workers‚Äù who performed at least
100 HITs and have accuracy greater than 95% on the relatively easy Ô¨Årst level of the hierarchy the leaf-level accuracy
rises to 68.5%. These 13 ‚Äúgood workers‚Äù accounted for just
over 50% of all HITs. For reference, an author involved in
the construction of the database achieved 97.5% Ô¨Årst-level
accuracy and 70.6% leaf-level accuracy. Therefore, these 13
good workers are quite trustable. In the remainder of the paper, all evaluations and comparisons of human performance
all [88.1]
SIFT [81.2]
hog2x2 [81.0]
texton histogram [77.8]
ssim [77.2]
gist [74.7]
sparse SIFT histograms [56.6]
geometric classification map [55.0]
straight line histograms [50.9]
Recognition rate
Number training samples per class
(a) 15 scene dataset
Number training samples per class
Recogntion rate
all [38.0]
hog2x2 [27.2]
geometry texton histograms [23.5]
ssim [22.5]
dense SIFT [21.5]
lbp [18.0]
texton histogram [17.6]
gist [16.3]
all (1NN) [13.0]
lbphf [12.8]
sparse SIFT histograms [11.5]
geometry color histograms [9.1]
color histograms [8.2]
geometric classification map [6.0]
straight line histograms [5.7]
tiny image [5.5]
(b) SUN database
Figure 4. Recognition performance on the 15 scene dataset , and our SUN database. For the 15 scene dataset, the combination of all features (88.1%) outperforms the current state of the art
(81.4%) .
utilize only the data from the good AMT workers.
Figures 2 and 3 show the SUN categories for which the
good workers were most and least accurate, respectively.
For the least accurate categories, Figure 3 also shows the
most frequently confused categories. The confused scenes
are semantically similar ‚Äì e.g. abbey to church, bayou to
river, and sandbar to beach. Within the hierarchy, indoor
sports and leisure scenes are the most accurately classiÔ¨Åed
(78.8%) while outdoor cultural and historical scenes were
least accurately classiÔ¨Åed (49.6%). Even though humans
perform poorly on some categories, the confusions are typically restricted to just a few classes.
Human and computer performance are compared extensively in Section 4.2. It is important to keep in mind that the
human and computer tasks could not be completely equivalent. The ‚Äútraining data‚Äù for AMT workers was a text label,
a single prototypical photo, and their lifetime visual experience. For some categories, a lifetime of visual experience
is quite large (e.g ‚Äúbedroom‚Äù) while for others it is quite
small (e.g. ‚Äúmedina‚Äù). On the other hand, the computational methods had (up to) 50 training examples. It is also
likely the case that human and computer failures are qualitatively different ‚Äì human misclassiÔ¨Åcations are between
semantically similar categories (e.g. ‚Äúfood court‚Äù to ‚Äúfastfood restaurant‚Äù), while computational confusions are more
likely to include semantically unrelated scenes due to spurious visual matches (e.g. ‚Äúskatepark‚Äù to ‚Äúvan interior‚Äù). In
Figure 8 we analyze the degree to which human and computational confusions are similar. The implication is that the
human confusions are the most reasonable possible confusions, having the shortest possible semantic distance. But
human performance isn‚Äôt necessarily an upper bound ‚Äì in
fact, for many categories the humans are less accurate than
the best computational methods (Figure 6).
driving range outdoor
swimming pool outdoor
stadium football
excavation
desert sand
hot spring
cottage garden
field wild
chicken coop indoor
oil refinery outdoor
doorway outdoor
cathedral outdoor
control tower outdoor
oast house
schoolhouse
manufactured home
greenhouse outdoor
ski resort
limousine interior
factory indoor
chemistry lab
physics laboratory
art school
auditorium
restaurant kitchen
veterinarians office
amusement arcade
cheese factory
butchers shop
florist shop indoor
hospital room
laundromat
cubicle office
escalator indoor
cloister indoor
phone booth
jail indoor
elevator interior
Figure 5. This Ô¨Ågure shows the pattern of confusion across categories. The classes have been re-arranged to reveal the blocky
structure. For clarity, the elements in the diagonal have been set
to zero in order to increase the contrast of the off-diagonal elements. On the Y axis we show a sample of the scene categories.
Confusions seem to be coherent with semantic similarities across
classes. The scenes seem to be organized as indoor (top), urban
(center) and nature (bottom).
4. Computational Scene ClassiÔ¨Åcation
In this section we explore how discriminable the SUN
categories are with a variety of image features and kernels
paired with 1 vs. all support vector machines.
4.1. Image Features and Kernels
We selected or designed several state-of-art features that
are potentially useful for scene classiÔ¨Åcation. GIST features
 are proposed speciÔ¨Åcally for scene recognition tasks.
Dense SIFT features are also found to perform very well at
the 15-category dataset . We also evaluate sparse SIFTs
as used in ‚ÄúVideo Google‚Äù . HOG features provide excellent performance for object and human recognition tasks
 , so it is interesting to examine their utility for scene
recognition. While SIFT is known to be very good at Ô¨Ånding repeated image content, the self-similarity descriptor
(SSIM) relates images using their internal layout of local self-similarities. Unlike GIST, SIFT, and HOG, which
are all local gradient-based approaches, SSIM may provide
a distinct, complementary measure of scene layout that is
somewhat appearance invariant. As a baseline, we also include Tiny Images , color histograms and straight line
histograms. To make our color and texton histograms more
invariant to scene layout, we also build histograms for speciÔ¨Åc geometric classes as determined by . The geometric classiÔ¨Åcation of a scene is then itself used as a feature,
hopefully being invariant to appearance but responsive to
 
 

 
  
   
  
 
  
  
  
 ! 
 
Figure 6. Categories with similar and disparate performance in human and ‚Äúall features‚Äù SVM scene classiÔ¨Åcation. Human accuracy
is the left percentage and computer performance is the right percentage. From top to bottom, the rows are 1) categories for which
both humans and computational methods perform well, 2) categories for which both perform poorly, 3) categories for which humans perform better, and 4) categories for which computational
methods perform better. The ‚Äúall features‚Äù SVM tended to outperform humans on categories for which there are semantically similar yet visually distinct confusing categories. E.g. sandbar and
beach, baseball stadium and baseball Ô¨Åeld, landÔ¨Åll and garbage
GIST: The GIST descriptor computes the output energy of a bank of 24 Ô¨Ålters. The Ô¨Ålters are Gabor-like Ô¨Ålters
tuned to 8 orientations at 4 different scales. The square output of each Ô¨Ålter is then averaged on a 4 √ó 4 grid.
HOG2x2: First, histogram of oriented edges (HOG)
descriptors are densely extracted on a regular grid at
steps of 8 pixels. HOG features are computed using the
code available online provided by , which gives a 31dimension descriptor for each node of the grid. Then, 2 √ó 2
neighboring HOG descriptors are stacked together to form
a descriptor with 124 dimensions. The stacked descriptors
spatially overlap. This 2 √ó 2 neighbor stacking is important
because the higher feature dimensionality provides more
descriptive power. The descriptors are quantized into 300
visual words by k-means. With this visual word representation, three-level spatial histograms are computed on grids
of 1 √ó 1, 2 √ó 2 and 4 √ó 4. Histogram intersection is
used to deÔ¨Åne the similarity of two histograms at the same
pyramid level for two images. The kernel matrices at the
three levels are normalized by their respective means, and
linearly combined together using equal weights.
Dense SIFT: As with HOG2x2, SIFT descriptors are
densely extracted using a Ô¨Çat rather than Gaussian window at two scales (4 and 8 pixel radii) on a regular grid at
steps of 5 pixels. The three descriptors are stacked together
for each HSV color channels, and quantized into 300 visual
words by k-means, and spatial pyramid histograms are used
as kernels .
Sample Traning Images
Sample Correct Predictions
Most Confident False Positives (with True Label)
Least Confident False Negatives (with Wrong
Predicted Label)
parking garage
jail indoor
atrium public
car interior
car interior
car interior
car interior
car interior
airplane cabin
car interior
residential
neighborhood
residential
neighborhood
van interior
wine cellar
barrel storage
discotheque
electrical
substation
industrial area
oil refinery
oil refinery
amusement park
clothing store
laundromat
booth indoor
kitchenette
kitchenette
church indoor
laundromat
church indoor
canal natural
canal natural
volleyball court
toll plaza
general store
parking lot
kindergarden
control tower
mosque indoor
pub indoor
restaurant
clothing store
engine room
dinette vehicle
Figure 7. Selected SUN scene classiÔ¨Åcation results using all features.
LBP: Local Binary Patterns (LBP) is a powerful
texture feature based on occurrence histogram of local binary patterns. We can regard the scene recognition as a
texture classiÔ¨Åcation problem of 2D images, and therefore
apply this model to our problem. We also try the rotation
invariant extension version of LBP to examine whether
rotation invariance is suitable for scene recognition.
Sparse SIFT histograms: As in ‚ÄúVideo Google‚Äù ,
we build SIFT features at Hessian-afÔ¨Åne and MSER interest points. We cluster each set of SIFTs, independently,
into dictionaries of 1,000 visual words using k-means. An
image is represented by two histograms counting the number of sparse SIFTs that fall into each bin. An image is
represented by two 1,000 dimension histograms where each
SIFT is soft-assigned, as in , to its nearest cluster centers. Kernels are computed with œá2 distance.
SSIM: Self-similarity descriptors are computed on
a regular grid at steps of Ô¨Åve pixels. Each descriptor is obtained by computing the correlation map of a patch of 5 √ó 5
in a window with radius equal to 40 pixels, then quantizing
it in 3 radial bins and 10 angular bins, obtaining 30 dimensional descriptor vectors. The descriptors are then quantized into 300 visual words by k-means and we use œá2 distance on spatial histograms for the kernels.
Tiny Images: The most trivial way to match scenes is
to compare them directly in color image space. Reducing
the image dimensions drastically makes this approach more
computationally feasible and less sensitive to exact alignment. This method of image matching has been examined
thoroughly by Torralba et al. for the purpose of object
recognition and scene classiÔ¨Åcation.
Line Features: We detect straight lines from Canny
edges using the method described in Video Compass .
For each image we build two histograms based on the statistics of detected lines‚Äì one with bins corresponding to line
angles and one with bins corresponding to line lengths.
We use an RBF kernel to compare these unnormalized histograms. This feature was used in .
Texton Histograms: We build a 512 entry universal texton dictionary by clustering responses to a bank of Ô¨Ålters with 8 orientations, 2 scales, and 2 elongations. For
each image we then build a 512-dimensional histogram by
assigning each pixel‚Äôs set of Ô¨Ålter responses to the nearest
texton dictionary entry. We compute kernels from normalized œá2 distances.
Color Histograms: We build joint histograms of color
in CIE L*a*b* color space for each image. Our histograms
have 4, 14, and 14 bins in L, a, and b respectively for a total
of 784 dimensions. We compute distances between these
histograms using œá2 distance on the normalized histograms.
Geometric Probability Map: We compute the geometric class probabilities for image regions using the method of
Hoiem et al. . We use only the ground, vertical, porous,
and sky classes because they are more reliably classiÔ¨Åed.
We reduce the probability maps for each class to 8 √ó 8 and
use an RBF kernel. This feature was used in .
tiny image
straight line histograms
geometric classification map
color histograms
geometry color histograms
sparse SIFT histograms
texton histogram
dense SIFT
geometry texton histograms
percentage of same most confusing classes with human subjects
Figure 8. For each feature, the proportion of categories for which
the largest incorrect (off-diagonal) confusion is the same category
as the largest human confusion.
Geometry SpeciÔ¨Åc Histograms: Inspired by ‚ÄúIllumination Context‚Äù , we build color and texton histograms
for each geometric class (ground, vertical, porous, and sky).
SpeciÔ¨Åcally, for each color and texture sample, we weight
its contribution to each histogram by the probability that it
belongs to that geometric class. These eight histograms are
compared with œá2 distance after normalization.
4.2. Experiments and Analysis
With the features and kernels deÔ¨Åned above, we train
classiÔ¨Åers with one-vs-all Support Vector Machines. To better establish comparisons with other papers we start by providing results on the 15 scene categories dataset 
(Figure 4(a)).
For the experiments with our SUN database, the performance of all features enumerated above is compared in Figure 4(b). For each feature, we use the same set of training
and testing splits. For trials with fewer training examples,
the testing sets are kept unchanged while the training sets
are consistently decimated. The ‚Äúall features‚Äù classiÔ¨Åer is
built from a weighted sum of the kernels of the individual
features. The weight of each constituent kernel is proportional to the fourth power of its individual accuracy. As an
additional baseline, we plot the performance of the ‚Äúall features‚Äù kernel using one-nearest-neighbor classiÔ¨Åcation. The
1-vs-all SVM has nearly three times higher accuracy. It is
interesting to notice that with increasing amounts of training data, the performance increase is more pronounced with
the SUN dataset than the 15 scene dataset. The confusion
matrix of the ‚Äúall features‚Äù combined classiÔ¨Åer is shown in
The best scene classiÔ¨Åcation performance with all features, 38%, is still well below the human performance of
68%. Computational performance is best for outdoor natural scenes (43.2%), and then indoor scenes (37.5%), and
worst in outdoor man-made scenes (35.8%).
Within the
hierarchy, indoor transportation (vehicle interiors, stations,
etc.) scenes are the most accurately classiÔ¨Åed (51.9%) while
indoor shopping and dining scenes were least accurately
classiÔ¨Åed (29.0%). In Figure 6, we examine the categories
Ground¬†Truth¬†Annotations
Figure 9. (a) Examples of photographs that contain multiple scene
types. (b) Ground truth annotations. (c)-(e): Detections of beach,
harbor, and village scene categories in a single image. In all visualizations, correct detections are green and incorrect detections
are red. Bounding box size is proportional to classiÔ¨Åer conÔ¨Ådence.
For this and other visualizations, all detections above a constant
conÔ¨Ådence threshold are shown. In this result, one harbor detection is incorrect because it does not overlap with enough ground
truth ‚Äúharbor‚Äù annotation. ‚ÄúBeach‚Äù or ‚Äúvillage‚Äù would have been
acceptable.
for which human and machine accuracies are most similar
and most dissimilar. In Figure 8 we examine the similarity in scene classiÔ¨Åcation confusions between humans and
machines. The better performing features not only tend to
agree with humans on correct classiÔ¨Åcations, they also tend
to make the same mistakes that humans make. However,
humans have dramatically fewer confusions ‚Äì for humans,
on average, the three largest entries in each row of the confusion matrix sum to 0.95, while the ‚Äúall features‚Äù SVM
needs 11 entries to reach this mark.
5. Scene Detection
The dominant view in the scene recognition literature is
that one image depicts one scene category. There are a few
exceptions to this trend ‚Äì In , each image is graded according to different scene categories. In , scenes are
represented along continuous dimensions. However, current approaches assume that there is a unique scene label
that can be assigned to an entire image, and this assumption
is reasonable within a relatively narrow and disjoint selection of categories (e.g. the 15 in ), but the real world
is not so neatly divided. Just as people can move continuously between scene categories (e.g. ‚ÄúofÔ¨Åce‚Äù into ‚Äúcorridor‚Äù, ‚Äústreet‚Äù into ‚Äúshopfront‚Äù), it is frequently the case that
a single photograph depicts multiple scenes types at different scales and locations within the image (see Figure 9). By
constructing a scene database with broad coverage we can
explore the possibility of distinguishing all of the scenes
types within single images.
We introduce the concept of Scene Detection ‚Äì recognizing the scene type within image regions rather than entire images. Our terminology is consistent with the object
detection literature where object classiÔ¨Åcation involves
classifying entire images, while object detection requires
Scene Category
(exterior)
gas station
playground
restaurant
skyscraper
All Features
Tiny Images
Table 1. Scene Detection Average Precision. We compare the scene detection performance of our algorithm using all features and 200
training examples per class to baselines using only the ‚Äútiny images‚Äù feature and random guessing. ‚ÄúSky‚Äù, ‚ÄúForest‚Äù, and ‚ÄúBuilding
Facade‚Äù make up a large portion of the test set and thus random guessing can achieve signiÔ¨Åcant AP.
localizing and recognizing objects within an image.
As in object detection, we adopt a multiscale scanningwindow approach to Ô¨Ånd sub-scenes. More speciÔ¨Åcally, we
examine sub-images of a Ô¨Åxed aspect ratio at three different scales (1, 0.65. 0.42). We classify ‚àº100 crops per
image using the same features, training data, and classiÔ¨Åcation pipeline used in Section 4. Finally we evaluate our
detections against a novel, spatially annotated test set as described below.
5.1. Test Set and Evaluation Criteria
For these experiments we restrict ourselves to outdoor,
urban environments to make the annotation process easier.
We use 24 of the 398 well-sampled SUN categories. Our
test set consists of 104 photographs containing an average
of four scene categories each. In every photo we trace the
ground truth spatial extent of each sub-scene and ensure that
sub-scenes obey the same deÔ¨Ånitions used to construct the
SUN database.
A ground truth annotation labeled ‚Äúvillage‚Äù implies if
a sub-image bounding box (Bp) has at least T % overlap
with polygon (Pgt), it can be correctly classiÔ¨Åed as a ‚Äúvillage‚Äù. More precisely, a correct detection has area(Bp ‚à©
Pgt)/area(Bp) ‚â•T . This notion of overlap is not symmetric as it is in object detection . We do not care if the
ground truth annotation is larger than the detection. A beach
detection is correct even if the beach has much greater spatial extent than the detection. In this regard, scenes behave
more like materials (or ‚Äústuff‚Äù ) in that they have unspeciÔ¨Åed spatial extent. The boundaries of a street scene or
a forest are not as well deÔ¨Åned as the boundaries of a chair.4
Annotations of differing categories can also spatially
overlap ‚Äì A ‚Äúrestaurant patio‚Äù can be wholly contained
within a ‚Äúplaza‚Äù. Overlap can also occur where scene types
transition or share dual meaning, e.g. the ‚Äúbeach‚Äù and ‚Äúharbor‚Äù in Figure 9. In our experiments, we set the overlap
threshold T = 15%. This is necessarily small because in
some scene types, e.g. ‚Äútower‚Äù and ‚Äústreet‚Äù, the deÔ¨Åning
element of the scene can occupy a relatively small percent-
4Along these same lines, non-maximum suppression of overlapped detections is not important in this domain. While it is reasonable to require
that one chair should generate one detection, a forest can generate an arbitrary number of smaller forest scenes.
age of pixels. A side effect of this low threshold, together
with possibly overlapped annotations, is that for some subimages there is more than one valid classiÔ¨Åcation. At a
given image location, the number of valid classiÔ¨Åcations
can change with scale. In Figure 9, classifying the entire
image ‚Äúvillage‚Äù would be incorrect, but some sub-images
are small enough such that the ground truth ‚Äúvillage‚Äù annotation exceeds T % overlap.
Unlike most object detection approaches, we have no
‚Äúbackground‚Äù class. Under our overlap criteria, more than
90% of the sub-images in our test set have a valid scene
classiÔ¨Åcation. Those that do not are excluded from all evaluations. In order to achieve a perfect recall rate under our
evaluation criteria an algorithm must densely classify nearly
‚àº100 sub-images per test image (not just one correct detection per scene annotation). This is a difÔ¨Åcult task because
the detector must recognize sub-scenes that are transitioning from one category to another, even though such transition examples are infrequent in the SUN database because
ambiguous scenes were Ô¨Åltered out. However, at lower recall rates our classiÔ¨Åers are quite accurate (on average, 80%
accuracy at 20% recall).
5.2. Experiments and Analysis
To distinguish between the 24 scene detection classes we
Ô¨Årst train 1-vs-all SVMs using the same training data (50
per class), features, and distance metrics described in Section 4. For each class, we compute the average precision
of the detector in a manner analogous to the PASCAL VOC
evaluation, except for our differing overlap criteria. Average precision varies from 62% for ‚Äúsky‚Äù to 2.8% for ‚Äúbalcony (exterior)‚Äù with an average of 30.1%. If we instead
use more of the SUN database to train our detectors (200
exemplars per class), we get the performance shown in Table 1.
6. Conclusion
To advance the Ô¨Åeld of scene understanding we need
datasets that encompass the richness and varieties of environmental scenes and knowledge about how scene categories are organized and distinguished from each other. In
this work, we have proposed a quasi-exhaustive dataset of
scene categories (899 environments). Using state-of-theart algorithms for image classiÔ¨Åcation, we have achieved
new performance bounds for scene classiÔ¨Åcation. We hope
that the SUN database will help the community advance the
state of scene understanding. Finally, we introduced a new
task of scene detection within images.
Acknowledgments
This work is funded by NSF CAREER Awards 0546262
to A.O, 0747120 to A.T. and partly funded by BAE Systems under Subcontract No. 073692 (Prime Contract No.
HR0011-08-C-0134 issued by DARPA), Foxconn and gifts
from Google and Microsoft. K.A.E is funded by a NSF
Graduate Research fellowship.