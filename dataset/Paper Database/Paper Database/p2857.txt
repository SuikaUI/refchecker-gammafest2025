HAL Id: hal-00476545
 
 
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Analysis of a Random Forests Model
Gérard Biau
To cite this version:
Gérard Biau. Analysis of a Random Forests Model. 2010. ￿hal-00476545v3￿
Analysis of a Random Forests Model
G´erard Biau
LSTA & LPMA1
Universit´e Pierre et Marie Curie – Paris VI
Boˆıte 158, Tour 15-25, 2`eme ´etage
4 place Jussieu, 75252 Paris Cedex 05, France
Ecole Normale Sup´erieure
45 rue d’Ulm
75230 Paris Cedex 05, France
 
Random forests are a scheme proposed by Leo Breiman in the 2000’s
for building a predictor ensemble with a set of decision trees that grow
in randomly selected subspaces of data. Despite growing interest and
practical use, there has been little exploration of the statistical properties of random forests, and little is known about the mathematical
forces driving the algorithm. In this paper, we oﬀer an in-depth analysis of a random forests model suggested by Breiman in , which
is very close to the original algorithm. We show in particular that
the procedure is consistent and adapts to sparsity, in the sense that
its rate of convergence depends only on the number of strong features
and not on how many noise variables are present.
Index Terms — Random forests, randomization, sparsity, dimension
reduction, consistency, rate of convergence.
2010 Mathematics Subject Classiﬁcation: 62G05, 62G20.
1Research partially supported by the French National Research Agency under grant
ANR-09-BLAN-0051-02 “CLARA”.
2Research carried out within the INRIA project “CLASSIC” hosted by Ecole Normale
Sup´erieure and CNRS.
Introduction
Random forests
In a series of papers and technical reports, Breiman demonstrated that substantial gains in classiﬁcation and regression accuracy can be
achieved by using ensembles of trees, where each tree in the ensemble is grown
in accordance with a random parameter. Final predictions are obtained by
aggregating over the ensemble. As the base constituents of the ensemble are
tree-structured predictors, and since each of these trees is constructed using
an injection of randomness, these procedures are called “random forests”.
Breiman’s ideas were decisively inﬂuenced by the early work of Amit and Geman on geometric feature selection, the random subspace method of Ho
 and the random split selection approach of Dietterich . As highlighted
by various empirical studies (see for instance), random
forests have emerged as serious competitors to state-of-the-art methods such
as boosting (Freund ) and support vector machines (Shawe-Taylor and
Cristianini ). They are fast and easy to implement, produce highly accurate predictions and can handle a very large number of input variables
without overﬁtting. In fact, they are considered to be one of the most accurate general-purpose learning techniques available. The survey by Genuer et
al. may provide the reader with practical guidelines and a good starting
point for understanding the method.
In Breiman’s approach, each tree in the collection is formed by ﬁrst selecting
at random, at each node, a small group of input coordinates (also called
features or variables hereafter) to split on and, secondly, by calculating the
best split based on these features in the training set.
The tree is grown
using CART methodology (Breiman et al. ) to maximum size, without
pruning. This subspace randomization scheme is blended with bagging ( ) to resample, with replacement, the training data set each time a
new individual tree is grown.
Although the mechanism appears simple, it involves many diﬀerent driving
forces which make it diﬃcult to analyse. In fact, its mathematical properties
remain to date largely unknown and, up to now, most theoretical studies
have concentrated on isolated parts or stylized versions of the algorithm. Interesting attempts in this direction are by Lin and Jeon , who establish a
connection between random forests and adaptive nearest neighbor methods
(see also for further results); Meinshausen , who studies the consistency of random forests in the context of conditional quantile prediction;
and Devroye et al. , who oﬀer consistency theorems for various simpliﬁed
versions of random forests and other randomized ensemble predictors. Nevertheless, the statistical mechanism of “true” random forests is not yet fully
understood and is still under active investigation.
In the present paper, we go one step further into random forests by working
out and solidifying the properties of a model suggested by Breiman in .
Though this model is still simple compared to the “true” algorithm, it is
nevertheless closer to reality than any other scheme we are aware of. The
short draft is essentially based on intuition and mathematical heuristics,
some of them are questionable and make the document diﬃcult to read and
understand. However, the ideas presented by Breiman are worth clarifying
and developing, and they will serve as a starting point for our study.
Before we formalize the model, some deﬁnitions are in order.
Throughout the document, we suppose that we are given a training sample Dn =
{(X1, Y1), . . . , (Xn, Yn)} of i.i.d. d × R-valued random variables (d ≥2)
with the same distribution as an independent generic pair (X, Y ) satisfying EY 2 < ∞. The space d is equipped with the standard Euclidean
metric. For ﬁxed x ∈ d, our goal is to estimate the regression function
r(x) = E[Y |X = x] using the data Dn. In this respect, we say that a regression function estimate rn is consistent if E[rn(X) −r(X)]2 →0 as n →∞.
The main message of this paper is that Breiman’s procedure is consistent
and adapts to sparsity, in the sense that its rate of convergence depends only
on the number of strong features and not on how many noise variables are
Formally, a random forest is a predictor consisting of a collection of randomized base regression trees {rn(x, Θm, Dn), m ≥1}, where Θ1, Θ2, . . . are i.i.d.
outputs of a randomizing variable Θ. These random trees are combined to
form the aggregated regression estimate
¯rn(X, Dn) = EΘ [rn(X, Θ, Dn)] ,
where EΘ denotes expectation with respect to the random parameter, conditionally on X and the data set Dn. In the following, to lighten notation a
little, we will omit the dependency of the estimates in the sample, and write
for example ¯rn(X) instead of ¯rn(X, Dn). Note that, in practice, the above expectation is evaluated by Monte Carlo, i.e., by generating M (usually large)
random trees, and taking the average of the individual outcomes (this procedure is justiﬁed by the law of large numbers, see the appendix in Breiman
 ). The randomizing variable Θ is used to determine how the successive
cuts are performed when building the individual trees, such as selection of
the coordinate to split and position of the split.
In the model we have in mind, the variable Θ is assumed to be independent of
X and the training sample Dn. This excludes in particular any bootstrapping
or resampling step in the training set. This also rules out any data-dependent
strategy to build the trees, such as searching for optimal splits by optimizing
some criterion on the actual observations. However, we allow Θ to be based
on a second sample, independent of, but distributed as, Dn. This important
issue will be thoroughly discussed in Section 3.
With these warnings in mind, we will assume that each individual random
tree is constructed in the following way. All nodes of the tree are associated
with rectangular cells such that at each step of the construction of the tree,
the collection of cells associated with the leaves of the tree (i.e., external
nodes) forms a partition of d. The root of the tree is d itself. The
following procedure is then repeated ⌈log2 kn⌉times, where log2 is the base-2
logarithm, ⌈.⌉the ceiling function and kn ≥2 a deterministic parameter,
ﬁxed beforehand by the user, and possibly depending on n.
1. At each node, a coordinate of X = (X(1), . . . , X(d)) is selected, with
the j-th feature having a probability pnj ∈(0, 1) of being selected.
2. At each node, once the coordinate is selected, the split is at the midpoint of the chosen side.
Each randomized tree rn(X, Θ) outputs the average over all Yi for which the
corresponding vectors Xi fall in the same cell of the random partition as
X. In other words, letting An(X, Θ) be the rectangular cell of the random
partition containing X,
rn(X, Θ) =
i=1 Yi1[Xi∈An(X,Θ)]
i=1 1[Xi∈An(X,Θ)]
where the event En(X, Θ) is deﬁned by
En(X, Θ) =
1[Xi∈An(X,Θ)] ̸= 0
(Thus, by convention, the estimate is set to 0 on empty cells.) Taking ﬁnally
expectation with respect to the parameter Θ, the random forests regression
estimate takes the form
¯rn(X) = EΘ [rn(X, Θ)] = EΘ
i=1 Yi1[Xi∈An(X,Θ)]
i=1 1[Xi∈An(X,Θ)]
Let us now make some general remarks about this random forests model.
First of all, we note that, by construction, each individual tree has exactly
2⌈log2 kn⌉(≈kn) terminal nodes, and each leaf has Lebesgue measure 2−⌈log2 kn⌉
(≈1/kn). Thus, if X has uniform distribution on d, there will be on
average about n/kn observations per terminal node. In particular, the choice
kn = n induces a very small number of cases in the ﬁnal leaves, in accordance
with the idea that the single trees should not be pruned.
Next, we see that, during the construction of the tree, at each node, each
candidate coordinate X(j) may be chosen with probability pnj ∈(0, 1). This
implies in particular Pd
j=1 pnj = 1.
Although we do not precise for the
moment the way these probabilities are generated, we stress that they may
be induced by a second sample. This includes the situation where, at each
node, randomness is introduced by selecting at random (with or without
replacement) a small group of input features to split on, and choosing to
cut the cell along the coordinate—inside this group—which most decreases
some empirical criterion evaluated on the extra sample. This scheme is close
to what the original random forests algorithm does, the essential diﬀerence
being that the latter algorithm uses the actual data set to calculate the best
splits. This point will be properly discussed in Section 3.
Finally, the requirement that the splits are always achieved at the middle
of the cell sides is mainly technical, and it could eventually be replaced by
a more involved random mechanism—based on the second sample—, at the
price of a much more complicated analysis.
The document is organized as follows. In Section 2, we prove that the random
forests regression estimate ¯rn is consistent and discuss its rate of convergence.
As a striking result, we show under a sparsity framework that the rate of
convergence depends only on the number of active (or strong) variables and
not on the dimension of the ambient space.
This feature is particularly
desirable in high-dimensional regression, when the number of variables can
be much larger than the sample size, and may explain why random forests
are able to handle a very large number of input variables without overﬁtting.
Section 3 is devoted to a discussion, and a small simulation study is presented
in Section 4. For the sake of clarity, proofs are postponed to Section 5.
Asymptotic analysis
Throughout the document, we denote by Nn(X, Θ) the number of data points
falling in the same cell as X, i.e.,
Nn(X, Θ) =
1[Xi∈An(X,Θ)].
We start the analysis with the following simple theorem, which shows that
the random forests estimate ¯rn is consistent.
Theorem 2.1 Assume that the distribution of X has support on d.
Then the random forests estimate ¯rn is consistent whenever pnj log kn →∞
for all j = 1, . . . , d and kn/n →0 as n →∞.
Theorem 2.1 mainly serves as an illustration of how the consistency problem
of random forests predictors may be attacked. It encompasses, in particular,
the situation where, at each node, the coordinate to split is chosen uniformly
at random over the d candidates. In this “purely random” model, pnj = 1/d,
independently of n and j, and consistency is ensured as long as kn →∞
and kn/n →0. This is however a radically simpliﬁed version of the random
forests used in practice, which does not explain the good performance of the
algorithm. To achieve this goal, a more in-depth analysis is needed.
There is empirical evidence that many signals in high-dimensional spaces
admit a sparse representation. As an example, wavelet coeﬃcients of images often exhibit exponential decay, and a relatively small subset of all
wavelet coeﬃcients allows for a good approximation of the original image.
Such signals have few non-zero coeﬃcients and can therefore be described
as sparse in the signal domain (see for instance ). Similarly, recent advances in high-throughput technologies—such as array comparative genomic
hybridization—indicate that, despite the huge dimensionality of problems,
only a small number of genes may play a role in determining the outcome
and be required to create good predictors ( for instance). Sparse estimation is playing an increasingly important role in the statistics and machine
learning communities, and several methods have recently been developed in
both ﬁelds, which rely upon the notion of sparsity (e.g. penalty methods like
the Lasso and Dantzig selector, see and the references therein).
Following this idea, we will assume in our setting that the target regression
function r(X) = E[Y |X], which is initially a function of X = (X(1), . . . , X(d)),
depends in fact only on a nonempty subset S (for Strong) of the d features.
In other words, letting XS = (Xj : j ∈S) and S = Card S, we have
r(X) = E[Y |XS]
or equivalently, for any x ∈ d,
r(x) = r⋆(xS)
where µ is the distribution of X and r⋆: S →R is the section of r
corresponding to S. To avoid trivialities, we will assume throughout that S
is nonempty, with S ≥2. The variables in the set W = {1, . . . , d} −S (for
Weak) have thus no inﬂuence on the response and could be safely removed.
In the dimension reduction scenario we have in mind, the ambient dimension
d can be very large, much larger than the sample size n, but we believe that
the representation is sparse, i.e., that very few coordinates of r are non-zero,
with indices corresponding to the set S. Note however that representation
(2.1) does not forbid the somehow undesirable case where S = d. As such, the
value S characterizes the sparsity of the model: The smaller S, the sparser
Within this sparsity framework, it is intuitively clear that the coordinatesampling probabilities should ideally satisfy the constraints pnj = 1/S for
j ∈S (and, consequently, pnj = 0 otherwise). However, this is a too strong
requirement, which has no chance to be satisﬁed in practice, except maybe
in some special situations where we know beforehand which variables are
important and which are not. Thus, to stick to reality, we will rather require
in the following that pnj = (1/S)(1+ξnj) for j ∈S (and pnj = ξnj otherwise),
where pnj ∈(0, 1) and each ξnj tends to 0 as n tends to inﬁnity. We will
see in Section 3 how to design a randomization mechanism to obtain such
probabilities, on the basis of a second sample independent of the training set
Dn. At this point, it is important to note that the dimensions d and S are
held constant throughout the document. In particular, these dimensions are
not functions of the sample size n, as it may be the case in other asymptotic
We have now enough material for a deeper understanding of the random
forests algorithm. To lighten notation a little, we will write
Wni(X, Θ) = 1[Xi∈An(X,Θ)]
so that the estimate takes the form
EΘ [Wni(X, Θ)] Yi.
Let us start with the variance/bias decomposition
E [¯rn(X) −r(X)]2 = E [¯rn(X) −˜rn(X)]2 + E [˜rn(X) −r(X)]2 ,
where we set
EΘ [Wni(X, Θ)] r(Xi).
The two terms of (2.2) will be examined separately, in Proposition 2.1 and
Proposition 2.2, respectively. Throughout, the symbol V denotes variance.
Proposition 2.1 Assume that X is uniformly distributed on d and, for
all x ∈Rd,
σ2(x) = V[Y | X = x] ≤σ2
for some positive constant σ2. Then, if pnj = (1/S)(1 + ξnj) for j ∈S,
E [¯rn(X) −˜rn(X)]2 ≤Cσ2
n(log kn)S/2d,
The sequence (ξn) depends on the sequences {(ξnj) : j ∈S} only and tends
to 0 as n tends to inﬁnity.
A close inspection of the end of the proof of Proposition 2.1
reveals that
(1 + ξnj)−1
In particular, if a < pnj < b for some constants a, b ∈(0, 1), then
The main message of Proposition 2.1 is that the variance of the forests estimate is O(kn/(n(log kn)S/2d)). This result is interesting by itself since it
shows the eﬀect of aggregation on the variance of the forest. To understand
this remark, recall that individual (random or not) trees are proved to be
consistent by letting the number of cases in each terminal node become large
(see [19, Chapter 20]), with a typical variance of the order kn/n. Thus, for
such trees, the choice kn = n (i.e., about one observation on average in each
terminal node) is clearly not suitable and leads to serious overﬁtting and
variance explosion. On the other hand, the variance of the forest is of the
order kn/(n(log kn)S/2d). Therefore, letting kn = n, the variance is of the
order 1/(log n)S/2d, a quantity which still goes to 0 as n grows! Proof of
Proposition 2.1 reveals that this log term is a by-product of the Θ-averaging
process, which appears by taking into consideration the correlation between
trees. We believe that it provides an interesting perspective on why random
forests are still able to do a good job, despite the fact that individual trees
are not pruned.
Note ﬁnally that the requirement that X is uniformly distributed on the
hypercube could be safely replaced by the assumption that X has a density
with respect to the Lebesgue measure on d and the density is bounded
from above and from below. The case where the density of X is not bounded
from below necessitates a speciﬁc analysis, which we believe is beyond the
scope of the present paper.
We refer the reader to for results in this
direction (see also Remark 5 in Section 5).
Let us now turn to the analysis of the bias term in equality (2.2). Recall that
r⋆denotes the section of r corresponding to S.
Proposition 2.2 Assume that X is uniformly distributed on d and r⋆
is L-Lipschitz on S. Then, if pnj = (1/S)(1 + ξnj) for j ∈S,
E [˜rn(X) −r(X)]2 ≤
S log 2 (1+γn)
x∈ d r2(x)
where γn = minj∈S ξnj tends to 0 as n tends to inﬁnity.
This result essentially shows that the rate at which the bias decreases to 0
depends on the number of strong variables, not on d. In particular, the quantity kn
−(0.75/(S log 2))(1+γn) should be compared with the ordinary partitioning
estimate bias, which is of the order kn
−2/d under the smoothness conditions
of Proposition 2.2 (see for instance ). In this respect, it is easy to see
−(0.75/(S log 2))(1+γn) = o(kn
−2/d) as soon as S ≤⌊0.54d⌋(⌊.⌋is the integer part function). In other words, when the number of active variables is
less than (roughly) half of the ambient dimension, the bias of the random
forests regression estimate decreases to 0 much faster than the usual rate.
The restriction S ≤⌊0.54d⌋is not severe, since in all practical situations we
have in mind, d is usually very large with respect to S (this is, for instance,
typically the case in modern genome biology problems, where d may be of the
order of billion, and in any case much larger than the actual number of active
features). Note at last that, contrary to Proposition 2.1, the term e−n/2kn
prevents the extreme choice kn = n (about one observation on average in
each terminal node). Indeed, an inspection of the proof of Proposition 2.2
reveals that this term accounts for the probability that Nn(X, Θ) is precisely
0, i.e., An(X, Θ) is empty.
Recalling the elementary inequality ze−nz ≤e−1/n for z ∈ , we may
ﬁnally join Proposition 2.1 and Proposition 2.2 and state our main theorem.
Theorem 2.2 Assume that X is uniformly distributed on d, r⋆is L-
Lipschitz on S and, for all x ∈Rd,
σ2(x) = V[Y | X = x] ≤σ2
for some positive constant σ2. Then, if pnj = (1/S)(1+ξnj) for j ∈S, letting
γn = minj∈S ξnj, we have
E [¯rn(X) −r(X)]2 ≤Ξn
S log 2 (1+γn)
(1 + ξn) + 2e−1
x∈ d r2(x)
The sequence (ξn) depends on the sequences {(ξnj) : j ∈S} only and tends
to 0 as n tends to inﬁnity.
As we will see in Section 3, it may be safely assumed that the randomization
process allows for ξnj log n →0 as n →∞, for all j ∈S. Thus, under this
condition, Theorem 2.2 shows that with the optimal choice
kn ∝n1/(1+
S log 2 ),
E [¯rn(X) −r(X)]2 = O
S log 2+0.75
This result can be made more precise. Denote by FS the class of (L, σ2)smooth distributions (X, Y ) such that X has uniform distribution on d,
the regression function r⋆is Lipschitz with constant L on S and, for all
x ∈Rd, σ2(x) = V[Y | X = x] ≤σ2.
Corollary 2.1 Let
x∈ d r2(x)
Then, if pnj = (1/S)(1 + ξnj) for j ∈S, with ξnj log n →0 as n →∞, for
the choice
S log 2 ),
E [¯rn(X) −r(X)]2
S log 2+0.75 n
S log 2+0.75
where Λ is a positive constant independent of r, L and σ2.
This result reveals the fact that the L2-rate of convergence of ¯rn(X) to r(X)
depends only on the number S of strong variables, and not on the ambient
dimension d. The main message of Corollary 2.1 is that if we are able to
properly tune the probability sequences (pnj)n≥1 and make them suﬃciently
fast to track the informative features, then the rate of convergence of the
random forests estimate will be of the order n
S log 2+0.75. This rate is strictly
faster than the usual rate n−2/(d+2) as soon as S ≤⌊0.54d⌋. To understand
this point, just recall that the rate n−2/(d+2) is minimax optimal for the
class Fd (see for example Ibragimov and Khasminskii ), seen as
a collection of regression functions over d, not S. However, in our
setting, the intrinsic dimension of the regression problem is S, not d, and the
random forests estimate cleverly adapts to the sparsity of the problem. As an
illustration, Figure 1 shows the plot of the function S 7→0.75/(S log 2+0.75)
for S ranging from 2 to d = 100.
It is noteworthy that the rate of convergence of the ξnj to 0 (and, consequently, the rate at which the probabilities pnj approach 1/S for j ∈S) will
eventually depend on the ambient dimension d through the ratio S/d. The
same is true for the Lipschitz constant L and the factor supx∈ d r2(x) which
both appear in Corollary 2.1. To ﬁgure out this remark, remember ﬁrst that
the support of r is contained in RS, so that the later supremum (respectively,
the Lipschitz constant) is in fact a supremum (respectively, a Lipschitz constant) over RS, not over Rd. Next, denote by Cp(s) the collection of functions
Figure 1: Solid line: Plot of the function S 7→0.75/(S log 2 + 0.75) for S
ranging from 2 to d = 100. Dotted line: Plot of the minimax rate power
S 7→2/(S + 2). The horizontal line shows the value of the d-dimensional rate
power 2/(d + 2) ≈0.0196.
η : p → for which each derivative of order s satisﬁes a Lipschitz
condition. It is well known that the ε-entropy log2(Nε) of Cp(s) is Φ(ε−p/(s+1))
as ε ↓0 (Kolmogorov and Tihomirov ), where an = Φ(bn) means that
an = O(bn) and bn = O(an). Here we have an interesting interpretation of
the dimension reduction phenomenon: Working with Lipschitz functions on
RS (that is, s = 0) is roughly equivalent to working with functions on Rd for
which all [(d/S)−1]-th order derivatives are Lipschitz! For example, if S = 1
and d = 25, (d/S) −1 = 24 and, as there are 2524 such partial derivatives
in R25, we note immediately the potential beneﬁt of recovering the “true”
dimension S.
The reduced-dimensional rate n
S log 2+0.75 is strictly larger than
the S-dimensional optimal rate n−2/(S+2), which is also shown in Figure 1 for
S ranging from 2 to 100. We do not know whether the latter rate can be
achieved by the algorithm.
The optimal parameter kn of Corollary 2.1 depends on the unknown distribution of (X, Y ), especially on the smoothness of the regression
function and the eﬀective dimension S. To correct this situation, adaptive
(i.e., data-dependent) choices of kn, such as data-splitting or cross-validation,
should preserve the rate of convergence of the estimate. Another route we
may follow is to analyse the eﬀect of bootstrapping the sample before growing
the individual trees (i.e., bagging). It is our belief that this procedure should
also preserve the rate of convergence, even for overﬁtted trees (kn ≈n), in
the spirit of . However, such a study is beyond the scope of the present
For further references, it is interesting to note that Proposition
2.1 (variance term) is a consequence of aggregation, whereas Proposition 2.2
(bias term) is a consequence of randomization.
It is also stimulating to keep in mind the following analysis, which has been
suggested to us by a referee. Suppose, to simplify, that Y = r(X) (no-noise
regression) and that Pn
i=1 Wni(X, Θ) = 1 a.s. In this case, the variance term
is 0 and we have
¯rn(X) = ˜rn(X) =
EΘ [Wni(Θ, X)] Yi.
Set Zn = (Y, Y1, . . . , Yn). Then
E [¯rn(X) −r(X)]2
= E [¯rn(X) −Y ]2
(¯rn(X) −Y )2 | Zn
(¯rn(X) −E[¯rn(X) | Zn])2 | Zn
+ E [E[¯rn(X) | Zn] −Y ]2 .
The conditional expectation in the ﬁrst of the two terms above may be rewritten under the form
E [Cov (EΘ [rn(X, Θ)] , EΘ′ [rn(X, Θ′)] | Zn)] ,
where Θ′ is distributed as, and independent of, Θ. Attention shows that this
last term is indeed equal to
E [EΘ,Θ′Cov (rn(X, Θ), rn(X, Θ′) | Zn)]
The key observation is that if trees have strong predictive power, then they
can be unconditionally strongly correlated while being conditionally weakly
correlated. This opens an interesting line of research for the statistical analysis of the bias term, in connection with Amit and Blanchard conditional
covariance-analysis ideas.
Discussion
The results which have been obtained in Section 2 rely on appropriate behavior of the probability sequences (pnj)n≥1, j = 1, . . . , d. We recall that these
sequences should be in (0, 1) and obey the constraints pnj = (1/S)(1 + ξnj)
for j ∈S (and pnj = ξnj otherwise), where the (ξnj)n≥1 tend to 0 as n
tends to inﬁnity. In other words, at each step of the construction of the
individual trees, the random procedure should track and preferentially cut
the strong coordinates. In this more informal section, we brieﬂy discuss a
random mechanism for inducing such probability sequences.
Suppose, to start with an imaginary scenario, that we already know which
coordinates are strong, and which are not. In this ideal case, the random
selection procedure described in the introduction may be easily made more
precise as follows. A positive integer Mn—possibly depending on n—is ﬁxed
beforehand and the following splitting scheme is iteratively repeated at each
node of the tree:
1. Select at random, with replacement, Mn candidate coordinates to split
2. If the selection is all weak, then choose one at random to split on. If
there is more than one strong variable elected, choose one at random
Within this framework, it is easy to see that each coordinate in S will be cut
with the “ideal” probability
Though this is an idealized model, it already gives some information about
the choice of the parameter Mn, which, in accordance with the results of
Section 2 (Corollary 2.1), should satisfy
This is true as soon as
This result is consistent with the general empirical ﬁnding that Mn (called
mtry in the R package RandomForests) does not need to be very large (see,
for example, Breiman ), but not with the widespread belief that Mn
should not depend on n. Note also that if the Mn features are chosen at
random without replacement, then things are even more simple since, in this
n = 1/S for all n large enough.
In practice, we have only a vague idea about the size and content of the set
S. However, to circumvent this problem, we may use the observations of an
independent second set D′
n (say, of the same size as Dn) in order to mimic
the ideal split probability p⋆
n. To illustrate this mechanism, suppose—to keep
things simple—that the model is linear, i.e.,
ajX(j) + ε,
where X = (X(1), . . . , X(d)) is uniformly distributed over d, the aj are
non-zero real numbers, and ε is a zero-mean random noise, which is assumed
to be independent of X and with ﬁnite variance. Note that, in accordance
with our sparsity assumption, r(X) = P
j∈S ajX(j) depends on XS only.
Assume now that we have done some splitting and arrived at a current set
of terminal nodes.
Consider any of these nodes, say A = Qd
j=1 Aj, ﬁx a
coordinate j ∈{1, . . . , d}, and look at the weighted conditional variance
V[Y |X(j) ∈Aj] P(X(j) ∈Aj). It is a simple exercise to prove that if X is
uniform and j ∈S, then the split on the j-th side which most decreases the
weighted conditional variance is at the midpoint of the node, with a variance
decrease equal to a2
j/16 > 0. On the other hand, if j ∈W, the decrease of
the variance is always 0, whatever the location of the split.
On the practical side, the conditional variances are of course unknown, but
they may be estimated by replacing the theoretical quantities by their respective sample estimates (as in the CART procedure, see Breiman et al. [11,
Chapter 8] for a thorough discussion) evaluated on the second sample D′
This suggests the following procedure, at each node of the tree:
1. Select at random, with replacement, Mn candidate coordinates to split
2. For each of the Mn elected coordinates, calculate the best split, i.e.,
the split which most decreases the within-node sum of squares on the
second sample D′
3. Select one variable at random among the coordinates which output the
best within-node sum of squares decreases, and cut.
This procedure is indeed close to what the random forests algorithm does.
The essential diﬀerence is that we suppose to have at hand a second sample
n, whereas the original algorithm performs the search of the optimal cuts
on the original observations Dn. This point is important, since the use of
an extra sample preserves the independence of Θ (the random mechanism)
and Dn (the training sample).
We do not know whether our results are
still true if Θ depends on Dn (as in the CART algorithm), but the analysis
does not appear to be simple. Note also that, at step 3, a threshold (or
a test procedure, as suggested in Amaratunga et al. ) could be used to
choose among the most signiﬁcant variables, whereas the actual algorithm
just selects the best one. In fact, depending on the context and the actual
cut selection procedure, the informative probabilities pnj (j ∈S) may obey
the constraints pnj →pj as n →∞(thus, pj is not necessarily equal to 1/S),
where the pj are positive and satisfy P
j∈S pj = 1. This should not aﬀect the
results of the article.
This empirical randomization scheme leads to complicate probabilities of cuts
which, this time, vary at each node of each tree and are not easily amenable
to analysis. Nevertheless, observing that the average number of cases per
terminal node is about n/kn, it may be inferred by the law of large numbers
that each variable in S will be cut with probability
(1 + ζnj),
where ζnj is of the order O(kn/n), a quantity which anyway goes fast to 0 as
n tends to inﬁnity. Put diﬀerently, for j ∈S,
S (1 + ξnj) ,
where ξnj goes to 0 and satisﬁes the constraint ξnj log n →0 as n tends to
inﬁnity, provided kn log n/n →0, Mn →∞and Mn/ log n →∞. This is
coherent with the requirements of Corollary 2.1. We realize however that
this is a rough approach, and that more theoretical work is needed here to
fully understand the mechanisms involved in CART and Breiman’s original
randomization process.
It is also noteworthy that random forests use the so-called out-of-bag samples
(i.e., the bootstrapped data which are not used to ﬁt the trees) to construct
a variable importance criterion, which measures the prediction strength of
each feature (see, e.g., Genuer et al. ). As far as we are aware, there is to
date no systematic mathematical study of this criterion. It is our belief that
such a study would greatly beneﬁt from the sparsity point of view developed
in the present paper, but is unfortunately much beyond its scope. Lastly, it
would also be interesting to work out and extend our results to the context
of unsupervised learning of trees. A good route to follow with this respect is
given by the strategies outlined in Amit and Geman [3, Section 5.5].
A small simulation study
Even though the ﬁrst vocation of the present paper is theoretical, we oﬀer in
this short section some experimental results on synthetic data. Our aim is
not to provide a thorough practical study of the random forests method, but
rather to illustrate the main ideas of the article. As for now, we let U( d)
(respectively, N(0, 1)) be the uniform distribution over d (respectively,
the standard Gaussian distribution). Speciﬁcally, three models were tested:
1. [Sinus] For x ∈ d, the regression function takes the form
r(x) = 10 sin(10πx(1)).
We let Y = r(X) + ε and X ∼U( d) (d ≥1), with ε ∼N(0, 1).
2. [Friedman #1] This is a model proposed in Friedman . Here,
r(x) = 10 sin(πx(1)x(2)) + 20(x(3) −.05)2 + 10x(4) + 5x(5)
and Y = r(X) + ε, where X ∼U( d) (d ≥5) and ε ∼N(0, 1).
3. [Tree] In this example, we let Y = r(X) + ε, where X ∼U( d)
(d ≥5), ε ∼N(0, 1) and the function r has itself a tree structure.
This tree-type function, which is shown in Figure 2, involves only ﬁve
variables.
We note that, although the ambient dimension d may be large, the eﬀective
dimension of model 1 is S = 1, whereas model 2 and model 3 have S = 5.
In other words, S = {1} for model 1, whereas S = {1, . . . , 5} for model 2
and model 3. Observe also that, in our context, the model Tree should be
considered as a “no-bias” model, on which the random forests algorithm is
expected to perform well.
In a ﬁrst series of experiments, we let d = 100 and, for each of the three
models and diﬀerent values of the sample size n, we generated a learning set
of size n and ﬁtted a forest (10 000 trees) with mtry = d. For j = 1, . . . , d, the
ratio (number of times the j-th coordinate is split)/(total number of splits
X2< 0.2342
X1< 0.2463
X5< 0.2452
X3>=0.2234
X2< 0.2701
X5< 0.5985
Figure 2: The tree used as regression function in the model Tree.
over the forest) was evaluated, and the whole experiment was repeated 100
times. Figure 3, Figure 4 and Figure 5 report the resulting boxplots for each
of the ﬁrst twenty variables and diﬀerent values of n. These ﬁgures clearly
enlighten the fact that, as n grows, the probability of cuts does concentrate
on the informative variables only and support the assumption that ξnj →0
as n →∞for each j ∈S.
Next, in a second series of experiments, for each model, for diﬀerent values of
d and for sample sizes n ranging from 10 to 1000, we generated a learning set
of size n, a test set of size 50 000 and evaluated the mean squared error (MSE)
of the random forests (RF) method via the Monte Carlo approximation
[RF(test data #j) −r(test data #j)]2 .
All results were averaged over 100 data sets. The random forests algorithm
was performed with the parameter mtry automatically tuned by the R package RandomForests, 1000 random trees and the minimum node size set to
5 (which is the default value for regression). Besides, in order to compare
the “true” algorithm with the approximate model discussed in the present
document, an alternative method was also tested. This auxiliary algorithm
has characteristics which are identical to the original ones (same mtry, same
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Figure 3: Boxplots of the empirical probabilities of cuts for model Sinus (S =
number of random trees), with the notable diﬀerence that now the maximum
number of nodes is ﬁxed beforehand. For the sake of coherence, since the
minimum node size is set to 5 in the RandomForests package, the number of
terminal nodes in the custom algorithm was calibrated to ⌈n/5⌉. It must be
stressed that the essential diﬀerence between the standard random forests algorithm and the alternative one is that the number of cases in the ﬁnal leaves
is ﬁxed in the former, whereas the latter assumes a ﬁxed number of terminal nodes. In particular, in both algorithms, cuts are performed using the
actual sample, just as CART does. To keep things simple, no data-splitting
procedure has been incorporated in the modiﬁed version.
Figure 6, Figure 7 and Figure 8 illustrate the evolution of the MSE value
with respect to n and d, for each model and the two tested procedures.
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Figure 4: Boxplots of the empirical probabilities of cuts for model Friedman
#1 (S = {1, . . . , 5}).
First, we note that the overall performance of the alternative method is very
similar to the one of the original algorithm. This conﬁrms our idea that
the model discussed in the present paper is a good approximation of the
authentic Breiman’s forests. Next, we see that for a suﬃciently large n, the
capabilities of the forests are nearly independent of d, in accordance with the
idea that the (asymptotic) rate of convergence of the method should only
depend on the “true” dimensionality S (Theorem 2.2). Finally, as expected,
it is noteworthy that both algorithms perform well on the third model, which
has been precisely designed for a tree-structured predictor.
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Variable index
Proportion of cuts
Figure 5: Boxplots of the empirical probabilities of cuts for model Tree (S =
{1, . . . , 5}).
Throughout this section, we will make repeated use of the following two facts.
Fact 5.1 Let Knj(X, Θ) be the number of times the terminal node An(X, Θ)
is split on the j-th coordinate (j = 1, . . . , d).
Then, conditionally on X,
Knj(X, Θ) has binomial distribution with parameters ⌈log2 kn⌉and pnj (by
independence of X and Θ). Moreover, by construction,
Knj(X, Θ) = ⌈log2 kn⌉.
Sample size
Original RF
Alternative RF
Figure 6: Evolution of the MSE for model Sinus (S = 1).
Recall that we denote by Nn(X, Θ) the number of data points falling in the
same cell as X, i.e.,
Nn(X, Θ) =
1[Xi∈An(X,Θ)].
Let λ be the Lebesgue measure on d.
Fact 5.2 By construction,
λ (An(X, Θ)) = 2−⌈log2 kn⌉.
In particular, if X is uniformly distributed on d, then the distribution
of Nn(X, Θ) conditionally on X and Θ is binomial with parameters n and
2−⌈log2 kn⌉(by independence of the random variables X, X1, . . . , Xn, Θ).
If X is not uniformly distributed but has a probability density
f on d, then, conditionally on X and Θ, Nn(X, Θ) is binomial with
parameters n and P(X1 ∈An(X, Θ) | X, Θ). If f is bounded from above and
from below, this probability is of the order λ (An(X, Θ)) = 2−⌈log2 kn⌉, and
Sample size
Original RF
Alternative RF
Figure 7: Evolution of the MSE for model Friedman #1 (S = 5).
the whole approach can be carried out without diﬃculty. On the other hand,
for more general densities, the binomial probability depends on X, and this
makes the analysis signiﬁcantly harder.
Proof of Theorem 2.1
Observe ﬁrst that, by Jensen’s inequality,
E [¯rn(X) −r(X)]2 = E [EΘ [rn(X, Θ) −r(X)]]2
≤E [rn(X, Θ) −r(X)]2 .
A slight adaptation of Theorem 4.2 in Gy¨orﬁet al. shows that ¯rn is
consistent if both diam(An(X, Θ)) →0 in probability and Nn(X, Θ) →∞
in probability.
Let us ﬁrst prove that Nn(X, Θ) →∞in probability. To see this, consider
the random tree partition deﬁned by Θ, which has by construction exactly
2⌈log2 kn⌉rectangular cells, say A1, . . . , A2⌈log2 kn⌉. Let N1, . . . , N2⌈log2 kn⌉denote
the number of observations among X, X1, . . . , Xn falling in these 2⌈log2 kn⌉
cells, and let C = {X, X1, . . . , Xn} denote the set of positions of these n + 1
Sample size
Original RF
Alternative RF
Figure 8: Evolution of the MSE for model Tree (S = 5).
points. Since these points are independent and identically distributed, ﬁxing
the set C and Θ, the conditional probability that X falls in the ℓ-th cell equals
Nℓ/(n + 1). Thus, for every ﬁxed M ≥0,
P (Nn(X, Θ) < M) = E [P (Nn(X, Θ) < M | C, Θ)]
ℓ=1,...,2⌈log2 kn⌉:Nℓ<M
≤M2⌈log2 kn⌉
which converges to 0 by our assumption on kn.
It remains to show that diam(An(X, Θ)) →0 in probability. To this aim,
let Vnj(X, Θ) be the size of the j-th dimension of the rectangle containing
Clearly, it suﬃces to show that Vnj(X, Θ) →0 in probability for all
j = 1, . . . , d. To this end, note that
D= 2−Knj(X,Θ),
where, conditionally on X, Knj(X, Θ) has a binomial B(⌈log2 kn⌉, pnj) distribution, representing the number of times the box containing X is split along
the j-th coordinate (Fact 5.1). Thus
E [Vnj(X, Θ)] = E
2−Knj(X,Θ)
2−Knj(X,Θ) | X
= (1 −pnj/2)⌈log2 kn⌉,
which tends to 0 as pnj log kn →∞.
Proof of Proposition 2.1
Recall that
EΘ [Wni(X, Θ)] Yi,
Wni(X, Θ) = 1[Xi∈An(X,Θ)]
En = [Nn(X, Θ) ̸= 0] .
Similarly,
EΘ [Wni(X, Θ)] r(Xi).
E [¯rn(X) −˜rn(X)]2 = E
EΘ [Wni(X, Θ)] (Yi −r(Xi))
Θ [Wni(X, Θ)] (Yi −r(Xi))2
(the cross terms are 0 since E[Yi|Xi] = r(Xi))
Θ [Wni(X, Θ)] σ2(Xi)
Θ [Wni(X, Θ)]
Θ [Wn1(X, Θ)]
where we used a symmetry argument in the last equality. Observe now that
Θ [Wn1(X, Θ)] = EΘ [Wn1(X, Θ)] EΘ′ [Wn1(X, Θ′)]
(where Θ′ is distributed as, and independent of, Θ)
= EΘ,Θ′ [Wn1(X, Θ)Wn1(X, Θ′)]
1[X1∈An(X,Θ)]1[X1∈An(X,Θ′)]
Nn(X, Θ)Nn(X, Θ′)
1En(X,Θ)1En(X,Θ′)
1[X1∈An(X,Θ)∩An(X,Θ′)]
Nn(X, Θ)Nn(X, Θ′) 1En(X,Θ)1En(X,Θ′)
Consequently,
E [¯rn(X) −˜rn(X)]2 ≤nσ2E
1[X1∈An(X,Θ)∩An(X,Θ′)]
Nn(X, Θ)Nn(X, Θ′) 1En(X,Θ)1En(X,Θ′)
E [¯rn(X) −˜rn(X)]2
1[X1∈An(X,Θ)∩An(X,Θ′)]
i=2 1[Xi∈An(X,Θ)]
i=2 1[Xi∈An(X,Θ′)]
1[X1∈An(X,Θ)∩An(X,Θ′)]
i=2 1[Xi∈An(X,Θ)]
i=2 1[Xi∈An(X,Θ′)]
 | X, X1, Θ, Θ′
1[X1∈An(X,Θ)∩An(X,Θ′)]E
i=2 1[Xi∈An(X,Θ)]
i=2 1[Xi∈An(X,Θ′)]
 | X, X1, Θ, Θ′
1[X1∈An(X,Θ)∩An(X,Θ′)]E
i=2 1[Xi∈An(X,Θ)]
i=2 1[Xi∈An(X,Θ′)]
 | X, Θ, Θ′
by the independence of the random variables X, X1, . . . , Xn, Θ, Θ′. Using the
Cauchy-Schwarz inequality, the above conditional expectation can be upper
bounded by
i=2 1[Xi∈An(X,Θ)]
i=2 1[Xi∈An(X,Θ′)]
2 | X, Θ′
≤3 × 22⌈log2 kn⌉
(by Fact 5.2 and technical Lemma 5.1)
It follows that
E [¯rn(X) −˜rn(X)]2 ≤12σ2k2
1[X1∈An(X,Θ)∩An(X,Θ′)]
1[X1∈An(X,Θ)∩An(X,Θ′)]
E [PX1 (X1 ∈An(X, Θ) ∩An(X, Θ′))] .
Next, using the fact that X1 is uniformly distributed over d, we may
PX1 (X1 ∈An(X, Θ) ∩An(X, Θ′)) = λ (An(X, Θ) ∩An(X, Θ′))
λ (Anj(X, Θ) ∩Anj(X, Θ′)) ,
An(X, Θ) =
An(X, Θ′) =
Anj(X, Θ′).
On the other hand, we know (Fact 5.1) that, for all j = 1, . . . , d,
λ (Anj(X, Θ))
D= 2−Knj(X,Θ),
where, conditionally on X, Knj(X, Θ) has a binomial B(⌈log2 kn⌉, pnj) distribution and, similarly,
λ (Anj(X, Θ′))
where, conditionally on X, K′
nj(X, Θ′) is binomial B(⌈log2 kn⌉, pnj) and independent of Knj(X, Θ). In the rest of the proof, to lighten notation, we write
Knj and K′
nj instead of Knj(X, Θ) and K′
nj(X, Θ′), respectively. Clearly,
λ (Anj(X, Θ) ∩Anj(X, Θ′))≤2−max(Knj,K′
nj2−(Knj−K′
and, consequently,
λ (Anj(X, Θ) ∩Anj(X, Θ′)) ≤2−⌈log2 kn⌉
(since, by Fact 5.1, Pd
j=1 Knj = ⌈log2 kn⌉). Plugging this inequality into
(5.1) and applying H¨older’s inequality, we obtain
E [¯rn(X) −˜rn(X)]2 ≤12σ2kn
2−d(Knj−K′
Each term in the product may be bounded by technical Proposition 5.1, and
this leads to
E [¯rn(X) −˜rn(X)]2 ≤288σ2kn
16⌈log2 kn⌉pnj(1 −pnj)
16(log kn)pnj(1 −pnj)
Using the assumption on the form of the pnj, we ﬁnally conclude that
E [¯rn(X) −˜rn(X)]2 ≤Cσ2
n(log kn)S/2d,
(1 + ξnj)−1
Clearly, the sequence (ξn), which depends on the {(ξnj) : j ∈S} only, tends
to 0 as n tends to inﬁnity.
Proof of Proposition 2.2
We start with the decomposition
E [˜rn(X) −r(X)]2
EΘ [Wni(X, Θ)] (r(Xi) −r(X))
EΘ [Wni(X, Θ)] −1
Wni(X, Θ) (r(Xi) −r(X)) +
Wni(X, Θ) −1
Wni(X, Θ) (r(Xi) −r(X)) +
Wni(X, Θ) −1
where, in the last step, we used Jensen’s inequality. Consequently,
E [˜rn(X) −r(X)]2
Wni(X, Θ) (r(Xi) −r(X))
r(X) 1Ecn(X,Θ)
Wni(X, Θ) (r(Xi) −r(X))
x∈ d r2(x)
n(X, Θ)) .
Let us examine the ﬁrst term on the right-hand side of (5.2). Observe that,
by the Cauchy-Schwarz inequality,
Wni(X, Θ) (r(Xi) −r(X))
Wni(X, Θ) |r(Xi) −r(X)|
Wni(X, Θ) (r(Xi) −r(X))2
Wni(X, Θ) (r(Xi) −r(X))2
(since the weights are subprobability weights).
Thus, denoting by ∥X∥S the norm of X evaluated over the components in S,
Wni(X, Θ) (r(Xi) −r(X))
Wni(X, Θ) (r⋆(XiS) −r⋆(XS))2
Wni(X, Θ)∥Xi −X∥2
Wn1(X, Θ)∥X1 −X∥2
(by symmetry).
Wn1(X, Θ)∥X1 −X∥2
1[X1∈An(X,Θ)]
1[X1∈An(X,Θ)]
i=2 1[Xi∈An(X,Θ)]
1[X1∈An(X,Θ)]
i=2 1[Xi∈An(X,Θ)]
| X, X1, Θ
Wn1(X, Θ)∥X1 −X∥2
S1[X1∈An(X,Θ)]E
i=2 1[Xi∈An(X,Θ)]
| X, X1, Θ
S1[X1∈An(X,Θ)]E
i=2 1[Xi∈An(X,Θ)]
(by the independence of the random variables X, X1, . . . , Xn, Θ).
By Fact 5.2 and technical Lemma 5.1,
i=2 1[Xi∈An(X,Θ)]
≤2⌈log2 kn⌉
Consequently,
Wni(X, Θ) (r(Xi) −r(X))
S1[X1∈An(X,Θ)]
An(X, Θ) =
Anj(X, Θ),
Wni(X, Θ) (r(Xi) −r(X))
1 −X(j)|21[X1∈An(X,Θ)]
ρj(X, X1, Θ)EX(j)
1 −X(j)|21[X(j)
1 ∈Anj(X,Θ)]
where, in the last equality, we set
ρj(X, X1, Θ) =
t=1,...,d,t̸=j
1 ∈Ant(X,Θ)].
Therefore, using the fact that X1 is uniformly distributed over d,
Wni(X, Θ) (r(Xi) −r(X))
ρj(X, X1, Θ)λ3 (Anj(X, Θ))
Observing that
λ (Anj(X, Θ)) × E[X(t)
: t=1,...,d,t̸=j] [ρj(X, X1, Θ)]
= λ (An(X, Θ))
= 2−⌈log2 kn⌉
(Fact 5.2),
we are led to
Wni(X, Θ) (r(Xi) −r(X))
λ2 (Anj(X, Θ))
2−2Knj(X,Θ)
2−2Knj(X,Θ) | X
where, conditionally on X, Knj(X, Θ) has a binomial B(⌈log2 kn⌉, pnj) distribution (Fact 5.1). Consequently,
Wni(X, Θ) (r(Xi) −r(X))
(1 −0.75pnj)⌈log2 kn⌉
log 2pnj log kn
S log 2 (1+ξnj)
S log 2 (1+γn)
with γn = minj∈S ξnj.
To ﬁnish the proof, it remains to bound the second term on the right-hand
side of (5.2), which is easier. Just note that
n(X, Θ)) = P
1[Xi∈An(X,Θ)] = 0
1[Xi∈An(X,Θ)] = 0 | X, Θ
 1 −2−⌈log2 kn⌉n
(by Fact 5.2)
Putting all the pieces together, we ﬁnally conclude that
E [˜rn(X) −r(X)]2 ≤
S log 2 (1+γn)
x∈ d r2(x)
as desired.
Some technical results
The following result is an extension of Lemma 4.1 in Gy¨orﬁet al. . Its
proof is given here for the sake of completeness.
Lemma 5.1 Let Z be a binomial B(N, p) random variable, with p ∈(0, 1].
(N + 1)(N + 2)p2.
Proof of Lemma 5.1
To prove statement (i), we write
pj(1 −p)N−j
pj+1(1 −p)N−j
pj(1 −p)N+1−j
The second statement follows from the inequality
and the third one by observing that
pj(1 −p)N−j.
pj+1(1 −p)N−j
pj+1(1 −p)N−j
pj(1 −p)N+1−j
(N + 1)(N + 2)p2
Lemma 5.2 Let Z1 and Z2 be two independent binomial B(N, p) random
variables. Set, for all z ∈C⋆, ϕ(z) = E[zZ1−Z2]. Then
(i) For all z ∈C⋆,
p(1 −p)(z + z−1) + 1 −2p(1 −p)
(ii) For all j ∈N,
P(Z1 −Z2 = j) =
where Γ is the positively oriented unit circle.
(iii) For all d ≥1,
2−d(Z1−Z2)+
 −4Np(1 −p)t2
Proof of Lemma 5.2
Statement (i) is clear and (ii) is an immediate
consequence of Cauchy’s integral formula (Rudin ). To prove statement
(iii), write
2−d(Z1−Z2)+
2−dj P ((Z1 −Z2)+ = j)
2−dj P (Z1 −Z2 = j)
2−dj P (Z1 −Z2 = j)
(by statement (ii))
1 −2−de−iθ dθ
(by setting z = eiθ, θ ∈[−π, π])
[1 + 2p(1 −p)(cos θ −1)]N
2deiθ −1dθ
(by statement (i)).
Noting that
2deiθ −1 =
22d −2d+1 cos θ + 1,
2−d(Z1−Z2)+
[1 + 2p(1 −p)(cos θ −1)]N
22d −2d+1 cos θ + 1dθ.
22d −2d+1 cos θ + 1 ≤
2−d(Z1−Z2)+
≤2d−1(2d + 1)
[1 + 2p(1 −p)(cos θ −1)]N dθ
= 2d(2d + 1)
[1 + 2p(1 −p)(cos θ −1)]N dθ
= 2d(2d + 1)
1 −4p(1 −p) sin2(θ/2)
(cos θ −1 = −2 sin2(θ/2))
= 2d+1(2d + 1)
1 −4p(1 −p) sin2 θ
Using the elementary inequality (1−z)N ≤e−Nz for z ∈ and the change
of variable
t = tan(θ/2),
we ﬁnally obtain
2−d(Z1−Z2)+
≤2d+2(2d + 1)
−16Np(1 −p)t2
 −4Np(1 −p)t2
Cd = 2d+2(2d + 1)
π(2d −1)2 .
The conclusion follows by observing that Cd ≤24/π for all d ≥1.
Evaluating the integral in statement (iii) of Lemma 5.2 leads to the following
proposition:
Proposition 5.1 Let Z1 and Z2 be two independent binomial B(N, p) random variables, with p ∈(0, 1). Then, for all d ≥1,
2−d(Z1−Z2)+
16Np(1 −p)
Acknowledgments.
I greatly thank the Action Editor and two referees for
valuable comments and insightful suggestions, which lead to a substantial
improvement of the paper. I would also like to thank my colleague Jean-
Patrick Baudry for his precious help on the simulation section.