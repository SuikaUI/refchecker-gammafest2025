Towards Efﬁcient Tensor Decomposition-Based DNN Model Compression with
Optimization Framework
Miao Yin1, Yang Sui1, Siyu Liao2: and Bo Yuan1
1Department of ECE, Rutgers University, 2Amazon
{miao.yin, yang.sui}@rutgers.edu, , 
Advanced tensor decomposition, such as tensor train
(TT) and tensor ring (TR), has been widely studied for deep
neural network (DNN) model compression, especially for
recurrent neural networks (RNNs). However, compressing
convolutional neural networks (CNNs) using TT/TR always
suffers signiﬁcant accuracy loss. In this paper, we propose
a systematic framework for tensor decomposition-based
model compression using Alternating Direction Method of
Multipliers (ADMM). By formulating TT decompositionbased model compression to an optimization problem with
constraints on tensor ranks, we leverage ADMM technique
to systemically solve this optimization problem in an iterative way. During this procedure, the entire DNN model is
trained in the original structure instead of TT format, but
gradually enjoys the desired low tensor rank characteristics. We then decompose this uncompressed model to TT
format, and ﬁne-tune it to ﬁnally obtain a high-accuracy TTformat DNN model. Our framework is very general, and it
works for both CNNs and RNNs, and can be easily modiﬁed
to ﬁt other tensor decomposition approaches. We evaluate
our proposed framework on different DNN models for image
classiﬁcation and video recognition tasks. Experimental results show that our ADMM-based TT-format models demonstrate very high compression performance with high accuracy. Notably, on CIFAR-100, with 2.3ˆ and 2.4ˆ compression ratios, our models have 1.96% and 2.21% higher
top-1 accuracy than the original ResNet-20 and ResNet-32,
respectively. For compressing ResNet-18 on ImageNet, our
model achieves 2.47ˆ FLOPs reduction without accuracy
1. Introduction
Deep Neural Network (DNNs) have already obtained
widespread applications in many computer vision tasks,
such as image classiﬁcation , video recognition
:This work was done when the author was with Rutgers University.
 , objective detection , and image caption
 . Despite these unprecedented success and popularity, executing DNNs on the edge devices is still very challenging. For most embedded and Internet-of-Things (IoT)
systems, the sizes of many state-of-the-art DNN models are
too large, thereby causing high storage and computational
demands and severely hindering the practical deployment
of DNNs. To mitigate this problem, to date many model
compression approaches, such as pruning 
and quantization , have been proposed to reduce
the sizes of DNN models with limited impact on accuracy.
Tensor Decomposition for Model Compression. Recently, tensor decomposition, as a mathematical tool that
explores the low tensor rank characteristics of the largescale tensor data, have become a very attractive DNN model
compression technique. Different from other model compression methods, tensor decomposition, uniquely, can provide ultra-high compression ratio, especially for recurrent
neural network (RNN) models. As reported in ,
the advanced tensor decomposition approaches, such as tensor train (TT) and tensor ring (TR), can bring more than
1,000ˆ parameter reduction to the input-to-hidden layers
of RNN models, and meanwhile the corresponding classiﬁcation accuracy in the video recognition task can be
even signiﬁcantly improved.
Motivated by such strong
compression performance, many prior research works have
been conducted on tensor decomposition-based DNN models . In addition, to fully utilize the beneﬁts provided by those models, several TT-format DNN hardware
accelerators have been developed and implemented in different chip formats, such as digital CMOS ASIC , memristor ASIC and IoT board .
Limitations of the State of the Art. Despite its promising potentials, the performance of tensor decomposition is
not satisﬁed enough as a mature model compression approach. Currently all the reported success of tensor decomposition are narrowly limited to compressing RNN models in video recognition tasks.
For compressing convolutional neural network (CNN) in the image classiﬁcation
task, which are the most commonly used and representa-
 
tive setting for evaluating model compression performance,
all the state-of-the-art tensor decomposition approaches, including TT and TR, suffer very signiﬁcant accuracy loss.
For instance, even the very recent progress using TR
still has 1.0% accuracy loss when the compression ratio is
only 2.7ˆ for ResNet-32 model on CIFAR-10 dataset. For
the larger compression ratio as 5.8ˆ, the accuracy loss further increases to 1.9% .
Why Limited Performance? The above limitation of
tensor decomposition is mainly due to the unique challenges
involved in training the tensor decomposed DNN models.
In general, there are two ways to use tensor decomposition
to obtain a compressed model: 1) Train from scratch in the
decomposed format; and 2) Decompose a pre-trained uncompressed model and then retrain.
In the former case,
when the required tensor decomposition-based, e.g. TTformat model, is directly trained from scratch, because the
structure of the models are already pre-set to low tensor
rank format before the training, the corresponding model
capacity is typically limited as compared to the full-rank
structure, thereby causing the training process being very
sensitive to initialization and more challenging to achieve
high accuracy. In the later scenario, though the pre-trained
uncompressed model provides good initialization position,
the straightforwardly decomposing full-rank uncompressed
model into low tensor rank format causes inevitable and
non-negligible approximation error, which is still very difﬁcult to be recovered even after long-time re-training period.
Besides, no matter which training strategy is adopted, tensor decomposition always brings linear increase in network
depth, which implies training the tensor decompositionformat DNNs are typically more prone to gradient vanishing
problem and hence being difﬁcult to be trained well.
Technical Preview and Contributions. To overcome
the current limitations of tensor decomposition and fully
unlock its potentials for model compression, in this paper we propose a systematic framework for tensor decomposition-based model compression using alternating direction method of multipliers (ADMM). By formulating TT
decomposition-based model compression to an optimization problem with constraints on tensor ranks, we leverage
ADMM technique to systemically solve this optimization problem in an iterative way.
During this procedure
the entire DNN model is trained in the original structure
instead of TT format, but gradually enjoys the desired low
tensor rank characteristics. We then decompose this uncompressed model to TT format, and ﬁne-tune it to ﬁnally obtain a high-accuracy TT-format DNN model. In overall, the
contributions of this paper are summarized as follows:
• We propose a systematic framework to formulate and
solve the tensor decomposition-based model compression problem. With formulating this problem to a constrained non-convex optimization problem, our framework gradually restricts the DNN model to the target
tensor ranks without explicitly training on the TT format, thereby maintaining the model capacity as well as
avoiding huge approximation error and increased network depth.
• We propose to use ADMM to efﬁciently solve this reformulated optimization problem via separately solving two sub-problems: one is to directly optimize the
loss function with a regularization of the DNN by
stochastic gradient descent, and the other is to use the
introduced projection to constraint the tensor ranks analytically.
• We evaluate our proposed framework on different
DNN models for image classiﬁcation and video recognition tasks.
Experimental results show that our
ADMM-based TT-format models demonstrate very
high compression performance with high accuracy.
Notably, on CIFAR-100, with 2.3ˆ and 2.4ˆ compression ratios, our models have 1.96% and 2.21%
higher top-1 accuracy than the original ResNet-20 and
ResNet-32, respectively. For compressing ResNet-18
on ImageNet, our model achieves 2.47ˆ FLOPs reduction with no accuracy loss.
2. Related Work on DNN Model Compression
Sparsiﬁcation. Sparsiﬁcation is the most popular DNN
compression approach. Different levels of network structure can be sparse, such as weight , ﬁlter 
and channel . To obtain the sparsity, a DNN model
can be either pruned or trained with sparsity-aware
regularization . Also, the introduced sparsity can be
either structured or unstructured. Unstructured sparse models enjoy high accuracy and compression ratio, but
brings irregular memory access and imbalanced workload
problems to the underlying hardware platform. Structured sparse models are more hardware friendly; however, their compression ratio and accuracy are typically inferior to the unstructured counterparts.
Quantization.
Quantization is another
widely adopt model compression approach.
By reducing the number of bits for weight representation, quantization enables immediate reduction in DNN model size.
The most aggressive quantization scheme brings binary networks , which only use 1-bit weight parameters.
Quantization is inherently hardware friendly, and have become a standard adopted model compression method for
most DNN hardware accelerators .
quantization is limited by the maximum compression ratio
that can be offered (up to 32ˆ).
Tensor Decomposition. Rooted in tensor theory, tensor decomposition approach factorizes weight tensors into
smaller tensors to reduce model sizes.
In , matrixoriented singular value decomposition (SVD), as the lowdimensional instance of tensor decomposition, is used to
perform model compression. However, using this method,
or other classical high-dimensional tensor decomposition
methods, such as Tucker and CP decomposition ,
causes signiﬁcant accuracy loss (ą 0.5%) with limited compression ratios .
Starting from , advanced tensor decomposition approaches, such as tensor
train (TT) and tensor ring (TR) decomposition, have become the more popular options. These methods have very
attractive advantages – the compression ratio can be very
high (e.g.
ą 1,000ˆ) because of their unique mathematical property.
Such beneﬁts have been demonstrated
on RNN compression in video recognition tasks. As reported in , 17,560ˆ to 34,203ˆ compression ratios can be achieved by using TT or TR decomposition on
the input-to-hidden layer of RNN models for video recognition. However, TT and TR approaches do not perform
well on CNN models. For instance, even the very recent
progress still suffers 1.0% accuracy loss with 2.7ˆ
compression ratio, or even 1.9% accuracy loss with 5.8ˆ
compression ratio, both for ResNet-32 model on CIFAR-
10 dataset. From the perspective of practical deployment,
such non-negligible accuracy degradation severely hinders
the widespread adoption of tensor decomposition for many
CNN-involved model compression scenarios.
3. Background and Preliminaries
3.1. Notation
X P Rn1ˆn2ˆ¨¨¨ˆnd, X P Rn1ˆn2, and x P Rn1 represent d-order tensor, matrix and vector, respectively. Also,
X pi1,¨¨¨ ,idq and Xpi,jq denote the single entry of tensor X
and matrix X, respectively.
3.2. Tensor Train (TT) Decomposition
Given a tensor A P Rn1ˆn2ˆ¨¨¨ˆnd, it can be decomposed to a sort of 3-order tensors via Tensor Train Decomposition (TTD) as follows:
Api1,i2,¨¨¨ ,idq “ G1p:,i1,:qG2p:,i2,:q ¨ ¨ ¨ Gdp:,id,:q
r0,r1,¨¨¨rd
α0,α1¨¨¨αd
G1pα0,i1,α1qG2pα1,i2,α2q ¨ ¨ ¨
Gdpαd´1,id,αdq,
where Gk P Rrk´1ˆnkˆrk are called TT-cores for k “
1, 2, ¨ ¨ ¨ , d, and r “ rr0, r1, ¨ ¨ ¨ , rds, r0 “ rd “ 1 are
called TT-ranks, which determine the storage complexity of
TT-format tensor. An example is demonstrated in Figure 1.
3.3. Tensor Train (TT)-format DNN
TT Fully-Connected Layer. Consider a simple fullyconnected layer with weight matrix W P RMˆN and in-
Figure 1: Illustration of Tensor Train Decomposition (TTD)
for a 4-order tensor. r0 and r4 are always equal to 1.
put x P RN, where M “ śd
k“1 mk and N “ śd
the output y P RM is obtained by y “ W x. In order to
transform this standard layer to TT fully-connected (TT-FC)
layer, we ﬁrst tensorize the weight matrix W to a weight
tensor W P Rpm1ˆn1qˆ¨¨¨ˆpmdˆndq by reshaping and order
transposing. Then W can be decomposed to TT-format:
Wppi1,j1q,¨¨¨ ,pid,jdqq “ G1p:,i1,j1,:q ¨ ¨ ¨ Gdp:,id,jd,:q.
Here, each TT-core Gk P Rrk´1ˆmkˆnkˆrk is a 4-order
tensor, which is one dimension more than the standard one
since the output and input dimensions of W are divided
separately. Hence, the forward progagation on the TT-FC
layer can be expressed in tensor format as follows:
Ypi1,¨¨¨ ,idq “
j1,¨¨¨ ,jd
G1p:,i1,j1,:q ¨ ¨ ¨ Gdp:,id,jd,:qX pj1,¨¨¨ ,jdq,
where X P Rm1ˆ¨¨¨ˆmd and Y P Rn1ˆ¨¨¨ˆnd are the tensorized input and output corresponding to x and y, respectively. The details about TT-FC layer is introduced in .
TT Convolutional Layer. For a conventional convolutional layer, its forward computation is to perform convolution between a 3-order input tensor rX P RW ˆHˆN and
a 4-order weight tensor Ă
W P RKˆKˆMˆN to produce the
3-order output tensor rY P RpW ´K`1qˆpH´K`1qˆM . In
a TT convolutional (TT-CONV) layer, the input tensor rX
is reshaped to a tensor X P RW ˆHˆn1ˆ¨¨¨ˆnd, while the
weight tensor Ă
W is reshaped and transposed to a tensor
W P RpKˆKqˆpm1ˆn1qˆ¨¨¨ˆpmdˆndq and then decomposed
to TT-format:
Wppk1,k2q,pi1,j1q,¨¨¨ ,pid,jdqq “G0pk1,k2qG1p:,i1,j1,:q ¨ ¨ ¨
Gdp:,id,jd,:q,
where M “ śd
k“1 mk and N “ śd
k“1 nk. Similar with
TT-FC layer, here Gk P Rrk´1ˆmkˆnkˆrk is a 4-order
tensor except G0 P RKˆK. Then the new output tensor
Y P RpW ´K`1qˆpH´K`1qˆm1ˆ¨¨¨ˆmd is obtained by
Ypw,h,i1,¨¨¨ ,idq “
j1,¨¨¨ ,jd
X pk1`w´1,k2`h´1,j1,¨¨¨ ,jdq
G0pk1,k2qG1p:,i1,j1,:q ¨ ¨ ¨ Gdp:,id,jd,:q.
The detailed description of TT-CONV layer is in .
Training on TT-format DNN. As TT-FC layer, TT-
CONV layer and the corresponding forward propagation
schemes are formulated, standard stochastic gradient descent (SGD) algorithm can be used to update the TT-cores
Train ADMM-
Regularized
Uncompressed Model
Fine-Tune TT-Format
optimizing
subproblem (11)
optimizing
subproblem (12)
to TT-cores
Uncompressed Model
with Randomly
Initialized
TT-Format Model
Figure 2: Procedure of the proposed compression framework using ADMM for a TT-format DNN model.
with the rank set r, which determines the target compression ratio. The initialization of the TT-cores can be either
randomly set or obtained from directly TT-decomposing a
pre-trained uncompressed model.
4. Systematic Compression Framework
Analysis on Existing TT-format DNN Training. As
mentioned in the last paragraph, currently a TT-format
DNN is either 1) trained from with randomly initialized
tensor cores; or 2) trained from a direct decomposition of
pre-trained model. For the ﬁrst strategy, it does not utilize
any information related to the high-accuracy uncompressed
model; while other model compression methods, e.g. pruning and knowledge distillation, have shown that proper utilization of the pre-trained models are very critical for DNN
compression. For the second strategy, though the knowledge of the pre-trained model is indeed utilized, because
the pre-trained model generally lacks low TT-rank property,
after direct low-rank tensor decomposition the approximation error is too signiﬁcant to be properly recovered even
using long-time re-training. Such inherent limitations of the
existing training strategies, consequently, cause signiﬁcant
accuracy loss for the compressed TT-format DNN models.
Our Key Idea. We believe the key to overcome these
limitations is to maximally retain the knowledge contained
in the uncompressed model, or in other words, minimize the
approximation error after tensor decomposition with given
target tensor ranks.
To achieve that, we propose to formulate an optimization problem to minimize the loss function of the uncompressed model with low tensor rank constraints. With proper advanced optimization technique (e.g.
ADMM)-regularized training procedure, the uncompressed
DNN models can gradually exhibit low tensor rank properties. After the ADMM-regularized training phase, the approximation error brought by the explicit low-rank tensor
decomposition becomes negligible, and can be easily recovered by the SGD-based ﬁne-tuning. Figure 2 shows the
main steps of our proposed overall framework.
4.1. Problem Formulation
As mentioned above, the ﬁrst phase of our framework is
to gradually impose low tensor rank characteristics onto a
high-accuracy uncompressed DNN model. Mathematically,
this goal can be formulated as a optimization problem to
minimize the loss function of the object model with constraints on TT-ranks of each layer (convolutional or fullyconnected):
s.t. rankpWq ď r˚,
where ℓis the loss function of the DNN , rankp¨q is a
function that returns the TT-ranks r “ rr0, ¨ ¨ ¨ , rds of the
weight tensor cores, and r˚ “ rr˚
0 , ¨ ¨ ¨ , r˚
ds are the desired TT-ranks for the layer. To simplify the notation, here
r ď r˚ means ri ď r˚
i , i “ 0, ¨ ¨ ¨ , d, for each ri in r.
4.2. Optimization Using ADMM
Obviously, solving the problem (6) is generally difﬁcult
via using normal optimization algorithms since rankp¨q is
non-differentiable.
To overcome this challenge, we ﬁrst
rewrite it as
s.t. W P S,
where S “ tW | rankpWq ď r˚u. Hence, the objective
form (7) is a classic non-convex optimization problem with
constraints, which can be properly solved by ADMM .
Speciﬁcally, we can ﬁrst introduce an auxiliary variable Z
and an indicator function gp¨q of S, i.e.
otherwise.
And then the problem (7) is equivalent to the following
W,Z ℓpWq ` gpZq,
s.t. W “ Z.
To ensure convergence without assumptions like strict convexity or ﬁniteness of ℓ, instead of Lagrangian, the corresponding augmented Lagrangian in the scaled dual form of
the above problem is given by
LρpW, Z, Uq “ℓpWq ` gpZq
2 }W ´ Z ` U}2
where U is the dual multiplier, and ρ ą 0 is the penalty
parameter. Thus, the iterative ADMM scheme can be explicitly performed as
Wt`1 “ argmin
W, Zt, Ut˘
Zt`1 “ argmin
Wt`1, Z, Ut˘
Ut`1 “ Ut ` Wt`1 ´ Zt`1,
where t is the iterative step. Now, the original problem (9)
is separated to two subproblems (11) and (12), which can
be solved individually. Next, we introduce the detailed solution of each subproblem.
W-subproblem. The W-subproblem (11) can be reformulated as follows:
››W ´ Zt ` Ut››2
where the ﬁrst term is the loss function, e.g. cross-entropy
loss in classiﬁcation tasks, of the DNN model, and the second term is the L2 regularization. This subproblem can be
directly solved by SGD since both these two terms are differentiable. Correspondingly, the partial derivative of (14)
with respect to W is calculated as
BLρpW, Zt, Utq
` ρpW ´ Zt ` Utq. (15)
And hence W can be updated by
Wt`1 “ Wt ´ η BLρpW, Zt, Utq
where η is the learning rate.
Z-subproblem. To solve Z-subproblem (12), we ﬁrst
explicitly formulate it as follows:
››Wt`1 ´ Z ` Ut››2
where the indicator function gp¨q of the non-convex set S
is non-differentiable. Then, according to , in this format
updating Z can be performed as:
Zt`1 “ ΠSpWt`1 ` Utq,
where ΠSp¨q is the projection of singular values onto S, by
which the TT-ranks of pWt`1 ` Utq are truncated to target
ranks r˚. Algorithm 1 describes the speciﬁc procedure of
this projection in the TT-format scenario.
In each ADMM iteration, upon the update of W and Z,
the dual multiplier U is updated by (13). In overall, to solve
(9), the entire ADMM-regularized training procedure is performed in an iterative way until convergence or reaching the
pre-set maximum iteration number. The overall procedure
is summarized in Algorithm 2.
4.3. Fine-Tuning
After ADMM-regularized training, we ﬁrst decompose
the trained uncompressed DNN model into TT format. Here
the decomposition is performed with the target TT-ranks r˚
Algorithm 1 TT-SVD-based Projection for Solving (17)
Input: d-order tensor A P Rn1ˆ¨¨¨ˆnd, target TT-ranks r˚.
Output: ˆA “ ΠSpAq.
1: Temporary tensor T “ A;
2: for k “ 1 to d ´ 1 do
T :“ reshapepT , rr˚
k´1nk, ´1sq;
Compute matrix SVD: U, S, V :“ SVDpT q;
U :“ Up1:r˚
S :“ Sp1:r˚
V :“ Vp:,1:r˚
Gk :“ reshapepU, rr˚
k´1, nk, r˚
T :“ SV T ;
10: T :“ G1;
11: for k “ 1 to d ´ 1 do
T1 :“ reshapepT , r´1, r˚
T2 :“ reshapepGk`1, rr˚
T :“ T1T2;
15: ˆA “ reshapepT , rn1, ¨ ¨ ¨ , ndsq.
Algorithm 2 ADMM-Regularized Training Procedure
Input: Weight tensor W, target TT-ranks r˚, penalty parameter ρ, feasibility tolerance ϵ, maximum iterations
Output: Optimized W.
1: Randomly initialize W;
2: Z :“ W, U :“ 0;
3: while }Wt ´ Zt} ą ϵ and t ď T do
Updating W via (16);
Updating Z via (18) (Algorithm 1);
Updating U via (13);
for tensor cores. Because the ADMM optimization procedure has already imposed the desired low TT-rank structure to the uncompressed model, such direction decomposition, unlike their counterpart in the existing TT-format
DNN training, will not bring signiﬁcant approximation error (More details will be analyzed in Section 5.1). Then,
the decomposed TT-format model is ﬁne-tuned using standard SGD. Notice that in the ﬁne-tuning phase the loss function is ℓptGiuq without other regularization term introduced
by ADMM. Typically this ﬁne-tuning phase is very fast
with requiring only a few iterations. This is because the
decomposed TT model at the starting point of this phase
already has very closed accuracy to the original uncompressed model.
5. Experiments
To demonstrate the effectiveness and generality of the
proposed compression framework, we evaluate different
DNN models in different computer vision tasks. For image classiﬁcation tasks, we evaluate multiple CNN models
on MNIST, CIFAR-10, CIFAR-100 and ImageNet datasets
 .
For video classiﬁcation tasks, we evaluate
different LSTM models on UCF11 and HMDB51 datasets
 . We follow the same rank selection scheme adopted
in prior works – set ranks to satisfy the need of the targeted
compression ratio. To simplify selection procedure, most of
the ranks in the same layer are set to equal.
5.1. Convergence and Sensitivity Analysis
As shown in (10), ρ is the additional hyperparameter
introduced in the ADMM-regularized training phase. To
study the effect of ρ on the performance as well as facilitating hyperparameter selection, we study the convergence and
sensitivity of the ADMM-regularized training for ResNet-
32 model with different ρ settings on CIFAR10 dataset.
Convergence. Figure 3a shows the loss curves in the
ADMM-regularized training phase. It is seen that different curves with very different ρ values (e.g. 0.001 vs 0.02),
exhibit very similar convergence speed. This phenomenon
therefore demonstrates that ρ has little impact on the convergence of ADMM-regularized training.
Sensitivity. Considering the similar convergence behavior does not necessarily mean that different ρ would bring
the similar accuracy, we then analyze the performance sensitivity of ADMM-regularized training with respect to ρ.
Notice that ideally after ADMM-regularized training, W,
though in the uncompressed format, should exhibit strong
low TT-rank characteristics and meanwhile enjoy high accuracy. Once W meets such two criteria simultaneously,
that means TT-cores tGiu, whose initialization is decomposed from W, will already have high accuracy even before
ﬁne-tuning.
To examine the required low TT-rank behavior of W, we
observe }W ´Z}2
F , which measures the similarity between
W and Z, in the ADMM-regularized training (see Figure
3b). Since according to (18) Z is always updated with low
TT-rank constraints, the curves shown in Figure 3b reveal
that W indeed quickly exhibits low TT-rank characteristics during the training, except when ρ “ 0.001. This phenomenon implies that to ensure the weight tensors are well
regularized to the target TT-ranks by ADMM, ρ should not
be too small (e.g. less than 0.001). On the other hand, Figure 3c shows the test accuracy of W as training progresses.
Here it is seen that smaller ρ tends to bring better performance. Based on these observations, ρ “ 0.005 can be an
appropriate choice to let the trained W meet the aforementioned two criteria.
5.2. Image Classiﬁcation
MNIST. Table 1 shows the experimental results of
LeNet-5 model on MNIST dataset. We compare our
Uncompressed
Standard TR 
PSTRN-M 
PSTRN-S 
Standard TT 
Table 1: LeNet-5 on MNIST dataset using different TT/TRformat compression approaches.
Uncompressed
Standard TR 
PSTRN-M 
PSTRN-S 
Standard TT 
Uncompressed
Standard TR 
PSTRN-M 
PSTRN-S 
Standard TT 
Table 2: ResNet-20 and ResNet-32 on CIFAR-10 dataset
using different TT/TR-format compression approaches.
ADMM-based TT-format model with the uncompressed
model as well as the state-of-the-art TT/TR-format works.
It is seen that our ADMM-based compression can achieve
the highest compression ratio and the best accuracy.
CIFAR-10. Table 2 compares our ADMM-based TTformat ResNet-20 and ResNet-32 models with the stateof-the-art TT/TR-format works on CIFAR-10 dataset. For
ResNet-20, it is seen that standard training on TT/TRformat models causes severe accuracy loss. Even for the
state-of-the-art design using some advanced techniques,
such as heuristic rank selection (PSTRN-M/S) and reinforcement learning (TR-RL), the performance degradation
is still huge, especially with high compression ratio 6.8ˆ.
On the other side, with the same high compression ratio our
ADMM-based TT-format model has only 0.22% accuracy
drop, which means 2.53% higher than the state-of-the-art
PSTRN-M. Furthermore, with moderate compression ratio
4.5ˆ our method can even outperform the uncompressed
(a) Training loss.
(b) }W ´ Z}2
(c) Top-1 test accuracy.
Figure 3: Training loss, Frobenius norm and test accuracy in ADMM-regularized training procedure with different ρ.
Uncompressed
Standard TR 
PSTRN-M 
PSTRN-S 
Standard TT 
Uncompressed
Standard TR 
PSTRN-M 
PSTRN-S 
Standard TT 
Table 3: ResNet-20 and ResNet-32 on CIFAR-100 dataset
using different TT/TR-format compression approaches.
model with 0.22% accuracy increase.
For ResNet-32, again, standard training on compressed
models using TT or TR decomposition causes huge performance degradation. The state-of-the-art PSTRN-S/M indeed brings performance improvement, but the test accuracy is still not satisﬁed. Instead, our highly compressed
(5.8ˆ) TT-format model only has 0.53% accuracy loss,
which means it has 1.36% higher accuracy than PSTRN-M
with the same compression ratio. More importantly, when
compression ratio is relaxed to 4.8ˆ, our ADMM-based TTformat model achieves 92.87%, which is even 0.38% higher
than the uncompressed model.
CIFAR-100. Table 3 shows the experimental results on
CIFAR-100 dataset. Again, our ADMM-based TT-format
model outperforms the state-of-the-art work. For ResNet-
20, with even higher compression ratio (Our 5.6ˆ vs 4.7ˆ
in PSTRN-M), our model achieves 1.3% accuracy increase.
With 2.3ˆ compression ratio, our model achieves 67.36%
Top-1 accuracy, which is even 1.96% higher than the uncompressed model. For ResNet-32, with the same 5.2ˆ
Uncompressed
Standard TR 
TRP+Nu 
Standard TT 
Table 4: ResNet-18 on ImageNet dataset using compression
approaches. We do not list PSTRN-M/S since does
not report results on ImageNet. Also the listed pruning and
SVD works do not report compression ratios in their papers. The uncompressed baseline model is from Torchvision.
Note that the reported Top-5 accuracy of 
in this table are obtained from pruning the baselines with
higher accuracy.
compression ratio, our approach brings 0.4% accuracy increase over the state-of-the-art PSTRN-M. With the same
2.4ˆ compression ratio, our approach has 2.26% higher accuracy than PSTRN-S. Our model even outperforms the uncompressed model with 2.21% accuracy increase.
ImageNet. Table 4 shows the results of compressing
ResNet-18 on ImageNet dataset. Because no prior TT/TR
compression works report results on this dataset, we use
standard TT and TR-based training in for comparison. We also compare our approach with other compression
methods, including pruning and matrix SVD. Since these
works report FLOPs reduction instead of compression ratio, we also report FLOPs reduction brought by tensor decomposition. It is seen that with the similar FLOPs reduction ratio (4.62ˆ), our ADMM-based TT-format model has
1.83% and 1.18% higher accuracy than standard TT and
TR, respectively.
Compared with other compression approaches with non-negligible accuracy loss, our ADMM-
Uncompressed
TR-LSTM 
TT-LSTM 
Table 5: LSTM on UCF11 dataset using different TT/TRformat compression approaches.
based TT-format models achieve much better accuracy with
more FLOPs reduction. In particular, with 2.47ˆ FLOPs
reduction, our model has the same accuracy as the uncompressed baseline model.
5.3. Video Recognition
UCF11. In this experiment, we use the same uncompressed LSTM model, data pre-processing and experimental settings adopted in .
To be consistent with
 , only the ultra-large input-to-hidden layer is compressed for fair comparison. Table 5 compares our ADMMbased TT-format LSTM with the uncompressed model and
the existing TT-LSTM and TR-LSTM . Note that
 does not report the performance of PSTRN-M/S on
UCF11 dataset.
From Table 5 , it is seen that both TT-LSTM and TR-
LSTM provide remarkable performance improvement and
excellent compression ratio. As analyzed in , such huge
improvement over the uncompressed model mainly comes
from the excellent feature extraction capability of TT/TRformat LSTM models on the ultra-high-dimensional inputs.
Compared with these existing works, our ADMM-based
TT-format model achieves even better performance. With
fewer parameters, our method brings 2.1% higher top-1 accuracy than the state-of-the-art TR-LSTM.
HMDB51. To be consistent with the setting adopted in
 , in this experiment we use the same Inception-V3
as the front-end pre-trained CNN model, and the same backend uncompressed LSTM model. For fair comparison, we
follow the compression strategy adopted in as only
compressing the ultra-large input-to-hidden layer of LSTM.
Table 6 summarizes the experimental results. It is seen
that comparing with the state-of-the-art TT/TR-format designs, our ADMM-based TT-format model shows excellent
performance. With the highest compression ratio (84.0ˆ),
our model achieves 64.09% top-1 accuracy.
with the state-of-the-art TR-LSTM, our model brings 3.35ˆ
more compression ratio with additional 0.29% accuracy increase.
5.4. Discussion on Tensor Format and Generality
Why Choosing TT-format.
Recently several stateof-the-art tensor decomposition-based compression works
 report that TT decomposition is inferior to
Uncompressed
TR-LSTM 
PSTRN-M 
PSTRN-S 
TT-LSTM 
LSTM on HMDB51 dataset using different
TT/TR-format compression approaches.
other advanced approach (e.g. TR) on DNN compression,
in terms of compression ratio and test accuracy. To fully
demonstrate the excellent effectiveness of our approach, in
this paper we choose TT, the tensor format that is believed
to be not the best for model compression, and adapt the
ADMM-regularized compression framework to TT-format.
As presented in the experimental results, all the ADMMbased TT-format models consistently outperform the existing TT/TR-format models with higher accuracy and higher
compression ratio over different datasets, thereby comprehensively demonstrating the huge beneﬁts brought by our
proposed framework.
Generality of Our Framework. Although in this paper our focus is to compress TT-format DNN models, because ADMM is a general optimization technique, our proposed framework is very general and can be easily applied
for model compression using other tensor decomposition
approaches, such as Tensor Ring (TR), Block-term (BT),
Tucker etc. To adapt to other tensor decomposition scenario, the main modiﬁcation on our proposed framework is
to modify the Euclidean projection (Algorithm 1) to make
the truncating methods being compatible to the corresponding tensor decomposition methods.
6. Conclusion
In this paper, we present a systematic compression
framework for tensor-format DNNs using ADMM. Under the framework, the tensor decomposition-based DNN
model compression is formulated to a nonconvex optimization problem with constraints on target tensor ranks. By
performing ADMM to solve this problem, a uncompressed
but low tensor-rank model can be obtained, thereby ﬁnally
bringing the decomposed high-accuracy TT-format model.
Extensive experiments for image and video classiﬁcation
show that our ADMM-based TT-format models consistently
outperform the state-of-the-art works in terms of compression ratio and test accuracy.
Acknowledgements
This work was partially supported by National Science
Foundation under Grant CCF-1955909.