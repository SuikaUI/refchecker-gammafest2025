META-DES: A Dynamic Ensemble Selection Framework using
Meta-Learning
Rafael M. O. Cruza,∗, Robert Sabourina, George D. C. Cavalcantib, Tsang Ing Renb
aLIVIA, École de Technologie Supérieure, University of Quebec, Montreal, Que., Canada - www.livia.etsmtl.ca
bCentro de Informática, Universidade Federal de Pernambuco, Recife, PE, Brazil - www.cin.ufpe.br/∼viisar
Dynamic ensemble selection systems work by estimating the level of competence of each
classiﬁer from a pool of classiﬁers. Only the most competent ones are selected to classify a
given test sample. This is achieved by deﬁning a criterion to measure the level of competence
of a base classiﬁer, such as, its accuracy in local regions of the feature space around the query
instance. However, using only one criterion about the behavior of a base classiﬁer is not sufﬁcient
to accurately estimate its level of competence. In this paper, we present a novel dynamic ensemble
selection framework using meta-learning. We propose ﬁve distinct sets of meta-features, each one
corresponding to a different criterion to measure the level of competence of a classiﬁer for the
classiﬁcation of input samples. The meta-features are extracted from the training data and used to
train a meta-classiﬁer to predict whether or not a base classiﬁer is competent enough to classify
an input instance. During the generalization phase, the meta-features are extracted from the query
instance and passed down as input to the meta-classiﬁer. The meta-classiﬁer estimates, whether a
base classiﬁer is competent enough to be added to the ensemble. Experiments are conducted over
several small sample size classiﬁcation problems, i.e., problems with a high degree of uncertainty
due to the lack of training data. Experimental results show the proposed meta-learning framework
greatly improves classiﬁcation accuracy when compared against current state-of-the-art dynamic
ensemble selection techniques.
Ensemble of Classiﬁers, Dynamic Ensemble Selection, Meta-Learning, Classiﬁer competence
 
October 3, 2018
1. Introduction
Multiple Classiﬁer Systems (MCS) aim to combine classiﬁers to increase the recognition accuracy in pattern recognition systems . MCS are composed of three phases : (1) Generation,
(2) Selection and (3) Integration. In the ﬁrst phase, a pool of classiﬁers is generated. In the second
phase, a single classiﬁer or a subset having the best classiﬁers of the pool is(are) selected. We refer
to the subset of classiﬁers as Ensemble of Classiﬁers (EoC). The last phase is the integration, and
the predictions of the selected classiﬁers are combined to obtain the ﬁnal decision .
For the second phase, there are two types of selection approaches: static and dynamic. In
static approaches, the selection is performed during the training stage of the system. Then, the
selected classiﬁer or EoC is used for the classiﬁcation of all unseen test samples. In contrast,
dynamic ensemble selection approaches (DES) select a different
classiﬁer or a different EoC for each new test sample. DES techniques rely on the assumption
that each base classiﬁer is an expert in a different local region of the feature space . So,
given a new test sample, DES techniques aim to select the most competent classiﬁers for the
local region in the feature space where the test sample is located. Only the classiﬁers that attain
a certain competence level, according to a selection criterion, are selected. Recent work in the
dynamic selection literature demonstrates that dynamic selection techniques is an effective tool
for classiﬁcation problems that are ill-deﬁned, i.e., for problems where the size of the training data
is small and there are not enough data available to model the classiﬁers .
The key issue in DES is to deﬁne a criterion to measure the level of competence of a base
classiﬁer. Most DES techniques use estimates of the classiﬁers’
local accuracy in small regions of the feature space surrounding the query instance as a search
criterion to perform the ensemble selection. However, in our previous work , we demonstrated
that the use of local accuracy estimates alone is insufﬁcient to achieve results close to the Oracle
performance. The Oracle is an abstract model deﬁned in which always selects the classiﬁer
that predicted the correct label, for the given query sample, if such classiﬁer exists. In other
∗Corresponding author. Email Address: .
Email addresses: (Rafael M. O. Cruz), (Robert
Sabourin), (George D. C. Cavalcanti), (Tsang Ing Ren)
words, it represents the ideal classiﬁer selection scheme. In addition, as reported by Ko et al. ,
addressing the behavior of the Oracle is much more complex than applying a simple neighborhood
On the other hand, DES techniques based on other criteria, such as the degree of consensus of
the ensemble classiﬁers , encounter some problems when the search cannot ﬁnd a consensus
among the ensembles. In addition, they neglect the local performance of the base classiﬁers.
As stated by the “No Free Lunch” theorem , no algorithm is better than any other over all
possible classes of problems. Using a single criterion to measure the level of competence of a base
classiﬁer is very error-prone. Thus, we believe that multiple criteria to measure the competence of
a base classiﬁer should be taken into account in order to achieve a more robust dynamic ensemble
selection technique.
In this paper, we propose a novel dynamic ensemble selection framework using meta-learning.
From the meta-learning perspective, the dynamic ensemble selection problem is considered as
another classiﬁcation problem, called meta-problem. The meta-features of the meta-problem are
the different criteria used to measure the level of competence of the base classiﬁer. We propose ﬁve
sets of meta-features in this paper. Each set captures a different property about the behavior of the
base classiﬁer, and can be seen as a different dynamic selection criterion such as, the classiﬁcation
performance in a local region of the feature space and the classiﬁer conﬁdence for the classiﬁcation
of the input sample. Using ﬁve distinct sets of meta-features, even though one criterion might fail
due to problems in the local regions of the feature space or due to low conﬁdence results ,
the system can still achieve a good performance as other meta-features are also considered by the
selection scheme. Furthermore, in a recent analysis we compared the criteria used to measure
the competence of base classiﬁers embedded in different DES techniques. The result demonstrates
that, given the same query sample, distinct DES criteria select a different base classiﬁer as the most
competent one. Thus, they are not fully correlated. Hence, we believe that a more robust dynamic
ensemble selection technique is achieved using ﬁve sets of meta-features rather than only one.
The meta-features are used as input to a meta-classiﬁer that decides whether or not a base
classiﬁer is competent enough for the classiﬁcation of an input sample based on the meta-features.
The use of meta-learning has recently been proposed in as an alternative for performing
classiﬁer selection in static scenarios. We believe that we can carry this further, and extend the use
of meta-learning to dynamically estimate the level of competence of a base classiﬁer.
The proposed framework is divided into three phases: overproduction, meta-training and generalization. In the overproduction stage, a pool of classiﬁers is generated using the training data.
In the meta-training stage, the ﬁve sets of meta-features are extracted from the training data, and
are used to train the meta-classiﬁer that works as the classiﬁer selector. During the generalization
phase, the meta-features are extracted from the query instance and passed down as inputs to the
meta-classiﬁer. The meta-classiﬁer estimates whether a base classiﬁer is competent enough to
classify the given test instance. Thus, the proposed system differs from the current state-of-the-art
dynamic selection techniques not only because it uses multiple criteria to perform the classiﬁer
selection, but also because the classiﬁer selection rule is learned by the meta-classiﬁer using the
training data.
The generalization performance of the system is evaluated over 30 classiﬁcation problems.
We compare the proposed framework against eight state-of-the-art dynamic selection techniques
as well as static combination methods. The evaluation is focused on small size dataset, since
DES techniques has shown to be an effective tool for problems where the level of uncertainty
for recognition is high due to few training samples . However, a few larger datasets were
also considered in order to evaluate the performance of the proposed framework under different
conditions. The goal of the experiments is to answer the following research questions: (1) Can the
use of multiple DES criteria, as meta-features, lead to a more robust dynamic selection technique?
(2) Does the proposed framework outperform current DES techniques for ill-deﬁned problems?
This paper is organized as follows: Section 2 introduces the notion of classiﬁer competence,
and the state-of-the-art techniques for dynamically measuring the classiﬁers’ competence are presented. The proposed framework is presented in Section 3. The experimental study is conducted
in Section 4. Finally, our conclusion is presented in the last section.
2. Classiﬁer competence for dynamic selection
Classiﬁer competence deﬁnes how much we trust an expert, given a classiﬁcation task. The
notion of competence used is extensively in the ﬁeld of machine learning as a way of selecting,
from the plethora of different classiﬁcation models, the one that best ﬁts the given problem. Let
C = {c1, . . . , cM} (M is the size of the pool of classiﬁers) be the pool of classiﬁers and ci a
base classiﬁer belonging to the pool C. The goal of dynamic selection is to ﬁnd an ensemble of
classiﬁers C′ ⊂C that has the best classiﬁers to classify a given test sample xj. This is different
from static selection, where the ensemble of classiﬁers C′ is selected during the training phase, and
considering the global performance of the base classiﬁers over a validation dataset .
Nevertheless, the key issue in dynamic selection is how to measure the competence of a base
classiﬁer ci for the classiﬁcation of a given query sample xj. In the literature, we can observe
three categories: the classiﬁer accuracy over a local region, i.e., in a region of the feature space
surrounding the query instance xj, decision templates , which are techniques that work in
the decision space (i.e, a space deﬁned by the outputs of the base classiﬁers) and the extent of
consensus or conﬁdence. The three categories are described in the following subsections.
2.1. Classiﬁer accuracy over a local region
Classiﬁer accuracy is the most commonly used criterion for dynamic classiﬁer and ensemble
selection techniques . Techniques that are based on local accuracy
ﬁrst deﬁne a small region in the feature space surrounding a given test instance xj, called the
region of competence. This region is computed using either the K-NN algorithm or by
Clustering techniques , and can be deﬁned either in the training set or in the validation
set, such as in the KNORA techniques .
Based on the samples belonging to the region of competence, a criterion is applied in order
to measure the level of competence of a base classiﬁer. For example, the Overall Local Accuracy
(OLA) technique uses the accuracy of the base classiﬁer in the whole region of competence
as a criterion to measure its level of competence. The classiﬁer that obtains the highest accuracy
rate is considered the most competent one. The Local Classiﬁer Accuracy (LCA) computes
the performance of the base classiﬁer in relation to a speciﬁc class label using a posteriori information . The Modiﬁed Local Accuracy works similarly to the LCA technique, with the
only difference being that each sample belonging to the region of competence is weighted by its
Euclidean distance to the query instance. That way, instances from the region of competence that
are closer to the test sample have a higher inﬂuence when computing the performance of the base
classiﬁer. The classiﬁer rank method uses the number of consecutive correctly classiﬁed samples as a criterion to measure the level of competence. The classiﬁer that correctly classiﬁes the
most consecutive samples coming from the region of competence is considered to have the highest
competence level or “rank”.
Ko et al. proposed the K-Nearest Oracles (KNORA) family of techniques, inspired by
the Oracle concept. Four techniques are proposed: the KNORA-Eliminate (KNORA-E) which,
considers that a base classiﬁer ci is competent for the classiﬁcation of the query instance xj if
ci achieves a perfect accuracy for the whole region of competence. Only the base classiﬁers
with a perfect accuracy are used during the voting scheme. In The KNORA-Union (KNORA-U)
technique, the level of competence of a base classiﬁer ci is measured by the number of correctly
classiﬁed samples in the deﬁned region of competence. In this case, every classiﬁer that correctly
classiﬁed at least one sample can submit a vote. In addition, two weighted versions, KNORA-
E-W and KNORA-U-W were also proposed, in which the inﬂuence of each sample belonging to
the region of competence was weighted based on its Euclidean distance to the query sample xj.
Lastly, Xiao et al. proposed the Dynamic Classiﬁer Ensemble for Imbalanced Data (DCEID),
which is based on the same principles as the LCA technique. However, this technique also takes
into account each class prior probability when computing the performance of the base classiﬁer
for the deﬁned region of competence in order to deal with imbalanced distributions.
The difference between these techniques lies in how they utilize the local accuracy information in order to measure the level of competence of a base classiﬁer. The main issue with the
techniques arises from the fact that they depend on the performance of the techniques that deﬁne
the region of competence such as K-NN or clustering techniques. In our previous work , we
demonstrated that the effectiveness of dynamic selection techniques is limited by the performance
of the algorithm that deﬁnes the region of competence. The dynamic selection technique is likely
to commit errors when outlier instances (i.e., mislabelled samples) exists around the query sample
in the feature space . Using the local accuracy information alone is not sufﬁcient to achieve
results close to the Oracle. Moreover, any difference between the distribution of validation and test
datasets may negatively affect the system performance. Consequently, we believe that additional
information should also be considered.
2.2. Decision Templates
In this class of methods, the goal is also to select samples that are close to the query instance
xj. However, the similarity is computed over the decision space through the concept of decision
templates . This is performed by transforming both the test instance xj and the validation data
into output proﬁles. The output proﬁle of an instance xj is denoted by ˜xj = {˜xj,1, ˜xj,2, . . . , ˜xj,M},
where each ˜xj,i is the decision yielded by the base classiﬁer ci for the sample xj.
Based on the information extracted from the decision space, the K-Nearest Output Proﬁle
(KNOP) is similar to the KNORA technique, with the difference being that the KNORA works
in the feature space, while the KNOP works in the decision space. The KNOP technique ﬁrst
deﬁnes a set with the samples that are most similar to the output proﬁle of the input sample, ˜xj in
the decision space, called the output proﬁles set. The validation set is used for this purpose. Then,
similarly to the KNORA-E technique, only the base classiﬁers that achieve a perfect recognition
accuracy for the samples belonging to the output proﬁles set are used during the voting scheme.
The Multiple Classiﬁer Behaviour (MCB) technique also deﬁnes a set with the most similar
output proﬁles to the input sample using the decision space. Here, the selection criterion is based
on a threshold. The base classiﬁers that achieve a performance higher than the predeﬁned threshold
are considered competent and are selected to form the ensemble.
The advantage of this class of methods is that they are not limited by the quality of the region
of competence deﬁned in the feature space, with the similarity computed based on the decision
space rather than the feature space. However, the disadvantage with this comes from the fact that
only global information is considered, while the local expertise of each base classiﬁer is neglected.
2.3. Extent of Consensus or conﬁdence
Different from other methods, techniques that are based on the extent of consensus work by
considering a pool of ensemble of classiﬁers (EoC) rather than a pool of classiﬁers. Hence, the
ﬁrst step is to generate a population of EoC, C∗= {C
2, . . . , C
′ is the number of EoC
generated) using an optimization algorithm such as genetic algorithms or greedy search .
Then, for each new query instance xj, the level of competence of an ensemble of classiﬁers C
equal to the extent of consensus among its base classiﬁers.
Several criterion based on this paradigm was proposed: the Margin-based Dynamic Selection
(MDS) , where the criterion is the margin between the most voted class and the second most
voted class. The margin is computed simply by considering the difference between the number of
votes received by the most voted class and those received by the second most voted class. Two
variations of the MDS where proposed in , the Class-Strength Dynamic Selection (CSDS),
which includes the ensemble decision in the computation of the MDS, and the GSDS, where
the global performance of each EoC is also taken into account . Another technique from this
paradigm is the Ambiguity-guided Dynamic Selection (ADS) , which uses the ambiguity among
the base classiﬁers of an EoC as the criterion for measuring the competence level of an EoC. The
ambiguity is calculated by the number of base classiﬁers of an ensemble that disagrees with the
ensemble decision. The lower the number of classiﬁers that disagree with the ensemble decision,
the higher the level of competence of the EoC.
The greatest advantage of this class of methods stems from the fact that it does not require
information from the region of competence. Thus, it does not suffer from the limitations of the
algorithm that deﬁnes the region of competence. However, these techniques present the following
disadvantages: In many cases, the search cannot ﬁnd an EoC with an acceptable conﬁdence level.
There is a tie between different members of the pool, and the systems end up performing a random
decision . In addition, some classiﬁers are more overtrained than others. In this case, they end
up dominating the outcome even though they do not present better recognition performance .
The pre-computation of ensembles also greatly increases the overall system complexity as we are
dealing with a pool of EoC rather than a pool of classiﬁers.
3. The Proposed Framework: META-DES
3.1. Problem deﬁnition
From the meta-learning perspective, the dynamic selection problem can be seen as another
classiﬁcation problem, called the meta-problem. This meta-problem uses different criteria regarding the behavior of a base classiﬁer in order to decide whether it is competent enough to classify
a given sample xj. Thus, a dynamic selection system can be deﬁned based on two environments.
A classiﬁcation environment in which the input features are mapped into a set of class labels
w = {w1, w2, ..., wL} and a meta-classiﬁcation environment in which information about the behavior of the base classiﬁer is extracted from the classiﬁcation environment and used to decide
whether a base classiﬁer ci is competent enough to classify xj.
To keep with the conventions of the meta-learning literature, we deﬁne the proposed dynamic
ensemble selection in a meta-learning framework as follows:
• The meta-problem consists in deﬁning whether a base classiﬁer ci is competent enough to
classify xj.
• The meta-classes of this meta-problem are either “competent” or “incompetent” to classify
• Each meta-feature fi corresponds to a different criterion to measure the level of competence
of a base classiﬁer.
• The meta-features are encoded into a meta-features vector vi,j which contains the information about the behavior of a base classiﬁer ci in relation to the input instance xj.
• A meta-classiﬁer λ is trained based on the meta-features vi,j to predict whether or not ci
will achieve the correct prediction for xj.
In other words, a meta-classiﬁer λ is trained, based on vi,j, to predict whether a base classiﬁer
ci is competent enough to classify given a test sample xj. Thus, the proposed system differs from
the current state-of-the-art dynamic selection techniques not only because it uses multiple criteria,
but also because the selection rule is learned by the meta-classiﬁer λ using the training data.
3.2. The proposed META-DES
The META-DES framework is divided into three phases (Figure 1):
1. The overproduction phase, where the pool of classiﬁers C = {c1, . . . , cM}, composed of M
classiﬁers, is generated using the training instances xj,train from the dataset T .
2. The meta-training stage, in which samples xj,trainλ from the meta-training dataset Tλ are
used to extract the meta-features. A different dataset Tλ is used in this phase in order to
prevent overﬁtting. The meta-feature vectors vi,j are stored in the set T ∗
λ that is later used to
train the meta-classiﬁer λ.
3. The generalization phase, given a test sample xj,test resulting from the generalization data G;
its region of competence is extracted using the samples from the dynamic selection dataset
DSEL in order to compute the meta-features. The meta-feature vector vi,j is then passed to
the selector λ, which decides whether ci is competent enough to classify xj,test and should
be added to the ensemble, C′. The majority vote rule is applied over the ensemble C′, giving
the classiﬁcation wl of xj,test.
3.2.1. Overproduction
In this work, the Overproduction phase is performed using the Bagging technique .
Bagging is an acronym for Bootstrap AGGregatING. The idea behind this technique is to build a
diverse ensemble of classiﬁers by randomly selecting different subsets of the training data. Each
subset is used to train one individual classiﬁer ci. As the focus of the paper is on classiﬁer selection,
and not on classiﬁer generation methods, only the bagging technique is considered.
3.2.2. Meta-training
As shown in Figure 1, the meta-training stage consists of three steps: the sample selection
process, the meta-features extraction process, and the training of the meta-classiﬁer λ. For every
sample xj,trainλ ∈Tλ, the ﬁrst step is to apply the sample selection mechanism in order to know
whether or not xj,trainλ should be used for the training of the meta-classiﬁer λ. The whole Metatraining phase is formalized in Algorithm 1.
Sample Selection
As demonstrated by Dos Santos et al. and Cavalin et al. , one of the main issues in dynamic
ensemble selection arises when classifying testing instances where the degree of consensus among
the pool of classiﬁer is low, i.e., when the number of votes from the winning class is close or even
Xj,Train„
Sample Selection
Meta-Feature
Extraction Process
Data Generation Process
Classifier
Generation
classifiers
C = {c1, Y, cM}
Meta-Feature
Extraction Process
Majority Vote
3.2.3) Generalization Phase
Meta Training
3.2.2) Meta-Training Phase
3.2.1) Overproduction
Dynamic Selection
Figure 1: Overview of the proposed META-DES framework. It is divided into three steps 1) Overproduction, where
the pool of classiﬁers C = {c1, . . . , cM} is generated, 2) The training of the meta-classiﬁer λ, and 3) The generalization phase where an ensemble C′ is dynamically deﬁned based on the meta-information extracted from xj,test
and the pool C = {c1, . . . , cM}. The generalization phase returns the label wl of xj,test. hC, K and Kp are the
hyper-parameters required by the proposed system.
equal to the number of votes from the second class. To tackle this issue, we decided to focus
the training of the meta-classiﬁer λ to speciﬁcally deal with cases where the extent of consensus
among the pool is low. This step is conducted using a threshold hC, called the consensus threshold.
Each instance xj,trainλ is ﬁrst evaluated by the whole pool of classiﬁers in order to compute the
degree of consensus among the pool, denoted by H (xj,trainλ, C). If the consensus H (xj,trainλ, C)
falls below the consensus threshold hC, the instance xj,trainλ is used to compute the meta-features.
Algorithm 1 The Meta-Training Phase
Input: Training data Tλ
Input: Pool of classiﬁers C = {c1, . . . , cM}
2: for all xj,trainλ ∈Tλ do
Compute the consensus of the pool H (xj,trainλ, C)
if H (xj,trainλ, C) < hC then
Find the region of competence θj of xj,trainλ using Tλ.
Compute the output proﬁle ˜xj,trainλ of xj,trainλ.
Find the Kp similar output proﬁles φj of ˜xj,trainλ using ˜Tλ.
for all ci ∈C do
vi,j = MetaFeatureExtraction(θj, φj, ci, xj,trainλ)
if ci correctly classiﬁes xj,trainλ then
αi,j = 1 “ci is competent for xj,trainλ”
αi,j = 0 “ ci is incompetent for xj,trainλ”
18: end for
19: Divide T ∗
λ into 25% for validation and 75% for training.
20: Train λ using the Levenberg-Marquadt algorithm.
21: return The meta-classiﬁer λ.
Before extracting the meta-features, the region of competence of the instance xj,trainλ, denoted by θj = {x1, . . . , xK}, must ﬁrst be computed. The region of competence θj is deﬁned
in the Tλ set, using the K-Nearest Neighbor algorithm (line 5). Then, xj,trainλ is transformed
into an output proﬁle.
The output proﬁle of the instance xj,trainλ is denoted by ˜xj,trainλ =
{˜xj,trainλ,1, ˜xj,trainλ,2, . . . , ˜xj,trainλ,M}, where each ˜xj,trainλ,i is the decision yielded by the base
classiﬁer ci for the sample xj,trainλ .
Next, with the region of competence θj and the set with the most similar output proﬁles φj
computed, for each base classiﬁer ci belonging to the pool of classiﬁers C, one meta-feature vector
vi,j is extracted (lines 8 to 14). Each vi,j contains ﬁve sets of meta-features:
Meta-feature extraction process
Five different sets of meta-features are proposed in this work. Each feature set fi, corresponds to
a different criterion for measuring the level of competence of a base classiﬁer. Each set captures
a different property about the behavior of the base classiﬁer, and can be seen as a different criterion to dynamically estimate the level of competence of base classiﬁer such as, the classiﬁcation
performance estimated in a local region of the feature space and the classiﬁer conﬁdence for the
classiﬁcation of the input sample. Using ﬁve distinct sets of meta-features, even though one criterion might fail due to imprecisions in the local regions of the feature space or due to low conﬁdence
results, the system can still achieve a good performance as other meta-features are considered by
the selection scheme. Table 1 shows the criterion used by each fi and its relationship with one
dynamic ensemble selection paradigm presented in Section 2.
Table 1: Relationship between each meta-features and different paradigms to compute the level of competence of a
base classiﬁer.
Meta-Feature
Local accuracy in the region of competence
Classiﬁer Accuracy over a local region
Extent of consensus in the region of competence
Classiﬁer consensus
Overall accuracy in the region of competence
Accuracy over a local region
Accuracy in the decision space
Decision Templates
Degree of conﬁdence for the input sample
Classiﬁer conﬁdence
Three meta-features, f1, f2 and f3, are computed using information extracted from the region
of competence θj. f4 uses information extracted from the set of output proﬁles φj. f5 is calculated
directly from the input sample xj,trainλ, and corresponds to the level of conﬁdence of ci for the
classiﬁcation of xj,trainλ.
f1 - Neighbors’ hard classiﬁcation: First, a vector with K elements is created. For each instance
xk, belonging to the region of competence θj, if ci correctly classiﬁes xk, the k-th position
of the vector is set to 1, otherwise it is 0. Thus, K meta-features are computed.
f2 - Posterior probability: First, a vector with K elements is created. Then, for each instance
xk, belonging to the region of competence θj, the posterior probability of ci, P(wl | xk) is
computed and inserted into the k-th position of the vector. Consequently, K meta-features
are computed.
f3 - Overall Local accuracy: The accuracy of ci over the whole region of competence θj is computed and encoded as f3.
f4 - Output proﬁles classiﬁcation: First, a vector with Kp elements is generated. Then, for each
member ˜xk belonging to the set of output proﬁles φj, if the label produced by ci for xk is
equal to the label wl,k of ˜xk, the k-th position of the vector is set to 1, otherwise it is 0. A
total of Kp meta-features are extracted using output proﬁles.
f5 - Classiﬁer’s Conﬁdence: The perpendicular distance between the input sample xj,trainλ and
the decision boundary of the base classiﬁer ci is calculated and encoded as f5. f5 is normalized to a [0 −1] range using the Min-max normalization.
Classification
Output Profiles
Probabilities
Confidence
K features
K features
Figure 2: Feature Vector containing the meta-information about the behavior of a base classiﬁer. A total of 5 different
meta-features are considered. The size of the feature vector is (2×K)+Kp+2. The class attribute indicates whether
or not ci correctly classiﬁed the input sample.
A vector vi,j = {f1 ∪f2 ∪f3 ∪f4 ∪f5} is obtained at the end of the process (Figure 2). If ci
correctly classiﬁes xj,trainλ, the class attribute of vi,j, αi,j = 1 (i.e., vi,j corresponds to the behavior
of a competent classiﬁer), otherwise αi,j = 0. vi,j is stored in the meta-features dataset T ∗
10 to 16).
For each sample xj,trainλ used in the meta-training stage, a total of M (M is the size of the pool
of classiﬁers C) meta-feature vectors vi,j are extracted, each one corresponding to one classiﬁer
from the pool C. In this way, the size of the meta-training dataset T ∗
λ is the pool size M× number
of training samples N. For instance, consider that 200 training samples are available for the metatraining stage (N = 200), if the pool C is composed of 100 weak classiﬁers (M = 100), the
meta-training dataset is the number of training samples N × the number classiﬁers in the pool M,
N ∗M = 20.000. Hence, even though the classiﬁcation problem may be ill-deﬁned due to the size
of the training set, we can overcome this limitation in the meta-problem by increasing the size of
the pool of classiﬁers.
The last step of the meta-training phase is the training of the meta-classiﬁer λ. The dataset T ∗
is divided on the basis of 75% for training and 25% for validation. A Multi-Layer Perceptron
(MLP) neural network is considered as the selector λ. The validation data was used to select the
number of nodes in the hidden layer. We use a conﬁguration of 10 neurons in the hidden layer
since there were no improvement in results with more than 10 neurons. The training process for λ
is performed using the Levenberg-Marquadt algorithm. In addition, the training process is stopped
if its performance on the validation set decreases or fails to improve for ﬁve consecutive epochs.
3.2.3. Generalization Phase
The generalization procedure is formalized by Algorithm 2. Given the query sample xj,test, in
this phase, the region of competence θj is computed using the samples from the dynamic selection
dataset DSEL (line 2). Following that, the output proﬁles ˜xj,test of the test sample, xj,test, are
calculated. The set with Kp similar output proﬁles φj, of the query sample xj,test, is obtained
through the Euclidean distance applied over the output proﬁles of the dynamic selection dataset,
Next, for each classiﬁer ci belonging to the pool of classiﬁers C, the meta-feature extraction
process is called (Section 3.2.2.2), returning the meta-features vector vi,j (lines 5 and 6). Then,
vi,j is used as input to the meta-classiﬁer λ. If the output of λ is 1 (i.e., competent), ci is included
in the ensemble C′ (lines 8 to 10). After every base classiﬁer, ci, is evaluated, the ensemble C′
is obtained. The base classiﬁers in C′ are combined through the Majority Vote rule , giving
the label wl of xj,test (line 12 and 13). The majority vote rule is used to combine the selected
Algorithm 2 Classiﬁcation steps using the selector λ
Input: Query sample xj,test
Input: Pool of classiﬁers C = {c1, . . . , cM}
Input: dynamic selection dataset DSEL
2: Find the region of competence θj of xj,test using DSEL.
3: Compute the output proﬁle ˜xj,test of xj,test.
4: Find the Kp similar output proﬁles φj of ˜xj,test using ˜DSEL.
5: for all ci ∈C do
vi,j = FeatureExtraction(θj, φj, ci, xj,test)
input vi,j to λ
if αi,j = 1 “ci is competent for xj,test” then
11: end for
12: wl = MajorityV ote(xj,test, C
13: return wl
classiﬁers since it has been successfully used by other DES techniques . Tie-breaking is handled
by choosing the class with the highest a posteriori probability.
4. Experiments
4.1. Datasets
A total of 30 datasets are used in the comparative experiments. sixteen coming from the UCI
machine learning repository , four from the STATLOG project , four from the Knowledge Extraction based on Evolutionary Learning (KEEL) repository , four from the Ludmila
Kuncheva Collection of real medical data , and two artiﬁcial datasets generated with the Matlab PRTOOLS toolbox . We consider both ill-deﬁned problems, such as, Heart and Liver
Disorders as well as larger databases, such as, Adult, Magic Gamma Telescope, Phoneme and
WDG V1. The key features of each dataset are shown in Table 2.
4.2. Experimental Protocol
The experiments were conducted using 20 replications. For each replication, the datasets were
randomly divided on the basis 50% for training, 25% for the dynamic selection dataset (DSEL),
Table 2: Key Features of the datasets used in the experiments.
No. of Instances
Dimensionality
No. of Classes
Liver Disorders
Breast (WDBC)
Blood transfusion
Lithuanian
Ionosphere
Haberman’s Survival
Cardiotocography (CTG)
Vertebral Column
Steel Plate Faults
Laryngeal1
Laryngeal3
German credit
Mammographic
MAGIC Gamma Telescope
and 25% for the test set (G). The divisions were performed maintaining the priors probabilities of
each class. For the proposed META-DES, 50% of the training data was used in the meta-training
process Tλ and 50% for the generation of the pool of classiﬁers (T ).
For the two-class classiﬁcation problems, the pool of classiﬁers was composed of 100 Perceptrons generated using the bagging technique . For the multi-class problems, the pool of
classiﬁers was composed of 100 multi-class perceptron classiﬁer. The use of Perceptron as base
classiﬁer comes from the following observations based on past works in the literature:
• The use of weak classiﬁers can show more differences between the DES schemes . Thus,
making it a better option for comparing different DES techniques.
• Past works in the DES literature demonstrate that the use of weak models as base classi-
ﬁer achieve better results , where the use of decision trees or Perceptrons
outperform strong classiﬁcation models such as KNN classiﬁers.
• As reported by Leo Breiman , the bagging technique achieves better results when
weak and unstable base classiﬁers are used.
4.3. Parameters Setting
The performance of the proposed selection scheme depends on three parameters: the neighborhood size, K, the number of similar patterns using output proﬁles Kp and the consensus threshold
hC. The dynamic selection dataset DSEL was used for the analysis. The following methodology
• For the sake of simplicity, we selected the parameters that performed best.
• The value of the parameter K was selected based on the results of our previous paper .
In this case, K = 7 showed the best overall results, considering several dynamic selection
techniques.
• The Kruskall-Wallis statistical test with a 95% conﬁdence interval was used to determine
whether the difference in results was statistically signiﬁcant. If two conﬁgurations yielded
similar results, we selected the one with the smaller parameter value as it leads to a smaller
meta-features vector.
• The parameter hC was evaluated with Kp initially set at 1.
• The best value of hc was used in the evaluation of the best value for Kp.
• Only a subset with eleven of the thirty datasets are used for parameters setting procedure:
Pima, Liver, Breast, Blood Transfusion, Banana, Vehicle, Lithuanian, Sonar, Ionosphere,
Wine, Haberman’s Survival.
4.3.1. The effect of the parameter hC
Consensus Threshold
Recognition Rate (%)
Lithuanian
Ionosphere
Figure 3: Performance of the proposed system based on the parameter hC on the dynamic selection dataset, DSEL.
K = 7 and Kp = 1.
We varied the parameter hc from 50% to 100% at 10 percentile point interval. Figure 3 shows
the mean performance and standard deviation for each hC value. We compared each pair of results
using the Kruskal-Wallis non-parametric statistical test with a 95% conﬁdence interval. For 6 out
of 11 datasets (Vehicle, Lithuanian, Banana, Blood transfusion, Ionosphere and Sonar) hC = 70%
presented a value that was statistically superior to the others. Hence, hC = 70% was selected.
4.3.2. The effect of the parameter Kp
Number of Output Profiles
Recognition Rate (%)
Lithuanian
Ionosphere
Figure 4: The performance of the system varying the parameter Kp from 1 to 10 on the dynamic selection dataset,
DSEL. hc = 70% and K = 7
Figure 4 shows the impact of the value of the parameter Kp in an 1-to-10 range. Once again,
we compared each pair of results using the Kruskal-Wallis non-parametric statistical test, with a
95% conﬁdence. The results were statistically different only for the Sonar, Ionosphere and liver
disorders datasets, where the value of Kp = 5 showed the best results. Hence, Kp was set at 5.
4.4. Comparison with the state-of-the-art dynamic selection techniques
In this section we compare the recognition rates obtained by the proposed META-DES, against
eight dynamic selection techniques found in the literature . The objective of this comparative
study is to answer the following research question: (1) Can the use of multiple DES criteria as
meta-features lead to a more robust dynamic selection technique? (2) Does the proposed framework outperform current DES techniques for ill-deﬁned problems?
The eight state-of-the-art DES techniques used in this study are: the KNORA-ELIMINATE ,
KNORA-UNION , DES-FA , Local Classiﬁer Accuracy (LCA) , Overall Local Accuracy (OLA) , Modiﬁed Local Accuracy (MLA) , Multiple Classiﬁer Behaviour (MCB) 
and K-Nearests Output Proﬁles (KNOP) . These techniques were selected because they presented the very best results in the dynamic selection literature according to a recent survey on
this topic . In addition, we also compare the performance of the proposed META-DES with
static combination methods (Adaboost and Bagging), the classiﬁer with the highest accuracy in
the validation data (Single Best), static ensemble selection based on the majority voting error 
and the abstract model (Oracle) . The Oracle represents the ideal classiﬁer selection scheme.
It always selects the classiﬁer that predicted the correct label, for any given query sample, if such
classiﬁer exists. For the static ensemble selection method, 50% of the classiﬁers of the pool are
selected. The comparison against static methods is used since it is suggested the DES literature
that the minimum requirement for a DES method is to surpass the performance of static selection
and combination methods in the same pool .
For all techniques, the pool of classiﬁers C is composed of 100 Perceptrons as base classiﬁer
(M = 100). For the state-of-the-art DES techniques (KNORA-E, KNORA-U, DES-FA, LCA,
OLA, MLA, MCB and KNOP), the size of the region of competence (neighborhood size), K is
set to 7, since it achieved the best result on previous publications . The size of the region
of competence K is the only hyper-parameter required for the eight DES techniques. For the
Adaboost and Bagging technique 100 iterations are used (i.e., 100 base classiﬁer are generated).
We split the results in two tables: Table 3 shows a comparison with the proposed META-DES
against the eight state-of-the-art dynamic selection techniques considered. A comparison of the
Table 3: Mean and standard deviation results of the accuracy obtained for the proposed DESD and the DES systems
in the literature. A pool of 100 Perceptrons as base classiﬁers is used for all techniques. The best results are in bold.
Results that are signiﬁcantly better (p < 0.05) are marked with a •.
KNORA-E 
KNORA-U 
DES-FA 
79.03(2.24) •
73.79(1.86)
76.60(2.18)
73.95(1.61)
73.95(2.98)
73.95(2.56)
77.08(4.56)
76.56(3.71)
73.42(2.11)
Liver Disorders
70.08(3.49) •
56.65(3.28)
56.97(3.76)
61.62(3.81)
58.13(4.01)
58.13(3.27)
58.00(4.25)
58.00(4.25)
65.23(2.29)
Breast (WDBC)
97.41(1.07)
97.59(1.10)
97.18(1.02)
97.88(0.78)
97.88(1.58)
97.88(1.58)
95.77(2.38)
97.18(1.38)
95.42(0.89)
Blood Transfusion
79.14(1.03) •
77.65(3.62)
77.12(3.36)
73.40(1.16)
75.00(2.87)
75.00(2.36)
76.06(2.68)
73.40(4.19)
77.54(2.03)
91.78(2.68)
93.08(1.67)
92.28(2.87)
95.21(3.18)
95.21(2.15)
95.21(2.15)
80.31(7.20)
88.29(3.38)
90.73(3.45)
82.75(1.70)
83.01(1.54)
82.54(1.70)
82.54(4.05)
80.33(1.84)
81.50(3.24)
74.05(6.65)
84.90(2.01)
80.09(1.47)
Lithuanian Classes
93.18(1.32)
93.33(2.50)
95.33(2.64)
98.00(2.46)
85.71(2.20)
98.66(3.85)
88.33(3.89)
86.00(3.33)
89.33(2.29)
80.55(5.39)
74.95(2.79)
76.69(1.94)
78.52(3.86)
76.51(2.06)
74.52(1.54)
76.91(3.20)
76.56(2.58)
75.72(2.82)
Ionosphere
89.94(1.96)
89.77(3.07)
87.50(1.67)
88.63(2.12)
88.00(1.98)
88.63(1.98)
81.81(2.52)
87.50(2.15)
85.71(5.52)
99.25(1.11) •
97.77(1.53)
97.77(1.62)
95.55(1.77)
85.71(2.25)
88.88(3.02)
88.88(3.02)
97.77(1.62)
95.50(4.14)
76.71(1.86)
71.23(4.16)
73.68(2.27)
72.36(2.41)
70.16(3.56)
69.73(4.17)
73.68(3.61)
67.10(7.65)
75.00(3.40)
Cardiotocography (CTG)
84.62(1.08)
86.27(1.57)
85.71(2.20)
86.27(1.57)
86.65(2.35)
86.65(2.35)
86.27(1.78)
85.71(2.21)
86.02(3.04)
Vertebral Column
86.89(2.46)
85.89(2.27)
87.17(2.24)
82.05(3.20)
85.00(3.25)
85.89(3.74)
77.94(5.80)
84.61(3.95)
86.98(3.21)
Steel Plate Faults
67.21(1.20)
67.35(2.01)
67.96(1.98)
68.17(1.59)
66.00(1.69)
66.52(1.65)
67.76(1.54)
68.17(1.59)
68.57(1.85)
84.56(0.36)
84.01(1.10)
84.01(1.10)
84.01(1.10)
80.50(0.56)
80.50(0.56)
79.95(0.85)
78.75(1.35)
84.21(0.45)
77.25(3.52)
76.47(2.76)
75.29(3.41)
75.29(3.41)
75.29(3.41)
75.29(3.41)
76.47(3.06)
76.47(3.06)
80.00(4.25) •
66.87(2.99)
57.65(5.85)
61.00(2.88)
55.32(4.98)
59.45(2.65)
57.60(3.65)
57.60(3.65)
67.92(3.24)
62.45(3.65)
69.40(1.64)
67.12(2.35)
69.17(1.58)
67.12(2.35)
69.86(2.20)
69.86(2.20)
69.86(2.20)
68.49(3.27)
68.49(3.27)
87.15(2.43) •
80.34(1.57)
79.76(2.26)
80.34(1.57)
83.58(2.32)
82.08(2.42)
80.34(1.32)
78.61(3.32)
79.76(2.26)
87.15(2.43) •
78.94(1.25)
81.57(3.65)
82.89(3.52)
77.63(2.35)
77.63(2.35)
80.26(1.52)
81.57(2.86)
82.57(3.33)
Laryngeal1
79.67(3.78) •
77.35(4.45)
77.35(4.45)
77.35(4.45)
77.35(4.45)
77.35(4.45)
75.47(5.55)
77.35(4.45)
77.35(4.45)
Laryngeal3
72.65(2.17)
70.78(3.68)
72.03(1.89)
72.03(1.89)
72.90(2.30)
71.91(1.01)
61.79(7.80)
71.91(1.01)
73.03(1.89)
96.78(0.87)
95.95(1.25)
95.95(1.25)
95.37(2.02)
95.95(1.25)
95.95(1.25)
94.79(2.30)
95.95(1.25)
95.95(1.25)
German credit
75.55(1.31) •
72.80(1.95)
72.40(1.80)
74.00(3.30)
73.33(2.85)
71.20(2.52)
71.20(2.52)
73.60(3.30)
73.60(3.30)
84.80(3.36)
83.82(4.05)
83.82(4.05)
83.82(4.05)
85.29(3.69)
85.29(3.69)
86.76(5.50)
83.82(4.05)
83.82(4.05)
96.21(0.87)
95.35(1.23)
95.86(1.07)
93.00(2.90)
95.00(1.40)
94.14(1.07)
93.28(2.10)
95.86(1.07)
95.86(1.07)
80.35(2.58)
79.06(2.50)
78.92(3.33)
79.06(2.50)
78.84(2.53)
78.84(2.53)
64.94(7.75)
73.37(5.55)
78.92(3.33)
83.24(2.19) •
80.55(3.32)
77.77(4.25)
75.92(4.25)
74.07(6.60)
74.07(6.60)
75.92(5.65)
74.07(6.60)
80.55(3.32)
Mammographic
84.82(1.55) •
82.21(2.27)
82.21(2.27)
80.28(3.02)
82.21(2.27
82.21(2.27)
75.55(5.50)
81.25(2.07)
82.21(2.27)
MAGIC Gamma Telescope
84.35(3.27) •
80,03(3.25)
79,99(3.55)
81.73(3.27)
81,53(3.35)
81,16(3.00)
73,13(6.35)
75,91(5.35)
80,03(3.25)
META-DES against static combination rules is shown in Table 4. Each pair of results is compared
using the Kruskal-Wallis non-parametric statistical test, with a 95% conﬁdence interval. The best
results are in bold. Results that are signiﬁcantly better (p < 0.05) are marked with a •.
We can see in Table 3 the proposed META-DES achieves results that are either superior or
equivalent to the state-of-the-art DES techniques in 25 datasets (84% of the datasets). In addition,
the META-DES achieved the highest recognition performance for 18 datasets, which corresponds
to 60% of the datasets considered. Only for the Ecoli, Heart, Vehicle, Banana and Lithuanian
datasets (16% of the datasets) the recognition rates of the proposed META-DES framework presented is statistically inferior to the best result achieved by state-of-the-art DES techniques.
Table 4: Mean and standard deviation results of the accuracy obtained for the proposed DESD and static ensemble
combination. A pool of 100 Perceptrons as base classiﬁer is used for all techniques The best results are in bold.
Results that are signiﬁcantly better (p < 0.05) are marked with a •.
Single Best 
Bagging 
AdaBoost 
Static Selection 
Oracle 
79.03(2.24) •
73.57(1.49)
73.28(2.08)
72.52(2.48)
72.86(4.78)
95.10(1.19)
Liver Disorders
70.08(3.49) •
65.38(3.47)
62.76(4.81)
64.65(3.26)
59.18(7.02)
93.07(2.41)
Breast (WDBC)
97.41(1.07)
97.04(0.74)
96.35(1.14)
98.24(0.89)
96.83(1.00)
99.13(0.52)
Blood Transfusion
79.14(1.03) •
75.07(1.83)
75.24(1.67)
75.18(2.08)
75.74(2.23)
94.20(2.08)
91.78(2.68)
84.07(2.22)
81.43(3.92)
81.61(2.42)
81.35(4.28)
94.75(2.09)
82.75(1.70)
81.87(1.47)
82.18(1.31)
80.56(4.51)
81.65(1.48)
96.80(0.94)
Lithuanian Classes
93.18(1.32) •
84.35(2.04)
82.33(4.81)
82.70(4.55)
82.66(2.45)
98.35 (0.57)
80.55(5.39)
78.21(2.36)
76.66(2.36)
74.95(5.21)
79.03(6.50)
94.46(1.63)
Ionosphere
89.94(1.96)
87.29(2.28)
86.75(2.75)
86.75(2.34)
87.50(2.23)
96.20(1.72)
99.25(1.11)
96.70(1.46)
95.56(1.96)
99.20(0.76)
96.88(1.80)
100.00(0.01)
76.71(1.86)
75.65(2.68)
72.63(3.45)
75.26(3.38)
73.15(3.68)
97.36(3.34)
Cardiotocography (CTG)
84.62(1.08)
84.21(1.10)
84.54(1.46)
83.06(1.23)
84.04(2.02)
93.08(1.46)
Vertebral Column
86.89(2.46)
82.04(2.17)
85.89(3.47)
83.22(3.59)
84.27(3.24)
97.40(0.54)
Steel Plate Faults
67.21(1.20)
66.05(1.98)
67.02(1.98)
66.57(1.06)
67.22(1.64)
88.72(1.89)
84.56(0.36)
83.17(0.76)
84.36(0.56)
84.04(0.37)
84.23(0.53)
97.82(0.54)
77.25(3.52) •
69.35(2.68)
72.22(3.65)
70.32(3.65)
67.80(4.60)
91.54(1.55)
66.87(2.99) •
52.92(4.53)
62.64(5.61)
55.89(3.25)
57.16(4.17)
90.65(0.00)
69.40(1.64)
67.53(2.83)
67.20(2.35)
69.38(4.28)
67.26(1.04)
99.10(0.72)
87.15(2.43) •
83.64(3.34)
85.60(2.27)
83.58(2.91)
84.37(2.79)
95.59(0.39)
79.67(3.78) •
74.86(4.78)
76.31(4.06)
74.47(3.68)
76.89(3.15)
92.10(0.92)
Laryngeal1
83.43(4.50)
80.18(5.51)
81.32(3.82)
79.81(3.88)
80.75(4.93)
98.86(0.98)
Laryngeal3
72.65(2.17)
68.42(3.24)
67.13(2.47)
62.32(2.57)
71.23(3.18)
96.78(0.87)
95.15(1.74)
95.25(1.11)
96.01(0.74)
96.24(1.25)
99.88(0.36)
German credit
75.55(2.31)
71.16(2.39)
74.76(2.73)
72.96(1.25)
73.60(2.69)
99.12(0.70)
84.80(3.36)
80.26(3.58)
82.50(4.60)
81.61(5.01)
82.05(3.72)
95.90(1.02)
96.21(0.87)
94.52(0.96)
95.23(0.87)
95.43(0.92)
95.31(0.92)
98.69(0.87)
80.35(2.58) •
75.87(1.33)
72.60(2.33)
75.90(1.06)
72.70(2.32)
99.34(0.24)
83.24(2.19)
79.25(3.78)
79.18(2.57)
80.27(2.76)
80.55(3.59)
98.98(1.19)
Mammographic
84.82(1.55)
83.60(1.85)
85.27(1.85)
83.07(3.03)
84.23(2.14)
99.59(0.15)
MAGIC Gamma Telescope
84.35(3.27)
80.27(3.50)
81.24(2.22)
87.35(1.45) •
85.25(3.25)
95.35(0.68)
For the 12 datasets where the proposed META-DES did not achieved the highest recognition
rate (WDBC, Banana, Vehicle, Lithuanian, Cardiotocography, Vertebral column, Steel plate faults,
Ecoli, Glass, ILPD, Laryngeal3 and Heart) we can see that each DES technique presented the best
accuracy for different datasets (as shown in Figure 5). The KNOP achieves the best results for three
datasets (Ecoli, Steel plate faults and Laryngeal3), the MCB for two datasets (Vehicle and Glass),
the DES-FA for 3 datasets (Banana, Breast cancer and Cardiotocography) and so forth. This can be
explained by the "no free lunch" theorem. There is no criterion to estimate the competence of base
classiﬁers that dominates all other when compared with several classiﬁcation problems. Since the
proposed META-DES uses a combination of ﬁve different criteria as meta-features, even though
one criterion might fail, the system can still achieve a good performance as other meta-features are
also considered by the selection scheme. In this way, a more robust DES technique is achieved.
KNORA−E KNORA−U
DES Technique
Number of datasets the technique achieved
the best result
Figure 5: Bar plot showing the number of datasets that each DES technique presented the highest recognition accuracy.
Moreover, another advantage of the proposed META-DES framework comes from the fact
that several meta-feature vectors are generated for each training sample in the meta-training phase
(Section 3.2.2). For instance, consider that 200 training samples are available for the meta-training
stage (N = 200), if the pool C is composed of 100 weak classiﬁers (M = 100), the metatraining dataset is the number of training samples N × the number classiﬁers in the pool M,
N × M = 20.000. Hence, there is more data to train the meta-classiﬁer λ than for the generation
of the pool of classiﬁers C itself. Even though the classiﬁcation problem may be ill-deﬁned, due
to the size of the training set, using the proposed framework we can overcome this limitation since
the size of the meta-problem is up to 100 times bigger than the classiﬁcation problem. So, our
proposed framework has more data to estimate the level of competence of base classiﬁers than
the other DES methods, where only the training or validation data is available. This fact can be
observed by the results obtained for datasets with less than 500 samples for training, such as,
Liver Disorders, Sonar, Weaning and Ionosphere where recognition accuracy of the META-DES
is statistically superior for those small size problems.
When compared against static ensemble techniques Table 4, the proposed META-DES achieves
the highest recognition accuracy for 24 out of 30 datasets. This can be explained by the fact that
the majority of datasets considered are ill-deﬁned. Hence, the results found in this paper also
support the claim made by Cavalin et al. that DES techniques outperform static methods for
ill-deﬁned problems.
We can thus answer the research question posed in this paper: Can the use of meta-features lead
to a more robust dynamic selection technique? As the proposed system achieved better recognition
rates in the majority of datasets the use of multiple properties from the classiﬁcation environment
as meta-features indeed leads to a more robust dynamic ensemble selection technique.
5. Conclusion
In this paper, we presented a novel DES technique in a meta-learning framework. The framework is based on two environments: the classiﬁcation environment, in which the input features
are mapped into a set of class labels, and the meta-classiﬁcation environment, in which different
properties from the classiﬁcation environment, such as the classiﬁer accuracy in the feature space
or the consensus in the decision space, are extracted from the training data and encoded as metafeatures. Five sets of meta-features are proposed. Each set corresponding to a different dynamic
selection criterion. These meta-features are used to train a meta-classiﬁer which can estimate
whether a base classiﬁer is competent enough to classify a given input sample. With the arrival of
new test data, the meta-features are extracted using the test data as reference, and used as input to
the meta-classiﬁer. The meta-classiﬁer decides whether the base classiﬁer is competent enough to
classify the test sample.
Experiments were conducted using 30 classiﬁcation datasets coming from ﬁve different data
repositories (UCI, KEEL, STATLOG, LKC and ELENA) and compared against eight state-of-theart dynamic selection techniques (each technique based on a single criterion to measure the level of
competence of a base classiﬁer), as well as ﬁve classical static combination methods. Experimental
results show the proposed META-DES achieved the highest classiﬁcation accuracy in the majority
of datasets, which can be explained by the fact that the proposed META-DES framework is based
on ﬁve different DES criteria. Even though one criterion might fail, the system can still achieve a
good performance as other criteria are also considered in order to perform the ensemble selection.
In this way, a more robust DES technique is achieved.
In addition, we observed a signiﬁcant improvement in performance for datasets with critical
training size samples. This gain in accuracy can be explained by the fact that during the Meta-
Training phase of the framework, each training sample generates several meta-feature vectors for
the training of the meta-classiﬁer. Hence, the proposed framework has more data to train the metaclassiﬁer and consequently to estimate the level of competence of base classiﬁers than the current
state-of-the-art DES methods, where only the training or validation data is available.
Future works on this topic will involve:
1. The deﬁnition of new sets of meta-features to better estimate the level of competence of the
base classiﬁers.
2. The selection of meta-features based on optimization algorithms in order to improve the
performance of the meta-classiﬁer, and consequently, the accuracy of the DES system.
3. The evaluation of different training scenarios for the meta-classiﬁer.
Acknowledgment
This work was supported by the Natural Sciences and Engineering Research Council of Canada
(NSERC), the École de technologie supérieure (ÉTS Montréal) and CNPq (Conselho Nacional de
Desenvolvimento Cientíﬁco e Tecnológico).