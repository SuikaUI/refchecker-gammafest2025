Communicated by Terrence J. Sejnowski
Information-Based Objective Functions for Active Data
David J. C. MacKay*
Computation and Neural Systems, California Institute of Technology 139-74,
Pasadena, C A 91125 USA
Learning can be made more efficient if we can actively select particu-
larly salient data points. Within a Bayesian learning framework, objec-
tive functions are discussed that measure the expected informativeness
of candidate measurements. Three alternative specifications of what
we want to gain information about lead to three different criteria for
data selection. All these criteria depend on the assumption that the
hypothesis space is correct, which may prove to be their main weak-
1 Introduction
Theories for data modeling often assume that the data are provided by a
source that we do not control. However, there are two scenarios in which
we are able to actively select training data. In the first, data measure-
ments are relatively expensive or slow, and we want to know where to
look next so as to learn as much as possible. According to Jaynes . Experimental design within a Bayesian framework using the
Shannon information as an objective function has been studied by Lind-
ley and by Luttrell . A distinctive feature of this approach is
that it renders the optimization of the experimental design independent
of the "tests" that are to be applied to the data and the loss functions
associated with any decisions. This paper uses similar information-based
'Present address: Cavendish Laboratory, Madingley Road, Cambridge, CB3 OHE,
United Kingdom. E-mail: .
Neural Computation 4,590-604 
@ 1992 Massachusetts Institute of Technology
Objective Functions for Data Selection
objective functions and discusses the problem of optimal data selection
within the Bayesian framework for interpolation described in previous
papers . Most of the results in this paper have direct
analogs in Fedorov .
Recent work in the neural networks literature on active data selection,
also known as “query learning,” has concentrated on slightly different
problems: The work of Baum and Hwang et al. relates to
perfectly separable classification problems only; in both these papers a
sensible query-based learning algorithm is proposed, and empirical re-
sults of the algorithm are reported; Baum also gives a convergence proof.
But since the algorithms are both human designed, it is not clear what
objective function their querying strategy optimizes, nor how the algo-
rithms could be improved. In contrast, this paper (which discusses noisy
interpolation problems) derives criteria from defined objective functions;
each objective function leads to a different data selection criterion. A fu-
ture paper will discuss the application of the same ideas to classification
problems .
Plutowski and White study a different problem from the above,
in the context of noise-free interpolation: they assume that a large amount
of data has already been gathered, and work on principles for selecting
a subset of that data for efficient training; the entire data set (inputs and
targets) is consulted at each iteration to decide which example to add to
the training subset, an option that is not permitted in this paper.
1.1 Statement of the Problem. Imagine that we are athering data
in the form of a set of input-output pairs DN = { ~ ( ~ ) , t ( ~ ~ ) ,
1 . . . N. These data are modeled with an interpolant y(x; w, A). An in-
terpolation model H specifies the “architecture” A, which defines the
functional dependence of the interpolant on the parameters wi, i = 1 . . . k.
The model also specifies a regularizer, or prior on w, and a cost function,
or noise model N describing the expected relationship between y and t.
We may have more than one interpolation model, which may be linear
or nonlinear in w. Two previous papers described
the Bayesian framework for fitting and comparing such models, assum-
ing a fixed data set. This paper discusses how the same framework for
interpolation relates to the task of selecting what data to gather next.
Our criterion for how informative a new datum is will depend on
what we are interested in. Several alternatives spring to mind:
1. If we have decided to use one particular interpolation model, we
might wish to select new data points to be maximally informative
about the values that that model’s parameters w should take.
David J. C. MacKay
2. Alternatively, we might not be interested in getting a globally well-
determined interpolant; we might only want to be able to predict
the value of the interpolant accurately in a limited region, perhaps
at a point in input space that we are not able to sample directly.
3. Lastly, we might be unsure which of two or more models is the best
interpolation model, and we might want to select data so as to give
us maximal information to discriminate between the models.
This paper will study each of these tasks for the case in which we wish
to evaluate the utility as a function of xN+', the input location at which
a single measurement of a scalar tN+' will be made. The more complex
task of selecting multiple new data points will not be addressed here, but
the methods used can be generalized to solve this task, as is discussed in
Fedorov and Luttrell . The similar problem of choosing the
xN+] at which a vector of outputs tN+' is measured will not be addressed
The first and third definitions of information gain have both been
studied in the abstract by Lindley . All three cases have been
studied by Fedorov , mainly in non-Bayesian terms. In this paper,
solutions will be obtained for the interpolation problem by using a gaus-
sian approximation and in some cases assuming that the new datum is a
relatively weak piece of information. In common with most other work
on active learning, the utility is evaluated assuming that the probability
distributions defined by the interpolation model are correct. For some
models, this assumption may be the Achilles' heel of this approach, as
discussed in Section 6.
1.2 Can Our Choice Bias Our Inferences? One might speculate that
the way we choose to gather data might be able to bias our inferences
systematically away from the truth. If this were the case we might need
to make our inferences in a way that undoes such biases by taking into
account how we gathered the data. In orthodox statistics many estimators
and statistical tests do depend on the sampling strategy.
However, the likelihood principle states that our inferences should de-
pend on the likelihood of the actual data received, not on other data that
we might have gathered but did not. Bayesian inference is consistent
with this principle; there is no need to undo biases introduced by the
data collecting strategy, because it is nut possible for such biases to be
introduced-as long as we perform inference using all the data gathered
 . When the models are concerned with esti-
mating the distribution of output variables t given input variables x, we
are allowed to look at the x value of a datum, and decide whether or not
to include the datum in the data set. This will not bias our inferences
about the distribution P(t 1 x).
Objective Functions for Data Selection
2 Choice of Information Measure
Before we can start, we need to select a measure of the information gained
about an unknown variable when we receive the new datum tN+'. Hav-
ing chosen such a measure we will then select the xN+' for which the
expected information gain is maximal. Two measures of information have
been suggested, both based on Shannon's entropy, whose properties as a
sensible information measure are well known. Let us explore this choice
for the first task, where we want to gain maximal information about the
parameters of the interpolant, w.
Let the probability distributions of the parameters before and after we
receive the datum tN+' be PN(w) and PN+'(w).
Then the change in entropy
of the distribution is AS = SN - &+I, where
where rn is the measure on w that makes the argument of the log dimen-
sionless.' The greater AS is, the more information we have gained about
w. In the case of the quadratic models discussed in (MacKay 1992a1, if
we set the measure m(w) equal to the prior F'"(w)' the quantity S N is
closely related to the log of the "Occam fa~tor."~
An alternative information measure is the cross entropy between PN(w)
and PN+'(w):
Let us define G' = -G so as to obtain a positive quantity; then G' is a
measure of how much information we gain when we are informed that
the true distribution of w is J"+'(W),
rather than PN(w).
These two information measures are not equal. Intuitively they differ
in that if the measure m(w) is flat, AS only quantifies how much the
probability "bubble" of P(w) shrinks when the new datum arrives; G'
also incorporates a measure of how much the bubble moves because of
the new datum. Thus according to G', even if the probability distribution
does not shrink and become more certain, we have learned something if
the distribution moves from one region to another in w-space.
The question of which information measure is appropriate is poten-
tially complicated by the fact that G' is not a consistent additive mea-
sure of information: if we receive datum A then datum B, in general,
GkB # GA + GL. This intriguing complication will not, however, hin-
der our task: we can only base our decisions on the expectations of AS
2This measure rn will be unimportant in what follows but is included to avoid com-
mitting dimensional crimes. Note that the sign of AS has been defined so that our
information gain corresponds to positive AS.
'If the Occam factor is O.F. = (2~)~/~det-''*A
exp(-crE$p)/Z~(a), then SN =
log O.F. + y/2, using notation from MacKay .
David J. C. MacKay
and G'; we will now see that in expectation AS and G' are equal, so
for our purposes there is no distinction between them. This result holds
independent of the details of the models we study and independent of
any gaussian approximation for P(w).
Proof. E(AS) = E(G'). To evaluate the expectation of these quantities,
we have to assume a probability distribution from which the datum fN+l
(hence abbreviated as t) comes. We will define this probability distribu-
tion by assuming that our current model, complete with its error bars, is
correct. This means that the probability distribution of t is P(t I D N , Z ) ,
where 'H is the total specification of our model. The conditioning vari-
ables on the right will be omitted in the following proof.
We can now compare the expectations of AS and G'.
where m is free to be any measure on w; let us make it the same measure
m as in equation 2.1. Then the first term in equation 2.3 is -&+I.
E(G') = -E(SN+l) + /dtP(t) /dkwP(w 1 t)log- m(w)
= E ( - S N + ~ + S N ) = E(AS)
Thus the two candidate information measures are equivalent for our pur-
poses. This proof also implicitly demonstrates that E(AS) is independent
of the measure m(w). Other properties of E(AS) are proved in Lindley
 . The rest of this paper will use AS as the information measure,
with m(w) set to a constant.
3 Maximizing Total Information Gain ___
Let us now solve the first task: how to choose xN+l so that the expected
information gain about w is maximized. Intuitively we expect that we
will learn most about the interpolant by gathering data at the x location
where our error bars on the interpolant are currently greatest. Within the
quadratic approximation, we will now confirm that intuition.
3.1 Notation. The likelihood of the data is defined in terms of a noise
level 0: = B-' by P({t} I w,,B,N) = exp[-PE~(w)]/Z,, where ED(w) =
C, $ [t"' - y(x("); w)]', and ZD is the appropriate normalizing constant.
The likelihood could also be defined with an x-dependent noise level
@-'(x), or correlated noise in multiple outputs (in which case P-' would
Objective Functions for Data Selection
be the covariance matrix of the noise). From here on y will be treated
as a scalar y for simplicity. When the likelihood for the first N data-is
combined with a prior P(w I a,R) = exp[-aEw(w)]/Zw, in which the
regularizing constant (or weight decay rate) cy corresponds to the prior
expected smoothness of the interpolant, we obtain our current probability
distribution for w, F"(w) = exp[-M(w)]/ZM, where M(w) = nEw + ,BED.
The objective function M(w) can be quadratically approximated near to
the most probable parameter vector, W M ~ , by
M(w) 21 W(W)
= M(wM~) + -AW'AAW
where Aw = w - W M ~
and the Hessian A = VVM is evaluated at the
minimum wMP. We will use this quadratic approximation from here on.
If M has other minima, those can be treated as distinct models as in
MacKay (199213).
First we will need to know what the entropy of a gaussian distribution
is. It is easy to confirm that if P(w) 0: ecM (w), then for a flat measure
S = -(1+log2s)+-log(m2detA-')
Thus our aim in minimizing S is to make the size of the joint error bars
on the parameters, det A-l, as small as possible.
Expanding y around wMP, let
where g, = l?yy/awj is the (x-dependent) sensitivity of the output variable
to parameter wj, evaluated at W M ~ .
Now imagine that we choose a particular input x and collect a new
datum. If the datum t falls in the region such that our quadratic approx-
imation applies, the new Hessian AN+* is
AN+I 5 A f PggT
where we have used the approximation VVi[t - y(x;w)I2 N gg'. This
expression neglects terms in d2y/dw,dwk; those terms are exactly zero for
the linear models discussed in MacKay , but they are not necessar-
ily negligible for nonlinear models such as neural networks. Notice that
this new Hessian is independent of the value that the datum t actually
takes, so we can specify what the information gain AS will be for any
datum, because we can evaluate AN+^ just by calculating g.
Let us now see what property of a datum causes it to be maximally
informative. The new entropy SN+I is equal to -1/2 log (m2 det AN+l),
David J. C. MacKay
neglecting additive constants. This determinant can be analytically eval-
uated , using the identities
det [A + &gT] = (detA)(l + PgTA-’g)
from which we obtain:
Total information gain = -A log (mZ
= - lOg(1 + ,OgTA-’g)
In the product PgTA-’g, the first term tells us that, not surprisingly, we
learn more information if we make a low noise (high /I) measurement.
The second term gTA-’g is precisely the variance of the interpolant at
the point where the datum is collected.
Thus we have our first result: to obtain maximal information about
the interpolant, take the next datum at the point where the error bars
on the interpolant are currently largest (assuming the noise C T ~ on all
measurements is the same). This rule is the same as that resulting from
the “D-optimal” and ”minimax” design criteria .
For many interpolation models, the error bars are largest beyond the
most extreme points where data have been gathered. This first criterion
would in those cases lead us to repeatedly gather data at the edges of
the input space, which might be considered non-ideal behavior; but we
do not necessarily need to introduce an ad hoc procedure to avoid this.
The reason we do not want repeated sampling at the edges is that we
do not want to h o w what happens there. Accordingly, we can derive
criteria from alternative objective functions which only value information
acquired about the interpolant in a defined region of interest.
4 Maximizing Information about the Interpolant in a Region of Interest
Thus we come to the second task. First assume we wish to gain maxi-
mal information about the value of the interpolant at a particular point
x ( ~ ) . Under the quadratic approximation, our uncertainty about the in-
terpolant y has a gaussian distribution, and the size of the error bars is
given in terms of the Hessian of the parameters by
where g(u) is i3y/dw evaluated at x(”). As above, the entropy of this
gaussian distribution is 1/2 logg: + const. After a measurement t is
Objective Functions for Data Selection
made at x where the sensitivity is g, these error bars are scaled down by
a factor of 1 - p2, where p is the correlation between the variables f and
y'")', given by p2 = [gTA-'g(u)]2/[~i(~2
+ u:)], where 0,' = gTA-'g. Thus
the information gain about y(") is
Marginal information gain = -A log 02
The term gTA-'giU) is maximized when the sensitivities g and g(u) are
maximally correlated, as measured by their inner product in the metric
defined by A-'. The second task is thus solved for the case of extrap-
olation to a single point. This objective function is demonstrated and
criticized in Section 6.
4.1 Generalization to Multiple Points. Now imagine that the objec-
tive function is defined to be the information gained about the interpolant
at a set of points {x'")}. These points should be thought of as represen-
tatives of the region of interest, for example, points in a test set. This
case also includes the generalization to more than one output variable
y; however, the full generalization, to optimization of an experiment in
which many measurements are made, will not be made here . The preceding objective function, the informa-
tion about y("), can be generalized in several ways, some of which lead
to dissatisfactory results.
4.1.1 First Objective Function for Multiple Points. An obvious objective
function is the joint enfropy of the output variables that we are interested
in. Let the set of output variables for which we want to minimize the
uncertainty be {y(")}, where u = 1.. . V runs either over a sequence of
different input locations .("I, or over a set of different scalar outputs,
or both. Let the sensitivities of these outputs to the parameters be g(,).
Then the covariance matrix of the values {y'")} is
Y = G ~ A - ~ G
where the matrix G = [gfl)g(2) . . . gcv)]. Disregarding the possibility that
Y might not have full rank, which would necessitate a more complex
treatment giving similar results, the joint entropy of our output variables
S[P({y(")})] is related to logdet Y-'. We can find the information gain for
a measurement with sensitivity vector g, under which A -+ A + PggT,
using the identities (equation 3.5).
Joint information gain = -A log det Y-'
(gTA-'G)Y-'(GTA-'
David J. C. MacKay
The row vector v = gTAP1G measures the correlations between the sensi-
tivities g and gf,). The quadratic form vY-'vT measures how effectively
these correlations work together to reduce the joint uncertainty in {y'")}.
The denominator cz + 0,' moderates this term in favor of measurements
with small uncertainty.
4.1.2 Criticism. I will now argue that actually the joint entropy
S[P( {y(")})] of the interpolant's values is not an appropriate objective
function. A simple example will illustrate this.
Imagine that V = k, that is, the number of points defining our region
of interest is the same as the dimensionality of the parameter space w.
The resulting matrix G = [g(l)g(Z). . .g(v)] may be almost singular if the
points x(') are close together, but typically it will still have full rank.
Then the parameter vector w and the values of the interpolant {y'")}
are in one to one (locally) linear correspondence with each other. This
means that the change in entropy of P({y(")}) is identical to the change in
entropy of P(w) . This can be confirmed by substitution of
Y-' = G-'AG-IT into (equation 4.3), which yields (equation 3.6). So if
the datum is chosen in accordance with equation 4.3, so as to maximize
the expected joint information gain about {y(")}, exactly the same choice
will result as is obtained maximizing the first criterion, the expected total
information gain about w (Section 3.1)! Clearly, this choice is independent
of our choice of {y'")}, so it will have nothing to do with our region of
This criticism of the joint entropy is not restricted to the case V = k.
The reason that this objective function does not achieve what we want
is that the joint entropy is decreased by measurements which introduce
correlations among predictions about {y'")') as well as by measurements
that reduce the individual uncertainties of predictions. However, we do
not want the variables {y'")} to be strongly correlated in some arbitrary
way; rather we want each y(") to have small variance, so that if we are
subsequently asked to predict the value of y at any one of the us, we will
be able to make confident predictions.
4.1.3 Second Objective Function for Multiple Points. This motivates an
alternative objective function: to maximize the average over u of the
information gained about y(") alone. Let us define the mean marginal
= C P, s[P(Y("))]
= - C P, log 0: + const
where P, is the probability that we will be asked to predict ~ ( " 1 , and
Objective Functions for Data Selection
0, - g(,,)
from (equation 4.1):
g[u). For a measurement with sensitivity vector
The mean marginal information gain is demonstrated and
Section 6.
g, we obtain
criticized in
Two simple variations on this objective function can be derived. If
instead of minimizing the mean marginal entropy of our predictions y@),
we minimize the mean marginal entropy of the predicted noisy variables
t('), which are modeled as deviating from y'") under additive noise of
variance a:, we obtain equation 4.4 with u,' replaced by u,' + 0;. This
alternative may lead to significantly different choices from equation 4.4
when any of the marginal variances 0,' fall below the intrinsic variance
0: of the predicted variable.
If instead we take an approach based on loss functions, and require
that the datum we choose minimizes the expectation of the mean squared
error of our predictions {y'")}, which is EM = C, Pug:, then we obtain as
our objective function, to leading order, AEM N C, P,(gTA-'g(u))2/(0: +
0:); this increases the bias in favor of reducing the variance of the vari-
ables y(") with largest 02. This is the same as the "Q-optimal" design
 .
4.2 Comment on the Case of Linear Models. It is interesting to note
that for a linear model [one for which y(x; w) = C Wh4h(X)] with quadratic
penalty functions, the solutions to the first and second tasks depend
only on the x locations where data were previously gathered, not on the
actual data gathered {t}; this is because g(x) = $(x) independent of w, so
A = aVVEw+/3 C, ggT is independent of {t}. A complete data-gathering
plan can be drawn up before we start. It is only for a nonlinear model
that our decisions about what data to gather next are affected by our
previous observations!
5 Maximizing the Discrimination between Two Models
Under the quadratic approximation, two models will make slightly dif-
ferent gaussian predictions about the value of any datum. If we measure
a datum t at input value x, then
P(t I Fff) = Normal(pi, 0;)
where the parameters pi, u: are obtained for each interpolation model Xi
from its own best fit parameters wMF(i), its own Hessian Ai, and its own
sensitivity vector g,:
David J. C. MacKay
Intuitively, we expect that the most informative measurement will be at
a value of x such that p1 and p2 are as separated as possible from each
other on a scale defined by 01,02. Further thought will also confirm that
we expect to gain more information if 0: and 0: differ from each other
significantly; at such points, the "Occam factor" penalizing the more
powerful model becomes more significant.
Let us define the information gain to be AS = SN - S N + ~ , where
S = - C, P(X,) logP('FI,). Exact calculations of AS are not analytically
possible, so I will assume that we are in the regime of small information
gain, that is, we expect measurement of f to give us a rather weak likeli-
hood ratio P ( f I 'FI,)/P(f I 8 2 ) . This is the regime where Ip, - p2/ << 01,02.
Using this assumption we can take the expectation over t, and a page
of algebra leads to the result:
E(AS) N p('F11)p(X2)
[ ($ f $) makes a similar derivation but he uses a poor approx-
imation that loses the second term.
6 Demonstration and Discussion
A data set consisting of 21 points from a one-dimensional interpolation
problem was interpolated with an eight-hidden-unit neural network. The
data were generated from a smooth function by adding noise with stan-
dard deviation CJ,, = 0.05. The neural network was adapted to the data
using weight decay terms ac, which were controlled using the methods
of MacKay and noise level p fixed to l/& The data and the
resulting interpolant, with error bars, are shown in Figure la.
The expected total information gain, that is, the change in entropy
of the parameters, is shown as a function of x in Figure lb. This is
just a monotonic function of the size of the error bars. The same figure
also shows the expected marginal information gain about three points of
interest, {x'")} = {-1.25,0.0,1.75}. Notice that the marginal information
gain is in each case peaked near the point of interest, as we would expect.
Note also that the height of this peak is greatest for x(') = -1.25, where
the interpolant oscillates rapidly, and lower for x(') = 1.75, where the
interpolant is smoother. At each x = x('), the marginal information gain
about x(") and the total information gain are equal.
Figure lc shows the mean marginal information gain, where the points
of interest, {d")},
were defined to be a set of equally spaced points on
the interval [-2.1,4.1] (the same interval in which the training data lie).
Objective Functions for Data Selection
total information .a-
r n a r g i n a L i n f ~ r m i t i o n
Figure 1: Demonstration of total and marginal information gain. (a) The data
set, the interpolant, and error bars. (b) The expected total information gain
and three marginal information gains. (c) The mean marginal information gain,
with the region of interest defined by 300 equally spaced points on the interval
[-2.1,4.1]. The information gains are shown on a scale of nats (1 nat = log2e
David J. C. MacKay
The mean marginal information gain gradually decreases to zero away
from the region of interest, as hoped. In the region to the left where the
characteristic period of the interpolant is similar to the data spacing, the
expected utility oscillates as x passes through the existing data points,
which also seems reasonable. The only surprising feature is that the esti-
mated utility in that region is lower on the data points than the estimated
utility in the smooth region toward the right.
6.1 The Achilles' Heel of These Methods. This approach has a po-
tential weakness: there may be models for which, even though we have
defined the region of interest by the points {x'")}, the expected marginal
information gain for a measurement at x still blows up as x + f m , like
the error bars. This can occur because the information gain estimates the
utility of a data point assuming that the model is correct; if we know that
the model is actually an approximation tool that is incorrect, then it is
possible that undesirable behavior will result.
A simple example that illustrates this problem is obtained if we con-
sider modeling data with a straight line y = wlx, where w1 is the un-
known parameter. Imagine that we want to select data so as to obtain a
model that predicts accurately at ,("I. Then if we assume that the model
is right, clearly we gain most information if we sample at the largest
possible 1x1, since such points give the largest signal-to-noise ratio for
determining wl.
If, however, we assume that the model is actually not
correct, but only an approximation tool, then common sense tells us we
should sample closer to d").
Thus if we are using models that we know are incorrect, the marginal
information gain is really the right answer to the wrong question. It is a
task for further research to formulate a new question whose answer is ap-
propriate for any approximation model. Meanwhile, the mean marginal
information gain seems a promising objective function to test further.
6.2 Computational Complexity. The computation of the suggested
objective functions is moderately cheap once the inverse Hessian A-'
has been obtained for the models concerned. This is a O(Nk2) + O(k3)
process, where N is the number of data points and k is the number of
parameters; this process may already have been performed in order to
evaluate error bars for the models, to evaluate the "evidence," to evaluate
parameter "saliencies," and to enable efficient learning. This cost can be
compared with the cost of locating a minimum of the objective function
M, which in the worst case scales as O(Nk3) (taking the result for a
quadratic function). Evaluation of the mean marginal information gain
at C candidate points x then requires O(Ck2) + O(CVk) time, where V is
the number of points of interest x(") [O(k2) to evaluate A-'g for each x,
and O(Vk) to evaluate the dot product of this vector with each g(,)]. So
if C = O(k) and V = O(k), evaluation of the mean marginal information
Objective Functions for Data Selection
gain will be less computationally expensive than the inverse Hessian
evaluation.
For contexts in which this is too expensive, work in progress is ex-
ploring the possibility of reducing these calculations to O(k2) or smaller
time by statistical methods.
The question of how to efficiently search for the most informative x
is not addressed here; gradient-based methods could be constructed, but
Figure lc shows that the information gain is locally nonconvex, on a scale
defined by the interdatum spacing.
7 Conclusion
For three specifications of the information to be maximized, a solution
has been obtained. The solutions apply to linear and nonlinear interpo-
lation models, but depend on the validity of a local gaussian approxima-
tion. Each solution has an analog in the non-Bayesian literature .
In each case a function of x has been derived that predicts the infor-
mation gain for a measurement at that x. This function can be used to
search for an optimal value of x (which in large-dimensional input spaces
may not be a trivial task). This function could also serve as a way of re-
ducing the size of a large data set by omitting the data points that are
expected to be least informative. And this function could form the basis
of a stopping rule, that is, a rule for deciding whether to gather more
data, given a desired exchange rate of information gain per measurement
 .
A possible weakness of these information-based approaches is that
they estimate the utility of a measurement assuming that the model is
correct. This might lead to undesirable results. The search for ideal
measures of data utility is still open.
Acknowledgments
I thank Allen Knutsen, Tom Loredo, Marcus Mitchell, and the referees
for helpful feedback. This work was supported by a Caltech Fellowship
and a Studentship from SERC, UK.