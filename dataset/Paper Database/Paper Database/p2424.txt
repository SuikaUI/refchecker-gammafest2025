Neuromorphic Electronic Systems
CARVER MEAD
Invited Paper
Biological in forma tion-processing systems operate on com-
pletely different principles from those with which most engineers
are familiar. For many problems, particularly those in which the
input data are ill-conditioned and the computation can be speci-
fied in a relative manner, biological solutions are many orders of
magnitude more effective than those we have been able to imple-
ment using digital methods. This advantage can be attributed prin-
cipally to the use of elementary physical phenomena as computa-
tional primitives, and to the representation of information by the
relative values of analog signals, rather than by the absolute values
of digital signals. This approach requires adaptive techniques to
mitigate the effects of component differences. This kind of adap-
tation leads naturally to systems that learn about their environ-
ment. Large-scale adaptive analog systems are more robust to com-
ponent degredation and failure than are more conventional
systems, and they use far less power. For this reason, adaptive ana-
log technology can be expected to utilize the full potential of wafer-
scale silicon fabrication.
TWO TECHNOLOGIES
Historically, the cost of computation has been directly
related to the energy used in that computation. Today's
electronic wristwatch does far more computation than the
Eniac did when it was built. It is not the computation itself
that costs-it is the energy consumed, and the system over-
head required to supply that energy and to get rid of the
heat: the boxes, the connectors, the circuit boards, the
power supply, the fans, all of the superstructure that makes
the system work. As the technology has evolved, it has
always moved in the direction of lower energy per unitcom-
putation. That trend took us from vacuum tubes to tran-
sisitors, and from transistors to integrated circuits. It was
the force behind the transition from n-MOS to CMOS tech-
nology that happened less than ten years ago. Today, it still
is pushing us down to submicron sizes in semiconductor
technology.
So it pays to look at just how much capability the nervous
system has in computation.There is a myth that the nervous
system is slow, is built out of slimy stuff, uses ions instead
of electrons, and is therefore ineffective. When the Whirl-
wind computer was first built back at M.I.T., they made a
movie about it, which was called "Faster than Thought."
The Whirwind did less computation than your wristwatch
Manuscript received February 1, 1990; revised March 23, 1990.
The author is with the Department of Computer Science, Cal-
IEEE Log Number 9039181.
ifornia Institute of Technology, Pasadena, CA 91125.
does. We have evolved by a factor of about 10 million in the
cost of computation since the Whirlwind. Yet we still can-
not begin to do the simplest computations that can be done
by the brains of insects, let alone handle the tasks routinely
performed by the brains of humans. So we have finally come
to the point where we can see what is difficult and what is
easy. Multiplying numbers to balance a bank account is not
that difficult. What is difficult is processing the poorly con-
ditioned sensory information that comes in through the lens
of an eye or through the eardrum.
A typical microprocessor does about 10 million opera-
tions/s, and uses about 1 W. In round numbers, it cost us
about l O - ' J to do one operation, the way we do it today,
on a single chip. If we go off the chip to the box level, a
whole computer uses about 10-5J/operation. Awhole com-
puter is thus about two orders of magnitude less efficient
than is a single chip.
Back in the late 1960's we analyzed what would limit the
electronic device technology as we know it; those calcu-
lations have held up quite well to the present [I]. The stan-
dard i ntegrated-ci rcu it fabricat ion processes available today
allow usto build transistorsthat have minimum dimensions
of about 1 p
m). By ten years from now, we will have
reduced these dimensions by another factor of 10, and we
will be getting close to the fundamental physical limits: if
we make the devices any smaller, they will stop working.
It is conceiveable that a whole new class of devices will be
invented-devices
that are not subject to the same limi-
tations. But certainly the ones we have thought of up to
now-including the superconducting ones-will not make
our circuits more than abouttwoordersof magnitude more
dense than those we have today. The factor of 100 in density
translates rather directly into a similar factor in computa-
tion efficiency. So the ultimate silicon technology that we
can envision today will dissipate on the order of
energy for each operation at the single chip level, and will
consume a factor of 100-1000 more energy at the box level.
We can compare these numbers to the energy require-
ments of computing in the brain. There are about 10"syn-
apases in the brain. A nerve pulse arrives at each synapse
about ten timesls, on average. So in rough numbers, the
brain accomplishes 10'' complex operations/s. The power
dissipation of the brain is a few watts, so each operation
costs only IO6 J. The brain is a factor of 1 billion more effi-
cient than our present digital technology, and a factor of
0018-9219/90/1000-1629$01.00
PROCEEDINGS OF THE IEEE, VOL. 78, NO. IO, OCTOBER 1990
10 million more efficient than the best digital technology
that we can imagine.
From the first integrated circuit in 1959 until today, the
cost of computation has improved by a factor about 1 mil-
lion. We can count on an additional factor of 100 before
fundamental limitations are encountered. At that point, a
state-of-the-art digital system will still require 10MW to pro-
cess information at the rate that it is processed by a single
human brain. The unavoidable conclusion, which I reached
aboutten years ago, isthatwe have something fundamental
to learn from the brain about a new and much more effec-
tive form of computation. Even the simplest brains of the
simplest animals are awesome computational instruments.
They do computations we do not koow how to do, in ways
we do not understand.
We might think that this big disparity in the effectiveness
of computation has to do with the fact that, down at the
device level, the nerve membrane is actually working with
single molecules. Perhaps manipulating single molecules
is fundamentally moreefficientthan is usingthecontinuum
physics with which we build transistors. If that conjecture
were true, we would have no hope that our silicon tech-
nology would ever compete with the nervous system. In
fact, however, the conjecture is false. Nerve membranes
use populations of channels, rather than individual chan-
nels, to change their conductances, in much the same way
that transistors use populations of electrons rather than sin-
gle electrons. It is certainly true that a single channel can
exhibit much more complex behaviors than can a single
electron in the active region of a transistor, but these chan-
nels are used in large populations, not in isolation.
We can compare the two technologies by asking how
much energy is dissipated in charging up the gate of a tran-
sistor from a 0 to a 1. We might imagine that a transistor
would compute a function that is loosely comparable to
synaptic operation. In today’s technology, it takes about
j to charge up the gate of a single minimum-size tran-
sistor. In tenyears,the numberwill beabout 10-15j-within
shooting range of the kind of efficiency realized by nervous
systems. So the disparity between the efficiency of com-
putation in the nervous system and that in a computer i s
primarily attributable not to the individual device require-
ments, but rather to the way the devices are used in the
DID THE ENERGY Go?
Where did all the energy go? There is a factor of 1 million
unaccounted for between what it costs to make a transistor
work and what is required to do an operation the way we
do it in a digital computer. There are two primary causes
of energy waste in the digital systems we build today.
1) Weloseafactorofabout100 because,thewaywe build
digital hardware, the capacitance of the gate is only a very
small fraction of capacitance of the node. The node is mostly
wire, so we spend most of our energy charging up the wires
and not the gate.
2) We use far more than one transistor to do an opera-
tion; in a typical implementation, we switch about 10 000
transistors to do one operation.
So altogether it costs 1 million times as much energy to
make what we call an operation in a digital machine as it
costs to operate a single transistor.
I do not believe that there is any magic in the nervous
system-that there is a mysterious fluid in there that is not
defined, some phenomenon that is orders of magnitude
more effective than anything we can ever imagine. There
is npthing that is done in the nervous system thatwecannot
emulate with electronics if we understand the principles of
neural information processing. I have spent the last decade
trying to understand enough about how it works to be able
to build systems that work in a similar way; I have had mod-
est success, as I shall describe.
So there are two big opportunities. The first factor-of-I00
opportunity, which can be done with either digital or ana-
log technology, is to make alogrithms more local, so that
we do not have to ship the data all over the place. That is
a big win-we have built digital chips that way, and have
achieved a factor of between 10 and 100 reduction in power
dissipation. That still leaves the factor of IO4, which is the
difference between making a digital operation out of
bunches of AND and OR gates, and using the physics of the
device to do the operation.
Evolution has made a lot of inventions, as it evolved the
nervous system. I think of systems as divided into three
somewhat arbitrarily levels. There is at the bottom the ele-
mentary functions, then the representation of information,
and at the top the organizingprinciples. All three levels must
worktogether;all threeareverygifferent from thoseweuse
in human-engineered systems. Furthermore, the nervous
system is not accompanied bya manual explainingthe prin-
ciples of operation. The blueprints and the early prototypes
were thrown away a long time ago. Now we are stuck with
an artifact, so we must try to reverse engineer it.
Let us consider the primitive operations and represen-
tations in the nervous system, and contrast them with their
counterparts in a digital system. As we think back, many of
us remember being confused when we were first learning
about digital design. First, we decide on the information
representation. There is only one kind of information, and
that is the bit: It is either a 1 or a 0. We also decide the ele-
mentary operations we allow, usually AND, OR, and NOT or
their equivalents. We start by confining ourselves to an
incredibly impoverished world, and out of that, we try to
build something that makes sense. The miracle is that we
can do it! But we pay the factor of I O 4 for taking all the beau-
tiful phyics that is built into those transistors, mashing it
down into a 1 or a 0, and then painfully building it back up,
with AND and OR gates to reinvent the multiply. We then
string together those multiplications and additions to get
morecomplexoperations-thosethat are useful in a system
we wish to build.
COMPUTATION
PRIMITIVES
What kind of computation primitives are implemented
by the device physics we have available in nervous tissue
or in a silicon integrated circuit? In both cases, the state
variables are analog, represented by an electrical charge.
In the nervous system, there are statevariables represented
by chemical concentrations as well. To build a nervous sys-
tem or a computer, we must be able to make specific con-
nections. A particular output is connected to certain inputs
and not to others. To achieve that kind of specificity, we
must beableto isolateone signal on asingleelectrical node,
with minimum coupling to other nodes. In both electronics
and the nervous system, that isolation is achieved by build-
ing an energy barrier, so that we can put some charge on
PROCEEDINGS OF THE IEEE, VOL. 78, NO. IO, OCTOBER 1990
an electrical node somewhere, and it does not leak over to
someother node nearby. In the nervous system, that energy
barrier is built by the difference in the dielectric constant
between fat and aqueous solutions. In electonics, it is built
by the difference in the bandgap between silicon and sil-
icon dioxide.
We do basic aggregation of information using the con-
servation of change. We can dump current onto an elec-
trical node at any location, and it all ends up as charge on
the node. Kirchhoff’s law implements a distributed addi-
tion, and the capacitance of the node integrates the current
into the node with respect to time.
In nervous tissue, ions are in thermal equilibrium with
their surroundings, and hence their energies are Boltz-
mann distributed. This distribution, together with the pres-
ence of energy barriers, computes a current that is an expo-
nential function of the barrier energy. If we modulate the
barrier with an applied voltage, the current will be an expo-
nential function of that voltage. That principle is used to
create active devices (those that produce gain or amplifi-
cation in signal level), both in the nervous system and in
electronics. In addition to providing gain, an individual
transistor computes a complex nonlinear function of its
control and channel voltages. That function is not directly
comparable to the functions that synapses evaluate using
their presynaptic and postsynaptic potentials, but a few
transistors can be connected strategically to compute
remarkably competent synaptic functions.
Fig. l(a) and (b) shows the current through a nerve mem-
Test pulse plenbal
Tesl pulse potenbal
Fig. 1. Current-voltage plots for several important devices,
each showing the ubiquitous exponential characteristic.
Curves A and 6 show the behavior of populations of active
ion channels in nerve membrane. Curve C illustrates the
exponential dependence of the arrival rate of packets of the
neurotransmitter at the postsynaptic membrane on the pre-
synaptic membrane potential. Curve D shows the saturation
current of a MOS transistor as a function of gate voltage.
brane as a function of the voltage across the membrane. A
plot of the current out of a synapse as the function of the
voltage across the presynaptic membrane is shown in (c).
The nervous system uses, as its basic operation, a current
that increases exponentially with voltage. The channel cur-
rent in atransistor as afunction of thegatevoltage is shown
in (d). The current increases exponentially over many orders
of magnitude, and then becomes limited by space charge,
which reduces the dependence to the familiar quadratic.
Note that this curve is hauntingly similar to others in the
same figure. What class of computations can be imple-
mented efficiently using expontential functions as primi-
tives? Analog electronic circuits are an ideal way to explore
this question.
Most important, the nervous system contains mecha-
nisms for long-term learning and memory. All higher ani-
mals undergo permanent changes in their brains as a result
of life experiences. Neurobiologists have identified at least
one mechanism for these permanent changes, and are
actively pursuing others. In microelectronics, we can store
a certain quantity of charge on a floating polysilicon node,
and that charge will be retained indefinitely. The floating
node is completely surrounded by high-quality silicon diox-
ide-the world’s most effective known insulator. We can
sense the charge by making the floating node the gate of
an ordinary MOS transistor.This mechanism has been used
since 1971 for storing digital information in EPROM’s and
similar devices, but there is nothing inherentlydigital about
the charge itself. Analog memory comes as a natural con-
sequence of this near-perfect charge-storage mechanism.
A silicon retina that does a rudimentary form of learning
and long-term memory is described in the next section .
This system uses ultraviolet light to move charge through
the oxide, onto or off the floating node. Tunneling to and
from the floating node is used in commercial EEPROM
devices. Several hot-electron mechanisms also have been
employed to transfer charge through the oxide. The ability
to learn and retain analog information for long periods is
thus a natural consequence of the structures created by
modern silicon processing technology.
The fact that we can build devices that implement the
same basic operations as those the nervous system uses
leads to the inevitable conclusion that we should be able
to build entire systems based on the organizing principles
used by the nervous system. I will refer to these systems
generically as neurornorphic systems. We start by letting
the device physics define our elementaryoperations. These
functions provide a rich set of computational primitives,
each a direct result of fundamental physical principles. They
are not the operations out of which we are accustomed to
buildingcomputers, but in manyways, they are much more
interesting. They are more interesting than AND and OR. They
are more interesting than multiplication and addition. But
they are very different. If we try to fight them, to turn them
into something with which we are familiar, we end up mak-
inga mess. So the real trick is to inventa representation that
takesadvantageof the inherent capabilities of the medium,
such as the abilities to generate exponentials, to do inte-
gration with respect to time, and to implement a zero-cost
addition using Kirchhoff’s law. These are powerful primi-
tives; using the nervous system as a guide, we will attempt
to find a natural way to integrate them into an overall sys-
tem-design strategy.
MEAD: NEUROMORPHIC ELECTRONIC SYSTEMS
RETINAL COMPUTATION
I shall usetwoexamplesfromtheevolutionof silicon reti-
nas to illustrate a number of physical principles that can be
used to implement computation primitives. These exam-
ples also serve to introduce general principles of neural
computation, and to show how these principles can be
applied to realize effective systems in analog electronic
integrated-circuit technology.
In 1868, Ernst Mach described the operation per-
formed by the retina in the following terms.
The illumination of a retinal point will, in proportion to
the difference between this illumination and the average
of the illumination on neighboring points, appear brighter
or darker, respectively, depending on whether the illumi-
nation of i t is above or below the average. The weight of
the retinal points in this average is to be thought of as rap-
idly decreasing with distance from the particular point con-
For many years, biologists have assembled evidence
about the detailed mechanism by which this computation
is accomplished. The neural machinery that performs this
first step in the chain of visual processing is located in the
outer plexiform layer of the retina, just under the photo-
receptors. The lateral spread of information at the outer
plexiform layer is mediated by a two-dimensional network
of cells coupled by resistive connections. The voltage at
every point in the network represents a spatially weighted
average of the photoreceptor inputs. The farther away an
input is from a point in the network, the less weight it is
given. The weighting function decreases in a generally
exponential manner with distance.
Using this biological evidence as a guide, Mahowald ,
 reported a silicon model of the computation described
by Mach. In the silicon retina, each node in the network is
linked to its six neighbors with resistive elements to form
a hexagonal array, as shown in Fig. 2. A single bias circuit
Fig. 2. Schematic of pixel from the Mahowald retina. The
output is the difference between the potential of the local
receptor and that of the resistive network. The network corn-
putes a weighted average over neighboring pixels.
associated with each node controls the strength of the six
associated resistive connections. Each photoreceptor acts
as avoltage input that drives the corresponding node of the
resistive network through a conductance. A transconduc-
tance amplifier is used to implement a unidirectional con-
ductance so the photoreceptor acts an effective voltage
source. No current can be drawn from the output node of
the photoreceptor because the amplifier input is con-
nected to only the gate of a transistor.
The resistive network computes a spatially weighted
average of photoreceptor inputs. The spatial scale of the
weighting function is determined by the product of the lat-
eral resistance and the conductance coupling the photo-
receptors into the network. Varying the conductanceof the
transconductance amplifier or the strength of the resistors
changes the space constant of the network, and thus
changes the effective area over which signals are averaged.
From an engineering point of view, the primaryfunction
of the computation performed by a silicon retina is to pro-
vide an automatic gain control that extends the useful oper-
ating range of the system. It is essential that a sensory sys-
tem be sensitive to changes in its input, no matter what the
viewing conditions. The structure executing this level-nor-
malization operation performs many other functions as
well, such as computing the contrast ratio and enhancing
edges in the image. Thus, the mechanisms responsible for
keeping the system operating over an enormous range of
image intensity have important consequences with regard
to the representation of data.
The imageenhancement performed bythe retinawasalso
described by Mach.
Let us call the intensity of illumunation U = f (x, y). The
brightness sensation v of the corresponding retinal point is
v = U - m , a typical 6 in diameter wafer
contains about lo’devices, partitioned into several hundred
chips. After fabrication, the chips are cut apart and are put
into packages. Several hundred of these packages are
placed on a circuit board, which forms interconnections
among them.
Why not just interconnect the chips on the wafer where
they started, and dispense with all the extra fuss, bother,
and expense? Many attempts by many groups to make a
digital wafer-scale technology have met with abysmal fail-
ure. There are two basic reasons why wafer-scale integra-
tion isverydifficult. First, atypical digital chipwill fail if even
a single transistor or wire on the chip is defective. Second,
the power dissipated by several hundred chips of circuitry
is over 100 W, and getting rid of all that heat is a major pack-
aging problem. Together, these two problems have pre-
vented even the largest computer companies from deploy-
ing wafer-scale systems successfully. The low-power
dissipation of adaptive analog systems eliminates the pack-
aging problem; wafers can be mounted on edge, and nor-
mal air convection will adequately remove the few hundred
milliwatts of heat dissipated per wafer. Due to the robust-
ness of the neural representation, the failure of a few com-
ponents per square centimeter will not materially affect the
performance of the system: its adaptive nature will allow
the system simply to learn to ignore these inputs because
they convey no information. In oneortwodecades, I believe
we will have 10” devices on a wafer, connected as a com-
plete adaptive analog system. We will be able to extract
information from connections made around the periphery
of the wafer, while processing takes place in massively par-
allel form over the entire surface of the wafer. Each wafer
operating in this manner will be capable of approximately
I O l 3 operationsls. At that time, we will still not understand
nearly as much about the brain as we do about the tech-
SCALING LAWS
The possibility of wafer-scale integration naturally raises
the question of the relative advantage conveyed by a three-
dimensional neural structure over a two-dimensional one.
Both approaches have been pursued in the evolution of ani-
mal brains so the question is of great interest in biology as
well. Let us take the point of view that whatever we are going
to build will be a space-filling structure. If it is a sheet, it will
have neurons throughout the whole plane; if it is a volume,
neurons will occupy the whole volume. If we allow every
wire from every neuron to be as long as the dimensions of
the entire structure, we will obviously get an explosion in
thesizeofthestructureasthe numberof neurons increases.
The brain has not done that. If we compare our brain to a
rat brain, we are not noticeably less efficient in our use of
wiring resources. So the brain has evolved a mostly local
wiring strategyto keepthe scalingfromgettingoutof hand.
What are the requirements of a structure that keep the frac-
tion of its resources devoted to wire from exploding as it
is made larger? If the structure did not scale, a large brain
would be all wire and would have no room for the com-
First, let us consider the two-dimensional case. For the
purpose of analysis, we can imagine that the width W of
each wire is independent of the wire’s length L, and that
the probability that a wire of length between L and L + dL
is dedicated to each neuron is p(L) dL. The expected area
of such a wire is the WL p(L) dL. The entire plane, of length
and width L,,,,
is covered with neurons, such that there is
one neuron per area A. Although the wires from many neu-
rons overlap, the total wire from any given neuron must fit
in area A. We can integrate the areas of the wires of all
lengths associated with a given neuron, assuming that the
shortest wire is of unit length:
WL p(L) dL = A.
The question is then: What are the bounds on the form of
p(L) such that theareaA required for each neuron does not
grow explosively as L,,,
becomes large? We can easily see
that if p(L) = 1/L2, the areaA grows as the logarithm of Lmax-
a quite reasonable behavior. If p(L) did not decrease at least
this fast with increasing L,,,,
the human brain would be
much more dominated by wire than it is, compared to the
brain of a rat or a bat. From this argument, I conclude that
the nervous system is organized such that, on the average,
the number of wires decreases no more slowly than the
inverse square of the wire’s length.
We can repeat the analysis for a three-dimensional neural
structureof extent L,,,,
in which each neuron occupiesvol-
ume V. Each wire has a cross-sectional area S, and thus has
an expected volume SL p(L). As before, the total wire asso-
ciated with each neuron must fit in volume v:
SL p(L) dL = v.
So the three-dimensional structure must follow the same
scaling law as its two-dimensional counterpart. If we build
a space-filling structure, the third dimension allows us to
contact more neurons, but it does not change the basic scal-
ing rule. The number of wires must decrease with wire
length in the same way in both two and three dimensions.
The cortex of the human brain, if it is stretched out, i s
about 1 mlside, and 1 mm thick. About half of that milli-
meter iswire (white matter), and theother half is computing
machinery (gray matter). This basically two-dimensional
strategywon out over the three-dimensional strategies used
by more primitive animals, apparently because it could
evolve more easily: new areas of cortex could arise in the
natural course of evolution, and some of them would be
MEAD: NEUROMORPHIC ELECTRONIC SYSTEMS
retained in the genome if they conveyed a competitive
advantage on their owners. This result gives us hope that
a neural structurecomptising manytwo-dimensional areas,
such as those we can make on silicon wafers, can be made
into a truly usbful, massively parallel, adaptive computing
CONCLUSION
Biological information-processing systems operate on
completely different principlesfromthosewith which engi-
neers are familiar. For many problems, particularly those
in which the input data are ill-conditioned and the com-
putation can be specified in a relative manner, biological
solutions are many orders of magnitude more effective than
those we have been able to implement using digital meth-
ods. I have shown that this advantage can be attributed
principally to the use of elementary physical phenomena
as computational primitives, and to the representation of
information by the relative values of analog signals, rather
than by the absolute values of digital signals. I have argued
that this approach requires adaptive techniques to correct
for differences between nominally identical components,
and that this adaptive capability leads naturally to systems
that learn about their environment. Although the adaptive
analog systems build up to the present time are rudimen-
tary, they have demonstrated important principles as a pre-
requisite to undertaking projects of much larger scope. Per-
haps the most intriguing result of these experiments has
been the suggestion that adaptive analog systems are 100
times more efficient in their use of silicon, and they use
10 000 times less power than comparable digital systems.
It is also clear that these systems are more robust to com-
ponent degradation and failure than are more conventional
systems. I have also argued that the basic two-dimensional
limitation of silicon technology is not a serious limitation
in exploiting the potential of neuromorphic systems. For
these reasons, I expect large-scale adaptive analog tech-
nology to permit the full utilization of the enormous, here-
tofore unrealized, potential of wafer-scale silicon fabrica-