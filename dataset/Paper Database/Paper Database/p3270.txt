Finding Opinionated Blogs Using Statistical Classiﬁers and Lexical Features
Feifan Liu, Bin Li and Yang Liu
The University of Texas at Dallas
800 W. Campbell Road, Richardson, TX 75080
{ffliu,leroy,yangl}@hlt.utdallas.edu
This paper systematically exploited various lexical features
for opinion analysis on blog data using a statistical learning framework. Our experimental results using the TREC
Blog track data show that all the features we explored effectively represent opinion expressions, and different classiﬁcation strategies have a signiﬁcant impact on opinion classiﬁcation performance. We also present results when combining opinion analysis with the retrieval component for the task
of retrieving relevant and opinionated blogs. Compared with
the best results in the TREC evaluation, our system achieves
reasonable performance, but does not rely on much human
knowledge or deep level linguistic analysis.
Introduction
Opinion analysis1 has drawn much attention in natural language processing community.
There are many previous
studies on sentiment analysis in some speciﬁc domains such
as movie and other product reviews , as well as cross-domain combination (Li & Zong
Compared with other online resources, blogs are
more ﬂexible in their content and styles, which gives rise to
new challenges in analyzing their opinion. Although opinion
analysis on blogs has been greatly advanced by the annual
Text REtrieval Conference (TREC) since 2006, performance
is still far from perfect.
Many previous studies have been done on correct identiﬁcation of the sentiment carrier using different levels of granularity, such as word, phrase, or sentence-level. Early research from suggested
that adjectives are important indicators of sentiment orientation. explored
using adverb and adjective combinations to evaluate the polarity degree. and built
their sentiment vocabularies according to co-occurrences
Copyright c⃝2009, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
1In literature, this is sometimes also called sentiment analysis.
of candidate terms and hand-picked sentiment seed words.
 presented a classiﬁerbased system to identify phrase-level contextual sentiment
polarity using a lexicon containing 8,000 single subjectivity
words. proposed a pattern learning
technique based on pre-deﬁned syntactic forms that proved
to be useful for opinion detection.
In this paper we investigate opinion analysis in the context
of the TREC Blog Track, and use a statistical classiﬁcation
approach for this problem. Our goal is to systematically explore various lexical features that are derived from both statistical analysis and heuristic knowledge. In addition, we examine the effect of different classiﬁcation settings. Our experimental results show that the features we explored prove
to be very helpful to improve both the classiﬁcation accuracy
and the MAP score in the TREC framework. Our system
obtains comparable results with the best one in the TREC
evaluation, but our system does not require a large opinion
vocabulary or performing deep level linguistic analysis such
as parsing.
TREC Blog Track and System Overview
This paper is focused on two tasks in the Blog track of TREC
2008: 2 opinion ﬁnding task and polarized opinion ﬁnding
task. Both can be considered as a ranking task based on
whether a blog is opinionated and whether it is relevant to
the given query. For the polarized task, positive opinionated
and negative opinionated blog posts should be ranked separately. The data used in this track is the Blog06 data .
Our system of ﬁnding opinionated blogs contains four parts:
preprocessing, topic retrieval, opinion analysis, and reranking based on their combination. We preprocessed the
original permalink documents in the Blog collection by removing noisy html tags and non-English blogs. This paper
focuses only on the opinion analysis module, for which we
use a statistical classiﬁcation approach. In 2008, for the 50
test topics, TREC provided ﬁve different baseline retrieval
results (1000 blogs for each topic) for participants to use
2 
Proceedings of the Third International ICWSM Conference 
in the opinion ﬁnding and polarized opinion ﬁnding task.
We use these TREC provided baselines as input in this paper. For each topic relevant blog, a posterior probability is
assigned according to the classiﬁer output, indicating how
likely it is opinionated.
Since our ultimate goal is to rank those blogs higher that
are more relevant and more opinionated, we believe it is
reasonable to conduct opinion analysis using only the topicrelevant part of a blog rather than the entire blog text. Therefore, we ﬁrst split a blog into sentences3, and used Lemur
toolkit4 to retrieve the top ﬁve relevant sentences corresponding to the topic. For each retrieved sentence, we also
extracted its preceding and following sentences. Thus for
each blog, we used a maximum 15 sentences to perform
classiﬁcation.
Let Opi, Pos, Neg be the probability of being opinionated,
positive, and negative respectively, and Rel for being relevant from the retrieval module. We rerank all the blogs using
a linear interpolation between the opinionated measurement
and the relevance score from topic retrieval as follows:
Final = λ ∗Rel + (1 −λ) ∗Opi (Pos or Neg)
where λ is a parameter used to adjust the balance between
being relevant and opinionated.
Features Used for Blog Opinion Analysis
Lexical Features
These include n-gram of words and part-of-speech (POS)
tags. In the following description, we use wi and pi to represent a word and its POS tag.
• Unigram features: This feature only carries information
of an individual word, including a combination of word
identify with its POS tag, for example, wi, wipi.
• Bigram and trigram of words and POS tags: These features are expected to capture the phrasal level feature and
some syntactic patterns of opinion expressions. Examples
of these features are: wi−1wi, wi−1wiwi+1, pi−1wipi+1,
wi−1piwi+1, pi−1pipi+1.
Sentiment Scores Based on Distributional
Association Among Sentimental Expressions
The following steps describe how we extract these features.
(A) Generate sentiment terms
We started with a small set of sentiment seed terms that
we think are context independent and are good indicators of
opinions, e.g., good, excellent, bad, terrible. Then we automatically identify adjectives that have a high co-occurrence
3This was done using the “mxterminator” sentence boundary
detection toolkit, developed by Adwait Ratnaparkhi.
4 
with these sentimental words based on a collection of reviews.5
For a reliable estimation, we used a large cooccurring frequency threshold of ten in a context window
of length three. Then a native English speaker manually examined the generated list of sentiment terms and kept 50
positive sentimental terms and 50 negative ones, for example, delicious, glorious, problematic, stupid.
(B) Calculate MI score for adjectives
We compute the MI scores between each of the sentimental terms we compiled above and any adjective in our blog
training data. This is used as a measurement for the polarity
strength (positive and negative) of an adjective. The positive
score for an adjective wi is obtained as follows:
C(wi, t, win)
where S+ is the set of positive sentiment terms with size of
N; C(wi, t, win) is the frequency that wi co-occurs with
a sentiment term t within a contextual window size of win
(ﬁve in our system). Similarly we calculate a word’s negative score using the negative sentiment terms.
(C) Compute sentiment score features
Finally, we calculate the sentiment score for each sentence
by simply adding the corresponding MI scores of all the adjectives in this sentence. Based on that, the following features are derived.
• Mean of sentence sentiment scores for positive and negative respectively.
• Mean of the difference between positive and negative
scores among all the sentences.
• Mean of the ratio of positive and negative scores among
all the sentences.
Polarized Features
We also explore the polarized features by combining the sentiment terms’ polarization tags with their neighboring words
and part-of-speech tags. We expect this to represent more
opinion indicative patterns. For example, “good” becomes
“POS” (positive), and polarized features include trigrams
such as wi−1POSwi+1, pi−1POSpi+1.
Experiments
Classiﬁcation Setting
In the TREC reference data, there are four opinion tags:
“1” denotes non-opinionated, “2” negative opinionated, “3”
mixed opinionated, and “4” positive opinionated.
5This corpus comprises of movie reviews from , custom reviews from , and
some hotel reviews.
training data, the percentages of these four classes are
42.17%, 17.28%, 18.24%, and 22.31% for tag “1”, “2”, “3”
and “4” respectively.
For the opinion ﬁnding task, we compare the following two
classiﬁcation paradigms (binary vs. 4-way).
• Binary classiﬁcation: All the instances with tags “2,3,4”
can be grouped together as positive class, and tag “1” corresponds to negative class.
• 4-way classiﬁcation: Obviously, we can simply train a 4way classiﬁer based on the four tags, and then we assign
blogs labeled with “2,3,4” hypotheses as opinionated.
For the polarized opinion ﬁnding, we evaluate three classiﬁcation strategies:
• One stage with 4-way classiﬁcation (1S+4W): A 4-way
classiﬁer was trained to distinguish blogs as no-opinion,
negative opinion, mixed opinion, and positive opinion.
Then based on the classiﬁer’s hypothesis, we can generate a positive and negative ranked list respectively with
the corresponding posterior probabilities.
• Two-stage with successive binary classiﬁcation and 3way classiﬁcation (2S+B+3W): The ﬁrst stage was simply a binary classiﬁcation for opinion ﬁnding. Then in
the second step, a 3-way classiﬁer trained using blog
instances with “2,3,4” tags determines the polarity tag
for the opinionated blogs generated from the ﬁrst stage.
Blogs classiﬁed as “2” and “4” are selected for the ﬁnal
negative and positive lists.
• One stage with 3-way classiﬁcation (1S+3W): A 3-way
classiﬁer trained to distinguish tags “2,3,4” is applied directly to all the retrieved relevant blogs, generating the
positive and negative lists.
In addition, considering that some blogs classiﬁed as mixed
polarity (tag 3) might also belong to the positive or negative
ones, we select the ones labeled as “mixed” tag with high
posterior probabilities for positive tags and add them to the
end of the existing positive ranked list in the order of the
posterior probability, until we reach 400 blogs (400 is an
empirical number we have chosen). The same rule is also
used for the negative ranked list.
We used the Maximum Entropy classiﬁer6 in our experiments where the Gaussian prior was 0.1, the number of iterations was 100, and the other parameters were the default
ones. We used two evaluation metrics: conventional classiﬁcation accuracy and mean average precision (MAP) in
Effects of Different Linguistic Features
In the following experiments, we used the topics in 2006 as
our training data, and topics in 2007 and 2008 as the development and test data respectively. Table 1 shows the 5-fold
6Available at 
toolkit.html
+ bigram and trigram
+ sentiment score
+ polarized features
Table 1: Effect of different features on opinion classiﬁcation
using 5-fold cross validation on the training data.
cross validation classiﬁcation accuracy on the training data
using different features.
We can see from Table 1 that the features we added gradually improve the accuracy for both 3-way and 4-way classiﬁcation. This is consistent with our expectation. However, a different pattern is observed for binary classiﬁcation.
Adding higher order n-gram features or sentiment score features does not help. In particular, there is a noticeable performance degradation after adding polarized features. We
will later use the development set and the MAP metric to
draw further conclusions on the effect of polarized features.
Comparison among Different Classiﬁcation
Settings in the TREC Framework
Next we compare different classiﬁcation strategies in the
TREC framework. Performance is measured using the MAP
score on the development data.
• Opinion ﬁnding task
Table 2 shows the results for opinion ﬁnding on the TREC
2007 topics using binary or 4-way classiﬁcation strategies, as well as with or without polarized features (PF).
We can see that the binary classiﬁcation framework outperforms the 4-way classiﬁcation. This may be because
that binary setting can help the classiﬁer better distinguish
opinionated features from non-opinionated features. We
also notice that adding the polarized features in binary
classiﬁcation yielded an improvement on the MAP score.
This is mostly likely due to the class distribution in the
data set, where the non-opinionated blogs are the majority
class. The MAP score is a more appropriate measurement
for this task with skewed data.
Classiﬁcation setting
Table 2: Opinion ﬁnding results on TREC Blog 2007 topics.
• Polarized task
Polarity results on the development data are shown in Table 3. The classiﬁcation strategy of one-stage with 3-way
classiﬁcation (1S+3W) obtained the best results.
one-stage approach can effectively avoid the error prop-
agation caused by using two stages. The 3-way classi-
ﬁcation could also alleviate the problem arisen from the
imbalanced training data as in 4-way classiﬁcation (the
non-opinionated class is the majority one). This yielded
a signiﬁcant gain in negative polarity MAP, from 0.07 to
0.16. Again, adding polarized features is useful in the polarity task, especially for the positive class. The expansion
using blogs with mixed tags based on their corresponding
posterior probability yielded signiﬁcant performance gain
(see the last two rows). Since there is a large number of
blogs with multiple polarity opinions, the classiﬁer often
predicts one instance as mixed class rather than positive or
negative class. Therefore a postprocessing step to handle
the mixed hypotheses helps improve performance.
Table 3: Results of polarity task on TREC Blog 2007 topics.
Performance on 2008 Test Data
We tested the opinion analysis system on the TREC 2008
data. The classiﬁers were trained using the reference annotation from the 2006 and 2007 data. For the opinion ﬁnding
task, the best MAP result our system achieved is 0.3844 (using baseline 4 provided by TREC), comparable with the best
result 0.4155 (using the same baseline input) in the TREC
evaluation . This is
reasonable because the best system used deep level features
such as parsing information, yet ours is only based on some
easily extracted lexical features. We also found that different baselines yielded quite different performance, suggesting that the quality of topic retrieval has a great impact on
overall opinion retrieval system. In addition, we examined
the performance curve while the weight λ changes when interpolating the opinion score and the relevance score. We
found that for three baselines (3, 4 and 5) a bigger λ is preferred, indicating a more dominant role of the topic retrieval
component.
This is not true for the other two baselines,
which we believe is because of the different quality of the
retrieval system as well as the appropriateness of the relevance scores.
For the polarity task in TREC 2008, our best result
(0.135/0.096) was obtained using baseline 4, which is
again slightly worse than the best TREC evaluation result
(0.161/0.148). We also observed that the performance for
polarized opinion ﬁnding is much worse than only opinion
ﬁnding. It suggests that the polarity task is more challenging, and for some blogs with mixed opinions, it is difﬁcult
to determine whether the overall opinionated orientation is
positive or negative, even for human subjects.
Conclusion and Future Work
In the context of TREC Blog track, we have examined various lexical features for opinion ﬁnding and polarized opinion ﬁnding tasks. In addition, we compared different classiﬁcation settings. For opinion ﬁnding (whether a blog is
opinionated or not), we found that adding more features
does not improve classiﬁcation performance based on accuracy metric; however, using all the features that we investigated proved to be useful according to the MAP scores, for
both opinion ﬁnding and polarized task. Our experiments
also show that different classiﬁcation settings signiﬁcantly
impacted the system performance for the two tasks. The
best system result in TREC 2008 is slightly better than ours;
however, our approach is much more simple and does not
need much human knowledge to create a large opinion vocabulary or perform deep linguistic analysis such as parsing.
One of our future work is to investigate the characteristics of
blogs and incorporate more effective features to better identify opinions in blogs. We also plan to ﬁnd a better approach
to determine the polarity of a blog, especially for those containing mixed opinions.