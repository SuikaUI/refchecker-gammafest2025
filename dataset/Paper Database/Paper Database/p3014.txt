It’s All in the Name: Mitigating Gender Bias with Name-Based
Counterfactual Data Substitution
Rowan Hall Maudslay1
Hila Gonen2
Ryan Cotterell1
Simone Teufel1
1 Department of Computer Science and Technology, University of Cambridge
2 Department of Computer Science, Bar-Ilan University
{rh635,rdc42,sht25}@cam.ac.uk 
This paper treats gender bias latent in word embeddings.
Previous mitigation attempts rely
on the operationalisation of gender bias as a
projection over a linear subspace.
An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by
swapping all inherently-gendered words in the
copy. We perform an empirical comparison of
these approaches on the English Gigaword and
Wikipedia, and ﬁnd that whilst both successfully reduce direct bias and perform well in
tasks which quantify embedding quality, CDA
variants outperform projection-based methods
at the task of drawing non-biased gender analogies by an average of 19% across both corpora.
We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant
of CDA in which potentially biased text is randomly substituted to avoid duplication, and
the Names Intervention, a novel name-pairing
technique that vastly increases the number of
words being treated. CDA/S with the Names
Intervention is the only approach which is able
to mitigate indirect gender bias: following debiasing, previously biased words are signiﬁcantly less clustered according to gender (cluster purity is reduced by 49%), thus improving
on the state-of-the-art for bias mitigation.
Introduction
Gender bias describes an inherent prejudice against
a gender, captured both by individuals and larger
social systems.
Word embeddings, a popular
machine-learnt semantic space, have been shown
to retain gender bias present in corpora used to
train them . This results in
gender-stereotypical vector analogies `a la Mikolov
et al. , such as man:computer programmer
:: woman:homemaker , and
such bias has been shown to materialise in a variety
of downstream tasks, e.g. coreference resolution
 .
By operationalising gender bias in word embeddings as a linear subspace, Bolukbasi et al. 
are able to debias with simple techniques from linear algebra. Their method successfully mitigates
direct bias: man is no longer more similar to computer programmer in vector space than woman.
However, the structure of gender bias in vector
space remains largely intact, and the new vectors
still evince indirect bias: associations which result
from gender bias between not explicitly gendered
words, for example a possible association between
football and business resulting from their mutual
association with explicitly masculine words . In this paper we continue
the work of Gonen and Goldberg, and show that another paradigm for gender bias mitigation proposed
by Lu et al. , Counterfactual Data Augmentation (CDA), is also unable to mitigate indirect
bias. We also show, using a new test we describe
(non-biased gender analogies), that WED might
be removing too much gender information, casting
further doubt on its operationalisation of gender
bias as a linear subspace.
To improve CDA we make two proposals. The
ﬁrst, Counterfactual Data Substitution (CDS), is
designed to avoid text duplication in favour of
substitution. The second, the Names Intervention,
is a method which can be applied to either CDA or
CDS, and treats bias inherent in ﬁrst names. It does
so using a novel name pairing strategy that accounts
for both name frequency and gender-speciﬁcity.
Using our improvements, the clusters of the most
biased words exhibit a reduction of cluster purity
by an average of 49% across both corpora following treatment, thereby offering a partial solution to
the problem of indirect bias as formalised by Gonen and Goldberg . Additionally, although
one could expect that the debiased embeddings
might suffer performance losses in computational
linguistic tasks, our embeddings remain useful
for at least two such tasks, word similarity and
sentiment classiﬁcation .
Related Work
The measurement and mitigation of gender bias relies on the chosen operationalisation of gender bias.
As a direct consequence, how researchers choose to
operationalise bias determines both the techniques
at one’s disposal to mitigate the bias, as well as the
yardstick by which success is determined.
Word Embedding Debiasing
One popular method for the mitigation of gender
bias, introduced by Bolukbasi et al. , measures the genderedness of words by the extent to
which they point in a gender direction. Suppose
we embed our words into Rd. The fundamental
assumption is that there exists a linear subspace
B ⇢Rd that contains (most of) the gender bias
in the space of word embeddings. (Note that B
is a direction when it is a single vector.) We term
this assumption the gender subspace hypothesis.
Thus, by basic linear algebra, we may decompose
any word vector v 2 Rd as the sum of the projections onto the bias subspace and its complement:
v = vB + v?B. The (implicit) operationalisation
of gender bias under this hypothesis is, then, the
magnitiude of the bias vector ||vB||2.
To capture B, Bolukbasi et al. ﬁrst construct N sets Di, each of which contains a pair
of words that differ in their gender but that are
otherwise semantically equivalent (using a prede-
ﬁned set of gender-deﬁnitional pairs). For example,
{man, woman} would be one set and {husband,
wife} would be another. They then compute the
average empirical covariance matrix
(~w −µi)(~w −µi)>
where µi is the mean embedding of the words in
Di, then B is taken to be the space spanned by the
top k eigenvectors of C associated with the largest
eigenvalues. Bolukbasi et al. set k = 1, and thus
deﬁne a gender direction.
Using this operalisation of gender bias, Bolukbasi et al.
go on to provide a linear-algebraic
method (Word Embedding Debiasing, WED, originally “hard debiasing”) to remove gender bias in
two phases: ﬁrst, for non-gendered words, the gender direction is removed (“neutralised”). Second,
pairs of gendered words such as mother and father
are made equidistant to all non-gendered words
(“equalised”). Crucially, under the gender subspace
All words captured by an embedding (3M)
Gender-speciﬁc words (6449)
Gender-neutral words (⇠3M)
Equalise pairs (52)
Deﬁnitional pairs (10)
breastfeed
monastery–convent
grandfather–grandmother
caregiving
Figure 1: Word sets used by WED with examples
hypothesis, it is only necessary to identify the subspace B as it is possible to perfectly remove the
bias under this operationalisation using tools from
numerical linear algebra.
The method uses three sets of words or word
pairs: 10 deﬁnitional pairs (used to deﬁne the gender direction), 218 gender-speciﬁc seed words (expanded to a larger set using a linear classiﬁer, the
compliment of which is neutralised in the ﬁrst step),
and 52 equalise pairs (equalised in the second step).
The relationships among these sets are illustrated
in Figure 1; for instance, gender-neutral words are
deﬁned as all words in an embedding that are not
gender-speciﬁc.
Bolukbasi et al. ﬁnd that this method results in a
68% reduction of stereotypical analogies as identi-
ﬁed by human judges. However, bias is removed
only insofar as the operationalisation allows. In
a comprehensive analysis, Gonen and Goldberg
 show that the original structure of bias in
the WED embedding space remains intact.
Counterfactual Data Augmentation
As an alternative to WED, Lu et al. propose Counterfactual Data Augmentation (CDA),
in which a text transformation designed to invert
bias is performed on a text corpus, the result of
which is then appended to the original, to form a
new bias-mitigated corpus used for training embeddings. Several interventions are proposed: in the
simplest, occurrences of words in 124 gendered
word pairs are swapped. For example, ‘the woman
cleaned the kitchen’ would (counterfactually) become ‘the man cleaned the kitchen’ as man–woman
is on the list. Both versions would then together be
used in embedding training, in effect neutralising
the man–woman bias.
The grammar intervention, Lu et al.’s improved
intervention, uses coreference information to veto
swapping gender words when they corefer to
a proper noun.1
This avoids Elizabeth ...she
. . . queen being changed to, for instance, Elizabeth
...he ...king. It also uses POS information to
avoid ungrammaticality related to the ambiguity of
her between personal pronoun and possessive determiner. In the context, ‘her teacher was proud of
her’, this results in the correct sentence ‘his teacher
was proud of him’.
Improvements to CDA
We prefer the philosophy of CDA over WED as it
makes fewer assumptions about the operationalisation of the bias it is meant to mitigate.
Counterfactual Data Substitution
The duplication of text which lies at the heart of
CDA will produce debiased corpora with peculiar
statistical properties unlike those of naturally occurring text. Almost all observed word frequencies
will be even, with a notable jump from 2 directly to
0, and a type–token ratio far lower than predicted
by Heaps’ Law for text of this length. The precise
effect this will have on the resulting embedding
space is hard to predict, but we assume that it is
preferable not to violate the fundamental assumptions of the algorithms used to create embeddings.
As such, we propose to apply substitutions probabilistically (with 0.5 probability), which results
in a non-duplicated counterfactual training corpus,
a method we call Counterfactual Data Substitution (CDS). Substitutions are performed on a perdocument basis in order to maintain grammaticality and discourse coherence. This simple change
should have advantages in terms of naturalness of
text and processing efﬁciency, as well as theoretical
foundation.
The Names Intervention
Our main technical contribution in this paper is to
provide a method for better counterfactual augmentation, which is based on bipartite-graph matching
of names. Instead of Lu et. al’s solution of
not treating words which corefer to proper nouns
in order to maintain grammaticality, we propose an
explicit treatment of ﬁrst names. This is because
we note that as a result of not swapping the gender
of words which corefer with proper nouns, CDA
1We interpret Lu et al.’s phrase “cluster” to mean
“coreference chain”.
Figure 2: Frequency and gender-speciﬁcity of names in
the SSA dataset
could in fact reinforce certain biases instead of mitigate them. Consider the sentence ‘Tom . . . He is a
successful and powerful executive’. Since he and
Tom corefer, the counterfactual corpus copy will
not replace he with she in this instance, and as the
method involves a duplication of text, this would result in a stronger, not weaker, association between
he and gender-stereotypic concepts present like executive. Even under CDS, this would still mean
that biased associations are left untreated (albeit
at least not reinforced). Treating names should in
contrast effect a real neutralisation of bias, with
the added bonus that grammaticality is maintained
without the need for coreference resolution.
The United States Social Security Administration (SSA) dataset contains a list of all ﬁrst names
from Social Security card applications for births
in the United States after 1879, along with their
gender.2 Figure 2 plots a few example names according to their male and female occurrences, and
shows that names have varying degrees of genderspeciﬁcity.3
We ﬁxedly associate pairs of names for swapping, thus expanding Lu et al.’s short list of gender pairs vastly. Clearly both name frequency and
the degree of gender-speciﬁcity are relevant to this
bipartite matching. If only frequency were considered, a more gender-neutral name (e.g. Taylor)
could be paired with a very gender-speciﬁc name
2 
background.html
3The dotted line represents gender-neutrality, and more
frequent names are located further from the origin.
(e.g. John), which would negate the gender intervention in many cases (namely whenever a male occurrence of Taylor is transformed into John, which
would also result in incorrect pronouns, if present).
If, on the other hand, only the degree of genderspeciﬁcity were considered, we would see frequent
names (like James) being paired with far less frequent names (like Sybil), which would distort the
overall frequency distribution of names. This might
also result in the retention of a gender signal: for
instance, swapping a highly frequent male name
with a rare female name might simply make the
rare female name behave as a new link between
masculine contexts (instead of the original male
name), as it rarely appears in female contexts.
Figure 3 shows a plot of various names’ number of primary gender4 occurances against their
secondary gender occurrences, with red dots for
primary-male and blue crosses for primary-female
names.5 The problem of ﬁnding name-pairs thus
decomposes into a Euclidean-distance bipartite
matching problem, which can be solved using the
Hungarian method . We compute
pairs for the most frequent 2500 names of each gender in the SSA dataset. There is also the problem
that many names are also common nouns (e.g. Amber, Rose, or Mark), which we solve using Named
Entity Recognition.
Experimental Setup
We compare eight variations of the mitigation
CDA is our reimplementation of Lu
et al.’s na¨ıve intervention, gCDA uses their
grammar intervention, and nCDA uses our new
Names Intervention. gCDS and nCDS are variants
of the grammar and Names Intervention using CDS.
WED40 is our reimplementation of Bolukbasi
et al.’s method, which (like the original)
uses a single component to deﬁne the gender
subspace, accounting for > 40% of variance. As
this is much lower than in the original paper (where
it was 60%, reproduced in Figure 4), we deﬁne a
second space, WED70, which uses a 2D subspace
accounting for > 70% of variance. To test whether
WED proﬁts from additional names, we use the
5000 paired names in the names gazetteer as
4Deﬁned as its most frequently occurring gender.
5The hatched area demarcates an area of the graph where
no names can exist: if any name did then its primary and
secondary gender would be reversed and it would belong to
the alternate set.
Figure 3: Bipartite matching of names by frequency
and gender-speciﬁcity
additional equalise pairs (nWED70).6 As control,
we also evaluate the unmitigated space (none).
We perform an empirical comparison of these
bias mitigation techniques on two corpora, the Annotated English Gigaword 
and Wikipedia. Wikipedia is of particular interest,
since though its Neutral Point of View (NPOV)
policy7 predicates that all content should be presented without bias, women are nonetheless less
likely to be deemed “notable” than men of equal
stature , and there are
differences in the choice of language used to describe them . We use the annotation native
to the Annotated English Gigaword, and process
Wikipedia with CoreNLP (statistical coreference;
bidirectional tagger). Embeddings are created using Word2Vec8. We use the original complex lexical input (gender-word pairs and the like) for each
algorithm as we assume that this beneﬁts each algorithm most. Expanding the set of gender-speciﬁc
words for WED (following Bolukbasi et al., using
a linear classiﬁer) on Gigaword resulted in 2141
such words, 7146 for Wikipedia.9
6We use the 70% variant as preliminary experimentation
showed that it was superior to WED40.
7 
Wikipedia:Neutral_point_of_view
8A CBOW model was trained over ﬁve epochs to produce
300 dimensional embeddings. Words were lowercased, punctuation other than underscores and hyphens removed, and
tokens with fewer than ten occurrences were discarded.
9We modify or remove some phrases from the training data
not included in the vocabulary of our embeddings.
Figure 4: Variance explained by the top Principal Components of the deﬁnitional word pairs (left) and random
unit vectors (right)
In our experiments, we test the degree to which
the spaces are successful at mitigating direct and indirect bias, as well as the degree to which they can
still be used in two NLP tasks standardly performed
with embeddings, word similarity and sentiment
classiﬁcation. We also introduce one further, novel
task, which is designed to quantify how well the
embedding spaces capture an understanding of
gender using non-biased analogies. Our evaluation
matrix and methodology is expanded below.
Direct bias
Caliskan et al. introduce the
Word Embedding Association Test (WEAT), which
provides results analogous to earlier psychological
work by Greenwald et al. by measuring the
difference in relative similarity between two sets
of target words X and Y and two sets of attribute
words A and B. We compute Cohen’s d (a measure
of the difference in relative similarity of the word
sets within each embedding; higher is more biased),
and a one-sided p-value which indicates whether
the bias detected by WEAT within each embedding
is signiﬁcant (the best outcome being that no such
bias is detectable). We do this for three tests proposed by Nosek et al. which measure the
strength of various gender stereotypes: art–maths,
arts–sciences, and careers–family.10
Indirect bias
To demonstrate indirect gender
bias we adapt a pair of methods proposed by Gonen and Goldberg . First, we test whether
the most-biased words prior to bias mitigation remain clustered following bias mitigation. To do
this, we deﬁne a new subspace, ~btest, using the
23 word pairs used in the Google Analogy family test subset following
Bolukbasi et al.’s method, and determine
10In the careers–family test the gender dimension is expressed by female and male ﬁrst names, unlike in the other
sets, where pronouns and typical gendered words are used.
the 1000 most biased words in each corpus (the 500
words most similar to~btest and −~btest) in the unmitigated embedding. For each debiased embedding
we then project these words into 2D space with
tSNE , compute
clusters with k-means, and calculate the clusters’ Vmeasure . Low
values of cluster purity indicate that biased words
are less clustered following bias mitigation.
Second, we test whether a classiﬁer can be
trained to reclassify the gender of debiased words.
If it succeeds, this would indicate that biasinformation still remains in the embedding. We
trained an RBF-kernel SVM classiﬁer on a random sample of 1000 out of the 5000 most biased
words from each corpus using~btest (500 from each
gender), then report the classiﬁer’s accuracy when
reclassifying the remaining 4000 words.
Word similarity
The quality of a space is traditionally measured by how well it replicates human
judgements of word similarity. The SimLex-999
dataset provides a ground-truth
measure of similarity produced by 500 native English speakers.11 Similarity scores in an embedding
are computed as the cosine angle between wordvector pairs, and Spearman correlation between
embedding and human judgements are reported.
We measure correlative signiﬁcance at ↵= 0.01.
classiﬁcation
Mikolov , we use a standard sentiment classiﬁcation task to quantify the downstream performance of the embedding spaces when they are used
as a pretrained word embedding input to Doc2Vec on the Stanford Large
Movie Review dataset. The classiﬁcation is performed by an SVM classiﬁer using the document
embeddings as features, trained on 40,000 labelled
reviews and tested on the remaining 10,000 documents, reported as error percentage.
Non-biased gender analogies
When proposing WED, Bolukbasi et al. use human
raters to class gender-analogies as either biased
(woman:housewife :: man:shopkeeper) or appropriate (woman:grandmother :: man::grandfather),
and postulate that whilst biased analogies are
undesirable, appropriate ones should remain. Our
new analogy test uses the 506 analogies in the fam-
11It explicitly quantiﬁes similarity rather than association
or relatedness; pairs of entities like coffee and cup have a low
Arts–Sciences
Career–Family
< 10−3 1.74 < 10−4
1.79 < 10−4
< 10−2 1.77 < 10−4
1.45 < 10−3
< 10−2 1.78 < 10−4
1.45 < 10−3
1.24 < 10−2
1.15 < 10−2
< 10−3 1.88 < 10−4
< 10−4 1.87 < 10−4
< 10−3 1.84 < 10−4
< 10−4 1.65 < 10−4
< 10−3 1.87 < 10−4
< 10−3 1.65 < 10−4
< 10−2 1.81 < 10−4
< 10−2 1.67 < 10−3
Nosek et al.
< 10−24 0.72 < 10−2
Table 1: Direct bias results
ily analogy subset of the Google Analogy Test set
 to deﬁne many such appropriate analogies that should hold even in a debiased
environment, such as boy:girl :: nephew:niece.12
We use a proportional pair-based analogy test,
which measures each embedding’s performance
when drawing a fourth word to complete each
analogy, and report error percentage.
Direct bias
Table 1 presents the d scores and
WEAT one-tailed p-values, which indicate whether
the difference in samples means between targets
X and Y and attributes A and B is signiﬁcant.
We also compute a two-tailed p-value to determine
whether the difference between the various sets is
signiﬁcant.13
On Wikipedia, nWED70 outperforms every
other method (p < 0.01), and even at ↵= 0.1
bias was undetectable. In all CDA/S variants, the
Names Intervention performs signiﬁcantly better
than other intervention strategies (average d for
nCDS across all tests 0.95 vs. 1.39 for the best nonnames CDA/S variants). Excluding the Wikipedia
careers–family test (in which the CDA and CDS
12The entire Google Analogy Test set contains 19,544 analogies, which are usually reported as a single result or as a pair
of semantic and syntactic results.
13Throughout this paper, we test signiﬁcance in the differences between the embeddings with a two-tailed Monte
Carlo permutation test at signiﬁcance interval ↵= 0.01 with
r = 10, 000 permutations.
Figure 5: Most biased cluster purity results
variants are indistinguishable at ↵= 0.01), the
CDS variants are numerically better than their CDA
counterparts in 80% of the test cases, although
many of these differences are not signiﬁcant.
Generally, we notice a trend of WED reducing
direct gender bias slightly better than CDA/S. Impressively, WED even successfully reduces bias in
the careers–family test, where gender information
is captured by names, which were not in WED’s
gender-equalise word-pair list for treatment.
Indirect bias
Figure 5 shows the V-measures of
the clusters of the most biased words in Wikipedia
for each embedding. Gigaword patterns similarly
(see appendix). Figure 6 shows example tSNE projections for the Gigaword embeddings method, Lu et al.’s 
method, and our new names variant). On both corpora, the new nCDA and nCDS techniques have signiﬁcantly lower purity of biased-word cluster than
all other evaluated mitigation techniques (0.420
for nCDS on Gigaword, which corresponds to a
reduction of purity by 58% compared to the unmitigated embedding, and 0.609 (39%) on Wikipedia).
nWED70’s V-Measure is signiﬁcantly higher than
either of the other Names variants (reduction of
11% on Gigaword, only 1% on Wikipedia), suggesting that the success of nCDS and nCDA is not
merely due to their larger list of gender-words.
Figure 7 shows the results of the second test of
indirect bias, and reports the accuracy of a classi-
ﬁer trained to reclassify previously gender biased
words on the Wikipedia embeddings (Gigaword
patterns similarly).14 These results reinforce the
ﬁnding of the clustering experiment: once again,
14The 95% conﬁdence interval is calculated by a Wilson
score interval, i.e., assuming a normal distribution.
Figure 6: Clustering of biased words (Gigaword)
Figure 7: Reclassiﬁcation of most biased words results
nCDS outperforms all other methods signiﬁcantly
on both corpora (p < 0.01), although it should
be noted that the successful reclassiﬁcation rate
remains relatively high (e.g. 88.9% on Wikipedia).
We note that nullifying indirect bias associations
entirely is not necessarily the goal of debiasing,
since some of these may result from causal links in
the domain. For example, whilst associations between man and engineer and between man and car
are each stereotypic (and thus could be considered
examples of direct bias), an association between
engineer and car might well have little to do with
gender bias, and so should not be mitigated.
Word similarity
Table 2 reports the SimLex-999
Spearman rank-order correlation coefﬁcients rs
(all are signiﬁcant, p < 0.01). Surprisingly, the
WED40 and 70 methods outperform the unmitigated embedding, although the difference in result
is small (0.386 and 0.395 vs. 0.385 on Gigaword,
Table 2: Word similarity Results
0.371 and 0.367 vs. 0.368 on Wikipedia). nWED70,
on the other hand, performs worse than the unmitigated embedding (0.384 vs. 0.385 on Gigaword,
0.367 vs. 0.368 on Wikipedia). CDA and CDS
methods do not match the quality of the unmitigated space, but once again the difference is small.
It should be noted that since SimLex-999 was produced by human raters, it will reﬂect the human
biases these methods were designed to remove, so
worse performance might result from successful
bias mitigation.
Sentiment classiﬁcation
Figure 8 shows the
sentiment classiﬁcation error rates for Wikipedia
(Gigaword patterns similarly). Results are somewhat inconclusive. While WED70 signiﬁcantly improves the performance of the sentiment classiﬁer
from the unmitigated embedding on both corpora
(p < 0.05), the improvement is small (never more
than 1.1%). On both corpora, nothing outperforms
WED70 or the Names Intervention variants.
Figure 8: Sentiment classiﬁcation results
Figure 9: Non-biased gender analogy results
Non-biased gender analogies
Figure 9 shows
the error rates for non-biased gender analogies
for Wikipedia. CDA and CDS are numerically
better than the unmitigated embeddings (an effect
which is always signiﬁcant on Gigaword, shown
in the appendices, but sometimes insigniﬁcant on
Wikipedia). The WED variants, on the other hand,
perform signiﬁcantly worse than the unmitigated
sets on both corpora (27.1 vs. 9.3% for the best
WED variant on Gigaword; 18.8 vs. 8.7% on Wikipedia). WED thus seems to remove too much gender information, whilst CDA and CDS create an
improved space, perhaps because they reduce the
effect of stereotypical associations which were previously used incorrectly when drawing analogies.
Conclusion
We have replicated two state-of-the-art bias mitigation techniques, WED and CDA, on two large
corpora, Wikipedia and the English Gigaword. In
our empirical comparison, we found that although
both methods mitigate direct gender bias and maintain the interpretability of the space, WED failed
to maintain a robust representation of gender (the
best variants had an error rate of 23% average when
drawing non-biased analogies, suggesting that too
much gender information was removed). A new
variant of CDA we propose (the Names Intervention) is the only to successfully mitigate indirect
gender bias: following its application, previously
biased words are signiﬁcantly less clustered according to gender, with an average of 49% reduction
in cluster purity when clustering the most biased
words. We also proposed Counterfactual Data Substitution, which generally performed better than the
CDA equivalents, was notably quicker to compute
(as Word2Vec is linear in corpus size), and in theory allows for multiple intervention layers without
a corpus becoming exponentially large.
A fundamental limitation of all the methods compared is their reliance on predeﬁned lists of gender words, in particular of pairs. Lu et al.’s pairs
of manager::manageress and murderer::murderess
may be counterproductive, as their augmentation
method perpetuates a male reading of manager,
which has become gender-neutral over time. Other
issues arise from differences in spelling (e.g. mum
vs. mom) and morphology (e.g. his vs. her and
hers). Biologically-rooted terms like breastfeed
or uterus do not lend themselves to pairing either.
The strict use of pairings also imposes a gender
binary, and as a result non-binary identities are all
but ignored in the bias mitigation literature.
Future work could extend the Names Intervention to names from other languages beyond the USbased gazetteer used here. Our method only allows
for there to be an equal number of male and female
names, but if this were not the case one ought to explore the possibility of a many-to-one mapping, or
perhaps a probablistic approach (though difﬁculties
would be encountered sampling simultaneously
from two distributions, frequency and genderspeciﬁcity). A mapping between nicknames (not
covered by administrative sources) and formal
names could be learned from a corpus for even
wider coverage, possibly via the intermediary of
coreference chains. Finally, given that names have
been used in psychological literature as a proxy
for race (e.g. Greenwald et al.), the Names Intervention could also be used to mitigate racial biases
(something which, to the authors’ best knowledge,
has never been attempted), but ﬁnding pairings
could prove problematic. It is important that other
work looks into operationalising bias beyond the
subspace deﬁnition proposed by Bolukbasi et al.
 , as it is becoming increasingly evident that
gender bias is not linear in embedding space.
Acknowledgments
We would like to thank Francisco Vargas Palomo
for pointing out a few typos in the proofs App. A
post publication.