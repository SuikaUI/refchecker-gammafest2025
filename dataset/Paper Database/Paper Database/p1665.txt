Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1967–1972,
Lisbon, Portugal, 17-21 September 2015. c⃝2015 Association for Computational Linguistics.
LCSTS: A Large Scale Chinese Short Text Summarization Dataset
Baotian Hu
Qingcai Chen
Fangze Zhu
Intelligent Computing Research Center
Harbin Institute of Technology, Shenzhen Graduate School
{baotianchina,qingcai.chen, zhufangze123}@gmail.com
Automatic text summarization is widely
regarded as the highly difﬁcult problem,
partially because of the lack of large
text summarization data set. Due to the
great challenge of constructing the large
scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging
website Sina Weibo, which is released to
the public1. This corpus consists of over
2 million real Chinese short texts with
short summaries given by the author of
each text. We also manually tagged the
relevance of 10,666 short summaries with
their corresponding short texts. Based on
the corpus, we introduce recurrent neural
network for the summary generation and
achieve promising results, which not only
shows the usefulness of the proposed corpus for short text summarization research,
but also provides a baseline for further research on this topic.
Introduction
Nowadays, individuals or organizations can easily share or post information to the public on the
social network.
Take the popular Chinese microblogging website (Sina Weibo) as an example,
the People’s Daily, one of the media in China,
posts more than tens of weibos (analogous to
tweets) each day. Most of these weibos are wellwritten and highly informative because of the text
length limitation (less than140 Chinese characters). Such data is regarded as naturally annotated
web resources . If we can mine these
high-quality data from these naturally annotated
web resources, it will be beneﬁcial to the research
that has been hampered by the lack of data.
1 
Figure 1: A Weibo Posted by People’s Daily.
In the Natural Language Processing (NLP)
community, automatic text summarization is a hot
and difﬁcult task. A good summarization system
should understand the whole text and re-organize
the information to generate coherent, informative,
and signiﬁcantly short summaries which convey
important information of the original text , . Most of traditional abstractive summarization methods divide
the process into two phrases .
First, key textual elements are extracted from the
original text by using unsupervised methods or linguistic knowledge. And then, unclear extracted
components are rewritten or paraphrased to produce a concise summary of the original text by
using linguistic rules or language generation techniques. Although extensive researches have been
done, the linguistic quality of abstractive summary is still far from satisfactory. Recently, deep
learning methods have shown potential abilities
to learn representation and generate language from large scale data
by utilizing GPUs. Many researchers realize that
we are closer to generate abstractive summarizations by using the deep learning methods. However, the publicly available and high-quality large
scale summarization data set is still very rare and
not easy to be constructed manually. For example, the popular document summarization dataset
DUC2, TAC3 and TREC4 have only hundreds of
human written English text summarizations. The
problem is even worse for Chinese. In this pa-
2 
3 
4 
filtering,,
cleaning,and
extracting
Social,Media
Chosen,Users
Users,Collection
Figure 2: Diagram of the process for creating the dataset.
per, we take one step back and focus on constructing LCSTS, the Large-scale Chinese Short Text
Summarization dataset by utilizing the naturally
annotated web resources on Sina Weibo. Figure 1
shows one weibo posted by the People’s Daily. In
order to convey the import information to the public quickly, it also writes a very informative and
short summary (in the blue circle) of the news.
Our goal is to mine a large scale, high-quality short
text summarization dataset from these texts.
This paper makes the following contributions:
(1) We introduce a large scale Chinese short text
summarization dataset. To our knowledge, it is
the largest one to date; (2) We provide standard
splits for the dataset into large scale training set
and human labeled test set which will be easier for
benchmarking the related methods; (3) We explore
the properties of the dataset and sample 10,666
instances for manually checking and scoring the
quality of the dataset; (4) We perform recurrent
neural network based encoder-decoder method on
the dataset to generate summary and get promising results, which can be used as one baseline of
Related Work
Our work is related to recent works on automatic
text summarization and natural language processing based on naturally annotated web resources,
which are brieﬂy introduced as follows.
Automatic Text Summarization in some form
has been studied since 1950. Since then, most researches are related to extractive summarizations
by analyzing the organization of the words in the
document ; Since it needs labeled data sets for supervised machine learning methods and labeling
dataset is very intensive, some researches focused
on the unsupervised methods .
The scale of existing data sets are usually very
small (most of them are less than 1000).
example, DUC2002 dataset contains 567 documents and each document is provided with two
100-words human summaries. Our work is also
related to the headline generation, which is a task
to generate one sentence of the text it entitles.
Colmenares et.al construct a 1.3 million ﬁnancial
news headline dataset written in English for headline generation . However, the data set is not publicly available.
Naturally Annotated Web Resources based
Natural Language Processing is proposed by
Sun . Naturally Annotated Web Resources is the data generated by users for communicative purposes such as web pages, blogs and
microblogs.
We can mine knowledge or useful
data from these raw data by using marks generated
by users unintentionally. Jure et.al track 1.6 million mainstream media sites and blogs and mine a
set of novel and persistent temporal patterns in the
news cycle . Sepandar et.al
use the users’ naturally annotated pattern ‘we feel’
and ‘i feel’ to extract the ‘Feeling’ sentence collection which is used to collect the world’s emotions.
In this work, we use the naturally annotated resources to construct the large scale Chinese short
text summarization data to facilitate the research
on text summarization.
Data Collection
A lot of popular Chinese media and organizations
have created accounts on the Sina Weibo. They
use their accounts to post news and information.
These accounts are veriﬁed on the Weibo and labeled by a blue ‘V’. In order to guarantee the quality of the crawled text, we only crawl the veriﬁed
organizations’ weibos which are more likely to be
clean, formal and informative. There are a lot of
human intervention required in each step. The process of the data collection is shown as Figure 2 and
summarized as follows:
1) We ﬁrst collect 50 very popular organization users as seeds. They come from the domains
of politic, economic, military, movies, game and
etc, such as People’s Daily, the Economic Observe
press, the Ministry of National Defense and etc. 2)
We then crawl fusers followed by these seed users
and ﬁlter them by using human written rules such
as the user must be blue veriﬁed, the number of
followers is more than 1 million and etc. 3) We
use the chosen users and text crawler to crawl their
weibos. 4) we ﬁlter, clean and extract (short text,
summary) pairs. About 100 rules are used to extract high quality pairs. These rules are concluded
by 5 peoples via carefully investigating of the raw
text. We also remove those paris, whose short text
length is too short (less than 80 characters) and
length of summaries is out of .
Data Properties
The dataset consists of three parts shown as Table 1 and the length distributions of texts are
shown as Figure 3.
Part I is the main content of LCSTS that contains 2,400,591 (short text, summary) pairs. These
pairs can be used to train supervised learning
model for summary generation.
Part II contains the 10,666 human labeled
(short text, summary) pairs with the score ranges
from 1 to 5 that indicates the relevance between
the short text and the corresponding summary. ‘1’
denotes “ the least relevant ” and ‘5’ denotes “the
most relevant”. For annotating this part, we recruit
5 volunteers, each pair is only labeled by one annotator. These pairs are randomly sampled from
Part I and are used to analysize the distribution of
pairs in the Part I. Figure 4 illustrates examples of
different scores. From the examples we can see
that pairs scored by 3, 4 or 5 are very relevant to
the corresponding summaries. These summaries
are highly informative, concise and signiﬁcantly
short compared to original text. We can also see
that many words in the summary do not appear
in the original text, which indicates the signiﬁcant
difference of our dataset from sentence compression datasets. The summaries of pairs scored by
1 or 2 are highly abstractive and relatively hard to
conclude the summaries from the short text. They
are more likely to be headlines or comments instead of summaries. The statistics show that the
percent of score 1 and 2 is less than 20% of the
Figure 3: Box plot of lengths for short text(ST),
text(Segmented
summary(SUM) and segmented summary(Segmented
SUM). The red line denotes the median, and the
edges of the box the quartiles.
data, which can be ﬁltered by using trained classi-
Part III contains 1,106 pairs. For this part, 3
annotators label the same 2000 texts and we extract the text with common scores. This part is
independent from Part I and Part II. In this work,
we use pairs scored by 3, 4 and 5 of this part as the
test set for short text summary generation task.
Number of Pairs
Human Score 1
Human Score 2
Human Score 3
Human Score 4
Human Score 5
Number of Pairs
Human Score 1
Human Score 2
Human Score 3
Human Score 4
Human Score 5
Table 1: Data Statistics
Experiment
Recently, recurrent neural network (RNN) have
shown powerful abilities on speech recognition and automatic dialog
response . However, there is
rare research on the automatic text summarization
by using deep models. In this section, we use RNN
as encoder and decoder to generate the summary
of short text. We use the Part I as the training set
Short Text: -
&$E BH+"
3#@561!)(C
>?561!)>?561%0-I2A8
1G ,A8-=.;<-:1
Mingzhong Chen, the Chief Secretary of the Water Devision of the Ministry of
Water Resources, revealed today at a press conference, according to the just<
completed assessment of water resources management system,
provinces are closed to the red line indicator, some provinces are over the red
line indicator. In some places over the red lineIt will enforce regional
approval restrictions on some water projects, implement strictly water
resources assessment andthe approval of water licensing.
Summarization: C
0-56!) -I29G
Some provinces exceeds the red line indicator of annual water using, some
water project will be. limitedapproved
Human Score: 5
Short Text: <:758@E.
0(?>PC8</
=+-FGO2O#
58!& FGO2O3
!19H= 9H
Groupons’ sales on mobile terminals are below 30 percent. User’s preference of
shopping through PCs can not be changed in the short term. In the future
Chinese O2O catering market, mobile terminals will become the strategic
development direction. And also, it will become offDline driving from onDline
driving. The first and second tier cities are facing growth difficulties. However,
O2O market in the third and fourth tier cities contains opportunities.
Summarization: 58&FGO2O3'2$")
The mobile terminals will become catering’s strategic development direction.
Human Score: 4
Short Text: 7,4
10347/"*53/0
0.87%9#6,-?714,3/0 $
0!<>10% &
In July, 1002cities’ average newly2built house prices is 10347 yuan per square,
which rose 0.87%. It rises for the 14th consecutive month. Among them,
Guangzhou, Beijing, Shenzhen, Nanjing rise more than 10%. Dawei Zhang, from
Centaline Property Agency, said that
because the first and second2tier city
gathers too many resources, the price of house is likelyto rise and hard to fall.
Summarization: 4'3/14?0
1002cities’ house prices gain “14th consecutive rising”, the first and second2tier
cities rise more.
Human Score: 3
Short Text: A:/312009
5+6 Q16.95%H
D"A79L8%4("K B'R!3J:7
O*0G("KB!3J:/N-E?7;
According to China’s Ministry of Commerce, China’s actually utilized foreign
capital in Julyfell sharply about 16.95% to 7.81 billion dollars, comparing to last
year. Analysis of the outside world believe that it is related to the recent official
intensive antitrust investigation. Danyang Shen responded, “It can not be linked
to the antitrust investigation of foreign investment, or do other unfounded
association”
Summarization:
China‘s Ministry of Commerce respond to antitrust investigation: Several cases
will not scare foreigninvestors away.
Human Score: 1
Figure 4: Five examples of different scores.
and the subset of Part III, which is scored by 3, 4
and 5, as test set.
Two approaches are used to preprocess the data:
1) character-based method, we take the Chinese
character as input, which will reduce the vocabulary size to 4,000. 2) word-based method, the
(y1,%y2,%...,%yn)
RNN%Decoder
Figure 5: The graphical depiction of RNN encoder
and decoder framework without context.
(y1,/y2,/...,/yn)
RNN/Decoder
Context/Generator
Figure 6: The graphical depiction of the RNN encoder and decoder framework with context.
text is segmented into Chinese words by using
jieba5. The vocabulary is limited to 50,000. We
adopt two deep architectures, 1) The local context is not used during decoding.
We use the
RNN as encoder and it’s last hidden state as the
input of decoder, as shown in Figure 5; 2) The
context is used during decoding, following , we use the combination of
all the hidden states of encoder as input of the
decoder, as shown in Figure 6.
For the RNN,
we adopt the gated recurrent unit (GRU) which is
proposed by and has been
proved comparable to LSTM .
All the parameters (including the embeddings) of
the two architectures are randomly initialized and
ADADELTA is used to update the
learning rate. After the model is trained, the beam
search is used to generate the best summaries in
the process of decoding and the size of beam is set
to 10 in our experiment.
5 
RNN context
The experiment result:
“Word” and
“Char” denote the word-based and characterbased input respectively.
Short Text: 9230+H09;:?5F!'
2014'2015)
8#GA)&3J, 71=
>28  I<%.
On September 3, the Geneva'based World Economic Forum released “ The
Global Competitiveness Report 2014'2015 ”. Switzerland topped the list for six
consecutive years , becoming the world‘s most competitive country. Singapore
and the United States are in the second and third place respectively. China is in
the 28th place, ranking highest among the BRICcountries.
Human: 7=
.D3 &2 8  &I<%J
the Global competitiveness ranking list, China is in the 28th place, the highest
among BRIC countries.
RNN+Char: 8#+@92300 :+@J4 *(8#+E2012)
>>$C8#-
7>$%5000B
It is not a fluent sentence and can not be translated. There are only some
segments related to original text.
RNN+Word: 7=
:8#&7>3>1>1>1>1
Global competitiveness ranking: Switzerland ranks 3rd place, China 1st place, 1st
place, China 1st place, Switzerland1st place, first
RNN+Context+Char: 7=
In the Global competitiveness ranking list, China is in the 28th place which is
highest among the BRICcountries.
RNN+Context+Word: 7=
>28I<%.
“The Global Competitiveness Report” is released. China is in the 28th place,
rank highest among the BRICcountries proposed by , which have been proved strongly correlated
with human evaluations.
ROUGE-1, ROUGE-
2 and ROUGE-L are used.
All the models are
trained on the GPUs tesla M2090 for about one
week.Table 2 lists the experiment results. As we
can see in Figure 7, the summaries generated by
RNN with context are very close to human written
summaries, which indicates that if we feed enough
data to the RNN encoder and decoder, it may generate summary almost from scratch.
The results also show that the RNN with context outperforms RNN without context on both
character and word based input. This result indicates that the internal hidden states of the RNN
encoder can be combined to represent the context
of words in summary. And also the performances
of the character-based input outperform the wordbased input. As shown in Figure 8, the summary
generated by RNN with context by inputing the
character-based short text is relatively good, while
the the summary generated by RNN with context
Short Text: $ R<Q=20
1D&#+,M
GN7.4*F@ is one possible direction. The rare
word problem is also very important for the generation of the summaries, especially when the input
is word-based instead of character-based. It is also
a hot topic in the neural generative models such
as neural translation machine(NTM) , which can beneﬁt to this task. We also plan
to construct a large document summarization data
set by using naturally annotated web resources.
Acknowledgments
Natural Science Foundation of China:
Development
JCYJ20140417172417105,
JCYJ20140508161040764
JCYJ20140627163809422.
We thank to Baolin
Peng, Lin Ma, Li Yu and the anonymous reviewers
for their insightful comments.