IMPROVING END-TO-END SPEECH RECOGNITION WITH POLICY LEARNING
Yingbo Zhou, Caiming Xiong, Richard Socher
Salesforce Research
Connectionist temporal classiﬁcation (CTC) is widely used
for maximum likelihood learning in end-to-end speech recognition models. However, there is usually a disparity between
the negative maximum likelihood and the performance metric used in speech recognition, e.g., word error rate (WER).
This results in a mismatch between the objective function and
metric during training. We show that the above problem can
be mitigated by jointly training with maximum likelihood and
policy gradient. In particular, with policy learning we are able
to directly optimize on the (otherwise non-differentiable) performance metric. We show that joint training improves relative performance by 4% to 13% for our end-to-end model as
compared to the same model learned through maximum likelihood. The model achieves 5.53% WER on Wall Street Journal dataset, and 5.42% and 14.70% on Librispeech test-clean
and test-other set, respectively.
Index Terms— end-to-end speech recognition, LVCSR,
policy gradient, deep neural networks
1. INTRODUCTION
Deep neural networks are the basis for some of the most accurate speech recognition systems in research and production
 . Neural network based acoustic models are commonly used as a sub-component in a Gaussian mixture model
(GMM) and hidden Markov model (HMM) based hybrid system. Alignment is necessary to train the acoustic model, and a
two-stage (i.e. alignment and frame prediction) training process is required for a typical hybrid system. A drawback of
such setting is that there is a disconnect between the acoustic model training and the ﬁnal objective, which makes the
system level optimization difﬁcult.
The end-to-end neural network based speech models bypass this two-stage training process by directly maximizing
the likelihood of the data. More recently, the end-to-end models have also shown promising results on various datasets . While the end-to-end models are commonly trained
with maximum likelihood, the ﬁnal performance metric for a
speech recognition system is typically word error rate (WER)
or character error rate (CER). This results a mismatch between the objective that is optimized and the evaluation metric. In an ideal setting the model should be trained to optimize
the ﬁnal metric. However, since the metrics are commonly
discrete and non-differentiable, it is very difﬁcult to optimize
them in practice.
Lately, reinforcement learning (RL) has shown to be effective on improving performance for problems that have
non-differentiable metric through policy gradient. Promising results are obtained in machine translation , image
captioning , summarization , etc.. In particular, REINFORCE algorithm enables one to estimate the
gradient of the expected reward by sampling from the model.
It has also been applied for online speech recognition .
Graves and Jaitly propose expected transcription loss that
can be used to optimize on WER. However, it is more computationally expensive. For example, for a sequence of length
T with vocabulary size K, at least T samples and K metric
calculations are required for estimating the loss.
We show that jointly training end-to-end models with self
critical sequence training (SCST) and maximum likelihood improves performance signiﬁcantly. SCST is also efﬁcient during training, as only one sampling process and two
metric calculations are necessary. Our model achieves 5.53%
WER on Wall Street Journal dataset, and 5.42% and 14.70%
WER on Librispeech test-clean and test-other sets.
2. MODEL STRUCTURE
The end-to-end model structure used in this work is very similar to that of Deep Speech 2 (DS2) . It is mainly composed
of 1) a stack of convolution layers in the front-end for feature extraction, and 2) a stack of recurrent layers for sequence
modeling. The structure of recurrent layers is the same as in
DS2, and we illustrate the modiﬁcations in convolution layers
in this section.
We choose to use time and frequency convolution (i.e. 2-
D convolution) as the front-end of our model, since it is able
to model both the temporal transitions and spectral variations
in speech utterances. We use depth-wise separable convolution for all the convolution layers, due to its computational efﬁciency and performance advantage . The depthwise separable convolution is implemented by ﬁrst convolving over the input channel-wise, and then convolve with 1×1
ﬁlters with the desired number of output channels. Stride size
only inﬂuences the channel-wise convolution; the following
1 × 1 convolutions always have stride size of one. More prearXiv:1712.07101v1 [cs.CL] 19 Dec 2017
Conv Layer
Channel-wise
Conv Layer
1x1 Conv Layer
Batch Norm
Bidirectional
Non-linearity
Fig. 1. Model architecture of our end-to-end speech model.
Different colored blocks represent different layers as shown
on the right, the lightning symbol indicates dropout happens
between the two layers.
cisely, let x ∈RF ×T ×D, c ∈RW ×H×D and w ∈RD×N
denote an input sample, the channel-wise convolution and the
1 × 1 convolution weights respectively. The depth-wise separable convolution with D input channels and N output channels performs the following operations:
s(i, j, d) =
x(f, t, d)c(i −f, j −t, d)
o(i, j, n) =
s(i, j, k)w(k, n)
where d ∈{1, . . . , D} and n ∈{1, 2, . . . , N}, s is the
channel-wise convolution result, and o is the result from
depth-wise separable convolution.
In addition, we add a
residual connection between the input and the layer
output for the depth-wise separable convolution to facilitate
Our model is composed of six convolution layers – one
standard convolution layer that has larger ﬁlter size, followed
by ﬁve residual convolution blocks . The convolution features are then fed to four bidirectional gated recurrent units
(GRU) layers, and ﬁnally two fully connected layers that
make the ﬁnal per-character prediction. The full end-to-end
model structure is illustrated in Fig. 1.
3. MODEL OBJECTIVE
3.1. Maximum Likelihood Training
Connectionist temporal classiﬁcation (CTC) is a popular
method for doing maximum likelihood training on sequence
labeling tasks, where the alignment information is not provided in the label. The alignment is not required since CTC
marginalizes over all possible alignments, and maximizes the
likelihood P(y|x). It achieves this by augmenting the original label set L to set Ω= L ∪{blank} with an additional
blank symbol. A mapping M is then deﬁned to map a length
T sequence of label ΩT to L≤T by removing all blanks and
repeated symbols along the path. The likelihood can then be
recovered by
where x, y and y′ denote an input example of length T, the
corresponding label of length ≤T and one of the augmented
label with length T.
3.2. Policy Learning
The log likelihood reﬂects the log probability of getting the
whole transcription completely correct. What it ignores are
the probabilities of the incorrect transcriptions.
words, all incorrect transcriptions are equally bad, which is
clearly not the case. Furthermore, the performance metrics
typically aim to reﬂect the plausibility of incorrect predictions. For example, WER penalizes less for transcription that
has less edit distance to the ground truth label. This results in
a disparity between the optimization objective of the model
and the (commonly discrete) evaluation criteria. This mismatch is mainly attributed to the inability to directly optimize
the criteria.
One way to remedy this mismatch is to view the above
problem in the policy learning framework. In this framework,
we can view our model as an agent and the training samples as
the environment. The parameters of the model θ deﬁnes a policy Pθ(y|x), the model interacts with the environment by following this policy. The agent then performs an action based
on its current state, in which case the action is the generated
transcription and the state is the model hidden representation
of the data. It then observes a reward that is deﬁned from
the evaluation metric calculated on the current sample (e.g.
1−WER for the current transcription). The goal of learning
is to obtain a policy that minimizes the negative expected reward:
Lp(θ) = −Eys∼Pθ(y|x)[r(ys)]
where r(·) denotes the reward function. Gradient of eq. 3 can
be obtained through REINFORCE as
∇θLp(θ) = −Eys∼Pθ(y|x)[r(ys)∇θ log Pθ(ys|x)]
≈−r(ys)∇θ log Pθ(ys|x)
Eq. 5 shows the Monte Carlo approximation of the gradient with a single example, which is a common practice when
training model with stochastic gradient descent.
The policy gradient obtained from eq. 5 is often of high
variance, and the training can get unstable. To reduce the
variance, Rennie et al. proposed self-critical sequence
training (SCST). In SCST, the policy gradient is computed
with a baseline, which is the greedy output from the model.
Formally, the policy gradient is calculated using
∇θLp(θ) = −Eys∼Pθ(y|x)[(r(ys) −r(ˆy)) ∇θ log Pθ(ys|x)]
≈−(r(ys) −r(ˆy)) ∇θ log Pθ(ys|x)
where ˆy is the greedy decoding output from the model for the
input sample x.
3.3. Multi-objective Policy Learning
A potential problem with policy gradient methods (including
SCST) is that the learning can be slow and unstable at the beginning of training. This is because it is unlikely for the model
to have reasonable output at that stage, which leads to implausible samples with low rewards. Learning will be slow in
case of small learning rate, and unstable otherwise. One way
to remedy this problem is to incorporate maximum likelihood
objective along with policy gradient, since in maximum likelihood the probability is evaluated on the ground truth targets,
and hence will get large gradients when the model output is
incorrect. This leads to the following objective for training
our end-to-end speech model:
L(θ) = −log Pθ(y|x) + λLscst(θ)
Lscst(θ) = −{g(ys, y) −g(ˆy, y)} log Pθ(ys|x)
where g(·, ·) is the reward function and λ ∈(0, +∞) is the
coefﬁcient that controls the contribution from SCST. In our
case we choose g(·, y) = 1 −max(1, WER(·, y)). Training
with eq. 8 is also efﬁcient, since both sampling and greedy
decoding is cheap. The only place that might be computationally more demanding is the reward calculation, however, we
only need to compute it twice per batch of examples, which
adds only a minimal overhead.
4. EXPERIMENTS
We evaluate the proposed objective by performing experiments on the Wall Street Journal (WSJ) and LibriSpeech 
datasets. The input to the model is a spectrogram computed
with a 20ms window and 10ms step size. We ﬁrst normalize
each spectrogram to have zero mean and unit variance. In addition, we also normalize each feature to have zero mean and
unit variance based on the training set statistics. No further
preprocessing is done after these two steps of normalization.
We denote the size of the convolution layer by the tuple
(C, F, T, SF, ST), where C, F, T, SF, and ST denote number of
channels, ﬁlter size in frequency dimension, ﬁlter size in time
dimension, stride in frequency dimension and stride in time
dimension respectively.
We have one convolutional layer
with size (32,41,11,2,2), and ﬁve residual convolution blocks
of size (32,7,3,1,1), (32,5,3,1,1), (32,3,3,1,1), (64,3,3,2,1),
(64,3,3,1,1) respectively. Following the convolutional layers
we have 4 layers of bidirectional GRU RNNs with 1024 hidden units per direction per layer. Finally, we have one fully
connected hidden layer of size 1024 followed by the output
layer. Batch normalization is applied to all layers’ preactivations to facilitate training. Dropout is applied to
inputs of each layer, and for layers that take sequential input
(i.e. the convolution and recurrent layers) we use the dropout
variant proposed by Gal and Ghahramani . The convolutional and fully connected layers are initialized uniformly
following He et al. . The recurrent layer weights are
initialized with a uniform distribution U(−1/32, 1/32). The
model is trained in an end-to-end fashion to minimize the
mixed objective as illustrated in eq. 8. We use mini-batch
stochastic gradient descent with batch size 64, learning rate
0.1, and with Nesterov momentum 0.95. The learning rate is
reduced by half whenever the validation loss has plateaued.
We set λ = 0.1 at the beginning of training, and increase it to
1 after the model has converged (i.e. the validation loss stops
improving). The gradient is clipped to have a maximum
ℓ2 norm of 1. For regularization, we use ℓ2 weight decay of
10−5 for all parameters. Additionally, we apply dropout for
inputs of each layer (see Fig. 1). The dropout probabilities
are set as 0.1 for data, 0.2 for all convolution layers, and 0.3
for all recurrent and fully connected layers. Furthermore, we
also augment the audio training data through random perturbations of tempo, pitch, volume, temporal alignment, along
with adding random noise.
4.1. Effect of Policy Learning
To study the effectiveness of our multi-objective policy learning, we perform experiments on both datasets with various
settings. The ﬁrst set of experiments was carried out on the
WSJ corpus. We use the standard si284 set for training, dev93
for validation and eval92 for test evaluation. We use the provided language model and report the result in the 20K closed
vocabulary setting with beam search. The beam width is set
to 100. Results are shown in table 1. Both policy gradient
methods improve results over baseline. In particular, the use
of SCST results in 13.8% relative performance improvement
on the eval92 set over the baseline.
On LibriSpeech dataset, the model is trained using all 960
Policy (eq. 5)
Policy (eq. 7)
Table 1. Performance from WSJ dataset. Baseline denotes
model trained without CTC only; policy indicates model
trained using the multi-objective policy learning. Equation in
parenthesis indicates the way used to obtain policy gradient.
test-clean
test-other
Table 2. Performance from LibriSpeech dataset. Policy denotes model trained with multi-objective shown in eq. 8.
Hannun et al. 
Bahdanau et al. 
Graves and Jaitly 
Wu et al. 
Miao et al. 
Chorowski and Jaitly 
Amodei et al. *
Ours (LibriSpeech)
Table 3. Comparative results with other end-to-end methods
on WSJ eval92 dataset. LibriSpeech denotes model trained
using LibriSpeech dataset only, and test on WSJ. Amodei et
al. used more training data.
hours of training data. Both dev-clean and dev-other are used
for validation and results are reported in table 2. The provided
4-gram language model is used for ﬁnal beam search decoding. The beam width is also set to 100 for decoding. Overall,
a relative ≈4% performance improvement over the baseline
is observed.
test-clean
test-other
Collobert et al. 
Amodei et al. *
Table 4. Word error rate comparison with other end-to-end
methods on LibriSpeech dataset. Amodei et al. used more
training data.
4.2. Comparison with Other Methods
We also compare our performance with other end-to-end
Comparative results from WSJ and LibriSpeech
dataset are illustrated in tables 3 and 4 respectively.
model achieved competitive performance with other methods on both datasets. In particular, with the help of policy
learning we achieved similar results as Amodei et al.
on LibriSpeech without using additional data. To see if the
model generalizes, we also tested our LibriSpeech model on
the WSJ dataset. The result is signiﬁcantly better than the
model trained on WSJ data (see table 3), which suggests
that the end-to-end models beneﬁt more when more data is
available.
5. CONCLUSION
In this work, we try to close the gap between the maximum
likelihood training objective and the ﬁnal performance metric
for end-to-end speech models. We show this gap can be reduced by using the policy gradient method along with the negative log-likelihood. In particular, we apply a multi-objective
training with SCST to reduce the expected negative reward
that is deﬁned by using the ﬁnal metric. The joint training is
computationally efﬁcient. We show that the joint training is
effective even with single sample approximation, which improves the relative performance on WSJ and LibriSpeech by
13% and 4% over the baseline.
6. REFERENCES
 G Hinton, L Deng, D Yu, G E Dahl, A Mohamed,
N Jaitly, A Senior, V Vanhoucke, P Nguyen, T Sainath,
et al., “Deep neural networks for acoustic modeling in
speech recognition: The shared views of four research
groups,” IEEE Signal Processing Magazine, vol. 29, no.
6, pp. 82–97, 2012.
 G Saon, HK J Kuo, S Rennie, and M Picheny, “The ibm
2015 english conversational telephone speech recognition system,” arXiv preprint arXiv:1505.05899, 2015.
 W Xiong, J Droppo, X Huang, F Seide, M Seltzer,
A Stolcke, D Yu, and G Zweig, “The microsoft 2016
conversational speech recognition system,” in Acoustics, Speech and Signal Processing (ICASSP), 2017
IEEE International Conference on. IEEE, 2017, pp.
5255–5259.
 A Graves and N Jaitly,
“Towards end-to-end speech
recognition with recurrent neural networks,” in ICML,
2014, pp. 1764–1772.
 Y Miao, M Gowayyed, and F Metze, “Eesen: End-toend speech recognition using deep rnn models and wfstbased decoding,” in ASRU. IEEE, 2015, pp. 167–174.
 D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai,
E. Battenberg, C. Case, J. Casper, B. Catanzaro,
Q. Cheng, G. Chen, et al., “Deep speech 2: End-to-end
speech recognition in english and mandarin,” in ICML,
2016, pp. 173–182.
 D Bahdanau, J Chorowski, D Serdyuk, P Brakel, and
Y Bengio, “End-to-end attention-based large vocabulary
speech recognition,” in ICASSP. IEEE, 2016, pp. 4945–
 M Ranzato, S Chopra, M Auli, and W Zaremba, “Sequence level training with recurrent neural networks,”
arXiv preprint arXiv:1511.06732, 2015.
 D Bahdanau, P Brakel, K Xu, A Goyal, R Lowe,
J Pineau, A Courville, and Y Bengio,
“An actorcritic algorithm for sequence prediction,” arXiv preprint
 
 S J Rennie, E Marcheret, Y Mroueh, J Ross, and V Goel,
“Self-critical sequence training for image captioning,”
arXiv preprint arXiv:1612.00563, 2016.
 R Paulus, C Xiong, and R Socher, “A deep reinforced
model for abstractive summarization,”
arXiv preprint
 
 R J Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine learning, vol. 8, no. 3-4, pp. 229–256, 1992.
 Y Luo, C Chiu, N Jaitly, and I Sutskever, “Learning
online alignments with continuous rewards policy gradient,” in ICASSP. IEEE, 2017, pp. 2801–2805.
 L Sifre and S Mallat, “Rotation, scaling and deformation invariant scattering for texture discrimination,” in
CVPR, 2013, pp. 1233–1240.
 F Chollet,
“Xception:
Deep learning with depthwise
convolutions,”
 
 K He, X Zhang, S Ren, and J Sun, “Deep residual learning for image recognition,” in CVPR, 2016, pp. 770–
 K Cho, Bart Van M, D Bahdanau, and Y Bengio, “On
the properties of neural machine translation: Encoderdecoder approaches,” arXiv preprint arXiv:1409.1259,
 A Graves, S Fern´andez, F Gomez, and J Schmidhuber,
“Connectionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks,”
in ICML. ACM, 2006, pp. 369–376.
 V Panayotov, G Chen, D Povey, and S Khudanpur, “Librispeech: an asr corpus based on public domain audio
books,” in ICASSP. IEEE, 2015, pp. 5206–5210.
 S Ioffe and C Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift.,” in ICML, Francis R. Bach and David M. Blei,
Eds. 2015, vol. 37 of JMLR Proceedings, pp. 448–456,
 N Srivastava, G Hinton, A Krizhevsky, I Sutskever, and
R Salakhutdinov, “Dropout: A simple way to prevent
neural networks from overﬁtting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958,
 Y Gal and Z Ghahramani, “A theoretically grounded
application of dropout in recurrent neural networks,” in
NIPS, 2016, pp. 1019–1027.
 K He, X Zhang, S Ren, and J Sun, “Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation,” in Proceedings of the IEEE international conference on computer vision, 2015, pp.
1026–1034.
 R Pascanu, T Mikolov, and Y Bengio, “On the difﬁculty
of training recurrent neural networks,” in ICML, 2013,
pp. 1310–1318.
 A Hannun, A Maas, D Jurafsky, and A Ng,
“Firstpass large vocabulary continuous speech recognition
using bi-directional recurrent dnns,”
arXiv preprint
 
 Y Wu, S Zhang, Y Zhang, Y Bengio, and R Salakhutdinov, “On multiplicative integration with recurrent neural
networks,” in NIPS, 2016, pp. 2856–2864.
 J Chorowski and N Jaitly,
“Towards better decoding
and language model integration in sequence to sequence
models,” arXiv preprint arXiv:1612.02695, 2016.
 R Collobert, C Puhrsch, and G Synnaeve, “Wav2letter:
an end-to-end convnet-based speech recognition system,” arXiv preprint arXiv:1609.03193, 2016.