Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition
Dan Claudiu Cires¸an1, 2,
Ueli Meier1, 2,
Luca Maria Gambardella1, 2,
J¨urgen Schmidhuber1, 2
1IDSIA, Galleria 2, 6928 Manno-Lugano, Switzerland.
2University of Lugano & SUPSI, Switzerland.
Keywords: NN (Neural Network) , MLP (Multilayer Perceptron), GPU (Graphics
Processing Unit), training set deformations, MNIST 1, BP (back-propagation).
Good old on-line back-propagation for plain multi-layer perceptrons yields a very
low 0.35% error rate on the famous MNIST handwritten digits benchmark. All we
need to achieve this best result so far are many hidden layers, many neurons per layer,
numerous deformed training images, and graphics cards to greatly speed up learning.
Introduction
Automatic handwriting recognition is of great academic and commercial interest. Current algorithms are already pretty good at learning to recognize handwritten digits. Post
1 
 
ofﬁces use them to sort letters; banks use them to read personal checks. MNIST is the most widely used benchmark for isolated handwritten digit recognition. More than a decade ago, artiﬁcial neural networks called Multilayer Perceptrons or
MLPs were among the ﬁrst classiﬁers tested on MNIST. Most had few layers or few artiﬁcial neurons (units) per layer
 , but apparently back then they were the biggest feasible MLPs,
trained when CPU cores were at least 20 times slower than today. A more recent MLP
with a single hidden layer of 800 units achieved 0.70% error .
However, more complex methods listed on the MNIST web page always seemed to
outperform MLPs, and the general trend went towards more and more complex variants
of Support Vector Machines or SVMs and combinations
of NNs and SVMs etc. Convolutional neural networks (CNNs)
achieved a record-breaking 0.40% error rate , using novel elastic
training image deformations. Recent methods pre-train each hidden CNN layer one by
one in an unsupervised fashion (this seems promising especially for small training sets),
then use supervised learning to achieve 0.39% error rate .
The biggest MLP so far also was pre-trained without supervision then piped its output into another classiﬁer to achieve an error of 1% without
domain-speciﬁc knowledge.
Are all these complexiﬁcations of plain MLPs really necessary? Can’t one simply
train really big plain MLPs on MNIST? Why is there no literature on this? One reason
is that at ﬁrst glance deep MLPs do not seem to work better than shallow networks
 . Training them is hard as back-propagated gradients quickly vanish exponentially in the number of layers , just like in the ﬁrst recurrent neural networks . Indeed, previous deep networks successfully trained with back-propagation
(BP) either had few free parameters due to weight-sharing or used unsupervised, layer-wise pre-training . But is it really true that deep BP-MLPs do not work at
all, or do they just need more training time? How to test this? Unfortunately, on-line
BP for hundreds/thousands of epochs on large MLPs may take weeks or months on
standard serial computers. But can’t one parallelize it? Well, on computer clusters this
is hard due to communication latencies between individual computers. Multi-threading
on a multi-core processor is not easy either. We may speed up BP using SSE (Streaming
Single Instruction, Multiple Data Extensions), either manually, or by setting appropriate
compiler ﬂags. The maximum theoretical speedup under single precision ﬂoating-point,
however, is four, which is not enough. And MNIST is large - its 60,000 images take
almost 50MB, too much to ﬁt in the L2/L3 cache of any current processor. This requires
to continually access data in considerably slower RAM. To summarize, currently it is
next to impossible to train big MLPs on CPUs.
We will show how to overcome all these problems by training large, deep MLPs on
graphics cards.
MNIST consists of two datasets, one for training (60,000 images) and one for testing
(10,000 images). Many studies divide the training set into two sets consisting of 50,000
images for training and 10,000 for validation. Our network is trained on slightly deformed images, continually generated in on-line fashion; hence we may use the whole
un-deformed training set for validation, without wasting training images. Pixel intensities of the original gray scale images range from 0 (background) to 255 (max foreground
intensity). 28 × 28 = 784 pixels per image get mapped to real values pixel intensity
in [−1.0, 1.0], and are fed into the NN input layer.
Architectures
We train 5 MLPs with 2 to 9 hidden layers and varying numbers of hidden units. Mostly
but not always the number of hidden units per layer decreases towards the output layer
(Table 1). There are 1.34 to 12.11 million free parameters (or weights, or synapses).
We use standard on-line BP , without
momentum, but with a variable learning rate that shrinks by a multiplicative constant after each epoch, from 10−3 down to 10−6. Weights are initialized with a uniform random
distribution in [−0.05, 0.05]. Each neuron’s activation function is a scaled hyperbolic
tangent: y(a) = A tanh Ba, where A = 1.7159 and B = 0.6666 .
Deforming images to get more training instances
So far, the best results on MNIST were obtained by deforming training images, thus
greatly increasing their number. This allows for training networks with many weights,
making them insensitive to in-class variability. We combine afﬁne (rotation, scaling
and horizontal shearing) and elastic deformations, characterized by the following realvalued parameters:
• σ and α: for elastic distortions emulating uncontrolled oscillations of hand muscles ;
• β: a random angle from [−β, +β] describes either rotation or horizontal shearing.
In case of shearing, tan β deﬁnes the ratio between horizontal displacement and
image height;
• γx, γy: for horizontal and vertical scaling, randomly selected from [1−γ/100, 1+
At the beginning of every epoch the entire MNIST training set gets deformed. Initial
experiments with small networks suggested the following deformation parameters: σ =
5.0 −6.0, α = 36.0 −38.0, γ = 15 −20. Since digits 1 and 7 are similar they get
rotated/sheared less (β = 7.5◦) than other digits (β = 15.0◦).
All simulations were performed on a computer with a Core2 Quad 9450 2.66GHz processor, 3GB of RAM, and a GTX280 graphics card. The GPU accelerates the deformation routine by a factor of 10 (only elastic deformations are GPU-optimized); the
forward propagation (FP) and BP routines are sped up by a factor of 40. Implementation details can be found in the Appendix. We pick the trained MLP with the lowest
validation error, and evaluate it on the MNIST test set. Results are summarized in
Most remarkably, the best network has an error rate of only 0.35% (35 out of 10,000
digits). This is signiﬁcantly better than the best previously published results, namely,
0.39% by Ranzato et al. and 0.40% by Simard et al. , both obtained by
more complex methods. The 35 misclassiﬁed digits are shown in Figure 1. Many of
them are ambiguous and/or uncharacteristic, with obviously missing parts or strange
strokes etc. Interestingly, the second guess of the network is correct for 30 out of the 35
misclassiﬁed digits.
The best test error of this MLP is even lower (0.32%) and may be viewed as the
maximum capacity of the network. Performance clearly proﬁts from adding hidden
layers and more units per layer. For example, network 5 has more but smaller hidden
layers than network 4 (Table 1).
Networks with up to 12 million weights can successfully be trained by plain gradient
descent to achieve test errors below 1% after 20-30 epochs in less than 2 hours of
training. How can networks with so many parameters generalize well on the unseen
test set? Answer: the continual deformations of the training set generate a virtually
inﬁnite supply of training examples, and the network rarely sees any training image
Table 1: Error rates on MNIST test set.
architecture
test error for
simulation
(number of neurons in each layer)
best validation [%]
1000, 500, 10
1500, 1000, 500, 10
2000, 1500, 1000, 500, 10
2500, 2000, 1500, 1000, 500, 10
9 × 1000, 10
Conclusion
In recent decades the amount of raw computing power per Euro has grown by a factor
of 100-1000 per decade. Our results show that this ongoing hardware progress may be
more important than advances in algorithms and software (although the future will belong to methods combining the best of both worlds). Current graphics cards (GPUs) are
already more than 40 times faster than standard microprocessors when it comes to training big and deep neural networks by the ancient algorithm, on-line back-propagation
(weight update rate up to 5 × 109/s, and more than 1015 per trained network). On
Figure 1: The 35 miss-classiﬁed digits of the best network from Table 1, together with
the two most likely predictions (bottom, from left to right) and the correct label according to MNIST (top, right).
the very competitive MNIST handwriting benchmark, single precision ﬂoating-point
GPU-based neural nets surpass all previously reported results, including those obtained
by much more complex methods involving specialized architectures, unsupervised pretraining, combinations of machine learning classiﬁers etc. Training sets of sufﬁcient
size are obtained by appropriately deforming images. Of course, the approach is not
limited to handwriting, and obviously holds great promise for many visual and other
pattern recognition problems.
Acknowledgments
Part of this work got started when Dan Cires¸an was a PhD student at University ”Politehnica” of Timis¸oara. He would like to thank his former PhD advisor, S¸tefan Holban,
for his guidance, and R˘azvan Mos¸incat for providing a CPU framework for MNIST.
This work was supported by Swiss CTI, Commission for Technology and Innovation,
Project n. 9688.1 IFF: Intelligent Fill in Form, and by Lifeware S.A. L.M.G. and J.S.
wrote a grant proposal for this work, acquired competitive funding for it, and supervised
it. C.D.C. wrote GPU-optimised code. C.D.C. and U.M. debugged it. C.D.C. designed
and performed the experiments. C.D.C., U.M., L.M.G. and J.S. wrote the paper.
Appendix - GPU implementation
Graphics Processing Unit
Until 2007 the only way to program a GPU was to translate the problem-solving algorithm into a set of graphical operations. Despite being hard to code and difﬁcult to
debug, several GPU-based NN implementations were developed when GPUs became
faster than CPUs. Two layer MLPs and CNNs have been previously implemented on GPUs. Although speedups were
relatively modest, these studies showed how GPUs can be used for machine learning.
More recent GPU-based CNNs trained in batch mode are two orders of magnitude faster
than CPU-based CNNs .
In 2007, NVIDIA developed the ﬁrst version of CUDA (Compute Uniﬁed Device
Architecture), a C-like general programming language. GPU speed and memory bandwidth are vastly superior to those of CPUs, and crucial for fast MLP implementations.
To fully understand our algorithm in terms of GPU / CUDA, please visit the NVIDIA
website . According to CUDA terminology, the CPU is called host and
the graphics card device or GPU.
Deformations
It takes 93 CPU seconds to deform the 60,000 MNIST training images, most of them
(87) for elastic distortions. Only the most time-consuming part of the latter – convolution with a gaussian kernel – is ported to the GPU. The MNIST
training set is split into 600 sequentially processed batches. MNIST digits are scaled
from the original 28 × 28 pixels to 29 × 29 pixels, to get a proper center, which simpli-
ﬁes convolution. An image grid has 290 × 290 cells, zero-padded to 300 × 300, thus
avoiding margin effects when applying a gaussian convolution kernel of size 21 × 21.
Our GPU program groups many threads into a block, where they share the same gaussian kernel and parts of the random ﬁeld. The blocks contain 21 (the kernel size) ×10
Listing 1: Convolution Kernel for elastic distortion.
void ConvolveField optimized(ﬂoat ∗randomﬁeld, int width, int height, ﬂoat ∗kernel, ﬂoat ∗outputﬁeld, ﬂoat elasticScale){
ﬂoat sum=0;
const int stride k=GET STRIDE(GAUSSIAN FIELD SIZE,pitch x>>2); //stride for gaussian kernel
ﬂoat K[GAUSSIAN FIELD SIZE][stride k];
//kernel (21 x 32 values)
ﬂoat R[GAUSSIAN FIELD SIZE+9][GAUSSIAN FIELD SIZE]; //random ﬁeld (30 x 21 values)
ﬂoat s [GAUSSIAN FIELD SIZE];
//partial sums (10 x 21 values)
int stride in=GET STRIDE(width,pitch x>>2);
//random ﬁeld stride as a multiple of 32
int stride out=GET STRIDE(width−GAUSSIAN FIELD SIZE+1,pitch x>>2); //output stride as a multiple of 32
//loading gaussian kernel into K (21 x 21 values)
K[ 0+threadIdx.y][threadIdx.x] = kernel[( 0+threadIdx.y)∗stride k + threadIdx.x];//rows 0..9
K[10+threadIdx.y][threadIdx.x] = kernel[(10+threadIdx.y)∗stride k + threadIdx.x];//rows 10..19
if(threadIdx.y==0)
K[20+threadIdx.y][threadIdx.x] = kernel[(20+threadIdx.y)∗stride k + threadIdx.x];//row 20
//loading randomﬁeld into R
//0..9 x 21 values
R[ 0+threadIdx.y][threadIdx.x] = randomﬁeld[(10∗blockIdx.y+ 0+threadIdx.y)∗stride in + blockIdx.x + threadIdx.x];
//10..19 x 21 values
R[10+threadIdx.y][threadIdx.x] = randomﬁeld[(10∗blockIdx.y+10+threadIdx.y)∗stride in + blockIdx.x + threadIdx.x];
//20..29 x 21 values
R[20+threadIdx.y][threadIdx.x] = randomﬁeld[(10∗blockIdx.y+20+threadIdx.y)∗stride in + blockIdx.x + threadIdx.x];
syncthreads(); //wait until everything is read into shared memory
//computing partial sums
#pragma unroll 21 //GAUSSIAN FIELD SIZE
for(int i=0;i<GAUSSIAN FIELD SIZE;i++)
sum += R[threadIdx.y + i][threadIdx.x] ∗K[i][threadIdx.x];
s[threadIdx.y][threadIdx.x]=sum;
syncthreads();
if(threadIdx.x==0){ //the ﬁrst column of threads compute the ﬁnal values of the convolutions
#pragma unroll 20//GAUSSIAN FIELD SIZE−1
for(int i=1;i<GAUSSIAN FIELD SIZE;i++) sum+=s[threadIdx.y][i];
outputﬁeld[(blockIdx.y∗10+threadIdx.y)∗stride out + blockIdx.x] = sum ∗elasticScale;
threads, each computing a vertical strip of the convolution operation (Listing 1).
Generating the elastic displacement ﬁeld takes only 3 seconds. Deforming the whole
training set is more than 10 times faster, taking 9 instead of the original 93 seconds. Further optimization would be possible by porting all deformations onto the GPU, and by
using the hardware’s interpolation capabilities to perform the ﬁnal bilinear interpolation. We omitted this since deformations are already pretty fast (deforming all images
of one epoch takes only 5-15 % of total computation time, depending on MLP size).
Training algorithm
We closely follow the standard BP algorithm , except that BP of deltas and weight updates are disentangled and performed sequentially. This allows for more parallelism within each routine.
Forward propagation
The algorithm is divided into two kernels. The weight matrix W is partitioned as illustrated in Figure 2.
connections
256 threads
32 threads
Figure 2: Forward propagation: a) mapping of kernel 1 grid onto the padded weight
matrix; b) mapping the kernel 2 grid onto the partial dot products matrix; c) output of
forward propagation.
Each block has 256 threads (Figure 2a), each computing a partial dot product of
32 component vectors. The dot products are stored in a temporary matrix (Figure 2b).
This kernel has a very high throughput: average memory bandwidth is 115GB/s. This
is possible because many relatively small blocks keep the GPU busy. Each block uses
shared memory for storing the previous layer activations, which are simultaneously
read by the ﬁrst 32 threads of each block and then used by all 256 threads. After
thread synchronization, the partial dot products are computed in parallel (Listing 2).
The number of instructions is kept to a minimum by pre-computing all common index
Listing 2: Forward propagation kernels.
void MLP FP reduction Kernel1(ﬂoat ∗prevLN, ﬂoat ∗W, ﬂoat ∗partialsum, unsigned int neurons, unsigned int prevneurons){
const int threads=256;
const int stride=GET STRIDE(neurons,pitch x>>2); //horizontal stride of W matrix
int X=blockIdx.x∗threads + threadIdx.x; //precomputing expressions
int Y=X+stride∗blockIdx.y;
int Z=blockIdx.y∗pitch y∗stride + X;
ﬂoat sum=0.0f;
ﬂoat output[pitch y];
if(blockIdx.y==0)
if(threadIdx.x==0) output =1.0f;
else if(threadIdx.x<pitch y) //there are only 32 values to read and 128 threads
output[threadIdx.x] = threadIdx.x−1<prevneurons ? prevLN[threadIdx.x−1] : 0.0f;
else if(threadIdx.x<pitch y) //there are only 32 values to read and 128 threads
output[threadIdx.x] = blockIdx.y∗pitch y+threadIdx.x−1<prevneurons ?
prevLN[blockIdx.y∗pitch y+threadIdx.x−1] : 0.0f;
syncthreads();
if(X<neurons){//compute partial sums
//#pragma unroll 32
int size=0;
if((blockIdx.y+1)∗pitch y>=prevneurons+1)
size = prevneurons + 1 −blockIdx.y∗pitch y;
else size=pitch y;
for (int ic=0; ic<size; ic++){
sum += output[ic] ∗W[Z];
Z+=stride;
partialsum[Y]=sum;
void MLP FP reduction Kernel2(ﬂoat ∗currLN, ﬂoat ∗partialsum, unsigned int neurons, unsigned int size){
ﬂoat sum=0.0f;
int idx = blockIdx.x∗(pitch x>>2) + threadIdx.x; //precomputed index
unsigned int stride = GET STRIDE(neurons,pitch x>>2); //stride for partialsum matrix
if(idx>=neurons)return;
//is this thread computing a true neuron?
for (int i=0; i<size; i++) sum += partialsum[i∗stride+idx]; //computing the ﬁnal dot product
currLN[idx] = SIGMOIDF(sum);
//applying activation
The thread grid (Figure 2b) has only one row of blocks consisting of warp threads,
since each thread has to compute a complete dot product (Figure 2c) and then pipe it
into the activation function. This kernel (Listing 2) is inefﬁcient for layers with fewer
than 1024 incoming connections per neuron, especially for the last layer which has only
ten neurons, one for each digit. That is, its grid will have only one block, occupying
only 3% of the GTX280 GPU.
Backward propagation
This is similar to FP, but we need W T for coalesced access. Instead of transposing the
matrix, the computations are performed on patches of data read from device memory into shared memory, similar to the optimized matrix transposition algorithm of
Ruetsch & Micikevicius . Shared memory access is much faster, without coalescing restrictions. Because we have to cope with layers of thousands of neurons,
back-propagating deltas uses a reduction method implemented in two kernels communicating partial results via global memory (Listing 3).
Kernel 1 The bi-dimensional grid is divided into blocks of warp (32) threads. The
kernel starts by reading a patch of 32 × 32 values from W. The stride of the shared
memory block is 33 (warp + 1), thus avoiding all bank conﬂicts and signiﬁcantly improving speed. Next, 32 input delta values are read and all memory locations that do
not correspond to real neurons (because of vertical striding) are zero-padded to avoid
branching in subsequent computations. The number of elements is ﬁxed to warp size,
and the computing loop is unrolled for further speedups. Before ﬁnishing, each thread
writes its own partial dot product to global memory.
This kernel completes BP of deltas by summing up partial deltas computed by the
previous kernel. It multiplies the ﬁnal result by the derivative of the activation function
applied to the current neuron’s state, and writes the new delta to global memory.
Weight updating
The algorithm (Listing 4) starts by reading the appropriate delta, and pre-computes
all repetitive expressions. Then the ﬁrst 16 threads read the states from global memory into shared memory. The “bias neuron” with constant activation 1.0 is dealt with
by conditional statements, which could be avoided through expressions containing the
conditions. Once threads are synchronized, each single thread updates 16 weights in a
ﬁxed unrolled loop.
Listing 3: Backpropagating deltas kernels.
void backPropagateDeltasFC s2 A(ﬂoat ∗indelta, ﬂoat ∗weights, unsigned int ncon, unsigned int nrneur, ﬂoat ∗partial){
const int px = pitch x>>2;
unsigned int stride x = GET STRIDE(nrneur,px);
unsigned int stride y = GET STRIDE(ncon,pitch y);
outd = 0.0;
idx = blockIdx.x∗px+threadIdx.x;
X = blockIdx.y∗pitch y∗stride x + idx;
Y = threadIdx.x;
ﬂoat w[32∗33];
//pitch y and px should be equal ! +1 to avoid bank conﬂict!
ﬂoat id[px];
//input delta
#pragma unroll 32 //read the weight patch in shared memory
for(int i=0;i<pitch y;i++){w[Y]=weights[X]; X+=stride x; Y+=33;}
//read the input delta patch in shared memory
if(idx>=nrneur) id[threadIdx.x]=0; //a fake input delta for inexistent indelta
else id[threadIdx.x]=indelta[idx];
syncthreads(); //not needed for block with warp number of threads: implicit synchronization
#pragma unroll 32 //compute partial results
for(int i=0;i<px;i++) outd+=w[threadIdx.x∗33+i]∗id[i];
//write out the partial results
partial[blockIdx.x∗stride y + blockIdx.y∗pitch y + threadIdx.x] = outd;
void backPropagateDeltasFC s2 B(ﬂoat ∗outdelta,ﬂoat ∗instates, unsigned int ncon, unsigned int nrneur, ﬂoat ∗partial){
int px=pitch x>>2;
unsigned int stride x = GET STRIDE(nrneur,px);
unsigned int stride y = GET STRIDE(ncon,pitch y);
ﬂoat outd = 0.0;
int size=stride x/px;
int idx=blockIdx.x∗pitch y+threadIdx.x;
if(idx==0); //true only for block and thread 0
for(int i=0;i<size;i++)
outd+=partial[i∗stride y + idx];
outdelta[idx−1] = outd ∗DSIGMOIDF(instates[idx−1]); //−1 BIAS ...
Listing 4: Weights adjustment kernel.
void adjustWeightsFC s1(ﬂoat ∗states,ﬂoat ∗deltas, ﬂoat ∗weights, ﬂoat eta, unsigned int ncon, unsigned int nrneur){
const int pitch y=16;
const int threads=256;
unsigned int px = pitch x >> 2;
unsigned int stride x = GET STRIDE(nrneur,px);
ﬂoat etadeltak = eta∗deltas[blockIdx.x∗threads+threadIdx.x],t;
int b=blockIdx.y∗stride x∗pitch y + threads∗blockIdx.x + threadIdx.x;
ﬂoat st[pitch y]; //for states
int cond1 = blockIdx.y || threadIdx.x;
int cond2 = (blockIdx.y+1)∗pitch y<=ncon;
int size = cond2 ∗pitch y + !cond2 ∗(ncon%pitch y);
if(threadIdx.x<pitch y) st[threadIdx.x] = cond1 ∗states[blockIdx.y∗pitch y + threadIdx.x −1] + !cond1;
syncthreads();
if (blockIdx.x∗threads + threadIdx.x < nrneur){
#pragma unroll 16
for (int j=0; j<16; j++){
t=weights[b];
t−= etadeltak ∗st[j];
weights[b]=t;
b+=stride x;}}