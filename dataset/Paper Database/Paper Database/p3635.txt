CBMM Memo No.
June 13, 2014
The Secrets of Salient Object Segmentation
Yin Li1∗, Xiaodi Hou2∗, Christof Koch3, James M. Rehg1, Alan L. Yuille4
1Gerogia Institute of Technology
2California Institute of Technology
3Allen Institute for Brain Science
4University of California, Los Angeles
 
Abstract: In this paper we provide an extensive evaluation of ﬁxation prediction and salient object segmentation
algorithms as well as statistics of major datasets. Our analysis identiﬁes serious design ﬂaws of existing salient object
benchmarks, called the dataset design bias, by over emphasising the stereotypical concepts of saliency. The dataset
design bias does not only create the discomforting disconnection between ﬁxations and salient object segmentation, but
also misleads the algorithm designing. Based on our analysis, we propose a new high quality dataset that oﬀers both
ﬁxation and salient object segmentation ground-truth. With ﬁxations and salient object being presented simultaneously,
we are able to bridge the gap between ﬁxations and salient objects, and propose a novel method for salient object
segmentation. Finally, we report signiﬁcant benchmark progress on three existing datasets of segmenting salient objects.
The research was supported by the ONR via an award made through Johns Hopkins
University, by the G. Harold and Leila Y. Mathers Charitable Foundation, by ONR N00014-
12-1-0883 and the Center for Minds, Brains and Machines (CBMM), funded by NSF STC
award CCF-1231216. This research was also supported by NSF Expeidtion award 1029679,
ARO MURI award 58144-NS-MUR, and Intel Science and Technology Center in Pervasive
Computing.
* These authors contribute equally.
 
Introduction
Bottom-up visual saliency refers to the ability to select important visual information for further processing. The
mechanism has proven to be useful for human as well as computer vision.
Unlike other topics such as object
detection/recognition, saliency is not a well-deﬁned term. Most of the works in computer vision focus on one of
the following two speciﬁc tasks of saliency: ﬁxation prediction and salient object segmentation.
In a ﬁxation experiment, saliency is expressed as eye gaze. Subjects are asked to view each image for seconds
while their eye ﬁxations are recorded. The goal of an algorithm is to compute a probabilistic map of an image
to predict the actual human eye gaze patterns.
Alternatively, in a salient object segmentation dataset, image
labelers annotate an image by drawing pixel-accurate silhouettes of objects that are believed to be salient. Then
the algorithm is asked to generate a map that matches the annotated salient object mask.
Various datasets of ﬁxation and salient object segmentation have provided objective ways to analyze algorithms.
However, existing methodology suﬀers from two major limitations: 1) algorithms focusing on one type of saliency
tend to overlook the connection to the other side. 2) Benchmarking primarily on one dataset tend to overﬁt the
inherent bias of that dataset.
In this paper, we explore the connection between ﬁxation prediction and salient object segmentation by augmenting 850 existing images from PASCAL 2010 dataset with eye ﬁxations, and salient object segmentation
labeling. In Sec. 3 we argue that by making the image acquisition and image annotation independent to each other,
we can avoid dataset design bias – a speciﬁc type of bias that is caused by experimenters’ unnatural selection of
dataset images.
With ﬁxations and salient object labels simultaneously presented in the same set of images, we report a series
of interesting ﬁndings. First, we show that salient object segmentation is a valid problem because of the high
consistency among labelers. Second, unlike ﬁxation datasets, the most widely used salient object segmentation
dataset is heavily biased. As a result, all top performing algorithms for salient object segmentation have poor
generalization power when they are tested on more realistic images. Finally, we demonstrate that there exists a
strong correlation between ﬁxations and salient objects.
Inspired by these discoveries, in Sec. 4 we propose a new model of salient object segmentation. By combining
existing ﬁxation-based saliency models with segmentation techniques, our model bridges the gap between ﬁxation
prediction and salient object segmentation. Despite its simplicity, this model signiﬁcantly outperforms state-ofthe-arts salient object segmentation algorithms on all 3 salient object datasets.
Related Works
In this section, we brieﬂy discuss existing models of ﬁxation prediction and salient object segmentation. We also
discuss the relationship of salient object to generic object segmentation such as CPMC . Finally, we review
relevant research pertaining to dataset bias.
Fixation prediction
The problem of ﬁxation based bottom-up saliency is ﬁrst introduced to computer vision community by . The
goal of this type of models is to compute a “saliency map” that simulates the eye movement behaviors of human.
Patch-based or pixel-based features are often used in these models, followed by a local or
global interaction step that re-weight or re-normalize features saliency values.
To quantitatively evaluate the performance of diﬀerent ﬁxation algorithms, ROC Area Under the Curve (AUC)
is often used to compare a saliency map against human eye ﬁxations. One of the ﬁrst systematic datasets in ﬁxation
prediction was introduced in . In this paper, Bruce et al. recorded eye ﬁxation data from 21 subjects on 120
natural images. In a more recent paper , Judd et al. introduced a much larger dataset with 1003 images and
15 subjects.
Due to the nature of eye tracking experiments, the error of recorded ﬁxation locations can go up to 1◦, or over
30 pixels in a typical setting. Therefore, there is no need to generate a pixel-accurate saliency map to match human
data. In fact, as pointed out in , blurring a saliency map can often increase its AUC score.
Salient object segmentation
It is not an easy task to directly use the blurry saliency map from a ﬁxation prediction algorithm. As an alternative,
Liu et al. proposed the MSRA-5000 dataset with bounding boxes on the salient objects. Following the idea of
“object-based” saliency, Achanta et al. further labeled 1000 images from MSRA-5000 with pixel-accurate object
silhouette masks. Their paper showed that existing ﬁxation algorithms perform poorly if benchmarked F-measures
of PR curve. Inspired by this new dataset, a line of papers has proposed to tackle this new challenge of
predicting full-resolution masks of salient objects. An overview of the characteristics and performances of salient
object algorithms can be found in a recent review by Borji et al.
Despite the deep connections between the problems of ﬁxation prediction and object segmentation, there is
a discomforting isolation between major computational models of the two types.
Salient object segmentation
algorithms have developed a set of techniques that have little overlapping with ﬁxation prediction models. This
is mainly due to a series of diﬀerences in the ground-truth and evaluation procedures. A typical ﬁxation groundtruth contains several ﬁxation dots, while a salient object ground-truth usually have one or several positive regions
composed of thousands of pixels. Having diﬀerent priors of sparsity signiﬁcantly limited the model of one type to
have good performance on tasks of the other type.
Objectness, object proposal, and foreground segments
In the ﬁeld of object recognition, researchers are interested in ﬁnding objects independent of their classes . Alexe
et al. used a combination of low/mid level image cues to measure the “objectness” of a bounding box. Other
models, such as CPMC and Object Proposal , generate segmentations of candidate objects without
relying on category speciﬁc information.
The obtained “foreground segments”, or “object proposals” are then
ranked or scored to give a rough estimate of the objects in the scene.
The role of a scoring/ranking function in the aforementioned literature shares a lot of similarities with the
notion of saliency. In fact, used saliency maps as a main feature for predicting objectness. In Sec. 4, we propose
a model based on the foreground segmentations generated by CPMC. One fundamental diﬀerence between these
methods to visual saliency is that an object detector is often exhaustive – it looks for all objects in the image
irrespective of their saliency value. In comparison, a salient object detector aims at enumerating a subset of objects
that exceed certain saliency threshold. As we will discuss in Sec. 4, an object model, such as CPMC oﬀers a ranking
of its candidate foreground proposals. However, the top ranked (e.g. ﬁrst 200) segments do not always correspond
to salient objects or their parts.
Datasets and dataset bias
Recently, researchers started to quantitatively analyze dataset bias and their detrimental eﬀect in benchmarking.
Dataset bias arises from the selection of images , as well as the annotation process . In the ﬁeld of visual
saliency analysis, the most signiﬁcant bias is center bias. It refers to the tendency that subjects look more often at
the center of the screen . This phenomenon might be partly due to experimental constraints that a subject’s
head being on a chin-rest during the ﬁxation experiment, and partly due to the photographer’s preference to align
objects at the center of the photos.
Center bias has been shown to have a signiﬁcant inﬂuence on benchmark scores . Fixation models either
use it explicitly ; or implicitly by padding the borders of a saliency map . To make a fair evaluation
of the algorithm’s true prediction power, Tatler proposed a shuﬄed-AUC (s-AUC) score to normalize the eﬀect
of center-bias. In s-AUC, positive samples are taken from the ﬁxations of the test image, whereas the negative
samples are from all ﬁxations across all other images.
Dataset Analysis
In this paper, we will benchmark on the following datasets: Bruce , Judd , Cerf , FT , and IS .
Among these 5 datasets, Judd and Cerf only provides ﬁxation ground-truth.
FT only provides salient object
ground-truth. IS provides both ﬁxations2 as well as salient object masks. While Bruce dataset was originally
designed for ﬁxation prediction, it was recently augmented by with 70 subjects under the instruction to label the
single most salient object in the image. In our comparison experiment, we include the following ﬁxation prediction
algorithms: ITTI , AIM , GBVS , DVA , SUN , SIG , AWS ; and following salient object
segmentation algorithms: FT , GC , SF , and PCAS . These algorithms are top-performing ones in
major benchmarks .
2IS provides raw gaze data at every time point. We use the following thresholds to determine a stable ﬁxation: min ﬁxation duration:
160ms, min saccade speed: 50px/100ms.
A: Original image
B: Full segmentation
C: Fixation ground-truth
D: Salient object ground-truth
Figure 1: An illustration of PASCAL-S dataset. Our dataset provides both eye ﬁxation (Fig. C) and salient object
(Fig. D) mask. The labeling of salient objects is based on the full segmentation (Fig. B). A notable diﬀerence
between PASCAL-S and its predecessors is that each image in PASCAL-S is labeled by multiple labelers without
restrictions on the number of salient objects.
Psychophysical experiments on the PASCAL-S dataset
Our PASCAL-S dataset is built on the validation set of the PASCAL VOC 2010 segmentation challenge. This
subset contains 850 natural images. In the ﬁxation experiment, 8 subjects were instructed to perform the “freeviewing” task to explore the images. Each image was presented for 2 seconds, and eye-tracking re-calibration was
performed on every 25 images. The eye gaze data was sampled using Eyelink 1000 eye-tracker, at 125Hz. In the
salient object segmentation experiment, we ﬁrst manually perform a full segmentation to crop out all objects in
the image. An example segmentation is shown in Fig. 1.B. When we build the ground-truth of full segmentation,
we adhere to the following rules: 1) we do not intentionally label parts of the image (e.g. faces of a person);
2) disconnected regions of the same object are labeled separately; 3) we use solid regions to approximate hollow
objects, such as bike wheels.
We then conduct the experiment of 12 subjects to label the salient objects. Given an image, a subject is asked
to select the salient objects by clicking on them. There is no time limitation or constraints on the number of objects
one can choose. Similar to our ﬁxation experiment, the instruction of labeling salient objects is intentionally kept
vague. The ﬁnal saliency value of each segment is the total number of click it receives, divided by the number of
Evaluating dataset consistency
Quite surprisingly, many of today’s widely used salient object segmentation datasets do not have any guarantee
on the inter-subject consistency. To compare the level of agreement among diﬀerent labelers in our PASCAL-S
dataset and other existing dataset, we randomly select 50% of the subjects as the test subset. Then we benchmark
the saliency maps of this test subset by taking the rest subjects as the new ground-truth subset. For ﬁxation
task, the test saliency map for each image is obtained by ﬁrst plotting all the ﬁxation points from the test subset,
and then ﬁlter the saliency map by a 2D Gaussian kernel with σ = 0.05 of the image width. For salient object
segmentation task, the test/ground-truth saliency maps are binary maps obtained by ﬁrst averaging the individual
segmentations from the test/ground-truth subset, and then threshold with Th = 0.53 to generate the binary masks
for each subset. Then we compute either AUC score or F-measure of the test subset and use this number to indicate
the inter-subject consistency.
We notice that the segmentation maps of Bruce dataset are signiﬁcantly sparser than maps in PASCAL-S or IS.
Over 30% of the segmentation maps in Bruce dataset are completely empty. This is likely a result of the labeling
process. In Borji et al.’s experiment , the labelers are forced to choose only one object for each image. Images
with two or more equally salient objects are very likely to become empty after thresholding. Although Bruce is one
of the very few datasets that oﬀer both ﬁxations and salient object masks, it is not suitable for our analysis.
For dataset PASCAL-S and IS, we benchmark the F-measure of the test subset segmentation maps by the
ground-truth masks. The result is shown in Tab. 1.
AUC scores
F-measures
Table 1: Inter-subject consistency of 5 ﬁxation datasets (AUC scores) and 2 salient object segmentation datasets
(F-measures).
Similar to our consistency analysis of salient object dataset, we evaluate the consistency of eye ﬁxations among
subjects (Tab. 1). Even though the notion of “saliency” under a context of complex natural scene is often considered
as ill-deﬁned, we observe highly consistent behaviors among human labelers in both eye-ﬁxation and salient object
segmentation tasks.
Benchmarking
In this section, we benchmark 7 ﬁxations algorithms, AWS , AIM , SIG , DVA , GBVS , SUN
 , and ITTI on 5 datasets, Bruce , Cerf , IS , Judd , and our PASCAL-S. For salient object
segmentation, we bench 4 algorithms SF , PCAS , GC , and FT on 4 datasets, FT , IS ,
and our PASCAL-S. For all algorithms, we use the original implementations from the authors’ websites.
purposes of this analysis are: 1) to highlight the generalization power of algorithms, and 2) to investigate interdataset diﬀerence among these independently constructed datasets. The benchmark results are presented in Fig. 2.
Sharply contrasted to the ﬁxation benchmarks, the performance of all salient object segmentation algorithms drop
signiﬁcantly when migrating from the popular FT dataset.
The average performance of all 4 algorithms have
dropped, from FT’s 0.8341, to 0.5765 (30.88% drop) on IS, and 0.5530 (33.70% drop) on PASCAL-S. This result
is alarming, because the magnitude of the performance drop from FT to any dataset by any algorithm, can easily
dwarf the 4-year progress of salient object segmentation on the widely used FT dataset. Moreover, the relative
ranking among algorithms also changes from one dataset to another.
Dataset design bias
The performance gap among datasets clearly suggests new challenges in salient object segmentation. However, it
is more important to pinpoint the cause to the performance degradation rather than to start benchmark race on
another new dataset. In this section, we analyze the following image statistics in order to ﬁnd the similarities and
diﬀerences of today’s salient object segmentation datasets:
Local color contrast: Segmentation or boundary detection is an inevitable step in most salient objects detectors.
It is important to check whether the boundaries are “unnaturally” easy to segment. To estimate the strength
of the boundary, we crop a 5 × 5 image patch at the boundary location of each labeled object, and compute
RGB color histograms for foreground and background separately.
We then calculate the χ2 distance to
measure the distance between two histograms. It is worth noting that some of the ground-truth of the IS
3At least half of the subjects within the subset agree on the mask.
Figure 2: Left: The s-AUC scores of ﬁxation prediction. Right: The F-Measure scores of salient object segmentation. According to , we choose β = 0.3 to calculate the F-measure from PR curve. In both ﬁgures, magenta lines
show the inter-subject consistency score of these datasets. These numbers can be interpreted as the upper-bounds
of algorithm scores.
dataset are not perfectly aligned with objects boundaries, resulting in an underestimate of local contrast
magnitude.
Global color contrast: The term “saliency” is also related to the global contrast of the foreground and background. Similar to the local color contrast measure, for each object, we calculate the χ2 distance between its
RGB histogram and the background RGB histogram.
Local gPB boundary strength: While color histogram distance captures some low-level image features, an
advanced boundary detector, such as gPB , combines local and global cues to give a more comprehensive
estimate of the presence of a boundary. As a complementary result to our local color contrast measure, for
object boundary pixel we compute the mean gPB response of a 3 × 3 local patch.
Object size: In each image, we deﬁne the size of an object as the proportion of pixels in the image. Most of the
objects in IS are very small.
As shown in Fig. 3, FT dataset stands out in local/global color contrast as well as the gPB boundary strength
statistics. At a ﬁrst glance, our observation that FT contains unnaturally strong object boundaries seem acceptable,
especially for a dataset focusing on salient object analysis. Strong boundaries are linked to the core concept of
saliency: a foreground object with discernable boundaries being surrounded by background that have contrastive
colors. In fact, many images in the FT dataset are textbook examples to demonstrate the deﬁnition of saliency. The
notion of “saliency” in FT is much less ambiguous than in other datasets. However, such reduction of ambiguity
during the image selection process is more destructive rather than constructive, for the purpose to test saliency.
The confusion between image selection process and image annotation process introduces a special kind of bias by
over-expressing the desired properties of the target concept, and reduce the presence of negative examples. We call
this type of bias the dataset design bias:
During the design of a dataset, the image annotation process should be independent of the image
selection process. Otherwise the dataset design bias will arise as a result of disproportionate sampling
of positive/negative examples.
Fixations and F-measure
Previous methods of salient object segmentation have reported big margin of F-measures over all ﬁxation algorithms.
However, most of these comparisons are done on the FT dataset, which has shown to have non-negligible dataset
bias. Another factor that contributes to the inferior performance of ﬁxation algorithms is the center-bias. Major
salient object segmentation algorithms, such as SF, PCAS, and GC, have implemented their own treatments for
the center-bias. In contrast, many ﬁxation prediction algorithms, such as AWS and SIG, do not implement center
bias as they expect to be benched by s-AUC score which cancels the center-bias eﬀect. To discount the inﬂuence
of center-bias, we add a ﬁxed Gaussian (σ = 0.4 of the image width) to the saliency maps generated by all ﬁxation
algorithms, and then benchmark these algorithms on all 3 salient object datasets. The result is shown in Fig. 4.
In addition, we also tested the F-measure of the ground-truth human ﬁxation maps on IS and PASCAL-S. Each
ﬁxation map is blurred by a Gaussian kernel with σ = 0.03 of the image width. No center bias is superimposed
because the human ﬁxations are already heavily biased towards the center of the image.
Local Color Contrast
Object Size
Global Color Contrast
Local gPb Boundary Strengh
Figure 3: Image statistics of the salient object segmentation datasets. The statistics of FT dataset is diﬀerent from
other datasets in local/global color contrast and boundary strength. As for object size, the PASCAL-S contains a
balanced mix of large and small objects.
When we remove the eﬀect of center bias and dataset design bias, the performance of ﬁxation algorithms
becomes very competitive. We also notice that the F-measure of the ground-truth human ﬁxation maps is rather
low compared to the inter-subject consistency scores in the salient object labeling experiment. This performance
gap could be either due to a weak correlation between the ﬁxation task and the salient object labeling task, or
the incompatibility of the representation of ﬁxations (dots) versus the representation of objects (regions). In the
next section, we will show that the latter is more likely to be true. Once equipped with appropriate underlying
representation, the human ﬁxation map, as well as their algorithm approximations, generate accurate results for
salient object segmentation.
From Fixations to Salient Object Detection
Many of today’s well-known salient object algorithms have the following two components: 1) a suitable representation for salient object segmentation, and 2) computational principles of feature saliency, such as region contrast 
or element uniqueness . However, none of these two components alone is new to computer vision. On one hand,
detecting boundaries of objects has been a highly desired goal for segmentation algorithms since the beginning of
computer vision On the other hand, deﬁning rules of saliency has been studied in ﬁxation analysis for decades.
In this section, we build a salient object segmentation model by combining existing techniques of segmentation
and ﬁxation based saliency. The core idea is to ﬁrst generate a set of object candidates, and then use the ﬁxation
algorithm to rank diﬀerent regions based on their saliency. This simple combination results in a novel salient object
segmentation method that outperforms all previous methods by a large margin.
Salient object, object proposal and ﬁxations
Our ﬁrst step is to generate the segmentation of object candidates by a generic object proposal method. We use
CPMC to obtain the initial segmentations. CPMC is an unsupervised framework to generate and rank plausible
Shape Features
Fixation Features
Min/Max Fixation Energy
Mean Fixation Energy
Convex Area
Weighted Fixation Centroid
Euler Number
Fixation Energy Ratio
Histogram of Fixations
Major/Minor Axes Length
Eccentricity
Orientation
Equivalent Diameter
Width/Height
Table 2: Shape and ﬁxation features used in our model. The ﬁnal feature dimension for each mask is 33.
hypotheses of object candidates without category speciﬁc knowledge. This method initializes foreground seeds
uniformly over the image and solves a set of min-cut problems with diﬀerent parameters. The output is a pool
of object candidates as overlapping ﬁgure-ground segments, together with their “objectness” scores. The goal of
CPMC is to produce a over-complete coverage of potential objects, which could be further used for tasks such as
object recognition.
The representation of CPMC-like object proposal is easily adapted to salient object segmentation. If all salient
objects can be found from the pool of object candidates, we can reduce the problem of salient object detection to a
much easier problem of salient segment ranking. Ranking the segments also simpliﬁes the post-processing step. As
the segments already preserve the boundary of the image, no explicit segmentation (e.g. GraphCut ) is required
to obtain the ﬁnal binary object mask.
To estimate the saliency of a candidate segment, we utilize the spatial distribution of ﬁxations within the object.
It is well known that the density of ﬁxation directly reveals the saliency of the segment. The non-uniform spatial
distribution of ﬁxations on the object also oﬀers useful cues to determine the saliency of an object. For example,
ﬁxation at the center of a segment will increase its probability of being an object. To keep our frame work simple,
we do not consider class speciﬁc or subject speciﬁc ﬁxation patterns in our model.
We use a learning based framework for the segment selection process. This is achieved by learning a scoring function
for each object candidate. Given a proposed object candidate mask and its ﬁxation map, this function estimates
the overlapping score (intersection over union) of the region with respect to the ground-truth, similar to .
Features: We extract two types of features: shape features and ﬁxation distribution features within the object.
The shape features characterizes the binary mask of the segment, which includes major axis length, eccentricity,
minor axis length, and the Euler number. For ﬁxation distribution features, we ﬁrst align the major axis of each
object candidate, and then extract a 4 × 3 histogram of ﬁxations density over the aligned object mask.
histogram captures the spatial distribution of the ﬁxations within a object.
Without loss of generality, we use the term ﬁxation energy to refer to pixel values in the ﬁxation map. For
human ﬁxations, the energy is a discrete value as the number of ﬁxations at current location.
For algorithm
generated ﬁxation maps, the energy is a continuous value ranging from 0 to 1 as the probability of a ﬁxation at
current location. Finally, the 33 dimensional feature vector is extracted for each object mask. The details of the
features can be found in Table. 2. In particular, the Fixation Energy Ratio is the deﬁned as the sum of ﬁxation
energy within the segment, divided by the sum of ﬁxation energy of the whole image.
Learning: For each dataset, we train a random forest with 30 trees, using a random sampling of 40% of the
images. The rest of images are used for testing. The results are averaged on a 10-fold random split of the training
and testing set.
We use random regression forest to predict the saliency score of an object mask.
regression forest is an ensemble of decision trees. For each branch node, a feature is selected from a random subset
of all features and a decision boundary is set by minimizing the Minimum Square Error (MSE). The leaf nodes keep
the mean value of all training samples that end up in the node. And the ﬁnal result is a weighted average of all
leaf nodes that a testing sample reaches. We choose random forest since our feature vector contains discrete values
(Euler Number), which can be easily handled in a decision tree. In the testing phase, each segment is classiﬁed
independently. We generate the salient object segmentation by averaging the top-K segments at pixel level, similar
to . We then use simple thresholding to generate the ﬁnal object masks. As our saliency scores are deﬁned over
image segments, this simple strategy leads to accurate object boundaries.
Note that no appearance feature is used in our method, because our goal is to demonstrate the connection
between ﬁxation and salient object segmentation. Our algorithm is independent of the underlying segmentation
and ﬁxation prediction algorithms, allowing us to switch between diﬀerent ﬁxation algorithm or even human
Limits of the model
Our model contains two separate parts: a segmenter that proposes regions, and a selector that gives each region a
saliency score. In this section we explore the limitation of our model, by replacing each part at a time. First, we
quantify the performance upper-bound of the selector, and then, best achievable results of the segmenter is also
presented.
To test the upper-bound of the selector, we train our model on the ground-truth segments of PASCAL-S (e.g.
Fig. 1.B) with human ﬁxation maps. With a perfect segmenter, this model can accurately estimate the saliency of
segments using ﬁxation and shape information. On the test set, it achieves a F-Measure of 0.9201 with P = 0.9328
and R = 0.7989. This result is a strong validation to our motivation, which is to bridge the gap between ﬁxations
and salient objects. It is worth mentioning that this experiment requires a full segmentation of all objects of the
entire dataset. Therefore, PASCAL-S is the only dataset that allows us to test the selector with an ideal segmenter.
Second, we test the upper-bound performance of CPMC segmentation algorithm. We match the each segment
from CPMC to the ground truth object annotations, and greedily choose the segments (out of the ﬁrst 200 segments)
with best overlapping scores. Again, the result is very positive. With the ﬁrst 200 segments, the best CPMC
segments achieved an F-Measure of 0.8699 (P = 0.8687, R = 0.883) on PASCAL-S dataset.
Similar results
are observed in FT (F-Measure = 0.9496, P = 0.9494, R = 0.9517) and IS (F-Measure = 0.8416, P = 0.8572,
R = 0.6982) datasets.
We conduct extensive experiments on three datasets: FT, IS and PASCAL-S using diﬀerent ﬁxation models. All
results are averaged on a 10-fold random split of the training and testing set. The results are grouped into 4
categories:
Salient Object Algorithms refer to the 4 algorithms FT , GC , PCAS , and SF that are originally proposed for salient object segmentation.
CPMC + Fixation Algorithms refer to the model proposed in our paper. We choose the top 10 segments for
each image, and assign scores based on the ﬁxation map of 7 algorithms: AIM , AWS , DVA , GBVS ,
ITTI , SIG , and SUN .
Original Fixation Algorithms refer to the 7 ﬁxation algorithms. To cancel the eﬀect of center bias, we add a
ﬁxed center bias with σ = 0.4 of the image width, to each generated ﬁxation map.
Baseline Models refers to 4 other models. CPMC Ranking refers to the original rankings of CPMC, with the same
choice of K = 10. CPMC+Human Fixations refers to a variation of our model that replaces the algorithm
ﬁxation maps with human ﬁxations -supposedly, human map should reﬂect the saliency of the scene more
accurately than algorithms. CPMC Best refers to the salient object maps generated by greedily selecting the
best CPMC segments with respect to the ground truth. This score estimates the upper limit of any algorithm
that is based on CPMC segmentation. Finally GT Seg + Human Fixations refers to the method that uses
ground-truth segmentations plus human ﬁxations. This score validates the strong connection between the
ﬁxation task and the salient object segmentation task.
The full results on PASCAL-S dataset are shown in Fig. 4 and Fig. 5. From these results, we make two key
observations. First, our method consistently outperforms the original CPMC ranking function by a large margin,
independently of the underlying ﬁxation prediction algorithm. Second, the performance of our model converges
much faster than the original CPMC with respect to K. Our model provides decent F-measure with a moderate
K = 20 segments, while the CPMC ranking function does not converge even with K = 2004 (see Fig. 5). The
ﬁnding suggests that our method can eﬀectively segment salient objects using a small number of segments.
The F-measures of each algorithm on all dataset are reported in Tab. 3, using only the ﬁrst 20 segments. More
results of FT and IS datasets with the combination of 7 ﬁxation algorithms 7 diﬀerent K can be found in our
project website5. Our methods achieve superior results and consistently outperform all state-of-the-art algorithms
on all datasets with a small number of segments. To summarize, we compare our best results (K = 20) with
the best state-of-the-art salient object algorithms in Tab. 4. Our method outperforms the state-of-the-art salient
object segmentation algorithms by a large margin. We achieved an improvement of 11.82% , 7.06% and 2.47% on
PASCAL-S, IS and FT, respectively, in comparison to the best performing salient object segmentation algorithm.
The combination of CPMC+GBVS leads to best results in all 3 datasets.
Salient object
algorithms
CPMC + ﬁxation
algorithms
Original ﬁxation
algorithms (with ﬁxed center bias mask)
CPMC Ranking
GT Seg + Human Fixations
CPMC + Human Fixations
Human Fixations
Figure 4: The F-measures of all algorithms on PASCAL-S dataset. All CPMC+Fixation results are obtained using
top K = 20 segments.
50 100 200
Figure 5: F-measures of our salient object segmentation method under diﬀerent choices of K, in comparison to
CPMC ranking function. Results are reported on the testing set (60% of the images) of PASCAL-S over 10 random
splits. Comparing to the original CPMC ranking function, our method obtains satisfactory F-measures with small
To better understand our results, we also provide qualitative visualization of salient object masks generated by
our methods as well as others. Fig. 6-8 demonstrate our results using 7 diﬀerent ﬁxation prediction algorithms in
4This might be explained by the diversiﬁcation ranking used in CPMC.
5 
Salient Object
Baseline Models
CPMC Ranking
CPMC + Human
GT Seg. + Human
Orig. Fixation
CPMC + Fixation
Table 3: The F-measures of all algorithms on all 3 datasets.
Fixation Model Used
Improvements
Best SalObj Model
Table 4: Best results of our model compared to best results of existing salient object algorithms.
achieves better F-measure than all major salient object segmentation algorithms on all three datasets, including
the heavily biased FT dataset. Results are reported on the testing set (60% of the images) over 10 random splits
in three datasets.
comparison to CPMC ranking function and major salient object segmentation methods, on FT, IS and PARSCAL-
S, respectively. We also present our results using human ﬁxations in PASCAL-S and IS. To illustrate the strength
and weakness of our method, we select the results based on the F-measures of each image. For each dataset, the
average F-measure of our method decreases from top row to the bottom row. Our method is able to capture the
full region of an salient object. We notice that our method does not segment most of the small salient objects.
This is largely due to the output from CPMC using sparse uniform seeds. An object is missing if CPMC does not
generate a segment for the object.
Conclusion
In this paper, we explore the connections between ﬁxation prediction and salient object segmentation by providing
a new dataset with both ﬁxations and salient object annotations. We conduct extensive experiments on the dataset
for both tasks and compare the results with major benchmarks. Our analysis suggests that the deﬁnition of a
salient object is highly consistent among human subjects. We also point out signiﬁcant dataset design bias in
major salient object benchmarks. The bias is largely due to deliberately emphasising the concept of saliency. We
argue that the problem of salient object segmentation should move beyond the textbook examples of visual saliency.
A possible new direction is to look into the strong correlation between ﬁxations and salient objects. Built on top of
this connection, we propose a new salient object segmentation algorithm. Our method decouple the problem into
a segment generation process, followed a saliency scoring mechanism using ﬁxation prediction. This simple model
outperforms state-of-the-art salient object segmentation algorithms on all major datasets. Our dataset together
with our method provide a new insight to the challenging problems of both ﬁxation prediction and salient object
segmentation.
Figure 6: Visualization of salient object segmentation results on FT. Each two-row compares the results of one
image. The ﬁrst row includes results from existing methods (Left to Right): Original image, Ground-truth mask,
FT, GC, PCAS, SF and CPMC ranking; The second row shows results of our method using diﬀerent ﬁxations (Left
to Right): AIM, AWS, DVA, GBVS, ITTI, SIG and SUN. We are not able to report results using human ﬁxations.
The images are selected by sorting the F-measure of our results in a decreasing order.
Figure 7: Visualization of salient object segmentation results on IS. Each two-row compares the results of one
image. The ﬁrst row includes results from existing methods (Left to Right): Original image, Ground-truth mask,
FT, GC, PCAS, SF and CPMC ranking; The second row shows results of our method using diﬀerent ﬁxations (Left
to Right): Human Fixation, AIM, AWS, DVA, GBVS, ITTI, SIG and SUN. The images are selected by sorting the
F-measure of our results in a decreasing order. We notice that IS favors sparse saliency maps, since it contains a
signiﬁcant portion of small salient objects.
Figure 8: Visualization of salient object segmentation results on PARSCAL-S. Each two-row compares the results
of one image. The ﬁrst row includes results from existing methods (Left to Right): Original image, Ground-truth
mask, FT, GC, PCAS, SF and CPMC ranking; The second row shows results of our method using diﬀerent ﬁxations
(Left to Right): Human Fixation, AIM, AWS, DVA, GBVS, ITTI, SIG and SUN. The images are selected by sorting
the F-measure of our results in a decreasing order.