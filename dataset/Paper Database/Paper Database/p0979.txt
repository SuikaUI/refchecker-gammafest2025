University of Massachusetts Amherst
From the SelectedWorks of Andrew McCallum
Reducing Labeling Effort for Structured
Prediction Tasks
Aron Culotta
Andrew McCallum, University of Massachusetts - Amherst
Available at: 
Reducing labeling effort for structured prediction tasks
Aron Culotta and Andrew McCallum
Department of Computer Science
140 Governor’s Drive
University of Massachusetts
Amherst, MA 01003–4601
{culotta, mccallum}@cs.umass.edu
A common obstacle preventing the rapid deployment of
supervised machine learning algorithms is the lack of
labeled training data. This is particularly expensive to
obtain for structured prediction tasks, where each training instance may have multiple, interacting labels, all
of which must be correctly annotated for the instance to
be of use to the learner. Traditional active learning addresses this problem by optimizing the order in which
the examples are labeled to increase learning efﬁciency.
However, this approach does not consider the difﬁculty
of labeling each example, which can vary widely in
structured prediction tasks. For example, the labeling
predicted by a partially trained system may be easier
to correct for some instances than for others.
We propose a new active learning paradigm which reduces not only how many instances the annotator must
label, but also how difﬁcult each instance is to annotate. The system also leverages information from partially correct predictions to efﬁciently solicit annotations from the user. We validate this active learning
framework in an interactive information extraction system, reducing the total number of annotation actions by
Introduction
Supervised machine learning algorithms require a set of
fully labeled training examples for accurate and robust performance. Unfortunately, for many tasks, this labeled data is
costly and time-consuming to obtain.
Active learning is a framework that aims to reduce this
burden, typically by optimizing the order in which the examples are labeled . For instance, one might order the examples such that those with the least conﬁdent predictions
are labeled ﬁrst. By seeing the most valuable examples early
in training, the algorithm can learn more efﬁciently.
Most active learners are evaluated by plotting a “learning
curve” that displays the learner’s performance on a held-out
data set as the number of labeled examples increases. An active learner is considered successful if it obtains better performance than a traditional learner given the same number
Copyright c⃝2005, American Association for Artiﬁcial Intelligence (www.aaai.org). All rights reserved.
of labeled examples. Thus, active learning expedites annotation by reducing the number of labeled examples required to
train an accurate model.
However, this paradigm assumes each example is equally
difﬁcult to annotate. While this assumption may hold in traditional classiﬁcation tasks, in structured classiﬁcation tasks
it does not. For example, consider an information extraction system that labels segments of free text with tags corresponding to entities of interest. An annotated example might
look like the following:
<name> Jane Smith </name>
<title> CEO </title>
<company> Unicorp, LLC </company>
Phone: <phone> (555)555-5555 </phone>
To label this example, the user must not only specify
which type of entity each token is, but also must determine
the start and end boundaries for each entity. Clearly, the
amount of work required to label an example such as this
will vary between examples, based on the number of entities. However, this effort is not reﬂected by the standard
evaluation metrics from active learning. Since our goal is to
reduce annotation effort, it is desirable to design a labeling
framework that considers not only how many instances the
annotator must label, but also how difﬁcult each instance is
to annotate.
Additionally, unlike in traditional classiﬁcation tasks, a
structured prediction system may be able to partially label
an example, which can simplify annotation. In the above example, the partially-trained system might correctly segment
the title ﬁeld, but mislabel it as a company name. We would
like to leverage these partial predictions to reduce labeling
We propose a framework to address these shortcomings
for machine learning applied to information extraction. We
ﬁrst provide a way to quantify the number of actions a user
must perform to label each training example, distinguishing between boundary and classiﬁcation annotations. We
then demonstrate an interactive information extraction system that minimizes the amount of effort required to train an
accurate extractor.
To expedite annotation for information extraction (IE), we
ﬁrst note that the main difference between labeling IE examples and labeling traditional classiﬁcation examples is the
problem of boundary annotation (or segmentation). Given
a sequence of text that is correctly segmented, choosing the
correct type for each entity is simply a classiﬁcation task: the
annotator must choose among a ﬁnite set of labels for each
entity. However, determining the boundaries of each entity
is an intrinsically distinct task, since the number of ways to
segment a sequence is exponential in the sequence length.
Additionally, from a human-computer interaction perspective, the “clicking and dragging” involved in boundary annotation generally requires more hand-eye coordination from
the user than does classiﬁcation annotation.
With this distinction in mind, our system reduces annotation effort in two ways. First, many segmentation decisions
are converted into classiﬁcation decisions by presenting the
user with multiple predicted segmentations to choose from.
Thus, instead of hand segmenting each ﬁeld, the user may
select the correct segmentation from the given choices.
Second, the system allows the user to correct partially labeled examples, and then constrains its predictions to respect
these corrections. This interaction further reduces the number of segmentation decisions the user must make: Corrections to one part of the sequence often propagate to ﬁx segmentation errors in other parts of the sequence.
The resulting system allows the user to constrain the
predictions of the learner without manually annotating the
boundaries of incorrect segments. Very often, these constraints will allow the user to simply select the correct annotation from among the provided choices. Thus, the annotator
can frequently label a record without explicitly annotating
the boundaries.
We demonstrate the performance of this framework in the
domain of contact record extraction. The task of the annotator is to train a system that can accurately extract contact information (such as names and addresses) from unstructured
text. In particular, the model we use is a linear-chain conditional random ﬁeld (CRF) . The probabilistic foundations of CRFs make them
well-suited to the conﬁdence estimation and correction propagation methods required by our framework.
We present results demonstrating that our framework reduces the total number of annotation actions required to train
an IE system by 22%, and furthermore that it reduces the
number of boundary annotations by 46%, as compared with
competing methods.
By reducing the effort required to train an extractor, this
work can lead to more wide-spread acceptance of end-user
information extraction systems that incorporate machine
learning techniques.
Related Work
To the best of our knowledge, this is the ﬁrst active learning framework that (1) is sensitive to the difﬁculty of labeling each training example and (2) uses partially labeled
instances to reduce this labeling difﬁculty.
Part of our framework can be viewed as a type of selective
sampling , which proposes an
order in which to label the training instances such that learning is most efﬁcient. In particular, ours is a certainty-based
method in that it prefers to label instances for which the system has low conﬁdence in its predictions . Our work, however, incorporates user feedback to
more efﬁciently solicit annotated examples.
Methods for computing conﬁdence estimates in natural
language tasks have been studied in domains such as text
classiﬁcation , information extraction , and speech recognition , although none of these consider labeling difﬁculty in their conﬁdence estimates.
Thompson, Califf, & Mooney present an active
learning system for information extraction and parsing,
which are instances of structured learning tasks. While they
demonstrate the advantage of active learning for these tasks,
they require the annotator to fully label each training instance, which is precisely what this paper aims to avoid.
Others have studied efﬁcient ways to interactively train an
extraction system ; however, these methods do not use partially labeled instances to reduce labeling effort. Partially
correct annotations are marked as incorrect.
This work can be viewed as an active learning extension to Kristjannson et al. , which presents a framework for interactive information extraction and describes
the details of correction propagation and conﬁdence estimation for CRFs. A CRF for contact record extraction is fully
trained and used to automatically populate a contact record
database. The interactive framework provides a minimaleffort way to iteratively correct system errors until the predicted database is error-free. However, that work requires
that all corrections be manually provided by the user, including segmentation decisions (with the exception of those
corrections enabled by correction propagation). Therefore,
it is not sensitive the amount of effort the user must invest
to correct each example. This paper presents a way to leverage correction propagation in an active learning setting to
directly reduce the number of segmentation labels the user
must provide, as well as a way to exploit multiple system
predictions to reduce overall labeling effort.
Additionally, Kristjannson et al. propose the Expected Number of User Actions (ENUA) measure to estimate the labeling effort to correctly enter all ﬁelds of a
record. This measure, however, does not address the distinction between boundary and classiﬁcation labels. In particular, ENUA assumes it takes one action to segment and label
an entity. In this paper, we present measures that account for
the effort required for each of these actions.
The main contributions of this paper are (1) a new active learning framework that incorporates the difﬁculty of
labeling each example, (2) a method to convert segmentation labeling into classiﬁcation labeling using partially correct annotations, (3) a more detailed estimate of the number
of annotation actions required to label each example, and (4)
a mechanism for performing correction propagation when
corrections are given across multiple system predictions.
Annotation framework
We ﬁrst provide a brief overview of the annotation framework applied to IE. Given an IE learning algorithm L and
a set of unlabeled data U, the task is to iteratively solicit
annotations from the user and retrain the extractor.
At iteration t, the system operates as follows:
1. Rank each unlabeled instance by its conﬁdence value
given by the current extractor Lt.
2. Select the least conﬁdent example u ∈U to be labeled.
3. Present the user the top k labelings of u predicted by Lt.
4. If the correct labeling exists in the top k choices, allow the
user to select that labeling, and add u to the labeled data
5. Otherwise, for any entity in these k predictions that is segmented correctly but classiﬁed incorrectly, allow the user
to provide the correct type for this entity.
6. Based on these corrections, generate a new set of k predictions, propagating these corrections to possibly ﬁx other
7. If the correct labeling exists in the top k choices, allow
the user to select that labeling and add u to the labeled
8. Otherwise, if the correct labeling still does not exist in
these k predictions, allow the user to manually correct one
of these incorrect k predictions with the true labeling.
Notice that the only step in which the user must manually segment entities is step 8. Steps 4 and 7 allow the user
to label the sequence by making a choice among k predictions. Step 5 allows the user to provide correct entity types
to the learner, without manually segmenting ﬁelds. In step
6, the system performs constrained inference to generate a
new set of predictions that conform to the user’s corrections.
It is in this step that the system often automatically corrects
segmentation errors present in the ﬁrst k choices.
This framework allows the user to rapidly and easily annotate examples and correct the system’s predictions, while
reducing the amount of effort spent labeling boundaries.
In the remaining sections, we describe in more detail the
components of this system. As some of these details are dependent on the learner being used, we ﬁrst brieﬂy describe
CRFs, which we will use in our experiments.
Conditional Random Fields
The machine learning method we apply is a conditional
random ﬁeld (CRF) ,
a model successfully used in information extraction for
tasks such as named entity recognition. CRFs are undirected
graphical models that encode the conditional probability of
a set of output variables Y given a set of evidence variables
X. The set of distributions expressible by a CRF is speciﬁed
by an undirected graph G, where each vertex corresponds to
a random variable. If C = {{yc, xc}} is the set of cliques
determined by the edges in G, then the conditional probability of y given x is
pΛ(y|x) = 1
φc(yc, xc; Λ)
where φ is a potential function parameterized by Λ and
c∈C φ(yc, xc) is a normalization factor. We assume φc factorizes as a log-linear combination of arbitrary
features computed over clique c, therefore
φc(yc, xc; Λ) = exp
λkfk(yc, xc)
The model parameters Λ = {λk} are a set of real-valued
weights typically learned from labeled training data by maximum likelihood estimation.
In the special case in which the designated output nodes
of the graphical model are linked by edges in a linear chain,
CRFs make a ﬁrst-order Markov independence assumption
among output nodes, and thus correspond to ﬁnite state machines (FSMs). In this case CRFs can be roughly understood
as conditionally-trained hidden Markov models, with additional ﬂexibility to effectively take advantage of complex
overlapping features.
Conﬁdence estimation
A common form of active learning is certainty-based selective sampling , which gives higher
priority to unlabeled examples for which the learner has
a low conﬁdence in its predictions. Culotta & McCallum
 describe the constrained forward-backward algorithm to estimate the conﬁdence of CRF predictions. This algorithm calculates the probability that an entity (or an entire
sequence) has a particular labeling, which follows directly
from the semantics of undirected models: The probability
of the hidden states corresponding to an entity’s labeling is
the marginal probability of those hidden states given the observed input. We refer the reader to Culotta & McCallum
 for more details on this algorithm.
Using this method, we can assign a conﬁdence estimate to
each unlabeled training example. By labeling the least con-
ﬁdent examples ﬁrst, we can increase the CRF learning rate.
Selecting top predictions
To present the user with the top k predictions, we must extend the CRF inference algorithm to return k predictions,
instead of simply the top prediction. For linear-chain CRFs,
inference is performed using an analog of the Viterbi algorithm, a dynamic program well-known for its use in inference in hidden Markov models . There are
also well-established, efﬁcient modiﬁcations to the Viterbi
algorithm that can calculate the top k optimal predictions,
often called n-best Viterbi . This
algorithm can be viewed as a beam search through the space
of possible predictions. We apply this algorithm to inference
in CRFs to generate the k most probable predictions.
Correction propagation
In step 5, the annotator provides the true type for entities
that have been correctly segmented but incorrectly classi-
ﬁed. The system must then produce the top k predictions
that conform to these new annotations.
Kristjannson et al. present the constrained Viterbi
algorithm, which modiﬁes the traditional Viterbi algorithm
to prune from the search space those labelings that do not
agree with the given annotations.
The interesting capability of this algorithm is that by constraining the predicted label for one entity, the prediction
for another entity may change as well. As a simple example, consider labeling the name “Charles Stanley” with
the ﬁelds FIRSTNAME and LASTNAME. If the system confuses the ﬁrst and last names, a na¨ıve correction system will
require two corrective actions. Using constrained Viterbi,
when the user corrects the FIRSTNAME ﬁeld to be “Stanley,” the system automatically changes the LASTNAME ﬁeld
to “Charles.” Kristjannson et al. call this capability
correction propagation.
We extend this to our current task using an algorithm we
call n-best constrained Viterbi, which, as its name suggests,
combines n-best Viterbi with constrained Viterbi. This extension can be straight-forwardly implemented by constraining the n-best Viterbi algorithm to prune predictions that do
not agree with the annotations.
Using this algorithm, we enable the system to solicit corrections for the classiﬁcation of entities, which are then
propagated to correct both the classiﬁcation and segmentation of other entities. In this way, we can reduce the amount
of effort expended on segmentation labeling.
Measuring annotation effort
To calculate the amount of effort required to label a training
example, we wish to abstract from the details of a particular
user interface, and instead quantify atomic user actions. In
the case of IE annotation, we deﬁne three atomic labeling
actions: START, END, and TYPE, corresponding to labeling
the start boundary, end boundary, and type of an entity.
Thus, labeling the input
<name> Jane Smith </name>
<title> CEO </title>
requires 2 START, 2 END, and 2 TYPE actions. The goal
of our annotation framework is to reduce the total number of
annotation actions.
We can see that a partially labeled example can require
fewer annotation actions. For example, consider the following partially labeled record:
<name> Jane </name> Smith
<company> CEO </company>
This requires one END action to ﬁx the ending boundary
of “Jane,” and one TYPE action to change “CEO” from a
company to a title. Thus, using the partial labeling has reduced the total number of required actions from 6 to 2.
By presenting the user with k predictions, we introduce
another action: If one of the k predictions is correct, the user
must choose this prediction. We call this action CHOICE.
When simulating corrections from the annotator, we accumulate the number of times each action is performed. In the
ﬁrst round, when the user corrects the types of correctly segmented ﬁelds, the only action incurred is the TYPE action. If
none of the k constrained predictions are correct, then (and
only then) the user must perform the segmentation actions
START and END.
training size
least-confidence
Figure 1: Testing label F1 as a function of training set size.
LEASTCONFIDENCE labels the least conﬁdent instances
ﬁrst, while RANDOM labels the instances in a random order.
It will generally be the case that some actions are more
expensive than others. For example, as mentioned earlier,
START and END actions may require more hand-eye coordination than TYPE actions. A cost-sensitive approach could
take this into account; however, in these experiments, we assume each atomic action has unit cost.
Experiments
Using the fully annotated collection of extracted contact
records from Kristjannson et al. , we simulate our
annotation framework and measure the performance of the
CRF with respect to the number of actions required to train
For training and testing 2187 contact records (27,560
words) were collected from web pages and e-mails and 25
classes of entities were hand-labeled.1 Some data came from
pages containing lists of addresses, and about half came
from disparate web pages found by searching for valid pairs
of city name and zip code.
The features used in the CRF consisted of capitalization
features, 24 regular expressions over the token text (e.g.
CONSTAINSHYPHEN), and offsets of these features within
a window of size 5. We also used 19 lexicons, including
“US Last Names,” “US First Names,” “State names,” “Titles/Sufﬁxes,” “Job titles,” and “Road endings.” Feature induction was not used in these experiments.
We use 150 examples to train an initial CRF, 1018 to simulate user annotation, and 1019 to evaluate performance.
We ﬁrst show that traditional active learning is beneﬁcial in this domain. Figure 1 plots the average label F1 versus training size where the order in which instances are
1The 25 ﬁelds are: FIRSTNAME, MIDDLENAME, LASTNAME,
NICKNAME, SUFFIX, TITLE, JOBTITLE, COMPANYNAME, DE-
PARTMENT, ADDRESSLINE, CITY1, CITY2, STATE, COUN-
TRY, POSTALCODE, HOMEPHONE, FAX, COMPANYPHONE, DI-
RECTCOMPANYPHONE, MOBILE, PAGER, VOICEMAIL, URL,
EMAIL, INSTANTMESSAGE
START + END
START + END + TYPE
Table 1: Number of actions to label 1018 examples. By converting segmentation actions into classiﬁcation actions, we can
reduce the total number of annotation actions by 22%.
start + end + type + choice actions
Figure 2: Testing label F1 as a function of the total number
of annotation actions. At k = 4, performance plateaus with
roughly 800 fewer actions than the baseline.
start + end actions
Figure 3: Testing label F1 as a function of the total number
of segmentation actions. The interactive system with k = 4
requires just over half the number of segmentation actions
of the baseline.
labeled is either random (RANDOM) or by order of leastconﬁdence (LEASTCONFIDENCE). Note that here each labeled instance must be manually labeled by the annotator.
This ﬁgure demonstrates that the order in which examples
are labeled can affect learning efﬁciency.
However, we desire a more explicit measure of labeling
effort, so we examine how F1 varies with the number of annotation actions. The next set of experiments all label the
training examples in order of least-conﬁdence. We compare
two competing methods. BASELINE presents the user with
the top prediction, and the user must hand annotate all corrections. The other method is the learning framework advocated in this paper, which presents the user with k possible segmentation, and interactively solicits label corrections.
We vary k from 1 to 4. Constrained inference and correction
propagation are used in one round of interactive labeling.
Figure 2 compares these methods, measuring the total
number of annotation actions required for each F1 value.
The interactive framework outperforms the baseline consistently. On average, interactive labeling with k = 4 requires
22% fewer actions than BASELINE.
Note that k = 1 closely tracks the BASELINE performance. This suggests that when we restrict the user corrections to TYPE errors only, there are not enough errors corrected by correction propagation to overcome the additional
cost of a round of user interaction. This is further conﬁrmed
by the fact that performance increases with k.
To demonstrate the reduction in segmentation labeling, Figure 3 displays the number of segmentation actions
(START or END) needed to achieve a particular F1 value. On
average across the sampled F1 values, interactive labeling
with k = 4 requires 42% fewer segmentation actions.
Note the steep learning curve of the interactive method.
This suggests that the CRF’s poor segmentation performance early in training is quickly overcome. The result is
that after a small number of actions, annotator can reduce
the number of boundary labels needed to train the CRF, and
instead mostly provide TYPE annotation.
Table 1 displays the total number of actions required to
label all the unlabeled data. Note that BASELINE incurs a
CHOICE action if the correct labeling is the top choice.
The results in Table 1 agree with the trends in Figures 2
and 3. Note that the increase in CHOICE actions is expected,
since there are many instances where the correct labeling is
in the top k choices. The advantage of this framework is that
the cost incurred by these CHOICE actions are outweighed by
the reduction in other actions that they enable. Note also that
this reduction in effort is manifest even assuming all actions
incur the same cost. If we assume that boundary annotation
is more costly than TYPE annotation, these difference will
be even more pronounced.
Discussion
It is invariably difﬁcult to simulate the effort of a user’s interaction with a system; ultimately we would like to perform
user studies to measure labeling time exactly. While the proposed metrics make few assumptions about the user interface, there are certainly some costs we have not considered.
For example, the current metrics do not explicitly account
for the time required to read a labeling. However, the action
CHOICE, which is incremented whenever a user picks the
correct labeling among the top k predictions, can be seen
to encompass this action. Placing a higher cost on CHOICE
can account for the reading effort, possibly altering the optimal value of k. Indeed, picking the best value of k can be
achieved by ﬁrst choosing a relative cost for reading, then
performing simulations.
Also note that this work can be seen as a way to facilitate the wide-spread use of machine learning information
extraction algorithms. End-user machine learning systems
often require additional training examples to personalize the
system to the user’s data (for example, Apple Inc.’s trainable junk mail ﬁlter). The easier it is for an end-user to train
a system, the more likely it is that the system will be wellreceived, frequently used, and be given enough training data
to provide high accuracy performance. This “learning in the
wild” capability can lead to end-users more rapidly adopting
learning technologies to perform information extraction on
their own data.
Conclusions
We have described an active learning framework that explicitly models the effort required to label each example, and
have demonstrated that it can reduce the total number of annotation actions to train an information extraction system by
From these results, we can conclude that methods aiming
to reduce labeling effort can beneﬁt from considering not
only how many examples an annotator must label, but also
how much effort is required to label each example.
Acknowledgments
This work was supported in part by the Center for Intelligent Information Retrieval, in part by U.S. Government contract #NBCH040171 through a subcontract with BBNT Solutions LLC, in part by The Central Intelligence Agency, the
National Security Agency and National Science Foundation
under NSF grant #IIS-0326249, and in part by the Defense
Advanced Research Projects Agency (DARPA), through the
Department of the Interior, NBC, Acquisition Services Division, under contract number NBCHD030010. Any opinions,
ﬁndings and conclusions or recommendations expressed in
this material are the author(s) and do not necessarily reﬂect
those of the sponsor.