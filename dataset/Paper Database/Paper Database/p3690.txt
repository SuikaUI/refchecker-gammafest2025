Detecting and Simulating Artifacts in GAN Fake
Images (Extended Version)
Xu Zhang, Svebor Karaman, and Shih-Fu Chang
Columbia University, Email: {xu.zhang,svebor.karaman,sc250}@columbia.edu
Abstract—To detect GAN generated images, conventional supervised machine learning algorithms require collection of a
number of real and fake images from the targeted GAN model.
However, the speciﬁc model used by the attacker is often unavailable. To address this, we propose a GAN simulator, AutoGAN,
which can simulate the artifacts produced by the common
pipeline shared by several popular GAN models. Additionally, we
identify a unique artifact caused by the up-sampling component
included in the common GAN pipeline. We show theoretically
such artifacts are manifested as replications of spectra in the
frequency domain and thus propose a classiﬁer model based
on the spectrum input, rather than the pixel input. By using
the simulated images to train a spectrum based classiﬁer, even
without seeing the fake images produced by the targeted GAN
model during training, our approach achieves state-of-the-art
performances on detecting fake images generated by popular
GAN models such as CycleGAN.
I. INTRODUCTION
Machine learning based approaches, such as those based
on Generative Adversarial Network (GAN) , have made
creation of near realistic fake images much more feasible
than before and have enabled many interesting applications
in entertainment and education. Some high-resolution images
generated by the latest GAN models are hardly distinguishable
from real ones for human viewers , . However, this
also raises concerns in security and ethics as the traditional
perspective of treating visual media as trustworthy content is
not longer valid. As a partial remedy, the development of an
automatic tool to distinguish real from GAN generated images
will provide great value.
A typical way to design a real vs. GAN fake image classiﬁer
is to collect a large number of GAN generated images from
one or multiple pre-trained GAN models and train a binary
classiﬁer , . Unfortunately, in real world applications,
we generally have no access to the speciﬁc model used by
the attacker. To train a classiﬁer with fewer or even no fake
image from the pre-trained GAN model, we explore two
directions. 1) We identify the key up-sampling component
of the generation pipeline and theoretically show the unique
artifacts generated by this component in the frequency domain,
thus calling for the use of spectrum, rather than pixels, as
input to GAN image classiﬁers. 2) We develop an emulator
framework which simulates the common generation pipeline
shared by a large class of popular GAN models.
First, we study the GAN generation pipeline and ﬁnd out
that many popular GAN models such as CycleGAN and
StarGAN share common up-sampling layers. The upsampling layer is one of the most important modules of GAN
models, as it produces high-resolution image/feature tensors
from low-resolution image/feature tensors. Odena et al. 
show that the up-sampling layer using transposed convolution
leaves checkerboard artifacts in the generated image. In this
paper, we extend the analysis in the frequency domain and
use signal processing properties to show that up-sampling
results in replications of spectra in the frequency domain. To
directly discriminate the GAN induced up-sampling artifacts,
we propose to train a classiﬁer using the frequency spectrum
as input instead of the raw RGB pixel. We will show through
experiments that the spectrum-based classiﬁer trained even
with images from only one semantic category (e.g. real and
fake horse images) generalizes well to other unseen categories.
We further propose to address the situation where there is no
access to pre-trained GAN models by using a GAN simulator,
AutoGAN. Using only real images during training, AutoGAN
simulates the GAN generation pipeline and generates simulated “fake” images. Then the simulated images can be used
in classiﬁer training. Experiment shows that, although having
never seen any fake image generated by CycleGAN, the model
trained with simulated images still achieves the state-of-the-art
performance on CycleGAN data. It outperforms all methods
that require access to the fake images generated by the actual
GAN models used in image faking.
In summary, the paper makes the following contributions:
1) This is the ﬁrst work proposing a novel approach based
on the GAN simulator concept to emulate the process
commonly shared by popular GAN models. Such a
simulator approach frees developers of the requirement
of having access to the actual GAN models used in
generating fake images when training the classiﬁer.
2) We revisit the artifact inducted by the up-sampling
module of GANs and present a new signal processing
analysis, from which we propose a new approach to the
classiﬁer design based on the spectrum input.
II. RELATED WORK
In image generation, GAN can be applied in the following scenarios: 1) taking noise as input to produce an image ,
 , ; 2) taking an image from one semantic category (such
as horse) as input to produce an image of another semantic
category (such as zebra) , , ; 3) taking a sketch or
a pixel level semantic map as input to produce a realistic
image that is constrained by the layout of the sketch ,
 , . The latter two scenarios give users more control to
 
Fig. 1. Typical pipeline for image2image translation.
the generated content since a speciﬁc input is chosen and the
output is expected to respect properties of that input.
In response, the forensics community has been working
on detecting such generated content , , , .
Marra et al. propose to use raw pixels and conventional
forensics features extracted from real and fake images to train
a classiﬁer. Nataraj et al. propose to use the co-occurrence
matrix as the feature and show better performance than that
of classiﬁers trained over raw pixels on CycleGAN data.
McCloskey and Albright observe that GAN generated
images have some artifacts in color cues due to the normalization layers. These artifacts can be exploited for detection.
 and study the ﬁngerprints of GAN models. All the
machine learning based methods require sufﬁcient training
images generated by one or multiple pre-trained GAN models
to ensure the generalization ability of the classiﬁer.
In real-world applications, it is often not possible to have
access to the pre-trained GAN model used in generating
the fake image. We study how to remove such requirements
of accessing pre-trained models when training GAN fake
image classiﬁer by understanding, detecting and simulating
the artifact induced in the GAN generation pipeline.
III. UP-SAMPLING ARTIFACTS IN GAN PIPELINES
A. GAN Pipelines
We ﬁrst review the general pipeline for image2image or
sketch2image translation, as illustrated in Fig. 1. During the
training phase, an image translation model takes images from
two categories (e.g. horse/zebra) as input and learns to transfer
images from one category (source) to the other (target). It
contains two main components: discriminator and generator.
The discriminator tries to distinguish real images of the target
category from those generated by the generator. The generator
takes an image of the source category as input and tries to
generate an image that is similar to images of the target
category, making them indistinguishable by the discriminator.
The generator and the discriminator are alternatively trained
until reaching an equilibrium. During the generation phase, an
image from the source category is passed through the generator
to obtain an image similar to the target category.
We show more details of the generator, since it directly
synthesizes the fake image. As shown in Fig. 1, the generator
contains two components, encoder and decoder. The encoder
contains a few down-sampling layers which try to extract
high-level information from the input image and generate
a low-resolution feature tensor. The decoder, on the other
Fig. 2. Transposed convolution and nearest neighbor interpolation.
hand, contains a few up-sampling layers which take the lowresolution feature tensor as input and output a high-resolution
image. It is important to understand how the decoder renders
ﬁne details of the ﬁnal output image from the low-resolution
feature tensor with the up-sampler.
B. The Up-sampler
Although the structures of GAN models are quite diverse,
the up-sampling modules used in different GAN models are
consistent. Two most commonly used up-sampling modules
in the literature are transposed convolution (a.k.a deconvolution) , , and nearest neighbor interpolation ,
 . Interestingly, both up-samplers can be formulated as a
simple pipeline, shown in Fig. 2, where ⋆denotes convolution.
Given a low-resolution feature tensor as input, the up-sampler
increases both the horizontal and vertical resolutions by a
factor of m. For illustration purposes, we assume m = 2,
which is the most common setting. The up-sampler inserts
one zero row/column after each row/column in the lowresolution feature tensor and applies a convolution operation
in order to assign appropriate values to the “zero-inserted”
locations. The difference between transposed convolution and
nearest neighbor interpolation is that the convolution kernel
in transposed convolution is learnable, while in the nearest
neighbor interpolation it is ﬁxed (as shown in Fig. 2).
The up-sampling artifact of transposed convolution is called
“checkerboard artifact” and has been studied by Odena et
al. in the spatial domain. Here we provide further analysis
in the frequency domain. According to the property of Discrete
Fourier Transform (DFT), as shown in Fig. 3, inserting zero to
the low-resolution image is equivalent to replicating multiple
copies of the spectrum of the original low-resolution image
over the high frequency part of the spectrum of the ﬁnal
high-resolution image. For illustration purpose, we show the
spectrum of the gray-scale image. Warmer color means higher
We prove it in the 1D case and it can be easily extended
to the 2D case for images. Assume the low-resolution signal
Fig. 3. The spectrum of the low resolution image and the spectrum of the
zero inserted image.
x(n), n = 0, . . . , N−1 has N points and its DFT is X(k), k =
0, . . . , N −1, X(k) = PN−1
n=0 x(n) exp( −i2π
N kn), By inserting
0, we have a 2N-point sequence x′(n), n = 0, . . . , 2N −1,
where x′(2n) = x(n) and x′(2n+1) = 0 for n = 0, . . . , N−1.
Assume the DFT of x′(n) is X′(k). For k < N, considering
the inserted 0,
x′(n) exp(−i2π
x(n) exp(−i2π
2N k(2n)) = X(k).
For k ≥N, let k′ = k −N, thus k′ = 0, . . . , N −1, and,
x(n) exp(−i2π
2N (k′ + N)(2n))
(x(n) exp(−i2π
nk′ −i2nπ)) = X(k′)
The ﬁnal equality is due to the periodic property of the
complex exponential function. Thus, there will be two copies
of the previous low-resolution spectrum, one at [0, N −1] and
the other at [N, 2N −1]. To avoid such artifacts to persist in
the ﬁnal output image, the high frequency component needs
be removed or at least reduced. Therefore, the subsequent
convolution kernels in Fig. 2 generally need to be low-pass
ﬁlters. For illustration propose, in the rest of the paper, we
shift the spectrum such that the low frequency components
are at the center of the spectrum.
If the up-sampler is the transposed convolution, it’s not
guaranteed that the learned convolution kernel is low-pass.
Thus, the checkerboard artifact can still be observed in many
images. We show one example in Fig. 4, where the two leftmost images are one real face image and its spectrum. Images
on the right are a fake face generated from the real image and
its spectrum. The checkerboard artifact is highlighted in the
red box. In the spectrum, there are bright blobs at 1/4 and 3/4
of the width/height, corresponding to the artifact generated by
the two successive up-sampling modules in the generator. It’s
the checkerboard artifact in the frequency domain. The nearest
neighbor up-sampler uses a convolution kernel that is a ﬁxed
low-pass ﬁlter, it does eliminate the artifacts better. However,
the artifacts are still not completely removed. If the lowpass ﬁlter removes too much high frequency content, the ﬁnal
image may become too blurry and thus easily distinguishable
from the real images.
Motivated by the theoretical properties discovered above,
we propose to train GAN fake image classiﬁers using image
spectrum as input, rather than raw pixels.
IV. DETECTING AND SIMULATING THE GAN ARTIFACT
A. Classiﬁer with Spectrum Input
To make the classiﬁer recognize the artifact in the frequency
domain, instead of using raw image pixels, we propose to use
frequency spectrum to train the classiﬁer.
Fig. 4. The spectra of a real face image and a fake face image generated by
this image. Note the checkerboard artifact in the zoomed out details.
Speciﬁcally, given an image I as input, we apply the 2D
DFT to each of the RGB channels and get 3 channels of
frequency spectrum F (the phase information is discarded).
We compute the logarithmic spectrum log(F) and normalize
the logarithmic spectrum to [−1, 1]. The normalized spectrum
is the input to the fake image classiﬁer. The main goal of the
classiﬁer would thus be to reveal the artifacts identiﬁed in the
previous section to classify an image as being generated by
a GAN model or not. The spectrum-based classiﬁer achieves
better performance than the pixel-based classiﬁer, especially
when the training data contains images from only one semantic
category (Sec.V-C).
To further allow us to train a classiﬁer without fake images,
in the next section, we propose AutoGAN, which is a GAN
simulator that can synthesize GAN artifacts in any image
without needing to access any pre-trained GAN model.
B. AutoGAN
We illustrate AutoGAN in Fig. 5, which takes a real image
(I) as input and passes it through a generator (G) that has a
structure similar to the generator used in image generation
GANs. The decoder contains up-sampling module such as
transposed convolution or nearest neighbor interpolation. Note
the only knowledge assumed available is the general architecture, but not the speciﬁcs (e.g., model weights and meta
parameters) of GAN models used in fake image generation.
Conceptually, this can be considered as a “grey-box” solution,
compared to the “white-box” solution where all details of
the model are known or the “black-box” solution where zero
knowledge is available about the attack model.
The AutoGAN incorporates a discriminator (D) and an ℓ1norm loss. Instead of making the distribution of the output
from the generator to be similar to that of images of another
semantic category, as show in general image2image translation
pipeline (Fig. 1), the output of the generator is matched to the
original image itself. Formally, assuming there is a training
set {I1, . . . , In} containing n images, the ﬁnal loss function
L can be written as,
log(D(Ii)) + log(1 −D(G(Ii))) + λ ∥Ii −G(Ii) ∥1
Fig. 5. The pipeline of AutoGAN.
where D(·) is the discriminator, G(·) is the generator, G(Ii) is
the output of G(·) when taking Ii as input and λ is the tradeoff parameter between two different losses. The ﬁrst two terms
are similar to the GAN loss, where the discriminator wants to
distinguish between the generator output and the real image,
while the generator wants to fool the discriminator. The third
term is the ℓ1-norm loss function to make the input and output
similar in terms of ℓ1 distance.
We show one of the real image, the corresponding AutoGAN reconstructed image and their frequency spectra in
Fig. 6. Although the reconstructed image looks very similar to
the real image, there are some artifacts in the reconstructed image, especially in the frequency domain, capturing the unique
GAN induced artifacts as discussed in Sec. III-B earlier. By
training a classiﬁer with the real and the reconstructed images,
the classiﬁer will focus on the artifact and thus can generalizes
to other GAN fake images that have similar artifact. The
AutoGAN pipeline provides two major beneﬁts:
• It does not require any fake image in training. It only
emulates and incorporates the artifacts induced by GAN
pipeline into a real image.
• Unlike training an image2image translation model which
requires images from a pair of carefully-selected categories (such as horse and zebra), AutoGAN can take
images from any semantic category as input. This greatly
simpliﬁes the data collection process.
V. EXPERIMENT
A. Dataset
Following , we conduct experiments on CycleGAN 
images. We split the dataset based on different semantic
categories. For example, in the horse category, there are only
real horse images and fake horse images generated from
zebra images. We use the training and test split of .
Only images from the training set are used in training the
classiﬁer. And the trained classiﬁer is tested with test images. We also exclude the sketch and pixel-level semantic map from the dataset. There are a total number of
14 categories: Horse (H, 2,401/260), Zebra (Z, 2,401/260),
Yosemite Summer (S, 2,193/547), Yosemite Winter (W,
2,193/547), Apple (A, 2,014/514), Orange (O, 2,014/514), Facades (F, 800/212), CityScape Photo (City, 5,950/1000), Satellite Image (Map, 2,192/2196), Ukiyoe (U, 2,062/1,014), Van
Gogh (V, 1,900/1,151), Cezanne (C, 2,025/809), Monet (M,
2,572/872) and Photo (P, 7,359/872). The numbers behind
each category are the numbers of training/test images in that
B. Implementation Detail
We use resnet34 pre-trained with ImageNet as base network
and treat the GAN detection task as a binary classiﬁcation
Fig. 6. The original image and the AutoGAN reconstructed image
problem: real vs fake. In the training phase, we randomly crop
a 224×224 region from the original image of size 256×256.
In the test phase, the central 224 × 224 crop is used. The
batch size is 16. The optimizer is SGD with momentum and
the starting learning rate is 0.01, with a decay of 1e −2 after
each epoch. We set λ = 0.5. The network is trained for 10
To train the AutoGAN model, we use the same generator and discriminator structures and the hyper-parameters
detailed in . We only use training images from one semantic category, e.g. horse, to train the AutoGAN model.
Horse (1,067), Zebra (1,334), Yosemite Summer (1,231),
Yosemite Winter (962), Apple (995), Orange (1,019), Facades (400), CityScape Photo (2,975), Satellite Image (Map,
1,096), Ukiyoe (U, 562), Van Gogh (400), Cezanne (525),
Monet (M, 1,072) and Photo (6,287). Our implementation is
available at 
C. Training with a Single Semantic Category
We ﬁrst show the performance of the classiﬁers trained
with images from one single category. Four different types
of classiﬁers are trained for comparison:
• Img: Learned with real images and fake images generated
by cycleGAN (for example, real horse images and fake
horse images generated from zebra images);
• Spec: The training data is the same as Img, the classiﬁer
is trained with the spectrum input;
• A-Img: Learned with real image and fake image generated by AutoGAN (for example, real horse images and
reconstructed horse images from AutoGAN);
• A-Spec: The training data is the same as A-Img, the
classiﬁer is trained with the spectrum input.
All 14 categories from CycleGAN are used for training.
The classiﬁers are evaluated on all the categories and the
test accuracy is reported in Table I. Sample images and
corresponding spectra are shown from Fig.8 - Fig.10. When
trained with cycleGAN images (Img and Spec), if the training
category and test category are the same, e.g. training and
testing with horse images, the accuracy is close to perfect.
However the classiﬁer trained with cycleGAN image (Img)
struggles to generalize well to other categories. The spectrumbased classiﬁer (Spec) greatly improves the generalization
ability, indicating the spectrum-based classiﬁer is able to
discover some common artifacts induced by a GAN pipeline.
The exceptions are Apple, Orange and Facades, whose original
images have been heavily compressed and don’t show a lot of
high frequency component compared to others. When used as
test category, Map seems to be an outlier, since all categories
except itself can’t achieve promising performance on it. The
reason is that the satellite image (Fig. 9(c)) is very different
from other images since it’s taken from a very high altitude,
thus all building are small and regular. The difference is
quite obvious in the frequency domain, since it shows tilted
pattern compared to all others. Overall, the training image for
TEST ACCURACY OVER ALL SEMANTIC CATEGORIES USING ONE CATEGORY FOR TRAINING.
Cozzalino2017 
DenseNet 
XceptionNet 
Nataraj2019 
TEST ACCURACY USING THE LEAVE-ONE-OUT SETTING IN . A-IMG AND A-SPEC ARE MODELS BASED ON AUTOGAN.
the spectrum detector needs to have good coverage and be
representative.
When trained with AutoGAN generated images (A-Img),
the performance of the classiﬁer is inferior. The reason is that
the images generated by AutoGAN and cycleGAN are quite
different, especially in terms of image quality. Training with
the image pixels as input may suffer from this domain gap.
When trained with the spectrum of the AutoGAN generated
image (A-Spec), the performance of the classiﬁer is quite
promising with some of the categories (Horse, Summer and
Photo). It shows that the proposed GAN simulator is able to
simulate the artifacts induced by GAN pipeline. However, the
training category needs to be carefully selected.
It requires
the spectrum of the training images to be diverse enough for
generalization. For example, due to the stride in zebra images
as well as the snow in winter images, the zebra images and the
winter images may not have good coverage over the frequency
domain. Another interesting ﬁnding is that all the artwork
categories (Ukiyoe, Vangogh, Cezanne and Monet), although
their performances with spectrum classiﬁer are satisfactory,
their performances with AutoGAN spectrum classiﬁer are not
promising. There may have two reasons: 1) the number of
training images for AutoGAN spectrum classiﬁer is smaller
than that used in the spectrum classiﬁer training; 2) When
training the spectrum classiﬁers, the fake images generated
from real photo category are available. Those images still
contain information form real photos, allowing the classiﬁer
to see diverse spectrum.
One solution to get sufﬁcient spectrum coverage is to use
a diverse image dataset such as ImageNet or MSCOCO to
train the AutoGAN model. We randomly select 4,000 images
from MSCOCO to train an AutoGAN model. The results of
the classiﬁer trained with real and reconstructed MSCOCO
images from AutoGAN are shown in the “COCO” category
of Table I. Although the spectrum-based classiﬁer trained with
COCO images has never seen any cycleGAN images during
training, it still works reasonably well. This again proves that
the AutoGAN can capture the general GAN artifacts.
D. Leave-One-Out Setting
To compare with the state-of-the-art methods, we follow
the experimental setting detailed in by splitting all the
images into 10 folds and perform a leave-one-out test. Following , we use a pre-trained DenseNet121 model as base
network. The results are shown in Table II. The spectrumbased classiﬁers trained with CycleGAN images (Spec) and
AutoGAN images (A-Spec) are competitive with the stateof-the-art methods. Considering A-Spec has never seen any
fake image generated by any pre-trained CycleGAN model, it
shows the effectiveness of the GAN simulator.
E. Effect of Different Frequency Bands
To show which part of the spectrum affects the performance
of the spectrum-based classiﬁer, we split the full spectrum into
3 parts: low-frequency, middle-frequency and high-frequency,
such that numbers of the data points in each bands are about
the same. The points outside the selected band are set to 0.
We train 6 different classiﬁers using 3 different frequency
bands and horse images from CycleGAN and AutoGAN
respectively. The performances are shown in Fig. 7. Since the
up-sampling artifacts appear in mid and high frequency bands,
the classiﬁers trained with mid and high frequency bands show
better performances.
Horse_Auto
Fig. 7. Performance of models trained with different frequency bands.
F. Robustness to Post-Processing
We show the robustness of the proposed method with
two different post-processing methods: JPEG compression and
image resize. For JPEG compression, we randomly select one
JPEG quality factor from and apply it to
each of the fake image. For image resize, we randomly select
one image size from .
We show the performances of 2 types of models in Table III,
1) trained with images without post-processing and tested
with images subject to post-processing (Mismatched), and
2) trained and tested with images subject to post-processing
(Retrained). Performances of models trained and tested with
images without any post-processing are also given (Original).
We use horse images from CycleGAN and AutoGAN for training respectively. Since both the JPEG compression and image
resize destroy the up-sampling artifact, the model trained with
images without post-processing does not generalize to the
post-processed images, also as reported in . Training a new
model with post-processed images improves the performance
on post-processed images. Note we do not assume the retained
model has information about the speciﬁc resize factor or JPEG
quality factor used in each test image. Each training image
used in the retrained model uses randomly selected factors.
Mismatched
Horse Spec
Horse A-Spec
Horse Spec
Horse A-Spec
TEST ACCURACY AGAINST DIFFERENT POST-PROCESSING METHODS.
G. Generalization Ability
1) Generalization to Different Up-samplers: As the artifacts are induced by the up-sampler in the GAN pipeline,
we are interested in assessing the performance on images
generated by GAN with different up-samplers. We tested
2 different up-samplers, transposed convolution (Trans.) 
and nearest neighbor interpolation (NN) . We change
the Trans. up-sampler in CycleGAN/AutoGAN to the NN
up-sampler used in . For each up-sampler, we train 6
image2image translation CycleGAN models, transferring between horse↔zebra, summer↔winter and apple↔orange. We
also train 6 AutoGAN models with different up-samplers. The
results are shown in Table IV.
If the classiﬁer is trained and tested with images generated by the same up-sampler, the performance is very good.
However, if there is mismatch between the up-samplers, the
performance drops noticeably, especially when trained with
Trans. and tested with NN. The reasons are 1) the artifacts induced by different up-samplers are different; 2) NN generated
images have less artifacts . To address this issue, we can
train the classiﬁer using data generated from both up-samplers.
This approach (Comb.) works well and the model achieves
excellent performance for both up-samplers (Table IV).
TEST ACCURACY OF MODELS TRAINED WITH DIFFERENT UP-SAMPLERS.
2) Generalization to Different Models: We further test
the generalization ability of the GAN classiﬁer over images
generated with different GAN models. StarGAN and Gau-
GAN are chosen as the test models. We tested 4 classi-
ﬁers, image-based and spectrum-based classiﬁers trained with
either CycleGAN or AutoGAN images. The results are shown
in Table V. The cycleGAN image classiﬁer fails at StarGAN
images, while the spectrum-based classiﬁers work well. This
again shows the generalization ability of the spectrum-based
classiﬁer. Note that StarGAN and CycleGAN have similar generators (with 2 transposed convolution up-samplers). However,
all classiﬁers fail at GauGAN images, since the generator
structure of GauGAN (5 nearest neighbor up-samplers) is
drastically different from the CycleGAN structure.
Trained with CycleGAN Images
TEST ACCURACY TESTED WITH UNSEEN MODELS
VI. CONCLUSION
We study the artifacts induced by the up-sampler of the
GAN pipelines in the frequency domain, in order to develop
robust GAN fake image classiﬁers. To detect such artifacts, we
propose to use the frequency spectrum instead of image pixels
as input for classiﬁer training. It greatly improves the generalization ability of the classiﬁer. We further propose AutoGAN
which simulates the common GAN pipeline and synthesizes
GAN artifacts in real images. The proposed AutoGAN allows
us to train a GAN fake image classiﬁer without needing
fake images as training data or speciﬁc GAN models used
for generating fake images. The AutoGAN spectrum-based
classiﬁer generalizes well to fake images generated by GANs
with similar structures. Our future work includes extension
of the proposed GAN simulator to study other processing
modules besides the up-sampling module.
ACKNOWLEDGMENT
This material is based upon work supported by the United
States Air Force Research Laboratory (AFRL) and the Defense Advanced Research Projects Agency (DARPA) under
Contract No. FA8750-16-C-0166. Any opinions, ﬁndings and
conclusions or recommendations expressed in this material are
solely the responsibility of the authors and does not necessarily
represent the ofﬁcial views of AFRL, DARPA, or the U.S.
Government.