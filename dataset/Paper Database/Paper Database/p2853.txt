Batch Mode Active Learning and Its Application to
Medical Image Classiﬁcation
Steven C. H. Hoi†
 
 
Jianke Zhu†
 
Michael R. Lyu†
 
†Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong
‡Department of Computer Science and Engineering, Michigan State University, USA
The goal of active learning is to select the
most informative examples for manual labeling. Most of the previous studies in active
learning have focused on selecting a single unlabeled example in each iteration. This could
be ineﬃcient since the classiﬁcation model
has to be retrained for every labeled example. In this paper, we present a framework
for “batch mode active learning” that applies the Fisher information matrix to select
a number of informative examples simultaneously. The key computational challenge is
how to eﬃciently identify the subset of unlabeled examples that can result in the largest
reduction in the Fisher information. To resolve this challenge, we propose an eﬃcient
greedy algorithm that is based on the property of submodular functions. Our empirical
studies with ﬁve UCI datasets and one realworld medical image classiﬁcation show that
the proposed batch mode active learning algorithm is more eﬀective than the state-ofthe-art algorithms for active learning.
1. Introduction
Data classiﬁcation has been an active research topic in
machine learning in recent years. One of the prerequisites for any data classiﬁcation scheme is the labeled
examples. To reduce the eﬀort involved in acquiring
labeled examples, a number of active learning methods 
Appearing in Proceedings of the 23 rd International Conference on Machine Learning, Pittsburgh, PA, 2006. Copyright 2006 by the author(s)/owner(s).
have been developed in order to identify the examples
that are most informative to the current classiﬁcation
model. In the past, active learning has been successfully employed in a number of applications, including
text categorization , computer vision , and information retrieval .
Despite extensive studies of active learning, one of the
main problems with most of the existing approaches is
that only a single example is selected for manual labeling. As a result, the classiﬁcation model has to be retrained after each labeled example is solicited. In this
paper, we propose a novel active learning algorithm
that is able to select a batch of unlabeled examples
simultaneously.
A simple strategy toward achieving
batch mode active learning is to select the top k most
informative examples. The problem with such an approach is that some of the selected examples could be
similar, or even identical, to each other, and therefore do not provide additional information for model
updating. Hence, the key of batch mode active learning is that, on the one hand, all the selected examples
should be informative, and on the other hand, each
selected example should be diﬀerent from the others
and should provide unique information.
To this end, we propose a framework of batch mode
active learning that applies the Fisher information matrix to measure the overall informativeness for a set
of unlabeled examples. The main computational challenge with the proposed framework is how to eﬃciently
identify the subset of examples that are overall the
most informative to the current classiﬁcation model.
To address the computational diﬃculty, we suggest an
eﬃcient greedy algorithm that is based on the properties of submodular functions.
To evaluate the eﬀectiveness of the proposed active
learning algorithms, we apply them to the task of med-
Batch Mode Active Learning and Its Application to Medical Image Classiﬁcation
ical image classiﬁcation. Recently, the application of
machine learning techniques to medical image retrieval
and analysis has received more and more attention
 .
Due to the rapid development of computer technology, it is becoming more and
more convenient to acquire, digitally store and transfer medical imagery. Nowadays, many hospitals need
to manage several tera-bytes of medical image data
each year . Therefore, categorization of medical images is becoming imperative for a
variety of medical systems, especially in the application of digital radiology such as computer-aided diagnosis or case-based reasoning .
Since annotating medical images can only be done by
doctors with special expertise, acquiring labeled examples in medical image classiﬁcation is usually substantially more expensive than in other classiﬁcation
problems. This special feature of medical image classiﬁcation makes it more suitable for active learning.
The rest of this paper is organized as follows.
Section 2 reviews the related work on medical image categorization and active learning algorithms. Section 3
presents the general framework for batch mode active
learning. Section 4 describes two eﬃcient algorithms
for identifying a batch of unlabeled examples that are
most informative to the classiﬁcation model. Section
5 presents the empirical evaluation of our active learning methods in medical image categorization. Section
6 sets out our conclusion.
2. Related Work
Although image categorization in general is not new
to researchers in computer science, only a few studies have been devoted to the medical domain. Only
in recent years have researchers started to pay more
attention to automatic categorization of medical images . Many classiﬁcation algorithms have been applied to medical image categorization, including the large margin classiﬁers, decision trees, and neural networks.
Among them, the
large margin classiﬁers, such as support vector machines (SVM) and kernel logistic regression (KLR), appear to be most eﬀective . One
of the prerequisites for any classiﬁcation scheme is the
availability of labeled examples. Acquiring labeling information is usually a costly task. This is particularly
true for medical image categorization since medical images can only be annotated and labeled by doctors.
Hence, it is critical to reduce the labeling eﬀorts for
medical image categorization. To address this problem, we apply active learning techniques that select
only the most informative medical images for doctors
to label. It is worth noting that another approach to
reduce the labeling eﬀorts is the semi-supervised learning methods , which learns a classiﬁcation model from a mixture of labeled and unlabeled
Active learning, or called pool-based active learning,
has been extensively studied in machine learning for
several years .
Most active learning algorithms are
conducted in an iterative fashion. In each iteration,
the example with the highest classiﬁcation uncertainty
is chosen for manual labeling. Then, the classiﬁcation
model is retrained with the additional labeled example.
The steps of training a classiﬁcation model and soliciting a labeled example are iterated alternatively until
most of the examples can be classiﬁed with reasonably
high conﬁdence. One of the key issues in active learning is how to measure the classiﬁcation uncertainty of
the unlabeled examples. In several recent studies , a number of distinct classiﬁcation models are ﬁrst generated. Then, the classiﬁcation uncertainty of a test example is measured
by the amount of disagreement among the ensemble
of classiﬁcation models in predicting the labels for the
test example. Another group of approaches measures
the classiﬁcation uncertainty of a test example by how
far the example is away from the classiﬁcation boundary (i.e., classiﬁcation margin) .
of the most well-known approaches within this group
is Support Vector Machine Active Learning, developed
by Tong and Koller . Due to its
popularity and success in previous studies, we use it
as the baseline approach in our study.
One of the main problems with most existing active
learning algorithms is that only a single example is selected for labeling each time. As a result, the classiﬁcation model has to be retrained after each labeled example is solicited. In this paper, we focus on the batch
mode active learning that selects a batch of unlabeled
examples in each iteration.
A simple strategy is to
choose the top k most uncertain examples. However,
it is likely that some of the most uncertain examples
are strongly correlated and therefore will provide similar information to the classiﬁcation model. In general,
the challenge in choosing a batch of unlabeled examples is twofold: on the one hand, the examples in the
selected batch should be informative to the classiﬁcation model; on the other hand, the examples should
be diverse enough such that information provided by
diﬀerent examples does not overlap. To address this
challenge, we employ the Fisher information matrix
Batch Mode Active Learning and Its Application to Medical Image Classiﬁcation
as the measurement of model uncertainty, and choose
the set of examples that eﬃciently reduces the Fisher
information.
3. A Framework of Batch Mode Active
In this section, we describe the framework for batch
mode active learning that is based on the Fisher information matrix. We choose the logistic regression
model as the underlying classiﬁcation model because
of its simplicity and probabilistic nature. To facilitate
our discussion, we start with the linear classiﬁcation
model, followed by the extension to the nonlinear classiﬁcation model using the kernel trick.
The theoretical foundation of our batch mode active
learning is based on the work of ,
in which the authors presented a framework of active
learning based on the maximization of the Fisher information matrix. Given that the Fisher information
matrix represents the overall uncertainty of a classiﬁcation model, our goal is to search for a set of examples
that can most eﬃciently reduce the Fisher information
matrix. More speciﬁcally, this goal can be formulated
into the following optimization problem:
Let p(x) be the distribution of all unlabeled examples, and q(x) be the distribution of unlabeled examples that are chosen for manual labeling. Let α denote the parameters of the classiﬁcation model. Let
Ip(α) and Iq(α) denote the Fisher information matrix
of the classiﬁcation model for the distribution p(x) and
q(x), respectively. Then, the set of examples that can
most eﬃciently reduce the uncertainty of classiﬁcation
model is found by minimizing the ratio between the
two Fisher information matrices Ip(α) and Iq(α), i.e.,
tr(Iq(α)−1Ip(α))
For logistic regression models, the Fisher information
matrix Iq(α) is obtained by:
∂α2 log p(y|x)dx
1 + exp(αT x)
1 + exp(−αT x)xxT q(x)dx
In order to estimate the optimal distribution q(x), we
replace the integration in the above equation with a
summation over the unlabeled data and the selected
examples. Let D = (x1, . . . , xn) be the unlabeled data,
and S = (xs
2, . . . , xs
k) be the subset of selected examples, where k is the number of examples to be selected. We can now rewrite the above expression for
Fisher information matrices Ip and Iq as:
Ip(ˆα) = 1
π(x)(1 −π(x))xxT + δId
Iq(S, ˆα) = 1
π(x)(1 −π(x))xxT + δId
π(x) = p(−|x) =
1 + exp(ˆαT x)
In the above, ˆα stands for the classiﬁcation model that
is estimated from the labeled examples. Id is the identity matrix of size d × d. δ ≪1 is the smoothing parameter. δId is added to the estimation of Ip(ˆα) and
Iq(S, ˆα) to prevent them from being singular matrices.
Hence, the ﬁnal optimization problem for batch mode
active learning is formulated as follows:
S⊆D∧|S|=k tr(Iq(S, ˆα)−1Ip(α))
To extend the above analysis to the nonlinear classi-
ﬁcation model, we follow the idea of imported vector
machine by introducing a kernel
function K(x′, x) and rewriting the logistic regression
1 + exp(−yK(w, x))
According to the representer theorem, φ(w) could be
written as a linear combination of φ(x) for the labeled
examples x, i.e.,
where θ(x) is the combination weight for labeled example x. L = ((y1, xL
1 ), (y2, xL
2 ), . . . , (ym, xL
m)) stands for
the set of labeled examples, where m is the number
of labeled examples. Using the result of representer
theorem, we have K(w, x) and p(y|x) rewritten as:
θ(x′)K(x′, x)
x′∈L θ(x′)K(x′, x)
Thus, by treating (K(xL
1 , x), K(xL
2 , x), . . . , K(xL
as the new representation for the unlabeled example x,
we can directly apply the result for the linear logistic
regression model to the nonlinear case.
Batch Mode Active Learning and Its Application to Medical Image Classiﬁcation
4. Eﬃcient Algorithms for Batch Mode
Active Learning
The challenge with solving the optimization problem
in Eqn. (4) is that the number of candidate sets for
S is exponential in the number of unlabeled examples
n. As a result, it is computationally prohibitive when
the number of unlabeled examples is large. In order to
resolve the diﬃculty with the combinatorial optimization, we present a greedy algorithm that is based on
the idea of submodular function.
The key idea of this approach is to explore the general
theorem about submodular functions in : consider the optimization problem that
searches for a subset S with k elements to maximize a
set function f(S), i.e.,
If f(S) is 1) a nondecreasing submodular function,
and 2) f(∅) = 0, then the greedy algorithm will
guarantee a performance (1 −1/e)f(S∗), where S∗=
arg max|S|=k f(S) is the optimal subset. Based on this
theorem, when a set function f(S) satisﬁes the two
conditions, namely nondescreasing submodular function and f(∅) = 0, the subset S that maximizes f(S)
can be well approximated by the solution obtained by
the greedy algorithm.
In order to utilize the above theorem, the key is to
approximate the objective function in Eqn. (4) by a
submodular function.
To this end, we simplify the
objective function as follows:
x/∈S π(x)(1 −π(x))xx⊤
n + δ n −k
π(x)(1 −π(x))x⊤I−1
We ignore the second term in the above expression,
i.e., δ(n −k)tr(I−1
(S))/n, and only focus on the last
term, i.e., 
x/∈S x⊤I−1
q (S)x. This is because the second term is proportional to the smoothing parameter δ
that is usually set to be small. To further simplify the
computation, we approximate the term x⊤I−1
Let {(λk, vk)}d
k=1 be the eigenvectors of matrix Iq(S).
Then, for any x, we have
k (xT vk)2
k=1 λk(x⊤vk)2/∥x∥2
In the above, we approximate the harmonic mean
of the eigenvalues λis by their arithmetic mean,
i=1(xT vi)
= (xT vi)2/∥x∥2
2 is a PDF.
Note that this approximation will make the optimal
solution more stable than the original objective function. This is because tr(I−1
q (S)Ip) is proportional to
and therefore is sensitive to the small eigenvalues
of Iq while the approximate one does not.
By assuming that each example x is normalized as 1,
namely ∥x∥2
2 = 1, we have
π(x)(1 −π(x))x⊤I−1
π(x)(1 −π(x))
π(x)(1 −π(x))k
x′∈S π(x′)(1 −π(x′))(x⊤x′)2
Hence, the entire optimization problem in Eqn. (4) is
simpliﬁed as follows:
π(x)(1 −π(x))
x′∈S π(x′)(1 −π(x′))(x⊤x′)2 (5)
In order to explore the theorem about submodular
functions described in , we
deﬁne the set function f(S) as follows:
π(x)(1 −π(x))
π(x)(1 −π(x))
x′∈S π(x′)(1 −π(x′))(x⊤x′)2
Evidently, the problem in Eqn. (5) is equivalent to the
following optimization problem:
It is easy to see that f(∅) = 0. It is also not diﬃcult to show that f(S) is a nondecreasing submodular
function. The detailed proof can be found in the Appendix. Hence, the set function f(S) satisﬁes the two
Batch Mode Active Learning and Its Application to Medical Image Classiﬁcation
• Initialize S = ∅
• For i = 1, 2, . . ., k
– Compute x∗= arg max
x/∈S f(S ∪x) −f(S)
– Set S = S ∪x∗
Figure 1. A greedy algorithm for arg max|S|=k f(S)
conditions of the theorem about submodular functions,
and the result of the theorem can be applied directly
to the problem in (7). More speciﬁcally, the value of
the subset found by the greedy algorithm is no less
than 1 −1/e of the value of the true optimal subset.
In Figure 1, we summarize the greedy algorithm that
solves the optimization problem in (7).
Remark. To see what type of examples will be chosen by the greedy algorithm, we analyze the diﬀerence
between f(S∪x) and f(S), which is written as follows:
f(S ∪x) −f(S)
x′ /∈(S∪x)
g(x′, S)g(x, S ∪x)(x⊤x′)2
where function g(x, S) is deﬁned as
π(x)(1 −π(x))
x′∈S π(x′)(1 −π(x′))(x⊤x′)2
Based on the above expressions, we can draw the following observations:
• f(S ∪x) −f(S) ∝π(x)(1 −π(x)). This indicates
that examples with large classiﬁcation uncertainty
are more likely to be selected than examples with
small classiﬁcation uncertainty.
• The ﬁrst term in f(S ∪x) −f(S) is inverse to
x′∈S π(x′)(1 −π(x′))(x⊤x)2.
This indicates
that the optimal choice of example x should not
be similar to examples in S, i.e., the set of selected
instances.
• The second term in f(S∪x)−f(S) is proportional
to (x′x)2 for all examples x′ /∈S. This indicates
that the optimal choice of example x should be
similar to the unselected examples.
In summary, the selected examples will have the following three properties: 1) uncertain to the current
classiﬁcation model, 2) dissimilar to the other selected
examples, and 3) similar to most of the unselected examples. Clearly, these three properties are the desirable properties for batch mode active learning.
5. Experimental Result
In this section, we report our empirical study of batch
mode active learning in the application to medical image categorization.
Table 1. List of UCI machine learning datasets.
#Instances
Australian
Breast-cancer
Ionosphere
Table 2. List of medical image categories.
Category Info
#Instances
cranium, MUS
cervical spine, MUS
thoracic spine, MUS
lumbar spine, MUS
radio carpal joint, MUS
elbow, MUS
shoulder, MUS
chest, bones, MUS
abdomen, GAS
pelvis, MUS
ankle joint, MUS
MUS:“musculoskeletal system”, GAS:“gastrointestinal system”.
5.1. Experimental Testbeds
To examine the eﬀectiveness of our active learning algorithm, we ﬁrst evaluate the performance of the proposed batch active learning algorithm on ﬁve datasets
from the UCI machine learning repository1. Table 1
shows the list of the datasets used in our experiment.
We then evaluate the proposed batch mode active learning algorithm on medical image classiﬁcation.
The medical image dataset is formed by randomly selecting 2, 785 medical images from the ImageCLEF that belong to 15
diﬀerent categories. Table 2 gives the details of the
medical image testbed. Each image is represented by
2560 visual features that are extracted by the Gabor
wavelet transform. To represent the visual characteristics of medical images, the Gabor wavelet ﬁlters are emplyed to extract the texture
features of medical images.
5.2. Empirical Evaluation
Since medical image categorization is a classiﬁcation
problem, we adopt the classiﬁcation F1 performance
as the evaluation metric. The F1 metric is deﬁned as
F1 = 2 ∗p ∗r/(p + r), the harmonic mean of precision
1www.ics.uci.edu/ mlearn/MLRepository.html
Batch Mode Active Learning and Its Application to Medical Image Classiﬁcation
p and recall r of classiﬁcation. Since the F1 metric
takes into account both the precision and the recall of
classiﬁcation, it is usually preferred over other metrics.
In our experiments,
two large margin classiﬁers,
i.e., the kernel logistic regressions (KLR) (Zhu &
(SVM) , are employed as the basis classiﬁers. Two active learning algorithms are employed as
the baseline models in our studies. The ﬁrst baseline
model is the kernel logistic regression active learning
algorithm that measures the classiﬁcation uncertainty
based on the entropy of the distribution p(y|x). The
examples with the largest entropy are selected for manual labeling. We refer to this baseline model as the
logistic regression active learning model, or KLR-AL
for short. The second reference model is based on support vector machine active learning . In this method, the classiﬁcation uncertainty
of an example x is determined by its distance from
the decision boundary wT x + b = 0. The unlabeled
examples with the smallest distance are selected for
We refer to this approach as SVM active
learning, or SVM-AL for short.
To evaluate the performance of the competing active
learning algorithms, we ﬁrst randomly pick l training
samples from the dataset for each category consisting
of an equal number of negative and positive examples.
We then train both SVM and KLR classiﬁers using
the l labeled examples, respectively. Based on these
two initially trained models, additional s (referred to
as the “batch size”) unlabeled examples are chosen for
manual labeling for each active learning method. To
see the eﬀects of the selected examples on the classiﬁcation models, we also train the two reference models
by randomly selecting s examples for manually labeling, which are referred to as SVM-Rand and KLR-
Rand, respectively. For performance comparison, every experiment is carried out 20 times, and the averaged classiﬁcation F1 with their standard errors are
calculated and used for ﬁnal evaluation.
5.3. Experimental Results
Table 3 summarizes the experimental results of the
ﬁve UCI datasets for the proposed batch mode active learning algorithm as well as the two baseline approaches for active learning. Both the number of initially labeled examples l and the number of selected
examples s for each iteration are set to be 10. Due to
the space limitation, we only presented the results of
the ﬁrst two iterations. Compared to the two reference
models using randomly selected examples, i.e., SVM-
Rand and KLR-Rand, all the three active learning algorithms are able to achieve noticeable improvement
in F1 across all ﬁve UCI datasets. We also observed
that the improvement made by the active learning algorithms in the second iteration is considerably smaller
than that of the ﬁrst iteration. In fact, after the third
iteration, the improvement made by the active learning algorithms starts to diminish.
Comparing to the two baseline active learning algorithms, we observe that the proposed approach performs signiﬁcantly (p < 0.05) better over the dataset
“Australian”, “Ionosphere”, and “Sonar”, according
to the student-t test. We further examine the performance of the proposed approach by varying the batch
size from 10 to 50. Fig. 2 shows the experimental results of the three active learning methods using different batch sizes. Again, we observe that the batch
mode active learning method consistently outperforms
the other methods across diﬀerent batch sizes.
Table 4 summarizes the experimental results of the
ﬁrst two iterations for the medical image dataset. 40
labeled examples are used for initially training, and
20 examples are selected for each iteration of active
learning. Similar to the UCI datasets, the three active learning algorithms perform considerably better
than the two reference models across all the categories.
The most noticeable case is category 3, where F1 is
improved from around 40% to about 50% by the active learning algorithms. Furthermore, the comparison between the batch mode active learning algorithm
and the two non-batch active learning algorithms revealed that the proposed algorithm for batch mode
active learning always improves the classiﬁcation performance. For a number of categories, including category 3, 10, 12, and 15, the improvement in the F1
measurement is statistically signiﬁcant (p < 0.05) according to the student-t test.
Similar improvements
were also observed for diﬀerent batch sizes. We did
not report those results due to the space limitation.
6. Conclusion
This paper presented a general framework for batch
mode active learning.
Unlike the traditional active
learning that focuses on selecting a single example in
each iteration, the batch mode active learning allows
multiple examples to be selected for manual labeling.
We use the Fisher information matrix for the measurement of model uncertainty and choose the set of
examples that will eﬀectively reduce the Fisher information.
In order to solve the related optimization
problem, we proposed an eﬃcient greedy algorithm
that approximates the objective function by a submodular function.
Empirical studies with ﬁve UCI
Batch Mode Active Learning and Its Application to Medical Image Classiﬁcation
Table 3. Evaluation of classiﬁcation F1 performance on the UCI datasets.
Active Learning Iteration-1
Active Learning Iteration-2
Australian
Ionosphere
Batch Size of Active Learning
Classification F1 Performance (%)
(a) Australian
Batch Size of Active Learning
Classification F1 Performance (%)
Batch Size of Active Learning
Classification F1 Performance (%)
Figure 2. Evaluation of classiﬁcation F1 performance on the UCI datasets with diﬀerent batch sizes.
datasets and one medical image dataset demonstrated
that the proposed batch mode active learning algorithm is more eﬀective than the margin-based active
learning approaches, which have been the dominant
methods for active learning.
Acknowledgements
The work described in this paper was fully supported
by two grants, one from the Shun Hing Institute of Advanced Engineering, and the other from the Research
Grants Council of the Hong Kong Special Administrative Region, China (Project No. CUHK4205/04E).
Theorem 1 The set function f(S) in Eq.
nondecreasing submodular function.
Proof. To prove that the set function f(S) is a submodular function, we use the suﬃcient and necessary
condition for submodular functions ,
i.e., for any two sets A ⊆B, for any element x /∈B,
f(S) is a submodular function if and only if the following condition holds:
f(A ∪x) −f(A) ≥f(B ∪x) −f(B)
In order to show the above property, we compute the
diﬀerence f(S ∪x) −f(S) for x /∈S, i.e.,
f(S ∪x) −f(S)
x′ /∈(S∪x)
g(x′, S)g(x, S ∪x)(x⊤x′)2
where the function g(x, S) is already deﬁned in Eqn.
First, according to the deﬁnition of function
g(x, S) in Eqn.
(8), g(x, S) ≥0 for any x and S.
Thus we have f(S ∪x) ≥f(S); therefore, f(S) is a
nondescreasing function. Second, as indicated by the
above expression, the diﬀerence f(S ∪x) −f(S) is a
monotonically decreasing function.
As a result, we
have f(A∪x)−f(A) ≥f(B ∪x)−f(B) when A ⊆B.
In conclusion, the function f(S) is a nondecreasing
submodular function.