Recover Canonical-View Faces in the 明Tild with Deep
Neural Networks
Zhenyao Zhu1
Xiaogang Wang2
Xiaoou Tang1,3
1 Departm巳nt of Information Engin巳ering ，Th巳Chines巳University of Hong Kong
2Department ofElectronic Engineering, The Chinese University ofHong Kong
3Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
zz 012 日ie
. cuh k. edu
p 1 uo .1 h 工@gm a i l
xgwang@ee . cuhk . edu . hk
xtang@ i e . cuhk. edu . hk
Face images in the wild undergo larg巳intra-p巳rsonal variations, such as poses,
illuminations, occlusions, and low resolutions, causing great challenges to facerelated applications. This paper addresses this challenge by proposing a new deep
learning framework that can recover the canonical view of face images. It dramatically reduces the intra-person variances, while maintaining the inter-person
discriminativeness. Unlike the existing face reconstruction methods that were either evaluated in controlled 2D environment or employed 3D information, our
approach directly learns the transformation from the face images with a complex
set of variations to their canonical views. At the training stage, to avoid the costly
process of labeling canonical-view images from the training set by hand, we have
devised a new measurement to automatically select or synthesize a canonical-view
image for each identity.
As an application, this face recovery approach is used for face v巳rification. Facial
features are learned from the recovered canonical-view face images by using a
facial component-based convolutional neural network. Our approach achieves the
state-of-the-art performance on the LFW dataset
1 Introduction
Dealing with variations of face images is the key challenge in many face-related applications. For
example, in face recognition, most research efforts have focus巳d on how to distinguish intra-personal
variations of poses, lightings, expressions, occlusions, ages, makeups, and resolutions from interpersonal variation which distinguishes face identities. The aim of face hallucination is to reconstruct
high-resolution face images from low-resolution ones , or to remove glasses from face images
口9 ]. For face synthesis, people produce images under di旺erent ages 口2] ，poses [町，and illuminations . There are also research works on matching face photos with sketches of di旺erent
styles and synthesizing sketches from photos. Recently, a 3D viewing system was proposed to
reconstruct 3D face models from real-world images.
To deal with face variations, the 巳xisting methods can be roughly divided into two categories: robust
feature extraction and face normalization. In the 白rst category, global features such as Eigen faces
 , Fisher faces , and their extensions can cover global variations due to small pose and
simple illumination chang口，but do not work well under large poses and complex illumination
conditions. They are not robust to local distortions, such as expressions and occlusions either. The
high dimensional concatenations of the local descriptors, such as Haar , Gabor , and LBP
 , have demonstrated their robustness to local distortions and achieved significant improvement in
Figure 1: The proposed method can recover the images of canonical view and illumination from images with
variations.
For 巳xample ，
in 巳ach row, w巳show
th巳images and the reconstruct巳d images of the sam巳
identity. The reconstructed imag巳s dramatically reduce th巳intra-p巳rson variances, while maintaining th巳inter­
person discriminativeness
face recognition . In addition to the above hand-crafted descriptors, other existing researches
studies have also tried to integrate multiple features or directly learn features from raw pixels, such
as using random-projection trees , local quantized paUerns , and deep learning . For example, Sun et al. learned face representation with a deep model through
face identification, which is a challenging multi-class prediction task. The comrnon weakness of the
feature extraction approaches is that they are all sensitive to large intra-person variations
s巳cond cat巳gory，approaches tend to recov巳r an imag巳
in th巳canonical
vi巳w (with frontal
pose and neutral lighting) from a face image under a large pose and a di旺巳rent lighting, so that
it can b巳used as a good normalization. There are 3D- and 2D-based methods. The 3D-based
methods aim to recover the frontal pose by 3D geometrical transformations , which first aligns
a 2D face image to a 3D face model and then rotates it to r巳nder the frontal view. The existing
2D-based methods infe町ed the frontal pose with graphical models, such as Markov Random
Fields (MRF), where the correspondences betwe巳n nodes in the MRF are learned from images in
di旺erent poses. However, capturing 3D data adds additional cost and resources, and MRF-based
face synthesis depends heavily on good alignment, while the results ar巳often not smooth on realworld images. The recent work directly learned transformation between face images in arbitrary
views and frontal views and obtained good results in the MultiPIE dataset .
In this paper, we aim to recover the canonical view from a 2D face image taken under an arbitrary
pose and lighting condition in the wild. It is a big challenge to learn such a complex set of pos巳and
lighting transforms in uncontrolled environment, and the learned transformation function must be
highly multi-modal. We wiU show that a carefully designed deep learning framework can overcome
this challenge, benefiting from its great learning capacity. Some examples of recovered face images
with our approach are shown in Figure 1.
Our framework contains two steps: (1) canonical-view image selection, and (2) face r巳covery. First,
in order to learn the transformation between face images and their canonical views, we must select
a representative image for 巳ach identity, which is taken in the 仕ontal view, under neutral lighting condition and with high resolution. To avoid selecting them by hand, we first develop a new
measurement, which measures the face images' symrnetry and sharpness. We then learn the transformation with a carefully designed deep network, which can be considered as a regression from
images in arbitrary views to the canonical-view.
A sample application of this framework is face verification. A facial component-based convolutional neural network is developed to learn hierarchical feature representations from the rl巳coverl巳d
canonical-view images. These features ar巳robust for face verification, since the recover巳d images
Figure 2: The images of two identities are ranked according to three different criterions in (a) and (b): face
symmetry (the first row), matrix rank (the second row), and symmetry combined with matrix rank (the third
row). In each row, the first five imag巳s and the last five images are visualized
(b) Face Selection
Figure 3: Pipeline of canonical view face selection (b) and face recovery (a)
already remove large face variations. It also has pot巳ntial applications to other problems, such as
fac巳hallucination ，fac巳sketch synthesis and r巳cognition ，and fac巳attribute estimation.
In summary, this work has the following key contributions. Firstly, to the best of our knowledge,
this is the first work that can recover canonical-view face images using only 2D information from
face images in the wild. This method shows stat巳-of-the-art performanc巳on face verification in th巳
wild. Secondly, the reconstructed images are of high-quality.
2 Canonical View Face Recovery
A New Measurement for Canonical View Face Images
Although various facial measurements have b巳en studied in th巳
lit巳rature ，
th巳y have mainly
focused on the image qualities, such as noise ratio and resolutions, and seldom considered how to
determine whether a face image is taken in frontal view. We have devised a facial measurement for
frontal view face images by combining the rank and symmetry of rnatrix. For exarnple, as shown
in Figure 2, we collect the images of a subject and visualize them according to the following three
criterions: (1) di旺巳rence b巳tween the left half face and the right half face in ascending order (face
symmetry), (2) rank of the image in descending order, and (3) the combination of (1) and (2). In
the first row of Figure 2, we observe that measuring symmetry as in (1) is effective for frontal view
images, but it prefers the images in low resolutions. Although the second row shows that larger rank
Figure 4: Examples of face reconstruction on the LFW dataset. For each pair, the left one is an original image
of the LFW dataset and the right one is the recovered image.
. Originallmages
• Recovered Images
Figure 5: Comparisons of face verification performance of di旺erent features on the LFW dataset
indicates sharper images, the images sometimes do not have frontal views. Th巳combination of (1)
and (2) achieves the best result as shown in the third row in Figure 2.
We formulate this measurement as shown below. Let a matrix Y i 巳]R64x64 denote a face image of
the i-th identity and ][])i be the set of images of identity i, Yi ε
ID\ . The frontal view measurement
can be written as,
M(Yi) = 11 Y iP - Y iQ II}λ11 Y i 11 川
whereλis a constant coefficient, 11 . II F is the Frobenius norm, and 11 . 11* denotes the nuclear norm,
which is the sum of the singular values of a matrix. P , Qε ]R64x 64 are two constant matrixes with
P = diαg([13 2 ' 032]) and Q = diαg( [032 ， 132]), whcrc diαg(.) indicatcs thc diagonal matrix. Thc
first term in Equation (1) measures the face's symmetry, which is the difference between the left half
and the right half of the face, and the second term measures the rank of the face. Small巳r value of
Equation (1) indicates the face is more likely to be in frontal view.
We can select a frontal face image as a representative for each identity and then learn a mapping,
which transforms the face image in arbitrary view to the frontal view. This sel巳ction can be achieved
in several ways. In this report, we simply choose the image with the minimum measurement as
the frontal face for each identity. However, using a linear combination to calculate the frontal face
image is also possible. We will report results in the future
2.2 Face Recovery
After face selection, we adopt a deep network to recover the frontal vi巳w image by minimizing the
loss error
E( {X?k} ; W) =艺艺
11 Y i - j(X?k ; W) 11 手，
where i is the index of identity and k indicates the k-th sample of identity i. XO and Y denot巳the
training image and the target image (the selected frontal face), respectively. W is a set of parameters
of the deep network
As shown in Figure 3, the deep network contains three convolution layers. The first two are followed
by the max pooling layers and the last one is followed by a fully-connect layer. Diff，巳rent from the
conventional CNN, whose filters share weights, our filters are localized and do not share weights
because we assume di旺erent face regions should employ different features. The input XO, the
output Y (predicted imag时，and the target Y 缸e in the size of 64 x 64. All of them are transformed
to gray-scale and their illuminations are coπected as in . At each convolutionallayer, we obtain
32 output channels by learning non-shared filters, each of which is in the size of 5 x 5. The cell size
Figur巳6 : Canonical view fac巳reconstruction s of s巳veral identiti巳s
of the sub-sampling layer is 2 x 2. The l-th convolutionallayer can by formulated as
x;1Ju= σ(艺w;川。(X~ ) uv + b~ ) ，
where Wι uv and (X~ ) uv denote the filter and the image patch at the image location (川)，re
sp巳ctlV巳ly . p, q are the index巳s of input and output channels. For instance, in the first convolutional
layer, p = 1, q = 1...32. Thus, X~;~v indicates the q-th channel ouψut at the location (u, v) ; that is
the input to the 1+ 1-th layer.σ (x ) = max(O, x) is the rectified linear function and 0 indicates the
element-wise product. The bias vectors are denoted as b . At the fully-connect layer, we recover the
image Y by
Y = W LX L+ bL.
Equation (2) is non-linear because of the activation functions in the deep network. We solve it by
stochastic gradi巳nt desc巳nt (SGD) with back-propagation as in . As shown in Figllre 3,
at the l-th ωlVolutional layer, the gradient of the filter at position u, v is comp叫by 满v
l ) ，川Xl- l )川，where E is the loss error defined in Equation (2) and e is the back-propagation
error. e is obtained in a recursive manner as e l = P' 0 (el+1 ( 1), where ( is the Kror肌kerproduct
that up-samples el+1 to the same size as e l , and 1l ' is the derivative of the activation function at
the l-th layer. At the l-th fully-connect layer, the gradient of the weight matrix is calculated by
= XI~iT, which is the 侧叫roduct of the back-propagation 巳err町
阳u川t of th
layer. e is also derived in a recursive way as e l
1l ' 0 (Wl+1 T el+1 ). For instance, if layer 1 is
activated using sigmoid function, then e l = X l 0 (1 -
X l ) 0 (Wl+1 T el+1 ). Furthermore, dropout
learning is adopted at each layer to avoid ov巳r-fitting .
2.3 Effectiveness of Face Recovery
S巳veral 巳xamples of the recovered canonical view images are shown in Figure 4. In ord巳r to demonstrate the quality of the recovered image, we compare the performance of the existing feature extraction methods, including LBP , HOG , and Gabor , when they are 巳xtracted from the
reconstructed image and the original image. We adopt the testing data of LFW dataset. For each
of the above features, we extract it from the face image in a regular grid of size 8 x 8 and then
apply PCA and LDA. Th巳performance of face verification are repo此ed in Figure 5, where shows
that the existing feature extraction methods can be improved when they are applied on the recovered
image, which is a good normalization to account for different face variations. More examples of
face recovery for one identity can b巳found in Figure 6.
τ…' t- t- t
Convolutions
Convolutiσns 5ubsampling
5ubsampling
Figure 7: Architecture of the facial component-based network. The network contains five CNNs, each of
which takes a pair of whole faces or facial components as input. The sizes of the whole face, forehead, eye,
nose, and mouth are 64 x 64, 22 x 64, 24 x 64, 28 x 30, and 20 x 56, resp巳ctlV巳ly.
First，巳ach CNN
leams the joint representation of the pairs of input. A logistic regression layer then concatenates all the joint
representations as features to predict whether the two face images belong to the same identity.
Facial Component Deep Network for Face Verification
The canonical view images can be used as input to a facial component deep network (FCN), which
learns relational features from two images for face verification, as shown in Fig.7. Similar architecture has been adopted by , where the original images are used as the input and a large number of
networks have to be trained. Unlike , the FCN is applied on the canonical images that reduce the
face variations. Therefore, five networks concatenation is enough to achieve good result as discussed
below. Learning FCN contains three steps, including facial component-based patch recovering and
cropping, feature learning, and feature reduction.
In the first step, for each pair of training images, we r巳cover their canonical view imag巳s and th巳n
extract 5 landmarks. Imag巳patches of different facial components are cropped based on the above
landmarks. Specifically, we extracted patches from forehead, eyes, nos巳，and mouth.
In the second step, each patch p创r is utilized to train a deep network. Then, multiple networks are
concatenated together by a fully-connected layer to learn the feature representation. Each network
compris巳s two convolutionallayers and two sub-sampling layers. Figure 7 specifies the archit巳cture
of concatenation of multiple networks, where the parameters are optimized using stochastic gradient
descent with back-propagation. In particular, as in Section 2.2, we pass the back-propagation 巳町or
backwards and then update the weights or filters in each layer. We adopt the entropy error instead
ofthe loss e町or because we need to predict the labels Y
Err = ylogy + (1 - y) 10g(1 - y) ,
where y are the predicted labels, and y , y ε {O ， l}K , with Y k = 1 indicating that the input images
belong to the k-th identity.
In the third step, we employ PCA and ensemble of Support Vector Machines (SVM) for face verificatlOn
Experiments
We evaluate our approach on the LFW dataset, which is collected from internet and contains 5749
people with 13, 233 face images in total, which vary in terms of their poses, illuminations, resolutions, makeups, and occlusions. The average number of images for each identity is 2 . 3土9 . 01 ，where
Accuracy( % )
Associate-Predict 
Joint Bayesian• 
Convnet-RBM 
Tom-vs-Pete 
Tom-vs-Pete+Attribute 
High-dim LBP• 
TL Joint Bayesian 
FR+FCNt (whol巳fac巳+components)
Joint Bayesian 
Fisher Vector Faces 
High-dimLBP 
FR+FCN (whole fac巳)
FR+FCN (whol巳fac巳+components)
Face++ 
Table 1: Method Comparisons
5438 people have less than 5 images and only 143 people have more than 10 images. Due to the
imbalance of LFW, it is not suitable to train the face recovery network because of the following reasons: (1) training examples are not enough for most of the identities, (2) they may not have frontal
view images, and (3) the size of the dataset is not 巳nough for a deep learning-based method. PubFig
 and WDRef are two larg巳r datasets than LFW. How巳ver，PubFig only has 200 people,
which means the identity variation is insufficient, while WDRef is not publicly available. We train
our models on the CelebFaces , which contains 87,628 face images of 5436 identities. The
average number of images for each identity is 15.9土8.0 ，which shows that it is more balanced than
We compare our results with the existing best-performing approaches suggested by the LFW benchmark' . There are two experimental settings. First, the upper part of Table 1 shows the results
employing outside training data other than LFW under the restricted protocol. Most of the bestperforming methods such as 凹，11, 7] belong to the second setting. Second, the m巳thods in the
lower part are trained on LFW under the unrestricted protocol, using only the training data in LFW.
Our methods achieve the state-of-the-art performance in both the above settings. For instance, in
th巳first setting, we train the FR+FCNt (whole face+components) on the outside data of two hundred thousand image pairs generated from the PubFig and CelebFaces. The FR+FCN (whole
face+components) achieves the accuracy of 96.45 percent, which performs slightly better than the
best results and improves 4 percent compared to . This is because the canonical view images can reduce large face variations. In the second setting, we achieve th巳second best result. The
best-performing method is a commercial system , where the number of faciallandmark alignment and the size of training data are not clear. Our method employs the recovered the canonical
view images to reduce the face variations. In this case, five facial key points alignment is enough to
achieve good result. Figure 8 and 9 plots the ROC curves of the above methods. For more details
please refer to the project page at . 工e . cuhk . edu . hk.
Conclusions and Discussions
In this pap巳r，we have proposed a new deep learning framework that can rl巳cover the canonical view
face images from images in arbitrary wild conditions. With this framework, given the face images
of any new identity, th巳canonical view of thes巳images can b巳巳fficiently recovered. This approach
has many potential applications, such as face hallucination, face sketch synthesis and recognition,
and face attribute estimation.
' .umass.edu/lfw/results.html
--------------, --------------- -----,-------------
-一- Associate-Predict
一一- Joint Bayesiant
一- Tom-vs-Pete
一一- Tom-vs-Pete+Attribute
一- High-dim LBpt
…------，可，
TL Joi nt Bayesian
一一- Convnet-RBM CelebFaces
一- FR+FCNt(whole face+components)
false positive rate
Figure 8: ROC curve under the LFW r，巳stricted protocol
一一- Fisher Vector Faces
一一- Joint Bayesian
一- high-dim LBP
一一- Convnet-RBM
一一- FR+FCN(whole face)
一一- FR+FCN(whole face+components)
false positive rate
Figure 9: ROC curv巳under 由e LFW unrestricted protocal
We apply our face recovery framework to the task of face verification and outperform the state-ofthe-art approaches. We also show that the existing face recognition methods can be improved when
they adopt our face recovery as normalization and pre-processing.
A recent work reported 98.5 percent accuracy with Gaussian Processes and combined multiple
training sets. This could b巳du巳
to fact that the nonparametric Bayesian kernel method can adapt
model complexity to data distribution. This could be another interesting direction to be explored in
the future.
Acknowledgement
This work is p征tially supported by the General Research Fund sponsored by the Research Grants
Council of Hong Kong (Project No.CUHK 416510 and 416312) and Guangdong Innovative Research TI巳am Program (No.201001D0104648280).