Islam and Zhang ﻿Brain Inf. 5:2
 
ORIGINAL RESEARCH
Brain MRI analysis for Alzheimer’s disease
diagnosis using an ensemble system of deep
convolutional neural networks
Jyoti Islam*  and Yanqing Zhang
Alzheimer’s disease is an incurable, progressive neurologicalbrain disorder. Earlier detection of Alzheimer’s disease
can help with proper treatment and prevent brain tissue damage. Several statistical and machine learning models
have been exploited by researchers for Alzheimer’s disease diagnosis. Analyzing magnetic resonance imaging (MRI) is
a common practice for Alzheimer’s disease diagnosis in clinical research. Detection of Alzheimer’s disease is exacting due to the similarity in Alzheimer’s disease MRI data and standard healthy MRI data of older people. Recently,
advanced deep learning techniques have successfully demonstrated human-level performance in numerous fields
including medical image analysis. We propose a deep convolutional neural network for Alzheimer’s disease diagnosis
using brainMRI data analysis. While most of the existing approaches perform binary classification, our model can identify different stages of Alzheimer’s disease and obtains superior performance for early-stage diagnosis. We conducted
ample experiments to demonstrate that our proposed model outperformed comparative baselines on the Open
Access Series of Imaging Studies dataset.
Keywords:  Neurological disorder, Alzheimer’s disease, Deep learning, Convolutional neural network, MRI, Brain
© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License
( which permits unrestricted use, distribution, and reproduction in any medium,
provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license,
and indicate if changes were made.
1  Background
Alzheimer’s disease (AD) is the most prevailing type
of dementia. The prevalence of AD is estimated to be
around 5% after 65  years old and is staggering 30% for
more than 85 years old in developed countries. It is estimated that by 2050, around 0.64 Billion people will be
diagnosed with AD . Alzheimer’s disease destroys
brain cells causing people to lose their memory, mental
functions and ability to continue daily activities. Initially,
Alzheimer’s disease affects the part of the brain that
controls language and memory. As a result, AD patients
suffer from memory loss, confusion and difficulty in
speaking, reading or writing. They often forget about
their life and may not recognize their family members.
They struggle to perform daily activities such as brushing hair or combing tooth. All these make AD patients
anxious or aggressive or to wander away from home.
Ultimately, AD destroys the part of the brain controlling
breathing and heart functionality which lead to death.
There are three major stages in Alzheimer’s disease—
very mild, mild and moderate. Detection of Alzheimer’s
disease (AD) is still not accurate until a patient reaches
moderate AD stage. For proper medical assessment of
AD, several things are needed such as physical and neurobiological examinations, Mini-Mental State Examination (MMSE) and patient’s detailed history. Recently,
physicians are using brain MRI for Alzheimer’s disease
diagnosis. AD shrinks the hippocampus and cerebral
cortex of the brain and enlarges the ventricles . Hippocampus is the responsible part of the brain for episodic
and spatial memory. It also works as a relay structure
between our body and brain. The reduction in hippocampus causes cell loss and damage specifically to synapses
and neuron ends. So neurons cannot communicate anymore via synapses. As a result, brain regions related to
remembering (short-term memory), thinking, planning
Open Access
Brain Informatics
*Correspondence: 
Department of Computer Science, Georgia State University, Atlanta, GA
30302‑5060, USA
 
Islam and Zhang ﻿Brain Inf. 5:2
and judgment are affected . The degenerated brain
cells have low intensity in MRI images . Figure 1 shows
some brain MRI images with four different AD stages.
For accurate disease diagnosis, researchers have developed several computer-aided diagnostic systems. They
developed rule-based expert systems from 1970s to 1990s
and supervised models from 1990s . Feature vectors
are extracted from medical image data to train supervised systems. Extracting those features needs human
experts that often require a lot of time, money and effort.
With the advancement of deep learning models, now we
can extract features directly from the images without the
engagement of human experts. So researchers are focusing on developing deep learning models for accurate disease diagnosis. Deep learning technologies have achieved
major triumph for different medical image analysis tasks
such as MRI, microscopy, CT, ultrasound, X-ray and
mammography. Deep models showed prominent results
for organ and substructure segmentation, several disease
detection and classification in areas of pathology, brain,
lung, abdomen, cardiac, breast, bone, retina, etc. .
As the disease progresses, abnormal proteins (amyloid-β [Aβ ] and hyperphosphorylated tau) are accumulated in the brain of an AD patient. This abnormal
protein accumulation leads to progressive synaptic, neuronal and axonal damage. The changes in the brain due to
AD have a stereotypical pattern of early medial temporal
lobe (entorhinal cortex and hippocampus) involvement,
followed by progressive neocortical damage . Such
changes occur years before the AD symptoms appear. It
looks like the toxic effects of hyperphosphorylated tau
and/or amyloid-β [Aβ ] which gradually erodes the brain,
and when a clinical threshold is surpassed, amnestic
symptoms start to develop. Structural MRI (sMRI) can
be used for measuring these progressive changes in the
brain due to the AD. Our research work focuses on analyzing sMRI data using deep learning model for Alzheimer’s disease diagnosis.
Machine learning studies using neuroimaging data for
developing diagnostic tools helped a lot for automated
brain MRI segmentation and classification. Most of
them use handcrafted feature generation and extraction from the MRI data. These handcrafted features are
fed into machine learning models such as support vector
machine and logistic regression model for further analysis. Human experts play a crucial role in these complex
multi-step architectures. Moreover, neuroimaging studies often have a dataset with limited samples. While
image classification datasets used for object detection
and classification have millions of images (for example,
ImageNet database ), neuroimaging datasets usually
contain a few hundred images. But a large dataset is vital
to develop robust neural networks. Because of the scarcity of large image database, it is important to develop
models that can learn useful features from the small dataset. Moreover, the state-of-the-art deep learning models
are optimized to work with natural (every day) images.
These models also require a lot of balanced training data
to prevent overfitting in the network. We developed a
deep convolutional neural network that learned features
directly from the input sMRI and eliminated the need
for the handcrafted feature generation. We trained our
model using the OASIS database that has only 416
sMRI data. Our proposed model can classify different
stages of Alzheimer’s disease and outperforms the offthe-shelf deep learning models. Hence, our primary contributions are threefold:
We propose a deep convolutional neural network
that can identify Alzheimer’s disease and classify the
current disease stage.
Our proposed network learns from a small dataset
and still demonstrates superior performance for AD
diagnosis.
We present an efficient approach to training a deep
learning model with an imbalanced dataset.
The rest of the paper is organized as follows. Section 2
discusses briefly about the related work on AD diagnosis. Section  3 presents the proposed model. Section  4
reports the experimental details and the results. Finally,
in Sect. 5, we conclude the paper with our future research
direction.
2  Related work
Detection of physical changes in brain complements clinical assessments and has an increasingly important role
for early detection of AD. Researchers have been devoting their efforts to neuroimaging techniques to measure
pathological brain changes related to Alzheimer’s disease.
Machine learning techniques have been developed to
build classifiers using imaging data and clinical measures
for AD diagnosis . These studies have identified
Fig. 1  Example of different brain MRI images presenting different
Alzheimer’s disease stages. a Non-demented; b Very mild dementia; c
Mild dementia; d Moderate dementia
 
Islam and Zhang ﻿Brain Inf. 5:2
the significant structural differences in the regions such
as the hippocampus and entorhinal cortex between the
healthy brain and brain with AD. Changes in cerebrospinal tissues can explain the variations in the behavior
of the AD patients . Besides, there is a significant
connection between the changes in brain tissues connectivity and behavior of AD patient . The changes
causing AD due to the degeneration of brain cells are
noticeable on images from different imaging modalities,
e.g., structural and functional magnetic resonance imaging (sMRI, fMRI), position emission tomography (PET),
single photon emission computed tomography (SPECT)
and diffusion tensor imaging (DTI) scans. Several
researchers have used these neuroimaging techniques for
AD Diagnosis. For example, sMRI , fMRI ,
PET , SPECT and DTI have been
used for diagnosis or prognosis of AD. Moreover, information from multiple modalities has been combined to
improve the diagnosis performance .
A classic magnetic resonance imaging (MRI)-based
automated AD diagnostic system has mainly two building blocks—feature/biomarker extraction from the MRI
data and classifier based on those features/biomarkers.
Though various types of feature extraction techniques
exist, there are three major categories—(1) voxel-based
approach, (2) region of interest (ROI)-based approach,
and (3) patch-based approach. Voxel-based approaches
are independent of any hypothesis on brain structures
 . For example, voxel-based morphometry measures local tissue (i.e., white matter, gray matter and
cerebrospinal fluid) density of the brain. Voxel-based
approaches exploit the voxel intensities as the classification feature. The interpretation of the results is simple
and intuitive in voxel-based representations, but they suffer from the overfitting problem since there are limited
(e.g., tens or hundreds) subjects with very high (millions)dimensional features , which is a major challenge for
AD diagnosis based on neuroimaging. To achieve more
compact and useful features, dimensionality reduction is
essential. Moreover, voxel-based approaches suffer from
the ignorance of regional information.
Region of interest (ROI)-based approach utilizes the
structurally or functionally predefined brain regions
and extracts representative features from each region
 . These studies are based on specific hypothesis on abnormal regions of the brain. For
example, some studies have adopted gray matter volume
 , hippocampal volume and cortical thickness . ROI-based approaches are widely used due
to relatively low feature dimensionality and whole brain
coverage. But in ROI-based approaches, the extracted
features are coarse as they cannot represent small or subtle changes related to brain diseases. The structural or
functional changes that occur in the brain due to neurological disorder are typically spread to multiple regions
of the brain. As the abnormal areas can be part of a single ROI or can span over multiple ROIs, voxel-based or
ROI-based approaches may not efficiently capture the
disease-related pathologies. Besides, the region of interest (ROI) definition requires expert human knowledge.
Patch-based approaches divide the whole
brain image into small-sized patches and extract feature
vector from those patches. Patch extraction does not
require ROI identification, so the necessity of human
expert involvement is reduced compared to ROI-based
approaches. Compared to voxel-based approaches,
patch-based methods can capture the subtle brain
changes with significantly reduced dimensionality. Patchbased approaches learn from the whole brain and better
captures the disease-related pathologies that results in
superior diagnosis performance. However, there is still
challenges to select informative patches from the MRI
images and generate discriminative features from those
A large number of research works focused on developing advanced machine learning models for AD diagnosis
using MRI data. Support vector machine SVM), logistic
regressors (e.g., Lasso and Elastic Net), sparse representation-based classification (SRC), random forest classifier, etc., are some widely used approaches. For example,
Kloppel et al.  used linear SVM to detect AD patients
using T1 weighted MRI scan. Dimensional reduction and
variations methods were used by Aversen  to analyze
structural MRI data. They have used both SVM binary
classifier and multi-class classifier to detect AD from MRI
images. Vemuri et  al.  used SVM to develop three
separate classifiers with MRI, demographic and genotype data to classify AD and healthy patients. Gray  
developed a multimodal classification model using random forest classifier for AD diagnosis from MRI and PET
data. Er et al.  used gray-level co-occurrence matrix
(GLCM) method for AD classification. Morra et al.  
compared several model’s performances for AD detection including hierarchical AdaBoost, SVM with manual
feature and SVM with automated feature. For developing these classifiers, typically predefined features are
extracted from the MRI data. However, training a classifier independent from the feature extraction process may
result in sub-optimal performance due to the possible
heterogeneous nature of the classifier and features .
Recently, deep learning models have been famous for
their ability to learn feature representations from the
input data. Deep learning networks use a layered, hierarchical structure to learn increasingly abstract feature
representations from the data. Deep learning architectures learn simple, low-level features from the data and
 
Islam and Zhang ﻿Brain Inf. 5:2
build complex high-level features in a hierarchy fashion.
Deep learning technologies have demonstrated revolutionary performance in several areas, e.g., visual object
recognition, human action recognition, natural language
processing, object tracking, image restoration, denoising,
segmentation tasks, audio classification and brain–computer interaction. In recent years, deep learning models
specially convolutional neural network (CNN) have demonstrated excellent performance in the field of medical
imaging, i.e., segmentation, detection, registration and
classification . For neuroimaging data, deep learning
models can discover the latent or hidden representation
and efficiently capture the disease-related pathologies.
So, recently researchers have started using deep learning
models for AD and other brain disease diagnosis.
Gupta et al.  have developed a sparse autoencoder
model for AD, mild cognitive impairment (MCI) and
healthy control (HC) classification. Payan and Montana  trained sparse autoencoders and 3D CNN
model for AD diagnosis. They also developed a 2D CNN
model that demonstrated nearly identical performance.
Brosch et al.  developed a deep belief network model
and used manifold learning for AD detection from MRI
images. Hosseini-Asl et al.  adapted a 3D CNN model
for AD diagnostics. Liu and Shen  developed a deep
learning model using both unsupervised and supervised
techniques and classified AD and MCI patients. Liu
et al.  have developed a multimodal stacked autoencoder network using zero-masking strategy. Their target was to prevent loss of any information of the image
data. They have used SVM to classify the neuroimaging features obtained from MR/PET data. Sarraf and
Tofighi  used fMRI data and deep LeNet model for
AD detection. Suk et  al.  developed an
autoencoder network-based model for AD diagnosis and
used several complex SVM kernels for classification. They
have extracted low- to mid-level features from magnetic
current imaging (MCI), MCI-converter structural MRI,
and PET data and performed classification using multikernel SVM. Cárdenas-Peña et al.  have developed a
deep learning model using central kernel alignment and
compared the supervised pre-training approach to two
unsupervised initialization methods, autoencoders and
principal component analysis (PCA). Their experiment
shows that SAE with PCA outperforms three hidden layers SAE and achieves an increase of 16.2% in overall classification accuracy.
So far, AD is detected at a much later stage when treatment can only slow the progression of cognitive decline.
No treatment can stop or reverse the progression of
AD. So, early diagnosis of AD is essential for preventive and disease-modifying therapies. Most of the existing research work on AD diagnosis focused on binary
classification problems, i.e., differentiating AD patients
from healthy older adults. However, for early diagnosis,
we need to distinguish among current AD stages, which
makes it a multi-class classification problem. In our previous work , we developed a very deep convolutional
network and classified the four different stages of the
AD—non-demented, very mild dementia, mild dementia and moderate dementia. For our current work, we
improved the previous model , developed an ensemble of deep convolutional neural networks and demonstrated better performance on the Open Access Series of
Imaging Studies (OASIS) dataset .
3  Methods
3.1  Formalization
Let x = {xi, i = 1, . . . , N} , a set of MRI data with
xi ∈[0, 1, 2, . . . , L −1]
h∗w∗l , a three-dimensional (3D)
image with L grayscale values, h ∗w ∗l voxels and
y ∈{0, 1, 2, 3} , one of the stages of AD where 0, 1, 2 and 3
refer to non-demented, very mild dementia, mild dementia and moderate dementia, respectively. We will construct a classifier,
which predicts a label y in response to an input image x
with minimum error rate. Mainly, we want to determine
this classifier function f by an optimal set of parameters
w ∈RP (where P can easily be in the tens of millions),
which will minimize the loss or error rate of prediction.
The training process of the classifier would be an iterative
process to find the set of parameters w, which minimizes
the classifier’s loss
where xi is ith image of X, f (xi, w) is the classifier function that predicts the class ci of xi given w, ci is the
ground-truth class for ith image xi and l(ci, ci) is the penalty function for predicting ci instead of ci . We set l to the
loss of cross-entropy,
3.2  Data selection
In this study, we use the OASIS dataset prepared by
Dr. Randy Buckner from the Howard Hughes Medical
Institute (HHMI) at Harvard University, the Neuroinformatics Research Group (NRG) at Washington University School of Medicine, and the Biomedical Informatics
Research Network (BIRN). There are 416 subjects aged
f : X →Y ; x
L(w, X) = 1
l(f (xi, w),
 
Islam and Zhang ﻿Brain Inf. 5:2
18–96, and for each of them, 3 or 4 T1-weighted sMRI
scans are available. Hundred of the patients having age
over 60 are included in the dataset with very mild to
moderate AD.
3.3  Data augmentation
Data augmentation refers to artificially enlarging the
dataset using class-preserving perturbations of individual data to reduce the overfitting in neural network
training . The reproducible perturbations will enable
new sample generation without changing the semantic
meaning of the image. Since manually sourcing of additional labeled image is difficult in medical domain due
to limited expert knowledge availability, data augmentation is a reliable way to increase the size of the dataset.
For our work, we developed an augmentation scheme
involving cropping for each image. We set the dimension of the crop similar to the dimension of the proposed
deep CNN classifier. Then, we extracted three crops
from each image, each for one of the image plane: axial
or horizontal plane, coronal or frontal plane, and sagittal or median plane. For our work, we use 80% data from
the OASIS dataset as training set and 20% as test dataset. From the training dataset, a random selection of 10%
images is used as validation dataset. The augmentation
process is performed separately for the train, validation
and test dataset. One important thing to consider is the
data augmentation process is different from classic crossvalidation scheme. Data augmentation is used to reduce
overfitting in a vast neural network while training with a
small dataset. On the other hand, cross-validation is used
to derive a more accurate estimate of model prediction
performance. Cross-validation technique is computationally expensive for a deep convolutional neural network training as it takes an extensive amount of time.
3.4  Network architecture
Our proposed network is an ensemble of three deep convolutional neural networks with slightly different configurations. We made a considerable amount of effort for
the design of the proposed system and the choice of the
architecture. All the individual models have a common
architectural pattern consisted of four basic operations:
convolution
batch normalization 
rectified linear unit, and
Each of the individual convolutional neural networks has
several layers performing these four basic operations illustrated in Fig. 2. The layers in the model follow a particular
connection pattern known as dense connectivity as
shown in Fig. 3. The dense connections have a regularizing
effect that reduces overfitting in the network while training with a small dataset. We keep these layers very narrow
(e.g., 12 filters per layer) and connect each layer to every
other layer. Similar to , we will refer to the layers as
dense layer and combination of the layers as dense block.
Since all the dense layers are connected to each other, the
ith layer receives the feature maps ( h0, h1, h2, . . . , hi−1 ),
from all previous layers ( 0, 1, 2, . . . , i −1) . Consequently,
the network has a global feature map set, where each
layer adds a small set of feature maps. In times of training,
each layer can access the gradients from the loss function
as well as the original input. Therefore, the flow of information improves, and gradient flow becomes stronger in
the network. Figure 4 shows the intermediate connection
between two dense blocks.
For the design of the proposed system, we experimented with several different deep learning architectures and finally developed an ensemble of three
homogeneous deep convolution neural networks. The
proposed model is shown in Fig.  5. We will refer to
the individual models as M1 , M2 and M3 . In Fig. 5, the
top network is M1 , the middle network is M2 , and the
bottom network is M3 . Each of the models consists of
several convolution layers, pooling layers, dense blocks
and transition layers. The transition layer is a combination of batch normalization layer, a 1*1 convolutional
layer followed by a 2  *  2 average pooling layer with
stride 2. Batch normalization acts as a regularizer and speeds up the training process dramatically.
Traditional normalization process (shifting inputs to
zero-mean and unit variance) is used as a preprocessing step. Normalization is applied to make the data
comparable across features. When the data flow inside
the network at the time of training process, the weights
and parameters are continuously adjusted. Sometimes
these adjustments make the data too big or too small,
Fig. 2  Common building block of the proposed ensemble model
 
Islam and Zhang ﻿Brain Inf. 5:2
a problem referred as ‘Internal Covariance Shift.’ Batch
normalization largely eliminates this problem. Instead
of doing the normalization at the beginning, batch normalization is performed to each mini-batches along
with SGD training. If B = {x1, x2, . . . , xm} is a minibatch of m activations value, the normalized values
are (x1,x2, . . . ,xm) and the linear transformations are
y1, y2, . . . , ym , then batch normalization is referred to
the transform:
Considering γ , β the parameters to be learned and ǫ , a
constant added to the mini-batch variance for numerical stability, batch normalization is given by the following
equations:
BNγ ,β : x1, x2, . . . , xm →y1, y2, . . . , ym
where µB is mini-batch mean and σ 2
B is mini-batch
variance .
Though each model has four dense blocks, they differ
in the number of their internal 1*1 convolution and 3*3
convolution layers. The first model, M1 , has six (1 * 1 convolution and 3 * 3 convolution layers) in the first dense
block, twelve (1*1 convolution and 3*3 convolution layers) in the second dense block, twenty-four (1*1 convolution and 3*3 convolution layers) in the third dense block
and sixteen (1*1 convolution and 3*3 convolution layers)
in the fourth dense block. The second model, M2 , and
third model, M3 , have (6, 12, 32, 32) and (6, 12, 36, 24)
arrangement respectively. Because of the dense connectivity, each layer has direct connections to all subsequent
layers, and they receive the feature maps from all preceding layers. So, the feature maps work as global state
of the network, where each layer can add their own feature map. The global state can be accessed from any part
xi ←xi −µB
yi ←γ xi + β ≡BNγ ,β(xi)
Fig. 3  Illustration of dense connectivity with a 5-layer dense block
Fig. 4  Illustration of two dense blocks and their intermediate connection
 
Islam and Zhang ﻿Brain Inf. 5:2
of the network and how much each layer can contribute
to is decided by the growth rate of the network. Since
the feature maps of different layers are concatenated
together, the variation in the input of subsequent layers
increases and results in more efficiency.
The input MRI is 3D data, and our proposed model
is a 2D architecture, so we devise an approach to convert the input data to 2D images. For each MRI data, we
created patches from three physical planes of imaging:
axial or horizontal plane, coronal or frontal plane, and
sagittal or median plane. These patches are fed to the
proposed network as input. Besides, this data augmentation technique increases the number of samples in
training dataset. The size of each patch is 112*112. We
trained the individual models separately, and each of
them has own softmax layer for classification decision.
The softmax layers have four different output classes:
non-demented, very mild, mild and moderate AD. The
individual models take the input image and generate its
learned representation. The input image is classified to
any of the four output classes based on this feature representation. To measure the loss of each of these models, we used cross-entropy. The softmax layer takes the
learned representation, fi , and interprets it to the output class. A probability score, pi , is also assigned for the
output class. If we define the number of output classes
as m, then we get
i exp(f i), i = 1, . . . , m
Fig. 5  Block diagram of proposed Alzheimer’s disease diagnosis framework
 
Islam and Zhang ﻿Brain Inf. 5:2
where L is the loss of cross-entropy of the network. Backpropagation is used to calculate the gradients of the network. If the ground truth of an MRI data is denoted as ti ,
To handle the imbalance in the dataset, we used cost-sensitive training . A cost matrix ξ was used to modify
the output of the last layer of the individual networks.
Since the less frequent classes (very mild dementia, mild
dementia, moderate dementia) are underrepresented
in the training dataset, the output of the networks was
modified using the cost matrix ξ to give more importance
to these classes. If o is the output of the individual model,
p is the desired class and L is the loss function, then y
denotes the modified output:
The loss function is modified as:
where yn incorporates the class-dependent cost ξ and is
related to the output on via the softmax function :
The weight of a particular class is dependent on the number of samples of that class. If class r has q times more
samples than those of s, the target is to make one sample
of class s to be as important as q samples of class r. So,
the class weight of s would be q times more than the class
weight of r.
We optimized the individual models with the stochastic gradient descent (SGD) algorithm. For regularization,
we used early stopping. We split the training dataset into
a training set and a cross-validation set in 9:1 proportion. Let Ltr(t) and Lva(t) are the average error per example over the training set and validation set respectively,
measured after t epoch. Training was stopped as soon as
it reached convergence, i.e., validation error Lva(t) does
not improve for t epoch and Lva(t) > Lva(t −1) . We
ti log(pi)
yi = L(ξp, oi), :
tn log(yn)
ξp,n exp(on)
k ξp,k exp(ok)
used Nesterov momentum optimization with Stochastic
Gradient Descent (SGD) algorithm for minimizing the
loss of the network. Given an objective function f (θ) to
be minimized, classic momentum is given by the following pair of equations:
where vt refers to the velocity, ǫ > 0 is the learning rate,
µ ∈ is the momentum coefficient and ∇f θt is the
gradient at θt . On the other hand, Nesterov momentum
is given by:
The output classification labels of the three individual
model are ensembled together using majority voting
technique. Each classifier ’votes’ for a particular class,
and the class with the majority votes would be assigned
as the label for the input MRI data.
4  Results and discussion
4.1  Experimental settings
We implemented the proposed model using Tensorflow , Keras and Python on a Linux X86-64
machine with AMD A8 CPU, 16 GB RAM and NVIDIA
GeForce GTX 770. We applied the SGD training with a
mini-batch size of 64, a learning rate of 0.01, a weight
decay of 0.06 and a momentum factor of 0.9 with Nesterov optimization. We applied early stopping in the
SGD training process, while there was no improvement
(change of less than 0.0001) in validation loss for last
six epoch.
To validate the effectiveness of the proposed AD detection and classification model, we developed two baseline
deep CNN, Inception-v4 and ResNet and modified their architecture two classify 3D brain MRI data.
Besides, we developed two different models, M4 and M5
having similar architecture like M1 , M2 and M3 model
except for the number of layers in the dense block. M4
has six (1*1 convolution and 3*3 convolution layers) in
the first dense block, twelve (1*1 convolution and 3*3
convolution layers) in the second dense block, forty-eight
(1*1 convolution and 3*3 convolution layers) in the third
dense block and thirty-two (1*1 convolution and 3*3 convolution layers) in the fourth dense block (Fig. 6). The layers in the dense blocks of M5 have the arrangement 6, 12,
64, 48 as shown in Fig. 7. Additionally, we implemented
two variants of our proposed model using M4 and M5.
vt = µvt−1 −ǫ∇f (θt−1)
θt = θt−1 + vt
vt = µvt−1 −ǫ∇f (θt−1 + µvt−1)
θt = θt−1 + vt
 
Islam and Zhang ﻿Brain Inf. 5:2
For the first variant, we implemented an ensemble of
four deep convolutional neural networks: M1 , M2 , M3
and M4 . We will refer to this model as E1.
For the second variant, we implemented an ensemble
system of five deep convolutional neural networks:
M1 , M2 , M3 , M4 and M5 . We will refer to this model
4.2  Performance metric
Four metrics are used for quantitative evaluation and
comparison, including accuracy, positive predictive
value (PPV) or precision, sensitivity or recall, and the
harmonic mean of precision and sensitivity (f1-score).
We denote TP, TN, FP and FN as true positive, true
negative, false positive and false negative, respectively.
The evaluation metrics are defined as:
accuracy =
(TP + FP + FN + TN)
precision =
f 1-score =
(2TP + FP + FN)
Fig. 6  Block diagram of individual model M4
 
Islam and Zhang ﻿Brain Inf. 5:2
4.3  Dataset
The OASIS dataset has 416 data samples. The dataset
is divided into a training dataset and a test dataset in 4:1
proportion. A validation dataset was prepared using 10%
data from the training dataset.
4.4  Results
We report the classification performance of M1 , M2 , M3 ,
M4 and M5 model in Tables 1, 2, 3, 4 and 5, respectively.
From the results, we notice that M1 , M2 and M3 model
are the top performers among all models. So, we choose
the ensemble of M1 , M2 , M3 for our final architecture.
Besides, the variants E1 ( M1 + M2 + M3 + M4 ) and E2
( M1 + M2 + M3 + M4 + M5 ) demonstrate inferior performance compared to the ensemble of M1 , M2 , M3 (proposed model) as shown in Fig. 8. From Fig. 8, we notice
Fig. 7  Block diagram of individual model M5
Table 1  Classification performance of M1 model
Non-demented
 
Islam and Zhang ﻿Brain Inf. 5:2
that E1 model has an accuracy of 78% with 68% precision,
78% recall and 72% f1 score. On the other hand, the E2
model demonstrates 77% accuracy with 73% precision,
77% recall and 75% f1-score.
Table 6 shows the per-class classification performance
of our proposed ensembled model on the OASIS dataset . The accuracy of the proposed model is 93.18%
with 94% precision, 93% recall and 92% f1-score. The
performance comparison of classification results of the
proposed ensembles model, and the two baseline deep
CNN models are presented in Fig. 9. Inception-v4 
and ResNet have demonstrated outstanding performance for object detection and classification. The
reason behind their poor performance for AD detection and classification can be explained by the lack of
enough training dataset.
Since these two networks are very deep neural networks, so without a large dataset, training process
would not work correctly. On the other hand, the
depth of our model is relatively low, and all the layers are connected to all preceding layers. So, there is
a strong gradient flow in times of training that eliminates the ‘Vanishing gradient’ problem. In each training
iteration, all the weights of a neural network receive an
update proportional to the gradient of the error function concerning the current weight. But in some cases,
the gradient will be vanishingly small and consequently
prevent the weight from changing its value. It may
completely stop the neural network from further training in worst-case scenario. Our proposed model does
not suffer this ‘Vanishing gradient’ problem, have better feature propagation and provides better classification result even for the small dataset. The performance
comparison of classification results of the proposed
ensembled model, the baseline deep CNN models
and the most recent work, ADNet is presented in
Fig.  10. It can be observed that proposed ensembled
model achieves encouraging performance and outperforms the other models.
5  Conclusion
We made an efficient approach to AD diagnosis using
brain MRI data analysis. While the majority of the
existing research works focuses on binary classification, our model provides significant improvement for
multi-class classification. Our proposed network can
be very beneficial for early-stage AD diagnosis. Though
the proposed model has been tested only on AD dataset, we believe it can be used successfully for other classification problems of medical domain. Moreover, the
Table 2  Classification performance of M2 model
Non-demented
Table 3  Classification performance of M3 model
Non-demented
Table 4  Classification performance of M4 model
Non-Demented
Table 5  Classification performance of M5 model
Non-demented
Table 6  Performance of the proposed ensembled model
Non-demented
 
Islam and Zhang ﻿Brain Inf. 5:2
proposed approach has strong potential to be used for
applying CNN into other areas with a limited dataset.
In future, we plan to evaluate the proposed model for
different AD datasets and other brain disease diagnosis.
Authors’ contributions
JI carried out the background study, proposed the ensembled deep convolutional neural network, implemented the network, evaluated the result and
drafted the manuscript. YZ supervised the work, proposed the variants of the
models, monitored result evaluation process, and drafted the manuscript.
Both authors read and approved the final manuscript.
Authors’ information
Jyoti Islam is a PhD student at the Department of Computer Science, Georgia
State University, Atlanta, GA, USA. Before joining GSU, she was a Senior Software Engineer at Samsung R&D Institute Bangladesh. She received her M.Sc.
degree in Computer Science and Engineering from University of Dhaka, Bangladesh, in 2012 under the supervision of Dr. Saifuddin Md. Tareeq. She received
her B.Sc. degree in Computer Science and Engineering from the University
of Dhaka, Bangladesh, in 2010. Her research is focused on deep learning and
in particular in the area of medical image analysis for neurological disorder
diagnosis. Her research interest extends to machine learning, computer vision,
health informatics and software engineering.
Yanqing Zhang is currently a full Professor at the Computer Science
Department at Georgia State University, Atlanta, GA, USA. He received the
Ph.D. degree in computer science from the University of South Florida in 1997.
His research areas include computational intelligence, data mining, deep
learning, machine learning, bioinformatics, web intelligence, and intelligent
parallel/distributed computing. He mainly focuses on research in computational intelligence (neural networks, fuzzy logic, evolutionary computation,
kernel machines, and swarm intelligence). He has co-authored two books,
co-edited two books and four conference proceedings. He has published 18
book chapters, 78 journal papers and 164 conference/workshop papers. He
has served as a reviewer for over 70 international journals and as a program
committee member for over 150 international conferences and workshops.
He was Program Co-Chair: the 2013 IEEE/ACM/WIC International Conference
on Web Intelligence, and the 2009 International Symposium on Bioinformatics
Research and Applications. He was Program Co-Chair and Bioinformatics Track
Chair of IEEE 7th International Conference on Bioinformatics & Bioengineering
in 2007, and Program Co-Chair of the 2006 IEEE International Conference on
Granular Computing.
Acknowledgements
This study was supported by Brains and Behavior (B&B) Fellowship program
from Neuroscience Institute of Georgia State University. Data were provided
by the Open Access Series of Imaging Studies [OASIS: Longitudinal: Principal
Investigators: D. Marcus, R, Buckner, J. Csernansky, J. Morris; P50 AG05681, P01
AG03991, P01 AG026276, R01 AG021910, P20 MH071616, U24 RR021382]
Competing interests
The authors declare that they have no competing interests.
Ethics approval and consent to participate
Not applicable.
Publisher’s Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Received: 12 February 2018 Accepted: 18 April 2018