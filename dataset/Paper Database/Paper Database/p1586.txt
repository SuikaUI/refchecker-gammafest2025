Log-Euclidean Kernels for Sparse Representation and Dictionary Learning
Peihua Li1, Qilong Wang2, Wangmeng Zuo3,4, Lei Zhang4
1Dalian University of Technology, 2Heilongjiang University, 3Harbin Institute of Technology
4The Hong Kong Polytechnic University
 , , , 
The symmetric positive deﬁnite (SPD) matrices have
been widely used in image and vision problems. Recently
there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success
of SR for vector data. Though the space of SPD matrices is
well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem
by proposing a kernel based method for SR and dictionary
learning (DL) of SPD matrices. We disclose that the space
of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication deﬁned in
the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels
that satisﬁes Mercer’s condition. These kernels characterize the geodesic distance and can be computed efﬁciently.
We also consider the geometric structure in the DL process
by updating atom matrices in the Riemannian space instead
of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable performance gains over state-of-the-arts.
1. Introduction
The symmetric positive deﬁnite (SPD) matrices can be
introduced in the imaging, pre-processing, feature extraction and representation processes, and have been widely
adopted in many computer vision applications .
For example, each voxel of the tensor-valued images produced by Diffusion Tensor Imaging (DTI) is a threedimensional SPD matrix, and the DTI images are very
promising in neuroscience study since they capture the tissue microstrucutral characteristics in an non-invasive way
through diffusion of water molecules. In pre-processing,
the structure tensor obtained by computing the second
(or higher) order moments of image features in a neighboring region can be represented by SPD matrices; it has been
used for orientation estimation and local structure analysis, ﬁnding applications in optical ﬂow, texture, corner and
edge detection , and recently in foreground segmentation . As to feature extraction, the covariance matrices
that model the second-order statistics of image features also
result in SPD matrices, which have been successfully applied to detection , recognition , and classiﬁcation
 , etc. Due to their wide applications, the investigations on
learning methods for SPD matrices have recently received
considerable research interests.
One key problem of SPD matrix-based learning methods is the model and computation of SPD matrices. It is
known that the space of n × n SPD matrices, denoted by
n , is not a linear space 1 but forms a Lie group that is a
Riemannian manifold . Hence, the mathematical modeling in this space is different from what is commonly done
in the Euclidean space , and new operators for SPD
matrices should be introduced for SPD matrix-based learning. In this work, we take sparse representation (SR) and
dictionary learning (DL) as examples, and focus on extending SR and DL to SPD matrix-based learning. Since
conventional SR and DL methods are proposed for vector
data in the Euclidean space rather than the SPD matrices in
the Riemannian manifold, in order to use SR and DL for
SPD matrix-based learning, we should consider the following issues in developing new operators in S+
n . (1) In the Euclidean space the linear combination of atom vectors can be
naturally obtained using the conventional matrix operators;
but it would be challenging to represent an SPD matrix as a
linear combination of atom matrices since S+
n is not a linear
space. (2) To evaluate the reconstruction error, ℓ2-norm is
commonly used in the Euclidean space; however, in S+
Riemannian metrics would be more appropriate as they can
measure the intrinsic distance between SPD matrices. (3)
The updating of dictionary atoms involves solving a constrained optimization problem in S+
n and it is more appro-
n is not a linear space with the operations of conventional matrix
addition and scalar-matrix multiplication. However, with the operations of
logarithmic multiplication and scalar logarithmic multiplication S+
only a linear space but also a complete inner product space as shown in
section 2.2.
Table 1. Comparison of various methods on sparse representation and dictionary learning in S+
Representation given atoms
Riemannian Metric?
Riemannian atom update?
Mercer’s condition?
TSC 
Linear in Euclidean space
No-LogDet divergence
No-Euclidean
Linear in Euclidean space
No-Frobenius norm
No-Euclidean
LogE-SR 
Linear in Log-domain
No-Euclidean
Linear in RKHS
Approximation-Stein divergence
No-Euclidean
Satisfy-conditionally
Proposed method
Linear in RKHS
Yes-Riemannian
priate to consider the geometry of S+
n . It has been shown
 that the methods failing to consider the geometric
structure of S+
n may result in unsatisfactory performance or
even break down.
Generally, there are two strategies to address the three issues mentioned above. First, one can extend SR and DL by
introducing proper linear decomposition and reconstruction
error measures for SPD matrices. In Tensor Spare Coding
(TSC) , an SPD matrix is linearly decomposed as a set
of of atom matrices and LogDet (or Bregman matrix) divergence was adopted to measure the reconstruction
error. Based on this framework, dictionary learning methods are further proposed to learn atom matrices . In the
generalized dictionary learning (GDL) algorithm , each
SPD matrix is represented as a linear combination of rank-1
atom matrices; the error between one SPD matrix and its
linear combination is evaluated by matrix Frobenius norm.
The GDL algorithm is fast and scalable to large datasets.
Second, we can explicitly or implicitly map SPD matrices to some Reproducing Kernel Hilbert Space (RKHS),
and use the kernel SR or DL framework for SPD matrixbased learning. Harandi et al. ﬁrst studied kernel-based
method for SR and DL in S+
n . They adopted Stein kernel
to map the SPD matrices to higher dimensional RKHS. This
method is in contrast with those methods which directly embed SPD matrices into Euclidean space (LogE-SR) 
and achieves state-of-the-art performance compared to its
counterparts.
Based on the previous analysis, the pros and cons of various methods are summarized in Table 1. We argue that
the previous work fails to take full advantage of the geometry of S+
n . Linear decomposition of SPD matrices as in
the Euclidean space is not natural and may induce errors as S+
n is not a linear space; the method of embedding SPD matrices into Euclidean space by matrix logarithm has improved performance but the gains are
limited . Furthermore, these work uses directly either
the Euclidean norm or the Bregman matrix divergence
 to evaluate the reconstruction error. The linear decomposition makes sense in high- or inﬁnite-dimensional
RKHS ; however, the Stein divergence is only an approximation of Riemannian metric and is a positive deﬁnite
(p.d.) kernel only under some restricted conditions .
Our work is inspired by . We also embed S+
RKHS as linear representation is a natural and reasonable
consequence in the Hilbert space. The main difference is
that we develop a novel family of kernel functions based
on the Log-Euclidean framework . The proposed kernels
characterize the geodesic distance and thus can accurately
measure the reconstruction error; they also satisfy the Mercer’s condition under broad conditions. These are in contrast to the Stein kernel which is only an approximation of
the geodesic distance and satisﬁes the Mercer’s condition
only under some restricted conditions. In addition, we explicitly consider in DL the geometric structure of S+
that the Gaussian kernel based on the Log-Euclidean metric
is simultaneously presented in . Our work differs
from them in that we disclose the inner product structure
n , by which we can develop a broad variety of kernel
functions and the Gaussian kernel is a special case of ours.
2. Log-Euclidean Kernel
This section starts with a brief introduction of Log-
Euclidean framework ; subsequently, we show that S+
forms an inner product space; based on this, we design a
family of kernel functions.
n as a Lie Group and Geodesic Distance
Let Sn be the space of n × n symmetric matrices. The
matrix exponential exp: Sn →S+
n is bijective and smooth
and its inverse map, denoted by log, is smooth as well. In
the Log-Euclidean framework, an operation of logarithmic
multiplication ⊙: S+
n is deﬁned as 
S1 ⊙S2 = exp(log(S1) + log(S2))
It can be veriﬁed that S+
n is a group with the identity element being the identity matrix and with the inverse operation the regular matrix inverse. S+
n is a Lie group because
the group operation and inverse operation are both smooth
and because it is a Riemannian manifold (half convex cone
in Euclidean space). Note that ⊙is commutative and S+
therefore a commutative Lie group (Abelian group).
The commutative Lie group S+
n admits a bi-invariant
metric. The geodesics equipped with a bi-invariant metric
are the left translates of the geodesics through the identity
element, given by one-parameter subgroup exp(tV), where
t ∈R and V ∈Sn. After some derivations and manipulations, one can obtain the geodesics, the Riemannian exponential and Riemannian logarithm, and ﬁnally the geodesic
distance between two SPD matrices S and T as follows:
ρgeo(S, T) = ∥log(S) −log(T)∥F
where ∥·∥F denotes the Frobenius norm. Interested readers
may refer to for details on the corresponding theory.
n as a Complete Inner Product Space
It is known that S+
n is not a linear space with the operations of the conventional matrix addition and scalar-matrix
multiplication but forms a Riemannian manifold .
However, as shown in , in the Log-Euclidean framework
it is endowed with a linear space structure with the logarithmic multiplication (1) and the following scalar logarithmic
multiplication :
λ ⊗S = exp(λ log(S)) = Sλ
where λ is a real number. It is straightforward to show that
two operations ⊙and ⊗satisfy the conditions of a linear
space, with the identity matrix being the identity element
and regular matrix inversion operation as inverse mapping.
Indeed, not only a linear space, S+
n is also an inner product space as described by the following corollary which is
not disclosed previously:
Corollary 1 With two operations ⊙and ⊗, the function
from the product space of S+
n to the space R of real number
⟨·, ·⟩log : S+
⟨S, T⟩log = tr(log(S) log(T))
is an inner product, where tr denotes the matrix trace, and
n is a complete inner product space (Hilbert space). The
induced norm can be used to deﬁne the distance that equals
to the geodesic distance.
Let S, T, R ∈S+
n , λ ∈R be arbitrary, below we
verify the axioms of inner product in terms of ⊙and ⊗:
tr(log(S) log(T))
tr(log(T) log(S)) = ⟨T, S⟩log. Here
we use the property that the trace of a
matrix equals to that of its transpose.
⟨S ⊙R, T⟩log
tr((log(S) +
log(R)) log(T))
⟨S, T⟩log +
⟨R, T⟩log,
tr((λ log(S)) log(T)) = λ⟨S, T⟩log.
Non-negativity
tr(log(S) log(S))
F ≥0, and it is obvious that
⟨S, S⟩log = 0 if and only if S is the
identity matrix I.
Obviously S+
n is of ﬁnite dimension and therefore it is
a complete inner product space (Hilbert space) .
The norm induced by the inner product is expressed as
∥S∥log = ⟨S, S⟩
log, and hence, S+
n is a normed linear
space. The distance (metric) between S and T is
ρlog(S, T) = ∥S ⊙T−1∥log = ⟨S ⊙T−1, S ⊙T−1⟩
= ∥log(S) −log(T)∥F = ρgeo(S, T)
where T−1 denotes the inverse of matrix T, which equals
to the geodesic distance when S+
n is viewed as a Lie group.
Arsigny et al. established the linear space structure
n and pointed out that a similarity invariant metric can
be deﬁned [1, Proposition 3.11]. However, a linear space
or a normed liner space is not necessarily an inner product
space unless a function that satisﬁes the axioms of the inner
product is deﬁned. In the above, we have shown that S+
a complete inner product space (Hilbert space). We argue
that Corollary 1 might change our philosophy of data processing on S+
n : since it is a Hilbert space, we can perform
data processing directly on S+
n with logarithmic multiplication ⊙and scalar logarithmic multiplication ⊗, unlike 
which involves mapping SPD matrices to logarithmic domain, performing data processing therein and then mapping
back to S+
We can deﬁne other inner products, for example
⟨S, T⟩log,A as described by the following corollary.
Corollary 2 Let A ∈S+
n be arbitrary, with two operations
⊙and ⊗, the function
⟨S, T⟩log,A = tr(log(S)A log(T))
is an inner product, and the induced norm is ∥S∥log,A =
It reduces to (4) if A is the identity matrix. In practice we
can learn A from training data on S+
n by designing distance
metric learning methods, and the learned distance may be
more descriminative and suitable for speciﬁc vision tasks.
2.3. Log-Euclidean Kernels
Based on Corollary 2, ⟨·, ·⟩log,A is a p.d.
i,j cicj⟨Si, Sj⟩log,A
⊙,i ci ⊗Si, 
Si⟩log,A ≥0. In a similar way, we can show that its normalized version ⟨S, T⟩log,A/(∥S∥log,A∥S∥log,A) is also a
kernel. Here 
⊙,i denotes the logarithmic multiplication
of terms indexed by i. The following statements are based
on [2, pp. 69∼70] (see proof therein):
Proposition 1 Let X be a non-empty set, and φ1, φ2 :
X × X →R be arbitrary p.d. kernels. We have: (1) The
pointwise product φ1φ2 : X × X →R is a p.d. kernel; (2)
The tensor product φ1 ∗φ2 : (X × X) × (X × X) is a p.d.
kernel; and (3) If f(z) = ∞
n=0 anzn is holomorphic (analytic) in its domain and an ≥0 for all n ≥0, the composed
function f ◦φ is a p.d. kernel.
We can develop a broad variety of p.d. kernels, such as
polynomial, exponential, radial basis, B-Spline kernels, or
Fourier kernel etc. . Below we give some commonly
used kernels.
Corollary 3 Let A ∈S+
n and pn be a polynomial of degree n ≥1 with positive coefﬁcients, we have p.d. kernels
Log-E poly. kernel
κpn(S, T) = pn (⟨S, T⟩log),
Log-E exp.
κen(S, T) = exp
pn(⟨S, T⟩log)
Log-E Gaus. kernel
κg(S, T) = exp(−∥S ⊙T−1∥2
Through Corollaries 1, 2 and Proposition 1, we can easily
see that κpn(S, T) is a p.d. kernel. From the series expansion of exp, we know κen(S, T) is a p.d. kernel as well.
Since exp(−∥S∥log,A) exp(−∥T∥log,A) is a p.d.
which can be proved straightforwardly by the deﬁnition,
exp(−∥S∥log,A) exp(−∥T∥log,A) exp(2⟨S, T⟩log,A)
κg(S, T) (after some manipuliation) is also a kernel.
κg(S, T) is an anisotropic Gaussian kernel; if A is a
diagonal matrix A = diag{β} with β > 0, κg(S, T)
reduces to a special form exp(−β∥log(S) −log(T)∥2
which is identical to the Gaussian kernel in .
Here we compare the proposed kernels with Stein kernel
 . Let S, T ∈S+
n and γ = [log(λ1) . . . log(λn)]T ,
where λi are the generalized eigenvalues between S and
T. The Afﬁne-Riemannian distance between the two matrices is dA(S, T) = ∥γ∥2 . The symmetric Stein divergence dS(S, T) = log
2 log(det(ST)),
derived from Bregman matrix divergence , satisﬁes the
the sandwiching inequality 
2dT (S,T) −n log(2) ≤
dS(S, T) ≤1
A(S, T), where dT (S, T) = ∥γ∥∞. From
our experience, most of the values of the leftmost hand side
term in the sandwiching inequality are negatives. Fig. 1
shows the histogram computed from the SPD matrices used
in texture classiﬁcation (described in Section 4.1). Hence,
dS(S, T) might be only upper-bounded by the geodesic distance 1
A(S, T). It is unclear that to what extent the Stein
divergence approximates the Riemannian metric.
all, κS(S, T) = exp(−βdS(S, T)) is a p.d. kernel under restricted condition, that is, β = 1
2, . . . , n−2
2 . In contrast, the proposed family of kernels κpn,
κen and κg characterize the true rather than the approximation of Riemannian metric. So they can evaluate the reconstruction error accurately. In addition, κpn, κen are kernels
for any order of polynomials with positive coefﬁcients and
κg is a kernel for any β > 0. This produces ﬂexibility for
adjusting the parameters to obtain better performance for
various problems.
The logarithm of SPD matrices can be computed through
the eigen-decomposition. Let S = UΛUT be the eigendecomposition of S ∈S+
n , where Λ is a diagonal matrix consisting of eigen-values λi, i = 1, . . . , n, of S, i.e.,
Λ = diag{λi}, and U is an orthonormal matrix consisting
2 /(2dT)−nlog(2)
Figure 1. Histogram of the values of d2
A(S, T)/(2dT (S, T)) −
of the corresponding eigen-vectors. As we have log(S) =
Udiag{log(λi)}UT , the computational complexity of the
proposed kernels is O(10n3) which is higher than that of the
Stein kernel. The logarithms of the involved SPD matrices
can generally be computed beforehand because of their “decoupling” property either in the inner product or distance;
in these cases, the complexity of the proposed kernels becomes O(n3) and is the same as that of the Stein kernel.
Representation
Dictionary
Kernel-based SR and DL have been studied in the literature for vector data in the Euclidean space Rn,
which have shown notable performance improvement over
the non-kernel based methods. Harandi et al. ﬁrst presented kernel-based SR and DL for SPD matrices. While
this method outperforms state-of-the-arts, the symmetric
Stein divergence only approximates the Riemannian metric
and the Stein kernel only satisﬁes Mercer’s condition under
restricted conditions. In this section, we develop SR and
DL methods based on the Log-Euclidean Kernels, which
address the shortcomings of the Stein kernel.
3.1. Sparse Representation
n and Si ∈S+
n , i = 1, . . . , N be a set of
atom matrices. Let φ be the function that maps SPD matrices to RKHS, SR of Y can be formulated as the following
kernelised LASSO problem :
subject to ∥φ(Si)∥2 ⩽1, ∀i, where x = [x1 . . . xN]T is
the sparse vector, λ > 0 is the regularization parameter, and
∥· ∥2 and ∥· ∥1 denote ℓ2-norm and ℓ1-norm, respectively.
In the kernel methods since ∥φ(Si)∥2 = 1 the constraints
are satisﬁed naturally and can thus be neglected. After some
manipulations, the SR (6) can be expressed in the form of
kernels as
xiκ(Y, Si)+
xixi′κ(Si, Si′)+λ∥x∥1
Minimaization of the above equation is similar to regular sparse coding in Euclidean space , and we use the
method introduced in for its solution.
3.2. Dictionary Learning
Given a set of training data Yj, j = 1, . . . , M, the atom
matrices can be obtained by learning method so that they
have more powerful representation capability. The learning
problem may be expressed as minimization of the function
f(S1, . . . , SN, xj, . . . , xM) =
φ(Yj) −
w.r.t Si, i = 1, . . . , N, and xj, j = 1, . . . , M, where
xj = [xj,1, . . . , xj,N] denotes the sparse vector of Yj.
The problem (7) is commonly solved by iterating two procedures .
First, suppose that the atom matrices
n , i = 1, . . . , N, are ﬁxed, the problem (7) reduces
to kernel-based SR problems: for each Yj, j = 1, . . . , M,
we compute its sparse vector xj as described in the previous section; then, let xj be ﬁxed, we update dictionary atom
matrices Si, i = 1, . . . , N.
In the following, we illustrate the atom matrices update
scheme using Gaussian kernel κg. As in , we also adopt
a method that is similar to K-SVD [8, Chap. 12] and update an atom matrix at one time. Re-writing (7) in kernel
function κ, we have the partial derivative of f(·) w.r.t Sr
xj,rκ(Sr, Yj)(log(Sr) −log(Yj))
xj,rxj,i(log(Sr) −log(Si))
One may update log Sr instead of Sr, which is equivalent
to transforming by logarithm the SPD matrices to Euclidean
space in which atoms are updated. However, in practice we
ﬁnd this update scheme is unstable. We thus instead update
the atom matrices in the Lie group as follows:
log(Sr) + dSr log(−ϵ∂f/∂Sr)
where dSr(U) denotes the differential of matrix logarithm
at Sr with the displacement of the tangent matrix U. Hence,
the marching now is along the geodesics and the algorithm
becomes more stable.
4. Experiments
In this section, we ﬁrst evaluate the performance of the
proposed family of kernels on sparse representation without dictionary learning. As in , the training samples are
adopted as atom matrices and the reconstruction errors are
used for classiﬁcation. Then we learn the atom matrices
from the training data and the sparse codes obtained from
the learned atom matrices are used for classiﬁcation with
the nearest neighbor classiﬁer or support machine vector
4.1. Sparse Representation
In the papers that focus on SR in S+
n , the FERET dataset
 and the Brodatz database are commonly used for
classiﬁcation performance evaluation . Hence,
to facilitate comparison with state-of-the-arts we also adopt
them here.
Face Recognition As in , we select the “b”
subset of FERET database for evaluation of classiﬁcation
performance. The subset consists of 198 subjects, each of
which has 7 images. The training examples are composed
of frontal face images with neutral expression “b”, smiling
expression “bj”, and illumination changes “bk”, while the
test examples involve face images of varying pose angle:
“bd”–+25◦, “be”–+15◦, “bf”–−15◦, and “bg” –−25◦. As
in , the image features to compute covariance descriptors consist of intensity value, x and y coordinates, and intensity values of the ﬁltered image via Gabor ﬁlters along 5
orientations and 8 angles. Thus each image is represented
by a 43 × 43 covariance matrix. We adopt the classiﬁcation
method in , and the preprocessing method in .
Fig. 2 shows the recognition accuracy of the proposed
kernels with the regularization parameter λ = 10−3, where
the recognition rates of RSR using Stein kernel are also
shown as baseline (red dash-dotted). The top row shows
the classiﬁcation rates of κpn, κen(pn(x) = xn) versus n.
Note that the recognition rates of κen are less sensitive to the
polynomial order than κpn. The bottom row shows those of
κg versus β, from which we see that the recognition rate increases as β gets larger, reaching peak at about 2×10−2 and
decreasing afterwards. It can be seen that the proposed kernels are clearly better than the Stein kernel on all datasets.
In particular, for two difﬁcult datasets that have large pose
variations, the performance gains are substantial.
Table 2 lists the comparison results on FERET dataset
with sate-of-the-arts: Sparse Representation Classiﬁcation
(SRC) , the Gabor feature-based sparse representation
in Euclidean space (GSRC) , Log-Euclidean sparse representation (logE-SR) which performs sparse representation in the logarithm domain , Tensor Sparse Coding (TSC) , Riemannian Sparse Representation (RSR)
based on Stein kernel . The results are reproduced from
Recognition rate
bg−pose: −25
Log−E poly. kernel (p)
Log−E exp Kernel (e)
RSR−Stein Kernel
Recognition rate
bf−pose: −15
Recognition rate
be−pose: +15
Recognition rate
bd−pose: +25
Recognition rate
bg−pose: −25
Log−E Gaus. kernel (g)
RSR−Stein Kernel
Recognition rate
bf−pose: −15
Recognition rate
be−pose: +15
Recognition rate
bd−pose: +25
Figure 2. Classiﬁcation rates on the FERET dataset. Top row: κpn, κen vs. n; bottom row: κg vs. β. From left to right are results on bg,
bf, be, and bd, respectively. The classiﬁcation rates of RSR that uses the Stein kernel are shown as baseline (red dash-dotted line).
the respective papers. TSC has unsatisfactory performance
and we owe it to the linear representation of SPD matrices in
the Euclidean space without use of the Riemannian metric.
By using the Riemannian metric, LogE-SR has improved
recognition rates but the sparse decomposition is performed
in the logarithm domain rather than in the original Riemannian manifold. The kernel-based method, RSR, compared
with TSR and LogE-SR, achieves larger performance gains.
Two reasons may account for this: (1) linear representation
in RKHS naturally makes sense; and (2) the Stein divergence is an approximation to the Riemannian metric. The
results of the proposed methods are obtained with κpn, κen
(pn(x) = x50), and κg (β = 2 × 10−2). The proposed three
kernels have comparable performance while κg is a little
better. We see that our methods achieve largely notable performance increase and we attribute this to full use of data
Table 2. Comparison with state-of-the-arts on the FERET database
Log-E kernel
Texture Classiﬁcation We employ the Brodatz dataset and
follow the experimental setting in for fair comparison. Note that our purpose here is not to develop competing texture classiﬁcation algorithm but to testify the
proposed method with closely related work. In the Brodatz
dataset each class contains only one image and we use the
mosaics of 5-texture (‘5c’, ‘5m’, ‘5v’, ‘5v2’, ‘5v3’), 10texture (‘10’,‘10v’), and 16-texture (‘16c’, ‘16v’). Every
image is resized to 256×256 which is then uniformly divided into 8×8 subimages. Each subimage is represented
by a covariance matrix computed from the feature vectors
of intensity and the absolute values of the 1st- and 2ndorder partial derivatives with respect to spatial coordinates.
Among the 64 covariance matrices per class, 5 are randomly selected for training and the remaining ones are for
The ﬁnal classiﬁcation rate is averaged over 20
trials. Figure 3 presents the comparison of classiﬁcation
rates on nine mosaics from the Brodatz dataset. We see
that for all mosaics but 5v, the proposed method (κg with
λ = 10−3, β = 2 × 10−2) has higher classiﬁcation rates
than RSR. For the 5v mosaics, the classiﬁcation rate of RSR
is a little higher (0.012). The average classiﬁcation rates on
all the nine mosaics are 0.66, 0.81, 0.87, and 0.92 for LogE-
SR, TSC, RSR, and Log-E Kernel, respectively.
LogE−SR 
Log−E Kernel
Figure 3. Classiﬁcation rates on nine mosaics from the Brodatz
dataset. Average rates on all nine mosaics are 0.66, 0.81, 0.87, and
0.92 for LogE-SR, TSC, RSR, and Log-E Kernel, respectively.
4.2. Dictionary Learning
To testify the effectiveness of the proposed dictionary
learning method, we compare three methods: random sampling, K-Means clustering and dictionary learning. In all
the methods, the sparse vectors are obtained via Log-E Kernel κg. The K-Means clustering is performed in the Log-
Euclidean framework : the covariance matrices are ﬁrst
mapped to the linear space Sn by matrix logarithm, in which
the clustering is performed and the results are then mapped
back to S+
Texture Classiﬁcation We use the Brodatz dataset and follow the experimental setting in . All the 111 texture
classes are used. In each image we randomly select 50 image patches of 32×32 pixels, from which a 5×5 covariance
matrix is computed. The 5-dimensional feature vectors to
compute the covariance matrix comprise grayscale intensity, and the 1st and 2nd partial derivatives with respect to
spatial coordinates. In each class, 20 samples are randomly
selected for training; in the remaining ones, 20 are used as
probe samples and 10 as gallery ones. We thus have 2200
covariance matrices in total for dictionary learning.
We use the k-nearest neighbour classiﬁer (k = 3) for
classiﬁcation. Figure 4 shows the curves of classiﬁcation
accuracy vs. the number of atom matrices. It can be seen
that the dictionary learning method is consistently superior to random dictionary and Log-E K-Means, particularly
when the number of atom matrices are small. It is interesting to notice that the random dictionary is better than
the learned dictionary via Log-E K-Means if the number of
atom matrices are less than 80. This may be because that in
this texture dataset, on the whole the textures tend to be regular, and generally any patch may be representative of the
texture while K-Means brings bias. We also observe that the
performance of both random sampling and Log-E K-Means
improves with the increase of atom matrix number.
Number of atoms
Average accuracy
Dictionary learning
Log−E K−Means
Figure 4. Classiﬁcation accuracy on the Brodatz dataset
Scene Categorization We use the popular benchmark
database Scene15 for classiﬁcation performance evaluation. It consists of 15 categories each of which includes
about 200∼400 images of average size of 300×250 pixels,
and there are 4,485 images in total. In each image, we extract 8×8 covariance matrices at dense grids with a stride of
8 pixels. The patch size to compute the covariance matrix
is 16×16 and the raw features are orientation histogram of
8 bins . Each image is represented by a histogram computed from the sparse vectors of sampled patches via the
max pooling strategy . The SVM is trained using the
LIBSVM package .
We adopt the methodology in (BoW+SPM+SVM)
for training and classiﬁcation. First, among covariance matrices of all images, 50,000 ones are randomly chosen which
are used to obtain atom matrices. For each class, 100 images are randomly selected as training data and the rest as
testing data. The experiments are repeated 20 times and
the results are averaged. Table 3 presents the classiﬁcation
rates of different methods vs. the number of atom matrices. It can be seen that in all cases the classiﬁcation rates
of the proposed method are over 18 percent higher than the
random dictionary. We can also observe that the proposed
method has over 8 percent, 4 percent, and 2 percent advantages over Log-E K-Means Clustering for 32, 64, and 128
atom matrices, respectively.
From both of the above experiments, we observe that as
atom matrix number grows, the performance gains of the
dictionary learning over the other two methods gets smaller.
As the current dictionary is generative without discriminative information, more powerful representational capability does not necessarily mean better discriminability. This
may explain the above ﬁndings and we think that the performance difference between the three methods will get
smaller or even negligible as the atom matrix number becomes much larger.
Table 3. Classiﬁcation accuracy on the Scene15 database
Num. of atoms
Random dictionary
44.80±0.90
57.64±0.59
62.25±0.65
LogE K-Means
67.69±0.56
76.25±0.48
78.80±0.53
Dictionary learning
75.84±0.64
79.27±0.65
80.92±0.44
5. Conclusion
This paper presented a novel Riemannian metric based
kernel method for SR and DL in S+
It embeds the
SPD matrices into RKHS so that the linear decomposition makes sense. The proposed kernels are based on the
Log-Euclidean framework. They not only characterize the
geodesic distance between SPD matrices, but also satisfy
the Mercer’s condition in general conditions. Our method
overcomes the disadvantages of existing work which fails
to make full use of the Riemannian manifold structure of
n . Experiments have shown the superiority of our method
to state-of-the-arts.
We disclosed that the space of SPD matrices is a
complete inner product space, and developed a broad
family of p.d. kernels. These kernels are readily suitable
for kernel-based regression,
function estimation,
classiﬁcation on the space of SPD matrices.
It is also
interesting, by using the proposed kernels, to explore
kernel-based distance metric learning methods to adapt to
various tasks of image retrieval and classiﬁcation based on
the covariance descriptors.
Acknowledgments: The work was supported by NSFC
60973080, 61170149, Program for New Century Excellent Talents in University (NCET-10-0151), the Fundamental Research Funds for the Central Universities
(DUT13RC(3)02), Key Project by Chinese Ministry of Education (210063). We thank Dr. Harandi for providing us
with the code of RSR and pre-processed face images.