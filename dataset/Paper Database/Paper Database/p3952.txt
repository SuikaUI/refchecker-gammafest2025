Unsupervised Medical Image Translation
with Adversarial Diffusion Models
Muzaffer ¨Ozbey∗, Onat Dalmaz∗, Salman UH Dar, Hasan A Bedel, S¸ aban ¨Ozturk, Alper G¨ung¨or,
and Tolga C¸ ukur
Abstract— Imputation of missing images via source-totarget modality translation can improve diversity in medical
imaging protocols. A pervasive approach for synthesizing
target images involves one-shot mapping through generative adversarial networks (GAN). Yet, GAN models that
implicitly characterize the image distribution can suffer
from limited sample ﬁdelity. Here, we propose a novel
method based on adversarial diffusion modeling, SynDiff,
for improved performance in medical image translation.
To capture a direct correlate of the image distribution,
SynDiff leverages a conditional diffusion process that progressively maps noise and source images onto the target
image. For fast and accurate image sampling during inference, large diffusion steps are taken with adversarial
projections in the reverse diffusion direction. To enable
training on unpaired datasets, a cycle-consistent architecture is devised with coupled diffusive and non-diffusive
modules that bilaterally translate between two modalities.
Extensive assessments are reported on the utility of Syn-
Diff against competing GAN and diffusion models in multicontrast MRI and MRI-CT translation. Our demonstrations
indicate that SynDiff offers quantitatively and qualitatively
superior performance against competing baselines.
Index Terms— medical image translation, synthesis, unsupervised, unpaired, adversarial, diffusion, generative
I. INTRODUCTION
Multi-modal imaging is key for comprehensive assessment
of anatomy and function in the human body . Complementary tissue information captured by individual modalities
serve to improve diagnostic accuracy and performance in
downstream imaging tasks . Unfortunately, broad adoption
of multi-modal protocols is fraud with challenges due to
economic and labor costs – . Medical image translation is
a powerful solution that involves synthesis of a missing target
modality under guidance from an acquired source modality
 – . This recovery is an ill-conditioned problem given
nonlinear variations in tissue signals across modalities –
 . At this juncture, learning-based methods are offering performance leaps by incorporating nonlinear data-driven priors
to improve problem conditioning – .
Learning-based image translation involves network models
trained to capture a prior on the conditional distribution of
∗M. Ozbey and O. Dalmaz contributed equally to this study. This
study was supported in part by TUBITAK BIDEB scholarships awarded
to A. Bedel, O. Dalmaz, A. Gungor, and by TUBA GEBIP 2015 and
BAGEP 2017 fellowships awarded to T. C¸ ukur (Corresponding author:
Tolga C¸ ukur). M. Ozbey, O. Dalmaz, S.UH. Dar, H.A. Bedel, S. Ozturk, A.
Gungor and T. C¸ ukur are with the Department of Electrical and Electronics Engineering, and the National Magnetic Resonance Research Center (UMRAM), Bilkent University, Ankara, Turkey (e-mails: {muzaffer,
onat, salman, abedel, sozturk, agungor, cukur}@ee.bilkent.edu.tr). S.
Ozturk is also with Amasya University, Turkey. A. Gungor is also with
ASELSAN Research Center, Turkey. The authors thank Mahmut Yurt for
discussions on diffusion models in early phases of the study.
target given source images – . In recent years, generative adversarial network (GAN) models have been broadly
adopted for translation tasks, given their exceptional realism
in image synthesis – . A discriminator that captures information regarding the target distribution concurrently guides
a generator to perform one-shot mapping from the source
onto the target image – . Based on this adversarial
mechanism, state-of-the-art results have been reported with
GANs in numerous translation tasks including synthesis across
MR scanners , multi-contrast MR synthesis , ,
 , , and cross-modal synthesis – .
While powerful, GAN models indirectly characterize the
distribution of the target modality through a generatordiscriminator interplay without evaluating likelihood .
Such implicit characterization is potentially amenable to learning biases, including premature convergence and mode collapse. Moreover, GAN models commonly employ a rapid oneshot sampling process without intermediate steps, inherently
limiting the reliability of the mapping performed by the
network. In turn, these issues can limit the quality and diversity of synthesized images . As a promising alternative,
recent computer vision studies have adopted diffusion models
based on explicit likelihood characterization and a gradual
sampling process to improve sample ﬁdelity in unconditional
generative modeling tasks , . However, the potential
of diffusion methods in medical image translation remains
largely unexplored, partly owing to the computational burden
of image sampling and difﬁculties in unpaired training of
regular diffusion models .
Here, we propose a novel adversarial diffusion model for
medical image synthesis, SynDiff, to perform efﬁcient and
high-ﬁdelity modality translation (Fig. 1). Given the source
image, SynDiff leverages conditional diffusion to generate the
target image. Unlike regular diffusion models, SynDiff adopts
a fast diffusion process with large step size for efﬁciency.
Accurate sampling in reverse diffusion steps is achieved by
a novel source-conditional adversarial projector that denoises
the target image sample with guidance from the source
image. To enable unsupervised learning, a cycle-consistent
architecture is devised with bilaterally coupled diffusive and
non-diffusive processes between the two modalities (Fig.
2). Comprehensive demonstrations are performed for multicontrast MRI and MRI-CT translation. Our results clearly
indicate the superiority of SynDiff against competing GAN
and diffusion models. Code for SynDiff is publicly available
at 
Contributions
• We introduce the ﬁrst adversarial diffusion model in the
 
Fig. 1: a) Regular diffusion models gradually
transform between actual image samples for
the target modality (x0) and isotropic Gaussian noise (xT ) in T steps, with T on the
order of thousands. Each forward step (right
arrows) adds noise to the current sample to
create a noisier sample with forward transition
probability q (xt+1|xt). Each reverse step
(left arrows) suppresses the added noise to
create a denoised sample. For image translation, a source modality (y) can also be
provided as conditioning input to the reverse
steps resulting in a reverse transition probability q (xt−1|xt, y) assumed to be Gaussian, and operationalized via a neural network
pθ (xt−1|xt, y) that estimates its mean. b)
The proposed adversarial diffusion model performs fast transformation between x0 and xT
in T/k steps, with step size k ≫1. Each
forward step adds a greater amount to noise
to compensate for large k, breaking apart the
normality assumption for reverse transition
probabilities q (xt−k|xt, y). To improve accuracy, reverse diffusion steps are operationalized via a novel adversarial projector that
uses a generator Gθ and a discriminator Dθ.
Gθ ﬁrst produces an estimate of the target
image ˜x0 given xt and y, and a denoised
image sample ˆxt−k is then synthesized from
the denoising distribution q (xt−k|xt, ˜x0).
Meanwhile, Dθ distinguishes between actual
(xt−k) and synthetic samples (ˆxt−k) for the
denoised image.
literature for high-ﬁdelity medical image synthesis.
• We introduce the ﬁrst diffusion-based method for unsupervised medical image translation that enables training
on unpaired datasets of source-target modalities.
• We propose a novel source-conditional adversarial projector to capture reverse transition probabilities over large
step sizes for efﬁcient image sampling.
II. RELATED WORK
To translate medical images, conditional GANs perform
one-shot source-to-target mapping via a generator trained
using an adversarial loss . Adversarial loss terms are
known to improve sensitivity to high-frequency details in
tissue structure over canonical pixel-wise losses . As
such, GAN-based translation has been broadly adopted in
many applications. Augmenting adversarial with pixel-wise
losses, a ﬁrst group of studies considered supervised training
on paired sets of source-target images matched across subjects , – . For improved ﬂexibility, other studies
proposed cycle-consistency or mutual information losses to
enable unsupervised learning from unpaired data , ,
 – . In general, enhanced spatial acuity and realism
have been reported in target images synthesized with GANs
when compared to vanilla convolutional models . That
said, several problems can arise in GAN models, including
lower mapping reliability for the one-shot sampling process
 , premature convergence of the discriminator before the
generator is properly trained , and poor representational
diversity due to mode collapse . In turn, these problems
can lower sample quality and diversity, limiting generalization
performance of GAN-based image translation.
As a recent alternative to GANs, deep diffusion models have
received interest for generative modeling tasks in computer
vision , . Starting from a pure noise sample, diffusion
models generate image samples from a desired distribution
through repetitive denoising. Denoising is performed via a
neural network architecture trained to maximize a correlate
on data likelihood. Due to the gradual stochastic sampling
process and explicit likelihood characterization, diffusion models can improve the reliability of the network mapping to
offer enhanced sample quality and diversity. Given this potential, diffusion-based methods have recently been adopted
for unimodal imaging tasks such as image reconstruction
 – , unconditional image generation , and anomaly
detection , . Nevertheless, these methods are typically
based on unconditional diffusion processes devised to process
single-modality images. Furthermore, current methods often
involve vanilla diffusion models that rely on a large number
of inference steps for accurate image generation. This prolonged sampling process introduces computational challenges
in adoption of diffusion models.
Here, we propose a novel adversarial diffusion model for
improved efﬁciency and performance in medical image translation. Unlike recent imaging methods based on unconditional
diffusion, SynDiff leverages conditional diffusion where a
source-contrast image anatomically guides the reverse diffusion process. A novel source-conditional adversarial projector
is employed for efﬁcient and accurate image sampling over few
OZBEY AND DALMAZ et al.: UNSUPERVISED MEDICAL IMAGE TRANSLATION WITH ADVERSARIAL DIFFUSION MODELS
large diffusion steps. Furthermore, a novel cycle-consistent architecture is introduced combining diffusive and non-diffusive
modules to enable unsupervised training. To our knowledge,
SynDiff is the ﬁrst adversarial diffusion model for medical
image synthesis, and the ﬁrst diffusion-based method for
unsupervised medical image translation in the literature. Based
on these unique advances, we provide the ﬁrst demonstrations
of unsupervised translation in multi-contrast MRI and multimodal MRI-CT based on diffusion modeling.
Few recent studies have considered improvements on vanilla
diffusion models with partially related aims to our proposed
method. A study on natural image generation has used an
adversarial diffusion model, DDGAN, to improve efﬁciency in
reverse diffusion steps . DDGAN is based on an unconditional diffusion process that generates random images starting
from noise; and it uses an adversarial generator for reverse
diffusion without guidance from a source image. In contrast,
SynDiff is based on a conditional diffusion process that
translates between source- and target-images of an anatomy.
It uses a source-conditional adversarial projector for reverse
diffusion to synthesize target images with anatomical correspondence to a guiding source image. Besides the diffusive
module, SynDiff also embodies a non-diffusive module to permit unsupervised image translation. A study on unsupervised
translation of natural images has proposed a non-adversarial
diffusion model, UNIT-DDPM . Based on the notion that
source-target modalities share a latent space, UNIT-DDPM
uses parallel diffusion processes to simultaneously generate
samples for both modalities in a large number of reverse
steps; and the noisy source-image samples drawn from the
source diffusion process are used to condition the generation
of target images in the target diffusion process. In contrast,
SynDiff uses an adversarial projector for efﬁcient sampling
in few steps; and it leverages source-image estimates that are
produced by a non-diffusive module to provide high-quality
anatomical guidance for synthesis of target images. A recent
study has independently considered a conditional score-based
method, UMM-CGSM, for imputation of missing sequences in
a multi-contrast MRI protocol . UMM-CGSM uses a nonadversarial model with relatively large number of inference
steps; and it performs supervised training on paired datasets
of source-target images. In contrast, SynDiff adopts an adversarial diffusion model for efﬁcient sampling over few steps;
and it can perform unsupervised learning.
III. THEORY
A. Denoising Diffusion Models
Regular diffusion models map between pure noise samples
and actual images through a gradual process over T time steps
(Fig. 1a). In the forward direction, a small amount of Gaussian
noise is added repeatedly onto an input image x0 ∼q(x0) to
obtain a sample xT from an isotropic Gaussian distribution
for sufﬁciently large T. Forward diffusion forms a Markov
chain where the mapping from xt−1 to xt and the respective
forward transition probability are:
1 −βtxt−1 +
ϵ ∼N (0, I)
q (xt|xt−1) = N
1 −βtxt−1, βtI
TABLE I: Description of variables related to images, diffusion
processes, networks and probability distributions. Throughout the
manuscript, vectorial quantities are annotated in bold font.
Actual target-image sample
Noisy target-image sample at time step t
Noisy target-image sample at time step T,
(i.e., drawn from isotropic Gaussian distribution)
Guiding source image
Actual target-image sample at time step t −k
Synthesized target-image sample at time step t −k
Unpaired training images from modalities A and B
Source images estimated by non-diffusive module
Target images synthesized by non-diffusive module
Target images synthesized by diffusive module
Regular diffusion
Noise variance for regular diffusion at time step t
Standard normal random vector
µ(xt, t), Σ(xt, t)
Network estimates for mean and covariance
of the conditional distribution of xt−1 given xt
r=[0,1,..,t] ψr
Network estimate for the added noise at time step t
Adversarial diffusion
Step size for fast diffusion
Noise variance for fast diffusion at time step t
βmin, βmax
Parameters that control the progression of noise variance
Feature maps in the ith subblock of the diffusive generator
Learnable temporal embedding added onto feature maps
to encode the time step t
r=[0,k,..,t] αr
Non-diffusive generator-discriminator pair for learning
to estimate a source image ˜yA given xB
Non-diffusive generator-discriminator pair for learning
to estimate a source image ˜yB given xA
Diffusive generator-discriminator pair for learning
to synthesize a target image ˆxA
t−k given xA
Diffusive generator-discriminator pair for learning
to synthesize a target image ˆxB
t−k given xB
Probability distributions
Actual image distribution
q(xt|xt−1)
Forward transition probability
q(xt−1|xt)
Reverse transition probability
pθ(xt−1|xt)
Network estimate for reverse transition probability
q(xt−k|xt, y)
Reverse transition probability for fast conditional
diffusion with step size k ≫1
pθ(xt−k|xt, y)
Network estimate for reverse transition probability
in fast conditional diffusion with step size k ≫1
where βt is noise variance, ϵ is added noise, N is a Gaussian
distribution, I is an identity covariance matrix. Reverse diffusion also forms a Markov chain from xT onto x0, albeit each
step aims to gradually denoise the samples. Under large T and
small βt, the reverse transition probability between xt−1 and
xt can be approximated as a Gaussian distribution , :
q(xt−1|xt) := N(xt−1; µ(xt, t), Σ(xt, t))
Diffusion models typically operationalize each reverse diffusion step as mapping through a neural network that provides
estimates for µ and/or Σ. Training is then performed by
minimizing a variational bound on log-likelihood:
Lvb = Eq(x0:T )
log pθ(x0:T )
q(x1:T |x0)
≤Eq(x0) [ log pθ(x0)]
where Eq denotes expectation over q, pθ is the network
parametrization of the joint distribution of input variables, θ
are network parameters, x0:T denote the collection of image
samples between time steps 0 and T, and x1:T |x0 denote
image samples between time steps 1 and T conditioned on
the sample at time step 0. The bound can be decomposed as:
log pθ(x0|x1)
KL(q(xt−1|xt, x0) || pθ(xt−1|xt))
Kullback-Leibler
divergence,
KL(q(xT |x0) || p(xT )) is omitted as it does not depend on
θ. A common parametrization omits Σ to focus on µ:
µθ(xt, t) =
where ψt = 1−βt and ψt = Q
r=[0,1,..,t] ψr. In Eq. 6, µθ can
be derived if the network is used to estimate the added noise
ϵ by minimizing the following loss :
Lerr = Et,x0,ϵ
1 −αtϵ, t)∥2
where t, x0 and ϵ are sampled from the discrete uniform
distribution U(0, T), q(x0) and N (0, I), respectively. During
inference, reverse diffusion steps are performed starting from
a random sample xT ∼N(0, I). For each step t ∈T...1, µ
is derived using Eq. 6 based on the network estimate ϵθ, and
xt−1 is sampled based on Eq. 3.
B. SynDiff
Here, we introduce a novel diffusion model for efﬁcient,
high-ﬁdelity translation between source and target modalities
of a given anatomy. SynDiff uses a diffusive module equipped
with a source-conditional adversarial projector for fast and
accurate reverse diffusion sampling (Fig. 1b). It also employs
a non-diffusive module for estimating source images paired
with corresponding target images, so as to enable unsupervised
learning (Fig. 2). The adversarial diffusion process that forms
the basis of the diffusive module, the network architecture,
and the learning procedures for SynDiff are detailed below.
1) Adversarial Diffusion Process: Regular diffusion models
prescribe relatively large T such that the step size is sufﬁciently small to satisfy the normality assumption in Eq. 3, but
this limits efﬁciency in image generation. Here, we instead
propose fast diffusion with the following forward steps:
1 −γtxt−k + √γtϵ
q(xt|xt−k) = N
1 −γtxt−k, γtI
where k ≫1 is step size. The noise variance γt is set as:
γt = 1 −eβmin
T −(βmax−βmin) 2tk−k2
βmin and βmax control the progression of noise variance in an
exponential schedule .
Guidance from a source image (y) is available during
medical image translation, so a conditional process is proposed
in the reverse diffusion direction. Note that, for k ≫1, there is
no closed form expression for q(xt−k|xt, y) and the normality
assumption used to compute Eq. 4 breaks down . Here
we introduce a novel source-conditional adversarial projector
to capture the complex transition probability q(xt−k|xt, y)
for large k in our conditional diffusion model, as inspired
by a recent report on unconditional generation of natural
images using adversarial learning to capture q(xt−k|xt) .
In SynDiff, a conditional generator Gθ(xt, y, t) performs
gradual denoising in each reverse step to synthesize ˆxt−k ∼
pθ(xt−k|xt, y). Gθ receives the image pair (xt,y) as a twochannel input, and it extracts intermediate feature maps f i
where i ∈[1, ..., N] is the subblock index in an encoderdecoder structure . A learnable temporal embedding m
is computed given t, and this embedding is added as a
channel-speciﬁc bias term onto the feature maps in each
subblock : f ′
i = f i + m. Meanwhile, a discriminator
Dθ({ˆxt−k or xt−k}, xt, t) distinguishes samples drawn from
estimated versus true denoising distributions (pθ(xt−k|xt, y)
vs. q(xt−k|xt, y)). Dθ receives either (xt,ˆxt−k) or (xt,xt−k)
as a two-channel input. The temporal embedding m is also
added as a bias term onto the feature maps across Dθ. A nonsaturating adversarial loss is adopted for Gθ :
LGθ = Et,q(xt|x0,y),pθ(xt−k|xt,y)[−log(Dθ(ˆxt−k))]
where t ∼U({0, k, ..., T}), and the discriminator arguments
are abbreviated for brevity. Dθ also adopts a non-saturating
adversarial loss with gradient penalty :
LDθ = Et,q(xt|x0,y)
Eq(xt−k|xt,y) [−log(Dθ(xt−k))]
+Epθ(xt−k|xt,y)[−log(1 −Dθ(ˆxt−k))]
+ηEq(xt−k|xt,y)
∇xt−kDθ(xt−k)
where η is the weight for the gradient penalty.
Evaluation
q(xt−k|xt, y)
non-linearly
conditionally
independent
transition
probability
q(xt−k|xt, x0, y) = q(xt−k|xt, x0) . Bayes’ rule can
then be used to express the denoising distribution in terms of
forward transition probabilities:
q(xt−k|xt, x0) = q(xt|xt−k, x0)q(xt−k|x0)
Using Eq. 8, it can then be shown that q(xt−k|xt, x0) =
N(xt−k; µ(xt, x0), γI) with the following parameters:
√αt (1 −αt−k)
xt, γ = 1 −αt−k
where αt = 1 −γt and αt = Q
r=[0,k,..,t] αr.
Eqs. 11-12 also require sampling from the networkparameterized denoising distribution pθ(xt−k|xt, y). A trivial
albeit deterministic sample would be the generator output, i.e.
ˆxt−k ∼δ(xt−k −Gθ(xt, y, t)). To maintain stochasticity, we
OZBEY AND DALMAZ et al.: UNSUPERVISED MEDICAL IMAGE TRANSLATION WITH ADVERSARIAL DIFFUSION MODELS
Fig. 2: For unsupervised learning, Syn-
Diff leverages a cycle-consistent architecture that bilaterally translates between two
modalities (A, B). For synthesizing a target image ˆxA
0 of modality A, the diffusive module in Fig. 1b requires guidance
from a source image yB of modality B
for the same anatomy. However, a paired
source image of the same anatomy might
be unavailable in the training set. To enable training on unpaired images, SynDiff
uses a non-diffusive module to ﬁrst estimate a paired source image ˜yB from
0 . Similarly, for synthesizing a target
0 of modality B with the diffusive module, the non-diffusive module ﬁrst
estimates a paired source image ˜yA from
0 . a) To do this, the non-diffusive module comprises two generator-discriminator
pairs (GφA,B, DφA,B) that generate initial translation estimates for xA
(orange) and xB
0 →˜yA (green). b) These
initial translation estimates ˜yA,B are then
used as guiding source-modality images in
the diffusive module. For cycle-consistent
learning, the diffusive module also comprises two generator-discriminator pairs
(GθA,B, DθA,B) to generate denoised image estimates for (xA
t , ˜yB, t) →ˆxA
(yellow) and (xB
t , ˜yA, t) →ˆxB
t−k (blue).
instead operationalize the generator distribution as follows:
pθ(xt−k|xt, y) := q(xt−k|xt, ˜x0 = Gθ(xt, y, t))
where Gθ predicts ˜x0 that is t/k steps away from xt. Following a total of T/k reverse diffusion steps, the eventual denoised
image will be obtained via sampling ˆx0 ∼pθ(x0|xk, y).
2) Network Architecture: To synthesize a target-modality
image, the reverse diffusion steps parametrized in Eq.15
require guidance from a source-modality image of the same
anatomy. However, the training set might include only unpaired images xA
0 for the modalities A, B, respectively.
To learn from unpaired training sets, we introduce a cycleconsistent architecture based on non-diffusive and diffusive
modules that bilaterally translate between the two modalities.
Non-diffusive module. SynDiff leverages a non-diffusive
module to estimate a source image paired with each target
image in the training set. A source-image estimate ˜yB of
modality B is produced given a target image xA
0 of modality
A; and a source-image estimate ˜yA is produced given a
target image xB
0 . To do this, two generator-discriminator
pairs (GφA,DφA) and (GφB,DφB) with parameters φA,B are
employed . The generators produce the estimates ˜yA,B as:
˜yB = GφB(xA
˜yA = GφA(xB
A non-saturating adversarial loss is adopted for GφA,B:
LGφ = Epφ(y|x0)[−log(Dφ(˜y))]
where pφ(y|x0) denotes the network parametrization of the
conditional distribution of the source given the target image,
and the conditioning input x0 to the discriminator is omitted
for brevity. Meanwhile, the discriminators distinguish samples
of estimated versus true source images by adopting a nonsaturating adversarial loss:
Eq(y|x0)[−log(Dφ(y))] +
Epφ(y|x0)[−log(1 −Dφ(˜y))]
where q(y|x0) is the true conditional distribution of the source
given the target image. Note that, for DφB, y corresponds to
0 and the conditioning input is xA
0 ; whereas for DφA, y
corresponds to xA
0 and the conditioning input is xB
Diffusive module. SynDiff then leverages a diffusive module to synthesize target images given source-image estimates
from the non-diffusive module as guidance. A synthetic target
image ˆxA is produced given ˜yB; and a synthetic target image
ˆxB is produced given ˜yA. To do this, two adversarial diffusion
processes are used with respective generator-discriminator
pairs (GθA,DθA) and (GθB,DθB) of parameters θA,B. Starting
with Gaussian noise images xA,B
at time step T, target images
are synthesized in T/k reverse diffusion steps. In each step,
the generators ﬁrst produce deterministic estimates of denoised
target images as noted in Sec. III-B.1:
0 = GθA(xA
t , y = ˜yB, t)
0 = GθB(xB
t , y = ˜yA, t)
Afterwards, the denoising distribution for each modality as
described in Eq. 15 is used to synthesize target images:
3) Learning Procedures: To achieve unsupervised learning,
SynDiff leverages a cycle-consistency loss by comparing true
target images against their reconstructions. In the diffusive
module, reconstructions are taken as synthetic target images
. In the non-diffusive module, source-image estimates are
projected to the target domain via the generators:
0 = GφA(˜yB)
0 = GφB(˜yA)
where ˘xA,B
denote the corresponding reconstructions. Afterwards, the cycle-consistency loss is deﬁned as:
Lcyc = Et,q(xA,B
0 |1) + λ1θ(|xA
0 |1 + |xB
where λ1φ,1θ are the weights for cycle-consistency loss terms
from the non-diffusive and diffusive modules respectively, and
ℓ1-norm of the difference between two images is taken as
a consistency measure . The diffusive and non-diffusive
modules are trained jointly without any pretraining procedures.
Accordingly, the overall generator loss is:
φ ) + λ2θ(LGA
θ ) + Lcyc (23)
where λ2φ,2θ are the weights for adversarial loss terms from
the non-diffusive and diffusive modules respectively, and for
each modality LGφ is deﬁned as in Eq. 17 and LGθ is deﬁned
as in Eq. 11. The overall discriminator loss is given as:
φ ) + λ2θ(LDA
with LDφ deﬁned as in Eq. 18 and LDθ deﬁned as in Eq. 12.
During training, the non-diffusive module must be used to
produce estimates of source images paired with given target
images. During inference, however, the task is to synthesize
an unacquired target image given the acquired source image
of an anatomy, so only the respective generator within the
diffusive module that performs the desired task is needed. For
instance, to perform the mapping A→B (i.e., source→target),
t , yA, t) is used where xB
t is the target-image sample
of modality B at time step t and yA is the acquired source
image of modality A provided as input. Inference starts at
time step T with a Gaussian noise sample xB
T drawn from
N (0, I), and the noisy target-image sample produced at the
end of each reverse diffusion step is taken as the input targetimage sample in the following step. A total of T/k reverse
diffusion steps are taken as outlined in Eqs. 19-20 to attain
0 at time step 0 as the synthetic target image.
IV. METHODS
A. Datasets
We demonstrated SynDiff on two multi-contrast brain MRI
datasets (IXI1, BRATS ), and a multi-modal pelvic MRI-
CT dataset . In each dataset, a three-way split was performed to create training, validation and test sets with no
subject overlap. While all unsupervised medical image translation models were trained on unpaired images, performance
assessments necessitate the presence of paired and registered
source-target volumes. Thus, in the validation and test sets,
1 
separate volumes of a given subject were spatially registered to
enable calculation of quantitative metrics. Registrations were
implemented in FSL via afﬁne transformation and mutual
information loss . In each subject, each imaging volume
was separately normalized to a mean intensity of 1. The
maximum voxel intensity across subjects was then normalized
to 1 to ensure an intensity range of . Cross-sectional
images were zero-padded as necessary to attain a consistent
256×256 image size in all datasets prior to modeling.
PD-weighted
40 healthy subjects were analyzed, with (25,5,10) subjects reserved for (training,validation,test). T2 and PD volumes were registered onto T1 volumes in validation/test
sets. In each subject, 100 axial cross-sections with brain
tissue were selected. Scan parameters were TE=4.6ms,
TR=9.81ms for T1; TE=100ms, TR=8178.34ms for T2;
TE=8ms, TR=8178.34ms for PD images; and a common
spatial resolution=0.94×0.94×1.2mm3.
2) BRATS Dataset: T1-, T2-, Fluid Attenuation Inversion
Recovery (FLAIR) weighted brain MR images from 55 glioma
patients were analyzed, with a (training, validation, test) split
of (25,10,20) subjects. T2 and FLAIR volumes were registered
onto T1 volumes in validation/test sets. In each subject, 100
axial cross-sections containing brain tissue were selected.
Diverse scan protocols were used at multiple institutions.
3) Pelvic MRI-CT Dataset: Pelvic T1-, T2-weighted MRI,
and CT images from 15 subjects were analyzed, with a
(training, validation, test) split of (9,2,4) subjects. T1 and CT
volumes were registered onto T2 volumes in validation/test
sets. In each subject, 90 axial cross-sections were selected.
For T1 scans, TE=7.2ms, TR=500-600ms, 0.88×0.88×3mm3
resolution, or TE=4.77ms, TR=7.46ms, 1.10×1.10×2mm3
resolution were prescribed. For T2 scans, TE=97ms, TR=6000-
6600ms, 0.88×0.88×2.50mm3 resolution, or TE=91-102ms,
TR = 12000-16000ms, 0.88-1.10×0.88-1.10×2.50mm3 resolution were prescribed. For CT scans, 0.10×0.10×3mm3 resolution, Kernel=B30f, or 0.10×0.10×2mm3 resolution, Kernel=FC17 were prescribed. To implement synthesis tasks from
accelerated MRI scans , , fully-sampled MRI data
were retrospectively undersampled 4-fold in two dimensions
to attain low-resolution images at a 16x acceleration rate .
B. Competing Methods
We demonstrated SynDiff against several state-of-the-art
non-attentional GAN, attentional GAN, and diffusion models.
All competing methods performed unsupervised learning on
unpaired source and target modalities. For each model, hyperparameter selection was performed to maximize performance
on the validation set. A common set of parameters that offered
near-optimal quantitative performance while maintaining high
spatial acuity was selected across translation tasks. The selected parameters included number of training epochs, learning
rate for the optimizer, and loss-term weightings for each
model. Additionally, the step size was selected for diffusion
1) SynDiff: In the non-diffusive module, generators used a
ResNet backbone with three encoding, six residual, and three
decoding blocks ; and discriminators used six blocks with
OZBEY AND DALMAZ et al.: UNSUPERVISED MEDICAL IMAGE TRANSLATION WITH ADVERSARIAL DIFFUSION MODELS
Fig. 3: SynDiff was demonstrated on IXI for translation between MRI contrasts. Synthesized images from competing methods are displayed
along with the source and the ground-truth target (reference) images for representative a) T1→T2, b) T2→PD tasks. Display windows of a)
[0 0.65], b) [0 0.80] are used. Compared to baselines, SynDiff yields lower noise and artifacts, and maintains higher anatomical ﬁdelity.
TABLE II: Performance for multi-contrast MRI translation tasks in IXI. PSNR (dB) and SSIM (%) are listed as mean±std across the test set.
Boldface marks the top-performing model in each task.
30.42±1.40
94.77±1.26
30.32±1.46
94.28±1.32
30.09±1.36
94.99±1.17
30.85±1.56
94.03±1.12
33.64±0.86
96.58±0.36
35.47±1.15
96.98±0.36
29.22±1.20
93.46±1.33
29.24±1.26
93.01±1.44
28.42±1.03
93.38±1.19
29.92±1.45
93.19±1.21
33.58±0.75
96.46±0.39
34.24±1.00
96.09±0.47
29.17±1.15
93.54±1.34
28.34±0.98
92.02±1.45
28.10±0.99
92.97±1.21
29.29±1.08
92.36±1.24
32.57±0.65
96.22±0.38
34.74±1.07
96.66±0.39
26.35±0.88
89.78±1.78
26.61±0.86
88.28±1.90
25.99±0.89
89.73±1.70
27.59±1.02
88.71±1.60
29.17±0.71
92.01±1.01
29.80±0.82
91.61±1.00
29.27±1.38
93.74±1.36
28.37±1.08
92.21±1.45
28.02±1.07
92.83±1.19
29.65±1.42
92.98±1.23
32.15±0.67
95.93±0.44
35.11±1.11
96.76±0.40
28.85±1.26
93.38±1.40
29.01±1.32
92.87±1.43
27.93±1.22
93.04±1.29
29.58±1.51
92.76±1.25
32.44±0.71
95.91±0.46
34.75±0.83
96.64±0.38
24.93±0.69
89.49±1.69
28.04±1.03
91.14±1.58
24.95±0.74
89.08±1.67
27.16±0.95
90.45±1.33
30.49±0.84
94.74±0.69
29.67±0.71
93.18±0.83
24.01±0.72
86.59±2.16
22.44±1.26
81.64±3.06
23.81±0.97
86.62±2.44
26.81±1.35
88.57±2.04
25.43±0.49
88.08±1.09
25.13±1.42
84.47±2.53
two convolutional layers followed by two-fold spatial downsampling. In the diffusive module, generators used a UNet
backbone with six encoding and decoding blocks . Each
block had two residual subblocks followed by a convolutional
layer. For encoding, the convolutional layer halved feature
map resolution and channel dimensionality was doubled every
other block. For decoding, the convolutional layer doubled
resolution and channel dimensionality was halved every other
block. Residual sublocks received a temporal embedding
derived by projecting a 32-dimensional sinusoidal position
encoding through a two-layer multi-layer perceptron (MLP)
 . They also received 256-dimensional random latents from
a three-layer MLP to modulate feature maps via adaptive
normalization . Discriminators used six blocks with two
convolutional layers followed by two-fold downsampling, and
the temporal embedding was added onto feature maps in each
block. Cross-validated hyperparameters were: 50 epochs, 10−4
learning rate, µ=0.5, T=1000, a step size of k=250, and T/k=4
diffusion steps. Weights for cycle-consistency and adversarial
loss terms were λ1φ,1θ=0.5 and λ2φ,2θ=1, respectively. Lower
and upper bounds on the noise variance schedule were set
according to βmin=0.1, βmax=20.
2) cGAN: A cycle-consistent GAN model was considered
with architecture and loss functions adopted from . cGAN
comprised two generators with ResNet backbones, and two
discriminators with a cascade of convolutional blocks followed
by instance normalization. Cross-validated hyperparameters
were 100 epochs, 2x10−4 learning rate linearly decayed to
0 in the last 50 epochs. Weights for cycle-consistency and
adversarial losses were 100 and 1.
3) UNIT: An unsupervised GAN model that assumes a
shared latent space between source-target modalities was considered, with architecture and loss functions adopted from
 . UNIT comprised two discriminators and two translators
with ResNet backbones in a cyclic setup. The translators contained parallel-connected domain image encoders and generators with a shared latent space. The discriminators contained
a cascade of downsampling convolutional blocks. Crossvalidated hyperparameters were 100 epochs, 10−4 learning
rate. Weights for cycle-consistency, adversarial, reconstruction
losses were 10, 1, and 10.
4) MUNIT: An unsupervised GAN model that assumes a
shared content space albeit distinct style distributions for
source-target modalities was considered, with architecture and
loss functions adopted from . MUNIT comprised pairs of
discriminators, content encoders with ResNet backbones, MLP
style encoders, and decoders with ResNet backbones. Crossvalidated hyperparameters were 100 epochs, 10−4 learning
rate. Weights for image, content, style reconstruction, adversarial losses were 10, 1, 1, and 1.
5) AttGAN: A cycle-consistent GAN model with attentional
generators was adopted for unsupervised translation.
Fig. 4: SynDiff was demonstrated on BRATS for translation between MRI contrasts. Synthesized images are displayed along with the source
and the ground-truth target (reference) images for representative a) T1→T2, b) T2→FLAIR tasks. Display windows of a) [0 0.75], b) [0
0.80] are used. SynDiff lowers noise/artifact levels and more accurately depicts detailed structure compared to baselines.
TABLE III: Performance for multi-contrast MRI translation tasks in BRATS. PSNR (dB) and SSIM (%) listed as mean±std across the test set.
28.90±0.73
93.79±0.95
27.10±1.26
92.35±1.27
26.47±0.69
89.37±1.50
26.45±1.02
87.79±1.67
26.75±1.18
91.69±1.50
28.17±0.90
90.44±1.48
27.41±0.45
92.07±0.92
27.00±1.11
91.90±1.13
26.35±0.77
89.03±1.51
26.44±0.73
85.98±1.51
25.99±1.30
90.02±1.67
27.41±0.78
88.48±1.45
25.76±0.69
87.99±1.08
23.72±1.15
86.62±1.34
26.28±0.75
88.40±1.46
26.41±0.75
86.12±1.43
25.29±1.34
88.41±1.73
26.92±0.69
86.84±1.43
25.88±0.73
88.16±1.17
23.70±1.12
86.03±1.34
25.08±0.64
86.38±1.42
24.91±0.76
82.73±1.49
24.22±1.11
85.78±1.39
25.26±0.65
83.19±1.42
27.22±0.47
91.87±0.89
26.05±1.16
91.11±1.36
25.59±0.60
87.37±1.32
23.71±1.13
82.12±2.04
24.36±1.14
87.19±1.52
26.56±0.73
86.44±1.38
26.94±0.54
91.70±0.96
26.60±1.10
91.55±1.19
21.70±1.02
79.82±3.03
23.95±1.19
81.40±2.44
20.33±1.49
79.72±2.00
22.52±1.02
81.02±1.76
27.36±0.58
91.94±0.96
26.34±1.17
91.50±1.27
23.41±0.64
81.55±2.43
24.49±1.12
82.12±1.97
21.23±1.50
82.38±2.45
25.49±0.60
84.71±1.40
19.84±1.54
85.92±2.28
23.71±1.50
88.75±2.49
20.31±0.84
79.30±2.08
21.33±1.18
81.80±1.99
20.03±1.61
77.21±2.03
24.15±1.03
82.07±1.84
AttGAN comprised two convolutional attention UNet generators and two patch discriminators . Cross-validated
hyperparameters were 100 epochs, 2x10−4 learning rate linearly decayed to 0 in the last 50 epochs. Weights for cycleconsistency and adversarial losses were 100 and 1.
6) SAGAN: A cycle-consistent GAN model with selfattention generators was adopted for unsupervised translation. SAGAN comprised two generators based on a ResNet
backbone with self-attention layers in the last two residual
blocks, and two patch discriminators . Cross-validated
hyperparameters were 100 epochs, 2x10−4 learning rate linearly decayed to 0 in the last 50 epochs. Weights for cycleconsistency and adversarial losses were 100 and 1.
7) DDPM: A recent diffusion model with improved sampling efﬁciency was considered, with architecture and loss
functions adopted from . The source modality was given
as a conditioning input to reverse diffusion steps, and cycleconsistent learning was achieved by including non-diffusive
modules as in SynDiff. Cross-validated hyperparameters were
50 epochs, 10−4 learning rate, T=1000, k=1, and 1000 diffusion steps. A cosine noise schedule was used as in .
Weight for cycle-consistency loss was 1.
8) UNIT-DDPM: A recent diffusion model allowing unsupervised training was considered, with architecture and loss
functions adopted from . UNIT-DDPM comprised two
parallel diffusion processes for the source and target modalities, where noisy samples from each modality were given
as conditioning input to reverse diffusion steps for the other
modality. Cross-validated hyperparameters were 50 epochs,
10−4 learning rate, T=1000, k=1, and 1000 diffusion steps.
A cosine noise schedule was used . Weight for cycleconsistency loss was 1, and the release time was 1 as in .
C. Modeling Procedures
All models were implemented in Python using the PyTorch
framework. Models were trained using Adam optimizer with
β1=0.5, β2=0.9. Models were executed on a workstation
equipped with Nvidia RTX 3090 GPUs. Model performance
was evaluated on the test set within each dataset. For fair
comparison, evaluations of both deterministic and stochastic
methods were performed based on a single target image
synthesized at each cross section given the respective source
image. Performance was assessed via peak signal-to-noise
ratio (PSNR), structural similarity index (SSIM) metrics in
conditional synthesis tasks where a ground-truth reference is
available. For unconditional tasks, Fr´echet inception distance
(FID) score was utilized to assess the perceptual quality of the
generated random synthetic images by comparing their overall
distribution to that of actual images. Prior to assessment,
all images were normalized by their mean, and all examined images in a given cross-section were then normalized
by the maximum intensity in the reference image. Signiﬁ-
OZBEY AND DALMAZ et al.: UNSUPERVISED MEDICAL IMAGE TRANSLATION WITH ADVERSARIAL DIFFUSION MODELS
Fig. 5: SynDiff was demonstrated on the pelvic dataset for multi-modal MRI-CT translation. Synthesized images are displayed along with
the source and the ground-truth target (reference) images for representative a) T2→CT, b) accelerated T1→CT tasks. A display window of
[0 0.75] is used. Compared to diffusion and GAN baselines, SynDiff achieves lower artifact levels, and more accurately estimates anatomical
structure near diagnostically-relevant regions.
TABLE IV: Performance for multi-modal MRI-CT translation tasks in
the pelvic dataset. PSNR (dB) and SSIM (%) listed as mean±std
across the test set. ‘acc.’ stands for accelerated.
acc. T2→CT
acc. T1→CT
cance of performance differences between competing methods
were assessed via non-parametric Wilcoxon signed-rank tests
V. RESULTS
A. Multi-Contrast MRI Translation
We demonstrated SynDiff for unsupervised MRI contrast
translation
state-of-the-art
non-attentional
attentional
UNIT-DDPM)
models. First, experiments were performed on brain images
from healthy subjects in IXI. Table II lists performance
metrics for T2→T1, T1→T2, PD→T1, T1→PD, PD→T2,
and T2→PD synthesis tasks. SynDiff yields the highest
performance in all tasks (p<0.05), except for PD→T2 where
cGAN performs similarly. On average, SynDiff outperforms
non-attentional GANs by 2.2dB PSNR and 2.5% SSIM,
attentional GANs by 1.4dB PSNR and 1.2% SSIM, and
regular diffusion models by 5.7dB PSNR and 6.6% SSIM
(p<0.05). Representative images are displayed in Fig. 3.
GANs show noise or local inaccuracies in tissue contrast.
Regular diffusion models suffer from a degree of spatial
warping and blurring. UNIT-DDPM shows relatively lower
anatomical accuracy, with occasional losses in tissue features.
In comparison, SynDiff yields lower noise and artifacts, and
higher accuracy in tissue depiction.
Next, experiments were conducted on brain images from
glioma patients in BRATS. Table III lists performance metrics
for T2→T1, T1→T2, FLAIR→T1, T1→FLAIR, FLAIR→T2,
and T2→FLAIR tasks. SynDiff again achieves the highest synthesis performance in all tasks (p<0.05), except for cGAN that
yields similar PSNR in T1→FLAIR, and performs similarly in
FLAIR→T1. On average, SynDiff outperforms non-attentional
GANs by 1.5dB PSNR and 3.5% SSIM, attentional GANs
by 2.7dB PSNR and 5.0% SSIM, and diffusion models by
4.2dB PSNR and 6.8% SSIM (p<0.05). Representative images
are displayed in Fig. 4. Non-attentional GANs show elevated
noise and artifact levels. Attentional GANs occasionally suffer
TABLE V: Average training times per cross-section (sec), inference
times per cross-section (sec) and memory load (gigabytes).
SynDiff cGAN UNIT MUNIT AttGAN SAGAN DDPM UNIT-DDPM
0.060 0.041
from leakage of contrast features from the source image (e.g.,
hallucination of regions with notably brighter or darker signal
levels). Regular diffusion models show a degree of blurring
and feature losses. Instead, SynDiff generates high-ﬁdelity
target images with low noise and artifacts.
B. Multi-Modal Translation
We also demonstrated SynDiff for unsupervised translation
between separate modalities. In particular, experiments were
performed using SynDiff, non-attentional GAN, attentional
GAN, and regular diffusion models on the pelvic dataset
for MRI-CT translation. Table IV lists performance metrics
for T2→CT, T1→CT, accelerated T2→CT, and accelerated
T1→CT synthesis tasks. SynDiff achieves the highest performance in all tasks (p<0.05). On average, SynDiff outperforms non-attentional GANs by 2.1dB PSNR and 7.6% SSIM,
attentional GANs by 3.3dB PSNR and 14.4% SSIM, and
diffusion models by 3.6dB PSNR and 7.2% SSIM (p<0.05).
Representative images are displayed in Fig. 5. Non-attentional
GANs and AttGAN show local contrast losses and artifacts,
SAGAN suffers from contrast leakage, and regular diffusion
models yield over-smoothing that can cause loss of ﬁne
features. While UNIT offers higher synthesis performance for
some segments near tissue boundaries, particularly around the
peripheral body-background boundary, SynDiff has generally
higher performance across the image. Overall, SynDiff synthesizes target images with high anatomical ﬁdelity.
C. Model Complexity
A practical concern for medical image translation is the
computational complexity of the applied models. Table V lists
the training time, inference time and memory use of competing
methods. As expected, one-shot GAN models have notably fast
training and inference compared to diffusion models. While
SynDiff has relatively comparable training times to other
diffusion models, its fast diffusion process improves inference
efﬁciency above two-orders-of-magnitude over DDPM and
UNIT-DDPM. In terms of memory utilization, SynDiff has
higher demand than cGAN, attentional GANs and UNIT,
comparable demand to MUNIT, albeit notably lower demand
than DDPM and UNIT-DDPM. Overall, SynDiff offers a more
favorable compromise between image ﬁdelity and computational complexity than regular diffusion models.
D. Image Variability
Image translation models involving random noise variables
produce stochastic outputs, which can induce variability in
target images independently synthesized for a given source
image. To assess image variability, we examined target-image
Fig. 6: The adversarial projector in SynDiff with T/k=4 steps was
compared against a variant model using an ℓ1-loss based projector
with T/k=4 and T/k=1000. Image samples are shown for the
unconditional synthesis tasks: a) T1 in IXI, b) T2 in BRATS and
c) CT in pelvic datasets. Display windows of a) [0 0.90], b) [0 0.80],
c) [0 0.80] are used.
TABLE VI: Performance of variant models in unconditional synthesis
tasks. FID is listed across the training set.
T1(IXI) T2(BRATS) CT(Pelvic)
Adv. proj. (T/k=4)
ℓ1 proj. (T/k=4)
ℓ1 proj. (T/k=1000)
samples from competing stochastic methods, SynDiff, MU-
NIT, DDPM and UNIT-DDPM. For each task, a random
selection of 50 cross sections was considered from the test
set. For each cross section, 10 target-image samples were
synthesized independently given the respective source image.
Mean and standard deviation (std.) of performance metrics
were computed across 10 samples. On average across cross
sections, the std. across samples is less than 0.02dB in PSNR
and 0.07% in SSIM for all methods, except for UNIT-DDPM
with std. less than 0.27dB in PSNR and 0.31% in SSIM.
Thus, all stochastic methods have minimal std. values relative
to mean values, suggesting limited variability in synthesized
target images.
E. Ablation Studies
We conducted a set of ablation studies to systematically
evaluate the importance of the main elements in SynDiff.
To demonstrate the importance of the adversarial diffusion
process, we compared the diffusive module in SynDiff based
on an adversarial projector against a variant diffusive module
based on an ℓ1-loss based projector for reverse diffusion. The
variant module shared the same overall loss function, albeit
it ablated the adversarial loss terms for the diffusive generators and discriminators. As such, the remaining loss terms
for the diffusive module were based on pixel-wise ℓ1-loss
similar to regular diffusion models. For focused assessment
of the diffusive module, demonstrations were performed in
unconditional synthesis tasks where guidance from the nondiffusive module was removed from all models. Synthetic
images in representative tasks are displayed in Fig. 6, and FID
scores are listed in Table VI. Compared to the ℓ1 projector
at T/k=4, the adversarial projector at T/k=4 substantially
improves visual image quality and FID scores over the ℓ1
OZBEY AND DALMAZ et al.: UNSUPERVISED MEDICAL IMAGE TRANSLATION WITH ADVERSARIAL DIFFUSION MODELS
TABLE VII: Performance of variant models ablated of adversarial loss,
cycle-consistency loss and the diffusive module. PSNR and SSIM
listed as mean±std across the test set.
±1.36 ±1.17 ±1.26 ±1.27 ±0.51 ±2.53
w/o adv. loss
±0.62 ±3.45 ±1.20 ±2.30 ±2.09 ±3.76
w/o cyc. loss
±0.73 ±1.39 ±1.51 ±2.21 ±0.30 ±2.29
Non-diff. module
±1.02 ±1.20 ±1.05 ±1.21 ±1.98 ±0.32
TABLE VIII: Performance of variant models for varying number of
steps T/k and varying loss-term weights (λ1φ, λ1θ, λ2φ, λ2θ). PSNR
and SSIM listed as mean±std across the test set.
29.47±1.35 94.46±1.24 27.11±1.31 92.48±1.27 26.97±0.53 87.76±2.56
30.09±1.36 94.99±1.17 27.10±1.26 92.35±1.27 26.86±0.51 87.94±2.53
29.83±1.28 94.85±1.18 27.24±1.24 92.42±1.24 26.95±0.59 87.95±2.78
λ1φ=0.25 30.04±1.33 94.96±1.14 26.67±1.20 91.89±1.26 26.81±0.56 87.69±2.66
30.09±1.36 94.99±1.17 27.10±1.26 92.35±1.27 26.86±0.51 87.94±2.53
30.11±1.29 94.97±1.14 27.33±1.19 92.72±1.17 27.07±0.63 88.22±2.74
λ1θ=0.25 29.85±1.23 94.81±1.14 27.18±1.21 92.28±1.20 27.09±0.58 88.01±2.81
30.09±1.36 94.99±1.17 27.10±1.26 92.35±1.27 26.86±0.51 87.94±2.53
30.06±1.32 94.98±1.15 27.90±1.25 92.03±1.27 27.06±0.44 88.23±2.52
30.12±1.29 95.04±1.14 27.82±1.20 93.12±1.18 26.80±0.45 88.10±2.46
30.09±1.36 94.99±1.17 27.10±1.26 92.35±1.27 26.86±0.51 87.94±2.53
29.66±1.22 94.69±1.15 26.99±1.24 91.82±1.34 26.72±0.53 87.69±2.64
29.97±1.22 94.91±1.14 27.29±1.24 92.59±1.22 26.76±0.50 87.34±2.55
30.09±1.36 94.99±1.17 27.10±1.26 92.35±1.27 26.86±0.51 87.94±2.53
29.60±1.21 94.77±1.14 27.20±1.26 92.48±1.24 26.83±0.48 87.97±2.55
projector at T/k=4, while performing competitively with the
ℓ1 projector at T/k=1000. These results demonstrate the utility
of adversarial projections for efﬁcient and accurate image
sampling during reverse diffusion.
We then examined the contributions of adversarial, cycleconsistent and diffusive learning in SynDiff. A ﬁrst variant
model was constructed by ablating adversarial loss; a second
variant model was constructed by ablating cycle-consistency
loss; and a third variant model was constructed by ablating the
diffusive module to synthesize target images directly using the
non-diffusive module. As listed in Table VII, SynDiff achieves
substantially higher performance than all variants, indicating
the importance of each learning strategy. We also assessed the
test performance of SynDiff as a function of the number of
diffusion steps (T/k), and as a function of weights that control
the balance between separate loss terms (λ1φ, λ1θ, λ2φ, λ2θ).
In each case, models were trained across a range of values
centered around the parameters selected based on validation
performance. As seen in Table VIII, there are generally minute
differences in image quality among variants based on different
parameter values. On average across tasks, we ﬁnd less than
0.2dB PSNR, 0.2% SSIM difference between the selected
and remaining T/k values, and less than 0.3dB PSNR, 0.4%
SSIM difference between the selected and remaining loss-term
TABLE IX: Performance of variant models as mean±std across the
test set. The non-diffusive module was pretrained in variant models.
In pretrained-frozen, the non-diffusive module was not updated while
training the diffusive module. In pretrained-trained, the non-diffusive
module was also updated while training the diffusive module.
±1.36 ±1.17 ±1.26 ±1.27 ±0.51 ±2.53
Pretrained-frozen
±1.28 ±1.23 ±1.30 ±1.31 ±0.82 ±2.81
Pretrained-trained
±1.17 ±1.21 ±1.23 ±1.24
TABLE X: Performance of variant models as mean±std across the test
set. In variant models, the non-diffusive module was only trained for
nND epochs while the diffusive module was fully trained.
±0.60 ±2.70 ±1.08 ±1.13 ±0.39 ±2.44
±0.70 ±1.17 ±1.13 ±1.26 ±0.19 ±2.71
±1.14 ±1.12 ±1.33 ±1.48 ±0.35 ±2.52
±1.36 ±1.17 ±1.26 ±1.27 ±0.51 ±2.53
weights. Overall, these results suggest that SynDiff shows a
degree of reliability against parameter variations.
Next, we questioned whether SynDiff would beneﬁt from
pretraining of the non-diffusive module to improve stability. To
address this question, SynDiff was compared against variant
models that pretrained the non-diffusive module for 50 epochs
to optimize its translation performance, and later combined
the pretrained non-diffusive module with a randomly initialized diffusive module. A pretrained-frozen variant trained the
combined model while the non-diffusive module was frozen.
A pretrained-trained variant trained the combined model while
both diffusive and non-diffusive modules were updated. As
listed in Table IX, there are marginal performance changes
between SynDiff and variants, with differences less than 0.3dB
PSNR and 0.3% SSIM on average across tasks. These results
suggest that the two modules in SynDiff can be jointly trained
without notable stability issues or performance losses.
Finally, we assessed the dependence of the diffusive module
on the quality of the source-image estimates provided by the
non-diffusive module. For this purpose, we trained variant
models in which the non-diffusive module was intentionally
undertrained to produce suboptimal source-image estimates.
Accordingly, the training of the non-diffusive module was
stopped early by freezing its weights after a certain number of
epochs (nND), while the training of the diffusive module was
continued for the full 50 epochs. Table X lists performance of
variant models across a range of nND values. Compared to
SynDiff at nND=50, we ﬁnd relatively modest performance
differences of 0.7dB PSNR, 1.1% SSIM at nND=25, and
more notable differences of 2.0dB PSNR, 3.6% SSIM starting
at nND=10. These results indicate that while training of
the diffusive module shows a degree of reliability against
suboptimal source-image estimates, a well-functioning nondiffusive module is key for the performance of the diffusive
module in unsupervised medical image translation.
VI. DISCUSSION
SynDiff adopts adversarial learning for two separate purposes. Adversarial loss in the diffusive module enables accurate reverse diffusion over large step sizes. Meanwhile,
adversarial loss in the non-diffusive module enables unsupervised training. In theory, these losses might introduce
vulnerability against training instabilities, typically manifested
as oscillatory patterns and suboptimal convergence in model
performance . To rule out this potential issue, we inspected
the validation performance of SynDiff across training epochs.
We do not ﬁnd any notable sign of instabilities as model
performance across epochs progresses smoothly towards a
convergent point, without abrupt jumps (not reported). We
also observe that pretraining of the non-diffusive module does
not yield a notable beneﬁt, suggesting that the joint training
of diffusive and non-diffusive modules can be performed
stably. In cases where instability is suspected during training
of SynDiff, stabilization of adversarial components can be
achieved via spectral normalization or feature matching .
SynDiff employs a non-diffusive module to provide sourceimage estimates paired with target images in the training
dataset. As such, training of the diffusive module naturally
depends on the quality of these source-image estimates. To
assess the reliance of the diffusive module on the nondiffusive module, we systematically undertrained the nondiffusive module to produce suboptimal source-image estimates. Note that although the diffusive module was trained
with low quality source-image estimates, it was still tested
with acquired source images during inference. This creates
discrepancy between the distribution of source-image inputs to
the diffusive module between the training and test sets. While
the diffusive module shows a degree of reliability against
moderate discrepancies, its performance degrades as expected
under signiﬁcant discrepancies towards more aggressive levels
of undertraining. Thus, a well-functioning non-diffusive module is key for training of the diffusive module.
Generative models for image translation draw samples from
the conditional distribution of the target given the source
modality. Depending on the use of random variables in this
process, generative models can produce either deterministic or
stochastic outputs. Among competing methods examined, all
GAN models except MUNIT receive only source images to
produce deterministic target images. Instead, MUNIT receives
random noise variables at intermediate stages. Similarly, all
diffusion models initiate sampling on random noise images.
While the inﬂuence of the initial sample diminishes across
diffusion steps, especially in the presence of a guiding source
image, target images are eventually sampled using the reverse
transition probabilities. Here, we observed that all examined
stochastic methods including SynDiff show limited variability across independent target samples synthesized from the
same source image, as reﬂected in performance metrics. Still,
future studies are warranted for an in-depth assessment of
the variability of translation estimates and their utility in
characterizing uncertainty in diffusion models.
GAN models leverage adversarial learning to achieve high
image quality, albeit they can suffer from limited training
stability and sample diversity . These limitations are particularly burdening in unconstrained image generation tasks,
where regular diffusion models have been reported to offer
signiﬁcant beneﬁts , . Note that training for such
unconditional tasks is typically performed on large training
sets with highly heterogeneous samples. In contrast, here we
consider constrained medical image translation tasks where the
model output is anatomically conditioned on a source image,
and the size and heterogeneity of the training sets analyzed
here are also limited compared to natural image datasets ,
 . As such, beneﬁts of diffusion models in terms of stability
and sample diversity might be less discernible. Furthermore,
medical images carry higher intrinsic noise than natural images. Regular diffusion models learn a denoising prior using
pixel-wise losses, which have lower sensitivity than adversarial
losses to ﬁne-grained features including noise . In turn,
DDPM might be prone to limited spatial acuity in synthesizing
medical images with signiﬁcant levels of intrinsic noise .
This might help explain the less competitive performance of
regular diffusion models against GANs in multi-contrast MRI
tasks where the target MRI images have higher noise compared
to target CT images in multi-modal tasks. Further work is
warranted to systematically explore the relative performance
of diffusion models against GANs as a function of the size,
heterogeneity, and noise levels of medical imaging datasets.
Starting at random noise, regular diffusion models gradually
generate images through repeated denoising over thousands
of time steps, resulting in poor sampling efﬁciency. Here, we
proposed to accelerate sampling by leveraging large step sizes
coupled with an adversarial projector to capture the complex
distribution of reserve transitions. A recent study on MRI
reconstruction has reported faster inference by initiating diffusion sampling with an intermediate image from a secondary
method . Improved efﬁciency in image generation has also
been reported for diffusion processes that run in a relatively
compact latent space . Combining our adversarial projector
with these alternative acceleration approaches may result in
further speed beneﬁts.
Several technical developments can be further pursued for
SynDiff. Here, we demonstrated one-to-one translation tasks
between two modalities. When a single source modality does
not contain sufﬁcient information to recover the target image,
many-to-one translation can be implemented to improve performance . To do this, SynDiff can receive multiple source
modalities as conditioning inputs , , . Second,
we considered synthesis tasks in which source and target
modalities were unpaired across subjects. When paired sourcetarget images are available, SynDiff can be adapted for supervised training by substituting a pixel-wise in place of cycleconsistency loss and providing actual source images as conditioning input , . Performance improvements might
also be viable by expanding the size of training datasets based
on a collection of undersampled source- and target-modality
acquisitions , or a combination of paired and unpaired
OZBEY AND DALMAZ et al.: UNSUPERVISED MEDICAL IMAGE TRANSLATION WITH ADVERSARIAL DIFFUSION MODELS
source-target modality data . Here the diffusive generators
were implemented based on convolutional backbones. Recent
studies have reported transformer architectures for improved
contextual sensitivity in medical imaging tasks , .
The importance of contextual representations for reverse diffusion remains to be demonstrated, yet an efﬁcient transformer
might help enhance the generalization performance to atypical
anatomy . Unlike regular diffusion models with slow
inference, SynDiff offers a more competitive inference time
with GAN models. Yet, its training time is notably higher than
GANs, and moderately longer than regular diffusion models
due to the computation of added adversarial components and
losses. When needed, training efﬁciency might be improved by
parallel execution on multiple GPUs . The fast conditional
diffusion process in SynDiff might also offer performance
beneﬁts over GANs in other applications such as denoising
and super-resolution , , .
A primary application of SynDiff is imputation of missing
scans in multi-contrast MRI and multi-modal imaging. In
clinical protocols, a subset of scans are typically omitted due
to time constraints, or due to motion artifacts in uncooperative
patients . To maintain the original protocol, omitted scans
can then be imputed from acquired scans. Here, high-quality
images were synthesized while mapping between native MRI
contrasts (e.g., T1, T2) and mapping MRI to CT. Yet, in other
cases, information required to synthesize the target image may
not be sufﬁciently encoded in the source image. For instance,
MRI contrasts enhanced with exogenous agents carry distinct
information from native contrasts, so it is relatively difﬁcult
to synthesize contrast-enhanced MRI images from native MRI
contrasts . While CT primarily yields strong contrast for
the dense outer bone layers based on X-ray attenuation, MRI
shows strong contrast among soft tissues and bone based on
tissue magnetization. As such, the primary information on soft
tissues needed to synthesize MRI images is scarcely present
in CT images, and mapping CT to MRI is a signiﬁcantly
ill-posed problem compared against mapping MRI to CT.
Here we observed notably poor performance in CT-to-MRI
mapping for all examined methods (not reported). It may be
possible to improve translation performance in ill-posed tasks
by incorporating multiple source modalities that capture more
diverse tissue information .
Another potential application for SynDiff is unsupervised
adaptation of learning-based models for downstream tasks
such as segmentation and classiﬁcation across separate domains (e.g., scanners, imaging sites, modalities). When the
amount of labeled data is limited in a primary domain, it
may be preferred to transfer a model adequately trained in
a secondary domain with a large labeled dataset , .
However, blind model transfer will incur substantial performance loss given inherent shifts in the data distribution across
domains. Assuming that a sufﬁciently large set of unlabeled
images are available in the primary domain, SynDiff can be
used to translate these images from primary to secondary domain . Performance of the transferred model can improve
when these translated images are given as input, since their
distribution is more closely aligned with secondary-domain
images. That said, similar to the case of scan imputation,
success in domain adaptation is bounded by the extent of
information shared between domains. Downstream models can
show suboptimal performance on translated images when information on the secondary domain is not sufﬁciently encoded
in the primary domain.
VII. CONCLUSION
In this study, we introduced a novel adversarial diffusion
model for medical image translation between source and target
modalities. SynDiff leverages a fast diffusion process to efﬁciently synthesize target images, and a conditional adversarial
projector for accurate reserve diffusion sampling. Unsupervised learning is achieved via a cycle-consistent architecture
that embodies coupled diffusion processes between the two
modalities. SynDiff achieves superior quality compared to
state-of-the-art GAN and diffusion models, and it holds great
promise for high-ﬁdelity medical image translation.