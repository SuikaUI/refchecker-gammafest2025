A Discriminative Framework for Anomaly
Detection in Large Videos
Allison Del Giorno, J. Andrew Bagnell, Martial Hebert
Carnegie Mellon University
Abstract. We address an anomaly detection setting in which training sequences are unavailable and anomalies are scored independently of
temporal ordering. Current algorithms in anomaly detection are based
on the classical density estimation approach of learning high-dimensional
models and ﬁnding low-probability events. These algorithms are sensitive to the order in which anomalies appear and require either training
data or early context assumptions that do not hold for longer, more complex videos. By deﬁning anomalies as examples that can be distinguished
from other examples in the same video, our deﬁnition inspires a shift
in approaches from classical density estimation to simple discriminative
learning. Our contributions include a novel framework for anomaly detection that is (1) independent of temporal ordering of anomalies, and
(2) unsupervised, requiring no separate training sequences. We show that
our algorithm can achieve state-of-the-art results even when we adjust
the setting by removing training sequences from standard datasets.
Keywords: anomaly detection, discriminative, unsupervised, context,
surveillance, temporal invariance
Introduction
Anomaly detection is an especially challenging problem because, while its applications are prevalent, it remains ill-deﬁned. Where there have been attempts
at deﬁnitions, they are often informal and vary across communities and applications. In this paper, we deﬁne and propose a solution for a largely neglected
subproblem within anomaly detection, where two constraints exist: (1) no additional training sequences are available; (2) the order in which anomalies occur
should not aﬀect the algorithm’s performance on each instance (Figure 1). This
is an especially challenging setting because we cannot build a model in advance
and ﬁnd deviations from it; much like clustering or outlier detection, the context
is deﬁned by the video itself. This setting is prominent in application ﬁelds such
as robotics, medicine, entertainment, and data mining. For instance:
– First-time data. A robotics team wants to create a robust set of algorithms.
They teleoperate a robot performing a new task or operating in a new environment. The team would like to ﬁnd out what special cases the robot
may have to handle on the perception side, so they ask for a list of the most
 
Allison Del Giorno, J. Andrew Bagnell, Martial Hebert
Fig. 1: Characteristics of our anomaly detection setting. Left: No training
sequences. This setting occurs when we want context to be drawn solely from the
test video (e.g. - the player’s shot distribution diﬀers for each opponent, and we want
anomalies relative to how they played that opponent), or unavailable (a player with
a new style debuts at a tournament). Right: Temporal independence.
want to ﬁnd the most anomalous frames regardless of the order they appear in.
anomalous instances according to the robot’s sensor data relative to that
day’s conditions and performance.
– Personalized results: context semantically deﬁned as coming only from the
test set. (a) A father wants to ﬁnd the most interesting parts of the 4-hour
home video of his family’s Christmas. (b) A healthcare professional wants to
review the most anomalous footage of an elderly patient while living under
at-home nursing care over the past week.
– Database sifting. A consulting analyst is told to ﬁnd abnormal behavior in
a large amount of video from a surveillance camera.
These illustrate just a few of the practical cases in which it is important to
identify all instances of anomalies, regardless of the order in which they appear,
and to do so within the context of the testing video. If videos are available
for providing context beyond the test video or it is acceptable to ignore later
anomalies as long as the ﬁrst instance is recognized, there are many mature
methods that apply (see 1.1). There are also natural extensions to our method
that could incorporate additional context if it were available. Here we consider
a challenging setting in which the context must be derived from the test video
and the order in which anomalies occur does not aﬀect their score.
In general anomaly detection settings, one cannot use traditional supervised
approaches because it is impossible to ﬁnd a suﬃciently representative set of
anomalies. In our setting, we are given no context ahead of time; unlike other
algorithms, we cannot even build a distribution for a representative set of familiar
events. We require the use of approaches that operate solely on the test sequence
and adapt to each video’s context. This leads us to denote frames as anomalous
if they are easily distinguished from other frames in the same video, and familiar
otherwise.
A Discriminative Framework for Anomaly Detection in Large Videos
Previous approaches
Anomaly detection presents a set of unique challenges beyond those seen in the
supervised learning paradigm. The inability to use training data for both classes
of data (familiar and anomalous) leads to two possible approaches: (1) Estimate
a model distribution of the familiar and then classify suﬃciently large deviations
as anomalous; or (2) Seek out points that are identiﬁable in the distribution of
all frames and label those anomalous. While these approaches seem similar on
the surface, they lead to distinct methodologies that diﬀer in the assumptions
and data required as well as the type of anomalies they identify. We show that
the latter will satisfy our setting while the former will not, and comes with a
few other advantages.
The traditional approach for anomaly detection involves learning a model of
familiarity for a given video. Subsequent time points can then be identiﬁed as
anomalous if they deviate from the model by some distance metric or inverse
likelihood. We call this set of approaches “scanning” techniques. Examples in
this area include sparse reconstruction , dynamic textures , and human
behavior models . The methods take a set of training data, either from separate
videos or from hand-chosen frames at the beginning of the video, and build a
model. Many of these methods update models online, and while some do not
need to update the model in temporal order , they still need a large amount
of training data for initialization. One method in particular achieves reasonable
performance with a small number of starting frames , but still requires manual
identiﬁcation of these frames for the algorithm. Generative models work well for
domains in which the assumed model of normalcy ﬁts the data well. This applies
when the model is complex enough to handle a variety of events, or when known
context can allow the learner to anticipate a model that will ﬁt the data. These
methods generally assume that the features come from a predetermined type of
distribution and therefore are likely to fail if the feature distribution changes. For
complex models, computational complexity and the amount of ‘normal’ training
data needed to initialize the model becomes a signiﬁcant bottleneck. Parameter
choices also have a larger eﬀect on the ability of the algorithm to ﬁt the data.
Scanning approaches do not satisfy our anomaly detection setting because
they violate the two conditions we speciﬁed: (1) they require training instances,
and (2) they depend on temporal ordering. Building a model in temporal order of the video makes strong assumptions about the way anomalies should
be detected. In our setting, we must ﬁnd the ‘most anomalous’ events in the
video, regardless of their order. By building models with updates in temporal
order, events that occur earlier in the video are more likely to be anomalous.
For instance, with an event type that occurs only twice in a large video, the
ﬁrst instance will be detected once but the second instance will be ignored. By
choosing a discriminative algorithm that acts independently of the ordering of
the video, we avoid these assumptions and pitfalls of the scanning techniques.
Our method shares the discriminative spirit of previous works using saliency
for anomaly detection . However, the saliency methods used require training
data to run and only use local context. Our objective is to obtain a fully unsu-
Allison Del Giorno, J. Andrew Bagnell, Martial Hebert
pervised method that uses the context of the entire video and is independent of
the ordering in which the anomalies occur. builds a graph and ﬁnds anomalies
independent of their ordering. However, it is model-based and only designed to
work with trajectories; our goal is discriminative and able to operate on any set
of features.
The primary challenge in our setting is our inability to assume the form
of the underlying distribution. A non-parametric method is preferable so that
it can generalize to many domains with few assumptions. Permutation tests are
nonparametric methods designed to handle such cases. The general idea is to test
the ﬁdelity of a given statistic against a set of other possible statistics from a
diﬀerently-labeled dataset. We use a similar approach to test the distinctiveness
of each frame. In our method, the analogous statistic is the ease with which a
given data point can be distinguished from other points sampled from the same
video. By testing a frame’s distinguishability from diﬀerent groups of frames, we
form a more accurate picture of its global anomaly score.
Our Approach
Our approach is to directly estimate the discriminability of frames with reference
to the context in the video. We do not need a model of every normal event to
generate scores for anomalous frames; we can simply attempt to discriminate
between anomalous frames and familiar frames to see if there is a diﬀerence
in the distributions. We present a framework that tests the discriminability of
frames. In this framework, we perform change detection on a sequence of data
from the video to see which frames are distinguishable from previous frames.
Because we want these comparisons to be independent of time, we create shuﬄes
of the data by permuting the frames before running each instance of the change
detection. Simple classiﬁers are used to avoid overﬁtting and so that each frame
will be compared against many permutations of other frames. This discriminative
framework allows us to perform in a unique setting.
Our contributions are as follows:
– A permutation-based framework for anomaly detection in a setting free from
training data and independent of ordering,
– A theory that guides the choice of key parameters and justiﬁes our framework,
– Experimental evaluation demonstrating that this technique achieves performance similar to other techniques that require training data
A Motivating Example
To motivate our method and demonstrate its advantages over scanning techniques, let’s walk through a toy example. Suppose we draw four images from
the MNIST dataset, each with a diﬀerent label (2, 3, 4, or 5). Then we create
a ‘video’ using noisy copies of these images. The order of these images is shown
in Figure 2. While the ﬁrst portion of the video contains only instances of ‘3’,
A Discriminative Framework for Anomaly Detection in Large Videos
Fig. 2: Detections from one-class SVM and our algorithm on a toy example.
The ground truth represents the digit classes from MNIST that were used to generate
each frame. The red dashes indicate locations of the anomalies. The red shaded region
represents detections made by each algorithm. Our algorithm without shuﬄing has the
same temporal disadvantages as online one-class SVM. By including shuﬄing, we do
not trigger false positives on prevalent examples when seen for the ﬁrst time. We also
detect the full extent of each anomaly and avoid assuming the beginning is familiar.
both the 3’s and 2’s are prevalent. In this case, we would hope that the algorithm classiﬁes all instances of 4’s and 5’s as anomalous and considers all 2’s
and 3’s familiar. We use one-class SVM with a RBF kernel as an instance of
scanning techniques. Figure 2 shows scores from a static one-class SVM trained
on the ﬁrst portion of the video, the same algorithm with an online update, and
our algorithm with and without shuﬄing. Our algorithm’s performance without shuﬄing is similar to that of the online one-class SVM. When the model
remains static after the ﬁrst third of the video, all of the 2’s are classiﬁed as
anomalous. Even with an online model update, the ﬁrst few 2’s are classiﬁed
as anomalous. In addition, not all of the 4’s and 5’s are given equal anomaly
weights within their respective classes. Our algorithm avoids these pitfalls once
shuﬄing is introduced, classifying only the 4’s and 5’s as anomalous. By using a
permutation-based framework, we are able to evade assumptions of familiarity
and remove the eﬀects of temporal ordering on anomaly scores.
The issues discussed extend beyond just this toy example. With scanning
methods, anomalies that appear more than once may be missed. In addition, it
is common to see failures due to the assumption that the beginning of the video
represents familiarity, both by anomalies appearing in the beginning and by other
familiar events appearing later in the video. For videos where context changes
frequently (imagine a concert light show whose theme changes every song), this
can create a dangerously high number of false positives. Also note that while this
example uses one-class SVM as an example, all scanning techniques have the
same inherent problems. Our method was developed in part to circumvent these
previously unavoidable failure cases. In addition, we hope to demonstrate that
simple discriminative techniques can match the performance of more complex
generative methods while operating in the new setting we have identiﬁed.
Allison Del Giorno, J. Andrew Bagnell, Martial Hebert
Taking the direct approach. Inspired by density ratio estimation for change
point detection , we take a more direct approach to anomaly detection
than the popular generative approach. The main objective of density ratio estimation is to avoid doing unnecessary work when deciding from which one of
two distributions a data point was generated: rather than model both distributions independently, we can directly compute the ratio of probabilities that a
data point is drawn from one or the other. This shortcut is especially helpful
in anomaly detection. We are more interested in the relative probability that
a given frame is anomalous rather than familiar, and are less interested in the
distribution of familiar events. The machine learning community has covered
several ways to estimate this ratio directly and has enumerated the several cost
functions and other paradigms in which this ratio appears . We note that
one such way to estimate these ratios directly is simple logistic regression, and
therefore we use this standard classiﬁer as a measure of the deviation between
two groups of points (see “Larger window sizes decrease the eﬀect of overﬁtting
classiﬁers” in Section 3 for formal justiﬁcation).
System overview. The full framework is depicted in Figure 3. Recall our
deﬁnition of anomalous frames: those that are easily distinguished from others in
the same video. Because this deﬁnition avoids domain-speciﬁc notions of anomalies, it relies on a robust set of features that can be used to distinguish anomalies
in a variety of domains. We assume an appropriate set of features has been computed and forms a descriptor for each frame. Because this is a discriminative
method, the choice of features has a smaller impact on the choice of algorithm
and parameters than it would for a generative method. The overall proposed
framework is agnostic to the feature choice; the user can plug in any relevant
or state-of-the-art features based on domain knowledge or novel feature methods. In addition, features can be aggregated within or across frames to obtain
diﬀerent levels of spatial or temporal resolution. Because the framework does
not make explicit assumptions about the distribution of the features, these are
simply design choices based on the cost of feature computation and the desired
resolution of detections.
No shuﬄes - change detection. If we were to remove permutation from
our algorithm, it would perform simple change detection by testing the distinguishability of a sliding window of tw frames, where all frames before it are
assumed to be familiar. A conceptual example is shown in Figure 4.
In the ﬁrst iteration, a classiﬁer f is learned on the set of 2tw points, where
the ﬁrst tw points are given the label 0 and the second tw points are given the
label 1. We call this set of labels a ‘split’ of the data. Each point x that is
labeled 1 is then given the score f(x), which is the probability it belongs to
class 1 instead of class 0 according to the classiﬁer f(x). In our implementation,
f(x) is simply
1+exp (−wT x) where w minimizes the l2-regularized logistic loss.
We say the second set of frames are within a ‘sliding time window’ or ‘sliding
window’, because in the next iteration these frames are reassigned a label of
A Discriminative Framework for Anomaly Detection in Large Videos
Fig. 3: The proposed anomaly detection framework. (A) Given an input video,
(B) a descriptor for each frame is passed into the anomaly detection algorithm, (C)
where the descriptors are shuﬄed K times. For each shuﬄe, the algorithm evaluates
anomaly scores for a sliding window of frames. This score is based on the density ratio
compared to frames that came before the sliding window. (D) Finally, the scores are
combined with averaging to produce the ﬁnal output signal. Image depicting dense
trajectory features is from Wang et al .
0 and the next tw points are labeled 11. The process repeats until the sliding
window reaches the end of the video. As the sliding window reaches the end,
events in the window are compared to all events in the past. The higher f(x) for
a given point x, the larger the classiﬁer’s conﬁdence that it can be distinguished
from previous points.
A sliding window is chosen rather than moving point-by-point for several
reasons. First of all, it provides inherent regularization for the classiﬁer, since
distinguishing any one point the rest can be misleadingly easy even if that point
is familiar. In addition, the number of splits the algorithm must compute is
inversely proportional to the window size tw. It may seem that ‘polluting’ the
sliding window with familiars would ruin an anomaly’s chance of being accurately
scored. However, this is not the case, as anomalies are more easily distinguished
from the rest of the video, and therefore the chance that they fall near the
resulting classiﬁer boundary is low, while the probability that a familiar event
does is high (see Figure 4 for intuition).
Adding in shuﬄes - full anomaly detection. As we pointed out earlier,
the disadvantage of this approach without shuﬄes is the same as with other
scanning techniques: temporal dependencies cause the algorithm to miss events
that occur more than once and raise false alarms to events that may be prevalent
later in the video but not in the beginning. We therefore shuﬄe the order of the
data and repeat the change detection process described to reduce the eﬀect of the
order. Producing a series of distinguishability scores from classiﬁers learned on
diﬀerent permutations of frames can be thought of as testing a set of hypotheses.
1 For simplicity, we describe the algorithm when the sliding window size and stride
equal each other, tw = ∆tw. It is just as valid to shorten the stride to increase
accuracy. See Algorithm 1.
Allison Del Giorno, J. Andrew Bagnell, Martial Hebert
Algorithm 1: Anomaly Detection Selects the most anomalous frames
1 Parameters: K (#permutations), tw (window size), ∆tw (window stride) ;
Input: {x1, .., xT } (descriptors for each frame)
Output: {a1, ..., aT } (estimates of anomalousness of each frame)
2 Generate a random set of K permutations {σ1([1, ..., T]), ..., σK([1, ..., T])}
3 for k = 1, ..., K do
for all sliding windows [tstart, tstart + tw) until T do
if t < tstart
if tstart ≤t < (tstart + tw)
w ←TrainLogisticRegression(xσk,yσk)
= 1|xt, w), ∀(y(k)
8 ¯pt ←mean(p(k)
) across all k
10 return {a1, ..., aT }
If a series of classiﬁers are all able to easily distinguish a frame labeled “1” from
many combinations of those labeled “0”, it is likely an anomaly.
Aggregating scores. Once the scores have been computed for each shuﬄe,
we average the results. The average outperforms other methods of aggregation
like the median and maximum. After aggregating over shuﬄes, log-odds are computed as the ﬁnal anomaly score. The full overview is explained in Algorithm 1.
Supporting Theory
Our method is based on distinguishing two labeled subsets of data. We consider
here how to think about the tradeoﬀs between increasing and decreasing the
window size and the number of shuﬄes given a choice of classiﬁer. We attempt
only to provide intuition by considering simple analyses that suggest how one
might understand the tradeoﬀs of parameters and classiﬁer choices.
Overall objective. In order for the algorithm to work, the classiﬁer needs to
have enough capacity to be able to tell anomalies and familiars apart, but simple
enough that it is unable to tell familiars apart from other familiars. This capacity
is a function of the classiﬁer complexity, the number of points being compared
(related to tw), and the subset of points being compared (the shuﬄing).
Classiﬁer assumptions. We must make basic assumptions about the classiﬁer. We assume that the classiﬁer f(x) is able to correctly distinguish between
an anomaly and familiars. In the case of logistic regression, for instance, we
assume that f will label an anomaly 1 with relatively high conﬁdence if it is
learned on a set of familiars labeled 0 and an anomaly within a set of familiars
labeled 1. This means f(x) = P(y = 1|x, w) will be signiﬁcantly higher than 0.5
if x is an anomaly and close to or below 0.5 if x is a familiar.
A Discriminative Framework for Anomaly Detection in Large Videos
Fig. 4: Visualizing two consecutive splits.
Points in red and blue have been labeled 1 and
0 respectively. Here the window size tw = 7 (#
red points). The values in red are the probability
values pt. Familiars are close to the boundary
(pt close to 0.5) and therefore yield a low log
odds score at.
Bound δ obtained, ǫp = 0.5
Fig. 5: Upper bound on the
probability that any one of
A anomalies appears ﬁrst in
fewer than 1/A −ϵ shuﬄes
(ϵ = 0.05). The resulting probability decreases exponentially
with the number of shuﬄes.
Increasing the number of shuﬄes, K, removes temporal dependencies. Here we show that the number of shuﬄes K needed to remove temporal
dependencies scales with the number of anomalies A at O(A log A).
Suppose a video has A anomalies and a signiﬁcantly larger number of total
frames, A ≪T. Consider the worst case scenario for our algorithm, where all A
anomalies are identical. Formally, each of these anomalies have identical feature
vectors: xn1 = xn2 = ... = xnA. This represents the worst case because if one
such anomaly xni is labeled 0 while another anomaly xnj is labeled 1, the score
of f(xnj) will be small. In other words, an anomaly in the negative window can
negate the score of another in the positive window. Therefore, the best split of
the data for a given anomaly instance is when it occurs ﬁrst in the video.2 The
goal of shuﬄing is to reorder the anomalies enough times that every anomaly
gets the opportunity to be the ﬁrst instance in the reordered set of frames.
More formally, let us deﬁne random variable F as the event that an anomaly x
appears ﬁrst in a given shuﬄe. F is a binary variable that occurs with probability
1/A, so in expectation, the fraction of shuﬄes in which it appears ﬁrst is also
µ = 1/A. However, we would like to use as few shuﬄes as possible to ensure that
the event F gets close to its mean 1/A. This means we need a bound on the
probability that ¯FK −µ is less than some ϵ for K shuﬄes. A relative tolerance
ϵ = (ϵp/A) is required because as the desired fraction 1/A gets smaller, the
eﬀect of deviations ϵ on the score of an anomaly grows proportionally larger. For
instance, ϵp = 0.25 requires that every anomaly is ﬁrst in at least 75% as many
2 ignoring the rare case when it occurs in the very ﬁrst negative window.
Allison Del Giorno, J. Andrew Bagnell, Martial Hebert
shuﬄes as it would be on average. To obtain this bound, we apply the Relative
Entropy ChernoﬀBound 3.
The Chernoﬀbound for Bernoulli random variables states that the average
¯FK of K variables with mean µ falls below µ−ϵ for some ϵ > 0 with probability:
  ¯FK ≤µ −ϵ
≤e−K KL(µ−ϵ,µ),
where KL(p1, p2) is the KL divergence between two Bernoulli distributions with
parameters p1 and p2. This is a well-known formula, and the KL divergence
KL(µ −ϵ, µ) with µ = 1/A, ϵ = ϵp/A is:
KL(µ −ϵ, µ) = 1
log(1 −ϵp)(1 −ϵp) + log
(A −1 + ϵp)
With Equations 1 and 2, we have bounded the probability that a single anomaly
will not appear ﬁrst in a large enough fraction of shuﬄes. To extend this to all
A anomalies, we apply the union bound to all events ¯F (i)
K to get our ﬁnal bound:
≤Ae−K KL(µ−ϵ,µ)
The bounding probability δ is deﬁned in terms of A, K, and ϵp. Values of
this bound for diﬀerent values of K, δ, and A are depicted in Figure 5. We are
interested in choosing the number of shuﬄes K for a given δ, A, and ϵp, so we
solve Equation 3 for K:
KL(µ −ϵ, µ)
In big O terms, for ﬁxed δ and ϵp, we need O(A log A) shuﬄes to reorder the
anomalies enough times to equally score them.
Larger window sizes decrease the eﬀect of overﬁtting classiﬁers.
Given a choice K, there is one more parameter in the framework that requires
care: the window size tw. Increasing the window stride decreases the computational load on the system (more splits per shuﬄe). In terms of performance, it
may also seem best to make tw as small as possible because anomalies are easier
to distinguish in a smaller window size. In addition, with large window sizes,
anomalies of diﬀerent types can fall within the same window and ‘interfere’ with
each others’ scores. However, decreasing the window size beyond a certain point
also reduces performance, as the classiﬁer overﬁts and familiars become distinguishable from other familiars. In other words, we must choose tw to be able to
trade oﬀour ability to distinguish anomalies without being able to distinguish
familiars. We consider here a theoretical sketch explicating the relation between
the complexity of the classiﬁer and the choice of window size tw.
3 Kakade provides a quick summary for those unfamiliar with these bounds :
 
lecture06.pdf
A Discriminative Framework for Anomaly Detection in Large Videos
Assume we have computed a complexity metric for our chosen classiﬁer
f. In this instance, we will work with the Rademacher complexity4 R(m), where
m is the size of the subset of points being classiﬁed . A higher Rademacher
complexity indicates the classiﬁer is able to more easily distinguish randomly
labeled data. For instance, a highly regularized linear classiﬁer has a much lower
Rademacher complexity than a RBF-kernel SVM or complex neural network.
This metric is especially convenient because it is measured relative to the data
distribution (so it adapts to the video) and can be empirically estimated by
simply computing a statistic over randomly labeled subsampled data 5.
Given that we have a classiﬁer of complexity R, we can adjust the window
size to decrease the probability that familiars can be distinguished from each
other. Generalization bounds provide a way to relate the error from overﬁtting
or noisy labels to the classiﬁer complexity and dataset size. In our case, the
true error, is the error from classifying anomalous or familiar points incorrectly
according to their true labels. The training error is the error when we trained
the classiﬁer on our synthetic labels.
While a careful analysis requires understanding errors in the ﬁxed design
 setting, the traditional i.i.d. random design provides crude guidance on algorithm behavior and trade-oﬀs. In this setting, the Rademacher complexity
provides us a generalization bound . For i.i.d. samples, the diﬀerence between the estimated and true error in classifying a set of m datapoints will be
err ≤R(m) + O
with probability 1 −δ. This gives a good intuition for how the overﬁtting of our classiﬁer relates to its complexity and the
number of points m that we train on. This bound follows our intuition: (1) as m
decreases, the chance of classifying incorrectly increases; (2) as R(m) increases,
the classiﬁer complexity increases and we have a better chance of incorrectly
distinguishing familiars from other familiars.
Due to this intuition, we choose a simple classiﬁer, l2-regularized logistic
regression. Window size tw is most easily chosen through empirical testing; if
the variance in the anomaly signal is large, familiars are too easy to tell apart
and the window size tw should be decreased. When no anomalies are visible, tw
should be increased.
Experiments
Dataset. We tested our algorithm on the Avenue Dataset 6 as well as the
Subway surveillance dataset , the Personal Vacation Dataset , and the
UMN Unusual Activity Dataset .
The Avenue dataset contains 16 training videos and 21 testing videos, and
locations of anomalies are marked in ground truth pixel-level masks for each
frame in the testing videos. The videos include a total of 15324 frames for testing.
4 The insights that follow generalize to VC dimension and other complexity measures.
5 For a useful introduction, see .
6 
Allison Del Giorno, J. Andrew Bagnell, Martial Hebert
Our algorithm was permitted to use only the testing videos, and performed
anomaly detection with no assumptions on the normality of any section of this
video. This is in stark contrast to other methods, which must train a model
on a set of frames from the training videos and/or from pre-marked sections of
video. There are several other datasets available for anomaly detection, and our
algorithm demonstrated reasonable success on all of the ones we tested7. We
focus on the Avenue Dataset speciﬁcally because it was more challenging than
staged datasets (such as the UMN Unusual Activity Dataset) and is more recent
with more speciﬁc labeling than others, such as the Personal Vacation dataset.
The dataset is also valuable because the method in has publicly available
code and results, so we were able to compare with the same implementation
and features as a recent standard in anomaly detection. The UCSD pedestrian
anomaly detection dataset is another well-labeled and recent dataset, but
nearly half of the frames in each test video contain anomalies, so the provided
anomaly labels are not applicable in our unsupervised setting. More precisely, in
our setting, no frames would be deﬁned as anomalous since the activities labeled
as such in the dataset often compose half of the video.8
Implementation. For the Avenue dataset, we follow the same feature generation procedure as Lu , courtesy of code they provided upon request. The features computed on the video match their method exactly, resulting in gradientbased features for 10x10x5 (rows x columns x frames) spatiotemporal subunits
in the video. After PCA and normalization, each subunit is represented by a
100 dimensional vector. Using the code provided by the authors, we are able to
run their algorithm alongside ours on the same set of features. Following their
evaluation, we treat each subunit as a ‘frame’ in our framework, classifying each
subunit independently. The results are smoothed with the same ﬁlter as .
We used liblinear’s l2-logistic regression for the classiﬁer f in the framework.
We experimented with several values of λ across other videos and found that
as long as the features are whitened, λ within an order of magnitude of 1 gives
reasonable results (see Table 1. We only needed 10 shuﬄes to get adequate
performance, likely because there were few anomalies per video. We display
our results for a window size of 10. The algorithm is trivially parallelizable
across shuﬄes and splits; we provide a multithreaded version that runs splits
simultaneously across all allocated CPUs. This ability to increase accuracy by
operating more units in parallel is a signiﬁcant beneﬁt of this framework. In
contrast, scanning techniques that update models in an online fashion must
operate serially; their computation cost is dependent on the desired accuracy.
An implementation of our method is available online.9
7 See supplementary material for more results, including results on individual videos
in the UMN and Avenue datasets.
8 Other videos are unusable for a similar reason; for instance, 2 of the 8 videos available
from contain more than 50% frames with at least one anomaly, and only 2 contain
fewer than 20% anomalous frames (one of which is the Subway exit sequence).
9 
A Discriminative Framework for Anomaly Detection in Large Videos
Evaluation metric. We are interested in identifying every instance of an
anomaly. We also care about proposing frames for a human to review, meaning
we would like to evaluate the ﬁdelity of the anomalousness with a human rating of
anomalousness. Consequently, we avoid metrics that score anomaly detections in
an event-detection style, where ﬂagging a single frame is counted as a successful
detection of adjacent frames. In addition, metrics like Equal Error Rate (EER)
can be misleading in the anomaly detection setting.10 Therefore, we using ROC
curves and the corresponding area under the curve (AUC) as the evaluation
metric, computed with reference to human-labeled frame and pixel ground truth.
Results. Figure 6 shows example detections and the resulting ROC curves
and AUC values for our algorithm and on the Avenue dataset. Note our
algorithm operates in a separate setting – it does not (a) use any sequence other
than an individual test video, (b) obtain a guiding form for models of familiarity,
or (c) assume any partition of the video is familiar. Even with these additional
challenges, we are able to obtain near-state-of-the-art performance.
The ROC curve sheds some light on the possible performance bottleneck
for these algorithms: the last half of the curves for the two algorithms match
closely. This highlights that for diﬃcult anomaly instances, a similar number of
false positives seem to be commonly detected by both algorithms. We believe
that both are hitting a limitation with the encoding of events in the feature
space: either the feature space is not descriptive enough or several other instances
appear as anomalous as the true anomalies in feature space. Example detections
and per-video analysis (see Supplementary) also shed light on our method’s
behavior. Since our algorithm is operating only on the test sequence, it exhibits
false positives such as the only time someone enters the foreground from the
right. This is penalized because the provided ground truth was marked relative
to the training data. In addition, by using features that operate on 15-frame
chunks of time, we often detect events as early as 15 frames too soon.
In Table 1 we show that our algorithm’s robust performance across a range
of parameters on the Avenue dataset. These results show that for sub-optimal
parameter choices (common in the unsupervised learning setting), shuﬄes can
improve performance. Imagine an under-regularized classiﬁer (λ too small). The
classiﬁer will too easily distinguish normal points from subsets of the data, but
this eﬀect is reduced as the number of shuﬄes increases. A similar argument
follows for other sub-optimal parameters. The major beneﬁts of shuﬄing cannot
be seen in the commonly used datasets for anomaly detection because the test
sequences are too short to show context changes or multiple instances of the
same anomaly that are more commonly found in real-world scenarios.
In addition, we report our AUC values on the Subway dataset (exit: 0.8236;
entrance: 0.6913). Results for one of the videos in the Personal Vacation Dataset
are shown in Figure 7. Detailed UMN Dataset results can be found in the sup-
10 Consider the case when only 1% of the video is anomalous: the EER on an algorithm
that markes all frames normal would be 1%, outperforming most modern algorithms.
This extreme class imbalance is less prevalent in current standard datasets, but will
become an apparent problem as more realistic datasets become prevalent.
Allison Del Giorno, J. Andrew Bagnell, Martial Hebert
Pixel-based ROC
Lu, AUC=0.929,
trained on 16 videos
Ours, AUC=0.910,
no training
(a) Pixel-based ROC on Avenue dataset
Frame-based ROC
Lu, AUC=0.809,
trained on 16 videos
Ours, AUC=0.783,
no training
(b) Frame-based ROC on
Avenue dataset
Pixel-based
Fig. 6: Performance on the Avenue Dataset. ROC curves in (a) and (b) show our
performance nearly matches this algorithm, while we require no training data. Detection examples shown in (c) show a correctly classiﬁed familiar frame, two detected
anomalous frames (running and dropping papers), and a false alarm (camera shake).
plementary material. Our method outperforms on all but one scene. While
the AUC values are good (average=0.91), the average AUC for both our method
and the sparse method is still lower than the sparse and model-based approaches
reported in ; this indicates that a change in features could make up for the
diﬀerence in the performance gap.
Table 1: Frame-based AUC averaged
across all 21 test videos of the Avenue
dataset. All instances of K = 10 outperform their K = 0 counterparts.
λ = 0.01 0.8697 0.8149 0.7731
0.8950 0.8951 0.8085
λ = 1 0.8921 0.8736 0.7731
0.8957 0.8954 0.8085
λ = 100 0.8947 0.8922 0.7751
0.8958 0.8953 0.8088
Fig. 7: Results on video 00016 in the Personal Vacation Dataset. As the number of
shuﬄes increases, the signal-to-noise ratio
increases.
Discussion
We have developed a method for identifying anomalies in videos in a setting
that is independent of the order in which anomalies appear and requires no
separate training sequences. The permutation-testing methodology requires no
assumptions about the content in the descriptors of each frame and the user
is able to “plug in” the latest optimal features for their video as long as the
anomalous frames are distinguishable in this space. No training data needs to be
collected or labeled within the test video. We show that our anomaly detection
algorithm is able to perform as well as state of the art on standard datasets, even
when we adjust the setting by removing training data. The lack of assumptions
on the content or location of familiars is valuable for ﬁnding true anomalies that
had previously remained unseen.
Acknowledgements. This research was supported through the DoD National Defense Science & Engineering Graduate Fellowship (NDSEG) Program
and NSF grant IIS1227495.
A Discriminative Framework for Anomaly Detection in Large Videos