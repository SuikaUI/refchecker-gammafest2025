Published as a conference paper at ICLR 2019
BENCHMARKING NEURAL NETWORK ROBUSTNESS
TO COMMON CORRUPTIONS AND PERTURBATIONS
Dan Hendrycks
University of California, Berkeley
 
Thomas Dietterich
Oregon State University
 
In this paper we establish rigorous benchmarks for image classiﬁer robustness.
Our ﬁrst benchmark, IMAGENET-C, standardizes and expands the corruption
robustness topic, while showing which classiﬁers are preferable in safety-critical
applications. Then we propose a new dataset called IMAGENET-P which enables
researchers to benchmark a classiﬁer’s robustness to common perturbations. Unlike
recent robustness research, this benchmark evaluates performance on common
corruptions and perturbations not worst-case adversarial perturbations. We ﬁnd
that there are negligible changes in relative corruption robustness from AlexNet
classiﬁers to ResNet classiﬁers. Afterward we discover ways to enhance corruption
and perturbation robustness. We even ﬁnd that a bypassed adversarial defense
provides substantial common perturbation robustness. Together our benchmarks
may aid future work toward networks that robustly generalize.
INTRODUCTION
The human vision system is robust in ways that existing computer vision systems are not . Unlike current deep learning classiﬁers , the human vision system is not fooled by small changes in query
images. Humans are also not confused by many forms of corruption such as snow, blur, pixelation,
and novel combinations of these. Humans can even deal with abstract changes in structure and style.
Achieving these kinds of robustness is an important goal for computer vision and machine learning. It
is also essential for creating deep learning systems that can be deployed in safety-critical applications.
Most work on robustness in deep learning methods for vision has focused on the important challenges
of robustness to adversarial examples , unknown
unknowns , and model or
data poisoning . In contrast, we develop and validate
datasets for two other forms of robustness. Speciﬁcally, we introduce the IMAGETNET-C dataset for
input corruption robustness and the IMAGENET-P dataset for input perturbation robustness.
To create IMAGENET-C, we introduce a set of 75 common visual corruptions and apply them to the
ImageNet object recognition challenge . We hope that this will serve as a general
dataset for benchmarking robustness to image corruptions and prevent methodological problems such
as moving goal posts and result cherry picking. We evaluate the performance of current deep learning
systems and show that there is wide room for improvement on IMAGENET-C. We also introduce a
total of three methods and architectures that improve corruption robustness without losing accuracy.
To create IMAGENET-P, we introduce a set of perturbed or subtly differing ImageNet images. Using
metrics we propose, we measure the stability of the network’s predictions on these perturbed images.
Although these perturbations are not chosen by an adversary, currently existing networks exhibit
surprising instability on common perturbations. Then we then demonstrate that approaches which
enhance corruption robustness can also improve perturbation robustness. For example, some recent
architectures can greatly improve both types of robustness. More, we show that the Adversarial
Logit Pairing ℓ∞adversarial example defense can yield substantial robustness gains on diverse and
common perturbations. By deﬁning and benchmarking perturbation and corruption robustness, we
facilitate research that can be overcome by future networks which do not rely on spurious correlations
or cues inessential to the object’s class.
 
Published as a conference paper at ICLR 2019
RELATED WORK
Adversarial Examples.
An adversarial image is a clean image perturbed by a small distortion
carefully crafted to confuse a classiﬁer. These deceptive distortions can occasionally fool black-box
classiﬁers . Algorithms have been developed that search for the smallest
additive distortions in RGB space that are sufﬁcient to confuse a classiﬁer . Thus
adversarial distortions serve as type of worst-case analysis for network robustness. Its popularity
has often led “adversarial robustness” to become interchangeable with “robustness” in the literature
 . In the literature, new defenses often quickly succumb to new
attacks , with some exceptions for perturbations
on small images . For some simple datasets, the existence
of any classiﬁcation error ensures the existence of adversarial perturbations of size O(d−1/2), d the
input dimensionality . For some simple models, adversarial robustness requires
an increase in the training set size that is polynomial in d . Gilmer et al. 
suggest modifying the problem of adversarial robustness itself for increased real-world applicability.
Robustness in Speech.
Speech recognition research emphasizes robustness to common corruptions
rather than worst-case, adversarial corruptions . Common acoustic
corruptions (e.g., street noise, background chatter, wind) receive greater focus than adversarial audio,
because common corruptions are ever-present and unsolved. There are several popular datasets
containing noisy test audio . Robustness in noisy environments
requires robust architectures, and some research ﬁnds convolutional networks more robust than fully
connected networks . Additional robustness has been achieved through
pre-processing techniques such as standardizing the statistics of the input .
ConvNet Fragility Studies.
Several studies demonstrate the fragility of convolutional networks
on simple corruptions. For example, Hosseini et al. apply impulse noise to break Google’s
Cloud Vision API. Using Gaussian noise and blur, Dodge & Karam demonstrate the superior
robustness of human vision to convolutional networks, even after networks are ﬁne-tuned on Gaussian
noise or blur. Geirhos et al. compare networks to humans on noisy and elastically deformed
images. They ﬁnd that ﬁne-tuning on speciﬁc corruptions does not generalize and that classiﬁcation
error patterns underlying network and human predictions are not similar. Temel et al. ;
Temel & AlRegib propose different corrupted datasets for object and trafﬁc sign recognition.
Robustness Enhancements.
In an effort to reduce classiﬁer fragility, Vasiljevic et al. ﬁnetune on blurred images. They ﬁnd it is not enough to ﬁne-tune on one type of blur to generalize to
other blurs. Furthermore, ﬁne-tuning on several blurs can marginally decrease performance. Zheng
et al. also ﬁnd that ﬁne-tuning on noisy images can cause underﬁtting, so they encourage the
noisy image softmax distribution to match the clean image softmax. Dodge & Karam address
underﬁtting via a mixture of corruption-speciﬁc experts assuming corruptions are known beforehand.
CORRUPTIONS, PERTURBATIONS, AND ADVERSARIAL PERTURBATIONS
We now deﬁne corruption and perturbation robustness and distinguish them from adversarial
perturbation robustness. To begin, we consider a classiﬁer f : X →Y trained on samples from
distribution D, a set of corruption functions C, and a set of perturbation functions E. We let
PC(c), PE(ε) approximate the real-world frequency of these corruptions and perturbations. Most
classiﬁers are judged by their accuracy on test queries drawn from D, i.e., P(x,y)∼D(f(x) = y).
Yet in a vast range of cases the classiﬁer is tasked with classifying low-quality or corrupted
In view of this, we suggest also computing the classiﬁer’s corruption robustness
Ec∼C[P(x,y)∼D(f(c(x) = y))]. This contrasts with a popular notion of adversarial robustness,
often formulated min∥δ∥p<b P(x,y)∼D(f(x + δ) = y), b a small budget. Thus, corruption robustness
measures the classiﬁer’s average-case performance on corruptions C, while adversarial robustness
measures the worst-case performance on small, additive, classiﬁer-tailored perturbations.
Average-case performance on small, general, classiﬁer-agnostic perturbations motivates us to deﬁne
perturbation robustness, namely Eε∼E[P(x,y)∼D(f(ε(x)) = f(x))]. Consequently, in measuring
perturbation robustness, we track the classiﬁer’s prediction stability, reliability, or consistency in the
Published as a conference paper at ICLR 2019
Gaussian Noise
Shot Noise
Impulse Noise
Defocus Blur Frosted Glass Blur
Motion Blur
Brightness
Figure 1: Our IMAGENET-C dataset consists of 15 types of algorithmically generated corruptions
from noise, blur, weather, and digital categories. Each type of corruption has ﬁve levels of severity,
resulting in 75 distinct corruptions. See different severity levels in Appendix B.
face of minor input changes. Now in order to approximate C, E and these robustness measures, we
designed a set of corruptions and perturbations which are frequently encountered in natural images.
We will refer to these as “common” corruptions and perturbations. These common corruptions and
perturbations are available in the form of IMAGENET-C and IMAGENET-P.
THE IMAGENET-C AND IMAGENET-P ROBUSTNESS BENCHMARKS
THE DATA OF IMAGENET-C AND IMAGENET-P
IMAGENET-C Design.
The IMAGENET-C benchmark consists of 15 diverse corruption types
applied to validation images of ImageNet. The corruptions are drawn from four main categories—
noise, blur, weather, and digital—as shown in Figure 1. Research that improves performance on this
benchmark should indicate general robustness gains, as the corruptions are diverse and numerous.
Each corruption type has ﬁve levels of severity since corruptions can manifest themselves at varying
intensities. Appendix A gives an example of the ﬁve different severity levels for impulse noise.
Real-world corruptions also have variation even at a ﬁxed intensity. To simulate these, we introduce
variation for each corruption when possible. For example, each fog cloud is unique to each image.
These algorithmically generated corruptions are applied to the ImageNet validation
images to produce our corruption robustness dataset IMAGENET-C. The dataset can be downloaded
or re-created by visiting IMAGENET-C
images are saved as lightly compressed JPEGs; this implies an image corrupted by Gaussian noise is
also slightly corrupted by JPEG compression. Our benchmark tests networks with IMAGENET-C
images, but networks should not be trained on these images. Networks should be trained on datasets
such as ImageNet and not be trained on IMAGENET-C corruptions. To enable further experimentation,
we designed an extra corruption type for each corruption category (Appendix B), and we provide
CIFAR-10-C, TINY IMAGENET-C, IMAGENET 64 × 64-C, and Inception-sized editions. Overall,
the IMAGENET-C dataset consists of 75 corruptions, all applied to ImageNet validation images for
testing a pre-existing network.
Published as a conference paper at ICLR 2019
Common Corruptions.
The ﬁrst corruption type is Gaussian noise. This corruption can appear
in low-lighting conditions. Shot noise, also called Poisson noise, is electronic noise caused by the
discrete nature of light itself. Impulse noise is a color analogue of salt-and-pepper noise and can be
caused by bit errors. Defocus blur occurs when an image is out of focus. Frosted Glass Blur appears
with “frosted glass” windows or panels. Motion blur appears when a camera is moving quickly. Zoom
blur occurs when a camera moves toward an object rapidly. Snow is a visually obstructive form of
precipitation. Frost forms when lenses or windows are coated with ice crystals. Fog shrouds objects
and is rendered with the diamond-square algorithm. Brightness varies with daylight intensity. Contrast
can be high or low depending on lighting conditions and the photographed object’s color. Elastic
transformations stretch or contract small image regions. Pixelation occurs when upsampling a lowresolution image. JPEG is a lossy image compression format which introduces compression artifacts.
Brightness
Figure 2: Example frames from the
beginning (T = 0) to end (T = 30)
of some Tilt and Brightness perturbation sequences.
IMAGENET-P Design.
The second benchmark that we
propose tests the classiﬁer’s perturbation robustness. Models
lacking in perturbation robustness produce erratic predictions
which undermines user trust. When perturbations have a high
propensity to change the model’s response, then perturbations
could also misdirect or destabilize iterative image optimization
procedures appearing in style transfer , decision explanations , feature visualization
 , and so on. Like IMAGENET-C, IMAGENET-
P consists of noise, blur, weather, and digital distortions. Also
as before, the dataset has validation perturbations; has difﬁculty
levels; has CIFAR-10, Tiny ImageNet, ImageNet 64 × 64,
standard, and Inception-sized editions; and has been designed
for benchmarking not training networks.
IMAGENET-P
departs from IMAGENET-C by having perturbation sequences
generated from each ImageNet validation image; examples are
in Figure 2. Each sequence contains more than 30 frames, so
we counteract an increase in dataset size and evaluation time
by using only 10 common perturbations.
Common Perturbations.
Appearing more subtly than
the corruption from IMAGENET-C, the Gaussian noise
perturbation sequence begins with the clean ImageNet image.
The following frames in the sequence consist in the same
image but with minute Gaussian noise perturbations applied.
This sequence design is similar for the shot noise perturbation
sequence. However the remaining perturbation sequences have
temporality, so that each frame of the sequence is a perturbation
of the previous frame. Since each perturbation is small, repeated application of a perturbation does
not bring the image far out-of-distribution. For example, an IMAGENET-P translation perturbation
sequence shows a clean ImageNet image sliding from right to left one pixel at a time; with each
perturbation of the pixel locations, the resulting frame is still of high quality. The perturbation
sequences with temporality are created with motion blur, zoom blur, snow, brightness, translate,
rotate, tilt (viewpoint variation through minor 3D rotations), and scale perturbations.
IMAGENET-C AND IMAGENET-P METRICS AND SETUP
IMAGENET-C Metrics.
Common corruptions such as Gaussian noise can be benign or destructive
depending on their severity. In order to comprehensively evaluate a classiﬁer’s robustness to a given
type of corruption, we score the classiﬁer’s performance across ﬁve corruption severity levels and
aggregate these scores. The ﬁrst evaluation step is to take a trained classiﬁer f, which has not been
trained on IMAGENET-C, and compute the clean dataset top-1 error rate. Denote this error rate
clean. The second step is to test the classiﬁer on each corruption type c at each level of severity s
(1 ≤s ≤5). This top-1 error is written Ef
s,c. Before we aggregate the classiﬁer’s performance across
severities and corruption types, we will make error rates more comparable since different corruptions
pose different levels of difﬁculty. For example, fog corruptions often obscure an object’s class more
than brightness corruptions. We adjust for the varying difﬁculties by dividing by AlexNet’s errors,
Published as a conference paper at ICLR 2019
but any baseline will do (even a baseline with 100% error rates, corresponding to an average of CEs).
This standardized aggregate performance measure is the Corruption Error, computed with the formula
Now we can summarize model corruption robustness by averaging the 15 Corruption Error values
Gaussian Noise, CEf
Shot Noise, . . . , CEf
JPEG. This results in the mean CE or mCE for short.
We now introduce a more nuanced corruption robustness measure. Consider a classiﬁer that withstands
most corruptions, so that the gap between the mCE and the clean data error is minuscule. Contrast
this with a classiﬁer with a low clean error rate which has its error rate spike in the presence of
corruptions; this corresponds to a large gap between the mCE and clean data error. It is possible that
the former classiﬁer has a larger mCE than the latter, despite the former degrading more gracefully in
the presence of corruptions. The amount that the classiﬁer declines on corrupted inputs is given by
the formula Relative CEf
s=1 EAlexNet
. Averaging these
15 Relative Corruption Errors results in the Relative mCE. This measures the relative robustness or
the performance degradation when encountering corruptions.
IMAGENET-P Metrics.
A straightforward approach to estimate Eε∼E[P(x,y)∼D(f(ε(x)) ̸= f(x))]
falls into place when using IMAGENET-P perturbation sequences. Let us denote m perturbation
sequences with S =
2 , . . . , x(i)
i=1 where each sequence is made with perturbation p.
The “Flip Probability” of network f : X →{1, 2, . . . , 1000} on perturbation sequences S is
= Px∼S(f(xj) ̸= f(xj−1)).
For noise perturbation sequences, which are not temporally related, x(i)
is clean and x(i)
(j > 1) are perturbed images of x(i)
1 . We can recast the FP formula for noise sequences as
= Px∼S(f(xj) ̸= f(x1) | j > 1). As was
done with the Corruption Error formula, we now standardize the Flip Probability by the sequence’s
difﬁculty for increased commensurability. We have, then, the “Flip Rate” FRf
p/FPAlexNet
Averaging the Flip Rate across all perturbations yields the mean Flip Rate or mFR. We do not deﬁne
a “relative mFR” since we did not ﬁnd any natural formulation, nor do we directly use predicted
class probabilities due to differences in model calibration .
When the top-5 predictions are relevant, perturbations should not cause the list of top-5 predictions
to shufﬂe chaotically, nor should classes sporadically vanish from the list. We penalize top-5
inconsistency of this kind with a different measure. Let the ranked predictions of network f on
x be the permutation τ(x) ∈S1000. Concretely, if “Toucan” has the label 97 in the output space
and “Pelican” has the label 145, and if f on x predicts “Toucan” and “Pelican” to be the most and
second-most likely classes, respectively, then τ(x)(97) = 1 and τ(x)(144) = 2. These permutations
contain the top-5 predictions, so we use permutations to compare top-5 lists. To do this, we deﬁne
d(τ(x), τ(x′)) =
max{i,σ(i)}
j=min{i,σ(i)}+1
1(1 ≤j −1 ≤5)
where σ = (τ(x))−1τ(x′). If the top-5 predictions represented within τ(x) and τ(x′) are identical,
then d(τ(x), τ(x′)) = 0. More examples of d on several permutations are in Appendix C. Comparing
the top-5 predictions across entire perturbation sequences results in the unstandardized Top-5
Distance uT5Df
j=2 d(τ(xj), τ(xj−1)) = Px∼S(d(τ(xj), τ(xj−1)). For noise
perturbation sequences, we have uT5Df
p = Ex∼S[d(τ(xj), τ(x1)) | j > 1]. Once the uT5D is
standardized, we have the Top-5 Distance T5Df
p/uT5DAlexNet
. The T5Ds averaged
together correspond to the mean Top-5 Distance or mT5D.
Preserving Metric Validity.
The goal of IMAGENET-C and IMAGENET-P is to evaluate the robustness of machine learning algorithms on novel corruptions and perturbations. Humans are able to
generalize to novel corruptions quite well; for example, they can easily deal with new Instagram ﬁlters.
Likewise for perturbations; humans relaxing in front of an undulating ocean do not give turbulent ac-
Published as a conference paper at ICLR 2019
Architecture Accuracy (%)
SqueezeNet 1.1
Architecture Corruption Robustness
Relative mCE
Figure 3: Robustness (mCE) and Relative mCE
IMAGENET-C values. Relative mCE values suggest robustness in itself declined from AlexNet to
ResNet. “BN” abbreviates Batch Normalization.
Architecture Accuracy (%)
SqueezeNet 1.1
Architecture Perturbation Robustness
Figure 4: Perturbation robustness of various
architectures as measured by the mT5D on
IMAGENET-P. Observe that corruption and perturbation robustness track distinct concepts.
Error mCE Gauss. Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG
43.5 100.0 100 100 100
100 100 100 100 100
100 100 100
SqueezeNet
41.8 104.4 107 106 105
100 101 103 97
106 109 134
VGG-19+BN 25.8
Table 1: Clean Error, mCE, and Corruption Error values of different corruptions and architectures
on IMAGENET-C. The mCE value is the mean Corruption Error of the corruptions in Noise, Blur,
Weather, and Digital columns. Models are trained only on clean ImageNet images.
counts of the scenery before them. Hence, we propose the following protocol. The image recognition
network should be trained on the ImageNet training set and on whatever other training sets the investigator wishes to include. Researchers should clearly state whether they trained on these corruptions or
perturbations; however, this training strategy is discouraged (see Section 2). We allow training with
other distortions (e.g., uniform noise) and standard data augmentation (i.e., cropping, mirroring), even
though cropping overlaps with translations. Then the resulting trained model should be evaluated on
IMAGENET-C or IMAGENET-P using the above metrics. Optionally, researchers can test with the separate set of validation corruptions and perturbations we provide for IMAGENET-C and IMAGENET-P.
EXPERIMENTS
ARCHITECTURE ROBUSTNESS
How robust are current methods, and has progress in computer vision been achieved at the expense of
robustness? As seen in Figure 3, as architectures improve, so too does the mean Corruption Error
(mCE). By this measure, architectures have become progressively more successful at generalizing to
corrupted distributions. Note that models with similar clean error rates have fairly similar CEs, and in
Table 1 there are no large shifts in a corruption type’s CE. Consequently, it would seem that architectures have slowly and consistently improved their representations over time. However, it appears that
corruption robustness improvements are mostly explained by accuracy improvements. Recall that the
Relative mCE tracks a classiﬁer’s accuracy decline in the presence of corruptions. Figure 3 shows
that the Relative mCEs of many subsequent models are worse than that of AlexNet . Full results are in Appendix D. In consequence, from AlexNet to ResNet, corruption
robustness in itself has barely changed. Thus our “superhuman” classiﬁers are decidedly subhuman.
On perturbed inputs, current classiﬁers are unexpectedly bad. For example, a ResNet-18 on Scale
perturbation sequences have a 15.6% probability of ﬂipping its top-1 prediction between adjacent
Published as a conference paper at ICLR 2019
frames (i.e., FPResNet-18
= 15.6%); the uT5DResNet-18
is 3.6. More results are in Appendix E. Clearly
perturbations need not be adversarial to fool current classiﬁers. What is also surprising is that while
VGGNets are worse than ResNets at generalizing to corrupted examples, on perturbed examples they
can be just as robust or even more robust. Likewise, Batch Normalization made VGG-19 less robust
to perturbations but more robust to corruptions. Yet this is not to suggest that there is a fundamental
trade-off between corruption and perturbation robustness. In fact, both corruption and perturbation
robustness can improve together, as we shall see later.
ROBUSTNESS ENHANCEMENTS
Be aware that Appendix F contains many informative failures in robustness enhancement. Those
experiments underscore the necessity in testing on a a diverse test set, the difﬁculty in cleansing
corruptions from image, and the futility in expecting robustness gains from some “simpler” models.
Histogram Equalization.
Histogram equalization successfully standardizes speech data for robust
speech recognition . For images, we ﬁnd that preprocessing
with Contrast Limited Adaptive Histogram Equalization is quite effective. Unlike
our image denoising attempt (Appendix F), CLAHE reduces the effect of some corruptions while not
worsening performance on most others, thereby improving the mCE. We demonstrate CLAHE’s net
improvement by taking a pre-trained ResNet-50 and ﬁne-tuning the whole model for ﬁve epochs on
images processed with CLAHE. The ResNet-50 has a 23.87% error rate, but ResNet-50 with CLAHE
has an error rate of 23.55%. On nearly all corruptions, CLAHE slightly decreases the Corruption
Error. The ResNet-50 without CLAHE preprocessing has an mCE of 76.7%, while with CLAHE the
ResNet-50’s mCE decreases to 74.5%.
Multiscale Networks.
Multiscale architectures achieve greater corruption robustness by propagating features across scales at each layer rather than slowly gaining a global representation of
the input as in typical convolutional neural networks. Some multiscale architectures are called
Multigrid Networks . Multigrid networks each have a pyramid of grids in each layer
which enables the subsequent layer to operate across scales. Along similar lines, Multi-Scale Dense
Networks (MSDNets) use information across scales. MSDNets bind network
layers with DenseNet-like skip connections. These two different multiscale
networks both enhance corruption robustness, but they do not provide any noticeable beneﬁt in
perturbation robustness. Now before comparing mCE values, we ﬁrst note the Multigrid network
has a 24.6% top-1 error rate, as does the MSDNet, while the ResNet-50 has a 23.9% top-1 error
rate. On noisy inputs, Multigrid networks noticeably surpass ResNets and MSDNets, as shown in
Figure 5. Since multiscale architectures have high-level representations processed in tandem with
ﬁne details, the architectures appear better equipped to suppress otherwise distracting pixel noise.
When all corruptions are evaluated, ResNet-50 has an mCE of 76.7%, the MSDNet has an mCE of
73.6%, and the Multigrid network has an mCE of 73.3%.
Feature Aggregating and Larger Networks.
Some recent models enhance the ResNet architecture by increasing what is called feature aggregation. Of these, DenseNets and ResNeXts are most prominent. Each purports to have stronger representations than ResNets, and the
evidence is largely a hard-won ImageNet error-rate downtick. Interestingly, the IMAGENET-C mCE
clearly indicates that DenseNets and ResNeXts have superior representations. Accordingly, a switch
from a ResNet-50 (23.9% top-1 error) to a DenseNet-121 (25.6% error) decreases the mCE from
76.7% to 73.4% (and the relative mCE from 105.0% to 92.8%). More starkly, switching from a
ResNet-50 to a ResNeXt-50 (22.9% top-1) drops the mCE from 76.7% to 68.2% (relative mCE
decreases from 105.0% to 88.6%). Corruption robustness results are summarized in Figure 5. This
shows that corruption robustness may be a better way to measure future progress in representation
learning than the clean dataset top-1 error rate.
Some of the greatest and simplest robustness gains sometimes emerge from making recent models
more monolithic. Apparently more representations, more redundancy, and more capacity allow these
massive models to operate more stably on corrupted inputs. We saw earlier that making models smaller
does the opposite. Swapping a DenseNet-121 (25.6% top-1) with the larger DenseNet-161 (22.9% top-
1) decreases the mCE from 73.4% to 66.4% (and the relative mCE from 92.8% to 84.6%). In a similar
fashion, a ResNeXt-50 (22.9% top-1) is less robust than the a giant ResNeXt-101 (21.0% top-1). The
mCEs are 68.2% and 62.2% respectively (and the relative mCEs are 88.6% and 80.1% respectively).
Published as a conference paper at ICLR 2019
Noise Corruption Robustness
Figure 5: Architectures such as Multigrid networks and DenseNets resist noise corruptions
more effectively than ResNets.
Size and Corruption Robustness
DenseNet-121
DenseNet-161
ResNeXt-50
ResNeXt-101
Figure 6: Larger feature aggregating networks
achieve robustness gains that substantially outpace their accuracy gains.
Both model size and feature aggregation results are summarized in Figure 6. Consequently, future
models with even more depth, width, and feature aggregation may attain further corruption robustness.
Feature aggregation and their larger counterparts similarly improve perturbation robustness. While a
ResNet-50 has a 58.0% mFR and a 78.3% mT5D, a DenseNet-121 obtains a 56.4% mFR and 76.8%
mT5D, and a ResNeXt-50 does even better with a 52.4% mFR and a 74.2% mT5D. Reﬂecting the
corruption robustness ﬁndings further, the larger DenseNet-161 has a 46.9% mFR and 69.5% mT5D,
while the ResNeXt-101 has a 43.2% mFR and 65.9% mT5D. Thus in two senses feature aggregating
networks and their larger versions markedly enhance robustness.
Stylized ImageNet.
Geirhos et al. propose a novel data augmentation scheme where ImageNet images are stylized with style transfer. The intent is that classiﬁers trained on stylized images
will rely less on textural cues for classiﬁcation. When a ResNet-50 is trained on typical ImageNet
images and stylized ImageNet images, the resulting model has an mCE of 69.3%, down from 76.7%.
Adversarial Logit Pairing.
ALP is an adversarial example defense for large-scale image classiﬁers
 . Like nearly all other adversarial defenses, ALP was bypassed and has unclear
value as an adversarial defense going forward , yet this is not a decisive reason
dismiss it. ALP provides signiﬁcant perturbation robustness even though it does not provide much
adversarial perturbation robustness against all adversaries. Although ALP was designed to increase
robustness to small gradient perturbations, it markedly improves robustness to all sorts of noise, blur,
weather, and digital IMAGENET-P perturbations—methods generalizing this well is a rarity. In point
of fact, a publicly available Tiny ImageNet ResNet-50 model ﬁne-tuned with ALP has a 41% and
40% relative decrease in the mFP and mT5D on TINY IMAGENET-P, respectively. ALP’s success
in enhancing common perturbation robustness and its modest utility for adversarial perturbation
robustness highlights that the interplay between these problems should be better understood.
CONCLUSION
In this paper, we introduced what are to our knowledge the ﬁrst comprehensive benchmarks for
corruption and perturbation robustness. This was made possible by introducing two new datasets,
IMAGENET-C and IMAGENET-P. The ﬁrst of which showed that many years of architectural advancements corresponded to minuscule changes in relative corruption robustness. Therefore benchmarking
and improving robustness deserves attention, especially as top-1 clean ImageNet accuracy nears its
ceiling. We also saw that classiﬁers exhibit unexpected instability on simple perturbations. Thereafter
we found that methods such as histogram equalization, multiscale architectures, and larger featureaggregating models improve corruption robustness. These larger models also improve perturbation
robustness. However, we found that even greater perturbation robustness can come from an adversarial
defense designed for adversarial ℓ∞perturbations, indicating a surprising interaction between adversarial and common perturbation robustness. In this work, we found several methods to increase robustness, introduced novel experiments and metrics, and created new datasets for the rigorous study of
model robustness, a pressing necessity as models are unleashed into safety-critical real-world settings.
Published as a conference paper at ICLR 2019
ACKNOWLEDGEMENTS
We should like to thank Justin Gilmer, David Wagner, Kevin Gimpel, Tom Brown, Mantas Mazeika,
and Steven Basart for their helpful suggestions. This research was supported by a grant from the
Future of Life Institute.