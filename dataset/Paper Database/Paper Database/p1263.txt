Contrastive Adaptation Network for Unsupervised Domain Adaptation
Guoliang Kang1, Lu Jiang2, Yi Yang1,3∗, Alexander G. Hauptmann4
1CAI, University of Technology Sydney, 2Google AI, 3Baidu Research, 4Carnegie Mellon University
 ,
 ,
 ,
 
Unsupervised Domain Adaptation (UDA) makes predictions for the target domain data while manual annotations
are only available in the source domain. Previous methods
minimize the domain discrepancy neglecting the class information, which may lead to misalignment and poor generalization performance. To address this issue, this paper proposes Contrastive Adaptation Network (CAN) optimizing a
new metric which explicitly models the intra-class domain
discrepancy and the inter-class domain discrepancy. We design an alternating update strategy for training CAN in an
end-to-end manner. Experiments on two real-world benchmarks Ofﬁce-31 and VisDA-2017 demonstrate that CAN
performs favorably against the state-of-the-art methods and
produces more discriminative features.
1. Introduction
Recent advancements in deep neural networks have successfully improved a variety of learning problems . For supervised learning, however, massive labeled training data is still the key to learning an accurate
deep model. Although abundant labels may be available for
a few pre-speciﬁed domains, such as ImageNet , manual
labels often turn out to be difﬁcult or expensive to obtain
for every ad-hoc target domain or task. The absence of indomain labeled data hinders the application of data-ﬁtting
models in many real-world problems.
In the absence of labeled data from the target domain,
Unsupervised Domain Adaptation (UDA) methods have
emerged to mitigate the domain shift in data distributions
 . It relates to unsupervised learning as it requires manual labels only from the source domain
and zero labels from the target domain. Among the recent
work on UDA, a seminal line of work proposed by Long
et al. aims at minimizing the discrepancy between
the source and target domain in the deep neural network,
where the domain discrepancy is measured by Maximum
∗Corresponding author. Part of this work was done when Yi Yang was
visiting Baidu Research during his Professional Experience Program.
Previous Methods
Proposed Method
Approaching:
Splitting:
Before Adaptation
Comparison between previous domain-discrepancy
minimization methods and ours. Left: The domain shift exists between the source and target data before adaptation. Middle: Classagnostic adaptation aligns source and target data at the domainlevel, neglecting the class label of the sample, and hence may lead
to sub-optimal solutions. Consequently, the target samples of one
label may be misaligned with source samples of a different label. Right: Our method performs class-aware alignment across
domains. To avoid the misalignment, only the intra-class domain
discrepancy is minimized. The inter-class domain discrepancy is
maximized to enhance the model’s generalization ability.
Mean Discrepancy (MMD) and Joint MMD (JMMD)
 . MMD and JMMD have proven effective in many computer vision problems and demonstrated the state-of-the-art
results on several UDA benchmarks .
Despite the success of previous methods based on MMD
and JMMD, most of them measure the domain discrepancy at the domain level, neglecting the class from which
the samples are drawn. These class-agnostic approaches,
hence, do not discriminate whether samples from two domains should be aligned according to their class labels
(Fig. 1). This can impair the adaptation performance due
to the following reasons. First, samples of different classes
may be aligned incorrectly, e.g. both MMD and JMMD
can be minimized even when the target-domain samples are
misaligned with the source-domain samples of a different
class. Second, the learned decision boundary may generalize poorly for the target domain. There exist many suboptimal solutions near the decision boundary. These solutions may overﬁt the source data well but are less discriminative for the target.
 
To address the above issues, we introduce a new Contrastive Domain Discrepancy (CDD) objective to enable
class-aware UDA. We propose to minimize the intra-class
discrepancy, i.e. the domain discrepancy within the same
class, and maximize the inter-class margin, i.e. the domain
discrepancy between different classes. Considering the toy
example in Fig. 1, CDD will draw closer the source and target samples of the same underlying class (e.g. the blue and
red triangles), while pushing apart the samples from different classes (e.g. the blue triangle and the red star).
Unfortunately, to estimate and optimize with CDD, we
may not train a deep network out-of-the-box as we need
to overcome the following two technical issues. First, we
need labels from both domains to compute CDD, however,
target labels are unknown in UDA. A straightforward way,
of course, is to estimate the target labels by the network outputs during training. However, because the estimation can
be noisy, we ﬁnd it can harm the adaptation performance
(see Section 4.3). Second, during the mini-batch training,
for a class C, the mini-batch may only contain samples from
one domain (source or target), rendering it infeasible to estimate the intra-class domain discrepancy of C. This can
result in a less efﬁcient adaptation. The above issues require
special design of the network and the training paradigm.
In this paper, we propose Contrastive Adaptation Network (CAN) to facilitate the optimization with CDD. During training, in addition to minimizing the cross-entropy
loss on labeled source data, CAN alternatively estimates
the underlying label hypothesis of target samples through
clustering, and adapts the feature representations according
to the CDD metric. After clustering, the ambiguous target
data (i.e. far from the cluster centers) and ambiguous classes
(i.e. containing few target samples around the cluster centers) are zeroed out in estimating the CDD. Empirically we
ﬁnd that during training, an increasing amount of samples
will be taken into account. Such progressive learning can
help CAN capture more accurate statistics of data distributions. Moreover, to facilitate the mini-batch training of
CAN, we employ the class-aware sampling for both source
and target domains, i.e. at each iteration, we sample data
from both domains for each class within a randomly sampled class subset. Class-aware sampling can improve the
training efﬁciency and the adaptation performance.
We validate our method on two public UDA benchmarks: Ofﬁce-31 and VisDA-2017 .
The experimental results show that our method performs favorably against the state-of-the-art UDA approaches, i.e. we
achieve the best-published result on the Ofﬁce-31 benchmark and very competitive result on the challenging VisDA-
2017 benchmark. Ablation studies are presented to verify
the contribution of each key component in our framework.
In a nutshell, our contributions are as follows,
• We introduce a new discrepancy metric Contrastive
Domain Discrepancy (CDD) to perform class-aware
alignment for unsupervised domain adaptation.
• We propose a network Contrastive Adaptation Network to facilitate the end-to-end training with CDD.
• Our method achieves the best-published result on
the Ofﬁce-31 benchmark and competitive performance compared to the state-of-the-art on the challenging VisDA-2017 benchmark .
2. Related Work
Class-agnostic domain alignment. A common practice for
UDA is to minimize the discrepancy between domains to
obtain domain-invariant features .
For example, Tzeng et al. proposed a kind of domain confusion loss to encourage the network to learn both
semantically meaningful and domain invariant representations. Long et al. proposed DAN and JAN to minimize the MMD and Joint MMD distance across domains respectively, over the domain-speciﬁc layers. Ganin et al. 
enabled the network to learn domain invariant representations in adversarial way by back-propagating the reverse
gradients of the domain classiﬁer. Unlike these domaindiscrepancy minimization methods, our method performs
class-aware domain alignment.
Discriminative domain-invariant feature learning. Some
previous works pay efforts to learn more disciminative features while performing domain alignment . Adversarial Dropout Regularization (ADR) 
and Maximum Classiﬁer Discrepancy (MCD) were
proposed to train a deep neural network in adversarial way
to avoid generating non-discriminative features lying in the
region near the decision boundary. Similar to us, Long et
al. and Pei et al. take the class information into
account while measuring the domain discrepancy. However, our method differs from theirs mainly in two aspects.
Firstly, we explicitly model two types of domain discrepancy, i.e. the intra-class domain discrepancy and the interclass domain discrepancy. The inter-class domain discrepancy, which has been ignored by most previous methods,
is proved to be beneﬁcial for enhancing the model adaptation performance. Secondly, in the context of deep neural
networks, we treat the training process as an alternative optimization over target label hypothesis and features.
Intra-class compactness and inter-class separability
modeling. This paper is also related to the work that explicitly models the intra-class compactness and the inter-class
separability, e.g. the contrastive loss and the triplet loss
 . These methods have been used in various applications,
e.g. face recognition , person re-identiﬁcation , etc.
Different from these methods designed for a single domain,
our work focuses on adaptation across domains.
3. Methodology
Unsupervised Domain Adaptation (UDA) aims at improving the model’s generalization performance on target
domain by mitigating the domain shift in data distribution
of the source and target domain. Formally, given a set of
source domain samples S = {(xs
1), · · · , (xs
and target domain samples T = {xt
1, · · · , xt
Nt}, xs, xt
represent the input data, and ys ∈{0, 1, · · · , M −1} denote the source data label of M classes. The target data
label yt ∈{0, 1, · · · , M −1} is unknown. Thus, in UDA,
we are interested in training a network using labeled source
domain data S and unlabeled target domain data T to make
accurate predictions {ˆyt} on T .
We discuss our method in the context of deep neural networks. In deep neural networks, a sample owns hierarchical
features/representations denoted by the activations of each
layer l ∈L. In the following, we use φl(x) to denote the
outputs of layer l in a deep neural network Φθ for the input x, where φ(·) denotes the mapping deﬁned by the deep
neural network from the input to a speciﬁc layer.
In the rest of this section, we start our discussions by
brieﬂy reviewing the relevant concepts in MMD in Section
3.1. Section 3.2 introduces a new domain discrepancy metric. Finally, Section 3.3 and Section 3.4 discuss the objective and the training procedure of proposed deep network.
3.1. Maximum Mean Discrepancy Revisit
In Maximum Mean Discrepancy (MMD), {xs
i} are i.i.d.
sampled from the marginal distributions
P(Xs) and Q(Xt) respectively. Based on the observed
samples, MMD performs a kernel two-sample test to
determine whether to accept the null hypothesis P = Q or
not. MMD is motivated by the fact that if two distributions
are identical, all of their statistics should be the same. Formally, MMD deﬁnes the difference between two distributions with their mean embeddings in the reproducing kernel
Hilbert space (RKHS), i.e.
DH(P, Q) ≜sup
(EXs[f(Xs)] −EXt[f(Xt)])H,
where H is class of functions.
In practice, for a layer l, the squared value of MMD is
estimated with the empirical kernel mean embeddings
where xs ∈S′ ⊂S, xt ∈T ′ ⊂T , ns = |S′|, nt = |T ′|.
The S′ and T ′ represent the mini-batch source and target
data sampled from S and T respectively. And kl denotes
the kernel selected for the l-th layer of deep neural network.
3.2. Contrastive Domain Discrepancy
We propose to explicitly take the class information into
account and measure the intra-class and inter-class discrepancy across domains. The intra-class domain discrepancy
is minimized to compact the feature representations of samples within a class, whereas the inter-class domain discrepancy is maximized to push the representations of each other
further away from the decision boundary. The intra-class
and inter-class discrepancies are jointly optimized to improve the adaptation performance.
The proposed Contrastive Domain Discrepancy (CDD)
is established on the difference between conditional data
distributions across domains.
Without any constraint on
the type (e.g. marginal or conditional) of data distributions, MMD is convenient to measure such difference between P(φ(Xs)|Y s) and Q(φ(Xt)|Y t), i.e. DH(P, Q) ≜
supf∼H (EXs[f(φ(Xs)|Y s)] −EXt[f(φ(Xt)|Y t)])H.
Supposing µcc′(y, y′) =
if y = c, y′ = c′;
otherwise.
two classes c1, c2 (which can be same or different), the kernel mean embedding estimation for squared DH(P, Q) is
ˆDc1c2(ˆyt
2, · · · , ˆyt
nt, φ) = e1 + e2 −2e3
j=1 µc1c1(ys
j=1 µc2c2(ˆyt
j=1 µc1c2(ys
Note that Eq. (3) deﬁnes two kinds of class-aware domain
discrepancy, 1) when c1 = c2 = c, it measures intraclass domain discrepancy; 2) when c1 ̸= c2, it becomes
the inter-class domain discrepancy. To compute the mask
j) and µc1c2(ys
j), we need to estimate target
labels {ˆyt
i}, which will be discussed in Section 3.4.
Based on the above deﬁnitions, the CDD is calculated as
2, · · · , ˆyt
nt is abbreviated as ˆyt
where the intra- and inter-class domain discrepancies will
be optimized in the opposite direction.
Note although the estimation of the labels {ˆyt
noisy, the CDD (which is established on MMD) in itself
is robust the the noise to an extent. Because MMD is determined by the mean embeddings of distributions in the
RKHS, the sufﬁcient statistics is less likely to be severely
affected by the label noise, especially when the amount of
data is large. We will discuss and verify this in Section 4.3.
3.3. Contrastive Adaptation Network
Deep convolutional neural networks (CNNs) is able
to learn more transferable features than shallow methods.
However, the discrepancy still exists for domain-speciﬁc
Speciﬁcally, the convolutional layers extracting
general features are more transferable, while the fullyconnected (FC) layers which exhibit abstract and domainspeciﬁc features should be adapted .
In this paper, we start from ImageNet pretrained
networks, e.g. ResNet , and replace the last FC
layer with task-speciﬁc ones. We follow the general practice that minimizes the domain discrepancy of last FC layers and ﬁne-tunes the convolutional layers through backpropagation. Then our proposed CDD can be readily incorporated into the objective as an adaptation module over the
activations of FC layers. We name our network Contrastive
Adaptation Network (CAN).
The overall objective. In a deep CNN, we need to minimize CDD over multiple FC layers, i.e. minimizing
Besides, we train the network with labeled source data
through minimizing the cross-entropy loss,
where ys ∈{0, 1, · · · , M −1} is the ground-truth label
of sample xs. Pθ(y|x) denotes the predicted probability of
label y with the network parameterized by θ, given input x.
Therefore, the overall objective can be formulated as
ℓ= ℓce + β ˆDcdd
where β is the weight of the discrepancy penalty term.
Through minimizing ˆDcdd
L , the intra-class domain discrepancy is minimized and the inter-class domain discrepancy
is maximized to perform class-aware domain alignment.
Note that we independently sample the labeled source
data to minimize the cross-entropy loss ℓce and those to estimate the CDD ˆDcdd
L . In this way, we are able to design more
efﬁcient sampling strategy (see Section 3.4) to facilitate
the mini-batch stochastic optimization with CDD, while not
disturbing the conventional optimization with cross-entropy
loss on labeled source data.
3.4. Optimizing CAN
The framework of CAN is illustrated in Fig. 2. In this
section, we mainly focus on discussing how to minimize
CDD loss in CAN.
Alternative optimization (AO). As shown in Eq. (5),
we need to jointly optimize the target label hypothesis ˆyt
and the feature representations φ1:L. We adopt alternative
steps to perform such optimization. In detail, at each loop,
given current feature representations, i.e. ﬁxing θ, we update target labels through clustering. Then, based on the
updated target labels ˆyt, we estimate and minimize CDD to
adapt the features, i.e. update θ through back-propagation.
We employ the input activations φ1(·) of the ﬁrst taskspeciﬁc layer to represent a sample.
For example, in
ResNet, each sample can be represented as the outputs of
the global average pooling layer, which are also the inputs
of the following task-speciﬁc layer. Then the spherical Kmeans is adopted to perform the clustering of target samples and attach corresponding labels. The number of clusters is the same as the number of underlying classes M.
For each class, the target cluster center Otc is initialized
as the source cluster center Osc, i.e. Otc ←Osc, where
c = {0, 1, · · · , M −1}. For the metric measuring the distance between points a and b in the feature space, we apply
the cosine dissimilarity, i.e. dist(a, b) = 1
clustering
iteratively
argminc dist(φ1(xt
and 2) updating the cluster centers: Otc ←PNt
i)∥, till convergence
or reaching the maximum clustering steps.
After clustering, each target sample xt
i is assigned
a label ˆyt
i same as its afﬁliated clusters.
ambiguous data, which is far from its afﬁliated cluster center, is discarded, i.e. we select a subset ˜T
{(xt, ˆyt)|dist(φ1(xt), Ot(ˆyt)) < D0, xt ∈T }, where
D0 ∈ is a constant.
Moreover, to give a more accurate estimation of the distribution statistics, we assume that the minimum number
of samples in ˜T assigned to each class, should be guaran-
CDD (intra)
Feature Extractor
(conv. layers of ResNet, VGG, etc.)
Deprecated
Clustering
K-step update
Feature Adaptation
Source/Target Centers
Source/Target Data Container
Outer Circle: cluster boundary
Inner Circle: confidence range
Representation:
CDD (intra)
CDD (inter)
CDD (inter)
The training process of CAN. To minimize CDD, we perform alternative optimization between updating the target label
hypothesis through clustering and adapting feature representations through back-propagation. For the clustering, we apply spherical Kmeans clustering of target samples based on their current feature representations. The number of clusters equal to that of underlying classes
and the initial center of each class cluster is set to the center of source data within the same class. Then ambiguous data (i.e. far from the
afﬁliated cluster centers) and ambiguous classes (i.e. containing few target samples around afﬁliated cluster centers) are discarded. For
the feature adaptation, the labeled target samples provided by the clustering stage , together with the labeled source samples, pass through
the network to achieve their multi-layer feature representations. The features of domain-speciﬁc FC layers are adopted to estimate CDD
(Eq. (5)). Besides, we apply cross-entropy loss on independently sampled source data. Back-propagating with minimizing CDD and
cross-entropy loss (Eq. (8)) adapts the features and provides class-aware alignment. Detailed descriptions can be found in Section 3.4.
teed. The class which doesn’t satisfy such condition will
not be considered in current loop, i.e. at loop Te, the selected subset of classes CTe = {c| P| ˜T |
i=c > N0, c ∈
{0, 1, · · · , M −1}}, where N0 is a constant.
At the start of training, due to the domain shift, it is more
likely to exclude partial classes. However, as training proceeds, more and more classes are included. The reason is
two folds: 1) as training proceeds, the model becomes more
accurate and 2) beneﬁting from the CDD penalty, the intraclass domain discrepancy becomes smaller, and the interclass domain discrepancy becomes larger, so that the hard
(i.e. ambiguous) classes are able to be taken into account.
Class-aware Sampling (CAS). In the conventional
training of deep neural networks, a mini-batch of data is
usually sampled at each iteration without being differentiated by their classes. However, it will be less efﬁcient for
computing the CDD. For example, for class C, there may
only exist samples from one domain (source or target) in
the mini-batch, thus the intra-class discrepancy could not
be estimated.
We propose to use class-aware sampling strategy to enable the efﬁcient update of network with CDD. It is easy
to implement. We randomly select a subset of classes C
from CTe, and then sample source data and target data for
each class in C
Te. Consequently, in each mini-batch of data
during training, we are able to estimate the intra-class discrepancy for each selected class.
Algorithm. Algorithm 1 shows one loop of the AO procedure, i.e. alternating between a clustering phase (Step 1-
4), and a K-step network update phase (Step 5-11). The
loop of AO is repeated multiple times in our experiments.
Because the feature adapting process is relatively slower,
we asynchronously update the target labels and the network
parameters to make the training more stable and efﬁcient.
Algorithm 1: Optimization of CAN at loop Te.
source data: S = {(xs
1), · · · , (xs
target data: T = {xt
1, · · · , xt
Procedure:
1 Forward S and compute the M cluster centers Osc ;
2 Initialize Otc: Otc ←Osc ;
3 Cluster target samples T using spherical K-means;
4 Filter the ambiguous target samples and classes;
5 for (k ←1; k ≤K; k ←k + 1) do
Class-aware sampling based on C
Te, ˜T , and S;
Compute ˆDcdd
using Eq. (6);
Sample from S and compute ℓce using Eq. (7);
Back-propagate with the objective ℓ(Eq.(8));
Update network parameters θ.
VisDA-2017
Figure 3. The gallery of Ofﬁce-31 and VisDA-2017 datasets.
4. Experiments
4.1. Setups
Datasets: We validate our method on two public benchmarks. Ofﬁce-31 is a common dataset for real-world
domain adaptation tasks. It consists of 4,110 images belonging to 31 classes. This dataset contains three distinct
domains, i.e., images which are collected from the 1) Amazon website (Amazon domain), 2) web camera (Webcam
domain), and 3) digital SLR camera (DSLR domain) under
different settings, respectively. The dataset is imbalanced
across domains, with 2,817 images in A domain, 795 images in W domain, and 498 images in D domain.
VisDA-2017 is a challenging testbed for UDA with the
domain shift from synthetic data to real imagery. In this
paper, we validate our method on its classiﬁcation task. In
total there are ∼280k images from 12 categories. The images are split into three sets, i.e. a training set with 152,397
synthetic images, a validation set with 55,388 real-world
images, and a test set with 72,372 real-world images. The
gallery of two datasets is shown in Fig. 3
Baselines: We compare our method with class-agnostic discrepancy minimization methods: RevGrad , DAN
 , and JAN . Moreover, we compare our method
with the ones which explicitly or implicitly take the class information or decision boundary into consideration to learn
more discriminative features:
MADA , MCD ,
and ADR .The descriptions of these methods can be
found in Section 2.
We implement DAN and JAN using the released code 1.
For a comparison under optimal parameter setting, we cite the performance of MADA,
RevGrad, MCD and ADR reported in their corresponding
papers .
Implementation details: We use ResNet-50 and ResNet-
101 pretrained on ImageNet as our backbone
We replace the last FC layer with the taskspeciﬁc FC layer, and ﬁnetune the model with labeled
source domain data and unlabeled target domain data. All
1 
the network parameters are shared between the source domain and target domain data other than those of the batch
normalization layers which are domain-speciﬁc. The hyperparameters are selected following the same protocol as described in , i.e. we train a domain classiﬁer and perform
selection on a validation set (of labeled source samples and
unlabeled target samples) by jointly evaluating the test errors of the source classiﬁer and the domain classiﬁer.
We use mini-batch stochastic gradient descent (SGD)
with momentum of 0.9 to train the network. We follow the
same learning rate schedule as described in , i.e.
the learning rate ηp is adjusted following ηp =
where p linearly increases from 0 to 1. The η0 is the initial
learning rate, i.e. 0.001 for the convolutional layers and 0.01
for the task-speciﬁc FC layer. For Ofﬁce-31, a = 10 and
b = 0.75, while for VisDA-2017, a = 10 and b = 2.25. The
β selected is 0.3. The thresholds (D0, N0) are set to (0.05,
3) for Ofﬁce-31 tasks A→W and A→D. And we don’t ﬁlter
target samples and classes for other tasks during training.
4.2. Comparison with the state-of-the-art
Table 1 shows the classiﬁcation accuracy on six tasks
of Ofﬁce-31. All domain adaptation methods yield notable
improvement over the ResNet model (ﬁrst row) which is
ﬁne-tuned on labeled source data only. CAN outperforms
other baseline methods across all tasks, achieving the stateof-the-art performance. On average, it boosts the accuracy
of JAN by a absolute 6.3% and that of MADA by 5.4%.
We visualize the distribution of learned features by t-
SNE . Fig. 4 illustrates a representative task W →A.
Compared to JAN, as expected, the target data representations learned by CAN demonstrate higher intra-class compactness and much larger inter-class margin. This suggests
that our CDD produces more discriminative features for the
target domain and substantiates our improvement in Table 1.
Table 2 lists the accuracy over 12 classes on VisDA-2017
with the validation set as the target domain. Our method
outperforms the other baseline methods. The mean accuracy of our model (87.2%) outperforms the self-ensembling
(SE) method (84.3%) which wins the ﬁrst place in the
VisDA-2017 competition, by 2.9%. It is worth noting that
SE mainly deals with UDA by ensemble and data augmentation, which is orthogonal to the topic of this paper and thus
can be easily combined to boost the performance further.
Moreover, we also perform adaptation on the VisDA-
2017 test set (as the target domain), and submit our predictions to ofﬁcial evaluation server. Our goal is to evaluate the effectiveness of our proposed technique based on a
vanilla backbone (ResNet-101). We choose not to use ensemble or additional data augmentation which is commonly
used to boost the performance in the competition. Anyhow,
our single model achieves a very competitive accuracy of
87.4%, which is comparable to the method which ranks at
Source-ﬁnetune
68.4 ± 0.2
96.7 ± 0.1
99.3 ± 0.1
68.9 ± 0.2
62.5 ± 0.3
60.7 ± 0.3
RevGrad 
82.0 ± 0.4
96.9 ± 0.2
99.1 ± 0.1
79.7 ± 0.4
68.2 ± 0.4
67.4 ± 0.5
80.5 ± 0.4
97.1 ± 0.2
99.6 ± 0.1
78.6 ± 0.2
63.6 ± 0.3
62.8 ± 0.2
85.4 ± 0.3
97.4 ± 0.2
99.8 ± 0.2
84.7 ± 0.3
68.6 ± 0.3
70.0 ± 0.4
90.0 ± 0.2
97.4 ± 0.1
99.6 ± 0.1
87.8 ± 0.2
70.3 ± 0.3
66.4 ± 0.3
Ours (intra only)
93.2 ± 0.2
98.4 ± 0.2
99.8 ± 0.2
92.9 ± 0.2
76.5 ± 0.3
76.0 ± 0.3
Ours (CAN)
94.5 ± 0.3
99.1 ± 0.2
99.8 ± 0.2
95.0 ± 0.3
78.0 ± 0.3
77.0 ± 0.3
Table 1. Classiﬁcation accuracy (%) for all the six tasks of Ofﬁce-31 dataset based on ResNet-50 . Our methods named “intra only”
and “CAN” are trained with intra-class domain discrepancy and contrastive domain discrepancy, respectively.
motorcycle
skateboard
Source-ﬁnetune
RevGrad 
Ours (intra only)
Ours (CAN)
Table 2. Classiﬁcation accuracy (%) on the VisDA-2017 validation set based on ResNet-101 . Our methods named “intra only” and
“CAN” are trained with intra-class domain discrepancy and contrastive domain discrepancy, respectively.
Figure 4. Visualization with t-SNE for different adaptation methods (bested viewed in color). Left: t-SNE of JAN. Right: CAN.
The input activations of the last FC layer are used for the computation of t-SNE. The results are on Ofﬁce-31 task W →A.
the second place on the leaderboard (87.7%).
From Table 1 and 2, we have two observations: 1)
Taking class information/decision boundary into account is
beneﬁcial for the adaptation. It can be seen that MADA,
MCD, ADR and our method achieve better performance
than class-agnostic methods, e.g. RevGrad, DAN, JAN, etc.
2) Our way of exploiting class information is more effective.
We achieve better accuracy than MADA (+5.4%),
ADR (+12.4%), and MCD (+15.3%).
4.3. Ablation studies
Effect of inter-class domain discrepancy. We compare
our method (“CAN”) with that trained using intra-class discrepancy only (“intra only”), to verify the merits of introducing inter-class domain discrepancy measure. The results
are shown in the last two rows in Table 1 and 2. It can be
seen that introducing the inter-class domain discrepancy improves the adaptation performance. We believe the reason is
that it is impossible to completely eliminate the intra-class
domain discrepancy, maximizing the inter-class domain discrepancy may alleviate the possibility of the model overﬁtting to the source data and beneﬁts the adaptation.
Effect of alternative optimization and class-aware
Table 3 examines two key components of
CAN, i.e. alternative optimization (or “AO”), and classaware sampling (or “CAS”). We perform ablation study by
leaving-one-component-out of our framework at a time. In
Table 3, the method “w/o. AO” directly employs the outputs
of the network at each iteration as pseudo target labels to
estimate CDD and back-propagates to update the network.
It can be regarded as updating the feature representations
and pseudo target labels simultaneously. The method “w/o.
CAS” uses conventional class-agnostic sampling instead of
CAS. The comparisons to these two special cases verify the
contributions of AO and CAS in our method.
Interestingly, even without alternative optimization, the
method “w/o. AO” improves over class-agnostic methods,
e.g. DAN, JAN, etc. This suggests our proposed CDD in
itself is robust to the label noise to some extent, and MMD
is a suitable metric to establish CDD (see Section 3.2).
(a-b) The curve of CDD and accuracy during training on task A →D of the Ofﬁce-31 dataset. The “CDD-G” denotes the
contrastive domain discrepancy computed with ground-truth target labels. (c-d) The sensitivity of accuracy of CAN to β. The results for
A →D (Left) and D →A (Right) are illustrated as examples. The trends for other tasks are similar.
VisDA-2017
Table 3. The effect of alternative optimization (AO) and CAS. The
mean accuracy over six tasks on Ofﬁce-31 and the mean accuracy
over 12 classes on VisDA-2017 validation set are reported.
pseudo1 90.2 ± 1.6 92.5 ± 0.4 75.7 ± 0.2 75.3 ± 0.6
94.5 ± 0.3 95.0 ± 0.3 78.0 ± 0.3 77.0 ± 0.3
Table 4. Comparison with different ways of utilizing pseudo target labels.The “pseudo0” means training with pseudo target labels
(achieved by our initial clustering) directly. The “pseudo1” is to
alternatively update target labels through clustering and minimize
the cross-entropy loss on pseudo labeled target data. In “pseudo1”,
the cross-entropy loss on source data is also minimized.
Ways of using pseudo target labels. The estimates for
the target labels can be achieved through clustering, which
enables various ways to train a model. In Table 4, we compare our method with two different ways of training with
pseudo target labels achieved by the clustering. One way
(“pseudo0”) is to ﬁx these pseudo labels to train a model
directly. The other (“pseudo1”) is to update the pseudo target labels during training, which is the same as CAN, but to
train the model based on the cross-entropy loss over pseudo
labeled target data rather than estimating the CDD.
As shown in Table 4, “pseudo0” leads to a model whose
accuracy exactly matches with that of the initial clustering, due to the large capacity of deep neural networks.
The “pseudo1” achieves signiﬁcantly better results than
“pseudo0”, but is still worse than our CAN, which veriﬁes
that our way of explicitly modeling the class-aware domain
discrepancy makes the model better adapted and less likely
to be affected by the label noise.
CDD value during training. In our training, we generate target label hypothesis to estimate CDD. We expect
that the underlying metric computed with the ground-truth
target labels would decrease steadily during training until
convergence. To do so, during training, we evaluate the
ground-truth CDD (denoted by CDD-G) for JAN and CAN
with the ground-truth target labels. The trend of CDD and
the test accuracy during training are plotted in Fig. 5.
As we see, for JAN (the blue curve), the ground-truth
CDD rapidly becomes stable at a high level after a short
decrease. This indicates that JAN cannot minimize the contrastive domain discrepancy effectively. For CAN (the red
curve), although we can only estimate the CDD using inaccurate target label hypothesis, its CDD value steadily decreases along training. The result illustrates our estimation
works as a good proxy of ground-truth contrastive domain
discrepancy. And from the accuracy curve illustrated in Fig.
5, we see that minimizing CDD leads to notable accuracy
improvement of CAN, compared to JAN.
Hyper-parameter sensitivity. We study the sensitivity
of CAN to the important balance weight β on two example
tasks A →D and D →A in Fig. 5. Generally, our model
is less sensitive to the change of β. In a vast range, the
performance of CAN outperforms the baseline method with
a large margin (the blue dashed curve). As the β gets larger,
the accuracy steadily increases before decreasing. The bellshaped curve illustrates the regularization effect of CDD.
5. Conclusion
In this paper, we proposed Contrastive Adaptation Network to perform class-aware alignment for UDA. The intraclass and inter-class domain discrepancy are explicitly modeled and optimized through end-to-end mini-batch training.
Experiments on real-world benchmarks demonstrate the superiority of our model compared with the strong baselines.
Acknowledgement.
This work was supported in part by
the Intelligence Advanced Research Projects Activity (IARPA) via
Department of Interior/Interior Business Center (DOI/IBC) contract number D17PC00340. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes not
withstanding any copyright annotation/herein. Disclaimer: The
views and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing the of-
ﬁcial policies or endorsements, either expressed or implied, of
IARPA, DOI/IBC, or the U.S.Government.