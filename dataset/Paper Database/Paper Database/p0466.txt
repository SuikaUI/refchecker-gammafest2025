MIT Open Access Articles
Ambient Sound Provides Supervision for Visual Learning
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Owens, Andrew, et al. “Ambient Sound Provides Supervision for Visual Learning.”
Lecture Notes in Computer Science 9905 : 801–816. © 2016 Springer
International Publishing AG
As Published: 
Publisher: Springer-Verlag
Persistent URL: 
Version: Original manuscript: author's manuscript prior to formal peer review
Terms of use: Creative Commons Attribution-Noncommercial-Share Alike
Ambient Sound Provides Supervision
for Visual Learning
Andrew Owens1, Jiajun Wu1, Josh H. McDermott1,
William T. Freeman1,2, and Antonio Torralba1
1Massachusetts Institute of Technology
2Google Research
Abstract. The sound of crashing waves, the roar of fast-moving cars –
sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train
a convolutional neural network to predict a statistical summary of the
sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about
objects and scenes. We evaluate this representation on several recognition tasks, ﬁnding that its performance is comparable to that of other
state-of-the-art unsupervised learning methods. Finally, we show through
visualizations that the network learns units that are selective to objects
that are often associated with characteristic sounds.
Keywords: Sound, convolutional networks, unsupervised learning.
Introduction
Sound conveys important information about the world around us – the bustle
of a caf´e tells us that there are many people nearby, while the low-pitched roar
of engine noise tells us to watch for fast-moving cars . Although sound is
in some cases complementary to visual information, such as when we listen
to something out of view, vision and hearing are often informative about the
same structures in the world. Here we propose that as a consequence of these
correlations, concurrent visual and sound information provide a rich training
signal that we can use to learn useful representations of the visual world.
In particular, an algorithm trained to predict the sounds that occur within
a visual scene might be expected to learn objects and scene elements that are
associated with salient and distinctive noises, such as people, cars, and ﬂowing
water. Such an algorithm might also learn to associate visual scenes with the
ambient sound textures that occur within them. It might, for example,
associate the sound of wind with outdoor scenes, and the buzz of refrigerators
with indoor scenes.
Although human annotations are indisputably useful for learning, they are
expensive to collect. The correspondence between ambient sounds and video is,
by contrast, ubiquitous and free. While there has been much work on learning
 
Owens et al.
Frequency channel
Frequency channel
Freq. channel
Freq. channel
Mod. channel
Time (sec.)
Freq. channel
Time (sec.)
Frequency channel
Frequency channel
Mod. channel
Freq. channel
Freq. channel
Freq. channel
(a) Video frame
(b) Cochleagram
(c) Summary statistics
Fig. 1: Visual scenes are associated with characteristic sounds. Our goal is to take
an image (a) and predict time-averaged summary statistics (c) of a cochleagram
(b). The statistics we use are (clockwise): the response to a bank of band-pass
modulation ﬁlters; the mean and standard deviation of each frequency band;
and the correlation between bands. We show two frames from the Flickr video
dataset . The ﬁrst contains the sound of human speech; the second contains
the sound of wind and crashing waves. The diﬀerences between these sounds
are reﬂected in their summary statistics: e.g., the water/wind sound, which is
similar to white noise, contains fewer correlations between cochlear channels.
from unlabeled image data , an audio signal may provide information
that that is largely orthogonal to that available in images alone – information
about semantics, events, and mechanics are all readily available from sound .
One challenge in utilizing audio-visual input is that the sounds that we hear
are only loosely associated with what we see. Sound-producing objects often lie
outside of our visual ﬁeld, and objects that are capable of producing characteristic sounds – barking dogs, ringing phones – do not always do so. A priori it is
thus not obvious what might be achieved by predicting sound from images.
In this work, we show that a model trained to predict held-out sound from
video frames learns a visual representation that conveys semantically meaningful
information. We formulate our sound-prediction task as a classiﬁcation problem,
in which we train a convolutional neural network (CNN) to predict a statistical
summary of the sound that occurred at the time a video frame was recorded.
We then validate that the learned representation contains signiﬁcant information
about objects and scenes.
We do this in two ways: ﬁrst, we show that the image features that we learn
through our sound-prediction task can be used for object and scene recognition. On these tasks, our features obtain similar performance to state-of-the-art
unsupervised and self-supervised learning methods. Second, we show that the
Ambient Sound Provides Supervision for Visual Learning
intermediate layers of our CNN are highly selective for objects. This augments
recent work showing that object detectors “emerge” in a CNN’s internal
representation when it is trained to recognize scenes. As in the scene recognition
task, object detectors emerge inside of our sound-prediction network. However,
our model learns these detectors from an unlabeled audio-visual signal, without
any explicit human annotation.
In this paper, we: (1) present a model based on visual CNNs and sound
textures that predicts a video frame’s held-out sound; (2) demonstrate that
the CNN learns units in its convolutional layers that are selective for objects,
extending the methodology of Zhou et al. ; (3) validate the eﬀectiveness
of sound-based supervision by using the learned representation for object- and
scene-recognition tasks. These results suggest that sound data, which is available
in abundance from consumer videos, provides a useful training signal for visual
Related Work
We take inspiration from work in psychology, such as Gaver’s Everyday Listening
 , that studies the ways that humans learn about objects and events using
sound. In this spirit, we would like to study the situations where sound tells
us about visual objects and scenes. Work in auditory scene analysis
meanwhile has provided computational methods for recognizing structures in
audio streams. Following this work, we use a sound representation that has
been applied to sound recognition and synthesis tasks .
Recently, researchers have proposed many unsupervised learning methods
that learn visual representations by solving prediction tasks (sometimes known
as pretext tasks) for which the held-out prediction target is derived from a natural
signal in the world, rather than from human annotations. This style of learning
has been called “self supervision” or “natural supervision” . With these
methods, the supervisory signal may come from video, for example by having the algorithm estimate camera motion or track content across frames
 . There are also methods that learn from static images, for example
by predicting the relative location of image patches , or by learning invariance to simple geometric and photometric transformations . The assumption
behind these methods is that, in order to solve the pretext task, the model has
to implicitly learn about semantics and, through this process, develop image
features that are broadly useful.
While we share with this work the high-level goal of learning image representations, and we use a similar technical approach, our work diﬀers in signiﬁcant
ways. In contrast to methods whose supervisory signal comes entirely from the
imagery itself, ours comes from a modality (sound) that is complementary to
vision. This is advantageous because sound is known to be a rich source of information about objects and scenes , and it is largely invariant to visual
transformations, such as lighting, scene composition, and viewing angle. Predicting sound from images thus requires some degree of generalization to visual
Owens et al.
Audio cluster
prediction
(a) Images grouped by audio cluster
Freq. channel
Freq. channel
(b) Clustered audio stats.
(c) CNN model
Fig. 2: Visualization of some of the audio clusters used in one of our models (5 of
30 clusters). For each cluster, we show (a) the images in the test set whose sound
textures were closest to the centroid (no more than one frame per video), and
(b) we visualize aspects of the sound texture used to deﬁne the cluster centroid –
speciﬁcally, the mean and standard deviation of the frequency channels. We also
include a representative cochleagram (that of the leftmost image). Although the
clusters were deﬁned using audio, there are common objects and scene attributes
in many of the images. We train a CNN to predict a video frame’s auditory cluster
assignment (c).
transformations. Moreover, our supervision task is based on solving a straightforward classiﬁcation problem, which allows us to use a network design that
closely resembles those used in object and scene recognition (rather than, for
example, the siamese-style networks used in video methods).
Our approach is closely related to recent audio-visual work that predicts
soundtracks for videos that show a person striking objects with a drumstick. A
key feature of this work is that the sounds are “visually indicated” by actions
in video – a situation that has also been considered in other contexts, such as in
the task of visually localizing a sound source or in evaluating the synchronization between the two modalities . In the natural videos that we use,
however, the sound sources are frequently out of frame. Also, in contrast to other
recent work in multi-modal representation learning , our technical approach is based on solving a self-supervised classiﬁcation problem (rather than a
generative model or autoencoder), and our goal is to learn visual representations
that are generally useful for object recognition tasks.
Ambient Sound Provides Supervision for Visual Learning
Learning to predict ambient audio
We would like to train a model that, when given a frame of video, can predict
its corresponding sound – a task that implicitly requires knowledge of objects
and scenes.
Statistical sound summaries
A natural question, then, is how our model should represent sound. Perhaps the
ﬁrst approach that comes to mind would be to estimate a frequency spectrum
at the moment in which the picture was taken, similar to . However, this
is potentially suboptimal because in natural scenes it is diﬃcult to predict the
precise timing of a sound from visual information. Upon seeing a crowd of people,
for instance, we might expect to hear the sound of speech, but the precise timing
and content of that speech might not be directly indicated by the video frames.
To be closer to the time scale of visual objects, we estimate a statistical
summary of the sound, averaged over a few seconds. We do this using the sound
texture model of McDermott and Simoncelli , which assumes that sound is
stationary within a temporal window (we use 3.75 seconds). More speciﬁcally,
we closely follow and ﬁlter the audio waveform with a bank of 32 bandpass ﬁlters intended to mimic human cochlear frequency selectivity. We then
take the Hilbert envelope of each channel, raise each sample of the envelope
to the 0.3 power (to mimic cochlear amplitude compression), and resample the
compressed envelope to 400 Hz. Finally, we compute time-averaged statistics of
these subband envelopes: we compute the mean and standard deviation of each
frequency channel, the mean squared response of each of a bank of modulation
ﬁlters applied to each channel, and the Pearson correlation between pairs of
channels. For the modulation ﬁlters, we use a bank of 10 band-pass ﬁlters with
center frequencies ranging from 0.5 to 200 Hz, equally spaced on a logarithmic
To make the sound features more invariant to gain (e.g., from the microphone), we divide the envelopes by the median energy (median vector norm)
over all timesteps, and include this energy as a feature. As in , we normalize the standard deviation of each cochlear channel by its mean, and each
modulation power by its standard deviation. We then rescale each kind of texture feature (i.e. marginal moments, correlations, modulation power, energy)
inversely with the number of dimensions. The sound texture for each image is a
502-dimensional vector. In Figure 1, we give examples of these summary statistics for two audio clips. We provide more details about our audio representation
in the supplementary material.
Predicting sound from images
We would like to predict sound textures from images – a task that we hypothesize
leads to learning useful visual representations. Although multiple frames are
available, we predict sound from a single frame, so that the learned image features
Owens et al.
will be more likely to transfer to single-image recognition tasks. Furthermore,
since the the actions that produce the sounds may not appear on-screen, motion
information may not always be applicable.
While one option would be to regress the sound texture vj directly from the
corresponding image Ij, we choose instead to deﬁne explicit sound categories
and formulate this visual recognition problem as a classiﬁcation task. This also
makes it easier to analyze the network, because it allows us to compare the
internal representation of our model to object- and scene-classiﬁcation models
with similar network architecture (Section 4.1). We consider two labeling models:
one based on a vector quantization, the other based on a binary coding scheme.
Clustering audio features
In the Clustering model, the sound textures
{vj} in the training set are clustered using k-means. These clusters deﬁne image
categories: we label each sound texture with the index of the closest centroid,
and train our CNN to label images with their corresponding labels.
We found that audio clips that belong to a cluster often contain common
objects. In Figure 2, we show examples of such clusters, and in the supplementary
material we provide their corresponding audio. We can see that there is a cluster
that contains indoor scenes with children in them – these are relatively quiet
scenes punctuated with speech sounds. Another cluster contains the sounds of
many people speaking at once (often large crowds); another contains many water
scenes (usually containing loud wind sounds). Several clusters capture general
scene attributes, such as outdoor scenes with light wind sounds. During training,
we remove examples that are far from the centroid of their cluster (more than
the median distance to the vector, amongst all examples in the dataset).
Binary coding model
For the other variation of our model (which we call
the Binary model), we use a binary coding scheme equivalent to a
multi-label classiﬁcation problem. We project each sound texture vj onto the
top principal components (we use 30 projections), and convert these projections
into a binary code by thresholding them. We predict this binary code using a
sigmoid layer, and during training we measure error using cross-entropy loss.
For comparison, we trained a model (which we call the Spectrum model) to
approximately predict the frequency spectrum at the time that the photo was
taken, in lieu of a full sound texture. Speciﬁcally, for our sound vectors vj in this
model, we used the mean value of each cochlear channel within a 33.3-millisecond
interval centered on the input frame (approximately one frame of a 30 Hz video).
For training, we used the projection scheme from the Binary model.
We trained our models to predict audio on a 360,000-video subset of
the Flickr video dataset . Most of the videos in the dataset are personal video
recordings containing natural audio, though many were post-processed, e.g. with
added subtitles, title screens, and music. We divided our videos into training and
test sets, and we randomly sampled 10 frames per video (1.8 million training
images total). For our network architecture, we used the CaﬀeNet architecture
 (a variation of Krizhevsky et al. ) with batch normalization . We
Ambient Sound Provides Supervision for Visual Learning
Training by sound (91 Detectors)
Training by labeled scenes (117 Detectors)
Training by visual tracking (72 Detectors)
Fig. 3: Histogram of object-selective units in networks trained with diﬀerent
styles of supervision. From top to bottom: training to predict ambient sound
(our Clustering model); training to predict scene category using the Places
dataset ; and training to do visual tracking . Compared to the tracking model, which was also trained without semantic labels, our network learns
more high-level object detectors. It also has more detectors for objects that make
characteristic sounds, such as person, baby, and waterfall, in comparison to the
one trained on Places . Categories marked with ∗are those that we consider
to make characteristic sounds.
trained our model with Caﬀe , using a batch size of 256, for 320,000 iterations
of stochastic gradient descent.
We evaluate the image representation that our model learned in multiple ways.
First, we demonstrate that the internal representation of our model contains
convolutional units (neurons) that are selective to particular objects, and we
analyze those objects’ distribution. We then empirically evaluate the quality of
the learned representation for several image recognition tasks, ﬁnding that it
Owens et al.
Neuron visualizations of the network trained by sound
snowy ground
grandstand
grandstand
grandstand
Neuron visualizations of the network trained by visual tracking 
Neuron visualizations of the network trained by egomotion 
Neuron visualizations of the network trained by patch positions 
Neuron visualizations of the network trained by labeled scenes 
Fig. 4: Top 5 responses for neurons of various networks, tested on the Flickr dataset.
Please see Section A2 for more visualizations.
Ambient Sound Provides Supervision for Visual Learning
achieves performance comparable to other feature-learning methods that were
trained without human annotations.
What does the network learn to detect?
Previous work has shown that a CNN trained to predict scene categories
will learn convolutional units that are selective for objects – a result that follows
naturally from the fact that scenes are often deﬁned by the objects that compose
them. We ask whether a model trained to predict ambient sound, rather than
explicit human labels, would learn object-selective units as well. For these experiments, we used our Clustering model, because its network structure is similar
to that of the scene-recognition model used in .
Quantifying object-selective units
Similar to the method in , we
visualized the images that each neuron in the top convolutional layer (conv5)
responded most strongly to. To do this, we sampled a pool of 200,000 images
from our Flickr video test set. We then collected, for each convolutional unit,
the 60 images in this set that gave the unit the largest activation. Next, we
applied the so-called synthetic visualization technique of to approximately
superimpose the unit’s receptive ﬁeld onto the image. Speciﬁcally, we found all
of the spatial locations in the layer for which the unit’s activation strength was
at least half that of its maximum response. We then masked out the parts of
the image that were not covered by the receptive ﬁeld of one of these highresponding spatial units. We assumed a circle-shaped receptive ﬁeld, obtaining
the radius from . To examine the eﬀect of the data used in the evaluation,
we also applied this visualization technique to other datasets (please see the
supplementary material).
Next, for each neuron we showed its masked images to three human annotators on Amazon Mechanical Turk, and asked them: (1) whether an object is
present in many of these regions, and if so, what it is; (2) to mark the images
whose activations contain these objects. Unlike , we only considered units
that were selective to objects, ignoring units that were selective to textures. For
each unit, if at least 60% of its top 60 activations contained the object, we considered it to be selective for the object (or following , we say that it is a
detector for that object). We then manually labeled the unit with an object category, using the category names provided by the SUN database . We found
that 91 of the 256 units in our model were object-selective in this way, and we
show a selection of them in Figure 4.
We compared the number of these units to those of a CNN trained to recognize human-labeled scene categories on Places . As expected, this model –
having been trained with explicit human annotations – contained more objectselective units (117 units). We also asked whether object-selective neurons appear in the convolutional layers when a CNN is trained on other tasks that do
not use human labels. As a simple comparison, we applied the same methodology
to the egomotion-based model of Agrawal et al. and to the tracking-based
method of Wang and Gupta . We applied these networks to whole images
Owens et al.
# Detectors
# Detectors for objects with characteristic sounds
Videos with object sound
Characteristic sound rate
Table 1: Row 1: the number of detectors (i.e. units that are selective to a particular object); row 2: the number of detectors for objects with characteristic
sounds; row 3: fraction of videos in which an object’s sound is audible (computed only for object classes with characteristic sounds); row 4: given that an
activation corresponds to an object with a characteristic sound, the probability
that its sound is audible. There are 256 units in total for each method.
(in all cases resizing the input image to 256 × 256 pixels and taking the center
227 × 227 crop), though we note that they were originally trained on cropped
image regions.
We found that the tracking-based method also learned object-selective units,
but that the objects that it detected were often textural “stuﬀ,” such as grass,
ground, and water, and that there were fewer of these detection units in total
(72 of 256). The results were similar for the egomotion-based model, which had
27 such units. In Figure 3 and in the supplementary material, we provide the
distribution of the objects that the units were selective to. We also visualized
neurons from the method of Doersch et al. (as before, applying the network
to whole images, rather than to patches). We found a signiﬁcant number of
the units were selective for position, rather than to objects. For example, one
convolutional unit responded most highly to the upper-left corner of an image
– a unit that may be useful for the training task, which involves predicting
the relative position of image patches. In Figure 4, we show visualizations of a
selection of object-detecting neurons for all of these methods.
The diﬀerences between the objects detected by these methods and our own
may have to do with the requirements of the tasks being solved. The other unsupervised methods, for example, all involve comparing multiple input images or
sub-images in a relatively ﬁne-grained way. This may correspondingly change the
representation that the network learns in its last convolutional layer – requiring
its the units to encode, say, color and geometric transformations rather than
object identities. Moreover, these networks may represent semantic information
in other (more distributed) ways that would not necessarily be revealed through
this visualization method.
Analyzing the types of objects that were detected
Next, we asked
what kinds of objects our network learned to detect. We hypothesized that the
object-selective neurons were more likely to respond to objects that produce
(or are closely associated with) characteristic sounds. To evaluate this, we (an
author) labeled the SUN object categories according to whether they were closely
Ambient Sound Provides Supervision for Visual Learning
associated with a characteristic sound. We denote these categories with a ∗in
Figure 3. Next, we counted the number of units that were selective to these
objects, ﬁnding that our model contained signiﬁcantly more such units than a
scene-recognition network trained on the Places dataset, both in total number
and as a proportion (Table 1). A signiﬁcant fraction of these units were selective
to people (adults, babies, and crowds).
Finally, we asked whether the sounds that these objects make were actually
present in the videos that these video frames were sampled from. To do this, we
listened to the sound of the top 30 video clips for each unit, and recorded whether
the sound was made by the object that the neuron was selective to (e.g., human
speech for the person category). We found that 43.7% of these videos contained
the objects’ sounds (Table 1).
Evaluating the image representation
We have seen through visualizations that a CNN trained to predict sound from
an image learns units that are highly selective for objects. Now we evaluate,
experimentally, how well the CNN’s internal representation conveys information
that is useful for recognizing objects and scenes.
Since our goal is to measure the amount of semantic information provided by
the learned representation, rather than to seek absolute performance, we used
a simple evaluation scheme. In most experiments, we computed image features
using our CNN and trained a linear SVM to predict object or scene category
using the activations in the top layers.
Object recognition
First, we used our CNN features for object recognition
on the PASCAL VOC 2007 dataset . We trained a one-vs.-rest linear SVM
to detect the presence of each of the 20 object categories in the dataset, using the activations of the upper layers of the network as the feature set (pool5,
fc6, and fc7). To help understand whether the convolutional units considered
in Section 4.1 directly convey semantics, we also created a global max-pooling
feature (similar to ), where we applied max pooling over the entire convolutional layer. This produces a 256-dimensional vector that contains the maximum
response of each convolutional unit (we call it max5). Following common practice, we evaluated the network on a center 227 × 227 crop of each image (after
resizing the image to 256 × 256), and we evaluated the results using mean average precision (mAP). We chose the SVM regularization parameter for each
method by maximizing mAP on the validation set using grid search (we used
{0.5k | 4 ≤k < 20}).
The other unsupervised (or self-supervised) models in our comparison 
use diﬀerent network designs. In particular, was trained on image patches,
so following their experiments we resized its convolutional layers for 227 × 227
images and removed the model’s fully connected layers1. Also, since the model
1 As a result, this model has a larger pool5 layer than the other methods: 7 × 7 vs.
6 × 6. Likewise, the fc6 layer of is smaller (1,024 dims. vs. 4,096 dims.).
Owens et al.
VOC Cls. (%mAP)
SUN397 (%acc.)
max5 pool5
max5 pool5
Sound (cluster) 36.7
Sound (binary) 39.4
Sound (spect.)
Texton-CNN
K-means 
Tracking 
Patch pos. 
Egomotion 
ImageNet 
Places 
(a) Image classiﬁcation with linear SVM
Random init. 
Sound (cluster)
Sound (binary)
Motion 
Egomotion 
Patch pos. 
Calib. + Patch 
ImageNet 
Places 
(b) Finetuning detection
aer bk brd bt btl bus car cat chr cow din dog hrs mbk prs pot shp sfa trn tv
Sound (cluster) 68 47
Sound (binary) 69
39 32 69 38
Sound (spect.)
Texton-CNN
Motion 
Patches 
28 39 62 43
Egomotion 
ImageNet 
79 71 73 75 25 60
62 56 82 62
Places 
56 80 23 66 84 54 57
45 61 88 63
(c) Per class mAP for image classiﬁcation on PASCAL VOC 2007
Table 2: (a) Mean average precision for PASCAL VOC 2007 classiﬁcation, and
accuracy on SUN397. Here we trained a linear SVM using the top layers of
diﬀerent networks. We note in Section 4.2 that the shape of these layers varies
between networks. (b) Mean average precision on PASCAL VOC 2007 using
Fast-RCNN . We initialized the CNN weights using those of our learned
sound models. (c) Per-class AP scores for the VOC 2007 classiﬁcation task with
pool5 features (corresponds to mAP in (a)).
of Agrawal et al. did not have a pool5 layer, we added one to it. We also
considered CNNs that were trained with human annotations: object recognition
on ImageNet and scene categories on Places . Finally, we considered using
the k-means weight initialization method of to set the weights of a CNN
model (we call this the K-means model).
We found that our best-performing of our model (the binary-coding method)
obtained comparable performance to other unsupervised learning methods, such
as . Both models based on sound textures (Clustering and Binary) outperformed the model that predicted only the frequency spectrum. This suggests
that the additional time-averaged statistics from sound textures are helpful. For
these models, we used 30 clusters (or PCA projections): in Figure A1a, we consider varying the number of clusters, ﬁnding that there is a small improvement
from increasing it, and a substantial decrease in performance when using just
Ambient Sound Provides Supervision for Visual Learning
two clusters. The sound-based models signiﬁcantly outperformed other methods
when we globally pooled the conv5 features, suggesting that the convolutional
units contain a signiﬁcant amount of semantic information (and are well suited
to being used at this spatial scale).
Scene recognition
We also evaluated our model on a scene recognition task
using the SUN dataset , a large classiﬁcation benchmark that involves recognizing 397 scene categories with 7,940 training and test images provided in
multiple splits. Following , we averaged our classiﬁcation accuracy across 3
splits, with 20 examples per scene category. We chose the linear SVM’s regularization parameter for each model using 3-fold cross-validation.
We again found that our features’ performance was comparable to other
models. In particular, we found that the diﬀerence between our models was
smaller than in the object-recognition case, with both the Clustering and Binary
models obtaining performance comparable to the patch-based method with pool5
Pretraining for object detection
Following recent work , we used
our model to initialize the weights of a CNN-based object detection system (Fast
R-CNN ), verifying that the results improved over random initialization. We
followed the training procedure of Kr¨ahenb¨uhl et al. , using 150,000 iterations
of backpropagation with an initial learning rate of 0.002, and we compared our
model with other published results (we report the numbers provided by ).
Our best-performing model (the Clustering model) obtains similar performance to that of Wang and Gupta’s tracking-based model , while the overall
best results were from variations of Doersch et al.’s patch-based model .
We note that the network changes substantially during ﬁne-tuning, and thus the
performance is fairly dependent on the parameters used in the training procedure. Moreover all models, when ﬁne-tuned in this way, achieve results that are
close to those of a well-chosen random initialization (within 6% mAP). Recent
work has addressed these optimization issues by rescaling the weights of
a pretrained network using a data-driven procedure. The unsupervised method
with the best performance combines the rescaling method of with the patchbased pretraining of .
Sound prediction
We also asked how well our model learned to solve its
sound prediction task. We found that on our test set, the clustering-based model
(with 30 clusters) chose the correct sound label 15.8% of the time. Pure chance
in this case is 3.3%, while the baseline of choosing the most commonly occurring
label is 6.6%.
Audio supervision
It is natural to ask what role audio plays in the learning
process. Perhaps, for example, our training procedure would produce equally
good features if we replaced the hand-crafted sound features with hand-crafted
visual features, computed from the images themselves. To study this, we replaced
our sound texture features with (512-dimensional) visual texton histograms ,
Owens et al.
using the parameters from , and we used them to train a variation of our
Clustering model.
As expected, the images that belong to each cluster are visually coherent, and
share common objects. However, we found that the network performed signiﬁcantly worse than the audio-based method on the object- and scene-recognition
metrics (Table 2a). Moreover, we found that its convolutional units rarely were
selective for objects (generally they responded responded to “stuﬀ” such as grass
and water). Likely this is because the network simply learned to approximate
the texton features, obtaining low labeling error without high-level generalization. In contrast, the audio-based labels – despite also being based on another
form of hand-crafted feature – are largely invariant to visual transformations,
such as lighting and scale, and therefore predicting them requires some degree of
generalization (one beneﬁt of training with multiple, complementary modalities).
Discussion
Sound has many properties that make it useful as a supervisory training signal: it is abundantly available without human annotations, and it is known to
convey information about objects and scenes. It is also complementary to visual
information, and may therefore convey information not easily obtainable from
unlabeled image analysis.
In this work, we proposed using ambient sound to learn visual representations. We introduced a model, based on convolutional neural networks, that
predicts a statistical sound summary from a video frame. We then showed, with
visualizations and experiments on recognition tasks, that the resulting image
representation contains information about objects and scenes.
Here we considered one audio representation, based on sound textures, but
it is natural to ask whether other audio representations would lead the model to
learn about additional types of objects. To help answer this question, we would
like to more systematically study the situations when sound does (and does not)
tell us about objects in the visual world. Ultimately, we would like to know what
object and scene structures are detectable through sound-based training, and we
see our work as a step in this direction.
Acknowledgments.
This work was supported by NSF grants #1524817 to
A.T; NSF grants #1447476 and #1212849 to W.F.; a McDonnell Scholar Award
to J.H.M.; and a Microsoft Ph.D. Fellowship to A.O. It was also supported by
Shell Research, and by a donation of GPUs from NVIDIA. We thank Phillip Isola
and Carl Vondrick for the helpful discussions, and the anonymous reviewers for
their comments (in particular, for suggesting the comparison with texton features
in Section 4.2).