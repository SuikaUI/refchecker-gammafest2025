ECA-Net: Efﬁcient Channel Attention for Deep Convolutional Neural Networks
Qilong Wang1, Banggu Wu1, Pengfei Zhu1, Peihua Li2, Wangmeng Zuo3, Qinghua Hu1,∗
1 Tianjin Key Lab of Machine Learning, College of Intelligence and Computing, Tianjin University, China
2 Dalian University of Technology, China
3 Harbin Institute of Technology, China
Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs).
However, most existing methods dedicate to developing
more sophisticated attention modules for achieving better
performance, which inevitably increase model complexity.
To overcome the paradox of performance and complexity
trade-off, this paper proposes an Efﬁcient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting
the channel attention module in SENet, we empirically show
avoiding dimensionality reduction is important for learning
channel attention, and appropriate cross-channel interaction can preserve performance while signiﬁcantly decreasing model complexity. Therefore, we propose a local crosschannel interaction strategy without dimensionality reduction, which can be efﬁciently implemented via 1D convolution. Furthermore, we develop a method to adaptively select
kernel size of 1D convolution, determining coverage of local cross-channel interaction. The proposed ECA module
is efﬁcient yet effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80
vs. 24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more than 2% in terms
of Top-1 accuracy. We extensively evaluate our ECA module on image classiﬁcation, object detection and instance
segmentation with backbones of ResNets and MobileNetV2.
The experimental results show our module is more efﬁcient
while performing favorably against its counterparts.
1. Introduction
Deep convolutional neural networks (CNNs) have been
widely used in computer vision community, and have
∗Qinghua Hu is the corresponding author.
Email: {qlwang, wubanggu, huqinghua}@tju.edu.cn. The work was supported by the National Natural Science Foundation of China (Grant No.
61806140, 61876127, 61925602, 61971086, U19A2073, 61732011), Major Scientiﬁc Research Project of Zhejiang Lab (2019DB0ZX01). Q. Wang
was supported by National Postdoctoral Program for Innovative Talents.
Comparison
SENet , CBAM , A2-Nets and ECA-Net) using
ResNets as backbone models in terms of classiﬁcation accuracy, network parameters and FLOPs, indicated by radiuses of
circles. Note that our ECA-Net obtains higher accuracy while having less model complexity.
achieved great progress in a broad range of tasks, e.g., image classiﬁcation, object detection and semantic segmentation. Starting from the groundbreaking AlexNet , many
researches are continuously investigated to further improve
the performance of deep CNNs .
Recently, incorporation of channel attention into convolution blocks has attracted a lot of interests, showing great potential in performance improvement .
One of the representative methods is squeeze-and-excitation
networks (SENet) , which learns channel attention for
each convolution block, bringing clear performance gain for
various deep CNN architectures.
Following the setting of squeeze (i.e., feature aggregation) and excitation (i.e., feature recalibration) in
SENet , some researches improve SE block by capturing more sophisticated channel-wise dependencies or by combining with additional spatial attention . Although these methods have achieved
 
: element-wise product
Adaptive Selection of
Kernel Size:
Figure 2. Diagram of our efﬁcient channel attention (ECA) module.
Given the aggregated features obtained by global average
pooling (GAP), ECA generates channel weights by performing a
fast 1D convolution of size k, where k is adaptively determined
via a mapping of channel dimension C.
higher accuracy, they often bring higher model complexity and suffer from heavier computational burden. Different
from the aforementioned methods that achieve better performance at the cost of higher model complexity, this paper
focuses instead on a question: Can one learn effective channel attention in a more efﬁcient way?
To answer this question, we ﬁrst revisit the channel attention module in SENet. Speciﬁcally, given the input features, SE block ﬁrst employs a global average pooling for
each channel independently, then two fully-connected (FC)
layers with non-linearity followed by a Sigmoid function
are used to generate channel weights. The two FC layers
are designed to capture non-linear cross-channel interaction, which involve dimensionality reduction for controlling
model complexity. Although this strategy is widely used in
subsequent channel attention modules , our empirical studies show dimensionality reduction brings side effect on channel attention prediction, and it is inefﬁcient and
unnecessary to capture dependencies across all channels.
Therefore, this paper proposes an Efﬁcient Channel Attention (ECA) module for deep CNNs, which avoids dimensionality reduction and captures cross-channel interaction in
an efﬁcient way. As illustrated in Figure 2, after channelwise global average pooling without dimensionality reduction, our ECA captures local cross-channel interaction by
considering every channel and its k neighbors. Such method
is proven to guarantee both efﬁciency and effectiveness.
Note that our ECA can be efﬁciently implemented by fast
1D convolution of size k, where kernel size k represents the
coverage of local cross-channel interaction, i.e., how many
neighbors participate in attention prediction of one channel.
To avoid manual tuning of k via cross-validation, we develop a method to adaptively determine k, where coverage
of interaction (i.e., kernel size k) is proportional to channel
dimension. As shown in Figure 1 and Table 3, as opposed
to the backbone models , deep CNNs with our ECA
Cross-channel Interaction
Lightweight
SENet 
GE-θ+ 
A2-Net 
GSoP-Net 
ECA-Net (Ours)
Table 1. Comparison of existing attention modules in terms of
whether no channel dimensionality reduction (No DR), crosschannel interaction and less parameters than SE (indicated by
lightweight) or not.
module (called ECA-Net) introduce very few additional parameters and negligible computations, while bringing notable performance gain. For example, for ResNet-50 with
24.37M parameters and 3.86 GFLOPs, the additional parameters and computations of ECA-Net50 are 80 and 4.7e-
4 GFLOPs, respectively; meanwhile, ECA-Net50 outperforms ResNet-50 by 2.28% in terms of Top-1 accuracy.
Table 1 summarizes existing attention modules in terms
of whether channel dimensionality reduction (DR), crosschannel interaction and lightweight model, where we can
see that our ECA module learn effective channel attention
by avoiding channel dimensionality reduction while capturing cross-channel interaction in an extremely lightweight
way. To evaluate our method, we conduct experiments on
ImageNet-1K and MS COCO in a variety of tasks
using different deep CNN architectures.
The contributions of this paper are summarized as follows. (1) We dissect the SE block and empirically demonstrate avoiding dimensionality reduction and appropriate
cross-channel interaction are important to learn effective
and efﬁcient channel attention, respectively. (2) Based on
above analysis, we make an attempt to develop an extremely
lightweight channel attention module for deep CNNs by
proposing an Efﬁcient Channel Attention (ECA), which increases little model complexity while bringing clear improvement. (3) The experimental results on ImageNet-1K
and MS COCO demonstrate our method has lower model
complexity than state-of-the-arts while achieving very competitive performance.
2. Related Work
Attention mechanism has proven to be a potential means
to enhance deep CNNs. SE-Net presents for the ﬁrst
time an effective mechanism to learn channel attention and
achieves promising performance. Subsequently, development of attention modules can be roughly divided into two
directions: (1) enhancement of feature aggregation; (2)
combination of channel and spatial attentions. Speciﬁcally,
CBAM employs both average and max pooling to aggregate features. GSoP introduces a second-order pooling for more effective feature aggregation. GE explores
spatial extension using a depth-wise convolution to aggregate features. CBAM and scSE compute spatial attention using a 2D convolution of kernel size k × k,
then combine it with channel attention. Sharing similar philosophy with Non-Local (NL) neural networks , GC-
Net develops a simpliﬁed NL network and integrates
with the SE block, resulting in a lightweight module to
model long-range dependency. Double Attention Networks
(A2-Nets) introduces a novel relation function for NL
blocks for image or video recognition. Dual Attention Network (DAN) simultaneously considers NL-based channel and spatial attentions for semantic segmentation. However, most above NL-based attention modules can only be
used in a single or a few convolution blocks due to their high
model complexity. Obviously, all of the above methods focus on developing sophisticated attention modules for better
performance. Different from them, our ECA aims at learning effective channel attention with low model complexity.
Our work is also related to efﬁcient convolutions, which
are designed for lightweight CNNs. Two widely used efﬁcient convolutions are group convolutions and
depth-wise separable convolutions . As given
in Table 2, although these efﬁcient convolutions involve less
parameters, they show little effectiveness in attention module. Our ECA module aims at capturing local cross-channel
interaction, which shares some similarities with channel local convolutions and channel-wise convolutions ;
different from them, our method investigates a 1D convolution with adaptive kernel size to replace FC layers in channel attention module. Comparing with group and depthwise separable convolutions, our method achieves better
performance with lower model complexity.
3. Proposed Method
In this section, we ﬁrst revisit the channel attention module in SENet (i.e., SE block). Then, we make a empirical diagnosis of SE block by analyzing effects of dimensionality reduction and cross-channel interaction. This
motivates us to propose our ECA module. In addition, we
develop a method to adaptively determine parameter of our
ECA, and ﬁnally show how to adopt it for deep CNNs.
3.1. Revisiting Channel Attention in SE Block
Let the output of one convolution block be X
RW ×H×C, where W, H and C are width, height and channel dimension (i.e., number of ﬁlters).
Accordingly, the
weights of channels in SE block can be computed as
ω = σ(f{W1,W2}(g(X))),
where g(X) =
i=1,j=1 Xij is channel-wise global
average pooling (GAP) and σ is a Sigmoid function. Let
σ(f{W1,W2}(y))
σ(GC16(y))
σ(GCC/16(y))
σ(GCC/8(y))
σ(ω) with Eq. (7)
ECA (Ours)
σ(C1Dk(y))
Table 2. Comparison of various channel attention modules using
ResNet-50 as backbone model on ImageNet. #.Param. indicates
number of parameters of the channel attention module; ⊙indicates
element-wise product; GC and C1D indicate group convolutions
and 1D convolution, respectively; k is kernel size of C1D.
y = g(X), f{W1,W2} takes the form
f{W1,W2}(y) = W2ReLU(W1y),
where ReLU indicates the Rectiﬁed Linear Unit . To
avoid high model complexity, sizes of W1 and W2 are
set to C × ( C
r ) and ( C
r ) × C, respectively. We can see
that f{W1,W2} involves all parameters of channel attention
block. While dimensionality reduction in Eq. (2) can reduce
model complexity, it destroys the direct correspondence between channel and its weight. For example, one single FC
layer predicts weight of each channel using a linear combination of all channels. But Eq. (2) ﬁrst projects channel features into a low-dimensional space and then maps
them back, making correspondence between channel and
its weight be indirect.
3.2. Efﬁcient Channel Attention (ECA) Module
After revisiting SE block, we conduct empirical comparisons for analyzing effects of channel dimensionality reduction and cross-channel interaction on channel attention
learning. According to these analyses, we propose our efﬁcient channel attention (ECA) module.
Avoiding Dimensionality Reduction
As discussed above, dimensionality reduction in Eq. (2)
makes correspondence between channel and its weight be
indirect. To verify its effect, we compare the original SE
block with its three variants (i.e., SE-Var1, SE-Var2 and SE-
Var3), all of which do not perform dimensionality reduction. As presented in Table 2, SE-Var1 with no parameter
is still superior to the original network, indicating channel
attention has ability to improve performance of deep CNNs.
Meanwhile, SE-Var2 learns the weight of each channel independently, which is slightly superior to SE block while
involving less parameters. It may suggest that channel and
its weight needs a direct correspondence while avoiding dimensionality reduction is more important than consideration of nonlinear channel dependencies. Additionally, SE-
Var3 employing one single FC layer performs better than
two FC layers with dimensionality reduction in SE block.
All of above results clearly demonstrate avoiding dimensionality reduction is helpful to learn effective channel attention. Therefore, we develop our ECA module without
channel dimensionality reduction.
Local Cross-Channel Interaction
Given the aggregated feature y ∈RC without dimensionality reduction, channel attention can be learned by
ω = σ(Wy),
where W is a C × C parameter matrix. In particular, for
SE-Var2 and SE-Var3 we have
where Wvar2 for SE-Var2 is a diagonal matrix, involving C
parameters; Wvar3 for SE-Var3 is a full matrix, involving
C×C parameters. As shown in Eq. (4), the key difference is
that SE-Var3 considers cross-channel interaction while SE-
Var2 does not, and consequently SE-Var3 achieves better
performance. This result indicates that cross-channel interaction is beneﬁcial to learn channel attention. However, SE-
Var3 requires a mass of parameters, leading to high model
complexity, especially for large channel numbers.
A possible compromise between SE-Var2 and SE-Var3
is extension of Wvar2 to a block diagonal matrix, i.e.,
where Eq. (5) divides channel into G groups each of which
includes C/G channels, and learns channel attention in each
group independently, which captures cross-channel interaction in a local manner. Accordingly, it involves C2/G parameters. From perspective of convolution, SE-Var2, SE-
Var3 and Eq. (5) can be regarded as a depth-wise separable convolution, a FC layer and group convolutions, respectively. Here, SE block with group convolutions (SE-GC) is
indicated by σ(GCG(y)) = σ(WGy). However, as shown
in , excessive group convolutions will increase memory
access cost and so decrease computational efﬁciency. Furthermore, as shown in Table 2, SE-GC with varying groups
bring no gain over SE-Var2, indicating it is not an effective
scheme to capture local cross-channel interaction. The reason may be that SE-GC completely discards dependences
among different groups.
In this paper, we explore another method to capture local
cross-channel interaction, aiming at guaranteeing both ef-
ﬁciency and effectiveness. Speciﬁcally, we employ a band
matrix Wk to learn channel attention, and Wk has
Clearly, Wk in Eq. (6) involves k × C parameters, which
is usually less than those of Eq. (5). Furthermore, Eq. (6)
avoids complete independence among different groups in
Eq. (5). As compared in Table 2, the method in Eq. (6)
(namely ECA-NS) outperforms SE-GC of Eq. (5). As for
Eq. (6), the weight of yi is calculated by only considering
interaction between yi and its k neighbors, i.e.,
i indicates the set of k adjacent channels of yi.
A more efﬁcient way is to make all channels share the
same learning parameters, i.e.,
Note that such strategy can be readily implemented by a fast
1D convolution with kernel size of k, i.e.,
ω = σ(C1Dk(y)),
where C1D indicates 1D convolution. Here, the method in
Eq. (9) is called by efﬁcient channel attention (ECA) module, which only involves k parameters. As presented in Table 2, our ECA module with k = 3 achieves similar results
with SE-var3 while having much lower model complexity,
which guarantees both efﬁciency and effectiveness by appropriately capturing local cross-channel interaction.
Coverage of Local Cross-Channel Interaction
Since our ECA module (9) aims at appropriately capturing
local cross-channel interaction, so the coverage of interaction (i.e., kernel size k of 1D convolution) needs to be determined. The optimized coverage of interaction could be
tuned manually for convolution blocks with different channel numbers in various CNN architectures. However, manual tuning via cross-validation will cost a lot of computing resources. Group convolutions have been successfully
Figure 3. PyTorch code of our ECA module.
adopted to improve CNN architectures , where
high-dimensional (low-dimensional) channels involve long
range (short range) convolutions given the ﬁxed number of
groups. Sharing the similar philosophy, it is reasonable that
the coverage of interaction (i.e., kernel size k of 1D convolution) is proportional to channel dimension C. In other
words, there may exist a mapping φ between k and C:
The simplest mapping is a linear function, i.e., φ(k) =
γ ∗k −b. However, the relations characterized by linear
function are too limited. On the other hand, it is well known
that channel dimension C (i.e., number of ﬁlters) usually is
set to power of 2. Therefore, we introduce a possible solution by extending the linear function φ(k) = γ ∗k −b to a
non-linear one, i.e.,
C = φ(k) = 2(γ∗k−b).
Then, given channel dimension C, kernel size k can be
adaptively determined by
k = ψ(C) =
where |t|odd indicates the nearest odd number of t. In this
paper, we set γ and b to 2 and 1 throughout all the experiments, respectively. Clearly, through the mapping ψ, highdimensional channels have longer range interaction while
low-dimensional ones undergo shorter range interaction by
using a non-linear mapping.
3.3. ECA Module for Deep CNNs
Figure 2 illustrates the overview of our ECA module. After aggregating convolution features using GAP without dimensionality reduction, ECA module ﬁrst adaptively determines kernel size k, and then performs 1D convolution followed by a Sigmoid function to learn channel attention. For
applying our ECA to deep CNNs, we replace SE block by
our ECA module following the same conﬁguration in .
The resulting networks are named by ECA-Net. Figure 3
gives PyTorch code of our ECA.
4. Experiments
In this section, we evaluate the proposed method
on large-scale image classiﬁcation, object detection and
segmentation
COCO , respectively. Speciﬁcally, we ﬁrst assess the
effect of kernel size on our ECA module, and compare with
state-of-the-art counterparts on ImageNet. Then, we verify the effectiveness of our ECA-Net on MS COCO using
Faster R-CNN , Mask R-CNN and RetinaNet .
4.1. Implementation Details
To evaluate our ECA-Net on ImageNet classiﬁcation, we
employ four widely used CNNs as backbone models, including ResNet-50 , ResNet-101 , ResNet-512 
and MobileNetV2 .
For training ResNets with our
ECA, we adopt exactly the same data augmentation and
hyper-parameter settings in . Speciﬁcally, the input
images are randomly cropped to 224×224 with random horizontal ﬂipping. The parameters of networks are optimized
by stochastic gradient descent (SGD) with weight decay of
1e-4, momentum of 0.9 and mini-batch size of 256. All
models are trained within 100 epochs by setting the initial
learning rate to 0.1, which is decreased by a factor of 10
per 30 epochs. For training MobileNetV2 with our ECA,
we follow the settings in , where networks are trained
within 400 epochs using SGD with weight decay of 4e-5,
momentum of 0.9 and mini-batch size of 96. The initial
learning rate is set to 0.045, and is decreased by a linear
decay rate of 0.98. For testing on the validation set, the
shorter side of an input image is ﬁrst resized to 256 and a
center crop of 224 × 224 is used for evaluation. All models
are implemented by PyTorch toolkit1.
We further evaluate our method on MS COCO using
Faster R-CNN , Mask R-CNN and RetinaNet ,
where ResNet-50 and ResNet-101 along with FPN are
used as backbone models. We implement all detectors by
using MMDetection toolkit and employ the default settings. Speciﬁcally, the shorter side of input images are resized to 800, then all models are optimized using SGD with
weight decay of 1e-4, momentum of 0.9 and mini-batch size
of 8 (4 GPUs with 2 images per GPU). The learning rate
is initialized to 0.01 and is decreased by a factor of 10 after 8 and 11 epochs, respectively. We train all detectors
within 12 epochs on train2017 of COCO and report the results on val2017 for comparison. All programs run on a
PC equipped with four RTX 2080Ti GPUs and an Intel(R)
1 
Backbone Models
ResNet 
SENet 
A2-Nets †
GSoP-Net1 
AA-Net †,♦
ECA-Net (Ours)
ResNet 
ResNet-101
SENet 
AA-Net †,♦
ECA-Net (Ours)
ResNet 
ResNet-152
SENet 
ECA-Net (Ours)
MobileNetV2 
MobileNetV2
ECA-Net (Ours)
Table 3. Comparison of different attention methods on ImageNet in terms of network parameters (#.Param.), ﬂoating point operations per
second (FLOPs), training or inference speed (frame per second, FPS), and Top-1/Top-5 accuracy (in %). †: Since the source code and
models of A2-Nets and AA-Net are publicly unavailable, we do not compare their running time. ♦: AA-Net is trained with Inception data
augmentation and different setting of learning rates.
Xeon Silver 4112 .
4.2. Image Classiﬁcation on ImageNet-1K
Here, we ﬁrst assess the effect of kernel size on our ECA
module and verify the effectiveness of our method to adaptively determine kernel size, then we compare with stateof-the-art counterparts and CNN models using ResNet-50,
ResNet-101, ResNet-152 and MobileNetV2.
Effect of Kernel Size (k) on ECA Module
As shown in Eq. (9), our ECA module involves a parameter k, i.e., kernel size of 1D convolution. In this part, we
evaluate its effect on our ECA module and validate the effectiveness of our method for adaptive selection of kernel
size. To this end, we employ ResNet-50 and ResNet-101
as backbone models, and train them with our ECA module
by setting k be from 3 to 9. The results are illustrated in
Figure 4, from it we have the following observations.
Firstly, when k is ﬁxed in all convolution blocks, ECA
module obtains the best results at k = 9 and k = 5 for
ResNet-50 and ResNet-101, respectively. Since ResNet-
101 has more intermediate layers that dominate performance of ResNet-101, it may prefer to small kernel size.
Besides, these results show that different deep CNNs have
various optimal k, and k has a clear effect on performance
of ECA-Net. Furthermore, accuracy ﬂuctuations (∼0.5%)
of ResNet-101 are larger than those (∼0.15%) of ResNet-
50, and we conjecture the reason is that the deeper net-
Number of k
Top-1 accuracy
ECA-Net101
ECA50-Adaptive
ECA101-Adaptive
Figure 4. Results of our ECA module with various numbers of k
using ResNet-50 and ResNet-101 as backbone models. Here, we
also give the results of ECA module with adaptive selection of
kernel size and compare with SENet as baseline.
works are more sensitive to the ﬁxed kernel size than the
shallower ones. Additionally, kernel size that is adaptively
determined by Eq. (12) usually outperforms the ﬁxed ones,
while it can avoid manual tuning of parameter k via crossvalidation. Above results demonstrate the effectiveness of
our adaptive kernel size selection in attaining better and stable results. Finally, ECA module with various numbers of
k consistently outperform SE block, verifying that avoiding dimensionality reduction and local cross-channel interaction have positive effects on learning channel attention.
Comparisons Using Different Deep CNNs
ResNet-50 We compare our ECA module with several
state-of-the-art attention methods using ResNet-50 on ImageNet, including SENet , CBAM , A2-Nets ,
AA-Net , GSoP-Net1 and GCNet . The evaluation metrics include both efﬁciency (i.e., network parameters, ﬂoating point operations per second (FLOPs) and
training/inference speed) and effectiveness (i.e., Top-1/Top-
5 accuracy). For comparison, we duplicate the results of
ResNet and SENet from , and report the results of other
compared methods in their original papers. To test training/inference speed of various models, we employ publicly
available models of the compared CNNs, and run them on
the same computing platform. The results are given in Table 3, where we can see that our ECA-Net shares almost the
same model complexity (i.e., network parameters, FLOPs
and speed) with the original ResNet-50, while achieving
2.28% gains in Top-1 accuracy. Comparing with state-ofthe-art counterparts (i.e., SENet, CBAM, A2-Nets, AA-Net,
GSoP-Net1 and GCNet), ECA-Net obtains better or competitive results while beneﬁting lower model complexity.
ResNet-101 Using ResNet-101 as backbone model, we
compare our ECA-Net with SENet , CBAM and
AA-Net . From Table 3 we can see that ECA-Net outperforms the original ResNet-101 by 1.8% with almost
the same model complexity.
Sharing the same tendency
on ResNet-50, ECA-Net is superior to SENet and CBAM
while it is very competitive to AA-Net with lower model
complexity. Note that AA-Net is trained with Inception data
augmentation and different setting of learning rates.
ResNet-152 Using ResNet-152 as backbone model, we
compare our ECA-Net with SENet . From Table 3 we
can see that ECA-Net improves the original ResNet-152
over about 1.3% in terms of Top-1 accuracy with almost
the same model complexity. Comparing with SENet, ECA-
Net achieves 0.5% gain in terms of Top-1 with lower model
complexity. The results with respect to ResNet-50, ResNet-
101 and ResNet-152 demonstrate the effectiveness of our
ECA module on the widely used ResNet architectures.
MobileNetV2 Besides ResNet architectures, we also verify
the effectiveness of our ECA module on lightweight CNN
architectures. To this end, we employ MobileNetV2 
as backbone model and compare our ECA module with SE
block. In particular, we integrate SE block and ECA module in convolution layer before residual connection lying in
each ’bottleneck’ of MobileNetV2, and parameter r of SE
block is set to 8. All models are trained using exactly the
same settings. The results in Table 3 show our ECA-Net improves the original MobileNetV2 and SENet by about 0.9%
and 0.14% in terms of Top-1 accuracy, respectively. Furthermore, our ECA-Net has smaller model size and faster
training/inference speed than SENet. Above results verify
the efﬁciency and effectiveness of our ECA module again.
CNN Models
ResNet-200
Inception-v3
ResNeXt-101
DenseNet-264 (k=32)
DenseNet-161 (k=48)
ECA-Net50 (Ours)
ECA-Net101 (Ours)
Table 4. Comparisons with state-of-the-art CNNs on ImageNet.
Comparisons with Other CNN Models
At the end of this part, we compare our ECA-Net50 and
ECA-Net101 with other state-of-the-art CNN models, including ResNet-200 , Inception-v3 , ResNeXt ,
DenseNet . These CNN models have deeper and wider
architectures, and their results all are copied from the original papers. As presented in Table 4, ECA-Net101 outperforms ResNet-200, indicating that our ECA-Net can improve the performance of deep CNNs using much less computational cost. Meanwhile, our ECA-Net101 is very competitive to ResNeXt-101, while the latter one employs more
convolution ﬁlters and expensive group convolutions. In addition, ECA-Net50 is comparable to DenseNet-264 (k=32),
DenseNet-161 (k=48) and Inception-v3, but it has lower
model complexity. All above results demonstrate that our
ECA-Net performs favorably against state-of-the-art CNNs
while beneﬁting much lower model complexity. Note that
our ECA also has great potential to further improve the performance of the compared CNN models.
4.3. Object Detection on MS COCO
In this subsection, we evaluate our ECA-Net on object
detection task using Faster R-CNN , Mask R-CNN 
and RetinaNet . We mainly compare ECA-Net with
ResNet and SENet. All CNN models are pre-trained on ImageNet, then are transferred to MS COCO by ﬁne-tuning.
Comparisons Using Faster R-CNN
Using Faster R-CNN as the basic detector, we employ
ResNets of 50 and 101 layers along with FPN as backbone models. As shown in Table 5, integration of either
SE block or our ECA module can improve performance of
object detection by a clear margin. Meanwhile, our ECA
outperforms SE block by 0.3% and 0.7% in terms of AP
using ResNet-50 and ResNet-101, respectively.
Comparisons Using Mask R-CNN
We further exploit Mask R-CNN to verify the effectiveness
of our ECA-Net on object detection task. As shown in Table 5, our ECA module is superior to the original ResNet
by 1.8% and 1.9% in terms of AP under the settings of
Faster R-CNN
+ SE block
+ ECA (Ours)
ResNet-101
+ SE block
+ ECA (Ours)
Mask R-CNN
+ SE block
+ GC block
+ ECA (Ours)
ResNet-101
+ SE block
+ ECA (Ours)
+ SE block
+ ECA (Ours)
ResNet-101
+ SE block
+ ECA (Ours)
Table 5. Object detection results of different methods on COCO val2017.
50 and 101 layers, respectively. Meanwhile, ECA module
achieves 0.3% and 0.6% gains over SE block using ResNet-
50 and ResNet-101 as backbone models, respectively. Using ResNet-50, ECA is superior to one NL , and is comparable to GC block using lower model complexity.
Comparisons Using RetinaNet
Additionally, we verify the effectiveness of our ECA-Net
on object detection using one-stage detector, i.e., RetinaNet.
As compared in Table 5, our ECA-Net outperforms the original ResNet by 1.8% and 1.4% in terms of AP for the networks of 50 and 101 layers, respectively. Meanwhile, ECA-
Net improves SE-Net over 0.2% and 0.4% for ResNet-50
and ResNet-101, respectively. In summary, the results in
Table 5 demonstrate that our ECA-Net can well generalize
to object detection task. Speciﬁcally, ECA module brings
clear improvement over the original ResNet, while outperforming SE block using lower model complexity. In particular, our ECA module achieves more gains for small objects, which are usually more difﬁcult to be detected.
4.4. Instance Segmentation on MS COCO
Then, we give instance segmentation results of our ECA
module using Mask R-CNN on MS COCO. As compared in
Table 6, ECA module achieves notable gain over the original ResNet while performing better than SE block with less
model complexity. For ResNet-50 as backbone, ECA with
lower model complexity is superior one NL , and is
comparable to GC block . These results verify our ECA
module has good generalization ability for various tasks.
+ SE block
+ GC block
+ ECA (Ours)
ResNet-101
+ SE block
+ ECA (Ours)
Table 6. Instance segmentation results of different methods using
Mask R-CNN on COCO val2017.
5. Conclusion
In this paper, we focus on learning effective channel attention for deep CNNs with low model complexity.
this end, we propose an efﬁcient channel attention (ECA)
module, which generates channel attention through a fast
1D convolution, whose kernel size can be adaptively determined by a non-linear mapping of channel dimension.
Experimental results demonstrate our ECA is an extremely
lightweight plug-and-play block to improve the performance of various deep CNN architectures, including the
widely used ResNets and lightweight MobileNetV2. Moreover, our ECA-Net exhibits good generalization ability in
object detection and instance segmentation tasks. In future,
we will apply our ECA module to more CNN architectures
(e.g., ResNeXt and Inception ) and further investigate
incorporation of ECA with spatial attention module.