Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6193–6202,
November 16–20, 2020. c⃝2020 Association for Computational Linguistics
BERT-ATTACK: Adversarial Attack Against BERT Using BERT
Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, Xipeng Qiu∗
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{linyangli19,rtma19,qpguo16,xyxue,xpqiu}@fudan.edu.cn
Adversarial attacks for discrete data (such as
texts) have been proved signiﬁcantly more
challenging than continuous data (such as images) since it is difﬁcult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually
adopt heuristic replacement strategies on the
character or word level, which remains challenging to ﬁnd the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency
and language ﬂuency. In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples
using pre-trained masked language models exempliﬁed by BERT. We turn BERT against its
ﬁne-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-theart attack strategies in both success rate and
perturb percentage, while the generated adversarial samples are ﬂuent and semantically preserved. Also, the cost of calculation is low,
thus possible for large-scale generations. The
code is available at 
LinyangLee/BERT-Attack.
Introduction
Despite the success of deep learning, recent works
have found that these neural networks are vulnerable to adversarial samples, which are crafted with
small perturbations to the original inputs . That is, these adversarial samples are
imperceptible to human judges while they can mislead the neural networks to incorrect predictions.
Therefore, it is essential to explore these adversarial attack methods since the ultimate goal is to
make sure the neural networks are highly reliable
∗Corresponding author.
and robust. While in computer vision ﬁelds, both
attack strategies and their defense countermeasures
are well-explored , the
adversarial attack for text is still challenging due
to the discrete nature of languages. Generating of
adversarial samples for texts needs to possess such
qualities: (1) imperceptible to human judges yet
misleading to neural models; (2) ﬂuent in grammar
and semantically consistent with original inputs.
Previous methods craft adversarial samples
mainly based on speciﬁc rules .
Therefore, these methods are difﬁcult to guarantee the ﬂuency and semantically preservation in
the generated adversarial samples at the same time.
Plus, these manual craft methods are rather complicated. They use multiple linguistic constraints
like NER tagging or POS tagging. Introducing
contextualized language models to serve as an automatic perturbation generator could make these
rules designing much easier.
The recent rise of pre-trained language models,
such as BERT , push the performances of NLP tasks to a new level. On the one
hand, the powerful ability of a ﬁne-tuned BERT
on downstream tasks makes it more challenging to
be adversarial attacked . On the
other hand, BERT is a pre-trained masked language
model on extremely large-scale unsupervised data
and has learned general-purpose language knowledge. Therefore, BERT has the potential to generate more ﬂuent and semantic-consistent substitutions for an input text. Naturally, both the properties of BERT motivate us to explore the possibility
of attacking a ﬁne-tuned BERT with another BERT
as the attacker.
In this paper, we propose an effective and
high-quality adversarial sample generation method:
BERT-Attack, using BERT as a language model
to generate adversarial samples. The core algorithm of BERT-Attack is straightforward and consists of two stages: ﬁnding the vulnerable words
in one given input sequence for the target model;
then applying BERT in a semantic-preserving way
to generate substitutes for the vulnerable words.
With the ability of BERT, the perturbations are
generated considering the context around. Therefore, the perturbations are ﬂuent and reasonable.
We use the masked language model as a perturbation generator and ﬁnd perturbations that maximize
the risk of making wrong predictions . Differently from previous attacking
strategies that require traditional single-direction
language models as a constraint, we only need to inference the language model once as a perturbation
generator rather than repeatedly using language
models to score the generated adversarial samples
in a trial and error process.
Experimental results show that the proposed
BERT-Attack method successfully fooled its ﬁnetuned downstream model with the highest attack
success rate compared with previous methods.
Meanwhile, the perturb percentage and the query
number are considerably lower, while the semantic
preservation is high.
To summarize our main contributions:
• We propose a simple and effective method,
named BERT-Attack, to effectively generate
ﬂuent and semantically-preserved adversarial
samples that can successfully mislead stateof-the-art models in NLP, such as ﬁne-tuned
BERT for various downstream tasks.
• BERT-Attack has a higher attacking success
rate and a lower perturb percentage with fewer
access numbers to the target model compared
with previous attacking algorithms, while
does not require extra scoring models therefore extremely effective.
Related Work
To explore the robustness of neural networks, adversarial attacks have been extensively studied for continuous data (such as images) . The key idea is to ﬁnd a minimal perturbation that maximizes the risk of making wrong
predictions. This minimax problem can be easily achieved by applying gradient descent over the
continuous space of images .
However, adversarial attack for discrete data such
as text remains challenging.
Adversarial Attack for Text
Current successful attacks for text usually adopt
heuristic rules to modify the characters of a word
 , and substituting words with synonyms . Li et al. ; Gao
et al. apply perturbations based on word embeddings such as Glove ,
which is not strictly semantically and grammatically coordinated. Alzantot et al. adopts language models to score the perturbations generated
by searching for close meaning words in the word
embedding space , using a trial
and error process to ﬁnd possible perturbations, yet
the perturbations generated are still not contextaware and heavily rely on cosine similarity measurement of word embeddings. Glove embeddings
do not guarantee similar vector space with cosine
similarity distance, therefore the perturbations are
less semantically consistent. Jin et al. apply
a semantically enhanced embedding , which is context unaware, thus less consistent with the unperturbed inputs. Liang et al. 
use phrase-level insertion and deletion, which produces unnatural sentences inconsistent with the
original inputs, lacking ﬂuency control. To preserve semantic information, Glockner et al. 
replace words manually to break the language inference system . Jia and
Liang propose manual craft methods to attack machine reading comprehension systems. Lei
et al. introduce replacement strategies using
embedding transition.
Although the above approaches have achieved
good results, there is still much room for improvement regarding the perturbed percentage, attacking
success rate, grammatical correctness and semantic
consistency, etc. Moreover, the substitution strategies of these approaches are usually non-trivial,
resulting in that they are limited to speciﬁc tasks.
Adversarial Attack against BERT
Pre-trained language models have become mainstream for many NLP tasks. Works such as 
have explored these pre-trained language models
from many different angles. Wallace et al. 
explored the possible ethical problems of learned
knowledge in pre-trained models.
BERT-Attack
Motivated by the interesting idea of turning BERT
against BERT, we propose BERT-Attack, using
the original BERT model to craft adversarial samples to fool the ﬁne-tuned BERT model.
Our method consists of two steps: (1) ﬁnding
the vulnerable words for the target model and then
(2) replacing them with the semantically similar
and grammatically correct words until a successful
The most-vulnerable words are the keywords
that help the target model make judgments. Perturbations over these words can be most beneﬁcial in
crafting adversarial samples. After ﬁnding which
words that we are aimed to replace, we use masked
language models to generate perturbations based
on the top-K predictions from the masked language
Finding Vulnerable Words
Under the black-box scenario, the logit output by
the target model (ﬁne-tuned BERT or other neural
models) is the only supervision we can get. We
ﬁrst select the words in the sequence which have a
high signiﬁcance inﬂuence on the ﬁnal output logit.
Let S = [w0, · · · , wi · · · ] denote the input sentence, and oy(S) denote the logit output by the
target model for correct label y, the importance
score Iwi is deﬁned as
Iwi = oy(S) −oy(S\wi),
where S\wi = [w0, · · · , wi−1, [MASK], wi+1, · · · ]
is the sentence after replacing wi with [MASK].
Then we rank all the words according to the
ranking score Iwi in descending order to create
word list L. We only take ϵ percent of the most important words since we tend to keep perturbations
This process maximizes the risk of making
wrong predictions, which is previously done by calculating gradients in image domains. The problem
is then formulated as replacing these most vulnerable words with semantically consistent perturbations.
Word Replacement via BERT
After ﬁnding the vulnerable words, we iteratively
replace the words in list L one by one to ﬁnd perturbations that can mislead the target model. Previous approaches usually use multiple human-crafted
subword of wi
Full-Permutation
of top-K predictions
Target model
Generated Sample
Figure 1: One step of our replacement strategy.
rules to ensure the generated example is semantically consistent with the original one and grammatically correct, such as a synonym dictionary
 , POS checker ,
semantic similarity checker , etc.
Alzantot et al. applies a traditional language
model to score the perturbed sentence at every attempt of replacing a word.
These strategies of generating substitutes are unaware of the context between the substitution positions (usually using language models to test the
substitutions), thus are insufﬁcient in ﬂuency control and semantic consistency. More importantly,
using language models or POS checkers in scoring
the perturbed samples is costly since this trial and
error process requires massive inference time.
To overcome the lack of ﬂuency control and semantic preservation by using synonyms or similar words in the embedding space, we leverage
BERT for word replacement.
The genuine nature of the masked language model makes sure
that the generated sentences are relatively ﬂuent
and grammar-correct, also preserve most semantic
information, which is later conﬁrmed by human
evaluators. Further, compared with previous approaches using rule-based perturbation strategies,
the masked language model prediction is contextaware, thus dynamically searches for perturbations
rather than simple synonyms replacing.
Different from previous methods using complicated strategies to score and constrain the perturbations, the contextualized perturbation generator
generates minimal perturbations with only one forward pass. Without running additional neural models to score the sentence, the time-consuming part
is accessing the target model only. Therefore the
process is extremely efﬁcient.
Algorithm 1 BERT-Attack
1: procedure WORD IMPORTANCE RANKING
S = [w0, w1, · · · ] // input: tokenized sentence
Y ←gold-label
for wi in S do
calculate importance score Iwi using Eq. 1
select word list L = [wtop−1, wtop−2, · · · ]
// sort S using Iwi in descending order and collect top −K words
8: procedure REPLACEMENT USING BERT
H = [h0, · · · , hn] // sub-word tokenized sequence of S
generate top-K candidates for all sub-words using BERT and get P ∈n×K
for wj in L do
if wj is a whole word then
get candidate C = Filter(P j)
replace word wj
get candidate C using PPL ranking and Filter
replace sub-words [hj, · · · , hj+t]
Find Possible Adversarial Sample
for ck in C do
′ = [w0, · · · , wj−1, ck, · · · ] // attempt
if argmax(oy(S
′))! = Y then
return Sadv = S
′ // success attack
′) < oy(Sadv) then
Sadv = [w0, · · · , wj−1, c, · · · ] // do one perturbation
return None
Thus, using the masked language model as a
contextualized perturbation generator can be one
possible solution to craft high-quality adversarial
samples efﬁciently.
Word Replacement Strategy
As seen in Figure 1, given a chosen word w to
be replaced, we apply BERT to predict the possible words that are similar to w yet can mislead
the target model. Instead of following the masked
language model settings, we do not mask the chosen word w and use the original sequence as input,
which can generate more semantic-consistent substitutes . For instance, given a
sequence ”I like the cat.”, if we mask the word cat,
it would be very hard for a masked language model
to predict the original word cat since it could be
just as ﬂuent if the sequence is ”I like the dog.”.
Further, if we mask out the given word w, for each
iteration we would have to rerun the masked language model prediction process which is costly.
Since BERT uses Bytes-Pair-Encoding (BPE)
to tokenize the sequence S = [w0, · · · , wi, · · · ]
into sub-word tokens: H = [h0, h1, h2, · · · ], we
need to align the chosen word to its corresponding
sub-words in BERT.
Let M denote the BERT model, we feed the
tokenized sequence H into the BERT M to get
output prediction P = M(H). Instead of using
the argmax prediction, we take the most possible
K predictions at each position, where K is a hyperparameter.
We iterate words that are sorted by word importance ranking process to ﬁnd perturbations. The
BERT model uses BPE encoding to construct vocabularies. While most words are still single words,
rare words are tokenized into sub-words. Therefore,
we treat single words and sub-words separately to
generate the substitutes.
Single words
For a single word wj, we make
attempts using the corresponding top-K prediction candidates P j. We ﬁrst ﬁlter out stop words
collected from NLTK; for sentiment classiﬁca-
tion tasks we ﬁlter out antonyms using synonym
dictionaries since BERT
masked language model does not distinguish synonyms and antonyms.
Then for given candidate ck we construct a perturbed sequence H
[h0, · · · , hj−1, ck, hj+1 · · · ]. If the target model is
already fooled to predict incorrectly, we break the
loop to obtain the ﬁnal adversarial sample Hadv;
otherwise, we select from the ﬁltered candidates
to pick one best perturbation and turn to the next
word in word list L.
For a word that is tokenized into subwords in BERT, we cannot obtain its substitutes
directly. Thus we use the perplexity of sub-word
combinations to ﬁnd suitable word substitutes from
predictions in the sub-word level. Given sub-words
[h0, h1, · · · , ht] of word w, we list all possible
combinations from the prediction P ∈t×K from M,
which is Kt sub-word combinations, we can convert them back to normal words by reversing the
BERT tokenization process. We feed these combinations into the BERT-MLM to get the perplexity
of these combinations. Then we rank the perplexity
of all combinations to get the top-K combinations
to ﬁnd the suitable sub-word combinations.
Given the suitable perturbations, we replace the
original word with the most likely perturbation and
repeat this process by iterating the importance word
ranking list to ﬁnd the ﬁnal adversarial sample.
In this way, we acquire the adversarial samples
Sadv effectively since we only iterate the masked
language model once and do perturbations using
the masked language model without other checking
strategies.
We summarize the two-step BERT-Attack process in Algorithm 1.
Experiments
We apply our method to attack different types of
NLP tasks in the form of text classiﬁcation and
natural language inference. Following Jin et al.
 , we evaluate our method on 1k test samples
randomly selected from the test set of the given task
which are the same splits used by Alzantot et al.
 ; Jin et al. . The GA method only uses
a subset of 50 samples in the FAKE, IMDB dataset.
Text Classiﬁcation
We use different types of text
classiﬁcation tasks to study the effectiveness of our
• Yelp Review classiﬁcation dataset, containing.
Following Zhang et al. , we process the
dataset to construct a polarity classiﬁcation
• IMDB Document-level movie review dataset,
where the average sequence length is longer
than the Yelp dataset. We process the dataset
into a polarity classiﬁcation task 1.
• AG’s News Sentence level news-type classi-
ﬁcation dataset, containing 4 types of news:
World, Sports, Business, and Science.
• FAKE Fake News Classiﬁcation dataset, detecting whether a news document is fake from
Kaggle Fake News Challenge 2.
Natural Language Inference
• SNLI Stanford language inference task . Given one premise and one
hypothesis, and the goal is to predict if the hypothesis is entailment, neural, or contradiction
of the premise.
• MNLI Language inference dataset on multigenre texts, covering transcribed speech, popular ﬁction, and government reports , which is more complicated with
diversiﬁed written and spoken style texts, compared with the SNLI dataset, including eval
data matched with training domains and eval
data mismatched with training domains.
Automatic Evaluation Metrics
To measure the quality of the generated samples,
we set up various automatic evaluation metrics.
The success rate, which is the counter-part of afterattack accuracy, is the core metric measuring the
success of the attacking method. Meanwhile, the
perturbed percentage is also crucial since, generally, less perturbation results in more semantic
consistency. Further, under the black-box setting,
queries of the target model are the only accessible
information. Constant queries for one sample is
less applicable. Thus query number per sample
is also a key metric. As used in TextFooler , we also use Universal Sentence Encoder to measure the semantic
consistency between the adversarial sample and the
original sequence. To balance between semantic
preservation and attack success rate, we set up a
threshold of semantic similarity score to ﬁlter the
less similar examples.
1 
2 
Original Acc Attacked Acc Perturb % Query Number Avg Len Semantic Sim
BERT-Attack(ours)
TextFooler 
GA 
BERT-Attack(ours)
TextFooler
BERT-Attack(ours)
TextFooler
BERT-Attack(ours)
TextFooler
BERT-Attack(ours)
TextFooler
BERT-Attack(ours)
TextFooler
BERT-Attack(ours)
mismatched
TextFooler
Table 1: Results of attacking against various ﬁne-tuned BERT models. TextFooler is the state-of-the-art baseline.
For MNLI task, we attack the hypothesis(H) or premises(P) separately.
Attacking Results
As shown in Table 1, the BERT-Attack method successfully fool its downstream ﬁne-tuned model. In
both text classiﬁcation and natural language inference tasks, the ﬁne-tuned BERTs fail to classify
the generated adversarial samples correctly.
The average after-attack accuracy is lower than
10%, indicating that most samples are successfully
perturbed to fool the state-of-the-art classiﬁcation
models. Meanwhile, the perturb percentage is less
than 10 %, which is signiﬁcantly less than previous
Further, BERT-Attack successfully attacked all
tasks listed, which are in diversiﬁed domains such
as News classiﬁcation, review classiﬁcation, language inference in different domains. The results
indicate that the attacking method is robust in different tasks. Compared with the strong baseline
introduced by Jin et al. 3 and Alzantot et al.
 4, the BERT-Attack method is more efﬁcient
3 
4 
and more imperceptible. The query number and the
perturbation percentage of our method are much
We can observe that it is generally easier to attack the review classiﬁcation task since the perturb
percentage is incredibly low. BERT-Attack can
mislead the target model by replacing a handful of
words only. Since the average sequence length is
relatively long, the target model tends to make judgments by only a few words in a sequence, which is
not the natural way of human prediction. Thus, the
perturbation of these keywords would result in incorrect prediction from the target model, revealing
the vulnerability of it.
Human Evaluations
For further evaluation of the generated adversarial
samples, we set up human evaluations to measure
the quality of the generated samples in ﬂuency and
grammar as well as semantic preservation.
We ask human judges to score the grammar correctness of the mixed sentences of generated ad-
versarial samples and original sequences, scoring
from 1-5 following Jin et al. . Then we ask
human judges to make predictions in a shufﬂed mix
of original and adversarial texts. We use the IMDB
dataset and the MNLI dataset, and for each task, we
select 100 samples of both original and adversarial
samples for human judges. We ask three human
annotators to evaluate the examples. For label prediction, we take the majority class as the predicted
label, and for semantic and grammar check we use
an average score among the annotators.
Seen in Table 2, the semantic score and the grammar score of the adversarial samples are close to
the original ones. MNLI task is a sentence pair
prediction task constructed by human crafted hypotheses based on the premises, therefore original
pairs share a considerable amount of same words.
Perturbations on these words would make it difﬁcult for human judges to predict correctly therefore
the accuracy is lower than simple sentence classiﬁcation tasks.
Accuracy Semantic Grammar
Adversarial
Adversarial
Table 2: Human-Evaluation Results.
BERT-Attack against Other Models
The BERT-Attack method is also applicable in
attacking other target models, not limited to its
ﬁne-tuned model only. As seen in Table 3, the
attack is successful against LSTM-based models,
indicating that BERT-Attack is feasible for a wide
range of models. Under BERT-Attack, the ESIM
model is more robust in the MNLI dataset. We assume that encoding two sentences separately gets
higher robustness. In attacking BERT-large models,
the performance is also excellent, indicating that
BERT-Attack is successful in attacking different
pre-trained models not only against its own ﬁnetuned downstream models.
Ablations and Discussions
Importance of Candidate Numbers
The candidate pool range is the major hyperparameter used in the BERT-Attack algorithm. As
seen in Figure 2, the attack rate is rising along with
the candidate size increasing. Intuitively, a larger
Ori Acc Atk Acc Perturb %
IMDB Word-LSTM
BERT-Large
BERT-Large
matched BERT-Large
Table 3: BERT-Attack against other models.
Attack success rate
Figure 2: Using different candidate number K in the
attacking process.
K would result in less semantic similarity. However, the semantic measure via Universal Sentence
Encoder is maintained in a stable range, (experiments show that semantic similarities drop less than
2%), indicating that the candidates are all reasonable and semantically consistent with the original
Further, a ﬁxed candidate number could be rigid
in practical usage, so we run a test using a threshold
to cut off candidates that are less possible as a
plausible perturbation.
As seen in Table 4, when using a ﬂexible threshold to cut off unsuitable candidates, the attacking
process has a lower query number. This indicates
that some candidates predicted by the masked language model with a lower prediction score may
not be meaningful so skipping these candidates can
save the unnecessary queries.
Ori Acc Atk Acc Queries %
With Threshold
Table 4: Flexible Candidates Using a threshold to cut
off unsuitable candidates.
Importance of Sequence Length
The BERT-Attack method is based on the contextualized masked language model. Thus the sequence
length plays an important role in the high-quality
perturbation process. As seen, instead of the previous methods focusing on attacking the hypothesis
of the NLI task, we aim at premises whose average length is longer. This is because we believe
that contextual replacement would be less reasonable when dealing with extremely short sequences.
To avoid such a problem, we believe that many
word-level synonym replacement strategies can be
combined with BERT-Attack, allowing the BERT-
Attack method to be more applicable.
Ori Acc Atk Acc Perturb %
matched +Adv Train
Table 5: Adversarial training results.
LSTM BERT-base BERT-large
BERT-large
ESIM BERT-base BERT-large
BERT-large
Table 6: Transferability analysis using attacked accuracy as the evaluation metric. The column is the target
model used in attack, and the row is the tested model.
Transferability and Adversarial Training
To test the transferability of the generated adversarial samples, we take samples aimed at different
target models to attack other target models. Here,
we use BERT-base as the masked language model
for all different target models. As seen in Table
6, samples are transferable in NLI task while less
transferable in text classiﬁcation.
Meanwhile, we further ﬁne-tune the target model
using the generated adversarial samples from the
train set and then test it on the test set used before.
As seen in Table 5, generated samples used in ﬁnetuning help the target model become more robust
while accuracy is close to the model trained with
clean datasets. The attack becomes more difﬁcult,
indicating that the model is harder to be attacked.
Therefore, the generated dataset can be used as
additional data for further exploration of making
neural models more robust.
Atk Acc Perturb % Semantic
w/o sub-word
w/o sub-word
Table 7: Effects on sub-word level attack.
Effects on Sub-Word Level Attack
BPE method is currently the most efﬁcient way to
deal with a large number of words, as used in BERT.
We establish a comparative experiment where we
do not use the sub-word level attack. That is we
skip those words that are tokenized with multiple
sub-words.
As seen in Table 7, using the sub-word level
attack can achieve higher performances, not only
in higher attacking success rate but also in less
perturbation percentage.
Dataset Method Atk Acc Perturb % Semantic
matched Random
Table 8: Most Importance Ranking (MIR) vs Least Importance Ranking (LIR)
Effects on Word Importance Ranking
Word importance ranking strategy is supposed to
ﬁnd keys that are essential to NN models, which
is very much like calculating the maximum risk of
wrong predictions in the FGSM algorithm . When not using word importance ranking, the attacking algorithm is less
successful.
Runtime(s/sample)
BERT-Attack(w/o BPE)
BERT-Attack(w/ BPE)
Textfooler 
GA 
Table 9: Runtime comparison.
Some rooms have balconies .
Hypothesis
All of the rooms have balconies off of them . Contradiction
Adv Many rooms have balconies .
Hypothesis
All of the rooms have balconies off of them . Neutral
it is hard for a lover of the novel northanger abbey to sit through this bbc adaptation and to
keep from throwing objects at the tv screen... why are so many facts concerning the tilney
family and mrs . tilney ’ s death altered unnecessarily ? to make the story more ‘ horrible ? ’
it is hard for a lover of the novel northanger abbey to sit through this bbc adaptation and to
keep from throwing objects at the tv screen... why are so many facts concerning the tilney
family and mrs . tilney ’ s death altered unnecessarily ? to make the plot more ‘ horrible ? ’
i ﬁrst seen this movie in the early 80s .. it really had nice picture quality too . anyways , i ’m Positive
glad i found this movie again ... the part i loved best was when he hijacked the car from this
poor guy... this is a movie i could watch over and over again . i highly recommend it .
i ﬁrst seen this movie in the early 80s .. it really had nice picture quality too . anyways , i ’m Negative
glad i found this movie again ... the part i loved best was when he hijacked the car from this
poor guy... this is a movie i could watch over and over again . i inordinately recommend it .
Table 10: Some generated adversarial samples. Origin label is the correct prediction while label is adverse prediction. Only red color parts are perturbed. We only attack premises in MNLI task. Text in FAKE dataset and IMDB
dataset is cut to ﬁt in the table. Original text contains more than 200 words.
Runtime Comparison
Since BERT-Attack does not use language models or sentence encoders to measure the output sequence during the generation process, also, the
query number is lower, therefore the runtime is
faster than previous methods. As seen in Table
9, BERT-Attack is much faster than generic algorithm and 3 times faster then
Textfooler.
Examples of Generated Adversarial
As seen in Table 10, the generated adversarial samples are semantically consistent with its original
input, while the target model makes incorrect predictions. In both review classiﬁcation samples and
language inference samples, the perturbations do
not mislead human judges.
Conclusion
In this work, we propose a high-quality and effective method BERT-Attack to generate adversarial
samples using BERT masked language model. Experiment results show that the proposed method
achieves a high success rate while maintaining a
minimum perturbation. Nevertheless, candidates
generated from the masked language model can
sometimes be antonyms or irrelevant to the original
words, causing a semantic loss. Thus, enhancing
language models to generate more semantically related perturbations can be one possible solution to
perfect BERT-Attack in the future.
Acknowledgments
We would like to thank the anonymous reviewers for their valuable comments. We are thankful for the help of Demin Song, Hang Yan and
Pengfei Liu. This work was supported by the National Natural Science Foundation of China (No.
61751201, 62022027 and 61976056), Shanghai
Municipal Science and Technology Major Project
(No. 2018SHZDZX01) and ZJLab.