“© 2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all
other uses, in any current or future media, including reprinting/republishing this material for advertising or
promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse
of any copyrighted component of this work in other works.”
Deep Additive Least Squares Support Vector
Machines for Classiﬁcation with Model Transfer
Guanjin Wang1,2, Guangquan Zhang1, Kup-Sze Choi2, and Jie Lu1
1Centre for Artiﬁcial Intelligence, School of Software, Faculty of Engineering and Information Technology,
University of Technology Sydney, Broadway, NSW, 2007, Australia
Email: , Jie.Lu, 
2Centre for Smart Health, School of Nursing,
The Hong Kong Polytechnic University, Hong Kong, China
Email: 
Abstract—The additive kernel least squares support vector
machine (AK-LS-SVM) has been well used in classiﬁcation tasks
due to its inherent advantages. For example, additive kernels
work extremely well for some speciﬁc tasks, such as computer
vision classiﬁcation, medical research, and some specialized
scenarios. Moreover, the analytical solution using AK-LS-SVM
can formulate leave-one-out cross-validation error estimates in a
closed form for parameter tuning, which drastically reduces the
computational cost and guarantee the generalization performance
especially on small and medium datasets. However, AK-LS-SVM
still faces two main challenges: (1) improving the classiﬁcation
performance of AK-LS-SVM, and (2) saving time when performing a grid search for model selection. Inspired by the stacked
generalization principle and the transfer learning mechanism, a
layer-by-layer combination of AK-LS-SVM classiﬁers embedded
with transfer learning is proposed in this study. This new
classiﬁer is called deep transfer additive kernel least square
support vector machine (DTA-LS-SVM) which overcomes these
two challenges. Also, considering that imbalanced datasets are
involved in many real-world scenarios, especially for medical data
analysis, the deep-transfer element is extended to compensate
for this imbalance, thus leading to the development of another
new classiﬁer iDTA-LS-SVM. In the hierarchical structure of
both DTA-LS-SVM and iDTA-LS-SVM, each layer has an AK-
LS-SVM and the predictions from the previous layer act as
an additional input feature for the current layer. Importantly,
transfer learning is also embedded to guarantee generalization
consistency between the adjacent layers. Moreover, both iDTA-
LS-SVM and DTA-LS-SVM can ensure the minimal leave-oneout error by using the proposed fast leave-one-out cross validation
strategy on the training set in each layer. We compared the
proposed classiﬁers DTA-LS-SVM and iDTA-LS-SVM with the
traditional LS-SVM and SVM using additive kernels on seven
public UCI datasets and one real world dataset. The experimental
results show that both DTA-LS-SVM and iDTA-LS-SVM exhibit
better generalization performance and faster learning speed.
Index Terms—Classiﬁcation, deep architectures, support vector
machine, transfer learning
I. INTRODUCTION
The additive kernel least squares support vector machine
(AK-LS-SVM) has been well used in many classiﬁcation tasks
due to its distinct advantages. For example, the generalization
performances of kernel-based learning methods depend highly
on the parameter selection, such as the model selection of
good values for regularization and kernel parameters. To make
an unbiased selection, parameter selection strategies based
on the minimisation of the leave-one-out cross validation
estimate is usually preferred but with unavoidably expensive
computation. However, since the analytical solution of AK-LS-
SVM can formulate a simple yet effective leave-one-out cross
validation estimate in a closed form, it only has a negligible
additional computational cost . This particularly beneﬁts
real world applications with small or medium size datasets. For
example, many medical applications involve small or medium
datasets due to the complex and/or expensive data collection
processes and the concerns of the patients’ privacy. Therefore,
it becomes necessary for us to minimize the leave-one-out
error on these datasets, which is an almost unbiased estimator
of the generalization error . In these scenarios LS-SVM
can improve the generalization performances by using the
fast leave-one-out cross validation estimate. Also, in terms
of additive kernels in AK-LS-SVM, a kernel is additive if
it can be broken down into a sum of kernel functions of each
dimension , i.e.,
Additive kernels are widely used in different classiﬁcation and
regression tasks – , such as the histogram intersection
kernel in Eq. (2), the χ2 kernel in Eq. (3) and the additive
Gaussian kernel used in this work (see Eq. (6)).
khist(x, y) =
min(xi, yi)
kχ2(x, y) = 2xiyi
In addition, additive kernels exhibit their advantages when
tackling special scenarios. For example, they have been used
in handling missing data in community health studies ,
where only the kernel functions of each dimension without
missing values are summed together. In , additive kernels
are expected to well represent the distance/similarity measures
for medical data with mixed types of features, such as features
that change over time or features that do not change.
Based on the above summary, we found that the application
of AK-LS-SVM has huge potential in the real world. However,
it still faces two main challenges: (1) The classiﬁcation performance of AK-LS-SVM is comparatively low, and (2) the grid
search for good values for model selection is still time consuming. The aim of this current work is to develop a stacked hierarchical AK-LS-SVM classiﬁer, i.e., deep transfer additive least
square support vector machine (DTA-LS-SVM), by integrating
the stacked generalization principle and the transfer learning
mechanism, to overcome these two challenges. Furthermore,
many real-world scenarios involve imbalanced datasets. For
example, in medicine, there may be only a few examples of
patients diagnosed as having contracted diseases compared to
those who are healthy. We consider these scenarios and extend
DTA-LS-SVM to iDTA-LS-SVM for handling imbalanced
datasets. In the proposed hierarchical structure, each layer
has an AK-LS-SVM. Following the stacked generalization
principle, the original data inputs and the predictions from the
previous layer become the new data input of the next layer
to enhance the performance of the whole stacked architecture.
Moreover, transfer learning is used to guarantee a consistency
between the adjacent layers such that the previously learned
model knowledge from the previous layer can be leveraged
for model construction in the adjacent higher layer to further
improve the generalization capability. The contributions of this
work can be summarized below:
1) The novel classiﬁers DTA-LS-SVM and iDTA-LS-SVM
proposed can signiﬁcantly enhance the generalization performance of AK-LS-SVM respectively on balanced and imbalanced datasets. Following the stacked generalization principle,
the proposed classiﬁers organize multiple modules in a layerby-layer structure in which every module contains an AK-LS-
SVM. From the second layer, the data input space for each
layer comprises the original data features and the predictions
from the previous layer as an appended feature. Through this
deep stacking architecture, the appended feature space helps
open the manifold structure in the original data space in a
stacked way so as to achieve the classiﬁcation performance
improvement.
2) Transfer learning is embedded from the second layer in
the proposed classiﬁers DTA-LS-SVM and iDTA-LS-SVM to
ensure that the consistency across adjacent layers is guaranteed
and the classiﬁcation capability of the higher layer can be
further enhanced. Moreover, the kernel and regularization
parameters are randomly selected in DTA-LS-SVM and iDTA-
LS-SVM, and AK-LS-SVM with transfer learning in each
layer to ensure the minimal leave-one-out error on the training
set of each layer is achieved by using the proposed fast leaveone-out cross validation strategy.
3) The experimental results show that both DTA-LS-SVM
and iDTA-LS-SVM give improved generalization performance
and fast learning speed on seven balanced and imbalanced UCI
datasets and one real community healthcare dataset, compared
with the traditional least square support vector machine (LS-
SVM) and support vector machine (SVM) using the same
additive kernels.
The remainder of this paper is organized as follows. The
related work is introduced in Section II. In Section III the
proposed classiﬁer DTA-LS-SVM is presented. In particular,
a fast leave-one-out cross validation strategy for parameter tuning is introduced. In Section IV, the experimental results are
provided on the seven UCI datasets and one real community
healthcare dataset. Finally, the conclusions and future work
are given in Section V.
II. RELATED WORK
In this section some background knowledge of LS-SVM
and AK-LS-SVM is described because our work involves both
proposed classiﬁers integrated with transfer learning in a deep
structure.
A. Shallow and deep architecture
Classiﬁers in shallow architecture traditionally consist of
an input layer, a single processing layer and an output layer.
For example, LS-SVM and AK-LS-SVM used in this work
have a layer of kernel function which is applied to the input,
followed by a linear combination of the kernel outputs. Such
shallow machines have been widely used in classiﬁcation and
regression with good performances . However,
shallow architectures have problems in providing an efﬁcient
representation for certain types of functions. Deep structures
are known to comprise several layers of non-linear operations,
such as in the neural network with many hidden layers,
which aim to learn hierarchical representations. Beneﬁting
from the multiple levels of representation and abstraction,
deep architectures learn the complicated functions mapping
the input to the output of the data.
Using shallow classiﬁers like SVM in combination with
deep architectures has been proposed in the literature. For
example, Abdullah et al. presented a deep support vector
machine (D-SVM), which used the non-linear combinations of
kernel activations of support vectors as inputs to train a SVM
on the upper layer. Moreover, the deep SVMs were combined
with the product rule ensemble for combining multiple image
descriptors. Tang used linear SVMs rather than a softmax
activation function in convolutional neural networks (CNNs)
to learn to minimize a margin-based loss instead of the crossentropy loss. In this work, we focus on the least squares
version of SVM in deep architecture.
B. Deep Stacked architecture
There are different types of deep architectures, such as
CNNs , deep belief networks (DBNs) , deep Boltzmann machines (DBM) and deep auto encoders etc.
In this work, the proposed classiﬁer DTA-LS-SVM is built in a
deep stacked architecture , which is trained in a supervised
and module-wise fashion. Unlike other deep architectures
such as DBNs, it does not employ back propagation over
all modules and does not aim to ﬁnd the transformed feature
representation. To learn complex functions effectively, it stacks
multiple modules in a chain, and the outputs from the previous
layer are fed into the inputs of the module of the next layer.
Due to this nature of building a hierarchy of simpliﬁed modules trained by itself, deep stacked architecture is relatively
Fig. 1: Two ways to expand the feature space in deep stacked
architecture. (a) fl is the prediction from the previous module
l. (b) f changes with the current module layer. For example,
in module 2, f = f1, in module 3, f = f2, · · · , in module L,
simple and easy to implement. Through the deep stacked
architecture, the appended feature space opens the original
data manifolds. This architecture follows the philosophy of
"stacked generalization " which indeed enhances classiﬁcation performances and improves generalization in learning
complex functions. In order to learn complex functions or
classiﬁers effectively, it stacks multiple modules in a chain,
and the outputs from the module(s) from the previous layer
are fed into the inputs of the module of the next layer. Deep
stacked architecture was ﬁrst introduced in 2011 . After
that, a kernel version DSNs (K-DSNs) was proposed by Deng
et al. , which integrates deep learning and kernel learning.
By using the kernel trick, the hidden neurons in each DSN
layer becomes inﬁnity. Another novel DSNs, which is called
tensor-DSNs (T-DSNs) was presented by Hutchinson et al.
 . In this method, each module has a bilinear mapping from
two hidden layers to the output layer by incorporating higher
order statistics of the hidden binary features through a weight
tensor. Vinyals et al. proposed a recursive perceptual
representation using layers of linear SVMs and incorporating
with random projections of weak predictions from each layer.
Through the deep stacked architecture, the appended feature space using the predictions from the module(s) of the
previous layers open the original data manifolds such that
the generalization performances may be improved. There are
two ways to append the feature space with the increase of
the depth in deep stacking architecture. As depicted in Fig.
1(a), the new feature space for the module in the higher layer
comes from the concatenation of the predicted outputs from
all the modules of the previous layers and the original input
features. The mentioned work above belongs to this category.
Fig 1(b) illustrates a different type to append feature space on
which our work concentrates. In our work, the new feature
space for the module of the higher layer comes from the
concatenation of the predicted outputs, that is, from the module
of the original input features and the predicted outputs only
for the module of the previous layer.
C. Transfer learning
Transfer learning has been another important research topic
in machine learning. Given a source domain DS and its
learning task TS, a target domain DT and its learning task TT ,
The goal of transfer learning is to help improve the learning
process in DT by leveraging the knowledge in DS, when
DS ̸= DT , or TS ̸= TT . In our scenario, from the bottom to
top in the deep stacked architecture, the module of the previous
layer is used as the source domain, and the adjacent module
of next layer is used as the target domain.
In the proposed deep stacking architecture, the adjacent
models always have the same original feature space, and an
additional feature is taken as the appended feature for the
predictions of the adjacent layers to reﬂect the discriminative
information about the classes in the original feature space.
Therefore, we can postulate that there is a certain consistency
between DS and DT such that transfer learning between
the adjacent layers can be introduced. In other words, it is
worthwhile embedding transfer learning into the deep stacking
architecture, which uses previously learned knowledge from
DS to help the learning process in DT .
To deploy transfer learning, the main question is "what
to transfer". It asks which part of knowledge is used to
transfer across domains or tasks. From the literature, the
leveraged knowledge can be categorized into instances-,
feature-, or model/parameters-based transfer learning .
The instance-transfer approach re-weights certain data points
in the source domain for use in the target domain ,
 . The feature-representation-transfer approach encodes the
transferred knowledge across domains into a shared representative feature structure, and the target model is guided
in the new feature space , . The model/parametertransfer approach assumes that the source and target domains
share parameters to some extent or prior distributions of
the classiﬁer , . For readers who are interested, the
following articles are suggested for further investigation: ,
 , . In this work, the transfer learning used belongs to
the model/parameters-based transfer category.
III. THE PROPOSED DEEP TRANSFER LEAST SQUARES
SUPPORT VECTOR MACHINE
We use S = {(x1, y1), · · · , (xN, yN)} to represent the
dataset adopted in this work. The input set is denoted as
X and the corresponding output set is denoted as Y . xi =
2, · · · , xi
d) ∈X ⊂Rd, yi ∈{+1, −1}, and each
sample xi (i = 1, 2, · · · , N) contains d features. The general
framework of DTA-LS-SVM is illustrated in Fig. 2. It can
be seen that the proposed approach is built with multiple
layers of AK-LS-SVM based modules in a stacked structure.
The original data are given as data inputs to layer 1. For
each module from layer 2, a transfer based AK-LS-SVM is
trained using the transformed data with the original features
and the appended feature where values are the previous layers’
predictions. Moreover, model transfer is embedded to leverage
relevant model knowledge from the adjacent module of the
previous layer.
In the whole framework, the stacked generalization principle
and the unfolding of manifold structure existing in the original
data input space by concatenating the predictions from the adjacent module of the previous layer guarantee the improvement
on the generalization capability. Moreover, transfer learning
is used to maintain similarity across the adjacent modules by
leveraging the previously learned knowledge from the previous
module such that the generlization capability also can be
improved as well.
A. AK-LS-SVM and its Adaptive Regularization
AK-LS-SVM is based on statistical learning theory which is
formulated on the structural risk minimization (SRM) principle. The learning process can be formalized as an optimization
problem that minimizes the structural risk as follows:
Remp(f(xi), yi)
where Ω(f) is a regularization term that encodes some notion
of smoothness for f and prevent overﬁtting. Remp is a chosen
convex loss function that evaluates the quality of f on the instance {xi, yi}. C gives a trade-off between the minimization
of Remp and the smoothness or simplicity enforced by a small
In the ﬁrst processing layer L1 in the framework of DTA-
LS-SVM, a traditional AK-LS-SVM is used to ﬁnd an optimal
hyperplane f(x) = wT ϕ(x)+b. The regularization term is set
to be Ω(f) = 1
2∥w∥2 and the loss function to be the weighted
square loss Remp(f(x)i, yi) = (f(xi) −yi)2. Therefore, the
learning problem in Eq. (4) becomes
w,b J(w, b) =
2∥w1∥2 + C1
1 ϕ(xi1) + b1 + ξi1
i = 1, 2, ..., n
where C is the regularization parameter and ϕ(xi1)
11), ˜ϕ(xi
21), · · · , ˜ϕ(xi
k1), · · · , ˜ϕ(xi
d1)), and ϕ(xi1) is a
feature mapping such that the additive kernel K below can
be adopted in Eq. (5):
K(xi1, xj1) = ϕ(xi1)T ϕ(xj1) = Pd
In this work, an additive Gaussian kernel is adopted, i.e.,
, where δ is the kernel width.
From the next processing layer Ll, (l = 2, 3, · · · , L)),
transfer learning is used to leverage model knowledge from
DS(l−1) (source domain) to DT l (target domain), For example, ﬁrst, the optimal wS(l−1) is found by minimizing Eq.
(5) in DS(l−1), then when DT l is encountered, a model is
constructed in which wT l gets as close as possible to the
known wS(l−1). Through editing the regularization term, the
learning classiﬁcation task from the second processing layer
becomes the corresponding transfer based AK-LS-SVM, i.e.,
wT l −wS(l−1)
Moreover, the regularization term can be further revised
wT l −λlwS(l−1)
by adding the weighting factor λl to
control the degree to which the new model is close to the
source model. This evaluates the similarity between wS(l−1)
and wT l in the optimization problem above.
B. Transfer learning in DTA-LS-SVM and its fast leave-oneout cross validation
In this section, we describe in detail how to use transfer
learning across modules in DTA-LS-SVM. As demonstrated
in Fig 2, the proposed classiﬁer is built by multiple layers
of modules, which from the second module, each learns a
transfer based AK-LS-SVM on the original data inputs with
an additional feature whose values are the predictions from the
module of the previous layer. The core motivation of DTA-LS-
SVM is that when the deeper structure is well trained, it tends
to do a better job at disentangling the underlying factors of
variation . The recursive leverage of previous predictions
helps to move apart the manifolds in the original data in a
stacked way such that a better separability can be achieved
As mentioned in III-A, in the ﬁrst processing layer L1,
a traditional AK-LS-SVM model is constructed using the
additive kernel deﬁned in Eq. (5) giving the decision function
f1(xi) = wT
1 ϕ(xi)+b1. Also the predicted label vector F 1 of
X is obtained, where F 1 = (f1(x1), f1(x2), · · · , f1(xN)).
From the processing layer Ll (l = 2, 3, ...L), the new data
l is the augmentation of the original data input set X
and the predicted label vector F l−1 from the previous layer,
which can be denoted as X ⊕F l−1 for simplicity. X′
illustrated in Fig. 3. The transfer-based AK-LS-SVM is used in
Ll, which leverages the source model in Ll−1 to have λlwl−1,
which is represented as below:
min J(wl, bl) =
2∥wl −λlwl−1∥2 + Cl
i) + bl + ξli
i = 1, 2, ..., N
The corresponding Lagrangian Ll of Eq. (5) is
Ll(wl, bl, ξl; αl) = J(wl, bl) + PN
i=1 αli(yi −wT
i) −bl −ξli)
where αl ∈RN is the vector of all Lagrangian multipliers.
With respect to wl, ξli, αli, the optimal condition can be
calculated by
∂wl = 0 ⇒wl = λlwl−1 + Pn
i=1 αliϕ(x
⇒ξli = αli/Cl
i) + bl + ξli
Note that the optimal solution wl in Eq. (10) is given by
the sum of the source model scaled by parameter λl and a
new model is built on the new data inputs. When λl is 0,
wl returns to the original formulation in a traditional AK-LS-
SVM model; no learned knowledge can be leveraged from the
previous source model.
Combining Eq. (10), Eq. (11) with Eq. (12), we obtain:
i=1 αliϕ(x
j) + bl + αli/Cl = yi −λlwT
The above equation can be written in matrix form:
Fig. 2: Hierarchical architecture and learning process of DTA-LS-SVM
Fig. 3: Augmented space X′
Y −λl eY l
j)]N∗N, I is a diagonal matrix with
unity diagonal entries, Y
is the actual labels of training
samples, and eY l is the predicted labels of training samples that
are obtained from the source model, i.e. Y = [y1; · · · ; yN],
l1; · · · ; y
l1); · · · ; wT
i=1 α(l−1)iK(x(l−1)i, x
l1), · · · , PN
i=1 α(l−1)iK(x(l−1)i, x
Here eY l can be obtained in a kernel form as long as the
previous model wl−1 and the current model wl use the same
kernel. Therefore, for the safe use of the adopted Gaussian
additive kernel, δ in each model must be the same.
Hl represents the ﬁrst matrix on the left-hand side of Eq.
(14); the model parameters can be calculated simply using a
matrix inversion:
Y −λl eY l
where Ql = H−1
. It can be observed that λl, αl and bl can
be calculated accordingly from Eq. (15), and wl and bl from
Eq. (10) and Eq. (12), respectively.
In order to ﬁnd the optimal value for parameter λl efﬁciently and effectively, we propose a fast leave-one-out crossvalidation method for parameter tuning.
Hl is decomposed into its block representation and the ﬁrst
row and the ﬁrst column are isolated, i.e.,
αl(−i) and bl(−i) represent the model parameters in the i-th
iteration of the leave-one-out cross validation procedure. In
the ﬁrst iteration where the ﬁrst training sample is excluded,
Y (−1) −λl eY (−1)l
where Q(−1)l = H−1
(−1)l and Y (−1) = (y2, y3, ..., yN, 0)T . We
denote the predicted label on the i-th sample excluded from
the training dataset by ˜yil, and the predicted label for the ﬁrst
training sample becomes
Y (−1) −λl eY (−1)
Considering the last N equations in Eq. (14), we get
h1l H(−1)l
Y (−1) −λl eY (−1)
h1l H(−1)l
[α1l, · · · , αNl, bl]T + λl eY (−1)
1 Ql(−1)h1α1 + hT
1 [α2l, · · · , αNl, bl]T + λl eY (−1)
From Eq. (14), the ﬁrst equation of the system is y1 −
λly1(−1)l = h11lα1l + hlT
1 [α2l, α3l, · · · , αNl, bl]T , and hence
˜y1 = y1 −α1l(h11l −hT
1lQ(−1)lh1l). Finally, by using Ql =
H(−1)l and the block matrix inversion lemma we can obtain
−v−1h1lQ(−1)l
Q(−1)l + v−1Q(−1)lhT
1lh1lQ(−1)l
−v−1Q(−1)lhT
where v = h11l −hT
1lQ(−1)lh1l. Since the system of linear
equations in Eq. (14) is insensitive to permutations of the
ordering of the equations, then
˜yil = yi −αil/Qiil
By deﬁning
, and αl = α
l , then we can obtain
˜yil = yi −α
It can be seen from Eq. (22) that αl and λl have a
linear relationship, which means that after determining λl, the
learning model can be obtained as well. The optimal λl is
supposed to keep the same sign of ˜yil and yi for all training
samples. However, it might cause many local minima issues
due to non-convex formulation. Thus, in the end the following
loss function is adopted, which is similar to the hinge loss:
l(˜yil, yi) = |1 −˜yilyi|+ =
where |x|+ = max{0, x}. This is a convex upper bound to the
leave-one-out misclassiﬁcation loss, and it prefers solutions in
which ˜yi has an absolute value equal to or larger than 1 and
the same sign as yi. Finally, the objective function is
l(˜yil, yi)
where D is a constant. A regularization based on this can
induce numerical stability. This optimization process can be
implemented by a projected sub-gradient descent algorithm,
and the pseudocode in given in Algorithm 1. After obtaining
λl, αl and bl can be calculated accordingly from Eq. (15), and
then wl and bl from Eq. (10), Eq. (11) and Eq. (12).
As a result, we can obtain the decision function fl(x
il) + bl on Ll (l ≥2), and the predicted label
vector F l = (fl(x
2l), · · · , fl(x
Nl)) for X′
processing layer continues to be added until the prediction accuracy performance has no improvement or the improvement is negligible, (i.e., ||F l −F l−1||2
F < ϵ). Here,
the complete learning algorithm of the proposed classiﬁer
DTA-LS-SVM is given in Algorithm 2 that outputs the
ﬁnal decision function. Please note that in this work we
select the parameter Cl from comparatively big intervals,
i.e., Cl ∈{1, 10, 50, 100, 150, 200, 250, 500}, to guarantee
diversities between the modules from adjacent layers. However
Cl can also be selected from different intervals depending
upon the piratical situations.
In DTA-LS-SVM, transfer learning is embedded from the
second layer. Referring to Eq. (10), we can observe that the
classiﬁcation on the l-th (l >= 2) layer is in fact achieved
in a way like a combination of multiple kernel functions
from different layers. According to the excellent generalization
performances of multi-kernel classiﬁers – , the module
from the second layer in the proposed approach is supposed to
obtain a better generalization performance than the previous
one. In this sense, the learning process under such a stacked
architecture tends to be greedy. Our experiments show that
normally L = 3, 4 or 5 is an appropriate reference for small
or medium datasets. If L is too big, it might lead to overﬁtting
The proposed classiﬁer DTA-LS-SVM can be extended
to explain the multi-classiﬁcation tasks. The one-against-all
strategy may be used to ﬁnd the multiple decision functions
that separate one class from the remaining classes. In the
end, the predicted label of the new input data sample xi is
determined by
k=1,··· ,M yk(xi), where M denotes the number
of the classes.
C. Computational complexity
One highlight of the proposed classiﬁer DTA-LS-SVM is
its fast computational ability of the leave-one-out cross validation in the deep architecture. Its computational cost can be
represented as O(N 3+(L−1)(N 3+N)). First it includes the
traditional AK-LS-SVM model construction on the ﬁrst layer.
Therefore, the complexity of this part is O(N 3). From the
layer (l ≥2), the computational cost of each module consists
of two parts. The ﬁrst part includes the calculation of the
Algorithm 1: Learning algorithm of transfer additive LS-SVM
Input: wl−1, X
l, Y , Cl and kernel width σ
Output: λl
Step 1: Calculate Ql by using Ql = H(−1)l and Eq. (17).
Step 2: Calculate α
Step 3: t = 1
Step 4: Repeat
˜yil = yi −
Qiil + λlα
Qiil , i = 1, 2, ..., N
di ←1{˜yilyi > 0}, i = 1, 2, ..., N
If λl > D then λl ←D
λl ←max(λl, 0)
Step 5: Until convergence
Step 6: Output λl
Algorithm 2: Learning algorithm of DTA-LS-SVM
Input: training set X = [x1, x2, · · · , xN], xi ∈Rd,
output set Y = [y1, y2, · · · , yN], yi ∈{+1, −1} for binary
classiﬁcation, kernel width δ, number of layers L, l = 1
Output: The stacked structure of DTA-LS-SVM with
tuned parameter values
1.1 Choose the regularization parameter C1 randomly from
seven values, i.e., C1 ∈{1, 10, 50, 100, 150, 200, 250, 500}.
1.2 Construct the 1st module using the additive kernel
LS-SVM shown in Eq. (4) and obtain w1, b1 and the predicted
labels F1 (f1(x11), f1(x21), · · · , f1(xN1)).
Step 2: l = l + 1
Step 3: For l = 2 : L do
l = X ⊕F l−1
3.2 Choose the regularization parameter Cl randomly from
intervals, i.e., Cl ∈{1, 10, 50, 100, 150, 200, 250, 500}.
3.3 Construct the lth module by invoking Algorithm 1 on
l and obtain λl.
3.4 Calculate wl using Eq.(12), Eq.(13), Eq.(15) and the
predicted labels F l (fl(x1l), fl(x2l), · · · , fl(xNl)).
Step 4: Calculate △F = ||F l −F l−1||2
Step 5: If △F ≤ϵ (a given threshold)
Step 6: l = l + 1
Step 7: Output the stacked structure of the proposed
classiﬁer DTA-LS-SVM with tuned parameter values and the decision
function in the L-th module as the ﬁnal decision function.
matrix Ql by the inverse related to the training set on the Ll, in
which the corresponding computational complexity is O(N 3).
The second part includes the computational complexity of each
iteration in Algorithm 1 to optimize Eq. (24), which can be
represented as O(N). Therefore, the entire computational cost
of DTA-LS-SVM becomes O(N 3 + (L −1)(N 3 + N)) =
O(LN 3 + (L −1)N).
Let us consider the traditional leave-one-out cross-validation
strategy on SVM. Theoretically, it takes O(N 3) to train a
SVM. By using speciﬁc speed-up strategies , the training time can be accelerated to O(N) −O(N 2.3) such that
the leave-one-out cross validation time on SVM becomes
O(N∗N)−O(N∗N 2.3) = O(N 2)−O(N 3.3). Considering the
grid search for generalization parameter C (s1 grid values) and
kernel width σ (s2 grid values), the computational complexity
of SVM becomes s1s2O(N 2) −s1s2O(N 3.3). In general, s1
and s2 are normally greater than 3 in the experiments, and the
number L of layers in DTA-LS-SVM is small (3 ≤L ≤5).
Therefore, although it seems that the computational complexity of SVM may be less than that of DTA-LS-SVM, our
experiments reveal that the actual running time of SVM with
grid search actually is much longer than that of DTA-LS-SVM.
In terms of LS-SVM, the computational complexity to train
a LS-SVM is O(N 3) due to the calculation of the matrix Q
by the inverse of H. Therefore, the computational complexity of leave-one-out cross validation on LS-SVM becomes
s1s2O(N ∗N 3) = s1s2O(N 4). Moreover, LS-SVM could
use the fast leave-one-out cross validation strategy discussed
in Subsection III-B. Referring to Eq. (22) and Eq. (23) with
λl equal to 0, the computational complexity of LS-SVM
could be accelerated into s1s2O(N 3 + N). In summary, the
proposed classiﬁer DTA-LS-SVM has a superior advantage in
the running speed, compared with SVM and LS-SVM.
D. Extension on imbalanced datasets
The proposed classiﬁer DTA-LS-SVM is based on the AK-
LS-SVM framework. However, it is often the case in many
real-world scenarios that the datasets are imbalanced. This is
especially true when dealing with datasets involving medical
problems. For example, a dataset will be imbalanced due to far
more cases of a diagnosis of malignancy within cancer cases
compared to those that are benign or cancer free. We found that
DTA-LS-SVM can be naturally extended to its cost-sensitive
or imbalanced version such that our method is also suitable
for imbalanced datasets.
The problem with imbalanced datasets is that the decision
boundary tends to get too close to the positive class (i.e.,
minority class). Therefore, the decision boundary needs to be
pushed away from positive instances. One solution is to give
different error costs to the positive and negative classes .
There are several ways to achieve this goal. However in this
work we only change the objective functions of AK-LS-SVM
into Eq. (25) for an imbalanced dataset:
w,b J(w, b) =
yi = wT ϕ(xi) + b + ξi
where N + and N −represents the numbers of samples in the
positive and negative classes respectively, in the N samples.
I.e., when Eq. (25) is applied to AK-LS-SVM, we term DTA-
LS-SVM as iDTA-LS-SVM for an imbalanced dataset.
Similar to the mathematical derivations from Eq. (9) to Eq.
(15), we easily ﬁnd that only Hl in the ﬁrst matrix in Eq. (14)
needs to be changed into the following representation:
and the remaining derivations remain the same such that
iDTA-LS-SVM can be applied on imbalanced datasets. By
comparing Hl in Eq. (26) with the ﬁrst matrix in Eq. (14),
we can ﬁnd that only the difference is between E in Eq. (26)
and I in Eq. (14). Only under the condition that N −equals
N +, E degenerates into I such that iDTA-LS-SVM can handle
balanced datasets as well. In addition, it is obvious that iDTA-
LS-SVM has the same computational complexity of leave-oneout cross validation as that for DTA-LS-SVM.
IV. EXPERIMENTS AND RESULTS
In the experiments, the proposed classiﬁers DTA-LS-SVM
and iDTA-LS-SVM were evaluated on balanced and imbalanced UCI datasets and one real world dataset. In order to
make the comparison fair, their classiﬁcation performances are
compared with those using the traditional LS-SVM and SVM
with additive kernels. During the data preparation process,
all values in the datasets were normalized. For any original
UCI datasets with missing data, the records with missing
values were ﬁrst removed and then the processed datasets were
used in the experiments. To evaluate methods on balanced
datasets, the accuracy and F-score were used as metrics. On
imbalanced datasets, only the F-score was used as a metric.
This considers for example, that with an imbalance ratio 99:1,
a method labelling each data input with a positive class is
99% accurate, but it is useless as a classiﬁer to ﬁnd out the
negative class. Therefore, metrics precision and recall were
adopted for imbalanced datasets. Precision is deﬁned as the
number of correct positive results divided by the number of all
positive results, while recall is deﬁned as the number of correct
positive results divided by the number of positive results that
should have been found. The F-score is a weighted average of
the precision and recall.
Precision =
true positive
true positive + false positive
true positive
true positive + false negative
F-score = 2 ∗precision ∗recall
precision + recall
The F-score was used to measure the performances on imbalanced datasets in Lin et al. , Zou et al. and Tan
 . In this work, this metric was also used to evaluate iDTA-
LS-SVM and comparative methods. In the experiments, each
dataset was randomly split into the corresponding training and
testing sets in the ratio 7:3. This process was repeated 10 times
such that every sample from the dataset might have a chance
TABLE I: UCI DATASETS DESCRIPTION
Sample size
Imbalanced
breast cancer
Pima Indians
Indians Liver
Australian
credit approval
mammographic
to be used in the training and testing sets. In the end, the mean
and standard deviation of metrics(s) on training and testing sets
were calculated. All the experiments were implemented using
64-bit MATLAB on a computer with an Intel Core i5-6300
2.40 GHz CPU and 8.00GB RAM.
A. UCI public datasets
In this subsection, DTA-LS-SVM and iDTA-LS-SVM with
the traditional LS-SVM and SVM using additive kernels are
compared. Seven public UCI datasets were adopted in which
three were imbalanced and four were balanced. These UCI
datasets are summarized in Table I.
In the experiments, different additive kernels were tried
to ﬁnd the most suitable one for DTA-LS-SVM, iDTA-LS-
SVM and the comparative methods on each dataset. Here
only the experimental results using the selected kernel (Gaussian additive kernel) are displayed. For DTA-LS-SVM and
iDTA-LS-SVM, the number of modules is set to 3 or 4
due to the small or medium sample size of the adopted
datasets. ϵ is set to 0.1. δ is set to be the average value
of the standard deviations for all respective features. For
the comparative methods, the grid search algorithm with 10fold cross-validation was used to ﬁnd the optimal values
for parameters C and δ which could give the best performances. The values of {1, 10, 50, 100, 150, 200, 250, 500} and
{0.1, 1, 5, 10, 20, 50, 100, 150, 200} were searched for C and
δ, respectively in the experiments. Here only the performances
using the optimal parameters are displayed.
Table II shows the experimental results of DTA-LS-SVM
and the comparative methods on imbalanced datasets. Table
III shows the experimental results of iDTA-LS-SVM and
the comparative methods on balanced datasets. During the
training process of both DTA-LS-SVM and iDTA-LS-SVM
classiﬁers, the ﬁrst module (i.e., the ﬁrst layer) was constructed
to obtain the training accuracy after comparing the predicted
and actual labels. If the training accuracy of the adjacent
higher layer was improving continuously, one more layer
was added and training continued. If the training accuracy
of the adjacent layer remained the same or even dropped,
the process was stopped. To elaborate, one complete training
process on the Australian dataset is described. For this dataset,
TABLE II: PERFORMANCE RESULTS ON BALANCED UCI DATASETS
Performances
DTA-LS-SVM
Australian
0.8936±0.0099
0.8678±0.0212
0.8583±0.0098
0.8500±0.0266
0.8589±0.0075
0.8572±0.0174
0.9050±0.0091
0.8766±0.0215
0.8665±0.0060
0.8630±0.0193
0.8620±0.0082
0.8619±0.0190
0.7720±0.0096
0.7395±0.0197
0.7391±0.0221
0.7220±0.0242
0.7275±0.0068
0.7188±0.0194
0.7668±0.0163
0.7368±0.0202
0.7434±0.0380
0.7262±0.0281
0.7262±0.0112
0.7153±0.0194
0.8985±0.0075
0.8786±0.0161
0.8611±0.0138
0.8536±0.0203
0.8678±0.0087
0.8648±0.0230
0.9065±0.0095
0.8895±0.0154
0.8683±0.0097
0.8632±0.0205
0.8684±0.0079
0.8682±0.0192
mammographic
0.8231±0.0078
0.8273±0.0233
0.8155±0.0103
0.8121±0.0313
0.8189±0.0116
0.8104±0.0349
0.8227±0.0102
0.8321±0.0266
0.8172±0.0173
0.8167±0.0357
0.8181±0.0120
0.8137±0.0350
TABLE III: PERFORMANCE RESULTS ON IMBALANCED UCI DATASETS
Performances
iDTA-LS-SVM
breast cancer
0.9861±0.0064
0.9801±0.0026
0.9760±0.0074
0.9738±0.0069
0.9806±0.0041
0.9728±0.0123
Pima Indians
0.8748±0.0167
0.8359±0.0097
0.8401±0.0131
0.8284±0.0317
0.8336±0.0084
0.8285±0.0248
Indians liver
0.8438±0.0829
0.8403±0.0083
0.8390±0.0166
0.7986±0.0580
0.8343±0.0080
0.8323±0.0188
TABLE IV: PERFORMANCE RESULTS ON THE Australian DATASET WITH DIFFERENT RATIOS OF TRAINING AND TESTING
Performances
0.9086±0.0087
0.8572±0.0111
0.9009±0.0148
0.8577±0.0110
0.9047±0.0215
0.8589±0.0141
0.9043±0.0175
0.8551±0.0149
0.9081±0.0083
0.8716±0.0125
0.9123±0.0121
0.8727±0.0098
0.9124±0.0199
0.8742±0.0131
0.9132±0.0171
0.8703±0.0132
0.8425±0.0384
0.8417±0.0431
0.8417±0.0598
0.8304±0.0692
0.8413±0.0544
0.8283±0.0559
0.8449±0.0695
0.8265±0.0762
0.8586±0.0200
0.8595±0.0281
0.8631±0.0307
0.8546±0.0396
0.8600±0.0272
0.8519±0.0325
0.8651±0.0337
0.8496±0.0418
0.8621±0.0148
0.8504±0.0221
0.8649±0.0143
0.8487±0.0139
0.8717±0.0202
0.8498±0.0176
0.8797±0.0251
0.8478±0.0161
0.8663±0.0139
0.8564±0.0194
0.8716±0.0150
0.8529±0.0126
0.8788±0.0176
0.8563±0.0164
0.8871±0.0242
0.8556±0.0155
TABLE V: PERFORMANCE RESULTS ON THE Diabetic DATASET WITH DIFFERENT RATIOS OF TRAINING AND TESTING
Performances
0.7990±0.0082
0.7386±0.0139
0.8028±0.0125
0.7241±0.0164
0.8052±0.0121
0.7159±0.0132
0.8183±0.0150
0.7076±0.0129
0.7974±0.0090
0.7331±0.0117
0.7986±0.0154
0.7127±0.0197
0.8084±0.0186
0.7071±0.0132
0.8115±0.0240
0.6965±0.0162
0.7445±0.0216
0.7180±0.0208
0.7431±0.0166
0.7163±0.0245
0.7315±0.0286
0.6991±0.0237
0.7209±0.0143
0.6815±0.0237
0.7451±0.0333
0.7270±0.0250
0.7377±0.0409
0.7096±0.0338
0.7355±0.0402
0.7021±0.0333
0.7268±0.0482
0.6770±0.0412
0.7330±0.0134
0.7015±0.0199
0.7297±0.0136
0.7038±0.0145
0.7193±0.0100
0.6987±0.0151
0.7075±0.0216
0.6875±0.0210
0.7350±0.0219
0.7038±0.0147
0.7262±0.0226
0.6936±0.0276
0.7339±0.0154
0.6965±0.0095
0.7004±0.0355
0.6733±0.0432
TABLE VI: PERFORMANCE RESULTS ON THE credit approval DATASET WITH DIFFERENT RATIOS OF TRAINING AND
TESTING DATA
Performances
0.8985±0.0071
0.8664±0.0115
0.9110±0.0100
0.8621±0.0116
0.9031±0.0119
0.8615±0.0112
0.9190±0.0149
0.8614±0.0126
0.9074±0.0075
0.8773±0.0122
0.9206±0.0107
0.8708±0.0124
0.9120±0.0107
0.8720±0.0097
0.9270±0.0134
0.8687±0.0168
0.8509±0.0305
0.8481±0.0556
0.8426±0.0763
0.8202±0.0890
0.8295±0.0798
0.8120±0.0997
0.8508±0.0649
0.8328±0.0630
0.8623±0.0182
0.8608±0.0406
0.8614±0.0382
0.8411±0.0506
0.8609±0.0396
0.8405±0.0556
0.8673±0.0363
0.8560±0.0423
0.8681±0.0050
0.8622±0.0127
0.8696±0.0164
0.8612±0.0169
0.8778±0.0149
0.8564±0.0108
0.8677±0.0164
0.8537±0.0193
0.8695±0.0059
0.8628±0.0149
0.8716±0.0159
0.8625±0.0167
0.8802±0.0167
0.8604±0.0117
0.8754±0.0172
0.8600±0.0132
TABLE VII: PERFORMANCE RESULTS ON THE mammographic DATASET WITH DIFFERENT RATIOS OF TRAINING AND
TESTING DATA
mammographic
Performances
0.8325±0.0045
0.8187±0.0171
0.8234±0.0120
0.8224±0.0118
0.8241±0.0173
0.8106±0.0138
0.8261±0.0240
0.8095±0.0165
0.8317±0.0078
0.8093±0.0149
0.8214±0.0130
0.8234±0.0132
0.8216±0.0264
0.8091±0.0213
0.8275±0.0267
0.8050±0.0171
0.8213±0.0137
0.8051±0.0173
0.8212±0.0086
0.8039±0.0181
0.8250±.0180
0.8036±0.0186
0.8309±0.0172
0.8024±0.0149
0.8202±0.0166
0.8017±0.0201
0.8151±0.0081
0.7988±0.0183
0.8240±0.0202
0.8006±0.0178
0.8326±0.0209
0.7975±0.0153
0.8139±0.0125
0.8060±0.0284
0.8089±0.0124
0.8000±0.0153
0.8108±0.0197
0.7972±0.0215
0.8092±0.0277
0.7914±0.0153
0.8165±0.0119
0.8079±0.0264
0.8135±0.0138
0.7978±0.0159
0.8110±0.0187
0.8008±0.0189
0.8052±0.0296
0.7867±0.0234
TABLE VIII: PERFORMANCE RESULTS ON THE breast cancer DATASET WITH DIFFERENT RATIOS OF TRAINING AND
TESTING DATA
breast cancer
iDTA-LS-SVM
0.9818±0.0038
0.9798±0.0053
0.9821±0.0040
0.9787±0.0044
0.9859±0.0065
0.9796±0.0031
0.9874±0.0043
0.9775±0.0053
0.9735±0.0051
0.9742±0.0115
0.9684±0.0183
0.9649±0.0236
0.9750±0.0044
0.9675±0.0111
0.9745±0.0093
0.9711±0.0057
0.9811±0.0042
0.9711±0.0063
0.9797±0.0081
0.9749±0.0064
0.9821±0.0042
0.9726±0.0083
0.9854±0.0088
0.9718±0.0043
TABLE IX: PERFORMANCE RESULTS ON THE Pima Indians DATASET WITH DIFFERENT RATIOS OF TRAINING AND TESTING
iDTA-LS-SVM
0.8665±0.0157
0.8345±0.0171
0.8697±0.0088
0.8345±0.0066
0.8700±0.0129
0.8336±0.0121
0.8918±0.0188
0.8254±0.0088
0.8399±0.0141
0.8265±0.0239
0.8409±0.0179
0.8295±0.0205
0.8360±0.0252
0.8263±0.0189
0.8385±0.0280
0.8222±0.0178
0.8379±0.0104
0.8185±0.0138
0.8417±0.0175
0.8198±0.0161
0.8426±0.0150
0.8174±0.0075
0.8478±0.0134
0.8141±0.0135
TABLE X: PERFORMANCE RESULTS ON THE Indians liver DATASET WITH DIFFERENT RATIOS OF TRAINING AND TESTING
Indians liver
iDTA-LS-SVM
0.8387±0.0104
0.8375±0.0135
0.8422±0.0085
0.8373±0.0078
0.8382±0.0113
0.8336±0.0096
0.8353±0.0177
0.8294±0.0177
0.9197±0.0583
0.8285±0.0153
0.8714±0.0537
0.8117±0.0212
0.8850±0.0779
0.7995±0.0542
0.9203±0.0760
0.7813±0.0542
0.8369±0.0106
0.8294±0.0160
0.8420±0.0133
0.8253±0.0133
0.8410±0.0175
0.8288±0.0117
0.8363±0.0203
0.8226±0.0088
the training (testing) accuracy is 0.8589 (0.8173), respectively,
after the ﬁrst module, and 0.8734 (0.8365), respectively, after
the second module (i.e., the second layer). Since the accuracies are increasing, training of the third and forth module
continued, and 0.8880 (0.8702) accuracy for the third module
and 0.8734 (0.8365) accuracy for the fourth module was
obtained. After the third module, a decrease in the accuracy
can be seen. Therefore the accuracy performances on the third
module are used, which indeed outperforms the comparative
methods. From the performance results shown in Table II,
DTA-LS-SVM obtains considerably higher testing accuracies
than those using the comparative methods on the adopted
balanced UCI datasets. In terms of the experimental results on
the imbalanced datasets displayed in Table III, iDTA-LS-SVM
maintains the advantage on the F-scores compared to other
methods. From these two tables, the performance increases
from DTA-LS-SVM and iDTA-LS-SVM are not dramatic.
However compared to LS-SVM, it becomes consistently better.
In some cases, LS-SVM performs worse than SVM, but DTA-
LS-SVM and iDTA-LS-SVM always perform better. Overall,
the proposed classiﬁers DT-AK-LS-SVM and iDTA-LS-SVM
exhibited good generalization performances on both balanced
and imbalanced datasets.
Moreover, to observe the behavior of the proposed classiﬁers, we divided the training and testing sets at different
ratios on each UCI dataset to evaluate their classiﬁcation
performances. The training and testing sets ratios were set
to 6:4, 5:5, 4:6 and 3:7 on each UCI datasets. Table IV-
VII display the experimental results of DTA-LS-SVM and
the comparative methods on all the balanced datasets in
terms of accuracy and F score. Tables VIII-X contain the
experimental results of iDTA-LS-SVM and the comparative
methods for all the imbalanced datasets, in terms of F-scores.
On each table, the best results have been highlighted in bold.
In terms of training accuracy and F-score, when changing the
ratios, DTA-LS-SVM and iDTA-LS-SVM do not exhibit an
increasing trend as seen in LS-SVM and SVM when the size
of the training set increases. The reason might be that DTA-
LS-SVM and iDTA-LS-SVM essentially are multiple kernel
combination methods, which have a stronger representation
capability for smaller training datasets. Importantly, it is easily
seen that after changing the ratios of training and testing sets,
the proposed classiﬁers DTA-LS-SVM and iDTA-LS-SVM
still outperform LS-SVM and SVM on the testing accuracies
of all seven datasets. This can be seen, for example, in Table V
for the mammographic dataset at the 5:5 ratio; DTA-LS-SVM
not only performs the best in terms of overall accuracy but also
has the highest F score on the testing sets. Another example is
for the imbalanced Pima Indians dataset in Table IX in which
iDTA-LS-SVM maintains an advantage over the comparative
methods in terms of the F-score on the testing set. Overall,
the experimental results demonstrate that DTA-LS-SVM and
iDTA-LS-SVM are tolerant to the change of training sample
size and is a favorable choice for balanced and imbalanced
datasets in terms of the generalization capability.
In terms of the average training and testing time, Table.
XI in the 7:3 ratio case shows that DTA-LS-SVM and iDTA-
LS-SVM take much less training time compared to the other
methods. As explained in subsection III-C, the running time
of DTA-LS-SVM and iDTA-LS-SVM is the summation of the
running time in each module plus the running time for the
parameter tuning of λl (l = 1, 2, · · · , L) using a fast leaveone-out cross validation strategy. The regularization parameter
Cl (l = 1, 2, · · · , L) in each module and the kernel width δ can
be randomly chosen instead of using model selection strategy,
which also signiﬁcantly reduces the running time. Conversely,
LS-SVM and SVM take much more time to produce the
optimal values for C and δ by searching from 8 and 9 grid
values respectively which is much more computationally expensive. After this comparison, DTA-LS-SVM and iDTA-LS-
SVM exhibited superior advantages in running times compared
to other methods.
B. Real-world dataset: the community healthcare dataset
In this subsection, a real community healthcare dataset was
used in the experiments to investigate performances of the
proposed classiﬁers DTA-LS-SVM and iDTA-LS-SVM.
This dataset was collected in the nurse-led PolyU-Henry G.
Leong Mobile Integrative Health Care Centre (MIHC) in
August 2013. It contains 444 patient records with 33 features,
such as demographic, socio-economic, social relationship, and
social participation data. Additionally, information on the
patients’ health history, such as smoking and drinking habits,
chronic illnesses, and data from a series of health assessments
with descriptions was included. Due to the nature of both the
tests performed on the mobile clinic and the patients’ themselves, some values in the dataset were missing. For example,
language barriers affect the communications between nurses
and patients. The missing values in the dataset were ﬁlled in by
using the K-nearest neighbor (KNN) imputation method 
with their corresponding values from the nearest-neighbour
columns using the corresponding Euclidean distance.
The label information is the overall quality of life (QOL)
score on a 1-5 scale of 444 patients obtained using the World
Heath Organization questionnaire on quality of life: from the
Hong Kong short version (WHOQOL-BREF(HK)) framework
 . After analysis, the proportion of the two constructed
classes ("poor" and "good") was 122 and 322, respectively.
We aim to construct a classiﬁer to predict the QOL of elderly
patients ("poor" or "good") using these 33 features from this
MIHC dataset.
Since the two classes were originally imbalanced, iDTA-
LS-SVM was used on the datasets, and its classiﬁcation
performance was compared with those using DTA-LS-SVM,
LS-SVM, and SVM. In addition, for iDTA-LS-SVM and DTA-
LS-SVM, experiments were divided into two parts for more
detailed comparisons:
Part (a): In layer 1 of DTA-LS-SVM and iDTA-LS-SVM,
the complete portion of the MIHC dataset, which had no missing data was used to train an AK-LS-SVM model. From layer
2, the whole dataset after imputation and its corresponding
predictions from the previous model were used as the new
data input.
Part (b): In layer 1 of DTA-LS-SVM and iDTA-LS-SVM,
the whole MIHC dataset was used after imputation as the
The number of processing layers in both DTA-LS-SVM
and iDTA-LS-SVM was set to 3 in these experiments. The
ratio of training and testing data sets was set to 7:3. The
classiﬁcation accuracies and the running time of DTA-LS-
SVM, iDTA-LS-SVM and the comparative methods on the
training and testing datasets are listed in Table XII, XIII,
and XIV. From the experimental results, iDTA-LS-SVM in
Part (b) has achieved the best testing accuracy (0.7425) and
a F-score of 0.8553 compared to DTA-LS-SVM, LS-SVM
and SVM on the MIHC dataset. In Part (a), it achieved an
accuracy of 0.7331 and an F-score of 0.8492, which are still
higher than other methods. It shows that iDTA-LS-SVM is
more suitable for classiﬁcation on imbalanced datasets. The
traditional LS-SVM had the worst accuracy and F-score on
the testing dataset, iDTA-LS-SVM based on the combination
of several AK-LS-SVM modules achieved the best accuracy.
We believe that this performance improvement has arisen from
the stacked generalization principle and the transfer learning
mechanism. In terms of the running time, DTA-LS-SVM and
iDTA-LS-SVM remains superior over LS-SVM and SVM.
In addition, we also notice that in the experimental results
of DTA-LS-SVM and iDTA-LS-SVM, both accuracy and Fscore on the testing data set are higher than those on the
training data set. There might be two reasons to explain
these results. First, DTA-LS-SVM and iDTA-LS-SVM used
the stacked hierarchical architecture to enhance generalization
performances and this is reﬂected in the obtained results.
Second, in the data preparation, the KNN imputation method
was used to ﬁll in missing values in the MIHC dataset.
Following this procedure, the distributions in the training and
testing sets might be mismatched. Referring to Gonzalez and
Abu-Mostafa’s conclusions, we postulate that mismatched
distributions also occurred in this experiment, which may have
caused a higher testing accuracy and F-score in DTA-LS-SVM
and iDTA-LS-SVM.
C. Statistical analysis
In order to detect signiﬁcant differences among the experimental results of the proposed methods and the comparative methods, we also carried out the Friedman ranking
test followed by Holm post-hoc test , for multiple
comparisons on the testing sets of four balanced and four
imbalanced datasets in which the ratio of training and testing
data sets is 7 : 3. The Friedman ranking test was used to
evaluate whether there was a statistically signiﬁcant difference
among these methods. The null hypothesis is that there is no
statistically difference. If the p-value is smaller than 0.5, the
null hypothesis is rejected. The Holm post-hoc test was used
to further verify if there was a statistical difference between
the best Friedman ranking method and each remaining method.
We used α = 0.05 as the level of conﬁdence in all cases. First,
we conducted two Friedman ranking tests to assess whether
there are signiﬁcant differences between (1) DTA-LS-SVM
and the comparative methods on four balanced UCI datasets
in terms of accuracy and F1-score; (2) iDTA-LS-SVM and
the comparative methods on three imbalanced UCI datasets
and one real-world TRUS dataset in terms of F1-score. Here,
experimental results of both Part (a) and (b) on the TRUS
dataset were included.
The ranking results of Friedman test (1) in terms of accuracy
and F1-score are shown in Tables. XV and XVII, respectively.
The results reveal that there are signiﬁcant differences in the
both performance metrics between DTA-LS-SVM and other
comparative methods. Then we conducted the Holm post-hoc
tests to compare the best ranking method DTA-LS-SVM with
LS-SVM and SVM in terms of accuracy and F1-score, and
TABLE XI: TRAINING AND TESTING TIME (SECONDS) ON SEVEN UCI DATASETS
Running time
(i)DTA-LS-SVM
Imbalanced
breast cancer
Pima Indians
Indians liver
australian
credit approval
mammographic
TABLE XII: PERFORMANCE RESULTS ON THE MIHC DATASET USING iDTA-LS-SVM
Performances
iDTA-LS-SVM
0.7261±0.0094
0.7331±0.0218
0.7210±0.0102
0.7425±0.0266
0.8413±0.0063
0.8492±0.0147
0.8358±0.0100
0.8553±0.0233
TABLE XIII: PERFORMANCE RESULTS ON THE MIHC DATASET USING DTA-LS-SVM AND THE OTHER COMPARATIVE
Performances
DTA-LS-SVM
0.7203±0.0133
0.7266±0.0309
0.7294±0.0146
0.7224±0.0391
0.7510±0.0452
0.7050±0.0398
0.7274±0.0166
0.7201±0.0384
0.8374±0.0090
0.8380±0.0205
0.8430±0.0101
0.8379±0.0262
0.8452±0.0103
0.8268±0.0439
0.8435±0.0057
0.8341±0.0133
TABLE XIV: TRAINING AND TESTING TIME (SECONDS) ON THE MIHC DATASET
iDTA-LS-SVM
DTA-LS-SVM
TABLE XV: AVERAGE RANKINGS OF DTA-LS-SVM AND
THE COMPARATIVE METHODS ON BALANCED DATASETS IN
TERMS OF ACCURACY (p-VALUE= 0.049787)
DTA-LS-SVM
TABLE XVI: HOLM POST-HOC COMPARISON RESULTS FOR
DTA-LS-SVM AND THE OTHER METHODS IN TERMS OF
ACCURACY WITH α = 0.05
Holm = α/i
presented the results in Tables. XVI and XVIII where the
methods are ranked according to the obtained z-values. Holm
post-hoc test rejects the hypothesis of equivalence for the
methods with p < α/i. According to the results in these tables,
we know that DTA-LS-SVM is at least comparable to LS-
SVM and SVM on balanced datasets in terms of accuracy; and
is at least comparable to LS-SVM and statistically outperforms
SVM on balanced datasets in terms of F1-score.
The ranking results of Friedman test (2) are shown in Table.
XIX. The results reveal that there are signiﬁcant differences
TABLE XVII: AVERAGE RANKINGS OF DTA-LS-SVM AND
THE COMPARATIVE METHODS ON BALANCED DATASETS IN
TERMS OF F1-SCORE (p-VALUE= 0.038774)
DTA-LS-SVM
TABLE XVIII: HOLM POST-HOC COMPARISON RESULTS
FOR DTA-LS-SVM AND THE OTHER METHODS IN TERMS
OF F1-SCORE WITH α = 0.05
Holm = α/i
in terms of F1-score between iDTA-LS-SVM and the other
comparative methods. Then we conducted the Holm post-hoc
test to compare the best ranking method iDTA-LS-SVM with
LS-SVM and SVM, and presented the results in Table. XX.
According to the obtained results, iDTA-LS-SVM outperforms
the other methods on imbalanced datasets in terms of F1-score
in our experiments.
In summary, the proposed methods are at least comparable
to or even better than LS-SVM and SVM in terms of accuracy
and/or F1-score with the faster learning speed.
TABLE XIX: AVERAGE RANKINGS OF iDTA-LS-SVM AND
THE COMPARATIVE METHODS ON IMBALANCED DATASETS
IN TERMS OF F1-SCORE (p-VALUE= 0.022371)
iDTA-LS-SVM
TABLE XX: HOLM POST-HOC COMPARISON RESULTS FOR
iDTA-LS-SVM AND THE OTHER METHODS WITH α = 0.05
Holm = α/i
V. CONCLUSIONS AND FURTHER STUDY
AK-LS-SVM has been applied recently in many classi-
ﬁcation tasks. In this work, to improve its generalization
performance and learning speed, we proposed the novel
classiﬁer DTA-LS-SVM to be applied to balanced datasets,
which follows the stacked generalization principle and transfer learning mechanism, and its extended version iDTA-LS-
SVM on imbalanced datasets. DTA-LS-SVM and iDTA-LS-
SVM stack multiple AK-LS-SVMs layer-by-layer where the
prediction from the previous module becomes one additional
feature space together with the original data inputs to train the
next module of the next layer. Moreover, transfer learning is
embedded into the deep architecture to guarantee consistency
across adjacent modules, and thus the classiﬁcation capability
of the module at the higher layer can be further enhanced. In
addition, transfer learning based on AK-LS-SVM can perfectly
formulate a fast leave-one-out cross validation strategy for
the learning parameter tuning such that even though the
kernel width and the regularization parameter are randomly
selected, the performance of the proposed method can remain
outstanding. We compared the proposed method with the
traditional LS-SVM and SVM using additive Gaussian kernels
on seven public UCI datasets and one real world community
healthcare dataset. The experimental results indicate that the
proposed classiﬁers DTA-LS-SVM and iDTA-LS-SVM have
the superior advantages on the generalization performance
and the running time. Moreover, the experimental results also
reveal that both DTA-LS-SVM and iDTA-LS-SVM have the
potential to be applied in a real world application.
Even though the proposed methods show a promising
performance, this study has some limitations that will be
addressed in the future work. For example, DT-AK-LS-SVM
has to use the same kernel in every processing layer. The
proposed model will be investigated using different kernels in
the stacked architecture to further improve the generalization
performance.
VI. ACKNOWLEDGEMENT
The work was supported by the Australian Research Council
(ARC) under Discovery Grant DP140101366, and was also
supported in part by the Hong Kong Research Grants Council
(PolyU152040/16E), the Hong Kong Polytechnic University
(G-UC93, G-YBKX), the Tai Hung Fai Charitable Foundation
and the YC Yu Scholarship for Centre for Smart Health.