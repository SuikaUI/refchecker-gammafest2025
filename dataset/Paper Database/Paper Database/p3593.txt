Deep Structured Energy Based Models for Anomaly Detection
Shuangfei Zhai∗†
 
Yu Cheng∗§
 
Weining Lu‡
 
Zhongfei (Mark) Zhang†
 
†Binghamton University, Vestal, NY 13902, USA.
§IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA.
‡Tsinghua University, Beijing 10084, China.
∗Equal contribution
In this paper, we attack the anomaly detection
problem by directly modeling the data distribution with deep architectures.
We propose
deep structured energy based models (DSEBMs),
where the energy function is the output of a deterministic deep neural network with structure.
We develop novel model architectures to integrate EBMs with different types of data such as
static data, sequential data, and spatial data, and
apply appropriate model architectures to adapt to
the data structure. Our training algorithm is built
upon the recent development of score matching , which connects an EBM
with a regularized autoencoder, eliminating the
need for complicated sampling method. Statistically sound decision criterion can be derived
for anomaly detection purpose from the perspective of the energy landscape of the data distribution. We investigate two decision criteria for
performing anomaly detection: the energy score
and the reconstruction error. Extensive empirical
studies on benchmark tasks demonstrate that our
proposed model consistently matches or outperforms all the competing methods.
1. Introduction
Anomaly detection (also called novelty or outlier detection)
is to identify patterns that do not conform to the expected
normal patterns . Existing methods
for outlier detection either construct a proﬁle for normal
data examples and then identify the examples not conform-
Proceedings of the 33 rd International Conference on Machine
Learning, New York, NY, USA, 2016. JMLR: W&CP volume
48. Copyright 2016 by the author(s).
ing to the normal proﬁle as outliers, or explicitly isolate
outliers based on statistical or geometric measures of abnormality. A variety of methods can be found in the survey . Anomaly detection is to correctly
characterize data distribution in nature, so the normality of
the data characteristic can be characterized as a distribution
and any future data can be benchmarked against the normality. Apparently, the statistical power and accuracy of
the anomaly detection methods depend on the capacity of
the model that is used to characterize the data distribution.
Our work is inspired by the extraordinary capacity of deep
models which are able to capture the complex distributions
in real-world applications. Recent empirical and theoretical
studies indicate that deep architectures are able to achieve
better generalization ability compared to the shallow counterparts on challenging recognition tasks .
The key ingredient to the success of deep learning is its
ability to learn multiple levels of representations with increasing abstraction. For example, it is shown that properly regularized autoencoders are able to effectively characterize the data
distribution and learn useful representations, which are not
achieved by shallow methods such as PCA or K-Means.
However, deep models have not been systematically studied and developed for anomaly detection. Central questions critical to this development include: 1) how to effectively model the data generating distribution with a deep
model? 2) how to generalize a model to a range of data
structures such as static data, sequential data, and spatial
data? 3) how to develop computationally efﬁcient training
algorithms to make them scalable? and 4) how to derive
statistically sound decision criteria for anomaly detection
To answer these questions in a systematic manner, in this
work, we propose deep structured energy based models
(DSEBMs). Our approach falls into the category of enarXiv:1605.07717v2 [cs.LG] 16 Jun 2016
Deep Structured Energy Based Models for Anomaly Detection
ergy based models (EMBs) , which is
a powerful tool for density estimation. An EBM works by
coming up with a speciﬁc parameterization of the negative
log probability, which is called energy, and then computing
the density with a proper normalization. In this work, we
focus on deep energy based models ,
where the energy function is composed of a deep neural
network. Moreover, we investigate various model architectures as to accommodate different data structures. For example, for data with static vector inputs, standard feed forward neural networks can be applied. However, for sequential data, such as audio sequence, recurrent neural networks
(RNNs) are known to be better choices. Likewise, convolutional neural networks (CNNs) are signiﬁcantly more
efﬁcient at modeling spatial structures , such as on images. Our model thus allows the energy function to be composed of deep neural networks with
designated structures (fully connected, recurrent or convolutional), signiﬁcantly extending the application of EBMs
to a wide spectrum of data structures.
Despite its powerful expressive ability, the training of
EBMs remains fairly complicated compared to the training
of a deterministic deep neural network, as the former requires carefully designed sampling algorithm to deal with
its intrinsic intractability.
The need of efﬁcient training
algorithms is even more severe with DSEBMs, given its
complicated structured parameterization. To this end, we
adopt the score matching method as the
training algorithm, instead of the default maximum likelihood estimation (MLE). Similarly to , we
are able to train a DSEBM in the same way as that of a deep
denoising autoencoder (DAE) Vincent et al. , which
only requires standard stochastic gradient descent (SGD).
This signiﬁcantly simpliﬁes the training procedure and allows us to efﬁciently train a DSEBM on large datasets.
In order to perform the actual anomaly detection with a
trained DSEBM, we investigate two decision criteria, the
energy score and the reconstruction error. We show that
the two criteria are closely connected from the view of the
energy landscape, and evaluate their effectiveness under
different scenarios. We perform extensive evaluations on
several benchmarks varying from static, sequential to spatial data. We show that our method consistently matches or
outperforms the competing algorithms.
2. Background
2.1. Energy Based Models (EBMs)
EBMs are a family of probabilistic models that can be used
to build probability density functions. An EBM parameterizes a density function for input x ∈Rd in the form:
p(x; θ) = e−E(x;θ)
where E(x; θ) is the energy (negative log probability) associated with instance x; Z(θ) =
x e−E(x;θ)dx is the partition function to ensure that the density function integrates
to probability 1; θ are the model parameters to be learned.
The nice property of EBM is that one is free to parameterize the energy in any sensible way, giving it much ﬂexibility and expressive power. Learning is conducted by assigning lower energy (hence higher probability) to observed instances and vice versa. However, directly applying MLE is
impossible due to the intractability of the partition fuction
Z(θ), and thus one usually needs to resort to MCMC methods and approximate the integration with the summation
over samples from a Markov chain.
2.2. Restricted Boltzmann Machine (RBM)
RBM is one of the most well know examples of EBM. For continuous input data, the energy function of an RBM takes the form:
E(x; θ) = 1
j x + bj),
∈Rd×K (with Wj being the jth column),
b ∈RK (with bj as the jth element) and b′ ∈Rd are the parameters to learn; g(x) is the soft plus function log(1+ex).
Multiple RBMs can be trained and stacked on top of each
other to formulate a deep RBM, which makes it useful for
initializing multi-layer neural networks. Although efﬁcient
training algorithms, such as contrastive divergence, are proposed to make RBM scalable, it is still considerably more
difﬁcult to train than a deterministic neural network .
2.3. Denoising Autoencoders and Score Matching
Autoencoders are unsupervised models that learn to reconstruct the input. A typical form of autoencoder minimizes
the following objective function:
∥xi −f(xi; θ)∥2
where f(·; θ) is the reconstruction function that maps
Rd →Rd which is usually composed of an encoder
followed by a decoder with symmetrical architecture and
shared parameters. One particularly interesting variant of
autoencoders is DAEs , which learn
to construct the inputs given their randomly corrupted versions:
Eϵ∥xi −f(xi + ϵ; θ)∥2
Deep Structured Energy Based Models for Anomaly Detection
where ϵ ∼N(0, σ2I) is an isotropic Gaussian noise. DAEs
are easy to train with standard stochastic gradient descent
(SGD) and perform signiﬁcantly better than unregularized
autoencoders.
While RBM and DAEs are typically considered as two alternative unsupervised deep models, it is recently shown
they are closely related to each other. In particular, shows that training an RBM with score matching (SM) is equivalent to a one-layer
DAE. SM is an alternative method to MLE, which is especially suitable for estimating non-normalized density functions such as EBM. Instead of trying to directly maximize
the probability of training instances, SM minimizes the following objective function:
px(x)∥ψ(x; θ) −ψx(x)∥2
where px(x) is the true data distribution which is unknown;
ψ(x; θ) = ∇x log p(x; θ) = −∇xE(x; θ) and ψx(x) =
∇x log px(x) are the score function of the model and the
true density function, respectively. In words, J(θ) measures the expected distance of the scores between the model
density and the true density. shows that by
approximating the px(x) with the Parzen window density
i=1 N(xi, σ2I), minimizing Equation 5 yields an objective function in the same form as that of an autoencoder
in Equation 4, with a reconstruction function deﬁned as:
f(x; θ) = x −∇xE(x; θ).
Substituting E(x; θ) with Equation 2 yields a typical onelayer DAE (up to a constant factor):
f(x; θ) = Wσ(W T x + b) + b′,
where σ(x) is the sigmoid function
Equation 6 plays a key role in this work, as it allows one
to efﬁciently train an arbitrary EBM in a similar way to
that of a DAE, as long as the energy function is differentiable w.r.t. x and θ. As will be demonstrated below, this
makes training a deep EBM with various underlying structures in an end-to-end fashion, without the need of resorting
to sophisticated sampling procedures 
or layer wise pretraining .
3. Deep Structured EBMs
Deep architectures allow one to model complicated patterns efﬁciently, which makes it especially suitable for high
dimensional data. On the other hand, it is often necessary
to adapt the architecture of a deep model to the structure of
data. For example, recurrent neural networks (RNNs) have
been shown to work very well at modeling sequential data;
convolutional neural networks (CNNs) are very effective
at modeling data with spatial structure. In this work, we
extend EBMs further to deep structured EBMs (DSEBMs),
where we allow the underlying deep neural network encoding the energy function to take architectures varying from
fully connected, recurrent, and convolutional. This generalizes EBMs as well as deep EBMs
 as it makes our model applicable to a
much wider spectrum of applications, including static data,
sequential data and spatial data. Moreover, we show that by
deriving the proper reconstruction functions with Equation
6, DSEBMs can be easily trained with SGD, regardless of
the type of the underlying architecture. In the following,
we will elaborate the formulation of DSEBMs for the three
3.1. Fully Connected EBMs
This case is conceptually the same as the deep EBMs proposed in . Without loss of generality, we express the energy function of an L-layer fully connected EBM as:
E(x; θ) = 1
s.t. hl = g(W T
l hl−1 + b1), l ∈[1, L]
where Wl ∈RKl−1×Kl, bl ∈RKl are the parameters for
the lth layer; Kl is the dimensionality of the lth layer. The
0th layer is deﬁned as the input itself; and thus we have
K0 = d, h0 = x. We have explicitly included the term
2 which acts as a prior, punishing the probability
of the inputs that are far away from b′ ∈Rd.
Following the chain rule of gradient computation, one can
derive the reconstruction function as follows:
f(x; θ) = x −∇xE(x; θ) = h′
l hl−1 + bl) · (Wlh′
for l ∈[1, L −1],
L = 1 with 1 ∈RKL denoting a column vector
of all ones; · is the element-wise product between vectors.
One can then plug in the resulting f(x; θ) into Equation 4
and train it as a regular L-layer DAE.
3.2. Recurrent EBMs
formulation
 , where an EBM is
built at each time step, with parameters determined by an
underlying RNN. Formally, given a sequence of length T
x = [x1, ..., xT ], xt ∈Rd, we factorize the joint probability as p(x) = QT
t=1 p(xt|x1,...,t−1) with the chain rule of
Deep Structured Energy Based Models for Anomaly Detection
probability. For each time step t, p(xt|x1,...,t−1) is modeled as an EBM with energy E(xt|θt). In contrast to the
conventional formulation of EBM, the parameters of the
EMB θt is a function of the inputs from all the previous
time steps x1,...,t−1. A natural choice of θt is to let it be
the output of an RNN. As a concrete example, consider the
energy function at each step follows that in Equation 2, by
replacing W, b, b′ with W t, bt, b′t. Directly letting the RNN
to update all the parameters at each step requires a large
RNN with lots of parameters. As a remedy, proposes to ﬁx W t = W for
all time steps, only letting bt and b′t to be updated by the
ht = g(Whhht−1 + Whxxt + bh)
bt = Wbhht + b, b′t = Wb′hht + b′,
where Whh ∈RKrnn×Krnn, Whx ∈RKrnn×d, bh ∈
parameters
RKebm×Krnn, b ∈RKebm, Wb′h ∈Rd×Krnn, b′ ∈Rd are
the weights with which to transform the hidden state of the
RNN to the adaptive biases.
Training the recurrent EBM with score matching is similar to that of a fully connected EBM. To see this, we now
have p(x) =
t=1 E(xt;θt)
, where Zt = P
x e−E(x;θt)
is the partition function for the tth step. Plugging p(x)
into Equation 5, we have ψ(x)
∇x log p(x)
x1E(x1; θ1), ..., ∇T
xT E(xT ; θT )]T . In the last step, we
have made a simpliﬁcation by omitting the gradient term of
∇xjE(xi; θi), for j < i 1. Accordingly, Equation 6 is modiﬁed to f(x) = x −[∇T
x1E(x1; θ1), ..., ∇T
xT E(xT ; θT )]T .
One is then able to train the recurrent EBM by plugging
f(x) into Equation 4 and perform the standard SGD. Note
that in this case the standard backpropagation is replaced
with backpropagation through time, due to the need of updating the RNN parameters.
3.3. Convolutional EBMs
Previously, the combination of CNN and RBM have
been proposed in , where several layers of RBMs are alternately convolved with an image then
stacked on top of each other. In this paper, we take a significantly different approach by directly building deep EBMs
with convolution operators (with optional pooling layers or
fully connected layers), simply by replacing hL in Equation 8 with the output of a CNN. Using a deterministic
deep convolutional EBM allows one to directly train the
model end-to-end with score matching, thus signiﬁcantly
simpliﬁes the training procedure compared with . Formally, consider the input of the (l −1)th layer
1While one can also choose to use the full gradient, we ﬁnd
this simpliﬁcation works well in practice, yielding an objective in
a much more succinct form.
hl−1 ∈RKl−1×dl−1×dl−1 is a dl−1×dl−1 image with Kl−1
channels. We deﬁne hl as the output of a convolution layer:
˜Wl,j,k ∗hl−1,k + bl,j), j ∈[1, Kl].
Here W ∈RKl×Kl−1×dw,l×dw,l are the Kl convolutional
ﬁlters of size dw,l × dw,l; bl ∈RKl is the bias for each
ﬁlter. We denote the tilde operator ( ˜A) as ﬂipping a matrix
A horizontally and vertically; ∗is the ”valid” convolution
operator, where convolution is only conducted where the
input and the ﬁlter fully overlap. dl = dl−1 −dw,l + 1 is
thus the size of the output image following the valid convolution. In order to compute the reconstruction function
following equation 9, we modify the recurrence equation
l−1 for a convolutional operator as:
˜Wl,j,k∗hl−1,k+bl,j)·(Wl,j,k⊙h′
Here we have denoted ⊙as the ”full” convolution operator,
where convolution is conducted whenever the input and the
ﬁlter overlap by at least one position. Besides the convolution layer, we can also use a max pooling layer which typically follows a convolution layer. Denote dp × dp as the
pooling window size, passing hl−1 through a max pooling
layer gives an output as:
hl,k,p,q, xl,k,p,q, yl,k,p,q
(p−1)dp+1≤i≤pdp,(q−1)dp+1≤j≤qdp hl−1,k,i,j.
Here the max operation returns the maximum value as the
ﬁrst term and the corresponding maximum coordinates as
the second and third. Rewriting the recurrence equation
corresponding to Equation 8 yields:
l−1,k,xl,k,p,q,yl,k,p,q = h′
l,k,p,q, p, q ∈[1, dl],
where the unassigned entries of h′
l−1 are all set as zero.
The derivation above shows that one is able to compute the
reconstruction function f(x) for a deep convolutional EBM
consisting of convolution, max pooling and fully connected
layers. Other types of layers such as mean pooling can also
be managed in a similar way, which is omitted due to the
space limit.
4. Deep Structured EBMs for Anomaly
Performing anomaly detection given a trained EBM naturally corresponds to identifying data points that are assigned low probability by the model.
With a trained
DSEBM, we can then select samples that are assigned
Deep Structured Energy Based Models for Anomaly Detection
probability lower than some pre-chosen threshold pth as
outliers. Although computing the exact probability according to Equation 1 is intractable, one can immediately recognize the following logic:
p(x; θ) < pth ⇒log p(x; θ) < log pth ⇒
E(x; θ) > log pth + log Z(θ) ⇒E(x; θ) > Eth.
Here we have used the fact that Z(θ) is a constant that does
not depend on x; hence selecting samples with probability
lower than pth is equivalent to selecting those with energy
higher than a corresponding energy threshold Eth. 2
Moreover, motivated by the connection of EBMs and
DAEs, we further investigate another decision criteria
which is based on the reconstruction error. In the early
work of , autoencoders (which
they call replicator neural networks) have been applied to
anomaly detection. However, their adoption autoencoders
are unregularized, and thus could not be interpreted as a
density model. The authors then propose to use the reconstruction error as the decision criterion, selecting those
with high reconstruction error as outliers. On the other
hand, with a DSEBM trained with score matching, we are
able to derive the corresponding reconstruction error as
∥x −f(x; θ)∥2
2 = ∥∇xE(x; θ)∥2
2. The corresponding decision rule is thus ∥∇xE(x; θ)∥2
2 > Errorth, with some
threshold Errorth. 3 In other words, examples with high
reconstruction errors correspond to examples whose energy
has large gradient norms. This view makes reconstruction
error a sensible criterion as on the energy surface inliers
usually sit close to local minimums, where the gradient
should be close to zero. However, using the reconstruction error might produce false positive examples (outliers
that are classiﬁed as inliers), as if a sample sits close to a
local maximum on the energy surface, the gradient of its
energy will also be small. We demonstrate the two criteria in Figure 4 with a 1D example. As we see, for x1 and
x3, both energy and reconstruction error produces the correct prediction. However, x2 is a false positive example
under the reconstruction error criterion, which energy correctly recognizes as an outlier. However, note that in a high
dimensional input space, the probability that an outlier resides around a local maximum grows exponentially small
w.r.t. the dimensionality. As a result, the reconstruction
error still serves as a reasonable criterion.
5. Experimental Evaluation
In this section, we evaluate the proposed anomaly detection
framework, where our two proposed anomaly detection cri-
2For a recurrent DSEBM, this decision rule is modiﬁed as
t=1 E(xt; θt) > Eth
3For a recurrent DSEBM, this decision rule is modiﬁed as
t−1 ∥∇xtE(xt; θt)∥2
2 > Errorth
Figure 1. A 1D demonstration of using energy E(x) and the reconstruction error ∥∇xE(x; θ)∥2
2 (denoted as error in the ﬁgure)
as the decision criterion. For each of the two criteria, samples with
value (energy or reconstruction error) above the chosen threshold
are identiﬁed as outliers.
teria using energy and reconstruction error are abbreviated
as DSEBM-e and DSEBM-r, respectively.
Our experiments consist of three types of data: static data, sequential data (e.g., audio) and spatial data (e.g., image), where
we apply fully connected EBM, recurrent EBM and convolutional EBM, respectively. The speciﬁcations of benchmark datasets used are summarized in Table 1. To demonstrate the effectiveness of DSEBM, we compare our approach with several well-established baseline methods that
are publicly available. Ground truth labels are available in
all data sets; and we report precision (mean precision), recall (mean recall), and F1 score (mean F1) for the anomaly
detection results achieved by all methods. Below we detail
the baselines, explain the experimental methodology, and
discuss the results.
5.1. Static Data
There benchmark datasets are used in this study: KDD99
10 percent, Thyroid and Usenet from the UCI repository
 . The training and test sets are split by 1:1
and only normal samples are used for training the model.
We compare DSEBMs (with 2-layer fully connected energy function) with a variety of competing methods, including two reconstruction-based outlier detection methods,
PCA and Kernel PCA, two density-based methods Kernel
Density Estimator (KDE) and Robust Kernel Density Estimator (RKDE) , along
with the traditional one-class learning method One-Class
SVM (OC-SVM) . We also include
the method proposed in , named
AutoEncoder Outlier Detection (AEOD) as one baseline.
Deep Structured Energy Based Models for Anomaly Detection
# Dimensions
# Instances
Outlier ratio (ρ)
KDD99 
CUAVE 
0.1 ≤ρ ≤0.4
NATOPS 
0.1 ≤ρ ≤0.4
0.1 ≤ρ ≤0.4
Caltech-101 
0.1 ≤ρ ≤0.4
MNIST 
0.1 ≤ρ ≤0.4
CIFAR-10 
0.1 ≤ρ ≤0.4
Table 1. Speciﬁcation of benchmark data sets we used. Avg(t) is the length of sequence for sequential data.
Kernel PCA
Table 2. KDD99, Thyroid, Usenet: precision, recall and F1 over over the static data sets of seven methods. For each column, the best
result is shown in boldface.
The results are shown in Table 2. We see that, overall,
DSEBM-e and DSEBM-r achieve comparable or better performances compared with the best baselines. On Thyroid,
the performances of DSEBMs are slightly worse than OC-
SVM and RKDE. We speculate that this is most likely
caused by the low dimensionality of the dataset (10), where
kernel based methods (which OC-SVM and RKDE are)
are very effective. However, on Usenet which has a much
higher dimensionality (659), DSEBM-e achieves the best
result, measured by recall and F1. This is consistent with
our intuition, as on high-dimensional datasets, deep models are more effective and necessary to resolve the underlying complication. On KDD99, DSEBM-e also achieves the
5.2. Sequential Data
For this task, we use three sequential datasets: (1) CUAVE
which contains audio-visual data of ten spoken digits (zero
to nine); (2) NATOPS which contains 24 classes of bodyand-hand gestures used by the US Navy in aircraft handling aboard aircraft carriers; (3) FITNESS which contains
users’ daily ﬁtness behaviors collected from health care devices, including diet, sleep and exercise information. According to the BMI change, the users are categorized into
two groups ”losing weight” and ”gaining weight”. For a
single category, the outlier samples are simulated with a
proportion 0.1 ≤ρ ≤0.4 from other categories.
datasets are split into training and test by 2:1, where 2/3
of the normal samples are used for training split. We compare DSEBM-r and DSEBM-e with three static baselines,
Kernel PCA, RKDE and OC-SVM. Also, we include two
sequential methods: 1) HMMs, where the model is trained
with the normal training sequences, and the posterior probability p(y|x) of each test sequence is computed as the normalized negative log-likelihood; 2) OCCRF , where the model learns from a one-class dataset and
captures the temporal dependence structure using conditional random ﬁelds (CRFs). Table 3 (with ρ = 0.3) shows
the performances of all the methods. We see that DSEBMe achieves the highest mean precision and mean F1 score
for most cases while DSEBM-r achieves the second best,
both beating the competing methods with a margin. HMM
shows high precision rates but low recall rates, resulting
in a low F1 score. OCCRF is the second best performing method, following our two DSEBM variants, due to
its ability to capture the temporal information with CRFs.
On FITNESS, DSEBMs improves over 4%, 6% and 5% on
mean precision, mean recall and mean F1 over the other
baselines. Similar trends can be seen in Figure 3 (with
ρ varying from 0.1 to 0.4). All these results demonstrate
DSEBMs’ ability to beneﬁt from the rich temporal (with
large Avg(t)) information, thanks to the underlying RNN.
5.3. Spatial Data
We use three public image datasets: Caltech-101, MNIST
and CIFAR-10 for this sub task.
On Caltech-101, we
choose 11 object categories as inliers, each of which contains at least 100 images, and sample outlier images with
a proportion 0.1 ≤ρ ≤0.4 from the other categories.
On MNIST and CIFAR-10, we use images from a single
Deep Structured Energy Based Models for Anomaly Detection
Figure 2. The means F1 scores on the three sequential datasets with outlier ratio from 0.1 to 0.4.
CUAVE (ρ = 0.3)
NATOPS (ρ = 0.3)
FITNESS (ρ = 0.3)
Kernel PCA
Table 3. CUAVE, NATOPS, FITNESS: mean precision (mPrec), mean recall (mRec) and mean F1 (mF1) over over the sequential data
sets of seven methods. For each column, the best result is shown in boldface.
Caltech-101 (ρ = 0.3)
MNIST (ρ = 0.3)
CIFAR-10 (ρ = 0.3)
Kernel PCA
Table 4. Caltech-101, MNIST, CIFAR-10 datasets: mean precision (mPrec), mean recall (mRec) and mean F1 (mF1) over over the
image data sets of eight methods. For each column, the best result is shown in boldface.
category as inliers, and sample images from the other categories with a proportion 0.1 ≤ρ ≤0.4. Each dataset
is split into a training and testing set with a ratio of 2:1.
We compare DSEBMs (with one convolutional layer +
one pooling layer + one fully connected layer) with several baseline methods including: High-dimensional Robust PCA (HR-PCA), Kernel PCA (KPCA), Robust Kernel
Density Estimator (RKDE), One-Class SVM (OC-SVM)
and Unsupervised One-Class Learning (UOCL) . All the results are shown in Table 4 with ρ = 0.3.
We see that DSEBM-e is the best performing method overall in terms of mean recall and mean F1, with particularly
large margins on large datasets (MNIST and CIFAR-10).
Measured by F1, DSEBM-e improves 3.5% and 2.3% over
the best-performing baselines.
Figure 3 with ρ varying
from 0.1 to 0.4 also demonstrates consistent results.
5.4. Energy VS. Reconstruction Error
In terms of the two decision criteria of DSEBM, we observe
that DSEBM-e consistently outperforms DSEBM-r on all
the benchmarks except for the Thyroid dataset. This veri-
ﬁes our conjecture that the energy score is a more accurate
decision criterion than reconstruction error. In addition, to
gain further insight on the behavior of the two criteria, we
demonstrate seven outliers selected from the Caltech-101
benchmark in Figure 4. For each image, the energy scores
are displayed at the second row in red, followed by the reconstruction error displayed in green and the correct inlier
class. Interestingly, all the seven outliers are visually similar to the inlier class and have small reconstruction errors
(compared with the threshold). However, we are able to
successfully identify all of them with energy (which are
higher than the energy threshold).
Deep Structured Energy Based Models for Anomaly Detection
Figure 3. The means F1 scores on the three image datasets with outlier ratio from 0.1 to 0.4.
Figure 4. Seven outliers from the Caltech-101 dataset. Each column is the image followed by its energy score (displayed in red),
reconstruction error (displayed in green) and the inlier class name. The thresholds for DSEBM-e and DSEBM-r are Eth = 0.4882
and Errorth = 0.5125, respectively. Samples with E(x) > Eth and Error(x) > Errorth are regarded as outliers by DSEBM-e and
DSEBM-r, respectively.
6. Related Work
There has been a large body of work concentrating on
anomaly detection , noticeably: (1)
the reconstruction based methods such as PCA and Kernel
PCA, Robust PCA and Robust Kernel PCA; (2) the probability density based methods, including parametric estimators and nonparametric estimators
such as the kernel density estimator (KDE) and the more recent robust kernel density estimator (RKDE); (3) methods
of learning a compact data model such that as many as possible normal samples are enclosed inside, for example, oneclass SVM and SVDD. Graham et al. proposed a method
based on autoencoder . However,
all the methods above are static in nature which does not
assume the structure of data. Two types of data are extensively studied in sequential anomaly detection: sequential
time series data and event data. Sun et al. proposes a technique that uses Probabilistic Sufﬁx Trees (PST) to ﬁnd the
nearest neighbors for a given sequence to detect sequential
anomalies in protein sequences . Song et
al. presents a one class conditional random ﬁelds method
for general sequential anomaly detection tasks . Our model is signiﬁcantly different from the above
mentioned methods, where our use of RNN encoded EBM
gives us much modeling power and statistical soundness
at the same time.
Among the few approaches designed
for spatial data, proposes to use
CNNs in least-squares direct density-ratio estimation, and
demonstrated its usefulness in inlier-based outlier detection
of images. Despite the usage similar use of CNNs, our
work takes a very different path by directly modeling the
density. Methodology-wise, there is also a recent surge of
training EBMs with score matching . However, most of
them are constrained to shallow models, thus limiting their
application to relatively simple tasks.
7. Conclusion
We proposed training deep structured energy based models
for the anomaly detection problem and extended EBMs to
deep architectures with three types of structures: fully connected, recurrent and convolutional. To signiﬁcantly simplify the training procedure, score matching is proposed
in stead of MLE as the training algorithm. In addition,
we have investigated the proper usage of DSEBMs for the
purpose of anomaly detection, in particular focusing on
two decision criteria: energy score and reconstruction error. Systematic experiments are conducted on three types
of datasets: static, sequential and spatial, demonstrating
that DSEBMs consistently match or outperform the stateof-the-art anomaly detection algorithms. To be best of our
knowledge, this is the ﬁrst work that extensively evaluates
deep structured models to the anomaly detection problem.
Deep Structured Energy Based Models for Anomaly Detection