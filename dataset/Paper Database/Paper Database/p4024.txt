Towards 6G zero touch networks:
The case of automated Cloud-RAN deployments
ISBN:978-1-6654-3162-0 (IEEE)
Bini Angui, Romuald Corbel, Veronica Quintuna Rodriguez, Emile Stephan
Orange Labs, 2 Avenue Pierre Marzin, 22300 Lannion, France
Abstract—The arrival of 6G technologies shall massively increase the proliferation of on-demand and
customized networks on the ﬂy notably requires fully
automation to guarantee commissioning-time acceleration. In this paper, we address the deployment automation of an end-to-end mobile network, with special
focus on RAN units (referred to as Cloud-RAN). The
Cloud-RAN automation is especially challenging due
to the strong latency constraints expected in 6G as
well as the required management of physical antennas.
To automatically instantiate a Cloud-RAN chain, we
introduce a Zero Touch Commissioning (ZTC) model
which performs resource discovery while looking for
both antennas and computing capacity as near as possible to the targeted coverage zone. We validate the
ZTC model by a testbed which deploys, conﬁgures and
starts the network service without human intervention
while using Kubernetes-based infrastructures as well as
open-source RAN and core elements implementing the
mobile network functions.
Keywords: Automation, zero-touch deployment,
Cloud-RAN, Kubernetes, resource discovery, management system, edge, on-demand networks, cloudnative.
I. Introduction
Automation, edge computing and artiﬁcial intelligence are
key enablers of 6G technologies. 6G use-cases particularly address sustainable development, massive twinning,
tele-presence, robots and local trust zones , . The
requirement of local coverage for temporary usage will
rapidly increase to achieve ephemeral and massive video
transmissions during sportive/cultural events or even to
launch medical emergency systems for specialized teleassistance. The hosting infrastructure of 6G technologies
should autonomously determine the best location to deploy 6G network functions, in order to fulﬁll the service
requirements.
6G cloud-native infrastructures require to support automated deployment pipelines, not only to hold zero
touch paradigms (i.e., fully automated procedures without human intervention) but moreover to dynamically
meet the performance requirements of 6G services in terms
of bandwidth, latency and computing capacity. Cloud
infrastructures shall natively able to append resources to
support the workload or to move speciﬁc services to other
locations to fulﬁll latency or computing requirements.
6G cloud-native network services can be conceived as a
chain of microservices running on cloud infrastructures.
One of the most challenging network services intended to
be fully cloudiﬁed is the Radio Access Network (RAN)
of mobile 6G networks, referred to as Cloud RAN. The
complexity of Cloud-RAN systems is on the tiny
latency requirements involved in the physical layer, i.e.,
on the base band processing of radio signals. It is worth
noting that the time budget to build and transmit RAN
subframes is one and two milliseconds in the downlink and
uplink, respectively (ultra low latency 6G usecases shall
even require more reduced time slots).
New RAN architectures plan to split up the RAN functions, and to deport part of the radio processing higher
in the network in order to have a common intelligence to
various radio sites. The 3GPP particularly aims to split
the RAN functions into three units Central Unit (CU),
Distributed Unit (DU) and Remote Unit (RU), this latter
placed near to antennas. , . When considering split
Cloud-RAN architectures the time budget needs to be
shared between the runtime of RAN functions and the
transmission time between the distant RAN units. Furthermore, the distance between the RU and DU (referred
to as fronthaul size) needs to be as long as possible to
enable centralization beneﬁts (i.e., DUs coordination) but
short enough to meet the latency requirements (less than
1 millisecond). Various Cloud-RAN deployment architectures have been studied to determine the most adapted
placement of CU, DU, RU (distance between them) units
in the Cloud infrastructure (involving Regional, Edge, and
Far Edge zones). O-RAN has particularly deﬁned six deployment scenarios and speciﬁed four of them . Furthermore, adapted resources discovery mechanisms need
to be held during the automated 6G Cloud-RAN set up,
to avoid exceeding the transmission time budget.
As various challenges need to be addressed to meet both
latency and computing constraints when automatically
deploying 6G Cloud-RAN systems, we introduce in this
paper, a specialized mechanism referred to as Bootstrapper of Cloud-RAN (BCR) to support the zero touch
Cloud-RAN management. It involves the resource discovery (fulﬁlling both computing and latency constraints to
 
cover the targeted zone), synchronization and automated
conﬁguration of RUs and DUs-CUs which are deployed on
distant Kubernetes-based nodes. The automated deployment can be launched by means of an API coming from
upper orchestration and management layers or by using a
dedicated Network Management System (NMS).
As a proof of concept, we implement the proposed ZTC
mechanism and the NMS including a graphic interface
to create, delete and monitor deployments and resources.
We validate the ZTC model while using open source
Cloud-RAN units , developed on the basis of Open
Air Interface (OAI) code. We ﬁnally evaluate the
performance of the proposed model in terms of agility and
resource consumption in a fully cloudiﬁed infrastructure.
Results demonstrate energy and resource consumption
eﬃciency when running and deploying the proposed zero
touch model. It is notably in line with 6G goals concerning
sustainable development.
This paper is organized as follows: In Section II, we
describe the architectural framework, as well as the placement patterns to be adopted when deploying Cloud RAN
systems. The ZTC model and the various automated deployment stages are described in Section III. In Section ??,
we exhibit the testbed, while the performance evaluation
is presented in Section IV. Main conclusions are ﬁnally
presented in Section V.
II. Architecture Patterns for 6G Cloud-RAN
at the Edge Network
6G network services requiring low latency aim to be placed
as close as possible to users. Edge computing paradigm
focuses on this principle and enables both services and network functions to be placed near to users to reduce latency
and bandwidth. Cloud-native network functions as those
of Cloud-RAN aim to be deployed on edge architectures,
particularly supported by a cloud infrastructure manager
as Kubernetes(K8s) .
The edge reference architecture for Cloud-RAN systems
considers three levels of nodes, namely, Regional Cloud,
Edge Cloud and Far Edge Cloud. This topology (referred
to as substrate network) has been notably adopted by O-
RAN for the Cloud-RAN deployment , . Figure 1
illustrates these cloud areas.
Data centers (nodes) placed at the regional level allow
centralizing network functions or application services that
are not latency sensitive, e.g., virtual Broadband Network
Gateway (vBNG), User Plane Function (UPF) or even
the upper layer RAN functions. While regional nodes are
placed around two hundred kilometers from end users (10
milliseconds), edge nodes are located only few tens of
kilometers from them (1 millisecond). Some RAN functions as the radio scheduler and radio link controller are
today clearly identiﬁed to be deployed at this level. Far
edge nodes aim to host latency sensitive functions as
those of the 6G RAN physical layer (e.g. modulation,
demodulation).
Fig. 1: Substrate Network
Various deployment scenarios intending to centralize or
distribute RAN units have been widely studied in the
literature, and considered by O-RAN , . In this
work, we adopt the deployment of CU, DU, and RU in
Regional, Edge, and Far Edge Cloud, respectively, which
is in line with O-RAN’s Scenario F. It is worth noting that
the following Zero Touch Commissioning (ZTC) model is
compliant with any deployment scenario.
Deploying Cloud-RAN network functions at the Edge
requires to address deployment challenges in the hosting
infrastructure, i.e., K8s. Today, two main approaches can
be used to deploy K8s at the edge: (i) Distributed, which
enables deploying the whole K8s cluster within edge nodes.
Beyond the complexity of coordinating remote independent clusters, the drawback of distributed architectures is
on the number of required replicas of control plane servers
at each Edge Cloud (e.g. 5 servers for a high availability
cluster). (ii) Centralized, which keeps the control plane
(master nodes) at the regional (central) cloud and manages
remote edge nodes. The centralized deployment particularly allows to limit the number of control servers reducing
CAPEX and OPEX. Thus, the centralized approach is
in line with sustainable development goals expected in
6G. However, in the last years, separating worker nodes
from their masters has been strongly avoided due to
latency constraints between these two K8s elements. A
recent study demonstrates the feasibility of distancing
worker nodes from masters. In the following, we use the
centralized approach as a basis to deﬁne the zero-touch
deployment Cloud-RAN solution.
III. ZTC Cloud-RAN Model
A. ZTC architectural framework
The general architectural framework of the proposed ZTC
model is presented in Figure 2. It includes the upper
layers (orchestration and core commerce), the hosting K8s
cluster running on bar-metal servers and the RAN units
implementing the various radio functions.
The proposed ZTC procedure can be launched either
from a Management and Orchestration (MANO) platform,
e.g., Open Network Automation Platform (ONAP) or by
using a dedicated NMS. Thus, the orchestration service
Fig. 2: Architectural Framework
order requiring the instantiation of the various functions
(containers) composing a Cloud-RAN system directly interacts with the K8s control elements, and notably with
the proposed ZTC module. The service order particularly
contains the negotiated service level (e.g., geographic location, coverage area, number of supported users, data
rate, etc) and the performance constraints to guarantee
the correct functioning of the service, (e.g., minimum
bandwidth between the hosting data-centers, maximum
end-to-end latency, etc).
B. Zero touch deployment procedure
To automate the Cloud-RAN deployment, we introduce in
this work a speciﬁc control element referred to as Bootstrapper of Cloud-RAN - Server (BCR-S) which performs
automated resource discovery and manages the ZTC procedures (instantiation, conﬁguration, data base updates,
etc). Independent control agents are also instantiated at
each Cloud-RAN element (RU, DU, CU). These agents are
referred to as Bootstrapper of Cloud-RAN - Client (BCR-
C). In addition to the control elements, the proposed
automation system uses two catalogs, namely the Resource
Catalog and the Deployment catalog, which respectively
contain the information of K8s infrastructure (nodes,
links) and the runtime information (e.g., IP addresses) of
containers belonging to a given service (i.e., Cloud-RAN).
The main steps involved in the automated Cloud-RAN
deployment are (see Figure 3):
Building the Resource Catalog (1): The Resource catalog
which is periodically updated by the BCR-S. It notably
contains infrastructure nodes information (CPU, RAM,
disk resources, geographical position, and connected antennas). The information stored in the catalog enables
the validation of the Cloud-RAN requirements in terms
of bandwidth, latency and computing capacity. This infrastructure catalog can be exposed to the orchestration
layer (e.g. ONAP) in order to manage the lifecycle of endto-end services.
Service deployment order (2): The automated instantiation of Cloud-RAN elements starts with the service order
coming from the orchestration layer or from the NMS.
Resource discovery (3): The discovery function selects the
potential hosting nodes that fulﬁll the requirements of the
service order, i.e., geographic location, resource availability (antenna, RAM, CPU, disk), latency (e.g., the delay
between RU and CU must be lower than 1 millisecond),
and bandwidth (the required capacity of links connecting
RUs and DUs which varies with the functional split and
cell features).
List of best RU/DU/CU/antenna aﬃliations (4): While
using the data obtained during the discovery stage, the
BCR-S builds a list of the most adapted K8s nodes to
host the RAN units RU, DU, CU according to the Cloud-
RAN service order. It proposes one or more chains of K8s
nodes that fullﬁl the aﬃliation requirements between the
RAN units.
Performance validation (5):
The BCR-S provides a
score to each proposed chain of nodes according to the
fulﬁllment of the Cloud-RAN service requirements. The
BCR-S launches the performance test for each chain of
nodes while evaluating Round Trip Time (RTT) (end-toend latency), hardware resources (nodes’ capacity), and
network throughput (available bandwidth in the optic
ﬁber linking the nodes). The highest ranked chain of nodes
will be then selected to host the service. If no chain meets
the requirements, the BCR-S aborts the deployment.
Automated Helm Charts creation (6): After performance
validation, the BCR-S creates the required Helms charts
(ﬁles used to deﬁne, install, and upgrade K8s applications)
to deploy each of the RAN units, i.e., RU, DU, and
CU, in the selected nodes. Helm charts uses the SLA
parameters extracted from the service order (number of
users, coverage, etc), as well as, the information of the
hosting K8s nodes previously selected.
Launching the deployment of RAN units (7): While using
created helm charts, the BCR-s triggers the deployment
of RU, DU, CU units from the K8s Master node.
Creation of RAN containers (8): The K8s master creates
the containers of each RAN unit, RU, DU, CU according
to the helm chart information. Each RAN container (RU,
DU, CU) gets a dynamic IP address while using DHCP.
A BCR-C agent is deployed together with each RAN
element to enable the ZTC. The BCR-C is then the
entity that interacts with the BCR-S to aﬃliate (link) the
RAN elements and to give rise to a complete gNodeB.
During the conﬁguration of RUs, the BCR-S provides the
serial number of the selected antenna to the RU’s BCR-
C. This latter sets up the antenna parameters in the RU
conﬁguration ﬁle.
Getting and saving the information of RAN containers deployment (9): After the container creation, all information
related to the deployment (e.g. IP addresses of RAN units
and nodes) is sent to the deployment catalog. This latter
stores the information of each RAN unit and enables the
coordination with each other during their lifecycle (deployment, removal, monitoring, activation, deactivation, etc).
Fig. 3: Call-ﬂow of ZTC process
Aﬃliating and starting RAN units (10): While using the
information of the deployment catalog, the BCR-S provides to the BCR-C all the required parameters required
to automatically conﬁgure and aﬃliate RAN units one
to each other (RU needs to get the IP address of DU
and this latter requires the RU IP, similarly, the DU and
CU requires to know their respective IP addresses). Once
conﬁguration and aﬃliation is ﬁnished the BCR-C starts
the RU, DU, and CU applications.
C. Testbed architecture
The testbed architecture includes the K8s entities, the
various control elements required for the Cloud-RAN zero
touch provisioning and the RAN units spread into three
cloud zones: Regional, Edge and Far Edge respectively
represented by three servers. Each cloud zone contains
a single K8s worker node which is provisioned of a GPS
exporter agent in order to get the exact position of servers
when automating the deployment as a function of coverage
needs. Figure 4 illustrates the testbed architecture.
The Regional Cloud hosts the main K8s entities, Ingress
Controller and Master node, as well as the Private Registry
containing the RU, DU, CU images. RAN units implement
the 7.3 functional split and have been developed by Orange
on the basis of OAI , , code.
The worker node of the Regional Cloud hosts the ZTC
control elements, namely Deployment Catalog, Resource
Catalog, BCR-S, as well as, an API BCR-S (to interconnect one or more BCR-S to catalogs) and the BCR-
NMS. Central monitoring elements are also placed in the
Regional worker node, namely, Prometheus, Grafana, State
Metrics, and Node exporter.
The Edge Cloud hosts the CU-DU units and the corresponding BCR-C agent. The Far Edge Cloud contains the
RUs and their corresponding BCR-C agents, as well as two
radio elements (USRP B210 cards) directly connected by
Both Edge and Far Edge Cloud contain a node exporter
instance, which is used to collect metrics required for the
Fig. 4: Testbed architecture
monitoring.
In order to evaluate the end-to-end service connection,
the testbed integrates an OAI-generic core network. A
user equipment can then be attached and traﬃc can be
exchanged.
D. Functional experiments
1) Instantiating a Cloud-RAN chain: When triggering the
creation of a ‘New Cloud-RAN’ chain, various parameters
are required in order to set up and to instantiate the RAN
units, such as, the geographical area to be covered, the
system name (referred to as tag), the maximum number
of supported users, etc. Conﬁguration settings are either
introduced by using the BCR-NMS, or sent into an API
coming from orchestration and service management layers.
The BCR-S uses the settings to select the most adapted
hardware (K8s nodes and antenna) to host the service
chain (Cloud-RAN). Thus, the BCR-S creates the Helm
charts while using the identiﬁed nodes and received settings. Helm charts are triggered by the BCR-S from the
K8s Master.
During the instantiation, the ZTC system, and more concretely the BCR-S and the BCR-C interact between each
other in order to interconnect (setup network parameters)
the RAN units. Thus, CU-DU registers the IP address of
RU and vice-versa; antenna ports are also identiﬁed, When
using OAI-based code, the selected antenna is speciﬁed by
sdr addrs = ”serial=SERIAL NUMBER”.
The automation system (ZTC system) registers the deployment of the various containers (RAN units) in the
Deployment Catalog and updates the system status while
including servers/nodes occupancy, antennas availability
in the Resource Catalog, Cloud RAN service health, coverage of the deployed RAN, etc.
Fig. 5: NMS - Available Clouds & management options
2) Cloud RAN Management: The implemented ZTC system make available a NMS which notably includes a
graphic interface to enable instantiating, deleting and
managing Cloud-RAN systems.
A map enables to locate in real-time all K8s nodes that
are registered in the system, as well as, the deployed
RAN units. The map additionally indicates the number
of occupied/available antennas at each K8s node (See
Figure 5 for an illustration).
IV. Performance Evaluation
We evaluate three main aspects: (i) deployment time
(ii) hosting infrastructure health and (iii) service (RAN
units) health. Main Key Performance Indicators (KPIs)
are: deployment time, CPU and RAM consumption during
deployment and execution.
A. Deployment time
While using Intel(R) Xeon(R) CPU E5-2640 v3 @
2.60GHz, 125GB for Regional and Edge Cloud, and Intel(R) Core(TM) i7-8559U CPU @ 2.70GHz, 32 GB RAM
for Far Edge Cloud, the experimented time to deploy
the CU-DU and RU is 32 seconds. This measured time
includes the service start of each unit, the Cloud-RAN
system is then ready to receive users connections. The
deployment time of the ZTC module is 51 seconds (only
required once).
B. Hosting infrastructure evaluation
The (Regional, Edge and Far Edge Cloud) evaluation is
carried out by means of the deployed monitoring tools:
Grafana, Prometheus and Node-Exporter. These entities
enable to obtain (in real-time) the resource consumption
at each K8s node.
We are particularly interested in evaluating the resource
consumption before, during and after the deployment of
both ZTC modules and Cloud RAN units. Thus, we deﬁne
four main time indicators t1, t2, t3 and t4, which respectively identify the moment when the deployment of ZTC
Helm charts is triggered from the K8s master node, the
instant where the ZTC system starts running, the moment
where the deployment of RAN units is launched and the
instant when the creation of all RAN units is ﬁnished
Number of slots
Regional Cloud
Edge Cloud
Far Edge Cloud
Fig. 6: CPU consumption
Regional Cloud
Edge Cloud
Far Edge Cloud
Fig. 7: RAM consumption
(i.e., Cloud-RAN execution starts). Figure 6 illustrates
the CPU consumption that is represented by the number
of used CPU slots, while Figure 7 shows the RAM occupancy in GigaBytes. Experimentation evidenced t1 = 12,
t2 = 63, t3 = 138 and t4 = 170 seconds.
ZTC system deployment (t1-t2): When the deployment in
Regional Cloud is ﬁnished the CPU consumption drops
back down to pre-deployment levels. Concerning Edge
and Far Edge nodes, we observe a spike up in the CPU
consumption due to the deployment of GPS simulators.
ZTC operation (t2-t3): The resource consumption during
the runtime (operation) of the proposed automation and
management system is negligible. The only signiﬁcant
variation is the CPU consumption observed in the Regional
Cloud during the update of databases containing K8s
nodes, antennas, and deployments.
RAN units deployment (t3-t4): We can observe an increment of the CPU consumption at the three Cloud levels.
In the Regional Cloud, it is due to the BCR processing
required to aﬃliate the RAN containers to the K8s hosting
nodes. In Edge and Far Edge the CPU consumption
Fig. 8: Core logs & list of deployed RUs
increment is due to the deployment of CU-DU and RU,
respectively.
Cloud RAN operation (after t4): After the deployment of
a new Cloud-RAN, the CPU usage in the Regional Cloud
slightly increases due to the operation of K8s. In the Far
Edge node, the CPU consumption increases due to radio
signal processing and antenna control (USRP B210). No
signiﬁcant increase in RAM is evidenced.
C. End-to-end service performance evaluation
In order to validate the end-to-end service availability,
beyond the Cloud-RAN deployment, we perform the
attachment and Connection of Commercial oﬀ-the-shelf
(COTS) user equipments. We additionally validate the
implemented multi-antenna management mechanism.
Figure 8 shows the list of the deployed RUs and the
associated antennas, as well as, the core network logs
showing two connected eNBs and a connected user to the
automatically deployed mobile network.
V. Conclusion
We have addressed in this work one of the key enablers of
incoming 6G services, i.e., the zero touch deployment of
end-to-end services, particularly required for on-demand
and ephemeral networks. As driving usecase we study the
deployment automation of Cloud RAN systems, which
are composed of CU, DU, RU units respectively placed
in three cloud levels: Regional, Edge and Far Edge. This
deployment architecture is compliant with the ‘Scenario
F’ of O-RAN. We have notably proposed a Zero Touch
Commissioning (ZTC) model which enables automated resource discovery (hosting servers and antennas) as well as
automated lifecycle management of RAN units, including
instantiation, conﬁguration, monitoring and deletion. The
implemented system makes available a dedicated Network
Management System (NMS) with a graphical interface to
facilitate the operation and management of Cloud-RAN
systems. The Cloud-RAN deployment is then launched by
a deployment order coming either from the NMS or from
the service orchestration layers. The deployment order
includes main required Cloud-RAN settings such as the
geographic area to be covered, the maximal number of
users, spectrum band, among others service features. The
ZTC system selects the adapted K8s nodes to host the
RAN units (CU, DU, RU) and automatically conﬁgures
them, i.e., it provides IP addresses to each unit, selects
available antennas, opens the required ports to enable
traﬃc exchange and starts the service. We have additionally evaluated the case of multi-antenna Far Edge nodes
in order to support TowerCos scenarios where various
RUs can be instantiated in a single shared infrastructure
(K8s node) placed near to towers. Performance results
evidence that the resource consumption of the proposed
ZTC systems is negligible. The automated deployment of
a fully cloudiﬁed end-to-end RAN system only requires
32 seconds without any human intervention, instead of
various hours required for standard deployments. Users
attachments and connections validated the end-to-end
service availability.