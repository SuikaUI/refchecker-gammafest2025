Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 272–303
Belgium, Brussels, October 31 - Novermber 1, 2018. c⃝2018 Association for Computational Linguistics
 
Findings of the 2018 Conference on Machine Translation (WMT18)
Ondˇrej Bojar
Charles University
Christian Federmann
Microsoft Cloud + AI
Mark Fishel
University of Tartu
Yvette Graham
Dublin City University
Barry Haddow
University of Edinburgh
Matthias Huck
LMU Munich
Philipp Koehn
JHU / University of Edinburgh
Christof Monz
University of Amsterdam
This paper presents the results of the premier
shared task organized alongside the Conference on Machine Translation (WMT) 2018.
Participants were asked to build machine
translation systems for any of 7 language pairs
in both directions, to be evaluated on a test set
of news stories. The main metric for this task
is human judgment of translation quality. This
year, we also opened up the task to additional
test sets to probe speciﬁc aspects of translation.
Introduction
The Third Conference on Machine Translation
(WMT) held at EMNLP 20181 host a number of
shared tasks on various aspects of machine translation. This conference builds on twelve previous
editions of WMT as workshops and conferences
 .
This year we conducted several ofﬁcial tasks.
We report in this paper on the news translation
task. Additional shared tasks are described in separate papers in these proceedings:
• biomedical translation ,
• multimodal machine translation ,
• metrics ,
• quality estimation ,
• automatic post-editing , and
• parallel corpus ﬁltering .
In the news translation task (Section 2), participants were asked to translate a shared test set,
optionally restricting themselves to the provided
training data (“constrained” condition). We held
1 
14 translation tasks this year, between English
and each of Chinese, Czech, Estonian, German,
Finnish, Russian, and Turkish.
The Estonian-
English language pair was new this year. Similarly to Latvian, which we had covered in 2017,
Estonian is a lesser resourced data condition on
a challenging language pair. System outputs for
each task were evaluated both automatically and
This year the news translation task had two additional sub-tracks: multilingual MT and unsupervised MT. Both sub-tracks were included into the
general list of news translation submissions and
are described in more detail in corresponding subsections of Section 2.
The human evaluation (Section 3) involves asking human judges to score sentences output by
anonymized systems. We obtained large numbers
of assessments from researchers who contributed
evaluations proportional to the number of tasks
they entered.
In addition, we used Mechanical
Turk to collect further evaluations. This year, the
ofﬁcial manual evaluation metric is again based
on judgments of adequacy on a 100-point scale,
a method we explored in the previous years with
convincing results in terms of the trade-off between annotation effort and reliable distinctions
between systems.
The primary objectives of WMT are to evaluate the state of the art in machine translation, to
disseminate common test sets and public training data with published performance numbers, and
to reﬁne evaluation and estimation methodologies
for machine translation.
As before, all of the
data, translations, and collected human judgments
are publicly available2. We hope these datasets
serve as a valuable resource for research into datadriven machine translation, automatic evaluation,
or prediction of translation quality. News transla-
2 
tions are also available for interactive visualization
and comparison of differences between systems at
 using MT-ComparEval
 .
In order to gain further insight into the performance of individual MT systems, we organized
a call for dedicated “test suites”, each focussing
on some particular aspect of translation quality. A
brief overview of the test suites is provided in Section 4.
News Translation Task
The recurring WMT task examines translation between English and other languages in the news domain. As in the previous year, we include Chinese,
Czech, German, Finnish, Russian, and Turkish. A
new language this year is Estonian.
We created a test set for each language pair by
translating newspaper articles and provided training data.
The test data for this year’s task was selected from
online sources, as in previous years.
about 1500 English sentences and translated them
into the other languages, and then additional 1500
sentences from each of the other languages and
translated them into English.
This gave us test
sets of about 3000 sentences for our English-X
language pairs, which have been either originally
written in English and translated into X, or vice
versa. The composition of the test documents is
shown in Table 1, the size of the test sets in terms
is given in Figure 2.
The stories were translated by professional
translators, funded by the EU Horizon 2020
projects CRACKER and QT21 (German, Czech),
by Yandex3, a Russian search engine company
(Turkish, Russian), by BAULT, a research community on building and using language technology funded by the University of Helsinki (Finnish)
and the University of Tartu4 (Estonian).
Chinese–English task was sponsored by Nanjing
University, Xiamen University, the Institutes of
Computing Technology and of Automation, Chinese Academy of Science, Northeastern University (China) and Datum Data Co., Ltd. All of the
3 
4Estonian Research Council institutional research grant
“Computational models of the Estonian Language”
translations were done directly, and not via an intermediate language.
Since Estonian-English was run for the ﬁrst
time, both the test and development set had to be
translated: the size of both was 2000 sentences
(4000 in total).
Training Data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune system parameters. Some training corpora
were identical from last year (Europarl,5 Common
Crawl, SETIMES2 , Russian-English parallel data
provided by Yandex, Wikipedia Headlines provided by CMU) and some were updated , News
Commentary v13, monolingual news data). A new
corpus is the EU Press Release parallel corpus for
German, Finnish, and Latvian.
For Latvian and Chinese, a number of new corpora were released. For Latvian, this data was prepared by the University of Latvia and Tilde, the
Chinese corpora were prepared by the Institutes of
Computing Technology and of Automation, Chinese Academy of Science, Northeastern University (China) and Datum Data Co., Ltd.
Some statistics about the training materials are
given in Figure 1.
Submitted Systems
We received 103 submissions from 32 institutions.
The participating institutions, organized into 35
teams are listed in Table 2 and detailed in the rest
of this section. Each system did not necessarily
appear in all translation tasks. We also included 39
online MT systems (originating from 5 services),
which we anonymized as ONLINE-A,B,F,G.
For presentation of the results, systems are
treated as either constrained or unconstrained, depending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial systems are treated as unconstrained during the automatic and human evaluations.
AALTO 
Aalto participated in the constrained condition of
the multi-lingual subtrack, with a single system
trained to translate from English to both Finnish
5As of Fall 2011, the proceedings of the European Parliament are no longer translated into all ofﬁcial languages.
Europarl Parallel Corpus
German ↔English
Czech ↔English
Finnish ↔English
Estonian ↔English
50,486,398 53,008,851 14,946,399 17,376,433 37,814,266 52,723,296 13,033,918 17,453,613
Distinct words
News Commentary Parallel Corpus
German ↔English
Czech ↔English
Russian ↔English
Chinese ↔English
Distinct words
Common Crawl Parallel Corpus
German ↔English
Czech ↔English
Russian ↔English
54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words
ParaCrawl Parallel Corpus
German ↔English
Czech ↔English
Estonian ↔English
36,351,593
10,020,250
595,027,749 623,361,284 116,797,931 122,699,058 37,887,435 39,060,095
Distinct Words
Finnish ↔English
Russian ↔English
1,2061,155
8,636,936 11,123,014 182,229,052 210,751,004
Distinct Words
EU Press Release Parallel Corpus
German ↔English
Finnish ↔English
Estonian ↔English
25,048,312
25,777,997
11,244,602
Distinct words
Chinese Parallel Corpora
casict2011
casict2015
Words (en)
20,571,578
34,866,598
22,802,353
24,632,984
25,182,185
29,696,442
Distinct words (en)
Yandex 1M Parallel Corpus
Russian ↔English
24,121,459
26,107,293
CzEng v1.7 Parallel Corpus
Czech ↔English
61,243,252
737,434,097
835,192,627
Wiki Headlines Parallel Corpus
Russian ↔English
Finnish ↔English
SE Times 2 Parallel Corpus
Turkish ↔English
United Nations Parallel Corpus
Russian ↔English
Chinese ↔English
23,239,280
15,886,041
482,966,738
524,719,646
372,612,596
Figure 1: Statistics for the training sets used in the translation task. The number of words and the number of distinct words
(case-insensitive) is based on the provided tokenizer.
Sources (Number of Documents)
ABC News (1), BBC (4), Brisbane Times (1), CBS News (1), Daily Mail (4), Euronews (3), Globe and
Mail (1), Guardian (4), Independent (4), Los Angeles Times (4), MSNBC (3), Novinte (2), New York
Times (2), Reuters (3), Russia Today (2), Scotsman (2), Sydney Morning Herald (2), Telegraph (2), The
Local (2), Time Magazine (2), UPI (1), Washington Post (3)
blesk.cz (16), deník.cz (5), Deník Referendum (1), DNES.cz (7), lidovky.cz (6), Novinky.cz (3), Re-
ﬂex (2), tyden.cz (12), ZDN (2)
Aachener Nachrichten (1), Abendzeitung Nürnberg (2), Braunschweiger Zeitung (1), Der Standard (1),
Die Presse (1), Euronews (1), Fehmarn24 (1), Handelsblatt (1), Hannoversche Allgemeine (2), Hessische/Niedersächsische Allgemeine (1), In Franken (4), Kreiszeitung (2), Krone (1), Mainpost (1),
Merkur (3), Morgenpost (1), n-tv (1), Neue Westfälische (1), oe24 (2), Peiner Allgemeine (1), Passauer
Neue Presse (2), Rheinzeitung (1), Rundschau (1), Schwarzwälder Bote (16), Segeberger Zeitung (2),
Südkurier (1), Thüringer Allgemeine (1), Thüringer Landeszeitung (1), Volksblatt (2), Volksfreund (3),
Westfälische Nachrichten (1), Westdeutsche Zeitung (8).
Arileht (7), Maaleht (3), Postimees (17), Sloleht (23).
Etelä-Saimaa (2), Etelä-Suomen Sanomat (3), Helsingin Sanomat (4), Iltalehti (13), Ilta-Sanomat (29),
Kaleva (12), Kansan Uutiset (1), Karjalainen (13), Kouvolan Sanomat (2).
aif (4), Altapress (1), Argumenti (19), ERR.ee (3), eg-online.ru (2), Euronews (2), Fakty (5), Infox (2), Izvestiya (25), Kommersant (16), Lenta (9), lgng (3), MK RU (5), nov-pravda.ru (1), pnp.ru (6),
rg.ru (4), Vedomosti (3), Versia (1), Vesti (3), zr.ru (1)
Hürriyet.com (48), Sabah (96), Sözcü (19)
Table 1: Composition of the test set. For more details see the XML test ﬁles. The docid tag gives the source and the date for
each document in the test set, and the origlang tag indicates the original source language.
BigEst Estonian Corpus
40,404,948
579,221,489
Distinct words
News Language Model Data
192,988,741
260,754,881
66,517,569
39,519,008
14,575,981
4,428,839,473
4,627,780,738
1,094,215,341
724,582,848
184,523,981
79,067,739
12,880,832
Distinct words
20,276,165
Common Crawl Language Model Data
3,074,921,453
2,872,785,485
333,498,145
1,168,529,851
157,264,161
100,779,314
511,196,951 1,672,324,647
Words 65,128,419,540 65,154,042,103 6,694,811,063 23,313,060,950 2,935,402,545 2,906,100,138 11,882,126,872
342,760,462
339,983,035
50,162,437
101,436,673
47,083,545
27,618,190
88,463,295
German ↔EN
Finnish ↔EN
Estonian ↔EN
Sentences.
Distinct words
Russian ↔EN
Turkish ↔EN
Chinese ↔EN
Sentences.
Distinct words
Figure 2: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
Institution
Aalto University 
Air Force Research Laboratory 
Alibaba Group 
CUNI-KOCMI
Charles University 
CUNI-TRANSFORMER
Charles University 
FACEBOOK-FAIR ⋆
Facebook AI Research 
Global Tone Communication Technology 
University of Helsinki 
Johns Hopkins University 
Jadavpur University 
Karlsruhe Institute of Technology 
Li Muze (no associated paper)
LMU Munich 
LMU Munich 
MICROSOFT-MARIAN
Microsoft 
MLLP, Technical University of Valencia 
MMT-PRODUCTION
ModernMT, MMT s.r.l. (no associated paper)
NEUROTOLGE.EE
University of Tartu 
National Institute of Information and Communications Technology
 
Northeastern University / NiuTrans Co., Ltd. 
NLP Group, Nanjing University (no associated paper)
NTT Corporation 
Bo˘gaziçi University 
PROMT LLC 
RWTH Aachen 
RWTH-UNSUPER
RWTH Aachen 
TALP, Technical University of Catalonia 
Tencent 
Tilde 
Ubiqus (no associated paper)
University of Cambridge 
University of Edinburgh 
University of Maryland 
Unisound (no associated paper)
UNSUPTARTU
University of Tartu 
Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from
the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “⋆” indicates invited participation with a late
submission, where the team is not considered a regular participant.
and Estonian. The system is based on the Transformer implementation in
OpenNMT-py . It is trained on
ﬁltered parallel and ﬁltered back-translated monolingual data.
The main contribution is a novel
cross-lingual Morfessor segmentation using cognates extracted from the parallel data. The aim is to improve the consistency
of the morphological segmentation. Aalto decode
using an ensemble of 3 (et) or 8 (ﬁ) models.
AFRL 
AFRL-SYSCOMB is a system-combination entry
consisting of three inputs. The ﬁrst is an Open-
NMT system trained on the provided parallel data
except ParaCrawl and the backtranslated corpus
used in the AFRL WMT17 system . This system uses a standard RNN
architecture and was ﬁne-tuned with the other
available news task test sets.
The second is a
Marian system
ensembling 5 Univ. Edinburgh “bi-deep” and 6
transformer models all trained on the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on
BPE’d corpora . A Marian
transformer model performed right-to-left rescoring for this system. The third system is trained
with Moses , using the same
data as the Marian system. Hierarchical reordering
and Operation Sequence Model were employed.
The 5-gram English language model was trained
with KenLM on the same corpus
as the AFRL WMT15 system with the same BPE
used in the Marian systems. Lastly, RWTH Jane’s
system combination was applied yielding approximately a +0.5 gain in BLEU.
ALIBABA 
Alibaba systems are based on the Transformer
model architecture, with the most recent features
from the academic research integrated, such as
weighted Transformer, Transformer with relative
position attention, etc. The system also employs
most techniques that have been proven effective
during the past WMT years, such as BPE-based
subword, back translation, ﬁne-tuning based on
selected data, model ensembling and reranking, at
industrial scale. For some morphologically-rich
languages, linguistic knowledge is also incorporated into the neural network.
CUNI-KOCMI 
The CUNI-KOCMI submission focuses on the
low-resource language neural machine translation
(NMT). The ﬁnal submission uses a method of
transfer learning: the model is pretrained on a related high-resource language (here Finnish) ﬁrst,
followed by a child low-resource language (Estonian) without any change in hyperparameters. Averaging and backtranslation are also experimented
CUNI-TRANSFORMER 
CUNI-TRANSFORMER is the Transformer model
trained according to Popel and Bojar plus
a novel concat-regime backtranslation with checkpoint averaging, tuned separately for CZ-domain
and nonCZ-domain articles, possibly handling
also translation-direction (“translationese") issues.
For cs→en also a coreference preprocessing was
used adding the female-gender pronoun where it
was pro-dropped in Czech, referring to a human
and could not be inferred from a given sentence.
FACEBOOK-FAIR ⋆ . Synthetic sources
are sampled instead of beam search, oversampling
the real bitext at a rate of 16, i.e., each bitext is
sampled 16 times more often per epoch than the
back-translated data. At inference time, translations which are copies of the source are ﬁltered
out, replacing them with the output of a very small
news-commentary only trained model.
The system FACEBOOK-FAIR has been submitted anonymously as ONLINE-Z and approval
for disclosing the authors’ identity has only been
granted after the ﬁnal results had become available. Due to the non-standard way of submission,
the system is not considered a regular participant,
but an invited/late submission and marked with
“⋆” throughout the paper.
GTCOM 
GTCOM-PRIMARY is based on the Transformer
“base” model architecture using Marian toolkit,
and it also applies some methods that have been
proven effective in NMT system, such as BPE,
back-translation, right-to-left reranking and ensembling decoding. In this experiment, right-toleft reranking does not help.
Another focus is
given to data ﬁltering through rules, translation
model and language model including parallel data
and monolingual data.
The language model is
based the Transformer architecture as well. The ﬁnal system is trained with four different seeds and
mixed data.
HY 
The University of Helsinki (HY) submitted four
systems: HY-AH, HY-NMT, HY-NMT-2STEP
and HY-SMT.
HY-AH 
is a rule-based machine
translation system, relying on a rule-based dependency parser for English, a hand-crafted translation lexicon (based on dictionary data extracted
from parallel corpora by word alignment), various
types of transfer rules, and a morphological generator for Finnish.
HY-NMT 
submissions
are based on the Transformer “base" model,
trained with all the parallel data provided by the
shared task plus back-translations, with a shared
vocabulary between source and target language
and a domain label for each source sentence.
For the multilingual sub-track synthetic data for
English→Estonian and Estonian→English was
also used. Ultimately, a single model for all language pairs was trained and then ﬁne-tuned for
each language pair.
HY-NMT-2STEP 
Transformer model trained on interleaved lemmas
and morphological tags on the Finnish side. Morphological categories (number, tense etc.) have
separate tags, and a tag is only added if the value of
the category differs from the default value (in the
same way that languages have morphemes only
for marked values of morphological categories).
The ﬁnal translation is deterministically generated
from the sequence of lemmas and morphological
tags which the model outputs.
HY-SMT 
Helsinki SMT system submitted at WMT 2016
(the constrained-basic+back-translated version).
The system was not retrained and it may thus
suffer from poor lexical coverage on recent test
data. The main motivation for including this baseline was to have a statistical machine translation
(SMT) submission for the Finnish morphology
test suite .
JHU 
The JHU systems are the result of two relatively
independent efforts on German–English language
directions and Russian–English, using the Marian
and Sockeye neural machine
translation toolkits, respectively. The novel contributions are iterative back-translation (for German)
and ﬁne-tuning on test sets from prior years (for
both languages).
JUCBNMT 
JUCBNMT is an encoder-decoder sequence-tosequence NMT model with character level encoding. The submission uses preprocessing like tokenization, truecasing and corpus cleaning. Both
encoder and decoder use a single LSTM layer
each. The batch size was set to 128, number of
epochs was set to 100, activation function was
softmax, optimizer chosen was RMSprop and the
loss function used was categorical cross-entropy.
Learning rate was set to 0.001.
KIT 
The KIT submission is the NMT Transformer architecture, enhanced in model depth. Techniques
for reducing memory consumption (recalculating
intermediate results at layers instead of caching
them), 4 times larger model could ﬁt on one GPU
and improve the performance by 1.2 BLEU points.
Sentences selection from the new ParaCrawl
improved the effectiveness of the corpus by 0.5
BLEU points, with an overall increase of 0.8
BLEU compared to the baseline of not using
ParaCrawl.
LI-MUZE is an ensembles of 4 averaged Transformer models with one right-to-left and one
target-to-source averaged Transformer model, the
conﬁguration of all the models is the same as
the Transformer big-model, trained on the ofﬁcial
training data with 4.5M back-translated data from
the monolingual news of 2016 and 2017 data. The
English vocabulary size is 36K BPE subwords.
Chinese is tokenized by Chinese characters and
the vocabulary size is 10K.
LMU-NMT 
For the WMT18 news translation shared task,
LMU Munich has trained ba-
sic shallow attentional encoder-decoder systems
 with the Nematus toolkit
 , like last year . LMU has participated with these NMT
systems for the English–German language pair in
both translation directions. The training data is
a concatenation of Europarl, News Commentary,
Common Crawl, and some synthetic data in the
form of backtranslated monolingual news texts.
The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl
corpora. The German data is preprocessed with
a linguistically informed word segmentation technique . By using a linguistically more sound word segmentation, advantages
over plain BPE segmentation are expected in three
important aspects: vocabulary reduction, reduction of data sparsity, and open vocabulary translation. The NMT system can learn linguistic word
formation processes from the segmented data. In
the English→German translation direction, LMU
furthermore conducted ﬁne-tuning towards the domain of news articles and
reranked the n-best list with a right-to-left neural
model which is trained for reverse word order .
LMU-UNSUP 
For the unsupervised track of the WMT18 news
translation task, LMU Munich submitted the
LMU-UNSUP system 
which is a neural translation model trained without any access to parallel data.
The model is
trained with ∼4M German and English sentences
each, which are sampled from NewsCrawl articles
from 2007 to 2017. Bilingual word embeddings
trained in an unsupervised manner were used to translate the monolingual data
by doing word-by-word translation and this synthetically created parallel data is used in the training as well. The same model is used to do both
German→English and English→German translation. The model is based on 
and it uses denoising and on-the-ﬂy backtranslation. Additionally the model uses the word-byword translated data in the initial training stages to
jump-start the training and disables the denoising
component as the last training step for further improvements. The NMT embeddings are initialized
with embeddings obtained from fasttext trained
jointly on German and English monolingual BPElevel data.
MICROSOFT-MARIAN
 
MICROSOFT-MARIAN
is the Transformer-big
model implemented in Marian with an updated version of Edinburgh’s training scheme for
WMT2017, following current common practices:
truecasing and tokenization using Moses scripts,
BPE subwords, backtranslation (using a shallow model), ensembling of four left-to-right deep
models and reranking of 12-best list with an ensemble of four right-to-left models.
The novelties are primarily in new data ﬁltering
(dual conditional cross-entropy ﬁltering) and sentence weighting methods.
MLLP-UPV (Iranzo-Sánchez et al.,
MLLP-UPV is an ensemble of Transformer
architecture-based neural machine translation systems.
To train the system under “constrained"
conditions, the provided parallel data was ﬁltered
with a scoring technique using character-based
language models, and was augmented based on
synthetic source sentences generated from the provided monolingual corpora.
The ensemble consists of 4 independent training runs of the Transformer “base” model, trained
with 10M ﬁltered sentences (including from
ParaCrawl) and 20M backtranslated sentences
from NewsCrawl2017.
MMT-PRODUCTION
MMT-PRODUCTION is the machine translation
system offered by MMT s.r.l. (www.modernmt.
eu) as of July 2018.
It is a Transformer-based
neural MT system trained on public and proprietary data, containing about 100M sentence pairs
and about 1.5G English words. It exploits a single model of ‘transformer-big’ size, and a single
pass-decoding; texts are processed using internal
NEUROTOLGE.EE 
NICT NMT systems were trained with the Transformer architecture using the provided parallel
data enlarged with a large quantity of backtranslated monolingual data generated with a new
incremental training framework. The primary submissions to the task are the result of a simple combination between NICT SMT and NMT systems.
NIUTRANS 
NIUTRANS baseline systems are based on the
Transformer architecture with the “base” model,
equipped with checkpoint averaging and backtranslation
techniques.
improve the translation performance 2.28-3.83
BLEU points from four aspects including model
variances (larger inner-hidden-size in FFN, using ReLU and attention dropout, Swish activation function, relative positional representation),
diverse ensemble decoding (ensemble decoding
with up to 15 models, generated by different
strategies), reranking (up to 14 features for reranking), and post-processing (aim at the inconsistent
translation of proper nouns, especially the English
literals in Chinese sentences).
The NJUNMT-PRIVATE is most likely the system developed by Natural Language Processing Group of Nanjing University based on highlevel API of TensorFlow, 
com/zhaocq-nlp/NJUNMT-tf.
Further details
on training are not available.
NTT 
NTT combine Transformer “big” model, corpus
cleaning technique for provided and synthetic parallel corpora, and right-to-left n-best re-ranking
techniques.
Through their experiments, NTT
found ﬁltering of noisy training sentences and
right-to-left re-ranking as the keys to better accuracy.
PARFDA 
PARFDA selects a subset of the training and LM
data to build task-speciﬁc SMT models. PARFDA
uses phrase-based Moses and all constrained available resources provided by WMT18. The datasets
are available at 
parfdaWMT2018.
PROMT 
PROMT submitted three systems:
HYB-MARIAN, PROMT-HYB-OPENNMT and
PROMT-RULE-BASED.
PROMT-HYB-MARIAN
is an ensemble of 5
transformer models trained on WMT data and inhouse news data.
PROMT-HYB-OPENNMT
is a hybrid system
based on PROMT Rule-based engine and a NMT
post-editing (PE) engine. The NMT PE component is a sequence-to-sequence model with attention and deep biRNN encoder trained with Open-
NMT toolkit.
PROMT-RULE-BASED
is a rule-based system,
without any speciﬁc training or tuning.
RWTH 
All systems submitted by RWTH Aachen for German to English are based on the Transformer architecture implemented in Sockeye.
RWTH system has been an ensemble of three
Transformer models, where each individual model
had been already very strong. The strength of the
RWTH systems is probably due to the following four key factors: (a) Using the Transformer
architecture.
(b) Rather large models and large
batch size which was made possible due to synchronous training on 4 GPUs and roughly 8 days
of training. (Details: num-embed: 1024; numlayers: 6; attention-heads: 16; transformer-feedforward-num-hidden: 4096; transformer-modelsize: 1024, no weight-tying. In sum, this results
in 291M trainable parameters.)
(c) Careful experiments on data conditions: E.g. oversampling
of parallel data, LM driven ﬁltering of ParaCrawl
(retained 50%), testing different amounts of BPE
merge operations. (d) Fine-tuning on old testsets
(newstest2008-newstest2014).
RWTH English→Turkish system is based on
6-layer encoder-decoder Transformer architecture.
Since the task has low resources, dropout with the
rate of 0.3 to all applicable layers was used. Even
though the two languages are not much related,
joint BPE and weight tying helped a lot as part of
regularization. For the ﬁnal submission, RWTH
used augmented training data with 1M-sentence
back-translations and ensembled four models with
different random seeds.
RWTH-UNSUPER and Artetxe et al. . RWTH-
UNSUPER best performing systems follow the
batch optimization strategy and are initialized with
cross-lingual embeddings. Furthermore, RWTH-
UNSUPER found that sharing a vocabulary performs better than having separate ones. Freezing
embeddings hurts performance and it was found
best to initialize embeddings with pre-trained ones
and train them as usual.
TALP-UPC 
TALP-UPC is the Transformer “base” model
trained with the Tensor2Tensor implementation
 and wordpieces vocabulary.
The training corpus is multilingual (concatenating Finnish–English and Estonian–English) and
includes ParaCrawl with garbage cleaned up via
langdetect.
TENCENT 
TENCENT-ENSEMBLE (called TenTrans) is an improved NMT system on Transformer based on
self-attention mechanism. In addition to the basic settings of Transformer training, TENCENT-
ENSEMBLE uses multi-model fusion techniques,
multiple features reranking, different segmentation models and joint learning. Additionally, data
selection strategies were adopted to ﬁne-tune the
trained system, achieving a stable performance
improvement.
An additional system paper describes a non-primary submission.
TILDE 
TILDE submitted four systems:
TILDE-C-NMT,
TILDE-C-NMT-COMB,
TILDE-C-NMT-2BT
TILDE-NC-NMT.
TILDE-C-NMT
are constrained English-Estonian
and Estonian-English NMT systems that were deployed as ensembles of averaged factored data
Transformer models. The models were trained using ﬁltered parallel data and back-translated data
in a 1-to-1 proportion. The parallel data were supplemented with synthetic data (generated from the
same parallel data) that contain unknown token
identiﬁers in order to acquire models that are more
robust to unknown phenomena.
TILDE-C-NMT-COMB
is a constrained Estonian-
English NMT system that is a system combination
of multiple constrained factored data NMT systems.
TILDE-C-NMT-2BT
systems were trained using
Sockeye and Transformer models. Before training the initial systems, parallel data were cleaned
using the parallel-corpora-tools.
back-translation, monolingual data were also ﬁltered.
After back-translation, the resulting synthetic corpora were ﬁltered again.
Intermediate
systems were trained with the ﬁrst batch of parallel+synthetic data. The back-translation and ﬁltering process was performed a second time with
additional monolingual data to train the ﬁnal systems with parallel and two sets of synthetic data.
TILDE-NC-NMT
unconstrained
English→Estonian
Estonian→English
NMT systems that were deployed as averaged
Transformer models.
These models were also
trained using back-translated data similarly to the
constrained systems, however, the data, taking
into account its relatively large size, was not
The UBIQUS-NMT system is probably developed
by the Ubiqus company (www.ubiqus.com). No
further information is available.
UCAM 
UCAM is a generalization of previous work
 to multiple architectures.
It is a system combination of two Transformer-like
models, a recurrent model, a convolutional model,
and a phrase-based SMT system. The output is
probably dominated by the Transformer, and to
some extend by the SMT system.
UEDIN 
For Estonian↔English and Finnish↔English, the
UEDIN systems are an ensemble of four left-toright systems, reranked with four right-to-left systems, built using Marian. Each ensemble consists
of two Transformers and two deep RNNs. The
RNNs use the UEDIN multi-head / multi-hop variant.
All available parallel data were used, plus
back-translated data from 2017 (for into-English)
and 2014-2017 (for out-of-English).
The natural parallel data was generally over-sampled to
give an equal mix of parallel and synthetic data.
For English↔Estonian, UEDIN selected 30% of
ParaCrawl based on translation model perplexity
for a model built on the rest of the data.
The UEDIN systems for other language pairs use
an ensemble of four deep RNN left-to-right systems, reranked with 4 deep RNN right-to-left systems. The RNN models use the UEDIN multi-head
/ multi-hop attention variant. All the provided parallel data (including ParaCrawl) were used, applying langid ﬁltering to remove some incorrect sentence pairs. Synthetic data were also used, created by back-translating the 2017 English news
crawl, and the 2017 and 2016 Czech news crawls.
For Czech→English, the synthetic data was oversampled 2x.
UMD 
The UMD best system is an ensemble of three
6-layer left-to-right Transformer models reranked
with target-to-source and left-to-right models.
Each Transformer model is trained with a 2:1 mixture of parallel and backtranslated monolingual
For parallel data, duplicates are removed
and “bad” sentence pairs ﬁltered out. Monolingual
data is sub-sampled from news 2017 (English) and
news 2011 (Chinese). Subwords (BPE) are used
for both English and Chinese sentences.
The UNISOUND systems are probably developed
by the Unisound company (www.unisound.com).
No further information is available.
UNSUPTARTU 
UNSUPTARTU is an unsupervised MT system using n-gram embedding cross-lingual mapping to
create a phrase table. An RNN LM is used in decoding.
Multilingual Sub-track
This year the news translation track included
an explicit sub-track on multilingual translation.
This covered any submissions that used any data
(monolingual or parallel) from a third language to
help the language pair in question: for example,
using English-Finnish data to improve English-
Estonian translation. All entries to this sub-track
had to use only the WMT-provided data sets, and
thus had to be constrained. Submissions to this
sub-track are joined with the main translation track
and evaluated without separation in the same way.
While there was no restriction in terms of language pairs, three language pairs were “verbally
endorsed”: English to/from Turkish, Estonian and
The motivation behind the choice of
languages was to test the effect of multilingual
(and unsupervised) methods on low-resource language pairs (Turkish-English, Estonian-English)
and to contrast the results with a resource-rich pair
(German-English).
Unsupervised Sub-track
In the unsupervised MT sub-track the participants
were constrained to using only the monolingual
training data from WMT; this additionally excluded the monolingual corpora that are largely
parallel (monolingual parts of Europarl and News
Commentary). The aim of this task was to see how
far can one get in terms of translation quality without any parallel data used for training. As an exception it was allowed to use a parallel dev set for
parameter tuning and/or model selection. The language pairs of this sub-track coincided with the
multilingual sub-track: English to/from Turkish,
Estonian and German.
Human Evaluation
A human evaluation campaign is run each year to
assess translation quality and to determine the ﬁnal
ranking of systems taking part in the competition.
This section describes how preparation of evaluation data, collection of human assessments, and
computation of the ofﬁcial results of the shared
task was carried out this year.
Work on evaluation over the past few years has
provided fresh insight into ways to collect direct
assessments (DA) of machine translation quality , and two
years ago the evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with relative ranking (RR)
and DA. DA has some clear advantages over RR,
namely the evaluation of absolute translation quality and the ability to carry out evaluations through
quality controlled crowd-sourcing. As established
in 2016 , DA results (via
crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several
source languages into English and at 0.975 for
English-to-Russian (the only pair evaluated outof-English).
Last year, we thus employed DA
for evaluation of systems taking part in the news
task and do so again this year. Where possible,
Figure 3: Screen shot of Direct Assessment in the Appraise interface used in the human evaluation campaign. The annotator
is presented with a reference translation and a single system output randomly selected from competing systems (anonymized),
and is asked to rate the translation on a sliding scale.
Figure 4: Screen shot of Direct Assessment as carried out by workers on Mechanical Turk.
we collect DA judgments via the crowd-sourcing
platform, Amazon’s Mechanical Turk, and as in
previous year’s we ask participating teams to provide manual evaluation of system outputs via Appraise. Researcher involvement was needed particularly for translations into Czech, German, Estonian, Finnish and Turkish.
Human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation (i.e.
no bilingual speakers are needed) on an analogue
scale, which corresponds to an underlying absolute 0–100 rating scale. Since DA involves evaluation of a single translation per screen, this allows
the sentence length restriction usually applied during manual evaluation to be removed for both researchers and crowd-sourced workers.6 Figure 3
6The maximum sentence length with RR was 30 in
shows one DA screen as completed by researchers
on Appraise, while Figure 4 provides a screenshot
of DA shown to crowd-sourced workers on Amazon’s Mechanical Turk.
The annotation is organized into “HITs” (following the Mechanical Turk’s term “human intelligence task”), each containing 100 such screens
and requiring about half an hour to ﬁnish. Appraise users were allowed to pause their annotation at any time, Amazon interface did not allow
any pauses. More details of composition of HITs
are given in Section 3.3 below.
Evaluation Campaign Overview
In terms of the News translation task manual evaluation, a total of 584 individual researcher ac-
counts were involved, and 915 turker accounts.7
Researchers in the manual evaluation came from
33 different research groups and contributed judgments of 118,705 translations, while 225,900
translation assessment scores were submitted in
total by the crowd.8
Under ordinary circumstances, each assessed
translation would correspond to a single individual scored segment. However, since distinct systems can produce the same output for a particular
input sentence, we are often able to take advantage of this and use a single assessment for multiple systems.
Similar to last year’s evaluation,
we only combine human assessments in this way
if the string of text belonging to multiple systems
is exactly identical. For example, even small differences in punctuation disqualify combination of
similar system outputs, and this is due to a general
lack of evidence about what kinds of minor differences may or may not impact human evaluation.
Table 3 shows the numbers of segments for
which distinct MT systems participating in the
News Translation Task produced identical outputs.
The biggest saving in terms of exact duplicate
translations, being produced by multiple systems,
was for German to English, where a 17.4% saving
of resources by combining identical outputs before
human evaluation.
Data Collection
System rankings are produced from a large set of
human assessments of translations, each of which
indicates the absolute quality of the output of a
system. Annotations are collected in an evaluation campaign that enlists the help of participants
in the shared task. Each team is asked to contribute
8 hours annotation time, which we estimated at 16
100-translation HITs per primary system submitted. We continue to use the open-source Appraise9
 tool for our data collection, in
addition to Amazon Mechanical Turk.10 Table 4
shows total numbers of human assessments col-
7Numbers do not include the 1,533 workers on Mechanical Turk and 7 Appraise evaluators who did not pass quality
8Numbers include quality control items for workers who
passed quality control but omit the additional 347,700 assessments collected on Mechanical Turk where a worker did not
pass quality control and equivalent 1,466 judgments for the
small number of Appraise workers who did not meet the quality control threshold. A 40% pass rate for quality control is
typical of DA evaluations on Mechanical Turk.
9 
10 
lected in WMT18 contributing to ﬁnal scores for
systems.11
The effort that goes into the manual evaluation campaign each year is impressive, and we
are grateful to all participating individuals and
teams. We believe that human annotation provides
the best decision basis for evaluation of machine
translation output and it is great to see continued
contributions on this large scale.
Crowd Quality Control
This year, two distinct HIT structures were run in
the overall evaluation campaign, the standard DA
set-up was employed for Mechanical Turk and a
portion of the Appraise evaluation, while an additional HIT structure was used for the remaining
part of the Appraise evaluation. Below we ﬁrstly
describe the standard DA HIT structure and quality control mechanism before describing the additional version used for part of the Appraise evaluation. In both set-ups, translations are arranged
in sets of 100-translation HITs to provide control
over assignment and positioning of quality control
items to human annotators.
In the standard DA HIT structure, three kinds of
quality control translation pairs are employed as
described in Table 5: we repeat pairs (expecting
a similar judgment), damage MT outputs (expecting signiﬁcantly worse scores) and use references
instead of MT outputs (expecting high scores).
In total, 60 items in a 100-translation HIT serve
in quality control checks but 40 of those are regular judgments of MT system outputs (we exclude
assessments of bad references and ordinary reference translations when calculating ﬁnal scores).
The effort wasted for the sake of quality control is
Also in the standard DA HIT structure, within
each 100-translation HIT, the same proportion of
translations are included from each participating
system for that language pair. This ensures the
ﬁnal dataset for a given language pair contains
roughly equivalent numbers of assessments for
each participating system. This serves three purposes for making the evaluation fair. Firstly, for
the point estimates used to rank systems to be reliable, a sufﬁcient sample size is needed and the
11Appraise ran evaluation of 150−1 = 149 systems due to
a single tr-en system having been omitted in the initial human
evaluation run. The 95 crowd-sourced systems includes all
into-English language pair (including the tr-en missing system), en-ru and en-zh systems.
Saving (%)
Saving (%)
Language Pair
Chinese→English
Czech→English
German→English
Estonian→English
Finnish→English
Russian→English
Turkish→English
English→Chinese
English→Czech
English→German
English→Estonian
English→Finnish
English→Russian
English→Turkish
Table 3: Total segments prior to sampling for manual evaluation and savings made by combining outputs produced by different
systems that were identical.
most efﬁcient way to reach a sufﬁcient sample
size for all systems is to keep total numbers of
judgments roughly equal as more and more judgments are collected. Secondly, it helps to make
the evaluation fair because each system will suffer or beneﬁt equally from an overly lenient/harsh
human judge. Thirdly, despite DA judgments being absolute, it is known that judges “calibrate”
the way they use the scale depending on the general observed translation quality. With each HIT
including all participating systems, this effect is
averaged out. Furthermore apart from quality control items, HITs are constructed using translations
sampled from the entire set of outputs for a given
language pair.
The alternate DA HIT structure employed by
Appraise this year for a subset of researcher HITs
is shown in Table 6. This set-up reduces the number of quality control items in a HIT and is therefore more efﬁcient (12% overhead) by omitting repeat pairs and good reference pairs. This comes at
the cost of a reduced ability to analyze the quality
of data provided by human annotators. In addition
for this set-up, an additional constraint (not originally applied in standard DA) was imposed. As
much as possible within a 100-translation HIT the
HIT included the output of all participating systems for each source input. This constraint has
the advantage of producing assessments from the
same human assessor for translations of the same
source input but is not ideal in terms of the original aim of DA – to as much as possible produce
absolute scores for translations (as opposed to relative ones) – because it positions assessment of
competing translations in close proximity within
a HIT and judges may attempt to remember their
judgment for a different candidate translation of a
given input sentence.
In all set-ups employed in the evaluation campaign, and as in previous years, bad reference pairs
were created automatically by replacing a phrase
within a given translation with a phrase of the
same length randomly selected from n-grams extracted from the full test set of reference translations belonging to that language pair. This means
that the replacement phrase will itself comprise a
ﬂuent sequence of words (making it difﬁcult to tell
that the sentence is low quality without reading the
entire sentence) while at the same time making its
presence highly likely to sufﬁciently change the
meaning of the MT output so that it causes a noticeable degradation. The length of the phrase to
be replaced is determined by the number of words
in the original translation, as follows:
Language Pair
Assessments
Assess/Sys
Chinese→English
Czech→English
German→English
Estonian→English
Finnish→English
Russian→English
Turkish→English
English→Chinese
English→Czech
English→German
English→Estonian
English→Finnish
English→Russian
English→Turkish
Total Researcher
Total Crowd
Total WMT18
Table 4: Amount of data collected in the WMT18 manual evaluation campaign (assessments after removal of quality control
items and “de-collapsing” multi-system outputs). The ﬁnal seven rows report summary information from previous years of the
Translation
# Words Replaced
Length (N)
in Translation
Annotator Agreement
When an analogue scale (or 0–100 point scale,
in practice) is employed, agreement cannot be
measured using the conventional Kappa coefﬁcient, ordinarily applied to human assessment
when judgments are discrete categories or preferences. Instead, to measure consistency we ﬁlter crowd-sourced human assessors by how consistently they rate translations of known distinct
quality using the bad reference pairs described
previously.
Quality ﬁltering via bad reference
pairs is especially important for the crowd-sourced
portion of the manual evaluation.
Due to the
anonymous nature of crowd-sourcing, when collecting assessments of translations, it is likely to
encounter workers who attempt to game the service, as well as submission of inconsistent evaluations and even robotic ones. We therefore employ DA’s quality control mechanism to ﬁlter out
low quality data, facilitated by the use of DA’s analogue rating scale.
Assessments belonging to a given crowdsourced worker who has not demonstrated that
he/she can reliably score bad reference translations signiﬁcantly lower than corresponding genuine system output translations are ﬁltered out.
A paired signiﬁcance test is applied to test if degraded translations are consistently scored lower
Repeat Pairs:
Original System output (10)
An exact repeat of it (10);
Bad Reference Pairs:
Original System output (10)
A degraded version of it (10);
Good Reference Pairs:
Original System output (10)
Its corresponding reference translation (10).
Table 5: Standard DA HIT structure quality control translation pairs hidden within 100-translation HITs, numbers of items
are provided in parentheses.
Bad Reference Pairs:
Original System output (12)
A degraded version of it (12).
Table 6: Additional DA HIT structure used for a portion of researchers in Appraise data collection, where quality control
translation pairs hidden within 100-translation HITs, numbers of items are provided in parentheses in adapted version of DA
used for a subset of researchers HITs.
than their original counterparts and the p-value
produced by this test is used as an estimate of
human assessor reliability. Assessments of workers whose p-value does not fall below the conventional 0.05 threshold are omitted from the evaluation of systems, since they do not reliably score
degraded translations lower than corresponding
MT output translations.
This year’s assessment includes the ﬁrst largescale DA evaluation where quality control items
were applied to assessments of a known-reliable
group, comprised of the portion of researchers
who completed HITs on Appraise with the original
DA HIT structure. Although this group should be
considered highly reliable compared to Mechanical Turk for example, we must however keep in
mind that a small part of this group are in fact hired
to complete assessments and their reliability could
vary more than what would be expected of volunteer researchers.
Table 7 shows the number of workers in the
crowd-sourced and researcher groups who met our
ﬁltering requirement by showing a signiﬁcantly
lower score for bad reference items compared to
corresponding MT outputs, and the proportion of
those who simultaneously showed no signiﬁcant
difference in scores they gave to pairs of identical
translations.
A main observation to be taken from Table 7
is the difference in proportions of human assessors on Mechanical Turk versus researchers who
passed the quality ﬁltering criteria for DA, by
scoring degraded translations signiﬁcantly lower
than the original MT output counterparts, as 37%
of Mechanical Turk workers were deemed reliable compared to 93% of evaluators in the researcher group. This low rate of workers passing quality ﬁltering is inline with past DA evaluations, and the high proportion of annotators passing quality control is expected of a mostly knownreliable group. For crowd-sourced workers, consistent with past DA evaluations, Table 7 shows a
substantially higher number of low quality workers encountered for evaluation of languages other
than English on Mechanical Turk. For example, in
the case of Russian and Chinese only a respective
22% and 10% of workers were considered reliable
enough to include their assessments in the evaluation, compared to around 42% on average for
English evaluations.
When we examine repeat assessments of the
same translation, both ﬁltered groups show similar levels of reliability with 96% of ﬁltered Mechanical Turk workers and 95% of researchers
showing no signiﬁcant difference in scores for repeat assessment of the same translation. The idea
is that the repeated input should receive a very
similar score.
Assuming that annotators do not
remember their previous assessment for the repeated sentence, the “Exact Rep.” corresponds to
intra-annotator agreement and it reaches very high
Within the researcher group, although assessors
have high levels of reliability overall, reliability
in this respect varies quite a bit for different languages. For example, only 75% of assessors in the
researcher group completing assessments for Estonian showed no signiﬁcant difference for repeat
assessment of the same translation, and 87% for
Turkish, both lower levels of reliability than usually encountered on Mechanical Turk even though
the research group is expected to be more reliable
than crowd-sourced workers. However, on closer
inspection, the number of human assessors who
took part in the Turkish and Estonian evaluations
is small and the seemingly large difference in percentages in fact correspond to as few as three indi-
12Repeat items are separated by a minimum of 40 intervening assessments to reduce the likelihood of annotators simply
remembering previous scores for repeat assessment of translations.
Sig. Diff.
(A) & No Sig. Diff.
Exact Rep.
Mechanical Turk Crowd
Czech→English
German→English
227 ( 44%)
216 ( 95%)
Estonian→English
157 ( 40%)
150 ( 96%)
Finnish→English
102 ( 43%)
Russian→English
Turkish→English
172 ( 36%)
166 ( 97%)
Chinese→English
153 ( 38%)
148 ( 97%)
English→Russian
English→Chinese
915 ( 37%)
880 ( 96%)
Researcher
German→English
Estonian→English
Finnish→English
Russian→English
Turkish→English
Chinese→English
English→Czech
English→German
English→Estonian
English→Finnish
English→Russian
English→Turkish
English→Chinese
Researcher
239 ( 93%)
227 ( 95%)
Researcheralt
Czech→English
German→English
Estonian→English
Finnish→English
Russian→English
Turkish→English
Chinese→English
English→Czech
English→German
English→Estonian
English→Finnish
English→Russian
English→Turkish
English→Chinese
Researcheralt
352 ( 97%)
Total WMT18
1,506 ( 49%)
1,107 ( 96%)
Table 7: Number of unique workers, (A) those whose scores for bad reference items were signiﬁcantly lower than corresponding MT outputs; (B) those of (A) whose scores also showed no signiﬁcant difference for exact repeats of the same translation.
Researcher denotes the portion of the evaluation carried out with the standard DA HIT structure, while Researcheralt denotes
the remaining part that employed the altered HIT structure in which some quality control items are omitted.
Producing the Human Ranking
All research and crowd data that passed quality control were combined to produce the overall
shared task results.
In order to iron out differences in scoring strategies of distinct human assessors, human assessment scores for translations
were ﬁrst standardized according to each individual human assessor’s overall mean and standard
deviation score, for both researchers and crowd.
Average standardized scores for individual segments belonging to a given system are then computed, before the ﬁnal overall DA score for that
system is computed as the average of its segment
scores (Ave z in Table 8). Results are also reported
for average scores for systems, computed in the
same way but without any score standardization
applied (Ave % in Table 8).
Table 8 includes ﬁnal DA scores for all systems participating in WMT18 News Translation
Task. Clusters are identiﬁed by grouping systems
together according to which systems signiﬁcantly
outperform all others in lower ranking clusters, according to Wilcoxon rank-sum test.
Note that for English→German, the system
FACEBOOK-FAIR is not considered a regular participant, but an invited/late submission, see Section 2.3.6.
Appendix A shows the underlying head-to-head
signiﬁcance test results for all pairs of systems.
Source-based Direct Assessment
A secondary bilingual manual evaluation was carried out involving an adaptation of the standard
monolingual DA evaluation in which the source
language input segment was used in place of the
reference. Figure 5 provides a screenshot of this
evaluation as implemented in Appraise, which we
refer to as source-based DA. In this set-up system
outputs are evaluated by bilinguals who have access to the source language input segment only and
no reference translation. The main motivation for
doing so was to free up reference translations to
allow them to be used instead as a “human system” in the evaluation. By structuring the evaluation as a bilingual task it allows a human system
to be manually evaluated under exactly the same
conditions as all other systems thus providing an
estimate of human performance.13
13An alternate method is to keep DA monolingual but to
employ secondary reference translations. No secondary ref-
The aim of source-based DA is to produce accurate rankings for systems as well as the human system to allow direct comparison of system
and human performance, motivated by recent indications that Machine Translation quality may in
some cases be approaching human performance
 . For sourcebased DA, annotators will ideally be bilingual, i.e.
understand the source language sufﬁciently well,
in addition to being native speakers of the target
language. However, we did not speciﬁcally stipulate in this year’s evaluation that human annotators
be native speakers of the target language.
We run source-based DA for evaluation of English to Czech translation. This language pair was
selected because sufﬁcient annotators were available, helped by the fact that the set of systems participating in this language pair is small. This part
of the campaign employs the alternate HIT structure described in Section 3.3 with reduced quality
control items, i.e. it does not include exact repeats
of translations or reference translations for quality
control purposes.
A total of 17 annotators worked on the sourcebased DA pilot. 100% of annotators proved reliable, meaning that they scored bad reference
items signiﬁcantly lower than corresponding MT
outputs (see Table 7 part (A) for corresponding
reference-based DA percentages). For six candidate systems we collected 2, 574 assessments, resulting in an average of 429 annotations per individual system. Enforcing segment overlap during
HIT creation resulted in 423 segments for which
all six candidate translations have been scored. In
total, annotators worked on 438 distinct segments.
Table 9 provides source-based DA scores for all
primary English→Czech systems participating in
WMT18 News Translation Task as well as the human system comprised of reference translations.
Clusters are identiﬁed by grouping systems together according to which systems signiﬁcantly
outperform all others in lower ranking clusters, according to Wilcoxon rank-sum test.
As can be seen from clusters in Table 9, one system, CUNI-TRANSFORMER, appears to achieve
quality better than that of the human reference,
NEWSTEST2018-REF, while another, UEDIN, appears to be on par with human performance, and
although both systems certainly achieve very impressive results, claims of human parity should be
erence translations were available for the test set, however.
Chinese→English
UNISOUND-A
TENCENT-ENSEMBLE
UNISOUND-B
English→Chinese
TENCENT-ENSEMBLE
GTCOM-PRIMARY
ALIBABA-ENS-RERANK
ALIBABA-GENERAL-A
ALIBABA-GENERAL-B
Czech→English
CUNI-TRANSFORMER
English→Czech
CUNI-TRANSFORMER
German→English
UBIQUS-NMT
NJUNMT-PRIVATE
RWTH-UNSUPER
English→German
FACEBOOK-FAIR ⋆
MICROSOFT-MARIAN
MMT-PRODUCTION
RWTH-UNSUPER
Estonian→English
TILDE-NC-NMT
TILDE-C-NMT
TILDE-C-NMT-2BT
TILDE-C-NMT-COMB
CUNI-KOCMI
NEUROTOLGE.EE
UNSUPTARTU
English→Estonian
TILDE-NC-NMT
TILDE-C-NMT
TILDE-C-NMT-2BT
CUNI-KOCMI
NEUROTOLGE.EE
Finnish→English
CUNI-KOCMI
English→Finnish
HY-NMT-2STEP
CUNI-KOCMI
Russian→English
AFRL-SYSCOMB
English→Russian
ALIBABA-ENS
PROMT-HYB-MARIAN
PROMT-HYB-OPENNMT
PROMT-RULE-BASED
Turkish→English
ALIBABA-ENS
English→Turkish
ALIBABA-ENS-A
ALIBABA-ENS-B
Table 8: Ofﬁcial results of WMT18 News Translation Task. Systems ordered by standardized mean DA score, though systems
within a cluster are considered tied. Lines between systems indicate clusters according to Wilcoxon rank-sum test at p-level
p < 0.05. Systems with gray background indicate use of resources that fall outside the constraints provided for the shared task.
Figure 5: Screen shot of source-based Direct Assessment in the Appraise interface used in the English→Czech pilot campaign. The annotator is presented with a source text and a single system output randomly selected from competing systems
(anonymized), and is asked to rate the translation on a sliding scale.
taken with a degree of caution for several reasons
which we outline below.
Firstly, the alternate HIT structure applied in
this version of DA has not been tested thoroughly
enough to be certain of high reliability. For example, as described in Section 3.3, forcing all translations of a given source segment to be assessed by
the same human judge within the same HIT could
cause individual DA ratings to become highly relative as opposed to the aim of DA ratings to be as
close as possible to absolute judgments of translation quality. Furthermore, an additional bias that
could cause problems for this HIT structure is one
associated with a past evaluation method, relative
ranking. When evaluating competing translations
of the same source that are situated in close proximity within a HIT, annotators may be primed by
high (or low) quality outputs resulting in overly severe (or lenient) judgments for subsequent translations of the same source segment (Bojar et al.,
Secondly, while standard monolingual DA employs annotators only required to be speakers of a
single language, source-based DA requires ﬂuency
in two languages and it is not known the degree to
which varying levels of native language ﬂuency in
at least one language may negatively impact the
reliability of DA rankings in the case of bilingual
annotators.
Thirdly, it is likely that the quality of reference
translations can vary and this could potentially
impact the reliability of human performance estimates in source-based DA. Although referencebased DA assumes high quality reference translations, in the unfortunate case of problematic references, the overall rankings are unlikely to suffer
to any large degree in terms of the reliability of
system rankings, since all competing systems are
likely to suffer equally from any lack of quality in
reference translations.
However, in the adapted source-based version
of DA, the effect of low quality reference translations is quite different.
Firstly, since assessment involves comparison of MT outputs with the
source, genuine participating systems will not suffer from the fact reference translations are low
quality, since references are not involved in their
evaluation.
On the other hand, human performance estimates certainly will, as a drop in reference quality is indeed highly likely to negatively
impact the placement of human performance estimates in system rankings. The reliability of comparisons with human performance with sourcebased DA is therefore highly dependent on high
quality reference translations, as employment of
a low quality set of references can only lead to
underestimates of human performance. Considering the manual evaluation included several reports
of ill-formed reference translations, conclusions of
human parity and/or superiority relative to humans
should be avoided.
Considerations as to Human Parity
As mentioned above, before making any statements about “machine translation outperforming
humans” or “machine-human parity in translation”
it may be important to consider the following ad-
English→Czech
CUNI-TRANSFORMER
NEWSTEST2018-REF
Source-based DA results for English→Czech
newstest2018, where systems are ordered by standardized
mean DA score, though systems within a cluster are considered tied. Lines between systems indicate clusters according to Wilcoxon rank-sum test at p-level p < 0.05. Systems with gray background indicate use of resources that
fall outside the constraints provided for the shared task.
NEWSTEST2018-REF denotes the human system comprised
of human-produced reference translations.
ditional points:
• Since none of WMT18 systems process
larger units than individual sentences and our
evaluation does not include any context beyond individual segments, it is possible that
the human estimate is under-rewarded for
correct cross-sentential phenomena.
• The sample size employed in the sourcebased DA evaluation was smaller than the
recommended 1,500 judgments per system.
• The way in which translations in the test sets
were originally created was as follows: one
half of the test data for a given language pair
was translated in one language direction and
the other half in the opposite direction. It is
well known that the translation direction affects translation quality in training and this
could also be the case for evaluation.
• The formal education in linguistics or translatology of human assessors has not been
taken into account: it is likely that whether or
not human assessors have received any formal training in translation might inﬂuence
their acceptance of varying levels of wellformedness in translations. For example, untrained assessors might not be as sensitive to
subtle differences in verb conjugation, based
on their own experience: In many real-life
situations, the exact verb tense or conditional
chosen in one sentence may not really impact the overall message because it can be
implied from the context (and thus left free
to the imagination of the annotator in our
sentence-based evaluation) or from general
knowledge.
Test Suites
Arguably, both the manual and automatic evaluations carried out at WMT News Translation Task
are rather opaque. We learn (for each language
pair and with a known conﬁdence) which systems
perform better on average over the sentences sampled from the news test set.
This average performance however does not
provide any insight into which particular phenomena are handled better or worse by the systems. It
is quite possible that the overall best-performing
system may be unreliable for long sentences, for
named entities, for pronouns or others. Such targeted evaluations may be important for particular
deployment settings and use cases, and they are
deﬁnitely important for us, MT system developers,
in order to focus on them in subsequent research.
Acknowledgments
This work was supported in part by funding from the European Union’s Horizon 2020 research and innovation programme
under grant agreement Nos. 645452 (QT21)
and 645357 (Cracker),
and from the Connecting Europe Facility under agreement No.
NEA/CEF/ICT/A2016/1331648 (ParaCrawl).
We would also like to thank the University
of Helsinki, the University of Tartu, Yandex and
Microsoft for supplying test data for the news
translation task.
The human evaluation campaign was very
gratefully supported by contributions from Amazon, Microsoft, and Science Foundation Ireland
in the ADAPT Centre for Digital Content Technology (www.adaptcentre.ie) at Dublin City
University funded under the SFI Research Centres
Programme (Grant 13/RC/2106) co-funded under
the European Regional Development Fund.
We would also like to give special thanks to the
small group of Turkish speakers who rescued our
English-Turkish human evaluation at very short
notice by contributing their time voluntarily. Finally, we are grateful to the large number of
anonymous Mechanical Turk workers who contributed their human intelligence to the human
evaluation.