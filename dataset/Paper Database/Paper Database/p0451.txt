Multi-Manifold Semi-Supervised Learning
†Computer Sciences Dept.
University of Wisconsin-Madison
Madison, WI 53706, USA
{goldberg,jerryzhu,zhiting}
@cs.wisc.edu
Andrew B. Goldberg†
Xiaojin Zhu†
Aarti Singh‡
Zhiting Xu†
Robert Nowak∗
‡Applied and Computational Math
Princeton University
Princeton, NJ 08544, USA
 
∗Elec. and Computer Engineering
University of Wisconsin-Madison
Madison, WI 53706, USA
 
We study semi-supervised learning when the
data consists of multiple intersecting manifolds.
We give a ﬁnite sample analysis to
quantify the potential gain of using unlabeled
data in this multi-manifold setting. We then
propose a semi-supervised learning algorithm
that separates diﬀerent manifolds into decision sets, and performs supervised learning
within each set.
Our algorithm involves a
novel application of Hellinger distance and
size-constrained spectral clustering. Experiments demonstrate the beneﬁt of our multimanifold semi-supervised learning approach.
INTRODUCTION
The promising empirical success of semi-supervised
learning algorithms in favorable situations has triggered several recent attempts 
at developing a theoretical understanding of semisupervised learning. In a recent paper , it was established using a ﬁnite sample analysis that if the complexity of the distributions under consideration is too high to be learnt using n labeled data points, but is small enough to
be learnt using m ≫n unlabeled data points, then
semi-supervised learning (SSL) can improve the performance of a supervised learning (SL) task.
have also been many successful practical SSL algorithms as summarized in . These theoretical analyses and prac-
Appearing in Proceedings of the 12th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)
2009, Clearwater Beach, Florida, USA. Volume 5 of JMLR:
W&CP 5. Copyright 2009 by the authors.
tical algorithms often assume that the data forms clusters or resides on a single manifold.
However, both a theory and an algorithm are lacking
when the data is supported on a mixture of manifolds.
Such data occurs naturally in practice. For instance,
in handwritten digit recognition each digit forms its
own manifold in the feature space; in computer vision
motion segmentation, moving objects trace diﬀerent
trajectories which are low dimensional manifolds . These manifolds may intersect or partially overlap, while having diﬀerent dimensionality,
orientation, and density. Existing SSL approaches cannot be directly applied to multi-manifold data.
instance, traditional graph-based SSL algorithms may
create a graph that connects points on diﬀerent manifolds near a manifold intersection, thus diﬀusing information across the wrong manifolds.
In this paper, we generalize the theoretical analysis
of to the case where the data is supported on a mixture of manifolds. Guided by the theory, we propose an SSL algorithm that handles multiple manifolds as well as clusters. The algorithm builds
upon novel Hellinger-distance-based graphs and sizeconstrained manifold clustering.
Experiments show
that our algorithm can perform SSL on multiple intersecting, overlapping, and noisy manifolds.
THEORETIC PERSPECTIVES
In this section, we ﬁrst review the conclusions of , which are based on the cluster assumption, and then give conjectured bounds in the single
manifold and multi-manifold case.
The cluster assumption, as formulated in , states that the target regression function or
class label is locally smooth over certain subsets of
the D-dimensional feature space that are delineated by
changes in the marginal density—throughout this paper, we assume the marginal density is bounded above
Multi-Manifold Semi-Supervised Learning
and below (away from zero). We refer to these delineated subsets as decision sets; i.e., all non-empty sets
formed by intersections between the cluster support
sets and their complements. If these decision sets, denoted by C, can be learnt using unlabeled data, the
learning task on each decision set is simpliﬁed. The
results of suggest that if the decision sets can be resolved using unlabeled data, but
not using labeled data, then semi-supervised learning
can help. However, this simple argument, and hence
the distinctions between SSL and SL, are not always
captured by standard asymptotic arguments based on
rates of convergence. used ﬁnite
sample bounds to characterize both the SSL gains and
the relative value of unlabeled data.
To derive the ﬁnite sample bounds, the ﬁrst step is
to understand when the decision sets are resolvable
using data.
This depends on the interplay between
the complexity of the class of distributions under consideration and the number of unlabeled points m and
labeled points n. For the cluster case, the complexity of the distributions is determined by the margin
γ, deﬁned as the minimum separation between clusters or the minimum width of a decision set . If the margin γ is larger than the typical distance between the data points (m−1/D if using
unlabeled data, or n−1/D if using only labeled data),
then with high probability the decision sets can be
learnt up to a high accuracy (which depends on m or
n, respectively) . This implies that
if γ > m−1/D (margin exists with respect to density
of unlabeled data), then the ﬁnite sample performance
(the expected excess error Err) of a semi-supervised
learner bfm,n relative to the performance of a clairvoyant supervised learner bfC,n, which has perfect knowledge of the decision sets C, can be characterized as
Err( bfm,n) ≤
Err( bfC,n) + δ(m, n). (1)
Here PXY (γ) denotes the cluster-based class of distributions with complexity γ, and δ(m, n) is the error incurred due to inaccuracies in learning the decision sets
using unlabeled data. Comparing this upper bound
on the semi-supervised learning performance to a ﬁnite sample minimax lower bound on the performance
of any supervised learner provides a sense of the relative performance of SL vs. SSL. Thus, SSL helps if
complexity of the class of distributions γ > m−1/D
and both of the following conditions hold: (i) knowledge of decision sets simpliﬁes the supervised learning task, that is, the error of the clairvoyant learner
supPXY (γ) Err( bfC,n) < inffn supPXY (γ) Err(fn), the
smallest error that can be achieved by any supervised
learner based on n labeled data. The diﬀerence quan-
Table 1: Conjectured ﬁnite sample performance of SSL
and SL for regression of a H¨older-α, α > 1, smooth
function (with respect to geodesic distance in the manifold cases). These bounds hold for D ≥2, d < D,
m ≫n, and suppress constants and log factors.
Complexity
Cluster Assumption
D > γ ≥m−1
D > γ ≥−m−1
Single Manifold κSM := min(r0, s0)
D > κSM ≥m−1
D > κSM ≥0
Multi-Manifold κMM := sgn(γ) · min(|γ|, r0, s0)
D > κMM ≥m−1
D > κMM ≥−m−1
tiﬁes the SSL performance gain. (ii) m is large enough
so that the error incurred due to using a ﬁnite amount
of unlabeled data to learn the decision sets is negligible: δ(m, n) = O
supPXY (γ) Err( bfC,n)
. This quantiﬁes the relative value of labeled and unlabeled data.
The ﬁnite sample performance bounds on SSL and
SL performance as derived in for
the cluster assumption are summarized in Table 1 for
the regression setting, where the target function is a
H¨older-α smooth function on each decision set and
α > 1. We can see that SSL provides improved performance, by capitalizing on the local smoothness of
the function on each decision set, when the separation
between the clusters is large compared to the typical
distance between unlabeled data m−1/D but less than
the typical distance between labeled data n−1/D. Negative γ refers to the case where the clusters are not
separated, but can overlap and give rise to decision
sets that are adjacent ). In this
case, SSL always outperforms SL provided the width
of the resulting decision sets is detectable using unlabeled data. Thus, the interplay between the margin
and the number of labeled and unlabeled data characterizes the relative performance of SL vs. SSL under
the cluster assumption. Similar results can be derived
in the classiﬁcation setting where an exponential improvement (from n−1/D to e−n) is possible provided
the number of unlabeled data m grows exponentially
Goldberg, Zhu, Singh, Xu, Nowak
with n .
SINGLE MANIFOLD CASE
In the single manifold case, the assumption is that
the target function lies on a lower d-dimensional manifold, where d < D, and is H¨older-α smooth (α > 1)
with respect to the geodesic distance on the manifold.
Hence knowledge of the manifold, or equivalently the
geodesic distances between all pairs of data points, can
be gleaned using unlabeled data and reduces the dimensionality of the learning task.
In the case of distributions supported on a single manifold, the ability to learn the geodesic distances well,
and hence the complexity κSM of the distributions, depends on two geometric properties of the manifold—
its minimum radius of curvature r0 and proximity to
self-intersection s0 (also known as branch separation)
 .
If κSM := min(r0, s0) is larger than the typical distance between the data points (m−1/D with unlabeled
data, or n−1/D with only labeled data), then with high
probability the manifold structure is resolvable and
geodesic distances can be learnt up to a high accuracy
(which depends on m or n, respectively). This can be
achieved by using shortest distance paths on an ϵ- or
k-nearest neighbor graph to approximate the geodesic
distances . The use of approximate geodesic distances to learn the target function
gives rise to an error-in-variable problem. Though the
overall learning problem is now reduced to a lowerdimensional problem, we are now faced with two types
of errors—the label noise and the error in the estimated distances. However, the error incurred in the
ﬁnal estimation due to errors in geodesic distances depends on m which is assumed to be much greater than
n. Thus, the eﬀect of the geodesic distance errors is
negligible, compared to the error due to label noise,
for m suﬃciently large.
This suggests that for the
manifold case, if κSM > m−1/D, then ﬁnite sample
performance of semi-supervised learning can again be
related to the performance of a clairvoyant supervised
learner bfC,n as in (1) above, since δ(m, n) is negligible
for m suﬃciently large.
Comparing this SSL performance bound to a ﬁnite
sample minimax lower bound on the performance of
any supervised learner indicates SSL’s gain in the single manifold case and is summarized in Table 1. These
are conjectured bounds based on the arguments above
and similar arguments in . The SSL upper bound can be achieved using a learning procedure
adaptive to both α and d, such as the method proposed
in 1. The SL lower bounds follow
1Note, however, that the analysis in 
from the results in and .
SSL provides improved performance by capitalizing on
the lower-dimensional structure of the manifold when
the minimum radius of curvature and branch separation are large compared to the typical distance between unlabeled data m−1/D, but small compared to
the typical distance between labeled data n−1/D.
MULTI-MANIFOLD CASE
The multi-manifold case addresses the generic setting
where the clusters are low-dimensional manifolds that
possibly intersect or overlap. In this case, the target
function is supported on multiple manifolds and can be
piecewise smooth on each manifold. Thus, it is of interest to resolve the manifolds, as well as the subsets of
each manifold where the decision label varies smoothly
(that are characterized by changes in the marginal density). The analysis for this case is a combination of the
cluster and single manifold case. The complexity of the
multi-manifold class of distributions, denoted κMM, is
governed by the minimum of the manifold curvatures,
branch separations, and the separations and overlaps
between distinct manifolds. For the regression setting,
the conjectured ﬁnite sample minimax analysis is presented in Table 1.
These results indicate that when there is enough unlabeled data, but not enough labeled data, to handle the
complexity of the class, then semi-supervised learning
can help by adapting to both the intrinsic dimensionality and smoothness of the target function. Extensions
of these results to the classiﬁcation setting are straightforward, as discussed under the cluster assumption.
AN ALGORITHM
Guided by the theoretical analysis in the previous section, we propose a “cluster-then-label” type of SSL algorithm, see Figure 1. It consists of three main steps:
(1) It uses the unlabeled data to form a small number of decision sets, on which the target function is
assumed to be smooth. The decision sets are deﬁned
in the ambient space, not just on the labeled and unlabeled points. (2) The target function within a particular decision set is estimated using only labeled data
that fall in that decision set, and using a supervised
learner speciﬁed by the user. (3) a new test point is
predicted by the target function in the decision set it
falls into.
There have been several cluster-then-label approaches
in the SSL literature.
For example, the early work
of Demiriz et al. modiﬁes the objective of standard
considers the asymptotic performance of SL, whereas here
we are studying the ﬁnite-sample performance of SSL.
Multi-Manifold Semi-Supervised Learning
k-means clustering algorithms to include a class impurity term . El-
Yaniv and Gerzon enumerate all spectral clusterings
of the unlabeled data with varying number of clusters,
which together with labeled data induce a hypothesis
space. They then select the best hypothesis based on
an Occam’s razor-type transductive bound . Some work in “constrained clustering” is also closely related to cluster-then-label from
an SSL perspective (Basu, Davidson & Wagstaﬀ2008).
Compared to these approaches, our algorithm has two
advantages: i) it is supported by our SSL minimax
theory; ii) it handles both overlapping clusters and intersecting manifolds by detecting changes in support,
density, dimensionality or orientation.
Our algorithm is also diﬀerent from the family of
graph-regularized SSL approaches, such as manifold
regularization 
and earlier variants .
Those approaches essentially add a
graph-regularization term in the objective. They also
depend on the “manifold assumption” that the target
function indeed varies smoothly on the manifold. In
contrast, i) our algorithm is a wrapper method, which
uses any user-speciﬁed supervised learner SL as a subroutine. This allows us to directly take advantage of
advances in supervised learning without the need to
derive new algorithms. ii) Our theory ensures that,
even when the manifold assumption is wrong, our SSL
performance bound is the same as that of the supervised learner (up to a log factor).
Finally, step 1 of our algorithm is an instance of manifold clustering. Recent advances on this topic include
Generalized Principal Component Analysis and lossy coding for mixtures of linear subspaces, multiscale manifold identiﬁcation with algebraic multigrid , tensor voting , spectral curvature
clustering , and translated Poisson mixture model for
mixtures of nonlinear manifolds.
Our algorithm is
unique in two ways: i) its use of Hellinger distance
oﬀers a new approach to detecting overlapping clusters and intersecting manifolds; ii) our decision sets
have minimum size constraints, which we enforce by
constrained k-means.
HELLINGER DISTANCE GRAPH
Let the labeled data be {(xi, yi)}n
i=1, and the unlabeled data be {xj}M
j=1, where M ≫n.
The building block of our algorithm is a local sample covariance matrix.
For a point x, deﬁne N(x) to be
a small neighborhood around x in Euclidean space.
Let Σx be the local sample covariance matrix at x:
x′∈N(x)(x′−µx)(x′−µx)⊤/(|N(x)|−1), where
x′∈N(x) x′/|N(x)| is the neighborhood mean.
In our experiments, we let |N(x)| ∼O(log(M)) so that
the neighborhood size grows with unlabeled data size
The covariance Σx captures the local geometry
Our intuition is that points xi, xj on diﬀerent manifolds or in regions with diﬀerent density should be
considered dissimilar.
This intuition is captured by
the Hellinger distance between their local sample covariance matrices Σi, Σj. The squared Hellinger distance is deﬁned between two pdf’s p, q: H2(p, q) =
N(x; 0, Σi), i.e., a Gaussian with zero mean and covariance Σi, and similarly q(x) = N(x; 0, Σj), we extend the deﬁnition of Hellinger distance to covariance
matrices: H(Σi, Σj) ≡H (N(x; 0, Σi), N(x; 0, Σi)) =
1 −2D/2|Σi|1/4|Σj|1/4/|Σi + Σj|1/2, where D is the
dimensionality of the ambient feature space. We will
also call H(Σi, Σj) the Hellinger distance between the
two points xi, xj. When Σi +Σj is rank deﬁcient, H is
computed in the subspace occupied by Σi + Σj. The
Hellinger distance H is symmetric and in .
is small when the local geometry is similar, and large
when there is signiﬁcant diﬀerence in density, manifold
dimensionality or orientation. Example 3D covariance
matrices and their H values are shown in Figure 2.
Cov. matrices
orientation
Figure 2: Hellinger distance
Hellinger distances between our dataset of n + M
points to form a graph, and apply a graph-cut algorithm to separate multiple manifolds or clusters. However, if xi and xj are very close to each other, their local neighborhoods N(xi), N(xj) will strongly overlap.
Then, even if the two points are on diﬀerent manifolds
the Hellinger distance will be small, because their covariance matrices Σi, Σj will be similar. Therefore, we
select a subset of m ∼O (M/ log(M)) unlabeled points
so that they are farther apart while still covering the
whole dataset. This is done using a greedy procedure,
Goldberg, Zhu, Singh, Xu, Nowak
Given n labeled examples and M unlabeled examples, and a supervised learner SL,
1. Use the unlabeled data to infer k ∼O(log(n)) decision sets c
(a) Select a subset of m < M unlabeled points
(b) Form a graph on the n + m labeled and unlabeled points, where the edge weights are computed
from the Hellinger distance between local sample covariance matrices
(c) Perform size-constrained spectral clustering to cut the graph into k parts, while keeping enough
labeled and unlabeled points in each part
2. Use the labeled data in c
Ci and the supervised learning SL to train bfi
3. For test point x∗∈c
Ci, predict bfi(x∗).
Figure 1: The Multi-Manifold Semi-Supervised Learning Algorithm
where we ﬁrst select an arbitrary unlabeled point x(0).
We then remove its unlabeled neighbors N(x(0)), including itself. We select x(1) to be the next nearest
neighbor, and repeat. This procedure thus approximately selects a cover of the dataset.
We focus on
the subset of m unlabeled and n labeled points. Each
of these n + m points has its local covariance Σ computed from the original full dataset. We then discard
the M −m unselected unlabeled points. Notice, however, that the number m of eﬀective unlabeled data
points is polynomially of the same order as the total
number M of available unlabeled data points.
Figure 3: The graph on the dollar sign dataset.
We can now deﬁne a sparse graph on the n+m points.
Each point x is connected by a weighted, undirected
edge to O(log(n+m)) of its nearest Mahalanobis neighbors chosen from the the set of n + m points too. The
choice of O(log(n + m)) allows neighborhood size to
grow with dataset size. Since we know the local geometry around x (captured by Σx), we “follow the manifold” by using the Mahalanobis distance as the local
distance metric at x: d2
M(x, x′) = (x−x′)⊤Σ−1
For example, a somewhat ﬂat Σx will preferentially
connect to neighbors in or near the same ﬂat subspace. The graph edges are weighted using the standard RBF scheme, but with Hellinger distance: wij =
 −H2(Σi, Σj)/(2σ2)
. Figure 3(a) shows a small
part of a synthetic “dollar sign” dataset, consisting of
two intersecting manifolds: “S” and “|”. The green
dots are the original unlabeled points, and the ellipsoids are the contours of covariance matrices around
the subset of selected unlabeled points within a small
region. Figure 3(b) shows the graph on the complete
dollar sign dataset, where red edges have large weights
and yellow edges have small weights. Thus the graph
combines locality and geometry: an edge has large
weight when the two nodes are close in Mahalanobis
distance, and have similar covariance structure.
SIZE-CONSTRAINED SPECTRAL
CLUSTERING
We perform spectral clustering on this graph of n + m
We hope each resulting cluster represents a
separate manifold, from which we will deﬁne a decision set. Of the many spectral clustering algorithms,
we chose ratio cut for its simplicity, though others can
be similarly adapted for use here. The standard ratio cut algorithm for k clusters has four steps : 1. Compute the unnormalized graph
Laplacian L = Deg−W, where W = [wij] is the weight
matrix, and Degii = P
j wij form the diagonal degree
matrix. 2. Compute the k eigenvectors v1 . . . vk of L
with the smallest eigenvalues. 3. Form matrix V with
v1 . . . vk as columns. Use the ith row of V as the new
representation of xi. 4. Cluster all x under the new
representation into k clusters using k-means.
Our ultimate goal of semi-supervised learning poses
new challenges; we want our SSL algorithm to degrade gracefully, even when the manifold assumption
does not hold. The SSL algorithm should not break
the problem into too many subproblems and increase
the complexity of the supervised learning task. This
is achieved by requiring that the algorithm does not
generate too many clusters and that each cluster contains “enough” labeled points. Because we will simply
do supervised learning within each decision set, as long
as the number of sets does not grow polynomially with
n, the performance of our algorithm is guaranteed to
be polynomially no worse than the performance of the
supervised learner when the manifold assumption fails.
Thus, we automatically revert to the supervised learn-
Multi-Manifold Semi-Supervised Learning
ing performance. One way to achieve this is to have
three requirements: i) the number of clusters grows
as k ∼O(log(n)); ii) each cluster must have at least
a ∼O(n/ log2(n)) labeled points; iii) each spectral
cluster must have at least b ∼O(m/ log2(n)) unlabeled points. The ﬁrst sets the number of clusters k,
allowing more clusters and thus handling more complex problems as labeled data size grows, while suﬀering only a logarithmic performance loss compared to
a supervised learner if the manifold assumption fails.
The second requirement ensures that each decision set
has O(n) labeled points up to log factor2. The third
is similar, and makes spectral clustering more robust.
Spectral clustering with minimum size constraints a, b
on each cluster is an open problem.
Directly enforcing these constraints in graph partitioning leads
to diﬃcult integer programs.
Instead, we enforce
the constraints in k-means (step 4) of spectral clustering. Our approach is a straightforward extension
to the constrained k-means algorithm of Bradley et
al. . For point xi,
let Ti1 . . . Tik ∈R be its cluster indicators: ideally,
Tih = 1 if xi is in cluster h, and 0 otherwise.
c1 . . . ck ∈Rd denote the cluster centers. Constrained
k-means is the iterative minimization over T and c of
the following problem:
h=1 Tih∥xi −ch∥2
h=1 Tih = 1, T ≥0
i=1 Tih ≥a, Pn+m
i=n+1 Tih ≥b, h = 1 . . . k, (2)
where we assume the points are ordered so that the
ﬁrst n points are labeled. Fixing T, optimizing over
c is trivial, and amounts to moving the centers to the
cluster means.
Bradley et al. showed that ﬁxing c and optimizing T
can be converted into a Minimum Cost Flow problem,
which can be exactly solved. In a Minimum Cost Flow
problem, there is a directed graph where each node is
either a “supply node” with a number r > 0, or a
“demand node” with r < 0. The arcs from i →j is
associated with cost sij, and ﬂow tij. The goal is to
ﬁnd the ﬂow t such that supply meets demand at all
nodes, while the cost is minimized:
tji = ri, ∀i.
For our problem (2), the corresponding Minimum Cost
Flow problem is shown in Figure 4. The supply nodes
are x1 . . . xn+m with r = 1. There are two sets of cluster center nodes. One set cℓ
1 . . . cℓ
k, each with demand
2The square allows the size ratio between two clusters
to be arbitrarily skewed as n grows. We do not want to ﬁx
the relative sizes of the decision sets a priori.
r = −a, is due to the labeled data size constraint.
The other set cu
1 . . . cu
k, each with demand r = −b,
is due to the unlabeled data size constraint. Finally,
a sink demand node with r = −(n + m −ak −bk)
catches all the remaining ﬂow. The cost from xi to ch
is sih = ∥xi −ch∥2, and from ch to the sink is 0. It is
then clear that the Minimum Cost Flow problem (3)
is equivalent to (2) with Tih = tih and c ﬁxed. Interestingly, (3) is proven to have integer solutions which
correspond exactly to the desired cluster indicators.
Figure 4: The Minimum Cost Flow problem
Once size-constrained spectral clustering is completed,
the n+m points will each have a cluster index in 1 . . . k.
We deﬁne k decision sets {c
i=1 by the Voronoi cells
around these points: c
Ci = {x ∈RD | x’s Euclidean
nearest neighbor among the n + m points has cluster
We train a separate predictor bfi for each
decision set using the labeled points in that decision
set, and a user-speciﬁed supervised learner.
test time, an unseen point x∗∈c
Ci is predicted as
bfi(x∗). Therefore, the unlabeled data in our algorithm
is used merely to determine the decision sets.
EXPERIMENTS
Data Sets. We experimented with ﬁve synthetic (Figure 5) and one real data sets. Data sets 1–3 are for
regression, and 4–6 are for classiﬁcation: (1).
Dollar sign contains two intersecting manifolds. The “S”
manifold has target y varying from 0 to 3π. The “|”
manifold has target function y = x·3 + 13, where x·3
is the vertical dimension. White noise ϵ ∼N(0, 0.012)
is added to y. (2) Surface-sphere slices a 2D surface through a solid ball. The ball has target function
y = ||x||, and the surface has y = x·2 −5. (3) Density change contains two overlapping rectangles. One
rectangle is wide and sparse with y = x·1, the other is
narrow and ﬁve times as dense with y = 10−5x·1. Together they produce three decision sets. (4) Surfacehelix has a 1D toroidal helix intersecting a surface.
Each manifold is a separate class.
(5) Martini is
a composition of ﬁve manifolds (classes) to form the
shape of a martini glass with an olive on a toothpick, as
shown in Figure 5(e). (6) MNIST digits. We scaled
Goldberg, Zhu, Singh, Xu, Nowak
Clairvoyant
Clairvoyant
Clairvoyant
Error rate
Error rate
(a) Dollar sign
(b) Surface-sphere
(c) Density change
(d) Surface-helix
(e) Martini
Figure 5: Regression MSE (a-c) and classiﬁcation error (d-e) for synthetic data sets. All curves are based on
M = 20000, 10-trial averages, and error bars plot ±1 standard deviation. Clairvoyant classiﬁcation error is 0.
down the images to 16 x 16 pixels and used the oﬃcial
train/test split, with diﬀerent numbers of labeled and
unlabeled examples sampled from the training set.
Methodology & Implementation Details. In all
experiments, we report results that are the average of
10 trials over random draws of M unlabeled and n labeled points. We compare three learners: [Global]:
supervised learner trained on all of the labeled data,
ignoring unlabeled data.
[Clairvoyant]:
knowledge of the true decision sets, trains one supervised learner per decision set.
[SSL]: our semisupervised learner that discovers the decision sets using unlabeled data, then trains one supervised learner
per decision set. After training, each classiﬁer is evaluated on a massive test set, also sampled from the underlying distribution, to estimate generalization error.
We implemented the algorithms in MATLAB, with
Minimum Cost Flow solved by the network simplex
method in CPLEX. We used the same set of parameters for all experiments and all data sets: We chose
the number of decision sets to be k = ⌈0.5 log(n)⌉.
To obtain the subset of m unlabeled points, we let
the neighborhood size |N(x)| = ⌊3 log(M)⌋.
creating the graph W, we used ⌊1.5 log(m + n)⌋nearest Mahalanobis neighbors, and an RBF bandwidth
σ = 0.2 to convert Hellinger distances to edge weights.
The size constraints were a = ⌊1.25n/log2(n)⌋, b =
⌊1.25m/log2(n)⌋. Finally, to avoid poor local optima
in spectral clustering, we ran 10 random restarts for
constrained k-means, and chose the result with the
lowest objective.
For the regression tasks, we used
kernel regression with an RBF kernel, and tuned the
bandwidth parameter with 5-fold cross validation using only labeled data in each decision set (or globally for “Global”). For classiﬁcation, we used a support vector machine (LIBSVM) with an RBF kernel,
and tuned its bandwidth and regularization parameter with 5-fold cross validation. We used Euclidean
distance in each decision region for the supervised
learner, but we expect performance with geodesic distance would be even better.
Results of Large M: Figure 5 reports the results
for the ﬁve synthetic data sets. In all cases, we used
M = 20000, n ∈{20, 40, 80, 160, 320, 640}, and the
resulting regressors/classiﬁers are evaluated in terms
of MSE or error rate using a test set of 20000 points.
These results show that our SSL algorithm can discover multiple manifolds and changes in density well
enough to consistently outperform SL in both regression and classiﬁcation settings of varying complexity3.
We also observed that even under- or over-partitioning
into fewer or more decision sets than manifolds can still
improve SSL performance4.
We performed three experiments with the digit recognition data: binary classiﬁcation of the digits 2 vs 3,
and three-way classiﬁcation of 1, 2, 3 and 7, 8, 9. Here,
we ﬁxed n = 20, M = 5000, 10 random training trials, each tested on the oﬃcial test set. Table 2 contains results averaged over these trials. SSL outperforms Global in all three digit tasks, and all diﬀerences
are statistically signiﬁcant (α = 0.05). Note that we
used the same parameters as the synthetic data experiments, which results in k = 2 decision sets for n = 20;
again, the algorithm performs well even when there are
fewer decision sets than classes. Close inspection revealed that our clustering step creates relatively pure
decision sets. For the binary task, this leads to two
3Though not shown in Figure 5, we found that a
standard graph-based SSL algorithm manifold regularization , using Euclidean kNN graphs with
RBF weights and all parameters tuned using cross validation, performs worse than Global on these datasets due to
the strong connections across manifolds.
4We compared Global and SSL’s 10 trials at each n
using two-tailed paired t-tests. SSL was statistically signiﬁcantly better (α = 0.05) in the following cases: dollar
sign at n = 20–80, density at n = 40–640, surface-helix at
n = 20–320, and martini at n = 40–320. The two methods
were statistically indistinguishable in other cases.
Multi-Manifold Semi-Supervised Learning
Table 2: 10-trial average test set error rates ± one
standard deviation for handwritten digit recognition.
0.17 ± 0.12
0.20 ± 0.10
0.33 ± 0.20
0.05 ± 0.01
0.10 ± 0.04
0.20 ± 0.10
0.19 ± 0.04
0.12 ± 0.02
0.04 ± 0.008
Figure 6: Eﬀect of varying M for the surface-helix data
set (n = 80). See text for details.
trivial classiﬁcation problems, and errors are due only
to incorrect assignments of test points to decision sets.
For the 3-way tasks, the algorithm creates 1+2 and 3
clusters, and 7+9 and 8 clusters.
Eﬀect of Too Small an M: Finally, we examine
our SSL algorithm’s performance with less unlabeled
data. For the surface-helix data set, we now ﬁx n = 80
(which leads to k = 3 decision sets) and reduce M.
Figure 6 depicts example partitionings for three M
values, along with 10-trial average error rates (± one
standard deviation) in each setting.
Note these are
top-down views of the data in Figure 5(d). When M
is small, the resulting subset of m unlabeled points
is too small, and the partition boundaries cannot be
reliably estimated. Segments of the helix shown in red
and areas of the surface in blue or green correspond to
such partitioning errors. Nevertheless, even when M is
as small as 1000, SSL’s performance is no worse than
Global supervised learning, which achieves an error
rate of 0.20 ± 0.05 when n = 80 (see Figure 5(d)).
Conclusions:
We have extended SSL theory and
practice to multi-manifolds.
A detailed analysis of
geodesic distances, automatic parameter selection, and
large scale empirical study remains as future work.
Acknowledgements
Alumni Research Foundation. AG is supported in part
by a Yahoo! Key Technical Challenges Grant.