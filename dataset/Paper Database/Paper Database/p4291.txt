IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Salient Object Detection in the
Deep Learning Era: An In-depth Survey
Wenguan Wang, Member, IEEE, Qiuxia Lai, Huazhu Fu, Senior Member, IEEE,
Jianbing Shen, Senior Member, IEEE, Haibin Ling, and Ruigang Yang, Senior Member, IEEE
Abstract—As an essential problem in computer vision, salient object detection (SOD) has attracted an increasing amount of research
attention over the years. Recent advances in SOD are predominantly led by deep learning-based solutions (named deep SOD). To
enable in-depth understanding of deep SOD, in this paper, we provide a comprehensive survey covering various aspects, ranging from
algorithm taxonomy to unsolved issues. In particular, we ﬁrst review deep SOD algorithms from different perspectives, including
network architecture, level of supervision, learning paradigm, and object-/instance-level detection. Following that, we summarize and
analyze existing SOD datasets and evaluation metrics. Then, we benchmark a large group of representative SOD models, and provide
detailed analyses of the comparison results. Moreover, we study the performance of SOD algorithms under different attribute settings,
which has not been thoroughly explored previously, by constructing a novel SOD dataset with rich attribute annotations covering various
salient object types, challenging factors, and scene categories. We further analyze, for the ﬁrst time in the ﬁeld, the robustness of SOD
models to random input perturbations and adversarial attacks. We also look into the generalization and difﬁculty of existing SOD
datasets. Finally, we discuss several open issues of SOD and outline future research directions. All the saliency prediction maps, our
constructed dataset with annotations, and codes for evaluation are publicly available at 
Index Terms—Salient Object Detection, Deep Learning, Benchmark, Image Saliency.
INTRODUCTION
ALIENT object detection (SOD) aims at highlighting visually salient object regions in images. Here, ‘visually
salient’ describes the property of an object or a region to
attract human observers’ attention. SOD is driven by and
applied to a wide spectrum of object-level applications in various areas. In computer vision, representative applications
include image understanding , , image captioning –
 , object detection , , unsupervised video object segmentation , , semantic segmentation – , person
re-identiﬁcation , , and video summarization ,
 . In computer graphics, SOD also plays an essential role
in various tasks, including non-photorealistic rendering ,
 , image cropping , , image retargeting , etc.
Several applications in robotics, such as human-robot interaction , , and object discovery , , also beneﬁt
from SOD for better scene/object understanding.
Though inspired by eye ﬁxation prediction (FP) ,
which originated from cognitive and psychology research
communities to investigate the human attention mechanism
Switzerland.
 )
Q. Lai is with the Department of Computer Science and Engineering,
the Chinese University of Hong Kong, Hong Kong, China. (Email:
 )
H. Fu is with Inception Institute of Artiﬁcial Intelligence, UAE. (Email:
 )
J. Shen is with Beijing Laboratory of Intelligent Information Technology,
School of Computer Science, Beijing Institute of Technology, China.
(Email: )
H. Ling is with the Department of Computer and Information Sciences,
Temple University, Philadelphia, PA, USA. (Email: )
R. Yang is with the University of Kentucky, Lexington, KY 40507. (Email:
 )
Corresponding author: Jianbing Shen
by predicting eye ﬁxation positions in visual scenes, SOD
differs in that it aims to detect the whole attentive object
regions. Since the renaissance of deep learning techniques,
signiﬁcant improvement for SOD has been achieved in
recent years, thanks to the powerful representation learning
methods. Since the ﬁrst introduction in 2015 – , deep
learning-based SOD (or deep SOD) algorithms have quickly
shown superior performance over traditional solutions, and
have continued to improve the state-of-the-art.
This paper provides a comprehensive and in-depth survey of SOD in the deep learning era. In addition to taxonomically reviewing existing deep SOD methods, it provides
in-depth analyses of representative datasets and evaluation
metrics, and investigates crucial but largely under-explored
issues, such as the robustness and transferability of deep
SOD models, their strengths and weaknesses under certain
scenarios (i.e., scene/salient object categories, challenging
factors), as well as the generalizability and difﬁculty of SOD
datasets. The saliency maps used for benchmarking, our
constructed dataset, and evaluation codes are available at
 
History and Scope
Humans are able to quickly allocate attention to important
regions in visual scenes. Understanding and modeling such
an astonishing ability, i.e., visual attention or visual saliency,
is a fundamental research problem in psychology, neurobiology, cognitive science, and computer vision. There are
two categories of computational models for visual saliency,
namely FP and SOD. FP originated from cognitive and
psychology communities , , , and targets at predicting where people look in images.
 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
(Liu et al.)
(Achanta et al.)
(Perazzi et al.)
(Zhu et al.)
(Yan et al.)
(Wang et al.)
(Zhao et al.)
(Zhang et al.)
(Liu et al.)
(Hou et al.)
(Liu et al.)
Very begining
Deep Methods
(Cheng et al.)
Traditional Methods
(Liu et al.)
(Qi et al.)
Capsule-based
Fig. 1. A brief chronology of SOD. The very ﬁrst SOD models date back to the work of Liu et al. and Achanta et al. . The ﬁrst incorporation of
deep learning techniques into SOD models was in 2015. Listed methods are milestones, which are typically highly cited. See §1.1 for more details.
Summary of previous reviews. For each work, the publication information and coverage are provided. See §1.2 for more detailed descriptions.
Year Venue Description
State-of-the-Art in Visual Attention Modeling 
2013 TPAMI This paper reviews visual attention (i.e. ﬁxation prediction) models before 2013.
Salient Object Detection: A Benchmark 
This paper benchmarks 29 heuristic SOD models and 10 FP methods over 7 datasets.
Attentive Systems: A Survey 
This paper reviews applications that utilize visual saliency cues.
A Review of Co-Saliency Detection Algorithms:
Fundamentals, Applications, and Challenges 
This paper reviews the fundamentals, challenges, and applications of co-saliency detection.
Review of Visual Saliency Detection with Comprehensive Information 
2018 TCSVT This paper reviews RGB-D SOD, co-saliency detection and video SOD.
Advanced Deep-Learning Techniques for Salient and
Category-Speciﬁc Object Detection: A Survey 
This paper reviews several sub-directions of object detection, namely objectness detection, SOD
and category-speciﬁc object detection.
Saliency Prediction in the Deep Learning Era: Successes
and Limitations 
2019 TPAMI This paper reviews image and video ﬁxation prediction models and analyzes speciﬁc questions.
Salient Object Detection: A Survey 
This paper reviews 65 heuristic and 21 deep SOD models up to 2017 and discusses closely related
areas like object detection, ﬁxation prediction, segmentation, etc.
The history of SOD is relatively short and can be traced
back to and . The rise of SOD has been driven by
a wide range of object-level computer vision applications.
Instead of FP models only predicting sparse eye ﬁxation
locations, SOD models aim to detect the whole entities of the
visually attractive objects with precise boundaries. Most traditional, non-deep SOD models , rely on low-level
features and certain heuristics (e.g., color contrast , background prior ). To obtain uniformly highlighted salient
objects and clear object boundaries, an over-segmentation
process that generates regions , super-pixels , , or
object proposals is often integrated into these models.
Please see for a more comprehensive overview.
With the compelling success of deep learning technologies in computer vision, more and more deep SOD methods
have begun springing up since 2015. Earlier deep SOD
models utilized multi-layer perceptron (MLP) classiﬁers to
predict the saliency score of deep features extracted from
each image processing unit – . Later, a more effective
and efﬁcient form, i.e., fully convolutional network (FCN)based model, became the mainstream SOD architecture.
Some recent methods , also introduced Capsule 
into SOD to comprehensively address object property modeling. A brief chronology of SOD is shown in Fig. 1.
Scope of the survey. Despite its short history, research in
deep SOD has produced hundreds of papers, making it
impractical (and fortunately unnecessary) to review all of
them. Instead, we comprehensively select inﬂuential papers
 
years, but for completeness and better readability, some
early related works are also included. Due to limitations
on space and our knowledge, we apologize to those authors
whose works are not included in this paper. It is worth noting that we restrict this survey to single-image SOD methods,
and leave RGB-D SOD, co-saliency detection, video SOD,
etc., as separate topics.
Related Previous Reviews and Surveys
Table 1 lists existing surveys that are related to ours. Among
them, Borji et al. reviewed SOD methods preceding
2015, thus do not refer to recent deep learning-based solutions. Zhang et al. reviewed methods for co-saliency detection, i.e., detecting common salient objects from multiple
relevant images. Cong et al. reviewed several extended
SOD tasks including RGB-D SOD, co-saliency detection
and video SOD. Han et al. looked into several subdirections of object detection, and outlined recent progress
in objectness detection, SOD, and category-speciﬁc object
detection. Borji et al. summarized both heuristic and
deep models for FP. Nguyen et al. focused on
categorizing the applications of visual saliency (including
both SOD and FP) in different areas. Finally, a more recently
published survey covers both traditional non-deep SOD
methods and deep ones until 2017, and discusses their
relation to several other closely-related research areas, such
as special-purpose object detection and segmentation.
Different from previous SOD surveys, which focus on
earlier non-deep learning SOD methods , other related
ﬁelds , – , practical applications or a limited
number of deep SOD models , this work systematically
and comprehensively reviews recent advances in the ﬁeld.
It features in-depth analyses and discussions on various
aspects, many of which, to the best of our knowledge,
have never been explored in this ﬁeld. In particular, we
comprehensively summarize and discuss existing deep SOD
methods under several proposed taxonomies (§2); review
datasets (§3) and evaluation metrics (§4) with their pros
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Taxonomies and representative publications of deep SOD methods. See §2 for more detailed descriptions.
Publications
Architectures
Multi-layer perceptron
1) Super-pixel/patch-based
 , , , 
(MLP)-based
2) Object proposal based
 , , 
Fully convolutional network
(FCN)-based
1) Single-stream
 , , , , , , 
2) Multi-stream
 , , , , 
3) Side-fusion
 , , , , , , , , 
4) Bottom-up/top-down
 , , , , , , , , , , , , , , 
5) Branched
 , , , , , , , 
Hybrid network-based
 , 
Capsule-based
 , 
Supervision
Fully-supervised
All others
Un-/Weakly-supervised
1) Category-level
 , , , 
2) Pseudo pixel-level
 , , , 
Single-task learning (STL)
All others
Mingle-task learning
1) Salient object subitizing
 , , 
2) Fixation prediction
 , 
3) Image classiﬁcation
 , 
4) Semantic segmentation
 , 
5) Contour/edge detection
 , , , , , , , , 
6) Image captioning
Object-/Instance-
Level (§2.4)
Object-level
All others
Instance-level
 , 
and cons; provide a deeper understanding of SOD models
through an attribute-based evaluation (§5.3); discuss the in-
ﬂuence of input perturbation (§5.4); analyze the robustness
of deep SOD models to adversarial attacks (§5.5); study the
generalization and difﬁculty of existing SOD datasets (§5.6);
and offer insight into essential open issues, challenges, and
future directions (§6). We expect our survey to provide novel
insight and inspiration that will facilitate the understanding
of deep SOD, and foster research on the open issues raised.
Our Contributions
Our contributions in this paper are summarized as follows:
1) A systematic review of deep SOD models from various perspectives. We categorize and summarize existing
deep SOD models according to network architecture,
level of supervision, learning paradigm, etc. The proposed taxonomies aim to help researchers gain a deeper
understanding of the key features of deep SOD models.
2) An attribute-based performance evaluation of SOD
models. We compile a hybrid dataset and provide annotated attributes for object categories, scene categories,
and challenging factors. By evaluating several representative SOD models on it, we uncover the strengths and
weaknesses of deep and non-deep approaches, opening
up promising directions for future efforts.
3) An analysis of the robustness of SOD models against
general input perturbations. To study the robustness of
SOD models, we investigate the effects of various perturbations on the ﬁnal performance of deep and non-deep
SOD models. Some results are somewhat unexpected.
4) The ﬁrst known adversarial attack analysis for SOD
models. We further examine the robustness of SOD
models against intentionally designed perturbations, i.e.,
adversarial attacks. The specially designed attacks and
evaluations can serve as baselines for further studying
the robustness and transferability of deep SOD models.
5) Cross-dataset generalization study. To analyze the generalization and difﬁculty of existing SOD datasets indepth, we conduct a cross-dataset generalization study
that quantitatively reveals the dataset bias.
6) Overview of open issues and future directions. We
thoroughly look over several essential issues (i.e., model
design, dataset collection, etc.), shedding light on potential directions for future research.
These contributions together comprise an exhaustive, up-todate, and in-depth survey, and differentiate it from previous
review papers signiﬁcantly.
The rest of the paper is organized as follows. §2 explains the proposed taxonomies, each accompanied with
one or two most representative models. §3 examines the
most notable SOD datasets, whereas §4 describes several
widely used SOD metrics. §5 benchmarks several deep SOD
models and provides in-depth analyses. §6 provides further
discussions and presents open issues and future research
directions of the ﬁeld. Finally, §7 concludes the paper.
DEEP LEARNING BASED SOD MODELS
Before reviewing recent deep SOD models in details, we
ﬁrst provide a common formulation of the image-based
SOD problem. Given an input image I ∈RW×H×3 of size
W × H, an SOD model f maps the input image I to a
continuous saliency map S =f(I)∈ W×H. For learningbased SOD, the model f is learned through a set of training
samples. Given a set of static images I ={In ∈RW×H×3}n
and corresponding binary SOD ground-truth masks G =
{Gn∈{0, 1}W×H}n, the goal of learning is to ﬁnd f ∈F that
minimizes the prediction error, i.e., P
nℓ(Sn, Gn), where ℓ
is a certain distance measure (e.g., deﬁned in §4), Sn=f(In),
and F is the set of potential mapping functions. Deep SOD
methods typically model f through modern deep learning
techniques, as will be reviewed later in this section. The
ground-truths G can be collected by different methodologies, i.e., direct human-annotation or eye-ﬁxation-guided
labeling, and may have different formats, i.e., pixel-wise or
bounding-box annotations, which will be discussed in §3.
In Table 2, we categorize recent deep SOD models according to four taxonomies, considering network architecture
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Summary of essential characteristics for popular SOD methods. Here, ‘#Training’ is the number of training images, and ‘CRF’ denotes whether the
predictions are post-processed by conditional random ﬁeld . See §2 for more detailed descriptions.
Architecture
Supervision
Obj.-/Inst.-
Training Dataset
SuperCNN 
MLP+super-pixel
Fully-Sup.
ECSSD 
CVPR MLP+super-pixel
Fully-Sup.
MSRA10K 
MLP+segment
Fully-Sup.
MSRA-B +PASCAL-S 
MLP+segment
Fully-Sup.
MSRA-B 
CVPR MLP+super-pixel
Fully-Sup.
MSRA10K 
DHSNet 
Fully-Sup.
MSRA10K +DUT-OMRON 
6,000+3,500
Fully-Sup.
MSRA-B 
RACDNN 
Fully-Sup.
DUT-OMRON +NJU2000 +RGBD 
Fully-Sup.
MSRA10K +SALICON 
10,000+15,000
MLP+obj. prop.
Fully-Sup.
MLP+obj. prop.
Fully-Sup.
MSRA-B 
CRPSD 
Fully-Sup.
MSRA10K 
Fully-Sup.
PASCAL VOC 2010 +MSRA10K 
10,103+10,000
MSRNet 
Fully-Sup.
MSRA-B +HKU-IS (+ILSO )
2,500+2,500 (+500)
Fully-Sup.
MSRA-B +HKU-IS 
Weakly-Sup.
ImageNet 
Fully-Sup.
MSRA10K 
Fully-Sup.
MSRA-B 
Fully-Sup.
Amulet 
Fully-Sup.
MSRA10K 
Fully-Sup.
MSRA10K 
MSRA10K 
Fully-Sup.
Fully-Sup.
MSRA10K 
Fully-Sup.
MSRA10K 
Weakly-Sup.
MS COCO +MSRA-B +HKU-IS 
82,783+2,500+2,500
LICNN 
Weakly-Sup.
ImageNet 
Fully-Sup.
MSRA-B 
Fully-Sup.
Fully-Sup.
RSDNet 
Fully-Sup.
PASCAL-S 
ASNet 
Fully-Sup.
SALICON +MSRA10K +DUT-OMRON 15,000+10,000+5,168
PiCANet 
VGGNet/ResNet50
Fully-Sup.
C2S-Net 
Weakly-Sup.
MSRA10K +Web
10,000+20,000
Fully-Sup.
MSRA-B 
SuperVAE 
Fully-Sup.
AFNet 
Fully-Sup.
BASNet 
Fully-Sup.
CapSal 
Fully-Sup.
COCO-CapSal /DUTS 
5,265/10,553
CPD-R 
Fully-Sup.
MLSLNet 
Fully-Sup.
Weakly-Sup.
ImageNet DET +MS COCO 
+ImageNet +DUTS 
456k+82,783
+300,000+10,553
PAGE-Net 
Fully-Sup.
MSRA10K 
Fully-Sup.
MSRA10K 
PoolNet 
Fully-Sup.
BANet 
Fully-Sup.
EGNet 
VGGNet/ResNet
Fully-Sup.
HRSOD 
Fully-Sup.
DUTS /HRSOD +DUTS 
10,553/12,163
JDFPR 
Fully-Sup.
MSRA-B 
SCRN 
Fully-Sup.
SSNet 
Desenet169
Fully-Sup.
PASCAL VOC 2012 +DUTS 
1,464+10,553
TSPOANet 
Fully-Sup.
(§2.1), level of supervision (§2.2), learning paradigm (§2.3), and
whether they works at an object or instance level (§2.4). In the
following, each category is elaborated on and exempliﬁed
by one or two most representative models. Table 3 summarizes essential characteristics of recent SOD models.
Representative Network Architectures for SOD
Based on the primary network architectures adopted, we
classify deep SOD models into four categories, namely MLPbased (§2.1.1), FCN-based (§2.1.2), hybrid network-based
(§2.1.3) and Capsule-based (§2.1.4).
Multi-Layer Perceptron (MLP)-Based Methods
MLP-based methods leverage image subunits (i.e., superpixels/patches , , and generic object proposals ,
 , , ) as processing units. They feed deep features extracted from the subunits into an MLP-classiﬁer for
saliency score prediction (Fig. 2(a)).
1) Super-pixel/patch-based methods use regular (patch) or
nearly-regular (super-pixel) image decomposition. As an
example of regular decomposition, MCDL uses two
pathways to extract local and global context from two superpixel-centered windows of different sizes. The global and
local feature vectors are fed into an MLP for classifying
background and saliency. In contrast, SuperCNN constructs two hand-crafted input feature sequences for each
irregular super-pixel, and use two separate CNN columns
to produce saliency scores from the feature sequences, respectively. Regular image decomposition can accelerate the
processing speed, thus most of the methods in this category
are based on regular decompostion.
2) Object proposal-based methods leverage object proposals , or bounding-boxes , as basic processing units in order to better encode object information. For
instance, MAP uses a CNN model to generate a set of
scored bounding-boxes, then selects an optimized compact
subset of bounding-boxes as the salient objects. Note that
this kind of methods typically produce coarse SOD results
due to the lack of object boundary information.
Though MLP-based SOD methods greatly outperform
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Super-pixel/Patch/
Object Proposal
[0.1, 0.9]
Fully-Connected Layer
Convolutional Layer
Capsule Layer
Input Image
[0.8, 0.2]
Fig. 2. Categorization of previous deep SOD models according to the adopted network architecture. (a) MLP-based methods. (b)-(f) FCN-based
methods, mainly using (b) single-stream network, (c) multi-stream network, (d) side-out fusion network, (e) bottom-up/top-down network, and (f)
branch network architectures. (g) Hybrid network-based methods. (h) Capsule-based methods. See §2.1 for more detailed descriptions.
their non-deep counterparts, they cannot fully leverage essential spatial information and are quite time-consuming, as
they need to process all visual subunits one-by-one.
Fully Convolutional Network (FCN)-Based Methods
To address the limitations of MLP-based methods, recent solutions adopt FCN architecture , leading to end-to-end
spatial saliency representation learning and fast saliency
prediction, within a single feed-forward process. FCN-based
methods are now dominant in the ﬁeld. Typical architectures
can be further classiﬁed as: single-stream, multi-stream, sidefusion, bottom-up/top-down, and branched networks.
1) Single-stream network is the most standard architecture,
having a stack of convolutional layers, interleaved with
pooling and non-linear activation operations (see Fig. 2(b)).
It takes a whole image as input, and directly outputs a
pixel-wise probabilistic map highlighting salient objects.
For example, UCF makes use of an encoder-decoder
network architecture for ﬁner-resolution saliency prediction.
It incorporates a reformulated dropout in the encoder to
learn uncertain features, and a hybrid upsampling scheme
in the decoder to avoid checkerboard artifacts.
2) Multi-stream network, as depicted in Fig. 2(c), typically
consists of multiple network streams to explicitly learn
multi-scale saliency features from multi-resolution inputs.
Multi-stream outputs are fused to form a ﬁnal prediction.
DCL , as one of the earliest attempts towards this
direction, contains two streams, which produce pixel- and
region-level SOD estimations, respectively.
3) Side-fusion network fuses multi-layer responses of a
backbone network together for SOD prediction, making use
of the complementary information of the inherent multiscale representations of the CNN hierarchy (Fig. 2(d)). Sideoutputs are typically supervised by the ground-truth, leading to a deep supervision strategy . As a well-known
side-fusion network based SOD model, DSS adds short
connections from deeper side-outputs to shallower ones.
In this way, higher-level features help lower side-outputs
to better locate salient regions, and lower-level features can
enrich deeper side-outputs with ﬁner details.
4) Bottom-up/top-down network reﬁnes rough saliency
maps in the feed-forward pass by gradually incorporating
spatial-detail-rich features from lower layers, and produces
the ﬁnest saliency maps at the top-most layer (Fig. 2(e)),
which resembles the U-Net for semantic segmentation.
This network architectures is ﬁrst adopted by PiCANet ,
which hierarchically embeds global and local pixel-wise attention modules to selectively attend to informative context.
5) Branched network typically addresses multi-task learning for more robust saliency pattern modeling. They have
a single-input-multiple-output structure, where bottom layers
are shared to process a common input and top ones are
specialized for different tasks (Fig. 2(f)). For example, C2S-
Net is constructed by adding a pre-trained contour
detection model to a main SOD branch. Then the two
branches are alternately trained for the two tasks, i.e., SOD
and contour detection.
Hybrid Network-Based Methods
Some other models combine both MLP- and FCN-based
subnets to produce edge-preserving results with multi-scale
context (Fig. 2(g)). Combining pixel-level and region-level
saliency cues is a promising strategy to yield improved performance, though it introduces extra computational costs.
CRPSD consolidates this idea. It combines pixel- and
region-level saliency. The former is generated by fusing
the last and penultimate side-output features of an FCN,
while the latter is obtained by applying an existing SOD
model to image regions. Only the FCN and fusion layers
are trainable.
Capsule-Based Methods
Recently, Hinton et al. proposed a new family of neural
networks, named Capsules. Capsules are made up of a group
of neurons which accept and output vectors as opposed to
scalar values of CNNs, allowing entity properties to be comprehensively modeled. Some researchers have thus been
inspired to explore Capsules in SOD , (Fig. 2(h)). For
instance, TSPOANet emphasizes part-object relations
using a two-stream capsule network. The input features of
capsules are extracted from a CNN, and transformed into
low-level capsules. These are then assigned to high-level
capsules, and ﬁnally recognized to be salient or background.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Level of Supervision
Based on the type of supervision, deep SOD models can be
classiﬁed into either fully-supervised or weakly-/unsupervised.
Fully-Supervised Methods
Most deep SOD models are trained with large-scale pixellevel human annotations, which are time-consuming and expensive to acquire. Moreover, models trained on ﬁne-labeled
datasets tend to overﬁt and generalize poorly to real-life
images . Thus, training SOD with weaker annotations
has become an increasingly popular research direction.
Weakly-/Unsupervised Methods
To get rid of laborious manual labeling, several weak supervision forms have been explored in SOD, including imagelevel category labels , , object contours , image
captions and pseudo ground-truth masks generated by
non-learning SOD methods , , .
1) Category-level supervision. It has been shown that deep
features trained with only image-level labels also provide
information on object locations , , making them
promising supervision signals for SOD training. WSS ,
as a typical example, ﬁrst pre-trains a two-branch network,
where one branch is used to predict image labels based on
ImageNet , and the other estimates SOD maps. The
estimated maps are reﬁned by CRF and used to further ﬁnetune the SOD branch.
2) Pseudo pixel-level supervision. Though informative,
image-level labels are weak. Some researchers therefore
instead use traditional non-learning SOD methods , ,
 , or contour information , to generate noisy yet ﬁnergrained cues for training. For instance, SBF fuses weak
saliency maps from a set of prior heuristic SOD models ,
 , at intra- and inter-image levels, to generate
supervision signals. C2S-Net trains the SOD branch
with the pixel-wise salient object masks generated from the
outputs of the contour branch using CEDN . The
contour and SOD branches alternatively update each other
and progressively output ﬁner SOD predictions.
Learning Paradigm
From the perspective of learning paradigms, SOD networks
can be divided into single-task learning (STL) and multi-task
learning (MTL) methods.
Single-Task Learning (STL) Based Methods
In machine learning, the standard practice is to learn one
task at a time , i.e., STL. Most deep SOD methods
belong to this realm of learning, i.e., they utilize supervision
from a single knowledge domain (SOD or anther related
ﬁeld such as image classiﬁcation ) for training.
Multi-Task Learning (MTL) Based Methods
Inspired by the human learning process, where knowledge
learned from related tasks can assist the learning of a
new task, MTL aims to improve the performance
of multiple related tasks by learning them simultaneously.
Beneﬁting from extra knowledge from related tasks, models
can gain improved generalizability. An extra advantage lies
in the sharing of samples among tasks, which alleviates
the lack of data for training heavily parameterized models.
These are the core motivations of MTL based SOD models,
and branched architectures (see §2.1.2) are usually adopted.
1) Salient object subitizing. The ability of humans to
rapidly enumerate a small number of items is known as
subitizing , . Inspired by this, some works learn
salient object subitizing and detection simultaneously ,
 , . RSDNet represents the latest advance in this
direction. It addresses detection, ranking and subitizing of
salient objects in a uniﬁed framework.
2) Fixation prediction aims to predict human eye-ﬁxation
locations in visual scenes. Due to its close relation with
SOD, learning shared knowledge from these two tasks can
improve the performance of both. For example, ASNet 
derives ﬁxation information as a high-level understanding
of the scene, from upper network layers. Then, ﬁne-grained
object-level saliency is progressively optimized under the
guidance of the ﬁxation in a top-down manner. 3) Image
classiﬁcation. Image-level tags are valuable for SOD, as they
provide the category information of dominant objects in the
images which are very likely to be the salient regions .
Inspired by this, some SOD models learn image classiﬁcation as an auxiliary task. For example, ASMO leverages
class activation maps from a neural classiﬁer and saliency
maps from previous non-learning SOD methods to train the
SOD network, in an iterative manner.
4) Semantic segmentation is for per-pixel semantic prediction. Though SOD is class-agnostic, high-level semantics
play a crucial role in saliency modeling. Thus, the task
of semantic segmentation can also be integrated into SOD
learning. A recent SOD model, SSNet , is developed
upon this idea. It uses a saliency aggregation module to predict a saliency score of each category. Then, a segmentation
network is used to produce segmentation masks of all the
categories. These masks are ﬁnally aggregated (according to
corresponding saliency scores) to produce a SOD map.
5) Contour/edge detection refers to the task of detecting
obvious object boundaries in images, which are informative
of salient objects. Thus, it is also explored in SOD modeling.
For example, PAGE-Net learns an edge detection module and embeds edge cues into the main SOD stream in a
top-down manner, leading to better edge-preserving results.
6) Image Captioning can provide extra knowledge about
the main content of visual scenes, enabling SOD models to
better capture high-level semantics. This has been explored
in CapSal , which incorporates semantic context from a
captioning network with local-global visual cues to achieve
improved performance for detecting salient objects.
Object-/Instance-Level SOD
According to whether or not they can identify different
salient object instances, current deep SOD models can be
categorized into object-level and instance-level methods.
Object-Level Methods
Most deep SOD models are object-level methods, i.e., designed to detect pixels that belong to salient objects without
being aware of individual object instances.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Statistics of popular SOD datasets, including the number of images, number of salient objects per image, area ratio of the salient objects in
images, annotation type, image resolution, and existence of ﬁxation data. See §3 for more detailed descriptions.
#Obj. Obj. Area(%)
SOD Annotation
Resolution
MSRA-A 
1,000/20,840
bounding-box object-level
MSRA-B 
20.82±10.29
bounding-box object-level, pixel-wise object-level max(w, h)=400,
min(w, h)=126
SED1 
26.70±14.26
pixel-wise object-level
max(w, h)=465,
min(w, h)=125
SED2 
21.42±18.41
pixel-wise object-level
max(w, h)=300,
min(w, h)=144
19.89±9.53
pixel-wise object-level
max(w, h)=400,
min(w, h)=142
Modern&Popular
2010 CVPR-W
27.99±19.36
pixel-wise object-level
max(w, h)=481,
min(w, h)=321
MSRA10K 
22.21±10.09
pixel-wise object-level
max(w, h)=400,
min(w, h)=144
ECSSD 
23.51±14.02
pixel-wise object-level
max(w, h)=400,
min(w, h)=139
DUT-OMRON 
14.85±12.15
pixel-wise object-level
max(w, h)=401,
min(w, h)=139
PASCAL-S 
24.23±16.70
pixel-wise object-level
max(w, h)=500,
min(w, h)=139
HKU-IS 
19.13±10.90
pixel-wise object-level
max(w, h)=500,
min(w, h)=100
23.17±15.52
pixel-wise object-level
max(w, h)=500,
min(w, h)=100
41.22±25.35
object number, bounding-box (train set)
max(w, h)=6132,
min(w, h)=80
39.51±24.85
object number, bounding-box instance-level
max(w, h)=3888,
min(w, h)=120
24.89±12.59
pixel-wise instance-level
max(w, h)=400,
min(w, h)=142
XPIE 
19.42±14.39
pixel-wise object-level, geographic information
max(w, h)=500,
min(w, h)=130
21.36±16.88
pixel-wise instance-level, object category, attribute max(w, h)=849,
min(w, h)=161
COCO-CapSal 
23.74±17.00
pixel-wise object-level, image caption
max(w, h)=640,
min(w, h)=480
HRSOD 
21.13±15.14
pixel-wise object-level
max(w, h)=10240, min(w, h)=600
Instance-Level Methods
Instance-level SOD methods further identify individual object instances in the detected salient regions, which is crucial for practical applications that need ﬁner distinctions,
such as semantic segmentation and multi-human parsing . As an early attempt, MSRNet performs salient
instance detection by decomposing it into three sub-tasks,
i.e., pixel-level saliency prediction, salient object contour
detection and salient instance identiﬁcation. It jointly performs the ﬁrst two sub-tasks by integrating deep features
for several different scaled versions of the input image.
The last sub-task is solved by multi-scale combinatorial
grouping to generate salient object proposals from the
detected contours and ﬁlter out noisy or overlapping ones.
SOD DATASETS
With the rapid development of SOD, numerous datasets
have been introduced. Table 4 summarizes 19 SOD datasets,
which are highly representative and widely used for training or benchmarking, or collected with speciﬁc properties.
Quick Overview
In an attempt to facilitate understanding of SOD datasets,
we present some main take-away points of this section.
• Compared with early datasets , , , recent ones
 , , , are typically more advanced with less
center bias, improved complexity, and increased scale. They
are thus better-suited for training and evaluation, and likely
to have longer life-spans.
• Some other recent datasets , , , , ,
 are enriched with more diverse annotations (e.g.,
subitizing, captioning), representing new trends in the ﬁeld.
More in-depth discussions regarding generalizability and
difﬁculty of several famous datasets will be presented in §5.6.
Early SOD Datasets
Early SOD datasets typically contain simple scenes where
1-2 salient objects stand out from a clear background.
• MSRA-A contains 20,840 images. Each image has
only one noticeable and eye-catching object, annotated by
a bounding-box. As a subset of MSRA-A, MSRA-B has 5,000
images and less ambiguity w.r.t. the salient object.
• SED 1 comprises a single-object subset and a twoobject subset; each has 100 images with mask annotations.
• ASD 2, also a subset of MSRA-A, has 1,000 images
with pixel-wise ground-truths.
Popular Modern SOD Datasets
Recent SOD datasets tend to include more challenging and
general scenes with relatively complex backgrounds and
multiple salient objects. All have pixel-wise annotations.
• SOD 3 consists of 300 images, constructed from .
Many images have more than one salient object that is
similar to the background or touches image boundaries.
• MSRA10K 4, also known as THUS10K, contains
10,000 images selected from MSRA-A and covers all the
images in ASD. Due to its large scale, MSRA10K is widely
used to train deep SOD models (see Table 3).
• ECSSD 5 is composed of 1,000 images with semantically meaningful but structurally complex natural contents.
• DUT-OMRON 6 has 5,168 images of complex backgrounds and diverse content, with pixel-wise annotations.
• PASCAL-S 7 comprises 850 challenging images selected from the PASCAL VOC2010 val set . With eye-
ﬁxation records, non-binary salient-object mask annotations
are provided. Note that the saliency value of a pixel is
calculated as the ratio of subjects that select the segment
containing this pixel as salient.
• HKU-IS 8 has 4, 447 complex scenes that typically
contain multiple disconnected objects with diverse spatial
distributions and similar fore-/background appearances.
1. Evaluation DB
2. material/RK CVPR09/
3. 
4. 
5. 
6. 
7. 
8. saliency.html
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Fig. 3. Annotation distributions of SOD datasets (see §3 for details).
• DUTS 9 is a large-scale dataset, where the 10, 553
training images were selected from the ImageNet train/val
set , and the 5, 019 test images are from the ImageNet
test set and SUN . Since 2017, SOD models are typically
trained on DUTS (Table 3).
Other Special SOD Datasets
In addition to the above “standard” SOD datasets, some
special ones have also recently been proposed, leading to
new research directions.
• SOS 10 is created for SOD subitizing . It contains
6,900 images (training set: 5,520, test set: 1,380). Each image
is labeled as containing 0, 1, 2, 3 or 4+ salient objects.
• MSO 11 is a subset of SOS-test , covering 1,224
images. It has a more balanced distribution of the number of
salient objects. Each object has a bounding-box annotation.
• ILSO 12 contains 1,000 images with precise instancelevel annotations and coarse contour labeling.
• XPIE 13 has 10,000 images with pixel-wise labels. It
has three subsets: Set-P has 625 images of places-of-interest
with geographic information; Set-I 8,799 images with object
tags; and Set-E 576 images with eye-ﬁxation records.
• SOC 14 consists of 6,000 images with 80 common
categories. Half of the images contain salient objects, while
the remaining have none. Each image containing salient
objects is annotated with an instance-level ground-truth
mask, object category, and challenging factors. The nonsalient object subset has 783 texture images and 2,217 realscene images.
• COCO-CapSal 15 is built from COCO and
SALICON . Salient objects were ﬁrst roughly localized
using the mouse-click data in SALICON, then precisely
annotated according to the instance masks in COCO. The
dataset has 5,265 and 1,459 images for training and testing,
respectively.
• HRSOD 16 is the ﬁrst high-resolution dataset for SOD.
It contains 1,610 training and 400 testing images collected
from websites. Pixel-wise ground-truths are provided.
9. 
10. 
11. 
12. 
13. 
14. 
15. 
16. 
Discussion
As shown in Table 4, early SOD datasets , , 
are comprised of simple images with 1-2 salient objects per
image, and only provide rough bounding-box annotations,
which are insufﬁcient for reliable evaluations , . Performance on these datasets has become saturated. Modern
datasets , , , , are typically large-scale
and offer precise pixel-wise ground-truths. The scenes are
more complex and general, and usually contain multiple
salient objects. Some special datasets contain challenging
scenes with background only , , provide more
ﬁne-grained, instance-level SOD ground-truths , 
or include other annotations such as image captions ,
inspiring new research directions and applications. Fig. 3
depicts the annotation distributions of 18 SOD datasets.
Here are some essential conclusions: 1) Some datasets ,
 , , have signiﬁcant center bias; 2) Datasets ,
 , have more balanced location distributions for
salient objects; and 3) MSO has less center bias, as
only bounding-box annotations are provided. We analyze
the generalizability and difﬁculty of several famous SOD
datasets in-depth in §5.6.
EVALUATION METRICS
This section reviews popular object-level SOD evaluation metrics, i.e., Precision-Recall (PR), F-measure ,
Mean Absolute Error (MAE) , weighted Fβ measure
(Fbw) , Structural measure (S-measure) , and
Enhanced-alignment measure (E-measure) .
Quick Overview
To better understand the characteristics of different metrics,
a quick overview of the main conclusions for this section are
provided as follows.
• PR, F-measure, MAE, and Fbw address pixel-wise errors,
while S-measure and E-measure consider structure cues.
• Among pixel-level metrics, PR, F-measure, and Fbw fail to
consider true negative pixels, while MAE can remedy this.
• Among structured metrics, S-measure is more favored than
E-measure, as SOD addresses continuous saliency estimates.
• Considering popularity, advantages and completeness, Fmeasure, S-measure and MAE are the most recommended
and are thus used for our performance benchmarking in §5.2.
Metric Details
• PR is calculated based on the binarized salient object mask
and ground-truth:
Precision =
where TP, TN, FP, FN denote true-positive, true-negative,
false-positive, and false-negative, respectively. A set of
thresholds ([0−255]) is applied to binarize the prediction.
Each threshold produces a pair of precision/recall values to
form a PR curve for describing model performance.
• F-measure comprehensively considers both precision
and recall by computing the weighted harmonic mean:
Fβ = (1 + β2)Precision × Recall
β2Precision + Recall
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Empirically, β2 is set to 0.3 to put more emphasis on
precision. Instead of plotting the whole F-measure curve,
some methods only report maximal Fβ, or binarize the predicted saliency map by an adaptive threshold, i.e., twice the
mean value of the saliency prediction, and report mean F.
• MAE measures the average pixel-wise absolute error
between normalized saliency prediction map S ∈ W×H
and binary ground-truth mask G∈{0, 1}W×H:
j=1|G(i, j) −S(i, j)|.
• Fbw intuitively generalizes F-measure by alternating
the way of calculating precision and recall. It extends the
four basic quantities TP, TN, FP and FN to real values, and
assigns different weights (ω) to different errors at different
locations, considering the neighborhood information:
β = (1 + β2)Precisionω × Recallω
β2Precisionω + Recallω
• S-measure evaluates the structural similarity between the real-valued saliency map and the binary groundtruth. It considers object-aware (So) and region-aware (Sr)
structure similarities:
S = α × So + (1 −α) × Sr,
where α is empirically set to 0.5.
• E-measure considers global means of the image and
local pixel matching simultaneously:
j=1 φS(i, j),
where φS is the enhanced alignment matrix, reﬂecting the
correlation between S and G after subtracting their global
means, respectively.
Discussion
These measures are typically based on pixel-wise errors
while ignoring structural similarities, with S-measure and
E-measure being the only exceptions. F-measure and Emeasure are designed for assessing binarized saliency prediction maps, while PR, MAE, Fbw, and S-measure are for
non-binary map evaluation.
Among pixel-level metrics, the PR curve is classic. However, precision and recall cannot fully assess the quality of
saliency predictions, since high-precision predictions may
only highlight a part of salient objects, while high-recall
predictions are typically meaningless if all the pixels are
predicted as being salient. In general, a high-recall response
may come at the expense of reduced precision, and vice
versa. F-measure and Fbw are thus used to consider precision and recall simultaneously. However, overlap-based
metrics (i.e., PR, F-measure, and Fbw) do not consider the
true negative saliency assignments, i.e., the pixels correctly
marked as non-salient. Thus, these metrics favor methods
that successfully assign high saliency to salient pixels but
fail to detect non-salient regions . MAE can remedy this,
but it performs poorly when salient objects are small. For the
structure-/image-level metrics, S-measure is more popular
than E-measure, as SOD focuses on continuous predictions.
Considering the popularity and characteristics of existing
metrics and completeness of evaluation, F-measure (maximal
Fβ), S-measure and MAE are our top recommendations.
BENCHMARKING AND EMPIRICAL ANALYSIS
This section provides empirical analyses to shed light on
some key challenges in the ﬁeld. Speciﬁcally, with our largescale benchmarking (§5.2), we ﬁrst conduct an attributebased study to better understand the beneﬁts and limitations of current arts (§5.3). Then, we study the robustness of
SOD models against input perturbations, i.e., random exerted
noises (§5.4) and manually designed adversarial samples
(§5.5). Finally, we quantitatively assess the generalizability
and difﬁculty of current mainstream SOD datasets (§5.6).
Quick Overview
For ease of understanding, we compile important observations and conclusions from subsequent experiments below.
• Overall benchmarks (§5.2). As shown in Table 5, deep
SOD models signiﬁcantly outperform heuristic ones, and
the performance on some datasets , has become
saturated. , , , are current state-of-the-arts.
• Attribute-based analysis (§5.3). Results in Table 7 reveal
that deep methods show signiﬁcant advantages in detecting
semantic-rich objects, such as animal. Both deep and nondeep methods face difﬁculties with small salient objects. For
application scenarios, indoor scenes pose great challenges,
highlighting potential directions for future efforts.
• Robustness against random perturbations (§5.4). As shown in
Table 9, surprisingly, deep methods are more sensitive than
heuristic ones to random input perturbations. Both types
of methods demonstrate more robustness against Rotation,
while being fragile towards Gaussian blur and Gaussian noise.
• Adversarial attack (§5.5). Table 10 suggests that adversarial
attacks cause drastic degradation in performance for deep
SOD models, and are even worse than that of random
perturbations. However, attacks rarely transfers between
different SOD networks.
• Generalizability and difﬁculty of datasets (§5.6). Table 11
shows that DUTS-train is a good choice for training
deep SOD models as it has the best generalizability, while
SOC , DUT-OMRON , and DUTS-test are more
suitable for evaluation due to their difﬁculty.
Performance Benchmarking
Table 5 shows the performances of 44 state-of-the-art deep
SOD models and three top-performing classic methods
(suggested by ) on six most popular modern datasets.
The performance is measured by three metrics, i.e., maximal
Fβ, S-measure and MAE, as recommended in §4.3. All the
benchmarked models are representative, and have publicly
available implementations or saliency prediction results. For
performance benchmarking, we either use saliency maps
provided by the authors or run their ofﬁcial codes. It is
worth mentioning that, for some methods, our benchmarking results are inconsistent with their reported scores. There
are several reasons. First, our community long lacked an
open, universally-adopted evaluation tool, while there are
many implementation factors would inﬂuence the evaluation scores, such as input image resolution, threshold
step, etc. Second, some methods , , , , ,
 use mean F-measure instead of maximal F-measure for
performance evaluation. Third, for some methods , ,
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Benchmarking results of 44 state-of-the-art deep SOD models and 3 top-performing classic SOD methods on 6 famous datasets (§5.2). Here
max F, S, and M indicate maximal Fβ, S-measure, and MAE, respectively. The three best scores are marked in red, blue, and green, respectively.
ECSSD 
DUT-OMRON 
PASCAL-S 
HKU-IS 
DUTS-test 
∗DRFI 
∗wCtr 
DHSNet 
CRPSD 
MSRNet 
Amulet 
RSDNet 
ASNet 
PiCANet 
†C2S-Net 
AFNet 
BASNet 
CapSal 
MLSLNet 
PAGE-Net 
PoolNet 
BANet-R 
EGNet-R 
HRSOD-DH 
JDFPR 
SCRN 
SSNet 
TSPOANet 
∗Non-deep learning model. † Weakly-supervised model. ⋄Bounding-box output. ‡ Training on subset. - Results not available.
the evaluation scores of ﬁnally released saliency maps are
inconsistent with the ones reported in papers. We hope
that our performance benchmarking, publicly released evaluation tools and SOD maps could help our community
build an open and standardized evaluation system and
ensure consistency and procedural correctness for results
and conclusions produced by different parties.
Not surprisingly, data-driven models greatly outperform
conventional heuristic ones, due to their strong learning
ability for visually salient pattern modeling. In addition,
the performance has gradually increased since 2015, demonstrating well the advancement of deep learning techniques.
However, after 2018, the rate of improvement began decrasing, calling for more effective model designs and new
machine learning technologies. We also ﬁnd that the performances tend to be saturated on older SOD datasets
such as ECSSD and HKU-IS . Hence, among the
44 famous deep SOD models, we would like to nominate
PoolNet , BANet , EGNet , and SCRN as
the four state-of-the-art methods, which consistently show
promising performance over diverse datasets.
Attribute-Based Study
Although the community has witnessed the great advances
made by deep SOD models, it is still unclear under which
speciﬁc aspects these models perform well. As there are
numerous factors affecting the performance of a SOD algorithm, such as object/scene category, occlusion, etc., it is crucial to evaluate the performance under different scenarios.
This can help reveal the strengths and weaknesses of deep
SOD models, identify pending challenges, and highlight
future research directions towards more robust algorithms.
Hybrid Benchmark Dataset with Attribute Annotations
To enable a deeper analysis and understanding of the performance of an algorithm, it is essential to identify the
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
HO, CS, CT, SO
OV, OC, CS
HO, OC, CT
Animal, NatObj
MO, HO, OV, OC, CS, CT
MO, HO, OC, BC, CT
HO, OC, CS, BC, LO
Human, Artifact
MO, HO, OC, CS, CT, SO
Artifact, NatObj
OC, CS, CT
Fig. 4. Sample images from the hybrid benchmark consisting of images randomly selected from 6 SOD datasets. Salient regions are uniformly
highlighted. Corresponding attributes are listed. See §5.3 for more detailed descriptions.
Attribute-based study w.r.t. salient object categories, challenges and scene categories. (·) indicates the percentage of images with a speciﬁc
attribute. ND-avg indicates the average score of three heuristic models: HS , DRFI and wCtr . D-avg indicates the average score of
three deep learning models: DGRL , PAGR and PiCANet . Best in red, and worst with underline. See §5.3 for more details.
Salient object categories
Challenges
Scene categories
∗DRFI 
∗wCtr 
PiCANet 
∗Non-deep learning model.
Descriptions of attributes that often bring difﬁculties to SOD (see §5.3).
Description
MO Multiple Objects. There exist more than two salient objects.
Heterogeneous Object. Salient object regions have distinct
colors or illuminations.
Out-of-View. Salient objects are partially clipped by image
boundaries.
Occlusion. Salient objects are occluded by other objects.
Complex Scene. Background regions contain confusing
objects or rich details.
Background Clutter. Foreground and background regions
around the salient object boundaries have similar colors (χ2
between RGB histograms less than 0.9).
Complex Topology. Salient objects have complex shapes, e.g.,
thin parts or holes.
Small Object. Ratio between salient object area and image is
less than 0.1.
Large Object. Ratio between salient object area and image is
larger than 0.5.
key factors and circumstances inﬂuencing it . To this
end, we construct a hybrid benchmark with rich attribute
annotations. It consists of 1,800 images randomly selected
from six SOD datasets (300 for each), namely SOD ,
ECSSD , DUT-OMRON , PASCAL-S , HKU-
IS and DUTS test set . Inspired by , , we
annotate each image with an extensive set of attributes covering typical object types, challenging factors and diverse
scene categories. Speciﬁcally, the annotated salient objects
are categorized into Human, Animal, Artifact and NatObj
(Natural Objects), where NatObj includes natural objects
such as fruit, plant, mountains, icebergs, lakes, etc. The
challenging factors describe speciﬁc situations that often
bring difﬁculties to SOD, such as occlusions, background
clutter, and complex shapes (see Table 6). The image scenes
include Indoor, Urban and Natural, where the last two indicate different outdoor environments. It is worth mentioning
that the attributes are not mutually exclusive. Some sample
images with attribute annotations are shown in Fig. 4. Please
note that this benchmark will also be used in §5.4 and §5.5.
For the baselines in our attribute-based analysis, we
choose the three top-performing heuristic models again, i.e.,
HS , DRFI and wCtr , and three recent famous
deep methods, i.e., DGRL , PAGR and PiCANet .
All three deep models are trained on DUTS-train and
have publicly released implementations.
In Table 7, we report the performance on subsets of our
hybrid dataset characterized by a particular attribute. To
provide better insight, in Table 8, we select images with
the best-100 and worst-100 model predictions, and compare
the portion distributions of attributes w.r.t. the ones over
the whole dataset. Below are some important observations
drawn from these experiments.
• ‘Easy’ and ‘hard’ object categories. Deep and non-deep
SOD models view object categories differently (Table 7).
For the deep methods (D-avg), NatObj is clearly the most
challenging one which is probably due to its small number
of training samples and complex topologies. Animal appears
to be the easiest, which can be attributed to its signiﬁcant semantics. By contrast, traditional methods (ND-avg) struggle
with Human, revealing their limitations in capturing highlevel semantics. We are surprised to ﬁnd that the deep models signiﬁcantly outperform the non-deep ones over almost
all the object categories, except NatObj. This demonstrates
the value of heuristic assumptions in certain scenes and
the potential of embedding human prior knowledge into
current deep learning schemes.
• Most and least challenging factors. Table 7 shows that,
interestingly, both deep and non-deep methods handle LO
well. In addition, both types of methods face difﬁculties
with SO, highlighting a promising direction for future
efforts. Besides, we ﬁnd that CS and MO are challenging for
deep models, showing that current solutions still fall short
at determining the relative importance of different objects.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Attribute statistics of top and bottom 100 images based on F-measure. (·) indicates the percentage of the images with a speciﬁc attribute. ND-avg
indicates the average results of three heuristic models: HS , DRFI and wCtr . D-avg indicates the average results of three deep
models: DGRL , PAGR and PiCANet . Two largest changes in red if positive, and blue if negative. See §5.3 for more details.
Salient object categories
Challenges
Scene categories
Input perturbation study on the hybrid benchmark. ND-avg indicates
the average score of three heuristic models: HS , DRFI and
wCtr . D-avg indicates the average score of three deep learning
models: SRM , DGRL and PiCANet . Best in red and worst
with underline. See §5.4 for more details.
Gaus. blur
Gaus. noise
+.015 +.009
∗DRFI 
∗wCtr 
+.006 -.000
PiCANet 
∗Non-deep learning model.
• Most and least difﬁcult scenes. Deep and heuristic
methods perform similarly when faced with different scenes
(Table 7). For both types of methods, Natural is the easiest,
which is reasonable as the scenes are typically simple. Further, though both contain numerous objects, Indoor is more
challenging than Urban as it often suffers from highly unevenly distributed illumination and more complex scenes.
Our experiments also show that the utility of SOD models
in real, and especially complex, environments is still limited.
• Additional advantages of deep models. As shown in
Table 7, deep models achieve great improvements on semantically rich objects (Human, Animal and Artifact), demonstrating advantages in semantic modeling. This is veriﬁed again
by their good performance on complex object shapes (HO,
OV, OC, CT ). Deep models also narrow the gap between
different scene categories (Indoor v.s. Natural), indicating an
improved robustness against various backgrounds.
• Best and worst predictions. From Table 8, in addition to
similar conclusions drawn from Table 7, some unique and
interesting observations can be made. First, for deep methods, NatObj spans a large range of challenge, containing
both the simplest and hardest samples. Thus, future efforts
should pay more attention to the hard samples in NatObj. In
addition, after considering data distribution bias, CS is the
most challenging factor for deep models.
Robustness Against General Input Perturbations
The robustness of a model lies in its stability against corrupt
inputs. Intuitively, the outputs of a robust SOD model
should be repeatable on slightly different images with the
same content. However, the recently introduced adversarial
examples, i.e. maliciously constructed inputs that fool machine learning models, can degrade the performance of deep
image classiﬁers signiﬁcantly. Current deep SOD models
likely face a similar challenge. Therefore, in this section, we
examine the robustness of SOD models by comparing their
outputs for randomly perturbed inputs, such as noisy or
blurred images. Then, in §5.5, we will study the robustness
to manually designed adversarial examples.
The input perturbations investigated include Gaussian
blur, Gaussian noise, Rotation, and Gray. For blurring, we
employ Gaussian blur kernels with a sigma of 2 or 4. For
noise, we select two variance values, i.e., 0.01 and 0.08, to
cover both tiny and medium magnitudes. For rotation, we
rotate the images by +15◦and −15◦, respectively, and cut
out the largest box with the original aspect ratio. The gray
images are generated using the Matlab rgb2gray function.
As in §5.3, we include three popular heuristic models , , and three deep methods , , 
in our experiments. Table 9 shows the results. Overall,
compared with deep models, heuristic methods are less
sensitive towards input perturbations. The compactness and
abstractness of superpixels likely explains much of this.
Speciﬁcally, heuristic methods are rarely affected by Rotation, but perform worse under strong Gaussian blur, strong
Gaussian noise and Gray. Deep methods suffer the most
under Gaussian blur and strong Gaussian noise, which may
be caused by the damage to shallow-layer features. Deep
methods are relatively robust against Rotation, revealing
the rotation invariance of DNNs brought by the pooling
operation. Interestingly, we further ﬁnd that, among the
three deep models, PiCANet demonstrates excellent
robustness against a wide range of input perturbations,
including Gaussian blur, Gaussian noise, and Rotation. We
attribute this to its effective non-local operation. This reveals
that effective network designs can improve the robustness
to random perturbations.
Robustness Against Manually Designed Input Perturbations
Given the signiﬁcant concerns with model robustness to
random perturbations, this section presents an analysis focusing speciﬁcally on manually designed adversarial perturbations. Recent years have witnessed great advance in
SOD driven by the progress of deep learning. However,
whether the deep SOD models are as powerful as they
seem is a question to worth pondering. Meanwhile, DNNs
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Gaussian noise
Gaussian blur
DGRL PiCANet
Gaussian blur
Fig. 5. Examples of saliency prediction under various input perturbations. The max F values are denoted in red. See §5.4 for more details.
have been previously found to be susceptible to adversarial
attacks, where visually imperceptible perturbations lead to
completely different predictions . Though intensively
studied in classiﬁcation tasks, adversarial attacks in SOD are
rarely explored. As SOD has been integrated as a critical
part in many security systems and commercial projects,
SOD models also have potential risks of being attacked.
Speciﬁcally, SOD plays a signiﬁcant role in many security
systems, for detecting the candidates of interest targets from
remote sensing images , video surveillance data ,
or sensor signals of autonomous vehicles . In such
situation, examining the robustness of SOD models is rather
important because the insecurity of SOD modules may
cause severe losses, e.g., the criminals may use inconspicuous adversarial perturbations to fool SOD modules and then
cheat the surveillance systems. Besides, SOD has beneﬁted
many commercial projects such as photo editing , and
image/video compression . The adversarial attacks
launched by hackers on the embedded SOD modules would
inevitably affect the functioning of commercial products and
impacting users, causing losses for the developers and companies. Therefore, studying the robustness of SOD models
is crucial for defending these applications against malicious
attacks. In this section, we study the robustness against
adversarial attacks and transferability of adversarial examples targeting different SOD models. Our observations are
expected to shed light on adversarial attacks and defenses
for SOD, providing a better understanding of vulnerabilities
of deep SOD models and improving the robustness of SOD
involved practical applications.
Robustness of SOD Against Adversarial Attacks
For measuring the robustness of deep SOD models, we
adopt and modify an adversarial attack algorithm designed
for semantic segmentation, i.e., Dense Adversary Generation
(DAG) . We choose three representative deep models,
i.e., SRM , DGRL and PiCANet for our study.
The experiment is conducted on the hybrid benchmark
introduced in §5.3. Following , we measure the perceptibility of the adversarial examples by computing the average perceptibility of the adversarial perturbations generated
SRM DGRL PiCANet
SRM DGRL PiCANet
Fig. 6. Examples of SOD prediction under adversarial perturbations
of different target networks. The perturbations are magniﬁed by 10 for
better visualization. Red for max F. See §5.5 for details.
from the hybrid benchmark. The values for the three models
are 3.54×10−3, 3.57×10−3, and 3.51×10−3, respectively.
Exemplar adversarial cases are shown in Fig. 6. As can
be seen, the adversarial attacks can prevent the SOD models
from producing reliable salient object candidates. Quantitative results are listed in Table 10. The underlined entries of
Table 10 reveal that the three deep SOD models investigated
are vulnerable to adversarial perturbations of the inputs.
However, as can be observed by comparing Tables 9 and 10,
the models are more robust to random input perturbations.
These differences in robustness might be interpretated by
the the distance from the inputs to the decision boundary in
high dimensional space. The intentionally designed adversarial inputs often lie closer to the decision boundary than
the random inputs , and can thus more easily cause
pixel-wise misclassiﬁcation.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Results for adversarial attack experiments. Max F↑on the hybrid
benchmark is presented when exerting adversarial perturbations from
different models. Worst results are underline. See §5.5 for details.
Attack from
PiCANet 
PiCANet 
Transferability Across Networks
Previous research has revealed that adversarial perturbations can be transferred across networks, i.e. adversarial
examples targeting one model can mislead another without any modiﬁcation . This transferability is widely
used for black-box attacks against real-world systems. To
investigate the transferability of perturbations for deep SOD
models, we use the adversarial perturbation computed on
one SOD model to attack another.
Table 10 shows the experimental results for the three
models under investigation (SRM , DGRL and Pi-
CANet ). While the DAG attack leads to severe performance drops for the targeted model (see the diagonal), it
causes much less degradation to other models, i.e., the transferability between models of different network structures is
weak for SOD task, which is similar to the transferability observed for semantic segmentation, as analyzed in . This
may be because the gradient directions of different models
are orthogonal to each other , so the gradient-based
attack in the experiment transfers poorly to non-targeted
models. However, adversarial images generated from an
ensemble of multiple models might generate non-targeted
adversarial instances with better transferability , which
would be a great threat to deep SOD models.
Cross-Dataset Generalization Evaluation
Datasets are responsible for much of the recent progress in
SOD, not just as sources for training deep models, but also as
means for measuring and comparing performance. Datasets
are collected with the goal of representing the visual world,
and to summarize the algorithm as a single number (i.e.,
benchmark score). A concern thus arises: it is necessary
to evaluate how well a particular dataset represents the
real world; or, more speciﬁcally, to quantitatively measuring
the dataset’s generalization ability. Unfortunately, previous
studies are quite limited – mainly concerning the degrees of center bias in different SOD datasets. Here, we follow to assess how general SOD datasets are. We study
the generalization and difﬁculty of several mainstream SOD
datasets by performing a cross-dataset analysis, i.e., training on one dataset, and testing on the others. We expect
our experiments to stimulate discussion in the community
regarding this essential but largely neglected issue.
We ﬁrst train a typical SOD model on one dataset, and
then explore how well it generalizes to a representative set
of other datasets, compared with its performance on the
“native” test set. Speciﬁcally, we implement the typical SOD
model as a bottom-up/top-down structure, which has been
the most standard and popular SOD architecture these years
and is the basis of many current top-performing models ,
Binary Cross-entropy Loss
Prediction
Ground-truth Map
56×56 28×28
Fig. 7. Network architecture of the SOD model used in cross-dataset
generalization evaluation. See §5.6 for more detailed descriptions.
Results for cross-dataset generalization experiment. Max F↑for
saliency prediction when training on one dataset (rows) and testing on
another (columns). “Self” refers to training and testing on the same
dataset (same as diagonal). “Mean Others” indicates average
performance on all except self. See §5.6 for details.
Test on: MSRA-
MSRA10K 
ECSSD 
DUT-OMRON 
HKU-IS 
Mean others
 , , . As shown in Fig. 7, the encoder part is
borrowed from VGG16 , and the decoder consists of
three convolutional layers that gradually reﬁne the saliency
prediction. We pick six representative datasets , ,
 , , , . For each dataset, we train the SOD
model with 800 randomly selected training images and test
it on 200 other validation images. Please note that a total of
1, 000 is the maximum possible number of images considering the size of the smallest selected dataset, ECSSD .
Table 11 summarizes the results of cross-dataset generalization, measured by max F. Each column corresponds to
the performance when training on all the datasets separately
and testing on one. Each row indicates training on one
dataset and testing on all of them. Since our training/testing
protocol is different from the one used in the benchmarks
mentioned in previous sections, the actual performance
numbers are not meaningful. Rather, it is the relative performance difference that matters. Not surprisingly, we observe
that the best results are achieved when training and testing
on the same dataset. By looking at the numbers across each
column, we can determine how easy a dataset is for models
trained on the other datasets. By looking at the numbers
across one row, we can determine how good a dataset is
at generalizing to the others. We ﬁnd that SOC is the
most difﬁcult dataset (lowest column, Mean others 0.614).
MSRA10K appears to be the easiest one (highest column, Mean others 0.811), and generalizes the worst (highest
row, Percent drop 17%). DUTS is shown to have the best
generalization ability (lowest row, Percent drop −16%).
Based on these analyses, we would make the following
recommendations for SOD datasets: 1) For training deep
models, DUTS is a good choice because it has the
best generalizability. 2) For testing, SOC is good for
assessing the worst-case performances, since it is the most
challenging dataset. DUT-OMRON and DUTS-test 
deserve more considerations as they are also very difﬁcult.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
MORE DISCUSSIONS
Our previous systematic review and empirical studies characterized the models (§2), datasets (§3), metrics (§4), and
challenges (§5) of deep SOD. Here we further posit active
research directions, and outline several open issues.
Model Design
Based on the review of deep SOD network architectures in
§2.1, as well as recent advances in related ﬁelds, we here
discuss several essential directions for SOD model design.
• Network topology. Network topology determines the
within-network information ﬂow, which directly affects
model capacity and training difﬁculty and thus inﬂuences
the best possible performance. To ﬁgure out an effective network topology for SOD, diverse architectures have
been explored (§2.1), such as multi-stream networks, sideout fusion networks, as well as bottom-up/top-down networks. However, all these network architectures are handdesigned. Thus, a promising direction would be to use
automated machine learning (AutoML) algorithms, such as
neural architecture search , to automatically search for
the best-performing SOD network topology.
• Loss function. Most deep SOD methods are trained with
the standard binary cross-entropy loss, which may fail to
fully capture the quality factors for the SOD task. Only
a few efforts have been made to derive losses from SOD
evaluation metrics . Thus, it is worth exploring more
effective SOD loss functions, such as the mean intersectionover-union loss and afﬁnity ﬁeld matching loss .
• Adaptive computation. Currently, all deep SOD models
are ﬁxed feed-forward structures. However, most parameters model high-level features that, in contrast to low-level
and many mid-level concepts, cannot be broadly shared
across categories/scenes. As such, we would like to ask the
following question: What if a SOD model could directly
execute certain layers that can best explain the saliency
patterns in a given scene? To answer this, one could leverage
adaptive computation techniques , to vary the
amount of computation on-the-ﬂy, i.e., by selectively activating part of the network in an input-dependent fashion. This
could bring a better trade-off between network depth and
computational cost. On the other hand, adapting inference
pathways for different inputs would provide ﬁner-grained
discriminative ability for various attributes. Therefore, exploring dynamic network structures in SOD is promising
for improving both efﬁciency and effectiveness.
Data Collection
Our previous discussions (§3) and analyses (§5.3 and §5.6)
on current SOD datasets revealed several factors that are
essential for future dataset collection.
• Annotation inconsistency. Though existing SOD datasets
play a critical role in training and evaluating modern SOD
models, annotation inconsistencies among different SOD
datasets have essentially been ignored by the community.
The inconsistencies are mainly caused by separate subjects
and rules/conditions during dataset annotation (see Fig. 8).
To ease annotation burdens, most current SOD datasets only
have a few human annotators directly identify the salient
Fig. 8. Examples for annotation inconsistency. Each row shows two
exemplar image pairs. See §6.2 for more detailed descriptions.
objects, instead of considering real human eye ﬁxation behavior. Maintaining annotation consistency among newly
collected datasets is an important consideration.
• Coarse v.s. ﬁne annotation. Modern SOD datasets all have
pixel-level annotations, which greatly boosts the performance of deep SOD models. However, pixel-wise groundtruths are very costly to collect considering the complex
object boundaries and the intense data requirement. Further,
the annotation qualities of different datasets are different
(see bicycles in Fig. 8). Finer labels are believed to be essential for high-quality saliency prediction, but usually take
more time to collect. Thus, given a limited budget, ﬁnding
the optimal annotation strategy is an open problem. Some
works have studied the relationship between label quality
and model performance in semantic segmentation ,
highlighting a possible research direction for SOD dataset
collection. In addition, current SOD models typically assume that the annotations are perfect. Thus, it would also
be of value to explore robust SOD models that can learn
saliency patterns from imperfectly annotated data.
• Domain-speciﬁc SOD datasets. SOD has shown potential
in a wide range of applications, such as autonomous vehicles, video games, medical image processing, etc. Due to the
different visual appearances and semantic components, the
saliency mechanisms in these applications are quite different
from that of conventional natural images. Thus, collecting
domain-speciﬁc datasets might beneﬁt the application of
SOD in certain scenarios, as observed in FP for crowds ,
webpages or driving , and better connect SOD
to the biological top-down visual attention mechanism and
human mental state.
Saliency Ranking and Relative Saliency
Current algorithms seems over-focused on directly regressing the saliency map to pursue a high benchmarking number, while neglecting the fact that the absolute magnitude
of values in a saliency map might be less important than
the relative saliency values among objects . Though the
relative value/rank order is rarely considered in the context
of benchmarking metrics (with the exception of ), it is
crucial for better modeling human visual attention behavior.
This is, in essence, a selection process that centers our
attention on certain important elements of the surroundings,
while blending other relatively unimportant things into
the background. This not only hints at one shortcoming
of existing benchmarking paradigms and data collection
strategies, but also reveals a common limitation of current
methods. Current state-of-the-arts fall short at determining
the relative importance of objects, such as identifying the
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
most important person in a crowded room. This is also
evidenced by the experiments in §5.3, which show that
deep models face great difﬁculties in complex (CS), indoor
(Indoor) or multi-object (MO) scenes. In other words, deep
SOD models, though good at semantic modeling, require
higher-level image understanding. Exploring more powerful network designs that explicitly reason the relative
saliency and revisiting classic cognitive theories are both
promising directions to overcome this issue.
Linking SOD to Visual Fixations
The strong correlation between eye movements (implicit
saliency) and explicit object saliency has been explored
throughout history , , – . However, despite
the deep connections between the problems of FP and SOD,
the major computational models of the two tasks remain
largely distinct; only a few SOD models consider both
tasks simultaneously , , . This is mainly due
to the overemphasis on the speciﬁc setting of SOD and
the design bias of current SOD datasets, which overlooks
the connection to eye ﬁxations during data annotation. As
stated in , such dataset design bias not only creates a
discomforting disconnection between FP and SOD, but also
further misleads the algorithm designing. Exploring classic
visual attention theories in SOD is a promising and crucial
direction which could make SOD models more consistent
with the visual processing of human visual system and
provide better explainability. In addition, the ultimate goal
of visual saliency modeling is to understand the underlying
rationale of the visual attention mechanism. However, with
the current focus on exploring more powerful neural network architectures and beating the latest benchmark numbers on different datasets, have we perhaps lost sight of the
original purpose? The solution to these problems requires
dense collaborations between the FP and SOD communities.
Learning SOD in a Weakly-/Unsupervised Manner
Deep SOD methods are typically trained in a fullysupervised manner with a plethora of ﬁnely-annotated
pixel-level ground-truths. However, it is highly costly and
time-consuming to construct a large-scale, well-annotated
SOD dataset. Though some efforts have been made to
achieve SOD with limited supervision, i.e., by leveraging
category-level labels , , or pseudo pixel-wise annotations , , , , , there is still a notable gap
with the fully-supervised counterparts. In contrast, humans
usually learn with little or even no supervision. Since the
ultimate goal of visual saliency modeling is to understand
the visual attention mechanism, learning SOD in an weakly-
/unsupervised manner would be of great value to both the
research community and real-world applications. Further, it
would also help us understand which factors truly drive our
attention mechanism and saliency pattern understanding.
Given the massive number of algorithmic breakthroughs
over the past few years, we can expect a ﬂurry of innovation
towards this promising direction.
Pre-training with Self-Supervised Visual Features
Current deep SOD methods are typically built on ImageNetpretrained networks, and ﬁne-tuned on SOD datasets. It is
believed that parameters trained on ImageNet can serve as a
good starting point to accelerate the convergence of training
and prevent overﬁtting on smaller-scale SOD datasets. Besides pre-training deep SOD models on the de facto dataset,
ImageNet, another option is to leverage self-supervised
learning techniques to learn effective visual features
from a vast amount of unlabeled images/videos. The visual
features can be learned through various pretext tasks like
image inpainting , colorization , clustering ,
etc., and can be generalized to other vision tasks. Fine-tuning
the SOD models on parameters trained from self-supervised
learning is promising to yield better performance compared
to the ImageNet initialization.
Efﬁcient SOD for Real-World Application
Current top-leading deep SOD models are designed to be
complicated in order to achieve increased learning capacity and improved performance. However, more ingenuous and light-weight architectures are required to fulﬁll
the requirements of mobile and embedded applications,
such as robotics, autonomous driving, augmented reality,
etc. The degradation of accuracy and generalization ability
caused by model scale deduction should be minimal. To
facilitate the application of SOD in real-world scenarios,
it is possible to utilize model compression or knowledge distillation , techniques to develop compact
and fast SOD models with competitive performance. Such
compression techniques have already been shown effective
in improving generalization ability and alleviating under-
ﬁtting for training efﬁcient object detection models .
CONCLUSION
In this paper we present, to the best of our knowledge, the
ﬁrst comprehensive review of SOD focusing on deep learning techniques. We ﬁrst provide novel testimonies for categorizing deep SOD models from several distinct perspectives, including network architecture, level of supervision,
etc. We then cover the contemporary literature on popular
SOD datasets and evaluation criteria, providing a thorough
performance benchmarking of major SOD methods and
offering recommendations for several datasets and metrics
that can be used to consistently assess different models.
Next, we consider several previously under-explored issues
related to benchmarking and baselines. In particular, we
study the strengths and weaknesses of deep and non-deep
SOD models by compiling and annotating a new dataset
and evaluating several representative models on it, revealing promising directions for future efforts. We also study
the robustness of SOD methods by analyzing the effects of
various perturbations on the ﬁnal performance. Moreover,
for the ﬁrst time in the ﬁeld, we investigate the robustness
of deep SOD models to maliciously designed adversarial
perturbations and the transferability of these adversarial examples, providing baselines for future research. In addition,
we analyze the generalization and difﬁculty of existing SOD
datasets through a cross-dataset generalization study, and
quantitatively reveal the dataset bias. We ﬁnally introduce
several open issues and challenges of SOD in the deep learning era, providing insightful discussions and identifying a
number of potentially fruitful directions forward.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
In conclusion, SOD has achieved notable progress thanks
to the striking development of deep learning techniques.
However, there are still under-explored problems on achieving more efﬁcient model designs, training, and inference
for both academic research and real-world applications. We
expect this survey to provide an effective way to understand
current state-of-the-arts and, more importantly, insight for
the future exploration of SOD.