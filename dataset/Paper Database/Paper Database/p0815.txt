Using Opcode Sequences in Single-Class Learning to
Detect Unknown Malware
Igor Santos∗, Felix Brezo, Borja Sanz, Carlos Laorden, Pablo G. Bringas
DeustoTech - University of Deusto, Laboratory for Smartness, Semantics and Security
(S3Lab), Avenida de las Universidades 24, 48007 Bilbao, Spain
Malware is any type of malicious code that has the potential to harm a
computer or network. The volume of malware is growing at a faster rate
every year and poses a serious global security threat. Although signaturebased detection is the most widespread method used in commercial antivirus
programs, it consistently fails to detect new malware. Supervised machinelearning models have been used to address this issue. However, the use of
supervised learning is limited because it needs a large amount of malicious
code and benign software to ﬁrst be labelled. In this paper, we propose a new
method that uses single-class learning to detect unknown malware families.
This method is based on examining the frequencies of the appearance of
opcode sequences to build a machine-learning classiﬁer using only one set
of labelled instances within a speciﬁc class of either malware or legitimate
software. We performed an empirical study that shows that this method can
reduce the eﬀort of labelling software while maintaining high accuracy.
∗Corresponding author
Email addresses: (Igor Santos), (Felix
Brezo), (Borja Sanz), (Carlos Laorden),
 (Pablo G. Bringas)
malware detection, computer security, data mining, machine
learning, supervised learning
1. Introduction
Malware is computer software designed to damage computers.
past, fame or glory were the main goals for malware writers, but nowadays,
the reasons have evolved mostly into economical matters .
However, there are several exceptions to this general trend like the recent
malware ‘Stuxnet’, which spies SCADA systems within industrial environments and reprograms them .
Commercial anti-malware solutions base their detection systems on signature databases . A signature is a sequence of bytes always
present within malicious executables together with the ﬁles already infected
by that malware. The main problem of such an approach is that specialists have to wait until the new malware has damaged several computers to
generate a ﬁle signature and thereby provide a suitable solution for that
speciﬁc malware. Suspect ﬁles subject to analysis are compared with the
list of signatures. When a match is found, the ﬁle being tested is ﬂagged
as malware. Although this approach has been demonstrated to be eﬀective
against threats that are known beforehand, signature methods cannot cope
with code obfuscation, previously unseen malware or large amounts of new
malware .
Two approaches exist that can deal with unknown malware that the classic signature method cannot handle, namely, anomaly detectors and machinelearning-based detectors. Regarding the way information is retrieved, there
are two malware analysis approaches: static analysis which is performed
without executing the ﬁle and dynamic analysis which implies running the
sample in an isolated and controlled environment monitoring its behaviour.
Anomaly detectors retrieve signiﬁcant information from non-malicious
software and use it to obtain benign behaviour proﬁles. Every signiﬁcant deviation from such proﬁles is ﬂagged as suspicious. Li et al. proposed
an static ﬁleprint (or n-gram) analysis in which a model or set of models attempt to construct several ﬁle types within a system based on their structural
(that is, byte) composition. This approach bases analysis on the assumption
that non-malicious code is composed of predictably regular byte structures.
In a similar vein, Cai et al. employed static byte sequence frequencies to detect malware by applying a Gaussian likelihood model ﬁtted with
Principal Component Analysis (PCA) . Dynamic anomaly detectors have been also proposed by the research community. For instance,
Milenkovi´c et al. employed a technique which guaranteed that only secure instructions were actually executed in the system. The system employed
signatures that were veriﬁed in execution time. Masri and Podgurski 
described Dynamic Information Flow Analysis (DIFA) which worked as a
speciﬁcation system. The system was designed only for Java applications.
Unfortunately, these methods usually show high false positive rates (i.e., benign software is incorrectly classiﬁed as malware), which presents diﬃculties
for their adoption by commercial antivirus vendors.
Machine-learning-based approaches build classiﬁcation tools that detect
malware in the wild (i.e., undocumented malware) by relying on datasets
composed of several characteristic features of both malicious samples and be-
nign software. Schultz et al. were the ﬁrst to introduce the concept of
applying data-mining models to the detection of malware based on respective
binary codes. Speciﬁcally, they applied several classiﬁers to three diﬀerent
feature extraction approaches, namely, program headers, string features and
byte sequence features. Subsequently, Kolter and Maloof improved
the results obtained by Schultz et al. by applying n-grams (i.e., overlapping
byte sequences) instead of non-overlapping sequences. The method employed
several algorithms to achieve optimal results using a Boosted1 Decision Tree.
Similarly, substantial research has focused on n-gram distributions of byte
sequences and data mining . Additionally, opcode sequences
have recently been introduced as an alternative to byte n-grams . This approach
appears to be theoretically better because it relies on source code rather than
the bytes of a binary ﬁle ).
There are also machine-learning approaches that employ a dynamic analysis to train the classiﬁers. Rieck et al. proposed the use of machinelearning for both variant and unknown malware detection. The system employed API calls to train the classiﬁers. In a similar vein, Devesa et al. 
employed a sandbox to monitor the behaviour of an executable and vectors
containing the binary occurrences of several speciﬁc behaviours .
gerous system calls) were extracted and used to train several classic machinelearning methods. Recently, machine-learning approaches have been used for
a complete system that includes early detection, alert and response Shabtai
et al. .
Machine-learning classiﬁers require a high number of labelled executables
for each of the classes (i.e., malware and benign). Furthermore, it is quite
diﬃcult to obtain this amount of labelled data in the real-world environment
in which malicious code analysis would take place. To generate these data, a
time-consuming process of analysis is mandatory, and even so, some malicious
executables can avoid detection. Within machine-learning analysis, several
approaches have been proposed to address this issue.
Semi-supervised learning is a type of machine-learning technique that is
especially useful when a limited amount of labelled data exist for each class.
These techniques train a supervised classiﬁer based on labelled data and
predict the label for unlabelled instances. The instances with classes that
have been predicted within a certain threshold of conﬁdence are added to
the labelled dataset. The process is repeated until certain conditions are
satisﬁed; one commonly used criterion is the maximum likelihood from the
expectation-maximisation technique . These approaches improve
the accuracy of fully unsupervised (i.e., no labels within the dataset) methods . However, semi-supervised approaches require a
minimal amount of labelled data for each class; therefore, they cannot be applied in domains in which only the instances belonging to a class are labelled
(e.g., malicious code).
Datasets of labelled instances for only a single class are known as partially
labelled datasets . The class that has labelled instances is
known as the positive class . Building classiﬁers using this
type of dataset is known as single-class learning or learning
from positive and unlabelled data.
With this background in mind, we propose the adoption of single-class
learning for the detection of unknown malware based on opcode sequences.
Because the amount of malware is growing faster every year, the task of
labelling malware is becoming harder, and approaches that do not require
all data to be labelled are thus needed. Therefore, we studied the potential
of a two-step single-class learner called Roc-SVM , which
has already been used for text categorisation problems ,
for unknown malware detection. The main contributions of our study are as
• We describe how to adopt Roc-SVM for unknown malware detection.
• We investigate whether it is better to label malicious or benign software.
• We study the optimal number of labelled instances and how it aﬀects
the ﬁnal accuracy of models.
• We show that labelling eﬀorts can be reduced in the anti-malware industry by maintaining a high rate of accuracy.
The remainder of this paper is organised as follows. Section 2 provides
background regarding the representation of executables based on opcodesequence frequencies. Section 3 describes the Roc-SVM method and how
it can be adopted for unknown malware detection. Section 4 describes the
experiments performed and presents the results.
Section 5 discusses the
obtained results and their implications for the anti-malware industry. Finally,
Section 6 concludes the paper and outlines avenues for future work.
2. Opcode-sequence Features for Malware Detection
To represent executables using opcodes, we extracted the opcode sequences and their frequency of appearance. More speciﬁcally, a program ρ
may be deﬁned as a sequence of instructions I, where ρ = (I1, I2, ..., In−1, In).
An instruction is a 2-tuple composed of an operational code and a parameter
or a list of parameters. Because opcodes are signiﬁcant by themselves , we discard the parameters and assume that the program is composed
of only opcodes. These opcodes are gathered into several blocks that we call
opcode sequences.
Speciﬁcally, we deﬁne a program ρ as a set of ordered opcodes o, ρ =
(o1, o2, o3, o4, ..., on−1, on), where n is the number of instructions I of a program ρ. An opcode sequence os is deﬁned as an ordered subgroup of opcodes
within the executable ﬁle, where os ⊆ρ. It is made up of ordered opcodes
o and os = (o1, o2, o3, ..., om1, om), where m is the length of the sequence of
opcodes os. We used the NewBasic Assembler2 as the tool for obtaining the
assembly ﬁles in order to extract the opcode sequences of the executables.
Consider an example based on the assembly code snippet shown in Figure
1. The following sequences of length 2 can be generated: s1 = (mov, add),
s2 = (add, push), s3 = (push, add), s4 = (add, and), s5 = (and, push), s6 =
2 fys/newbasic.htm
(push, push) and s7 = (push, and). Because most of the common operations
that can be used for malicious purposes require more than one machine code
operation, we propose the use of sequences of opcodes instead of individual
opcodes. As adding syntactical information with opcode sequences, we aim
at identifying better the blocks of instructions (that is, opcode sequences)
that pass on the malicious behaviour to an executable.
We used this approach to choose the lengths of the opcode sequences.
Nevertheless, it is hard to establish an optimal value for the lengths of the
sequences; a small value will fail to detect complex malicious blocks of operations whereas long sequences can easily be avoided with simple obfuscation
techniques.
We use ‘term frequency inverse document frequency’ (tf · idf ) to obtain the weight of each opcode sequence;
the weight of the ith n-gram in the jth executable, denoted by weight(i, j),
is deﬁned by:
weight(i, j) = tfi,j · idfi
Note that term frequency tﬁ,j is deﬁned as:
Note that ni,j is the number of times the sequence si,j (in our case an
opcode sequence) appears in an executable e, and P
k nk,j is the total number
of terms in the executable e (in our case the total number of possible opcode
sequences)
We compute this measure for every possible opcode sequence of ﬁxed
length n, thereby acquiring a vector ⃗v of the frequencies of opcode sequences
si = (o1, o2, o3, ..., on−1, on). We weight the frequency of occurrence of this
opcode sequence using inverse document frequency idfi is deﬁned as:
|E : ti ∈e|
|E| is the total number of executables and |E : ti ∈e| is the number of
documents containing the opcode sequence ti.
Finally, we obtain a vector ⃗v composed of opcode-sequence frequencies,
⃗v = ((os1, weight1), (os2, weight2), ..., (osm−1, weightm−1), (osm, weightm)),
where osi is the opcode sequence and weighti is the tf ·idf for that particular
opcode sequence.
3. The Roc-SVM Method for Learning from Partially-labelled Data
Roc-SVM is based on a combination of the Rocchio
method and SVM . The method utilises the
Rocchio method to select some signiﬁcant negative instances belonging to the
unlabelled class; SVM is then applied iteratively to generate several classiﬁers
and then to select one of them.
For the ﬁrst step and used in Li and Liu .
The model is then employed to predict the class of instances within U.
For the prediction, each test instance e ∈U is compared with each prototype
vector e ∈P using the cosine measure .
instances that are classiﬁed as negative are considered signiﬁcant negative
data and are denoted by N.
In the second step (shown in Figure 3), Roc-SVM trains and tests several
SVMs Li and Liu iteratively and then selects a ﬁnal classiﬁer. The
SVM algorithms divide the n-dimensional spatial representation of the data
into two regions using a hyperplane. This hyperplane always maximises the
margin between the two regions or classes. The margin is deﬁned by the
longest distance between the examples of the two classes and is computed
based on the distance between the closest instances of both classes, which are
called supporting vectors . The selection of the ﬁnal classiﬁer
is determined by the amount of positive examples in P which are classiﬁed
as negative. In Liu et al. they deﬁne that if more than the 8% of
the positive documents are classiﬁed as negatives, SVM has been wrongly
chosen, therefore S1 is used. In other cases, Slast is employed. As they stated
in Liu et al. , they used 8% because they wanted to be conservative
enough not to select a weak last SVM classiﬁer.
This generation is performed using the datasets P and N. Q is the set
of remaining unlabelled instances such that Q = U −N.
4. Empirical study
The research questions we aimed to answer with this empirical study were
as follows.
• What class (that is, malware or benign software) is of better use to label
when using an opcode-sequence-based representation of executables?
• What is the minimum number of labelled instances required to assure
suitable performance when using an opcode-sequence-based representation of executables?
To this end, we conformed a dataset comprising 1,000 malicious executables and 1,000 benign ones. For the malware, we gathered random samples
from the website VxHeavens3, which assembles a malware collection of more
than 17,000 malicious programs, including 585 malware families that represent diﬀerent types of current malware such as Trojan horses, viruses and
worms. Since our method would not be able to detect packed executable, we
removed any packed malware before selecting the 1,000 malicious executables. Although they had already been labelled with their family and variant
names, we analysed them using Eset Antivirus4 to conﬁrm this labelling.
This malware dataset contains executables coded with diverse purposes,
as shown in Table 1, where backdoors, email worms and hacktools represent
half of the whole malware population. The average ﬁlesize is 299 KB, ranging
from 4 KB to 5,832 KB, representing the ﬁles smaller than 100 KB the 43.8%
of the dataset, the ﬁles between 100 KB and 1,000 KB the 49.6% and the
ﬁles bigger than 1,000 KB the ﬁnal 6.6%.
These executables were compiled with very diﬀerent generic compilers
including Borland C++, Borland Delphi, Microsoft Visual C++, Microsoft
Visual Basic and FreeBasic as it is shown in Table 2. Note that 44 of them
were compiled with debugger versions and 70 were compiled with overlaying versions of the platforms shown in the table, while the other 886 were
3 
4 
generated with standard versions of these compilers.
For the benign dataset, we collected legitimate executables from our own
computers.
We also performed an analysis of the benign ﬁles using Eset
Antivirus to conﬁrm the correctness of their labels.
In a previous work
Moskovitch et al. , a larger dataset was employed to validate the
This benign dataset is composed of diﬀerent applications, such as installers or uninstallers, updating packages, tools of the Operative System,
printer drivers, registry editing tools, browsers, PDF viewers, maintenance
and performance tools, instant messaging applications, compilers, debuggers,
etc. The average ﬁle size is 222 KB, ranging from 4 KB to 5,832 KB, representing the ﬁles smaller than 100 KB the 69.6% of the dataset, the ﬁles
between 100 KB and 1,000 KB the 25.4% and the ﬁles bigger than 1,000 KB
the ﬁnal 5.0%.
Again, these executables were compiled with very diﬀerent generic compilers like Borland C++, Borland Delphi, Dev-C++, Microsoft Visual C++,
MingWin32 and Nullsoft; and two packers: ASProtect and UPX; as it is
shown in Table 3. Note that 69 of them were compiled with debugger versions and 28 were compiled with overlaying versions of the already mentioned
platforms, while 179 were generated with standard versions of the aforementioned compilers.
Using these datasets, we formed a total dataset of 2,000 executables. In
a previous work Moskovitch et al. , a larger dataset was employed
to validate the model. We did not use a larger training dataset because of
technical limitations. However, the randomly selected dataset was heteroge-
neously enough to raise sound conclusions. In a further work, we would like
to test how this technique scales with larger datasets.
Next, we extracted the opcode-sequence representations of opcode-sequence
length n = 2 for every ﬁle in each dataset. The number of features obtained
with an opcode length of two was very high at 144,598 features.
To address this, we applied a feature selection step using Information Gain , selecting the top 1,000 features. We selected 1,000 features because
it is a usual number to work with in text categorisation .
However, this value may change performance: a low number of features can
decrease representativeness while a high number of features slows down the
training step. The reason of not extracting further opcode-sequence lengths
is that the underlying complexity of the feature selection step and the huge
amount of features obtained would render the extraction very slow. Besides,
an opcode-sequence length of 2 has proven to be the best conﬁguration in a
previous work .
We performed two diﬀerent experiments.
In the ﬁrst experiment, we
selected the positive and labelled class stored in P as malware, whereas in
the second experiment, we selected the benign executables as the positive
class. For both experiments, we split the dataset into 10 subsets of training
and test datasets using cross-validation . In this way, we have
the same training and test sets for both experiments. Later, we changed the
number of labelled instances in the training datasets of each subset to 100,
200, 300, 400, 500, 600, 700, 800 and 900, taking into account which class is
going to be the labelled on in each experiment. The unlabelled ones within
the training set still belonged to the training set but their labels would be
unknown until the ﬁrst step of the algorithm ﬁnishes. In this way, we measure
the eﬀects of the number of labelled instances on the ﬁnal performance of
Roc-SVM’s ability to detect unknown malware. In summary, we performed
7 runs of Roc-SVM for each possible labelled class (malware or legitimate
software) for each of the 10 subsets in each experiment (malware or legimate
software labelled).
To evaluate the results of Roc-SVM, we measured the precision of the
malware (MP) instances in each run, which is the amount of malware correctly classiﬁed divided by the amount of malware correctly classiﬁed and
the number of legitimate executables misclassiﬁed as malware:
where TP is the number of true positives i.e., number of malware instances
correctly classiﬁed and FP is the number of false positives i.e., number of
legitimate executables misclassiﬁed as malware.
In addition, we measured the precision of the legitimate executables (LP),
which is the number of benign executables correctly classiﬁed divided by
the number of legitimate executables correctly classiﬁed and the number of
malicious executables misclassiﬁed as benign executables:
where TN is the number of legitimate executable correctly classiﬁed i.e., true
negatives and FN, or false negatives, is the number of malicious executables
incorrectly classiﬁed as benign software.
We also measured the recall of the malicious executables (MR), which
is the number of malicious executables correctly classiﬁed divided by the
amount of malware correctly classiﬁed and the number of malicious executables misclassiﬁed as benign executables:
where TP is the number of true positives i.e., number of malware instances
correctly classiﬁed and FN, or false negatives, is the number of malicious
executables incorrectly classiﬁed as benign software. This measure is also
known as false positive rate.
Next, we measured the recall of legitimate executables (LR) in each run,
which is the number of benign executables correctly classiﬁed divided by
the number of legitimate executables correctly classiﬁed and the number of
legitimate executables misclassiﬁed as malware:
where TN is the number of legitimate executable correctly classiﬁed i.e., true
negatives and FP is the number of false positives i.e., number of legitimate
executables misclassiﬁed as malware.
We also computed the F-measure, which is the harmonic mean of both
the precision and recall:
F-measure = 2 · Precision ∗Recall
Precision + Recall
where Precision is the mean value between both malware and legitimate
precision (MP and LP) and Recall is the mean value between both malware
and legitimate recall (MR and LR).
Finally, we measured the accuracy of Roc-SVM, which is the number of
the classiﬁer’s hits divided by the total number of classiﬁed instances:
Accuracy =
TP + TN + FP + FN
Figure 4 shows the results from selecting malware as the class for labelling. In this way, we can appreciate how the overall results improve when
more malware executables are added. With regards to malware recall, when
the size of the set of labelled instances increases, the rate of malware recall decreases. In other words, the more malicious executables are added to
the labelled set, the less capable Roc-SVM is of detecting malware. Malware precision increases with the size of the labelled dataset, meaning that
the conﬁdence of Roc-SVM’s detection of malware also increases. Legitimate precision decreases when the size of the labelled set increases, which
indicates that more malicious executables are classiﬁed as benign software.
However, legitimate recall increases, which shows that as the amount of labelled malware increases, so does the number of correctly classiﬁed instances
of software. Both the F-measure and accuracy increase along with the size
of the labelled dataset.
Figure 5 shows the results when we select benign software as the labelled
class. Overall, the results improve when more labelled executables are added.
However, it only increases until 600 benign executables are labelled. Then,
the classiﬁer worsens. This indicates that too much legitimate software is
redundant for the classiﬁer. These general trends are very similar to the
previous results. Malware recall decreases when the number of labelled in-
stances increases. Malware precision increases with the size of the labelled
dataset, and legitimate precision decreases when the size of the labelled set
increases.
To compare the results obtained by Roc-SVM, we have deﬁned two type
of baselines: simple euclidean distance with malware labelled and the same
distance measure with legitimate software labelled. For both baselines, we
have used a 10-fold cross validation and the maximum amount of labelled
software we have used to validate Roc-SVM: 900 instances. We have not
used lower training set sizes because the results obtained with 900 instances,
which will be the highest possible using this simple measure, are lower than
the ones obtained with our single-class approach (as shown in Table 4). In
order to provide a better distance measure we have weighted each feature
with its information gain value with respect to the class.
Thereafter, we have measured the euclidean distance between the test
dataset, composed of 100 malicious instances and 100 benign executable for
each fold, the 900 training instances for each fold. In order to select the global
deviation from the training set (that can be either malware or legitimate
software) three combination rules were used: (i) the mean value, (ii) the
lowest distance value and (iii) the highest value of the computed distances.
Next, we have selected the threshold as the value with highest f-measure,
selected from 10 possible values between the value that minimised the false
positives and the value that minimised the false negatives.
Table 4 shows the obtained results with Euclidean distance. The distance
approach, although we have used the maximum number of training examples,
obtained much worse results than the Roc-SVM approach proposed in this
Indeed, several conﬁgurations were as bad as a random classiﬁer,
showing that this simplistic approach is not feasible and that our single-class
approach is far much better for classifying malware using the information of
only one class of executables.
In summary, the obtained results show that it is better to label benign
software rather than malware when we can only label a small number of
benign executables. This results are in concordance with the work of Song
et al. regarding feasibility of blacklisting. However, if we can label a
large amount of malware, the classiﬁer would likely improve. The impact of
the number of labelled instances is positive, enhancing the results when the
size of the labelled dataset increases.
5. Discussion
We believe that our results will have a strong impact on the study of unknown malware detection, which usually relies on supervised machine learning. The use of supervised machine-learning algorithms for model training
can be problematic because supervised learning requires that every instance
in the dataset be properly labelled. This requirement means that a large
amount of time is spent labelling. We have dealt with this problem using
single-class learning that only needs a limited amount of a class (whether malware or benign) to be labelled. Our results outline the amount of labelled
malware that is needed to assure a certain performance level in unknown
malware detection. In particular, we found out that if we labelled 60% of the
benign software, which is the 30% of the total corpus, the Roc-SVM method
can achieve an accuracy and F-measure above 85%.
Although these results of accuracy are high, they may be not enough
for an actual working environment. A solution to this problem is to employ
user feedback and generate both blacklisting of the known malicious ﬁles and
whitelisting of the conﬁrmed benign applications.
It should also be interesting to evaluate how our method behaves chronologically in order to establish the importance of keeping updated the training
set as suggested in Moskovitch and Elovici , but we did not have an
accurate information about the actual date each executable was retrieved.
We would like to test this capability in a further work. In a similar vein, the
imbalance problem has been introduced in previous work Moskovitch et al.
 ; basically it is stated that the balance of each class depends on
the ﬁnal results of a classiﬁer. In our context, where we use a set of labelled
instances and a set of unlabelled ones to train, an investigation of the eﬀects
in the balance between labelled and unlabelled instances is interesting as
further work.
However, because of the static nature of the features we used with Roc-
SVM, it cannot counter packed malware. Packed malware results from ciphering the payload of the executable and deciphering it when it ﬁnally loads
into memory. Indeed, broadly used static detection methods can deal with
packed malware only by using the signatures of the packers. As such, dynamic
analysis seems like a more promising solution to this problem . One solution to solve this obvious limitation of our malware detection
method may involve the use of a generic dynamic unpacking schema, such as
PolyUnpack , Renovo , OmniUnpack
 and Eureka . These methods ex-
ecute the sample in a contained environment and extract the actual payload,
allowing for further static or dynamic analysis of the executable. Another
solution is to use concrete unpacking routines to recover the actual payload,
but this method requires one routine per packing algorithm .
Obviously, this approach is limited to a ﬁxed set of known packers. Likewise,
commercial antivirus software also applies X-ray techniques that can defeat
known compression schemes and weak encryption .
Nevertheless, these techniques cannot cope with the increasing use of packing techniques, and we thus suggest the use of dynamic unpacking schema
to address this problem.
6. Conclusions
Unknown malware detection has become a research topic of great concern
owing to the increasing growth in malicious code in recent years. In addition,
it is well known that the classic signature methods employed by antivirus
vendors are no longer completely eﬀective against the large volume of new
malware. Therefore, signature methods must be complemented with more
complex approaches that allow the detection of unknown malware families.
Although machine-learning methods are a suitable solution for combating
unknown malware, they require a high number of labelled executables for
each of the classes under consideration (i.e., malware and benign datasets).
Because it is diﬃcult to obtain this amount of labelled data in a real-word
environment, a time-consuming analysis process is often mandatory.
In this paper, we propose the use of a single-class learning method for
unknown malware detection based on opcode sequences. Single-class learning
does not require a large amount of labelled data, as it only needs several
instances that belong to a speciﬁc class to be labelled. Therefore, this method
can reduce the cost of unknown malware detection. Additionally, we found
that it is more important to obtain labelled malware samples than benign
software. By labelling 60% of the legitimate software, we can achieve results
above 85% accuracy.
Future work will be oriented towards four main directions. First, we will
use diﬀerent features as data for training these kind of models. Second, we
will focus on detecting packed executables using a hybrid dynamic-static approach. Third, we plan to perform a chronological evaluation of this method,
where the update need of the training set will be determined. Finally, we
would like to investigate in the eﬀect of the balance between labelled and
unlabelled instances in single-class learning.