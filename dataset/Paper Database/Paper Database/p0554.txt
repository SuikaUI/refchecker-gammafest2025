Deep generative models of genetic variation capture mutation effects
Adam J. Riesselman* John B. Ingraham*
Program in Biomedical Informatics Program in Systems Biology
Harvard Medical School Harvard University
 
Debora S. Marks
Department of Systems Biology
Harvard Medical School
 
* Equal contribution
The functions of proteins and RNAs are determined by a myriad of interactions between their
constituent residues, but most quantitative models of how molecular phenotype depends on
genotype must approximate this by simple additive effects. While recent models have relaxed
this constraint to also account for pairwise interactions, these approaches do not provide a
tractable path towards modeling higher-order dependencies. Here, we show how latent variable
models with nonlinear dependencies can be applied to capture beyond-pairwise constraints in
biomolecules. We present a new probabilistic model for sequence families, DeepSequence, that
can predict the effects of mutations across a variety of deep mutational scanning experiments
significantly better than site independent or pairwise models that are based on the same
evolutionary data. The model, learned in an unsupervised manner solely from sequence
information, is grounded with biologically motivated priors, reveals latent organization of
sequence families, and can be used to extrapolate to new parts of sequence space.
Introduction
Modern medicine and biotechnology are routinely challenged to both interpret and exploit how
mutations will affect biomolecules. From interpreting which genetic variants in humans underlie
disease, to developing modified proteins that have useful properties, to synthesizing large
molecular libraries that are enriched with functional sequences, there is need to be able to rapidly
assess whether a given mutation to a protein or RNA will disrupt its function . Motivated
by these diverse applications, new technologies have emerged that simultaneously assess the
effects of thousands of mutations in parallel (sometimes referred to as “deep mutational
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint . In these assays, the measured attributes range from ligand
binding, splicing and catalysis to cellular or organismal fitness under
selection pressure .
Figure 1. A nonlinear latent variable model captures higher-order dependencies in proteins
and RNAs. a. In contrast to sitewise and pairwise models that factorize dependency in sequence families with
low-order terms, a nonlinear latent variable model posits hidden variables z that can jointly influence many
positions at the same time. b. The dependency p(x|z) of the sequence x on the latent variables z is modeled by a
neural network, and inference and learning is made tractable by jointly training with an approximate inference
network q(z|x). This combination of model and inference is also known as a variational autoencoder
Since sequence space is exponentially large and experiments are resource-intensive, accurate
computational methods are an important component for high-throughput sequence annotation
and design. Many computational tools have been developed for predicting the effects of
mutations, and most progress in efficacy of predictions has been driven by the ability of models
to leverage the signal of evolutionary conservation among related sequences . While
previous approaches analyzed this signal in a residue-independent manner, recent work has
demonstrated that incorporating inter-site dependencies using a pairwise model can power state
of art predictions for high-throughput mutational experiments . Although this
incorporation of pairwise epistasis represented an important step forward, contemporary models
based on natural sequence variation are still unable to model higher-order effects. This is despite
the frequent observation that higher order epistasis pervades the evolution of proteins and RNAs
 . Naïvely, one way to address this would be to simply extend the pairwise models with
third or higher terms, but this is statistically unfeasible: fully-parameterized extensions of the
pairwise models to third-order interactions will already have approximately ~109 interaction
terms for a protein of length only 200 amino acids. Even if such a model could be engineered or
coarse-grained to be computationally and statistically tractable, it will only marginally
improve the fraction of higher-order terms considered, leaving 4th and higher order interactions
seemingly out of reach.
The intractability of higher order interaction models for modeling epistasis in proteins is a
consequence of how these models describe data: in general, every possible higher order
MHAEKLYSTCVR
Approximate posterior
Generative model
Sitewise factors
Pairwise factors
Latent factors
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint |𝜽)
((𝐱(Wild-Type)|𝜽) as a heuristic metric for the relative favorability of a mutated sequence,
𝐱(Mutant), versus a wild-type 𝐱(Wild-Type). This log-ratio heuristic has been previously shown to
accurately predict the effects of mutations across multiple kinds of generative models 𝑝𝐱𝜽
 . Our innovation is to instead consider another class of probabilistic models for 𝑝𝐱𝜽,
nonlinear latent variable models (Figure 1). It is important to emphasize that this new approach,
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint p(x|z) that specifies
a joint distribution over hidden variables and observed variables. Inference this model is
challenging, as the marginal probability of the observed data, p(x), requires integrating over all
possible hidden z with
𝑝(𝐱|𝜽) = ∫𝑝(𝐱|𝐳, 𝜽)𝑝(𝐳)𝑑𝐳.
While directly computing this probability is intractable in the general case, we can use
variational inference to instead form a lower bound on the (log) probability. This bound,
known as the Evidence Lower Bound (ELBO), takes the form
log 𝑝𝒙𝜽≥𝔼7 log 𝑝𝐱𝒛, 𝜽 −𝐷;< 𝑞𝐳𝒙, 𝝓||𝑝𝒛
where q(z|x) is an approximate posterior for hidden variables given the observed variables
p(z|x). Modeling both the conditional distribution p(x|z) of the generative model and the
approximate posterior q(z|x) with neural networks results in a flexible model-inference
combination, known as a Variational Autoencoder ( Figure 1b).
Neural network-parameterized latent variable models can in principle model complex
correlations in data, but without additional architectural and statistical considerations may be
hard to interpret and unlikely to generalize. We encourage generalization in three ways: First, we
encourage sparse interactions by placing a group sparsity prior over the last layer of the neural
network for p(x|z) that encourages each hidden unit in the network to only influence a few
positions at a time. This is motivated by the observation that higher order interactions in proteins,
while importantly higher than second order, are nevertheless low-valence compared to the
number of residues in the protein. Second, we encourage correlation between amino acid usage,
by convolving the final layer with a width-1 convolution operation. Thirdly, we estimate all
global parameters with variational Bayes by estimating approximate posterior distributions over
each model parameter. The result is that rather than learning a single neural network for p(z|x),
we learn an infinite ensemble of networks. This joint variational approximation is then optimized
by stochastic gradient ascent on the ELBO to give a fully trained model (Methods).
After optimizing the model on a given family, it can be readily applied to predict the effects of
arbitrary mutations to arbitrary sequences. Following the previous heuristic of quantifying effects
with a log ratio, 𝑙𝑜𝑔
((𝐱(Mutant)|𝜽)
((𝐱(Wild-Type)|𝜽) , we approximate this quantity by replacing each log
probability with a lower bound, the ELBO. For example, given a starting wild type sequence,
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint . Below: mutation effect scores for all
possible point mutations to β-lactamase.
Since all model parameters are computed for any combinations of mutations compared to wild
type, sequences can be assessed for fitness that are multiple steps away from the wild type and
A deep latent variable model captures the effects of mutations
Deep mutational scanning (DMS) experiments provide a systematic survey of the mutational
landscape of proteins and can be used to benchmark computational predictors for the effects of
mutations . Here we surveyed 28 in vivo and in vitro deep mutational scanning experiments
comprising of 21 different proteins and a tRNA to assess the ability of the deep latent variable
model to predict the effect of mutations purely from natural sequence variation . For each multiple sequence alignment of a family, we fit five replicas of the
model from 5 different initializations both to assess reproducibility as well as to create an
ensemble predictor. We calculate mutation effects as the difference in ELBOs (above and
Methods). Our deep latent variable model, DeepSequence, is predictive of the effect of
mutations with better performance than a site-independent model without dependencies between
Deleterious
p(xmutant)
p(xwildtype)
Effect prediction
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint (average 0.110 Spearman
ρ increase 0.11, Figure 3a).
Figure 3. A deep latent variable model predicts the effects of mutations better than siteindependent or pairwise models. a. A nonlinear latent variable model (DeepSequence) captures the effects
of mutations across deep mutational scanning experiments as measured by rank correlation (Supplemental Figure
1). b. The latent variable model tends to be more predictive of mutational effects than pairwise and site-independent
models when fit to deeper, more evolutionarily diverse sequence alignments as measured by the effective family size
(Methods). c. Average Spearman ρ before and after bias calibration of representative single-mutant datasets
(Methods, Supplementary Figure 3).
DeepSequence matches or is more predictive than the current state-of-the-art pairwise model in
22 out of 28 datasets (average Spearman ρ increase 0.03) and, as expected, the ensembled
prediction of the five model replicas is more predictive than the average performance of
individual predictors (28 out of 28 datasets) (Figure 3a, Supplementary Figure 1). A deep
alignment is necessary but not sufficient for reasonable agreement between experimental
measurements and model predictions. Where the effective family size is greater than 100
DeepSequence
| Spearman ρ |
Independent
| Spearman ρ |
EVmutation
| Spearman ρ |
family size
β-lactamase
DNA methyltransferase HaeIII
PABP singles (RRM domain)
β-glucosidase
GAL4 (DNA-binding domain)
Kanamycin kinase APH(3’)-II
YAP1 (WW domain)
PSD 95 (PDZ domain)
TIM barrel (S. solfataricus)
HSP90 (ATPase domain)
Influenza polymerase PA subunit
Influenza hemagglutinin
UBE4B (U-box domain)
Hepatitis C NS5A
PABP doubles (RRM domain)
TIM barrel (T. thermophilus)
Yeast tRNA (CCU, Arg)
Toxin-antitoxin complex
TIM barrel (T. maritima)
| Spearman ρ |
Aliphatic amide hydrolase
Translation initiation factor IF1
GTPase HRas
Latent (DeepSequence)
Pairwise (EVmutation)
Independent
Mean | Spearman ρ |
Independent
EVmutation
DeepSequence
Calibrated
prediction
28 Experiments
22 Sequence families
p(xmutant)
p(xwildtype)
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint =
92.6, 44.6, 21.8, 3.0), the independent and pairwise model outperform the deep generative model
(Figure 3b). We anticipate effective family size can guide model selection for best predictive
performance.
Figure 4. Latent variables capture organization of sequence space. In a two-dimensional latent
space for the β-lactamase family, closeness in latent space reflects phylogenetic groupings. When examining the
variation within a single deep mutational scanning experiment, it occupies only a very small portion of the sequence
space of the entire evolutionary family.
We compared the residuals of the rankings of the predictions versus the experiment for each
amino acid transition and observed a similar prediction bias for all three evolutionary models
(independent, EVmutation, DeepSequence, Supplementary Figure 2). When averaged across all
possible starting amino acids, positions mutated to prolines and charged amino acids are
consistently predicted as too deleterious, while sulphur-containing residues and aromatics are
consistently predicted as too fit, (Supplementary Figure 2). Although the unsupervised
DeepSequence model presented in this paper is improved by accounting for this bias in datasets
with only single mutants, the improvements are small (Figure 3c, Supplementary Figure 3)
suggesting that most of the disagreement between DeepSequence and the experimental
measurements is more complex.
We found that the combination of using biologically motivated priors and Bayesian approaches
for inference on the weights was important to learning models that generalize. To test the
importance of these various aspects of the model and inference, we performed an ablation study
across a subset of the proteins. We found that using (i) Bayesian variational approximations on
the weights, (ii) sparse priors on the last layer, (iii) a final width 1 convolution for amino acid
correlations, and (iv) a global temperature parameter all improved the ability of the model to
predict the effects of mutations across this subset of the experiments.
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint Sparse sub-groups targeted by the latent factors with a group sparsity prior are enriched as closer in
structure than expected for random sub-groups. (Right) When visualizing structures with only median values of
the structural enrichment, structural proximity is apparent. Representative sparse sub-groups are plotted on DNA
methyltransferase HaeIII (pdb: 1dct; log-ratio distance from left to right: -0.11,-0.11,-0.13) and β-lactamase
(pdb: 1axb; log-ratio distance from left to right: -0.11,-0.11,-0.10). b. Correlations in the weights of the final
width-1 convolution reflect known amino acid correlations as captured by a well-known substitution matrix
BLOSUM62 (Spearman ρ = 0.79).
Moreover, when comparing to other common approaches for regularization such as Dropout 
or point estimation with group sparsity priors , we found that our variational Bayesian
approach performed better, (Table 1).Most importantly, only the Bayesian approaches for
inference of the global parameters that estimate approximate posteriors were able to consistently
outperform the previous pairwise-interaction models.
DNA methyltransferase HaeIII
Aspartic acid
Glutamic acid
Asparagine
Isoleucine
Methionine
Phenylalanine
Tryptophan
Aspartic acid
Glutamic acid
Asparagine
Isoleucine
Methionine
Phenylalanine
Tryptophan
β-lactamase
Dictionary correlation coefficient
BLOSUM 62 Matrix
Spearman ρ = 0.79
Correlation coeffientions
between amino acids
β-lactamase
DNA methyltransferase HaeIII
β-glucosidase
GAL4 (DNA-binding domain)
Kanamycin kinase APH(3’)-II
YAP1 (WW domain)
PSD 95 (PDZ domain)
TIM barrel (S. solfataricus)
HSP90 (ATPase domain)
Influenza polymerase PA subunit
Influenza hemagglutinin
UBE4B (U-box domain)
Hepatitis C NS5A
PABP (RRM domain)
TIM barrel (T. thermophilus)
FYN (SH3 domain)
Aliphatic amide hydrolase
Translation initiation factor IF1
GTPase HRas
Antitoxin protein
Dihydrofolate reductase
TIM barrel (T. maritima)
BRCA1 (Ring domain)
Photoactive yellow protein
Mitogen-activated protein kinase 1
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint 
GAL4 (DNA-binding domain)
HSP90 (ATPase domain)
Kanamycin kinase APH(3’)-II
DNA methyltransferase HaeIII
PABP singles (RRM domain)
YAP1 (WW domain)
Table 1. Biologically motivated priors and Bayesian learning improve model performance.
Ablation studies of critical components of DeepSequence, showing the average Spearman ρ of predictions from five
randomly-initialized models. We include combinations of components of the structured matrix decomposition and
use either Bayesian approximation or Maximum a posteriori (MAP) estimation of decoder weights. These can be
compared to predictions made from EVmutation (Pair) and the site-independent model (site). Inclusion is indicated
with (✓), and top performing model configurations for each dataset are bolded.
The latent variables and global variables capture biological structure
Examining the low-dimensional latent spaces learned by a latent variable model can give insight
into relationships between data points (sequences), so we fit an identical replica of the model for
Beta-lacatamse that was constrained to have 2-dimensional z. We observe that sequence
closeness in latent space largely reflects phylogenetic groupings, though with some deviations
(Figure 4). Interestingly, when we examine the distribution of single-point mutation sequences in
latent, they are tightly clustered. It is important to note that these sequences need not be
separated at all; the conditional distribution p(x|z) can in principle model all of this variation
without additional need for variation in latent variables.
For the pairwise model of sequence families, it is well established that strongly coupled positions
in the model are also close in protein 3D structure . Assessing an analogous pattern in a
latent variable model is difficult, however, because explicit correlations between sites in p(x)
will be implicitly captured by the couplings between observed variables and latent variables.
Since these dependencies are mediated by the neural network for p(x|z) and the observed
variables x are only directly affected via connections from the last hidden layer, we can focus our
attention on those neural network weights. The group sparsity prior over this set of weights
(Methods) learns 500 soft sub-groups of positions, which can be seen as subsets of the entire
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint . We observe that the bulk of these average subgroup distances tends to be
less than the null expectation for distance (Figure 5a). When focusing on subgroups with
enrichment under the null near the median for that protein, we see that they have many nearby
subsets of residues on the crystal structures (Figure 5b). The final width-1 convolution in the
network is parameterized by a matrix that captures patterns of amino acid usage. To visualize
these patterns, we plot the correlations between amino acid types across the input channels of
this matrix and find that it groups amino acids of similar types. Moreover, it is well correlated
with the widely used BLOSUM62 substituion matrix (Figure 5c).
Discussion
We have shown that a deep latent variable model can model variation in biological sequence
families and be applied to predict the effects of mutations across diverse classes of proteins and
RNAs. We find that the predictions of the deep latent variable model are more accurate than a
previously published pairwise-interaction approach to model epistasis , which in turn was
more accurate than commonly used supervised methods . In addition, both the latent
variables and global variables of the model learn interpretable structure both for macrovariation
and phylogeny as well as structural proximity of residues.
However, while deep latent variable models introduce additional flexibility to model higherorder constraints in sequence families, this comes at the price of reduced interpretability and
increased potential for overfitting. We find that a Bayesian approach to inference, where
averages are computed over an ensemble of models and global parameters are controlled by
group sparsity priors, was a crucial step towards attaining generality. This suggests that future
work could benefit from additional biologically-motivated, hierarchical priors as well as more
accurate methods for variational inference . Additionally, incorporating more rigidly
structured probabilistic graphical models to model dependencies between latent variables could
improve generality and interpretability . Even our preliminary results with group sparsity
priors suggest that fear of a tradeoff between interpretability and flexibility for using deep
models on biological data may be largely remedied by hierarchical Bayesian approaches for
A second challenge for all approaches that predict the effects of mutations from evolutionary
sequence variation concerns the data themselves. DeepSequence, as with the majority of
previous mutation prediction methods, rely critically on the multiple sequences alignments used
for training data . At present, the criteria for the numbers of non-redundant
sequences and the level of diversity in multiple sequence alignments is ad hoc and this should be
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint 
Alignments.
We used the multiple sequence alignments that were published with EVmutation for the 19
families that overlapped and repeated the same alignment-generation protocol for the 4
additional proteins that were added in this study. Briefly, for each protein (target sequence),
multiple sequence alignments of the corresponding protein family were obtained by five search
iterations of the profile HMM homology search tool jackhmmer against the UniRef100
database of non-redundant protein sequences . We used a bit score of 0.5
bits/residue as a threshold for inclusion unless the alignment yielded < 80% coverage of the
length of the target domain, or if there were not enough sequences (redundancy-reduced number
of sequences ≥10L). For <10L sequences, we decreased the required average bit score until
satisfied and when the coverage was < 80% we increased the bit score until satisfied. Proteins
with < 2L sequences at < 70% coverage were excluded from the analysis. See previous work for
ParE-ParD toxin-antitoxin and tRNA alignment protocols.
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint human sampling, where the sequences of certain highly-studied organisms
may be overrepresented, and (b) evolutionary sampling, where some types of species may have
undergone large radiations that may not have anything to do with the particular molecule we are
studying. We aim to reduce these biases in a mechanistically-agnostic way by reweighting the
empirical data distribution to make it smoother. We use the previously established procedure of
computing each sequence weight 𝜋@ as the reciprocal of the number of sequences within a
given Hamming distance cutoff. If 𝐷A 𝑋@, 𝑋C is the normalized hamming distance between the
query sequence 𝑋@ and another sequence in the alignment 𝑋D and 𝜃 is a pre-defined
neighborhood size, the sequence weight is:
𝐼𝐷A 𝑋@, 𝑋C < 𝜃
The effective sample size of a multiple sequence alignment can then be computed as the sum of
these weights as
To fit a model to reweighted data, there are two common approaches. First, as was done
previously , one can reweight every log-likelihood in the objective by its sequence weight 𝜋@.
While this works well for batch optimization, we found it to lead to high-variance gradient
estimates with mini-batch optimization that make stochastic gradient descent unstable. We
instead used the approach of sampling data points with probability 𝑝@ proportional to their
weight in each minibatch as
Following prior work , we set 𝜃= 0.2 for all multiple sequence alignments sequences
(80%sequence identity) except those for viral proteins where we set 𝜃= 0.01 (99% sequence
identity) due to limited sequence diversity and the expectation that small differences in viral
sequences have higher probability of containing constraint information that the same diversity
might from a sample of mammals, for instance.
Background: latent factor models. Probabilistic latent variable models reveal structure in data
by positing an unobserved generative process that created the data and then doing inference to
learn the parameters of the generative process. We will focus on models with a generative
process in which an unobserved set of factors 𝒛 are drawn from an in-dependent distribution and
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint has been a foundational model for the analysis of genetic
variation since its introduction by Cavalli-Sforza. PCA can be realized in this probabilistic
framework as the zero-noise limit of Probabilistic PCA ]. With linear conditional
dependencies 𝑝𝐱𝒛, 𝜽, PCA can only model additive interactions between the latent factors 𝒛.
This limitation could in principle be remedied by using a conditional model 𝑝𝐱𝒛, 𝜽 with
nonlinear dependencies on 𝑧.
Nonlinear categorial factor model. We will consider a conditional model 𝑝𝐱𝒛, 𝜽 that differs
from PCA in two ways: First, the conditional distribution of the data 𝑝𝐱𝒛, 𝜽 will be categorical
rather than Gaussian to model discrete characters. Second, the conditional distribution
𝑝𝐱𝒛, 𝜽 will be parameterized by a neural network rather than a linear map. In this sense, our
latent variable model may be thought of as a discrete, nonlinear analog of PCA.
For this work, we considered a simple two hidden-layer neural network parameterizations of
𝑝𝐱𝒛, 𝜽. The generative process 𝑝𝐱𝒛, 𝜽 specifying the conditional probability of letter 𝑎 a at
position 𝑖 is
𝑧 ~ 𝒩0, 𝐼T
ℎ(I) = 𝑓I 𝑊(I)𝑧+ 𝑏(I)
ℎ(]) = 𝑓] 𝑊(])ℎ(I) + 𝑏(])
ℎ(^,_) = 𝑊(^,_)ℎ(]) + 𝑏(^,_)
𝑝𝑥_ = 𝑎|𝑧=
where 𝑓I = max (0, 𝑢) and 𝑓] =
Structured sparsity. Motivated by the observation that sequences have been well described by
models with low-order interactions (such as pairwise undirected models), we structure the final
layer of the decoder to be sparse such that each hidden unit may only affect a few positions in the
sequence. We parameterize each final weight matrix as
𝑊(^,_) = log 1 + 𝑒j
p) 𝑊(^,_)𝐷
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint is a sigmoid function representing a continuous relaxation of a spike and slab
prior over the group of dense factors using a logit normal prior. A single set of scale parameters
can control the sparsity of 𝑘 dense factors out of the total number factors 𝐻 by tiling. log 1 +
𝑒j is a softmax function representing the inverse-temperature of the sequence family and 𝐷 is
the dictionary.
The priors over the decoder weights are:
𝑠 ~ 𝒩𝜇@, 𝜎@
The factors 𝑆=
p) are a-priori logit-normally distributed, which can be though
of as a smooth relaxation of a Bernoulli that can be made sharper by increasing the variance 𝜎@
We set 𝜇@ such that the prior probability of approximate inclusion, 𝑃
> 0.5 , was
0.01. Given a fixed logit-variance of 𝜎@
] = 16 and an inclusion probability 𝑝_}~•€•K = 0.01, we
set the prior mean for the logit as 𝜇@ = −9.3053 using
2𝜎@]𝑒𝑟𝑓HI 2𝑝_}~•€•K −1
Variational approximation to 𝒑𝐳𝒙, 𝜽. Nonlinear latent factor models are difficult to infer.
Since the latent variables 𝑧 are not observed, computing the marginal likelihood of the data
requires integrating them out:
log 𝑝𝐱𝜽 = log
𝑝𝐱𝒛, 𝜽𝑝𝒛𝑑𝒛
We must do this integral because we do not know a priori which 𝒛 is responsible for each data
point 𝒙, and so we average over all possible explanations weighted by their relative probability.
In principle, the conditional probability 𝑝𝐳𝒙, 𝜽 is given by Bayes’ Theorem as the posterior,
𝑝𝐳𝒙, 𝜽= 𝑝𝐱, 𝐳𝜽
𝑝𝐱𝒛, 𝜽𝑝𝒛𝑑𝒛,
which is a challenging calculation that requires integrating over 𝐳.
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint = 𝑓I 𝑊(I)𝒙+ 𝑏(I)
𝑔(]) = 𝑓I 𝑊(])𝑔(I) + 𝑏(])
(^)𝑔(]) + 𝑏†
(^)𝑔(]) + 𝑏‡
𝑞𝐳𝒙, 𝝓= 𝒩𝜇, 𝑑𝑖𝑎𝑔(𝜎])
The latent variable 𝐳 can be reparameterized using an auxillary random variable 𝜖 ~ 𝒩0, 𝐼:
𝑧 = 𝜇+ 𝜎∗ 𝜖
Variational approximation to 𝒑𝜽𝑿. We apply a Bayesian approach to learning global
parameters by extending the variational approximations to include both the latent variables z as
well as the global parameters 𝜽. Because the posterior for the global parameters is conditioned
on the entire dataset, we must consider the marginal likelihood of the full dataset 𝑿=
𝒙(𝟏), … , 𝒙(𝑵) which integrates out all the corresponding latent factors 𝒁= 𝒛(𝟏), … , 𝒛(𝑵)
log 𝑝𝑿= log
𝒑𝑿𝒁, 𝜽𝑝𝜽𝑝𝒁𝑑𝒁𝑑𝜽
𝑝𝐗𝒁, 𝜽𝑝𝒁𝑝𝜽
𝑞𝐙, 𝜽𝑿, 𝝓𝑑𝒁𝑑𝜽
≥ℒ(𝜽, 𝝓) ≜
log 𝑝𝐗𝒁, 𝜽𝑝𝒁𝑝𝜽
𝑞𝐙, 𝜽𝑿, 𝝓𝑑𝒁𝑑𝜽
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint 𝒙(𝒊), 𝝓
The approximate posterior for factorizes over the model parameters:
To incorporate both of these factors into the likelihood, the ELBO is then:
log 𝑝𝑿𝜽≥𝑁𝔼”∈– 𝔼7(𝜽)7(𝒛|𝒙) log 𝑝𝐱𝒛, 𝜽 −𝐷;< 𝑞𝐳𝒙, 𝝓||𝑝𝒛
𝐷;< 𝑞𝜽(_) ||𝑝𝜽(_)
We model all variational distributions over the parameters with fully-factorized mean-field
Gaussian distributions. In accordance with our data reweighting scheme, we set 𝑁= 𝑁KLL, the
effective number of sequences that is the sum of the sequence weights.
Model hyperparameters.
We used a fixed architecture across all sequence families. The encoder has architecture 1500-
1500-30 with fully connected layers and ReLU nonlinearities. The decoder has two hidden
layers: the first with size 100 and a ReLU nonlinearity, and the second with size 2000 with a
sigmoid nonlinearity. The dictionary D is a 40 by q matrix where the alphabet size q was 20 for
proteins and 4 for nucleic acids. A single set of sparsity scale parameters controlled 4 sets of
dense weights. Dropout was set to 0.5 when used in ablation studies. Models were
optimized with Adam with default parameters using a batch size of 100 until convergence,
completing 300000 updates.
Each model was fit five times to the same multiple sequence alignment using a different random
seed. For mutation effect prediction, 2000 ELBO samples were taken of a mutated and wildtype
sequence and averaged to estimate the log probability of that mutation.
Group sparsity analysis. The sparse scale parameters were introduced into the structured
weight matrix decomposition to enable the model to capture low-valence interactions between
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint ,we denoted these dead scale
parameter vectors 𝑠— as those which do not have any scale parameter above 0.001, and were
removed from downstream analysis.
We then determined a co-occurrence distance distribution of these scale parameters by first
taking the upper triangle of the outer product of the scale parameters and normalizing it such that
it sums to 1:
𝑖< 𝐿, 𝑗< 𝑖
A normalized distance per vector of scale parameters 𝐷—
(aCCK•} can then be reported in
Angstroms:
This value was compared to 𝐷}€••
(aCCK•}, in which the null distribution of scale parameters 𝑆}€•• are
isotropically distributed, generating a 𝐷}€••
(aCCK•} which is the average pairwise distance between
residues. Moreover, bootstrapped samplings of 𝑆— converge to the same null value. The
distribution of all distances 𝐷(aCCK•} can be compared to the using a one-sided Student’s t-test
with a known mean.
Residual analysis. Spearman ρ is calculated by transforming paired data to ranked quantiles and
then computing the Pearson correlation between the ranks. To determine where the model over
or under predicted the ∆E for each mutation, we transformed the experimental measurements and
mutation effect predictions to normalized ranks on the interval . Thus, we define the
residual effects as the residuals of a least-squares linear fit between the normalized ranks.
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint or after mutation (mutant).
Bias correction. To correct for biases between mutation effect predictions and experimental
measurements, we created a feature matrix for each mutation that included ∆E, amino acid
identity before and after mutation, alignment column statistics (conservation and amino acid
frequency), and residue hydrophobicity . Leave-one-out cross validation (LOOCV) was used
to correct the bias for each dataset. Using the most recent DMS experiment as the representative
of that protein family (15 DMS datasets), the mutants of 14 datasets were used to fit a regression
model to predict the residuals of each known mutation, 𝜀∆Ÿ , given the feature matrix. After this
model was fit, it was used to predict 𝜀∆Ÿ for the mutants in the test dataset. This predicted
residual bias 𝜀∆Ÿwas subtracted off the normalized predicted rank 𝑑∆Ÿ = 𝑑∆Ÿ −𝜀∆Ÿ.These
corrected predictions were then reranked and compared to the experimental results to calculate a
corrected Spearman ρ. To predict the effects of mutations solely from DMS data, the same
LOOCV procedure was used excluding all evolutionary information in the feature matrix for
each mutation. In this case, the feature matrix was used to directly compute predict a rank
𝑑ª«¬. These values were subsequently reranked and compared to the ranked experimental results
to calculate a corrected Spearman ρ.
CC-BY-NC-ND 4.0 International license
not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available
The copyright holder for this preprint (which was
this version posted December 18, 2017.
 
bioRxiv preprint