Information-Theoretic Metric Learning
Jason Davis, Brian Kulis, Suvrit Sra and Inderjit Dhillon
Dept. of Computer Science
University of Texas at Austin
Austin, TX 78712
We formulate the metric learning problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the
Mahalanobis distance function. Via a surprising equivalence, we show that this
problem can be solved as a low-rank kernel learning problem. Speciﬁcally, we
minimize the Burg divergence of a low-rank kernel to an input kernel, subject to
pairwise distance constraints. Our approach has several advantages over existing methods. First, we present a natural information-theoretic formulation for the
problem. Second, the algorithm utilizes the methods developed by Kulis et al.
 , which do not involve any eigenvector computation; in particular, the running
time of our method is faster than most existing techniques. Third, the formulation
offers insights into connections between metric learning and kernel learning.
Introduction
We propose a new formulation for learning a Mahalanobis distance under constraints. We model the
problem in an information-theoretic setting by leveraging an equivalence between the multivariate
Gaussian distribution and the Mahalanobis distance. We show that the problem of learning an optimal Mahalanobis distance translates to learning the optimal Gaussian with respect to an entropic
objective. Thus, our problem can be thought of as maximizing the entropy of a multivariate Gaussian
subject to pairwise constraints on the associated Mahalanobis distance.
To solve our problem, we show an interesting connection to a recently proposed low-rank kernel
learning problem . Here, a low-rank kernel K is learned that satisﬁes a set of given distance
constraints as well as minimizes the Burg matrix divergence to the given kernel K0. It was shown
that this problem can be optimized using an iterative optimization procedure with cost O(cd2) per
iteration, where c is the number of distance constraints, and d is the dimensionality of the data. In
particular, this method does not require costly eigenvalue computations, unlike many other metric
learning algorithms .
Problem Formulation
Given a set of n points {x1, ..., xn} in ℜd, we seek a positive deﬁnite matrix A which parameterizes
the Mahalanobis distance:
dA(xi, xj) = (xi −xj)T A(xi −xj).
We assume that some prior knowledge about the distances between these points is known. Speciﬁcally, we consider relationships constraining the similarity or dissimilarity between pairs of points.
Two points are similar if the Mahalanobis distance between them is smaller than a given upper
bound, dA(xi, xj) ≤u for a relatively small value of u. Similarly, two points are dissimilar if
dA(xi, xj) ≥l for sufﬁciently large l.
In particular, for a classiﬁcation setting where class labels are known for each instance (as in Globerson and Roweis ), distances between points in the same class can be constrained to be small, and
distances between two points in different classes can be constrained to be large.
Our problem is to learn a matrix A which parameterizes a Mahalanobis distance that satisiﬁes a
given set of constraints. Typically, this learned distance function is used for k-nearest neighbor
search, k-means clustering, etc. We note that, in the absence of prior knowledge, these algorithms
typically use the standard squared Euclidean distance, or equivalently, the Mahalanobis distance
parameterized by the identity matrix I.
In general, the set of distance functions in our feasible set will be inﬁnite (we discuss later how to
re-formulate the problem for the case when the feasible set is empty). Therefore, we regularize the
problem by choosing the Mahalanobis matrix A that is as close as possible to the identity matrix I
(which parameterizes the baseline Euclidean distance function). To quantify this more formally, we
propose the following information-theoretic framework.
There exists a simple bijection between the set of Mahalanobis distances and the set of multivariate Gaussians with ﬁxed mean m. Given a Mahalanobis distance parameterized by A, we express
its corresponding multivariate Gaussian as p(x; m, A) =
Z exp (−dA(x, m)), where Z is a normalizing constant. Using this bijection, we deﬁne the distance between two Mahalanobis distance
functions parametrized by A1 and A2 as the (differential) relative entropy between their corresponding multivariate Gaussians:
KL(p(x; m, A1)∥p(x; m, A2)) =
p(x; m, A1) log p(x; m, A1)
p(x; m, A2) dx.
Given a set of pairs of similar points S and pairs of dissimilar points D, our distance metric learning
problem is
KL(p(x; m, A)∥p(x; m, I))
subject to
dA(xi, xj) ≤u
(i, j) ∈S,
dA(xi, xj) ≥l
(i, j) ∈D.
Note that m is an arbitrary ﬁxed vector.
In this section, we demonstrate how to solve the information-theoretic metric learning problem (2)
by proving its equivalence to a low-rank kernel learning problem. Using this equivalence, we appeal
to the algorithm developed in to solve our problem.
Equivalence to Low-Rank Kernel Learning
Let X = [x1 x2 ... xn], and the Gram matrix over the input points be K0 = XT X. Consider the
following kernel learning problem, to be solved for K:
DBurg(K, K0)
subject to
Kii + Kjj −2Kij ≤u
(i, j) ∈S,
Kii + Kjj −2Kij ≥l
(i, j) ∈D,
The Burg matrix divergence is a Bregman matrix divergence generated by the convex function
φ(X) = −log det X over the cone of semi-deﬁnite matrices, and it is deﬁned as
DBurg(K, K0) = Tr(KK−1
0 ) −log det(KK−1
Formulation (3) attempts to ﬁnd the nearest kernel matrix in Burg-divergence to the input Gram
matrix, subject to linear inequality constraints. It can be shown that the Burg divergence between
two matrices is ﬁnite if and only if their range spaces are the same . This fact allows us to
conclude that the range spaces of K and K0 are the same if the problem has a feasible solution.
Furthermore, the learned matrix K can be written as a rank-d kernel K = XT W T WX, for some
(d × d) full-rank matrix W.
We now state a surprising equivalence between problems (2) and (3). By solving (3) for K =
XT W T WX, the optimal A for (2) can be easily constructed via A = W T W. We will not provide
a detailed proof of this result; however, we present the two key lemmas.
Lemma 1: DBurg(K, K0) = 2KL(p(x; m, A)∥p(x; m, I)) + c, where c is a constant.
Lemma 1 establishes that the objectives for information-theoretic metric learning and low-rank kernel learning are essentially the same. It was recently shown that the differential relative entropy
between two multivariate Gaussians can be expressed as the convex combination of a Mahalanobis
distance between mean vectors and the Burg matrix divergence between the covariance matrices.
Here, the two mean vectors are the same, so their Mahalanobis distance is zero. Thus, the relative
entropy, KL(p(x; m, A)∥p(x; m, I)), is proportional to the Burg matrix divergence from A to I.
Therefore, the proof of the Lemma 1 reduces to showing that DBurg(K, K0) and DBurg(A, I) differ
by only a constant.
Interestingly, the dimensions of the matrices in these two divergences are
different: K and K0 are (n × n), while A and I are (d × d).
Lemma 2: Given K = XT AX, A is feasible for (2) if and only if K is feasible for (3).
This lemma conﬁrms that if we have a feasible kernel matrix K satisfying the constraints of (3), the
corresponding Mahalanobis distance parameterized by A satisﬁes the constraints of (2). Note that
by associating the kernel matrix with the Mahalanobis distance, we can generalize to unseen data
points, thus circumventing a problem often associated with kernel learning.
Metric Learning Algorithm
Given the connection stated above, we can use the methods in to solve (3). Since the output of
the low-rank kernel learning algorithm is W, and we prefer A in its factored form W T W for most
applications, no additional work is required beyond running the low-rank kernel learning algorithm.
Our metric learning algorithm is given as Algorithm 1; each constraint projection costs O(d2) per
iteration and requires no eigendecomposition. Thus, an iteration of the algorithm (i.e., looping
through all c constraints) requires O(cd2) time. Note that a naive implementation would cost O(cd3)
time per iteration (because of the multiplication WL), but the Cholesky factorization can be combined with the matrix multiplication into a single O(d2) routine, leading to the more efﬁcient O(cd2)
per iteration running time.
The low-rank kernel learning algorithm which forms the basis for Algorithm 1 repeatedly computes
Bregman projections, which project the current solution onto a single constraint. By employing the
Sherman-Morrison-Woodbury inverse formula appropriately, this projection—which generally has
no closed-form solution—can be computed analytically. Furthermore, it can be computed efﬁciently
on a low-rank factorization of the kernel matrix.
Discussion
In this work we formulate the Mahalanobis metric learning problem in an information-theoretic
setting and provide an explicit connection to low-rank kernel learning. We now brieﬂy discuss
extensions to the basic framework, and we contrast our approach with other work on metric learning.
We consider ﬁnding the Mahalanobis distance closest to the baseline Euclidean distance as measured
by differential relative entropy. In some applications, it may be more appropriate to consider ﬁnding
a Mahalanobis distance closest to some other baseline; for example, one could use the Mahalanobis
distance parametrized by the sample covariance matrix S as a baseline, in which case the resulting
Burg divergence problem becomes a minimization of DBurg(A, S). We note that extensions of this
sort can be solved by variants of our proposed framework.
ALGORITHM 1: Algorithm for information-theoretic metric learning
ITMETRICLEARN(X, S, D, u, l)
Input: X: input d × n matrix, S: set of similar pairs, D: set of dissimilar pairs, u, l:
distance thresholds
Output: W: output factor matrix, where W T W = A
1. Set W = Id and λij = 0 ∀i, j
2. Repeat until convergence:
• Pick a constraint (i, j) ∈S or (i, j) ∈D
• Let vT be row i of X minus row j of X
• Set the following variables:
2. if (similarity constraint)
β = γ/(1 −γ∥w∥2
else if (dissimilarity constraint)
β = −γ/(1 + γ∥w∥2
3. λij = λij −γ
• Compute the Cholesky factorization LLT = I + βwwT
• Set W ←LT W
3. Return W
We consider simple distance constraints for similar and dissimilar points, though it is straightforward
to incorporate other constraints. For example, Schutz and Joachims consider a formulation where
the distance metric is learned subject to relative nearness constraints on the input points (as in,
the distance between i and j is closer than the distance between i and k). Our approach can be
adapted to handle this setting. In fact, it is possible to incorporate arbitrary linear constraints into
our framework.
Finally, our basic formulation assumes that there exists a feasible point that satisﬁes all of the distance constraints, but in practice, this may fail to hold. A simple extension to our framework can
incorporate slack variables on the distance constraints to handle such infeasible cases.
Related Work
Xing et al. use a semideﬁnite programming formulation for learning a Mahalanobis distance
metric. Their algorithm aims to minimize the sum of squared distances between input points that are
“similar”, while at the same time aiming to separate the “dissimilar” points by a speciﬁed minimum
amount. Our formulation differs from theirs in two respects. First, we minimize a Burg-divergence,
and second, instead of considering the sum of distortions over dissimilar points, we consider pairs
of constrained points.
Weinberger et al. formulate the metric learning problem in a large margin setting, with a focus
on kNN classiﬁcation. They formulate the problem as a semideﬁnite programming problem and
consequently solve it using a combination of sub-gradient descent and alternating projections. Our
formulation does not solely have kNN as a focal point, and differs signiﬁcantly in the algorithmic
machinery used.
The paper of Globerson and Roweis proceeds to learn a Mahalanobis metric by essentially
shrinking the distance between similar points to zero, and expanding the distance between dissimilar
points to inﬁnity. They formulate a convex optimization problem which they propose to solve by
a projected-gradient method. Our approach allows more reﬁned interpoint constraints than just a
zero/one approach.
Chopra et al. presented a discriminative method based on pairs of convolutional neural networks.
Their method aims to learn a distance metric, wherein the interpoint constraints are approximately
enforced by penalizing large distances between similar points or small distances between dissimilar points. Our method is solved more efﬁciently, and the constraints are enforced incrementally.
Furthermore, as discussed above, by including slacks on our constraints, we can accommodate “softmargin” constraints.
Shalev-Shwartz et al. consider an online metric learning setting, where the interpoint constraints
are similar to ours. They also provide a margin interpretation, similar to that of . Their formulation considers distances between all pairs of similar and dissimilar points, whereas we consider only
a ﬁxed set of input pairwise constrained points.
Other notable work includes the articles . Crammer et al. applies boosting to kernel
learning, for a connection of our method kernel learning see Section 3. Lanckriet et al. study
the problem of kernel learning via semideﬁnite programming. Goldberger et al. proposed neighborhood component analysis to explicitly aid kNN; however, the formulation is non-convex and can
lead to local optima.
Acknowledgements This research was supported by NSF grant CCF-0431257, NSF Career
Award ACI-0093404, and NSF-ITR award IIS-0325116.