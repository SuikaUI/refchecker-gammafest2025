Extremely Low Bit Neural Network:
Squeeze the Last Bit Out with ADMM
Cong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, Rong Jin
Alibaba Group
{lengcong.lc, zesheng.dzs, lihao.lh, shenghuo.zhu, jinrong.jr}@alibaba-inc.com
Although deep learning models are highly effective for various learning tasks, their high computational costs prohibit
the deployment to scenarios where either memory or computational resources are limited. In this paper, we focus on compressing and accelerating deep models with network weights
represented by very small numbers of bits, referred to as extremely low bit neural network. We model this problem
as a discretely constrained optimization problem. Borrowing the idea from Alternating Direction Method of Multipliers (ADMM), we decouple the continuous parameters from
the discrete constraints of network, and cast the original hard
problem into several subproblems. We propose to solve these
subproblems using extragradient and iterative quantization algorithms that lead to considerably faster convergency compared to conventional optimization methods. Extensive experiments on image recognition and object detection verify
that the proposed algorithm is more effective than state-ofthe-art approaches when coming to extremely low bit neural
Introduction
These years have witnessed the success of convolutional
neural networks (CNNs) in a wide range computer vision
tasks, such as image classiﬁcation, object detection and segmentation. The success of deep learning largely owes to the
fast development of computing resources. Most of the deep
learning models are trained on high-ended GPUs or CPU
clusters. On the other hand, deeper networks impose heavy
storage footprint due to the enormous amount of network
parameters. For example, the 16-layers VGG involves 528
MBytes of model parameters. Both the high computational
and storage cost become impediments to popularize the deep
neural networks to scenarios where either memory or computational resources are limited. The great interest to deploy
deep learning systems on low-ended devices motives the research in compressing deep models to have smaller computation cost and memory footprints.
Considerable efforts have been mounted to reduce the
model size and speed up the inference of deep models. Denil et al. pointed out that network weights have a signiﬁcant redundancy, and proposed to reduce the number of pa-
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
rameters by exploiting the linear structure of network , which motivated a series of low-rank matrix/tensor factorization based compression algorithms, e.g.
 .
Alternatively, multiple studies were devoted to discritizing
network weights using vector quantization methods , which often outperformed the
matrix/tensor factorization based methods .
Han et al. presented the deep compression method that integrates multiple compression methods to achieve a large reduction in model size . Another
line of work for model compression is to restrict network
weights to low precision with a few bits. The advantage of
this restriction is that an expensive ﬂoating-point multiplication operation can now be replaced by a sequence of cheaper
and faster binary bit shift operations. This not only reduces
the memory footprints but also accelerates the computation of the network. These approaches work well when pretrained weights are quantized into 4-12 bits . When coming to extremely low bit networks, i.e. only one or two bits are used
to represent weights , they
only work well on simple datasets (e.g. MNIST and CI-
FAR10), and usually incur a large loss on challenging
datasets like ImageNet.
In this work, we focus on compressing and accelerating deep neural networks with extremely low bits weights,
and present a uniﬁed strategy for learning such low bits
networks. We overcome the limitation of the existing approaches by formulating it as a discretely constrained nonconvex optimization problem, which is usually referred to as
mixed integer programs (MIP). Given the NP hard nature of
MIPs, we proposed a framework for learning extremely low
bit neural network using the technique of alternating direction method of multipliers (ADMM) .
The main idea behind our method is to decouple the continuous variables from the discrete constraints using an auxiliary variable in the discrete space. This leads to a convenient form of the objective which is amenable to existing nonconvex optimization algorithms. Unlike previous low
bit quantization methods that
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
incorporate an ad-hoc modiﬁcation of the gradients for continuous weights, we simultaneously optimize in both continuous and discrete spaces, and connect the two solutions
using an augmented Lagrangian. This is consistent with the
previous observation from that, by
decoupling discrete constraints in MIP, one can use the information from the dual problem through ADMM to obtain
a better upper bound. As a result of this reformulation, we
can divide the problem of low bits quantized neural network
into multiple subproblems which are signiﬁcantly easier to
solve. The main contributions of this paper are summarized
as follows:
• We model the low bits neural network as a discretely constrained nonconvex optimization problem, and introduce
auxiliary variables to decouple the continuous weights
from the discrete constraints. With the use of ADMM,
the originally hard problem are decomposed into several
subproblems including proximal step, projection step and
dual update.
• We show how the resulting subproblems can be efﬁciently
solved. We utilize extragradient method to accelerate the
convergence of proximal step, and propose an iterative
quantization algorithm to solve the projection step. The
proposed algorithm enjoys a fast convergence in practice.
• We apply the proposed method to various well-known
convolutional neural networks. Extensive experiments on
multiple vision tasks including image classiﬁcation and
object detection demonstrate that the proposed method
signiﬁcantly outperforms the state-of-the-art approaches.
Related Work
Due to the high efﬁciency in both computation and memory footprints, low bits quantization of deep neural networks
have received much attention in the literature. In this section,
we have a brief review of the representative techniques. We
also give a brief introduction to ADMM algorithm and its
nonconvex extension.
Low bits quantization of neural network
The research of low bits quantization of neural network can
be traced back to 1990s . Most of the beneﬁts of low bits
quantization, such as memory efﬁciency and multiplication
free, had already been explored in these papers. However,
the networks are shallow at that age so these approaches
do not verify their validity in deep networks and large scale
In recent years, with the explosion of deep learning in various tasks, low bits quantization techniques have been revisited. Some early works quantize the pretrained weights with
4-12 bits and ﬁnd such approximations do not decrease predictive performance . More recent works focus on training extremely low bits network from scratch with binary or ternary
weights. Among these works, BinaryConnect is the most representative one.
BinaryConnect directly optimizes the loss of the network
with weights W replaced by sign(W). In order to avoid the
zero-gradient problem of sign function, the authors approximate it with the “hard tanh” function in the backward process. This simple idea inspired many following works. BinaryConnect only achieves good results on simple datasets
such as MNIST, CIFAR10 and SVHN, but suffers a large
degradation on challenging datasets like ImageNet.
Many efforts have been devoted to improve the performance of BinaryConnect. For example, Binary Weight Network (BWN) proposes to improve
the performance of BinaryConnect with a better approximation by introducing scale factors for the weights during binarization. Ternary Weight Network (TWN) extends the idea of BWN to network with ternary
weights and achieves a better performance. Inspired by BinaryConnect, in order to avoid the zero-gradient problem,
both BWN and TWN modify the backward process by applying the gradients of the loss at the quantized weights.
Unlike previous works, we mathematically formulated the
low bits quantization problem as a discretely constrained
problem and present a uniﬁed framework based on ADMM
to solve it in an efﬁcient way. We simultaneously optimize
the problem in both continuous and discrete space, and the
two solutions are closely connected in the learning process.
ADMM and its nonconvex extension
Alternating Direction Method of Multipliers (ADMM)
 is an algorithm that is intended to blend
the decomposability of dual ascent with the superior convergence properties of the method of multipliers. The algorithm
solves problems in the form:
f(x) + g(z)
Ax + Bz = c
with variables x ∈Rn and z ∈Rm, where A ∈Rp×n,
B ∈Rp×m and c ∈Rp.
The augmented Lagrangian of Eq.(1) can be formed as:
Lρ(x, z, y) = f(x) + g(z) + yT (Ax + Bz −c)
+(ρ/2)∥Ax + Bz −c∥2
where y is the Lagrangian multipliers, and ADMM consists
of three step iterations:
Lρ(x, zk, yk)
Lρ(xk+1, z, yk)
yk + ρ(Axk+1 + Bzk+1 −c)
Even though ADMM was originally introduced as a tool
for convex optimization problems, it turns out to be a powerful heuristic method even for NP-hard nonconvex problems.
Recently, this tool has successfully been used as a heuristic
to ﬁnd approximate solutions to nonconvex mixed program
problems , which is very similar to our problem as noted later.
The Proposed Method
Let us ﬁrst deﬁne the notion in this paper. Denote f(W)
as the loss function of a normal neural network, where
W = {W1, W2, · · · , WL}. Wi denotes the weights of the
i-th layer in the network, which for example can be a 4dimension tensor in convolutional layer or a 2-dimension
matrix in fully connected layer. For the simplicity of notation, we regard all the entries in Wi as a di-dimension vector
in Rdi, and take W as the concatenation of these vectors so
that W ∈Rd with d = 
In this work, we concentrate on training extremely low
bits quantized neural networks. In speciﬁc, the weights of
the network are restricted to be either zero or powers of two
so that the expensive ﬂoating-point multiplication operation
can be replaced by cheaper and faster bit shift operation. In
this section, we aim to mathematically model this problem
and efﬁciently solve it.
Objective function
Intuitively, training a low bits neural network can be modeled as discretely constrained optimization, or in particular, mixed integer programs. For example, the weights in
a ternary neural network are restricted to be −1, 0 or +1.
Training such network can be mathematically formulated as
mixed integer programs:
s.t. W ∈C = {−1, 0, +1}d
Since the weights are restricted to be zero or powers of
two, we have constraints of this form
C = {−2N, · · · , −21, −20, 0, +20, +21, · · · , +2N}
where N is an integer which determines the number of
bits. As in , we further introduce a
scaling factor α to the constraints, i.e., instead of requiring C = {· · · , −2, −1, 0, +1, +2. · · · }, we simply restrict C
to C = {· · · , −2α, −α, 0, +α, +2α, · · · } with an arbitrary
scaling factor α > 0 that is strictly positive. It is worthy noting that the scale factor α in various layers can be different.
In other words, for a neural network with L layers, we actually introduce L different scaling factors {α1, α2, · · · , αL}.
Formally, the objective function of low bits quantized neural
networks can be formulated as:
W ∈C = C1 × C2 × · · · × CL
where Ci = {0, ±αi, ±2αi, · · · , ±2Nαi} and αi > 0. We
emphasize that the scaling factor αi in each layer doesn’t
incur more computation to the convolutional operator, because it can be multiplied after the efﬁcient convolution with
{0, ±1, ±2, · · · , ±2N} done.
From the perspective of constrained optimization, the
scaling factor αi helps to expand the constraint space. As
an example, Fig.1 gives an illustration of how it works for
ternary network. In two dimensional space, for constraint
{−1, 0, +1}, the possible solutions of ternary neural network are nine discrete points in the space. In contrast, with
the scaling factor added, the constrained space is expanded
to be four lines in the space. This large expansion of the constrained space will make the optimization easier.
Figure 1: In ternary neural network, scaling factor expands
the constrained space from (a) nice discrete points to (b) four
lines in the space (two dimensional space as an example).
Decouple with ADMM
The optimization in Eq.(3) is NP-hard in general because
the weights are constrained in a discrete space. Most previous works try to directly train low bits models to minimize
the loss. For example, BinaryConnect replace the weights W with sign(W)
in the forward process so that the constraints will be automatically satisﬁed. Since the gradients of sign(W) to W
is zero everywhere, the authors replace the sign function
with “hard tanh” in the backward process. The same idea
is also adopted by BWN and TWN
 . However, as indicated in , the use of different forward and backward approximations causes the mismatch of gradient, which makes the
optimization instable.
We overcome the limitation of previous approaches by
converting the problem into a form which is suitable to existing nonconvex optimization techniques. We introduce an
auxiliary variable which is subject to the discrete restriction
and equal to original variable. This is used with ADMM,
which will result in the effect that the discrete variables being decoupled when we consider their minimization. Our basic idea is largely inspired by recent successful application
of ADMM in mixed integer programs .
First of all, deﬁning an indicator function IC for whether
W ∈C, the objective in Eq.(3) can be written as
f(W) + IC(W)
where IC(W) = 0 if W ∈C, otherwise IC(W) = +∞.
By introducing an auxiliary variable G, we can rewrite
the optimization in Eq.(4) with an extra equality constraint
so that the weights is constrained to be equal to the discrete
variable, but not subject to that restriction. In detail, the objective can be reformulated as:
f(W) + IC(G)
Now we are considering a nonconvex optimization with
convex linear constraints. Problems of such form can be conveniently solved with ADMM. The augmented Lagrange of
Eq.(5), for parameter ρ > 0, can be formulated as:
Lρ(W, G, μ) = f(W)+IC(G)+ ρ
2∥W −G∥2+⟨μ, W −G⟩
where μ denotes the Lagrangian multipliers and ⟨·, ·⟩denotes the inner product of two vectors. With some basic collection of terms and a change of variable λ = (1/ρ)μ, Eq.(6)
can be equivalently formed as:
Lρ(W, G, λ) = f(W)+IC(G)+ ρ
2∥W −G+λ∥2 −ρ
Following the standard process of ADMM, this problem
can be solved by repeating the following iterations:
Lρ(W, Gk, λk)
Lρ(W k+1, G, λk)
λk + W k+1 −Gk+1
which is respectively the proximal step, projection step and
dual update.
Unlike previous works, we simultaneously optimize the
problem in both continuous space (i.e., proximal step) and
discrete space (i.e., projection step), and the two solutions
are brought together by ADMM in the learning process.
Algorithm subroutines
In this section, we elaborate on how the consequent subproblems in the above algorithm can be efﬁciently solved.
Proximal step
For the proximal step, we optimize in the
continuous space. Formally, we need to ﬁnd the weights that
Lρ(W, Gk, λk) = f(W) + ρ
2∥W −Gk + λk∥2
Due to the decouple of ADMM, we are dealing with an
unconstrained objective here. The loss can be interpreted as
a normal neural network with a special regularization. Naturally, this problem can be solved with standard gradient decent method. It is easy to obtain the gradient with respect to
the weights W:
∂W L = ∂W f + ρ(W −Gk + λk)
However, we ﬁnd the vanilla gradient descent method
converges slowly in this problem. Since the second quadratic
term occupies a large proportion of the whole lost, SGD will
quickly pull the optimizer to the currently quantized weights
so that the second term vanishes, and stack in that point. This
results in a suboptimal solution since the loss of neural network is not sufﬁciently optimized.
To overcome this challenge, we resort to the extragradient
method . An iteration of the extragradient method consists of two very simple steps, prediction and
correction:
W (p) := W −βp∂W L(W),
W (c) := W −βc∂W L(W (p))
where βp and βc are the learning rates. A distinguished feature of the extragradient method is the use of an additional
gradient step which can be seen as a guide during the optimization process. Particularly, this additional iteration allows to foresee the geometry of the problem and take the
curvature information into account, which leads to a better convergency than standard gradient descent . Speciﬁc to our problem, there is a more intuitive understanding of the above iterations. For the prediction step,
the algorithm will quickly move to a point close to Gk −λk
so that the loss of quadratic regularization vanishes. Then in
the correction step, the algorithm moves another step which
tries to minimize the loss of neural network f(W). These
two steps avoid the algorithm stacking into a less valuable
local minima. In practice, we ﬁnd this extragradient method
largely accelerate the convergence of the algorithm.
Projection step
For the projection step, we optimize in the
discrete space. By neglecting those terms that do not depend
on G, the objective can be rewritten as
∥G −W k+1 −λk∥2
A key observation of (12) is that while minimizing over
G, all the components Gi are decoupled, therefore the auxiliary variables of each layer can be optimized independently. Recall that Wi, Gi, λi, Ci denote the weights, auxiliary variables, Lagrangian multipliers and constraints of
the i-th layer respectively. We are essentially looking for
the Euclidean projection of (W k+1
i ) onto a discrete
set Ci. Since the constraint is discrete and nonconvex, this
optimization is nontrivial.
For convenience, we denote (W k+1
i ) as Vi. The projection of Vi onto Ci can be formulated as
Gi ∈{0, ±αi, ±2αi, · · · , ±2Nαi}di
Taking the scaling factor away from the constraints, the
objective can be equivalently formulated as:
∥Vi −αi · Qi∥2
Qi ∈{0, ±1, ±2, · · · , ±2N}di
We propose an iterative quantization method to solve this
problem. The algorithm alternates between optimizing αi
with Qi ﬁxed and optimizing Qi with αi ﬁxed. In speciﬁc,
with Qi ﬁxed, the problem becomes an univariate optimization. The optimal αi can be easily obtained as
With αi ﬁxed, the optimal Qi is actually the projection of
αi onto {0, ±1, ±2, · · · , ±2N}, namely,
Qi = Π{0,±1,±2,··· ,±2N}
Full Precision
Table 1: Accuracy of AlexNet and VGG-16 on ImageNet classiﬁcation.
where Π denotes the projection operator. Moreover, the projection onto a discrete set is simply the closest point in it.
This iterative quantization algorithm is guaranteed to converge to a local minimum since we can get a decrease of loss
in each step. In practice, we also ﬁnd such a simple algorithm converges very fast. In most cases, we only need less
than ﬁve iterations to get a stable solution.
Dual update
In ADMM, dual update is actually gradient
ascent in the dual space . The iterate λk+1
in Eq.(10) can be interpreted as a scaled dual variable, or as
the running sum of the error values W k+1 −Gk+1.
Experiments
In order to verify the effectiveness of the proposed algorithm, in this section we evaluate it on two benchmarks: ImageNet for image classiﬁcation and Pascal VOC for object
detection.
Image Classiﬁcation
To evaluate the performance of our proposed method on
image recognition task, we perform extensive experiments
on the large scale benchmark ImageNet (ILSVRC2012),
which is one of the most challenging image classiﬁcation
benchmarks. ImageNet dataset has about 1.2 million training images and 50 thousand validation images, and these images cover 1000 object classes. We comprehensively evaluate our method on almost all well-known deep CNN architectures, including AlexNet , VGG-16 ,
ResNet-18 , ResNet-50 and
GoogleNet .
Experimental setup
In the ImageNet experiments, all the
images are resized to 256 × 256. The images are then randomly clipped to 224 × 224 patches with mean subtraction
and randomly ﬂipping. No other data augmentation tricks
are used in the learning process. We report both the top-1
and top-5 classiﬁcation accurate rates on the validation set,
using single-view testing (single-crop on central patch only).
We study different kinds of bit width for weight quantization. Speciﬁcally, we tried binary quantization, ternary quantization, one-bit shift quantization and two-bits shift quantization. For one-bit shift quantization, the weights are restricted to be {-2a, -a, 0, +a, +2a}, which we denote as
{-2, +2} in the comparison. Similarly, two-bits shift quantization are denoted as {-4, +4}. Binary quantization and
ternary quantization need one bit and two bits to represent
one weight respectively. Both {-2, +2} quantization and {-
4, +4} quantization need three bits to represent one weight.
For binary and ternary quantization, we compare the proposed algorithm with the state-of-the-art approaches Binary
Weight Network (BWN) and Ternary
Weight Network (TWN) . Both
BWN1 and TWN2 release their source code so we can evaluate their performance on different network architectures.
Our method is implemented with Caffe . The
referenced full precision CNN models VGG-16, ResNet-50
and GoogleNet are taken from the Caffe model zoo3.
Results on AlexNet and VGG-16
AlexNet and VGG-16
are “old fashion” CNN architectures. AlexNet consists of 5
convolutional layers and 3 fully-connected layers. VGG-16
uses much wider and deeper structure than AlexNet, with 13
convolutional layers and 3 fully-connected layers. Table 1
demonstrates the comparison results on these two networks.
For fair comparison with BWN, we report the performance
of the batch normalization version
of AlexNet. The accuracy of the improved AlexNet is higher
than the original one (Top-1 60.0% vs. 57.4%, Top-5 82.4%
vs. 80.4%).
On these two architectures, the proposed algorithm
achieves a lossless compression with only 3 bits compared
with the full precision references. For {-2, +2} and {-4, +4}
quantization, the performance of the our quantized networks
is even better than the original full precision network on
VGG-16. Similar results are observed in BinaryConnect on
small datasets. This is because discrete weights could provide a form of regularization which can help to generalize
better. These results also imply the heavy redundancy of the
parameters in full precision AlexNet and VGG-16 models.
This ﬁnding is consistent with that in other studies such as
SqueezeNet . In SqueezeNet, the authors suggest that one can achieve AlexNet-level accuracy
on ImageNet with 50x fewer parameters.
Our binary quantization and ternary quantization slightly
outperforms BWN and TWN on these two architectures.
Comparing the accuracy of ternary quantization and binary quantization, we ﬁnd that ternary network consistently
works better than binary network. We also emphasize that
the ternary network is more computing efﬁcient than binary
network because of the existence of many zero entries in the
weights, as indicated in (Venkatesh, Nurvitadhi, and Marr
Results on ResNet
The results on ResNet-18 are shown in
Table 2. ResNet-18 has 18 convolutional layers with shortcut
1 
2 
3 
Full Precision
Table 2: Accuracy of ResNet-18 and ResNet-50 on ImageNet classiﬁcation.
Full Precision
Table 3: Accuracy of GoogleNet on ImageNet classiﬁcation.
connections. For the proposed method, both the binary and
ternary quantization substantially outperform their competitors on this architecture. For example, our binary network
outperforms BWN by 4 points in top-1 accuracy and 3.2
points in top-5 accuracy. The proposed ternary quantization
outperforms TWN by 5.2 points and 3.3 points in top-1 and
top-5 accuracy respectively. All these gaps are signiﬁcant on
ImageNet. We also observe over two percent improvement
for our ternary quantization over binary quantization.
The effectiveness of our method is also veriﬁed on very
deep convolutional network such as ResNet-50. Besides signiﬁcantly increased network depth, ResNet-50 has a more
complex network architecture than ResNet-18. Table 2 details the results on ResNet-50. It is easy to observe the similar trends as in ResNet-18. Our method is considerably better than the compared BWN and TWN. For example, our
binary quantization obtains about 5 points improvement on
top-1 accuracy over BWN.
For both ResNet-18 and ResNet-50, there is a more noticeable gap between the low bits quantized networks and
full precision reference. Different from AlexNet and VGG-
16, on ResNet we notice about 1 point gap in top-1 accuracy
between {-4, +4} quantized network and full precision reference. These results suggest that training extremely low bits
quantized network is easier for AlexNet and VGG than for
ResNet, which also implies the parameters in AlexNet and
VGG-16 are more redundant than those in ResNet-18 and
ResNet-50.
Results on GoogleNet
The results on GoogleNet are illustrated in Table 3. GoogleNet is a 22 layers deep network,
organized in the form of the “Inception module”. Similar
to ResNet, GoogleNet is more compact than AlexNet and
VGG-16, so it will be more difﬁcult to compress it. There
exists a gap of more than 2 points in top-1 accuracy between {-4, +4} quantized network and full precision version.
The loss of binary quantization is more signiﬁcant, which
reaches 8 points in top-1 accuracy. Despite this, our method
stills outperforms BWN4 and TWN on this network.
Compare with the most recent works
To our knowledge, Trained Ternary Quantization (TTN) 
4Note that the GoogleNet used in BWN paper is an improved
variant of the original version used in this paper.
and Incremental Network Quantization (INQ) are two of the most recent published works on low
bits quantization of deep neural network. Instead of quantizing the ternary weights to be {−α, 0, +α}, TTN makes it
less restrictive as {−α, 0, +β}. Note that our method can
be easily extended to deal with constraints of such form.
Nevertheless, the computation of such form of ternary network is less efﬁcient than the original one. As an example, for fast implementation the inner product between vector x and vector (−α, −α, 0, +β) will be decomposed as
βx · (0, 0, 0, 1) −αx · (1, 1, 0, 0), having to do two ﬂoatingpoint multiplications with α and β.
Since TTN only reports its results on AlexNet and
ResNet-18, we compare the performance on these two architectures. Detailed results are summarized in Table 4 and
Table 5. Our approach performs better than TTN on AlexNet
(the results of ternary INQ on AlexNet is not available), and
better than both TTN and INQ on ResNet-18. INQ shows
more results on 5-bits networks in the paper. For example,
the reported top-1 and top-5 accuracy of ResNet-50 with 5bits are 73.2% and 91.2% . In contrast, our
method achieves such accuracy with only 3 bits.
Top-1 acc.
Top-5 acc.
TTN 
Ours (Ternary)
Table 4: Comparison with TTN on AlexNet.
Top-1 acc.
Top-5 acc.
TTN 
INQ 
Ours (Ternary)
Table 5: Comparison with TTN and INQ on ResNet-18.
INT8 quantized 1×1 kernel
We notice the extremely low
bits quantization of GoogleNet suffers a large degradation.
We guess this may be due to the 1×1 kernel in each inception. In order to verify this point, we perform another
experiment on GoogleNet. In this version, the 1×1 kernels
in the network are quantized with relatively more bits, i.e.,
Table 6: Accuracy of GoogleNet. 1×1 kernels are quantized
with INT8.
INT8, and kernels of other size are quantized as usual. Table
6 shows the results.
By comparing the results in Table 6 and those in Table
3, we observe a considerable improvement, especially for
binary and ternary quantization. As we have discussed, discrete weights can be interpret as a strong regularizer to the
network. However, the parameters in 1×1 kernel is much
less than those in other kernels. Imposing a very strong regularizer to such kernels may lead to underﬁtting of the network. These results suggest that we should quantize different parts of the networks with different bit width in practice.
Letting the algorithm automatically determine the bit width
will be our future work.
To accelerate the inference of low-bits neural network,
we have developed a fast low-bits GEMM which is optimized for Intel and Arm architectures. Compared with fullprecision network, our implementation of low-bits Resnet-
18 is two times faster on Intel CPU (Core i5-3200) and 1.85
times faster on ARM CPU (Qualcomm Snapdragon 835).
Object Detection
In order to evaluate the proposed method on object detection task, we apply it to the state of arts detection framework
SSD . The models are trained on Pascal
VOC2007 and VOC2012 train dataset, and tested on Pascal VOC 2007 test dataset. For SSD, we adopt the open implementation released by the authors. In all experiments, we
follow the same setting as in and the input
images are resized to 300 × 300.
The proposed method are evaluated on two base models, i.e., VGG-16 and Darknet reference model. Both base
networks are pre-trained on ImageNet dataset. The VGG-
16 network here is a variant of original one . In detail, the fc6 and fc7 are converted to
convolutional layers with 1 × 1 kernel, and the fc8 layer is
removed. The parameters of fc6 and fc7 are also subsampled. The darknet reference model is borrowed from YOLO
 , which is another fast detection framework. Darknet is designed to be small yet power, which attains comparable accuracy performance as AlexNet but only
with about 10% of the parameters. We utilize the base darknet model downloaded from the website5.
To the best of our knowledge, there is no other works on
low bits quantization applied their algorithms to the object
detection tasks. We compare our quantized network with full
precision network in this experiment. We only implement
ternary and {-4,+4} quantization for this experiment. Darknet has utilized many 1×1 kernels as in GoogleNet to accelerate the inference process. We implement two versions of
5 
Darknet+SSD
0.609 (0.621)
0.624 (0.639)
Full Precision
Table 7: mAP of VGG16+SSD and Darknet+SSD on Pascal
Darknet. In the ﬁrst version, the 1×1 kernels are also quantized as usual, while in the second version these kernels are
quantized with INT8. Table 7 shows the mean average precision (mAP) on both models.
For {-4,+4} quantization, we ﬁnd that the mAP of both
modes are very close to the full precision version. On
VGG16+SSD, we only suffer a loss of 0.002 in mAP.
Comparing two versions of Darknet+SSD, the ﬁrst version achieves a mAP of 0.624, and the second version obtains a improvement of 1.5 points. For ternary quantization, the accuracy degradation of Darknet+SSD is larger
than VGG16+SSD, because the parameters of Darknet is
less redundant than VGG-16. All these results indicate that
our proposed method is also effective on the object detection
Conclusion
This work focused on compression and acceleration of deep
neural networks with extremely low bits weight. Inspired by
the efﬁcient heuristics proposed to solve mixed integer programs, we proposed to learn low bits quantized neural network in the framework of ADMM. Extensive experiments
on convolutional neural network for image recognition and
object detection have shown the effectiveness of the proposed method.
Acknowledgments
The authors really appreciate the help from our colleagues.
They are Mou Li, Jiangang Kong, Yu Li and Zhenyu Gu.
Without their help this work would not be possible.