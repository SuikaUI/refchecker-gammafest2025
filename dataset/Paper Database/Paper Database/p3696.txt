MIT Open Access Articles
Dimensionality reduction and polynomial chaos
acceleration of Bayesian inference in inverse problems
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Marzouk, Youssef M., and Habib N. Najm. “Dimensionality reduction and polynomial
chaos acceleration of Bayesian inference in inverse problems.” Journal of Computational
Physics 228.6 : 1862-1902.
As Published: 
Publisher: Elsevier
Persistent URL: 
Version: Author's final manuscript: final author's manuscript post peer review, without
publisher's formatting or copy editing
Terms of Use: Article is made available in accordance with the publisher's policy and may be
subject to US copyright law. Please refer to the publisher's site for terms of use.
Dimensionality reduction and polynomial
chaos acceleration of Bayesian inference in
inverse problems
Youssef M. Marzouk a,1 and Habib N. Najm b
aMassachusetts Institute of Technology, Cambridge, MA 02139 USA
bSandia National Laboratories, Livermore, CA 94551 USA
We consider a Bayesian approach to nonlinear inverse problems in which the unknown quantity is a spatial or temporal ﬁeld, endowed with a hierarchical Gaussian
process prior. Computational challenges in this construction arise from the need
for repeated evaluations of the forward model (e.g., in the context of Markov chain
Monte Carlo) and are compounded by high dimensionality of the posterior. We address these challenges by introducing truncated Karhunen-Lo`eve expansions, based
on the prior distribution, to eﬃciently parameterize the unknown ﬁeld and to specify a stochastic forward problem whose solution captures that of the deterministic
forward model over the support of the prior. We seek a solution of this problem
using Galerkin projection on a polynomial chaos basis, and use the solution to construct a reduced-dimensionality surrogate posterior density that is inexpensive to
evaluate. We demonstrate the formulation on a transient diﬀusion equation with
prescribed source terms, inferring the spatially-varying diﬀusivity of the medium
from limited and noisy data.
Key words: inverse problems, Bayesian inference, dimensionality reduction,
polynomial chaos, Markov chain Monte Carlo, Galerkin projection, Gaussian
processes, Karhunen-Lo`eve expansion, RKHS
Email addresses: (Youssef M. Marzouk), 
(Habib N. Najm).
1 Corresponding author
 
29 November 2008
Introduction
Inverse problems arise from indirect observations of a quantity of interest. Observations may be limited in number relative to the dimension or complexity
of the model space, and the action of the forward operator may include ﬁltering or smoothing eﬀects. These features typically render inverse problems
ill-posed—in the sense that no solution may exist, multiple solutions may exist,
or solutions may not depend continuously on the data. In practical settings,
where observations are inevitably corrupted by noise, this presents numerous
challenges.
Classical approaches to inverse problems have used regularization methods to
impose well-posedness, and solved the resulting deterministic problems by optimization or other means . However, important insights and methodologies
emerge by casting inverse problems in the framework of statistical inference
 . Here we focus on Bayesian approaches, which provide a foundation for
inference from noisy and limited data, a natural mechanism for regularization in the form of prior information, and in very general cases—e.g., nonlinear forward operators, non-Gaussian errors—a quantitative assessment of
uncertainty in the results . Indeed, the output of Bayesian inference is
not a single value for the quantity of interest, but a probability distribution
that summarizes all available information about this quantity, be it a vector of parameters or a function (i.e., a signal or spatial ﬁeld). Exploration of
this posterior distribution—and thus estimating means, higher moments, and
marginal densities of the inverse solution—may require repeated evaluations
of the forward operator. For complex physical models and high-dimensional
model spaces, this can be computationally prohibitive.
Our previous work sought to accelerate the Bayesian solution of inverse
problems through the use of stochastic spectral methods. Based on polynomial chaos (PC) representations of random variables and processes ,
stochastic spectral methods have been used extensively for forward uncertainty propagation—characterizing the probability distribution of the output
of a model given a known distribution on the input. These methods constitute
attractive alternatives to Monte Carlo simulation in numerous applications:
transport in porous media , structural mechanics , thermo-ﬂuid systems , electrochemical microﬂuid systems , and reacting ﬂow .
In the inverse context, the Bayesian formulation in constructs a stochastic forward problem whose random inputs span the support of the prior and
seeks its solution using Galerkin methods. The prior support may be partitioned, but for each partition the stochastic forward problem is solved only
once. The resulting spectral representations of the forward operator enter the
likelihood function, and exploration of the posterior is recast as Monte Carlo
sampling of the variables underlying the PC expansion. We used this scheme
to infer parameters appearing nonlinearly in a transient diﬀusion equation,
demonstrating exponential convergence to the true posterior and substantial
Other attempts at accelerating Bayesian inference in computationally intensive inverse problems have relied on reductions or surrogates for the forward
model, constructed through repeated forward simulations. Wang & Zabaras
 use proper orthogonal decomposition (POD) to accelerate forward
model calculations in a radiative source inversion problem. The empirical basis used for model reduction is pre-constructed using full forward problem
simulations. The choice of inputs to these simulations—in particular, how
closely the inputs must resemble the inverse solution—can be important .
Balakrishnan et al. introduce a PC representation of the forward model in
a groundwater transport parameter identiﬁcation problem, but obtain the PC
coeﬃcients by collocation; again, this process depends on a series of “snapshots” obtained from repeated forward simulations. In the statistical literature,
under the headline of “Bayesian parameter calibration,” Gaussian processes
have been used extensively as surrogates for complex computational models
 . These approaches treat the forward model as a black box, and thus require
careful attention to experimental design and to modeling choices that specify
the mean and covariance of the surrogate Gaussian process. A diﬀerent set of
approaches retain the full forward model but use simpliﬁed or coarsened models to guide and improve the eﬃciency of Markov chain Monte Carlo (MCMC).
Christen & Fox use a local linear approximation of the forward model to
improve the acceptance probability of proposed moves, reducing the number
of times the likelihood must be evaluated with the full forward model. Higdon
et al. focus on the estimation of spatially distributed inputs to a complex
forward model. They introduce coarsened representations of the inputs and
apply a Metropolis-coupled MCMC scheme in which “swap proposals”
allow information from the coarse-scale formulation to inﬂuence the ﬁne-scale
chain. Efendiev et al. also develop a two-stage MCMC algorithm, using a
coarse-scale model based on multiscale ﬁnite volume methods to improve the
acceptance rate of MCMC proposals.
This paper extends the stochastic spectral methodology of to inverse problems whose solutions are unknown functions—i.e., spatial or temporal ﬁelds.
In doing so, we also explore dimensionality reduction in the Bayesian formulation of inverse problems, and the dependence of dimensionality on both the
prior and the data. Inverse problems involving ﬁelds are vital to applications
ranging from geophysics to medical imaging. Spatial ﬁelds may correspond
to inhomogeneous material properties, such as permeabilities, diﬀusivities, or
densities, or may represent distributed source terms in transport equations.
Estimating ﬁelds rather than parameters typically increases the ill-posedness
of the inverse problem, since one is recovering an inﬁnite-dimensional object
from ﬁnite amounts of data. Obtaining physically meaningful results requires
the injection of additional information on the unknown ﬁeld—i.e., regularization . A standard Bayesian approach is to employ Gaussian process (GP)
or Markov random ﬁeld (MRF) priors . Most studies then explore
the value of the ﬁeld on a ﬁnite set of grid points ; the dimension of the
posterior is tied to the discretization of the ﬁeld. This recipe presents diﬃculties for stochastic spectral approaches, however, as the size of a PC basis does
not scale favorably with dimension . Moreover, with any degree of smoothness, the value of the ﬁeld at each grid point hardly represents an independent
direction.
Ideally, one should employ a representation that reﬂects how much information is truly required to capture variation among realizations of the unknown
ﬁeld. To this end, we introduce a Karhunen-Lo`eve (K-L) expansion based on
the prior random process, transforming the inverse problem to inference on
a truncated sequence of weights of the K-L modes. Other recent work has
also employed K-L expansions in the context of statistical inverse problems.
Li & Cirpka emphasize the role of K-L expansions in enabling geostatistical inversion on unstructured grids. Efendiev et al. use K-L expansions
to parameterize the log-permeability ﬁeld in their two-stage MCMC scheme,
and introduce constraints among the weights in order to match known values of the permeability at selected spatial locations. In contrast to , we
use a fully Bayesian approach, generating true conditional realizations from a
non-Gaussian posterior.
A more fundamental distinction of the present work is that we combine a K-L
representation of the unknown ﬁeld with spectral methods for uncertainty
propagation. In particular, the Karhunen-Lo`eve representation of a scaled
Gaussian process prior deﬁnes the uncertainty that is propagated through
the forward model with a stochastic Galerkin scheme. The deterministic forward model, originally speciﬁed by (a system of) partial diﬀerential equations,
is thus replaced by stochastic PDEs; numerical approaches to such systems, in
which random ﬁelds appear as boundary conditions or coeﬃcients, have seen
extensive development . Uncertainty propagation yields a polynomial approximation of the forward operator over the support of the prior.
This approximation then enters a reduced-dimensionality surrogate posterior,
which we explore with MCMC. The overall scheme avoids repeated forward
simulations, and the computational cost per MCMC iteration becomes negligible compared to the cost of a full forward solution.
We demonstrate our scheme with a nonlinear forward model, and develop a
fully Bayesian treatment of the problem in which hyperparameters describing
the prior covariance are estimated simultaneously with the unknown ﬁeld in
a joint posterior distribution. To place the present Bayesian formulation in
broader context, we recall connections between the K-L expansion and reg-
ularization penalties in the reproducing kernel Hilbert space (RKHS) norm
corresponding to the prior covariance. We explore convergence both with respect to the number of K-L modes and the order of the PC basis. We also
examine the eﬃciency of MCMC, quantify the limiting distribution of the
K-L modes, and explore the impact of data resolution on the approach to this
distribution. Signiﬁcant gains in computational eﬃciency are achieved, with
speedups of more than two orders of magnitude over a grid-based Bayesian
scheme employing repeated foward solutions.
Formulation
We begin by introducing the essential building blocks of the present formulation: Gaussian processes (§2.1), the Karhunen-Lo`eve (K-L) representation
of a stochastic process (§2.2), and the Bayesian approach to inverse problems
(§2.3). All three pieces are brought together in §2.4, where the K-L expansion of a Gaussian process prior enables a reduced-dimensionality approach to
Bayesian inference. Stochastic spectral methods for forward uncertainty propagation are then introduced in §2.5, and used to accelerate the K-L/Bayesian
approach in §2.5.2. Finally, connections among Gaussian priors, the K-L expansion, and regularization are reviewed in §2.6.
Gaussian processes
Let (Ω, U, P) be a probability space, where Ωis a sample space, U is a σalgebra over Ω, and P is a probability measure on U. Also, let D ⊂Rn be
a bounded spatial domain. If M(x) : Ω→R is a U-measurable mapping for
every x ∈D, then M : Ω× D →R is a random ﬁeld. M(x, ω), for ω ∈Ω,
can thus be seen as a collection of real-valued random variables indexed by
x ∈D. Alternatively, one can view M(·) as a random variable taking values in
RD, the space of all real-valued functions on D. Though our presentation will
focus on ‘random ﬁelds’ (typically signifying processes indexed by a spatial
coordinate), the developments below are applicable to processes indexed by
time or by both time and space.
If, for any n ≥1 we have
(M(x1), . . . , M(xn)) i.d.
= (M(x1 + s), . . . , M(xn + s))
where i.d.
= denotes equality in distribution, s is a spatial shift, and {xi, xi +
i=1 ∈D, then M is said to be stationary . If in addition, all ﬁnitedimensional distributions of M are multivariate normal, then M is a stationary
Gaussian random ﬁeld, or simply a stationary Gaussian process (GP). Let
M(n) = (M(x1), . . . , M(xn)) denote the restriction of M to a ﬁnite set of
indices. Then the characteristic function of M(n) is 
where the mean is spatially invariant, µ ≡µ1n, and entries of Σ are values of
the covariance function C:
Σij = C(xi, xj)
≡Cov [M(xi), M(xj)] = E [(M(xi) −µ) (M(xj) −µ)]
= ˜C(xi −xj).
Gaussian processes have ﬁnite second moments; that is, M(x) ∈L2(Ω) for
every x . If Σ is invertible, the ﬁnite-dimensional density of order n of the
Gaussian process is then
p(m|µ, Σ) =
(2π)n/2|Σ|1/2 exp
2 (m −µ)T Σ−1 (m −µ)
where m = (m(x1), . . . , m(xn)). If we further restrict C to depend only on
the distance between xi and xj, that is we put ˜C(d) = f (∥d∥), then the
stationary GP is called isotropic . It is common to specify the covariance
function with scale and range parameters θ1 and θ2 respectively :
˜C(d) = θ1ρ
Here ρ(·) is a correlation function, positive deﬁnite with ρ(0) = 1 , e.g.,
ρ(d) = e−d or ρ(d) = e−d2.
Gaussian processes are extensively employed as priors in Bayesian inference
 . In particular, conceiving of the GP as a prior over functions motivates
Gaussian process regression , also known as kriging in spatial statistics
 ; further applications include classiﬁcation, with ties to support vector
machines and other kernel methods . Depending on the covariance kernel,
realizations of a Gaussian process may be smooth or periodic, or for nonstationary kernels, capture certain trends . Gaussian process priors can
thus inject regularity by assigning low probability to ﬁelds with undesirable
properties. See §2.6 for a more formal discussion of Gaussian process priors
and regularization.
Karhunen-Lo`eve expansion
Let M(x, ω) be a real-valued random ﬁeld with ﬁnite second moments, mean
µ(x), and a covariance function that is continuous on D×D, with D bounded.
Then M has the following representation, termed a Karhunen-Lo`eve (K-L)
expansion :
M(x, ω) = µ(x) +
λkck(ω)φk(x).
In general, this equality holds pointwise and in the mean square sense; that
is, convergence is in L2(Ω) for each x ∈D. If M is Gaussian and almost
surely continuous, then convergence is uniform over D with probability one
 . b λk and φk(x) are eigenvalues and eigenfunctions of the linear operator
corresponding to the covariance kernel C:
D C(x1, x2)φk(x2)dx2 = λkφk(x1).
By the assumptions on M, the covariance kernel is symmetric and positive
semideﬁnite, and thus by Mercer’s theorem we have 
C(x1, x2) =
λkφk(x1)φk(x2)
where the eigenfunctions φk(x) are continuous and form a complete orthonormal system in L2(D). The random variables ck(ω) are uncorrelated with zero
mean and unit variance:
E [cjck] = δjk.
These variables are in general non-Gaussian
D (M(x, ω) −µ(x)) φk(x)dx
but if M is also a Gaussian process, the ck are Gaussian and independent,
ck ∼N(0, 1).
The K-L expansion is optimal in the following sense. Of all possible orthonormal bases for L2(D), the {φk(x)} satisfying (8) minimize the mean-squared
b Suﬃcient conditions for the continuity of Gaussian processes are detailed in Adler
 . Abrahamsen suggests that any Gaussian process on compact D ∈Rn with a
continuous mean and a continuous and “reasonable” covariance function will satisfy
these conditions. Covariance functions that provably yield a.s. continuous Gaussian
processes include Gaussian, exponential, spherical, Mat´ern, spline, and polynomial
kernels, along with numerous others .
error in a ﬁnite linear representation of M(·) . That is, they minimize
M(x, ω) −µ(x) −
λkck(ω)φk(x)
for any K ≥1. As a result, the K-L expansion is an extremely useful tool
for the concise representation of stochastic processes. It has close analogues in
data reduction (i.e., principal components analysis), model reduction (proper
orthogonal decomposition) , and linear algebra (SVD). If M(·) is approximated by a K-term K-L expansion,
MK(x, ω) = µ(x) +
λkck(ω)φk(x)
the covariance function of MK is simply
CK(x1, x2) =
λkφk(x1)φk(x2)
which converges uniformly to (9) as K →∞ . In particular, the total
variance or “energy” of MK is
D E [MK(x, ω) −µ(x)]2 dx =
D CK(x, x)dx =
following from the orthonormality of the {φk(x)}.
Bayesian approach to inverse problems
Bayesian approaches to inverse problems have received much recent interest
 , with applications ranging from geophysics and climate
modeling to heat transfer . We review this approach brieﬂy below;
for more extensive introductions, see .
Consider a forward problem deﬁned as follows:
Here m is a vector of model parameters or inputs and d is a vector of observable quantities, or data; for simplicity, we let both be real-valued and
ﬁnite-dimensional. The forward model G yields predictions of the data as a
function of the parameters. In the Bayesian setting, m and d are random
variables. We use Bayes’ rule to deﬁne a posterior probability density for m,
given an observation of the data d:
p(m|d) ∝p(d|m)pm(m)
In the Bayesian paradigm, probability is used to express knowledge about the
true values of the parameters. In other words, prior and posterior probabilities represent degrees of belief about possible values of m, before and after
observing the data d.
Data thus enters the formulation through the likelihood function p(d|m),
which may be viewed as a function of m: L(m) ≡p(d|m). A simple model
for the likelihood assumes that independent additive errors account for the
deviation between predicted and observed values of d:
d = G(m) + η
where components of η are i.i.d. random variables with density pη. The likelihood then takes the form
L(m) = pη (d −G(m)) =
pη (di −Gi(m)) .
Additional information on the model parameters may enter the formulation
through the prior density, pm(m). Prior models may embody simple constraints on m, such as a range of feasible values, or may reﬂect more detailed
knowledge about the parameters, such as correlations or smoothness. In the
absence of additional information, one may choose a prior that is uninformative. Here we will focus on Gaussian process priors, which for ﬁnite-dimensional
m take the form of (5).
If parameters φm of the prior density pm(m|φm) or parameters φη of the error
model pη(ηi|φη) are not known a priori, they may become additional objects
for Bayesian inference. In other words, these hyperparameters may themselves
be endowed with priors and estimated from data :
p(m, φm, φη|d) ∝p(d|m, φη)pm(m|φm)p(φη)p(φm).
The resulting joint posterior over model parameters and hyperparameters may
then be interrogated in various ways—e.g., by marginalizing over the hyperparameters to obtain p(m|d); or ﬁrst marginalizing over m and using the
maximizer of this density as an estimate of the hyperparameters; or by seeking the joint maximum a posteriori estimate or posterior mean of m, φm,
and φη . In the present study, we will introduce hyperparameters to
describe aspects of the prior covariance.
Dimensionality reduction in inverse problems
Now we integrate concepts from the previous three sections. Consider an inverse problem in which the unknown quantities comprise a real-valued ﬁeld
M(x). In a computational setting, this ﬁeld and the forward model must be
discretized. If M(x) can be adequately represented on a ﬁnite collection of
points {xi}n
i=1 ∈D, then we can write both the prior and posterior densities in terms of m = (M(x1), . . . , M(xn)). That is, we can directly apply
the Bayesian formulation described in the preceding section and explore the
posterior density of m with Markov chain Monte Carlo (MCMC) . The
vector m, however, will likely be high-dimensional. High dimensionality not
only renders MCMC exploration of the posterior more challenging and costly,
but taxes the polynomial chaos formulation we introduce below to accelerate
evaluations of the posterior density .
Instead of exploring the value of M(x) on each of n index points, we appeal to
the K-L expansion. Let M(x) be endowed with a Gaussian process prior with
mean µ(x) and covariance kernel C(x1, x2); we denote this as M ∼GP(µ, C).
Introduce the corresponding K-term K-L representation of M(x) (13), with
eigenvalues λk and eigenfunctions φk(x) satisfying (8). In general, M(x, ω)
is approached pointwise in mean square (and therefore in distribution) by
MK(x, ω) as K →∞. For M a.s. continuous (see §2.2), realizations M(x, ω)
can be uniformly approximated as closely as desired by MK(x, ω)—implying
a corresponding realization c(ω) ≡(c1(ω), . . . , cK(ω))—with probability one.
Updating distributions of M, by conditioning on the data, is thus equivalent
to updating the joint distribution of the mode strengths ck. We emphasize this
viewpoint by writing MK(x, ω) = MK(x, c(ω)) = MK(c), parameterizing M
by the vector of weights c. Components ck are independent under the Gaussian
process prior, with ck ∼N(0, 1). We thus truncate the K-L expansion at K
terms and write a posterior density for c:
∝pη (d −G (MK(c)))
The inverse problem has been transformed to an inference problem on the
weights ck of a ﬁnite number of K-L modes. Note that the spatial discretization
of M(x) and of the forward model is now independent of the dimension of
the posterior distribution. Here we have assumed the prior covariance to be
completely known, thus ignoring hyperparameters in the expression for the
posterior; we will relax this assumption in §3.
Truncating the K-L expansion in this context amounts to using a “modi-
ﬁed” prior covariance kernel given by (14). Since the eigenvalues λk decay—
exponentially fast for a smooth covariance kernel , algebraically fast in
other cases—a small number of terms may be suﬃcient to capture almost all
of the prior covariance. The linear operator corresponding to the modiﬁed
covariance kernel now has ﬁnite rank; φk(x) that are not eigenfunctions of
this operator cannot contribute to the inverse solution. The impact of this
truncation is explored in §3.3.
Polynomial chaos acceleration
MCMC exploration of the reduced-dimensionality posterior (21) still requires
repeated solutions of the forward model, once for each proposed move of the
Markov chain. While dimensionality reduction may reduce the number of such
evaluations, depending on the details of the MCMC sampler, it is desirable to
avoid repeated forward solutions altogether.
Our previous work introduced methods for accelerating Bayesian inference in this context, by using stochastic spectral methods to propagate prior
uncertainty through the forward problem. These methods eﬀectively create a
“surrogate” posterior containing polynomial chaos (PC) representations of the
forward model outputs. This density may be evaluated orders of magnitude
more quickly than the “direct” posterior containing the full forward problem. Here, we will use the Gaussian process prior on M (and thus the prior
distribution on c) to deﬁne an appropriate stochastic forward problem. The
K-L expansion of M ensures that we have chosen a concise representation of
the prior uncertainty that is yet suitable for reconstructing inverse solutions.
Beginning with polynomial chaos expansions, elements of this approach are
described below.
Forward propagation of uncertainty
Once again let (Ω, U, P) be a probability space on which we deﬁne a random
process X : Ω→RD with index set D ⊆RN. Also, let {ξi(ω)}∞
i=1 be i.i.d.
standard normal random variables on Ω. Then any square-integrable X has
the following representation:
X(ω) = a0Γ0 +
ai1Γ1(ξi1) +
ai1i2Γ2(ξi1, ξi2)
ai1i2i3Γ3(ξi1, ξi2, ξi3) + · · ·
where Γp is the Wiener polynomial chaos of order p and the ai1i2...
may be functions on D. This expansion can be re-written in a more compact
ˆakΨk(ξ1, ξ2, . . .)
where there is a one-to-one correspondence between the coeﬃcients and functionals in (22) and in (23) . For the standard normal ξi chosen above,
orthogonality of successive Γp requires that the Γp be multivariate Hermite
polynomials; both these and the corresponding Ψk may be generated from
univariate Hermite polynomials by taking tensor products.
Of course, in computations it is not useful to retain inﬁnite summations, and
one truncates the expansion both in order p and in dimension n—i.e., by
choosing a subset ξ = {ξλi}n
i=1 of the inﬁnite set {ξi}, λi ∈N. The total
number of terms P in the ﬁnite polynomial chaos expansion
xkΨk(ξ1, ξ2, . . . , ξn)
P + 1 = (n + p)!
Polynomial chaos (PC) expansions have been generalized to broader classes
of orthogonal polynomials in the Askey scheme, each family resulting from a
diﬀerent choice of distribution for the ξi . For each of these choices,
orthogonality of the polynomials Ψk(ξ) with respect to the inner product on
L2(Ω) is maintained:
Ψi (ξ(ω)) Ψj (ξ(ω)) dP (ω)
Ψi (ξ) Ψj (ξ) ρ(ξ)dξ
where ρ(ξ) denotes the probability density of ξ. This property can be used to
calculate the truncated PC representation of a random variable f ∈L2(Ω) by
projecting onto the PC basis:
fk = ⟨f(X)Ψk⟩
This orthogonal projection minimizes the error ∥f −˜f∥2 on the space spanned
k=0, where ∥· ∥2 is the inner-product norm on L2(Ω).
Suppose that the behavior of f can be expressed as O (f, X) = 0, where O
is some deterministic operator and X is a random variable or process with
a known PC expansion X = PP
i=0 xiΨi(ξ). Substituting PC expansions for
f and X into this operator and requiring the residual to be orthogonal to
Ψj for j = 0 . . . P yields a set of coupled, deterministic equations for the PC
coeﬃcients fk:
j = 0 . . . P.
This Galerkin approach is known as “intrusive” spectral projection , in
contrast to “non-intrusive” approaches in which the inner product ⟨f(X)Ψk⟩
is evaluated by sampling or quadrature, thus requiring repeated evaluations
of f(X) corresponding to diﬀerent realizations of ξ .
In practice, we employ a pseudospectral construction to perform intrusive projections eﬃciently for higher powers of random variables, e.g. f(X) = Xj, j ≥
3, and have developed additional techniques for nonpolynomial functions f.
These operations are incorporated into a library for “stochastic arithmetic,”
detailed in .
Stochastic spectral formulation of Bayesian inference
In we described three accelerated schemes for computing posterior estimates, all based on spectral solutions of a stochastic forward problem: Monte
Carlo sampling from the prior distribution, importance sampling, and MCMC.
Here we focus on the latter case. The essential idea is to construct a stochastic forward problem whose solution approximates the deterministic forward
model over the support of the prior.
Let us begin with (i) a ﬁnite-dimensional representation of the unknown quantity that is the object of inference, and (ii) a prior distribution on the parameters of this representation. For instance, if the unknown quantity is a ﬁeld
M(x) endowed with a Gaussian process prior, the ﬁnite representation may
be a truncated K-L expansion with mode strengths c and priors ci ∼N(0, 1).
The Bayesian formulation in §2.4 describes the inverse solution in terms of
the posterior density of c, which includes evaluations of the forward model
G(MK(·)). For simplicity, we shall abbreviate G ◦MK as Gc; inputs to this
model are parameterized by c. Also, let C denote the support of the prior.
Now deﬁne a random vector ˇc = g( ˇξ), each component of which is given by a
PC expansion
ˇci = gi( ˇξ) =
gikΨk( ˇξ).
This vector will serve as input to Gc, thus specifying a stochastic forward
problem. Recall that the distribution of ˇξ (e.g., standard normal) and the
polynomial form of Ψ (e.g., multivariate Hermite) are intrinsic properties of
the PC basis. We do not require that g be chosen such that ˇc is distributed
according to the prior on c. Rather, we require only (1) that Ξc = g−1(C), the
inverse image of the support of the prior, be contained within the range of ˇξ,
and (2) that g be a diﬀeomorphism from Ξc to C.
Next, using Galerkin projection to solve the stochastic forward problem, we
obtain a PC representation for each component of the model output. Here Gi
is the i-th component of Gc, and ˜Gi( ˇξ) is its PC representation:
˜Gi( ˇξ) =
dikΨk( ˇξ).
The forward prediction ˜G obtained in this fashion is a function of ˇξ, and
is a polynomial chaos approximation of Gc(g( ˇξ)). Note that both of these
quantities are random variables, since ˇξ is a random variable. But ˜G can also
be evaluated with a deterministic argument c ; in this sense, ˜G is a polynomial
approximation of the deterministic forward model Gc ◦g.
We would like to use this approximation to replace Gc in the likelihood function L(c) ≡pη (d −Gc(c)):
L (g(ξ)) ≈˜L (ξ) ≡pη
Implicit in this substitution is the change of variables c = g(ξ), i.e., from
the input parameterization of Gc to the input parameterization of ˜G, enabled
because g satisﬁes conditions (1) and (2) above.
We write the change of variables in terms of the posterior expectation of an
arbitrary function f:
Eπcf = Eπξ(f ◦g)
where πc ≡p(c|d) is the posterior density on c-space, and πξ is the corresponding posterior density on ξ-space:
πξ(ξ) ∝L (g(ξ)) pc (g(ξ)) |det Dg(ξ)| .
Here, Dg denotes the Jacobian of g and pc is the prior density of c. Eliminating
the forward model from the likelihood function via (31) ﬁnally yields the
“surrogate” posterior density ˜πξ:
c In this exposition we have used ˇ to identify the random variables ˇc and ˇξ in order
to avoid confusion with deterministic arguments to probability density functions,
e.g., c and ξ below. Elsewhere, we revert to the usual notational convention and let
context make clear the distinction between the two.
pc (g(ξ)) |det Dg(ξ)|
pc (g(ξ)) |det Dg(ξ)| .
This distribution may be explored with any suitable sampling strategy, in
particular MCMC. Evaluating the density for purposes of sampling may have
negligible cost; nearly all the computational time may be spent in intrusive
spectral projection, obtaining the PC expansions in (30). Depending on model
nonlinearities, the necessary size of the PC basis, and the number of posterior
samples required, this computational eﬀort may be orders of magnitude less
costly than exploring the posterior via direct sampling. Accuracy of the surrogate posterior depends on the order and family of the PC basis, as well as
on the choice of transformation g—for instance, whether the distribution of ˇc
assigns suﬃcient probability to regions of C favored by the posterior on c. A
detailed discussion of these issues can be found in .
Gaussian processes, K-L expansions, RKHS, and regularization
There are important connections between Gaussian process priors and regularization penalties in the corresponding reproducing kernel Hilbert space
(RKHS) norm. These connections can be understood in terms of the spectral
expansion of the covariance kernel, and it is useful to review these relationships
in the present context.
The deﬁnition and properties of reproducing kernel Hilbert spaces are brieﬂy
reviewed in the Appendix. It is natural to think of a positive deﬁnite reproducing kernel K as a covariance kernel, and indeed any Gaussian process can
be associated with a RKHS. Let X(t), t ∈T be a centered Gaussian process
with covariance kernel K. If K has more than a ﬁnite number of non-zero
eigenvalues, realizations of X(t) are almost surely not in the corresponding
RKHS H(K) . d However, there exists an isometry between the two. In
particular, let H be the Hilbert space spanned by X(t): H = span{Xt, t ∈T }
with inner product ⟨Zi, Zj⟩= E[ZiZj] for Zi, Zj ∈H. It can be shown that H
is isometrically isomorphic to H(K) .
Bayesian estimates with Gaussian process priors may lie in the corresponding
RKHS, however . Consider the case of an inverse problem with Gaussian
process prior GP(0, K) on the unknown function f. Details of the likelihood
function and forward model are unimportant here. For simplicity, we assume
d As an example of a GP whose realizations are not in the RKHS, consider standard
Brownian motion. Sample paths are nowhere diﬀerentiable with probability one, but
members of the RKHS are diﬀerentiable almost everywhere, with square-integrable
derivatives.
that the prior covariance is completely known. Let fi denote the projection of
f onto the i-th K-L eigenfunction φi:
D f(s)φi(s)ds
where φi and λi are, as usual, eigenfunctions and eigenvalues of the linear
operator corresponding to K. According to (11), the prior distribution on
each fi is N(0, λi). Then the posterior probability of the function f, π(f), has
J = −log π(f) = −log-likelihood +
= . . . + 1
H(K) + . . .
J is thus a RKHS-norm penalized cost functional, in which the Gaussian
process prior provides the regularization penalty. Minimizing J to obtain the
MAP estimate of f is equivalent to ﬁnding a Tikhonov-regularized solution
to the inverse problem, with argmin J = fMAP ∈H(K). Changing the prior
covariance kernel amounts to changing the RKHS norm and thus the nature
of the regularization penalty.
Moreover, there is an equivalence between the RKHS regularization functional
∥f∥H(K) and a standard L2(D)-norm penalty ∥Lf∥2 containing the diﬀerential
operator L: the reproducing kernel K is the Green’s function of the operator
L∗L, where L∗denotes the adjoint of L . Thus a Gaussian kernel leads
to a penalty on derivatives of all orders; the exponential covariance kernel
penalizes the square of the function value f(s) and its derivative ˙f(s) (i.e., a
Sobolev H1 norm); and the covariance kernel of Brownian motion, K(s, t) =
min(s, t), leads to a penalty on the MAP estimate’s squared derivatives.
Finally, we note that the present scheme of K-L based inversion (§2.4) recalls
the “weight space” view of Gaussian process regression , in that we ﬁnd
weights on the set of feature vectors φk implied by the Gaussian process prior.
Numerical implementations and results
We explore the accuracy and eﬃciency of our dimensionality reduction approach by estimating inhomogeneous diﬀusivity ﬁelds in a transient diﬀusion
problem. We pursue these inverse problems both with and without the added
step of solving the stochastic forward problem to construct a surrogate posterior (§2.5). In particular, we consider a diﬀusion equation on the unit interval
D = with adiabatic boundaries:
[1 −H(t −Ti)]
u(x, t = 0) = 0.
The source term in (37) involves N localized sources, each active on the interval t ∈[0, Ti] and centered at li ∈D with strength si and width σi, i = 1 . . . N.
Source parameters are prescribed, and we infer ν(x) from noisy measurements
of the u-ﬁeld at a ﬁnite set of locations and times. This problem can be considered a prototype for the inverse estimation of an inhomogeneous conductivity
ﬁeld or any analogous material or transport property, such as the permeability
ﬁeld in a porous medium .
Inverse problem setup
The transient diﬀusion equation above may be cast as a forward model that
predicts the value of the ﬁeld at speciﬁc locations and times. Taking the diﬀusivity to be uniformly bounded away from zero, ν(x) > ν0 > 0, we deﬁne the
log-diﬀusivity M(x) ≡log[ν(x) −ν0]; this function is the input to the forward
model. We evaluate the ﬁeld at mn points {u(xi, tj) : 1 ≤i ≤m, 1 ≤j ≤n}.
The “sensor locations” {x1 . . . xm} are uniformly spaced on D, including the
endpoints, and the measurement times {t1 . . . tn} are uniformly spaced on an
arbitrary time interval. Below, unless otherwise speciﬁed, we use m = 13
sensors, n = 9 measurement times, and N = 3 sources. The source locations are staggered with respect to the sensors; i.e., they are placed at
li ∈{0.25, 0.50, 0.75}. We prescribe identical strengths si = 100, shutoﬀtimes
Ti = 0.01, and widths σ2
i = 10−3 for all three sources. We ﬁx ν0 = 0.1. Measurements take place over the time interval t ∈[0.01, 0.03].
The u-ﬁeld is described on a uniform grid with spacing h = 1/48. Second-order
centered diﬀerences are used to discretize the diﬀusion terms. Time integration is via an explicit, second-order-accurate, Runge-Kutta-Chebyshev (RKC)
scheme with ∆t = 10−4. For any input ν(x), the number of substeps in the
RKC scheme is automatically determined by stability constraints upon setting
ϵ, the damping parameter that controls the extent of the stability region, to
2/13 . Numerical resolution studies were conducted to validate the present
choices of h and ∆t.
Note that the forward model is nonlinear in ν(x). Consider the simple case of a
uniform diﬀusivity, ν(x) = ¯ν. Figure 1 shows the resulting forward maps, from
log[¯ν−ν0] to u, at a single measurement location and two successive times. The
measurement location, x∗= 1/6, is adjacent to a source at x = 1/4. For very
small diﬀusivities, the scalar introduced by the source does not diﬀuse towards
the sensor in appreciable quantity, and hence u is small; in this regime, the
magnitude of the scalar ﬁeld rises with t and with ¯ν. At very large diﬀusivities,
the scalar introduced by all N sources rapidly diﬀuses towards all the sensors
and the u-ﬁeld quickly becomes uniform, approaching u =
i siTi = 3 as
¯νt →∞. For intermediate diﬀusivities, the measured value of u may decrease
with rising ¯ν: the scalar ﬁeld, locally peaked at the nearby source, ﬂattens as
the diﬀusivity increases, until the inﬂuence of the remaining sources is felt at
suﬃciently high ¯ν, raising the local value of u once again. The behavior of
analogous forward maps in the case of nonuniform ν(x) is expected to be even
more complicated.
The inverse problem thus consists of inferring M(x) ≡log[ν(x) −ν0] from
noisy measurements of u(xi, tj). In the Bayesian setting, we provide statistical
information about the measurement process and about our prior knowledge of
M(x). We let independent zero-mean Gaussian random variables ηi ∼N(0, ς2)
express the diﬀerence between “real-world” measurements and model predictions, as speciﬁed in (18). In the examples below, we choose ς = 0.1. We endow
M(x) with a zero-mean Gaussian process prior M ∼GP(0, C), where C is a
stationary Gaussian covariance kernel:
Cθ(x1, x2) = ˜Cθ(|x1 −x2|) = θ exp
−|x1 −x2|2
For simplicity, we assume that the correlation length L is known; in practical
applications, an estimate of L will often be available . We do not, on
the other hand, presume to know the scale θ of the prior covariance. Adopting
a fully Bayesian approach, we let θ be a hyperparameter endowed with a
conjugate inverse gamma hyperprior, θ ∼IG(α, β) :
Γ(α)θ−α−1 exp
In the examples below, we ﬁx the shape parameter α = 1 and the scale parameter β = 1. This yields a proper but long-tailed prior for θ, with undeﬁned
mean and variance. The magnitude of the prior covariance θ joins the remaining parameters describing M(x) in the joint posterior density; we can then
marginalize over θ to obtain a posterior describing M(x) alone (see §2.3).
Note that when considering MAP estimates of M conditioned on θ, the ratio
ς2/θ is akin to the regularization parameter appearing in deterministic inversion; thus we are eﬀectively estimating the strength of the regularization when
conditioning on the data .
In §§3.2–3.4, we will solve the inverse problem for four diﬀerent “target pro-
ﬁles” M(x): a simple linear proﬁle, a sinusoidal proﬁle, a proﬁle randomly
drawn from the Gaussian process prior with L = 0.3, θ = 1.0, and a wellshaped proﬁle. An additional target proﬁle is introduced in §3.6. For each
proﬁle, a noisy data vector d is generated by solving the deterministic forward problem with the target log-diﬀusivity, then perturbing the resulting
value of u at each sensor location/time with independent samples of Gaussian
noise ηi ∼N(0, ς2). To avoid an “inverse crime” , we generate the mn values
of u(xi, tj) by solving the forward problem at a much higher resolution than
that used in the inversion, i.e., with h = 1/408 and a correspondingly ﬁner
Grid-based inversion
We begin with a straightforward full-dimensional Bayesian approach to the
inverse problem, as described at the start of §2.4. Let M(x) be represented
on a ﬁnite collection of points {xi}n
i=1 ∈D; an obvious choice with adequate
resolution is the collection of grid points used to discretize the forward model,
uniformly spaced on the unit interval with xi+1 −xi = h. Then we can write
both the prior and posterior densities in terms of m = (M(x1), . . . , M(xn)):
p(m, θ|d) ∝p(d|m) × p(m|θ) × p(θ)
∝pη (d −G (m)) × θ−n
−[d −G(m)]T [d −G(m)]
× θ−α−1 exp
where (Σθ)ij ≡Cθ(xi, xj).
Directly applying a Metropolis-Hastings algorithm to this posterior, however,
is not likely to be successful. Simple proposal distributions for m, such as
normal distributions centered at the current position of the chain, generate
candidate points with very low acceptance probabilities—even when applied
component-at-a-time . These proposals do not account for correlations
among neighboring components of m. We surmount this issue with a change
of variables, using the Cholesky factorization of the prior covariance matrix,
using θ = 1: Σ(θ=1) = LLT. If z is vector of n i.i.d. standard normal random
variables, then, ∀θ,
will have a zero-mean multivariate normal distribution with covariance Σθ.
(Multiplication by L is analogous to, in the continuous case, generating samples of a Gaussian process by convolution with white noise .) Thus the
N(0, Σθ) prior distribution on m reduces to a Gaussian prior on z with diagonal covariance, N(0, I). Equivalently, we can write m = Lz and let the scale
parameter θ control the prior variance of z ∼N(0, θI), thus reparameterizing
the posterior density as follows:
p(z, θ|d) ∝exp
−[d −G(Lz)]T [d −G(Lz)]
× θ−α−1 exp
We use a Metropolis-Hastings algorithm to simulate samples from this distribution . For the scale parameter θ, we apply Gibbs updates: the full
conditional p(θ|z, d) is proportional to IG(α + n/2, β + (
i )/2) , so
we sample directly from this distribution with acceptance probability 1. For
the remaining parameters z, we use single-component random-walk Metropolis updates: each proposal distribution q(·|·) is a univariate normal centered on
the current position of the chain. It may be possible to increase the eﬃciency of
this sampler by using single-component updating for the ﬁrst few components
of z and block updating for the higher-index, less important components ,
but we do not pursue such ﬁne-tuning here. MCMC yields a series of samples
{(z(s), θ(s))}, which are easily transformed to {(m(s), θ(s))}. From these samples, we can estimate posterior expectations (e.g., means, variances, higher
moments), extract marginal densities p(M(xi)|d), and estimate quantiles of
the marginal distributions.
Figures 2–4 show the results of grid-based inversion with several target proﬁles.
Since we are not inferring the prior correlation length, in each case we have
assumed a value that reﬂects characteristic length scales of the target: L = 1.0
for the linear proﬁle (not shown), L = 0.2 for the sinusoidal proﬁle, L =
0.1 for the well-shaped proﬁle, and L = 0.3 for the proﬁle corresponding
to a random draw from the GP prior. Figures 2(a)–4(a) show the posterior
mean and standard deviation, along with ﬁve samples from each posterior
distribution. In part (b) of these ﬁgures, we extract one-dimensional marginal
distributions of M(x) at each grid point xi, then plot the median and 5% and
95% quantiles of the distributions. Even though statistical dependence among
diﬀerent spatial locations has been marginalized away, these proﬁles reﬂect
an envelope of uncertainty in the inverse solution. In all cases, uncertainty in
the log-diﬀusivity is greatest near the boundaries, with some additional rise
near the center of the domain. All of the results presented here are based on
6×105 MCMC samples; we ﬁnd negligible change in the estimated moments
and quantiles with further iterations.
In three of the four cases above (the linear, sinusoidal, and random-draw
targets), the posterior mean and median are good estimates of the true proﬁle;
the true log-diﬀusivity is generally contained within the credibility intervals
bounded by the marginal quantiles. Mismatch with the true proﬁle may be
ascribed to limited sensor resolution (in space and in time), noise in the data,
and the interaction of these conditions with the physics of the forward model.
Reducing the noise in the data or introducing more ﬁnely-spaced sensors then
yields closer agreement with the true proﬁle. This is demonstrated in Figure 6,
which compares the posterior mean, conditioned on several diﬀerent data sets,
to the true random-draw target proﬁle.
With the well-shaped target (Figure 4), however, the inferred proﬁle is smoother
than the true proﬁle. While the location of the well (0.4 < x < 0.7) may be
surmised from the posterior, the true proﬁle does not fall entirely within the
marginal quantiles. Here, information encoded in the prior is actually inconsistent with the well-shaped log-diﬀusivity. Even with a small correlation length,
a GP prior with a Gaussian covariance encodes signiﬁcant smoothness, assigning very small probability to sharp variations. The posterior distribution
reﬂects this belief in the character of the log-diﬀusivity proﬁle. To obtain more
appropriate reconstructions and credibility intervals in this case, the prior distribution must be chosen more carefully. Tarantola suggests that if discontinuities are expected, their geometric properties should enter explicitly into
the parameterization of the inverse problem. One may also construct structural priors, typically Gaussian but not isotropic or stationary, that encode
the location and geometry of non-smooth features .
Since the full posterior is a distribution on n + 1-dimensional space, it contains much more information than can be shown in Figures 2–4. Consider, for
instance, the change in the covariance of M(x) from the prior to the posterior.
Computing Var[m] = Cov [M(xi), M(xj)] requires marginalizing over the hyperparameter θ. The prior marginal, with density p(m) =
p(m|θ)p(θ)dθ,
is a multivariate t-distribution; its covariance can be obtained analytically as
βΣ/(α −1) for α > 1. The posterior covariance is estimated numerically from
the MCMC samples. Figure 5(a) shows the prior covariance with L = 0.3;
in this case only, we put α = 3 and β = 2 so the magnitude of the marginal
prior covariance is well-deﬁned. Figure 5(b) shows the corresponding posterior
covariance, again with α = 3 and β = 2, conditioned on the noisy data vector
used to infer the random-draw target in Figure 2. The posterior covariance
clearly reﬂects a nonstationary process, and its overall scale is more than an
order of magnitude smaller than the prior covariance. The diagonal of the posterior covariance is analogous to the square of the standard deviation in Figure
2(a). Decay of the covariance away from the diagonal reﬂects the character of
spatial variation in the log-diﬀusivity proﬁles comprising the inverse solution.
It is important to note that, because the forward operator G is nonlinear,
the posterior distributions shown here (whether 1-D marginals or full joint
distributions) are not in general Gaussian or even symmetric.
Reduced-dimensionality inversion
Now we pursue a reduced-dimensionality solution of the inverse problem by
exploring the posterior distribution of the weights ck of a ﬁnite number of
K-L modes, as described in §2.4. First, we must determine the eigenfunctions
and eigenvalues appearing in the K-L expansions. For the Gaussian covariance kernel (38) on D = , there is no analytical solution for the spectral
decomposition of the corresponding integral operator. Instead, we solve the integral equation (8) numerically, using the Nystrom method with a Gauss-
Legendre quadrature rule and a LAPACK solver for the ﬁrst K eigenvalues
and eigenvectors of the resulting real symmetric matrix.
The hyperparameter θ is treated as in the previous section. The scale of the
covariance kernel does not aﬀect the eigenfunctions φk(x); it simply multiplies
the eigenvalues λk. Thus, we can compute the K-L expansion of M(x) ∼
GP(0, Cθ) while ﬁxing θ = 1, and let the hyperparameter control the prior
variance of the random variables ck, c ∼N(0, θI). The posterior density in
(21) is re-written as follows:
p(c, θ|d) ∝
p(d|c) p(c|θ) p(θ)
−(d −G (MK(c)))T (d −G (MK(c)))
× θ−α−1 exp
where the forward model G now maps functions M : D →R, representing
the log-diﬀusivity, to Rmn. MK(c) denotes the K-term K-L expansion (13)
evaluated at c:
MK(x; c) =
λkckφk(x).
MCMC simulation from the posterior distribution proceeds as in §3.2: randomwalk Metropolis updates are used for the parameters representing the ﬁeld
(here, c) and Gibbs updates are used for the hyperparameter θ. Figures 7–9
show the results of K-L-based inversion with various target proﬁles. Results
obtained with a suﬃciently large number of K-L modes K become indistinguishable from the grid-based inverse solutions. As expected, shorter prior
correlation lengths require a larger number of K-L modes for accurate inverse
solutions. Consider the remainder of the total prior variance integrated over
the domain D (15), i.e., 1−PK
k=1 λk, shown in Figure 10. This quantity decays
exponentially fast with increasing K, reﬂecting the decay of the eigenvalues of
the Gaussian covariance kernel (38), but eigenvalues corresponding to large-L
kernels decay more quickly than those corresponding to small L. Since the
distributions of ck are altered by conditioning on d, the relative importance
of the K-L modes changes in the posterior, but still decays at larger index.
Figure 11 compares MCMC estimates of the posterior moments of M(x), obtained via grid-based inversion, to MCMC-estimated posterior moments of
MK(x) obtained via K-L-based inversion with varying numbers of modes.
In particular, we compute the L2 distance between estimates of the posterior mean, ∥bµ (MK(x)) −bµ (M(x)) ∥2 = (
D |bµ (MK(x)) −bµ (M(x)) |2dx)1/2,
and the L2 distance between estimates of the posterior standard deviation,
∥bσ (MK(x)) −bσ (M(x)) ∥2. Diﬀerences between these posterior estimates at
ﬁrst fall rapidly with increasing K, but then plateau. The plateau region appears at smaller K for the large-L cases (e.g., the line proﬁle) and at larger K
for the small-L cases (e.g., the well proﬁle), and reﬂects the fact that diﬀerences between moments of the grid and K-L-based inverse solutions eventually
become comparable to the variability of the MCMC estimates themselves. Indeed, each realization of a Markov chain yields slightly diﬀerent estimates of
the posterior mean and standard deviation, and diﬀerences among these realizations account for continued “jitter” in the plateau regions. To illustrate the
spread in these estimates, we have plotted results from additional realizations
of the MCMC chain at K = 6, 8, and 10, for the random-draw target. (Each
realization corresponds to a distinct choice of random seed.) Diﬀerences in the
magnitude of the plateau region associated with each target proﬁle reﬂect the
fact that variance of an MCMC estimate is dependent on the variance of the
actual quantity being estimated ; the posterior associated with the wellshaped target, for instance, shows much larger variances than the posterior
associated with the linear target.
Further insight into the contribution of each K-L mode to the inverse solution is obtained by examining boxplots of the posterior marginals of the mode
strengths. In particular, we consider marginal densities of √λkck, the scaled
contribution of each K-L mode. The K-L eigenfunctions multiplied by these
factors each have an L2 norm of unity, and thus the relative importance of each
eigenfunction—e.g., each mode’s contribution to the mean and spread of the
posterior—is captured by the boxplots in Figure 12. Results are reported for
the random-draw and well-shaped targets. The horizontal line at the center of
each box marks the median of the posterior marginal p(√λkck|d); the extent of
each box marks the 25% and 75% quantiles of the posterior marginal; and the
vertical lines span the entire range of the MCMC samples. The importance of
each mode does not decrease strictly with k. For instance, K-L mode φL=0.3
contributes more to the posterior of the random-draw target than φL=0.3
and φL=0.3
(x); with the well-shaped target, mode φL=0.1
(x) contributes more
to the posterior than φL=0.1
(x). At suﬃciently large index, however, the (exponential) decrease of the λk takes over: variances of the mode strengths decrease
and the medians tend towards zero.
Spatial correlations are also well-reproduced by the reduced-dimensionality inverse solution. Consider contours of the posterior covariance Cov [M(x1), M(x2)],
shown in Figure 13. Solid lines are obtained via the grid-based inversion described in §3.3, while dashed lines represent the posterior covariance computed
with K K-L modes. Very close agreement is observed with increasing K. This
result may be somewhat surprising, as φk(x) are not eigenfunctions of the
posterior covariance and thus not an optimal basis for posterior in the sense
of a K-L representation. Nonetheless, a modest number of these modes is able
to capture the posterior covariance.
Eigenfunctions aside, the ability to reproduce the posterior covariance also
depends on the emergence of correlations in the joint posterior of the K-L
mode strengths ck. Figure 14 shows all of the one- and two-dimensional posterior marginals of (θ, c) conditioned on the random-draw target. Signiﬁcant
correlations are apparent among the lower-indexed modes, and between these
modes and the hyperparameter θ. Higher-indexed modes, on the other hand,
appear uncorrelated—and based on the shape of their 2-D marginals, mutually independent. We examine the 1-D marginals of these modes more closely
in Figure 15. The solid lines are conditional densities p(ck|d, θ) extracted from
the posterior via kernel density estimation and plotted for diﬀerent values of
θ. The dashed lines are the corresponding conditional prior densities p(ck|θ);
recall that these are simply ck ∼N(0, θ). The posterior densities of c6, shown
in Figure 15(a), are shifted and somewhat narrower than their priors. (Much
more dramatic departures from the prior are observed for c1 through c5.) Conditional posteriors of c8 in Figure 15(b), on the other hand, match the prior
conditionals quite closely. Similarly close matching is observed for modes c9,
c10, and so on. This pattern leads us to conjecture that, at suﬃciently large
m, the posterior distribution of K-L modes ck≥m approaches the prior:
p(cm, cm+1, . . . |d, θ) →
This conjecture is consistent with the decay of λk at large k; since higher-index
modes have smaller λk, these modes should have less impact on the predicted
u(x, t) and on the data d. Absent conditioning on the data, these modes revert
to being independent and conditionally Gaussian. §3.5 revisits this issue in the
case of coarser data.
A practical beneﬁt of using K-L modes to compute the inverse solution is more
eﬃcient MCMC sampling. Figure 16(a) plots γ(s)/γ(0), the empirical autocorrelation at lag s, for several components of the Markov chain used to explore
p(z, θ|d) (i.e., grid-based inversion) and several components of the chain ex-
ploring p(c, θ|d) (K-L-based inversion). In both cases, the noisy data vector
d is obtained from the random-draw target. With the grid-based posterior,
lower-index zi multiply columns nearer the left side of the Cholesky factor L,
which contain a larger number of non-zero entries (speciﬁcally, n−i+1). These
modes mix less eﬃciently than their larger-i counterparts, even though we have
individually tuned the proposal width of each single-component Metropolis update. Mode strengths ck and the hyperparameter θ of the K-L-based posterior
show much more rapid decay of their autocorrelations, reﬂecting improved
mixing. In Figure 16(b), we transform the ck and zi into realizations of M(x)
and compare the chain autocorrelations at particular spatial locations. With
the grid-based posterior, mixing improves toward the right side of the physical
domain D, as the value of the solution at larger x is inﬂuenced by a larger
number of modes zi—and in particular, better-mixing modes. The autocorrelations of K-L-based M(xi) still decay more rapidly, however.
Reduced-dimensionality inversion with polynomial chaos acceleration
We now construct a stochastic forward problem whose solution captures the
output of the deterministic forward model over the support of the prior distribution, and use this solution to formulate a surrogate posterior distribution. The resulting scheme is intended to achieve an accelerated, reduceddimensionality Bayesian solution of the inverse problem, as described in §2.5.2.
We begin with the K-L representation of the log-diﬀusivity ﬁeld and the hierarchical priors on ck derived in the previous section. Following the procedure in
§2.5.2, we must introduce an invertible/diﬀerentiable transformation c = g(ξ),
where ξ is a vector of i.i.d. random variables underlying a PC basis. Here, we
let g be a simple scaling transformation
c = g(ξ) = ϖξ
where ξ is a K-vector of independent standard normal random variables and
ϖ is a free parameter to be speciﬁed. This transformation is just a ﬁrstorder Gauss-Hermite PC expansion, deﬁning the uncertainty that we propagate through the forward model. Since the prior distribution of each ck has
support over the entire real line, as does the N(0, 1) distribution of each ξk,
we have considerable freedom in choosing ϖ > 0; any choice of ϖ will map
the range of ξ onto the range of c . The choice is particularly open since θ,
the prior variance of ck, is itself unknown. Here we will ﬁx ϖ2 = 0.5, which is
the mode of the hyperprior p(θ).
Together, (37), (44), and (46) deﬁne a stochastic forward problem. The input
is the Gaussian random ﬁeld M(x) ≡log[ν(x, ξ(ω)) −ν0] represented with a
truncated K-L expansion, and the outputs are random variables u(xi, tj, ω)
giving the value of the scalar ﬁeld at each measurement location and time. We
write these random variables in terms of their PC expansions u(xi, tj, ω) =
l Ψl(ξ(ω)), and collect them in a vector ˜G(ξ) ≡(u(x1, t1; ξ), . . . , u(xm, tn; ξ)).
Solving the stochastic forward problem—i.e., using Galerkin projection to
compute the coeﬃcients uij
l —requires transforming the input log-diﬀusivity
into an actual diﬀusivity ν(x, ω). The Gauss-Hermite PC representation of
this log-normal process may be evaluated analytically . Recall that the
multivariate polynomial functionals comprising the PC basis are given by the
tensor product of one-dimensional polynomials ψi(ξ), here Hermite polynomials of order i. Each multivariate polynomial Ψl is associated with a multi-index
1, . . . , αl
K) ∈NK, where PK
k=1 αk ≤p:
The diﬀusivity ν(x, ω) = ν0 + exp(MK(x, ξ(ω)) then has a PC expansion
ν(x, ω) = PP
l eνl(x)Ψl(ξ) with coeﬃcients
eνl=0(x) = ν0 + eσ2(x)/2
eνl≥1(x) = eσ2(x)/2
σ2(x) = ϖ2
This PC expansion is introduced into the transient diﬀusion equation (37).
Using a pseudospectral stochastic Galerkin scheme coupled with the same
ﬁnite-diﬀerence spatial discretization and RKC time integrator as in the deterministic problem, we obtain PC expansions for the outputs of interest ˜G(ξ).
The surrogate posterior density may then be written in terms of ξ:
e Note that as K →∞, we have σ2(x)/ϖ2 →1.
p(ξ, θ|d) ∝
˜L(ξ) pc|θ(g(ξ)|θ) |det (Dg(ξ))| p(θ)
p(d|ξ) pξ|θ(ξ|θ) p(θ)
× θ−α−1 exp
MCMC sampling from this posterior proceeds as in the previous two sections,
except that the full conditional p(θ|ξ, d) used for Gibbs updates is now IG(α+
K/2, β + ϖ2(PK
A useful diagnostic of the stochastic forward solution’s ﬁdelity is the probability density of the forward model outputs u(xi, tj, ξ). Fixing the number
of terms K in the K-L expansion of M(x), these densities may be estimated
in one of two ways. A direct (and computationally expensive) method is to
sample c and solve the forward problem for each sample, forming a histogram
or kernel density estimate from the resulting collection of forward model outputs. Alternatively, one can sample ξ and substitute it into the PC expansion
˜G(ξ), again forming a histogram of the resulting values. Figure 17 shows such
estimates at two measurement locations and times. While a lower-order PC
basis (p = 2) produces a somewhat poor density estimate, the probability density quickly converges to its true shape—obtained by the direct method—as
p increases. Reasonable agreement is obtained even at p = 4. Also, note that
these densities are not log-normal; their shapes reﬂect the nonlinearity of the
forward maps from ν(x) to u(xi, tj).
Figures 18–19 are based on inversion of the random-draw target proﬁle. We
ﬁx the number of K-L modes to K = 6, since this value provided accurate results in §3.3, and vary the order p of the PC basis used to solve the stochastic
forward problem. MCMC samples from the surrogate posterior (51) are then
transformed into realizations of M(x). Even at low order, the posterior mean,
standard deviation, and quantiles thus obtained are not far from their direct
counterparts; at p = 4 and p = 6, we ﬁnd that these summaries of the posterior
distribution are visually indistinguishable from the proﬁles in Figure 7. Figure 18 quantiﬁes diﬀerences between the posterior means/standard deviations
obtained with 6 K-L modes and direct forward problem solutions, and those
obtained with 6 K-L modes and sampling of the surrogate posterior. Again,
we plot the L2 norm of the diﬀerences between MCMC estimates of these
quantities: ∥bµ (M p
K(x)) −bµ (MK(x)) ∥2 and ∥bσ (M p
K(x)) −bσ (MK(x)) ∥2. The
diﬀerence in posterior mean estimates drops more than an order of magnitude
from p = 2 to p = 4 and continues to fall towards p = 6. At both p = 4 and
p = 6, we plot results from four separate realizations of the MCMC chain on ˜πξ
(corresponding to diﬀerent random seeds) in order to illustrate the variability
of the MCMC estimates. In this regime, as in the case of K-convergence in
§3.3, the distance between estimated means of the direct and surrogate posteriors becomes comparable to the standard deviations of the MCMC estimates
themselves. (Both Figure 18 and Figure 11(a) show plateaus around 10−3 for
the random-draw target.) Diﬀerences between the estimates of posterior standard deviation show similar dependence on p.
The surrogate posterior accurately captures spatial correlations among possible values of the inverse solution. Figure 19 shows contours of the posterior
covariance with varying p: solid lines correspond to the direct posterior, while
dashed lines represent the surrogate posterior. Very close agreement is observed at p = 4, and this agreement improves further at p = 6.
Of course, the ultimate goal of introducing the stochastic forward problem and
surrogate posterior is greater computational eﬃciency. Signiﬁcant speedups
were obtained with a similar approach in for inverse estimation of parameters in PDEs. In the present context, even though the inverse solution is a
spatial ﬁeld, the pattern of computational costs is the same as in . The
initial cost of the scheme is oﬀset by the computation of stochastic forward
solutions, but then grows very slowly, because the cost per MCMC iteration
is orders of magnitude smaller for the surrogate posterior (51) than for direct solutions of the transient diﬀusion equation (43). Table 1 summarizes the
cost at each stage of three representative calculations: inferring the randomdraw target by the methods of §3.2, §3.3, and §3.4. For a ﬁxed number of
MCMC iterations, K-L based inversion with K = 6 is approximately one
order of magnitude faster than grid-based inversion, because fewer posterior
evaluations are required per MCMC iteration in the former case (using the
single-component MCMC sampler). It may be possible to design more eﬃcient
MCMC methods for both cases, perhaps updating blocks of components at a
time with suitably shaped proposals, but reducing the chain dimension will
inevitably enable greater eﬃciency and reduce the number of posterior
evaluations required. We also note that comparing the grid-based and K-L
based solutions at a ﬁxed number of MCMC iterations is not truly fair to the
K-L parameterization, since the (c, θ) chain mixes more rapidly than the (z, θ)
chain (see Figure 16). The Monte Carlo error obtained with 2×105 MCMC
samples from p(c, θ|d) is thus matched at a larger number of MCMC samples
from p(m, θ|d).
The third row of Table 1 shows even greater speedups, entirely independent of
the MCMC implementation. The majority of the computational time in this
case is spent on the stochastic forward solve. Yet we emphasize that, because it
depends only on the prior and forward model, the stochastic forward solve may
be performed “oﬄine” before introducing any data. Afterwards, sampling is
inexpensive. Here, the cost per posterior evaluation and per MCMC iteration
is 1.8 orders of magnitude smaller than for the direct K-L based posterior.
Including the time for the stochastic forward solve, inference via exploration
of the surrogate reduced-dimensionality posterior is 2.3 orders of magnitude
faster than exploration of the direct full-dimensional posterior, with negligible
loss of accuracy.
Data length scales
It is useful to consider the behavior of the inverse solution as one coarsens the
length scale on which data is collected. For instance, what if observations of
the scalar ﬁeld u(x, t) were limited to the boundaries of the domain, x = 0.0
and x = 1.0? We still take noisy measurements at 9 successive times, spaced
uniformly over the interval t ∈[0.01, 0.03]. Figure 20 shows the inverse solution
corresponding to the sinusoidal target, obtained with K = 10 K-L modes and
direct forward problem solutions as described in §3.3. Contrast these results
with those in Figure 8 or Figure 3. The two-sensor results show much greater
variability, particularly in the center of the domain, compared to their 13sensor counterparts. The posterior mean and median still appear sinusoidal,
along with the majority of the posterior realizations. All of these proﬁles are
closest to the true solution near the domain boundaries. Asymmetry in the
standard deviation proﬁle may be ascribed to asymmetry in the realizations
of the sensor noise perturbing observations of u(x, t).
Examining the posterior distribution of M(x) does not complete the story,
however. In particular, the posterior distributions of the K-L mode strengths
ck exhibit interesting features when conditioned on coarser data. Figure 21
shows boxplots of the marginal posteriors of ck and θ, contrasting 13-sensor
and 2-sensor inference of the sinusoidal target. First, we note that the hyperparameter θ and the mode strengths ck—particularly the lower-index ck—have
narrower posterior distributions in the data-rich case. As discussed in §3.3,
higher-index modes approach a zero-mean limiting distribution, but crucially,
this tendency is observed at much lower k in the 2-sensor case. To further elucidate this point, consider the matrix of one- and two-dimensional posterior
marginals, shown in Figure 22 for two sensors. The marginal distributions of
c6, c7, . . . do indeed appear quite similar in shape and range, and moreover,
correlations among the modes weaken at larger k, becoming quite negligible
for k > 6. We examine the limiting distributions quantitatively as in §3.3,
plotting posterior conditionals p(c6|d, θ) in Figure 23. The posterior conditionals of c6 in the 13-sensor case are far from the corresponding Gaussian
prior conditionals p(c|θ), but in the the 2-sensor case p(c6|d, θ) matches the
Gaussian prior N(0, θ) quite closely for various values of θ. Even closer matching is observed at higher k. These observations lead us to expand upon the
conjecture of (45), by suggesting that the posterior distribution of K-L modes
ck≥m approaches the prior at smaller index m as the length scale of the data
is coarsened.
Implications of this statement are numerous. One possibility is that when the
data is scarce relative to the complexity of the model (understood here as
the spatial complexity of log ν(x) as constrained by the prior), dimensionality reduction based on the prior may be improved upon. For instance, in
the present case of 2-sensor inversion of the sinusoidal proﬁle, the fraction
of the posterior standard deviation contained in modes 6 and higher (integrated over D) is 14.1%. These modes are thus important to the posterior
distribution, but if their joint distribution is essentially unchanged by conditioning on the data, the corresponding ck could be removed from the inference
process. Adding unconditioned realizations of P∞
√λkckφk(x) to posterior
realizations of MK=5(x) would then yield samples from the full posterior.
Prior and posterior hyperparameters
As a further numerical demonstration, we use the combined PC/K-L formulation to infer a log-diﬀusivity proﬁle that, unlike the random-draw target, has
no particular relationship to the Gaussian process prior. At the same time,
we treat the prior mean µ0, heretofore ﬁxed at zero, as a hyperparameter and
more closely analyze the posterior distributions of the hyperparameters µ0 and
The new target log-diﬀusivity is obtained by integrating the three-dimensional
Lorenz system at parameter values (¯σ = 10, ¯β = 8/3, ¯ρ = 28) that produce
chaotic behavior. f A portion of the trajectory of the second state variable y
is extracted, rescaled to the D = domain, and translated so that it has
non-zero mean on D. We emphasize that the this procedure is simply a device
for producing a “generic” continuous target proﬁle without any particular
symmetries; the dynamics of the Lorenz system in no way enter the inference
procedure.
Treating the prior mean µ0 as a hyperparameter entails only small modiﬁcations to the inference procedure described in §3.4. First, µ0 must be assigned a hyperprior; we use a standard normal, µ0 ∼N(0, 1). The mean then
f The Lorenz system is governed by the following ODEs:
dx/dt = ¯σ (y −x)
dy/dt = x (¯ρ −z) −y
dz/dt = xy −¯βz.
appears as the ﬁrst term in the K-L expansion of M(x, ω): MK(x; µ0, c) =
√λkckφk(x). Since this mean is now uncertain, we must add an
additional stochastic dimension to the PC basis in order to solve the stochastic forward problem. A K-term K-L expansion now implies K + 1 stochastic
dimensions. The scaling transformation in (46) can be partitioned as follows:
ci = ϖ1ξi, i = 1 . . . K
with ξ ≡(ξ0, ξ1, . . . , ξK). The PC coeﬃcients of the log-normal diﬀusivity
ν(x, ω) are still obtained analytically with a simple modiﬁcation to (49) and
(50). To these equations, we add a k = 0 term with φ0(x) = 1, λ0 = 1, and
ϖ = ϖ0. The joint posterior density may then be written as:
p(ξ0, ξ1, . . . , ξK, θ|d) ∝exp
× θ−α−1 exp
MCMC sampling from the posterior proceeds as in §3.4, with Gibbs updates
for θ and Metropolis updates for ξ. Since the hyperprior on ξ0 is independent
of θ, the full conditional p(θ|ξ, d) is unchanged.
Results from inversion of the “Lorenz target” proﬁle are shown in Figure 24.
As before, data are obtained from 13 sensors with noise standard deviation
ς = 10−1, and with the prior correlation length ﬁxed at L = 0.2. For simplicity,
we put ϖ0 = ϖ1. The true proﬁle, shown in black, is generally contained within
the marginal quantiles. The posterior standard deviation is, as usual, largest
near the edges of the domain. These results are obtained with 10 K-L modes
(the same number previously used for L = 0.2) and a PC basis of order p = 4.
To verify the accuracy of this inversion, Figure 25 directly compares the posterior mean obtained with the combined K-L/PC approach to that obtained with
the full-dimensional grid-based approach of §3.2. The grid-based approach is
applied to identical data from the Lorenz target, retaining the hyperpriors on
µ0 and θ. Mean proﬁles obtained with the two approaches are nearly indistinguishable. We perform this comparison not only for the 13 sensor, ς = 10−1
case of Figure 24, but for a “ﬁner data” case in which 25 sensors are evenly
distributed throughout the domain, each with noise magnitude ς = 10−2. Results obtained with ﬁner data preserve the extremely close agreement between
the K-L/PC and grid-based inversions. Moreover, ﬁner data yields much closer
reconstruction of the true Lorenz target proﬁle.
The impact of the prior-mean hyperparameter can be understood by examining the posterior marginals of µ0 and the ﬁrst few K-L mode strengths ci (or
equivalently, ξi). These one- and two-parameter marginal densities are shown
in Figure 26, for 13-sensor inversion of the Lorenz target with the K-L/PC
approach. Very tight negative correlation between µ0 and ξ1 is apparent. This
relationship is not surprising, as the ﬁrst K-L mode φ1(x) is the ﬂattest; it
contributes nearly the same shape to the solution proﬁle as does the mean.
Changing the weight on this mode shifts the log-diﬀusivity up and down in
much the same way as changing the prior mean. Additional correlation is
observed between µ0 and ξ3; φ3(x) is the next even eigenfunction, and this dependence accounts for deviation of φ1(x) from the ﬂat mean near the edges of
the domain. Similar posterior correlations are observed in the 25-sensor, small
sensor noise case. Because of these strong correlations, inclusion of the prior
mean as a hyperparameter does not have a signiﬁcant impact on posterior
proﬁles of M(x) in these cases.
The posterior marginal of the prior-variance hyperparameter θ shows interesting changes with the data. In Figure 27 we contrast the posterior density
p(θ|d), in both the 13-sensor/larger-noise and 25-sensor/smaller-noise cases,
with the inverse gamma hyperprior p(θ) (39). Both posteriors diﬀer signiﬁcantly from the hyperprior. The 13-sensor case favors lower values of θ, while
the ﬁner-data case favors higher values of θ. This result is borne out in the
posterior distribution of the K-L mode strengths ξi (equivalently ci). Finer
data, and thus closer reconstruction of the Lorenz target, move the posterior
distributions of the higher-index K-L modes further away from zero. Recall
that the full conditional p(θ|ξ, d) is an inverse gamma distribution with scale
parameter equal to β + ϖ2(PK
k)/2). As the magnitude of P
k increases,
the scale parameter grows, pushing the posterior distribution of θ towards
higher values. The strength of the regularization, inversely proportional to θ,
therefore decreases as the quality of the data increases. Notably, this result
emerges automatically from the hierarchical Bayesian formulation.
Conclusions
This paper pursues a Bayesian approach to inverse problems in which the unknown quantity is a spatiotemporal ﬁeld. Regularization enters the formulation
through a hierarchical Gaussian process prior, and the posterior distribution
provides a quantitative assessment of uncertainty in the inverse solution. We
address computational challenges associated with repeated evaluations of a
complex forward model, and with high dimensionality of the posterior.
We introduce a Karhunen-Lo`eve expansion of the prior Gaussian process in
order to parameterize the unknown ﬁeld with fewer degrees of freedom. This
representation also deﬁnes a stochastic forward problem whose solution approximates the deterministic forward model over the support of the prior. We
use Galerkin projection to solve the stochastic forward problem on a polynomial chaos basis. The polynomial representation of the forward model then
replaces the full forward model in the likelihood function, yielding a reduceddimensionality surrogate posterior density that may be evaluated orders of
magnitude more quickly than the original posterior.
This approach is demonstrated on a transient diﬀusion equation with prescribed source terms, where our goal is to recover an inhomogeneous diﬀusivity ﬁeld from sparse and noisy observations of the solution. We compare three
Bayesian solution methods corresponding to three diﬀerent posterior distributions: a full-dimensional, grid-based posterior with direct forward model
evaluations (42); a posterior based on the K-L mode strengths, with direct
forward model evaluations (43); and a posterior based on transformation of
the K-L mode strengths to the variables underlying a PC representation of
the forward model (51). In each case, we apply a single-component MCMC
sampler with Metropolis updates for the ﬁeld parameters and Gibbs updates
for the scale of the prior covariance, treated as a hyperparameter. The prior
mean may also be treated as a hyperparameter. We invert for several “target”
proﬁles of the log-diﬀusivity, representing diﬀerent scales of spatial variation
and diﬀerent degrees of smoothness.
Our ﬁndings are summarized as follows.
• Moments and quantiles of the inverse solution calculated from the latter two
posterior distributions converge rapidly to those of the baseline case (gridbased, direct forward problem evaluations) as we increase the number of K-L
modes K and the PC basis order p. Shorter prior correlation lengths require
a larger number of K-L modes to be retained. Convergence of the PC-based
surrogate posterior to the baseline case may depend on the nonlinearity of
the forward operator, but very close agreement is obtained here at order
• The relative importance of the K-L modes changes from the prior to the
posterior, but still decays at large index, consistent with the smoothing
eﬀect of the prior.
• Both the lower-dimensional posterior and the PC-based surrogate posterior
accurately capture spatial correlations among possible values of the inverse
• The K-L parameterization allows for more eﬃcient MCMC sampling than
the full-dimensional parameterization.
• At suﬃciently high index, we ﬁnd that the posterior distribution of each
K-L mode reverts to the prior. With less data—in particular, with sensors
placed only at the boundaries—we observe greater posterior variability. The
weights of the K-L modes then revert to the prior distribution at lower index.
In these cases, certain modes may be relatively unaﬀected by the data, yet
make signiﬁcant contributions to the posterior variance.
• Measures of the speedup associated with the new scheme are quite promising. We observe nearly one order of magnitude in speedup, along with more
eﬃcient MCMC sampling, in moving from a grid-based parameterization of
the unknown ﬁeld to a truncated K-L parameterization, while retaining direct forward solutions. An additional 1–1.5 orders of magnitude of speedup
are then obtained by using the reduced-dimensionality surrogate posterior.
These ﬁgures include the time needed to solve the stochastic forward problem. In practical settings, this step may be performed oﬄine before observing any data.
Extensions to the basic methodology proposed here are numerous. For instance, more advanced Bayesian modeling may take the form of estimating the
prior correlation length L, here presumed known. If suﬃcient data are available, one could infer L by treating it as an additional hyperparameter; unlike
the prior variance or prior mean, this hyperparameter would aﬀect the K-L
eigenfunctions. Also, the K-L parameterization used here will apply equally
well to non-Gaussian priors, though in such a case, additional eﬀort may be
required to evaluate or sample from the non-Gaussian joint prior density on
the weights of the K-L modes.
Extension to multi-dimensional spatial domains or prior covariance kernels
with slower eigenvalue decay is straightforward in principle, but the resulting
stochastic forward problems may be larger and thus more computationally
challenging. Recent algorithmic developments enabling more eﬃcient solution
of stochastic PDEs with random-ﬁeld coeﬃcients would be useful to apply
within the present Bayesian inverse framework. These developments include
adaptive polynomial degree and sparse truncations of the PC basis ,
and other methods of stochastic model reduction . It is also crucial to
note that solution of the stochastic forward problem need not be restricted to
Galerkin methods: stochastic collocation , particularly on sparse grids ,
could be used as well. One may also explore partitioning the prior support,
as described in , and solving several smaller stochastic forward problems in
Finally, the basis on which we represent the unknown ﬁeld in this work has
been determined from the prior. An important implication is that the present
dimensionality reduction scheme requires some degree of correlation or structure in the prior; without the corresponding decay of eigenvalues in the K-L
expansion, the scheme would have no opportunity for truncation. On the other
hand, results in §3.5 suggest that features prominent in the prior may not always have a signiﬁcant eﬀect on the measurements, and that the forward model
and the data could therefore also be taken into consideration when pursuing
dimensionality reduction. In this sense, one would ultimately like to ﬁnd a
basis emphasizing features of the unknown ﬁeld that are most aﬀected by the
data ; such a basis may be yet more eﬃcient than the bases considered
here. Eﬀective means of computing such a basis, for nonlinear forward models
in a Bayesian framework, are a promising direction for further work.
Acknowledgments
This research was supported in part by an appointment to the Sandia National
Laboratories Truman Fellowship in National Security Science and Engineering, sponsored by Sandia Corporation (a wholly owned subsidiary of Lockheed
Martin Corporation) as operator of Sandia National Laboratories under US
Department of Energy contract number DE-AC04-94AL85000. Support was
also provided by the US Department of Energy, Oﬃce of Science, Oﬃce of
Basic Energy Sciences, Division of Chemical Sciences, Geosciences, and Biosciences.