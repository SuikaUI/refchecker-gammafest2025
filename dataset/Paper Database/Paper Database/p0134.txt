The Cityscapes Dataset for Semantic Urban Scene Understanding
Marius Cordts1,2
Mohamed Omran3
Sebastian Ramos1,4
Timo Rehfeld1,2
Markus Enzweiler1
Rodrigo Benenson3
Uwe Franke1
Stefan Roth2
Bernt Schiele3
1Daimler AG R&D, 2TU Darmstadt, 3MPI Informatics, 4TU Dresden
www.cityscapes-dataset.net
train/val – ﬁne annotation – 3475 images
train – coarse annotation – 20 000 images
test – ﬁne annotation – 1525 images
Visual understanding of complex urban street scenes is
an enabling factor for a wide range of applications. Object detection has beneﬁted enormously from large-scale
datasets, especially in the context of deep learning. For
semantic urban scene understanding, however, no current
dataset adequately captures the complexity of real-world
urban scenes. To address this, we introduce Cityscapes, a
benchmark suite and large-scale dataset to train and test
approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of
stereo video sequences recorded in streets from 50 different
cities. 5000 of these images have high quality pixel-level
annotations; 20 000 additional images have coarse annotations to enable methods that leverage large volumes of
weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness,
scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset
characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.
1. Introduction
Visual scene understanding has moved from an elusive
goal to a focus of much recent research in computer vision . Semantic reasoning about the contents of a scene
is thereby done on several levels of abstraction.
recognition aims to determine the overall scene category
by putting emphasis on understanding its global properties,
e.g. . Scene labeling methods, on the other hand,
seek to identify the individual constituent parts of a whole
scene as well as their interrelations on a more local pixeland instance-level, e.g. . Specialized object-centric
methods fall somewhere in between by focusing on detecting a certain subset of (mostly dynamic) scene constituents,
e.g. . Despite signiﬁcant advances, visual scene
understanding remains challenging, particularly when taking human performance as a reference.
The resurrection of deep learning has had a major
impact on the current state-of-the-art in machine learning
and computer vision. Many top-performing methods in a
variety of applications are nowadays built around deep neural networks . A major contributing factor to
their success is the availability of large-scale, publicly available datasets such as ImageNet , PASCAL VOC ,
PASCAL-Context , and Microsoft COCO that allow deep neural networks to develop their full potential.
Despite the existing gap to human performance, scene
understanding approaches have started to become essential components of advanced real-world systems. A particularly popular and challenging application involves selfdriving cars, which make extreme demands on system
performance and reliability. Consequently, signiﬁcant research efforts have gone into new vision technologies for
understanding complex trafﬁc scenes and driving scenarios . Also in this area, research progress
can be heavily linked to the existence of datasets such as
the KITTI Vision Benchmark Suite , CamVid , Leuven
 , and Daimler Urban Segmentation datasets. These
urban scene datasets are often much smaller than datasets
addressing more general settings. Moreover, we argue that
they do not fully capture the variability and complexity
of real-world inner-city trafﬁc scenes. Both shortcomings
currently inhibit further progress in visual understanding
of street scenes. To this end, we propose the Cityscapes
benchmark suite and a corresponding dataset, speciﬁcally
 
construction
1 instance-level annotations are available
2 ignored for evaluation
rail track2
guard rail2
motorcycle1
caravan1,2
trailer1,2
trafﬁc sign
trafﬁc light
pole group2
number of pixels
Figure 1. Number of ﬁnely annotated pixels (y-axis) per class and their associated categories (x-axis).
tailored for autonomous driving in an urban environment
and involving a much wider range of highly complex innercity street scenes that were recorded in 50 different cities.
Cityscapes signiﬁcantly exceeds previous efforts in terms of
size, annotation richness, and, more importantly, regarding
scene complexity and variability. We go beyond pixel-level
semantic labeling by also considering instance-level semantic labeling in both our annotations and evaluation metrics.
To facilitate research on 3D scene understanding, we also
provide depth information through stereo vision.
Very recently, announced a new semantic scene labeling dataset for suburban trafﬁc scenes. It provides temporally consistent 3D semantic instance annotations with
2D annotations obtained through back-projection. We consider our efforts to be complementary given the differences
in the way that semantic annotations are obtained, and in the
type of scenes considered, i.e. suburban vs. inner-city traf-
ﬁc. To maximize synergies between both datasets, a common label deﬁnition that allows for cross-dataset evaluation
has been mutually agreed upon and implemented.
2. Dataset
Designing a large-scale dataset requires a multitude of
decisions, e.g. on the modalities of data recording, data
preparation, and the annotation protocol. Our choices were
guided by the ultimate goal of enabling signiﬁcant progress
in the ﬁeld of semantic urban scene understanding.
2.1. Data speciﬁcations
Our data recording and annotation methodology was
carefully designed to capture the high variability of outdoor
street scenes. Several hundreds of thousands of frames were
acquired from a moving vehicle during the span of several
months, covering spring, summer, and fall in 50 cities, primarily in Germany but also in neighboring countries. We
deliberately did not record in adverse weather conditions,
such as heavy rain or snow, as we believe such conditions
to require specialized techniques and datasets .
Our camera system and post-processing reﬂect the current state-of-the-art in the automotive domain.
were recorded with an automotive-grade 22 cm baseline
stereo camera using 1/3 in CMOS 2 MP sensors (OnSemi
AR0331) with rolling shutters at a frame-rate of 17 Hz.
The sensors were mounted behind the windshield and yield
high dynamic-range (HDR) images with 16 bits linear color
depth. Each 16 bit stereo image pair was subsequently debayered and rectiﬁed. We relied on for extrinsic and
intrinsic calibration. To ensure calibration accuracy we recalibrated on-site before each recording session.
For comparability and compatibility with existing
datasets we also provide low dynamic-range (LDR) 8 bit
RGB images that are obtained by applying a logarithmic
compression curve. Such tone mappings are common in
automotive vision, since they can be computed efﬁciently
and independently for each pixel. To facilitate highest annotation quality, we applied a separate tone mapping to each
image. The resulting images are less realistic, but visually
more pleasing and proved easier to annotate. 5000 images
were manually selected from 27 cities for dense pixel-level
annotation, aiming for high diversity of foreground objects,
background, and overall scene layout. The annotations (see
Sec. 2.2) were done on the 20th frame of a 30-frame video
snippet, which we provide in full to supply context information. For the remaining 23 cities, a single image every 20 s
or 20 m driving distance (whatever comes ﬁrst) was selected
for coarse annotation, yielding 20 000 images in total.
In addition to the rectiﬁed 16 bit HDR and 8 bit LDR
stereo image pairs and corresponding annotations, our
dataset includes vehicle odometry obtained from in-vehicle
sensors, outside temperature, and GPS tracks.
2.2. Classes and annotations
We provide coarse and ﬁne annotations at pixel level including instance-level labels for humans and vehicles.
Our 5000 ﬁne pixel-level annotations consist of layered
polygons (à la LabelMe ) and were realized in-house
to guarantee highest quality levels. Annotation and quality
control required more than 1.5 h on average for a single image. Annotators were asked to label the image from back to
front such that no object boundary was marked more than
once. Each annotation thus implicitly provides a depth ordering of the objects in the scene. Given our label scheme,
construction
proportion
Our dataset
Figure 2. Proportion of annotated pixels (y-axis) per category (x-axis) for Cityscapes, CamVid , DUS , and KITTI .
annotations can be easily extended to cover additional or
more ﬁne-grained classes.
For our 20 000 coarse pixel-level annotations, accuracy
on object boundaries was traded off for annotation speed.
We aimed to correctly annotate as many pixels as possible
within a given span of less than 7 min of annotation time per
image. This was achieved by labeling coarse polygons under the sole constraint that each polygon must only include
pixels belonging to a single object class.
In two experiments we assessed the quality of our labeling. First, 30 images were ﬁnely annotated twice by different annotators and passed the same quality control. It
turned out that 96 % of all pixels were assigned to the same
label. Since our annotators were instructed to choose a void
label if unclear (such that the region is ignored in training
and evaluation), we exclude pixels having at least one void
label and recount, yielding 98 % agreement. Second, all our
ﬁne annotations were additionally coarsely annotated such
that we can enable research on densifying coarse labels. We
found that 97 % of all labeled pixels in the coarse annotations were assigned the same class as in the ﬁne annotations.
We deﬁned 30 visual classes for annotation, which are
grouped into eight categories: ﬂat, construction, nature,
vehicle, sky, object, human, and void. Classes were selected based on their frequency, relevance from an application standpoint, practical considerations regarding the annotation effort, as well as to facilitate compatibility with
existing datasets, e.g. . Classes that are too rare
are excluded from our benchmark, leaving 19 classes for
evaluation, see Fig. 1 for details. We plan to release our
annotation tool upon publication of the dataset.
2.3. Dataset splits
We split our densely annotated images into separate
training, validation, and test sets. The coarsely annotated
images serve as additional training data only. We chose not
to split the data randomly, but rather in a way that ensures
each split to be representative of the variability of different
street scene scenarios. The underlying split criteria involve
a balanced distribution of geographic location and population size of the individual cities, as well as regarding the
time of year when recordings took place. Speciﬁcally, each
of the three split sets is comprised of data recorded with the
#pixels 
annot. density [%]
Ours (ﬁne)
Ours (coarse)
Table 1. Absolute number and density of annotated pixels for Cityscapes, DUS, KITTI, and CamVid (upscaled to
1280 × 720 pixels to maintain the original aspect ratio).
following properties in equal shares: (i) in large, medium,
and small cities; (ii) in the geographic west, center, and east;
(iii) in the geographic north, center, and south; (iv) at the beginning, middle, and end of the year. Note that the data is
split at the city level, i.e. a city is completely within a single split. Following this scheme, we arrive at a unique split
consisting of 2975 training and 500 validation images with
publicly available annotations, as well as 1525 test images
with annotations withheld for benchmarking purposes.
In order to assess how uniform (representative) the splits
are regarding the four split characteristics, we trained a fully
convolutional network on the 500 images in our validation set. This model was then evaluated on the whole test
set, as well as eight subsets thereof that reﬂect the extreme
values of the four characteristics. With the exception of the
time of year, the performance is very homogeneous, varying
less than 1.5 % points (often much less). Interestingly, the
performance on the end of the year subset is 3.8 % points
better than on the whole test set. We hypothesize that this
is due to softer lighting conditions in the frequently cloudy
fall. To verify this hypothesis, we additionally tested on
images taken in low- or high-temperature conditions, ﬁnding a 4.5 % point increase in low temperatures (cloudy) and
a 0.9 % point decrease in warm (sunny) weather. Moreover, speciﬁcally training for either condition leads to an
improvement on the respective test set, but not on the balanced set. These ﬁndings support our hypothesis and underline the importance of a dataset covering a wide range of
conditions encountered in the real world in a balanced way.
2.4. Statistical analysis
We compare Cityscapes to other datasets in terms of (i)
annotation volume and density, (ii) the distribution of visual
Ours (ﬁne)
Table 2. Absolute and average number of instances for Cityscapes,
KITTI, and Caltech (1 via interpolation) on the respective training
and validation datasets.
classes, and (iii) scene complexity. Regarding the ﬁrst two
aspects, we compare Cityscapes to other datasets with semantic pixel-wise annotations, i.e. CamVid , DUS ,
and KITTI . Note that there are many other datasets
with dense semantic annotations, e.g. .
However, we restrict this part of the analysis to those with a
focus on autonomous driving.
CamVid consists of ten minutes of video footage with
pixel-wise annotations for over 700 frames. DUS consists
of a video sequence of 5000 images from which 500 have
been annotated. KITTI addresses several different tasks including semantic labeling and object detection. As no of-
ﬁcial pixel-wise annotations exist for KITTI, several independent groups have annotated approximately 700 frames
 . We map those labels to our
high-level categories and analyze this consolidated set. In
comparison, Cityscapes provides signiﬁcantly more annotated images, i.e. 5000 ﬁne and 20 000 coarse annotations.
Moreover, the annotation quality and richness is notably
better. As Cityscapes provides recordings from 50 different cities, it also covers a signiﬁcantly larger area than previous datasets that contain images from a single city only,
e.g. Cambridge (CamVid), Heidelberg (DUS), and Karlsruhe (KITTI). In terms of absolute and relative numbers
of semantically annotated pixels (training, validation, and
test data), Cityscapes compares favorably to CamVid, DUS,
and KITTI with up to two orders of magnitude more annotated pixels, c.f. Tab. 1. The majority of all annotated pixels
in Cityscapes belong to the coarse annotations, providing
many individual (but correlated) training samples, but missing information close to object boundaries.
Figures 1 and 2 compare the distribution of annotations
across individual classes and their associated higher-level
categories. Notable differences stem from the inherently
different conﬁgurations of the datasets. Cityscapes involves
dense inner-city trafﬁc with wide roads and large intersections, whereas KITTI is composed of less busy suburban
trafﬁc scenes.
As a result, KITTI exhibits signiﬁcantly
fewer ﬂat ground structures, fewer humans, and more nature. In terms of overall composition, DUS and CamVid
seem more aligned with Cityscapes.
Exceptions are an
abundance of sky pixels in CamVid due to cameras with a
comparably large vertical ﬁeld-of-view and the absence of
certain categories in DUS, i.e. nature and object.
number of trafﬁc participant instances per image
number of images
Cityscapes
Figure 3. Dataset statistics regarding scene complexity. Only MS
COCO and Cityscapes provide instance segmentation masks.
number of vehicles
Our dataset
Figure 4. Histogram of object distances in meters for class vehicle.
Finally, we assess scene complexity, where density and
scale of trafﬁc participants (humans and vehicles) serve as
proxy measures. Out of the previously discussed datasets,
only Cityscapes and KITTI provide instance-level annotations for humans and vehicles. We additionally compare
to the Caltech Pedestrian Dataset , which only contains
annotations for humans, but none for vehicles. Furthermore,
KITTI and Caltech only provide instance-level annotations
in terms of axis-aligned bounding boxes. We use the respective training and validation splits for our analysis, since test
set annotations are not publicly available for all datasets.
In absolute terms, Cityscapes contains signiﬁcantly more
object instance annotations than KITTI, see Tab. 2. Being a specialized benchmark, the Caltech dataset provides
the most annotations for humans by a margin. The major
share of those labels was obtained, however, by interpolation between a sparse set of manual annotations resulting in
signiﬁcantly degraded label quality. The relative statistics
emphasize the much higher complexity of Cityscapes, as
the average numbers of object instances per image notably
exceed those of KITTI and Caltech. We extend our analysis
to MS COCO and PASCAL VOC that also contain
street scenes while not being speciﬁc for them. We analyze
the frequency of scenes with a certain number of trafﬁc participant instances, see Fig. 3. We ﬁnd our dataset to cover a
greater variety of scene complexity and to have a higher portion of highly complex scenes than previous datasets. Using
stereo data, we analyze the distribution of vehicle distances
to the camera. From Fig. 4 we observe, that in comparison to KITTI, Cityscapes covers a larger distance range.
We attribute this to both our higher-resolution imagery and
the careful annotation procedure. As a consequence, algorithms need to take a larger range of scales and object sizes
into account to score well in our benchmark.
3. Semantic Labeling
The ﬁrst Cityscapes task involves predicting a per-pixel
semantic labeling of the image without considering higherlevel object instance or boundary information.
3.1. Tasks and metrics
To assess labeling performance, we rely on a standard
and a novel metric. The ﬁrst is the standard Jaccard Index,
commonly known as the PASCAL VOC intersection-overunion metric IoU =
TP+FP+FN , where TP, FP, and FN
are the numbers of true positive, false positive, and false
negative pixels, respectively, determined over the whole test
set. Owing to the two semantic granularities, i.e. classes
and categories, we report two separate mean performance
scores: IoUcategory and IoUclass. In either case, pixels labeled
as void do not contribute to the score.
The global IoU measure is biased toward object instances that cover a large image area. In street scenes with
their strong scale variation this can be problematic. Specifically for trafﬁc participants, which are the key classes in
our scenario, we aim to evaluate how well the individual
instances in the scene are represented in the labeling. To
address this, we additionally evaluate the semantic labeling using an instance-level intersection-over-union metric
iTP+FP+iFN. Here, iTP, and iFN denote weighted
counts of true positive and false negative pixels, respectively. In contrast to the standard IoU measure, the contribution of each pixel is weighted by the ratio of the class’
average instance size to the size of the respective ground
truth instance. As before, FP is the number of false positive
pixels. It is important to note here that unlike the instancelevel task in Sec. 4, we assume that the methods only yield a
standard per-pixel semantic class labeling as output. Therefore, the false positive pixels are not associated with any
instance and thus do not require normalization. The ﬁnal
scores, iIoUcategory and iIoUclass, are obtained as the means
for the two semantic granularities, while only classes with
instance annotations are included.
3.2. Control experiments
We conduct several control experiments to put our baseline results below into perspective. First, we count the relative frequency of every class label at each pixel location of
the ﬁne (coarse) training annotations. Using the most frequent label at each pixel as a constant prediction irrespective
of the test image (called static ﬁne, SF, and static coarse,
SC) results in roughly 10 % IoUclass, as shown in Tab. 3.
These low scores emphasize the high diversity of our data.
SC and SF having similar performance indicates the value
of our additional coarse annotations. Even if the ground
truth (GT) segments are re-classiﬁed using the most frequent training label (SF or SC) within each segment mask,
the performance does not notably increase.
Secondly, we re-classify each ground truth segment using FCN-8s , c.f. Sec. 3.4. We compute the average
scores within each segment and assign the maximizing label. The performance is signiﬁcantly better than the static
predictors but still far from 100 %. We conclude that it is
necessary to optimize both classiﬁcation and segmentation
quality at the same time.
Thirdly, we evaluate the performance of subsampled
ground truth annotations as predictors. Subsampling was
done by majority voting of neighboring pixels, followed
by resampling back to full resolution. This yields an upper bound on the performance at a ﬁxed output resolution
and is particularly relevant for deep learning approaches
that often apply downscaling due to constraints on time,
memory, or the network architecture itself. Downsampling
factors 2 and 4 correspond to the most common setting of
our 3rd-party baselines (Sec. 3.4). Note that while subsampling by a factor of 2 hardly affects the IoU score, it clearly
decreases the iIoU score given its comparatively large impact on small, but nevertheless important objects. This underlines the importance of the separate instance-normalized
evaluation. The downsampling factors of 8, 16, and 32 are
motivated by the corresponding strides of the FCN model.
The performance of a GT downsampling by a factor of 64 is
comparable to the current state of the art, while downsampling by a factor of 128 is the smallest (power of 2) downsampling for which all images have a distinct labeling.
Lastly, we employ 128-times subsampled annotations
and retrieve the nearest training annotation in terms of the
Hamming distance. The full resolution version of this training annotation is then used as prediction, resulting in 21 %
While outperforming the static predictions, the
poor result demonstrates the high variability of our dataset
and its demand for approaches that generalize well.
3.3. State of the art
Drawing on the success of deep learning algorithms, a
number of semantic labeling approaches have shown very
promising results and signiﬁcantly advanced the state of
the art. These new approaches take enormous advantage
from recently introduced large-scale datasets, e.g. PASCAL-
Context and Microsoft COCO . Cityscapes aims
to complement these, particularly in the context of understanding complex urban scenarios, in order to enable further
research in this area.
The popular work of Long et al. showed how a topperforming Convolutional Neural Network (CNN) for image classiﬁcation can be successfully adapted for the task
of semantic labeling. Following this line, 
propose different approaches that combine the strengths of
CNNs and Conditional Random Fields (CRFs).
Other work takes advantage of deep learning for explicitly integrating global scene context in the prediction
Average over
Categories
Metric [%]
static ﬁne (SF)
static coarse (SC)
GT segmentation with SF
GT segmentation with SC
GT segmentation with 
GT subsampled by 2
GT subsampled by 4
GT subsampled by 8
GT subsampled by 16
GT subsampled by 32
GT subsampled by 64
GT subsampled by 128
nearest training neighbor
Table 3. Quantitative results of control experiments for semantic
labeling using the metrics presented in Sec. 3.1.
Categories
 extended
Table 4. Quantitative results of baselines for semantic labeling using the metrics presented in Sec. 3.1. The ﬁrst block lists results
from our own experiments, the second from those provided by 3rd
parties. All numbers are given in percent and we indicate the used
training data for each method, i.e. train ﬁne, val ﬁne, coarse extra
as well as a potential downscaling factor (sub) of the input image.
of pixel-wise semantic labels, in particular through CNNs
 or Recurrent Neural Networks (RNNs) .
Furthermore, a novel CNN architecture explicitly designed
for dense prediction has been proposed recently by .
Last but not least, several studies 
lately have explored different forms of weak supervision,
such as bounding boxes or image-level labels, for training
CNNs for pixel-level semantic labeling. We hope our coarse
annotations can further advance this area.
3.4. Baselines
Our own baseline experiments (Tab. 4, top) rely on fully
convolutional networks (FCNs), as they are central to most
state-of-the-art methods .
We adopted
VGG16 and utilize the PASCAL-context setup 
with a modiﬁed learning rate to match our image resolution under an unnormalized loss. According to the notation
in , we denote the different models as FCN-32s, FCN-
16s, and FCN-8s, where the numbers are the stride of the
ﬁnest heatmap. Since VGG16 training on 2 MP images exceeds even the largest GPU memory available, we split each
image into two halves with sufﬁciently large overlap. Additionally, we trained a model on images downscaled by a
factor of 2. We ﬁrst train on our training set (train) until the
performance on our validation set (val) saturates, and then
retrain on train+val with the same number of epochs.
To obtain further baseline results, we asked selected
groups that have proposed state-of-the-art semantic labeling approaches to optimize their methods on our dataset
and evaluated their predictions on our test set. The resulting
scores are given in Tab. 4 (bottom) and qualitative examples of three selected methods are shown in Fig. 5. Interestingly enough, the performance ranking in terms of the main
IoUclass score on Cityscapes is highly different from PAS-
CAL VOC . While DPN is the 2nd best method
on PASCAL, it is only the 6th best on Cityscapes. FCN-
8s is last on PASCAL, but 3rd best on Cityscapes. Adelaide performs consistently well on both datasets with
rank 1 on PASCAL and 2 on Cityscapes.
From studying these results, we draw several conclusions: (1) The amount of downscaling applied during training and testing has a strong and consistent negative inﬂuence on performance (c.f. FCN-8s vs. FCN-8s at half resolution, as well as the 2nd half of the table). The ranking
according to IoUclass is strictly consistent with the degree
of downscaling. We attribute this to the large scale variation present in our dataset, c.f. Fig. 4. This observation
clearly indicates the demand for additional research in the
direction of memory and computationally efﬁcient CNNs
when facing such a large-scale dataset with high-resolution
images. (2) Our novel iIoU metric treats instances of any
size equally and is therefore more sensitive to errors in
predicting small objects compared to the IoU.
that leverage a CRF for regularization tend
to over smooth small objects, c.f. Fig. 5, hence show a
larger drop from IoU to iIoU than or FCN-8s . 
is the only exception; its speciﬁc FCN-derived pairwise
terms apparently allow for a more selective regularization.
(3) When considering IoUcategory, Dilated10 and FCN-
8s perform particularly well, indicating that these approaches produce comparatively many confusions between
the classes within the same category, c.f. the buses in Fig. 5
(top). (4) Training FCN-8s with 500 densely annotated
Best reported result
Our result
Camvid 
KITTI 
KITTI 
Table 5. Quantitative results (avg. recall in percent) of
our half-resolution FCN-8s model trained on Cityscapes
images and tested on Camvid and KITTI.
images (750 h of annotation) yields comparable IoU performance to a model trained on 20 000 weakly annotated images (1300 h annot.), c.f. rows 5 & 6 in Tab. 4. However, in
both cases the performance is signiﬁcantly lower than FCN-
8s trained on all 3475 densely annotated images. Many ﬁne
labels are thus important for training standard methods as
well as for testing, but the performance using coarse annotations only does not collapse and presents a viable option. (5)
Since the coarse annotations do not include small or distant
instances, their iIoU performance is worse. (6) Coarse labels can complement the dense labels if applying appropriate methods as evidenced by outperforming , which
it extends by exploiting both dense and weak annotations
(e.g. bounding boxes). Our dataset will hopefully stimulate
research on exploiting the coarse labels further, especially
given the interest in this area, e.g. .
Overall, we believe that the unique characteristics of our
dataset (e.g. scale variation, amount of small objects, focus
on urban street scenes) allow for more such novel insights.
3.5. Cross-dataset evaluation
In order to show the compatibility and complementarity
of Cityscapes regarding related datasets, we applied an FCN
model trained on our data to Camvid and two subsets of
KITTI . We use the half-resolution model (c.f. 4th
row in Tab. 4) to better match the target datasets, but we do
not apply any speciﬁc training or ﬁne-tuning. In all cases,
we follow the evaluation of the respective dataset to be able
to compare to previously reported results . The obtained results in Tab. 5 show that our large-scale dataset
enables us to train models that are on a par with or even
outperforming methods that are speciﬁcally trained on another benchmark and specialized for its test data. Further,
our analysis shows that our new dataset integrates well with
existing ones and allows for cross-dataset research.
4. Instance-Level Semantic Labeling
The pixel-level task, c.f. Sec. 3, does not aim to segment
individual object instances.
In contrast, in the instancelevel semantic labeling task, we focus on simultaneously
detecting objects and segmenting them. This is an extension to both traditional object detection, since per-instance
segments must be provided, and semantic labeling, since
each instance is treated as a separate label.
4.1. Tasks and metrics
For instance-level semantic labeling, algorithms are required to deliver a set of detections of trafﬁc participants
in the scene, each associated with a conﬁdence score and
a per-instance segmentation mask. To assess instance-level
performance, we compute the average precision on the region level (AP ) for each class and average it across a
range of overlap thresholds to avoid a bias towards a speciﬁc value. Speciﬁcally, we follow and use 10 different
overlaps ranging from 0.5 to 0.95 in steps of 0.05. The
overlap is computed at the region level, making it equivalent to the IoU of a single instance. We penalize multiple
predictions of the same ground truth instance as false positives. To obtain a single, easy to compare compound score,
we report the mean average precision AP, obtained by also
averaging over the class label set. As minor scores, we add
AP50% for an overlap value of 50 %, as well as AP100m and
AP50m where the evaluation is restricted to objects within
100 m and 50 m distance, respectively.
4.2. State of the art
As detection results have matured (70 % mean AP on
PASCAL ), the last years have seen a rising interest in more difﬁcult settings. Detections with pixel-level
segments rather than traditional bounding boxes provide a
richer output and allow (in principle) for better occlusion
handling. We group existing methods into three categories.
The ﬁrst encompasses segmentation, then detection and
most prominently the R-CNN detection framework , relying on object proposals for generating detections. Many
of the commonly used bounding box proposal methods
 ﬁrst generate a set of overlapping segments, e.g.
Selective Search or MCG . In R-CNN, bounding
boxes of each segment are then scored using a CNN-based
classiﬁer, while each segment is treated independently.
The second category encompasses detection, then segmentation, where bounding-box detections are reﬁned to
instance speciﬁc segmentations. Either CNNs or
non-parametric methods are typically used, however,
in both cases without coupling between individual predictions.
Third, simultaneous detection and segmentation is signiﬁcantly more delicate. Earlier methods relied on Hough
voting . More recent works formulate a joint inference problem on pixel and instance level using CRFs
 .
Differences lie in the generation
of proposals (exemplars, average class shape, direct regression), the cues considered (pixel-level labeling, depth ordering), and the inference method (probabilistic, heuristics).
4.3. Lower bounds, oracles & baselines
In Tab. 6, we provide lower-bounds that any sensible
method should improve upon, as well as oracle-case results
Figure 5. Qualitative examples of selected baselines. From left to right: image with stereo depth maps partially overlayed, annotation,
DeepLab , Adelaide , and Dilated10 . The color coding of the semantic classes matches Fig. 1.
MCG regions
MCG bboxes
GT regions
MCG regions
MCG bboxes
Table 6. Baseline results on instance-level semantic labeling task
using the metrics described in Sec. 4. All numbers in %.
(i.e. using the test time ground truth). For our experiments,
we rely on publicly available implementations. We train a
Fast-R-CNN (FRCN) detector on our training data in
order to score MCG object proposals . Then, we use
either its output bounding boxes as (rectangular) segmentations, the associated region proposal, or its convex hull
as a per-instance segmentation. The best main score AP is
4.6 %, is obtained with convex hull proposals, and becomes
larger when restricting the evaluation to 50 % overlap or
close instances. We contribute these rather low scores to
our challenging dataset, biased towards busy and cluttered
scenes, where many, often highly occluded, objects occur
at various scales, c.f. Sec. 2. Further, the MCG bottom-up
proposals seem to be unsuited for such street scenes and
cause extremely low scores when requiring large overlaps.
We conﬁrm this interpretation with oracle experiments,
where we replace the proposals at test-time with ground
truth segments or replace the FRCN classiﬁer with an oracle. In doing so, the task of object localization is decoupled from the classiﬁcation task. The results in Tab. 6 show
that when bound to MCG proposals, the oracle classiﬁer is
only slightly better than FRCN. On the other hand, when the
proposals are perfect, FRCN achieves decent results. Overall, these observations unveil that the instance-level performance of our baseline is bound by the region proposals.
5. Conclusion and Outlook
In this work, we presented Cityscapes, a comprehensive
benchmark suite that has been carefully designed to spark
progress in semantic urban scene understanding by: (i) creating the largest and most diverse dataset of street scenes
with high-quality and coarse annotations to date; (ii) developing a sound evaluation methodology for pixel-level and
instance-level semantic labeling; (iii) providing an in-depth
analysis of the characteristics of our dataset; (iv) evaluating
several state-of-the-art approaches on our benchmark. To
keep pace with the rapid progress in scene understanding,
we plan to adapt Cityscapes to future needs over time.
The signiﬁcance of Cityscapes is all the more apparent
from three observations. First, the relative order of performance for state-of-the-art methods on our dataset is notably
different than on more generic datasets such as PASCAL
VOC. Our conclusion is that serious progress in urban scene
understanding may not be achievable through such generic
datasets. Second, the current state-of-the-art in semantic labeling on KITTI and CamVid is easily reached and to some
extent even outperformed by applying an off-the-shelf fullyconvolutional network trained on Cityscapes only, as
demonstrated in Sec. 3.5. This underlines the compatibility and unique beneﬁt of our dataset. Third, Cityscapes will
pose a signiﬁcant new challenge for our ﬁeld given that it is
currently far from being solved. The best performing baseline for pixel-level semantic segmentation obtains an IoU
score of 67.1 %, whereas the best current methods on PAS-
CAL VOC and KITTI reach IoU levels of 77.9 % and
72.5 % , respectively. In addition, the instance-level
task is particularly challenging with an AP score of 4.6 %.
Acknowledgments. S. Roth was supported in part by the European Research Council under the EU’s 7th Framework Programme
 /ERC Grant agreement no. 307942. The authors
acknowledge the support of the Bundesministerium für Wirtschaft
und Technologie (BMWi) in the context of the UR:BAN initiative.
We thank the 3rd-party authors for their valuable submissions.