HAL Id: inria-00140549
 
Submitted on 6 Apr 2007
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Parameter Control in Evolutionary Algorithms
Aguston Eiben, Zbigniew Michalewicz, Marc Schoenauer, Jim Smith
To cite this version:
Aguston Eiben, Zbigniew Michalewicz, Marc Schoenauer, Jim Smith. Parameter Control in Evolutionary Algorithms. Fernando G. Lobo and Cláudio F. Lima and Zbigniew Michalewicz. Parameter
Setting in Evolutionary Algorithms, 54 (54), Springer Verlag, pp.19-46, 2007, Studies in Computational Intelligence, 978-3-540-69431-1. ￿10.1007/978-3-540-69432-8￿. ￿inria-00140549￿
Parameter Control in Evolutionary Algorithms
A.E. Eiben1, Z. Michalewicz2, M. Schoenauer3, and J.E. Smith4
1 Free University Amsterdam, The Netherlands 
2 University of Adelaide, Australia 
3 INRIA Futurs, France 
4 UWE, United Kingdom 
1 Abstract
The issue of setting the values of various parameters of an evolutionary algorithm is crucial for good performance. In this paper we discuss how to do
this, beginning with the issue of whether these values are best set in advance
or are best changed during evolution. We provide a classiﬁcation of diﬀerent
approaches based on a number of complementary features, and pay special
attention to setting parameters on-the-ﬂy. This has the potential of adjusting
the algorithm to the problem while solving the problem.
This paper is intended to present a survey rather than a set of prescriptive
details for implementing an EA for a particular type of problem. For this
reason we have chosen to interleave a number of examples throughout the
text. Thus we hope to both clarify the points we wish to raise as we present
them, and also to give the reader a feel for some of the many possibilities
available for controlling diﬀerent parameters.
2 Introduction
Finding the appropriate setup for an evolutionary algorithm is a long standing
grand challenge of the ﬁeld . The main problem is that the description of a speciﬁc EA contains its components, such as the choice of representation,selection, recombination, and mutation operators, thereby setting a
framework while still leaving quite a few items undeﬁned. For instance, a simple GA might be given by stating it will use binary representation, uniform
crossover, bit-ﬂip mutation, tournament selection, and generational replacement. For a full speciﬁcation, however, further details have to be given, for
instance, the population size, the probability of mutation pm and crossover
pc, and the tournament size. These data – called the algorithm parameters or strategy parameters – complete the deﬁnition of the EA and are
necessary to produce an executable version. The values of these parameters
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
greatly determine whether the algorithm will ﬁnd an optimal or near-optimal
solution, and whether it will ﬁnd such a solution eﬃciently. Choosing the right
parameter values is, however, a hard task.
Globally, we distinguish two major forms of setting parameter values: parameter tuning and parameter control. By parameter tuning we mean
the commonly practised approach that amounts to ﬁnding good values for the
parameters before the run of the algorithm and then running the algorithm
using these values, which remain ﬁxed during the run. Later on in this section
we give arguments that any static set of parameters having the values ﬁxed
during an EA run seems to be inappropriate. Parameter control forms an alternative, as it amounts to starting a run with initial parameter values that
are changed during the run.
Parameter tuning is a typical approach to algorithm design. Such tuning
is done by experimenting with diﬀerent values and selecting the ones that give
the best results on the test problems at hand. However, the number of possible
parameters and their diﬀerent values means that this is a very time-consuming
activity. Considering four parameters and ﬁve values for each of them, one has
to test 54 = 625 diﬀerent setups. Performing 100 independent runs with each
setup, this implies 62,500 runs just to establish a good algorithm design.
The technical drawbacks to parameter tuning based on experimentation
can be summarised as follows:
Parameters are not independent, but trying all diﬀerent combinations systematically is practically impossible.
The process of parameter tuning is time consuming, even if parameters
are optimised one by one, regardless of their interactions.
For a given problem the selected parameter values are not necessarily
optimal, even if the eﬀort made for setting them was signiﬁcant.
This picture becomes even more discouraging if one is after a “generally
good” setup that would perform well on a range of problems or problem instances. During the history of EAs considerable eﬀort has been spent on ﬁnding parameter values (for a given type of EA, such as GAs), that were good
for a number of test problems. A well-known early example is that of , determining recommended values for the probabilities of single-point crossover
and bit mutation on what is now called the DeJong test suite of ﬁve functions.
About this and similar attempts , it should be noted that genetic algorithms used to be seen as robust problem solvers that exhibit approximately
the same performance over a wide range of problems [34, page 6]. The contemporary view on EAs, however, acknowledges that speciﬁc problems (problem
types) require speciﬁc EA setups for satisfactory performance . Thus, the
scope of “optimal” parameter settings is necessarily narrow. There are also
theoretical arguments that any quest for generally good EA, thus generally
good parameter settings, is lost a priori, such as the No Free Lunch theorem
Parameter Control in Evolutionary Algorithms
To elucidate another drawback of the parameter tuning approach recall
how we deﬁned it: ﬁnding good values for the parameters before the run of the
algorithm and then running the algorithm using these values, which remain
ﬁxed during the run. However, a run of an EA is an intrinsically dynamic,
adaptive process. The use of rigid parameters that do not change their values
is thus in contrast to this spirit. Additionally, it is intuitively obvious, and
has been empirically and theoretically demonstrated, that diﬀerent values of
parameters might be optimal at diﬀerent stages of the evolutionary process
 .
To give an example, large mutation steps can be good in the early generations, helping the exploration of the search space, and small mutation steps
might be needed in the late generations to help ﬁne-tune the suboptimal
chromosomes. This implies that the use of static parameters itself can lead to
inferior algorithm performance.
A straightforward way to overcome the limitations of static parameters
is by replacing a parameter p by a function p(t), where t is the generation
counter (or any other measure of elapsed time). However, as indicated earlier,
the problem of ﬁnding optimal static parameters for a particular problem is
already hard. Designing optimal dynamic parameters (that is, functions for
p(t)) may be even more diﬃcult. Another possible drawback to this approach
is that the parameter value p(t) changes are caused by a “blind” deterministic
rule triggered by the progress of time t, without taking any notion of the actual
progress in solving the problem, i.e., without taking into account the current
state of the search. A well-known instance of this problem occurs in simulated
annealing where a so-called cooling schedule has to be set before the
execution of the algorithm.
Mechanisms for modifying parameters during a run in an “informed” way
were realised quite early in EC history. For instance, evolution strategies
changed mutation parameters on-the-ﬂy by Rechenberg’s 1/5 success rule
using information on the ratio of successful mutations. Davis experimented
within GAs with changing the crossover rate based on the progress realised
by particular crossover operators . The common feature of these and similar approaches is the presence of a human-designed feedback mechanism that
utilises actual information about the search process for determining new parameter values.
Yet another approach is based on the observation that ﬁnding good parameter values for an evolutionary algorithm is a poorly structured, ill-deﬁned,
complex problem. This is exactly the kind of problem on which EAs are often
considered to perform better than other methods. It is thus a natural idea
to use an EA for tuning an EA to a particular problem. This could be done
using two EAs: one for problem solving and another one – the so-called meta-
EA – to tune the ﬁrst one . It could also be done by using only
one EA that tunes itself to a given problem, while solving that problem. Selfadaptation, as introduced in Evolution Strategies for varying the mutation
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
parameters, falls within this category. In the next section we discuss various
options for changing parameters, illustrated by an example.
3 Case study: Evolution Strategies
The history of Evolution Strategies (ES) is a typical case study for parameter
tuning, as it went through several successive steps pertaining to many of the
diﬀerent approaches listed so far.
Typical ES work in a real-valued search space (typically IRn, or a subset
of IRn, for some integer n).
3.1 Gaussian mutations
The main operator (and almost the trademark) of ES is the Gaussian mutation, that adds centred normally distributed noise to the variables of the
individuals. The most general Gaussian distribution in IRn is the multivariate
normal distribution N(m, C), with mean m and covariance matrix C, a
n × n positive deﬁnite matrix, that has the following Probability Distribution
Φ(X) = exp(−1
2(X −m)tC−1(X −m))
where |C| is the determinant of C.
It is then convenient to write the mutation of a vector X ∈IRn as
X →X + σN(0, C)
i.e. to distinguish a scaling factor σ, also called the step-size, from the
directions of the Gaussian distribution given by the covariance matrix C.
For example, the simplest case of Gaussian mutation assumes that C is the
identity matrix in IRn (the diagonal matrix that has only 1s on the diagonal).
In this case, all coordinates will be mutated independently, and will have
added to them some Gaussian noise with variance σ2.
Tuning an ES algorithm therefore amounts to tuning the step-size and
the covariance matrix – or simply tuning the step-size in the simple case
mentioned above.
3.2 Adapting the step-size
The step-size of the Gaussian mutation gives the scale of the search. To make
things clear, suppose that you are minimising x2 in one dimension, running a
(1+1)-ES (one parent gives birth to one oﬀspring, and the best of both is the
next parent) with a ﬁxed step-size σ. Then the average distance between parent and successful oﬀspring is proportional to σ. This has two consequences:
Parameter Control in Evolutionary Algorithms
ﬁrst, starting from distance d0 from the solution, it will take an average of
d0/σ steps to reach a region close to the optimum. On the other hand, when
hovering around the optimum, the precision you can hope is again proportional to σ. Those arguments naturally lead to the optimal adaptive setting
of the step-size for the sphere function: σ should be proportional to the distance to the optimum. Details can be found in the studies of the so-called
progress rate: early work was done by Schwefel , completed and extended
by Beyer and recent work by Auger gave a formal global convergence proof
of this ... impractical algorithm: indeed, the distance to the optimum is not
known in real situations!
But another piece of information is always available to the algorithm: the
success rate ( the proportion of successful mutations, where the oﬀspring is
better than the parent). This can indirectly give information about the stepsize: this was Rechenberg’s main idea to propose the ﬁrst practical method for
an adaptive step-size, the so-called one-ﬁfth rule: if the success rate over some
time window is larger than the success rate when the step-size is optimal (0.2,
or one-ﬁfth!), the the step-size should be increased (the algorithm is making
too many small steps); on the other hand, if the success rate is smaller than
0.2, then the step-size should be decreased (the algorithm is constantly missing
the target, because it shoots too far). Though formally derived from studies on
the sphere function and the corridor function (a half-bounded linear function),
the one-ﬁfth rule was generalised to any function.
However, there are many situations where the one-ﬁfth rule can be mislead.
Moreover, it does not in any way handle the case of non-isotropic functions,
where a non-diagonal covariance matrix is mandatory. Hence, it is no longer
used today.
3.3 Self-Adaptive ES
The next big step in ESs was the invention of the self-adaptive mutation:
the parameters of the mutation (both the step-size and the covariance matrix)
are attached to each individual, and are subject to mutation, too. Those personal mutation parameters range from a single step-size, leading to isotropic
mutation, where all coordinates are mutated independently with the same
variance, to the non-isotropic mutation, that use a vector of n “standard deviations” σi, equivalent to a diagonal matrix C with σi on the diagonal, and
to the correlated mutations, where a full covariance matrix is attached to each
individual. Mutating an individual then amounts to ﬁrst mutating the mutation parameters themselves, and then mutating the variables using the new
mutation parameters. Details can be found in .
The rationale for SA-ES are that the algorithm relies on the selection step
to keep in the population not only the best ﬁt individuals, but also the individuals with the best mutation parameters – according to the region of the
search space they are in. Indeed, although the selection acts based on the
ﬁtness, the underlying idea beneath Self-Adaptive ES (SA-ES) is that if two
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
individuals starts with the same ﬁtness, the oﬀspring of one that has “better”
mutation parameters will reach regions of higher ﬁtness faster than the oﬀspring of the other: selection will hence keep the ones with the good mutation
parameters. This is what has often been stated as “mutation parameters are
optimised for free” by the evolution itself. And indeed, SA-ES have long been
the state-of-the-art in parametric optimisation .
But what are “good” mutation parameters? The issue has already been discussed for the step-size in previous section, and similar arguments can be given
for the covariance matrix itself. Replace the sphere model (min P x2
with a quadratic function5 (min 1
2XtHX for some positive deﬁnite matrix H).
Then it is clear that the mutation should progress slower along the directions
of steepest descent of H: the covariance matrix should be proportional to H−1.
And whereas the step-size actually self-adapts to quasi-optimal values ,
the covariance matrix that is learned by the correlated SA-ES is not the actual
inverse of the Hessian .
3.4 CMA-ES: a clever adaptation
Another defect of SA-ES is the relative slowness of adaptation of the mutation
parameters: even for the simple case of the step-size, if the initial value is not
the optimal one (proportional to the distance to the optimum in the case of
the sphere function), it takes some time to reach that optimal value and to
start being eﬃcient.
This observation led Hansen and Ostermeier to propose deterministic
schedules to adapt the mutation parameters in ES, hence heading back to
an adaptive method for parameter tuning. Their method was ﬁrst limited
to the step-size , and later addressed the adaptation of the full covariance
matrix . The complete Covariance Matrix Adaptation (CMA-ES) algorithm was ﬁnally detailed (and its parameters carefully tuned) in and an
improvement for the update of the covariance matrix was proposed in .
The basic idea in CMA-ES is to use the path followed by the algorithm to
deterministically update the diﬀerent mutation parameters, and a simpliﬁed
view is given by the following: suppose that the algorithm has made a series
of steps in co-linear directions; then the step-size should be increased, to allow
larger steps and increase speed. Similar ideas undermine the covariance matrix
update(s).
Indeed, using such clever learning method, CMA-ES proved to outperform
most other stochastic algorithms for parametric optimisation, as witnessed by
its success in the 2005 contest that took place at CEC’2005.
5The Evolutionary Computation community usually calls this type of function
elliptic – as iso-ﬁtness surfaces are ellipsoids. However, elliptic functions have a
completely diﬀerent meaning in Mathematics, and we will hence come back here to
the more standard terminology, using the word quadratic.
Parameter Control in Evolutionary Algorithms
3.5 Lessons learnt
This brief summary of ES history witnesses that
Static parameters are not only hard but can be impossible to tune: there
doesn’t exist any good static value for the step-size in Gaussian mutation
Adaptive methods use some information about the current state of the
search, and are as good as the information they get: the success rate is a
very raw information, and lead to the “easy-to-defeat” one-ﬁfth rule, while
CMA-ES uses high-level information to cleverly update all the parameters
of the most general Gaussian mutation
Self-adaptive methods are eﬃcient methods when applicable, i.e. when the
only available selection (based on the ﬁtness) can prevent bad parameters
from proceeding to future generations. They outperform basic static and
adaptive methods, but are outperformed by clever adaptive methods.
4 Case Study: Changing the Penalty Coeﬃcients
Let us assume we deal with a numerical optimisation problem to minimise
f(x) = f(x1, . . . , xn),
subject to some inequality and equality constraints
gi(x) ≤0, i = 1, . . . , q,
hj(x) = 0, j = q + 1, . . . , m,
where the domains of the variables are given by lower and upper bounds
li ≤xi ≤ui for 1 ≤i ≤n.
For such a numerical optimisation problem we may consider an evolutionary algorithm based on a ﬂoating-point representation, where each individual x in the population is represented as a vector of ﬂoating-point numbers
x = ⟨x1, . . . , xn⟩.
In the previous section we described diﬀerent ways to modify a parameter controlling mutation. Several other components of an EA have natural
parameters, and these parameters are traditionally tuned in one or another
way. Here we show that other components, such as the evaluation function
(and consequently the ﬁtness function) can also be parameterised and thus
varied. While this is a less common option than tuning mutation (although it
is practised in the evolution of variable-length structures for parsimony pressure ), it may provide a useful mechanism for increasing the performance
of an evolutionary algorithm.
When dealing with constrained optimisation problems, penalty functions
are often used. A common technique is the method of static penalties ,
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
which requires ﬁxed user-supplied penalty parameters. The main reason for
its widespread use is that it is the simplest technique to implement: it requires
only the straightforward modiﬁcation of the evaluation function as follows:
eval(x) = f(x) + W · penalty(x),
where f is the objective function, and penalty(x) is zero if no violation occurs, and is positive,6 otherwise. Usually, the penalty function is based on the
distance of a solution from the feasible region, or on the eﬀort to “repair” the
solution, i.e., to force it into the feasible region. In many methods a set of
functions fj (1 ≤j ≤m) is used to construct the penalty, where the function
fj measures the violation of the jth constraint in the following way:
max{0, gj(x)} if
q + 1 ≤j ≤m.
W is a user-deﬁned weight, prescribing how severely constraint violations are
weighted. In the most traditional penalty approach the weight W does not
change during the evolution process. We sketch three possible methods of
changing the value of W.
First, we can replace the static parameter W by a dynamic parameter,
e.g., a function W(t). Just as for the mutation parameter σ, we can develop a
heuristic that modiﬁes the weight W over time. For example, in the method
proposed by Joines and Houck , the individuals are evaluated (at the
iteration t) by a formula, where
eval(x) = f(x) + (C · t)α · penalty(x),
where C and α are constants. Since
W(t) = (C · t)α,
the penalty pressure grows with the evolution time provided 1 ≤C, α.
Second, let us consider another option, which utilises feedback from the
search process. One example of such an approach was developed by Bean and
Hadj-Alouane , where each individual is evaluated by the same formula as
before, but W(t) is updated in every generation t in the following way:
W(t + 1) =
(1/β1) · W(t)
for all t −k + 1 ≤i ≤t,
i ∈S−F for all t −k + 1 ≤i ≤t,
otherwise.
In this formula, S is the set of all search points (solutions), F ⊆S is a set of all
feasible solutions, b
i denotes the best individual in terms of the function eval
in generation i, β1, β2 > 1, and β1 ̸= β2 (to avoid cycling). In other words,
6For minimisation problems.
Parameter Control in Evolutionary Algorithms
the method decreases the penalty component W(t + 1) for the generation
t + 1 if all best individuals in the last k generations were feasible (i.e., in
F), and increases penalties if all best individuals in the last k generations
were infeasible. If there are some feasible and infeasible individuals as best
individuals in the last k generations, W(t + 1) remains without change.
Third, we could allow self-adaptation of the weight parameter, similarly
to the mutation step sizes in the previous section. For example, it is possible
to extend the representation of individuals into
⟨x1, . . . , xn, W⟩,
where W is the weight. The weight component W undergoes the same changes
as any other variable xi (e.g., Gaussian mutation and arithmetic recombination).
To illustrate this method,which is analogous to using a separate σi for each
xi, we need to redeﬁne the evaluation function. Let us ﬁrst introduce penalty
functions for each constraint as per Eq. (1). Clearly, these penalties are all
non-negative and are at zero if no constraints are violated. Then consider a
vector of weights w = (w1, . . . , wm), and deﬁne
eval(x) = f(x) +
as the function to be minimised and also extend the representation of individuals into
⟨x1, . . . , xn, w1, . . . , wm⟩.
Variation operators can then be applied to both the x and the w part of
these chromosomes, realising a self-adaptation of the constraint weights, and
thereby the ﬁtness function.
It is important to note the crucial diﬀerence between self-adapting mutation step sizes and constraint weights. Even if the mutation step sizes are
encoded in the chromosomes, the evaluation of a chromosome is independent
from the actual σ values. That is,
eval(⟨x, σ⟩) = f(x),
for any chromosome ⟨x, σ⟩. In contrast, if constraint weights are encoded in
the chromosomes, then we have
eval(⟨x, w⟩) = fw(x),
for any chromosome ⟨x, W⟩. This could enable the evolution to “cheat” in the
sense of making improvements by minimising the weights instead of optimising
f and satisfying the constraints. Eiben et al. investigated this issue in and
found that using a speciﬁc tournament selection mechanism neatly solves this
problem and enables the EA to solve constraints.
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
4.1 Summary
In the previous sections we illustrated how the mutation operator and the
evaluation function can be controlled (adapted) during the evolutionary process. The latter case demonstrates that not only can the traditionally adjusted
components, such as mutation, recombination, selection, etc., be controlled by
parameters, but so can other components of an evolutionary algorithm. Obviously, there are many components and parameters that can be changed and
tuned for optimal algorithm performance. In general, the three options we
sketched for the mutation operator and the evaluation function are valid for
any parameter of an evolutionary algorithm, whether it is population size,
mutation step, the penalty coeﬃcient, selection pressure, and so forth.
The mutation example above also illustrates the phenomenon of the scope
of a parameter. Namely, the mutation step size parameter can have diﬀerent
domains of inﬂuence, which we call scope. Using the ⟨x1, . . . , xn, σ1, . . . , σn⟩
model, a particular mutation step size applies only to one variable of a single
individual. Thus, the parameter σi acts on a subindividual, or component,
level. In the ⟨x1, . . . , xn, σ⟩representation, the scope of σ is one individual,
whereas the dynamic parameter σ(t) was deﬁned to aﬀect all individuals and
thus has the whole population as its scope.
These remarks conclude the introductory examples of this section. We
are now ready to attempt a classiﬁcation of parameter control techniques for
parameters of an evolutionary algorithm.
5 Classiﬁcation of Control Techniques
In classifying parameter control techniques of an evolutionary algorithm, many
aspects can be taken into account. For example:
1. What is changed? (e.g., representation, evaluation function, operators, selection process, mutation rate, population size, and so on)
2. How the change is made (i.e., deterministic heuristic, feedback-based
heuristic, or self-adaptive)
3. The evidence upon which the change is carried out (e.g., monitoring performance of operators, diversity of the population, and so on)
4. The scope/level of change (e.g., population-level, individual-level, and so
In the following we discuss these items in more detail.
5.1 What is Changed?
To classify parameter control techniques from the perspective of what component or parameter is changed, it is necessary to agree on a list of all major
components of an evolutionary algorithm, which is a diﬃcult task in itself.
For that purpose, let us assume the following components of an EA:
Parameter Control in Evolutionary Algorithms
Representation of individuals
Evaluation function
Variation operators and their probabilities
Selection operator (parent selection or mating selection)
Replacement operator (survival selection or environmental selection)
Population (size, topology, etc.)
Note that each component can be parameterised, and that the number of
parameters is not clearly deﬁned. For example, an oﬀspring v produced by an
arithmetical crossover of k parents x1, . . . , xk can be deﬁned by the following
v = a1x1 + . . . + akxk,
where a1, . . . , ak, and k can be considered as parameters of this crossover. Parameters for a population can include the number and sizes of subpopulations,
migration rates, and so on for a general case, when more then one population
is involved. Despite the somewhat arbitrary character of this list of components and of the list of parameters of each component, we will maintain the
“what-aspect” as one of the main classiﬁcation features, since this allows us
to locate where a speciﬁc mechanism has its eﬀect.
5.2 How are Changes Made?
As discussed and illustrated in the two earlier case studies, methods for changing the value of a parameter (i.e., the “how-aspect”) can be classiﬁed into one
of three categories.
Deterministic parameter control
This takes place when the value of a strategy parameter is altered by
some deterministic rule. This rule modiﬁes the strategy parameter in a
ﬁxed, predetermined (i.e., user-speciﬁed) way without using any feedback
from the search. Usually, a time-varying schedule is used, i.e., the rule is
used when a set number of generations have elapsed since the last time
the rule was activated.
Adaptive parameter control
This takes place when there is some form of feedback from the search that
serves as inputs to a mechanism used to determine the direction or magnitude of the change to the strategy parameter. The assignment of the value
of the strategy parameter may involve credit assignment, based on the
quality of solutions discovered by diﬀerent operators/parameters, so that
the updating mechanism can distinguish between the merits of competing strategies. Although the subsequent action of the EA may determine
whether or not the new value persists or propagates throughout the population, the important point to note is that the updating mechanism used
to control parameter values is externally supplied, rather than being part
of the “standard” evolutionary cycle.
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
Self-adaptive parameter control
The idea of the evolution of evolution can be used to implement the selfadaptation of parameters (see for a good review). Here the parameters
to be adapted are encoded into the chromosomes and undergo mutation
and recombination. The better values of these encoded parameters lead to
better individuals, which in turn are more likely to survive and produce
oﬀspring and hence propagate these better parameter values. This is an
important distinction between adaptive and self-adaptive schemes: in the
latter the mechanisms for the credit assignment and updating of diﬀerent
strategy parameters are entirely implicit, i.e., they are the selection and
variation operators of the evolutionary cycle itself.
This terminology leads to the taxonomy illustrated in Fig. 1.
before the run
during the run
Parameter setting
Parameter tuning
Parameter control
Deterministic
Self−adaptive
Fig. 1. Global taxonomy of parameter setting in EAs
Some authors have introduced a diﬀerent terminology. Angeline distinguished “absolute” and “empirical” rules, which correspond to the “uncoupled” and “tightly-coupled” mechanisms of Spears . Let us note that
the uncoupled/absolute category encompasses deterministic and adaptive
control, whereas the tightly-coupled/empirical category corresponds to selfadaptation. We feel that the distinction between deterministic and adaptive
parameter control is essential, as the ﬁrst one does not use any feedback from
the search process. However, we acknowledge that the terminology proposed
here is not perfect either. The term “deterministic” control might not be the
most appropriate, as it is not determinism that matters, but the fact that
the parameter-altering transformations take no input variables related to the
progress of the search process. For example, one might randomly change the
mutation probability after every 100 generations, which is not a deterministic
process. The name “ﬁxed” parameter control might provide an alternative
that also covers this latter example. Also, the terms “adaptive” and “selfadaptive” could be replaced by the equally meaningful “explicitly adaptive”
and “implicitly adaptive” controls, respectively. We have chosen to use “adaptive” and “self-adaptive” for the widely accepted usage of the latter term.
Parameter Control in Evolutionary Algorithms
5.3 What Evidence Informs the Change?
The third criterion for classiﬁcation concerns the evidence used for determining the change of parameter value . Most commonly, the progress of
the search is monitored, e.g., by looking at the performance of operators, the
diversity of the population, and so on. The information gathered by such a
monitoring process is used as feedback for adjusting the parameters. From
this perspective, we can make further distinction between the following two
Absolute evidence
We speak of absolute evidence when the value of a strategy parameter is
altered by some rule that is applied when a predeﬁned event occurs. The
diﬀerence from deterministic parameter control lies in the fact that in deterministic parameter control a rule ﬁres by a deterministic trigger (e.g.,
time elapsed), whereas here feedback from the search is used. For instance,
the rule can be applied when the measure being monitored hits a previously set threshold – this is the event that forms the evidence. Examples
of this type of parameter adjustment include increasing the mutation rate
when the population diversity drops under a given value , changing
the probability of applying mutation or crossover according to a fuzzy rule
set using a variety of population statistics , and methods for resizing
populations based on estimates of schemata ﬁtness and variance . Such
mechanisms require that the user has a clear intuition about how to steer
the given parameter into a certain direction in cases that can be speciﬁed
in advance (e.g., they determine the threshold values for triggering rule
activation). This intuition may be based on the encapsulation of practical experience, data-mining and empirical analysis of previous runs, or
theoretical considerations (in the order of the three examples above), but
all rely on the implicit assumption that changes that were appropriate to
make on another search of another problem are applicable to this run of
the EA on this problem.
Relative evidence
In the case of using relative evidence, parameter values are compared according to the ﬁtness of the oﬀspring that they produce, and the better
values get rewarded. The direction and/or magnitude of the change of
the strategy parameter is not speciﬁed deterministically, but relative to
the performance of other values, i.e., it is necessary to have more than
one value present at any given time. Here, the assignment of the value of
the strategy parameter involves credit assignment, and the action of the
EA may determine whether or not the new value persists or propagates
throughout the population. As an example, consider an EA using more
crossovers with crossover rates adding up to 1.0 and being reset based
on the crossovers performance measured by the quality of oﬀspring they
create. Such methods may be controlled adaptively, typically using “bookkeeping” to monitor performance and a user-supplied update procedure
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
 , or self-adaptively with the selection
operator acting indirectly on operator or parameter frequencies via their
association with “ﬁt” solutions.
5.4 What is the Scope of the Change?
As discussed earlier, any change within any component of an EA may aﬀect
a gene (parameter), whole chromosomes (individuals), the entire population,
another component (e.g., selection), or even the evaluation function. This is
the aspect of the scope or level of adaptation . Note, however, that
the scope or level is not an independent dimension, as it usually depends
on the component of the EA where the change takes place. For example, a
change of the mutation step size may aﬀect a gene, a chromosome, or the entire
population, depending on the particular implementation (i.e., scheme used),
but a change in the penalty coeﬃcients typically aﬀects the whole population.
In this respect the scope feature is a secondary one, usually depending on the
given component and its actual implementation.
It should be noted that the issue of the scope of the parameter might be
more complicated than indicated in Sect. 4.1. First of all, the scope depends
on the interpretation mechanism of the given parameters. For example, in a
non-isotropic SA-ES (see section 3.3), an individual might be represented as
⟨x1, . . . , xn, σ1, . . . , σn, α1, . . . , αn(n−1)/2⟩,
where the vector α denotes the covariances between the variables x1, . . . , xn.
In this case the scope of the strategy parameters in α is the whole individual,
although the notation might suggest that they act on a sub-individual level.
The next example illustrates that the same parameter (encoded in the
chromosomes) can be interpreted in diﬀerent ways, leading to diﬀerent algorithm variants with diﬀerent scopes of this parameter. Spears , following , experimented with individuals containing an extra bit to determine
whether one-point crossover or uniform crossover is to be used (bit 1/0 standing for one-point/uniform crossover, respectively). Two interpretations were
considered. The ﬁrst interpretation was based on a pairwise operator choice: If
both parental bits are the same, the corresponding operator is used; otherwise,
a random choice is made. Thus, this parameter in this interpretation acts at
an individual level. The second interpretation was based on the bit distribution over the whole population: If, for example, 73% of the population had bit
1, then the probability of one-point crossover was 0.73. Thus this parameter
under this interpretation acts on the population level. Spears noted that there
was a deﬁnite impact on performance, with better results arising from the individual level scheme, and more recently Smith compared three versions of
a self-adaptive recombination operator, concluding that the component-level
version signiﬁcantly outperformed the individual or population-level versions.
However, the two interpretations of Spears’ scheme can be easily combined.
For instance, similar to the ﬁrst interpretation, if both parental bits are the
Parameter Control in Evolutionary Algorithms
same, the corresponding operator is used, but if they diﬀer, the operator is
selected according to the bit distribution, just as in the second interpretation.
The scope/level of this parameter in this interpretation is neither individual
nor population, but rather both. This example shows that the notion of scope
can be ill-deﬁned and very complex. This, combined with the arguments that
the scope or level entity is primarily a feature of the given parameter and only
secondarily a feature of adaptation itself, motivates our decision to exclude it
as a major classiﬁcation criterion.
5.5 Summary
In conclusion, the main criteria for classifying methods that change the values
of the strategy parameters of an algorithm during its execution are:
1. What component/parameter is changed?
2. How is the change made?
3. Which evidence is used to make the change?
Our classiﬁcation is thus three-dimensional. The component dimension
consists of six categories: representation, evaluation function, variation operators (mutation and recombination), selection, replacement, and population.
The other dimensions have respectively three (deterministic, adaptive, selfadaptive) and two categories (absolute, relative). Their possible combinations
are given in Table 1. As the table indicates, deterministic parameter control with relative evidence is impossible by deﬁnition, and so is self-adaptive
parameter control with absolute evidence. Within the adaptive scheme both
options are possible and are indeed used in practise.
Deterministic Adaptive Self-adaptive
Table 1. Reﬁned taxonomy of parameter setting in EAs: types of parameter control along the type and evidence dimensions. The – entries represent meaningless
(nonexistent) combinations
6 Examples of Varying EA Parameters
Here we review some illustrative examples from the literature concerning all
major components. For a more comprehensive overview the reader is referred
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
6.1 Representation
The choice of representation forms an important distinguishing feature between diﬀerent streams of evolutionary computing. From this perspective GAs
and ES can be distinguished from (historical) EP and GP according to the
data structure used to represent individuals. In the ﬁrst group this data structure is linear, and its length is ﬁxed, that is, it does not change during a run
of the algorithm. For (historical) EP and GP this does not hold: ﬁnite state
machines and parse trees are nonlinear structures, and their size (the number
of states, respectively nodes) and shape can change during a run. It could be
argued that this implies an intrinsically adaptive representation in traditional
EP and GP. On the other hand, the main structure of the ﬁnite state machines
does not change during the search in traditional EP, nor do the function and
terminal sets in GP (without automatically deﬁned functions, ADFs). If one
identiﬁes “representation” with the basic syntax (plus the encoding mechanism), then the diﬀerently sized and shaped ﬁnite state machines, respectively
trees, are only diﬀerent expressions in this unchanging syntax. Based on this
view we do not consider the representations in traditional EP and GP intrinsically adaptive.
We illustrate variable representations with the delta coding algorithm of
Mathias and Whitley , which eﬀectively modiﬁes the encoding of the function parameters. The motivation behind this algorithm is to maintain a good
balance between fast search and sustaining diversity. In our taxonomy it can
be categorised as an adaptive adjustment of the representation based on absolute evidence.
The GA is used with multiple restarts; the ﬁrst run is used to ﬁnd an
interim solution, and subsequent runs decode the genes as distances (delta
values) from the last interim solution. This way each restart forms a new
hypercube with the interim solution at its origin. The resolution of the delta
values can also be altered at the restarts to expand or contract the search
space. The restarts are triggered when population diversity (measured by
the Hamming distance between the best and worst strings of the current
population) is not greater than one. The sketch of the algorithm showing the
main idea is given in Fig. 2.
Note that the number of bits for δ can be increased if the same solution
INTERIM is found. This technique was further reﬁned in to cope with
deceptive problems.
Another similar idea to reﬁne the grain of the representation for real numbers was also proposed in : the Real-Coded Adaptive Range Genetic Algorithm gradually reﬁnes the range of the variables around the best-so-far
6.2 Evaluation function
Evaluation functions are typically not varied in an EA because they are often
considered as part of the problem to be solved and not as part of the problem-
Parameter Control in Evolutionary Algorithms
/* given a starting population and genotype-phenotype encoding */
HD > 1 ) DO
RUN GA with k bits per object variable;
REPEAT UNTIL (
global termination is satisfied ) DO
save best solution as INTERIM;
reinitialise population with new coding;
k-1 bits as the distance δ to the object value in
INTERIM and one sign bit */
HD > 1 ) DO
RUN GA with this encoding;
Fig. 2. Outline of the delta coding algorithm
solving algorithm. In fact, an evaluation function forms the bridge between
the two, so both views are at least partially true. In many EAs the evaluation
function is derived from the (optimisation) problem at hand with a simple
transformation of the objective function. In the class of constraint satisfaction
problems, however, there is no objective function in the problem deﬁnition
 . Rather, these are normally posed as decision problems with an Boolean
outcome φ denoting whether a given assignment of variables represents a valid
solution. One possible approach using EAs is to treat these as minimisation
problems where the evaluation function is deﬁned as the amount of constraint
violation by a given candidate solution. This approach, commonly known as
the penalty approach, can be formalised as follows. Let us assume that we
have constraints ci (i = {1, . . . , m}) and variables vj (j = {1, . . . , n}) with the
same domain S. The task is to ﬁnd one variable assignment ¯s ∈S satisfying
all constraints. Then the penalties can be deﬁned as follows:
wi × χ(¯s, ci),
χ(¯s, ci) =
 1 if ¯s violates ci,
0 otherwise.
Obviously, for each ¯s ∈S we have that φ(¯s) = true if and only if f(¯s) = 0,
and the weights specify how severely the violation of a certain constraint is
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
penalised. The setting of these weights has a large impact on the EA performance, and ideally wi should reﬂect how hard ci is to satisfy. The problem
is that ﬁnding the appropriate weights requires much insight into the given
problem instance, and therefore it might not be practicable.
The stepwise adaptation of weights (SAW) mechanism, introduced by
Eiben and van der Hauw as an improved version of the weight adaptation
mechanism of Eiben, Rau´e, and Ruttkay , provides a simple and eﬀective way to set these weights. The basic idea behind the SAW mechanism is
that constraints that are not satisﬁed after a certain number of steps (ﬁtness
evaluations) must be diﬃcult, and thus must be given a high weight (penalty).
SAW-ing changes the evaluation function adaptively in an EA by periodically
checking the best individual in the population and raising the weights of those
constraints this individual violates. Then the run continues with the new evaluation function. A nice feature of SAW-ing is that it liberates the user from
seeking good weight settings, thereby eliminating a possible source of error.
Furthermore, the used weights reﬂect the diﬃculty of constraints for the given
algorithm on the given problem instance in the given stage of the search .
This property is also valuable since, in principle, diﬀerent weights could be
appropriate for diﬀerent algorithms.
6.3 Mutation
A large majority of work on adapting or self-adapting EA parameters concerns
variation operators: mutation and recombination (crossover). As we discussed
in section 3, the 1/5 rule of Rechenberg constitutes a ﬁrst historical example
for adaptive mutation step size control. It was followed by the traditional selfadaptive control of mutation step sizes, and more recently by the adaptive
CMA method.
Hesser and M¨anner derived theoretically optimal schedules within GAs
for deterministically changing pm for the counting-ones function. They suggest:
where α, β, γ are constants, L is the chromosome length, λ is the population
size, and t is the time (generation counter). This is a purely deterministic
parameter control mechanism.
A self-adaptive mechanism for controlling mutation in a bit-string GA is
given by B¨ack . This technique works by extending the chromosomes by
an additional 20 bits that together encode the individuals’ own pm. Mutation
then works by:
1. Decoding these bits ﬁrst to pm
2. Mutating the bits that encode pm with mutation probability pm
3. Decoding these (changed) bits to p′
4. Mutating the bits that encode the solution with mutation probability p′
Parameter Control in Evolutionary Algorithms
This approach is highly self-adaptive since even the rate of variation of the
search parameters is given by the encoded value, as opposed to the use of an
external parameters such as learning rates for step-sizes. More recently Smith
 showed theoretical predictions, veriﬁed experimentally, that this scheme
gets “stuck” in suboptimal regions of the search space with a low, or zero,
mutation rate attached to each member of the population. He showed that a
more robust problem-solving mechanism can simply be achieved by ignoring
the ﬁrst step of the algorithm above, and instead using a ﬁxed learning rate as
the probability of applying bitwise mutation to the encoding of the strategy
parameters in the second step.
6.4 Crossover
The classical example for adapting crossover rates in GAs is Davis’s adaptive
operator ﬁtness. The method adapts the rates of crossover operators by rewarding those that are successful in creating better oﬀspring. This reward is
diminishingly propagated back to operators of a few generations back, who
helped setting it all up; the reward is a shift up in probability at the cost
of other operators . This, actually, is very close in spirit to the “implicit
bucket brigade” credit assignment principle used in classiﬁer systems .
The GA using this method applies several crossover operators simultaneously within the same generation, each having its own crossover rate pc(opi).
Additionally, each operator has its “local delta” value di that represents the
strength of the operator measured by the advantage of a child created by using that operator with respect to the best individual in the population. The
local deltas are updated after every use of operator i. The adaptation mechanism recalculates the crossover rates after K generations. The main idea is
to redistribute 15% of the probabilities biased by the accumulated operator
strengths, that is, the local deltas. To this end, these di values are normalised
so that their sum equals 15, yielding dnorm
for each i. Then the new value for
each pc(opi) is 85% of its old value and its normalised strength:
pc(opi) = 0.85 · pc(opi) + dnorm
Clearly, this method is adaptive based on relative evidence.
6.5 Selection
It is interesting to note that neither the parent selection nor the survivor selection (replacement) component of an EA has been commonly used in an
adaptive manner, even though there are selection methods whose parameters
can be easily adapted. For example, in linear ranking there is a parameter s
representing the expected number of oﬀspring to be allocated to the best individual. By changing this parameter within the range of [1 . . . 2] the selective
pressure of the algorithm can be varied easily. Similar possibilities exist for
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
tournament selection, where the tournament size provides a natural parameter.
Most existing mechanisms for varying the selection pressure are based on
the so-called Boltzmann selection mechanism, which changes the selection
pressure during evolution according to a predeﬁned “cooling schedule” .
The name originates from the Boltzmann trial from condensed matter physics,
where a minimal energy level is sought by state transitions. Being in a state
i the chance of accepting state j is
P[accept j] =
where Ei, Ej are the energy levels, Kb is a parameter called the Boltzmann constant, and T is the temperature. This acceptance rule is called the
Metropolis criterion.
We illustrate variable selection pressure in the survivor selection (replacement) step by simulated annealing (SA). SA is a generate-and-test search
technique based on a physical, rather than a biological analogy . Formally,
however, SA can be envisioned as an evolutionary process with population
size of 1, undeﬁned (problem-dependent) representation and mutation, and a
speciﬁc survivor selection mechanism. The selective pressure changes during
the course of the algorithm in the Boltzmann style. The main cycle in SA is
given in Fig. 3.
/* given a current solution i ∈S */
/* given a function to generate the set of neighbours Ni of i */
generate j ∈Ni;
IF (f(i) < f(j)) THEN
set i = j;
> random[0, 1)) THEN
set i = j;
Fig. 3. Outline of the simulated annealing algorithm
In this mechanism the parameter ck, the temperature, decreases according to a predeﬁned scheme as a function of time, making the probability of
Parameter Control in Evolutionary Algorithms
accepting inferior solutions smaller and smaller (for minimisation problems).
From an evolutionary point of view, we have here a (1+1) EA with increasing
selection pressure.
A successful example of applying Boltzmann acceptance is that of Smith
and Krasnogor , who used it in the local search part of a memetic algorithm
(MA), with the temperature inversely related to the ﬁtness diversity of the
population. If the population contains a wide spread of ﬁtness values, the
“temperature” is low, so only ﬁtter solutions found by local search are likely
to be accepted, concentrating the search on good solutions. However, when
the spread of ﬁtness values is low, indicating a converged population which
is a common problem in MAs, the “temperature” is higher, making it more
likely that an inferior solution will be accepted, thus reintroducing diversity
and oﬀering a potential means of escaping from local optima.
6.6 Population
An innovative way to control the population size is oﬀered by Arabas et
al. in their GA with variable population size (GAVaPS). In fact, the
population size parameter is removed entirely from GAVaPS, rather than
adjusted on-the-ﬂy. Certainly, in an evolutionary algorithm the population
always has a size, but in GAVaPS this size is a derived measure, not a controllable parameter. The main idea is to assign a lifetime to each individual
when it is created, and then to reduce its remaining lifetime by one in each
consecutive generation. When the remaining lifetime becomes zero, the individual is removed from the population. Two things must be noted here. First,
the lifetime allocated to a newborn individual is biased by its ﬁtness: ﬁtter individuals are allowed to live longer. Second, the expected number of oﬀspring
of an individual is proportional to the number of generations it survives. Consequently, the resulting system favours the propagation of good genes.
Fitting this algorithm into our general classiﬁcation scheme is not straightforward because it has no explicit mechanism that sets the value of the population size parameter. However, the procedure that implicitly determines how
many individuals are alive works in an adaptive fashion using information
about the status of the search. In particular, the ﬁtness of a newborn individual is related to the ﬁtness of the present generation, and its lifetime is
allocated accordingly. This amounts to using relative evidence.
6.7 Varying Several Parameters Simultaneously
One of the studies explicitly devoted to adjusting more parameters (and also
on more than one level) is that of Hinterding et al. on a “self-adaptive GA”
 . This GA uses self-adaptation for mutation rate control, plus relativebased adaptive control for the population size.7 The mechanism for controlling
7Strictly speaking, the authors’ term “self-adaptive GA” is only partially correct.
However, this paper is from 1996, and the contemporary terminology distinguishing
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
mutation is similar to that from B¨ack , (Sect. 6.3), except that mutating the
bits encoding the mutation strength is not based on the bits in question, but is
done by a universal mechanism ﬁxed for all individuals and all generations. In
other words, the self-adaptive mutation parameter is only used for the genes
encoding a solution. As for the population size, the GA works with three subpopulations: a small, a medium, and a large one, P1, P2, and P3, respectively
(the initial sizes respectively being 50, 100, and 200). These populations are
evolved in parallel for a given number of ﬁtness evaluations (an epoch) independently by the same GA setup. After each epoch, the subpopulations are
resized based on some heuristic rules, maintaining a lower and an upper bound
(10 and 1000) and keeping P2 always the medium-sized subpopulation. There
are two categories of rules. Rules in the ﬁrst category are activated when
the ﬁtnesses in the subpopulations converge and try to move the populations
apart. For instance, if P2 and P3 have the same ﬁtness, the size of P3 is doubled. Rules from another set are activated when the ﬁtness values are distinct
at the end of an epoch. These rules aim at maximising the performance of
P2. An example of one such rule is: if the performance of the subpopulations
ranks them as P2 < P3 < P1 then size(P3) = (size(P2) + size(P3))/2. In
our taxonomy, this population size control mechanism is adaptive, based on
relative evidence.
Lis and Lis also oﬀer a parallel GA setup to control the mutation
rate, the crossover rate, and the population size during a run. The idea here
is that for each parameter a few possible values are deﬁned in advance, say
lo, med, hi, and only these values are allowed in any of the GAs, that is, in
the subpopulations evolved in parallel. After each epoch the performances of
the applied parameter values are compared by averaging the ﬁtnesses of the
best individuals of those GAs that use a given value. If the winning parameter
1. hi, then all GAs shift one level up concerning this parameter in the next
2. med, then all GAs use the same value concerning this parameter in the
next epoch;
3. lo, then all GAs shift one level down concerning this parameter in the
next epoch.
Clearly, the adjustment mechanism for all parameters here is adaptive, based
on relative evidence.
Mutation, crossover, and population size are all controlled on-the-ﬂy in
the GA “without parameters” of B¨ack et al. in . Here, the self-adaptive
mutation from (Sect. 6.3) is adopted without changes, a new self-adaptive
technique is invented for regulating the crossover rates of the individuals,
and the GAVaPS lifetime idea (Sect. 6.6) is adjusted for a steady-state GA
dynamic, adaptive, and self-adaptive schemes as we do it here was only published
in 1999 .
Parameter Control in Evolutionary Algorithms
model. The crossover rates are included in the chromosomes, much like the
mutation rates. If a pair of individuals is selected for reproduction, then their
individual crossover rates are compared with a random number r ∈ and
an individual is seen as ready to mate if its pc > r. Then there are three
possibilities:
1. If both individuals are ready to mate then uniform crossover is applied,
and the resulting oﬀspring is mutated.
2. If neither is ready to mate then both create a child by mutation only.
3. If exactly one of them is ready to mate, then the one not ready creates a
child by mutation only (which is inserted into the population immediately
through the steady-state replacement), the other is put on the hold, and
the next parent selection round picks only one other parent.
This study diﬀers from those discussed before in that it explicitly compares GA variants using only one of the (self-)adaptive mechanisms and the
GA applying them all. The experiments show remarkable outcomes: the completely (self-)adaptive GA wins, closely followed by the one using only the
adaptive population size control, and the GAs with self-adaptive mutation
and crossover are signiﬁcantly worse. These results suggest that putting effort into adapting the population size could be more eﬀective than trying to
adjust the variation operators. This is truly surprising considering that traditionally the on-line adjustment of the variation operators has been pursued
and the adjustment of the population size received relatively little attention.
The subject certainly requires more research.
7 Parameter Control in DyFor GP
Several studies have applied genetic programming (GP) to the task of forecasting with favourable results. However, these studies, like those applying
other heuristic and statistical techniques, have assumed a static environment,
making them unsuitable for many real-world time series which are generated
by varying processes. The study of Wagner et al. investigated the development of a new “dynamic” GP model (DyFor GP) that is speciﬁcally tailored
for forecasting in non-static environments. Note that if the underlying data
generating process shifts, the forecasting methods must be reevaluated in order to accommodate the new process. Additionally, these forecasting methods
require that the number of historical time series data used for analysis be designated shape a priori. This presents a problem in non-static environments
because diﬀerent segments of the time series may have diﬀerent underlying
data generating processes.
Usually some degree of human judgement is necessary to assign the number of historical data to be used for analysis. If the time series is not wellunderstood, then the assignment may contain segments with disparate underlying processes. This situation highlights the need for forecasting methods that
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
can automatically determine the correct analysis “window” (i.e., the correct
number of historical data to be analysed). Research reported in attempted
to develop a dynamic forecasting model (DyFor GP) that can do just that.
Furthermore, this study explores methods that can retain knowledge learnt
from previously encountered environments. Such past-learnt knowledge may
prove useful when current environmental conditions resemble those of a prior
setting. Speciﬁcally, this knowledge allows for faster convergence to current
conditions by giving the model searching process a “head-start” (i.e., by narrowing the model search space).
The DyFor GP model were tested for forecasting eﬃcacy on real-world
economic time series, namely the U.S. Gross Domestic Product and Consumer
Price Index Inﬂation. Results showed that the DyFor GP model outperformed
benchmark models from leading studies for both experiments. These ﬁndings
aﬃrmed the DyFor GP’s potential as an adaptive, non-linear model for realworld forecasting applications.
In addition to automatic time-horizon window adjustment, the DyFor GP
also contains features that automatically adjust population size and diversity.
The following subsections describe all these features.
7.1 Adapting the Analysis Window
As indicated earlier, designating the correct size for the analysis window is critical to the success of any forecasting model. Automatic discovery of this windowsize is indispensable when the forecasting concern is not well-understood.
With each slide of the window, the DyFor GP adjusts its windowsize dynamically. This is accomplished in the following way.
1. Select two initial windowsizes, one of size n and one of size n + i where n
and i are positive integers, i < n.
2. Run dynamic generations at the beginning of the historical data with
windowsizes n and n + i, use the best solution for each of these two independent runs to predict a number of future data points, and measure
their predictive accuracy.
3. Select another two windowsizes based on which windowsize had better
accuracy. For example if the smaller of the 2 windowsizes (size n) predicted
more accurately, then choose 2 new windowsizes, one of size n and one of
size n −i. If the larger of the 2 windowsizes (size n + i) predicted more
accurately, then choose windowsizes n + i and n + 2i.
4. Slide the analysis window to include the next time series observation. Use
the two selected windowsizes to run another two dynamic generations,
predict future data, and measure their prediction accuracy.
5. Repeat the previous two steps until the the analysis window reaches the
end of historical data.
Thus, at each slide of the analysis window, predictive accuracy is used to
determine the direction in which to adjust the windowsize.
Parameter Control in Evolutionary Algorithms
Consider the following example. Suppose the time series given in ﬁgure 4 is
to be analysed and forecast. As depicted in the ﬁgure, this time series consists
of two segments each with a diﬀerent underlying data generating process. The
22, 33, 30, 27, 24, 20, 21, 20, 20
, 23, 26, 29, 30, 28, 29, 32, 30, 31
Fig. 4. Time series containing segments with diﬀering underlying processes.
second segment’s underlying process represents the current environment and
is valid for forecasting future data. The ﬁrst segment’s process represents an
older environment that no longer exists but may contain patterns that can
be learnt and exploited when forecasting the current environment. If there
is no knowledge available concerning these segments, automatic techniques
are required to discover the correct windowsize needed to forecast the current
setting. The DyFor GP starts by selecting two initial windowsizes, one larger
than the other. Then, two separate dynamic generations are run at the beginning of the historical data, each with its own windowsize. After each dynamic
generation, the best solution is used to predict some number of future data
and the accuracy of this prediction is measured. Figure 5 illustrates these
steps. In the ﬁgure win1 and win2 represent data analysis windows of size 3
and 4, respectively, and pred represents the future data predicted.
33, 30, 27,
24, 20, 21, 20, 20
, 23, 26, 29, 30, 28, 29, 32, 30, 31
Fig. 5. Initial steps of window adaptation.
The data predicted in these initial steps lies inside the ﬁrst segment’s
process and, because the dynamic generation involving analysis window win2
makes use of a greater number of appropriate data than that of win1, it
is likely that win2’s prediction accuracy is better. If this is true, two new
windowsizes for win1 and win2 are selected with sizes of 4 and 5, respectively.
The analysis window then slides to include the next time series value, two new
dynamic generations are run, and the best solutions for each used to predict
future data. Figure 6 depicts these steps. In the ﬁgure data analysis windows
win1 and win2 now include the next time series value, 24, and pred has
shifted one value to the right.
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
33, 30, 27, 24,
20, 21, 20, 20
, 23, 26, 29, 30, 28, 29, 32, 30, 31
Fig. 6. Window adaptation after the ﬁrst window slide. Note: win1 and win2 have
size 4 and 5, respectively.
This process of selecting two new windowsizes, sliding the analysis window,
running two new dynamic generations, and predicting future data is repeated
until the analysis window reaches the end of historical data. It may be noted
that while the prediction data, pred, lies entirely inside the ﬁrst segment, the
data analysis windows, win1 and win2, are likely to expand to encompass
a greater number of appropriate data. However, after several window slides,
when the data analysis window spans data from both the ﬁrst and second
segments, it is likely that the window adjustment reverses direction.
The DyFor GP uses predictive accuracy to adapt the size of its analysis
window automatically. When the underlying process is stable (i.e., the analysis window is contained inside a single segment), the windowsize is likely to
expand. When the underlying process shifts (i.e., the analysis window spans
more than one segment), the windowsize is likely to contract.
7.2 The Problem of Bloat
Bloat in GP is the tendency for solution trees to grow large as they approach
the optimal. Solutions may become so large that they exhaust computer resources. Additionally, bloat hinders a GP model’s adaptability as solutions
become too specialised to adjust to changing conditions. Bloat is a problem
for any GP model regardless of the application. In order to allow the Dy-
For GP to search eﬃciently for an appropriate forecasting model, bloat must
be minimised without sacriﬁcing the quality of solutions. Two methods are
proposed to overcome this obstacle:
1. natural non-static population control and
2. dynamic increase of diversity.
GP models evolve a population of solutions for a given problem. Thus,
a GP model must contain some method to control the number and size of
solutions in any population. The standard method of GP population control
is due to Koza and uses a static population cardinality and a maximum
tree depth for solutions.However, this method does not protect a GP model
from bloat. If numerous solutions in a population have full or nearly full
trees of depth close to the maximum, available resources may be exhausted.
Additionally, the artiﬁcial limit for tree depth prohibits the search process
Parameter Control in Evolutionary Algorithms
from exploring solutions of greater complexity, which, especially for many
real-world problems, may be solutions of higher quality.
An alternative method for GP population control is presented to allow
natural growth of complex solutions in a setting that more closely emulates
the one found in nature. In nature the number of organisms in a population is
not static. Instead, the population cardinality varies as ﬁtter organisms occupy
more available resources and weaker organisms make do with less. Thus, from
generation to generation, the population cardinality changes depending on the
quality and type of individual organisms present. The proposed natural nonstatic population control (NNPC) is based on a variable population cardinality
with a limit on the total number of tree nodes present in a population and no
limit for solution tree depth. This method addresses the following issues:
1. allowing natural growth of complex solutions of greater quality,
2. keeping resource consumption within some speciﬁed limit, and
3. allowing the population cardinality to vary naturally based on the makeup of individual solutions present.
By not limiting the tree depth of individual solutions, natural evolution of
complex solutions is permitted. By restricting the total number of tree nodes
in a population, available resources are conserved. Thus, for a GP model that
employs NNPC, the number of solutions in a population grows or declines
naturally as the individual solutions in the population vary. This method is
described in more detail below.
NNPC works in the following way. Two node limits for a population are
speciﬁed as parameters: the soft node limit and the hard node limit. The soft
node limit is deﬁned as the limit for adding new solutions to a population.
This means that if adding a new solution to a population causes the total
nodes present to exceed the soft node limit, then that solution is the last one
added. The hard node limit is deﬁned as the absolute limit for total nodes in
a population. This means that if adding a new solution to a population causes
the total nodes present to exceed the hard node limit, then that solution may
be added only after it is repaired (the tree has been trimmed) so that the total
nodes present no longer exceeds this limit. During the selection process of the
DyFor GP, a count of the total nodes present in a population is maintained.
Before adding a new solution to a population, a check is made to determine if
adding the solution will increase the total nodes present beyond either of the
speciﬁed limits.
Wagner and Michalewicz provide a study comparing a GP forecasting
model with NNPC to one with the standard population control (SPC) method
introduced by Koza . Observed results indicate that the model with NNPC
was signiﬁcantly more eﬃcient in its consumption of computer resources than
the model with SPC while the quality of forecasts produced by both models
remained equivalent.
An important issue in GP is that of how diversity of GP populations can be
achieved and maintained. Diversity refers to non-homogeneity of solutions in
A.E. Eiben, Z. Michalewicz, M. Schoenauer, and J.E. Smith
a population . A population that is spread out over the search space has a
greater chance of ﬁnding an optimal solution than one that is concentrated in
a small area of the search space. The signiﬁcance of this concern is recognised
in .
A method that dynamically increases the diversity of a DyFor GP population is proposed to accomplish these objectives. The dynamic increase of
diversity (DIOD) method increases diversity by building a new population
before each dynamic generation using evolved components from the previous
dynamic generation. The following steps outline this procedure.
1. An initial population is constructed (using randomly generated trees in
the usual way) for the ﬁrst dynamic generation.
2. After the dynamic generation is completed, a new initial population is
constructed for the next dynamic generation that consists of two types of
solution trees:
a) randomly generated solution trees and
b) solution trees that are subtrees of ﬁtter solutions from the last population of the previous dynamic generation.
3. The previous step is repeated after each successive dynamic generation.
Thus, each new dynamic generation after the ﬁrst starts with a new initial
population whose solution trees are smaller than those of the last population of
the previous dynamic generation but have not lost the adaptations gained from
past dynamic generations. In this way, solution tree bloat is reduced without
harming the quality of solutions. Additionally, because randomly generated
trees make up a portion of the new population, diversity is increased.
8 Discussion
Summarising this paper a number of things can be noted. First, parameter
control in an EA can have two purposes. It can be done to avoid suboptimal
algorithm performance resulting from suboptimal parameter values set by the
user. The basic assumption here is that the applied control mechanisms are
intelligent enough to do this job better than the user could, or that they can do
it approximately as good, but they liberate the user from doing it. Either way,
they are beneﬁcial. The other motivation for controlling parameters on-the-
ﬂy is the assumption that the given parameter can have a diﬀerent “optimal”
value in diﬀerent phases of the search. If this holds, then there is simply no
optimal static parameter value; for good EA performance one must vary this
parameter.
The second thing we want to note is that making a parameter (self-
)adaptive does not necessarily mean that we have an EA with fewer parameters. For instance, in GAVaPS the population size parameter is eliminated at
the cost of introducing two new ones: the minimum and maximum lifetime of
Parameter Control in Evolutionary Algorithms
newborn individuals. If the EA performance is sensitive to these new parameters then such a parameter replacement can make things worse. This problem
also occurs on another level. One could say that the procedure that allocates
lifetimes in GAVaPS, the probability redistribution mechanism for adaptive
crossover rates (Sect. 6.4), or the function specifying how the σ values are mutated in ES are also (meta) parameters. It is in fact an assumption that these
are intelligently designed and their eﬀect is positive. In many cases there are
more possibilities, that is, possibly well-working procedures one can design.
Comparing these possibilities implies experimental (or theoretical) studies
very much like comparing diﬀerent parameter values in a classical setting.
Here again, it can be the case that algorithm performance is not so sensitive
to details of this (meta) parameter, which fully justiﬁes this approach.
Finally, let us place the issue of parameter control in a larger perspective. Over the last 20 years the EC community shifted from believing that
EA performance is to a large extent independent from the given problem instance to realising that it is. In other words, it is now acknowledged that EAs
need more or less ﬁne-tuning to speciﬁc problems and problem instances. Ideally, it should be the algorithm that performs the necessary problem-speciﬁc
adjustments. Parameter control as discussed here is a step towards this.