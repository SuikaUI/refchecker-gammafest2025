Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 650–655
Melbourne, Australia, July 15 - 20, 2018. c⃝2018 Association for Computational Linguistics
Breaking NLI Systems
with Sentences that Require Simple Lexical Inferences
Max Glockner1, Vered Shwartz2 and Yoav Goldberg2
1Computer Science Department, TU Darmstadt, Germany
2Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel
{maxg216,vered1986,yoav.goldberg}@gmail.com
We create a new NLI test set that shows
the deﬁciency of state-of-the-art models in
inferences that require lexical and world
knowledge. The new examples are simpler than the SNLI test set, containing sentences that differ by at most one word
from sentences in the training set.
the performance on the new test set is substantially worse across systems trained on
SNLI, demonstrating that these systems
are limited in their generalization ability,
failing to capture many simple inferences.
Introduction
Recognizing textual entailment (RTE) , recently framed as natural language
inference (NLI) is a task
concerned with identifying whether a premise sentence entails, contradicts or is neutral with the hypothesis sentence.
Following the release of the
large-scale SNLI dataset ,
many end-to-end neural models have been developed for the task, achieving high accuracy on the
test set. As opposed to previous-generation methods, which relied heavily on lexical resources,
neural models only make use of pre-trained word
embeddings. The few efforts to incorporate external lexical knowledge resulted in negligible performance gain .
This raises
the question whether (1) neural methods are inherently stronger, obviating the need of external lexical knowledge; (2) large-scale training data allows
for implicit learning of previously explicit lexical
knowledge; or (3) the NLI datasets are simpler
than early RTE datasets, requiring less knowledge.
1The contradiction example follows the assumption in
Bowman et al. that the premise contains the most
prominent information in the event, hence the premise can’t
describe the event of a man holding both instruments.
Premise/Hypothesis
The man is holding a saxophone
contradiction1
The man is holding an electric guitar
A little girl is very sad.
entailment
A little girl is very unhappy.
A couple drinking wine
A couple drinking champagne
Table 1: Examples from the new test set.
In this paper we show that state-of-the-art NLI
systems are limited in their generalization ability,
and fail to capture many simple inferences that require lexical and world knowledge. Inspired by
the work of Jia and Liang on reading comprehension, we create a new NLI test set with examples that capture various kinds of lexical knowledge (Table 1). For example, that champagne is
a type of wine (hypernymy), and that saxophone
and electric guitar are different musical instruments (co-hyponyms). To isolate lexical knowledge aspects, our constructed examples contain
only words that appear both in the training set and
in pre-trained embeddings, and differ by a single
word from sentences in the training set.
The performance on the new test set is substantially worse across systems, demonstrating that the
SNLI test set alone is not a sufﬁcient measure of
language understanding capabilities. Our results
are in line with Gururangan et al. and Poliak et al. , who showed that the label can
be identiﬁed by looking only at the hypothesis and
exploiting annotation artifacts such as word choice
and sentence length.
Further investigation shows that what mostly
affects the systems’ ability to correctly predict
a test example is the amount of similar examples found in the training set. Given that training data will always be limited, this is a rather
inefﬁcient way to learn lexical inferences, stressing the need to develop methods that do this more
effectively.
Our test set can be used to evaluate such models’ ability to recognize lexical inferences, and it is available at 
com/BIU-NLP/Breaking_NLI.
Background
NLI Datasets.
The SNLI dataset 
consists of 570k sentence-pairs manually labeled
as entailment, contradiction, and neutral. Premises
are image captions from Young et al. , while
hypotheses were generated by crowd-sourced
workers who were shown a premise and asked to
generate entailing, contradicting, and neutral sentences. Workers were instructed to judge the relation between sentences given that they describe
the same event. Hence, sentences that differ by a
single mutually-exclusive term should be considered contradicting, as in “The president visited Alabama” and “The president visited Mississippi”.
This differs from traditional RTE datasets, which
do not assume event coreference, and in which
such sentence-pairs would be considered neutral.
Following criticism on the simplicity of the
dataset, stemming mostly from its narrow domain,
two additional datasets have been collected. The
MultiNLI dataset was collected
similarly to SNLI, though covering a wider range
of genres, and supporting a cross-genre evaluation.
The SciTail dataset , created
from science exams, is somewhat different from
the two datasets, being smaller (27,026 examples),
and labeled only as entailment or neutral. The domain makes this dataset different in nature from
the other two datasets, and it consists of more factual sentences rather than scene descriptions.
Neural Approaches for NLI.
Following the release of SNLI, there has been tremendous interest in the task, and many end-to-end neural models were developed, achieving promising results.2
Methods are divided into two main approaches.
Sentence-encoding models encode the premise and hypothesis individually, while attention-based models align words
in the premise with similar words in the hypothesis, encoding the two sentences together .
2See the SNLI leaderboard for a comprehensive list:
 
External Lexical Knowledge.
Traditional RTE
methods typically relied on resources such as
WordNet to identify lexical inferences. Conversely, neural methods rely solely
on pre-trained word embeddings, yet, they achieve
high accuracy on SNLI.
The only neural model to date that incorporates external lexical knowledge (from WordNet)
is KIM , however, gaining only
a small addition of 0.6 points in accuracy on the
SNLI test set. This raises the question whether the
small performance gap is a result of the model not
capturing lexical knowledge well, or the SNLI test
set not requiring this knowledge in the ﬁrst place.
Data Collection
We construct a test set with the goal of evaluating
the ability of state-of-the-art NLI models to make
inferences that require simple lexical knowledge.
We automatically generate sentence pairs (§3.1)
which are then manually veriﬁed (§3.2).
Generating Adversarial Examples
In order to isolate the lexical knowledge aspects,
the premises are taken from the SNLI training set.
For each premise we generate several hypotheses
by replacing a single word within the premise by
a different word. We also allow some multi-word
noun phrases (“electric guitar”) and adapt determiners and prepositions when needed.
We focus on generating only entailment and
contradiction examples, while neutral examples
may be generated as a by-product.
Entailment
examples are generated by replacing a word with
its synonym or hypernym, while contradiction examples are created by replacing words with mutually exclusive co-hyponyms and antonyms (see
Table 1). The generation steps are detailed below.
Replacement Words.
We collected the replacement words using online resources for English
learning.3
The newly introduced words are all
present in the SNLI training set:
from occurrence in a single training example (“Portugal”)
up to 248,051 examples (“man”), with a mean of
3,663.1 and a median of 149.5. The words are
also available in the pre-trained embeddings vocabulary. The goal of this constraint is to isolate
lexical knowledge aspects, and evaluate the models’ ability to generalize and make new inferences
for known words.
3www.enchantedlearning.com, www.smart-words.org
Instances:
contradiction
entailment
contradiction
entailment
Estimated human performance:
Table 2: Statistics of the test sets. 9,815 is the
number of samples with majority agreement in the
SNLI test set, whose full size is 9,824.
Replacement words are divided into topical categories detailed in Table 4. In several categories
we applied additional processing to ensure that examples are indeed mutually-exclusive, topicallysimilar, and interchangeable in context. We included WordNet antonyms with the same part-ofspeech and with a cosine similarity score above a
threshold, using GloVe .
In nationalities and countries we focused on countries which are related geographically (Japan,
China) or culturally (Argentina, Spain).
Sentence-Pairs.
To avoid introducing new information not present in the training data, we sampled premises from the SNLI training set that contain words from our lists, and generated hypotheses by replacing the selected word with its replacement. Some of the generated sentences may be ungrammatical or nonsensical, for instance, when replacing Jordan with Syria in sentences discussing
Michael Jordan. We used Wikipedia bigrams4 to
discard sentences in which the replaced word created a bigram with less than 10 occurrences.
Manual Veriﬁcation
We manually verify the correctness of the automatically constructed examples using crowdsourced workers in Amazon Mechanical Turk. To
ensure the quality of workers, we applied a quali-
ﬁcation test and required a 99% approval rate for
at least 1,000 prior tasks. We assigned each annotation to 3 workers.
Following the SNLI guidelines, we instructed
the workers to consider the sentences as describing
the same event, but we simpliﬁed the annotation
process into answering 3 simple yes/no questions:
1. Do the sentences describe the same event?
4github.com/rmaestre/Wikipedia-Bigram-Open-Datasets
2. Does the new sentence (hypothesis) add new
information to the original sentence (premise)?
3. Is the new sentence incorrect/ungrammatical?
We then discarded any sentence-pair in which
at least one worker answered the third question
positively. If the answer to the ﬁrst question was
negative, we considered the label as contradiction.
Otherwise, we considered the label as entailment
if the answer to the second question was negative
and neutral if it was positive. We used the majority vote to determine the gold label.
The annotations yielded substantial agreement,
with Fleiss’ Kappa κ = 0.61 . We estimate human performance to 94.1%,
using the method described in Gong et al. ,
showing that the new test set is substantially easier
to humans than SNLI. Table 2 provides additional
statistics on the test set.5
Evaluation
Without External Knowledge.
We chose 3 representative models in different approaches (sentence encoding and/or attention):
STACKED-ENCODER is
a biLSTM-based single sentence-encoding model
without attention. As opposed to traditional multilayer biLSTMs, the input to each next layer is
the concatenation of the word embedding and
the summation of outputs from previous layers. ESIM is a hybrid TreeLSTM-based
and biLSTM-based model.
We use the biL-
STM model, which uses an inter-sentence attention mechanism to align words across sentences.
Finally, DECOMPOSABLE ATTENTION performs soft alignment of words
from the premise to words in the hypothesis using attention mechanism, and decomposes the task
into comparison of aligned words. Lexical-level
decisions are merged to produce the ﬁnal classiﬁcation. We use the AllenNLP re-implementation,6
which does not implement the optional intrasentence attention, and achieves an accuracy of
84.7% on the SNLI test set, comparable to 86.3%
by the original system.
5We note that due to its bias towards contradiction, the
new test set can neither be used for training, nor serve as a
main evaluation set for NLI. Instead, we suggest to use it in
addition to the original test set in order to test a model’s ability to handle lexical inferences.
6 
SNLI test set
New test set
Decomposable Attention
 
MultiNLI + SNLI
SciTail + SNLI
ESIM 
MultiNLI + SNLI
SciTail + SNLI
Residual-Stacked-Encoder
 
MultiNLI + SNLI
SciTail + SNLI
WordNet Baseline
KIM 
Table 3: Accuracy of various models trained on SNLI or a union of SNLI with another dataset (MultiNLI,
SciTail), and tested on the original SNLI test set and the new test set.
We chose models which are amongst the best
performing within their approaches (excluding ensembles) and have available code.
All models
are based on pre-trained GloVe embeddings , which are either ﬁne-tuned
during training (RESIDUAL-STACKED-ENCODER
and ESIM) or stay ﬁxed (DECOMPOSABLE AT-
TENTION). All models predict the label using a
concatenation of features derived from the sentence representations (e.g. maximum, mean), for
example as in Mou et al. . We use the recommended hyper-parameters for each model, as
they appear in the provided code.
With External Knowledge.
We provide a simple WORDNET BASELINE, in which we classify
a sentence-pair according to the WordNet relation
that holds between the original word wp and the
replaced word wh. We predict entailment if wp is
a hyponym of wh or if they are synonyms, neutral
if wp is a hypernym of wh, and contradiction if wp
and wh are antonyms or if they share a common
hypernym ancestor (up to 2 edges). Word pairs
with no WordNet relations are classiﬁed as other.
performance
 , an extension of ESIM with external knowledge from WordNet, which was kindly provided
to us by Qian Chen. KIM improves the attention
mechanism by taking into account the existence
of WordNet relations between the words. The lexical inference component, operating over pairs of
aligned words, is enriched with a vector encoding
the speciﬁc WordNet relations between the words.
Experimental Settings
We trained each model on 3 different datasets: (1)
SNLI train set, (2) a union of the SNLI train set
and the MultiNLI train set, and (3) a union of the
SNLI train set and the SciTail train set. The motivation is that while SNLI might lack the training
data needed to learn the required lexical knowledge, it may be available in the other datasets,
which are presumably richer.
Table 3 displays the results for all the models on
the original SNLI test set and the new test set. Despite the task being considerably simpler, the drop
in performance is substantial, ranging from 11 to
33 points in accuracy. Adding MultiNLI to the
training data somewhat mitigates this drop in accuracy, thanks to almost doubling the amount of
training data. We note that adding SciTail to the
training data did not similarly improve the performance; we conjecture that this stems from the differences between the datasets.
KIM substantially outperforms the other neural
models, demonstrating that lexical knowledge is
the only requirement for good performance on the
new test set, and stressing the inability of the other
models to learn it. Both WordNet-informed models leave room for improvement: possibly due to
limited WordNet coverage and the implications of
applying lexical inferences within context.
We take a deeper look into the predictions of the
models that don’t employ external knowledge, focusing on the models trained on SNLI.
Accuracy by Category
Table 4 displays the accuracy of each model per
replacement-word category.
The neural models
tend to perform well on categories which are frequent in the training set, such as colors, and badly
Decomposable
loves - dislikes
ﬁve - seven
nationalities
Greek - Italian
lemonade - beer
antonyms (WN)
sitting - standing
red - blue
ﬁfth - 16th
Mexico - Peru
kitchen - bathroom
stone - glass
vegetables
tomato -potato
instruments
harmonica - harp
Mars - Venus
happy - joyful
Table 4: The number of instances and accuracy per category achieved by each model.
on categories such as planets, which rarely occur
in SNLI. These models perform better than the
WordNet baseline on entailment examples (synonyms), suggesting that they do so due to high
lexical overlap between the premise and the hypothesis rather than recognizing synonymy. We
therefore focus the rest of the discussion on contradiction examples.
Accuracy by Word Similarity
The accuracies for ordinals, nationalities and
countries are especially low. We conjecture that
this stems from the proximity of the contradicting words in the embedding space. Indeed, the
Decomposable Attention model—which does not
update its embeddings during training—seems to
suffer the most.
Grouping its prediction accuracy by the cosine
similarity between the contradicting words reveals
a clear trend that the model errs more on contradicting pairs with similar pre-trained vectors:7
Similarity
Accuracy by Frequency in Training
Models that ﬁne-tune the word embeddings may
beneﬁt from training examples consisting of test
replacement pairs. Namely, for a given replacement pair (wp, wh), if many training examples labeled as contradiction contain wp in the premise
and wh in the hypothesis, the model may update
their embeddings to optimize predicting contradiction. Indeed, we show that the ESIM accuracy on
test pairs increases with the frequency in which
7We ignore multi-word replacements in §5.2 and §5.3.
their replacement words appear in contradiction
examples in the training data:
40.2% 70.6% 91.4% 92.1% 97.5% 98.5%
This demonstrates that the model is capable of
learning lexical knowledge when sufﬁcient training data is given, but relying on explicit training
examples is a very inefﬁcient way of obtaining
simple lexical knowledge.
Conclusion
We created a new NLI test set with the goal of
evaluating systems’ ability to make inferences that
require simple lexical knowledge. Although the
test set is constructed to be much simpler than
SNLI, and does not introduce new vocabulary, the
state-of-the-art systems perform poorly on it, suggesting that they are limited in their generalization
ability. The test set can be used in the future to assess the lexical inference abilities of NLI systems
and to tease apart the performance of otherwise
very similarly-performing systems.
Acknowledgments
We would like to thank Qian Chen for evaluating KIM on our test set.
This work was supported in part by the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1), an Intel ICRI-CI
grant, Theo Hoffenberg, and the Israel Science
Foundation grants 1951/17 and 1555/15. Vered is
also supported by the Clore Scholars Programme
 , and the AI2 Key Scientiﬁc Challenges
Program .