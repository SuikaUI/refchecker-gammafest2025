Cosmological parameters from CMB and other data: a Monte-Carlo approach
Antony Lewis1, ∗and Sarah Bridle2, †
1DAMTP, CMS, Wilberforce Road, Cambridge CB3 0WA, UK.
2Institute of Astronomy, Madingley Road, Cambridge, CB3 0HA, UK.
We present a fast Markov Chain Monte-Carlo exploration of cosmological parameter space. We
perform a joint analysis of results from recent CMB experiments and provide parameter constraints,
including σ8, from the CMB independent of other data. We next combine data from the CMB, HST
Key Project, 2dF galaxy redshift survey, supernovae Ia and big-bang nucleosynthesis. The Monte
Carlo method allows the rapid investigation of a large number of parameters, and we present results
from 6 and 9 parameter analyses of ﬂat models, and an 11 parameter analysis of non-ﬂat models.
Our results include constraints on the neutrino mass (mν ≲0.3 eV), equation of state of the dark
energy, and the tensor amplitude, as well as demonstrating the eﬀect of additional parameters on
the base parameter constraints. In a series of appendices we describe the many uses of importance
sampling, including computing results from new data and accuracy correction of results generated
from an approximate method. We also discuss the diﬀerent ways of converting parameter samples
to parameter constraints, the eﬀect of the prior, assess the goodness of ﬁt and consistency, and
describe the use of analytic marginalization over normalization parameters.
INTRODUCTION
There is now a wealth of data from cosmic microwave background (CMB) observations and growing amount of
information on large scale structure from a wide range of sources. We would like to extract the maximum amount of
information from this data, usually conveniently summarized by estimates of a set of cosmological parameter values.
With high quality data one can constrain a large number of parameters, which in principle allows us not only to
put estimates and error bars on various quantitative parameters, but also to address more fundamental qualitative
questions: Do we observe primordial gravitational waves? Do the neutrinos have a cosmologically signiﬁcant mass?
Is the universe ﬂat? Are the standard model parameters those that best account for the data? In addition we can
also assess the consistency of the diﬀerent data sets with respect to a cosmological model.
Recent work involving parameter estimation from the CMB includes Refs. . In this paper
we employ Markov Chain Monte Carlo (MCMC) techniques , as advocated for Bayesian CMB analysis in
Ref. and demonstrated by Refs. . By generating a set of MCMC chains we can obtain a set of independent
samples from the posterior distribution of the parameters given the data. From a relatively small number of these
samples one can estimate the marginalized posterior distributions of the parameters, as well as various other statistical
quantities. The great advantage of the MCMC method is that it scales, at its best, approximately linearly with the
number of parameters, allowing us to include many parameters for only small additional computational cost. The
samples also probe the shape of the full posterior, giving far more information that just the marginalized distributions.
Throughout we assume models with purely adiabatic Gaussian primordial perturbations in the growing mode (our
approach could easily be generalized to include isocurvature modes), three neutrino species, non-interacting cold dark
matter, and standard general relativity. We compute all theoretical (CMB and matter power spectrum) predictions
numerically using using the fast Boltzmann code camb (a parallelized version of cmbfast ). Our results are
therefore limited by the accuracy of the data and could be generalized very easily to include any additional parameters
that can be accounted for by modiﬁcation of a Boltzmann code.
In section II we present constraints from the latest CMB data, illustrating the MCMC method. We defer a brief
introduction to the MCMC method and a description of our implementation and terminology to Appendix A. The
method of importance sampling is also illustrated in section II, and is described in detail Appendix B, where we
explain how it can be used to take into account diﬀerent priors on the parameters, new data, and for accurate but fast
estimation given a good approximation to the theoretical model predictions. We add large scale structure, supernova
and nucleosynthesis constraints in section III so that more parameters can be constrained. We compare ﬂat models
∗Electronic address: 
†Electronic address: 
Cl l(l+1) /2π /10−12
The CMB temperature anisotropy band-power data used in this paper. The line shows the model with the parameters
at their mean values, given all data after marginalizing in 6 dimensions (i.e. ﬁrst column of Table. II).
with ‘inﬂationary’ priors (9 cosmological parameters) and then a more general models (11 cosmological parameters).
This includes constraints on the neutrino mass, equation of state of the dark energy, and the tensor amplitude. In
addition our results show the sensitivity of constraints on the standard parameters to variations in the underlying
CMB CONSTRAINTS
We use the results of the COBE , BOOMERANG , MAXIMA , DASI , VSA and CBI 
observations in the form of band power estimates for the temperature CMB power spectrum. These data are plotted
in Fig. 1. The very small angular scale results from CBI have been discussed extensively in and do not ﬁt the
linear predictions of standard acoustic oscillation models. Therefore for the purposes of this paper we assume that
the small scale power observed by CBI has its origin in non-linear or non-standard small scale eﬀects that we do not
attempt to model, and so use only the mosaic (rather than deep pointing) data points throughout. In addition we
use only the ﬁrst 8 points from the odd binning of the mosaic ﬁelds since above that the noise in the band
powers becomes much larger than the prediction for the class of models we consider.
For COBE we use the oﬀset-lognormal band-powers and covariance matrix from RADPACK . For DASI, VSA
and CBI we also use the oﬀset-lognormal band powers and integrate numerically over an assumed Gaussian calibration
uncertainty. For BOOMERANG, MAXIMA we assume top hat window functions and uncorrelated Gaussian bandpower likelihood distributions, and marginalize analytically over the calibration and beam uncertainties assuming they
are also Gaussian . We assume a correlated calibration uncertainty of 10% on the CBI and VSA data (neglecting
the ∼3% uncorrelated diﬀerence in calibration), but otherwise assume all the observations are independent. Using
the sets of samples obtained it is a simple matter to account for small corrections when the required information is
publicly available (see Appendix B).
The base set of cosmological parameters we sample over are ωb = Ωbh2 and ωc ≡Ωch2, the physical baryon and
CDM densities relative to the critical density, h = H0/(100 km s−1Mpc−1), the Hubble parameter, ΩK ≡1 −Ωtot
measuring the spatial curvature, zre, the redshift at which the reionization fraction is a half 1, As, measuring the
initial power spectrum amplitude and ns, the spectral index of the initial power spectrum. We derive ΩΛ, the ratio
1 The CMB temperature anisotropy is very insensitive to the duration of reionization epoch, and we also neglect the small eﬀect of Helium
reionization and inhomogeneities.
0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95
Left: 2000 samples from the posterior distribution of the parameters plotted by their Ωm and ΩΛ values. Points are
colored according to the value of h of each sample, and the solid line shows the ﬂat universe parameters. We assume the base
parameter set with broad top-hat priors.
Right: bottom layer (green): supernova constraints; next layer up (red): CMB data alone; next (blue): CMB data plus HST
Key Project prior; top layer (yellow): all data combined (see text). 68 and 95 per cent conﬁdence limits are shown.
of the critical density in the form of dark energy, from the constraint ΩK + ΩΛ + Ωm = 1 (where Ωm ≡Ωc + Ωb is
the total matter density in units of the critical density). Throughout we use at least the priors that 4 < zre < 20,
0.4 < h < 1.0, −0.3 < ΩK < 0.3, ΩΛ > 0, and that the age of the universe, t0, is 10 Gyr < t0 < 20 Gyr. The
signiﬁcance of this base set is that this deﬁnes the Bayesian priors: there is a ﬂat prior on each parameter of the
base set. We discuss later how we assess the signiﬁcance of these priors, and highlight our main results, which are
largely independent of the priors. (We choose h as a base parameter since the HST Key Project provides a direct
constraint on this quantity, whereas there are no direct constraints on, e.g. ΩΛ; see Appendix C for discussion). The
above additional constraints on h, ΩΛ, ΩK and the age have little eﬀect on the joint results since the cut-oﬀvalues
are well into the tails of the distribution. However for the purpose of the Monte-Carlo it is very convenient to be able
to quickly reject models in the extreme tails without having to compute the theoretical predictions.
MCMC illustration
An MCMC sampler provides an eﬃcient way to generate a list of samples from a probability distribution (see
Appendix A for an explanation). All that is required is a function for calculating the probability given a set of
parameter values. A single sample is a coordinate in the n-D parameter space, and the sampling method ensures that
the number density of samples is asymptotically proportional to the probability density. As an illustration, in the left
hand panel of Figure 2 we show the values of Ωm = Ωb + Ωc, and ΩΛ, for samples collected from an MCMC run using
the CMB data and base parameters discussed in the previous paragraphs.
Note that although Ωm is not one of our base set of parameters, it is simple to ﬁnd probabilities as a function of Ωm
by taking the parameter values of each sample and deriving the corresponding values of Ωm. Since the MCMC method
produces samples from the full posterior, it follows that the number density of samples in this two-dimensional plane
is proportional to the probability density of the two parameters (marginalized over all the other parameters; note that
this refers to the fully marginalized probability density rather than any conditional or projected probability density).
The familiar direction of CMB degeneracy along the ﬂat universe line is apparent. The colors of the dots indicate the
Hubble constant value for each sample, as given in the color bar. This shows that the high Ωm tail of samples are
due entirely to low h regions of parameter space, illustrating the point made in e.g. that a Hubble constant prior
alone combined with the CMB can put useful limits on Ωm and ΩΛ without the need for supernova constraints.
The likelihood as a function of position in the Ωm-ΩΛ plane from CMB data alone is shown by the red contours in
pre VSA/CBI
0.0215 ± 0.0022 0.0216 ± 0.0022 0.0212 ± 0.0014
0.985 ± 0.051
0.982 ± 0.050
0.980 ± 0.037
v1 ≡Ωmh2.4(σ8e−τ/0.7)−0.85
0.114 ± 0.0052
0.113 ± 0.0048
0.111 ± 0.0038
v2 ≡σ8e−τ(h/0.7)0.5(Ωm/0.3)−0.08
0.71 ± 0.07
0.70 ± 0.06
0.70 ± 0.04
TABLE I: Marginalized parameter constraints (68 per cent conﬁdence) on four well constrained parameters (see text) from
two combinations of CMB data assuming a ﬂat universe and varying [Ωbh2, Ωch2, h, ns, zre, As]. Extra digits are inserted to
help comparison between pre and post VSA and CBI. For comparison the last column gives the results including HST, 2dF,
BBN and SnIA data.
the right hand panel of Figure 2. Note that compared to some other CMB only plots the contours close slightly at
high Ωm, which is due to our lower limit on the Hubble constant of h > 0.4. The 2-D likelihoods shown are calculated
from the samples by estimating the probability density at a grid of points using a Gaussian weighting kernel and
then using interpolation to draw the contour lines. This produces a plot very similar to that obtained using a grid
integration calculation of the marginalized posterior (assuming there are no small scale features in the posterior).
Extra data can be taken into account quickly by re-weighting the samples, a technique known as importance
sampling (described in detail in Appendix B). The posterior is simply re-calculated at each sample position, and a
weight assigned to the sample according to the ratio of the new posterior to the old posterior. For example, using a
Gaussian HST Key Project prior on the Hubble constant h = 0.72 ± 0.08 we obtain the blue contours plotted on
the right hand panel of Figure 2. By using the weighted number of points as a function of ΩK we ﬁnd the marginalized
result ΩK = 0.00 ± 0.03, and therefore that the universe is close to ﬂat (given our assumptions).
Using importance sampling we have checked that the CMB datasets are consistent using the hyperparameter
method, as described in Appendix E.
Quantitative constraints from CMB data alone
Above we illustrated the MCMC method in the Ωm-ΩΛ plane for a 7 parameter cosmological model. Since there is
good observational evidence and theoretical motivation for ﬂat models, we now ﬁx ΩK = 0 giving 6 base parameters.
The only parameters that can be reasonably disentangled are ωb and ns, from the heights of the 2nd and 3rd
acoustic peaks relative to the ﬁrst. These constraints are given in the ﬁrst two rows of Table I and are in accordance
with previous similar analyses and nucleosynthesis constraints . However note that these two parameters remain
signiﬁcantly correlated, so that more detailed information is contained in the result ns−(ωb−0.022)/0.043 = 0.98±0.04,
ns + (ωb −0.022)/0.043 = 0.98 ± 0.09.
Qualitatively, having ‘used up’ constraints from the 2nd and 3rd acoustic peak heights to ﬁnd Ωbh2 and ns we
might estimate that there remain three more pieces of information in the CMB data, from the large scale (COBE)
amplitude, the ﬁrst peak height and the ﬁrst peak position. We have four remaining parameters e.g. parameterised
by σ8, Ωm, h and τ (τ is the optical depth to reionization2, and σ8 is the root mean square mass perturbation in
8h−1Mpc spheres today assuming linear evolution). Since Ωbh2 and ns are not too correlated with these parameters,
we marginalize over them and explore the remaining four dimensional parameter space.
From the set of samples it is straightforward to perform an independent component analysis to identify the well
determined orthogonal parameter combinations. Calculating the eigenvectors of the correlation matrix in the logs of
σ8e−τ, Ωm and h we ﬁnd the two combinations given in the bottom two rows of Table I. The errors are too large on
any third constraint to be worth quoting. We expect σ8 to be roughly degenerate with e−τ because the CMB power
on scales smaller than the horizon size at reionization (ℓ≳20) is damped by a factor e−2τ, and σ2
8 scales with the
power in the primordial perturbation. We ﬁnd that marginalized values of v1 and v2 are independent of τ to better
than 2 per cent for 0.02 < τ < 0.14 (4 < zre < 16). As demonstrated in Appendix C these constraints are almost
independent of the choice of base parameters (which deﬁne the prior).
If we marginalize over σ8 and zre we ﬁnd the principle component Ωmh2.9 = 0.096 ± 9%, consistent with the
constraint found and discussed in Ref. . However since this quantity is signiﬁcantly correlated with the amplitude
2 Assuming rapid reionization the optical depth can be calculated from zre using τ = σT
dzne(z)/[(1 + z)2H(z)], where ne is
the number density of electrons and σT is the Thompson scattering cross-section.
For ﬂat models with cosmological constant τ ≈
0.048 Ωb h Ω−0.47
(to a few percent over the region of interest), though we do not use this in our analysis.
we have quoted a tighter constraint (4%) in Table I by including the σ8e−τ dependence in our results.3
While restricted to this relatively small parameter space we take this opportunity to investigate the impact of the
new VSA and CBI results. Using importance sampling we compare the results with and without the VSA/CBI data
in Table I. For simplicity we assume the same power law approximation for the combination of σ8e−τ, h and Ωm as
derived above. The peaks move by a fraction of the error, and the error bars are fractionally smaller.
ADDITIONAL COSMOLOGICAL CONSTRAINTS
The CMB data alone can only provide a limited number of constraints, so before extending the parameter space
to make full use of the Monte-Carlo method it is useful to include as much relatively reliable data as possible. Care
must be taken, since some published parameter constraints assume particular values for parameters that we wish to
vary. As a simple example, the Supernova Cosmology Project has made available constraints in Ωm-ΩΛ space, which
could only be used if the dark energy is a cosmological constant. Fortunately the full supernova data are available,
which we use (described below). However is is not always practical to use the full observed data and it may be best
to simply increase the error bars to encompass systematic eﬀects due to other parameters. This ensures that the
resulting MCMC chains will cover all of the relevant parameter space, and can then be importance sampled later
with a tighter constraint. If the chains are generated too tightly constrained one cannot recover information about
the excluded parameter space.
Nucleosynthesis constraints suggest ωb ≈0.02 , and we assume the Gaussian prior ωb = 0.020 ± 0.002, (1σ)
which is somewhat broader than the error quoted in Ref. to allow for diﬀerences with other estimations. We
include Type 1A supernovae data from , using the eﬀective magnitudes and errors from the 54 supernovae that
they included in the primary ﬁt (ﬁt C). We marginalize analytically with a ﬂat prior on the intrinsic magnitudes, which
is equivalent to evaluating the likelihood at the best ﬁt value (see Appendix F). We neglect the small correlations
but multiply the log likelihood by 50/54 to account for the eﬀective degrees of freedom quoted in Ref. . We
use the galaxy power spectrum from the ﬁrst 147,000 redshifts of the 2dF galaxy redshift survey, using the scales
0.02 < k/(hMpc−1) < 0.15 where non-linear eﬀects were found to be negligible . We assume that this is directly
proportional to the matter power spectrum at z = 0, in other words that the bias and evolution are scale independent
and also that the redshift space distorted power spectrum is proportional to the real space power spectrum (on the
large scales used). We assume a ﬂat prior on the proportionality constant and marginalize analytically as described
in Appendix F. We also use the HST Key Project prior on the Hubble constant as discussed earlier.
The top contours in the right hand panel of Figure 2 shows the eﬀect of the full set of constraints on the basic 7
parameter model, with the combined constraint on the curvature becoming ΩK = 0.00 ± 0.02. For the 6 parameter
ﬂat models the extra data constrains most of the parameters rather well. Table I shows that the new constraints are
very consistent with those from the CMB alone. The v2 constraint on σ8 of Table I is slightly changed and becomes
σ8e−τ(h/0.67)0.58 = 0.72 ± 0.05 almost independently of Ωm. The marginalized results on all parameters are shown
in Table II. The Hubble parameter is shifted to slightly lower values relative to the HST Key Project constraint we
used. The matter and dark energy densities are spot on the popular 0.3, 0.7 model, although with error bars of 0.05 at
68 per cent conﬁdence. The combination Ωmh is slightly tighter but has the same central value as quoted in Ref. 
using 2dF data alone and assuming ns = 1.
The σ8 result depends on the range used for the reionization redshift zre since the data used mostly constrains
the combination σ8e−τ rather than σ8 on its own. Our prior of 4 < zre < 20 should be reasonably conservative,
however we also quote the result for σ8e−τ. Looking at the maximum and minimum values contained in the 95
percent conﬁdence region of the full n-dimensional space (see Appendix C) we ﬁnd 0.62 < σ8 exp(0.04 −τ) < 0.92.
This may be compared to values of σ8 found by other independent methods and could in principle be combined with
these methods to estimate the optical depth (e.g. see Ref. ). If τ = 0.04 and Ωm = 0.3 then our result is very
consistent with the new lower cluster normalisation found by Refs. and just consistent with the cosmic
shear measurements of Refs. . The high clustering amplitude required to ﬁt the small scale clustering
observed by CBI of σ8 ∼1 (Ref. ) is in the tail of the distribution and may require an optical depth rather larger
than expected in simple models.
By using subsets of the chains we have checked that the Monte-Carlo sampling noise is negligible at the accuracy
quoted. The results are Monte-Carlo marginalized over all of the other parameters and also analytically or numerically
marginalized over the calibration type uncertainties discussed in Section 2.
3 Note that the fractional errors depend on the choice of normalization for the log eigenvectors; here we have chosen to normalize so the
exponent of Ωm is unity.
We now demonstrate the power of the Monte-Carlo method by using the above data to constrain a larger number
of parameters, using the proposal density described in Appendix A to exploit the diﬀering computational costs of
changing the various parameters. We consider separately the case of inﬂationary models, which are ﬂat, and more
general models less constrained by theoretical prejudice. In both cases we include fν, the fraction of the dark matter
that is in the form of massive neutrinos4, and allow for an eﬀective constant equation of state parameter5 w ≡p/ρ
for the dark energy, and assume that −1 ≤w < 0.
Inﬂationary models
The simplest single-ﬁeld inﬂationary models predict a ﬂat universe and can be described quite accurately by the
slow-roll approximation. The shape and amplitude of the initial curvature perturbation depends on the shape of
the inﬂationary potential, often encoded in ‘slow-roll parameters’ which are assumed to be small, plus an overall
normalization which depends on the Hubble rate when the modes left the horizon during inﬂation. The initial scalar
and tensor power spectra are parameterized as usual by6
Pχ(k) = As
Ph(k) = At
where ns and nt are the conventional deﬁnitions of the spectral indices. At the lowest order approximation the slowroll initial power spectra are determined from the inﬂationary potential V by the slow-roll parameter parameters ǫ1,
ǫ2 by 
ns = 1 −2ǫ1 −ǫ2 ≡1 −m2
nt = −2ǫ1 = −At
where quantities are evaluated when Ha = k⋆(we use k⋆= ks0 = kt0 = 0.01Mpc−1). For our analysis we use the
parameterization of Equations (1) and deﬁne ǫ1 ≡At/16As. We also impose the slow-roll constraint that spectral
index of the tensor modes is given by nt = −2ǫ1. Our results will be consistent with inﬂationary models in the region
of parameter space in which ǫ1 ≪1, ns ≈1, but elsewhere can be interpreted more generally (the results are not very
sensitive to the tensor spectral index). From the deﬁnition it is clear that ǫ1 ≥0, and except in contrived models one
also expects ns ≤1, though we do not enforce this. Simple ekpyrotic models are consistent with this parameterization
when there are no tensor modes . If there were evidence for tensor modes (ǫ1 > 0) then this would be direct
evidence against simple ekpyrotic models.
Figure 3 shows the fully marginalized posterior constraints on the various parameters using the CMB, supernovae,
HST, and nucleosynthesis constraints, with and without the 2dF data, generated from 7700 weakly correlated samples.
We generate samples without the 2dF or CBI data, and then importance sample including CBI to compute results
with and without the 2dF data. The constraints on Ωmh and fν are sharpened signiﬁcantly on adding in 2dF. The
large shift in the σ8 distribution comes from the exclusion of the high fν parameter space due to the new constraint
on the shape of the matter power spectrum (see discussion of degeneracy below).
It is important to check that the parameters are really being constrained, in the sense that the results are relatively
insensitive to the priors, so in addition to the marginalized posterior we also plot the mean likelihood of the samples.
These will diﬀer in general, particularly when the result is sensitive to the parameter space volume available, which
can change as the result of choosing diﬀerent priors (see Appendix C). In most of the 1-D plots the two methods are
4 We assume three neutrinos of degenerate mass, as indicated by the atmospheric and solar neutrino oscillation observations , and
compute the evolution using the fast but accurate method described in Ref. .
Many quintessence models can be described accurately by a constant eﬀective equation of state parameter .
We compute the
perturbations by using a quintessence potential V (φ) with V,φ = −3
2 (1 −w)H ˙φ and V,φφ = −3
2(1 −w)[ ˙H −3
2(1 + w)H2] that gives a
constant equation of state.
Here deﬁned so ⟨|χ|2⟩= R d ln k Pχ(k) and ⟨hijhij⟩= R d ln k Ph(k), where χ is the initial curvature perturbation and hij is the
transverse traceless part of the metric tensor. These deﬁnitions ensure P = const corresponds to scale invariant.
FIG. 3: Posterior constraints for 9-parameter ﬂat models using all data. The top nine plots show the constraints on the base
MCMC parameters, the remaining plots show various derived parameter constraints.
Red lines include CMB, HST, SnIA
and BBN constraints, black lines also include the 2dF data. The solid lines show the fully marginalized posterior, the dotted
lines show the relative mean likelihood of the samples. The curves are generated from the MCMC samples using a Gaussian
smoothing kernel 1/20th the width of each plot.
in good agreement indicating that the likelihood is well constrained in n-D space and the priors are not biasing our
results. However the marginalized value of σ8 is brought down by the fν > 0 phase space (since massive neutrinos
damp the small scale power), even though the best ﬁts to the data occur where the neutrinos are very light (the
correlation is shown in the bottom right hand panel of Figure 4.) Similarly the marginalized value of ns is slightly
increased by the phase space with ǫ1 > 0; this increases the CMB power on large scales, and hence requires a higher
spectral index for the scalar modes (bottom left panel of Figure 4).
We also show in Figure 4 that a small degeneracy between the Hubble constant and the matter density remains
(top left) after the ∼Ωmh constraint from the galaxy power spectrum shape is combined with the ∼Ωmh3 CMB
constraint (Table I). Geometrical information from the CMB peak position and the SnIA work together to constrain
w, but this remains slightly correlated with Ωm (top right).
The results from the 6 and 9 parameter analyses can be compared using the 68 per cent limits given in Table II
and the plots in Figure 5. Many of the results are quite robust to the addition of the extra three degrees of freedom.
The biggest change is in σ8e−τ which is brought down by contributions from non-zero fν.
As discussed in Appendix C, parameter conﬁdence limits from the full n-D distribution can also easily be calculated
from a list of samples. We show the marginalized and n-D parameter constraints with the inﬂationary assumptions
FIG. 4: All-data posterior constraints for ﬂat inﬂationary models using. The contours show the 68% and 95% conﬁdence limits
from the marginalized distribution. The shading shows the mean likelihood of the samples, and helps to demonstrate where
the marginalized probability is enhanced by a larger parameter space rather than by a better ﬁt to the data (e.g. low ns values
ﬁt the data better).
6 parameters
9 parameters
0.021 ± 0.001 0.022 ± 0.001 0.022 ± 0.001 0.018 −0.025 0.017 −0.026
ΩDMh2 0.113 ± 0.008 0.099 ± 0.014 0.106 ± 0.010 0.082 −0.130 0.072 −0.142
0.67 ± 0.03
0.67 ± 0.05
0.66 ± 0.03
0.59 −0.75
0.55 −0.78
0.98 ± 0.04
1.02 ± 0.05
1.03 ± 0.05
0.91 −1.13
0.87 −1.19
0.70 ± 0.04
0.72 ± 0.06
0.71 ± 0.04
0.58 −0.80
0.54 −0.82
0.30 ± 0.04
0.28 ± 0.05
0.29 ± 0.04
0.20 −0.42
0.18 −0.46
14.1 ± 0.4
14.3 ± 0.4
14.1 ± 0.4
13.3 −15.0
13.0 −15.2
0.20 ± 0.02
0.18 ± 0.03
0.19 ± 0.02
0.15 −0.25
0.13 −0.26
0.79 ± 0.06
0.54 ± 0.13
0.67 ± 0.08
0.49 −0.93
0.45 −0.95
0.72 ± 0.04
0.50 ± 0.12
0.61 ± 0.07
0.47 −0.81
0.41 −0.84
0.40 ± 0.05
0.27 ± 0.08
0.34 ± 0.05
0.22 −0.51
0.19 −0.53
TABLE II: Parameter constraints for 6 and 9 parameter ﬂat models with all data with or without 2dF. The top section shows
the constraints on the additional parameters that were ﬁxed in the basic 6 parameter model, the bottom half shows the eﬀect
these additional parameters have on the results for the basic parameters. 1D limits are from the conﬁdence interval of the fully
marginalized 1D distribution, the full limits give the extremal values of the parameters in the full n-dimensional conﬁdence
region (see Appendix C for discussion). Bold parameters are base Monte-Carlo parameters, non-bold parameters are derived
from the base parameters.
in Table II.7 As expected, the n-D limits are much wider than those from the marginalized distributions, most being
more than twice as wide.
The combined datasets provide good constraints on the neutrino mass, despite the large parameter space. The
massive neutrino fraction fν translates into the neutrino mass via
Ωνh2 = fνΩDMh2 =
mν ≈31 Ωνh2 eV,
where the last equality follows from our assumption that there are three neutrinos of approximately degenerate mass,
as indicated by the small mass-squared diﬀerences detected by the neutrino oscillation experiments . At 95%
conﬁdence we ﬁnd the marginalized result mν ≲0.27 eV and the more conservative n-D result mν ≲0.5 eV. The
tightness of the constraint is predominantly due to the 2dF data, as shown in Figure 3, via the damping eﬀect of
massive neutrinos on the shape of the small scale matter power spectrum.8 The result is consistent with the weaker
limits found in Refs. under more restricted assumptions. The marginalized result is only slightly aﬀected by
the large parameter space: computing chains with w = −1 and At = 0 we obtain the marginalized 95% conﬁdence
result mν ≲0.30 eV (the n-D limit is much less sensitive to the parameter space, and the result does not change
signiﬁcantly). Thus the simplest model where all the neutrino masses are very small is still a good bet.
The result for the quintessence parameter w is consistent with w = −1, corresponding to a cosmological constant.
The marginalized limit is w < −0.75 at 95% conﬁdence, consistent with Ref. . If we neglect the quintessence
perturbations it is a simple matter to relax the assumption that w ≥−1; for ﬂat models with no tensors or massive
neutrinos we ﬁnd the marginalized result −1.6 < w < −0.73 at 95% conﬁdence, and the n-D result −2.6 < w < −0.6
with the best ﬁt close to w = −1, broadly consistent with Ref. . Note that including quintessence perturbations
leads to a tighter constraint on w due to the increased large scale power. Although perturbations are required for
consistency with General Relativity, it is possible that a quintessence model may be approximated better by a constant
w model neglecting perturbations than one including the perturbations.
The constraint on the tensor mode amplitude (encoded by ǫ1) is weak, as expected due to the large cosmic variance
on large scales. In table II we also show the result for r10 ≡CT
10, the ratio of the large scale CMB power in tensor
and scalar modes. For comparison, with perfect knowledge of all the other parameters and a noise-free sky map, the
CMB temperature power spectrum cosmic variance detection limit is r10 ≳0.1.
The method we have used could be generalized for a more accurate parameterization of the initial power spectrum,
for example going to second order in the slow-roll parameters , which in general introduces a running in the
spectral index. The current data is however clearly consistent with the simplest scale invariant power spectrum with
no tensor modes. As a check we have generated chains for ﬂat models with a cosmological constant, no massive
neutrinos or tensors, but allowing for a running spectral index, and found the 68%-conﬁdence marginalized result
−0.06 < nrun < 0.02 at k = k0 = 0.05Mpc−1 where nrun ≡d2(ln Pχ)/d(ln k)2. This corresponds to the running
spectral index ns,eﬀ(k) ≡d ln Pχ/d ln k = ns(k0) + nrun ln(k/k0).
Non-ﬂat 11-parameter models
We now relax the constraint on the curvature, and allow the tensor spectral index to be a free parameter (we
assume nt ≤0.1). We parameterize the power spectra of the initial curvature and tensor metric perturbations as in
the inﬂationary case9, except that we now report results for At/As rather than a slow-roll parameter, and choose the
scalar and tensor pivot scales ks0 = 0.05Mpc−1, kt0 = 0.002Mpc−1 (the standard cmbfast parameterization).
In Figure 5 we show the parameter constraints that we get using about 10000 weakly correlated samples importance
sampled to include the 2dF data and CBI. For comparison we plot the equivalent constraints with the 9 and 11
Monte-Carlo samples from the posterior do not provide accurate estimates of the parameter best-ﬁt values (in high dimensions the
best-ﬁt region typically has a much higher likelihood than the mean, but it occupies a minuscule fraction of parameter space) therefore
we do not quote best ﬁt points. The high-signiﬁcance limits are also hard to calculate due to the scarcity of samples in these regions.
To compute accurate estimates in the tails of the distribution and to ensure the tails are well explored, we sample from a broader
distribution and then importance sample to the correct distribution, by originally sampling from P 1/T where T > 1 (we use T = 1.3).
8 Our strategy of generating chains without the 2dF data and then importance sampling ensures that we have many samples in the tail
of the distribution, and hence that our upper limit is robust (since we have have much lower Monte-Carlo noise in the tails than if we
had generated chains including the 2dF data), and also makes the eﬀect of the 2dF data easy to assess.
9 For non-ﬂat models our deﬁnitions follow .
In open models we assume the tensor power spectrum has an additional factor of
−k2/K −3/2) on the right hand side.
Posterior constraints for 11-parameter non-ﬂat models (black lines) using all data, compared with 6 (red) and 9
(green) parameter models. Dotted lines show the mean likelihood of the samples for the 11-parameter model. Some sampling
noise is apparent due to the relatively small number of samples used.
parameter models. The additional freedom in the curvature broadens some of the constraints signiﬁcantly, though
the Ωm and Ωbh2 constraints are quite robust.
The tensor spectral index is essentially unconstrained, the diﬀerence between the mean likelihood and marginalized
1D plots being due to the assumed ﬂat prior on the tensor amplitude — at very small amplitudes nt could be anything
and still be undetectable. The 95% marginalized limit on the curvature is −0.02 < ΩK < 0.07. Slightly open models
ﬁt the data marginally better on average, though a ﬂat universe is well within the 68% conﬁdence contour. The limit
on the equation of state parameter is slightly weakened to w < −0.69, and neutrino mass is now mν < 0.4 eV at 95%
conﬁdence.
Which model ﬁts best?
We have explored the posterior distribution in various parameter spaces, deriving parameter constraints in the
diﬀerent models. Since we obtain only upper limits on fν, w and At/As there is no evidence for massive neutrinos,
w ̸= −1 or tensor modes using current data.
One can make a more quantitative comparison of the diﬀerent models by comparing how well each ﬁts the data.
As discussed in Appendix C a natural measure is the mean likelihood of the data obtained for the diﬀerent models.
Equivalently, if one chose a random sample from the possible parameter values, on average how well would it ﬁt the
data? We ﬁnd that the six, nine and eleven parameter models have mean likelihoods ratios 1 : 0.4 : 0.3 using all
the data. So by moving away from the basic model we have not increased the goodness of ﬁt on average (rather
FIG. 6: Posterior constraints for 11-parameter non-ﬂat models using all data.
the reverse), which is not surprising given how well the basic model ﬁts the data. Most of the distributions of the
additional parameters peak at their ﬁxed values.
We also considered the probability of each model, found from marginalizing out all parameters (the ‘Evidence’ as
e.g. explained in Ref. ). Since the 9 and 11 parameter models are totally consistent with the 6 parameter model
then it is already clear that using this method will favor the 6 parameter model for any choice of prior. The numerical
evidence ratio depends very strongly on the prior, and without a well motivated alternative to the null hypothesis
(that there are only 6 varying parameters), its value is not useful. The mean likelihood of the samples (above) uses
the posterior as the prior, which is at least not subjective, and has a convenient interpretation in terms of goodness
Whilst certainly not ruled out, at the moment there is no evidence for observable eﬀects from the more complicated
models we have considered. Nonetheless, when considering parameter values, it is important to assess how dependent
these are on the assumptions, and this can be seen by comparing the results we have presented. We conclude that at
the moment simple inﬂationary models with small tilt and tensor amplitude (e.g. small ﬁeld models with a nearly ﬂat
potential; or observationally equivalently, ekpyrotic models) account for the data well. On average the ﬁt to the data
is not improved by adding a cosmologically interesting neutrino mass or by allowing the dark energy to be something
other than a cosmological constant.
CONCLUSIONS
In this paper we have demonstrated the following beneﬁts of sampling methods for cosmological parameter estimation:
• The practicality of exploring the full shape of high-dimensional posterior parameter distributions using MCMC.
• The use of principle component analysis to identify well constrained non-linear combinations of parameters and
identify degeneracies.
• Simple calculation of constraints on any parameters that can be derived from the base set (e.g. age of the
universe, σ8, r10, etc.).
• Use of the mean likelihood of the samples as an alternative to marginalization to check robustness of results
and relative goodness of ﬁt.
• The calculation of extremal values within the n-D hyper-surface to better represent the range of the full probability distribution.
• The use of importance sampling to quickly compare results with diﬀerent subsets of the data, inclusion of new
data, and correction for small theoretical eﬀects.
Our Monte-Carlo code and chains are publicly available at 
With the current cosmological data we found that the Monte-Carlo approach works well, though simply picking
the best ﬁt sample does not identify best-ﬁt model to high accuracy (and therefore we do not quote these numbers),
and there are potential diﬃculties investigating posterior distributions with multiple local minima (although this is
not a problem given the parameters and data used here).
We investigated a 6-D cosmological parameter space and found, for the ﬁrst time, a concise description of the CMB
constraints on the matter power spectrum normalization, in addition to tight constraints on Ωb and ns in agreement
with previous analyses. The new information from the CBI and VSA interferometers is in good agreement with the
older data points and we ﬁnd that our results are negligibly changed on removing this information.
On adding in constraints from a wide range of cosmological data we evaluated constraints on the above 6 parameter
model as well as extending to more complicated 9 and 11 parameter models. Many of the constraints on the base
set of parameters were fairly robust to the addition of this extra freedom, for example the matter density changed
from Ωm = 0.30 ± 0.04 for the basic 6 parameter model to Ωm = 0.28 ± 0.07 for the 11 parameter model (68 per
cent conﬁdence). On the other hand the value for the matter power spectrum normalization on 8h−1Mpc scales is
quite dependent on the neutrino mass, and allowing for a signiﬁcant neutrino mass decreases the mean value of σ8
(the constraint on the amplitude could be improved by better constraints on the small scale CMB amplitude and the
reionization redshift.) Parameters aﬀecting or sensitive to the late time evolution tend to be rather degenerate, and
constraints on these are considerably weakened on adding additional freedom in the model.
We ﬁnd that the 9 parameter model is quite well constrained by the amount of data used and obtain upper limits
on a number of interesting cosmological parameters, given our assumptions of a ﬂat universe with slow-roll inﬂation
constraints. In particular we ﬁnd the marginalized constraint mν ≲0.3 eV on the neutrino mass and w ≲−0.75 for
the equation of state parameter (95 per cent conﬁdence). There is no evidence for tensor modes, though the constraint
is currently quite weak, with the constraint on the ratio of the large scale CMB power being r10 ≲0.7. This constraint
could be sharpened considerably by restricting the allowed range of scalar spectral indices and neutrino masses.
In the 11 parameter space the limits are weakened slightly and the standard cosmology of w = −1 and ΩK = 0 is
near the peak of the posterior probability. The tensor spectral index is essentially unconstrained as expected given
that the only information comes from the large scale (COBE) CMB data.
While a detailed investigation of the eﬀect of using all the diﬀerent combinations of cosmological constraints is
beyond the scope of this paper we do show the eﬀect of removing the second most powerful constraint (the galaxy
power spectrum) on the 9 parameter model in Figure 3. The limits on most of the parameters are aﬀected remarkably
little. The neutrino mass is the most aﬀected, with the upper limit doubling on removing 2dF. The neutrino mass is
correlated with the matter power spectrum shape parameter (roughly Ωmh) and amplitude, and these constraints are
correspondingly weakened on removing 2dF.
As new better data become available our general method should also be applicable into the future. Due to the enormously decreased number of likelihood evaluations in the MCMC method compared to other approaches, theoretical
predictions can be computed essentially exactly, and one can account for the available data in detail.
Acknowledgments
We thank the members of the Leverhulme collaboration for many useful discussions, in particular Ofer Lahav,
Carolina ¨Odman, Oystein Elgaroy, Jerry Ostriker and Jochen Weller. We are grateful to David MacKay and Steve Gull
for encouraging the use of MCMC. We thank Anze Slozar, Keith Grainge, Alexandre R´efr´egier and Kev Abazajian
for helpful suggestions. We thank George Efstathiou for making his CMBﬁt code available to us. AL thanks the
Leverhulme Trust for support. SLB acknowledges support from Selwyn College and PPARC and thanks the Aspen
Center for Physics where part of this work was done. We thank PPARC and HEFCE for support of the COSMOS
APPENDIX A: THE METROPOLIS-HASTINGS ALGORITHM
The algorithm that we use for generating samples from the posterior distribution using a Markov Chain is the
Metropolis-Hastings algorithm. For an introduction and overview of MCMC methods see Ref. . A Markov
Chain moves from a position in parameter space θ1 to the next position θ2 with transition probability T (θ1, θ2),
where θ labels a vector of parameter values. The Metropolis-Hastings transition kernel T (θ1, θ2) is chosen so that
the Markov Chain has a stationary asymptotic distribution equal to P(θ), where P(θ) is the distribution we wish
to sample from. This is done by using an arbitrary proposal density distribution q(θn, θn+1) to propose a new point
θn+1 given the chain is currently at θn. The proposed new point is then accepted with probability
α(θn, θn+1) = min
1, P(θn+1)q(θn+1, θn)
P(θn)q(θn, θn+1)
so that T (θn, θn+1) = α(θn, θn+1)q(θn, θn+1). This construction ensures that detailed balance holds,
P(θn+1)T (θn+1, θn) = P(θn)T (θn, θn+1),
and hence that P(θ) is the equilibrium distribution of the chain.
If the chain is started in a random position in parameter space it will take a little time, burn in, to equilibrate
before it starts sampling from the posterior distribution. After that time each chain position is a correlated sample
from the posterior. The correlation is particularly obvious if the proposal is not accepted as then there are two or
more samples at exactly the same point. However by using only occasional chain positions (thinning the chain) one
can give the chain time to move to an uncorrelated position in parameter space, and independent samples are then
obtained. Small residual correlations between samples are unimportant for almost all calculations, though they do
make the Monte-Carlo error on the results harder to assess.
For the cases we consider the chains equilibrate rapidly, at worst after thousand or so points. The results can be
checked easily by using a longer burn in and comparing results. We thin the chain positions by a factor of 25-50
depending on the number of parameters, leaving weakly correlated samples that we use for importance sampling (see
Appendix B).
If the proposal density is symmetrical it cancels out when working out the acceptance probability, which then
becomes just the ratio of the posteriors. This is the case when the proposal density is independent of the current
position of the chain, which is the case we consider.
The proposal density
The choice of proposal density can have a large eﬀect on how the algorithm performs in practice. In general it
is best to have a proposal density that is of similar shape to the posterior, since this ensures that large changes are
proposed to parameters along the degeneracy directions. Fortunately with cosmological data we have a reasonable
idea of what the posterior might look like, and so choosing a sensible proposal density is not diﬃcult.
If posteriors from models with common parameters are much easier to compute it can be beneﬁcial to use a proposal
density that changes only a subset of the parameters on each iteration, ensuring that consecutive posterior evaluations
only diﬀer in a subset of the parameters. Proposing a change to a random subset of the parameters also increases
the acceptance rate, especially in high dimensions, giving faster piecewise movement around parameter space. In the
case of CMB parameter estimation, models that diﬀer only by a diﬀerent normalization of the theoretical CMB power
spectrum are very quick to compute once the Cl values for a single model have been calculated. Similarly changing
parameters that govern calibration uncertainties in the data can also be very quick. However changing parameters
that govern the perturbation evolution, for example Ωb, Ωc, etc, will be much slower as in general it requires a detailed
recalculation of the linear physics.
If we are comparing CMB data with theoretical models, the most general way to compute the theoretical Cl power
spectrum is using a fast Boltzmann code such as camb (a parallelized version of cmbfast ; we discuss less
accurate and general schemes below).
Since the perturbation evolution is assumed to be linear, any parameters
governing the initial power spectra of the scalar and tensor perturbations will be fast to compute once the transfer
function for each wavenumber has been computed. Parameters governing the initial power spectrum are therefore
‘fast’ parameters.
We therefore use a proposal density that makes changes only within the subsets of the fast and slow parameters,
at least when we do not have an approximate covariance matrix available for the posterior.10 We made the fairly
arbitrary choice to change a subset of one to three parameters at a time, cycling through the parameters to be changed
10 When changing the slow parameters it is possible to also change the fast parameters at the same time. This can be a good idea when
there are highly correlated slow and fast parameters, for example the reionization redshift and the tensor amplitude.
in random order, which gives a high acceptance rate (∼50%) for the cases we considered. After one initial run one
can transform to a set of parameters which diagonalize the covariance matrix before doing subsequent runs, allowing
eﬃcient exploitation of degeneracy information as long as the posterior is reasonably Gaussian.
The above scheme is suﬃcient for parameter estimation from current data, however as more data becomes available
the posterior may become highly non-Gaussian or disjoint, in which case it may become necessary to use more
sophisticated schemes using simulated annealing, hybrid Monte Carlo, or schemes using cross-chain information . However when the posterior is not disjoint one can often transform to a set of base parameters which are
relatively independent, in which case a simple Monte-Carlo scheme should continue to work well (see Appendix C for
further discussion).
APPENDIX B: IMPORTANCE SAMPLING
Given a set of samples from a distribution P, one can estimate quantities with respect to a diﬀerent similar
distribution P ′, by weighting the samples in proportion to the probability ratios. This eﬀectively gives a collection
of non-integer weighted samples for computing Monte-Carlo estimates. For example the expected value of a function
f(θ) under P ′ is given by
⟨f(θ)⟩P ′ =
dθP ′(θ)f(θ) =
P(θ) P(θ)f(θ)
Given a set {θn} of N samples from P a Monte-Carlo estimate is therefore
⟨f(θ)⟩P ′ ≈1
P(θn) f(θn).
For this to work it is essential that P/P ′ is never very small, and for a good estimate without massively oversampling
from P one needs P ′/P ∼constant everywhere where P ′ is signiﬁcantly non-zero. If P ′ is non-zero over only a very
small region compared to P it will be necessary to proportionately oversample from P.
If the distributions are not normalized, so that
dθP(θ) = Z, the ratio of the normalizing constants can be
estimated using
⟨f(θ)⟩P ′ ≈
n=1 P ′(θn)/P(θn)f(θn)
n=1 P ′(θn)/P(θn)
In Bayesian analysis it can be useful to compute the ratio of the evidences P(D) =
dθP(D, θ), given as above by
P ′(D|θn)P ′(θn)
P(D|θn)P(θn) ,
where the samples {θn} are drawn from P(θ|D). Assuming the distributions are suﬃciently similar, the evidence
under P ′ can therefore easily be computed from the probability ratios at a sample of points under P, and a known
evidence under P. In many cases only the ratio is of interest — the ratio is larger than one if on average the probability
of the samples under P ′ is higher than under P. In the case where the distributions are very diﬀerent one may need
to introduce a series of intermediate distributions that are all not too dissimilar to each other, and perform Monte
Carlo simulations for each. The evidence ratio one requires is then just the product of that for all the intermediate
distributions. Many more general schemes are described in , though in this paper we only consider importance
sampling to similar or subset distributions.
The simplest application of importance sampling is to adjust results for diﬀerent priors. For example if one computes
a chain with ﬂat priors on the parameters, one may wish to importance sample to several diﬀerent distributions with
diﬀerent priors on various parameters. This will work well as long as the prior does not skew the distribution too
much or give non-zero weight to only a very small fraction of the models.
Faster Monte-Carlo
MCMC runs produce correlated samples from the probability distribution. To obtain independent samples one
thins out the chain by a suﬃciently large factor that the chain has had time to move to a randomly diﬀerent point
between the thinned samples. Depending on how one implements the MCMC, the shape of the posterior and the
number of dimensions the thinning factor can be quite large, typically of the order ten to a thousand.
By performing Monte-Carlo simulations with a good approximation to the true probability distribution one can
use importance sampling to correct the results with an accurate calculation of the probabilities. This can be useful
if computing the probabilities accurately is much slower than computing an approximation, since one only ever
importance samples independent samples. The burn-in and random walking stages of the Monte-Carlo involve a much
larger number of probability evaluations, so using a fast approximation when generating the chain saves a lot of time.
Calculating the posterior from CMB data requires a calculation of the theoretical CMB power spectra, Cl. Using
accurate codes like camb and cmbfast is typically much slower than computing the likelihoods from the data once
the Cl are known (assuming one uses a radical data-compression scheme, e.g. see Ref. ). In the not so distant
future we will require to high accuracy Cl up to l ∼2500, including second order eﬀects such as lensing, and also the
matter power spectrum at various redshifts. Without access to a fast supercomputer this may be prohibitive.
With a small number of parameters it is possible to use a grid of models and interpolate to generate accurate Cl
quickly, however as the number of parameters grows the computational cost of computing the grid grows exponentially.
Also, as second order eﬀects such as gravitational lensing become important, fast grid generation schemes such as the
k-splitting scheme of Ref. become much more diﬃcult to implement accurately. However these may still be useful
as a fast approximation, as long as the independent samples are corrected with a more accurate calculation. Ref. 
describe a scheme for generating Cls very quickly from a small number of base models, a set of optimized parameters,
and an accurate calculation of how the Cl vary with these parameters. This gives a very fast approximator over a
restricted range of parameters that may prove useful combined with importance sampling correction.
It is also possible to use fast semi-analytic schemes. Typically these are based on a smallish grid of base models,
from which the Cls in general models are computed quickly on the ﬂy by accounting for changes in the angular
diameter distance to last scattering, diﬀering perturbation growth rates, etc. These approximate schemes can be
made quite accurate at at small scales, with signiﬁcant errors mainly at low l, precisely where the cosmic variance
is large. So whilst an approximate scheme may produces small systematic errors in the likelihood, if the error is of
the same order as the cosmic variance or less, the probabilities given the data are bound to be suﬃciently similar for
importance sampling to be valid.
A particular approximate Cl generator we have tried is CMBﬁt , which uses of combination of base Cl grids and
analytic ﬁts. This achieves quite good few percent level accuracy at high l, though larger systematic errors at low
l. However the code is fast, and we found that importance sampling the results with an exact calculation of the Cl
gives good results, and removes systematic biases introduced by the low l approximations. Such an approach can
be generalized for more general late time evolution, for example models with quintessence where the eﬀect on small
scales is due almost entirely to changes in the background equation of state.
An alternative scheme based on grids of the transfer functions for each wavenumber can produce more accurate
results, like the recently released DASH . However this is not much faster than generating the Cls exactly using
camb on a fast multi-processor machine, and relies on a large pre-computed grid (which introduces its own limitations).
The only real advantage over CMBﬁt is that more general initial power spectrum parameterization could be accounted
for easily — something that is impossible with schemes based on grids of Cls.
Even without a fast semi-analytic scheme, there are a variety of small corrections that can be applied post hoc. For
example lensing aﬀects the CMB Cl at the few percent level, so one may wish to compute chains without including the
lensing, then importance sample to correct the results using an accurate calculation including the lensing11. For small
scales at high precision one may also wish to run camb at a high-accuracy setting to check that numerical errors in
the default output are not aﬀecting the results. Also chains could be generated to lower l and the eﬀect of the high-l
constraints accounted for by importance sampling. For example we generated the chains using lmax = 1300, and then
for the nearly independent samples re-computed the power spectra up to lmax = 2000 for importance sampling with
the CBI data.
Similar methods could be applied for the matter power spectrum using approximate ﬁttings, see e.g. Refs. .
However when a fast multi-processor machine is available, and one is interested in a very large number of parameters,
11 However if one is also computing the matter power spectrum numerically the additional cost of including the lensing eﬀect is small.
We have checked that the lensing correction to the results we present is much smaller than the errors (the lensed power spectra can be
computed with camb using the harmonic approach of Ref. ).
it is much simpler to Monte-Carlo using camb to generate the CMB power spectra and matter power spectrum, which
is what we did for the results we present. The great advantage of this approach is that it generalizes trivially if one
wishes to include changes in the physics, for example diﬀerent quintessence models, or changes in the initial power
Constraints with new data
Assuming that one has some new data which is broadly consistent with the current data, in the sense that the
posterior only shrinks, one can use importance sampling to quickly compute a new posterior including the new data.
We have made our MCMC chains publicly available, so these can be used to rapidly compute new posteriors from new
data without incurring any of the considerable computational cost of generating the original chain. For example if you
have a new constraint on σ8, you just need to loop over the samples adjusting the weights of the samples proportional
to the likelihood under the new constraint. Using importance sampling has the added beneﬁt of making it very easy
to assess how the new data is changing the posterior.
APPENDIX C: PARAMETER CONSTRAINTS
The great advantage of the Monte-Carlo approach is that you have a set of samples from the full parameter space.
To answer any particular question one can examine the points and compute results reliably, taking full account of the
shape of the posterior in N dimensions. However for human consumption it is usual to summarize the results as a set
of parameter values and error bars.
One way to do this is to use the samples for a principle component analysis to identify the degeneracy directions, as
we demonstrated in Section 2. By quoting constraints on a set of orthogonalized parameters one retains most of the
information in the original distribution, as long as it is suﬃciently Gaussian (or Gaussian in the log, or some other
function). However ultimately one is usually interested in the values of some fundamental parameters, and it is also
useful to ﬁnd constraints on these alone.
The simplest approach is to compute the marginalized 1-dimensional distributions for each parameter, essentially
counting the number of samples within binned ranges of parameter values. Note that this is extremely hard to do
using a brute-force numerical grid integration calculation as it scales exponentially with the number of dimensions,
but is quite trivial from a set of Monte-Carlo samples. One can then quote the value at the maximum or mean of
the 1D distribution, along with extremal values of the parameter which contain a fraction f of the samples, where f
deﬁnes the conﬁdence limit. The extremal values could be chosen so that there were the same number of outliers at
both ends of the distribution, or such that the value of the marginalized probability is the same at each limit. This is a
good way of summarizing the current state of knowledge as long as you have included everything you know, including
using a believable prior over parameter space.
However, frequently one wants to use the parameter estimates to assess consistency with new data or theories, and
the prior can be very hard to deﬁne. For example, on putting in a top hat prior on the age and h, the marginalized
prior probabilities are not ﬂat, even if all of the other priors are ﬂat broad top hats. This is because the marginalized
distribution includes the eﬀect of the amount of parameter space available at each point, which can depend quite
strongly on the value of the parameter. Likewise it is possible to have a region in parameter space which ﬁts the data
rather well, but because the region is small the marginalized probability of those parameter values can be very low.
When assessing consistency with new data (or theories), one really wants to know whether the posterior for the
new data intersects the N-dimensional posterior for the current data in a region where both are likely. For example
one could deﬁne the region of parameter space enclosing a fraction f of the points with the highest likelihood as the
N-dimensional conﬁdence region, and then see whether this region intersects with the corresponding region for the
new data. It is clearly sub-optimal to try to perform this comparison using only 1D parameter values and limits,
however if one quotes the extremal values of each parameter contained in the N-dimensional conﬁdence region it is
at least possible to assess whether the N-dimensional regions might overlap. At least if the new data is outside these
limits it is a clear indication that there is an inconsistency, whereas using the marginalized limits it shows no such
thing (just that if there is a consistent region it makes up a small fraction of the original parameter space — something
one would hope for if the new data is informative!) However it is of course easily possible for the 1D likelihood limits
to be consistent but the full N-dimensional regions to be highly inconsistent.
In order to be as informative as possible it can be useful to quote both the marginalized and likelihood limits,
though of course one should study the full set of samples to make use of as much information as possible. When there
are strong degeneracies one can quote the constraints on the well-determined orthogonalized parameters.
Mean likelihoods
Often it is useful to show the projected shape of the distribution in one or two dimensions. The marginalized
distribution, proportional to the number of samples at each point in the projected space, gives the probability density
in the reduced dimensions, ignoring the values of the parameters in the marginalized dimensions, and is therefore
usually the quantity of interest. However this looses all the information about the shape of the distribution in the
marginalized directions, in particular about the goodness of ﬁt and skewness with respect to marginalized parameters.
Useful complementary information is given by plotting the likelihood of the best ﬁt model at each point, for example
see Ref. . However it is not so easy to compute this using a small set of Monte-Carlo samples — mean values
within each bin can be obtained quite accurately from a small number of samples, but getting a good value for the
maximum in each bin requires a much larger number. Instead we plot the mean likelihood of the samples at each
value of the parameter, which is easy to compute from the samples. It shows how good a ﬁt you could expect if you
drew a random sample from the marginalized distribution at each point in the subspace.
From a distribution P(θ) one can derive the (marginalized) distribution of a derived parameter vector of interest
v = h(θ) by
P(v) = M(P, v) ≡
dθ P(θ)δ(h(θ) −v).
Assuming ﬂat priors on θ the expected mean likelihood of samples with h(θ) = v is
⟨P(θ : h(θ) = v)⟩≡
dθ P(θ)2δ(h(θ) −v)
dθ P(θ)δ(h(θ) −v) = M(P 2, v)
Frequently h(θ) is a projection operator into a subspace of θ (for example h(θ) = θ1 for marginalization down to the
ﬁrst parameter). If this is the case and P(θ) is a multivariate Gaussian distribution, the marginalized distribution
M(P, v) is also a Gaussian (readily proved using Fourier transforms; the covariance is given by the projected covariance
matrix). Since the square of a Gaussian is a Gaussian it follows that M(P 2, v) ∝M(P, v)2, and hence the mean
likelihood is proportional to the marginalized distribution M(P, v). This also follows trivially if P is separable with
respect to the subspace. In the case of Gaussian or separable distributions the mean likelihood curve is therefore
proportional to the marginalized distribution and the two curves look the same. Diﬀerences in the curves therefore
indicate non-Gaussianity, for example when one of the marginalized parameters is skewing the distribution in a
particular direction (for example the eﬀect of massive neutrinos fν > 0 on the σ8 curve in Figure 3; if fν was ﬁxed at
it’s maximum likelihood value the marginalized result for σ8 would change signiﬁcantly in the direction of the mean
likelihood curve). The converse does not hold of course, it is possibly to have a non-Gaussian distribution where both
curves are the same. If the priors on the parameters are not ﬂat this will also show up as diﬀerences in the curves
even if the likelihood distribution is Gaussian.
Eﬀect of the prior
In our analysis we chose a particular set of base parameters which were assigned ﬂat priors. This choice was fairly
arbitrary, and there are other possible choices. For example one might instead use ΩΛ as a base parameter and derive
h from the constraint
Ωbh2 + Ωch2
In this case the prior on h is given by
P(h, Ωbh2, Ωch2, ΩK) = P(ΩΛ, Ωbh2, Ωch2, ΩK)∂ΩΛ
h P(ΩΛ, Ωbh2, Ωch2, ΩK),
and so the prior on h is proportional to Ωm/h if the prior on ΩΛ is ﬂat. Using h as a derived parameter therefore tends
to give results which favor lower h values and higher Ωm values. Using importance sampling it is straightforward to
adjust results from one set of base parameters to another by weighting the samples by the corresponding ratio of the
12 However the tails of the distributions can change signiﬁcantly, so it may be necessary to generate the original chain at a higher
temperature to importance sample accurately. In this example we generated the chain at a temperature of 1.3, so the samples were
Parameter constrains from the CMB alone with ﬂat prior on h (ΩΛ derived, black lines) and ﬂat prior on ΩΛ (h
derived, red lines). Dotted lines show the mean likelihood of the samples, solid lines the estimated marginalized distribution.
In most cases both sets of dotted lines are almost on top of one another.
For the results of the parameter estimation to be meaningful it is essential that the priors on the base set are well
justiﬁed, or that the results are independent of the choice. In Figure C we show the eﬀect on the posterior constraints
from the CMB data from the 6 parameter analysis using diﬀerent base sets. The distributions shift by a fraction of
their width, though this can have quite a large eﬀect on the high-signiﬁcance limits of weakly constrained parameters
(for example the 95% conﬁdence limit is h < 0.89 with h a base parameter, h < 0.59 with ΩΛ a base parameter).
For well constrained parameters the prior eﬀectively becomes ﬂatter over the region of interest and the eﬀect is
much less signiﬁcant. As shown on the right of Figure C the posteriors of four parameters that are well constrained
by the CMB are almost independent of the choice of prior.
As shown in Figure C plotting the mean likelihood of the samples gives a clear indication of the direction in which
results may be biased relative to a diﬀerent choice of prior. It is also clear that by choosing h as a base parameter we
are getting more samples in the region of interest for comparison with other data. In particular using ΩΛ as a base
parameter gives a sharp cut-oﬀat the higher values of h, which are allowed by the HST prior. One slight disadvantage
of using h rather than ΩΛ is that the correlation of h with some of the other base parameters is signiﬁcant, which
may make the Monte-Carlo sampling less eﬃcient. However since we use the covariance matrix to rotate to a set of
orthogonalized parameters after one short initial run this is not a major problem.
The eﬃciency of the MCMC implementation can be improved by using a set of parameters for which the posterior
is as symmetric as possible . It may therefore be a good idea to transform to a better set of base parameters,
for example one could transform to a set of orthogonalized parameters derived from a principle component analysis
using some less constraining data. However when performing a non-linear transformation of parameters it is also
necessary to transform the ﬂat priors on the parameters to obtain equivalent results. If one assumes ﬂat priors in the
transformed parameters it is wise to check that this does not induce a strong prior bias on the cosmological parameters
of interest.
APPENDIX D: GOODNESS OF FIT
To consider whether an enlarged parameter space is justiﬁed, one ideally wants to compare the evidences P(D) with
the diﬀerent parameter sets. In some cases, for example when using hyperparameter weights on experiments, it may
be possible to deﬁne a prior on the extra parameters in which case one can compute the evidence ratio directly. The
ratio does however depend quite strongly on the prior put on the parameters, which in general it is not straightforward
drawn from P 10/13 and then importance sampled.
to quantify. If one puts a broad prior on a parameter, but the likelihood is much narrower, the probability of the
data is down-weighted because the likelihood takes up a much smaller region of parameter space. One simple, but
non-Bayesian, way to get round this is to set the prior equal to the normalized posterior for computing the evidence,
in which case one compare the values of
dθP(D|θ)P(θ|D)
This is just the expected probability of the data in the posterior distribution, which can be estimated trivially from a
set of Monte-Carlo samples as the mean likelihood of the samples. For Gaussian distributions this is the exponential
mean of the chi-squareds under the posterior distribution, and is thus a smeared-out version of the common practice
of quoting the chi-squared of the best ﬁt. The smearing out helps to down-weight extra parameters which have to
be ﬁne tuned to obtain better ﬁts. If the mean likelihood is bigger with the extra parameters it suggests they are
improving the ﬁt to the data on average. Although we know no way to use the value rigorously for hypothesis testing,
it seems nonetheless to be useful as a rule of thumb measure of goodness of ﬁt.
APPENDIX E: CONSISTENCY OF DATA SETS
It is important to assess whether the datasets being used are consistent, or whether one or more is likely to be
erroneous. This can be done by introducing hyperparameter weights on the diﬀerent datasets when performing
the analysis. If a dataset is inconsistent, its posterior hyperparameter will have a low value, and the dataset then only
contributes weakly to the posterior probability of the parameters. In the case that the likelihoods are of Gaussian form
it is a simple matter to marginalize over the hyperparameters analytically given a simple prior. To assess whether the
introduction of hyperparameters is justiﬁed (i.e. whether the data are inconsistent with respect to the model), one
can compare the probability of obtaining the data in the two hypotheses: H0, no hyperparameters are needed; H1,
hyperparameters are needed because one or more datasets are inconsistent. Using a maximum entropy prior assuming
that on average the hyperparameter weights are unity, Ref. gives
P(D|θ, H1)
P(D|θ, H0) =
2nk/2+1Γ(nk/2 + 1)
k + 2)e−χ2
where k labels the datasets, each containing nk points. Given a set of independent samples from H0 it is straightforward
to compute an estimate of the evidence ratio using Eq. (B5). If the datasets are inconsistent the importance sampling
estimate would be very inaccurate as the probability distributions would be signiﬁcantly diﬀerent.
However this
should be clear when one computes the estimate since the probability ratios will vary wildly. If one suspects that one
of the datasets is inconsistent it would be better to start with sampling from H1, and conﬁrm that the evidence ratio
supports using the hyperparameters.
An even simpler way of assessing consistency of the datasets might be to compare the mean likelihood of the samples
in the region of overlap of the posterior distributions to the overall mean likelihood under the original posterior. If
the mean likelihood of the samples in the region of overlap is much less than the original mean, it is an indication
than the regions of high likelihood under each dataset do not overlap well in N-dimensions, and hence there may be
an inconsistency. In practice the samples in the region of overlap can be found by importance sampling additional
datasets. The mean likelihoods should always be computed with respect to the same, original, dataset (or group of
datasets). However importance sampling may fail to identify inconsistencies in particular cases when the distributions
have multiple maxima.
APPENDIX F: ANALYTIC MARGINALIZATION
Frequently one has data in which there is an unknown calibration uncertainty, or an unknown normalization. These
parameters can be marginalized over analytically following as long as the likelihoods are Gaussian, and the prior
on the amplitude parameter is Gaussian or ﬂat. Typically one has a marginalization of the form
dαP(α) exp[−(αv −d)T N −1(αv −d)/2]
where v and d are vectors, N is the noise covariance matrix, and P(α) is the prior. For example for the supernovae
data v is assumed to be a vector of equal constants giving the intrinsic magnitudes of the supernovae, and d is a
vector of the theoretical minus the observed eﬀective magnitudes. If the prior P(α) = const it clearly cannot be
normalized, however the marginalization is trivial giving
−2 ln L = dT
N −1 −N −1vvT N −1
d + ln(vT N −1v) + const.
In the case that v is a constant (independent of the data and parameters), one has L ∝e−χ2
eff/2 where
N −1 −N −1vvT N −1
This is exactly the same as the best-ﬁt one obtains by minimizing the likelihood w.r.t. α, and so in this case the
maximization technique of Ref. is exactly equivalent to full marginalization. For example, in the case of the
supernovae data, marginalization with a ﬂat prior over the magnitudes is equivalent to using the best-ﬁt magnitude.
In general this is not true as the logarithmic dependence ln(vT N −1v) can depend on the parameters. For example
with the 2dF data v would be the predicted matter power spectrum values, and α would be the unknown amplitude
relative to the galaxy power spectrum at z = 0.17.
The marginalized result is only ‘correct’ if the assumed ﬂat prior is correct; it is an advantage of the maximization
technique that the result does not depend on the prior.