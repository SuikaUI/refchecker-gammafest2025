Accepted to CVPR’05, San Diego, CA, USA, June 20-25
WaldBoost – Learning for Time Constrained Sequential Detection
Jan ˇSochman
Jiˇr´ı Matas
Center for Machine Perception, Dept. of Cybernetics, Faculty of Elec. Eng.
Czech Technical University in Prague, Karlovo n´am. 13, 121 35 Prague, Czech Rep.
{sochmj1,matas}@cmp.felk.cvut.cz
In many computer vision classiﬁcation problems, both
the error and time characterizes the quality of a decision.
We show that such problems can be formalized in the framework of sequential decision-making. If the false positive and
false negative error rates are given, the optimal strategy
in terms of the shortest average time to decision (number
of measurements used) is the Wald’s sequential probability
ratio test (SPRT). We built on the optimal SPRT test and
enlarge its capabilities to problems with dependent measurements. We show how to overcome the requirements of
SPRT – (i) a priori ordered measurements and (ii) known
joint probability density functions. We propose an algorithm with near optimal time and error rate trade-off, called
WaldBoost, which integrates the AdaBoost algorithm for
measurement selection and ordering and the joint probability density estimation with the optimal SPRT decision strategy. The WaldBoost algorithm is tested on the face detection problem. The results are superior to the state-of-the-art
methods in the average evaluation time and comparable in
detection rates.
1. Introduction
In many computer vision problems such as detection,
both error rates and computational complexity reﬂected by
time to decision, characterize the quality of a given algorithm. We show that such problems can be formalized in
the framework of sequential decision-making. The optimal
strategy in terms of the shortest average decision time subject to a constraint on error rates (false positive and false
negative rates) is the Wald’s sequential probability ratio test
(SPRT). In the paper, we build on Wald’s theory and propose an algorithm for two-class classiﬁcation problems with
near optimal trade-off between time and error rate.
Wald’s sequential decisions are based on measurements
that are assumed to be selected and ordered a priori. Moreover, it is assumed that either the measurements are classconditionally independent or their joint probability density
functions are known. We show how this limitation can be
overcome by selecting the relevant measurements by Ada-
Boost. The joint conditional density of all measurements,
whose estimation is computationally intractable, is approximated by the class-conditional response of the sequence
of strong classiﬁers. The choice is justiﬁed by asymptotic
properties of AdaBoost trained strong classiﬁer.
The proposed algorithm, called WaldBoost, integrates
AdaBoost-based measurement selection and Wald’s optimal
sequential probability ratio test. The WaldBoost approach
was applied and evaluated on the face detection problem.
On the CMU dataset , the results are superior to the
state-of-the-art in average evaluation time and comparable
in detection rates. In the face detection context, the Wald-
Boost algorithm can be also viewed as a theoretically justiﬁable ”boosted cascade of classiﬁers” proposed by Viola
and Jones .
To our knowledge, the trade-off between the quality of
solution (error rate) and time-to-decision inherent in detection problems has not been explicitly formulated as a constrained optimization in computer vision literature. “Focus of attention” (e.g. ), cascaded classiﬁer , Float-
Boost , boosting chain or nesting-structured cascade implicitly minimize the time to decision while
keeping the error rates at a low level. However, the necessary compromise is achieved by ad hoc parameter setting
and no attempt is made to achieve optimality.
The paper is structured as follows. The two-class sequential decision-making problem is formulated and its optimal solution, the sequential probability ratio test, is described in Section 2. The selection and ordering of the measurements and the joint probability density function estimation using AdaBoost is explained in Section 3. In Section 4,
the WaldBoost algorithm is proposed and its application to
the face detection problem is discussed. The experimental
validation of the algorithm is given in Section 5 and the paper is concluded in Section 6.
2. The Two-class Sequential Decision-making
Let x be an object belonging to one of two classes
{−1, +1}, and let x1, x2, . . . , be a given ordered sequence
of measurements on x. A sequential decision strategy is
a sequence of decision functions S = S1, S2, . . ., where
St : (x1, . . . , xt) →{−1, +1, ♯}. The strategy S takes one
measurement at a time and in step t makes a decision St
based on (x1, . . . , xt). The ’♯’ sign stands for a “continue”
(do not decide yet) decision1. If a decision is ’♯’, xt+1 is
measured and St+1 is evaluated. Otherwise, the output of S
is the class returned by St.
In two-class classiﬁcation problems, errors of two kinds
can be made by strategy S. Let us denote by αS the probability of error of the ﬁrst kind (x belongs to +1 but is classi-
ﬁed as −1) and by βS the probability of error of the second
kind (x belongs to −1 but is classiﬁed as +1).
A sequential strategy S is characterized by its error rates
αS and βS and its average evaluation time
¯TS = E(TS(x)),
where the expectation E is over p(x) and TS(x) is the expected evaluation time (or time-to-decision) for strategy
TS(x) = arg min
t (St ̸= ♯).
An optimal strategy for the sequential decision making
problem for speciﬁed α and β is deﬁned as
S∗= arg min
The sequential decision-making theory was developed
by Wald , who proved that the solution of the optimization problem (3) is the sequential probability ratio test.
2.1. Sequential Probability Ratio Test
Let x be an object characterized by its class (hidden
state) y ∈{−1, +1}. The class (or hidden state) is not
observable and has to be determined based on successive
measurements x1, x2, . . ..
Let the joint conditional density p(x1, . . . , xt|y = c) of the sequence of measurements
x1, ..., xt be known for c ∈{−1, +1} and for all t.
SPRT is a sequential strategy S∗, which is deﬁned as:
B < Rt < A
1In pattern recognition, this is called “the rejection option”
where Rt is the likelihood ratio
Rt = p(x1, ..., xt|y = −1)
p(x1, ..., xt|y = +1).
The constants A and B are set according to the required
error of the ﬁrst kind α and error of the second kind β. Optimal A and B are difﬁcult to compute in practice, but tight
bounds are easily derived.
Theorem 1 (Wald). A is upper bounded by (1 −β)/α and
B is lower bounded by β/(1 −α).
Proof. For each sequence of measurements (x1, . . . , xt),
for which SPRT returns the class −1 we get from (4) and (5)
p(x1, . . . , xt|y = −1) ≥A · p(x1, . . . , xt|y = +1).
Since this holds for all sequences of measurements classi-
ﬁed to class −1 (S∗= −1), summing over these sequences
P{S∗= −1|y = −1} ≥A · P{S∗= −1|y = +1}.
The term on the left is the probability of correct classiﬁcation of an object from the class −1 and is therefore 1 −β.
The term on the right is the probability of incorrect classiﬁcation of an object to the class +1, and is equal to α. After
this substitution and rearranging, we get the upper bound on
A. Repeating this derivation with samples classiﬁed to +1
by SPRT, the lower bound on B is derived.
In practical applications, Wald suggests to set the thresholds A and B to their upper and lower bound respectively
The effect of this approximation on the test error rates
was summarized by Wald in the following theorem.
Theorem 2 (Wald). When A′ and B′ deﬁned in (8) are used
instead of the optimal A and B, the real error probabilities
of the test change to α′ and β′ for which
α′ + β′ ≤α + β.
Proof. From Theorem 1 it follows that
Multiplying the ﬁrst inequality by (1 −β′)(1 −β) and the
second by (1 −α′)(1 −α) and summing both inequalities,
the result follows.
This result shows that at most one of the probabilities α
and β can be increased and the other has to be decreased by
the approximation.
Theorem 3 (Wald). SPRT (with optimal A and B) is an
optimal sequential test in a sense of the optimization problem (3).
Proof. The proof is complex. We refer the interested reader
Wald analyzed SPRT behavior when the upper bound A′
and B′ is used instead of the optimal A and B. He showed
that the effect on the speed of evaluation is negligible.
However, Wald did not consider the problem of optimal
ordering of measurements, since in all of his applications
the measurements are i.i.d. and the order does not matter.
Secondly, Wald was not concerned with the problem of estimating (5) from a training set, since in the i.i.d case
p(x1, . . . , xt|y = c) =
p(xq|y = c)
and thus Rt can be computed incrementally from a one dimensional probability density function.
3. SPRT for non i.i.d. Samples
For dependent measurements, which is the case in many
computer vision tasks, SPRT can still be used if the likelihood ratio Rt, equation (5), can be estimated. However, that
usually encompasses many-dimensional density estimation,
which becomes infeasible even for a moderate number of
measurements.
We suggest to use the AdaBoost algorithm for measurement selection and ordering and for the conditional density
estimation. This is described in the following section. In
Section 3.2 an approximation for the likelihood ratio estimation is proposed for such (statistically dependent) measurements. The ﬁnal algorithm combining SPRT and Ada-
Boost is described in Section 4.
3.1. AdaBoost
The AdaBoost algorithm 2 is a greedy learning algorithm.
Given a labelled training set T
{(x1, y1), . . . , (xl, yl)}, where yi ∈{−1, +1}, and a set
of weak classiﬁers H, AdaBoost produces a classiﬁer of the
where h(t) ∈H are selected weak classiﬁers and usually
T ≪|H|. Weak classiﬁers can be of an arbitrary complexity but are often chosen to be very simple.
2The real valued version is used.
In AdaBoost training, an upper bound on the training error is minimized. The upper bound has an exponential form
e−yiHT (xi) =
t=1 h(t)(xi).
Training of the strong classiﬁer runs in a loop. One weak
classiﬁer is selected and added to the sum in each loop cycle. A selected weak classiﬁer is the one which minimizes
the exponential loss function (14)
h(T +1) = arg min
h J(HT + h),
It has been shown that the weak classiﬁer minimizing (15) is
h(T +1) = 1
2 log P(y = +1|x, w(T )(x, y))
P(y = −1|x, w(T )(x, y)),
where w(T )(x, y) = e−yHT (x) is a weight of a sample
(x, y) at cycle T . Furthermore, J is guaranteed to be lowered in each step if weighted error of h(T +1) is below 0.5.
As shown in , choosing a weak classiﬁer according
to (16) in each cycle of the AdaBoost learning converges
asymptotically to
T →∞HT (x) = ˜H(x) = 1
2 log P(y = +1|x)
P(y = −1|x).
Note that ˜H is proportional to the likelihood ratio (5). In the
following, the outputs of selected weak classiﬁers are taken
as measurements used in SPRT and the connection between
equations (17) and (5) is used to determine the thresholds A
3.2. Likelihood Ratio Estimation with AdaBoost
The likelihood ratio (5) computed on the outputs of weak
classiﬁers found by AdaBoost has the form
Rt(x) = p(h(1)(x), ..., h(t)(x)|y = −1)
p(h(1)(x), ..., h(t)(x)|y = +1),
where h(i)(x) was substituted for xi. Nevertheless, the outputs of the weak classiﬁers cannot be treated as statistically
independent.
To avoid the computation of Rt(x) involving a high dimensional density estimation, we propose to approximate it
so that this task simpliﬁes to a one dimensional likelihood
ratio estimation. The t-dimensional space is projected into
a one dimensional space by the strong classiﬁer function Ht
(see equation (13)). All points (h(1), ..., h(t)) are projected
to a value given by the sum of their individual coordinates.
Using this projection, the ratio (18) is estimated by
Rt(x) ∼= ˆRt(x) = p(Ht(x)|y = −1)
p(Ht(x)|y = +1).
Justiﬁcation of this approximation can be seen from
equation (17) which can be rewritten using Bayes formula
to the form
˜H(x) = −1
2 log R(x) + 1
2 log P(+1)
Thus, in the asymptotic case, the strong classiﬁer is related
directly to the likelihood ratio. In particular, it maps all
points with the same likelihood ratio to the same value of
˜H. Hence, the ratio in (19) is exactly equal the likelihood
ratio in the asymptotic case. For the non-asymptotic case
we adopt the assumption that a similar relation holds approximately between Ht(x) and ˆRt(x). Equation (19) simpliﬁes the problem to one dimensional density estimation,
for which we adopted the Parzen windows technique (see
Having obtained the likelihood ratio estimate ˆRt, the
SPRT can be applied directly. Assuming monotonicity of
the likelihood ratio, only two thresholds are needed on Ht
values. These two thresholds θ(t)
A and θ(t)
B , each one corresponding to one of the conditions in (4), are determined
uniquely by the bounds A and B. The SPRT then becomes
Ht(x) ≥θ(t)
Ht(x) ≤θ(t)
A < Ht(x) < θ(t)
The inequalities are inverted since Ht is proportional to
−ˆRt (see equation (20)).
The approximation (19) is approaching the correct value
with increasing t. However, if for low t inaccurate Rt is
used for estimation of the thresholds θ(t)
A and θ(t)
B , the conditions αS < α and βS < β may be violated. To reduce this
effect, we estimate the likelihood ratio in the following way.
The densities p(Ht(x)|y = +1) and p(Ht(x)|y = −1) are
estimated not from the training set directly, but from an independent validation set to get an unbiased estimate. Moreover, the estimation uses the Parzen windows technique
with the kernel width set according to the oversmoothing
rule for the Gaussian kernel 
hOS = 1.144σn−1/5,
where σ is the sample standard deviation and n the number
of samples. The hOS is an upper bound on an optimal kernel
width and thus, the density estimate is smoother than necessary for an optimal density estimation. Due to this conservative strategy, the evaluation time can be prolonged but
the danger of wrong and irreversible decisions is reduced.
Algorithm 1 WaldBoost Classiﬁcation
Given: h(t), θ(t)
(t = 1, . . . , T)
Input: a classiﬁed object x.
For t = 1, . . . , T
(SPRT execution)
If Ht(x) ≥θ(t)
B , classify x to the class +1 and terminate
If Ht(x) ≤θ(t)
A , classify x to the class −1 and terminate
If HT (x) > γ, classify x as +1. Classify x as −1 otherwise.
4. WaldBoost
4.1. Classiﬁcation
The structure of the WaldBoost classiﬁer is summarized
in Algorithm 1. The classiﬁcation executes the SPRT test
via a trained strong classiﬁer HT with a sequence of thresholds θ(t)
A and θ(t)
B . If Ht exceeds the respective threshold,
a decision is made. Otherwise, the next weak classiﬁer is
taken. If a decision is not made within T cycles, the input is
classiﬁed by thresholding HT on a value γ speciﬁed by the
For practical reasons, only limited number of weak classiﬁers is used, which implies truncation of the sequential
test. Wald studies the effect of truncation of the sequential test procedure, however, his derivations hold only for
cases where i.i.d. measurements are taken. For that case, he
shows, how the effect of truncation on the false negative and
false positive rates of the test declines with number of measurements taken. In our implementation, the ﬁnal threshold
is left unspeciﬁed. It is used to control the false positive and
the false negative rate in the application. It is also used in a
ROC curve generation in the experiment section.
4.2. Learning with Bootstrapping
WaldBoost learning is summarized in Algorithm 2. Beside a labelled training set, two additional parameters specifying desired ﬁnal false negative rate α and false positive
rate β of the output classiﬁer have to be speciﬁed. These
rates are used to compute the two thresholds A and B
according to equation (8).
They correspond to the ﬁnal
false positive and detection rates in the Viola-Jones cascade
building . Contrary to Viola-Jones, no stage false positive and detection rates are required.
The training runs in a loop, where the ﬁrst step is a standard AdaBoost search for the best weak classiﬁer (Step 1),
as described in Section 3.1. Then, the likelihood ratio is estimated (Step 2) and the thresholds θ(t)
A and θ(t)
B are found
(Step 3), as described in Section 3.2. Based on the thresholds, the training set is pruned (Step 4) and then, pruned
Algorithm 2 WaldBoost Learning with Bootstrapping
Input: (x1, y1), ..., (xl, yl); xi ∈X, yi ∈{−1, 1},
desired ﬁnal false negative rate α and false
positive rate β.
Initialize weights w1(xi, yi) = 1/l
Set A = (1 −β)/α and B = β/(1 −α)
For t = 1, ..., T
1. Choose ht according to equation (16),
2. Estimate the likelihood ratio Rt according to eq. (19)
3. Find thresholds θ(t)
A and θ(t)
4. Throw away samples from training set for which
B or Ht ≤θ(t)
5. Sample new data into the training set
Output: strong classiﬁer HT and thresholds θ(t)
A and θ(t)
training set is enlarged again by newly bootstrapped samples (Step 5). Step 4 and 5 are similar to the cascade building procedure with the substantial difference that the
pruning and new data collection in the WaldBoost learning
are run after every weak classiﬁer is trained.
4.3. WaldBoost Applied to Face Detection
The proposed algorithm can be used in any classiﬁcation
task. Nevertheless, it is specially designed for tasks where
the classiﬁcation time is an important factor. In our experiments (Section 5) the abilities of the proposed algorithm
are demonstrated on the face detection task. Except for the
time constraints, the face detection problem has two other
properties: (i) highly unbalanced face and background class
sizes and complexities, and (ii) particular requirements on
error of the ﬁrst and the second kind.
The face class size is relatively small and compact compared to the background class. The face class samples are
difﬁcult to collect and too much pruning can reduce the size
of the face training set irreversibly. The background class,
on the other hand, consists of all images except the images
of a face itself. Such a huge and complex class cannot be
represented by a small training set sufﬁciently. So, the goal
of the learning is to explore the largest possible subspace of
the background class while keeping most of the face samples during the learning process.
The second property of the face detection is that error
of the ﬁrst kind (missed face) is considered as more serious
than error of the second kind (falsely detected face). An
ideal way of training a classiﬁer would be to require a zero
false negative rate and the smallest possible false positive
false positives
detection rate
Viola−Jones 
Boosting chain 
FloatBoost 
Figure 1. ROC curve comparison of the WaldBoost algorithm with
the state-of-the-art methods.
Having the above properties in mind, WaldBoost can be
speciﬁed in the following way. Let the required false positive rate β is set to zero and the required false negative rate α
to some small constant (note the inverse initialization compared to the above reasoning). In this setting, equations (8)
and the SPRT strategy (4) becomes
0 < Rt < 1/α
Since Rt is always positive, the algorithm will never classify a sample to the face class. The only allowed decision is
the classiﬁcation to the background class. Hence, the learning process will never prune the face part of the training set
while pruning the background part. Such initialization thus
leads to an exploration of the background class (by pruning
and new sample collection) while working with a small and
unchanging face training set. Moreover, the detection rate
of the ﬁnal classiﬁer is assured to be 1 −α while the false
positive rate is progressively reduced by each training cycle.
5. Experiments
The proposed WaldBoost algorithm was tested on the
frontal face detection problem. The classiﬁer was trained
on 6350 face images divided into a training and a validation set. In each training cycle, the non-face part of the
training and the validation set included 5000 non-face samples sampled randomly from a pool of sub-windows from
more than 3000 non-face images. The weak classiﬁer set H
used in training is the same as in but WaldBoost is not
feature-speciﬁc and any other weak classiﬁers can be used.
Unlike , the weak classiﬁers are real valued (deﬁned by
equation (16)) and implemented as in . The allowed false
negative rate α was set to 5·10−4. The training was run with
T = 600, i.e. till the strong classiﬁer consisted of 600 weak
classiﬁers.
The WaldBoost classiﬁer was tested on the MIT+CMU
dataset consisting of 130 images containing 507 labeled
faces. A direct comparison with the methods reported in
literature is difﬁcult since they use different subsets of this
dataset with the most difﬁcult faces removed (about 5 % in
 !). Nevertheless, we tested the WaldBoost classiﬁer
on both full and reduced test sets with similar results, so we
report the results on the full dataset and plot them in one
graph with the other methods (see Figure 1). However, the
results of the other methods are not necessarily mutually
comparable.
The speed and the error rates of a WaldBoost classiﬁer
are inﬂuenced by the classiﬁer length. To examine this effect, four classiﬁers of different lengths (300, 400, 500 and
600 weak classiﬁers) were compared. The average evaluation time ¯TS (for deﬁnition see (1)) for these four classiﬁers
is reported in Table 1. As expected, the average evaluation
time decreases when less weak classiﬁers are used. However, shortening of the classiﬁer affects the detection rates
as well. The ROC curves for the four classiﬁers are depicted
in Figure 2. Detection rates are comparable for the classi-
ﬁers consisting of 400, 500 and 600 weak classiﬁers but the
detection rate drops signiﬁcantly when only 300 weak classiﬁers are used. Thus, using the classiﬁer consisting of 400
weak classiﬁers only may be preferred for its faster evaluation. However, further reducing the classiﬁer length leads
to a substantial detection results degradation.
For a comparison of the WaldBoost classiﬁer length with
the other methods see Table 2. From the compared methods,
the WaldBoost classiﬁer needs the least number of weak
classiﬁers, or in other words it produces the most compact
classiﬁer.
The bottom row of Table 2 shows the average evaluation times to decision ¯TS (sometimes reffered to as the average number of weak classiﬁers evaluated) for the compared
methods. The WaldBoost learning results in the fastest classiﬁer among the compared methods except for the Viola-
Jones method which, despite its high speed gains signiﬁcantly worse detection results.
To conclude the experiments, the WaldBoost algorithm
applied to the face detection problem reduced the number
of measurements needed for a reliable classiﬁcation. The
detection rates reached by the proposed algorithm are com-
Table 1. Speed for different length WaldBoost classiﬁers.
Table 2. The number of weak classiﬁers used and a speed comparison with the state-of-the-art methods. The parentheses around
¯TS of Li’s method indicate that this result was not reported by the
authors but in .
parable to the state-of-the-art methods. The only method
outperforming the proposed algorithm in the quality of
detection is the “nesting-structured cascade” approach by
Wu . This can be caused by different features used, different subset of the MIT+CMU dataset used or any other
implementation details.
6. Summary and Conclusions
In this paper, the two-class classiﬁcation problems with
a decision quality and time trade-off are formulated in the
framework of the sequential decision-making. We adopted
the optimal SPRT test and enlarged its applicability to problems with dependent measurements.
In the proposed WaldBoost algorithm, the measurements
are selected and ordered by the AdaBoost algorithm. The
joint probability density function is approximated by the
class-conditional response of the sequence of strong classiﬁers. To reduce the effect of inaccurate approximation
in early cycles of training, a conservative method using
Parzen windows with a kernel width set according to the
oversmoothing rule was used.
The proposed algorithm was tested on the face detection
problem. On a standard dataset, the results are superior to
the state-of-the-art methods in average evaluation time and
comparable in detection rates. In the face detection context,
the WaldBoost algorithm can be also viewed as a theoretically justiﬁable ”boosted cascade of classiﬁers” proposed
by Viola and Jones .
Acknowledgments
The authors were supported by The Czech Ministry of
Education under project 1M6840770004 and by The European Commission under project IST-004176.
false positives
detection rate
Figure 2. The effect of reducing the number of weak classiﬁers in
WaldBoost classiﬁer on the detection rate.