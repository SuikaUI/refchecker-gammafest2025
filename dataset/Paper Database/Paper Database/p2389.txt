DOPING: Generative Data Augmentation for
Unsupervised Anomaly Detection with GAN
Swee Kiat Lim, Yi Loo, Ngoc-Trung Tran, Ngai-Man Cheung, Gemma Roig, Yuval Elovici
ST Electronics - SUTD Cyber Security Laboratory, Singapore University of Technology and Design
{sweekiat lim, loo yi, ngoctrung tran, ngaiman cheung, gemma roig, yuval elovici}@sutd.edu.sg
Abstract—Recently, the introduction of the generative adversarial network (GAN) and its variants has enabled the
generation of realistic synthetic samples, which has been used
for enlarging training sets. Previous work primarily focused on
data augmentation for semi-supervised and supervised tasks. In
this paper, we instead focus on unsupervised anomaly detection
and propose a novel generative data augmentation framework
optimized for this task. In particular, we propose to oversample
infrequent normal samples - normal samples that occur with small
probability, e.g., rare normal events. We show that these samples
are responsible for false positives in anomaly detection. However,
oversampling of infrequent normal samples is challenging for
real-world high-dimensional data with multimodal distributions.
To address this challenge, we propose to use a GAN variant
known as the adversarial autoencoder (AAE) to transform
the high-dimensional multimodal data distributions into lowdimensional unimodal latent distributions with well-deﬁned tail
probability. Then, we systematically oversample at the “edge”
of the latent distributions to increase the density of infrequent
normal samples. We show that our oversampling pipeline is a
uniﬁed one: it is generally applicable to datasets with different
complex data distributions. To the best of our knowledge, our
method is the ﬁrst data augmentation technique focused on
improving performance in unsupervised anomaly detection. We
validate our method by demonstrating consistent improvements
across several real-world datasets1.
Index Terms—unsupervised learning, anomaly detection, generative adversarial network, GAN, adversarial autoencoders,
data augmentation
I. INTRODUCTION
Data augmentation and oversampling are important approaches to handle imbalanced data in machine learning.
Previous data augmentation and oversampling approaches have
focused on the supervised setting. For example, Synthetic Minority Over-sampling Technique (SMOTE) and Structure
Preserving Oversampling (SPO) use minority class samples
to generate synthetic samples. In such methods, class labels are
essential for identifying and selecting these minority samples.
In this work, instead, we focus on the unsupervised setting.
This is important for many anomaly detection problems: the
anomalous events, which are the minority samples, may be
undetected or unknown previously, e.g. a zero-day cyberattack. For unsupervised anomaly detection, previous data
argumentation approaches are ineffective, since labels and
1Corresponding author: Ngai-Man Cheung (ngaiman ). Earlier
version was accepted in IEEE International Conference on Data Mining 2018 (ICDM-
18). Our code will be published here: 
Fig. 1: Our DOPING method is a form of unsupervised data
augmentation. Consider the above 2D dataset with 5% contamination, where the anomalies are clustered in the middle.
Note that we focus on unsupervised setting, and the class
labels are unknown to the methods. In this case, it is unclear
how data augmentation should be performed since no class
label is provided. Using an interpolation method such as in
SMOTE , we can randomly synthesize samples across the
training set (red), but this increases the density of all samples
indiscriminately, which is not helpful for most unsupervised
anomaly detection algorithms. Instead, we propose DOPING, a
method that strategically synthesizes samples at the edge of the
normal distribution (green). Our DOPING samples help to reduce false positive rates by increasing the density of infrequent
normal samples. In the above example, both the False Positives
and DOPING samples occur in overlapping areas. Using our
novel idea of sampling in the latent distribution imposed by
an adversarial autoencoder (AAE), infrequent normal samples
can be easily generated even for high-dimensional data with
multi-modal distributions. We emphasize that we achieve this
without using class labels.
 
knowledge of minority classes are unavailable to the systems
(Figure 1).
One of the key challenges of anomaly detection systems is
the pervasiveness of false positives (positives are anomalous events). This is especially true when high recall is
required, such as in malware detection, where it is preferable
to raise a false alarm rather than let slip an undetected
virus. High false positive rates can be due to the difﬁculty
in deﬁning a distribution which contains all of the normal
data. In particular, infrequent normal samples, deﬁned here as
normal samples that occur with very small probability, occur
rarely in the training dataset and are poorly characterized in
many density-based anomaly detectors , . The boundary
between normal and anomalous data is typically imprecise,
resulting in misclassiﬁcation of samples close to the boundary
We hypothesize that in unsupervised anomaly detection, targeted oversampling of infrequent normal samples will be most
effective, since such samples are primarily responsible for false
positive errors. In this paper, we ﬁrst proceed to validate this
hypothesis in our study. Based on this idea, we then introduce a
novel data augmentation method for systematically generating
such infrequent normal samples without using class labels.
Speciﬁcally, we propose a data augmentation method targeted at unsupervised anomaly detection, which we coin
DOPING. Generating infrequent normal samples can be difﬁcult for high-dimensional data with complicated distributions,
e.g., multi-modal (Figure 1). We propose to overcome this
challenge by using the adversarial autoencoder (AAE) , a
variant of the generative adversarial network (GAN) . In
particular, using the AAE, we impose a multivariate Gaussian
on the latent space of the training data, thereby transforming
different data distributions into a unimodal distribution with
well-deﬁned tail probability in the latent space. Then, infrequent normal samples can be easily generated by sampling
this unimodal latent distribution. We will go on to show
that our uniﬁed and systematic latent space sampling can
handle diverse datasets with complicated data distributions. In
summary, our DOPING technique differs both from previous
works where GAN architectures were used in anomaly detection – , as well as previous works on data augmentation
for supervised classiﬁcation – :
• It is an unsupervised technique that does not require any
normal or anomalous labels. Our proposed method trains
the AAE on the entire dataset without labels and utilizes
general knowledge of the latent distribution to generate
desired samples.
• Rather than an independent anomaly detection technique,
DOPING is a form of unsupervised data augmentation.
This makes it complimentary with any anomaly detection
algorithm, by augmenting the dataset before training.
• DOPING differs from previous data augmentation methods, in that it uses a systematic sampling strategy that has
been validated in our analysis to be effective for anomaly
detection. In contrast, previous methods mainly focused
on generating artiﬁcial samples in a minority class.
In the following sections, we discuss details of our DOP-
ING method for unsupervised anomaly detection. We perform
studies to validate the effectiveness of oversampling infrequent
normal samples. Then, we compare DOPING to other data
augmentation techniques and validate our method on several
real-world datasets.
II. RELATED WORK
Autoencoders have previously been used directly for
anomaly detection – . Munawar et al. used both
normal and anomalous labeled data to train an autoencoder by
minimizing reconstruction loss of normal data and maximizing
the loss for anomalous data. The trained autoencoder then
detects anomalies by attempting to reconstruct test samples
and measuring reconstruction loss, which is larger for anomalies. A recent work by Zhou et al. also introduced
robust deep autoencoders (RDA), an unsupervised anomaly
detection method that combined the autoencoder and Robust
Principal Component Analysis (PCA). The model is trained to
decompose the dataset as the sum of two matrices - a normal
component that can be reconstructed with little loss via an
autoencoder and a sparse matrix consisting of anomalies.
Recent anomaly detection methods have also utilized the
GAN – . Typically, such methods are similar to the
approach above where autoencoders are used directly for
anomaly detection and are trained on a known normal dataset.
Schlegl et al. trained a GAN on the normal dataset and
deﬁned a loss function for mapping a new sample to a latent
vector. The anomaly score is given as the difference between
the sample generated by the latent vector and the original
sample. Similarly, Ravanbakhsh et al. trained a conditional
GAN to learn two generators that map pixel data to
motion and vice versa. Normal frames will then have low
reconstruction loss, whereas anomalous frames will be poorly
reconstructed. More recently, Zenati et al. utilized the
BiGAN for anomaly detection by training on normal data
and then deﬁning a score function based on reconstruction loss
and discriminator-based loss.
As a generative model, GANs have also been used for
data augmentation, generating samples in imbalanced or small
datasets, particularly in supervised classiﬁcation – .
More traditional data augmentation techniques for synthesizing data samples include geometric transformation or oversampling , .
Although our work uses the AAE, which combines the
GAN and the autoencoder, it is fundamentally different from
previous approaches, since ours is an unsupervised approach
and does not require a known set of normal samples or labels.
Furthermore, our novel method is a form of unsupervised data
augmentation that can be applied to any dataset or anomaly
detection algorithm.
Several oversampling techniques have been proposed:
SMOTE , Borderline-SMOTE and SPO and Integrated Oversampling (INOS) . However, these techniques
focus on oversampling the minority class for use in supervised
classiﬁcation. In contrast, our method focuses on unsupervised
anomaly detection, where we do not have access to label
information of the minority samples, which is required for
the aforementioned methods. Our data augmentation scheme is
derived from our analysis and speciﬁcally targets unsupervised
anomaly detection. In particular, we show that conventional
data augmentations do not lead to optimal improvement in
unsupervised anomaly detection.
In this paper, we propose a new mechanism to generate
synthetic data for improving anomaly detection systems in a
purely unsupervised setting. To the best of our knowledge,
our method is the ﬁrst data augmentation technique focused
on improving performance and reducing false positive rates
in unsupervised anomaly detection. We also compare against
previous data augmentation methods and show more consistent
and larger improvements in performance.
III. DOPING
We hereby introduce Doping with Infrequent Normal Generator (DOPING), a data augmentation technique for anomaly
detection algorithms (Figure 2). In physics, doping is a process whereby a material is introduced into a semiconductor,
typically to improve its electrical conductivity. Similarly, our
method introduces artiﬁcial samples to the original training set,
to improve the performance of anomaly detection algorithms.
These artiﬁcial samples take the form of infrequent normal
samples that are synthetically generated with AAEs.
Adversarial Autoencoders (AAE) . Consider a dataset
with data distribution pd(x) and an encoding function q(z|x)
from the autoencoder. The aggregated posterior distribution
q(z) on the latent vector is then deﬁned as:
q(z|x)pd(x)dx
The AAE tries to match q(z) to an arbitrary prior p(z), similar
to the variational autoencoder (VAE) . While the VAE uses
the KL-divergence for regularization, the AAE borrows the
adversarial concept from the GAN and attaches a discriminator
network that discriminates between outputs from the prior p(z)
and the encoder q(z). After training, q(z) then matches p(z).
Using this mechanism, demonstrated several interesting
latent spaces including a mixture of 10 2-D Gaussians and a
swiss roll distribution.
A. Data augmentation with DOPING: Details
Train Unlabeled AAE In DOPING, we apply the vanilla
Unlabeled AAE architecture . The encoder, decoder and
discriminator each have two layers of 1000 hidden units with
ReLU activation function, except for the activation of the last
layer of the decoder, which is linear. No label is used in the
training of the Unlabeled AAE. We use the ADAM optimizer
 with a learning rate of 10−4 for all networks. We use a
2-D Gaussian with mean 0.0 and standard deviation 10.0 on
both dimensions as the prior p(z). Note that other recent GAN
with encoder and decoder can also be used .
Sample Latent Space After training the AAE, we selectively sample and decode latent vectors to generate synthetic
samples. As will be discussed, our analysis ﬁnds improved
performance when we decode latent vectors at the boundary
of the normal latent distribution. Hence, we formalize the
DOPING technique as in Algorithm 2 and use an edge-based
sampling approach to sample the latent vectors for decoding.
Speciﬁcally, we use the AAE’s encoder E(·) to encode
the entire training set X and generate the corresponding set
of latent vectors Z. Z is then ﬁltered by the norm of the
latent vectors to form the subset Zedge (see Equation 1),
which contains latent vectors near the tail-end of the latent
distribution.
Zedge = {z ∈Z | α < ∥z∥< β}
Based on a validation dataset, we ﬁx β and α as follows for
all experiments in this paper: we set β as 3 standard deviations
larger than the mean of the latent vector norms for the training
set; we set α as the 90th percentile norm from the remaining
Interpolate Sampled Vectors Thereafter, we randomly sample latent vectors from Zedge and new latent vectors are
generated by interpolating these selected latent vectors with
their nearest neighbor in the latent space (Algorithm 1). These
new latent vectors form the set Zsynth.
Decode and Synthesize Samples We then decode the set of
new latent vectors Zsynth to generate synthetic samples Xsynth
for data augmentation. The synthetic samples Xsynth are added
to the original dataset X and the augmented dataset is used
to train the anomaly detector algorithm.
Algorithm 1 InterNN (interpolate with nearest neighbor)
Require: zsample: latent vector to interpolate
Sample α ∼U(0, 1)
zNN ←nearest neighbor of zsample in latent space
return α(zNN −zsample) + zsample
Algorithm 2 DOPING
Require: K: no. of samples to synthesize, X: set of training
samples, {α, β}: hyperparameters for edge-based sampling
(see Equation 1)
Train AAE on X, with multivariate Gaussian as prior
E(·) ←encoder in AAE
D(·) ←decoder in AAE
Zedge ←{z ∈Z | α < ∥z∥< β} (see Section III-A)
Initialize list of new latent vectors Zsynth ←[]
for i = 1, 2, ...K do
Sample infrequent latent vector zedge ∈Zedge
znew ←InterNN(zedge)
Zsynth append znew
Xsynth ←D(Zsynth)
Train anomaly detector on augmented dataset (X + Xsynth)
Fig. 2: Data augmentation with DOPING: System overview
B. Use of Adversarial Autoencoders
We speciﬁcally choose to adopt AAE over other GAN
variants in our framework for the following reasons:
Explicit Encoding to Latent Space While other GAN variants are able to synthesize new samples from a latent space
after training, it is non-trivial to encode an arbitrary sample
back into the latent space, e.g., requiring iterative estimation
 . In contrast, the AAE incorporates an autoencoder architecture, which allows explicit decoding from and encoding to the
latent space. The encoding network is crucial for our analysis
of samples in the latent space, while the decoding network
enables generation of synthetic samples from speciﬁc regions
in the latent space based on our analysis. The encoding and
decoding are tightly coupled in AAE, enabling our analysis
outcomes to directly guide sample synthesis.
Explicit Control of Latent Space Using the AAE, we are able
to impose a variety of prior distributions on the latent space,
similar to the VAE . However, the VAE requires access to
the exact functional form of the prior distribution. In contrast,
the adversarial method used by AAE only requires samples
from the desired prior, which allows us to impose more
complex distributions. By imposing the same prior distribution
and systematically decoding from this consistent latent space,
we have a general method that can be applied to any dataset.
In our method, we use the AAE to impose a multivariate
Gaussian on the latent space of the normal data and treat
that as a proxy for the data distribution. Our proposed use of
multivariate Gaussian as the prior transforms different complex
data distributions (e.g. multimodal, skewed) into unimodal
ones with well-deﬁned tail probability in the latent space.
This enables decoding from the edge of the latent Gaussian
distribution to generate synthetic samples that are infrequent
normals. As a data augmentation technique, DOPING is used
with another anomaly detection algorithm, such as Isolation
Forest (iForest) .
IV. ANALYSIS OF DOPING WITH SYNTHETIC DATASETS
In this section, we analyze and validate fundamental components of DOPING with synthetic datasets. We ﬁrst demonstrate
that the use of the AAE in our framework enables mapping
of diverse data distributions to a consistent latent distribution.
This enables a uniﬁed analysis, which reveals that there exists
a consistent region in the latent space for generating synthetic
samples that improves the anomaly detection performance.
Most importantly, this sampling method for data augmentation
is consistent despite the different data distributions.
A. Dataset and Experimental Design
Dataset We introduce three synthetic datasets that each comprise samples from a normal distribution and an anomalous
distribution (Figure 3a). Each dataset contains 1000 training
samples and 1000 test samples, with 95% of the samples
belonging to the normal distribution and 5% of the samples
belonging to the anomalous distribution.
• Dataset A The normal distribution is a 2-D Gaussian
with mean at origin and standard deviation 10.0 on each
dimension, while the anomalous distribution is a 2-D
Gaussian with mean [30.0, 0.0] and standard deviation
5.0 on each dimension.
• Dataset B A 3-D version of Dataset A.
• Dataset C The normal distribution follows a ring with
radius of mean 30.0 and standard deviation 5.0, while the
anomalous distribution is a 2-D Gaussian with mean at
origin and standard deviation 5.0 on each dimension. This
is meant to be a challenging dataset, with the anomalies
surrounded by normal samples, making it more difﬁcult
for anomaly detection algorithms to recognize anomalies.
Baseline Method In this experiment, we use the popular iForest anomaly detection algorithm. We use the implementation
in version 0.19.1 of the scikit-learn package and vary the
contamination hyperparameter from 0.01 to 0.69 in increments
of 0.04 to generate the receiver operating characteristic (ROC)
curve for evaluation.
Experiment Details In this section, we ﬁrst adopt a Labeled
AAE architecture to analyze our method. It is similar to
the Unlabeled AAE, with the exception of label information
provided to the discriminator network in the form of a onehot vector. In our case, the label indicates if the sample is
normal or anomalous. If the sample is anomalous, we use a
ring with radius 100.0 as the prior p(z), by randomly sampling
a latent vector with l2-norm of 100.0. If the sample is normal,
we follow the prior used in the Unlabeled AAE. All other
conditions are the same as described in the Unlabeled AAE.
This Labeled AAE is used only in this analysis to understand
the effectiveness of sampling at different latent regions. In
practice, Unlabeled AAE is used, as will be further discussed.
In this section, we also use a magnitude-based sampling
method for synthesizing new samples, in order to demonstrate
the effects of sampling from different regions in the latent
distribution. In magnitude-based sampling, we sample the
latent vectors from an n-dimensional spherical surface of
increasing magnitudes. In this case, with a 2-D latent space,
we essentially sample from rings of increasing magnitudes.
We sample latent vectors at different l2-norms from 5.0
to 100.0 at increments of 5.0. At each l2-norm, we decode
Fig. 3: (a) The three synthetic datasets used and (b) the corresponding latent spaces after training with the AAE.
and add the 100 random samples to the original dataset. Each
augmented dataset of 1100 samples is then used to train the
anomaly detectors and tested on the test set, to generate the
ROC curves. We then measure the area-under-curve (AUC)
for each ROC curve and plot the AUC against the l2-norm,
as shown in Figure 5. We treat predictions of anomalies as
positive instances for the calculations of the AUC.
B. Mapping of Data Spaces to Latent Distributions
Figure 3a shows 3 different datasets. By training an AAE
with the same prior on each of these datasets, we are able to
map these datasets to similar latent distributions, in Figure
3b. In particular, this happens even for Dataset C, which
is particularly challenging since the AAE has to switch the
distributions of the normal and anomalous samples.
Furthermore, we are able to systematically sample the latent
space to generate corresponding samples in the data space.
With the use of 2-D Gaussian, which has well-deﬁned tail
probability, as the prior, we can sample latent vectors of
different l2-norms, corresponding to different probabilities or
likelihoods. This is akin to sampling latent vectors at rings of
different radii. Since the anomalous latent distribution forms a
ring around the normal distribution, as we increase the l2norm of the sampled latent vectors, the generated samples
should intuitively transform from normal to anomalous. Figure
4 shows the data samples generated by decoding from different
rings in the latent space for Datasets A and C.
C. Optimal Latent Space Sampling for Synthesizing Samples
In this section, we test our hypothesis that decoding from
the boundary of the normal latent distribution to generate
infrequent normal samples provides improvement in anomaly
detection performance, when such samples are added to the
training set.
All three graphs in Figure 5 clearly show the same trends,
with signiﬁcantly better AUC performance when DOPING
with latent vectors of l2-norms in the range of 15.0 to 20.0.
With reference to Figure 3b, this region corresponds to the
boundary of the normal latent distribution, which agrees with
our hypothesis.
The optimal samples for data augmentation is not necessarily intuitive when viewed in the data space. For instance,
in Dataset C, it is not immediately clear if the ideal synthetic
samples should lie in the inner or outer boundary of the normal
distribution. Despite that, Figure 5 shows that an improvement
can be consistently achieved with our method, when decoding
from the boundary of the latent distribution. Referencing to
Figure 4 reveals that latent vectors at the boundary of the
normal distribution map to data samples on the outer boundary
of the normal data.
Thereafter, as l2-norm increases, the AUC falls to below that
of the original dataset, implying that addition of samples in
this region actually worsens the performance of the anomaly
detectors. In the context of our hypothesis, samples decoded
from latent vectors in this region corresponds to anomalous
samples. By increasing the density of anomalies in the training set, the anomaly detector algorithms are more likely to
misclassify anomalies as normal, which worsens performance.
Unsupervised Data Augmentation Since samples at the
boundary of the normal latent distribution provide improvement in anomaly detection performance, in practice, the La-
Fig. 4: Visualization of samples decoded from the latent space
of (a) Dataset A and (b) Dataset C. Latent vectors at the
boundary of the latent normal distribution (blue) decode to
points at the boundary of the normal distribution in the original
data space.
beled AAE can be replaced with the unsupervised Unlabeled
AAE. We reiterate that the nature of anomaly detection tasks
is that “anomalies are ‘few and different’, which make them
more susceptible to isolation than normal points” . With
low contamination, an Unlabeled AAE trained on the entire
dataset can approximate the normal latent distribution in a
Labeled AAE and the contamination due to the ’few and
different’ anomalies can be neglected. We demonstrate this
in the following sections where we only use the Unlabeled
V. ANALYSIS OF DOPING WITH MNIST EXPERIMENT
In the previous section we introduced a consistent data augmentation method to improve anomaly detection performance.
Here we validate this methodology on the MNIST dataset, a
more complex dataset with much higher dimension, and show
that the method works in an unsupervised manner with the
Unlabeled AAE. We also compare different prior distributions
for the Unlabeled AAE. We then compare our method against
other data augmentation methods across several state-of-the-art
anomaly detectors and show that our method gives consistent
and substantial improvement.
A. Dataset and Experimental Setup
Dataset We follow the setup used by Zhou et al. in their
paper on anomaly detection with Robust Deep Autoencoders
(RDA) . Our dataset comprises 5000 MNIST images,
of which 4750 (95%) normal images are from the class ‘4’ and
250 (5%) anomalous images are sampled randomly from the
remaining nine classes.
Baseline Method We use iForest as the anomaly detector.
We use the implementation in version 0.19.1 of the scikit-learn
package and vary the contamination hyperparameter from
0.01 to 0.69 in increments of 0.02 to generate the ROC curve
for calculating AUC.
Baseline Data Augmentation Techniques We compare with
two data augmentation techniques - adding random noise and
elastic deformation , , an optimized deformation for
image augmentation.
• Addition of Random Noise We randomly select a sample
from the original dataset and then randomly pick 10% of
the pixels in a sample and set the value of each selected
pixel to 1 if the original pixel value is less than 0.5 and
0 if the original pixel value is more than 0.5.
• Elastic Deformation We randomly select a sample from
the original dataset and apply the elastic deformation
algorithm with optimal settings of σ = 4 and α = 34,
described by Simard et al. .
In each case, we generate 500 augmented or synthesized
samples and add them to the original dataset. We then train
the detection algorithm on the 5500 samples while testing on
5000 samples without augmentation.
Experiment Details For DOPING, we follow the Unlabeled
AAE implementation described in Section III-A, since we
consider anomaly detection as a primarily unsupervised task.
Fig. 5: Graphs of AUC against l2-norm/magnitude for the different synthetic datasets, showing a consistent trend.
GG(µ, α, β) =
2αΓ(1/β)e−(|x−µ|)/αβ
For comparison across different priors, we use the same architecture as the Unlabeled AAE described earlier but replace
the Gaussian prior with a generalized Gaussian (Eqn. 2) of
different β parameters, where β = 2 gives the regular Gaussian
distribution and as β →∞, the distribution approaches a
uniform distribution . For this experiment, we vary β from
0.5 to 3, keeping α and µ (other parameters in the generalized
Gaussian function) consistent at 10.0 and 0.0 respectively. We
also implement a version with a prior distribution of an 8-D
multivariate Gaussian with isotropic covariance matrix of 10I.
We ﬁrst conduct analysis of the latent space with magnitudebased sampling as described in Section IV-A. Subsequently,
we show the results with the full DOPING method described
in Section III-A.
B. Optimal Sampling with Unlabeled AAE
Here we validate our hypothesis on a high-dimensional
dataset and show the effectiveness of the Unlabeled AAE,
which allows our method to be unsupervised.
Figure 6a shows the encodings of the original 5000 samples
in the 2-D latent space, using an Unlabeled AAE, which follow
a 2-D Gaussian. Since no label was used in the training of
the AAE, both normal and anomalous samples are mixed in
the distribution, although we observe some clustering of the
anomalies.
Since the latent distribution follows a 2-D Gaussian, we are
able to generate different probabilities of samples by sampling
from rings of different radii, similar to the previous section.
Figure 6b shows decoded samples drawn from different rings
in the latent space. Intuitively, samples drawn close to the
center of the latent distribution are more likely to look like
common samples in the original data and hence more normal,
whereas samples drawn from outside of the latent distribution
show signiﬁcant degradation. Finally, samples drawn from the
boundary of the latent distribution resemble infrequent but
normal images from the dataset.
Figure 7 plots AUC against l2-norm/magnitude of sampled
latent vectors for the iForest algorithm. We observe that the
maximum AUC is achieved at magnitude 15.0, which again
corresponds to the boundary of the latent distribution.
Fig. 6: (a) 2D latent space encodings of the 5000 MNIST
training images, (b) synthesized samples from latent vectors
with l2-norms/magnitudes 1 (inside), 20 (boundary) and 60
(outside) and (c) synthesized samples from latent vectors
sampled with edge-based sampling.
Fig. 7: Graph of AUC against l2-norm/magnitude of the latent
vectors sampled for magnitude-based sampling applied to iForest on the MNIST experiment, compared against performance
without DOPING and with edge-based DOPING.
Similar to the previous section, as we sample from increasing magnitudes, we see that the AUC worsens and falls below
the performance of the original data without augmentation,
implying that addition of samples at this region worsens the
performance of the iForest algorithm.
Finally, we also observe that addition of 500 samples
generated using DOPING reduces False Positive Rate from
0.31 to 0.26, at a consistent True Positive Rate of 0.80.
C. Comparison of Prior Distributions
Following the previous sections, we see that AAE is able to
induce similar latent distributions for diverse datasets. Through
empirical results on the synthetic datasets, we also validate
our hypothesis that decoding from samples at the boundary of
the latent normal distribution results in the best improvement
in anomaly detection and that we are able to do this with
an Unlabeled AAE. In this section, we experiment with
various prior distributions for generating artiﬁcial samples,
using various settings from the generalized Gaussian ,
detailed in Section V-A.
We use our method with the iForest algorithm for each of
the prior distributions in Section V-A and sample from a range
of l2-norms from the latent. Table I shows the best AUC
results obtained for each prior, which is consistently better
than the performance of iForest on the original dataset without
More interestingly, as β increases, the optimal magnitude
for obtaining the best AUC performance decreases. This makes
sense intuitively, since the tails of the Generalized Gaussian
grow lighter as β increases, which also decreases the boundary
of the distribution. In previous sections, we demonstrated that
decoding from the boundary of the latent distribution gives
signiﬁcant improvement in anomaly detection performance.
So, we see here that as β increases, the boundary decreases
and the optimal magnitude decreases as well.
This intuition is also true for the 8-D Gaussian. The
distribution of l2-norms for a multivariate Gaussian follows
a χ-distribution with n degrees of freedom where n is the
number of dimensions for the original multivariate Gaussian.
As n increases, the mean of the χ-distribution increases as
well, which increases the boundary of the distribution. Hence
we see here that the optimal magnitude for the 8-D Gaussian
prior is higher than that of the 2-D Gaussian.
Here we show that the prior distribution in DOPING is not
limited to the regular Gaussian (β = 2). For the rest of the
paper, we use the regular 2-D and 8-D Gaussian for simplicity.
D. Comparison Against Other Data Augmentation Methods
Earlier, we demonstrated that the optimal region for sampling latent vectors is the edge of the latent distribution. From
hereon, we show the results of the full DOPING method as
in Algorithm 2 and Section III-A. The edge-based sampling
described in Section III-A allows us to reliably sample latent
vectors from the edge of the distribution, without manually
tuning the magnitude as in magnitude-based sampling.
TABLE I: AUC performance of DOPING with different priors,
using the iForest algorithm in MNIST experiment.
Optimal Mag.
Original Dataset
2-D Generalized Gaussian (β = 0.5)
2-D Generalized Gaussian (β = 1)
2-D Regular Gaussian (β = 2)
2-D Generalized Gaussian (β = 3)
8-D Regular Gaussian
TABLE II: AUC Performance of the iForest algorithm with
different data augmentations in the MNIST experiment.
No Augmentation
Random Noise
Elastic Deformation
DOPING 2-D (Our Method)
DOPING 8-D (Our Method)
Figure 6c shows the decoded samples when we use edgebased sampling. Qualitatively, the edge-based samples are very
similar to the samples obtained by magnitude-based sampling
at magnitude 20, since the sampling from magnitude 20 also
decodes from the boundary of the latent distribution.
We proceed to compare our method with other data augmentation methods when used with various state-of-the-art
anomaly detection algorithms. For our method, we report
results using the Unlabeled 2-D Gaussian prior described in
the previous section. In addition, we also report a variant with
an Unlabeled 8-D Gaussian prior, since recommends a 5-D
to 8-D latent space to adequately encode the MNIST dataset.
Results Table II shows the AUC performance of Isolation
Forest in the MNIST experiment, with different types of
data augmentation. In general, we see that data augmentation
methods provide moderate improvement to the AUC performance. However, both 2-D and 8-D DOPING consistently
show comparable or much better performance, with the 8-D
DOPING variant performing far better. This is likely due to
the higher latent dimension which allows the latent space to
better encode the dataset.
Figure 7 also compares the performance of edge-based
sampling to the AUC results for magnitude-based sampling at
different magnitudes. We see that edge-based sampling shows
similar performance to the best result obtained by magnitudebased sampling.
Latent Space and t-SNE Visualization For visualization
and analysis, we begin by isolating the samples that are
frequently classiﬁed as false positives by the detectors when
no augmentation is used. This is done by tagging all the
false positive samples that occur at the iForest contamination
hyperparameter of 0.13. We accumulate a total of 411 samples,
which we hereon denote as False Positives.
We then encode the False Positives, as well as the encodings
of samples from the data augmentation techniques, back to
the latent space to visualize the respective latent distributions.
Figure 8a shows the encodings of samples from the data augmentation techniques, with 500 samples from each technique,
Fig. 8: (a) Latent space encodings of samples from augmentation methods in the MNIST experiment and (b) same samples
visualized with t-SNE . Note that Common False Positives
and DOPING samples occur in similar regions.
as well as the 411 false positive samples.
For Random Noise and Elastic Deformation, the distributions follow the distribution of the original dataset. In contrast,
samples generated by 2-D and 8-D DOPING resemble ringlike distributions in the latent space. Most importantly, the latent distributions of the 2-D and 8-D DOPING samples overlap
strongly with that of the False Positives. The similar clustering
between the False Positives and the samples generated by our
method are even more apparent when we visualize the images
with the t-SNE algorithm . Again, the False Positives
and samples generated by both 2-D and 8-D DOPING are
signiﬁcantly denser towards the edge of the distribution.
This demonstrates that the samples generated by DOPING
help to increase the density of these False Positives in the
training set, which allows the anomaly detection algorithm to
better recognize these samples as normal. It is important to
note that DOPING is able to achieve this without explicitly
going through a round of anomaly detection testing to highlight the false positives.
VI. EVALUATION WITH REAL-WORLD DATASETS
Here, we evaluate the performance of the formal DOPING
algorithm, as described in Section III-A and Algorithm 2, on
TABLE III: Comparing best F1 on real-world datasets.
iForest 
TABLE IV: Comparing G-measure on real-world datasets.
iForest 
several real-world datasets.
A. Dataset and Experimental Setup
Datasets We show the results of DOPING on four anomaly detection datasets from the Outlier Detection DataSets (ODDS)
repository2.
• Mammography The Mammography dataset is obtained from the ODDS repository, with each sample having 6 features. The dataset comprises of 11183 samples,
including 260 anomalies (2.3%contamination).
• Thyroid The Thyroid dataset is obtained from the
ODDS repository, with each sample having 6 features.
The dataset comprises of 3772 samples, including 93
anomalies (2.5% contamination).
• Lymphography The Lymphography dataset is obtained from the ODDS repository, with each sample
having 18 features. The dataset comprises of 148 samples,
including 6 anomalies (4.0% contamination).
• Cardiotocography The Cardiotocography dataset is
obtained from the ODDS repository, with each sample
having 21 features. The dataset comprises of 1831 samples, including 176 anomalies (9.6% contamination).
For all datasets, we follow the settings in , with
completely clean training data: in each run, we randomly
sample 50% of the data for training with the remaining 50%
reserved for testing, and only data samples from the normal
class are used for training models.
Baseline Method iForest is used as the anomaly detector.
We use the implementation in version 0.19.1 of the scikit-learn
package and vary the contamination hyperparameter from
0.01 to 0.69 in increments of 0.02 to calculate the AUC and
search for the best F1.
For each dataset, we train iForest with four variants: the
original training set and three training sets augmented with
DOPING, a SMOTE variant and an INOS variant 
respectively. For each augmentation method, we synthesize an
additional 10% of the original training set size.
2 
Experiment Details When implementing DOPING on these
datasets, we ﬁrst train an Unlabeled AAE on the training sets
with a 2-D regular Gaussian prior with standard deviation 10.0
on both dimensions. We then synthesize samples using edgebased sampling as described in Section III-A.
For comparison, we also show the results of iForest with
two other data augmentation methods. Since the earlier data
augmentation methods in Section V do not transfer readily to
non-image domains, we use variants of SMOTE and INOS
 as comparisons. For both of the variants, we randomly
select samples from the training set and apply interpolation in
the data space. In the original SMOTE and INOS algorithms,
only the minority class is oversampled in this manner. Here
we randomly sample from the entire training set since we do
not have labels in unsupervised anomaly detection.
For the SMOTE variant, we implement interpolation with
nearest neighbor (Algorithm 1) on randomly selected samples
across the whole training set. For INOS, we implement the
technique using the OSTSC package and set the r variable
as 0.8 where 80% of the synthetic samples are generated using
ESPO and 20% using ADASYN.
B. Results and Discussion
Tables III and IV show the performance of Isolation Forest
 with and without augmentation, via F1 score and Gmeasure, where F1 is the harmonic mean of precision and
recall and G-measure is the geometric mean.
We ﬁrst observe that training with the augmented datasets
mostly give better results than baseline iForest, implying that
data augmentation has a positive effect on anomaly detection.
Since a clean training set is used here, we postulate that
general data augmentation of the uncontaminated training
set helps to increase the density and variety of normal data
samples in the training set.
Furthermore, we see that iForest with DOPING demonstrates the best performance across all datasets, with the
exception of Mammography. Rather than randomly synthesizing training samples as with other augmentation methods,
DOPING purposefully synthesizes infrequent normal samples
and the better performance may be attributed to the anomaly
detector better recognizing these infrequent normal samples in
the dataset.
VII. CONCLUSION AND FUTURE WORK
This paper proposes a novel form of data augmentation
designed to tackle the problem where infrequent normal instances are misclassiﬁed, thereby reducing false positives. This
is done by using an AAE to impose a multivariate Gaussian on
the latent space and subsequently decoding from the edge of
this latent distribution to generate infrequent normal samples.
In this paper we explained the intuition behind why DOP-
ING helps to improve the performance of anomaly detectors
and analyzed the performance of DOPING on several datasets.
We show that the AUC against l2-norm/magnitude trend is
consistent across different datasets and DOPING makes use of
this consistent trend to synthesize speciﬁc samples for dataset
augmentation. Our experiments demonstrate empirically that
our data augmentation technique helps to improve anomaly detection performance when applied across a variety of datasets.
Finally, the experiments reported here are meant to demonstrate the potential of our method and used a plain multivariate
Gaussian prior with a ﬁxed mean and covariance. More work
can be done to investigate the effects of varying the distribution
parameters and the type of distribution used and optimize
the performance improvement on the unsupervised anomaly
detection task. In addition, the proposed method can be applied
for some large-scale anomaly detection, e.g. network-wide
anomalous trafﬁc detection .
ACKNOWLEDGMENT
This work was supported by both ST Electronics and the
National Research Foundation (NRF), Prime Minister’s Ofﬁce,
Singapore under Corporate Laboratory @ University Scheme
(Programme Title: STEE Infosec - SUTD Corporate Laboratory). The authors would also like to thank the anonymous
reviewers for their valuable comments and helpful suggestions.