Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7929–7942
July 5 - 10, 2020. c⃝2020 Association for Computational Linguistics
Logical Natural Language Generation from Open-Domain Tables
Wenhu Chen1, Jianshu Chen2, Yu Su3, Zhiyu Chen1 and William Yang Wang1
University of California, Santa Barbara, CA, USA1
Tencent AI Lab, Bellevue, WA, USA2
The Ohio State University, Columbus, Ohio, USA3
{wenhuchen, zhiyuchen, william}@cs.ucsb.edu
 , 
Neural natural language generation (NLG)
remarkable
progress in ﬂuency and coherence. However,
existing studies on neural NLG are primarily
focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language. In
this paper, we suggest a new NLG task where
a model is tasked with generating natural language statements that can be logically entailed
by the facts in an open-domain semi-structured
table. To facilitate the study of the proposed
logical NLG problem, we use the existing Tab-
Fact dataset featured with
a wide range of logical/symbolic inferences as
our testbed, and propose new automatic metrics to evaluate the ﬁdelity of generation models w.r.t. logical inference. The new task poses
challenges to the existing monotonic generation frameworks due to the mismatch between
sequence order and logical order. In our experiments, we comprehensively survey different
generation architectures (LSTM, Transformer,
Pre-Trained LM) trained with different algorithms (RL, Adversarial Training, Coarse-to-
Fine) on the dataset and made following observations: 1) Pre-Trained LM can signiﬁcantly
boost both the ﬂuency and logical ﬁdelity metrics, 2) RL and Adversarial Training are trading ﬂuency for ﬁdelity, 3) Coarse-to-Fine generation can help partially alleviate the ﬁdelity
issue while maintaining high language ﬂuency.
The code and data are available at https:
//github.com/wenhuchen/LogicNLG.
Introduction
Neural network models, especially the recent wave
of massive models like BERT 
and GPT-2 , have shown the
ability to generate natural language text at an astonishing level of ﬂuency and coherence. For the
generated text to fulﬁll its purpose, however, a crit-
Gold Medal
Silver Medal
Bronze Medal
Ice Hockey
Roller Skating
Sentence: Canada obtained 1 more gold medal than Mexico.
Sentence: Canada obtained the most gold medals in the game.
Medal Table from Tournament
Sentence: Canada has got 3 gold medals in the tournament.
Sentence: Mexico got 3 silver medals and 1 bronze medal.
Surface-level Generation
Logical Natural Language Generation
Figure 1: Table-to-text generation examples with and
without implicit logical inference.
Logical NLG requires a generation model to generate natural language
statements that can be logically entailed by the facts in
the table instead of simply restating certain superﬁcial
facts in natural language.
ical property that is necessary but often overlooked
is ﬁdelity, i.e., what is generated should be faithful to the underlying data, knowledge, or meaning
representation. A line of recent work has started
to address the surface-level ﬁdelity issue of natural language generation (NLG) by encouraging the
model to learn to reuse the verbatim of certain inputs through copy mechanism ,
structured attention , or planning
and selection/entity modeling . While shown to be effective, most such
methods so far are primarily focused on surfacelevel realization and simply restate the facts in the
underlying data (Figure 1).
However, humans have the ability to generalize beyond superﬁcial facts (e.g., “Canada has got
3 gold medals.”) by inferring and communicating with new statements that can be entailed from
these facts (e.g., “Canada obtained the most gold
medals.”). We believe it is important for NLG models to be able to generalize beyond the superﬁcla
facts given to them as well. Therefore, we propose
a new task, logical NLG, where a model is tasked
Colombia has
more silver medals than Canada.[Logic: Diff]
[Logic: Total]
Figure 2: When making the decision at the third step,
the model needs to foresee the future tokens to ensure
logical consistency. There is no back-tracking once the
model makes a wrong decision like “5”.
with generating natural language statements that
can be logically entailed by the given data (i.e., the
premises). The new task requires a model to jointly
reason and generate sentences that are consistent
both linguistically and logically. Since there are a
variety of reasoning/inference tasks such as natural language inference and
commonsense reasoning , to
avoid confusion, this paper is speciﬁcally focused
on inferences involving symbolic operations over
the given table .
To empower research in this direction, we collect a new corpus LOGICNLG based on the existing TabFact , which brings two
major renovations to the existing NLG paradigm:
1) the text involves diversiﬁed types of logical inferences including math operations like
max/min/sum/add, comparison operations like
same/different, and counting operations like total/only. A more detailed description of logical
inference is listed in the Appendix. 2) while existing datasets are often restricted to a speciﬁc domain such as weather , restaurant , NBA , etc, LOGICNLG uses open-domain tables
without prior knowledge about their schema. As
such, existing methods based on surface-level copying becomes insufﬁcient, so are the
existing ﬁdelity evaluation based on the surfacelevel information extraction , which
extracts surface triples in a certain pre-deﬁned form
(i.e. subj-pred-obj, n-gram) and compare them with
the surface content given in the knowledge.
Most neural generation models follow a
monotonic generation schema from left to right
with the current prediction only depending on
the preceding words. Logical NLG poses unique
challenges to the traditional generation scheme
due to the mismatch between sequence order
and logical order. As illustrated in Figure 2, the
word “2” is derived from the logical inference of
‘diff(Silver medal of Colombia, Silver medal of
Canada)) →2.’ In other words, the logical order
of word “2” should be after “more”, “silver”, and
“Canada”, while the sequence order of “2” is before
those words.
Since the monotonic generation
scheme is purely based on sequence order while
agnostic to logical order, existing NLG models
struggle to maintain the ﬁdelity as they cannot
model the logical dependency on future tokens. To
alleviate such an order mismatch, an NLG model
must have the capability to plan ahead for the next
few steps before generation. In this context, we
believe LOGICNLG to be an important testbed to
study such a planing/inference ability in generation
models . In
this paper, we further propose a non-monotonic
coarse-to-ﬁne generation model and show that it is
able to alleviate the order mismatch problem and
achieve better performance. The contribution of
this work is three-fold:
i) We propose a new research problem of logical
natural language generation, and provide novel
metrics to approximately evaluate the logical
ﬁdelity of generation models.
ii) We justify the mismatch problem between
sequence order and logical order of the traditional
monotonic generation scheme in logical NLG.
iii) We conduct comprehensive experiments
with state-of-the-art neural generation models
under both automatic and human evaluation, which
demonstrates the challenges and opportunities for
future research on logic NLG.
Dataset and Problem Deﬁnition
Existing NLG datasets are mainly composed of surface-level description over the given records.
Though RO-
TOWIRE involves sporadic inference in the long document, and the inference is restricted to domain-speciﬁc knowledge
(e.g. double-double, smash, triple-double and other
NBA-related terms).
Hence, we need a better
testbed for studying the proposed problem.
Statistics
We construct a dataset based on Tab-
Fact , which is a table-based factchecking dataset with rich logical inferences in the
annotated statements. Speciﬁcally, we took their
positive statements (the sentences which are en-
Vocab/Sent
WEATHERGOV
Table 1: Comparison of LOGICNLG against existing NLG datasets in different aspects.
Gold Medal
Silver Medal
Canada obtained 3 gold medals during the tournament.
Canada obtained 1 more gold medal than Mexico.
Canada obtained the most gold medals.
Colombia has 4 medals in total.
(Canada,Gold,3)
Fail to extract triple
Fail to extract triple
(Colombia, Medal, 4)
Verify: Supported
Verify: Refuted
Figure 3: Evaluation of surface-level generation vs. logical natural language generation. It sufﬁces to use IE-based
evaluation to verify surface-level generation, but it causes either
“empty triple” or “false negative” problems to verify logical NLG.
tailed by the knowledge in the table) collected from
“complex channel” (required to annotate sentences
with logical inference) as our target text. To prevent
confusion with the original dataset, we name this
table-to-text dataset as LOGICNLG, which contains 28,450 training, 4,260 validation and 4,305
test examples based on 7,392 open-domain tables
crawled from Wikipedia. Each table has 5 different
examples covering diverse types of logical inference. More detailed statistics and comparisons are
listed in Table 1. LOGICNLG is distinguished from
the existing datasets due to:
i) It involves very rich logical inference, every
annotated sentence involves certain types of inference with minimum domain-speciﬁc knowledge.
The open-domain characteristic simulates a realistic setting, where we cannot enumerate the possible
inference based on the scheme, which poses great
challenges to the model’s generalization capability.
ii) It is mainly composed of short sentences with
an average length of 11 and a simple syntactic structure, which isolates from other linguistic complexity to focus on the problem of logical inference.
The dataset contains tables with open schema
crawled from diversiﬁed domains Figure 4. The
major categories are sports, politics, and entertainment. The schema diversity of the tables make
the rule-based system infeasible to apply. Besides,
most of the tables have very rich numeral records,
which provide a great testbed for logical inference.
Problem Deﬁnition
Here, we formally deﬁne
our proposed table-to-text generation task. The
input is a table T with its title denoted as a natural
language sequence W. The table T = {Ti,j|i ≤
RT , j ≤CT } has RT rows and CT columns with
Domain Distribution of Tables
Team/Player (Sports)
Compeition (Sports)
Entertaiment
Figure 4: The domain distribution of LOGICNLG.
the Tij being the content in the (i, j)-th cell. Tij
could be a word, a number, a phrase or even a natural language sentence. The annotated statement is a
sentence Y = y1, y2, · · · , yn, we aim to train a neural generation model p(Y |T) to generate statement
ˆY which are both ﬂuent and logically (numerically)
supported by the given table T.
Automatic Evaluation
In this section, we discuss the evaluation of our proposed NLG task. The ﬂuency evaluation is simply
based on the standard metrics like Perplexity and BLEU-1,2,3 based on NLTK . The most challenging problem is to evaluate the logical ﬁdelity
of the generated sentences, which is also the core
problem of our paper. The existing IE-based extractive evaluation leads to
two issues as shown in Figure 3: 1) Empty Extraction: the sentence can not be formulated as (subject,
predicate, object) structure, thus the IE system fail
to extract triples for veriﬁcation. 2) False Negative:
the sentence is a logical composition (instead of surface form) of the fact from the table, the IE system
cannot match it against the table. For these reasons,
we test two approximate automatic metrics:
Sentence: Canada obtained 1 more gold medal than Mexico
Eq(Hop(Filter(Nation==Canada), Gold Medal)… 1)
Parsing [Link->Search]
Sentence: Canada obtained 1 more gold medal than Mexico
Table: In the first row …. In
the second row, ….
Orig: Canada obtained 1 more gold medal than Mexico
Adv: Canada obtained 1 less gold medal than Mexico
Figure 5: The parsing-based and adversarial evaluation to measure model’s correctness in logical reasoning.
Parsing-based Evaluation
We ﬁrst propose a
model-based evaluation method, which aims to directly extract the meaning representation from the
generated sentence and execute it against the table
to verify its correctness. Our evaluation is based on
weakly-supervised semantic parsing , the basic idea is to ﬁrst link entities
and predicates in the sentence, and then use linked
entities to perform a breadth-ﬁrst search to synthesize potential logical forms, ﬁnally, a scorer is used
to re-rank these logical forms and ﬁlter out spurious
ones. The logical form returns a binary value of
True to indicate whether its logic is supported by
the knowledge. The basic idea is shown in the upper part of Figure 5, the implementation details are
in the Appendix. We pre-train the semantic parser
fγ on the training set (T, Y ) ∈Dtrain with weakly
supervised algorithm, at test time, we use it to parse
a sentence Y into a set of logical forms, which is
re-ranked to obtain the highest logical form Pbest.
We compute the ratio of Pbest returning “true” on
Dtest to approximate model’s ﬁdelity.
(T, ˆY )∈Dtest
I(Pbest →True|Pbest = fγ( ˆY ))
where I is the indicator function.
NLI-based Evaluation
We then propose another model-based evaluation method to complement the parsing-based evaluation (which is sensitive to semantic variation), the basic idea follows to evaluate the entailment score between the table and the generated sentence. The NLI model is based on Table-
BERT , which linearizes the table into textual form and uses it as the evidence for
natural language inference. The model is trained
with TabFact dataset containing both positive/negative samples. During the
evaluation, we use this NLI model to predict the
entailment relationship based on the likelihood of
pNLI(Y |T). Finally, we compute the ratio of “entailed” to approximate model’s ﬁdelity:
(T, ˆY )∈Dtest
I(pNLI(Y |T) > 0.5)
where I is the indicator function.
Adversarial
Evaluation
Adversarial
evaluation is used to study the generation model’s robustness in logical reasoning.
Speciﬁcally, we
hire human workers from Amazon Mechanical
Turk1 to annotate adversarial examples for the
test/validation set by simply changing minimum
words to revert the logic of the sentence. Such
adversarial examples preserve linguistic components like length and style except the logic-related
words to speciﬁcally disentangle the generation
model’s reasoning skill. As drawn in the lower
part of Figure 5, the original sentence modiﬁes its
word “more” into “less” as an adversarial example.
There are two principles the workers need to follow to make their jobs accepted: 1) the modiﬁed
words/phrases should be roughly equally frequent
to balance the language prior, for example, the number “1” is better swapped with “2,3” rather than
“9999” which rarely appears in the corpus. 2) the
perturbation should be diverse enough to cover different aspects of logical reasoning skills. We use
the generation model p(Y |T; β) to score the original sentence Y and the adversarial sentence Yadv.
If the conﬁdence of the original example is higher
than its adversarial counterpart, we count it as a
successful defense, otherwise as a failed defense.
We use the success rate to approximate model’s
logical reasoning capability.
(T,Y,Yadv)∈Dtest[I(p(Y |T) > p(Yadv|T))]
where I is the indicator function.
1 
Discussion
Both types of metrics have pros and
cons, the SP-Acc and NLI-Acc are two metrics
unbiased as it measures the peak samples in the
model’s likelihood, however, both metrics are
based on imperfect models and thus their evaluation scores are inaccurate. SP-Acc is more sensitive to number/calculation errors, and NLI-Acc
is more sensitive to semantic errors, therefore, we
report both of them to help increase the metrics’
robustness. In contrast, the adversarial evaluation
score is accurate in terms of reﬂecting the model’s
reasoning capability on the given samples. However, as the provided samples might not lie in the
high-conﬁdence area of the model’s distribution, it
is biased in reﬂecting the model’s general reasoning capability. Though these ﬁdelity metric models
are prone to errors, in section 6, we show their consistency with human judgment, which reveals their
potential to assist human evaluation.
In this section, we design comprehensive baseline
models to perform logical NLG. Speciﬁcally, we
consider the following two cases: non-pretrained
models (LSTM/Transformer) with copy mechanism and pre-trained models (GPT-2 and BERT)
with sub-word unit. We train these models with
three different algorithms: Maximum Likelihood,
Adversarial Training, and Reinforcement Learning.
Non-pretrained Models
Here we mainly consider two table encoding methods, namely ﬁeld-infusing and ﬁeld-gating. These
two methods differ in their strategies to coalesce the
ﬁeld information into cells. After the table is represented as a sequence of vectors, a decoder based
on LSTM or
Transformer is applied to
generate text token by token. The two methods are
depicted in the upper part of Figure 6:
Field-Infusing
This strategy is inspired by Lebret et al. . We ﬁrst use an LSTM to encode the table
ﬁeld text word by word and then use the last output zi as ﬁeld representation. This representation
is concatenated with the embedding of row index
#j and word embedding at each cell to obtain a
position-aware cell embedding ek for each word
inside the cell. We stack transformers layers on top
of the cell embedding to obtain the table representation as hi ∈RD with D as the dimension.
Field-Gating
This strategy is inspired by by Liu
et al. . Like the previous strategy, we ﬁrst
use an LSTM 
to obtain ﬁeld representation zi. The ﬁeld representation is concatenated with ending distance information as the input to an additional ﬁeld gate built
inside the LSTM as suggested in Liu et al. ,
such a ﬁeld gate is used to control whether the current cell is already encoded. Such a mechanism
can help LSTM to identify the boundary between
different cells to grasp local information.
Pre-trained Models
To further enhance the ﬂuency and resolve the
out-of-vocabulary problem, we use pre-trained language models and ﬁnetune them on LOGICNLG.
Speciﬁcally, we consider two models based on
GPT-2 and BERT , respectively, and name them as GPT-
TableGen and BERT-TableGen.
Table Linearization
We follow previous work
on linearizing knowledge base as natural language to
propose “table linearization”, which uses template to ﬂatten the table T as a document PT =
w1, · · · , w|T| fed into pre-trained language models
to generate statement Y , where we use wi to denote the i-th word in the generated paragraph PT
and |T| to denote the length of the paragraph (the
word wi is either a table entry or a functional word
in the template). As depicted in the left bottom
part of Figure 6, the original table T is transformed
into a paragraph by horizontally scanning each cell
T11 →T1,CT →TRT ,CT in the table.
GPT-TabGen
we directly feed the paragraph PT
as the input to the pre-trained GPT-2 model and
generate the output sentence Y . We ﬁnetune the
model on LOGICNLG by maximizing the likelihood of p(Y |PT ; β), with β denoting the parameters of GPT-2 model .
BERT-TabGen
1) we encode the linearized paragraph PT using the pre-trained BERT model into
the source representation h1, · · · , h|T|. 2) at the
i-th time step, we replace all the words in the
groundtruth statement Y after i-th time step by
<MASK> token and use BERT to encode the partially masked Y i as gi
1, · · · , gi
n. 3) we use an attention layer fθ to obtain the output hidden states
1, · · · , ˆgi
n, where ˆgi
i is used to predict the word ˆyi.
We jointly optimize β of BERT and θ to maximize
Field-Infusing Encoder
Gold, 0 Medal, 0
Pre-trained GPT-2
Columbia has 4 medals in total.
Pre-trained BERT
Columbia has [MASK] [MASK] [MASK].
Pre-trained BERT
Multi-Layered Transformer
Given the table of “Tournament Medal Table”. In the 1st row, the
nation is Canada, Gold Medal is 1, Silver Medal is 1, Sports is
Ice Hockey. In the 2nd row, the nation is Mexico, Gold Medal is
2, Silver Medal 3, Sports is Baseball, … Roller Skating.
Table Templatization 𝑃+
GPT-TabGen
BERT-TabGen
Non-Pretrained Model
Pretrained Model
Decoder (LSTM/Transformer)
Field-Gated Encoder
Decoder (LSTM/Transformer)
Field words
Figure 6: The Non-pretrained and Pre-trained generation models, the detailed table is shown in Figure 1.
the likelihood of generating text Y conditioned on
the table and the masked partial sentence. As BERT
is a bidirectional model, we need to re-encode the
target sentence at each step to get gi
1:n. Therefore,
the generation is ﬁnished with n passes.
Except for the standard maximum likelihood training, we also use the following training algorithms:
Adversarial Regularization
To encourage the
model to ground on the table rather than relying
on artiﬁcial language priors , we use an adversarial regularization to enhance the maximum likelihood training. Speciﬁcally, we ﬁrst perform entity resolution to locate
all the numbers, count, entities in the sentence and
then randomly replace them with entities or numbers appearing in the table T. These perturbed
samples Yadv are used as adversarial examples to
regularize the model’s behavior. Formally, we optimize β to maximize the objective:
log p(Y |T; β) −λ log p(Yadv|T; β)
where λ is the controlling hyper-parameter.
Reinforcement Learning
The maximum likelihood training is a ﬂuency-driven objective, which
is inconsistent with the goal of logical consistency.
To bridge the gap, we view the generation problem from the reinforcement learning perspective
to optimize the long-term ﬁdelity.
We use the
trained semantic parser to assign reward to the policy p(yi|y1:i−1; β). At i-th step, the generator will
sample different actions yi and roll-out from i + 1th step to produce a full sequence starting from yi
using greedy search. The full sentence receives a
binary score r(Y, T) from the semantic parser as
reward. Formally, we optimize the objective:
yi∼p(yi|y1:i−1)[
yi+1:n[r(y1:n, T)]] log p(yi|y1:i−1; β)
where we only use one trajectory to approximate
the inner roll-out expectation for efﬁciency.
Coarse-to-Fine Generation
As discussed before, the baseline models follow
the monotonic generation scheme and suffer from
the mismatch between sequence order and logical
order (Figure 2). In this section, we propose an
imperfect remedy for such a situation based on the
coarse-to-ﬁne generation paradigm.
Before plunging into technical details, it is helpful to ﬁrst realize the resemblance between logical NLG and semantic parsing . Compared to traditional NLG tasks like machine translation and summarization, logical NLG
is closer to semantic parsing in the sense that a
model may make catastrophic errors that are impossible to be corrected at later steps (Figure 2).
Therefore, we take inspiration from semantic parsing models that have
proven effective in mitigating such errors and propose a coarse-to-ﬁne generation scheme. We break
down generation into two phases. In the ﬁrst phase,
Canada obtained 1 more gold medal than Mexico.
more [ENT] than
Figure 7: Coarse-to-ﬁne generation scheme: ﬁrst generates a template, and then realize the surface form. It exposes
more context to the surface realization model for better capturing logical dependency.
the model only generates a template which determines the global logical structure, while in the second phase the model generates the ﬁnal, grounded
sentence conditioned on the template generated in
the ﬁrst phase. As depicted in Figure 7, we use
the entity linker (Section 3) to identify the entities and numbers in the original sentence Y and
replace them with placeholder “[ENT]”, which we
call as the template YT . During the generation of
GPT-TabGen, instead of directly predicting the ﬁnal sentence Y , we ﬁrst predict the template YT
and then Y . The process is simply realized by maximizing the overall likelihood of p( ˜Y |T; β), where
˜Y = [YT ; [SEP]; Y ].
Unlike template-based or delexicalized generation ,
which uses rigid slot ﬁlling prone to grammatic
errors, our ﬁne-grained generation has the ﬂexibility to modify the surface form of non-slot
words, which alleviates the linguistic coherence
problem .
By decoupling sentence structure generation
and entity grounding, our proposed coarse-to-ﬁne
scheme could partially alleviate the mismatch problem. For example, the generation of “Canada” is
now aware of “more than” in the latter part of the
sentence, which exposes the model to more context
than standard monotonic models to help make logically consistent decisions though the dependency
on the “1” and “Mexico” is still not captured. The
proposed two-step generation could be viewed as
the ﬁrst step towards a fully non-monotonic generation model to solve such mismatch problem.
Experiments
In this section, we explain the experimental details and then comprehensively report the automatic
evaluation of different generation models and training algorithms. Finally, we will conduct detailed
human evaluation and error analysis.
Experiment Setup
For the non-pretrained models, we ﬁx the hidden
size of both LSTM and transformer to be 256, the
transformer is 3-layered with 4 heads, while LSTM
is also 3-layered. We use Adam optimizer with a learning rate of 2e-4 to jointly
optimize the parameters and keep the model with
the best perplexity on the validation set. During
test time, we use a greedy search to generate text
and calculate the BLEU-1,2,3 scores with the 5
references from the table. For the pre-trained models, we base our implementation on Huggingface’s
Transformer for both BERT and GPT-2 
with subword unit vocabulary of 30K. During linearization, we found that using the whole table
compromises the performance greatly, partly due
to 1) over-length issue with pre-trained LM, 2) too
much irrelevant information input. Therefore, we
propose to use partial table as input, speciﬁcally,
we run entity linking over the sentences to detect
the linked columns of the table and only linearize
the partial table as input PT .
optimizer with a learning
rate of 1e-6. In both adversarial training and reinforcement learning algorithms, we add maximum
likelihood objective to stabilize the training, we
select the appropriate balancing factor based on
the validation Adv-Acc socre. For coarse-to-ﬁne
training, we ﬁrst warm up the model to generate
the template sequence and then ﬁnetune it on the
concatenated full sequence. Model selection is
based on the bleu-3 score on validation split.
Field-Gating + LSTM
Field-Gating + Trans
Field-Infusing + LSTM
Field-Infusing + Trans
BERT-TabGen (sm)
GPT-TabGen (sm)
GPT-TabGen (sm)
GPT-TabGen (sm)
GPT-Coarse-to-Fine (sm)
BERT-TabGen (lg)
GPT-TabGen (med)
GPT-TabGen (med)
GPT-TabGen (med)
GPT-Coarse-to-Fine (med)
Table 2: The experimental results of different models on the test split of LOGICNLG, where we split the table into
non-pretrained LSTM/Transformer, small pre-trained LM (sm) and medium/large pre-trained LM (med/lg).
Experimental Results
We ﬁrst perform an automatic evaluation to approximately measure the performance of different models and then conduct an in-depth human evaluation
to have a better understanding.
Automatic Evaluation:
The experimental results are summarized in Table 2, where we comprehensively survey different architectures and training algorithms. For the non-pretrained models,
we observe that Transformer is slightly better than
LSTM and two different table encoding strategies
achieve similar results. In contrast, pre-trained
models are much better at lowering the perplexity,
besides the generated sentences signiﬁcantly outperform the non-pretrained models in terms of both
ﬂuency and ﬁdelity score with GPT-TabGen and
BERT-TabGen achieving similar performance. As
the BERT-TabGen runs much slower due to multiple passes of decoding, we favor GPT-TabGen
in the following experiments. With the adversarial regularization and reinforcement training, the
model can only improve the optimized ﬁdelity metric, with the ﬂuency scores dropping signiﬁcantly.
Such phenomena conﬁrm our assumption about the
caveats of the monotonic generation paradigm. For
the proposed coarse-to-ﬁne generation scheme, as
the “[ENT]” tokens are replaced by entity names,
which normally contain a phrase like “Feb 2nd”.
Such n-gram phrase substitution preserves the completeness of entity names and thus leads to higher
2/3/4-gram matches, which translates to higher
BLEU-3 and lower BLEU-1 in Table 2. The proposed coarse-to-ﬁne generation can yield reasonable improvement over NLI-Acc and Adv-Acc,
which demonstrates its advantages of in capturing
logical dependency.
Human Evaluation
To further investigate the
quality of the generated text, we propose to perform human evaluation. Speciﬁcally, we sample
200 sentences from different models and distribute
them independently to human experts (graduate
students from the computer science department) to
verify their quality. Speciﬁcally, the quality measure is categorized into categories: 1) non-sense:
the sentence does not make much sense, which
is mainly due to disﬂuency or repetition problem.
2) wrong: a ﬂuent sentence with wrong logic. 3)
partial-correct: the sentence contains more than one
fact, at least one of them is correct 4) correct: the
high-quality in both ﬂuency and logic correctness.
We demonstrate the results in Figure 8, from which
we observe that pre-training signiﬁcantly decreases
the non-sense proportion. However, the RL and
Adv-Reg both harm the ﬂuency and lead to more
non-sense sentences. In contrast, the coarse-to-ﬁne
model can maintain the non-sense proportion while
signiﬁcantly increasing correct/partial-correct sentences. From human evaluation, even the best performing model can get slightly over 20% of its
prediction logically correct, which reﬂects the challenges of LOGICNLG for existing paradigm.
Evaluation Metrics
We here analyze the effectiveness of the deﬁned automatic evaluation metrics for ﬁdelity evaluation. For the Parsing-based
evaluation and NLI-based evaluation, we use the
adversarial set (containing positive/negative sample pairs) to evaluate their consistency with human
judges. Parsing-based model only achieves an ac-
curacy of 60%, while NLI-based model achieves
a higher accuracy of 65%. It indicates that the
ﬁdelity measurement model is itself a very challenging problem and the existing models are still in
a premature stage. Therefore, the exact number of
SP-Acc or NLI-Acc cannot reliably reﬂect the exact proportion of sentences logically entailed by the
table. However, we still believe they are informative for model development based on the following
reasons: 1) the automatic ﬁdelity scores are quite
stable, not sensitive to random initialization or different conﬁgurations, 2) when comparing different
models (Transformer vs. GPT-2 vs. RL/Adv-Reg
vs. Coarse-to-Fine), the trends of different automatic scores are consistent with human evaluation,
which indicates its potential in assisting the development of new models.
Fine-grained Analysis
To better understand the
generation model’s reasoning capability in regarding different logical operations, we pick the most
frequent 9 operations (deﬁnition in the Appendix)
and analyze the best model’s capability in expressing these different logic. We demonstrate our human evaluation in Figure 8 to make the following
inspections: 1) the model performs best in justifying the order of different entities (before/after) and
relating two entities (both/neither/comparison). 2)
the model performs reasonably well at superlative
and count operation. 3) the generation model performs much worse in operations like “only, unique”.
4) the model is not able to perform mathematical
aggregation like average, sum, etc. Overall, the
string-based operations are easier than numericbased operations, how to infuse the numeric knowledge is an open research question to move forward.
Related Work
Natural Language Generation
Natural language generation is a long-standing problem , which involves generating text from records
or data. Recently, many neural-based generation
models have been proposed 
to achieve impressive performance on the existing
datasets since the annotated text are
mostly surface-level annotation without logical inference. Unlike them, LOGICNLG has rich inference, which poses great challenges to existing
models and evaluations.
Non-monotonic Generation
There have been
attempts recently to study the problem of nonmonotonic text generation, which aims to teach the
generation model to learn the generation order without external supervision .
These models have shown to learn rational generation order to approach similar performance as the
left-to-right case. These approaches are useful at
capturing more sophisticated dependency within
the sentence, which provides a plausible direction
to pursue in LOGICNLG.
Factualness Evaluation
Fidelity is an important
research topic in generation, In ROTOWIRE and MSCOCO ,
IE-based extractive evaluation are adopted for surfacelevel matching to replace costly human evaluation. In abstractive summarization, Goodrich et al.
 proposes NER + Relation Classiﬁcation
method to investigate ﬁdelity in generated summarization while Kry´sci´nski et al. proposes
to use NLI models to understand the entailment
between generated text with the given document.
These evaluations are beyond surface-level to study
more sophisticated linguistic phenomena like paraphrasing, compression, entailment, inclusion, etc,
which are common in summarization tasks.
Conclusion
In this paper, we propose logical NLG to study
the logical inference problem in generation. We
conduct comprehensive experiments to show the
existing NLG models are restricted by its monotonic nature and conclude this to be a proper nextstep problem to study NLG systems. There are
still some unsolved problems for Logical NLG, e.g.
how to improve the quality of automatic metrics
to better help human automatically judge models’
performances. To promote the research in this direction, we host a LogicNLG challenge2 to help
better benchmark the current progress.
Acknowledgement
The authors would like to thank the anonymous
reviewers for their thoughtful comments.
2 
competitions/24527