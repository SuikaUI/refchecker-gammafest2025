Dream Distillation: A Data-Independent Model Compression Framework
Kartikeya Bhardwaj 1 Naveen Suda 2 Radu Marculescu 1
Model compression is eminently suited for deploying deep learning on IoT-devices. However,
existing model compression techniques rely on
access to the original or some alternate dataset.
In this paper, we address the model compression problem when no real data is available, e.g.,
when data is private. To this end, we propose
Dream Distillation, a data-independent model
compression framework. Our experiments show
that Dream Distillation can achieve 88.5% accuracy on the CIFAR-10 test set without actually
training on the original data!
1. Introduction
Complex deep neural networks with millions of parameters
have achieved breakthrough results for many vision, and
speech recognition applications. In the IoT-era, however, the
edge devices are heavily hardware-constrained. Therefore,
model compression of deep networks has now emerged as an
important problem. Towards this end, many state-of-the-art
model compression techniques such as pruning , quantization and Knowledge Distillation (KD) have been proposed. Pruning aims at
removing redundant or useless weights from deep networks,
while quantization reduces the number of bits used to represent weights and activations. On the other hand, KD trains
a signiﬁcantly smaller student model to mimic the outputs
of a large pretrained teacher model.
The existing model compression techniques above rely on
access to the original training data or some unlabeled dataset.
Hence, most model compression research implicitly assumes access to the original dataset. However, for many
applications, the original data may not be available due to
1Department of Electrical and Computer Engineering, Carnegie
Mellon University, Pittsburgh, PA, USA 2Arm Inc., San Jose,
CA, USA. Correspondence to:
Kartikeya Bhardwaj < >.
Presented at the ICML 2019 Joint Workshop on On-Device Machine Learning & Compact Deep Neural Network Representations
(ODML-CDNNR), Long Beach, California, 2019. Copyright 2019
by the author(s).
privacy or regulatory reasons (e.g., private medical images,
speech data, etc.). Consequently, the industries deploying large deep learning models at the edge must compress
them without access to any original, private, or alternate
datasets1 . Therefore, in this paper, we
address the following key question: How can we perform
model compression when the original or unlabeled data for
an application is not available? We call this problem as
data-independent model compression.
To answer this question, we propose a new framework called
Dream Distillation. Our framework uses ideas from the ﬁeld
of deep network interpretability to distill the
relevant knowledge from the teacher to the student, in absence of access to the original training data. Speciﬁcally,
our approach consists of two steps: (i) We ﬁrst exploit a
small amount of metadata and the pretrained teacher model
to generate a dataset of synthetic images, and (ii) We then
use these synthetic images for KD. To this end, our key goal
is to generate synthetic data while preserving the features
from the original dataset such that the teacher can transfer
the knowledge about these features to the student. This
effective transfer of knowledge via synthetic data can make
the student model learn characteristics about original classi-
ﬁcation problem without actually training on any real data!
By allowing users to deploy a model on IoT-devices without
access to the private third-party datasets, data-independent
model compression techniques can truly accelerate the adoption of AI on edge devices.
2. Background and Related Work
We ﬁrst describe KD and feature visualization, which are
necessary for Dream Distillation. Then, we discuss the
related work on data-independent model compression and
show how our approach differs from existing works.
2.1. Knowledge Distillation
KD refers to the teacher-student paradigm, where the teacher
model is a large deep network we want to compress . In KD, we
train a signiﬁcantly smaller student neural network to mimic
this large teacher model (see Fig. 1(a)). KD has also been
1Collecting alternate datasets for model compression may not
always be possible, or can be very expensive/time-consuming and,
hence, infeasible.
 
Dream Distillation
shown to work with unlabeled datasets . Of note, since the term “model compression” usually
refers to pruning and quantization, we assume KD to be a
part of model compression, as it also leads to signiﬁcantly
compressed models.
2.2. Feature Visualization
Feature visualization domain aims to visualize and understand which patterns activate various neurons in deep networks . Towards this, tools such as Deep-
Dream and Tensorﬂow Lucid can generate an image that maximizes a given objective. For example, Tensorﬂow Lucid
can be used to generate an image that maximally activates a
given hidden unit (say, a neuron or a channel). These generated synthetic images are called as the Dreams of a neural
network. Since our work is inspired by KD and feature
visualization, we call our approach Dream Distillation.
Of note, in order to use feature visualization for dataindependent model compression, we assume access to some
small amount of metadata which is used to generate the
synthetic images for KD.
2.3. Data-Independent Model Compression
Despite its signiﬁcance, the literature on model compression
in absence of real data is very sparse. A relevant prior work
is where the authors propose a Data-
Free KD (DFKD) framework. However, there are major
differences between DFKD and the present work:
1. DFKD requires signiﬁcantly more metadata than our
Speciﬁcally, argue
that using metadata from only the ﬁnal layer underconstrains the image generation problem, and results
in very poor student accuracy. Consequently, DFKD
assumes access to metadata at all layers. In contrast,
Dream Distillation assumes that metadata is available
only at one layer of the teacher network. Hence, in this
paper, we precisely demonstrate that metadata from a
single layer is sufﬁcient to achieve high student accuracy, something that DFKD failed to accomplish.
2. When using metadata from only one layer, DFKD
dataset ; this means the accuracy of DFKD will be even lower for signiﬁcantly
more complex, natural image classiﬁcation datasets
like CIFAR-10. On the other hand, we demonstrate
81-88% accuracy on CIFAR-10 dataset without
training on any real data.
3. DFKD also proposes a spectral method-based metadata for synthetic image generation. However, both
spectral methods and all-layer metadata can be computationally very expensive and do not scale for larger
networks. Compared to these, we follow a clusteringbased approach which helps generate diverse images
while using signiﬁcantly less computation.
Finally, focus on data-free ﬁnetuning for pruning, and show the effectiveness of their approach
for fully-connected layers. In comparison, our work is much
more general, as we do not focus on just the ﬁnetuning of
a compressed model, but rather on training a compressed
student model from scratch.
3. Proposed Dream Distillation
We propose Dream Distillation to address the following research question: In absence of the original training dataset
(or any alternate unlabeled datasets), how can we perform
model compression without compromising on accuracy?
Speciﬁcally, for KD with teacher model trained on CIFAR-
10, datasets such as CIFAR-100 and tinyImagenet have
been shown to be effective alternatives for distilling relevant
knowledge . However, since alternate
datasets may not always be available, our focus here is to
generate a synthetic dataset which can be just as effective at
distilling knowledge as these alternate data. Hence, we assume that the alternate/original data is not available; rather,
a small amount of metadata is given for model compression.
3.1. Metadata
We will use CIFAR-10 dataset throughout this paper. To
generate our metadata, we start with the activation vectors
generated by passing 10% real CIFAR-10 images through
the given pretrained teacher model. The activation vectors
are simply the output of average-pool layer of the teacher
model (i.e., average output of ﬁnal convolution layer; see
Fig. 1(a)). Then, we cluster these vectors via k-means
clustering. Finally, we use the cluster-centroids and the
orthogonal principal components (PCs) of clusters as our
metadata. Fig. 1(b) illustrates our metadata for the airplane
class for CIFAR-10 dataset and 2-D visualization of activation vector clusters. By deﬁnition, centroids refer to mean
activations of clusters. Hence, using centroids reduces the
privacy-concerns since we do not use activations (or any
other identifying information) directly from real images.
For WRN40-4 teacher model, our metadata merely amounts
to 0.58MB which is about 100× smaller than the size of
even 10% real CIFAR-10 images (around 58MB). We next
use this metadata (centroids and PCs) and teacher model to
generate synthetic images. Of note, image generation techniques such as Generative Adversarial Networks (GANs)
cannot be used for our problem since GANs also rely on
availability of real data .
3.2. Dream Generation and Distillation
Let ck be the centroid for cluster k, and pk
j , j ∈{1, . . . , m}
denote its m PCs. We ﬁrst create objectives from the metadata, and then optimize them to generate images. Specifically, we add a small noise to the centroids in the di-
Dream Distillation
Figure 1. (a) Knowledge Distillation: A signiﬁcantly smaller student model mimics the outputs of a large teacher network. (b) Metadata
used in Dream Distillation.
rection of PCs to generate new target activations: ti =
j , i ∈{1, . . . , n}. Here, n is the number of
images to be generated, and ϵj is Gaussian noise for jth PC.
To compute ϵj, explained variance of jth PC is used as the
variance in the Gaussian noise. Adding noise proportional
to the explained variance of corresponding PCs makes our
target activations mimic the real activations.
Therefore, by adding small noise to mean activations (i.e.,
cluster-centroids), the target activations emulate the behavior of real samples at teacher’s average-pool layer. Then,
to generate the images, we must ﬁnd an image Xi whose
average-pool activations are as close as possible to ti. Therefore, we generate the synthetic images Xi as follows:
Xi ||g(Xi) −ti||2
2 i ∈{1, . . . , n}
where, the function g refers to the average-pool output of
the teacher network. We used about m = 50 PCs per cluster
and generated a total of n = 50, 000 synthetic images for
CIFAR-10 classes. To generate the synthetic images, we
minimize the objective in (1) by using Adam optimizer with
a learning rate of 0.05, β1 = 0.9, and β2 = 0.999. We
optimize the image Xi for 500 iterations. Finally, these
synthetic images are used to train the student via KD.
The main advantage of our clustering-based approach is
that it enables more diversity among the generated synthetic
images and, hence, achieves high accuracy. To summarize,
the idea is to generate synthetic images, and then use them
to distill knowledge about real data to the student.
4. Experimental Setup and Results
For the CIFAR-10 dataset, our teacher is a large Wide Resnet
(WRN) WRN40-4 (8.9M
parameters, 95% accuracy) model, and our student model is
WRN16-1 (100K parameters). Training the WRN16-1 student via Attention Transfer KD on WRN40-4 teacher results in 91% accuracy and
89× fewer parameters than the teacher.
Fig. 2(a) shows samples generated by our approach for
different CIFAR-10 classes. As evident, for classes such
as car, truck, deer, etc., key features like distorted-animalfaces/wheels are visible. On the other hand, images from
classes such as cat, frog, ship are hard to interpret. For
instance, to generate cat samples, the teacher network generally creates a striped pattern which may not be the most
distinguishing feature of a cat (although many cats do have
a striped pattern!). Therefore, the generated images do contain key features learned by the teacher network for various
classes (e.g., stripes for cats, etc.) even though the images
look far from real. Hence, these synthetic images can transfer relevant knowledge about the real data.
Next, in Fig. 2(b), we compare student models trained via
KD on four datasets: (i) random noise images, (ii) images
generated via Dream Distillation, (iii) CIFAR-100 as the
alternate data, and (iv) CIFAR-10 training set. The accuracy is reported for CIFAR-10 test set. The solid blue line
shows how the accuracy of Dream Distillation varies with
the number of synthetic images used for training (e.g., 10%
data means 5000 synthetic images since we generated total
50, 000 images). Fig. 2(b) demonstrates that the accuracy
of Dream Distillation is comparable to that of CIFAR-100
(around 80%), both being around 10% lower accuracy than
the student trained on real CIFAR-10 data. Further, we
demonstrate that the WRN40-4 student model (which is the
same as the teacher) trained via Dream Distillation achieves
88.5% accuracy on CIFAR-10 test set without training on
any real data! Again, for metadata available only at one
layer, the prior DFKD model achieves merely 68-77% accuracy even for MNIST dataset . Hence, it
would achieve much lower accuracy for CIFAR-10.
Finally, if these generated images are used to train a model
without the teacher, WRN40-4 model achieves only 44% accuracy, whereas the same model achieves 2× better accuracy
with the pretrained teacher model (88.5%). This shows that
the generated images can very effectively transfer knowledge from the teacher to the student. Hence, the synthetic
Dream Distillation
Figure 2. (a) Generated synthetic data for CIFAR-10 classes, and (b) Accuracy of student models trained on random, synthetic, alternate
CIFAR-100, and real CIFAR-10 images.
images generated via Dream Distillation can transfer significant knowledge about the real data without accessing any
real or alternate datasets. This can greatly increase the scale
of deep learning at the edge since industries can quickly
deploy models without the need for proprietary datasets.
5. Conclusion and Future Work
In this paper, we have proposed Dream Distillation, a new
approach to address model compression in absence of real
data. To this end, we use a small amount of metadata to
generate synthetic images. Our experiments have shown
that models trained via Dream Distillation can achieve up to
88.5% accuracy on the CIFAR-10 test set without training
on any real data.
In future, we plan to run experiments to validate our approach on applications with medical/speech data, where
different classes’ representations may not be fully-separable.
We will also conduct more ablation studies to analyze the
impact of using more real data to generate metadata (e.g.,
instead of 10% data, what if we use 20% data, etc.).
Acknowledgments
The authors would like to thank Dr. Liangzhen Lai (Facebook) for many useful discussions throughout this project.