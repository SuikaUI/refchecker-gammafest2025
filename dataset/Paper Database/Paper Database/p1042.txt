Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059
November 7–11, 2021. c⃝2021 Association for Computational Linguistics
The Power of Scale for Parameter-Efﬁcient Prompt Tuning
Brian Lester∗Rami Al-Rfou Noah Constant
Google Research
{brianlester,rmyeid,nconstant}@google.com
In this work, we explore “prompt tuning,”
a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform speciﬁc downstream
tasks. Unlike the discrete text prompts used by
GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate
signals from any number of labeled examples.
Our end-to-end learned approach outperforms
GPT-3’s few-shot learning by a large margin.
More remarkably, through ablations on model
size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method
“closes the gap” and matches the strong performance of model tuning (where all model
weights are tuned). This ﬁnding is especially
relevant because large models are costly to
share and serve and the ability to reuse one
frozen model for multiple downstream tasks
can ease this burden. Our method can be seen
as a simpliﬁcation of the recently proposed
“preﬁx tuning” of Li and Liang and we
provide a comparison to this and other similar
approaches. Finally, we show that conditioning a frozen model with soft prompts confers
beneﬁts in robustness to domain transfer and
enables efﬁcient “prompt ensembling.”
release code and model checkpoints to reproduce our experiments.1
Introduction
With the wide success of pre-trained large language models, a range of techniques has arisen to
adapt these general-purpose models to downstream
tasks. ELMo proposed freezing
the pre-trained model and learning a task-speciﬁc
weighting of its per-layer representations. However, since GPT and BERT
∗Work done as a Google AI Resident.
1 
prompt-tuning
Model Parameters
SuperGLUE Score
Model Tuning
Model Tuning (Multi-task)
Prompt Design
Prompt Tuning
Figure 1: Standard model tuning of T5 achieves strong
performance, but requires storing separate copies of the
model for each end task. Our prompt tuning of T5
matches the quality of model tuning as size increases,
while enabling the reuse of a single frozen model for
all tasks. Our approach signiﬁcantly outperforms fewshot prompt design using GPT-3. We show mean and
standard deviation across 3 runs for tuning methods.
 , the dominant adaptation technique has been model tuning (or “ﬁne-tuning”),
where all model parameters are tuned during adaptation, as proposed by Howard and Ruder .
More recently, Brown et al. showed that
prompt design (or “priming”) is surprisingly effective at modulating a frozen GPT-3 model’s behavior
through text prompts. Prompts are typically composed of a task description and/or several canonical
examples. This return to “freezing” pre-trained
models is appealing, especially as model size continues to increase. Rather than requiring a separate
copy of the model for each downstream task, a
single generalist model can simultaneously serve
many different tasks.
Unfortunately, prompt-based adaptation has several key drawbacks. Task description is error-prone
and requires human involvement, and the effectiveness of a prompt is limited by how much condition-
Pre-trained
(11B params)
Task A Model
(11B params)
Task B Model
(11B params)
Task C Model
(11B params)
Pre-trained
(11B params)
Model Tuning
Prompt Tuning
Mixed-task
(20K params each)
Task Prompts
Model tuning requires making a taskspeciﬁc copy of the entire pre-trained model for each
downstream task and inference must be performed in
separate batches. Prompt tuning only requires storing a small task-speciﬁc prompt for each task, and
enables mixed-task inference using the original pretrained model. With a T5 “XXL” model, each copy
of the tuned model requires 11 billion parameters. By
contrast, our tuned prompts would only require 20,480
parameters per task—a reduction of over ﬁve orders of
magnitude—assuming a prompt length of 5 tokens.
ing text can ﬁt into the model’s input. As a result,
downstream task quality still lags far behind that
of tuned models. For instance, GPT-3 175B fewshot performance on SuperGLUE is 17.5 points below ﬁne-tuned T5-XXL (71.8
vs. 89.3) despite using 16 times more parameters.
Several efforts to automate prompt design have
been recently proposed. Shin et al. propose
a search algorithm over the discrete space of words,
guided by the downstream application training data.
While this technique outperforms manual prompt
design, there is still a gap relative to model tuning.
Li and Liang propose “preﬁx tuning”
and show strong results on generative tasks. This
method freezes the model parameters and backpropagates the error during tuning to preﬁx activations prepended to each layer in the encoder
stack, including the input layer. Hambardzumyan
et al. simplify this recipe by restricting the
trainable parameters to the input and output subnetworks of a masked language model, and show
reasonable results on classiﬁcations tasks.
In this paper, we propose prompt tuning as a
further simpliﬁcation for adapting language models.
We freeze the entire pre-trained model and only allow an additional k tunable tokens per downstream
task to be prepended to the input text. This “soft
prompt” is trained end-to-end and can condense
the signal from a full labeled dataset, allowing our
method to outperform few-shot prompts and close
the quality gap with model tuning (Figure 1). At
the same time, since a single pre-trained model is
recycled for all downstream tasks, we retain the ef-
ﬁcient serving beneﬁts of frozen models (Figure 2).
While we developed our method concurrently
with Li and Liang and Hambardzumyan
et al. , we are the ﬁrst to show that prompt
tuning alone (with no intermediate-layer preﬁxes or
task-speciﬁc output layers) is sufﬁcient to be competitive with model tuning. Through detailed experiments in sections 2–3, we demonstrate that language model capacity is a key ingredient for these
approaches to succeed. As Figure 1 shows, prompt
tuning becomes more competitive with scale.
We compare with similar approaches in Section 4. Explicitly separating task-speciﬁc parameters from the “generalist” parameters needed for
general language-understanding has a range of additional beneﬁts. We show in Section 5 that by
capturing the task deﬁnition in the prompt while
keeping the generalist parameters ﬁxed, we are able
to achieve better resilience to domain shifts. In Section 6, we show that “prompt ensembling”, learning multiple prompts for the same task, can boost
quality and is more efﬁcient than classic model ensembling. Finally, in Section 7, we investigate the
interpretability of our learned soft prompts. In sum,
our key contributions are:
1. Proposing prompt tuning and showing its competitiveness with model tuning in the regime
of large language models.
2. Ablating many design choices, and showing
quality and robustness improve with scale.
3. Showing prompt tuning outperforms model
tuning on domain shift problems.
4. Proposing “prompt ensembling” and showing
its effectiveness.
Prompt Tuning
Following the “text-to-text” approach of T5 , we cast all tasks as text generation.
Instead of modeling classiﬁcation as the probability of an output class given some input, Pr(y|X),
where X is a series of tokens and y is a single class
label, we now model it as conditional generation,
where Y is a sequence of tokens that represent a
class label. T5 models classiﬁcation as Prθ(Y |X),
parameterized by the weights, θ, of the transformers that make up its encoder
and decoder.
Prompting is the approach of adding extra information for the model to condition on during its
generation of Y . Normally, prompting is done
by prepending a series of tokens, P, to the input X, such that the model maximizes the likelihood of the correct Y , Prθ(Y |[P; X]), while keeping the model parameters, θ, ﬁxed.
the representations of the prompt tokens, P =
{p1, p2, . . . , pn}, are part of the model’s embedding table, parameterized by the frozen θ. Finding an optimal prompt thus requires the selection
of prompt tokens, through either manual search
or non-differentiable search methods . Prompt tuning removes
the restriction that the prompt P be parameterized
by θ; instead the prompt has its own dedicated parameters, θP , that can be updated. While prompt
design involves selecting prompt tokens from a
ﬁxed vocabulary of frozen embeddings, prompt
tuning can be thought of as using a ﬁxed prompt
of special tokens, where only the embeddings of
these prompt tokens can be updated. Our new conditional generation is now Prθ;θP (Y |[P; X]) and
can be trained by maximizing the likelihood of Y
via backpropagation, while only applying gradient
updates to θP .
Given a series of n tokens, {x1, x2, . . . , xn}, the
ﬁrst thing T5 does is embed the tokens, forming
a matrix Xe ∈Rn×e where e is the dimension of
the embedding space. Our soft-prompts are represented as a parameter Pe ∈Rp×e, where p is the
length of the prompt. Our prompt is then concatenated to the embedded input forming a single matrix [Pe; Xe] ∈R(p+n)×e which then ﬂows though
the encoder-decoder as normal. Our models are
trained to maximize the probability of Y , but only
the prompt parameters Pe are updated.
Design Decisions
There are many possible ways to initialize the
prompt representations. The simplest is to train
from scratch, using random initialization. A more
sophisticated option is to initialize each prompt
token to an embedding drawn from the model’s
vocabulary. Conceptually, our soft-prompt modulates the frozen network’s behavior in the same
way as text preceding the input, so it follows that
a word-like representation might serve as a good
initialization spot. For classiﬁcation tasks, a third
option is to initialize the prompt with embeddings
that enumerate the output classes, similar to the
“verbalizers” of Schick and Schütze . Since
we want the model to produce these tokens in the
output, initializing the prompt with the embeddings
of the valid target tokens should prime the model
to restrict its output to the legal output classes.
Another design consideration is the length of the
prompt. The parameter cost of our method is EP,
where E is the token embedding dimension and P
is the prompt length. The shorter the prompt, the
fewer new parameters must be tuned, so we aim to
ﬁnd a minimal length that still performs well.
Unlearning Span Corruption
Unlike autoregressive language models like GPT-3,
the T5 models we experiment with use an encoderdecoder architecture and pre-train on a span corruption objective. Speciﬁcally, T5 is tasked with
“reconstructing” masked spans in the input text,
which are marked with unique sentinel tokens. The
target output text consists of all the masked content, separated by sentinels, plus a ﬁnal sentinel.
For instance, from the text “Thank you for inviting
me to your party last week” we might construct
a pre-training example where the input is “Thank
you ⟨X⟩me to your party ⟨Y⟩week” and the target
output is “⟨X⟩for inviting ⟨Y⟩last ⟨Z⟩”.
While Raffel et al. ﬁnd this architecture
and pre-training objective more effective than traditional language modeling, we hypothesize that this
setup is not a good ﬁt for producing a frozen model
that can be readily controlled through prompt tuning. In particular, a T5 model pre-trained exclusively on span corruption, such as T5 1.1, has never
seen truly natural input text (free of sentinel tokens), nor has it ever been asked to predict truly
natural targets. In fact, due to the details of T5’s
span corruption preprocessing, every pre-training
target will begin with a sentinel. While this “unnatural” tendency to output sentinels is easy to overcome through ﬁne-tuning, we suspect that it would
be much harder to override through a prompt alone,
as the decoder priors cannot be adjusted.
Given these concerns, we experiment with T5
models in three settings. (1) “Span Corruption”:
We use pre-trained T5 off-the-shelf as our frozen
model, and test its ability to output the expected
text for downstream tasks. (2) “Span Corruption
+ Sentinel”: We use the same model, but prepend
all downstream targets with a sentinel, so as to
more closely resemble the targets seen in pretraining. (3) “LM Adaptation”: We continue T5’s
self-supervised training for a small number of additional steps, but using the “LM” objective dis-
cussed by Raffel et al. ; given a natural text
preﬁx as input, the model must produce the natural
text continuation as output. Crucially, this adaptation happens only once, producing a single frozen
model that we can reuse for prompt tuning across
any number of downstream tasks.
Through LM adaptation, we hope to “quickly”
transform T5 into a model more similar to GPT-3,
which always outputs realistic text, and is known to
respond well to prompts as a “few-shot learner”. It
is not obvious how successful this late-stage transformation will be compared to pre-training from
scratch, and it has not been investigated previously
to our knowledge. As such, we experiment with
various lengths of adaptation up to 100K steps.
Our frozen models are built on top of pre-trained
T5 checkpoints of all sizes (Small, Base, Large, XL,
XXL). We leverage the public T5 1.1 checkpoints,
which include improvements over the original T5.2
Our “default” conﬁguration, plotted with a green
) throughout, uses an LM-adapted version
of T5 trained for an additional 100K steps, initializes using class labels (see Section 3.2), and
uses a prompt length of 100 tokens. While this
is longer than the default 10-token preﬁx used by
Li and Liang , our method still uses fewer
task-speciﬁc parameters, as we only tune the input
layer, as opposed to overwriting activations in all
network layers. See Figure 4 for a detailed comparison. We will also see shortly that even much
shorter prompts are viable as model size increases.
We measure performance on the SuperGLUE
benchmark , a collection of
eight challenging English language understanding
tasks.3 We report metrics on the development set
associated with each dataset.
Each of our prompts train on a single Super-
GLUE task; there was no multi-task setup or mixing of training data across tasks. We translate each
SuperGLUE dataset into a text-to-text format following Raffel et al. , except that we omit the
2These improvements are (1) the removal of all supervised
data from pre-training, (2) adjustments to hyperparameters
dmodel and dff, and (3) the use of GeGLU 
over ReLU activations.
3The tasks are BoolQ , CB , COPA , MultiRC
 , ReCoRD , RTE
 , WiC , and WSC .
task names prepended to inputs indicating which
SuperGLUE task an example belongs to.
We train our prompts for 30,000 steps using T5’s
standard cross-entropy loss, with a constant learning rate of 0.3 and a batch size of 32. Checkpoints
are selected via early stopping on the development
set, where the stopping metric is the default metric for the dataset, or the average of metrics for
datasets evaluated with multiple metrics. All experiments were run in JAX 
using the Adafactor optimizer with weight decay 1e−5, β2 decay 0.8, and
parameter scaling off. The models were implemented in Flax . More details
are available in Appendix A.
Closing the Gap
To compare our method with standard model tuning, we tune the public T5 1.1 checkpoints on
SuperGLUE using the default hyperparameters
speciﬁed in the T5 library (learning rate 0.001,
and Adafactor optimizer with pre-training parameter states restored). We consider two baselines.
(1) “Model Tuning”: For an apples-to-apples comparison, we tune on each task separately, as in our
prompt tuning setup.4 (2) “Model Tuning (Multitask)”: We use T5’s multi-task tuning setup to
achieve a more competitive baseline.5 In this case,
a single model is tuned on all tasks jointly, with a
text preﬁx indicating the task name.
In Figure 1 (p. 1), we see that prompt tuning
becomes more competitive with model tuning as
scale increases. At the XXL size (11 billion parameters), prompt tuning matches even the stronger
multi-task model tuning baseline, despite having
over 20,000 times fewer task-speciﬁc parameters.
To compare with prompt design, we include
GPT-3 few-shot performance on the SuperGLUE
dev split, as reported by Brown et al. .6
Figure 1 shows that prompt tuning beats GPT-3
4To improve this baseline, we performed a sweep over the
batch size hyperparameter and selected 216 tokens per batch.
5The T5 SuperGLUE submission used a more complex
setup, ﬁrst mixing multi-task supervised data into pre-training,
and then performing single-task ﬁne-tuning. Since we use T5
1.1 throughout, this setup is unavailable, as the pre-training
phase is fully self-supervised. We follow Raffel et al. 
in using 220 tokens per batch and including DPR data in
the multi-task mixture, which is known to boost WSC task
performance .
6We also experimented with using GPT-3’s manual text
prompts directly with our LM-adapted T5 checkpoints. However performance was far below GPT-3 for comparable model
sizes. This may be due to differences in pre-training data and
model architecture, as well as T5’s shorter sequence length.
Model Parameters
SuperGLUE Score
(a) Prompt length
Model Parameters
SuperGLUE Score
Random Uniform
Sampled Vocab
Class Label
(b) Prompt initialization
Model Parameters
SuperGLUE Score
Span Corruption
Span Corruption
+ Sentinel
LM Adaptation
(c) Pre-training method
Model Parameters
SuperGLUE Score
(d) LM adaptation steps
Figure 3: Ablations of various hyperparameters on prompt tuning performance (mean and stddev across 3 runs). In
our “default” (
) conﬁguration, quality improves stably with model size. Across all ablations, the largest (XXL)
model is the most robust to hyperparameter choice. (a) Prompt length: Increasing to 20+ tokens generally confers
a large boost, but XXL performs well even with single-token prompts. (b) Prompt initialization: Random uniform
initialization lags behind more “advanced” initializations using sampled vocabulary or class label embeddings, but
the difference vanishes at XXL size. (c) Pre-training objective: LM adaptation outperforms span corruption, even
when a sentinel is added to downstream task targets, but XXL works well with any method. (d) LM adaptation:
Longer adaptation generally gives larger gains, but XXL is robust to even short adaptation.
prompt design by a large margin, with prompttuned T5-Small matching GPT-3 XL (over 16
times larger), and prompt-tuned T5-Large beating
GPT-3 175B (over 220 times larger).
Ablation Study
Prompt Length
We train prompts for each
model size while varying the prompt length in
{1, 5, 20, 100, 150} and ﬁxing other settings to our
default conﬁguration. Figure 3(a) shows that for
most model sizes, increasing prompt length beyond
a single token is critical to achieve good performance. Notably, the XXL model still gives strong
results with a single-token prompt, suggesting that
the larger the model, the less conditioning signal
is needed to achieve a target behavior. Across all
models, increasing beyond 20 tokens only yields
marginal gains.7
7Going past 100 tokens appears mildly detrimental for
larger models. A similar pattern of diminishing performance
past a certain preﬁx length is observed by Li and Liang .
Prompt Initialization
We ablate the effect of
prompt initialization by training models at all sizes
while ﬁxing other hyperparameters to their default
values. For random initialization, we sample uniformly from the range [−0.5, 0.5]. When initializing from sampled vocabulary, we restrict to the
5,000 most “common” tokens in T5’s Sentence-
Piece vocabulary ,
which is ordered by likelihood in the pre-training
corpus. For “class label” initialization, we take
the embeddings for the string representations of
each class in the downstream task and use them to
initialize one of the tokens in the prompt.8 When
a class label is multi-token, we average the token
embeddings. At longer prompt lengths, we often
run out of class labels before we have initialized all
8T5’s handling of the ReCoRD and WSC tasks requires
the model to generate short, free-form text. In these cases, we
initialize the prompts with words related to the task: commonsense, reasoning, reading, and comprehension for ReCoRD
and commonsense, pronoun, and resolution for WSC.
of the prompt tokens. In this case we fall back to
our sampled vocab strategy to ﬁll in the prompt.
Figure 3(b) shows our ablation of initialization
strategy across model sizes, where we ﬁnd that
the class based initialization performs best. At
smaller model sizes, there are large gaps between
the different initializations, but once the model is
scaled to XXL size, those differences disappear.
With “class label” initialization, we observe that
the class labels typically persist in the learned
prompts, such that the nearest token embeddings
(in cosine distance) match the tokens used for initialization. Beyond this, we did not ﬁnd our learned
prompts to be interpretable, similar to those of Shin
et al. . See Section 7 for details.
Pre-training Objective
In Figures 3(c) and 3(d),
we see pre-training objective has a clear effect on
prompt tuning quality. As hypothesized in Section 2.2, T5’s default “span corruption” objective
is not well-suited for training frozen models to be
later conditioned by prompts. Intuitively, models
pre-trained to read and write sentinel tokens are
hard to apply directly to tasks of reading and writing text without sentinels. As seen in Figure 3(c),
even the “workaround” of adding a sentinel to the
downstream targets has little beneﬁt. While LM
adaptation adds value across all model sizes, we
note our largest XXL model is the most forgiving
and gives strong results even with span corruption.
Given the beneﬁt of LM adaptation, we also
explore how long of an adaptation is helpful. Figure 3(d) shows that longer adaptation provides additional gains, up to 100K steps. This suggests
that the “transition” from span corruption to a language modeling objective is not a trivial change,
and making an effective switch takes an investment
of training resources (10% of the steps of the original T5 pre-training). At the same time, as in our
other ablations, we observe that the XXL model
is robust to even non-ideal conﬁgurations. At this
size, the gains from adaptation are quite modest.
In the non-optimal “span corruption” setting, we
observe instability across model sizes, with the
Small model outperforming the larger Base, Large,
and XL models. On inspection, we ﬁnd that for
many tasks, these mid-sized models never learn to
output a legal class label and thus score 0%. The
two most common error modes are copying subspans from the input and predicting an empty string.
Furthermore, this poor performance is not due to
random variance in prompt tuning, as we observe
Model Parameters
Task Parameters
Model Tuning
Prefix Tuning (Train)
Prefix Tuning (Infer)
Prompt Tuning
Prompt Design
Task Parameters (%)
Figure 4: Parameter usage of various adaptation techniques, ﬁxing architecture to T5 1.1 and prompt/preﬁx
length to 1–100 tokens (bands show mean and stddev).
Model Tuning: All parameters are task-speciﬁc. Pre-
ﬁx Tuning: Activations are tuned in the preﬁx of each
layer, requiring 0.1–1% task-speciﬁc parameters for inference, but more are used for training. WARP: Task
parameters are reduced to under 0.1% by only tuning
input and output layers. Prompt Tuning: Only prompt
embeddings are tuned, reaching under 0.01% for most
model sizes.
Prompt Design: Only a sequence of
prompt IDs is required.
low variance across 3 runs for each size. These
results indicate that using models pre-trained with
the “span corruption” objective can be unreliable,
with only 2 out of 5 models working well, whereas
the LM adapted versions work reliably across all
model sizes.
Comparison to Similar Approaches
In this section, we review recent work on learning continuous prompts, and draw comparisons
with our method. One important axis of comparison is the number of task-speciﬁc parameters each
method requires, as shown in Figure 4. Among
methods with learnable parameters, prompt tuning
is the most parameter efﬁcient, requiring less than
0.01% task-speciﬁc parameters for models over a
billion parameters.9
Li and Liang propose “preﬁx tuning”:
learning a sequence of preﬁxes that are prepended
at every transformer layer. This is akin to learning
transformer activations that are ﬁxed across exam-
9To compare with prompt design, we count each token
ID in the prompt as a parameter, and assume a prompt of
between 500–2000 tokens to match the GPT-3 setting. While
this technique is by far the most parameter efﬁcient, it comes
at the cost of task quality.
ples at every network layer. In contrast, prompt
tuning uses a single prompt representation that
is prepended to the embedded input. Beyond requiring fewer parameters, our approach allows the
transformer to update the intermediate-layer task
representations, as contextualized by an input example. Their work builds on GPT-2 and BART , while ours focuses on T5 and examines changes in performance
and robustness to design choices as model size increases. When using BART, preﬁx tuning includes
preﬁxes on both the encoder and decoder network,
while prompt tuning only requires prompts on the
encoder. Li and Liang also rely on a reparameterization of the preﬁx to stabilize learning,
which adds a large number of parameters during
training, whereas our conﬁguration does not require this reparameterization and is robust across
SuperGLUE tasks and model sizes.
Hambardzumyan et al. propose “WARP”,
where prompt parameters are added to the input
layer. This method works with masked language
models, relying on a [MASK] token and a learnable output layer to project the mask to class logits.
This formulation restricts the model to producing a
single output, limiting it to classiﬁcation. Prompt
tuning does not require any changes to the input or
a task-speciﬁc head. The performance of prompt
tuning is also considerably closer to the strong performance of model tuning.
Liu et al. propose “P-tuning” where learnable continuous prompts are interleaved throughout
the embedded input, using patterns based on human
design. Our approach removes this complication
by simply prepending the prompt to the input. To
achieve strong SuperGLUE results, P-tuning has to
be used in conjunction with model tuning, that is,
models jointly update both the prompt and the main
model parameters, whereas our approach keeps the
original language model frozen.10
Qin and Eisner use “soft words” to learn
prompts to extract knowledge from pre-trained
LMs. Prompts are positioned in relation to the
input based on hand-designed prompt prototypes,
and a learned ∆ℓ
i parameter is included for each
layer, so parameter cost scales with model depth.
Logeswaran et al. use a learnable
prepended token to adapt transformer models to var-
10As another difference, P-tuning requires the addition of
“anchor” tokens in the input (e.g. a question mark following
the hypothesis in the RTE task) to achieve strong performance,
while prompt tuning leaves inputs untouched.
TextbookQA
Table 1: F1 mean and stddev for models trained on
SQuAD and evaluated on out-of-domain datasets from
the MRQA 2019 shared task. Prompt tuning tends to
give stronger zero-shot performance than model tuning, especially on datasets with large domain shifts like
TextbookQA.
ious tasks, but focus on small synthetic datasets designed to accommodate a compositional task representation, as opposed to larger real-world datasets.
Their base models are small transformers trained
from scratch jointly with the task representations,
whereas we keep the base model frozen and investigate scaling laws using larger transformers.
More generally, work on task prompts is closely
aligned with work on “adapters” , small bottleneck layers inserted between frozen pre-trained network
layers. Adapters offer another means of reducing task-speciﬁc parameters, with Houlsby et al.
 achieving GLUE performance close to full
model tuning when freezing BERT-Large and only
adding 2–4% additional parameters. Pfeiffer et al.
 use multiple adapters in a multilingual context to explicitly separate language understanding
from task speciﬁcation, similar to our approach. A
core difference between adapters and prompt tuning is how the approaches change model behavior.
Adapters modify the actual function that acts on the
input representation, parameterized by the neural
network, by allowing the rewriting of activations at
any given layer. Prompt tuning modiﬁes behavior
by leaving the function ﬁxed and adding new input representations that can affect how subsequent
input is processed.
Resilience to Domain Shift
By freezing the core language model parameters,
prompt tuning prevents the model from modifying its general understanding of language. Instead,
prompt representations indirectly modulate the representation of the input. This reduces the model’s
ability to overﬁt to a dataset by memorizing speciﬁc lexical cues and spurious correlations. This restriction suggests that prompt tuning may improve
Table 2: Mean and stddev of zero-shot domain transfer
between two paraphrase detection tasks.
robustness to domain shifts, where the distribution
of inputs differs between training and evaluation.
We investigate zero-shot domain transfer on two
tasks: question answering (QA) and paraphrase detection. For question answering, we use the MRQA
2019 shared task on generalization . This task collects extractive QA datasets
in a uniﬁed format and tests how models trained
on “in-domain” datasets perform when evaluated
on “out-of-domain” datasets. For our experiments,
we train on SQuAD and
evaluate on each of the out-of-domain datasets.11
Table 1 shows that prompt tuning outperforms
model tuning on the majority of out-of-domain
datasets, with a remarkable 12.5 point F1 gap between the two approaches on TextbookQA. We observe larger gains from prompt tuning in cases of
larger domain shifts (e.g. to Biomedical in BioASQ
or to Textbooks in TextbookQA). Of the datasets
where model tuning is better, we see that DROP
shares a domain (Wikipedia) with SQuAD and is
thus one of the smallest domain transfers.
As a second test of robustness to domain shift,
we explore transfer between two paraphrase detection tasks from GLUE . The
ﬁrst task is QQP , which asks
if two questions from the community Q&A site
Quora are “duplicates”. The second task is MRPC
 , which asks if two sentences drawn from news articles are paraphrases.
We test transfer in both directions (QQP ⇔MRPC).
As before, we train on the “in-domain” task, select
checkpoints using in-domain validation, and evaluate zero-shot on the “out-of-domain” task.
Table 2 shows that training a lightweight prompt
on the QQP data and evaluating on MRPC gives
much better performance than tuning the entire
model (+3.2 accuracy and +3.1 F1). The results
are much closer in the other direction, with prompt
11We select checkpoints based on SQuAD validation F1.
The out-of-domain datasets are TextbookQA , RACE , BioASQ ( 
org/), RE , DuoRC ,
and DROP .
99.3 / 99.0
100.00 / 100.00
100.0 / 100.0
65.7 / 88.7
66.3 / 89.0
67.1 / 89.4
92.7 / 93.4
92.9 / 93.5
93.2 / 93.9
SuperGLUE (dev)
Table 3: Performance of a ﬁve-prompt ensemble built
from a single frozen T5-XXL model exceeds both the
average and the best among the ﬁve prompts.
tuning showing a small improvement in accuracy
and a small drop in F1. These results support the
view that model tuning may be over-parameterized
and more prone to overﬁt the training task, to the
detriment of similar tasks in different domains.
Prompt Ensembling
Ensembles of neural models trained from different
initializations on the same data are widely observed
to improve task performance and are useful for estimating model uncertainty . However,
as model size increases, ensembling can become
impractical. Beyond the space required to store N
models (e.g. 42 GiB for each copy of T5-XXL),
there is a substantial inference cost to running N
distinct models, whether in parallel or in series.
Prompt tuning provides a more efﬁcient way to
ensemble multiple adaptations of a pre-trained language model. By training N prompts on the same
task, we create N separate “models” for a task,
while still sharing the core language modeling parameters throughout. Beyond drastically reducing
storage costs, the prompt ensemble makes inference more efﬁcient. To process one example, rather
than computing forward passes of N different models, we can execute a single forward pass with a
batch size of N, replicating the example across
the batch and varying the prompt. These savings
mirror those seen for multi-tasking in Figure 2.
To demonstrate the viability of prompt ensembling, we train ﬁve prompts for each SuperGLUE
task, using a single frozen T5-XXL model with
our default hyperparameters. We use simple majority voting to compute predictions from the ensemble. Table 3 shows that across all tasks, the ensemble beats the single-prompt average and beats, or
matches, the best individual prompt.
Interpretability
An ideally interpretable prompt would consist of
natural language that clearly describes the task at
hand, explicitly asks the model for some result or
action, and makes it easy to understand why the
prompt elicited such behavior from the model.
As prompt tuning works in the continuous embedding space rather than the discrete token space,
interpreting prompts becomes more difﬁcult. To
test the interpretability of our learned soft prompts,
we compute the nearest neighbors to each prompt
token from the frozen model’s vocabulary. We use
cosine distance between the vocabulary embedding
vector and the prompt token representation as the
similarity metric.
We observe that for a given learned prompt token, the top-5 nearest neighbors form tight semantic clusters. For example, we see lexically similar
clusters such as { Technology / technology / Technologies / technological / technologies }, as well
as more diverse but still strongly related clusters
such as { entirely / completely / totally / altogether
/ 100% }. The nature of these clusters suggests that
the prompts are in fact learning “word-like” representations. We found that random vectors drawn
from the embedding space do not show this sort of
semantic clustering.
When initializing the prompts using the “classlabel” strategy, we often ﬁnd that the class labels
persist through training. Speciﬁcally, if a prompt
token is initialized to a given label, that label is
often among the learned token’s nearest neighbors
after tuning. When initializing with the “Random
Uniform” or “Sampled Vocab” methods, the class
labels can also be found in the nearest neighbors
of the prompts; however they tend to appear as
neighbors to multiple prompt tokens. This suggests
that the model is learning to store the expected
output classes in the prompts as reference, and
initializing the prompt to outputs classes makes
this easier and more centralized.
When examining longer prompts (e.g. size 100),
we often ﬁnd several prompt tokens with the same
nearest neighbors. This suggests there is either
excess capacity in the prompt, or that the lack of
sequential structure in the prompt representation
makes it difﬁcult for the model to localize information to a speciﬁc position.
While the learned prompts taken as sequences
show little interpretability, we do observe a high
frequency of words like science, technology and
engineering as the nearest neighbors for prompts
trained on the BoolQ dataset and approximately
20% of the questions are in the “Nature/Science”
category. While more investigation is needed, this
suggests that one role of the prompt may be to
prime the model to interpret inputs in a speciﬁc
domain or context (e.g. “scientiﬁc”).
Conclusion
In this paper, we showed that prompt tuning is
a competitive technique for adapting frozen pretrained language models to downstream tasks. On
the popular SuperGLUE benchmark, its task performance rivals that of traditional model tuning, with
the gap vanishing as model size increases. On zeroshot domain transfer, we found that prompt tuning
leads to improved generalization. This plausibly indicates that freezing general-purpose language understanding parameters and restricting downstream
learning to a lightweight parameter footprint can
help to avoid overﬁtting to a speciﬁc domain.
Beyond task quality metrics, we discussed the
appeal of moving to frozen pre-trained models in
terms of storage and serving costs. This move
enables both efﬁcient multi-task serving, as well
as efﬁcient high-performing prompt ensembling.
Looking forward, we believe that factoring out
task-deﬁning parameters as distinct from general
language-modeling parameters is an exciting step
that opens up many avenues for new research.
Acknowledgements
We thank Lucas Dixon, Waleed Ammar, Slav
Petrov and Sebastian Ruder for comments on an
earlier draft, and the following people for helpful
discussion: Colin Raffel, Adam Roberts, and Noam
Shazeer. We thank Linting Xue for help with the
LM adaptation training.