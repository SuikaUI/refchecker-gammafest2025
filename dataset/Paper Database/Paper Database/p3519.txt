Journal of Statistical Software
September 2010, Volume 36, Issue 11.
 
Feature Selection with the Boruta Package
Miron B. Kursa
University of Warsaw
Witold R. Rudnicki
University of Warsaw
This article describes a R package Boruta, implementing a novel feature selection
algorithm for ﬁnding all relevant variables. The algorithm is designed as a wrapper around
a Random Forest classiﬁcation algorithm. It iteratively removes the features which are
proved by a statistical test to be less relevant than random probes. The Boruta package
provides a convenient interface to the algorithm. The short description of the algorithm
and examples of its application are presented.
Keywords: feature selection, feature ranking, random forest.
1. Introduction
Feature selection is often an important step in applications of machine learning methods
and there are good reasons for this.
Modern data sets are often described with far too
many variables for practical model building. Usually most of these variables are irrelevant to
the classiﬁcation, and obviously their relevance is not known in advance. There are several
disadvantages of dealing with overlarge feature sets. One is purely technical — dealing with
large feature sets slows down algorithms, takes too many resources and is simply inconvenient.
Another is even more important — many machine learning algorithms exhibit a decrease of
accuracy when the number of variables is signiﬁcantly higher than optimal . Therefore selection of the small (possibly minimal) feature set giving best possible
classiﬁcation results is desirable for practical reasons.
This problem, known as minimaloptimal problem , has been intensively studied
and there are plenty of algorithms which were developed to reduce feature set to a manageable
Nevertheless, this very practical goal shadows another very interesting problem — the identiﬁcation of all attributes which are in some circumstances relevant for classiﬁcation, the so-called
all-relevant problem. Finding all relevant attributes, instead of only the non-redundant ones,
Feature Selection with the Boruta Package
may be very useful in itself. In particular, this is necessary when one is interested in understanding mechanisms related to the subject of interest, instead of merely building a black box
predictive model. For example, when dealing with results of gene expression measurements
in context of cancer, identiﬁcation of all genes which are related to cancer is necessary for
complete understanding of the process, whereas a minimal-optimal set of genes might be more
useful as genetic markers. A good discussion outlining why ﬁnding all relevant attributes is
important is given by Nilsson et al. .
The all-relevant problem of feature selection is more diﬃcult than usual minimal-optimal one.
One reason is that we cannot rely on the classiﬁcation accuracy as the criterion for selecting
the feature as important (or rejecting it as unimportant). The degradation of the classiﬁcation
accuracy, upon removal of the feature from the feature set, is suﬃcient to declare the feature
important, but lack of this eﬀect is not suﬃcient to declare it unimportant. One therefore
needs another criterion for declaring variables important or unimportant.
Moreover, one
cannot use ﬁltering methods, because the lack of direct correlation between a given feature
and the decision is not a proof that this feature is not important in conjunction with the other
features (Guyon and Elisseeﬀ2003). One is therefore restricted to wrapper algorithms, which
are computationally more demanding than ﬁlters.
In a wrapper method the classiﬁer is used as a black box returning a feature ranking, therefore
one can use any classiﬁer which can provide the ranking of features. For practical reasons, a
classiﬁer used in this problem should be both computationally eﬃcient and simple, possibly
without user deﬁned parameters.
The current paper presents an implementation of the algorithm for ﬁnding all relevant features
in the information system in a R package Boruta (available from the Comprehensive R Archive Network at 
Boruta). The algorithm uses a wrapper approach built around a random forest classiﬁer (Boruta is a god of the forest in the Slavic mythology). The algorithm is an
extension of the idea introduced by Stoppiglia, Dreyfus, Dubois, and Oussar to determine relevance by comparing the relevance of the real features to that of the random probes.
Originally this idea was proposed in the context of ﬁltering, whereas here it is used in the
wrapper algorithm. In the remaining sections of this article ﬁrstly a short description of the
algorithm is given, followed by the examples of its application on a real-world and artiﬁcial
2. Boruta algorithm
Boruta algorithm is a wrapper built around the random forest classiﬁcation algorithm implemented in the R package randomForest .
The random forest
classiﬁcation algorithm is relatively quick, can usually be run without tuning of parameters
and it gives a numerical estimate of the feature importance. It is an ensemble method in
which classiﬁcation is performed by voting of multiple unbiased weak classiﬁers — decision
trees. These trees are independently developed on diﬀerent bagging samples of the training
set. The importance measure of an attribute is obtained as the loss of accuracy of classiﬁcation caused by the random permutation of attribute values between objects. It is computed
separately for all trees in the forest which use a given attribute for classiﬁcation. Then the
average and standard deviation of the accuracy loss are computed. Alternatively, the Z score
Journal of Statistical Software
computed by dividing the average loss by its standard deviation can be used as the importance measure. Unfortunately the Z score is not directly related to the statistical signiﬁcance
of the feature importance returned by the random forest algorithm, since its distribution is
not N(0, 1) . Nevertheless, in Boruta
we use Z score as the importance measure since it takes into account the ﬂuctuations of the
mean accuracy loss among trees in the forest.
Since we cannot use Z score directly to measure importance, we need some external reference
to decide whether the importance of any given attribute is signiﬁcant, that is, whether it
is discernible from importance which may arise from random ﬂuctuations. To this end we
have extended the information system with attributes that are random by design. For each
attribute we create a corresponding ‘shadow’ attribute, whose values are obtained by shuﬄing
values of the original attribute across objects.
We then perform a classiﬁcation using all
attributes of this extended system and compute the importance of all attributes.
The importance of a shadow attribute can be nonzero only due to random ﬂuctuations. Thus
the set of importances of shadow attributes is used as a reference for deciding which attributes
are truly important.
The importance measure itself varies due to stochasticity of the random forest classiﬁer. Additionally it is sensitive to the presence of non important attributes in the information system
(also the shadow ones). Moreover it is dependent on the particular realization of shadow
attributes. Therefore we need to repeat the re-shuﬄing procedure to obtain statistically valid
In short, Boruta is based on the same idea which forms the foundation of the random forest
classiﬁer, namely, that by adding randomness to the system and collecting results from the
ensemble of randomized samples one can reduce the misleading impact of random ﬂuctuations
and correlations. Here, this extra randomness shall provide us with a clearer view of which
attributes are really important.
The Boruta algorithm consists of following steps:
1. Extend the information system by adding copies of all variables (the information system
is always extended by at least 5 shadow attributes, even if the number of attributes in
the original set is lower than 5).
2. Shuﬄe the added attributes to remove their correlations with the response.
3. Run a random forest classiﬁer on the extended information system and gather the
Z scores computed.
4. Find the maximum Z score among shadow attributes (MZSA), and then assign a hit to
every attribute that scored better than MZSA.
5. For each attribute with undetermined importance perform a two-sided test of equality
with the MZSA.
6. Deem the attributes which have importance signiﬁcantly lower than MZSA as ‘unimportant’ and permanently remove them from the information system.
7. Deem the attributes which have importance signiﬁcantly higher than MZSA as ‘important’.
Feature Selection with the Boruta Package
8. Remove all shadow attributes.
9. Repeat the procedure until the importance is assigned for all the attributes, or the
algorithm has reached the previously set limit of the random forest runs.
In practice this algorithm is preceded with three start-up rounds, with less restrictive importance criteria. The startup rounds are introduced to cope with high ﬂuctuations of Z scores
when the number of attributes is large at the beginning of the procedure. During these initial rounds, attributes are compared respectively to the ﬁfth, third and second best shadow
attribute; the test for rejection is performed only at the end of each initial round, while the
test for conﬁrmation is not performed at all.
The time complexity of the procedure described above in realistic cases is approximately
O(P · N), where P and N are respectively the numbers of attributes and objects. That may
be time consuming for large data sets; still, this eﬀort is essential to produce a statistically
signiﬁcant selection of relevant features.
To illustrate the scaling properties of Boruta algorithm we performed following experiment
using Madalon data set. It is an artiﬁcial data set, which was one of the NIPS2003 problems.
 The data set contains 2000 objects described with
500 attributes. We generated subsamples of Madelon set containing 250, 500, 750, . . . , 2000
objects. Then for each subsample we created seven extended sets containing respectively 500,
1000, . . . , 3500 superﬁcial attributes obtained as a uniform random noise. Then we performed
standard feature selection with Boruta on each of 64 test sets and measured the execution
time. The results of the experiment are displayed in Figure 1. One may see almost perfect
linear scaling for the increasing number of attributes. On the other hand execution times
grow faster than the number of objects, but the diﬀerence is not very big and it seems to
converge to linear scaling for large number of objects.
The timings are reported in CPU hours. Using the values from the largest data set, one can
estimate the time required to complete Boruta run on a single core of modern CPU to be one
hour per one million (attribute × objects).
One should notice that in many cases, in particular for a biomedical problems, the computation time is a small fraction of the time required to collect the data. One should also
note, that the prime reason for running the ’all-relevant’ feature selection algorithm is not
the reduction of computation time (altough it can be achieved if the data set pruned from
non-informative attributes will be subsequently analysed numerous times). The main reason
is to ﬁnd all attributes for which their correlation with decision is higher than that of the random attributes. Moreover, while Boruta is generally a sequential algorithm, the underlying
random forest classiﬁer is a trivially parallel task and thus Boruta can be distributed even
over a hundreds of cores, provided that a parallel version of the random forest algorithm is
3. Using the Boruta package
The Boruta algorithm is implemented in Boruta package.
R> library("Boruta")
Journal of Statistical Software
Attributes
Attributes
Figure 1: The scaling properties of Boruta with respect to the number of attributes (left) and
number of objects (right). Each line on the left panel corresponds to the set with identical
number of objects and on the right panel it corresponds to the set with identical number of
attributes. One may notice that scaling is linear with respect to number of attributes and
not far from linear with respect to the number of objects.
The ozone data from UCI Machine Learning Repository and
available in mlbench package is used as the ﬁrst example:
R> library("mlbench")
R> data("Ozone")
R> Ozone <- na.omit(Ozone)
The algorithm is performed by the Boruta function. For its arguments, one should specify the
model, either using a formula or predictor data frame with a response vector; the conﬁdence
level (which is recommended to be left default) and the maximal number of random forest
One can also provide values of mtry and ntree parameters, which will be passed to
randomForest function. Normally default randomForest parameters are used, they will be
suﬃcient in most cases since random forest performance has rather a weak dependence on its
parameters. If it is not the case, one should try to ﬁnd mtry and ntree for which random
forest classiﬁer achieves convergence at minimal value of the OOB error.
Setting doTrace argument to 1 or 2 makes Boruta report the progress of the process; version
2 is a little more verbose, namely it shows attribute decisions as soon as they are cleared.
R> set.seed(1)
R> Boruta.Ozone <- Boruta(V4 ~ ., data = Ozone, doTrace = 2, ntree = 500)
Feature Selection with the Boruta Package
Initial round 1: ..........
attributes rejected after this test:
Initial round 2: ..........
attributes rejected after this test:
Initial round 3: ..........
Final round: ..........
attributes confirmed after this test:
V1 V5 V7 V8 V9 V10 V11 V12
attributes confirmed after this test:
attributes rejected after this test:
R> Boruta.Ozone
Boruta performed 48 randomForest runs in 2.540633 mins.
attributes confirmed important: V1 V5 V7 V8 V9 V10 V11 V12 V13
attributes confirmed unimportant: V2 V3 V6
The Ozone set consists of 12 attributes; three of them are rejected, two after the initial
round 2, and one during the ﬁnal round. The remaining attributes are indicated as conﬁrmed.
Figure 2 shows the Z scores variability among attributes during the Boruta run. It can be
easily generated using the plot method of Boruta object:
R> plot(Boruta.Ozone)
One can see that Z score of the most important shadow attribute clearly separates important
and non important attributes.
Moreover, it is clearly evident that attributes which consistently receive high importance
scores in the individual random forest runs are selected as important. On the other hand, one
can observe quite sizeable variability of individual scores. The highest score of a random attribute in a single run is higher than the highest importance score of two important attributes,
and the lowest importance score of ﬁve important attributes. It clearly shows that the results
of Boruta are generally more stable than those produced by feature selection methods based
on a single random forest run, and this is why several iterations are required.
Due to the fact that the number of random forest runs during Boruta is limited by the maxRuns
argument, the calculation can be forced to stop prematurely, when there are still attributes
which are judged neither to be conﬁrmed nor rejected — and thus ﬁnally marked tentative.
For instance1:
R> set.seed(1)
R> Boruta.Short <- Boruta(V4 ~ ., data = Ozone, maxRuns = 12)
1The number of steps and the seed were intentionally selected to show this eﬀect in the familiar data
set. Due to slight diﬀerences between Windows and Linux versions of randomForest package, which probably
arise due to compilation, the actual results of the procedure described above might diﬀer slightly from the
results shown here (these were obtained in R version 2.10.0 and randomForest version 4.5-33 on x86-64 Linux
workstation).
Journal of Statistical Software
Attributes
Boruta result plot for ozone data. Blue boxplots correspond to minimal, average
and maximum Z score of a shadow attribute. Red and green boxplots represent Z scores of
respectively rejected and conﬁrmed attributes.
R> Boruta.Short
Boruta performed 42 randomForest runs in 2.3612 mins.
attributes confirmed important: V1 V5 V7 V8 V9 V10 V11 V12
attributes confirmed unimportant: V2 V3
tentative attributes left: V6 V13
One should consider increasing the maxRuns parameter if tentative attributes are left. Nevertheless, there may be attributes with importance so close to MZSA that Boruta won’t be
able to make a decision with the desired conﬁdence in realistic number of random forest runs.
Therefore Boruta package contains a TentativeRoughFix function which can be used to ﬁll
missing decisions by simple comparison of the median attribute Z score with the median
Z score of the most important shadow attribute:
R> TentativeRoughFix(Boruta.Short)
Boruta performed 42 randomForest runs in 2.3612 mins.
Tentatives roughfixed over 12 last randomForest runs.
attributes confirmed important: V1 V5 V7 V8 V9 V10 V11 V12 V13
attributes confirmed unimportant: V2 V3 V6
Feature Selection with the Boruta Package
One can obviously treat such attributes manually.
For easy transfer of Boruta results to other classiﬁers and tools, the Boruta package contains functions that extract the results and convert them into a convenient form.
getConfirmedFormula and getNonRejectedFormula create a formula object that deﬁnes
a model based respectively only on conﬁrmed or on conﬁrmed and tentative attributes:
R> getConfirmedFormula(Boruta.Ozone)
V4 ~ V1 + V5 + V7 + V8 + V9 + V10 + V11 + V12 + V13
The attStats function creates a data frame containing each attribute’s Z score statistics and
the fraction of random forest runs in which this attribute was more important than the most
important shadow one:
R> attStats(Boruta.Ozone)
13.3911279 13.6373356 10.505555 15.1610346 1.0000000 Confirmed
-2.0475252 -1.5112547 -4.741706 -0.6750894 0.0000000
-1.2097874 -1.4335204 -2.202290
0.5520193 0.0000000
8.8074357 0.9166667 Confirmed
0.6179196 -1.491181
2.2507610 0.1250000
6.244625 12.0112148 0.9791667 Confirmed
17.3302697 17.1651707 16.186920 18.8550455 1.0000000 Confirmed
20.3332547 20.2826539 18.530345 21.8499295 1.0000000 Confirmed
6.391154 10.7939586 0.9791667 Confirmed
V11 10.0848916 10.4122110
6.179540 12.8348468 0.9583333 Confirmed
V12 13.9761395 14.1462836 11.335510 15.5130497 1.0000000 Confirmed
9.0149381 0.9166667 Confirmed
4. Example: Madelon data
Madelon is an artiﬁcial data set, which was one of the NIPS2003 problems. The data set contains 2000 objects corresponding to points located in 32 vertices of a
5-dimensional hypercube. Each vertex is randomly assigned one of two classes: −1 or +1,
and the decision of each object is a class of its vertex. 500 attributes are constructed in the
following way: 5 of them are randomly jittered coordinates of points; 15 others are random
linear combinations of the ﬁrst 5; ﬁnally the rest of the system is a uniform random noise.
The task is to extract 20 important attributes from the system.
Madelon data is available from UCI Machine Learning Repository (loading of this data set may take several minutes):
R> root <-
" "
R> predictors <- read.table(paste(root, "madelon_train.data", sep = ""))
R> decision <- read.table(paste(root, "madelon_train.labels", sep = ""))
R> Madelon <- data.frame(predictors, decision = factor(decision[, 1]))
Journal of Statistical Software
Random Forest run
Z score evolution during Boruta run. Green lines correspond to conﬁrmed attributes, red to rejected ones and blue to respectively minimal, average and maximal shadow
attribute importance. Gray lines separate rounds.
Running Boruta (execution may take a few hours):
R> set.seed(7777)
R> Boruta.Madelon <- Boruta(decision ~ ., data = Madelon)
R> Boruta.Madelon
Boruta performed 51 randomForest runs in 1.861855 hours.
attributes confirmed important: V29 V49 V65 V106 V129 V154 V242
V282 V319 V337 V339 V379 V434 V443 V452 V454 V456 V473 V476 V494
attributes confirmed unimportant: V1 V2 V3 V4 V5 V6 V7 V8 V9
V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28
(the rest of the output was omitted)
One can see that we have obtained 20 conﬁrmed attributes. The plotZScore function visualizes the evolution of attributes’ Z scores during a Boruta run:
R> plotZHistory(Boruta.Madelon)
Feature Selection with the Boruta Package
The result can be seen on Figure 3. One may notice that consecutive removal of random
noise increases the Z score of important attributes and improves their separation from the
unimportant ones; one of them is even ‘pulled’ out of the group of unimportant attributes just
after the ﬁrst initial round. Also, on certain occasions, unimportant attributes may achieve
a higher Z score than the most important shadow attribute, and this is the reason why we
need multiple random forest runs to arrive at a statistically signiﬁcant decision.
The reduction of attribute number is considerable (96%). One can expect that the increase
of accuracy of a random forest classiﬁer can be obtained on the reduced data set due to the
elimination of noise.
It is known that feature selection procedure can introduce signiﬁcant bias in resulting models.
For example Ambroise and McLachlan have shown that, with the help of feature
selection procedure, one can obtain a classiﬁer, which is using only non-informative attributes
and is 100% accurate on the training set. Obviously such classiﬁer is useless and is returning
random answers on the test set.
Therefore it is necessary to check whether Boruta is resistant to this type of error.
achieved with the help of cross-validation procedure. The part of the data is set aside as a
test set. Then the complete feature selection procedure is performed on the remaining data
set – a training set. Finally the classiﬁer obtained on the training set is used to classify objects
from the test set to obtain classiﬁcation error. The procedure is repeated several times, to
obtain estimate of the variability of results.
Boruta performs several random forest runs to obtain statistically signiﬁcant division between
important and irrelevant attributes. One should expect that ranking obtained in the single
RF run should be quite similar to that obtained from Boruta. We can check if this is the
case, taking advantage of the cross-validation procedure described above.
Madelon data was split ten times into train and test sets containing respectively 90% and
10% of objects. Than, Boruta was run on each train set. Also, three random forest classiﬁers
were generated on each train set: ﬁrst using all attributes, the second one using only these
attributes that were selected by Boruta, and the third one using the same number of attributes
as found by Boruta, but selected as a top important by the ﬁrst random forest trained on all
attributes. Finally, the OOB error estimate on a train set and the error on a test set for all
classiﬁers was collected.
The results are shown in the Table 1. One can see that both the OOB error as well as the
error on the test set is consistently smaller for random forest runs performed on the reduced
set of attributes. This observation is veriﬁed by a t test:
R> t.test(CV.Boruta$"Test conf.", CV.Boruta$"Test all", paired = TRUE)
Paired t-test
CV.Boruta$"Test conf." and CV.Boruta$"Test all"
t = -24.2727, df = 9, p-value = 1.636e-09
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
-0.198962 -0.165038
sample estimates:
Journal of Statistical Software
Test conf.
Table 1: Cross-validation of the error reduction due to limiting the information system to
attributes claimed conﬁrmed by Boruta.
mean of the differences
As one may expect, the feature ranking provided by plain random forest agrees fairly well
with Boruta results. This explains why the simple heuristic feature selection procedure in
random forest, namely selecting a dozen or so top scoring attributes, works well for obtaining
good classiﬁcation results. Nevertheless, this will not necessarily be a case when dealing with
larger and more complex sets, where stochastic eﬀects increase the variability of the random
forest importance measure and thus destabilize the feature ranking.
One should note that the Boruta is a heuristic procedure designed to ﬁnd all relevant attributes, including weakly relevant attributes. Following Nilsson et al. , we say that
attribute is weakly important when one can ﬁnd a subset of attributes among which this
attribute is not redundant. The heuristic used in Boruta implies that the attributes which
are signiﬁcantly correlated with the decision variables are relevant, and the signiﬁcance here
means that correlation is higher than that of the randomly generated attributes. Obviously
the set of all relevant attributes may contain highly correlated but still redundant variables.
Also, the correlation of the attribute with the decision does not imply causative relation; it
may arise when both decision attribute and descriptive attribute are independently correlated with some other variable. An illustrative example of such situation was given by Strobl,
Hothorn, and Zeileis . Users interested in ﬁnding a set of highly relevant and uncorrelated attributes within the result returned by Boruta may use for example package party
 , caret , varSelRF or FSelector for further
reﬁnement.
5. Summary
We have developed Boruta, a novel random forest based feature selection method, which
provides unbiased and stable selection of important and non-important attributes from an
information system. Due to the iterative construction, our method can deal both with the
Feature Selection with the Boruta Package
ﬂuctuating nature of a random forest importance measure and the interactions between attributes. We have also demonstrated its usefulness on an artiﬁcial data set. The method is
available as an R package.
Acknowledgments
Computations were performed at ICM, grant G34-5. We would like to thank the reviewers
and the technical editor for a helpful discussions which led to improvement of the paper.