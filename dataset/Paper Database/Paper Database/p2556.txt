Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846–1851,
October 25-29, 2014, Doha, Qatar. c⃝2014 Association for Computational Linguistics
Constructing Information Networks Using One Single Model
Sujian Li¶
†Computer Science Department, Rensselaer Polytechnic Institute, USA
‡School of Computer Science and Technology, Soochow University, China
¶Key Laboratory of Computational Linguistics, Peking University, MOE, China
†{liq7,hongy2,jih}@rpi.edu, ¶ 
In this paper, we propose a new framework that uniﬁes the output of three information extraction (IE) tasks - entity mentions, relations and events as an information network representation, and extracts
all of them using one single joint model
based on structured prediction. This novel
formulation allows different parts of the
information network fully interact with
each other.
For example, many relations can now be considered as the resultant states of events.
Our approach
achieves substantial improvements over
traditional pipelined approaches, and signiﬁcantly advances state-of-the-art end-toend event argument extraction.
Introduction
Information extraction (IE) aims to discover entity
mentions, relations and events from unstructured
texts, and these three subtasks are closely interdependent: entity mentions are core components
of relations and events, and the extraction of relations and events can help to accurately recognize
entity mentions. In addition, the theory of eventualities suggested that relations can
be viewed as states that events start from and result
in. Therefore, it is intuitive but challenging to extract all of them simultaneously in a single model.
Some recent research attempted to jointly model
multiple IE subtasks ). For example, Roth and
Yih conducted joint inference over entity
mentions and relations; Our previous work jointly
extracted event triggers and arguments , and entity mentions and relations . However, a single model that can extract all of them has never been studied so far.
Asif Mohammed Hanif detonated explosives in
Geopolitical Entity
Instrument
Agent-Artifact
Figure 1: Information Network Representation.
Information nodes are denoted by rectangles. Arrows represent information arcs.
For the ﬁrst time, we uniformly represent the IE
output from each sentence as an information network, where entity mentions and event triggers are
nodes, relations and event-argument links are arcs.
We apply a structured perceptron framework with
a segment-based beam-search algorithm to construct the information networks . In addition to the perceptron update, we also apply k-best MIRA , which reﬁnes the perceptron
update in three aspects: it is ﬂexible in using various loss functions, it is a large-margin approach,
and it can use mulitple candidate structures to tune
feature weights.
In an information network, we can capture the
interactions among multiple nodes by learning
joint features during training. In addition to the
cross-component dependencies studied in , we are able to capture interactions between relations and events. For
example, in Figure 1, if we know that the Person
mention “Asif Mohammed Hanif” is an Attacker
of the Attack event triggered by “detonated”, and
the Weapon mention “explosives” is an Instrument,
we can infer that there exists an Agent-Artifact
relation between them.
Similarly we can infer
the Physical relation between “Asif Mohammed
Hanif” and “Tel Aviv”.
However, in practice many useful interactions
are missing during testing because of the data spar-
sity problem of event triggers. We observe that
21.5% of event triggers appear fewer than twice in
the ACE’051 training data. By using only lexical
and syntactic features we are not able to discover
the corresponding nodes and their connections. To
tackle this problem, we use FrameNet to generalize event triggers so that
semantically similar triggers are clustered in the
same frame.
The following sections will elaborate the detailed implementation of our new framework.
We uniformly represent the IE output from each
sentence as an information network y = (V, E).
Each node vi
∈V is represented as a triple
⟨ui, vi, ti⟩of start index ui, end index vi, and node
type ti. A node can be an entity mention or an
event trigger. A particular type of node is ⊥(neither entity mention nor event trigger), whose maximal length is always 1.
Similarly, each information arc ej ∈E is represented as ⟨uj, vj, rj⟩,
where uj and vj are the end offsets of the nodes,
and rj is the arc type.
For instance, in Figure 1, the event trigger “detonated” is represented
as ⟨4, 4, Attack⟩, the entity mention “Asif Mohammed Hanif” is represented as ⟨1, 3, Person⟩,
and their argument arc is ⟨4, 3, Attacker⟩. Our
goal is to extract the whole information network y
for a given sentence x.
Decoding Algorithm
Our joint decoding algorithm is based on extending the segment-based algorithm described in
our previous work .
(x1, ..., xm) be the input sentence. The decoder
performs two types of actions at each token xi
from left to right:
• NODEACTION(i, j):
appends a new node
⟨j, i, t⟩ending at the i-th token, where i −dt <
j ≤i, and dt is the maximal length of type-t
nodes in training data.
• ARCACTION(i, j): for each j < i, incrementally creates a new arc between the nodes ending
at the j-th and i-th tokens respectively: ⟨i, j, r⟩.
After each action, the top-k hypotheses are selected according to their features f(x, y′) and
1 
weights w:
f(x, y′) · w
Since a relation can only occur between a pair of
entity mentions, an argument arc can only occur
between an entity mention and an event trigger,
and each edge must obey certain entity type constraints, during the search we prune invalid AR-
CACTIONs by checking the types of the nodes
ending at the j-th and the i-th tokens. Finally, the
top hypothesis in the beam is returned as the ﬁnal
prediction. The upper-bound time complexity of
the decoding algorithm is O(d · b · m2), where d
is the maximum size of nodes, b is the beam size,
and m is the sentence length. The actual execution
time is much shorter, especially when entity type
constraints are applied.
Parameter Estimation
For each training instance (x, y), the structured
perceptron algorithm seeks the assignment with
the highest model score:
z = argmax
f(x, y′) · w
and then updates the feature weights by using:
wnew = w + f(x, y) −f(x, z)
We relax the exact inference problem by the aforementioned beam-search procedure.
The standard perceptron will cause invalid updates because of inexact search. Therefore we apply earlyupdate , an instance of
violation-ﬁxing methods . In
the rest of this paper, we override y and z to denote
preﬁxes of structures.
In addition to the simple perceptron update, we
also apply k-best MIRA ,
an online large-margin learning algorithm. During
each update, it keeps the norm of the change to
feature weights w as small as possible, and forces
the margin between y and the k-best candidate z
greater or equal to their loss L(y, z). It is formulated as a quadratic programming problem:
min ∥wnew −w∥
s.t. wnewf(x, y) −wnewf(x, z) ≥L(y, z)
∀z ∈bestk(x, w)
We employ the following three loss functions
for comparison:
Relation Type
Event Type
Destination
He(arg-1) was escorted(trigger) into Iraq(arg-2).
Many people(arg-1) were in the cafe(arg-2) during the blast(trigger).
Agent-Artifact
Instrument
Terrorists(arg-1) might use(trigger) the devices(arg-2) as weapons.
The truck(arg-1) was carrying(trigger) Syrians ﬂeeing the war in Iraq(arg-2).
They(arg-1) have reunited(trigger) with their friends in Norfolk(arg-2).
Two Marines(arg-1) were killed(trigger) in the ﬁghting in Kut(arg-2).
Protesters(arg-1) have been clashing(trigger) with police in Tehran(arg-2).
ORG-Afﬁliation
End-Position
NBC(arg-2) is terminating(trigger) freelance reporter Peter Arnett(arg-1).
Table 1: Frequent overlapping relation and event types in the training set.
• The ﬁrst one is F1 loss:
L1(y, z) = 1 −2 · |y ∩z|
When counting the numbers, we treat each node
and arc as a single unit. For example, in Figure 1, |y| = 6.
• The second one is 0-1 loss:
L2(y, z) =
It does not discriminate the extent to which z
deviates from y.
• The third loss function counts the difference between y and z:
L3(y, z) = |y| + |z| −2 · |y ∩z|
Similar to F1 loss function, it penalizes both
missing and false-positive units. The difference
is that it is sensitive to the size of y and z.
Joint Relation-Event Features
By extracting three core IE components in a joint
search space, we can utilize joint features over
multiple components in addition to factorized features in pipelined approaches. In addition to the
features as described in , we can make use of joint features between relations and events, given the fact that
relations are often ending or starting states of
events . Table 1 shows the most
frequent overlapping relation and event types in
our training data. In each partial structure y′ during the search, if both arguments of a relation participate in an event, we compose the corresponding argument roles and relation type as a joint feature for y′. For example, for the structure in Figure 1, we obtain the following joint relation-event
Instrument
Agent-Artifact
Table 2: Data set
Number of instances
Trigger Words
Figure 2: Distribution of triggers and their frames.
Semantic Frame Features
One major challenge of constructing information
networks is the data sparsity problem in extracting event triggers.
For instance, in the sentence: “Others were mutilated beyond recognition.” The Injure trigger “mutilated” does not occur in our training data. But there are some similar words such as “stab” and “smash”. We utilize FrameNet to solve
this problem. FrameNet is a lexical resource for
semantic frames. Each frame characterizes a basic type of semantic concept, and contains a number of words (lexical units) that evoke the frame.
Many frames are highly related with ACE events.
For example, the frame “Cause harm” is closely
related with Injure event and contains 68 lexical
units such as “stab”, “smash” and “mutilate”.
Figure 2 compares the distributions of trigger
words and their frame IDs in the training data. We
can clearly see that the trigger word distribution
suffers from the long-tail problem, while Frames
reduce the number of triggers which occur only
Entity Mention (%)
Relation (%)
Event Trigger (%)
Event Argument (%)
Pipelined Baseline
Pipeline + Li et al. 
Li and Ji 
Joint w/ Avg. Perceptron
Joint w/ MIRA w/ F1 Loss
Joint w/ MIRA w/ 0-1 Loss
Joint w/ MIRA w/ L3 Loss
Table 3: Overall performance on test set.
once in the training data from 100 to 60 and alleviate the sparsity problem. For each token, we
exploit the frames that contain the combination of
its lemma and POS tag as features. For the above
example, “Cause harm” will be a feature for “mutilated”. We only consider tokens that appear in
at most 2 frames, and omit the frames that occur
fewer than 20 times in our training data.
Experiments
Data and Evaluation
We use ACE’05 corpus to evaluate our method
with the same data split as in . Table 2 summarizes the statistics of the data set. We
report the performance of extracting entity mentions, relations, event triggers and arguments separately using the standard F1 measures as deﬁned
in :
• An entity mention is correct if its entity type (7
in total) and head offsets are correct.
• A relation is correct if its type (6 in total) and the
head offsets of its two arguments are correct.
• An event trigger is correct if its event subtype
(33 in total) and offsets are correct.
• An argument link is correct if its event subtype,
offsets and role match those of any of the reference argument mentions.
In this paper we focus on entity arguments while
disregard values and time expressions because
they can be most effectively extracted by handcrafted patterns .
Based on the results of our development set, we
trained all models with 21 iterations and chose the
beam size to be 8. For the k-best MIRA updates,
we set k as 3. Table 3 compares the overall performance of our approaches and baseline methods.
Our joint model with perceptron update outperforms the state-of-the-art pipelined approach
in , and further
improves the joint event extraction system in (p < 0.05 for entity mention extraction, and p < 0.01 for other subtasks, according to Wilcoxon Signed RankTest).
For the kbest MIRA update, the L3 loss function achieved
better performance than F1 loss and 0-1 loss on
all sub-tasks except event argument extraction. It
also signiﬁcantly outperforms perceptron update
on relation extraction and event argument extraction (p < 0.01). It is particularly encouraging to
see the end output of an IE system (event arguments) has made signiﬁcant progress (12.2% absolute gain over traditional pipelined approach).
Discussions
Feature Study
Frame=Killing
Frame=Travel
Physical(Artifact, Destination)
Frame=Arriving
ORG-AFF(Person, Entity)
Lemma=charge
Charge-Indict
Lemma=birth
Physical(Artifact,Origin)
Frame=Cause harm
Table 4: Top Features about Event Triggers.
Table 4 lists the weights of the most signiﬁcant
features about event triggers.
The 3rd, 6th, and
9th rows are joint relation-event features. For instance, Physical(Artifact, Destination) means the
arguments of a Physical relation participate in a
Transport event as Artifact and Destination. We
can see that both the joint relation-event features
and FrameNet based features are of vital importance to event trigger labeling. We tested the impact of each type of features by excluding them in
the experiments of “MIRA w/ L3 loss”. We found
that FrameNet based features provided 0.8% and
2.2% F1 gains for event trigger and argument labeling respectively. Joint relation-event features
also provided 0.6% F1 gain for relation extraction.
Remaining Challenges
Event trigger labeling remains a major bottleneck.
In addition to the sparsity problem, the remaining errors suggest to incorporate external world
knowledge. For example, some words act as triggers for some certain types of events only when
they appear together with some particular arguments:
• “Williams picked up the child again and this
time, threwAttack her out the window.”
The word “threw” is used as an Attack event
trigger because the Victim argument is a “child”.
• “Ellison to spend $10.3 billion to getMerge Org
his company.”
The common word “get” is
tagged as a trigger of Merge Org, because its
object is “company”.
likelihood
usingAttack those weapons goes up.”
The word “using” is used as an Attack event
trigger because the Instrument argument is
“weapons”.
Another challenge is to distinguish physical and
non-physical events. For example, in the sentence:
• “we are paying great attention to their ability to
defendAttack on the ground.”,
our system fails to extract “defend” as an Attack
trigger. In the training data, “defend” appears multiple times, but none of them is tagged as Attack.
For instance, in the sentence:
• “North Korea could do everything to defend itself. ”
“defend” is not an Attack trigger since it does not
relate to physical actions in a war. This challenge
calls for deeper understanding of the contexts.
Finally, some pronouns are used to refer to actual events. Event coreference is necessary to recognize them correctly. For example, in the following two sentences from the same document:
• “It’s important that people all over the world
know that we don’t believe in the warAttack.”,
• “Nobody questions whether thisAttack is right
“this” refers to “war” in its preceding contexts.
Without event coreference resolution, it is difﬁcult
to tag it as an Attack event trigger.
Conclusions
We presented the ﬁrst joint model that effectively
extracts entity mentions, relations and events
based on a uniﬁed representation:
information
Experiment results on ACE’05 corpus demonstrate that our approach outperforms
pipelined method, and improves event-argument
performance signiﬁcantly over the state-of-the-art.
In addition to the joint relation-event features, we
demonstrated positive impact of using FrameNet
to handle the sparsity problem in event trigger labeling.
Although our primary focus in this paper is information extraction in the ACE paradigm, we believe that our framework is general to improve
other tightly coupled extraction tasks by capturing
the inter-dependencies in the joint search space.
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments. This work was supported by
the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-
CTA), U.S. NSF CAREER Award under Grant
IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the Deep Exploration and Filtering
of Text (DEFT) Program, IBM Faculty Award,
Google Research Award, Disney Research Award
and RPI faculty start-up grant. The views and conclusions contained in this document are those of
the authors and should not be interpreted as representing the ofﬁcial policies, either expressed or
implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notation here on.