Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3719–3728
Brussels, Belgium, October 31 - November 4, 2018. c⃝2018 Association for Computational Linguistics
Pathologies of Neural Models Make Interpretations Difﬁcult
Shi Feng1 Eric Wallace1 Alvin Grissom II2 Mohit Iyyer3,4
Pedro Rodriguez1 Jordan Boyd-Graber1
1University of Maryland 2Ursinus College
3UMass Amherst 4Allen Institute for Artiﬁcial Intelligence
{shifeng,ewallac2,entilzha,jbg}@umiacs.umd.edu,
 , 
One way to interpret neural model predictions is to highlight the most important input features—for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for
NLP, a word’s importance is determined by
either input perturbation—measuring the decrease in model conﬁdence when that word is
removed—or by the gradient with respect to
that word.
To understand the limitations of
these methods, we use input reduction, which
iteratively removes the least important word
from the input. This exposes pathological behaviors of neural models: the remaining words
appear nonsensical to humans and are not the
ones determined as important by interpretation methods. As we conﬁrm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high conﬁdence. To explain these
counterintuitive results, we draw connections
to adversarial examples and conﬁdence calibration: pathological behaviors reveal difﬁculties in interpreting neural models trained with
maximum likelihood. To mitigate their deﬁciencies, we ﬁne-tune the models by encouraging high entropy outputs on reduced examples.
Fine-tuned models become more interpretable
under input reduction without accuracy loss on
regular examples.
Introduction
Many interpretation methods for neural networks
explain the model’s prediction as a counterfactual:
how does the prediction change when the input is
modiﬁed? Adversarial examples highlight the instability of neural network predictions by showing
how small perturbations to the input dramatically
change the output.
In 1899, John Jacob Astor IV invested
$100,000 for Tesla to further develop
and produce a new lighting system.
Instead, Tesla used the money to fund his
Colorado Springs experiments.
What did Tesla spend Astor’s money on ?
0.78 →0.91
Figure 1: SQUAD example from the validation set.
Given the original Context, the model makes the same
correct prediction (“Colorado Springs experiments”)
on the Reduced question as the Original, with even
higher conﬁdence. For humans, the reduced question,
“did”, is nonsensical.
A common, non-adversarial form of model interpretation is feature attribution:
features that
are crucial for predictions are highlighted in a
heatmap. One can measure a feature’s importance
by input perturbation. Given an input for text classiﬁcation, a word’s importance can be measured
by the difference in model conﬁdence before and
after that word is removed from the input—the
word is important if conﬁdence decreases signiﬁcantly. This is the leave-one-out method . Gradients can also measure feature importance; for example, a feature is inﬂuential to the
prediction if its gradient is a large positive value.
Both perturbation and gradient-based methods can
generate heatmaps, implying that the model’s prediction is highly inﬂuenced by the highlighted, important words.
Instead, we study how the model’s prediction is
inﬂuenced by the unimportant words. We use input reduction, a process that iteratively removes
the unimportant words from the input while maintaining the model’s prediction.
Intuitively, the
words remaining after input reduction should be
important for prediction.
Moreover, the words
should match the leave-one-out method’s selections, which closely align with human perception . However, rather than providing explanations of the
original prediction, our reduced examples more
closely resemble adversarial examples.
In Figure 1, the reduced input is meaningless to a human but retains the same model prediction with
high conﬁdence. Gradient-based input reduction
exposes pathological model behaviors that contradict what one expects based on existing interpretation methods.
In Section 2, we construct more of these counterintuitive examples by augmenting input reduction with beam search and experiment with three
tasks: SQUAD for reading comprehension, SNLI 
for textual entailment, and VQA for visual question answering.
Input reduction with beam search consistently reduces the
input sentence to very short lengths—often only
one or two words—without lowering model conﬁdence on its original prediction. The reduced examples appear nonsensical to humans, which we
verify with crowdsourced experiments.
In Section 3, we draw connections to adversarial examples and conﬁdence calibration; we explain why
the observed pathologies are a consequence of the
overconﬁdence of neural models. This elucidates
limitations of interpretation methods that rely on
model conﬁdence.
In Section 4, we encourage
high model uncertainty on reduced examples with
entropy regularization.
The pathological model
behavior under input reduction is mitigated, leading to more reasonable reduced examples.
Input Reduction
To explain model predictions using a set of important words, we must ﬁrst deﬁne importance. After deﬁning input perturbation and gradient-based
approximation, we describe input reduction with
these importance metrics. Input reduction drastically shortens inputs without causing the model
to change its prediction or signiﬁcantly decrease
its conﬁdence. Crowdsourced experiments con-
ﬁrm that reduced examples appear nonsensical to
humans: input reduction uncovers pathological
model behaviors.
Importance from Input Gradient
Ribeiro et al. and Li et al. de-
ﬁne importance by seeing how conﬁdence changes
when a feature is removed; a natural approximation is to use the gradient . We formally deﬁne these
importance metrics in natural language contexts
and introduce the efﬁcient gradient-based approximation. For each word in an input sentence, we
measure its importance by the change in the con-
ﬁdence of the original prediction when we remove
that word from the sentence. We switch the sign
so that when the conﬁdence decreases, the importance value is positive.
Formally, let x = ⟨x1, x2, . . . xn⟩denote the input sentence, f(y | x) the predicted probability of
label y, and y = argmaxy′ f(y′ | x) the original
predicted label. The importance is then
g(xi | x) = f(y | x) −f(y | x−i).
To calculate the importance of each word in a sentence with n words, we need n forward passes of
the model, each time with one of the words left
out. This is highly inefﬁcient, especially for longer
sentences.
Instead, we approximate the importance value with the input gradient. For each word
in the sentence, we calculate the dot product of
its word embedding and the gradient of the output
with respect to the embedding. The importance
of n words can thus be computed with a single
forward-backward pass. This gradient approximation has been used for various interpretation methods for natural language classiﬁcation models ; see Ebrahimi
et al. for further details on the derivation.
We use this approximation in all our experiments
as it selects the same words for removal as an exhaustive search (no approximation).
Removing Unimportant Words
Instead of looking at the words with high importance values—what interpretation methods commonly do—we take a complementary approach
and study how the model behaves when the supposedly unimportant words are removed.
Intuitively, the important words should remain after
the unimportant ones are removed.
Our input reduction process iteratively removes
the unimportant words. At each step, we remove
the word with the lowest importance value until the model changes its prediction. We experi-
ment with three popular datasets: SQUAD for reading comprehension,
SNLI for textual entailment, and VQA for visual
question answering.
We describe each of these
tasks and the model we use below, providing full
details in the Supplement.
In SQUAD, each example is a context paragraph and a question. The task is to predict a span
in the paragraph as the answer. We reduce only
the question while keeping the context paragraph
unchanged. The model we use is the DRQA Document Reader .
In SNLI, each example consists of two sentences: a premise and a hypothesis. The task is
to predict one of three relationships: entailment,
neutral, or contradiction. We reduce only the hypothesis while keeping the premise unchanged.
The model we use is Bilateral Multi-Perspective
Matching (BIMPM) .
In VQA, each example consists of an image
and a natural language question. We reduce only
the question while keeping the image unchanged.
The model we use is Show, Ask, Attend, and Answer .
During the iterative reduction process, we ensure that the prediction does not change (exact
same span for SQUAD); consequently, the model
accuracy on the reduced examples is identical to
the original. The predicted label is used for input
reduction and the ground-truth is never revealed.
We use the validation set for all three tasks.
Most reduced inputs are nonsensical to humans
(Figure 2) as they lack information for any reasonable human prediction. However, models make
conﬁdent predictions, at times even more conﬁdent than the original.
To ﬁnd the shortest possible reduced inputs
(potentially the most meaningless), we relax the
requirement of removing only the least important word and augment input reduction with beam
search. We limit the removal to the k least important words, where k is the beam size, and decrease
the beam size as the remaining input is shortened.1
We empirically select beam size ﬁve as it produces comparable results to larger beam sizes with
reasonable computation cost. The requirement of
maintaining model prediction is unchanged.
1We set beam size to max(1, min(k, L −3)) where k is
maximum beam size and L is the current length of the input
Well dressed man and woman dancing in
the street
Two man is dancing on the street
Contradiction
0.977 →0.706
What color is the ﬂower ?
0.827 →0.819
Figure 2: Examples of original and reduced inputs
where the models predict the same Answer. Reduced
shows the input after reduction. We remove words from
the hypothesis for SNLI, questions for SQUAD and
VQA. Given the nonsensical reduced inputs, humans
would not be able to provide the answer with high con-
ﬁdence, yet, the neural models do.
With beam search, input reduction ﬁnds extremely short reduced examples with little to no
decrease in the model’s conﬁdence on its original predictions.
Figure 3 compares the length
of input sentences before and after the reduction.
For all three tasks, we can often reduce the sentence to only one word. Figure 4 compares the
model’s conﬁdence on original and reduced inputs. On SQUAD and SNLI the conﬁdence decreases slightly, and on VQA the conﬁdence even
increases.
Humans Confused by Reduced Inputs
On the reduced examples, the models retain their
original predictions despite short input lengths.
The following experiments examine whether these
predictions are justiﬁed or pathological, based on
how humans react to the reduced inputs.
For each task, we sample 200 examples that are
correctly classiﬁed by the model and generate their
reduced examples. In the ﬁrst setting, we compare the human accuracy on original and reduced
examples. We recruit two groups of crowd workers and task them with textual entailment, reading
comprehension, or visual question answering. We
show one group the original inputs and the other
the reduced. Humans are no longer able to give
Mean Length
Example Length
Figure 3: Distribution of input sentence length before and after reduction. For all three tasks, the input is often
reduced to one or two words without changing the model’s prediction.
SQuAD Start
Confidence
Figure 4: Density distribution of model conﬁdence on
reduced inputs is similar to the original conﬁdence. In
SQUAD, we predict the beginning and the end of the
answer span, so we show the conﬁdence for both.
the correct answer, showing a signiﬁcant accuracy
loss on all three tasks (compare Original and Reduced in Table 1).
The second setting examines how random the
reduced examples appear to humans.
of the original examples, we generate a version
where words are randomly removed until the
length matches the one generated by input reduction. We present the original example along with
the two reduced examples and ask crowd workers their preference between the two reduced ones.
The workers’ choice is almost ﬁfty-ﬁfty (the vs.
Random in Table 1): the reduced examples appear
almost random to humans.
These results leave us with two puzzles: why
are the models highly conﬁdent on the nonsensical
reduced examples? And why, when the leave-oneout method selects important words that appear
reasonable to humans, the input reduction process
selects ones that are nonsensical?
vs. Random
Table 1: Human accuracy on Reduced examples drops
signiﬁcantly compared to the Original examples, however, model predictions are identical. The reduced examples also appear random to humans—they do not
prefer them over random inputs (vs. Random).
SQUAD, accuracy is reported using F1 scores, other
numbers are percentages. For SNLI, we report results
on the three classes separately: entailment (-E), neutral
(-N), and contradiction (-C).
Making Sense of Reduced Inputs
Having established the incongruity of our deﬁnition of importance vis-`a-vis human judgements,
we now investigate possible explanations for these
We explain why model conﬁdence can
empower methods such as leave-one-out to generate reasonable interpretations but also lead to
pathologies under input reduction. We attribute
these results to two issues of neural models.
Model Overconﬁdence
Neural models are overconﬁdent in their predictions .
One explanation for
overconﬁdence is overﬁtting: the model overﬁts
the negative log-likelihood loss during training by
learning to output low-entropy distributions over
classes. Neural models are also overconﬁdent on
examples outside the training data distribution. As
Goodfellow et al. observe for image classi-
ﬁcation, samples from pure noise can sometimes
trigger highly conﬁdent predictions.
These socalled rubbish examples are degenerate inputs that
a human would trivially classify as not belonging
to any class but for which the model predicts with
high conﬁdence. Goodfellow et al. argue
that the rubbish examples exist for the same reason that adversarial examples do: the surprising
linear nature of neural models. In short, the conﬁdence of a neural model is not a robust estimate of
its prediction uncertainty.
Our reduced inputs satisfy the deﬁnition of rubbish examples: humans have a hard time making
predictions based on the reduced inputs (Table 1),
but models make predictions with high conﬁdence
(Figure 4). Starting from a valid example, input
reduction transforms it into a rubbish example.
The nonsensical, almost random results are best
explained by looking at a complete reduction path
(Figure 5). In this example, the transition from
valid to rubbish happens immediately after the ﬁrst
step: following the removal of “Broncos”, humans
can no longer determine which team the question is asking about, but model conﬁdence remains
high. Not being able to lower its conﬁdence on
rubbish examples—as it is not trained to do so—
the model neglects “Broncos” and eventually the
process generates nonsensical results.
In this example, the leave-one-out method will
not highlight “Broncos”.
However, this is not
a failure of the interpretation method but of the
model itself.
The model assigns a low importance to “Broncos” in the ﬁrst step, causing it to be
removed—leave-one-out would be able to expose
this particular issue by not highlighting “Broncos”. However, in cases where a similar issue only
appear after a few unimportant words are removed,
the leave-one-out method would fail to expose the
unreasonable model behavior.
Input reduction can expose deeper issues of
model overconﬁdence and stress test a model’s uncertainty estimation and interpretability.
Second-order Sensitivity
So far, we have seen that the output of a neural
model is sensitive to small changes in its input. We
call this ﬁrst-order sensitivity, because interpretation based on input gradient is a ﬁrst-order Taylor
expansion of the model near the input .
However, the interpretation also
shifts drastically with small input changes (Figure 6). We call this second-order sensitivity.
The shifting heatmap suggests a mismatch between the model’s ﬁrst- and second-order sensi-
Context: The Panthers used the San Jose State practice facility and stayed
at the San Jose Marriott. The Broncos practiced at Stanford University and
stayed at the Santa Clara Marriott.
(0.90, 0.89) Where did the Broncos practice for the Super Bowl ?
(0.92, 0.88) Where did the practice for the Super Bowl ?
(0.91, 0.88) Where did practice for the Super Bowl ?
(0.92, 0.89) Where did practice the Super Bowl ?
(0.94, 0.90) Where did practice the Super ?
(0.93, 0.90) Where did practice Super ?
(0.40, 0.50) did practice Super ?
Figure 5: A reduction path for a SQUAD validation example. The model prediction is always correct and its
conﬁdence stays high (shown on the left in parentheses) throughout the reduction. Each line shows the input at that step with an underline indicating the word to
remove next. The question becomes unanswerable immediately after “Broncos” is removed in the ﬁrst step.
However, in the context of the original question, “Broncos” is the least important word according to the input
tivities. The heatmap shifts when, with respect to
the removed word, the model has low ﬁrst-order
sensitivity but high second-order sensitivity.
Similar issues complicate comparable interpretation methods for image classiﬁcation models.
For example, Ghorbani et al. modify image inputs so the highlighted features in the interpretation change while maintaining the same
prediction. To achieve this, they iteratively modify the input to maximize changes in the distribution of feature importance. In contrast, the shifting heatmap we observe occurs by only removing the least impactful features without a targeted
optimization. They also speculate that the steepest gradient direction for the ﬁrst- and secondorder sensitivity values are generally orthogonal.
Loosely speaking, the shifting heatmap suggests
that the direction of the smallest gradient value
can sometimes align with very steep changes in
second-order sensitivity.
When explaining individual model predictions,
the heatmap suggests that the prediction is made
based on a weighted combination of words, as
in a linear model, which is not true unless the
model is indeed taking a weighted sum such as
in a DAN . When the model
composes representations by a non-linear combination of words, a linear interpretation oblivious
to second-order sensitivity can be misleading.
Context: QuickBooks sponsored a “Small Business Big Game” contest,
in which Death Wish Coffee had a 30-second commercial aired free of
charge courtesy of QuickBooks. Death Wish Coffee beat out nine other
contenders from across the United States for the free advertisement.
What company won free advertisement due to QuickBooks contest ?
What company won free advertisement due to QuickBooks ?
What company won free advertisement due to ?
What company won free due to ?
What won free due to ?
What won due to ?
What won due to
What won due
Figure 6: Heatmap generated with leave-one-out shifts
drastically despite only removing the least important
word (underlined) at each step. For instance, “advertisement”, is the most important word in step two but
becomes the least important in step three.
Mitigating Model Pathologies
pathologies from the perspective of overconﬁdence: models are too certain on rubbish examples when they should not make any prediction.
Human experiments in Section 2.3 conﬁrm that
the reduced examples ﬁt the deﬁnition of rubbish
examples. Hence, a natural way to mitigate the
pathologies is to maximize model uncertainty on
the reduced examples.
Regularization on Reduced Inputs
To maximize model uncertainty on reduced examples, we use the entropy of the output distribution as an objective. Given a model f trained
on a dataset (X, Y), we generate reduced examples using input reduction for all training examples
X. Beam search often yields multiple reduced versions with the same minimum length for each input x, and we collect all of these versions together
X as the “negative” example set.
Let H (·) denote the entropy and f(y | x) denote
the probability of the model predicting y given x.
We ﬁne-tune the existing model to simultaneously
maximize the log-likelihood on regular examples
and the entropy on reduced examples:
(x,y)∈(X,Y)
log(f(y | x)) + λ
H (f(y | ˜x)) ,
where hyperparameter λ controls the trade-off between the two terms. Similar entropy regularization is used by Pereyra et al. , but not in
Reduced length
Table 2: Model Accuracy on regular validation examples remains largely unchanged after ﬁne-tuning.
However, the length of the reduced examples (Reduced
length) increases on all three tasks, making them less
likely to appear nonsensical to humans.
combination with input reduction; their entropy
term is calculated on regular examples rather than
reduced examples.
Regularization Mitigates Pathologies
On regular examples, entropy regularization does
no harm to model accuracy, with a slight increase
for SQUAD (Accuracy in Table 2).
After entropy regularization, input reduction
produces more reasonable reduced inputs (Figure 7). In the SQUAD example from Figure 1, the
reduced question changed from “did” to “spend
Astor money on ?” after ﬁne-tuning. The average
length of reduced examples also increases across
all tasks (Reduced length in Table 2). To verify
that model overconﬁdence is indeed mitigated—
that the reduced examples are less “rubbish” compared to before ﬁne-tuning—we repeat the human
experiments from Section 2.3.
Human accuracy increases across all three tasks
(Table 3). We also repeat the vs. Random experiment: we re-generate the random examples to
match the lengths of the new reduced examples
from input reduction, and ﬁnd humans now prefer the reduced examples to random ones. The increase in both human performance and preference
suggests that the reduced examples are more reasonable; model pathologies have been mitigated.
While these results are promising, it is not clear
whether our input reduction method is necessary
to achieve them. To provide a baseline, we ﬁnetune models using inputs randomly reduced to the
same lengths as the ones generated by input reduction. This baseline improves neither the model accuracy on regular examples nor interpretability under input reduction (judged by lengths of reduced
examples). Input reduction is effective in generating negative examples to counter model overcon-
In 1899, John Jacob Astor IV invested
$100,000 for Tesla to further develop
and produce a new lighting system.
Instead, Tesla used the money to fund his
Colorado Springs experiments.
What did Tesla spend Astor’s money on ?
Colorado Springs experiments
spend Astor money on ?
0.78 →0.91 →0.52
Well dressed man and woman dancing in
the street
Two man is dancing on the street
Contradiction
two man dancing
0.977 →0.706 →0.717
What color is the ﬂower ?
What color is ﬂower ?
0.847 →0.918 →0.745
Figure 7: SQUAD example from Figure 1, SNLI and
VQA (image omitted) examples from Figure 2. We apply input reduction to models both Before and After entropy regularization. The models still predict the same
Answer, but the reduced examples after ﬁne-tuning appear more reasonable to humans.
Discussion
Rubbish examples have been studied in the image
domain , but to our knowledge not for NLP. Our input reduction process gradually transforms a valid
input into a rubbish example. We can often determine which word’s removal causes the transition
to occur—for example, removing “Broncos” in
Figure 5. These rubbish examples are particularly
interesting, as they are also adversarial: the difference from a valid example is small, unlike image rubbish examples generated from pure noise
which are far outside the training data distribution.
The robustness of NLP models has been studied
extensively , and
most studies deﬁne adversarial examples similar
to the image domain: small perturbations to the
input lead to large changes in the output. Hot-
Flip uses a gradient-based
approach, similar to image adversarial examples,
to ﬂip the model prediction by perturbing a few
characters or words.
Our work and Belinkov
and Bisk both identify cases where noisy
vs. Random
Table 3: Human Accuracy increases after ﬁne-tuning
the models.
Humans also prefer gradient-based reduced examples over randomly reduced ones, indicating that the reduced examples are more meaningful to
humans after regularization.
user inputs become adversarial by accident: common misspellings break neural machine translation models; we show that incomplete user input
can lead to unreasonably high model conﬁdence.
Other failures of interpretation methods have
been explored in the image domain. The sensitivity issue of gradient-based interpretation methods, similar to our shifting heatmaps, are observed
by Ghorbani et al. and Kindermans et al.
They show that various forms of input
perturbation—from adversarial changes to simple
constant shifts in the image input—cause signiﬁcant changes in the interpretation. Ghorbani et al.
 make a similar observation about secondorder sensitivity, that “the fragility of interpretation is orthogonal to fragility of the prediction”.
Previous work studies biases in the annotation
process that lead to datasets easier than desired
or expected which eventually induce pathological
We attribute our observed pathologies
primarily to the lack of accurate uncertainty estimates in neural models trained with maximum
likelihood. SNLI hypotheses contain artifacts that
allow training a model without the premises ; we apply input reduction
at test time to the hypothesis.
Similarly, VQA
images are surprisingly unimportant for training
a model; we reduce the question.
The recent
SQUAD 2.0 augments the
original reading comprehension task with an uncertainty modeling requirement, the goal being to
make the task more realistic and challenging.
Section 3.1 explains the pathologies from the
overconﬁdence perspective. One explanation for
overconﬁdence is overﬁtting: Guo et al. 
show that, late in maximum likelihood training,
the model learns to minimize loss by outputting
low-entropy distributions without improving validation accuracy. To examine if overﬁtting can explain the input reduction results, we run input reduction using DRQA model checkpoints from every training epoch. Input reduction still achieves
similar results on earlier checkpoints, suggesting
that better convergence in maximum likelihood
training cannot ﬁx the issues by itself—we need
new training objectives with uncertainty estimation in mind.
Methods for Mitigating Pathologies
We use the reduced examples generated by input
reduction to regularize the model and improve its
interpretability. This resembles adversarial training , where adversarial examples are added to the training set to improve model robustness. The objectives are different: entropy regularization encourages high uncertainty on rubbish examples, while adversarial
training makes the model less sensitive to adversarial perturbations.
Pereyra et al. apply entropy regularization on regular examples from the start of training to improve model generalization. A similar
method is label smoothing .
In comparison, we ﬁne-tune a model with entropy
regularization on the reduced examples for better
uncertainty estimates and interpretations.
To mitigate overconﬁdence, Guo et al. 
propose post-hoc ﬁne-tuning a model’s conﬁdence
with Platt scaling. This method adjusts the softmax function’s temperature parameter using a
small held-out dataset to align conﬁdence with accuracy. However, because the output is calibrated
using the entire conﬁdence distribution, not individual values, this does not reduce overconﬁdence
on speciﬁc inputs, such as the reduced examples.
Generalizability of Findings
To highlight the erratic model predictions on short
examples and provide a more intuitive demonstration, we present paired-input tasks. On these tasks,
the short lengths of reduced questions and hypotheses obviously contradict the necessary number of words for a human prediction (further supported by our human studies). We also apply input
reduction to single-input tasks including sentiment
analysis and Quizbowl , achieving similar results.
Interestingly, the reduced examples transfer
to other architectures.
In particular,
we feed ﬁfty reduced SNLI inputs from each
class—generated with the BIMPM model —through the Decomposable Attention Model ,2 the same prediction is triggered 81.3% of the time.
Conclusion
We introduce input reduction, a process that iteratively removes unimportant words from an input while maintaining a model’s prediction. Combined with gradient-based importance estimates
often used for interpretations, we expose pathological behaviors of neural models. Without lowering
model conﬁdence on its original prediction, an input sentence can be reduced to the point where
it appears nonsensical, often consisting of one
or two words. Human accuracy degrades when
shown the reduced examples instead of the original, in contrast to neural models which maintain
their original predictions.
We explain these pathologies with known issues of neural models: overconﬁdence and sensitivity to small input changes. The nonsensical
reduced examples are caused by inaccurate uncertainty estimates—the model is not able to lower
its conﬁdence on inputs that do not belong to
any label. The second-order sensitivity is another
issue why gradient-based interpretation methods
may fail to align with human perception: a small
change in the input can cause, at the same time, a
minor change in the prediction but a large change
in the interpretation. Input reduction perturbs the
input multiple times and can expose deeper issues
of model overconﬁdence and oversensitivity that
other methods cannot. Therefore, it can be used to
stress test the interpretability of a model.
Finally, we ﬁne-tune the models by maximizing
entropy on reduced examples to mitigate the de-
ﬁciencies. This improves interpretability without
sacriﬁcing model accuracy on regular examples.
To properly interpret neural models, it is important to understand their fundamental characteristics: the nature of their decision surfaces, robustness against adversaries, and limitations of their
training objectives. We explain fundamental difﬁculties of interpretation due to pathologies in neural models trained with maximum likelihood. Our
2 
textual-entailment
work suggests several future directions to improve
interpretability: more thorough evaluation of interpretation methods, better uncertainty and con-
ﬁdence estimates, and interpretation beyond bagof-word heatmap.
Acknowledgments
subcontract
Raytheon BBN Technologies by DARPA award
HR0011-15-C-0113.
JBG is supported by NSF
Grant IIS1652666.
Any opinions,
conclusions, or recommendations expressed here
are those of the authors and do not necessarily
reﬂect the view of the sponsor. The authors would
like to thank Hal Daum´e III, Alexander M. Rush,
Nicolas Papernot, members of the CLIP lab at
the University of Maryland, and the anonymous
reviewers for their feedback.