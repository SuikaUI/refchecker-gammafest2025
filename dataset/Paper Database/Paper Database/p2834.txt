KERNEL METHODS MATCH DEEP NEURAL NETWORKS ON TIMIT
Po-Sen Huang†, Haim Avron‡, Tara N. Sainath‡, Vikas Sindhwani‡, Bhuvana Ramabhadran‡
†Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, USA
‡IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
 , {haimav, tsainath, vsindhw, bhuvana}@ibm.com
Despite their theoretical appeal and grounding in tractable convex
optimization techniques, kernel methods are often not the ﬁrst choice
for large-scale speech applications due to their signiﬁcant memory
requirements and computational expense. In recent years, randomized approximate feature maps have emerged as an elegant mechanism to scale-up kernel methods. Still, in practice, a large number
of random features is required to obtain acceptable accuracy in predictive tasks. In this paper, we develop two algorithmic schemes to
address this computational bottleneck in the context of kernel ridge
regression. The ﬁrst scheme is a specialized distributed block coordinate descent procedure that avoids the explicit materialization
of the feature space data matrix, while the second scheme gains ef-
ﬁciency by combining multiple weak random feature models in an
ensemble learning framework. We demonstrate that these schemes
enable kernel methods to match the performance of state of the art
Deep Neural Networks on TIMIT for speech recognition and classiﬁcation tasks. In particular, we obtain the best classiﬁcation error
rates reported on TIMIT using kernel methods.
Index Terms— large-scale kernel machines, random features,
distributed computing, deep learning, speech recognition
1. INTRODUCTION
The recent dramatic success of Deep Neural Networks (DNNs) in
speech recognition highlights the statistical beneﬁts of marrying highly non-linear and near-nonparametric models with large
datasets, with efﬁcient optimization algorithms running in distributed computing environments.
Deep learning models project
input data through several layers of nonlinearity and learn different
levels of abstraction. The composition of multiple layers of nonlinear functions can approximate a rich set of naturally occurring
input-output dependencies.
At the same time, the combinatorial
difﬁculty of performing exhaustive model selection in the discrete
space of DNN architectures and the potential to get trapped in local
minima are well recognized as valid concerns. In this paper, we
revisit kernel methods, considered “shallow” in the DNN sense, for
large-scale nonparameteric learning. We ask whether, despite their
shallow architecture and convex formulations, recent advances in
scaling-up kernel methods via randomized algorithms allow them to
match DNN performance.
Let X ⊆Rd be in input domain of acoustic features. With a
kernel function k : X ×X →R, there is an associated Reproducing
Kernel Hilbert Space of functions Hk, with inner product ⟨·, ·⟩Hk
and norm ∥· ∥Hk, in which model estimation can be performed by
minimizing a regularized objective function,
f ⋆= argminf∈Hk
V (yi, f(xi)) + λ∥f∥2
where {(xi, yi)}n
i=1 denotes a training set, λ is a regularization parameter and V (·, ·) denotes a loss function. In this paper, we work
with the squared loss V (y, t) = (y −t)2 function so that the model
estimation above is for kernel ridge regression, although all our techniques generalize to other loss functions. We also mainly work with
the Gaussian kernel, though the randomization techniques considered in this paper apply to a broader family of kernels. According to
the classical Representer Theorem , the minimizer for the above
problem has the form,
αik(x, xi) .
Plugging this formula back into the objective function with squared
loss yields a dense linear system for the coefﬁcients α:
(K + λI)α = y
where y is the vector of labels and K is the n × n Gram matrix
given by Kij = k(xi, xj). It is the O(n2) storage requirements
of the Gram matrix, the O(n3) computational expense of solving a
large dense linear system during training, and the need to evaluate in
O(nd) time the sum in (1) during testing (assuming that evaluation
of the kernel takes O(d) time) that make kernel methods in this form
rather unappealing for large datasets.
To handle the limitations in large-scale tasks, several approaches
have been proposed. One of the popular directions is to approximate
the kernel matrix by linearization, such as the Nystr¨om method 
and random Fourier features based methods . Although the
Nystr¨om method has shown better performance theoretically , it
involves more expensive computations and larger memory requirements for large-scale tasks. Hence, our focus in this paper is on
improving the efﬁciency of random Fourier features based methods.
A kernel function k(x, y), x, y
Rd can be associated
with a high dimensional inner product feature space H such that
k(x, y) = ⟨φ(x), φ(y)⟩H. One such feature space is Hk itself,
while another can be obtained through the Mercer’s Theorem. For
the Gaussian kernel, the dimensionality of these feature spaces is
inﬁnite and hence it is preferred to perform computations implicitly
using the kernel and its Gram matrix. However, this is the source of
increased computational complexity with respect to the number of
datapoints.
Instead of using the implicit feature mapping in the kernel trick,
Rahimi and Recht proposed a random feature method for approximating kernel evaluation . The idea is to explicitly map the data
to a Euclidean inner product space using a randomized feature map
z : Rd →RD such that the kernel evaluation can be approximated
by the Euclidean inner product between the transformed pair:
k(x, y) = ⟨φ(x), φ(y)⟩≈z(x)Tz(y)
This feature map is justiﬁed via Bochner’s theorem which associates each continuous shift-invariant kernel on Rd with a unique
probability density, from which a Monte-Carlo sampling procedure
deﬁnes a random feature map. In this construction, the Gaussian
kernel can be expressed as the expectation of z(x)Tz(y), where
z(x) = cos(ωTx + b), ω is drawn from a Gaussian distribution, b
is drawn from a uniform distribution on [0, π), and the cosine function is applied entry-wise. To obtain a lower-variance estimate, we
can concatenate D randomly chosen zω into a column vector z and
normalize each component by 1/
D. Therefore, the inner product
z(x)Tz(y) =
j=1 zωj(x)zωj(y) is a lower variance approximation of the kernel function k(x, y). With this approximation,
instead of solving (2), one can instead solve a D × D standard regularized linear regression problem,
(ZT Z + λI)w = ZT y
where Z is a feature space data matrix whose rows are z(xi), i =
1 . . . n. Predictions can be made with f(x) = z(x)Tw. The storage cost of materializing Z is O(nD); while the training and testing
complexity is O(nD2) and O(dD) respectively. These are much
more appealing due to linear scaling with respect to number of training points, provided D is small enough. More recent techniques 
offer similarly performing random Fourier feature maps, with faster
mapping, which translates into reducing the time needed for generating Z and faster predictions.
In practice, when the number of training examples increase and
the dimension of original features is large, the minimum random feature space dimensionality D⋆needed for competitive performance
tends to be very large, and that poses a scalability problem. As an
example, the TIMIT dataset needs D = 200, 000 to get performance
on par with state-of-the-art methods. In such settings, materializing
the corresponding feature space matrix Z requires several terabytes
of storage, leading to a large, dense least-squares regression problem, which likely needs an out-of-core solver to solve exactly.
This paper is motivated by the question of how to make randomized feature map techniques for estimating nonlinear kernel models
more scalable. Towards this objective, we propose two complimentary techniques:
• We describe a parallel least squares solver that is specialized for
kernel ridge regression with random Fourier feature approximations. Our solver does not materialize Z explicitly, but rather
computes blocks of the associated covariance matrix ZT Z in one
pass, followed by on-the-ﬂy calculations in a distributed block coordinate descent procedure. As a side beneﬁt of using an iterative
solver, we ﬁnd that with early-stopping, an explicit regularizer is
not needed. In practice, we therefore set λ = 0. This implies that
our approach only requires a single kernel parameter (the bandwidth of the Gaussian kernel). The parallelization details of our
least squares solver and its benchmarking in high-performance
computing environments will be described in more detail in a separate paper. Here, we demonstrate its value for speech recognition
• We develop an ensemble learning approach where multiple lowdimensional random Fourier models are combined together.
While the initial models focus on the entire dataset, the subsequent models focus on high-error areas of the input domain.
This enables more efﬁcient data ﬁtting.
Our results indicate that on TIMIT, the generalization performance of deep learning non-convex models can be matched by
“shallow” kernel machines with convex optimization using the proposed schemes. Informally, when we say “match” we mean that
the best generalization performance obtained on TIMIT with our
scalable kernel ridge regression approaches is very similar or better
than achieved with DNN (with MSE loss), with comparable number
of parameters. We do not report a training time comparison due to
differences in the underlying compute environment: our distributed
solvers for kernel-methods were trained on an IBM BlueGene/Q
Nodecard (32 nodes x 16 cores per node) while DNNs were trained
The remainder of this paper is organized as follows. Section 2
reviews some related work. Section 3 introduces the proposed algorithms: the ensemble kernel machines and the scalable solver. The
experimental setups, along with results, are described in Section 4.
Section 5 concludes the paper and discusses future work.
2. RELATED WORK
Cho and Saul proposed an arc-cosine kernel function, which
mimics the computation in a multilayer neural network. Cheng and
Kingsbury further applied the arc-cosine kernel to TIMIT speech
recognition task . The sub-sampling strategy in computing the
kernel functions with millions of training data, and the usage of
MFCC-LDA features lead to a worse result compared with deep neural networks. Huang et al. proposed a kernel deep convex network
 . The architecture consists of kernel ridge regression with
random Fourier approximation in each layer. The model is convex
in each layer, but is not convex overall. Furthermore, the dimensionality of random Fourier approximation is relatively small such that
the approximation error of the kernel machine is high.
3. PROPOSED METHODS
In this section, we consider the multivariate kernel ridge regression setting with a k-dimensional output space. We denote Y =
[y1 . . . yk] where Yij = 1 if the i-th training data has label j and
Yis = −1, s ̸= j otherwise. Correspondingly, the matrix of coefﬁcients is represented by W = [w1 . . . wk].
3.1. Ensemble of Kernel Machines
As mentioned in Section 1, when the number of training examples
increases and the original feature dimension is high, the dimensionality D⋆also needs to be high for competitive generalization performance approaching that of exact kernel methods. When the dimensionality is high, the memory usage and computation of the feature
map is also costly.
We can tradeoff speed and accuracy with feature maps with dimensionality D < D⋆. One may view such a random features model
as a weak learner, whose performance can be improved by generating an ensemble of multiple weak learners. Hence, we propose
to incorporate the idea of ensemble methods to aggregate the
models from different weak learners trained with D < D⋆random
features, as shown in Algorithm 1. Initially all training samples are
assigned with uniform weights. For each learner t, we ﬁrst sample
the training samples according to the weights. Given the sampled
training data Zt with labels Yt, we solve the least-squares problem
||ZtW −Yt||2
F . Then, each training sample is reweighted based on
its classiﬁcation result. Difﬁcult samples - with higher classiﬁcation
error - become more likely to be sampled for training the (t + 1)-th
Algorithm 1: A Proposed Ensemble Algorithm
1: Input: training data: X and target vector y
2: Initialize distribution: D1(i) ←1/n, i = 1, . . . , n
3: for each learner t = 1, . . . , T do
Sample N samples from X according to distribution Dt,
generating Xt.
Generate a random Fourier feature map, mapping Xt to Zt.
Solve minWt ∥ZtWt −Y ∥2
Prediction ˆyt ←ZtWt.
Compute the weighted training error:
i=1 Dt(i)(ˆyti ̸= yi)
Deﬁne αt = 1
Update Dt+1(i):
Dt+1(i) ←Dt(i)
if ˆyti = yi
if ˆyti ̸= yi
St is a normalization factor.
11: end for
12: Output: for each t = 1, . . . , T: Wt, αt, feature map
learner. For samples which were classiﬁed incorrectly and are classiﬁed correctly in learner t, the weights will be reduced. The ﬁnal
decision is made based on the prediction from learners t = 1, . . . , T,
weighted by the classiﬁcation error of each learner. This ensemble
mechanism may be seen as a non-uniform sampling approach where
the capacity of the overall model is distributed as per the hardness of
classiﬁcation.
3.2. Scalable Solver
We now discuss a more direct approach to handle very large random feature spaces, which in turn leads to solving very large leastsquares problems of the form minW ∥ZW −Y ∥2
F . Large number
of random features poses a serious scalability problem, if the least
squares solver is implemented naively. For example, consider the
TIMIT dataset dataset with 100, 000 random features. Just storing
the matrix Z in memory in double-precision requires over 1 terabyte
of data. Once in memory, one has to solve the dense regression
problem, which will take too long for a direct method. If terabyte
order memory is not available, one traditional approach is to resort
to slower out-of-core solution with costly disk I/O per iteration.
The key for a scalable solver is observing that the matrix Z is implicitly deﬁned by the input matrix X. To take advantage of this we
use a block coordinate descent algorithm. At each iteration block coordinate descent ﬁxes the part of W that corresponds to coordinates
(columns of Z) that are outside the current block, and optimizes the
part of W that corresponds to coordinates inside the current block
(i.e., a set of rows of W). This leads to solving the following leastsquares problem in each iteration
∆Wb ∥Zb∆Wb −R∥F
where Zb is the part of Z that corresponds to the coordinates in block
b (a set of columns of Z), ∆Wb is the update to Wb, the part of W
that corresponds to the coordinates in block b (a set of rows of W),
and R = ZW −Y is the current residual vector. The update is then
Wb ←Wb + ∆Wb.
As we see each iteration requires only a part of Z, and there
is no need to form the entire matrix. This is much more economical in memory use; our solver simply forms the relevant Zb in each
iteration, and solves (4).
Let sb denote the size of block b. To avoid paying O(ns2
each iteration, we form the Gram matrix ZT
b Zb and factor it only
during the ﬁrst epoch. Subsequent epochs require only O(nsb) per
iteration in addition to the time required to form Zb.
We use parallel processing to allow us to work on large datasets.
We designed our solver as a distributed-memory solver. We split the
input matrix X and right-hand-size row-wise, with each processor
receiving a subset of the rows of X and of the right-hand side. The
solver also updates a copy of R, which is distributed row-wise with
exactly the same split as X. All processors keep a synchronized copy
of W. The row-wise split enables us to take advantage of the fact
that mapping a row of X to a row of Z can be done independently of
other rows of X, so the distribution of X implicitly deﬁnes a distribution of Z and Zb. This enables the block coordinate descent to be
performed with little synchronization and communication overhead.
See Algorithm 2 for a pseudo-code description. Detailed benchmarking and enhancements of this solver in high-performance computing environments will be reported in a separate paper.
Algorithm 2: Scalable Distributed-Memory Solver
1: Input: training data: X and target Y
2: Distribute X and Y row-wise across machines;
3: Initialize: W ←0, R ←Y .
4: Generate a random Fourier feature map (maps X to Z).
5: while weights have not converged do
for each block b do
▷Zb denotes the columns of Z corresponding to block b.
Wb denotes the rows of W corresponding to block b.
Compute Zb
If (ﬁrst epoch): compute and factor ZT
Compute ZT
Wb ←Wb + ∆Wb
R ←R −Zb∆Wb
15: end while
16: Output: W, feature map
4. EXPERIMENTS
In this section, we examine the effectiveness of our approaches on
the TIMIT corpus for phone classiﬁcation and recognition tasks.
4.1. Corpus
Phone classiﬁcation and recognition experiments were evaluated on
the TIMIT corpus. The training set contains 3696 SI and SX sentences from 462 speakers. A separate development set of 50 speakers was used for hyperparameter tuning. Experiments are evaluated
on the core test set, which consists of 192 utterances, with 8 utterances from each of the 24 speakers. The SA sentences are excluded
from tests.
In all experiments, we use 40 dimensional feature space maximum likelihood linear regression (fMLLR) features and then
concatenate the neighboring 5 frames (11 frames in total) as the input
feature for the experiments, which is reported as the state-of-the-art
feature for deep neural network by Mohamed et al. . In this paper, we focus on training kernel machines with the mean square error
(MSE) objective and compare the results with DNNs using MSE and
cross entropy (CE) objectives. To have a fair comparison, we select
the best architecture (number of hidden units and number of layers)
for DNNs based on the development set. We report the number of
hidden units and the number of hidden layers for the selected DNNs
in Table 1, 2, and 3.
4.2. Classiﬁcation task
We ﬁrst look at the frame-level classiﬁcation results on the TIMIT
dataset. We use 147 (49 phonemes × 3 states) context independent
states as targets. The training data consists of 2 million frames of
fMLLR features, extracted using a 5 ms step size.
As mentioned in Section 1, the training and evaluation of an exact kernel ridge regression model is computationally constrained by
the number of training samples. Hence, in order to evaluate performance with an exact kernel method, given computational limitations,
we use a subset of 100,000 training samples (2 million frames). Table 1 shows the results. We can observe that the exact kernel achieves
the best results by ﬁtting the training data well. The error of random Fourier (RF) models decreases as the number of random features increases. By constructing an ensemble of several RF models,
the kernel machines are able to get closer to the full kernel performance. The performance of kernel machines is signiﬁcantly better
than DNN with MSE objectives.
Table 1. Classiﬁcation results (147 classes) on 100K samples, where
D is the number of random features
Model (D - T learners)
Training Error (%)
Test Error (%)
MSE-RF (5K - 1)
MSE-RF (5K - 30)
MSE-RF (10K - 1)
MSE-RF (10K - 30)
Model (hidden units - layers)
Training Error (%)
Test Error (%)
MSE-DNN (1K - 1)
MSE-DNN (1K - 3)
MSE-DNN (1K - 5)
Table 2 shows the frame-level classiﬁcation results on the full
training set. Here, the exact kernel method becomes computationally infeasible. By increasing the number of random features of RF
models using the scalable solver or combining several relatively lowdimensional RF models, the performance of MSE-RF is similar to
the best results by MSE-DNN. The DNN with CE objective, minimizing the classiﬁcation objectives directly, achieves the best results.
4.3. Recognition task
To convert the output of kernel machines for the hidden Markov
model (HMM) Viterbi decoding, we follow the hybrid approach in
 . We trained a weighted softmax function with the CE objective
for phone posterior p(q = i|x).
p(q = i | x) =
j aijpij + bi)
j akjpkj + bk),
where aij is a trainable weight for kernel output pij and bi is a trainable bias for phone i. The weights are trained using stochastic gradient descent. By Bayes rule, p(x|q = i) ∝p(q = i|x)/p(q = i),
the posteriors can be transformed to HMM state emission probabilities. The priors p(q = i) are estimated from the training data. Then,
Table 2. Classiﬁcation results (147 classes) on the full training set,
where D is the number of random features
Model (D - T learners)
Training Error (%)
Test Error (%)
MSE-RF (60K - 1)
MSE-RF (60K - 7)
Scal. MSE-RF (200K - 1)
Scal. MSE-RF (400K - 1)
Scal. MSE-RF (600K - 1)
Model (hidden units - layers)
Training Error (%)
Test Error (%)
MSE-DNN (4K - 2)
CE-DNN (1K - 3)
Table 3. Recognition results, where D is the number of random
Model (hidden units - layers)
Test CI Error (%)
Test PER (%)
MSE-DNN (4K - 2)
MSE-DNN (2K - 2)
CE-DNN (1K - 3)
CE-DNN (4K - 3)
Model (D - T learners)
Test CI Error (%)
Test PER (%)
Scal. MSE-RF (400K - 1)
Viterbi decoding is used for phone recognition. Table 3 shows the
recognition results on TIMIT. The MSE-RF achieves better performance compared to MSE-DNN and slightly worse performance than
CE-DNN. Note that the DNNs chosen are the best performing models in terms of the number of hidden units and number of layers on
the development set.
5. CONCLUSION
In this paper, we explore the comparison between deep learning
models and shallow kernel machines. By using an ensemble approach and a complimentary scalable solver, we show that the model
expressibility of kernel machines can match deep neural network
models, for very similar generalization performance. Furthermore,
kernel machines have the advantages of having a convex optimization based formulation and simpler model selection. To improve the
phone classiﬁcation and recognition performance, optimizing kernel machines with the CE objective can be further explored, as an
alternative to MSE-based models explored in this paper. Another
promising direction is the use of Quasi-Monte Carlo techniques to
improve the efﬁciency of explicit feature map approximations along
the lines proposed in .
6. ACKNOWLEDGMENT
Thanks to Ewout van den Berg for help with TIMIT experiments.
H. Avron and V. Sindhwani acknowledge the support of
the XDATA program of the Defense Advanced Research Projects
Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C-0323. This work was done while P.-S.
Huang was a summer intern at IBM Research.
7. REFERENCES
 G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury, “Deep neural networks for acoustic modeling in speech
recognition,” IEEE Signal Processing Magazine, vol. 29, pp.
82–97, Nov. 2012.
 B. Sch¨olkopf and A. J. Smola, Learning with kernels: support vector machines, regularization, optimization, and beyond, MIT press, 2001.
 P. Drineas and M. W. Mahoney, “On the Nystr¨om method for
approximating a gram matrix for improved kernel-based learning,” Journal of Machine Learning Research, vol. 6, pp. 2153–
2175, Dec. 2005.
 S. Kumar, M. Mohri, and A. Talwalkar, “Ensemble Nystr¨om
method,” in Advances in Neural Information Processing Systems (NIPS), 2009.
 A. Rahimi and B. Recht, “Random features for large-scale kernel machines,” in Advances in Neural Information Processing
Systems (NIPS), 2007.
 Q. Le, T. Sarl´os, and A. Smola, “Fastfood – Approximating
kernel expansions in loglinear time,” in International Conference on Machine Learning (ICML), 2013.
 T. Yang, Y.-F. Li, M. Mahdavi, R. Jin, and Z.-H. Zhou,
“Nystr¨om method vs random fourier features: A theoretical
and empirical comparison,” in Advances in Neural Information Processing Systems (NIPS), 2012, pp. 485–493.
 Y. Cho and L. K. Saul, “Kernel methods for deep learning,” in
Advances in Neural Information Processing Systems (NIPS),
 C.-C. Cheng and B. Kingsbury, “Arccosine kernels: Acoustic modeling with inﬁnite neural networks,” in IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 2011, pp. 5200–5203.
 L. Deng, G. Tur, X. He, and D. Hakkani-Tur, “Use of kernel
deep convex networks and end-to-end learning for spoken language understanding,” in IEEE Workshop on Spoken Language
Technology, 2012.
 P.-S. Huang, L. Deng, M. Hasegawa-Johnson, and X. He,
“Random features for kernel deep convex network,” in IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.
 H. Xia and S. C.H. Hoi, “Mkboost: A framework of multiple
kernel boosting,” IEEE Transactions on Knowledge and Data
Engineering, vol. 25, no. 7, pp. 1574–1586, 2013.
 M.J.F. Gales, “Maximum likelihood linear transformations for
hmm-based speech recognition,”
Computer Speech & Language, vol. 12, no. 2, pp. 75 – 98, 1998.
 A. Mohamed, T. N. Sainath, G. Dahl, B. Ramabhadran, G. Hinton, and M. Picheny,
“Deep belief networks using discriminative features for phone recognition,”
in IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), May 2011, pp. 5060–5063.
 J. Yang, V. Sindhwani, H. Avron, and M. Mahoney, “Quasi-
Monte Carlo feature maps for shift-invariant kernels,” in International Conference on Machine Learning (ICML), 2014.