Graph Convolutional Networks with
Argument-Aware Pooling for Event Detection
Thien Huu Nguyen
Department of Computer and Information Science
University of Oregon
Eugene, Oregon 97403, USA
 
Ralph Grishman
Computer Science Department
New York University
New York, NY 10003 USA
 
The current neural network models for event detection have
only considered the sequential representation of sentences.
Syntactic representations have not been explored in this area
although they provide an effective mechanism to directly link
words to their informative context for event detection in the
sentences. In this work, we investigate a convolutional neural
network based on dependency trees to perform event detection. We propose a novel pooling method that relies on entity mentions to aggregate the convolution vectors. The extensive experiments demonstrate the beneﬁts of the dependencybased convolutional neural networks and the entity mentionbased pooling method for event detection. We achieve the
state-of-the-art performance on widely used datasets with
both perfect and predicted entity mentions.
Introduction
Event Detection (ED) is an important information extraction
task of natural language processing that seeks to recognize
instances of speciﬁed types of events (event mentions) in
text. Each event mention is often presented within a single
sentence in which an event trigger is selected to associate
with that event mention. Event triggers are generally single verbs or nominalizations that serve as the main words
to evoke the corresponding events. The event detection task,
more precisely stated, aims to detect event triggers and classify them into speciﬁc types of interest. For instance, consider the following sentence with two words “ﬁred”:
“The police ofﬁcer who ﬁred into a car full of teenagers was
ﬁred Tuesday”
In this example, an ED system should be able to realize that
the ﬁrst occurrence of “ﬁred” is an event trigger of type Attack while the second “ﬁred” takes End-Position as its event
type. ED is a challenging task, as an expression might evoke
different events depending on contexts (illustrated in our
previous example with the word “ﬁred”), and the same event
might be presented in various expressions (e.g, the trigger
words “killed”, “shot” or “beat for the event type Attack).
The current state-of-the-art approach for ED employs
deep learning models in which convolutional neural networks (CNN) are the typical architectures . All rights reserved.
2016b). In the basic implementation, CNNs apply the temporal convolution operation over the consecutive k-grams1
in the sentences, attempting to generate the latent structures
that are informative for ED . The disadvantage of such consecutive
convolution is the inability to capture the non-consecutive
k-grams that can span words far apart in the sentences.
Those non-consecutive k-grams are necessary to recognize event triggers in some situations. For example, in the
example above, the non-consecutive 3-grams “ofﬁcer was
ﬁred” should be considered to correctly identify the event
type End-Position for the second word “ﬁred”. The nonconsecutive CNN model (NCNN) in seeks to overcome this problem by operating the
temporal convolution over all the non-consecutive k-grams
in the sentences, leading to the state-of-the-art CNN model
Unfortunately, due to the consideration of all possible
non-consecutive k-grams, the non-consecutive CNN architecture might model unnecessary and noisy information, potentially impairing the prediction performance for ED. In
particular, NCNN utilizes the max-pooling operation to aggregate the convolution scores over all the non-consecutive
k-grams. As such k grams might include irrelevant or misleading sequences of words, the max-pooling might incorrectly focus on those k-grams and make a wrong ﬁnal prediction for ED. One example is the non-consecutive 3-gram
“car was ﬁred” in the example above. In contrast to the correct 3-gram “ofﬁcer was ﬁred”, “car was ﬁred” suggests the
event type Attack for the second word “ﬁred”, causing the
confusion or failure of NCNN to predict the true event type
of “End-Position” in this situation.
One way to circumvent this issue for NCNN is to notice
that “police ofﬁcer” is the subject of the second word “ﬁred”
while “a car” does not have much direct connection with
the second “ﬁred” in this example. Guided by this intuition,
in this paper, we propose to perform the convolution operation over the syntactic dependency graphs of the sentences to
perform event detection. Syntactic dependency graphs represents sentences as directed trees with head-modiﬁer dependency arcs between related words . Each word in such
1k is often chosen to be some ﬁxed value.
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
teenagers was
I-PER B-PER
Figure 1: BIO annotation for entity mentions and dependency parse tree using universal dependency relations for the example
sentence. The label “B-X” for entity mentions indicates the beginning of an entity mention of type “X” while “I-X” is used for
tokens that are inside (but do not start) the range of an entity mention of type “X”. The label “O” is reserved for other tokens
that do not belong to any entity mentions. In this ﬁgure, “PER” and “VEH” stands for PERSON and VEHICLE respectively.
graphs is surrounded by its direct syntactic governor and
dependent words (the neighbors), over which the convolution can focus on the most relevant words for the current word and avoid the modeling of unrelated words/kgrams. In the experiments, we demonstrate that these syntactic connections for words provide effective constraints
to implement convolution for ED. Note that the governor
and dependent words has also been found as useful features for ED in the traditional feature approaches . This further helps to justify the convolution over dependency graphs for ED in this paper. The
dependency parse tree for the previous example sentence is
shown in Figure 1. As we can see from this ﬁgure, the dependency tree helps to link the second word “ﬁred” directly
to the dependent words “ofﬁcer” and “was” that altogether
constitute an effective evidence to predict the event type for
“ﬁred” via convolution.
In order to implement the syntactic convolution, we employ the graph convolutional networks (GCNs) that are studied very recently to use graph structures
to form connections between layers of multilayer neural networks. In GCNs, the convolution vector for each node is
computed from the representation vectors of the immediate neighbors. GCNs has been mainly applied for the node
classiﬁcation tasks in which the convolution representation
vector for a node functions as the only features to classify
that node . For event detection, we can also utilize the graphbased convolution vector of the current word (the current
node in the dependency graphs) to perform prediction. Unfortunately, as the convolution vector tends to preserve only
the most important information of the local context for the
current word (i.e, the immediate neighbors in the dependency graph), it might not have the capacity to encode the
speciﬁc (detailed) information about the entity mentions distributed at different positions in the sentences. An entity
mention is a reference to an object or a set of objects in
the world, including names, nominals and pronouns such
as the entity mentions “police ofﬁcer”, “car”, “teenagers”
and “Tuesday” in the example sentence above2. The spe-
2For convenience, we also consider time and value expressions
ciﬁc knowledge about entity mentions (e.g, entity types),
especially the participants (arguments) of the events, is important as it might provide models with more conﬁdence
to make prediction for ED . For instance, the ﬁrst and the second words
“ﬁred” in the example sentence might be aware of the entity
mentions (arguments) “car” and “ofﬁcer” in their syntactic
context respectively; however, the types of such entity mentions (i.e, VEHICLE for “car” and PERSON for “ofﬁcer”)
might not be encapsulated or be less pronounced in the convolution vectors for the two words “ﬁred” due to the local
attention. These entity types are crucial to accurately predict
the event types for the two words “ﬁred” in this case.
In this work, we propose to overcome this issue by operating a pooling over the graph-based convolution vectors of
the current word as well as the entity mentions in the sentences. This aggregates the convolution vectors to generate
a single vector representation for event type prediction. The
rationale is to explicitly model the information from the entity mentions to improve the performance for ED. We extensively evaluate the proposed pooling method with both
manually annotated (perfect) entity mentions and automatically predicted entity mentions to demonstrate its beneﬁt in
the experiments.
To summary, our contribution in this work is as follows:
• We are the ﬁrst to integrate syntax into neural event detection and show that GCNs are effective for ED.
• We propose a novel pooling method based on entity mentions for ED.
• We achieve the state-of-the-art performance on the widely
used datasets for ED using the proposed model with
GCNs and entity mention-based pooling.
Event detection can be cast as a multi-class classiﬁcation
problem . Each word
in the document is associated with the sentence containing
the word (the context) to form an event trigger candidate or
an example in the multi-class classiﬁcation terms. Our task
as entity mentions in this work.
is to predict the event label for every event trigger candidate
in the document. The label can be one of the pre-deﬁned
event types (subtypes) in the datasets or NONE to indicate a
non-trigger candidate. Consequently, we have an equivalent
problem of (L+1)-class classiﬁcation for ED where L is the
number of pre-deﬁned event types.
Let w = w1, w2, . . . , wn be a sentence of length n of
some event trigger candidate, in which wa (1 ≤a ≤n) is
the current word for trigger prediction (wi is the i-th token
in the sentence ∀1 ≤i ≤n). In addition, as we assume the
availability of the entity mentions (i.e, the positions and the
types) in w, we can utilize the BIO annotation scheme to
assign the entity type label ei to each token wi of w using
the non-overlapping heads (the most important tokens) of
the entity mentions. This results in the sequence of entity
type labels e1, e2, . . . , en for w, demonstrated in Figure 1
for the example sentence. Note that in such a scheme, ei ̸=
O implies that wi is within the range of an entity mention in
The graph convolutional networks for ED in this work
consists of three modules: (i) the encoding module that represents the input sentence with a matrix for GCN computation, (ii) the convolution module that performs the convolution operation over the dependency graph structure of w for
each token in the sentence, and (iii) the pooling module that
aggregates the convolution vectors based on the positions of
the entity mentions in the sentence to perform ED.
1. Encoding
In the encoding module, each token wi in the input sentence
is transformed into a real-valued vector xi by concatenating
the following vectors:
• The word embedding vector of wi: This is a real-valued
vector that captures the hidden syntactic and semantic
properties of wi . Word embeddings
are often pre-trained on some large unlabeled corpus
 .
• The position embedding vector of wi: In order to indicate that wa is the current word, we encode the relative distance from wi to wa (i.e, i −a) as a realvalued vector (called as the position embedding vector)
and use this vector as an additional representation of
wi. We obtain the position embedding vector by looking up the position embedding table that maps the possible values of the relative positions (i.e, i −a) into randomly initialized vectors .
• The entity type embedding vector of wi: Similar to the
position embeddings, we maintain a table of entity type
embeddings that maps entity type labels of tokens (i.e,
the BIO labels for entity mentions) to real-valued random
vectors. We look up this table for the entity type label ei
of wi to retrieve the corresponding embedding.
As each token wi is represented by the vector xi with dimensionality of d0, the input sentence w can be seen as a
sequence of vectors X = x1, x2, . . . , xn. X would be used
as input for the graph convolution module in the next step.
2. Graph Convolution
Let G = {V, E} be the dependency parse tree for w with V
and E as the sets of nodes and edges of G respectively. V contains n nodes corresponding to the n tokens w1, w2, . . . , wn
in w. For convenience, we also use wi to denote the i-th node
in V : V = {w1, w2, . . . , wn}. Each edge (wi, wj) in E is directed from the head word wi to the dependent word wj and
has the dependency label L(wi, wj). For instance, in the dependency tree of Figure 1, there is a directed edge from the
node for the second word wi = “ﬁred” (the head word) to the
node for the word wj = “ofﬁcer” (the dependent word) with
the edge label L(wi, wj) = L(“ﬁred”, “ofﬁcer”) = nsubjpass.
In order to allow the convolution for each token wi in G
to involve the word wi itself as well as its governor word (if
any) in the dependency graph, we add the self loops (wi, wi)
and the inverse edges (wj, wi) ((wi, wj) ∈E) into the initial
edge sets E, resulting in a new set of edges E′ :
E′ = E∪{(wi, wi) : 1 ≤i ≤n}∪{(wj, wi) : (wi, wj) ∈E}
Note that the additional edges of E′ are also directed and
labeled. The label for the self loops is a special symbol
“LOOP” while the label for the inverse edge (wj, wi) involves the label of the corresponding original edge (wi, wj)
followed by an apostrophe to emphasize the opposite direction with respect to the original edge (wi, wj) in G:
L(wi, wi) = LOOP ∀1 ≤i ≤n
L(wj, wi) = L′(wi, wj) ∀(wi, wj) ∈E
The new edge set E′ along with the node set V constitute a new graph G′ = {V, E′} on which the convolution operation can rely. In particular, the graph convolution
vector hk+1
at the (k + 1)-th layer (k ≥0) for a node
u ∈G′ (corresponding to a word wi in the input sentence
w : u = wi ∈{w1, w2, . . . , wn}) is computed by:
where N(u) is the set of neighbors of u in G′: N(u) = {v :
(u, v) ∈E′}; W k
L(u,v) ∈Rdk+1×dk and bk
L(u,v) ∈Rdk+1 are
the weight matrix and the bias (respectively) for the edge
(u, v) in G′ (dk is the number of hidden units or the dimensionality of hk
u in the k-th layer); and g is a nonlinear
activation function3. For convenience, we assume the same
number of hidden units for all the graph convolution layers
in this work (i.e, d1 = d2 = . . . = d). Note that the initial vectors h0
u are set to the representation vectors obtained
from the encoding module:
wi = xi ∀u ∈V
Limiting the capacity
The convolution in Equation (1) assumes different weight matrices W k
L(u,v) for different edge
labels L(u, v). The capacity of such parameters might be too
3g is the rectify function g(x) = max(0, x) in this paper.
high, given that the datasets for ED often have moderate size
with respect to the deep learning perspectives. In order to reduce the capacity, following ,
we only use three different weight matrices W k
L(u,v) in each
layer depending on whether the corresponding edge (u, v) is
an original edge in E, a self loop or an added inverse edge in
L(u,v) = W k
where “type(u, v)” returns the type of the edge (u, v) (i.e,
original edges, self loops and inverse edges).
Weighting the edges
The second word “ﬁred” in the example sentence of Figure 1 has three immediate neighbors
in the dependency graph: “ofﬁcer”, “was” and “Tuesday”.
While “ofﬁcer” and “was” are crucial to determine the event
type of End-Position for “ﬁred”, “Tuesday” do not contribute
much information in this case. It is thus not appropriate to
weight the neighbors uniformly in the graph convolution for
ED. Consequently, for the k-th layer, we compute a weight
(u,v) for each neighboring edge (u, v) of a node u ∈V to
quantify its importance for ED in GCNs :
(u,v) = σ(hk
type(u,v) + ¯bk
where ¯W k
type(u,v) ∈Rdk and ¯bk
L(u,v) ∈R are weight matrix
and the bias (respectively); and σ is a nonlinear activation
function4.
The edge weights in Equation (4) and the weight matrices in Equation (3) transform the convolution operation in
Equation (1) into:
type(u,v)hk
Note that edge weighting also helps to alleviate the effect of
the potentially wrong syntactic edges that are automatically
predicted by imperfect syntactic parsers.
Abstracting the initial representation with LSTM
graph convolution induces a hidden representation for the
local graph context of each node (word) in V (i.e, the word
itself, the governor and the dependents), functioning as features for ED. The hidden representations of a single layer of
GCNs can only capture the information for the immediate
neighbors while those of multiple layers (e.g, K layers) can
incorporate nodes (words) that are at most K hops aways in
the dependency tree. In other words, the context coverage of
the representation vectors for the nodes is restricted by the
number of convolution layers, causing the inability of the
representation vectors to encode the dependencies between
words far away from each other in the dependency graph. Increasing the number of convolution layers might help to mitigate this problem, but it might fail to capture the word dependencies with shorter distances due to the redundant modeling of context. It is thus preferable to have a mechanism
to adaptively accumulate the context rather than ﬁxing the
4The sigmoid function in this case.
context coverage with K layers in the current formulation
of GCNs. In this work, we employ a bidirectional long-short
term memory network (BiLSTM) to ﬁrst abstract the initial representation vectors
xi whose outputs are later consumed by GCNs for ED.
Speciﬁcally, we run a forward LSTM and a backward LSTM over the representation vector sequence
(x1, x2, . . . , xn)
backward hidden vector sequences (i.e, (−→
r2, . . . , −→
r2, . . . , ←−
rn) respectively). We then concatenate the
hidden vectors at the corresponding positions to obtain
the abstract representation vector sequence (r1, r2, . . . , rn)
ri ]. The new representation vectors
r1, r2, . . . , rn would then replace the initial vector sequences x1, x2, . . . , xn in Equation (2) for further computation of GCNs in Equations (4) and (5):
wi = ri ∀u ∈V
The convolution of GCNs over these new representation
vectors would allow the adaptive integration of long-range
dependencies of words with fewer convolution layers in
GCNs .
3. Pooling
convolution
produces the sequence of convolution representation vectors
w2, . . . , hK
wn. The role of the pooling module is to aggregate such convolution vectors to generate a single vector
representation vED, that would be fed into a standard feedforward neural network with softmax in the end to perform
classiﬁcation for ED.
There are several methods to aggregate the convolution
vectors for ED in the literature. In this section, we ﬁrst review these methods to emphasize the entity mention-based
pooling in this paper.
• Anchor Pooling (ANCHOR): In this case, vED is set to the
convolution vector of the current word: vED = hK
method is used in 
and most work on GCNs so far .
• Overall Pooling (OVERALL): vED is computed by
taking the element-wise max over the entire convolution vector sequence hK
w2, . . . , hK
max element-wise(hK
w2, . . . , hK
wn). This methods is
employed in .
• Dynamic Pooling (DYNAMIC) : The convolution vector sequence is divided into
two parts based on the position of the current word
w2, . . . , hK
wa) and (hK
wa+2, . . . , hK
These two subsequences are then aggregated by an
element-wise max operation whose outputs are concatenated to generate vED:
vED = [max element-wise(hK
w2, . . . , hK
max element-wise(hK
wa+2, . . . , hK
The common limitation of these methods is the failure to
explicitly model the convolution representation vectors for
the entity mentions in the sentence. Such representation vectors are helpful as they encode speciﬁc information for the
entity mentions that might help to improve the ED performance. In particular, ANCHOR ignores the representation
vectors for the entity mentions while OVERALL and DY-
NAMIC consider both the entity mentions’ representations
and the others uniformly in vED, potentially rejecting the
representation vectors of the entity mentions if the representation vectors from the other words in the sentence accidentally receive higher values. In this paper, we propose to
exclusively rely on the representation vectors of the entity
mentions to perform the pooling operation for ED. To be
more speciﬁc, the representation vector vED in this entity
mention-based pooling (called ENTITY) is computed by:
vED =max element-wise({hK
wi : 1 ≤i ≤n, ei ̸= O})
To summarize, the proposed model for ED in this paper
works in the following order:
1. Initial encoding with word embeddings, position embeddings and entity type embeddings
2. Abstracting the initial encoding with bidirectional LSTM
3. Performing convolution over the dependency trees using
the BiLSTM representation (Equation 5)
4. Pooling over the convolution vector based on the positions
of the entity mentions (Equation 7)
5. Feed-forward neural networks with softmax for prediction
In order to train the networks, following the previous work
on ED , we minimize the negative log-likelihood on the training dataset using stochastic gradient descent with shufﬂed mini-batches
and the AdaDelta update rule. The gradients are computed
via back-propagation while dropout is employed to avoid
overﬁtting. We also rescale the weights whose l2-norms exceed a predeﬁned threshold.
Experiments
1. Datasets and Settings
We evaluate the networks in this paper using the widely used
datasets for ED, i.e, the ACE 2005 dataset and the TAC
KBP 2015 dataset. We employ the ACE 2005 dataset in the
setting with golden (perfect) annotation for entity mentions
as do the prior work . TAC KBP 2015, on the other hand, is exploited to test the networks for the setting with predicted entity mentions ). Although the predicted entity mentions might involve some errors, it is a more realistic setting
as we usually do not have the golden entity mentions for the
datasets in practice.
The ACE 2005 dataset annotate 33 event subtypes that,
along with, the NONE class, function as the pre-deﬁned label set for a 34-class classiﬁcation problem for this dataset.
In order to ensure a compatible comparison with the previous work on this dataset , we use the same data split with 40 newswire
articles for the test set, 30 other documents for the development set and 529 remaining documents for the training set.
The TAC KBP 2015 dataset is the ofﬁcial evaluation data
from the Event Nugget Detection Evaluation of the 2015
Text Analysis Conference (TAC). It has 38 event subtypes,
thus requiring a 39-class classiﬁcation problem with the
“NONE” class for ED. We use the ofﬁcial data split provided by the 2015 Event Nugget Detection, including 360
documents for the training dataset and 202 documents for
the test dataset.
2. Parameters, Resources and Settings
The parameters are tuned on the development data of the
ACE 2005 dataset. The selected values for the parameters include the mini-batch size = 50, the pre-deﬁned threshold for
the l2 norms = 3, the dropout rate = 0.5, the dimensionality
of the position embeddings and the entity type embeddings
= 50 and the number of hidden units for the convolution layers d = 300. We employ the pre-trained word embeddings
with 300 dimensions from to initialize
the word embeddings. These parameters and resources are
used for both datasets in this paper.
In order to parse the sentences in the datasets, we employ
the Stanford Syntactic Parser with the universal dependency
relations. Following the previous work , we utilize a ﬁxed length
n = 31 of sentences in the experiments5. This implies that
we need to pad the shorter sentences with a special character or trim the longer sentences to ﬁt the ﬁxed length of n.
While the syntactic edges in the dependency trees for the
short sentences can be preserved, we remove the syntactic
edges that are linked to at least one trimmed word for the
longer sentences.
3. Evaluating Network Architectures
This section evaluates different model architectures to
demonstrate the effectiveness of GCNs and BiLSTM for
GCNs. In particular, we compare the proposed model with
its corresponding versions where the GCNs layers or the
BiLSTM layers are excluded. For the versions with GCN
layers, we incrementally increase the number of graph-based
convolution layers (ie. K) until the performance drops. Table 1 reports the performance of the models (Precision (P),
Recall (R), and F-measure (F1)) on the development portion of the ACE 2005 dataset. Note that the experiments in
this section use the proposed pooling mechanism (i.e, entity
mention-based pooling ENTITY).
There are three blocks in this table. The ﬁrst block corresponds to the full proposed models (i.e, BiLSTM + GCNs);
the second block amounts to the proposed model excluding
the BiLSTM layers (i.e, GCNs (no BiLSTM)); and the third
block shows the performance of the proposed model when
the GCN layers are not included (i.e., only using BiLSTM
5This is also the best value on the development data in our case.
BiLSTM + GCNs (K = 1)
BiLSTM + GCNs (K = 2)
BiLSTM + GCNs (K = 3)
GCNs (no BiLSTM) (K = 1)
GCNs (no BiLSTM) (K = 2)
GCNs (no BiLSTM) (K = 3)
Table 1: Model performance on the ACE 2005 development
dataset for the ENTITY pooling method.
layers). Importantly, we optimize the number of BiLSTM
layers in this experiment6 to measure the actual contribution of GCNs
for ED in the presence of BiLSTM more accurately.
The table indicates that both the proposed model and the
proposed model without BiLSTM (i.e, blocks 1 and 2 respectively) achieve the best performance when the number
of GCN layers is 2. The best performance of the former (i.e,
the full proposed model “BiLSTM + GCNs (K = 2)” with
F1 score of 71.8%) is better than the best performance of the
latter (i.e, the full proposed excluding BiLSTM “GCNs (no
BiLSTM) (K = 2)” with F1 score of 71.2%). Consequently,
BiLSTM captures some useful dependencies for ED that are
not encoded in GCNs. Thus, BiLSTM is complementary to
GCNs for ED and the utilization of BiLSTM with GCNs
would further improve the performance for GCNs. However,
as BiLSTM only adds 0.6% (i.e, from 71.2% to 71.8%) into
the performance of GCNs, most of the necessary information for ED has been captured by GCNs themselves. More
importantly, comparing the proposed model (i.e, BiLSTM +
GCNs (K = 2) in block 1 of the table) with the BiLSTM
model in block 3, we see that GCNs signiﬁcantly improve
the performance of BiLSTM (i.e, from 70.5% to 71.8%),
thus demonstrating the effectiveness of GCNs for ED.
In the following experiments, we would always use the
best network architecture for the proposed model discovered
in this section, i.e, BiLSTM + GCNs (K = 2).
4. Evaluating Pooling Mechanisms
In order to show the beneﬁt of the entity mention-based
pooling method (ENTITY) for GCNs, we compare it with
the other pooling methods for ED in the literature , OVERALL , DYNAMIC as discussed in
the section about pooling above). Speciﬁcally, we repeat the
model selection procedure in Table 1 of the previous section to select the best network architecture for each pooling method of comparison in {ANCHOR, OVERALL, DY-
NAMIC} . For
each pooling method, the selection includes the model with
both BiLSTM and GCNs (BiLSTM + GCNs), the model
with just GCNs (GCNs (no BiLSTM)) and the model with
just BiLSTM (BiLSTM). We also optimize the number of
GCN layers and the number of BiLSTM layers for each
6The optimal number of BiLSTM layers is 2.
model as do the previous section. This procedure ensures
that each pooling method has its best network architecture
to facilitate a fair comparison. The best network architecture for each pooling method and their corresponding performance on the ACE 2005 test set are shown in Table 2.
Best Architecture
BiLSTM + GCNs (K = 2)
BiLSTM + GCNs (K = 3)
GCNs (no BiLSTM) (K = 1)
GCNs (no BiLSTM) (K = 2)
Table 2: ED performance for pooling mechanisms.
As we can see from the table, the best architectures for
ENTITY and ANCHOR have BiLSTM layers while this is
not the case for OVERALL and DYNAMIC whose best architectures only include GCN layers. We attribute this phenomenon to the fact that both OVERALL and DYNAMIC aggregate the convolution vectors of every word in the sentences, potentially encapsulating useful long-range dependencies of word in the sentences for ED. This makes BiL-
STM redundant as BiLSTM also attempts to capture such
long-range dependencies in this case. This is in contrast to
ENTITY and ANCHOR that only aggregate the convolution
vectors at some speciﬁc positions in the sentences (i.e, the
entity mentions and the current word) and lack the capacity to model the long-range dependencies of words. This
necessitates BiLSTM to incorporate the long-range dependencies for ENTITY and ANCHOR. Finally, we see that EN-
TITY signiﬁcantly outperforms all the other pooling methods (p < 0.05) with large margins (i.e, 1.7% better than
the second best method of ANCHOR in terms of F1 score),
demonstrating the effectiveness of the entity mention-based
pooling (ENTITY) for ED with GCN models.
5. Comparing to the State of the art
This section compares the proposed model (i.e, BiLSTM
+ GCNs (K = 2) with ENTITY pooling) (called GCN-
ED) with the state-of-the-art ED systems on the ACE 2005
dataset in Table 3. These systems include:
1) Perceptron: the structured perceptron model for joint
beam search with both local and global hand-desgined features in 
2) Cross-Entity: the cross-entity model 
3) PSL: the probabilistic soft logic model to capture the
event-event correlation 
4) Framenet: the model that leverages the annotated corpus
of FrameNet to improve ED 
5) CNN: the CNN model 
6) DM-CNN: the dynamic multi-pooling CNN model 
7) DM-CNN+: the dynamic multi-pooling CNN model augmented with automatic labeled data 
8) B-RNN: the bidirectional recurrent neural network model
 
9) NCNN: the nonconsecutive CNN model 
10) ATT: the attention-based model 
11) ATT+: the attention-based model augmented with annotated data in Framenet 
12) CNN-RNN: the ensemble model of CNN and LSTM in
 
Perceptron
Cross-Entity †
Framenet ‡
Table 3: Comparison to the state of the art. †beyond the sentence level. ‡using additional data.
From the table, we see that GCN-ED is a single model,
but it still performs comparably with the ensemble model
CNN-RNN in , and signiﬁcantly outperforms all the other compared models. In particular, GCN-
ED is 1.2% better than ATT+ although GCN-ED does not
utilize the annotated data from Framenet as ATT+ does. Besides, although GCN-ED only uses the sentence-level information, it is still greatly better than the methods that employ the document-level information (i.e, Cross-Entity and
PSL) with large margins (an improvement of about 4.8% on
the F1 score). Finally, among the single convolution-based
models (i.e, CNN, DM-CNN, NCNN and GCN-ED), GCN-
ED is superior to the others (an improvement of 1.9% on F1
score with respect to the best reported convolutional model
NCNN). This is signiﬁcant with p < 0.05 and demonstrates
the beneﬁts of the proposed model for ED.
6. Investigating the effect of predicted entity
The previous sections have demonstrated the effectiveness
of the the proposed model in which the pooling mechanism ENTITY plays an important role. The operation of EN-
TITY requires entity mentions that are obtained from the
manual annotation (perfect entity mentions) in the previous experiments. It remains to test if the proposed model
in general and the pooling method ENTITY in particular can
still perform well when the entity mentions are predicted by
an automatic system. The TAC KBP 2015 is used for the
experiments in this section. We ﬁrst utilize the RPI Joint
Information Extraction System to label this dataset for entity mentions, and
then employ the predicted entity mentions as inputs for the
models. In order to ensure consistency, we train the models
with the best network architectures for each pooling mechanism in Table 2 on the training portion and report the performance on the test portion of the TAC KBP 2015 dataset.
We also use the same hyper-parameters and resources as
those of the experiments for the ACE 2005 dataset (in Table 2) to achieve compatibility. Table 4 shows the results.
We also include the performance of the best system in the
Event Nugget Detection Evaluation of the 2015 Text Analysis Conference for reference , conﬁrming the effectiveness of entity mentions to specify pooling positions for GCNs in ED
even when the entity mentions are predicted. In addition, the
proposed model GCN-ED (corresponding to the row of EN-
TITY in the table) outperforms the best reported system in
the 2015 TAC evaluation, further demonstrating the advantages of GCN-ED for ED.
Related Work
Event detection has attracted much research effort in the last
decade. The early and successful approach for ED has involved the feature-based methods that hand-design feature
sets for different statistical models for ED .
The last couple of years witness the success of the neural network models for ED. The typical models employs
CNNs , recurrent neural networks and
attention-based networks . However, none
of these works consider syntax for neural ED as we do in
this work.
Syntactic information has also been employed in neural network models for various natural language processing tasks, including sentiment analysis , dependency parsing , relation extraction , machine translation
 etc. However, this is
the ﬁrst work to integrate syntactic information in the neural
network models for event detection.
Conclusion
We propose a novel neural network model for event detection that is based on graph convolutional networks over dependency trees and entity mention-guided pooling. We extensively compare the proposed models with various baselines and settings, including both perfect entity mention
setting and predicted entity mention setting. The proposed
model achieves the state-of-the-art performance on two
widely used datasets for ED, i.e, ACE 2005 and TAC KBP
2015. In the future, we expect to investigate the joint models for event extraction (i.e, both event detection and argument prediction) that employ the syntactic structures. We
also plan to apply the GCN models to other information extraction tasks such as relation extraction, entity linking etc.