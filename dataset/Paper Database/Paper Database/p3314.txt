Detecting Anomalous Events in Videos
by Learning Deep Representations of Appearance and Motion
Dan Xua,∗, Yan Yana, Elisa Riccib, Nicu Sebea
aDepartment of Computer Science, University of Trento, Trento, Italy
bFondazione Bruno Kessler (FBK), Trento, Italy
Anomalous event detection is of utmost importance in intelligent video surveillance. Currently, most approaches for
the automatic analysis of complex video scenes typically rely on hand-crafted appearance and motion features. However, adopting user deﬁned representations is clearly suboptimal, as it is desirable to learn descriptors speciﬁc to the
scene of interest. To cope with this need, in this paper we propose Appearance and Motion DeepNet (AMDN), a novel
approach based on deep neural networks to automatically learn feature representations. To exploit the complementary
information of both appearance and motion patterns, we introduce a novel double fusion framework, combining the
beneﬁts of traditional early fusion and late fusion strategies. Speciﬁcally, stacked denoising autoencoders are proposed
to separately learn both appearance and motion features as well as a joint representation (early fusion). Then, based
on the learned features, multiple one-class SVM models are used to predict the anomaly scores of each input. Finally,
a novel late fusion strategy is proposed to combine the computed scores and detect abnormal events. The proposed
ADMN is extensively evaluated on publicly available video surveillance datasets, showing competitive performance
with respect to state of the art approaches.
Keywords: Video surveillance, abnormal event detection, unsupervised learning, stacked denoising auto-encoders,
feature fusion
1. Introduction
In the last few years the massive deployment of distributed camera systems in public spaces has increased
the need for advanced tools performing the automatic
analysis of video surveillance streams. A fundamental challenge in intelligent video surveillance is to automatically detect anomalous events in complex and
crowded scenes. This problem has attracted considerable attention in the computer vision research community .
Early works in the literature are based on the analysis
of individual moving objects in the scene .
First, visual tracking is performed to compute the trajectories of the targets and a model is learned describing
typical activities. Then, anomalous events are identiﬁed
by looking at patterns which distinctly diverge from the
model. However, these methods are not suitable for analyzing complex scenes, as the accuracy of visual tracking algorithms signiﬁcantly degrades in case of several
∗Corresponding author: Dan Xu. Email address: 
occluded targets.
Therefore, more recently, unsupervised non-object centric approaches have gained popularity .
These methods address the anomaly detection task by analyzing the cooccurence of atomic spatio/temporal patterns and are
based on hand-crafted features extracted from low-level
appearance and motion cues. Commonly used low-level
features include histogram of oriented gradients (HOG),
3D spatio-temporal gradient, histogram of optical ﬂow
(HOF). However, adopting generic user deﬁned features
is a clear limitation of these approaches and improved
performance can be obtained by learning scene speciﬁc
descriptors.
Recently, deep learning approaches have been successfully used to tackle various computer vision tasks,
such as object classiﬁcation , object detection 
and activity recognition .
While these works
mostly focus on supervised learning tasks and Convolutional Neural Networks, unsupervised approaches have
also gained popularity. In particular, autoencoder networks have been investigated to address fundamental tasks such as object tracking and face align-
 
October 24, 2016
Image Sequences
Optical Flow Maps
Foreground
Extract multi-scale
image patches and warp
Extract ﬁxed-sized
optical ﬂow patches
Decision-level
Late Fusion
Anomaly Detection
Appearance Representations
Motion Representations
Pixel-level
Early Fusion
Joint Representations
Figure 1: Overview of the proposed AMDN approach for abnormal video event detection.
ment . In both scenarios, improved performance
over traditional methods can be achieved, since using
deep architectures rich and discriminative features can
be learned via multi-layer nonlinear transformations.
Following this intuition, in this paper we propose
a novel approach for detecting anomalous activities in
complex video surveillance scenes. Opposite to previous works which rely on hand-crafted
features to model spatio/temporal activity patterns, we
propose to learn discriminative feature representations
in a fully unsupervised manner adopting stacked denoising autoencoders (SDAE) . Figure 1 shows an
overview of the proposed method, named Appearance
and Motion DeepNet (AMDN). Our AMDN is based
on a novel double fusion scheme (integrating both traditional early fusion and late fusion strategies) for combining low-level features of appearance and motion.
Speciﬁcally, in the ﬁrst phase, still image patches and
optical ﬂow ﬁelds are provided as input to two separate SDAE networks, to learn appearance and motion
features, respectively. A third SDAE is used to learn
a joint representation of appearance and motion from
the concatenation of image pixels and the corresponding optical ﬂow (early fusion). In the second phase,
multiple one-class SVM models, corresponding to the
learned feature representations, are used to compute a
set of anomaly scores. Then, a novel late fusion scheme
is proposed to combine the computed scores for abnormal event prediction.
The proposed AMDN is evaluated on three challenging video surveillance datasets
and compared with several state of the art methods. Our
experiments clearly demonstrate the eﬀectiveness of the
proposed double fusion framework as well as the importance of learning features with SDAEs.
To summarize, the main contributions of this work
are threefold:
• To the best of our knowledge, this paper represents
the ﬁrst attempt to address the anomalous event
detection task using deep learning architectures.
In this way, discriminative feature representations
can be automatically learned for the scene of interest, showing signiﬁcant advantages over previous
methods based on hand-crafted features.
• Our AMDN learns appearance and motion features
as well as their correlations. Deep learning methods for combining multiple modalities have been
investigated in previous works . However,
none of these works consider the anomaly detection task.
• A double fusion scheme is proposed to combine
appearance and motion features. The advantages
of combining early and late fusion approaches have
been demonstrated in previous works . However in the authors did not consider a deep
learning framework, neither the problem of discovering unusual activities in video surveillance
This paper extends our previous work in . Specifically, with respect to in this paper we added a section discussing related work on abnormal video event
detection and on learning deep representations in unsupervised settings (Section 2). Moreover, in Section 3 we
provide further insights on the proposed AMDN framework, enriching the descriptions of the main components of our systems (SDAEs - Section 3.2.2, one-class
SVM - Section 3.3.1) and introducing a novel late fusion strategy based on ℓp-norm (Section 3.3.2). Finally,
we signiﬁcantly expanded the experimental evaluation,
adding results on a third publicly available dataset and
performing a detailed analysis of the diﬀerent components of our method (Section 4).
In the rest of this paper, we ﬁrst review the related
work in Section 2. Then, we introduce the proposed
AMDN framework in Section 3, describing the AMDN
structure in detail and the approach we used for training
our SDAEs. Finally, the proposed method is evaluated
extensively and the experimental results are presented
in Section 4. Conclusions are drawn in Section 5.
2. Related Work
In this section we review previous works considering:
(i) the addressed task, i.e. abnormal video event detection and (ii) deep learning approaches in unsupervised
2.1. Abnormal Event Detection in Videos
Existing techniques tackling the abnormal video
event detection are extensively reviewed in . These
methods can be mostly partitioned in two categories depending on the types of event representations adopted,
namely trajectory-based methods and non-object centric
2.1.1. Trajectory-based Methods
Trajectories are widely used features for abnormal
video event detection , due to their ability of describing synthetically the dynamic information of foreground objects. Trajectory-based methods usually rely
on two phases. First, visual tracking algorithms are used
to estimate the motion of the objects and the people in
the scene. Then, features representing the trajectories
of the targets are employed to construct statistical models describing typical activities. In the second phase,
the activities corresponding to trajectories which deviate signiﬁcantly from the learned model are identiﬁed
as anomalous .
A pioneering work on this research line is , where
object trajectories are modeled using probability density functions. In Hu et al. developed a multiple
objects tracking algorithm to collect trajectories, which
which are then used to learn statistical distributions.
Both spatial and temporal information are considered
for anomaly detection. Markris and Ellis proposed
a Bayesian approach for detecting abnormal trajectories
based on annotated scene semantics. Jiang et al. introduced a dynamic hierarchical clustering framework
for trajectory grouping using Hidden Markov Models
(HMMs) to represent each group of trajectories.
In general, trajectory-based methods guarantee satisfactory performance when foreground objects are easy
to detect and track, e.g. in indoor environments or when
there are few targets in the scene. On the contrary, performance signiﬁcantly degrades in unconstrained scenarios (e.g. in case of dense crowds), when the several
occlusions make traditional tracking and detection approaches not reliable.
2.1.2. Non-object Centric Methods
This category of approaches address the anomaly
detection task by learning representative activity patterns from behavior-related attributes of objects and
people within spatial/temporal contexts.
considered behavioral attributes include size, gradient, direction and speed of the targets in the scene,
which are described with low-level representations
such as HOG , 3D spatio-temporal gradient ,
HOF and dense spatial-temporal interest points
(Dense STIPs) .
Cong et al. employed multi-scale histograms of
optical ﬂow and a sparse coding model and used the
reconstruction error as a metric for outlier detection.
Mehran et al. proposed a “social force” model
based on optical ﬂow features to represent crowd activity patterns and identify anomalous activities. In 
co-occurrence statistics of spatio-temporal events are
adopted in combination with Markov Random Fields
(MRFs) to discover unusual activities. Spatio-temporal
MRFs are also employed in .
Multiple spatiotemporal ﬁlters at diﬀerent scales and local feature descriptors are considered in . Kratz et al. introduced an HMMs-based approach for detecting abnormal events through analyzing the motion variation of
local space-time volumes. Ricci et al. proposed a
convex hierarchical clustering approach to detect abnormal events in time and space at diﬀerent scale.
The advantage of these methods over trajectory-based
ones is that, working at pixel level or, more generally,
on 2D cells/3D cubes, they are more robust in case of
complex scenes. However, all these approaches rely on
hand-crafted features which are diﬃcult to deﬁne a priori due to the huge variations of anomalous behaviors.
Our AMDN represents one of the ﬁrst attempts in the
computer vision community to overcome these issues
by learning deep feature representations.
2.2. Deep Learning Models in Unsupervised Setting
Deep learning techniques have recently achieved remarkable success in the computer vision ﬁeld, beating
the state-of-the-art in various challenging tasks . Unsupervised deep learning approaches have also
received increasing popularity, as a large amount of annotated training data is usually relatively diﬃcult to obtain in a variety of real-world applications. Commonly
used unsupervised deep models include deep belief networks and stacked autoencoders , which can be
eﬃciently trained with layer-wise pretraining and ﬁnetuning . These models have shown much more representative power than their shallow counterparts, resulting in improved performance on several tasks. For
instance, Kan et al. addressed the cross-pose face
recognition problem using stacked autoencoders. In this
way pose-invariant features are learned from faces with
diﬀerent poses, obtaining superior performance than
previous methods. In , the authors proposed a deep
denoising autoencoder network for robust visual tracking and learning eﬀective target representations.
our knowledge, no existing works in the literature considered an unsupervised deep learning framework for
learning multiple features to tackle the abnormal event
detection problem.
3. AMDN for Abnormal Event Detection
3.1. Overview
As discussed in Section 1, the proposed AMDN
framework for detecting anomalous activities is based
on two main steps (Fig.1). In the ﬁrst phase, SDAEs
are used to learn appearance and motion representations
of visual data, as well as a joint representation capturing the correlation between appearance and motion features (Sec. 3.2). In the second phase (Sec. 3.3), three
separate one-class SVMs are learned based on the
diﬀerent types of feature representations. Once the oneclass SVM models are trained, given a test sample corresponding to an image patch, three anomaly scores are
computed and combined. The combination of the oneclass SVM scores is obtained with a novel late fusion
scheme. In the following we describe the proposed approach in details.
3.2. SDAEs in AMDN
In this subsection we ﬁrst review some basic concepts
about denoising autoencoders (DAEs) and then describe
the details of the proposed approach for learning deep
representations of appearance and motion.
Bottleneck
hidden layer
Figure 2: (a) The structure of the (a) appearance and motion and (b)
joint SDAEs for learning feature representations.
3.2.1. Background on Denoising Autoencoders
A denoising autoencoder is a one-hidden-layer
neural network which is trained to reconstruct a sample
xi from its (partially) corrupted version ˜xi. Typical corrupted inputs are obtained by drawing samples from a
conditional distribution p(x|˜x) (e.g. common choices for
corrupting samples are additive Gaussian white noise or
salt-pepper noise).
A DAE can be divided into two parts, i.e. encoder and
decoder, connected by a single hidden layer. The two
parts are used to learn two mapping functions, fe(W, b)
and fd(W′, b′), where W, b denote the weights and the
bias term of the encoder part, while W′, b′ refer to the
corresponding parameters of the decoder. For a corrupted input ˜xi, a compressed hidden layer representation hi can be obtained through hi = fe(˜xi | W, b) =
σ(W˜xi + b). Then, the decoder tries to recover the original input xi from hi computing ˆxi = fd(hi | W′, b′) =
s(W′hi + b′). The function σ(·) and s(·) are activation
functions, which are typically nonlinear transformations
such as the sigmoid. Using this encoder/decoder structure, the network can learn a more stable and robust feature representations of the input.
At training time, given a training set T = {xi}N
DAE learns its parameters (W, W′, b, b′) by solving the
following optimization problem:
∥xi −ˆxi∥2
2 + λ(∥W∥2
where ∥· ∥F denotes the Frobenius norm.
term represents the average reconstruction error, while
the weight penalty term is introduced for regularization. The parameter λ balances the importance of the
two terms. Typically, sparsity constraints are imposed
on the output of the hidden units to discover meaningful representations from the data . If we let µj be
the target sparsity level and ˆµj = 1
i be the average activation values all over all training samples for the
j-th unit, an extra penalty term based on cross-entropy,
ϕ(µ||ˆµ) = −PH
j=1[µ j log(ˆµj)+(1−µj) log(1−ˆµj)], can be
added to (2) to learn a sparse representation. Here, H is
the number of hidden units. The optimization problem
(2) has a non-convex objective function and gradient descent can be used to compute a local optima.
3.2.2. SDAEs Structure and Training
Structure. The proposed approach for detecting anomalous event rely on three SDAE networks (Fig.1) associated to diﬀerent types of low-level inputs. These SDAE
are used to learn appearance and motion features as well
as a joint representation of them. The basic structures of
the proposed SDAE networks is illustrated in Fig. 2 (a)
and (b). For the encoder part, we use an over-complete
set of ﬁlters in the ﬁrst layer to capture a representative
information from the data. Then, the number of neurons is reduced by half in the next layer until reaching
the “bottleneck” hidden layer. The decoder part has a
symmetric structure with respect to the encoder part.
Speciﬁcally, the ﬁrst SDAE learns mid-level appearance representations from the original image pixels. To
capture rich appearance attributes, a multi-scale slidingwindow approach with a stride d is used to extract dense
image patches, which are then warped into equal size
wa × ha × ca, where wa, ha are the width and height of
each patch and ca is the number of the channels (ca = 1
for gray images). The warped patches xA
i ∈IRwa×ha×ca
are used for training. All the patches are linearly normalized into a range . We stack 4 encoding layers
with νa × wa × ha × ca neurons in the ﬁrst layer, where
νa > 1 is an ampliﬁcation factor for constructing an
over-complete set of ﬁlters. The use of over-complete
representations in combination with sparsity terms have
been shown to be eﬀective in learning meaningful compressed representations in previous works .
The second SDAE is used to learn the motion features.
We compute dense optical ﬂow and we use a
sliding window approach with windows of ﬁxed size
wm × hm × cm (cm = 2 for optical ﬂow magnitude along
x and y axes) for motion representation learning. Similar to the appearance feature pipeline, the patches xM
IRwm×hm×cm are normalized into within each channel
and 4 encoding layers are used. The number of neurons
of the ﬁrst layer is set to νm × wm × hm × cm, νm > 1.
While the ﬁrst two SDAEs learn appearance and motion features separately, to take into account the correlations between motion and appearance we propose
to couple these two pipelines to learn a joint representation (Fig. 2 (b)).
The network training data xJ
IRw j×hj×(ca+cm) are obtained through a pixel-level early fusion of the gray image patches and the corresponding
optical ﬂow patches.
Training. The proposed SDAEs are trained separately.
We rely on the typical learning scheme based on two
steps: pretraining and ﬁne-tuning. The network parameters are initialized through pretraining all layers, and
then ﬁne-tuning is used to adjust parameters over the
whole network.
Given a training set T k = {xk
i=1, k ∈{A, M, J} corresponding to appearance, motion and joint representation, the layer-wise pretraining learns the parameters of
each SDAE minimizing the reconstruction loss regularized by a sparsity-inducing term, i.e.:
∥xi −ˆxi∥2
2 + λ(∥W∥2
F) + γϕ(µ||ˆµ). (2)
In each layer, the input is corrupted to learn the mapping
function, which is then used to produce the representation for the next layer with uncorrupted inputs. Finetuning consider all the layers of each SDAE as a single
model. The backpropagation algorithm can be used to
ﬁne-tune the network.
The following objective function is used for ﬁnetuning:
where λF is a user deﬁned parameter and 2L + 1 is the
number of layers in the SDAEs. Similar to previous
works , here we remove the sparsity regularization
because the pre-trained weights will serve as regularization to the network. To speed up the convergence during
training, stochastic gradient descent is employed and the
training set is divided into mini-batches.
Once training is complete, the appearance, motion
and joint feature representations can be computed to
perform video anomaly detection.
In this work, we choose the output of the “bottleneck”
hidden layer to obtain a more compact representation.
i be the i-th input data sample, and σk
the mapping function of the l-th hidden layer of the kth SDAE pipeline. The learned features, sk
i , can be extracted through a forward pass computing, i.e.
i = σL(σL−1(· · · σ1(Wk
where the L-th hidden layer is the “bottleneck” hidden
3.3. Abnormal Event Detection with Deep Representations
In this work the video anomaly detection problem is
formulated as a patch-based binary categorization problem, i.e. given a test frame we adopt a sliding window
approach and classify each patch as corresponding to a
normal or an abnormal event. Speciﬁcally, given the t-th
test patch, we compute the associated deep features representations sk
t , k ∈{A, M, J}. Then, we rely on three
one-class SVM models to calculate a set of anomaly
scores A(sk
t ) (Subsection 3.3.1). Finally, the scores are
linearly combined to obtain the global anomaly score
k∈{A,M,J} αkA(sk
t ) (Subsection 3.3.2). In the
following we describe these phases in details.
3.3.1. One-class SVM Modeling
One-class SVM is a widely used algorithm for outlier
detection, where the main idea is to learn a hypersphere
in the feature space and map most of the training data
into it. The outliers of the data distribution correspond
to point lying outside the hypersphere. While other approaches can be considered to compute anomaly scores,
we consider one-class SVMs as it has been shown to
be eﬀective in previous works on abnormal event detection from surveillance videos using hand-crafted features . Formally, given a set of training samples
i=1, the underlying problem of one-class SVM
can be formulated as the following quadratic program:
i ) ≥ρ −ξi, ξi ≥0.
where θ is the learned weight vector, ρ is the oﬀset, φ(·)
is a feature projection function which maps the feature
i into a higher dimensional feature space. The
user deﬁned parameter µ ∈(0, 1] regulates the expected
fraction of outliers distributed outside the hypersphere.
Introducing a nonlinear mapping, the projection function φ(·) can be deﬁned implicitly by introducing an associated kernel function k(sk
j) and (5)
can be solved in the corresponding dual form . In
our experiments we consider a radial basis function kernel, k(sk
. Given the optimal θ and ρ
obtained by solving (5), an outlier score for a test sample sk
t of the k-th SDAE pipeline can be estimated by
computing:
t ) = ρ −θTφ(sk
3.3.2. Flexible ℓp-norm Late Fusion for Anomaly Detection
In this subsection, we propose a ﬂexible unsupervised
late fusion scheme to automatically learn the weights
α = [αA, αM, αJ]. These parameters are used to compute the anomaly score A(sk
k∈{A,M,J} αkA(sk
test time for each patch t an abnormal activity is identi-
ﬁed by computing A(sk
t ) and comparing it with a threshold η, i.e. A(sk
t ) > η denotes an anomalous event. The
weights αk are meant to reﬂect the importance of diﬀerent feature representations, corresponding to diﬀerent
one-class SVM models. While many choices are possible to learn αk, in this paper we propose to solve the
following optimization problem:
where λs is a regularization parameter and P = {P :
Similarly to Principal Component Analysis, the matrix Pk ∈IRm×M, m << M, maps the
samples sk
i ∈IRM associated to the k-th modality into
a new subspace in order to maximize the variance of
the ﬁrst m-components, subject to orthogonality constraints.
The matrix PkSk 
PkSkT represents the covariance of k-th feature type in the new subspace and
measures the spread of the projected samples for each
modality. Setting the weights αk by solving the optimization problem (7) we favor feature types associated
with data sets with smaller variance: our intuition is that
scattered data sets correspond to noisy features which
must be deemphasized.
In the proposed optimization problem (7) we also introduce a ℓp-norm term, which, compared with traditional ℓ2-norm and ℓ1-norm terms, guarantees an enhanced ﬂexibility, by allowing to tune for p .
Intuitively, ℓ1-norm imposes sparsity on the learned
weights, while ℓ2 norms produces an “averaging” effect. Setting a priori one of the two may be suboptimal in term of performance. Moreover, the complexity of solving the problem (7) with ℓp-norm is the same
as for ℓ2-norm .
Therefore, in our experiments,
we tune the parameter p with the interval of 0.1 from
[1.1, 1.2, ... , 2.5]. We also set the parameter m = 100,
as it empirically provides the best performance. The
projection matrices Pk are introduced for learning the
feature weights as explained above and they are not used
in the test phase. The same value m = 100 is chosen
for the three feature types. The proposed optimization
problem (7) is a convex problem and if p > 1 an alternating minimization algorithm can be used to solve
it with respect to Pk and α respectively . Finally,
(a) Input test images
(b) Foreground estimation
(c) Binarization
(d) Detection on foreground
Figure 3: Illustration of the proposed foreground detection scheme
using background subtraction for improving computational eﬃciency
in the test phase. Green patches are selected foreground regions, red
ones correspond to anomalies.
it is worth noting that, while the proposed late fusion
scheme is applied to AMDN considering three underlying SDAEs, our strategy is general and can be used also
in case of a diﬀerent number of models.
4. Experiments
In this section we evaluate the performance of the
proposed AMDN framework for abnormal event detection on three challenging video surveillance datasets.
Speciﬁcally, we consider the UCSD pedestrian anomaly
dataset (Ped1 and Ped2) , the Subway dataset 
and the Train dataset .
4.1. Datasets
The UCSD pedestrian dataset includes two subsets: Ped1 and Ped21.
The video sequences depict
diﬀerent crowded scenes and anomalies include bicycles, vehicles, skateboarders and wheelchairs. In some
frames the anomalies occur at multiple locations. Ped1
has 34 training and 16 test image sequences with about
3,400 anomalous and 5,500 normal frames, and the image resolution is 238 × 158 pixels. Ped2 has 16 training
and 12 test image sequences with about 1,652 anomalous and 346 normal frames. The image resolution is
360 × 240 pixels.
The Subway dataset is collected using CCTV
cameras and consists of two video streams corresponding to two diﬀerent subway station scenarios (an entrance and an exit gate). The length of the videos is
1 
dataset.html
96 min and 43 min, respectively. In the entrance subset,
there are 66 abnormal events including people moving
in a wrong direction, unusual gesture interactions between people and sudden stopping or running. In the
exit subset 19 abnormal events are included, such as
people moving in a wrong direction and loitering near
the exit gate. The image resolution is 512 × 384 pixels.
The Train dataset depicts moving people inside
a train2. The dataset consists of 19218 frames, and the
anomalous events are mainly due to unusual movements
of people on the train.
This is a challenging abnormal event detection dataset due to dynamic illumination
changes and camera shake problems.
4.2. Implementation details
The proposed method is mainly implemented in Matlab and C++ based on the Caﬀe framework . The
code for optical ﬂow calculation is written in C++ and
wrapped with Matlab mex for computational eﬃciency
 . For one-class SVMs, we use the LIBSVM library
(version 3.2) . The experiments are carried out on a
PC with a middle-level graphics card (NVIDIA Quadro
K4000) and a multi-core 2.1 GHz CPU with 32 GB
To improve the computational speed of our framework in the test phase, in this paper we introduce a foreground detection approach based on background subtraction. This is motivated by the fact that abnormal
events are typically found in correspondence of moving
pixels. An illustration of the proposed foreground detection scheme is provided in Fig. 3. For an input test
image, the probability map of the foreground pixels is
estimated with a background subtraction algorithm and
binarized. The foreground regions are detected by identifying the patches which contains more than a certain
number of foreground pixels (10% of the patch size in
our test). We use the ViBe method to perform background subtraction. ViBe has a low computational complexity and can obtain near real-time performance (almost 16 frames/second with a resolution of 360×240 in
our Matlab environment).
4.3. Evaluation on the UCSD pedestrian dataset
In the ﬁrst series of experiments we evaluate the
performance of the proposed method on the UCSD
dataset. For learning appearance features, patches are
extracted using a sliding window approach at three different scales, i.e. 15 × 15, 18 × 18 and 20 × 20 pixels.
2 
anomalous-behaviour-data/
False Positive Rate (FPR)
True Positive Rate (TPR)
Social Force + MPPCA 
Social Force 
Sparse reconstruction 
Local statistical 
Detection at 150FPS 
(a) Frame-level ROC curve
False Positive Rate (FPR)
True Positive Rate (TPR)
Social Force + MPPCA 
Social Force 
Sparse Reconstruction 
Detection at 150FPS 
(b) Pixel-level ROC curve
Figure 4: UCSD dataset (Ped1 sequence): comparison of diﬀerent methods.
Table 2: UCSD dataset: comparison of diﬀerent feature fusion schemes in terms of EER and AUC.
Ped1(frame)
Ped1(pixel)
Joint representation (early fusion)
Fusion of appearance and motion pipelines (late fusion)
AMDN (double fusion)
Table 1: UCSD dataset: comparison (AUC) with the state of the art
Ped1(frame)
Ped1(pixel)
MPPCA 
Social force 
Social force+MPPCA 
Sparse reconstruction 
Mixture dynamic texture 
Local Statistical Aggregates 
Detection at 150 FPS 
This generates more than 50 million image patches, 10
million of which are randomly sampled and warped into
the same size (wa × ha = 15 × 15 pixels) for training.
For learning the motion representation, the patch size is
ﬁxed to wm × hm = 15 × 15 pixels, and 6 million training patches are randomly sampled. In the test phase,
we use a sliding widow approach with stride d = 15
and consider patches with size 15 × 15. The number of
neurons of the ﬁrst layer of the appearance and motion
network is both set to 1024, while for the joint pipeline
is 2048. Then, for the appearance and motion SDAE
the structure of the encoder part can be simply deﬁned
as 1024 →512 →256 →128. The decoder part has
a symmetric structure. Similarly, for the joint SDAE
the structure of the encoder part is 2048 →1024 →
512 →256. For the pretraining of the SDAEs, the corrupted inputs are produced by adding a Gaussian noise
with variance 0.0003. The network ﬁne-tuning is based
on stochastic gradient descent with the momentum parameter set to 0.9. We set the parameters λ = 0.01,
λF = 0.0001 and the mini-batch size Nb = 256. For
one-class SVMs, the parameter µ is tuned with cross
validation.
To perform a quantitative evaluation, we use both a
frame-level ground truth and a pixel-level ground truth.
The frame-level ground truth indicates whether one or
more anomalies occur in a test frame. The pixel-level
ground truth is used to assess the anomaly localization
performance. If the detected anomaly region overlaps
more than 40% with the annotated region, it is considered a true detection. We carry out a frame-level evaluation on both Ped1 and Ped2. Ped1 also provides 10
test image sequences with pixel-level ground truth. The
pixel-level evaluation is performed on these sequences.
The proposed approach is compared with several
state of the art methods. Speciﬁcally, we consider the
Mixture of Probabilistic Principal Component Analyz-
False Positive Rate (FPR)
True Positive Rate (TPR)
Social Force + MPPCA
Social Force
JointRepresentation(early fusion)
AMDN(double fusion)
Figure 5: UCSD dataset (Ped2 sequence): comparison of frame-level
performance (ROC curve) with diﬀerent methods.
ers (MPPCA) approach in , the social force model
in and its extension in , the sparse reconstruction
method in , mixture of dynamic texture (MDT) ,
Local Statistical Aggregates and detection at 150
Table 1 and Fig.6 show a quantitative comparison of
diﬀerent methods respectively in terms of Area Under
Curve (AUC) and Equal Error Rate (EER). Figure 5 and
Fig. 4 (a) and (b) report the associated ROC curves. The
ROC curves are produced by varying the threshold parameter η. The performance of the baseline methods are
taken from the original papers (when available). From
the frame-level evaluation, it is evident that our method
outperforms most previous approaches and that its performance are competitive with the best two baselines
 and . Moreover, considering pixel-level evaluation, i.e. accuracy in anomaly localization, our method
outperforms all the competing approaches.
Table 2 demonstrates the advantages of the proposed
double fusion strategy, comparing our AMDN with
early fusion and late fusion approaches. Speciﬁcally, for
early fusion we only consider the learned joint appearance/motion representation and a single one-class SVM.
For late fusion we use the two separate appearance and
motion pipelines and the proposed fusion scheme but
we discard the joint representation pipeline. We observe
that the late fusion strategy outperforms the early fusion
and that the combination of the two schemes lead to a
clear advantage. Finally, we also report some examples
of anomalous events detected with our method on the
UCSD dataset in Fig. 10.
4.4. Evaluation on the Subway dataset
To train the AMDN network, we follow previous
works and use the ﬁrst 15 minutes of the video
Equal Error Rate (%)
Social Force
Social Force + MPPCA
Sparse reconstruction
Detection at 150 FPS
Figure 6: UCSD dataset: comparison of frame-level performance
(Equal Error Rate) with diﬀerent methods. Note that for the sparse reconstruction method and the detection at 150 fps performance
on Ped2 dataset are not available.
Table 3: Comparison of diﬀerent methods on the Subway dataset. In
the third column, the ﬁrst number denotes the detected anomalous
events, while the second is the actual number of anomalous events.
Abnormal events
False alarm
MPPCA 
reconstruction 
optical ﬂow 
Joint representation
(early fusion)
Fusion of appearance
& motion (late fusion)
AMDN (double fusion)
sequences.
The rest of the videos is used for testing.
The frames are resized to a pixel resolution of
320 × 240 for computational eﬃciency. In these experiments the patch size for learning both appearance
and motion features is 15 × 15 pixels. For SDAEs we
use the same network conﬁguration and training parameters of the experiments on the UCSD dataset. As baseline methods we consider recent approaches adopting
the same dataset, including Spatio-Temporal Composition (STC) , MPPCA , Spatio-Temporal Oriented Energy (STOE) , Dynamic Sparse Coding
(DSC) , Sparse Reconstruction and Local Optical Flow .
Table 3 provides the results of the comparison of
AMDN with other baseline methods. AMDN obtains
(a) AUC on Ped1, Ped2 and Subway(Exit) datasets
(b) EER on Ped1, Ped2 and Subway(Exit) datasets
Figure 7: Performance comparison of SDAE and SAE when embedded in our framework.
the best detection performance from both the perspectives of abnormal events detection (61/66) and false
alarm (1). The eﬀectiveness of the proposed double fusion scheme is also veriﬁed in Table 3. Our AMDN outperforms both early fusion and late fusion techniques.
Figure 11 shows some examples of the detected abnormal events, such as people entering through the exit
gate, people entering without payment and people exiting from the entrance gate.
4.5. Evaluation on the Train dataset
In the third series of experiments we consider the
Train dataset. The frames of the dataset are resized to
280 × 380 pixels for computational eﬃciency. For the
AMDN parameters, we use the same experimental setting of the UCSD dataset experiments, except from the
parameters λF and Nb which are set to 0.00001 and 100,
respectively.
We compare the proposed approach with several methods in the literature considering the same
datasets, including Spatio-Temporal Composition ,
Spatio-Temporal Oriented Energies , Local Optical Flow , Behavior Templates and Mixture
of Gaussian. From the precision/recall curve shown in
Fig. 12 (c), it is clear that our method outperforms all
the baselines.
4.6. Analysis of AMDN
In this section, we further analyze the proposed
method to underline the importance of its main components. Fist of all, in order to demonstrate the eﬀectiveness of the adopted SDAEs, we replace the stacked
False Positive Rate (FPR)
True Positive Rate (TPR)
AMDN(Joint)-RecError
AMDN-RawPatches-PCA128
AMDN-RawPatches-PCA256
AMDN-RawPatches
Figure 8: Comparison of diﬀerent feature representations in AMDN.
denoising autoencoder structure with a regular stacked
autoencoder (SAE) into AMDN. Figure 7 shows the
results of our comparison. Using SDAE we obtain a
slight improvement in terms of AUC and EER on three
We believe that the corrupted training data
used with SDAE helps to learn more eﬀective feature
representations, as the corruption increases the variability of training data. This in line with what observed in
previous works on deep architectures.
To further demonstrate the validity of the learned
deep representations, we evaluate the performance
of AMDN by using diﬀerent settings,
(i) AMDN-RawPatches: removing SDAEs from our
framework and directly input patches to one-class
SVMs; (ii) AMDN-RawPatches-PCA256: the same setting as (i) but using PCA for reducing the dimension
of the patches to 256; and (iii) AMDN-RawPatches-
PCA128: the same setting as (i) but using PCA for reducing the dimension of the patches to 128. Figure 8
compares the performance of AMDN with the diﬀerent
settings on the UCSD ped1 dataset. It is clear that the
performance of AMDN-RawPatches is the worst among
all the baselines. By using PCA on raw patches the performance is slightly improved, while AMDN is signiﬁcantly better than all the baseline methods, thus demonstrating the eﬀectiveness of the learned deep representations.
We also report the TPR and FPR computed
considering as anomaly score the reconstruction error
of the SDAE model which uses the joint motion and
appearance representation (denoted as AMDM (Joint)
RecError). As shown in the ﬁgure, the proposed approach outperforms this model. We believe that it is
probably because the simple reconstruction error is sensitive for identifying the anomaly regions. These results
thus conﬁrm the beneﬁt of adopting one-class SVMs
for anomaly detection in combination with a late fusion
Table 4: Comparison of diﬀerent methods in terms of computational time during test (seconds per frame).
Running time
Subway (exit)
Mixture dynamic texture 
Sparse reconstruction 
Detection at 150 FPS 
AMDN without foreground detection
Nvidia Quadro K4000
AMDN with foreground detection
Figure 9: Performance at varying p.
Table 5 compares AMDN with another method based
on deep representations .
As shown in the table, the proposed framework outperforms in the
Ped1, Ped2 and Subway exit datasets. The approach
in is a frame-based deep learning method, which
uses a convolution-deconvolution autoencoder network
to learn a representation of the whole frame. We believe
that our approach is more successful for the anomaly
detection task, as it operates on a patch-level basis.
Finally, to evaluate the inﬂuence of the ℓp-norm in
the ﬂexible late fusion scheme, we perform a sensitivity
analysis of the parameter p. Figure 9 shows the EER at
varying p on UCSD Ped1 and Ped2 datasets. It is immediate to observe that in this case when p is set to 2,
AMDN yields the best performance. Moreover, changing the parameter p around 2 only have a small eﬀect on
the ﬁnal performance.
4.7. Computational Cost Analysis
To evaluate the performance of our approach in terms
of computational cost, we conduct an empirical analysis on the considered datasets. As discussed above, different datasets have diﬀerent image resolutions which
aﬀect the time required to process each frame. Speciﬁcally, the resolutions are 238×158, 360×240, 320×240
and 280 × 380 pixels, for the UCSD Ped1, UCSD Ped2,
Subway and Train datasets, respectively.
Table 5: Performance comparison of diﬀerent deep learning based
methods in terms of EER and AUC.
Subway (exit)
Conv-AE 
Table 6: Impact of foreground detection (FD) on performance.
Subway (exit)
AMDN without FD
AMDN with FD
During the training phase, the learning of the proposed AMDN takes about 9, 4, 2.5 and 3.5 hours on the
UCSD Ped1, UCSD Ped2, Subway and Train datasets,
respectively. Table 4 shows the average running time
of each frame during the test phase. Processing one
frame to detect anomalous events takes about 9 −14
seconds when the cell size is 15 × 15 pixels and no
foreground detection is applied. Adopting the proposed
foreground detection scheme (Subsection 4.2), a significant improvement in terms of computational speed is
achieved (average improvement ∼44.3%). For sake of
completeness, we also analyze the impact of the foreground detection approach on the performance in terms
of EER and AUC. As shown in Table 6, no signiﬁcant
variations are observed.
This is probably due to the
fact that most false positive detections correspond to the
foreground region.
Finally, in Table 4 we also provide a comparison with
some previous methods in terms of computational cost
during test time. Since the original implementations of
baseline approaches are not publicly available, we report the running times taken from specifying the
working environment. The methods in and are
advantageous with respect to our approach in terms of
detection speed. This is somehow expected as, similarly to other applications, the gain in terms of accuracy
obtained with deep architectures comes at a price of an
increased computational cost. Future works will be devoted to address this issue.
(a) skaters and bikers
(b) skaters
(c) bikers and carters
(d) bikers and vehicles
(e) bikers
(f) vehicles
(g) bikers
(h) bikers and skaters
Figure 10: Examples of anomaly detection results on Ped1 (top) and Ped2 (bottom) sequences.
Figure 11: Examples of anomaly detection results on the Subway exit (top) and entrance (bottom) datasets. The regions with abnormal events are
marked with red color. (a) and (e) show examples of normal frames of the exit and entrance scenarios. The detected anomalies in the examples
include: people entering through the exit gate shown in (b), (c) and (d); people entering without payment shown in (f) and (g); people exiting
through the entrance gate shown in (h).
Figure 12: Results of anomaly detection on the Train dataset: (left) a frame depicting typical normal activities; (center) examples of detected
anomalies; (right) precision/recall curve.
5. Conclusions
This paper introduced a novel unsupervised learning
approach for video anomaly detection based on deep
learning architectures. The proposed AMDN method
is based on multiple SDAEs for learning both appearance and motion representations of activities in a video
A double fusion scheme is designed to combine the learned feature representations.
We carried
out an extensive experimental evaluation, considering
three challenging publicly available video anomaly detection datasets (UCSD, Subway and Train), and we
demonstrated the eﬀectiveness and robustness of the
proposed approach, showing competitive performance
with respect to existing methods. The fundamental advantages of our approach are that it does not rely on
any prior knowledge for designing features (the input
of our framework are raw pixels) and does not require
any object-level analysis (e.g. object detection or tracking). From the experimental results, it is obvious that
our learned deep features are more powerful than traditional hand-crafted descriptors for representing the dynamic of the video scenes.
Currently, the computational overhead of AMDN
in the test phase is too high for real-time processing.
Strategies to reduce the computational cost will be studied in future works. Further research directions will include investigating other deep network architectures as
well as alternative approaches for fusing data of multiple modalities in the context of SDAEs. Moreoever,
an interesting follow-up of this work will be to extend
ADMN in order to include contextual information. In
fact, while at the present anomalous behaviours are detected by considering patches in isolation, it will be beneﬁcial to look at co-occurrence of multiple patterns to
spot additional unusual events.
Acknowledgments
This work was partially supported by the MIUR
Cluster project Active Aging at Home, the EC H2020
project ACANTO. The authors also would like to thank
NVIDIA for GPU donation.