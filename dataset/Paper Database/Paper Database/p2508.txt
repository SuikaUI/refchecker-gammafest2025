Contents lists available at ScienceDirect
Artificial Intelligence In Medicine
journal homepage: www.elsevier.com/locate/artmed
Deep learning to find colorectal polyps in colonoscopy: A systematic
literature review
Luisa F. Sánchez-Peraltaa,*, Luis Bote-Curiela, Artzai Picónb, Francisco M. Sánchez-Margalloa,
J. Blas Pagadora
a Jesús Usón Minimally Invasive Surgery Centre, Ctra. N-521, km 41.8, 10071 Cáceres, Spain
b Tecnalia, Parque Científico y Tecnológico de Bizkaia, C/ Astondo bidea, Edificio 700, 48160 Derio, Spain
A R T I C L E I N F O
Colorectal cancer
Deep learning
Localization
Segmentation
A B S T R A C T
Colorectal cancer has a great incidence rate worldwide, but its early detection significantly increases the survival
rate. Colonoscopy is the gold standard procedure for diagnosis and removal of colorectal lesions with potential to
evolve into cancer and computer-aided detection systems can help gastroenterologists to increase the adenoma
detection rate, one of the main indicators for colonoscopy quality and predictor for colorectal cancer prevention.
The recent success of deep learning approaches in computer vision has also reached this field and has boosted the
number of proposed methods for polyp detection, localization and segmentation. Through a systematic search,
35 works have been retrieved. The current systematic review provides an analysis of these methods, stating
advantages and disadvantages for the different categories used; comments seven publicly available datasets of
colonoscopy images; analyses the metrics used for reporting and identifies future challenges and re­
commendations. Convolutional neural networks are the most used architecture together with an important
presence of data augmentation strategies, mainly based on image transformations and the use of patches. End-to-
end methods are preferred over hybrid methods, with a rising tendency. As for detection and localization tasks,
the most used metric for reporting is the recall, while Intersection over Union is highly used in segmentation.
One of the major concerns is the difficulty for a fair comparison and reproducibility of methods. Even despite the
organization of challenges, there is still a need for a common validation framework based on a large, annotated
and publicly available database, which also includes the most convenient metrics to report results. Finally, it is
also important to highlight that efforts should be focused in the future on proving the clinical value of the deep
learning based methods, by increasing the adenoma detection rate.
1. Introduction
Colorectal cancer (CRC) is defined as a carcinoma, usually an ade­
nocarcinoma, in the colon or rectum. Colorectal cancer is considered
primarily as a “lifestyle” disease; its incidence is higher in countries
with a diet high in calories and animal fat and with a largely sedentary
population . CRC accounts for a 10% of overall new cancer cases
worldwide, with a higher incidence rate in developed countries .
Only in the United States, it has increased from over 132,000 estimated
new cases and nearly 50,000 estimated deaths in 2015 to over
145,000 estimated new cases and 51,000 estimated deaths in 2019 .
In Europe, CRC represents the second most common cancer and also the
second cause of death from cancer .
Nevertheless, an early detection of the CRC increases the 5-year
survival rate from 18% when CRC is detected in the highest grade to
88.5% when it is detected in an initial grade due to symptoms.
Furthermore, screening programs achieve a significant increase of the
survival rate as they allow to start treatment even before the appear­
ance of those symptoms, so up to 222 deaths out of 1000 patients de­
tected with symptomatic CRC could be avoided .
 
Received 27 August 2019; Received in revised form 3 March 2020; Accepted 1 July 2020
Abbreviations: ADR, adenoma detection rate; CAD, computer aided detection; CDDN, cascaded deep decision network; CI, confidence interval; CNN, convolutional
neural network; CRC, colorectal cancer; DWD, distance-weighted discrimination; FCN, fully convolutional network; GAN, generative adversarial network; IoU,
Intersection over Union; LSTM, long short term memory; mAP, mean average precision; PIVI, preservation and incorporation of valuable endoscopic innovations;
RNN, recurrent neural network; ReLU, rectified linear unit; RPN, region proposal network; SVM, support vector machine
⁎ Corresponding author.
E-mail addresses: (L.F. Sánchez-Peralta), (L. Bote-Curiel), (A. Picón),
 (F.M. Sánchez-Margallo), (J.B. Pagador).
Artificial Intelligence In Medicine 108 101923
0933-3657/ © 2020 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license
( 
Colonoscopy is a standard technique for visual exploration of the
colon and rectum by inserting a flexible endoscope through the patient
anus and is considered the gold standard for detection and removal
of colorectal lesions, associated with important reduction of CRC
mortality . The European Society of Gastrointestinal Endoscopy re­
commends the use of high definition white light endoscopes for de­
tection of colorectal neoplasms in middle risk population , as it is
estimated that 70–80% of CRC has a sporadic origin . Therefore, it
is clear the need for increasing the adenoma detection rate (ADR),
defined as the proportion of patients with at least one colorectal ade­
noma detected among all patients examined by the gastroenterologist,
which is both a colonoscopy quality measure and a validated predictor
for CRC prevention, having an inverse relationship .
Recently, the success of deep learning has also boosted the
applications on medical imaging analysis , achieving expert per­
formance in several cases . Deep learning approaches rely on
the ability of networks with several layers to automatically learn hier­
archical features characterizing the input data through the application
of non-linear operations together with backpropagation for training.
Deep learning architectures stack blocks of different types of layers
(fully connected, convolutional, pooling or activation layers) to si­
multaneously be sensitive to minute details and insensitive to large ir­
relevant details.
Computer-aided detection (CAD) systems have the potential to re­
volutionize the endoscopic practice by (1) improving the adequacy of
inspection technique; (2) providing automatic detection of precursor
lesions of CRC; and (3) facilitating real-time diagnosis with optical
biopsy . In this review, we focus on the second topic. Traditionally,
CAD systems for polyp detection have been based on the manual ex­
traction of polyp features, or so called hand-crafted methods: shape-
based , texture-based , depth of valleys-based or
combined-based methods. Nevertheless, results of the MICCAI
2015 Automatic Polyp Detection in Colonoscopy Videos challenge
proved that convolutional neural networks (CNNs) are the state-of-the-
art regarding polyp detection methods .
Within the scope of this systematic review, three main tasks are
considered:
1. Detection: identifying whether a polyp is shown or not in the frame,
but information on the polyp location is not given.
2. Localization: identifying the position of the polyp within a given
frame, but exact shape of the polyp is not relevant.
3. Segmentation: marking the exact polyp area in a given frame.
Polyp classification (benign vs malign, Paris classification, NICE
classification, etc.) is out of the scope of the current systematic review.
Besides, classification in this case is done once the presence of the polyp
is confirmed and in this review we place the focus on the prior stage
(identifying the presence of the polyp in the frame). Therefore, we
analyze the published methods for detection, localization and seg­
mentation of colorectal polyps based on deep learning approaches. In
this sense, methods are classified according to their main aim, the used
database, their approach and the reported metrics. The state-of-art
approaches are compared, showing advantages and disadvantages of
the different categories, to identify the most auspicious trends.
There are several surveys on deep learning for medical imaging
analysis , where at most methods for colorectal polyps
identification are roughly analyzed. A more specific review was done by
Prasath but the focus was placed on video capsule endoscopy ra­
ther than on colonoscopy images. More recently, Ahmad et al. 
summarized the evidence for clinical applications of computer-aided
diagnosis and artificial intelligence in colonoscopy of key studies in a
narrative manner. Therefore, to the authors’ knowledge, there is no
previous systematic review on the proposed topic with a comparative
analysis of retrieved works.
Nevertheless, the lack of a common framework makes difficult to
provide a reliable comparison of the state-of-art methodologies. The
main limitation comes from the different datasets and/or testing set
used for reporting results as well as selected metrics. Ideally, all
methods should be applied on a common database . Regretfully,
this ideal situation is not real in many of the published works, where
authors use different datasets, in a different manner, and reporting on
different test sets with different metrics. Therefore, in the current re­
view, methods are compared accordingly to their reported results,
identifying characteristics that might influence on them. To overcome
this heterogeneous situation, efforts have been lately performed by the
organization of challenges under the hosting of international congresses
in order to unify criteria. It is of special relevance the Endoscopic Vision
Challenge , which has organized a subchallenge focused on de­
tection, localization and segmentation of polyps in 2015 , 2017 and
2018. In these cases, methods are easily and reliably compared. Besides,
Vázquez et al. also propose a benchmark for endoluminal scene
segmentation of colonoscopy images, with the aim of boostering com­
parative research.
The main contributions of this systematic review are:
1. We analyze publicly available datasets of colonoscopy images.
2. We provide a comprehensive analysis of polyp detection, localiza­
tion and segmentation methods based on deep learning, discussing
advantages and disadvantages of the different categories.
3. We analyze and discuss the reporting metrics used.
4. We identify future challenges and recommendations based on the
findings of the review to be addressed by the scientific community
to advance the field.
This review is organized as follows. In Section 2 we present the
material and methods to carry out the systematic literature review,
including search strategy, study selection and data extraction and
management. In Section 3 we show the results of the search and sum­
marize the works found. Then, datasets of colonoscopy images and how
they are used are described in Section 4. In Section 5 and its corre­
sponding subsections, we present and discuss the methods for detection
(Section 5.1), localization (Section 5.2) and segmentation (Section 5.3)
of colorectal polyps using deep learning approaches. We conclude this
section with the advantages and disadvantages of each category (Sec­
tion 5.4). After this, metrics are analyzed in Section 6, to conclude the
paper with an analysis from a clinical perspective (Section 7), future
challenges and recommendations (Section 8) and final conclusions
(Section 9).
2. Material and methods
2.1. Search strategy
The Preferred Reporting Items for Systematic Reviews and Meta-
Analysis (PRISMA) has been followed to perform this systematic
review. The basic search string was (“colon” OR “colorectal”) AND
(“cancer” OR “polyp”) AND (“deep learning”) AND (“detection” OR
“localization” OR “segmentation”) and searches were performed on
February 2nd 2019 using ACM Digital Library, IEEE Digital Library,
Web of Science, PubMed, Science@Direct, Scopus and Springer Link
databases. The search string syntax was adapted when necessary, de­
pending on the database requirements. Search was performed on title,
abstract and keywords. Previously identified articles were also included
in the process.
2.2. Study selection
Studies included in the analysis were full text articles or full-length
proceedings published in English. The exclusion criteria were papers
published before 2015; in other languages; about a different topic;
applying deep learning in a field other than CRC; pure clinical studies;
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
use of endoscopic capsules as imaging source; not using white light
imaging; not using deep learning techniques; short proceedings; and
meta-analyses or reviews. Retrieved abstracts were read by two authors
(LBC and LFSP), searching for the full text when information in the
abstract was not enough to determine its inclusion or exclusion. Full
texts of selected abstracts were retrieved (LFSP) and independently
revised (AP, LBC, JBP and LFSP) for final agreement on inclusion cri­
teria. The reference list of selected works was also scrutinized for po­
tential interesting articles (LFSP).
2.3. Data extraction
Information from papers selected for analysis was extracted for
comparison (JBP, AP and LFSP). Papers were initially categorized ac­
cording to their main objective (detection, localization or segmenta­
tion). For each work, we also identified findings related to the used
dataset, the data augmentation approach, the proposed method, the
reporting metrics and reproducibility aspects (Table 1).
2.4. Review and data management
Parsif.al1 was used for abstract review and data management.
Search results from each database were imported into the platform.
Authors, title, year and journal were automatically extracted, facil­
itating the selection procedure and the creation of the PRISMA flow
diagram. A custom data extraction form was developed in the platform
and used for the data extraction process.
3. Results of the systematic search
In all, 1,332 abstracts were found (Table 2 and Fig. 1). 35 papers
were previously identified and used to select the search string based on
their keywords, while 1,297 were retrieved from searches in the dif­
ferent databases. After removing duplicates, 1,123 abstracts were
screened. 1,071 were excluded based on the exclusion criteria and 52
were revised in full text. From those, 33 were included in this review
analysis. From manual inspection of references lists, 2 more works were
During the revision process, detection was also considered as clas­
sification between healthy tissue and polyp classes or polyp and non-
polyp classes, but we excluded papers aiming at multi-class classifica­
tion (polyp among other classes) (such as Pogorelov et al. or Park
and Sargent ) or polyp classification (neoplastic/non-neoplastic)
(such as Ribeiro et al. ).
Full text documents were carefully read, and data included in
Table 1 were extracted for each work when available in the document.
Authors were contacted when deemed necessary for clarification.
Table 3 summarizes the 35 articles included in the analysis, together
with some relevant aspects. Some works address more than one task, so
they appear once per task in the table, resulting in 39 cases. There are 5
book sections (usually proceedings of major conferences), 18 con­
ference proceedings, 11 journal articles and one preprint. To unify
criteria for results reporting in Table 1, metrics reported as percentages
by authors have been expressed in the normalized range .
As it might be expected, there is an overall increasing trend since
2016 in the application of deep learning techniques for polyp detection,
localization and segmentation (Fig. 2).
Fig. 3 summarizes the networks used by the different authors. Ar­
chitectures have been grouped into 4 clusters: (1) CNNs, such as
AlexNet , VGG16 and VGG19 or GoogLeNet , including
also residual networks such as ResNet50 ; (2) fully convolutional
networks (FCNs), based on any CNN architecture, including also the
encoder-decoder architectures, such as SegNet or U-Net ; (3)
generative adversarial networks (GANs), and (4) recurrent neural net­
works (RNNs), including long short term memory (LSTM). It can be
clearly seen that the use of CNNs surpasses the rest of networks in the
three analyzed tasks.
A baseline comparison is needed to assess whether the proposed
method actually improves the results of the state-of-art. Out of the 35
analyzed works, only 10 of them present a baseline against which the
proposed method is compared. The baseline is usually the network on
which the proposed method is based, or hand-crafted methods. In other
12 cases, authors provide a comparison of their method against other
similar works. The most repeated comparison is against the methods
participating in the MICCAI 2015 Automatic Polyp Detection in
Colonoscopy Videos challenge . The remaining 13 works only
present their work, without any type of comparison.
As for reproducibility, we considered two aspects: the use of public
datasets and the availability of the code. Most works (29) use only
public datasets, while 3 use only private datasets and other 3 use both
public and private ones. The use of proprietary datasets hampers the
reproducibility and fair comparison of methods. The code of only three
works have been found . Therefore, Vázquez et al. and
Wickstrøm et al. outstand in terms of reproducibility, as the code is
available, and they use CVC-EndoSceneStill, which, as it will be ex­
plained in the following section, provides a division into training, va­
lidation and test sets.
4. Datasets of colonoscopy images
4.1. Currently available public datasets
The creation of large, annotated datasets has also contributed to the
tremendous growth of deep learning for the last years. Although they
are easily accessible for natural images with different ground truths (i.e.
ImageNet , MSCoco or Pascal VOC ), the limited
size of medical imaging datasets is a well-known problem, especially for
supervised learning . This situation also applies to colonoscopy
images datasets. The type of ground truth highly influences the size of
the dataset, since manual annotation of frames is a cumbersome, time-
consuming task . Table 4 shows a summary of the currently pub­
licly available datasets of colonoscopy images for polyp detection, lo­
calization and segmentation, although prior registration might be re­
quired by the dataset owner to grant access to the content. All datasets
are mentioned in at least one of the analyzed papers. Although these
datasets are widely used in the retrieved papers in this systematic re­
view, some authors also use proprietary datasetss, which compromises
a fair comparison of methods and raise concerns about reproducibility,
as mentioned in the previous section. Datasets of natural images have
been also included in Table 4 for scale comparison between computer
vision and biomedical image datasets. Works retrieved in this review
only employ these larger datasets to initialize the weights of the net­
work before training.
The organization of challenges under the umbrella of major con­
ferences has meant a great step towards the establishment of a common
framework. As a result, most of the currently used datasets were pro­
vided within a challenge. The Gastrointestinal Image ANAlysis (GIANA)
sub-challenge , as part of the EndoVis challenge , was last
hosted at MICCAI2017 and MICCAI2018. CVC-VideoClinicDB 
was provided as training dataset for the polyp detection task, while
CVC-ColonDB, CVC-ClinicDB and CVC-ClinicHDSegment datasets
 were provided for the polyp segmentation task. During
MICCAI 2015 Automatic Polyp Detection in Colonoscopy Videos chal­
lenge , two more datasets were released. On one side, ETIS-LARIB
was provided as test set for detection on still frames. On the other hand,
the ASU-Mayo Clinic was intended for exploring detection on vi­
deos. In all cases, the provided ground truth is a binary mask indicating
the polyp area. This segmentation is a precise manual delineation ex­
cept for the CVC-VideoClinicDB dataset, where the polyp area has been
1 
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
approximated to the most convenient elliptical shape. CVC-En­
doSceneStill has not been used in any challenge, but it is publicly
available. It compiles CVC-ColonDB and CVC-ClinicDB 
adding ground truth masks for other classes, establishing the distribu­
tion of images into training, validation and test sets and indicating the
metrics for reporting. Lastly, and not used as much as the previously
reported datasets, Kvasir is a multi-class image dataset containing
the polyp class among other labels for anatomical landmarks and pa­
thological findings.
There are as well other public datasets used by authors in the cur­
rent review. The Nerthus database , used by Pogorelov et al. ,
Data extraction.
Description
General information
Type of publication
Journal article, conference proceeding, book section (usually proceedings of a major conference), preprint
 
Title of the journal, conference or preprint repository
Country of the first author affiliation
Aim of the work: detection, localization and/or segmentation
Data information
Name of the public dataset and reference (if available) or proprietary
Training set
Description of the samples used for training, indicating the number (with and without data augmentation, if applied)
Validation set
Description of the samples used for validation, indicating the number
Description of the samples used for testing, indicating the number
Data augmentation
Data augmentation approach to generate new training samples: creation of images, on the flow, patch-based or none
Transformations
If data augmentation is applied, description of the transformations and ranges used
Type of approach used: feature extractor, classification, patch-based, bounding-box or semantic segmentation
Architecture
Type of the network (AlexNet, VGG, fully convolutional network, GAN, etc.)
Loss function
Loss function used for optimization
Use of pre-trained models or training from scratch
Reporting test
Samples used for reporting
Metrics used for reporting (accuracy, Dice, Intersection over Union, etc.)
Whether the authors establish an initial baseline model or compare their results to other works
Reproducibility
Whether the dataset is proprietary or publicly available
Whether the code is publicly available or not
Number of abstracts retrieved.
# abstracts
ACM Digital Library
IEEE Digital Library
Web of Science
Science@Direct
Previously identified
Fig. 1. Literature flow diagram.
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
Summary of reviewed works.
Data augmentation
Main results
Feature extractor, patch-
Taha et al. 
CVC-ColonDB
Prec = 0.927; Rec = 0.960
Comparison
Feature extractor, bounding-
Shin et al. 
CVC-ClinicDB, ETIS-LARIB, ASU-
Acc = 0.9126; Prec = 0.9271;
Rec = 0.9082; Spec = 0.9176
Patch-based
Tajbakhsh et al.
Sens = 0.7 @ FPPF = 0.02
Yuan et al. 
Acc = 0.9147; Rec = 0.9176
Tajbakhsh et al.
Sens = 0.7 @ FPPF = 0.02
Tajbakhsh et al.
Sens = 0.5 @ FPPF = 0.002
Tajbakhsh et al.
Sens = 0.5 @ FPPF = 0.002
Classification, patch-based
Axyonov et al.
CVC-ColonDB, ASU-Mayo
AUC = 0.92; Sens = 0.75 @ FPPF = 0.1
Comparison
End-to-end
Classification
Akbari et al. 
Acc = 0.9028; Rec = 0.6832;
Prec = 0.7434; Spec = 0.9497;
FPPF = 0.06
Comparison
Aksenov et al.
CVC-ColonDB, ASU-Mayo
TPR ≈ 0.98 @FPR = 0.025
Itoh et al. 
Proprietary
AUC = 0.83; Acc = 0.747; Sens = 0.881;
Spec = 0.617
Misawa et al. 
Proprietary
AUC = 0.87; Spec = 0.63 @ Sens = 0.90
Murthy et al. 
ISBI2014 challenge
Acc = 0.8743
Pogorelov et al.
CVC-ColonDB, CVC-ClinicDB, CVC-
VideoClinicDB, Kvasir, Nerthus
Xception, VGG19, ResNet50,
Acc = 0.909; Spec = 0.94
Bounding-box
Mo et al. 
CVC-ClinicDB, CVC-VideoClinicDB,
CVC-ColonDB, CVC-EndoSceneStill
Faster R-CNN (VGG16)
Acc = 0.985; Prec = 1; Rec = 0.985;
F1-score = 0.971; F2-score = 0.992
Comparison
Urban et al. 
Proprietary
On the flow
VGG16, VGG19, ResNet50
Acc = 0.964; ROC-AUC = 0.991
Semantic segmentation
Mohammed et al.
Creation, On the
Prec = 0.874; Rec = 0.844;
F1-score = 0.859; F2-score = 0.850
Brandao et al.
CVC-ClinicDB, ETIS-LARIB, ASU-
On the flow
FCN-AlexNet, FCN-
GoogLeNet, FCN-VGG
Prec = 0.7361; Rec = 0.8631
Comparison
Localization
Patch-based
Park et al. 
CVC-ClinicDB, ASU-Mayo
Prec = 0.6575; Rec = 0.8276
Bounding-box
Shin et al. 
CVC-ClinicDB, ETIS-LARIB, ASU-
Mayo, CVC-VideoClinicDB
Faster R-CNN (Inception
ResNet-v2)
Prec = 0.914; Rec = 0.803;
F1-score = 0.833; F2-score = 0.815
Comparison
Yu et al. 
Prec = 0.881; Rec = 0.710
Zhang et al. 
ASU-Mayo, CVC-ClinicDB, ETIS-
On the flow
Prec = 0.886; Rec = 0.716; Spec = 0.970
Bounding-box, semantic
segmentation
Wang et al. 
CVC-ClinicDB, proprietary
Sens = 0.9438; Spec = 0.9592
Feature extractor, patch-
Billah et al. 
Mesejo, ASU-Mayo, CVC-ClinicDB,
ETIS-LARIB
Acc = 0.9865; Sens = 0.9879;
Spec = 0.9852
Comparison
End-to-end
Bounding-box
Mo et al. 
CVC-ClinicDB, CVC-VideoClinicDB,
CVC-ColonDB, CVC-EndoSceneStill
Faster R-CNN (VGG16)
Prec = 0.862; Rec = 0.981;
F1-score = 0.917; F2-score = 0.956
Comparison
Zheng et al. 
CVC-ColonDB, CVC-ClinicDB, ETIS-
LARIB, proprietary
Prec = 0.774; Sens = 0.740;
F1-Score = 0.757; F2-score:0.747
Pogorelov et al.
ASU-Mayo, proprietary
TensorBox, Darknet-YOLO
Acc TensorBox = 0.316; Acc
Darknet-YOLO = 0.422
Classification
Urban et al. 
Proprietary
On the flow
VGG16, VGG19, ResNet50
Dice = 0.827 ± 0.003
Patch-based, classification,
semantic segmentation
Pogorelov et al.
CVC-ColonDB, CVC-ClinicDB, CVC-
VideoClinicDB, Kvasir, Nerthus
Xception, VGG19, ResNet50,
Acc = 0.946; Spec = 0.984
(continued on next page)
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
provides a classified set of videos depending on the Boston bowel
preparation scale, therefore not providing any polyp information and
being excluded from Table 4. Mesejo et al. also provide a labelled
dataset of 76 videos of different lesions (serrated adenomas, hyper­
plastic lesions and adenomas), which is used by Billah et al. . Since
optical biopsy for polyp classification is out of the scope of the current
review, the dataset has not been included in Table 4.
Regarding clinical variability, few datasets indicate the type of
polyps included. The Paris classification is a general framework
for the endoscopic classification of superficial lesions of the oeso­
phagus, stomach, and colon. Fig. 4 shows the different types of polyps,
both in schematic view and actual endoscopic images. Pedunculated
and sessile polyps are easier to detect than flat polyps and CAD systems
to assist their detection would be more useful for gastroenterologists,
but regretfully they are underrepresented in the public datasets .
4.2. Use of the datasets and data augmentation
Authors do not follow a standard methodology to distribute the
dataset into training, validation and test sets, except for those using
CVC-EndoSceneStill, because the distribution is provided by the dataset
owner; or those following the rules of the MICCAI 2015 Automatic
Polyp Detection in Colonoscopy Videos challenge. Works for detection
use a greater number of images than those for segmentation, which
might be because labelling frames is easier than manually segmenting
polyps for ground truth creation (Fig. 5). Test sets are usually one or
two degrees of magnitude smaller than the training sets. Due to this
heterogeneity in training, validation and test sets, it is no easy to make a
fair comparison of the methods and their reported metrics.
Data augmentation is the process whereby the training dataset is
artificially increased in size, which in medical imaging is typically done
with transformations that are applied to only the image in the case of
detection (as each image only have a label that remains unaffected) or
to the image and mask in the same way in the case of localization and
segmentation. Augmentation methods commonly employ transforma­
tions such as rotations, reflections, and elastic deformations . Data
augmentation strategies are used by 24 out of the 35 analyzed papers
(Table 3). The most common approach (8 works) is to enlarge the
training set by creating new images through the application of trans­
formations to the original images. Other authors (7 works) make data
augmentation on the flow, i.e. transformations to the original image are
randomly applied at training time, increasing the variability of the
training set but without specifically creating new transformed images.
Lastly, some other authors (8 works) train the models using patches
extracted from the original images rather than using the full image.
More recently, Nguyen and Lee proposed a data augmentation
approach at a pixel level for polyp segmentation.
To create new images, there is a wide variability on the transfor­
mations applied and their ranges (Table 5), as data augmentation is
typically performed by trial and error and transformations are selected
based on the imagination, time and experience of the researcher .
None of the authors justify the selection of neither the transformations
nor the ranges. Few authors analyze the influence of data augmentation
in the results. Vázquez et al. compare results with and without
different transformations, while Shin et al. compare the influence
of including more or fewer transformations. While the former identified
that the combination of transformations leads to better results, the
latter found that more augmentation does not guarantee better per­
The different methods for data augmentation lead to a wide varia­
bility in terms of the actual training samples used (Fig. 5).
5. Comparison and discussion of methods
CNN architectures are a type of neural networks which
are specialized for data with grid-structured topology. CNNs are
Table 3 (continued)
Data augmentation
Main results
Segmentation
Patch-based
Zhang et al. 
CVC-ColonDB
Acc = 0.9754; Rec = 0.7566;
Spec = 0.9881; Dice = 0.7014
Comparison
End-to-end
Semantic segmentation
Nguyen and Lee
CVC-ClinicDB, ETIS-LARIB
Pixel level
Encoder-decoder
Dice = 0.889; IoU = 0.8935; Acc = 0.984
Comparison
Wichakam et al.
CVC-EndoSceneStill
On the flow
Compressed FCN-8s
Prec = 0.8848; Rec = 0.7814;
IoU = 0.6936; Dice = 0.9594
Wickstrøm et al.
CVC-EndoSceneStill
On the flow
Enhanced FCN-8, enhanced
IoU = 0.767; Acc = 0.949
Comparison
Xiao et al. 
CVC-ClinicDB
DeepLab-v3+LSTM
IoU = 0.9321
Comparison
Zhou et al. 
IoU = 0.3345
Bardhi et al. 
CVC-ColonDB, CVC-ClinicDB, ETIS-
On the flow
Acc = 0.967
Brandao et al.
CVC-ClinicDB, ETIS-LARIB, ASU-
On the flow
FCN-AlexNet, FCN-
GoogLeNet, FCN-VGG
Prec = 0.7023; Rec = 0.5420
Comparison
Li et al. 
CVC-ClinicDB
Acc = 0.9698; Rec = 0.7732;
Spec = 0.9905
Vázquez et al.
CVC-EndoSceneStill
On the flow
IoU Polyp = 0.5160; MGA = 0.9677
N/A: Not applicable; CNN: convolutional neural network; FCN: fully convolutional network; Prec: precision; Rec: recall; Sens: sensitivity; IoU: Intersection over Union; MGA: mean global accuracy; MCA: mean classes
accuracy; Spec: specificity; FPPF: false positive per frame; TPR: true positive rate; FPR: false positive rate; NF: not found.
a 
b 
c 
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
composed of different hierarchical stages that take advantage of local
connections, shared weights, pooling layers and the use of many layers.
The first set of stages may include a certain number of convolutional
layers (namely two or three) followed by subsequent pooling layers.
Convolutional layers exploit the local connections and shared weights
by using the convolution operation instead of matrix multiplication and
pooling layers subsample the data and merge similar features. Blocks of
convolutional and pooling layers are then stacked to create a feature
vector that represents the input data. Fully connected layers are latterly
connected to this vector for the final object classification. One of the
simpler and classical implementations of this stackable approach is
VGG , where different stackable layers of 3 × 3 convolutions and
maximum pooling layers are concatenated. However, this network
presents a large number of parameters and slow convergence. He et al.
 proposed the so called residual neural networks, which include
skip connections so learnt filters are applied not to the final transformed
image but to the residual over the input image instead, allowing deep
neural networks to go deeper by mitigating the vanishing gradient
problem and providing the first layers with larger scale gradients during
backpropagation. Any CNN architecture can be extended into a FCN
 by using a classification network as an encoder that is convolved
over larger images producing spatially dense prediction tasks. However,
these FCNs lacked the capability of generating fine shape delineation as
high resolution reconstruction was calculated by interpolation. These
capabilities were introduced by the addition of deconvolution layers,
skip connections and pooling indices . This segmentation ar­
chitecture was improved by using the fully convolutional DenseNet for
image segmentation .
Based on the aforementioned works, different efforts have been
followed during the last years for polyp detection, localization and
segmentation, where deep learning-based approaches have proven to
excel hand-crafted methods. An initial division of methods has been
established by Bernal et al. . Two different types of methods are of
interest in this review: (1) hybrid methods that combine deep learning
approaches with other hand-crafted methods and (2) end-to-end
methods that use one single deep learning approach to obtain the result.
A third type also mentioned by Bernal et al. , hand-crafted
methods, lie out of the scope of this review. As secondary classification,
we have grouped the approaches into five types, depending on their use
of the deep learning network (Fig. 6):
1. Feature extractor. Deep learning architectures are used for auto­
matically creating a feature vector, instead of the manual extraction
of features. The computed vector is afterwards the input to a clas­
sical classifier, such as support vector machine (SVM) or distance-
weighted discrimination (DWD) classifier , usually more ro­
bust than SVM. These methods are therefore always hybrid, as deep
learning is combined with classical classifiers.
2. Classification. A classification network is used to label an image as
containing a polyp or not, without position information of the polyp.
3. Patch-based. The method uses image patches or tiles and the pre­
sence of polyp is obtained for each patch. Location of the polyp
might be obtained based on the patch location.
4. Bounding-box. The method provides the location of the polyp
through a bounding-box (coordinates of the upper right corner,
height and width), generally using a regression layer.
5. Semantic segmentation. Each pixel of the image is labelled as polyp
or background. Networks based on encoder-decoder blocks are
usually selected. The first half of the layers encode the image de­
scription highlighting the discriminative features for the entrusted
task, while the second half is responsible for mapping the low-re­
solution encoding into full input resolution feature maps. These
FCNs can be initialized from the weights of a classification network
(encoder) that acts as a feature extractor.
All papers have been categorized using these primary and secondary
classifications. While the primary classification is mutually exclusive,
some works falls in more than one group of the secondary classification.
Fig. 7 shows the distribution of papers. Since tasks and primary
Fig. 2. Literature trends.
Fig. 3. Networks for each of the tasks. CNNs: Convolutional neural networks; FCNs: fully convolutional networks; GANs: generative adversarial networks; RNNs:
recurrent neural networks.
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
White-light datasets for polyp detection, localization and segmentation. Main natural image datasets are also included for comparison.
Ground truth
Delineation
Resolution
# patients # items
Training set
Validation set
CVC-EndoSceneStill
BM (border, polyp,
lumen and specular
lights classes)
Manual by experts
500 × 574;
CVC-ColonDB 
BM (polyp)
Manual by experts
CVC-ClinicDB 
BM (polyp)
Manual by experts
0-Ip, 0-Is,
0-IIa, 0-IIb
ETIS-LARIB 
BM (polyp)
Manual by experts
1255 × 966
ASU-Mayo Clinic 
Video (WP and WO
BM (polyp) (only
training set)
Manual by experts
712 × 480;
856 × 480;
1920 × 1080
38 videos (8,591
WP; 27,979 WO)
(4,278 WP;
14,718 WO)
(4,313 WP;
13,261 WO)
CVC-VideoClinicDB
Video (WP and WO
BM (polyp) (only
training set)
Elliptical
approximation by
0-Is, 0-Ip,
Kvasir 
Multi-Class Image
Dataset, containing
the polyp class
from 720 × 576 up
to 1920 × 1072
500 images in
polyp class
ImageNet 
Natural images
Label and BB
Manual by annotators
in Amazon Mechanical
14,197,122
Up to 150,000 12
MSCoco 
Natural images
BM (171 classes),
BB, person
keypoints, captions
Manual by annotators
in Amazon Mechanical
Up to 118,287
Up to 5,000
Up to 40,670
Pascal VOC 
Natural images
BM (20 classes), BB
Manual by annotators
in Amazon Mechanical
19,041 images
instancesc
9,477 images
9,564 images
WP: With polyp; WO: without polyp; BM: binary mask; BB: bounding-box; seqs: sequences; N/A: not available.
a According to Paris classification.
b Total add more than retrieved works because several authors use more than one database.
c Only training and validation sets.
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
Fig. 4. Paris classification. Adapted from .
Fig. 5. Number of elements used for training and test, categorized by the main task of the work. ‘Elements’ refers to images for training and test sets as well as
training samples, which might differ from training images if patches are extracted or data augmentation is applied.
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
Transformations for data augmentation on full images.
Creation/flow
Translation
Warping Flip
Contrast Brightness Crop Increment
Aksenov et al. 
Mohammed et al.
Creation, on the
(10°,350°)
Murthy et al. 
90°, 180°, 270° –
Localization
Shin et al. 
90°, 180°, 270° –
90°, 180°, 270° –
90°, 180°, 270° –
resolutions
Zhang et al. 
On the flow
Zheng et al. 
90°, 180°, 270° –
Yu et al. 
90°, 180°, 270° (−3, 3)
Segmentation
Wichakam et al. On the flow
Up to 180°
(0, 20%) H/V –
(-0.8, 1.2) (0, 0.2)
Wickstrøm et al. On the flow
(-90°, 90°)
(0.8, 1.2)
Bardhi et al. 
On the flow
0.5 dropout
Li et al. 
Vázquez et al. 
On the flow
(0°, 180°)
(0.9, 1.1)
Detection and localization
Pogorelov et al. 
Urban et al. 
On the flow
Detection and
segmentation
Brandao et al. 
a They modify 40% of the images to increase variability, but do not create more new training examples. H: horizontal; V: vertical.
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
classification are mutually exclusive, six Venn's diagrams are necessary
to show the overlap between the categories of the secondary classifi­
cation. Venn's diagrams have been created with InteractiVenn .
End-to-end and hybrid methods have similar proportion in detection
and localization (close to 50% each), but in segmentation, the end-to-
end methods vastly surpass hybrid ones (9 vs 1). Regarding the sec­
ondary categorization, classification and patch-based ranked equally
for the detection task; while bounding-box and semantic segmentation
are the preferred approaches for localization and segmentation, re­
spectively. Hybrid, patch-based methods and end-to-end, semantic
segmentation methods are the most usual combinations.
Fig. 8 shows how the different types of methods have had presence
along the years analyzed in this systematic review. In summary, the
tendency goes towards the use of end-to-end methods over hybrid ones,
as deep learning is gaining more and more capabilities to address
complex problems as a whole, rather than being used as a component to
codify the image into features that are afterwards further analyzed.
Similarly, semantic segmentation has raised interest over the last two
years as it comprises a straightforward and seamless method for polyp
identification.
The use of data augmentation and pre-trained networks in the re­
trieved works have also been analyzed. Fine-tuning is a well-known
alternative to training a network from scratch when the labelled
training data is limited. In this case, pre-trained networks on a large
labelled dataset from a different application is used as starting point
 . Fig. 9 shows the number of training samples after applying data
augmentation, if any, of each method according to the type of ap­
proach. There is no clear trend in the use of data augmentation stra­
tegies combined with pre-trained models in the considered approaches.
It might be expected that using pre-trained models would be linked to a
lower number of training samples, but findings do not show so.
Lastly, it is worth mentioning that all retrieved works apply su­
pervised learning, relying its training on a labelled set of images. Other
learning approaches already applied in different medical fields, such as
unsupervised learning or few-shot learning have not
been applied for polyp detection, localization or segmentation yet.
Unsupervised learning might be more difficult to apply because the
polyp area is usually a small portion of the image, while the rest pre­
sents a high level of similarity, as there is also healthy mucosa in an
image labelled as with polyp.
In the following sections, methods are briefly described, grouped
accordingly to the primary and secondary classifications. When
methods fall into two groups of the secondary classification, they are
each indicated in a different paragraph.
5.1. Methods for polyp detection
5.1.1. Hybrid methods
5.1.1.1. Feature extractor and patch-based. Several authors have
compared the use of hand-crafted advanced features with simple fine-
tuned classification CNNs, and in all cases the CNN-based approaches
overcame the manual feature extraction. Shin et al. demonstrate
that features obtained with a classification CNN perform much better
than hand-crafted features even when using state-of-art histogram of
oriented gradients descriptors. They employ a basic architecture of
three convolution:max pooling layers followed by a fully connected
layer with 256 neurons image patches to feed an SVM.
Similarly, Taha et al. perform a comparative testing of a
shallow classifier fed either with hand-crafted methods or with features
extracted using a pre-trained classification CNN. These authors employ
AlexNet to obtain a 1,000-dimensional feature vector that is the input
to an SVM.
In these works, images patches are used, so these methods can also
be classified as patch-based detection but without localization in­
formation, as location of patches is not considered.
5.1.1.2. Patch-based. In this case, Yuan et al. propose a 2-stage
method. In first place, candidates are detected by the analysis of edges.
Patches are then cropped around the polyp candidates and the resulting
candidate patches are analyzed by a classification network based on
Tajbakhsh et al. in various works extend this method by
using a pre-detection stage based on edge maps and voting schemas to
extract all suitable candidates that are classified by AlexNet, either
trained from scratch or fine-tuned, depending on the work. Oriented
patches are extracted based on shape features, conforming the set of
candidates.
5.1.1.3. Classification and patch-based. In this case, Axyonov et al. 
combine image contrasting and the K-means-with-connectivity-
constraint segmentation method to identify regions with similar
pixels, which are then classified into polyp or non-polyp region using
AlexNet as classifier.
5.1.2. End-to-end methods
5.1.2.1. Classification. Akbari et al. use a CNN made of four
convolutional and pooling layers plus two fully connected layers. This
CNN is combined with binarized weights and kernels to reduce the CNN
size, so it is suitable for implementation in portable medical hardware
with limited memory.
On the other hand, Aksenov et al. combine various classifica­
tion networks through classifier assembling to get higher accuracies,
basing the final result on the average result of the three ensembled
models. Each model presents a different number of layers as well as
different configurations for the filters.
Itoh et al. aim at detecting polyps as a prior stage to polyp size
estimation. In their approach, they use a 3D CNN (C3dNet or C3D)
 that exploits both the spatial structure and the temporal features
present in colonoscopy videos by using 3D convolutional filters and 3D
pooling layers. The input is a sequence of 16 consecutive frames for
which the CAD system provides an output probability of being a se­
quence with or without polyp. The same network is used by Misawa
et al. , who focused their effort on a clinical trial to measure the
sensibility of the C3dNet.
A different approach is followed by Mo et al. , who employ a
Faster R-CNN with VGG16 as backbone. In this case, they use an ap­
proximately joint optimization, which takes a mini-batch as input and
optimizes both the classification and regression losses at the same time.
The classification tail allows for detecting polyps in a frame by giving a
probability level.
Classification networks have also been proven successful even on
colon cancer screening programmes . In this case, they test pre-
trained VGG16, VGG19 and ResNet50 with a final binary classification
layer, as well as custom, trained-from-scratch CNNs for polyp detection
without further processing. Images were classified on polyp or non-
polyp classes.
Lastly, Murthy et al. introduce a cascaded deep decision net­
work (CDDN) for classification. In the first learning stage, samples are
classified by a pre-trained network. In the second training stage, sam­
ples classified with high confidence are discarded to place efforts on the
most challenging samples. This process is repeated on successive stages,
using previous stage's features, and obtaining a better separation over
samples. In this particular case, a 2-stage network is proposed.
5.1.2.2. Classification and semantic segmentation. Pogorelov et al. 
compare three detection approaches: (1) hand-crafted global features
and a logistic model tree classifier; (2) fine-tuned well-known
architectures, such as Xception , VGG19 and ResNet50; and (3)
a pixel-wise segmentation GAN with a threshold on the number of
positively labelled pixels. They use different training and testing sets for
the three suggested methods and find that the GAN approach performs
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
Fig. 6. Schematic representation of the five considered approaches in this review. From top to bottom, (1) feature extractor, (2) classification, (3) patch-based, (4)
bounding-box, and (5) semantic segmentation. Each type of layer is represented by a different colour: convolutional layer (conv); pooling layer (pool), fully
connected layer (FC), upsampling layer (upsamp) and deconvolutional layer (deconv). The receptive field is marked with a green square. (For interpretation of the
references to colour in this figure legend, the reader is referred to the web version of this article.)
Fig. 7. Categorization of works per tasks.
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
5.1.2.3. Semantic segmentation. Mohammed et al. design the Y-
Net, which combines two encoders (both use VGG19 as backbone; pre-
trained for encoder 1, while trained-from-scratch for encoder 2) and
one decoder to produce a pixel-wise segmentation. Detection is
considered when the Intersection over Union of the predicted result
and the ground truth is greater than 0.90. In contrast, although Brandao
et al. also base detection on the semantic segmentation performed
with different FCNs, they consider any degree of overlap between the
predicted result and the ground truth. Therefore, not setting an overlap
minimum might lead to better metrics in comparison to Mohammed
et al. , who set their overlap threshold at 0.9.
5.1.3. Comparison of detection methods
Although it is difficult to compare results because of the different
datasets, we focus firstly on accuracy as an important indicator to
compare between models and approaches (Table 6). With this in mind,
Mo et al. deliver the best results in detection. These results are
obtained using three different test sets (CVC-ClinicDB, CVC-ColonDB
and CVC-EndoSceneStill) and averaging the results obtained for each
one. Urban et al. also achieve a high accuracy. In this case, the
accuracy is identical in both cross-validation and independent test set of
a proprietary dataset. Remarkable results in a test set from ASU-Mayo
Clinic database are also those presented by Yuan et al. , who
achieve an accuracy of 0.9147, with a recall of 0.9176. Akbari et al.
 also use the same ASU-Mayo Clinic database but with a distinct
test set. In this work, the accuracy obtained is 0.908, and the recall,
precision and specificity are 0.6838, 0.7434 and 0.9497, respectively.
Confidence intervals (CI) are indicated when provided by authors. This
applies also to Sections 5.2.3 and 5.3.3.
5.2. Methods for polyp localization
5.2.1. Hybrid methods
5.2.1.1. Patch-based. Park et al. use an architecture composed of
three convolutional layers and three max-pooling layers to obtain a 60-
dimension feature vector for each patch obtained at three different
image scales. The resulting 180-dimension feature vector is used to
classify the centre pixel of the patch either as polyp or non-polyp,
through a fully connected network with 256 hidden nodes. A
probability map is created based on these classified pixels. This map
is smoothed with a 5 × 5 Gaussian filter and 9 × 9 median filter.
Afterwards, it is thresholded, setting to 0 those pixels with probability
lower than 0.65. Lastly, connected components of non-zero regions are
identified, and the polyp centre is obtained by calculating the centre of
mass of each connected component.
5.2.1.2. Feature extractor and patch-based. Billah et al. use a 10-
layer CNN to extract features from the last fully connected layer, which
are then combined with wavelet features and all of them fed into an
SVM for classification into polyp or non-polyp. Patches corresponding
to a sliding window are the input, so location of the polyp is found by
averaging regions with higher probabilities of being polyp.
5.2.1.3. Bounding-box. The method of Shin et al. combines a
region proposal network (RPN), a detector and post-learning
approach in the so-called Faster R-CNN, using Inception ResNet-v2
 as backbone. Out of an image, the RPN proposes rectangular
candidate regions that are the input to the detector, which classifies
them into containing or not containing a polyp. The detection is further
improved by the post-learning approach, based on false positive
learning and off-line learning.
Yu et al. integrate temporal information into the model. Their
3D-FCN is capable of learning more representative spatio-temporal
features from colonoscopy videos and hence has more powerful dis­
crimination capability. This 3D network consists of a 3D extension of a
2D fully convolutional segmentation network that uses a 16 frames
video entrance to extract the temporal features. An offline 3D-FCN is
firstly trained, which is combined with an online 3D-FCN incrementally
updated for each input video to remove false positives. Outputs of these
two 3D-FCNs are combined to obtain the final detection results.
Moreover, Zhang et al. propose an evolution of a YOLO de­
tection network , ResYOLO. The previous detection output of the
network is integrated into the following prediction to assure prediction
regularization. Besides, temporal information is incorporated as an
online object tracker. They also analyzed that the inclusion of temporal
information on the network improves the detection rates.
5.2.1.4. Bounding-box and semantic segmentation. SegNet is well-known
for semantic segmentation of natural images, so it is the network
selected by Wang et al. . Although SegNet provides a pixel-wise
labelling in the form of a probability map, there is a post-processing
stage that transforms it into the corresponding bounding-box for the
polyp class.
5.2.2. End-to-end methods
5.2.2.1. Bounding-box. As mentioned before, Mo et al. use a Faster
Fig. 8. Trends of the different approaches along the years.
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
R-CNN with VGG16 as backbone. In this case, the regression tail
provides the coordinates of the bounding-box indicating the location
of the polyp.
Urban et al. compare models trained from scratch to pre-
trained models (VGG16 and VGG19 with a final regression layer), ob­
taining a higher Dice for the latter when ground truth and predicted
bounding-boxes are compared.
On the other hand, YOLO is selected by Zheng et al. as the
detector network, without further modifications on the network and
proceeding only with a fine-tuning of a pre-trained model.
Pogorelov et al. compare two different approaches, TensorBox
and Darknet-YOLO, both aiming at detecting objects in images. On one
hand, TensorBox avoids multiple detections of the same object by using
an RNN with an LSTM. On the other hand, Darknet-YOLO is based on a
CNN, therefore encoding contextual information about classes as well
as their appearance. This results in a better generalization of objects’
representation. In both cases, the methods return sets of rectangles
Fig. 9. Training samples per approach and type of data augmentation. Solid items correspond to methods where networks are trained from scratch, while hollow
items correspond to fine-tuned networks. In the horizontal axis, the different approaches are indicated. The series corresponds to the different strategies for data
augmentation.
Summary of detection methods results.
Specificity
Mo et al. 
Urban et al. 
Yuan et al. 
Akbari et al. 
AUC:area under the curve
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
marking possible polyp locations together with corresponding location
confidence values.
5.2.2.2. Classification and patch-based. Pogorelov et al. use sliding
windows to feed the classification models (hand-crafted global features,
fine-tuned networks and GAN approach grounded on V-GAN 
modified by adding an activation layer to generate a per-pixel image
segmentation, so detection is based on a minimum number of activated
pixels) and then reconstruct a coarse localization map by grouping-back
the processed patches.
5.2.3. Comparison of localization methods
Out of all localization methods (Table 7), Billah et al. report
the highest metrics. In this case, colour wavelet features and CNN
features are combined, so it is not possible to determine which type of
feature has a greater influence on the result. Besides, they do not report
the testing data set, neither in terms of number of images nor their
origin, which makes it more difficult to compare results. On the con­
trary, Mo et al. clearly indicate the testing datasets (CVC-ClinicDB,
CVC-ColonDB and CVC-EndoSceneStill), reporting a mean recall which
is comparable to the previous work, but providing a more solid evi­
dence and reproducibility of results. Nevertheless, no further works use
the same validation set. In this regard, Yu et al. and Zhang et al.
 do follow the rules of the MICCAI 2015 Automatic Polyp Detection
in Colonoscopy Videos challenge in terms of testing dataset. This way,
they provide a fair and straightforward comparison to other methods
participating in the challenge . Although none of them outperform
ASU, the winning method, in precision, they do provide better recall,
F1-score and F2-score, showing a more balanced performance.
5.3. Methods for polyp segmentation
5.3.1. Hybrid methods
5.3.1.1. Patch-based. The focus of Zhang et al. is placed on the use
of texton-based spatial features for detailed classification that is used to
remove false positives based on local textural analysis. In this case,
results of the FCN-8s are used to extract the image region
proposals. These regions are refined using texton-based patch
representation, which is followed by a random forest classifier to
provide the final segmentation.
5.3.2. End-to-end methods
5.3.2.1. Semantic segmentation. Nguyen and Lee take an encoder-
decoder model as basis and then produce the polyp segmentation using
a model combination by training the encoder-decoder model with three
different resolutions databases.
Besides, Wichakam et al. present a compressed FCN that re­
duce the number of parameters of the feature vector extracted by the
network to minimize the computational time showing faster con­
vergence and increased performance for polyp segmentation. The
model is compressed by substituting two 7 × 7 ×4096 con­
volutionalized layers by one 7 × 7 ×512 convolutionalized layer, re­
ducing the number of trainable weights in a significant manner.
The proposal of Wickstrøm et al. is to enhance two traditional
encoder-decoder networks (FCN-8s and SegNet, using both VGG16 as
encoder) by including batch normalization after each layer and dropout
after the three central encoders and decoders. They also analyze un­
certainty and interpretability of the models.
On the other hand, Xiao et al. combine LSTM with DeepLab-v3
in parallel. While the latter learns and extracts polyp features thanks its
wide field-of-view and higher resolution, the former aims at preserving
the information of the polyp location using the information stored in
the memory cells These are regulated through the input, forget and
output gates.
The U-Net has been modified into the U-Net++ by Zhou et al. .
The main difference is the inclusion of nested dense convolutional
blocks that bridge the semantic gap between the feature maps of the
encoder and decoder prior to fusion.
Bardhi et al. directly use SegNet and train it from scratch on
different datasets, while Brandao et al. transform several tradi­
tional classification backbones for the segmentation networks into FCNs
and prove that VGG16 backbone works better than GoogLeNet and
Similarly, Li et al. propose an FCN and U-Net based segmen­
tation network. The encoder is composed of 8 convolution layers, 8
rectified linear unit (ReLU) layers and 5 pooling layers, while the de­
coder includes 5 deconvolution layers, 5 concat layers, 6 convolution
layers and 6 ReLU layers. Both stages are linked using skipping con­
Finally, Vázquez et al. provide an exhaustive benchmark
showing that FCNs outperform previous results. They implement the
FCN-8s architecture and test the influence of data augmentation and
number of classes to be segmented on the network performance.
5.3.3. Comparison of segmentation methods
Table 8 summarizes the results of the segmentation methods. The
highest Intersection over Union (IoU) values are obtained by Xiao et al.
 , in 345 images from CVC-ClinicDB, and Nguyen and Lee ,
reporting on the ETIS-LARIB dataset. In both cases, mean IoU is re­
ported, so both polyp and background classes are considered, which
explains to a great extent those values close to 1. This fact can be clearly
seen in the work by Wickstrøm et al. . IoU for the polyp class is
0.587 but mean IoU is equal to 0.767 thanks to the value of IoU for the
background, as high as 0.946.
The comparison of methods that use CVC-EndoSceneStill is fair and
straight-forward, as the dataset owners provide its division into
training, validation and testing sets. Therefore, Vázquez et al. ,
Wichakam et al. and Wickstrøm et al. report results on the
same 182 images. Regretfully, and although the CVC-EndoSceneStill
benchmark provides a set of metrics (IoU and accuracy), not all authors
calculate them. Wichakam et al. follow instead the metrics given
by the Pascal VOC challenge . Vázquez et al. report slightly
higher values than Wickstrøm et al. in terms of accuracy but it is
on the contrary when IoU is considered. In this regard, Wichakam et al.
 report an intermediate value of IoU.
Bardhi et al. do not clearly state the division into training and
testing datasets; therefore results, although showing high values, should
be interpreted carefully. The only hybrid method for segmentation 
obtains comparable results to the rest of end-to-end methods.
In the segmentation task, it is important to remark the influence of
including the background class when calculating metrics. Since back­
ground usually means the largest area within a frame in comparison to
the polyp class, background affects the results by increasing the metric
value even when the segmented result is poor. This issue is further
discussed in Section 6.
5.4. Advantages and disadvantages
Table 9 gathers the main advantages and disadvantages of the ca­
tegories on which the works have been classified.
Summary of localization methods results.
Billah et al. 
Mo et al. 
Yu et al. 
Zhang et al.
0.716 101923
5.5. Loss functions
The final goal against which the network is optimized is completely
defined by the loss function. While traditional loss functions for clas­
sification (such as negative log-likelihood – NLL – or cross entropy),
regression (i.e. L1-loss or mean squared error – MSE) and distribution
matching tasks (such as the Kullback-Leibler – KL – divergence) might
be generalizable enough for most of the problems, the selection of the
loss function is more relevant for complex problems such as the seg­
mentation of unbalanced classes . In these cases, an inappropriate
formulation of the loss function might cause that the network converges
into a minima where the task is not achieved because the network does
not behave as expected. In this review, only 22.86% of the analyzed
works present an analysis on the selected loss function and less than
15% of the authors use a custom loss function. These works are briefly
commented below.
The loss in the Faster R-CNN used by Mo et al. consist of a
classification loss and a bounding-box regression loss, using para­
meterized coordinates to minimize the influence of scales during
training. Besides, Zhang et al. employ a loss function comprised of
loss for grids labelled as object (polyp) and non-object. Furthermore,
other three works include the Dice coefficient. On one hand, Wichakam
et al. optimize the network with a custom loss function simply
computed as 1 − Dice. On the other hand, while Zhou et al. 
combine this coefficient with binary cross-entropy, Mohammed et al.
 use the weight binary entropy instead.
When deciding the loss function for detection, it is important to
know beforehand the proportion of polyp and non-polyp images in the
training set. If it is balanced, traditional loss functions might be enough.
Otherwise, it would be advisable to use a loss function intended to
overcome the unbalance towards the positive class, such as the
weighted binary cross entropy. We also agree on including overlap
measures such as the Dice coefficient in the loss function for segmen­
tation tasks when classes are highly unbalanced, as this type of func­
tions have been proved to be more robust . Since segmentation
can be done as a pixel-wise binary classification of unbalanced classes,
comments for detection can also be applied here.
6. Metrics
There is a wide variety of metrics found in this systematic review. In
all cases, metrics are intended to compare the prediction of the method,
either for detection, localization or segmentation, against the ground
truth, which might be a label, a bounding-box or a binary mask. In this
regard, many metrics are calculated based on the confusion matrix and
its four basic elements (Fig. 10):
• True positives (TP): number of polyp items correctly predicted as
• True negatives (TN): number of non-polyp items correctly predicted
as non-polyp.
• False positives (FP): number of non-polyp items incorrectly pre­
dicted as polyp.
• False negatives (FN): number of polyp items incorrectly predicted as
non-polyp.
It is also important to point out that some authors compute the con­
fusion matrix at a frame or image level, while others compute it at a
pixel level. This mainly depends on the main objective of the work,
being more usual to calculate the confusion matrix at frame level for
detection and localization, and at pixel level for segmentation.
Table 10 compiles all metrics found during the analysis, providing
the mathematical calculation and alternative names. The calculation of
the Dice coefficient and IoU metrics based on the confusion matrix can
be used when calculating the confusion matrix at pixel level by com­
paring two binary masks. These metrics are also more appropriate as
they reduce the unbalance due to the large value of true negatives. Two
definitions for accuracy have been found in this review. On one hand,
only TP are considered in the numerator. This definition can be inter­
preted as class accuracy, as the sum of accuracies for all classes would
reach 1 in an ideal situation. On the other hand, both TP and TN are
considered in the numerator. In both cases, the denominator remains
unchanged, being the total sum of elements (TP+TN+FP+FN). In the
case where the formula is not explicitly provided, accuracy has been
considered as correct classified items (TN+TP) divided by the total
number of elements.
Table 11 summarizes the metrics employed for the different con­
sidered tasks. There is no standard to follow in terms of reporting
metrics; therefore for one single task authors report different metrics.
Overall, recall is the most used metric followed by precision, accuracy
and F1-score in similar proportion. As for each of the analyzed tasks,
recall at frame level is the predominant metric for detection and loca­
lization, while IoU is the most used metric for segmentation, closely
followed by accuracy at pixel-level.
For detection and localization, recall stands out. This metrics pe­
nalizes a high number of FN but does not consider FP. In a clinical
setting, both parameters are equally important, as if gastroenterologists
are warned unnecessarily, the exploration time might be increased
without benefits and they would eventually pay no attention to the CAD
system ringing false alarms. Therefore, we consider a more suitable
metrics the use of the F1-score, where importance of recall and preci­
sion is balanced.
Regarding segmentation metrics, it is important to point out that
their selection must consider the properties of the segmentation under
evaluation, so when the segment size is smaller than the background
(less than 5% of the background in any of the axis, as it is usually the
case in polyp segmentation), metrics based on the confusion matrix
elements are not the more suitable and it is recommended to substitute
them by distance metrics . Therefore, when background is not
considered in the calculation of the metric, a smaller increment in its
value might be more significant than the same increment in a metric
considering the background. Bearing this in mind, reporting accuracy,
where true negatives corresponding to the background are included,
does not seem to be the most adequate one, despite being one of the
most common.
It is noteworthy to remark that traditional object detection metrics
have not been found in any of the analyzed works. These metrics, such
as average precision (AP) or mean average precision (mAP), measure
the average precision of a model for a specific IoU and are commonly
used on computer visions challenges . This is relevant as these
metrics provide a single value estimation on the performance of object
detection models. However, although mAP can be used for computa­
tional performance estimation, it does not reflect the clinical perfor­
mance due to its sensitivity for small objects and integration of the
different. Thus, in clinical gastrointestinal works where mAP is used to
measure the overall performance of the method, this has to be com­
plemented with further analysis such as precision/recall curve analysis
In general terms, metrics are reported for the overall testing set.
Only few authors report metrics in a detailed manner, considering
polyps characteristics. Misawa et al. analyze the percentage of the
video for each of the 50 polyps, for which their Paris classification, size,
Summary of segmentation methods results.
Xiao et al. 
Nguyen and Lee 
Wickstrøm et al. 
Vázquez et al. 
Bardhi et al. 
Wichakam et al. 
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
location and pathologic diagnosis are also provided. While most polyps
present a detected ratio over 80%, only 3 flat polyps (0-IIa in the Paris
classification) do not achieve that level, proving the difficulty for de­
tection of this type of polyps.
Characteristics of the polyps are also considered for metrics re­
porting in the work of Urban et al. . They compare polyp detection
by gastroenterologists with and without using the CAD system. In their
first study, they found that 9 sessile polyps were missed (i.e. detected
with the CAD system and not found without it). Therefore, it might be
expected that such system could improve the ADR. Nevertheless, in
both studies flat polyps represent a minority: only 2 out of 45 in the first
study and 3 out of 73 in the second one. On the other hand, Wang et al.
 select to separately report results for small (< 0.5 cm), flat, iso­
chromatic polyps, as they are associated with a higher missing rate,
finding that the per-image-sensitivity decreased from 0.9438 (95% CI:
0.9380, 0.9496) in the overall dataset to 0.9165 (95% CI: 0.9021,
0.9309). Lastly, Mo et al. , although not providing detailed metrics,
found that their method behaved differently for sequences showing
small polyps, encountering difficulties for their detection.
7. Clinical perspective
The American Society for Gastrointestinal Endoscopy has a set of
publications within the Preservation and Incorporation of Valuable
endoscopic Innovations (PIVI) initiative to establish thresholds for
incorporating innovative technologies into the clinical practice. While
there is a PIVI paper related to the in-vivo real-time assessment of di­
minutive polyps, there is so far no statement regarding the application
of CAD systems which could connect the technical development and
metrics to clinical performance metrics, such as the ADR.
In this regard, the ADR is considered one of the main indicators for
colonoscopy quality, which is influenced both by the endoscopist and
the technical factors . Despite presenting a wide variability, ran­
ging from 12.5% to 68.1% in conventional colonoscopy, it has been
proven that new technologies such as Endocuff, G-Eye or full-spectrum
colonoscopy might help to increase this indicator, reaching values
higher than 80% in some of the works systematically reviewed and
meta-analyzed by Castaneda et al. . Only two papers in this review
 mention this clinical concept in their technical studies. So,
additional efforts should be taken in future works to connect both
technical and clinical outcomes to increase the acceptance of CAD
systems based on deep learning in the daily clinical practice.
It has been already proven that a second observer, such as an ex­
perienced nurse, improves the ADR even in the event of an experienced
gastroenterologist performing the colonoscopy . Therefore, and as
already Wang et al. suggest, CAD systems might be used as an
“extra pair of eyes” to avoid missing subtle lesions. In this regard, they
have recently analyzed the influence of their method on the ADR .
They have carried out a non-blinded trial, where patients were pro­
spectively randomized into diagnostic colonoscopies with or without
Advantages and disadvantages of each category of methods.
Advantages
Disadvantages
End-to-end
1. Automatic learning of relevant features
1. Requires a large dataset for training
2. Superior performance over hand-crafted methods
2. Selection of network, dataset and hyperparameters might highly influence the
performance
3. High degree of automation
1. Combines useful information of well-known hand-crafted
1. Selection of features based on the researcher experience and knowledge
2. More convenient with small datasets
Feature extractor
1. Combine useful information of well-known hand-crafted
1. Not tune the network to the target dataset, potentially leading to suboptimal or even
negative transfer results when domain shift is large
2. Using pre-trained networks without fine-tuning does not
require any labelled data
Classification
1. Cheapest label is required (polyp/non-polyp per frame)
1. Requires considerably more data to converge
Patch-based
1. Increments the size of the training set by obtaining several
patches from one single image
1. Slow method in general
Bounding-box
1. Provides enough information for the clinician, focusing
their attention on a suspicious area
1. Does not provide a pixel-level labelling
2. Better computational speed and inter-patch classification
2. Dense objects might lead to overlapping bouding-boxes
3. Able to converge without pixel-level labels and still
predicting location information
4. Bounding-box annotations are cheaper to label than pixel-
level ones
5. Preferred type for localization task
Semantic segmentation
1. Preferred type for segmentation task
1. Accurate borders might be difficult to obtain
Fig. 10. Confusion matrix. Left side, confusion matrix calculated at frame level. On the right side, elements of the confusion matrix calculated at pixel level by
overlapping of two binary masks.
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
CAD assistance. In this trial, the main outcome was the ADR. Despite
not obtaining the highest metrics for the localization task, they prove a
significant increment from 20.3% to 29.1% in the ADR, showing the
clinical potential of their method. Therefore, the final aim of any
technical development should be to prove the benefit in the clinical
practice, rather than only raking on the top considering technical me­
Another relevant aspect for CAD systems based on deep learning is
matching real time constraints to facilitate clinical application in a live
procedure. Efforts should therefore be oriented to exploit the use of
videos, rather than isolated images, minimizing the processing time to
keep it under the restriction of processing 25 or 30 frames per second.
Current deep learning algorithms are able to run in nearly real-time
speed and have not been fully tested in real clinical
conditions. As far as the authors know, there is only one commercial
system for detection assistance based on artificial intelligence ,
but no technical information has been found on its algorithms. We
highly advise for the design of real-time clinical essays to analyze the
usability and real performance of the algorithms. On the contrary,
images in the datasets usually show polyps in a clean, well-centred
state. This is though not the situation in which the CAD system is useful.
It would be highly interesting to provide “fly-by” explorations as well as
difficult polyps to locate, such as partially hidden or located in folders,
to mimic situations with higher clinical value. The difference between
images available in the dataset and those in the clinical situations might
lead to the fact that is not possible to guarantee that success in the
dataset will be reproduced in the clinical environment.
Lastly, it is also important to recognize the limitation of CAD sys­
tems in the identification of polyps, as they might be missing due to two
main situations: (1) they never appear on the visual field, due to an
inappropriate bowel preparation, an inappropriate exposure technique
or more importantly, because it is in the 20% of the colon surface that is
never surveyed ; and (2) missed by the gastroenterologist due to a
lack of training or short withdrawal time. While CAD systems might
Definition of metrics used in the retrieved works.
Metric – alternative names
Calculation
Recall – true positive rate, sensitivity, pre-class accuracy
Specificity – pre-class accuracy
2·Prec·Rec
5·Prec·Rec
Matthew correlation coefficient
False positive rate
False positive per frame
Intersection over Union – Jaccard index
IoU(PR, GR)
Dice coefficient
Dice(PR, GR)
Receiver operating characteristic (ROC) Curve
Plot of TPR against FPR
Area under the curve (AUC) − area under the ROC curve (AUROC),
Free response receiver operating characteristic curve
Plot of TPR against FPR/frame
Temporal coherence
#correctly detected consecutive frame pairs
#consecutive frame pairs
Polyp detection rate
# polyps detected at least in one frame
#polyps in the dataset
Mean processing time per frame
Actual detection processing time taken by a method to process a frame and display the detection result
Reaction time
RT = frame of first detection − frame of first appearance
Mean distance
Mean Euclidean distance between polyp centres
TP: True negatives; TN: true negatives; FP: false positives; FN: false negatives; PR: predicted binary mask; GT: ground truth binary mask.
Metrics for reporting detection, localization and segmentation tasks.
References
Accuracy-2
 
 
 
Specificity
 
 
 
 
Localization
Accuracy-2
Accuracy-2
 
 
Specificity
Specificity
 
 
Segmentation
Accuracy-1
Accuracy-2
 
 
 
Specificity
 
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
help the gastroenterologist to not miss polyps, they will not counteract
the difficulties of poor bowel preparation or exposure technique.
8. Recommendations and future challenges
Polyp detection, localization and segmentation have been boosted
in the last years by the application of deep learning strategies. In this
review, we have analyzed the datasets, methods and metrics used up to
now. Nevertheless, and despite the success of deep learning over hand-
crafted methods, there are still challenges to be faced by the scientific
community in the upcoming years. In this section we aim at pointing
out trends and/or give recommendations on future research lines.
Lack of reproducibility has been raising concerns lately as a critical
flaw, especially in the field of health as explained by McDermott et al.
 . In their paper, they pose recommendations for data providers,
researchers and journals and conferences, bearing in mind technical,
statistical and conceptual replicability as the three main aspects of re­
producibility. Many of our recommendations are aligned with their
work, particularized to our field of interest.
8.1. Datasets
The collection of images to create a large dataset is one of the
challenges that should be addressed by the clinical community, which
would eventually help the technical researchers when developing CAD
systems. In this regard, it would be useful to collaboratively work under
common guidelines, such as the methodology proposed by Sánchez-
Peralta et al. , that allows for the systematic acquisition and an­
notation of colonoscopy videos without modifying the clinical routine.
This relates to three issues to consider: (1) collecting abnormal cases,
(2) the quality of annotations and (3) the variability of acquisition
systems. In the first case, since we are dealing with medical images,
images from some particular classes might be more difficult to collect,
since they are less frequent to be found in the clinical practice despite
having even more relevance for the application of the CAD system than
other more frequent classes. This is for instance the situation of flat
polyps (types 0-IIb and 0-IIc in the Paris classification as showed in
Fig. 4). While gastroenterologists find that CAD systems would be more
useful to help in their detection, those types are underrepresented in the
publicly available databases . This fact might hinder the efficacy of
CAD systems. On the other hand, medical datasets must be annotated
by clinicians, taking considerable time. Tools such as GTCreator 
have been designed to ease the process of ground truth creation, al­
lowing for image and text-based annotations, so it would be advisable
to use it to share and revise annotations among different clinicians.
Strictly speaking, ground truth is not available for detection methods in
colonoscopy as annotated datasets are made by expert calls that re­
present the best knowledge that can be extracted by an expert from the
colonoscopy. For that, agreement with expert's praxis can be analyzed
by the Cohen's kappa coefficient. In order to minimize this issue, da­
tasets should be independently annotated by different clinicians and the
inconsistencies should be handled by an additional clinician. This will
allow to create a dataset closer to an actual ground truth. Furthermore,
inter-observer variability is a well-known problem in manual segmen­
tation of medical images . It would be advisable that future da­
tasets provide uncertainty maps to reflect the variability of experts
opinion on the same image. Lastly, a collaborative dataset would in­
crease the variability of the acquisition systems, therefore strength­
ening the CAD systems to accurately work regardless of the endoscope
manufacturer.
When producing a dataset, it is also important to establish a set of
criteria and guidelines, similarly to the challenge rules, so authors can
follow them, facilitating the posterior comparison of methods. In this
regard, the distribution of images into training, validation and test sets
is a minimum where cross validation performed over the different
subjects of the dataset or over fixed sets can be employed for deriving
the statistical metrics and confidence intervals. It is essential to assure
patient independence of the sets, so all images originated from a patient
must fall into one of the sets. For testing, it is recommended that the
dataset owners establish a bootstrapping methodology , what has
been successfully used in other domains , identifying the
number of testing images and the corresponding sample size and re­
petitions, clearly indicating which images must be considered in each
iteration. Bootstrapping consists on analyzing different testing subsets
and measuring the posterior distribution of the results. Authors are
encouraged to report information on the posterior probability of the
metrics or, at least, information on their mean and standard deviation,
giving a more realistic vision of the method performance.
All currently available datasets exposed in Section 4.1 only contain
medical images and videos from actual patients. None of the articles
mentions the use of synthetic image datasets, which could be an al­
ternative to increase the number of samples. There are already efforts in
this direction, as in the work of Shin et al. . They employ a con­
ditional GAN to synthetize polyp frames from normal colonoscopy
images, by using a filtering-based binary image as input, modified to
include the position and size of the polyp. Even though the generated
images are qualitatively realistic, they result on deterministic polyps
without many variations on colour and texture. Hence, the potential of
synthetic image can be further explored and exploited.
8.2. Metrics
Another element to define for fair comparison is the set of metrics to
be used for reporting. As seen in Section 6, there is a lack of criteria
among the authors to select the most convenient one. In this choice,
there are two aspects that play a role. On one side, the type of ground
truth available and on the other side, the task to be accomplished.
While the former limits the available information (label per image/
frame or binary mask), the latter relates to the information worthy to
measure. For methods aiming at detecting and locating polyps, it would
be advisable to calculate the F1-score at frame level, as it gives a ba­
lanced measure between missing polyps (or false negatives – FN) and
false alarms (or false positives – FP) . In order to trace easier par­
allels with the literature of computer vision when analyzing results, we
would also recommend including mAP to perform a global technical
evaluation of the algorithm, as commented in Section 6. However, mAP
analysis should be complemented with a more detailed analysis to va­
lidate the real clinical performance of the model. As for segmentation
methods, despite being useful, metrics based on elements of the con­
fusion matrix at pixel-level do not detect whether the two masks are
similar in shape , so it would be useful to complement them with
distance metrics that are valid for small segments (such as Hausdorff
distance or Mahalanobis distance). In this regard, it is also important to
mention the recommendation to calculate agreement measurements
with the experts. Specially in the case of detection methods, it would be
highly desirable to compute inter-rater measurements, selecting the
most suitable method depending on the type of variable (categorical or
continuous) and the number of observers .
Clinically speaking and agreeing with the concern raised by
Robinson et al. , we are of the opinion that reporting metrics per
patient or per polyp might be more convenient than averaging results
all over the test set, as long as the database provides information to
identify which polyp and/or patient originate each frame. Polyps and/
or patients might have an unequal presence in the database, for ex­
ample polyp A and B having 8 and 2 frames, respectively, in the test set.
If a detection method has 80% accuracy, it might be that all detected
frames corresponds to polyp A. If metrics are averaged all over the test
set, this situation cannot be identified, but if metrics are provided per
polyp (and later averaged), accuracy would be 50%. This way, it would
be possible to identify the cases in which the method presents flaws to
further work on.
L.F. Sánchez-Peralta, et al.
Artificial Intelligence In Medicine 108 101923
8.3. Data augmentation
In terms of data augmentation, it has been shown that transforma­
tions are selected based on subjective criteria upon the researcher ex­
perience and that there is no general strategy as the differences in
Table 5 show. Efforts are therefore now focused on the identification of
the most convenient transformations and ranges in a more objective
way. Initiatives to find the most convenient data augmentation policies,
such as Smart Augmentation , AutoAugment or the use of a
Bayesian data augmentation approach have been mostly devel­
oped for classification of natural images. Thus, the application of these
methodologies to colonoscopy images might boost performance of
methods and is therefore worth research.
Besides, it would be also interesting that data augmentation trans­
formations would address particularities of the colonoscopy images,
such as illumination effects (specular lights and lack of uniformity);
sensor acquisition effects (colour phantoms); image interlacing; shar­
pening (to improve the quality of the visualized image but increasing
the image noise at the same time); information overlay or the presence
of the black mask . As these effects might negatively affect the
CAD system performance, their inclusion in the training dataset could
lead to the model invariance when they are present.
8.4. Network design
The utility of CNNs for polyp detection, localization and segmen­
tation have been already widely explored using supervised learning. In
the future, semi-supervised or unsupervised training should be further
exploited, relying on smaller datasets which would be eventually easier
to compile. On the other hand, LTSMs or RNNs, with small presence in
the current review, will be more widely employed in the field of CRC
detection. The capability of recurrent networks to model temporal re­
lationships can help creating models tackling temporal information into
account for colonoscopy videos. On the other hand, GANs can be em­
ployed in the future to generate synthetic data from polyp models to
increase the variability of the dataset or to be able to over-express
difficult to detect lesions into existing datasets.
Besides, networks have to be designed for taking advantage for the
new very high-resolution colonoscopy devices. This can help detecting
micro-patterns that can be missed by veteran gastroenterologists, spe­
cially from small or flat polyps. This implies challenges on the defini­
tion of a network capable of real-time inspection capabilities for very
high resolution images.
Reproducibility of deep learning methods comes with associated
difficulties. It should be clearly stated in the papers the following in­
formation:
• Dataset and distribution of training, validation and test subsets.
• Data pre-processing, if any.
• Model training, including learning parameters such as learning rate,
early stopping or weight initialization. The use of seeds when ap­
• Loss function, as it highly impacts the model performance.
• Hardware and software details, as software packages are con­
tinuously updated, and some models might require exceptional
hardware conditions.
Randomization of parameters hampers reproducibility, e.g. when
images are randomly transformed on the flow. Although ideally all
particular seeds and values should be reported, or the code made
available, so other researchers could reproduce the experiment, re­
leasing at least the trained models could be an intermediate solution.
Some efforts on reproducibility have also been taken from major
conferences organizations such as NeurIPS, with the request of a re­
producibility checklist . We strongly advise its use in future
publications related to CAD systems based on deep learning.
9. Conclusions
CRC is one of the major causes of death by cancer worldwide. Early
detection of precursor lesions has been proved to minimize its in­
cidence, so screening programs are essential. Colonoscopy is a gold
standard technique for the detection and treatment of polyps and
adenomas. CAD systems might help endoscopists to identify lesions and
minimize the ADR.
In the current work, we provided a systematic and comprehensive
review of 35 works for detection, localization and segmentation of
polyps using deep learning approaches since 2015. We further analyzed
seven currently available public databases of colonoscopy images as
well as the most common metrics used for reporting. Retrieved methods
have been classified according to the approach they follow in a primary
(end-to-end vs hybrid methods) and secondary (feature extractor,
classification, patch-based, bounding-box and semantic segmentation)
classifications. Although there is no common dataset or framework for
easy and direct comparison of methods, some trends, advantages and
disadvantages have been identified and discussed. Lastly, re­
commendations and future challenges have been identified.
Despite the great success of deep learning approaches, clinical va­
lidation and application is still a must. The creation of larger, more
assorted, public datasets; new algorithms requesting less training
samples and the creation of a common validation framework will
maintain the upwards tendency and will end in the clinical application
of CAD systems to assist gastroenterologists to increase the ADR and
early detect CRC.
Conflict of interest
The authors declare that there is no conflict of interest.
Acknowledgments
This work was partially supported by PICCOLO project. This project
has received funding from the European Union's Horizon2020 Research
and Innovation Programme under grant agreement No. 732111. The
sole responsibility of this publication lies with the author. The
European Union is not responsible for any use that may be made of the
information contained therein. The authors would also like to thank Dr.
Federico Soria for his support on this manuscript and Dr. José Carlos
Marín, from Hospital 12 de Octubre, and Dr. Ángel Calderón and Dr.
Francisco Polo, from Hospital de Basurto, for the images in Fig. 4.