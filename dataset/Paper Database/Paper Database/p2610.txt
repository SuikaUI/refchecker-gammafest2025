CvT: Introducing Convolutions to Vision Transformers
Haiping Wu1,2*
Bin Xiao2†
Noel Codella2
Mengchen Liu2
Xiyang Dai2
Lei Zhang2
1McGill University
2Microsoft Cloud + AI
 , {bixi, ncodella, mengcliu, xidai, luyuan, leizhang}@microsoft.com
We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision
Transformer (ViT) in performance and efﬁciency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through two primary modiﬁcations: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer
block leveraging a convolutional projection. These changes
introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale,
and distortion invariance) while maintaining the merits of
Transformers (i.e. dynamic attention, global context, and
better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves
state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs.
In addition, performance gains
are maintained when pretrained on larger datasets (e.g.
ImageNet-22k) and ﬁne-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally, our
results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks.
Code will be released at https:
//github.com/leoxiaobin/CvT.
1. Introduction
Transformers have recently dominated a wide
range of tasks in natural language processing (NLP) .
The Vision Transformer (ViT) is the ﬁrst computer vision model to rely exclusively on the Transformer architecture to obtain competitive image classiﬁcation performance at large scale. The ViT design adapts Transformer
*This work is done when Haiping Wu was an intern at Microsoft.
†Corresponding author
ImageNet top-1 accuracy (%)
Model Paramters (M)
ImageNet top-1 accuracy (%)
Figure 1: Top-1 Accuracy on ImageNet validation compared to other methods with respect to model parameters. (a) Comparison to CNN-based model BiT and
Transformer-based model ViT , when pretrained on
ImageNet-22k. Larger marker size indicates larger architectures. (b) Comparison to concurrent works: DeiT ,
T2T , PVT , TNT when pretrained on
ImageNet-1k.
architectures from language understanding with minimal modiﬁcations. First, images are split into discrete nonoverlapping patches (e.g. 16 × 16). Then, these patches are
treated as tokens (analogous to tokens in NLP), summed
with a special positional encoding to represent coarse spatial information, and input into repeated standard Transformer layers to model global relations for classiﬁcation.
Despite the success of vision Transformers at large scale,
the performance is still below similarly sized convolutional
neural network (CNN) counterparts (e.g., ResNets )
when trained on smaller amounts of data. One possible reason may be that ViT lacks certain desirable properties inherently built into the CNN architecture that make CNNs
uniquely suited to solve vision tasks.
For example, images have a strong 2D local structure: spatially neighboring pixels are usually highly correlated. The CNN archi-
 
Needs Position Encoding (PE)
Token Embedding
Projection for Attention
Hierarchical Transformers
ViT , DeiT 
non-overlapping
no (w/ PE Generator)
non-overlapping
non-overlapping (patch+pixel)
overlapping (concatenate)
partial (tokenization)
non-overlapping
spatial reduction
CvT (ours)
overlapping (convolution)
convolution
Table 1: Representative works of vision Transformers.
tecture forces the capture of this local structure by using
local receptive ﬁelds, shared weights, and spatial subsampling , and thus also achieves some degree of shift,
scale, and distortion invariance. In addition, the hierarchical structure of convolutional kernels learns visual patterns
that take into account local spatial context at varying levels
of complexity, from simple low-level edges and textures to
higher order semantic patterns.
In this paper, we hypothesize that convolutions can be
strategically introduced to the ViT structure to improve
performance and robustness, while concurrently maintaining a high degree of computational and memory efﬁciency.
To verify our hypothesises, we present a new architecture,
called the Convolutional vision Transformer (CvT), which
incorporates convolutions into the Transformer that is inherently efﬁcient, both in terms of ﬂoating point operations
(FLOPs) and parameters.
The CvT design introduces convolutions to two core sections of the ViT architecture. First, we partition the Transformers into multiple stages that form a hierarchical structure of Transformers. The beginning of each stage consists
of a convolutional token embedding that performs an overlapping convolution operation with stride on a 2D-reshaped
token map (i.e., reshaping ﬂattened token sequences back
to the spatial grid), followed by layer normalization. This
allows the model to not only capture local information, but
also progressively decrease the sequence length while simultaneously increasing the dimension of token features
across stages, achieving spatial downsampling while concurrently increasing the number of feature maps, as is performed in CNNs . Second, the linear projection prior
to every self-attention block in the Transformer module is
replaced with our proposed convolutional projection, which
employs a s × s depth-wise separable convolution operation on an 2D-reshaped token map. This allows the model
to further capture local spatial context and reduce semantic ambiguity in the attention mechanism. It also permits
management of computational complexity, as the stride of
convolution can be used to subsample the key and value matrices to improve efﬁciency by 4× or more, with minimal
degradation of performance.
In summary, our proposed Convolutional vision Transformer (CvT) employs all the beneﬁts of CNNs: local receptive ﬁelds, shared weights, and spatial subsampling,
while keeping all the advantages of Transformers: dynamic
attention, global context fusion, and better generalization.
Our results demonstrate that this approach attains state-ofart performance when CvT is pre-trained with ImageNet-
1k, while being lightweight and efﬁcient: CvT improves the
performance compared to CNN-based models (e.g. ResNet)
and prior Transformer-based models (e.g. ViT, DeiT) while
utilizing fewer FLOPS and parameters. In addition, CvT
achieves state-of-the-art performance when evaluated at
larger scale pretraining (e.g. on the public ImageNet-22k
dataset). Finally, we demonstrate that in this new design, we
can drop the positional embedding for tokens without any
degradation to model performance. This not only simpliﬁes
the architecture design, but also makes it readily capable of
accommodating variable resolutions of input images that is
critical to many vision tasks.
2. Related Work
Transformers that exclusively rely on the self-attention
mechanism to capture global dependencies have dominated
in natural language modelling . Recently, the
Transformer based architecture has been viewed as a viable
alternative to the convolutional neural networks (CNNs) in
visual recognition tasks, such as classiﬁcation , object detection , segmentation , image enhancement , image generation , video processing and 3D point cloud processing .
Vision Transformers.
The Vision Transformer (ViT) is
the ﬁrst to prove that a pure Transformer architecture can
attain state-of-the-art performance (e.g. ResNets , Ef-
ﬁcientNet ) on image classiﬁcation when the data is
large enough (i.e. on ImageNet-22k, JFT-300M). Speciﬁcally, ViT decomposes each image into a sequence of tokens
(i.e. non-overlapping patches) with ﬁxed length, and then
applies multiple standard Transformer layers, consisting of
Multi-Head Self-Attention module (MHSA) and Positionwise Feed-forward module (FFN), to model these tokens.
DeiT further explores the data-efﬁcient training and
distillation for ViT. In this work, we study how to combine
Figure 2: The pipeline of the proposed CvT architecture. (a) Overall architecture, showing the hierarchical multi-stage
structure facilitated by the Convolutional Token Embedding layer. (b) Details of the Convolutional Transformer Block,
which contains the convolution projection as the ﬁrst layer.
CNNs and Transformers to model both local and global dependencies for image classiﬁcation in an efﬁcient way.
In order to better model local context in vision Transformers, some concurrent works have introduced design
For example, the Conditional Position encodings Visual Transformer (CPVT) replaces the prede-
ﬁned positional embedding used in ViT with conditional
position encodings (CPE), enabling Transformers to process input images of arbitrary size without interpolation.
Transformer-iN-Transformer (TNT) utilizes both an
outer Transformer block that processes the patch embeddings, and an inner Transformer block that models the relation among pixel embeddings, to model both patch-level
and pixel-level representation. Tokens-to-Token (T2T) 
mainly improves tokenization in ViT by concatenating multiple tokens within a sliding window into one token. However, this operation fundamentally differs from convolutions
especially in normalization details, and the concatenation
of multiple tokens greatly increases complexity in computation and memory. PVT incorporates a multi-stage
design (without convolutions) for Transformer similar to
multi-scales in CNNs, favoring dense prediction tasks.
In contrast to these concurrent works, this work aims
to achieve the best of both worlds by introducing convolutions, with image domain speciﬁc inductive biases, into the
Transformer architecture. Table 1 shows the key differences
in terms of necessity of positional encodings, type of token
embedding, type of projection, and Transformer structure in
the backbone, between the above representative concurrent
works and ours.
Introducing Self-attentions to CNNs.
Self-attention
mechanisms have been widely applied to CNNs in vision
tasks. Among these works, the non-local networks are
designed for capturing long range dependencies via global
attention. The local relation networks adapts its weight
aggregation based on the compositional relations (similarity) between pixels/features within a local window, in contrast to convolution layers which employ ﬁxed aggregation weights over spatially neighboring input feature. Such
an adaptive weight aggregation introduces geometric priors into the network which are important for the recognition tasks. Recently, BoTNet proposes a simple yet
powerful backbone architecture that just replaces the spatial convolutions with global self-attention in the ﬁnal three
bottleneck blocks of a ResNet and achieves a strong performance in image recognition. Instead, our work performs
an opposite research direction: introducing convolutions to
Transformers.
Introducing Convolutions to Transformers.
and speech recognition, convolutions have been used to
modify the Transformer block, either by replacing multihead attentions with convolution layers , or adding
additional convolution layers in parallel
 or sequentially , to capture local relationships. Other prior work
 proposes to propagate attention maps to succeeding
layers via a residual connection, which is ﬁrst transformed
by convolutions. Different from these works, we propose
to introduce convolutions to two primary parts of the vision Transformer: ﬁrst, to replace the existing Position-wise
Linear Projection for the attention operation with our Convolutional Projection, and second, to use our hierarchical
multi-stage structure to enable varied resolution of 2D reshaped token maps, similar to CNNs. Our unique design
affords signiﬁcant performance and efﬁciency beneﬁts over
prior works.
3. Convolutional vision Transformer
The overall pipeline of the Convolutional vision Transformer (CvT) is shown in Figure 2.
We introduce two
convolution-based operations into the Vision Transformer
architecture, namely the Convolutional Token Embedding
and Convolutional Projection. As shown in Figure 2 (a), a
multi-stage hierarchy design borrowed from CNNs 
is employed, where three stages in total are used in this
Each stage has two parts.
First, the input image
(or 2D reshaped token maps) are subjected to the Convolutional Token Embedding layer, which is implemented as a
convolution with overlapping patches with tokens reshaped
to the 2D spatial grid as the input (the degree of overlap
can be controlled via the stride length). An additional layer
normalization is applied to the tokens. This allows each
stage to progressively reduce the number of tokens (i.e. feature resolution) while simultaneously increasing the width
of the tokens (i.e. feature dimension), thus achieving spatial downsampling and increased richness of representation,
similar to the design of CNNs. Different from other prior
Transformer-based architectures , we do not
sum the ad-hod position embedding to the tokens. Next,
a stack of the proposed Convolutional Transformer Blocks
comprise the remainder of each stage. Figure 2 (b) shows
the architecture of the Convolutional Transformer Block,
where a depth-wise separable convolution operation ,
referred as Convolutional Projection, is applied for query,
key, and value embeddings respectively, instead of the standard position-wise linear projection in ViT . Additionally, the classiﬁcation token is added only in the last stage.
Finally, an MLP (i.e. fully connected) Head is utilized upon
the classiﬁcation token of the ﬁnal stage output to predict
the class.
We ﬁrst elaborate on the proposed Convolutional Token
Embedding layer. Next we show how to perform Convolutional Projection for the Multi-Head Self-Attention module,
and its efﬁcient design for managing computational cost.
3.1. Convolutional Token Embedding
This convolution operation in CvT aims to model local
spatial contexts, from low-level edges to higher order semantic primitives, over a multi-stage hierarchy approach,
similar to CNNs.
Formally, given a 2D image or a 2D-reshaped output token map from a previous stage xi−1 ∈RHi−1×Wi−1×Ci−1
as the input to stage i, we learn a function f(·) that maps
xi−1 into new tokens f(xi−1) with a channel size Ci, where
f(·) is 2D convolution operation of kernel size s × s, stride
s −o and p padding (to deal with boundary conditions).
The new token map f(xi−1) ∈RHi×Wi×Ci has height and
Hi−1 + 2p −s
Wi−1 + 2p −s
f(xi−1) is then ﬂattened into size HiWi × Ci and normalized by layer normalization for input into the subsequent
Transformer blocks of stage i.
The Convolutional Token Embedding layer allows us to
adjust the token feature dimension and the number of tokens at each stage by varying parameters of the convolution
operation. In this manner, in each stage we progressively
decrease the token sequence length, while increasing the
token feature dimension. This gives the tokens the ability
to represent increasingly complex visual patterns over increasingly larger spatial footprints, similar to feature layers
3.2. Convolutional Projection for Attention
The goal of the proposed Convolutional Projection layer
is to achieve additional modeling of local spatial context,
and to provide efﬁciency beneﬁts by permitting the undersampling of K and V matrices.
Fundamentally, the proposed Transformer block with
Convolutional Projection is a generalization of the original Transformer block. While previous works try
to add additional convolution modules to the Transformer
Block for speech recognition and natural language processing, they result in a more complicated design and additional computational cost. Instead, we propose to replace
the original position-wise linear projection for Multi-Head
Self-Attention (MHSA) with depth-wise separable convolutions, forming the Convolutional Projection layer.
Implementation Details
Figure 3 (a) shows the original position-wise linear projection used in ViT and Figure 3 (b) shows our proposed
s × s Convolutional Projection. As shown in Figure 3 (b),
tokens are ﬁrst reshaped into a 2D token map. Next, a Convolutional Projection is implemented using a depth-wise
separable convolution layer with kernel size s. Finally, the
projected tokens are ﬂattened into 1D for subsequent process. This can be formulated as:
= Flatten (Conv2d (Reshape2D(xi), s)) ,
where xq/k/v
is the token input for Q/K/V matrices at
layer i, xi is the unperturbed token prior to the Convolutional Projection, Conv2d is a depth-wise separable convolution implemented by: Depth-wise Conv2d →
BatchNorm2d →Point-wise Conv2d, and s refers
to the convolution kernel size.
The resulting new Transformer Block with the Convolutional Projection layer is a generalization of the original
Figure 3: (a) Linear projection in ViT . (b) Convolutional projection. (c) Squeezed convolutional projection. Unless
otherwise stated, we use (c) Squeezed convolutional projection by default.
Transformer Block design. The original position-wise linear projection layer could be trivially implemented using a
convolution layer with kernel size of 1 × 1.
Efﬁciency Considerations
There are two primary efﬁciency beneﬁts from the design
of our Convolutional Projection layer.
First, we utilize efﬁcient convolutions. Directly using
standard s×s convolutions for the Convolutional Projection
would require s2C2 parameters and O(s2C2T) FLOPs,
where C is the token channel dimension, and T is the number of tokens for processing. Instead, we split the standard
s × s convolution into a depth-wise separable convolution
 . In this way, each of the proposed Convolutional Projection would only introduce an extra of s2C parameters
and O(s2CT) FLOPs compared to the original positionwise linear projection, which are negligible with respect to
the total parameters and FLOPs of the models.
Second, we leverage the proposed Convolutional Projection to reduce the computation cost for the MHSA operation. The s × s Convolutional Projection permits reducing
the number of tokens by using a stride larger than 1. Figure 3 (c) shows the Convolutional Projection, where the key
and value projection are subsampled by using a convolution with stride larger than 1. We use a stride of 2 for key
and value projection, leaving the stride of 1 for query unchanged. In this way, the number of tokens for key and
value is reduced 4 times, and the computational cost is reduced by 4 times for the later MHSA operation. This comes
with a minimal performance penalty, as neighboring pixels/patches in images tend to have redundancy in appearance/semantics. In addition, the local context modeling of
the proposed Convolutional Projection compensates for the
loss of information incurred by resolution reduction.
3.3. Methodological Discussions
Removing Positional Embeddings:
The introduction of
Convolutional Projections for every Transformer block,
combined with the Convolutional Token Embedding, gives
us the ability to model local spatial relationships through the
network. This built-in property allows dropping the position
embedding from the network without hurting performance,
as evidenced by our experiments (Section 4.4), simplifying
design for vision tasks with variable input resolution.
Relations to Concurrent Work:
Recently, two more related concurrent works also propose to improve ViT by incorporating elements of CNNs to Transformers. Tokensto-Token ViT implements a progressive tokenization,
and then uses a Transformer-based backbone in which the
length of tokens is ﬁxed. By contrast, our CvT implements
a progressive tokenization by a multi-stage process – containing both convolutional token embeddings and convolutional Transformer blocks in each stage. As the length of
tokens are decreased in each stage, the width of the tokens
(dimension of feature) can be increased, allowing increased
richness of representations at each feature spatial resolution. Additionally, whereas T2T concatenates neighboring
tokens into one new token, leading to increasing the complexity of memory and computation, our usage of convolutional token embedding directly performs contextual learning without concatenation, while providing the ﬂexibility
of controlling stride and feature dimension. To manage the
complexity, T2T has to consider a deep-narrow architecture
design with smaller hidden dimensions and MLP size than
ViT in the subsequent backbone. Instead, we changed previous Transformer modules by replacing the position-wise
linear projection with our convolutional projection
Pyramid Vision Transformer (PVT) overcomes the
difﬁculties of porting ViT to various dense prediction tasks.
In ViT, the output feature map has only a single scale with
low resolution. In addition, computations and memory cost
are relatively high, even for common input image sizes. To
address this problem, both PVT and our CvT incorporate
pyramid structures from CNNs to the Transformers structure. Compared with PVT, which only spatially subsamples the feature map or key/value matrices in projection, our
CvT instead employs convolutions with stride to achieve
this goal. Our experiments (shown in Section 4.4) demon-
Output Size
Layer Name
Conv. Embed.
7 × 7, 64, stride 4
7 × 7, 192, stride 4
Conv. Proj.
H1 = 1, D1 = 64
H1 = 1, D1 = 64
3 × 3, 192
H1 = 3, D1 = 192
Conv. Embed.
3 × 3, 192, stride 2
3 × 3, 768, stride 2
Conv. Proj.
3 × 3, 192
H2 = 3, D2 = 192
3 × 3, 192
H2 = 3, D2 = 192
3 × 3, 768
H2 = 12, D2 = 768
Conv. Embed.
3 × 3, 384, stride 2
3 × 3, 1024, stride 2
Conv. Proj.
3 × 3, 384
H3 = 6, D3 = 384
3 × 3, 384
H3 = 6, D3 = 384
3 × 3, 1024
H3 = 16, D3 = 1024
Table 2: Architectures for ImageNet classiﬁcation. Input image size is 224 × 224 by default. Conv. Embed.: Convolutional
Token Embedding. Conv. Proj.: Convolutional Projection. Hi and Di is the number of heads and embedding feature
dimension in the ith MHSA module. Ri is the feature dimension expansion ratio in the ith MLP layer.
strate that the fusion of local neighboring information plays
an important role on the performance.
4. Experiments
In this section, we evaluate the CvT model on large-scale
image classiﬁcation datasets and transfer to various downstream datasets. In addition, we perform through ablation
studies to validate the design of the proposed architecture.
4.1. Setup
For evaluation, we use the ImageNet dataset, with 1.3M
images and 1k classes, as well as its superset ImageNet-22k
with 22k classes and 14M images . We further transfer the models pretrained on ImageNet-22k to downstream
tasks, including CIFAR-10/100 , Oxford-IIIT-Pet ,
Oxford-IIIT-Flower , following .
Model Variants
We instantiate models with different parameters and FLOPs by varying the number of Transformer
blocks of each stage and the hidden feature dimension used,
as shown in Table 2. Three stages are adapted. We de-
ﬁne CvT-13 and CvT-21 as basic models, with 19.98M and
31.54M paramters. CvT-X stands for Convolutional vision
Transformer with X Transformer Blocks in total. Additionally, we experiment with a wider model with a larger token
dimension for each stage, namely CvT-W24 (W stands for
Wide), resulting 298.3M parameters, to validate the scaling
ability of the proposed architecture.
AdamW optimizer is used with the weight
decay of 0.05 for our CvT-13, and 0.1 for our CvT-21 and
We train our models with an initial learning
rate of 0.02 and a total batch size of 2048 for 300 epochs,
with a cosine learning rate decay scheduler. We adopt the
same data augmentation and regularization methods as in
ViT . Unless otherwise stated, all ImageNet models are
trained with an 224 × 224 input size.
Fine-tuning
We adopt ﬁne-tuning strategy from ViT .
SGD optimizor with a momentum of 0.9 is used for ﬁnetuning. As in ViT , we pre-train our models at resolution 224 × 224, and ﬁne-tune at resolution of 384 × 384.
We ﬁne-tune each model with a total batch size of 512,
for 20,000 steps on ImageNet-1k, 10,000 steps on CIFAR-
10 and CIFAR-100, and 500 steps on Oxford-IIIT Pets and
Oxford-IIIT Flowers-102.
4.2. Comparison to state of the art
We compare our method with state-of-the-art classiﬁcation methods including Transformer-based models and representative CNN-based models on ImageNet , ImageNet
Real and ImageNet V2 datasets in Table 3.
Compared to Transformer based models, CvT achieves
a much higher accuracy with fewer parameters and FLOPs.
CvT-21 obtains a 82.5% ImageNet Top-1 accuracy, which
is 0.5% higher than DeiT-B with the reduction of 63% parameters and 60% FLOPs. When comparing to concurrent
works, CvT still shows superior advantages. With fewer
paramerters, CvT-13 achieves a 81.6% ImageNet Top-1 accuracy, outperforming PVT-Small , T2T-ViTt-14 ,
TNT-S by 1.7%, 0.8%, 0.2% respectively.
Our architecture designing can be further improved in
terms of model parameters and FLOPs by neural architecture search (NAS) . In particular, we search the proper
stride for each convolution projection of key and value
(stride = 1, 2) and the expansion ratio for each MLP
layer (ratioMLP = 2, 4).
Such architecture candidates
with FLOPs ranging from 2.59G to 4.03G and the num-
Method Type
Convolutional Networks
ResNet-50 
ResNet-101 
ResNet-152 
Transformers
ViT-B/16 
ViT-L/16 
DeiT-S [arxiv 2020]
DeiT-B [arxiv 2020]
PVT-Small [arxiv 2021]
PVT-Medium [arxiv 2021]
PVT-Large [arxiv 2021]
T2T-ViTt-14 [arxiv 2021]
T2T-ViTt-19 [arxiv 2021]
T2T-ViTt-24 [arxiv 2021]
TNT-S [arxiv 2021]
TNT-B [arxiv 2021]
Convolutional Transformers
Ours: CvT-13
Ours: CvT-21
Ours: CvT-13↑384
Ours: CvT-21↑384
Ours: CvT-13-NAS
Convolution Networks22k
BiT-M↑480 
Transformers22k
ViT-B/16↑384 
ViT-L/16↑384 
ViT-H/16↑384 
Convolutional Transformers22k
Ours: CvT-13↑384
Ours: CvT-21↑384
Ours: CvT-W24↑384
Table 3: Accuracy of manual designed architecture on ImageNet , ImageNet Real and ImageNet V2 matched frequency . Subscript 22k indicates the model pre-trained on ImageNet22k , and ﬁnetuned on ImageNet1k with the input
size of 384 × 384, except BiT-M ﬁnetuned with input size of 480 × 480.
ber of model parameters ranging from 13.66M to 19.88M
construct the search space. The NAS is evaluated directly
on ImageNet-1k. The searched CvT-13-NAS, a bottlenecklike architecture with stride = 2, ratioMLP = 2 at the ﬁrst
and last stages, and stride = 1, ratioMLP = 4 at most layers of the middle stage, reaches to a 82.2% ImageNet Top-1
accuracy with fewer model parameters than CvT-13.
Compared to CNN-based models, CvT further closes the
performance gap of Transformer-based models. Our smallest model CvT-13 with 20M parameters and 4.5G FLOPs
surpasses the large ResNet-152 model by 3.2% on ImageNet Top-1 accuracy, while ResNet-151 has 3 times the
parameters of CvT-13.
Furthermore, when more data are involved, our wide
model CvT-W24* pretrained on ImageNet-22k reaches to
87.7% Top-1 Accuracy on ImageNet without extra data
(e.g. JFT-300M), surpassing the previous best Transformer
based models ViT-L/16 by 2.5% with similar number of
model parameters and FLOPs.
4.3. Downstream task transfer
We further investigate the ability of our models to transfer by ﬁne-tuning models on various tasks, with all models
being pre-trained on ImageNet-22k. Table 4 shows the results. Our CvT-W24 model is able to obtain the best performance across all the downstream tasks considered, even
when compared to the large BiT-R152x4 model, which
has more than 3× the number of parameters as CvT-W24.
4.4. Ablation Study
We design various ablation experiments to investigate
the effectiveness of the proposed components of our architecture. First, we show that with our introduction of convolutions, position embeddings can be removed from the
BiT-M 
ViT-B/16 
ViT-L/16 
ViT-H/16 
Ours: CvT-13
Ours: CvT-21
Ours: CvT-W24
Table 4: Top-1 accuracy on downstream tasks. All the models are pre-trained on ImageNet-22k data
Every stage
First stage
Last stage
Table 5: Ablations on position embedding.
model. Then, we study the impact of each of the proposed
Convolutional Token Embedding and Convolutional Projection components.
Removing Position Embedding
Given that we have introduced convolutions into the model, allowing local context to be captured, we study whether position embedding is still needed for CvT. The results are shown in Table 5, and demonstrate that removing position embedding
of our model does not degrade the performance. Therefore, position embeddings have been removed from CvT
by default. As a comparison, removing the position embedding of DeiT-S would lead to 1.8% drop of ImageNet
Top-1 accuracy, as it does not model image spatial relationships other than by adding the position embedding. This
further shows the effectiveness of our introduced convolutions. Position Embedding is often realized by ﬁxed-length
learn-able vectors, limiting the trained model adaptation of
variable-length input. However, a wide range of vision applications take variable image resolutions.
Recent work
CPVT tries to replace explicit position embedding of
Vision Transformers with a conditional position encodings
module to model position information on-the-ﬂy. CvT is
able to completely remove the positional embedding, providing the possibility of simplifying adaption to more vision
tasks without requiring a re-designing of the embedding.
Table 6: Ablations on Convolutional Token Embedding.
Conv. Proj. KV.
Table 7: Ablations on Convolutional Projection with different strides for key and value projection. Conv. Proj. KV.:
Convolutional Projection for key and value. We apply Convolutional Projection in all Transformer blocks.
Conv. Projection
Ablations on Convolutional Projection v.s.
Position-wise Linear Projection.
 indicates the use of
Convolutional Projection, otherwise use Position-wise Linear Projection.
Convolutional Token Embedding
We study the effectiveness of the proposed Convolutional Token Embedding,
and Table 6 shows the results.
Table 6d is the CvT-13
model. When we replace the Convolutional Token Embedding with non-overlapping Patch Embedding , the performance drops 0.8% (Table 6a v.s. Table 6d). When position embedding is used, the introduction of Convolutional
Token Embedding still obtains 0.3% improvement (Table 6b
Table 6c).
Further, when using both Convolutional
Token Embedding and position embedding as Table 6d, it
slightly drops 0.1% accuracy. These results validate the introduction of Convolutional Token Embedding not only improves the performance, but also helps CvT model spatial
relationships without position embedding.
Convolutional Projection
First, we compare the proposed Convolutional Projection with different strides in Table 7. By using a stride of 2 for key and value projection,
we observe a 0.3% drop in ImageNet Top-1 accuracy, but
with 30% fewer FLOPs. We choose to use Convolutional
Projection with stride 2 for key and value as default for less
computational cost and memory usage.
Then, we study how the proposed Convolutional Projection affects the performance by choosing whether to use
Convolutional Projection or the regular Position-wise Linear Projection for each stage. The results are shown in Table 8. We observe that replacing the original Position-wise
Linear Projection with the proposed Convolutional Projection improves the Top-1 Accuracy on ImageNet from 80.6%
to 81.5%. In addition, performance continually improves as
more stages use the design, validating this approach as an
effective modeling strategy.
5. Conclusion
In this work, we have presented a detailed study of introducing convolutions into the Vision Transformer architecture to merge the beneﬁts of Transformers with the beneﬁts of CNNs for image recognition tasks. Extensive experiments demonstrate that the introduced convolutional token embedding and convolutional projection, along with the
multi-stage design of the network enabled by convolutions,
make our CvT architecture achieve superior performance
while maintaining computational efﬁciency. Furthermore,
due to the built-in local context structure introduced by convolutions, CvT no longer requires a position embedding,
giving it a potential advantage for adaption to a wide range
of vision tasks requiring variable input resolution.