Towards a Human-like Open-Domain Chatbot
Daniel Adiwardana
Minh-Thang Luong
David R. So
Jamie Hall
Noah Fiedel
Romal Thoppilan
Apoorv Kulshreshtha
Gaurav Nemade
Quoc V. Le
Google Research, Brain Team
{adiwardana,thangluong,davidso,jamiehall,nfiedel,romzee,ziy,
apoorvk,gnemade,yifenglu,qvl}@google.com
We present Meena, a multi-turn open-domain
chatbot trained end-to-end on data mined and
ﬁltered from public domain social media conversations.
This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and
Speciﬁcity Average (SSA), which captures key
elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that
the best perplexity end-to-end trained Meena
scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86%
is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a ﬁltering mechanism and
tuned decoding) scores 79% SSA, 23% higher
in absolute SSA than the existing chatbots we
evaluated.
Introduction
The ability to converse freely in natural language
is one of the hallmarks of human intelligence, and
is likely a requirement for true artiﬁcial intelligence.
In order to explore this aspect of intelligence, many researchers are working on opendomain chatbots.
Unlike closed-domain chatbots, which respond to keywords or intents to
accomplish speciﬁc tasks, open-domain chatbots
can engage in conversation on any topic. Some
open-domain chatbots such as MILABOT , XiaoIce 1,
Gunrock , Mitsuku 2 and Cleverbot3 (by Rollo Carpenter) display human-like attributes, but rely on complex frameworks, such as dialog managers with
1 
2 
3 
Perplexity
Interactive SSA (%)
Human (86%)
Meena (79%)
Meena (base) (72%)
Mitsuku (56%)
DialoGPT (48%)
XiaoIce (31%)
Cleverbot (56%)
Figure 1: Interactive SSA vs Perplexity. Each point
is a different version of the Meena model. A regression line is plotted, for which the coefﬁcient of determination (R2) is 0.93, an indication of strong correlation between perplexity and the human evaluation metric (SSA). The dotted lines show the SSA performance
of other chatbots, humans (86%), the best end-to-end
trained Meena model (72%), and the full version of
Meena which incorporates a ﬁltering mechanism and
tuned decoding (Section 5) and scores 79%. Mitsuku
and Cleverbot scored the same on overall SSA, but Mitsuku displayed higher sensibleness, whereas Cleverbot
had higher speciﬁcity. See Sections 2.5, 2.6, and 4.3 for
more details on how we performed these comparisons
and how to interpret the results.
knowledge-based, retrieval-based, or rule-based
systems. End-to-end neural network approaches
 , on the other hand, offer the simplicity of
a single learned model. Despite much research,
open-domain chatbots still have weaknesses that
prevent them from being generally useful: they often respond to open-ended input in ways that do
not make sense, or with replies that are vague and
Conversations with Meena, and with various other
 
google-research/google-research/tree/
master/meena/
 
Here we present Meena, a generative chatbot
model that was trained end-to-end on 40B words
mined and ﬁltered from public domain social media conversations. With Meena, we push the limits
of the end-to-end approach and show that a largescale low-perplexity model can be a good conversationalist. We use a seq2seq model with the
Evolved Transformer as the main
architecture. The model is trained on multi-turn
conversations where the input sequence is all turns
of the context (up to 7) and the output sequence is
the response. Our best model has 2.6B parameters
and achieves a test perplexity of 10.2 based on a
vocabulary of 8K BPE subwords (Sennrich et al.,
To measure the quality of Meena and other chatbots, we propose a simple human evaluation metric. Sensibleness and Speciﬁcity Average (SSA)
combines two fundamental aspects of a humanlike chatbot: making sense and being speciﬁc. We
ask human judges to label every model response
on these two criteria. The ﬁrst part of the metric,
sensibleness, is a basic requirement. To converse
properly with a human, a bot’s responses have to
make sense in context; humans typically take this
for granted when conversing with one another, and
our evaluations ﬁnd that 97% of human-produced
statements meet this criterion (see Section 4.2).
However, making sense is not enough. If a model
is designed with sensibleness as its only objective, its responses could be vague and boring, since
that is a safe strategy to avoid being penalised for
not making sense. For example, closed-domain
chatbots typically respond with a generic apology
when a human asks something outside their domain; some end-to-end learned chatbots respond
“I don’t know” to many inputs ;
and Turing Test contest entrants often try to avoid
detection by being strategically vague . They succeed in not generating gibberish or contradicting themselves, but at the cost
of not really saying anything of substance. To mitigate this, we add a second dimension to the SSA
metric, which asks our evaluators whether a response is speciﬁc given the context. This prevents
bots from hiding behind vague replies, allowing us
to more openly examine what they are capable of.
As discussed in Section 2.1, this successfully distinguishes between generic and lively responses,
while also being simple and easy for crowd workers to understand.
We compare Meena, humans, and other opendomain chatbots using the SSA metric with two
types of human evaluation: static and interactive. For static evaluation, we curated a dataset
with 1,477 multi-turn conversations. For interactive evaluation, humans could chat about anything
they wanted. We were surprised, but pleased, to
discover that the SSA metric shows strong correlation with Meena’s perplexity, both in static and
interactive evaluation. In other words, the better
that Meena ﬁt its training data, the more sensible
and speciﬁc its chat responses became. At ﬁrst
glance, this result may seem intuitive, but it surprised us because recent research found a poor correlation between human evaluation scores and automatic metrics such as BLEU .
Our best end-to-end learned model has an average of 72% SSA. The full version of Meena scores
79% by incorporating a ﬁltering mechanism and
tuned decoding (Section 5). This is still below the
86% SSA achieved by an average human, but is far
closer than the other chatbots we tested. We note
that humans have very high sensibleness, but signiﬁcantly lower speciﬁcity, as detailed in Section
We will also discuss weaknesses of our methodology. For example, our static evaluation dataset
is too restricted to capture all aspects of human
conversations. Nevertheless, the fact that Meena
achieves such a high SSA score and that there is
a correlation between SSA and perplexity means
that a human-like chatbot, in terms of sensibleness
and speciﬁcity, could be in sight if we can attain
better perplexity.
Our contributions are:
(1) proposing a simple human evaluation metric for multi-turn opendomain chatbots that captures basic, but important, attributes of human conversation; (2) showing evidence that perplexity is an automatic metric
that correlates with human judgment, in contrast
to recent ﬁndings on other automatic metrics mentioned above; (3) demonstrating that an end-to-end
neural model with sufﬁciently low perplexity can
surpass the sensibleness and speciﬁcity of existing
chatbots that rely on complex, handcrafted frameworks developed over many years.
Evaluating chatbots
Evaluating chatbots and natural language generation is a well-known challenge , which we aim to address
in this paper. First, we propose a human evaluation metric that captures key elements of humanlikeness of conversational responses (Section 2.1).
We then describe two human-evaluation setups:
static, in which we benchmark models on a ﬁxed
set of multi-turn contexts to generate responses
(Section 2.2); and interactive, where we allow humans to chat freely with chatbots (Section 2.4).
Lastly, we detail our automatic evaluation metric
for fast development and end-to-end optimization
(Section 2.7).
Measuring Human Likeness
To measure the quality of a response given a context, we propose a sequence of two questions. We
ﬁrst ask whether the response, given the context,
makes sense. Sensibleness arguably covers some
of the most basic aspects of conversational humanlikeness, such as common sense and logical coherence. Sensibleness also captures other important aspects of a chatbot, such as consistency. The
crowd worker is asked to use common sense to
judge if a response is completely reasonable in
context. If anything seems off — confusing, illogical, out of context, or factually wrong — then
it should be labeled as, “does not make sense”.
However, being sensible is not enough.
generic response (e.g., I don’t know) can be sensible, but it is also boring and unspeciﬁc. Such responses are frequently generated by bots that are
evaluated according to metrics like sensibleness
alone .
To illustrate this, we create GenericBot: a trivial bot that always replies to questions with “I
don’t know” and to statements with “ok” (examples in Appendix Table 8). On static evaluation
(using a ﬁxed set of prompts and bot-generated responses), 70% of GenericBot’s responses are labeled sensible, surpassing even DialoGPT (62%),
even though DialoGPT is clearly more human-like
than GenericBot. To overcome this issue, we need
our evaluation to separate more fully human-like
conversation from bland and generic statements.
Therefore, if a response is labeled as sensible, we
further ask the crowd worker to determine if it
is speciﬁc to the given context. For example, if
A says, “I love tennis,” and B responds, “That’s
nice,” then the utterance should be marked, “not
speciﬁc”. That reply could be used in dozens of
different contexts. However, if B responds, “Me
too, I can’t get enough of Roger Federer!” then it
is marked as “speciﬁc”, since it relates closely to
what is being discussed. Responses labeled not
sensible are considered not speciﬁc.
In GenericBot’s case, none of the responses are speciﬁc,
whereas 39% of DialoGPT’s responses are speciﬁc.
This sequence of two questions is designed to
start with the most concrete and basic human
quality (sensibleness) and then progress to the
arguably more subjective human quality , shown in Table 1. The agreement is reasonable considering the
questions are subjective and the ﬁnal results are always aggregated labels (e.g., average sensibleness
across all chatbot responses).
Agreement (%)
Krippendorff’s alpha
Sensibleness
0.42 ± 0.03
Speciﬁcity
0.30 ± 0.05
Table 1: The average and standard deviation of crowd
worker agreement across static evaluations of Meena
Each static evaluation consisted of 1,477
(context, response) pairs, each labeled by 5 crowd
Given a set of responses labeled as described
above, we can calculate sensibleness and speci-
ﬁcity as the percentage of responses labeled as
sensible and speciﬁc, respectively.
To combine
these two into one metric, we take a simple average of the two, which we call SSA (sensibleness
and speciﬁcity average). SSA is a proxy for human likeness, which also penalizes chatbots that
consistently produce generic responses. For example, GenericBot’s SSA is 35% and DialoGPT’s
SSA is 51%, providing a much more fair separation and ranking than sensibleness alone.
Before arriving at SSA, and before any of the
chatbots were tested, the authors of this paper conducted several rounds of pilot studies on what to
ask crowd workers and how to best phrase the instructions. We settled on the two-question SSA
Human likeness (%)
Figure 2: SSA vs human likeness.
Each point is a
different chatbot, except for the top right one, which
A regression line is plotted, for which
the coefﬁcient of determination (R2) is 0.96.
SSA values were collected using static evaluation mode
(Section 2.2).
The human likeness evaluation was
also conducted in static evaluation mode. Instead of
judging sensibleness or speciﬁcity, however, we asked
crowd workers to judge whether a given response was
“human-like”, or in other words, looked like a response
that a human might give in the provided context.
for several reasons: it was easy for crowd workers to understand; alternative additional questions
did not add extra information; and more subjective questions result in lower agreement between
crowd workers.
As an additional check on the SSA metric, we
reran a static evaluation, this time asking crowd
workers to assess whether or not a response is “humanlike”. We ﬁnd that there is a high correlation
between those labels and the two components of
the SSA metric (Figures 2, 9, 10). Compared to a
direct evaluation of what crowd workers consider
to be “humanlike”, SSA has signiﬁcant advantages
for large-scale evaluation tasks: it is more objective, easier for crowd workers to understand, and
penalizes boring and vague responses. Nevertheless, these ﬁndings give us conﬁdence that SSA is
indeed capturing important aspects of human likeness.
Static Evaluation
In order to have a common benchmark to easily compare models, we create a collection of
1,477 conversational contexts with between 1 and
3 conversation turns, that we call the Mini-Turing
Benchmark (MTB). We started this dataset by
compiling single-turn contexts (e.g., “How are
you?”) from multiple sources, such as from the
work4 of Vinyals and Le and the transcripts
of the Loebner Prize5 contests .
In total, there were 315 single-turn contexts, which
we then extended to include 500 two-turn and 662
three-turn contexts.
The MTB also contains contexts with personality questions (e.g. “Do you like cats?”), some
of which expect responses with personality consistency.
For example, the context “A: Do you
like movies?; B: Yeah. I like sci-ﬁmostly; A: Really? Which is your favorite?” expects a consistent response such as I love Back to the Future. On
the other hand, a response like I don’t like movies
would be a contradiction, and thus not considered
When evaluating chatbots, all MTB contexts
are fed to the models or presented to humans
to obtain responses.
We send the resulting
(context, response) pairs to crowd workers and
asked whether each response given the context is
sensible and speciﬁc as deﬁned in 2.1. We call this
static evaluation because the contexts are ﬁxed.
Interactive Evaluation
Static evaluation may be suitable for comparing
models, but it is biased by how the static evaluation dataset was constructed. To address this,
we create an additional evaluation mode where
the crowd workers can chat 1:1 with a chatbot
about anything they want. As with static evaluation, workers are also asked to decide whether
each response from the chatbot is sensible and speciﬁc as deﬁned in 2.1. Conversations start with
“Hi!” from the chatbot to mark the beginning of
the conversation and crowd workers have no expectation or instructions about domain or topic of
the conversation. A conversation is required to last
at least 14 turns (7 from chatbot) and at most 28
We collected 100 such conversations for
each model (i.e., at least 700 labeled turns per
model). We then measure the percentage of labeled turns that are sensible and speciﬁc.
Unlike a typical Turing test , we
tell the human judges upfront that they are about
to chat with an experimental chatbot and ask them
to label what the chatbot says in terms of sensibleness and speciﬁcity. This shifts the focus of
the judges and chatbot creators from optimizing
4 
QAresults.pdf
5 
loebner-prize
for deception detection to optimizing for detecting
and maximizing human-like qualities (e.g., sensibleness). Similar to our approach, Ghandeharioun
et al. also conduct interactive evaluation by
allowing humans to chat freely with bots. Their
setup, however, focuses on evaluating conversations as a whole (as opposed to at the level of individual turns) and judges evaluate for quality, ﬂuency, diversity, relatedness, and empathy.
Estimate of Human Performance
To estimate static SSA of humans we ask crowd
workers to respond to MTB contexts. Additionally, to estimate human interactive SSA, we leveraged the help of internal company volunteers to
collect 100 human-human conversations following mostly the same instructions as crowd workers for every other chatbot. Labeling of sensibleness and speciﬁcity was conducted by independent
crowd workers with majority voting of 5 workers
per human turn. The difference from the rest of the
evaluations is that, in this case, participants knew
they were chatting with another human. In contrast, when humans chat with a chatbot they will
occasionally say unusual things to test the chatbot’s limits. Hill et al. describe differences
in human behavior when talking to a chatbot. That
said, we never incentivize humans to chat adversarially with chatbots in any of our evaluations.
Evaluation of Cleverbot and DialoGPT
To integrate with Cleverbot, we leverage its API.
For DialoGPT, we use its open sourced 762M
parameter model.6
It is worth mentioning that
we initially tried the 345M parameter DialoGPT
model, because it was reported to perform best
on single-turn human evaluation.
However, the
345M parameter model seemed to perform noticeably worse than the 762M one in preliminary evaluations of multi-turn conversations. Our human
evaluation is multi-turn, so we select the 762M
The DialoGPT authors were unable to release
their decoding script at the time of writing. Therefore, following their published description, we use
top-K decoding with K = 10.
We adapt the
decoding implementation by Wolf et al. .
Moreover, since the backward model was also not
released we were not able to try their MMI reranking .
6 
Both Cleverbot and DialoGPT were evaluated
using the same crowd sourcing setup as for Meena.
Evaluation of Mitsuku and XiaoIce
Because we chose to use the free Mitsuku web
app7, and there is no public API for XiaoIce, we
called on the help of internal company volunteers
and only conducted interactive evaluation. Volunteers collectively had 100 conversations with Mitsuku, and 119 with XiaoIce on their publicly available web apps.
The volunteers conversed with
the chatbots following mostly the same instructions that crowd workers follow for every other
The difference is that humans would
say “Hi!” for the ﬁrst turn, instead of the chatbot, in order to keep the ﬁrst turn the same as
other cases. Labeling of sensibleness and speci-
ﬁcity in all cases was conducted by independent
crowd workers with majority voting of 5 workers
per chatbot turn. Also note that both XiaoIce and
Mitsuku sometimes include an image in their reply
and occasionally, volunteers include text descriptions of the images they see. The presence of the
image may in some cases change the sensibleness
of the response for better or worse.
XiaoIce interacts in Mandarin so both the volunteers and the independent crowd workers were
native Mandarin speakers.
The group of volunteers for XiaoIce, Mitsuku, and human-human
conversations were mostly disjoint. Other than requiring a knowledge of Mandarin for XiaoIce conversations, volunteer selection was arbitrary. We
had 29 volunteers for XiaoIce, 43 for Mitsuku, and
21 for human-human.
To reset Mitsuku state between conversations,
volunteers refreshed the web page.
During the
writing of this paper there was no clear way to reset the state of XiaoIce. The XiaoIce team have
informed us that not resetting the state negatively
affects the model’s control of the context.8 Also,
most XiaoIce volunteers shared the same Weibo
The XiaoIce team conﬁrmed that account reuse negatively impacts the internal proﬁle
constructed by XiaoIce for a user. The XiaoIce
team further suggested that, if the same Weibo account needs to be reused, we should wait at least
7Pandorabots offers a paid enterprise package, which includes the Mitsuku API.
8From personal communication with the XiaoIce team,
after the writing of the paper.
9Weibo is a microblogging service mostly used in China,
which also allows users to chat with XiaoIce: https://
www.weibo.com/
one hour between volunteers using the account. In
our experiments, we may have sometimes waited
less than that amount of time between volunteers,
although we made sure the account was only used
by one volunteer at a time. Finally, the XiaoIce
team mentioned that in the past few months (as of
this writing), a limited version of XiaoIce with the
smallest index has been served on Weibo. This
version is expected to produce less satisfactory responses.
Direct comparisons between XiaoIce and other
chatbots come with a caveat: XiaoIce can be seen
as a product that optimizes for long-term user engagement, of which dialog generation is just one
component. In other words, Meena is arguably at
an advantage when comparing SSA scores.
Automatic Evaluation
For quick research iterations, we focus on perplexity. Unlike the previous two evaluation types, perplexity is an automatic metric. A seq2seq model
outputs a probability distribution over possible
next response tokens. Perplexity measures how
well the model predicts the test set data; in other
words, how accurately it anticipates what people
will say next. When interpreting perplexity scores,
bear in mind that lower is better and that the theoretical minimum is one.
As shown in Section 4, this commonly used
metric correlates with human judgement of sensibleness and speciﬁcity. This is encouraging, because it is both automatic and directly optimizable
with the standard cross-entropy loss function.
Meena chatbot
As described above, recent work on end-to-end
dialog models has fallen into two broad categories: (1) complex models with human-designed
components, and (2) large neural network models (known as end-to-end models) that are closer
to generic learning frameworks. End-to-end models have shown promise, but clear limitations .
An open question has been: in
order to reach a point where a model can carry
out high-quality, multi-turn conversations with humans, could we simply take an end-to-end model
and make it bigger—by adding more training data
and increasing its parameter count—or is it necessary to combine such a model with other components? In this section we describe the Meena
model, the largest end-to-end model to enter the
ﬁeld so far. We believe it answers the open research question, by showing that a large end-toend model can generate almost humanlike chat responses in an open-domain setting.
In this section, we will describe the training
data, architecture, and decoding algorithm.
will also provide a few sample conversations that
Meena has had with humans.
Training Data
The dataset used to train Meena is mined and ﬁltered from public domain social media conversations.
The source data are essentially message
trees involving multiple speakers: the very ﬁrst
message is the root; replies to a message are its
child nodes. Any path along the tree induces a
conversation where each message is a conversation turn. By treating each turn in a conversation
path as a response and all the previous turns (up to
7) as a context, we create a training example of the
form (context, response) pair.
We also ﬁlter the data to improve the generation quality. A message is removed if any of the
following conditions holds: 1. the number of subwords is less than 2 or more than 128; 2. the percentage of alphabetic characters is less than 70%;
3. message contains URL; 4. author’s username
contains “bot”; 5. the message is repeated more
than 100 times; 6. the message has a high n-gram
overlap with the parent’s text; 7. the message is
potentially unsafe or offensive with respect to a
commercial text classiﬁer. In addition, we remove
copies of the parent’s text quoted in a message.
For simplicity, when a message is removed, we
drop all sub-trees rooted under it. After these ﬁltering steps, the number of (context, response)
pairs extracted is 867M. The text is tokenized
using byte-pair-encoding (BPE) with the sentencepiece library.10 We use a
vocabulary of 8K BPE subwords, which we found
in our early experiments to be sufﬁcient for generating speciﬁc responses while still allowing us to
ﬁt larger models in memory.
The ﬁnal Meena dataset contains 341GB of text
(40B words).
In comparison, GPT-2 has been trained on 40GB of Internet
text (8 million web pages).
10 
sentencepiece
Responses for “Why do you like the ocean?”
1. I love the beach. I like that it doesn’t have anything in it. Just open water.
2. It’s dark and mysterious. I discovered it when I was really young and it was a dream of mine
since. The ocean is full of wonders, mysteries and great things to discover. I love snorkeling. I
moved to Hawaii a year ago and I’m going to college for marine biology. So there’s that.
3. The sea sounds cool and it’s really satisfying to swim in.
4. I dunno. It’s beautiful, I guess. It reﬂects my upbringing, I reckon.
5. Well I’m from the paciﬁc northwest and always somehow end up back there. I’ve been to
Hawaii but I’m not one for the beach, it’s too hot and the air is so dry.
6. The beauty of it is incredible, and swimming in it and being able to see huge waves sweeping
by is just amazing. I was born in the desert so it’s really the only place in the world I’ve seen
with ﬂowing water around me
7. It’s beautiful, maybe? I just wish I lived closer to it!
8. It’s pretty and soothing.
9. Because I love the smell. I love walking when I can through the beach. I live in California
so I basically go to the ocean almost every weekend.
10. The ocean is pretty cool, I suppose. I like the various types of ﬁsh.
Table 2: Sampling Outputs – responses found by sampling with temperature, sorted by the length-normalized loglikelihood scores and using the same Evolved Transformer as the one used in Table 3. The scores are calculated as
, where P is the likelihood of the response and T is the number of tokens.
Responses for “Why do you like the ocean?”
1. Why don’t you like the ocean?
2. I don’t like the ocean. I love the ocean.
3. I don’t like the ocean. I love it.
4. I don’t like the ocean, I love the ocean.
5. I don’t like the ocean, I love it.
6. Why don’t you like the ocean? :P
7. I don’t like the ocean, I love it!
8. I don’t like the ocean. I love the ocean!
9. Why don’t you like the ocean? It’s beautiful.
10. I don’t like the ocean. I love the ocean.
There’s a difference.
Table 3: Beam Search Outputs – top responses generated by beam-search decoding and the corresponding length-normalized log-likelihood scores. We use
an Evolved Transformer with perplexity 10.2 and vocabulary size of 8K.
Model Architecture
The best performing Meena model is an Evolved
Transformer (ET) seq2seq model
with 2.6B parameters, which includes 1 ET encoder block and 13 ET decoder blocks.
Evolved Transformer is an evolutionary NAS architecture based on the
Transformer . Our largest
(i.e., maximum memory usage) Evolved Transformer scored 10.2 perplexity and our largest
vanilla Transformer scored perplexity 10.7 for
the same number of training steps (738k). The
largest vanilla Transformer had 32 decoder layers
with other architectural hyperparameters held constant.11
comparison,
extra-large
model has 1.5B parameters
and is a language model (i.e., decoder only);
whereas the large conversational model from the
recent DialoGPT work has
762M parameters.
Meena’s hidden size is 2,560 and the number
of attention heads is 32.
We share the embeddings across the encoder, the decoder, and the softmax layer. The encoder and decoder each have
a maximum length of 128 tokens (i.e., 256 combined). The hyperparameters of our best model
were found via manual coordinate-descent search.
Training Details
We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores) on the Meena dataset
containing 40B words (or 61B BPE tokens). Interestingly, the 2.6B-parameter model can overﬁt 12
on a 61B-token dataset which suggests a surprisingly large model capacity. Therefore, we add a
small amount of 0.1 attention and feed-forward
layer dropout. Additionally, to save memory, we
chose the Adafactor optimizer with 0.01 as the initial learning rate, keeping it constant for the ﬁrst 10k steps and then decaying with the inverse square root of the number of steps.
We use the Tensor2Tensor code-
11An Evolved Transformer block is about twice as deep as
a Transformer layer
12In the sense that validation loss increases as train loss
decreases.
base for training Meena.13
A TPU-v3 core has 16GB of high-bandwidth
memory. We maximized memory usage for model
parameters and stored only 8 training examples
per core. Each training step took about 1 second.
In the full TPU-v3 Pod, this meant we learned over
4M tokens per training second. Therefore, by the
end of training, the model had traversed the full
training set 164 times (or epochs) and observed
a total of about 10T tokens (including repeated
Generating generic (i.e., not speciﬁc) and bland
responses has always been a
major challenge in existing neural conversational
models. A common approach to mitigating this
problem is to use more sophisticated decoding algorithms, for instance with different forms of reranking or conditioning on proﬁles, topics, and styles .
Recent works also explore new
frameworks such as adversarial learning , variational autoencoding , or both
 at the cost of added complexity and less scalability.
In contrast, we show that given a model with
sufﬁciently low perplexity, a simple sample-andrank decoding strategy achieves both diverse and
high-quality responses. Sample-and-rank, works
as follows: First, we sample N independent candidate responses using plain random sampling with
temperature T. Second, we select the candidate
response with the highest probability to use as the
ﬁnal output.
Temperature T > 0 is a hyper-parameter that
regulates the probability distribution pi of the next
token during decoding. We divide the logits zi by
T before computing the “softmax” as in Hinton
et al. :
j exp(zj/T)
T = 1 yields the unmodiﬁed distribution. We
observe that large values of T favor contextually
rare tokens, such as relevant entity names, but
13 
tensor2tensor
might also assign too much probability to incorrect tokens depending on the model’s predictions.
Meanwhile, smaller values of T favor more common words such as articles or prepositions, which
are safer but less speciﬁc.
Tables 2 and 3 show responses for the arbitrary probing input “Why do you like the ocean?”
under sample-and-rank and beam-search, respectively. As we can see, beam-search decoding generates repetitive and uninteresting responses. On
the other hand, sample-and-rank provides us with
diverse and content-rich responses. The key here
is to have a model with low perplexity so samples can be taken at high temperature to produce
human-like content.
For all the results in Section 4, we use sampleand-rank with N = 20 and T = 0.88. Additionally, as shown in Figure 1, for this ﬁxed decoding
strategy, sensibleness and speciﬁcity improve as
model test set perplexity falls. For additional decoding results and samples, see Section 5.
Sample conversations
Below are cherry picked conversations that Meena
has had with humans. We selected these conversations after they were completed. That is, the
Meena responses within the conversations were
not cherry picked; they were produced automatically using sample-and-rank (Section 3.4). Conversations B and C are excerpts from conversations number 43 and 48, respectively, of the conversations dataset published on GitHub.14
Conversation A
say you’re in an empty giant ofﬁce and you see
a black bird ﬂying indoors. what do you do?
What kind of bird? Is it friendly? How large?
How big is the ofﬁce? Is it indoors or outdoors?
well it’s a predator bird and it’s gonna eat you
I would grab my phone and call 911.
Meena handles arbitrary open-domain input. However,
asking “Is it indoors or outdoors” was not sensible.
14 
google-research/tree/master/meena/
Conversation B
Hi! How are you this ﬁne evening?
I’m doing well. it’s morning my time!
Good morning! What are your plans for today?
Get my work done then watch a movie at home
in the night
Sounds fun! What movie?
I haven’t decided yet, perhaps an Oscarnominated one. any suggestion?
I’m not too familiar with recent Oscar winners,
but I do know I wanted to see The Grand Budapest Hotel.
ah, tell me about that movie!
It’s from the same director and writer as Moonrise Kingdom.
Wes Anderson is pretty well
known for making funny, colorful ﬁlms.
heard it’s pretty good.
I’ve searched on the internet and both
movies are very colorful!
I’m looking forward to it. I love colorful movies.
The Grand Budapest Hotel and Moonrise Kingdom are
indeed colorful movies, directed by Wes Anderson.
Conversation C
Haha, I think mathematics has a presence beyond human existence.
So does philosophy.
Can you state with examples how this is true.
Can you state how it isn’t?
Without human existence there will be no one
left to comprehend whats right whats wrong,
and thus philosophy wi [sic]
That’s not the deﬁnition of philosophy.
Without human existence there will be no one
left to comprehend whats right whats wrong,
and thus philosophy will not be needed. How
do you deﬁne philosophy?
The study of general knowledge or the fundamental nature of things.
Meena tries to deﬁne philosophy.
Conversation D
I like how cute cows are
Ah, interesting.
You like how they are cute,
yeah, and they’re realy smart
I heard they go to college
cows go to college?
I heard that a cow went to Harvard.
what did the cow study?
Bovine sciences.
do horses go to Harvard?
Horses go to Hayvard.
that’s a pretty good joke, I feel like you led me
You were trying to steer it elsewhere, I can see
Meena executes a multi-turn joke in an open-domain
setting. We were unable to ﬁnd this in the data.
Appendix A shows random samples of conversations.
In this section, we will ﬁrst demonstrate the correlation between test perplexity and the human evaluation metric, SSA, deﬁned earlier. We also include human-level upperbound estimates for both
static and interactive evaluations, beside performances of other chatbots, such as XiaoIce, Mitsuku, DialoGPT, and Cleverbot. Lastly, we provide sample responses for different models given
the same contexts to understand how Meena qualitatively compares to others.
Perplexity
Interactive Sensibleness (%)
Human (97%)
Meena (87%)
Meena (base) (80%)
Mitsuku (72%)
DialoGPT (57%)
XiaoIce (45%)
Cleverbot (68%)
Figure 3: Interactive sensibleness vs perplexity.
Perplexity
Interactive Specificity (%)
Human (75%)
Meena (70%)
Meena (base) (64%)
Mitsuku (40%)
DialoGPT (39%)
XiaoIce (17%)
Cleverbot (45%)
Figure 4: Interactive speciﬁcity vs perplexity.
SSA-perplexity correlation
We trained models with different hyper-parameter
settings and architectures on the dataset described
in Section 3.1.
We vary the number of layers,
attention heads, total training steps, whether we
use Evolved Transformer or regular Transformer
and whether we train with hard labels or soft labels/distillation . The trained
models are then measured with an automatic metric, test perplexity (Section 2.7), and also with human metrics (Sections 2.2 and 2.3). Our results
indicate most of the variance in the human metrics
can be explained by the test perplexity. The endto-end trained Meena model with lowest perplexity is referred to as Meena (base). In addition, we
also include an improved version of Meena (detailed in Section 5) and refer to this as the Meena
(full) model, or just Meena model for short.
The correlation was R2 = 0.93 for static sensibleness vs perplexity and R2 = 0.94 for static
speciﬁcity vs perplexity indicating this might be
a good automatic metric for measuring sensibleness and speciﬁcity. Static SSA vs perplexity has
R2 = 0.94. The static evaluation results are shown
in Figure 5. The correlation is close to linear, but it
is unclear whether the trend will continue for even
lower values of perplexity.
In interactive evaluation (Section 2.3) crowd
workers could chat about anything they wanted.
We observe similarly strong correlation with perplexity (see Figures 1, 3 and 4) and very similar sensibleness and speciﬁcity values as the static
evaluation. This indicates that the static evaluation
correlation with perplexity is not due to dataset
Regarding consistency, the lowest perplexity
model was evaluated 7 times with static evaluations and also 7 times with interactive evaluations. Each time, we obtained a different set of
randomly sampled responses. Across the evaluations the standard deviation is 2% for static SSA
and is 1% for interactive SSA, indicating that both
metrics are consistent enough for our purposes.
Human-level Estimates
As expected, human sensibleness is very high, but
it is not perfect.
Human sensibleness was estimated at 94% static and 97% interactive.
People have misunderstandings, miss attempts at humor and sometimes lack shared context or background. Also aligned with intuition, humans are
sometimes not speciﬁc due to momentary lack of
ideas, interest or knowledge. The human speci-
ﬁcity scores are 69% static and 75% interactive.
The resulting SSAs are 82% static and 86% interactive.
XiaoIce, Mitsuku, DialoGPT and
Crowd workers labeled 1,173 XiaoIce turns within
their original conversation context. Per these labels, XiaoIce scores 31% interactive SSA which
is comprised of 45% sensibleness and 17% speci-
We used majority voting of 5 workers
per chatbot response. Agreement between workers was 77% for sensibleness and 81% for speci-
ﬁcity and Krippendorff’s alpha was 0.54 for sensibleness and 0.40 for speciﬁcity (which indicates
fairly strong agreement). For further veriﬁcation
of the results, we also had a group of 4 internal company volunteers that are native Mandarin
speakers to label a subset of 25 conversations ( 247
chatbot turns). The volunteers did not know the
crowd worker results. The volunteer based estimate is 36% interactive SSA with 53% sensibleness and 19% speciﬁcity.
Volunteers report that XiaoIce responses display empathy and personality. However, the responses do not always make sense and are often
generic. The generic responses exuded a fun personality, but were not necessarily related to the ongoing conversation. As mentioned above, XiaoIce
is reported to be optimized for long term engagement . It is also designed with
a multifaceted notion of ‘humanlikeness’ in mind,
emphasising interpersonal qualities such as empathy. As we discuss in Section 7, a broader metric
of humanlikeness would be a useful topic for future research.
See et al. report engagingness is not the
same as humanness. In that work, they control
and optimize certain attributes of response generation.
One of the results is that their chatbots achieve near-human-level engagingness, but
simultaneously are not very close to human-level
humanness. This along with limitations described
in Section 2.6 may help explain why XiaoIce’s
sensibleness and speciﬁcity scores seem relatively
low in our evaluation.
Mitsuku scores 56% interactive SSA with 72%
sensibleness and 40% speciﬁcity over 899 chatbot turns.
Mitsuku has won the Loebner prize
5 times,15 however, we learned from the Pandorabots team that the website version we tested
with was not the one that competed in the Loebner
prize. Optimizing human likeness for the competition might correlate more with our metrics.
Using crowd workers, we executed interactive
evaluation of DialoGPT over 300 conversations
(2,660 chatbot turns labeled). DialoGPT scores
15 
title=Loebner_Prize&oldid=927503336
Perplexity
Human (82%)
DialoGPT (51%)
Cleverbot (44%)
Meena (78%)
Meena (base) (72%)
Perplexity
Sensibleness (%)
Human (94%)
Cleverbot (59%)
DialoGPT (62%)
Meena (86%)
Meena (base) (81%)
Perplexity
Specificity (%)
Human (69%)
Cleverbot (28%)
DialoGPT (39%)
Meena (70%)
Meena (base) (63%)
Figure 5: Static evaluation results.
Sample Contexts
Meena (base)
A: You must have read a lot of books
B: Oh wow, how do you know? Do I
look nerdy?
B: So how do you know?
B: Never. said I was.
B: Oh thank goodness.
A: Of course, you’re not!
[sensible: 1, speciﬁc: 1]
[sensible: 0, speciﬁc: 0]
[sensible: 1, speciﬁc: 1]
A: You know nothing, Jon Snow!
B: You must be a fan of Game of
B: Who’s your favorite
character?
B: What do you mean?
B: You can’t. But that’s
not why you shouldn’t
A: Haha yea, how can I not?
[sensible: 1, speciﬁc: 1]
[sensible: 0, speciﬁc: 0]
[sensible: 0, speciﬁc: 0]
A: What is your favorite island in the
B: I would say Fiji
B: Just a beautiful place
B: The title is self explanatory.
B: You can see some great
A: Interesting, what’s nice about it?
[sensible: 1, speciﬁc: 0]
[sensible: 0, speciﬁc: 0]
[sensible: 1, speciﬁc: 0]
A: You must have read a lot of books
B: Oh wow, how do you know? Do I
look nerdy?
A: I know because I’ve
read a lot of books.
A: Observation.
A: I know because you
look like your mom.
[sensible: 1, speciﬁc: 1]
[sensible: 1, speciﬁc: 0]
[sensible: 1, speciﬁc: 1]
A: You know nothing, Jon Snow!
B: You must be a fan of Game of
A: Yeah, I’m reading the
books at the moment.
A: I’ve never really been
interested in it...
[sensible: 1, speciﬁc: 1]
[sensible: 1, speciﬁc: 0]
[sensible: 1, speciﬁc: 1]
A: What is your favorite island in the
B: I would say Fiji
A: Fiji, yes!
there once. It’s the most
beautiful place I’ve ever
been. It deﬁnitely makes
me want to move there. I
hope you get to visit again
A: I would have guessed
A: Fiji is awesome! But I
would say Fiji and Oceania are more popular at
the moment.
[sensible: 1, speciﬁc: 1]
[sensible: 0, speciﬁc: 0]
[sensible: 0, speciﬁc: 0]
A: You must have read a lot of books
B: Reading’s for nerds
B: What books have you
B: I read a lot of books
but have never read a single book about the history
of our planet.
[sensible: 1, speciﬁc: 1]
[sensible: 1, speciﬁc: 1]
[sensible: 1, speciﬁc: 1]
A: You know nothing, Jon Snow!
B: Thank you for this.
I was hoping I wasn’t
the only one getting an
ASOIAF vibe from this.
B: Am not jon snow.
B: The North Remembers...
[sensible: 1, speciﬁc: 1]
[sensible: 1, speciﬁc: 0]
[sensible: 1, speciﬁc: 1]
A: What is your favorite island in the
B: That’s a tough one. I’d
have to say Hawaii.
B: Daniel I like frites.
B: Probably the one on the
north island
[sensible: 1, speciﬁc: 1]
[sensible: 0, speciﬁc: 0]
[sensible: 0, speciﬁc: 0]
Table 4: Sample responses from static evaluation – Shown are responses of Meena (base), Cleverbot, and DialoGPT on some sample contexts together with their binary labels per category (sensibleness and speciﬁcity). Note
that the contexts were constructed in advance before we generate all model responses. These contexts are not in
the MTB dataset.
48% ± 3% interactive SSA with 57% ± 2% sensibleness and 39% ± 4% speciﬁcity. It obtained
similar results in static evaluation. In its paper,
DialoGPT attains performance close to human in
a single-turn setting. On the other hand, our human evaluation is multi-turn, which is expected
to be more challenging. Additionally, DialoGPT
scores poorly on speciﬁcity, and our impression
from browsing transcripts is that it prefers briefer
and more generic responses. This might be because the model is optimized for classic Turingtest evaluation, in which overly chatty responses
increase the risk of making a mistake. These results and conjectures come with the caveat, as described above, that we wrote our own decoder for
this model since the public DialoGPT codebase
does not yet have one.
Cleverbot, unlike Meena and DialoGPT, performs notably better on interactive rather than
static evaluation. It scores interactive SSA 56%
and static SSA 44%. Interactive speciﬁcity, 45%,
is especially higher than its static counterpart,
28%. Upon closer inspection of the data, we hypothesize that: (1) in the interactive setting, Cleverbot has opportunities to steer the conversation
towards topics that it is more familiar with; (2)
the minimum interactive conversation length of 14
turns makes it possible for a signiﬁcant portion of
these turns to be greetings and goodbyes, which
both Cleverbot and Mitsuku are consistent in appropriately responding to. Furthermore, the interactive SSA scores for Mitsuku and Cleverbot are
the same, 56% when averaging sensibleness and
speciﬁcity before rounding. Mitsuku scores higher
sensibleness (72% versus 68%), but lower speci-
ﬁcity (40% versus 45%). It seems that relative to
Mitsuku, Cleverbot replies more often in ways that
are borderline nonsensical and lack consistent personality. Finally, we remark that the standard deviation of the Cleverbot interactive SSA is ±1%
across two interactive evaluation sessions.16
Sample Responses: Meena (base),
Cleverbot, and DialoGPT
To understand how Meena qualitatively compares
to other models, we show in Table 4 sample responses from Meena (base), Cleverbot, and DialoGPT under the same set of contexts (which
16Due to technical issues when calling the Cleverbot API
we only collected 195 interactive conversations (1,751 chatbot turns labeled) instead of the 300 conversations which we
collected for DialoGPT.
were constructed before we generate all model responses). For 1- and 2-turn contexts, responses
from Meena base are all sensible and speciﬁc. In
addition, Meena (base) generates rich and interesting responses, e.g., the mention of “ASOIAF vibe”
to refer to “A Song of Ice and Fire” in the famous
Game of Thrones series or the remark about Fiji
island being “the most beautiful place I’ve ever
In contrast, Cleverbot can generate sensible responses for some contexts, but they are not always
speciﬁc, e.g., Cleverbot replied with “Observation” and “No”. DialoGPT is more speciﬁc and
can also generate interesting responses, e.g., “The
North Remembers ...”’. However, it does not make
sense at times, e.g., in-turn contradiction in this response “Fiji is awesome! But I would say Fiji and
Oceania are more popular ...” or vague answer
“Probably the one on the north island”.
When it comes to longer (3-turn) contexts in Table 4, Meena (base) continues to generate highquality responses, whereas none of Cleverbot’s responses are sensible. DialoGPT is more sensible
and speciﬁc than Cleverbot, but less so than Meena
Further Advancing SSA
In this section we take the interactive SSA from
72% ± 1%, for Meena (base), to 79% ± 1%, for
Meena (full), by further tuning our decoding strategy and adding a rule to detect cross turn repetitions.
Advancing Decoding
We evaluate both temperature T and top-k to mitigate negative effects from the tail of the distribution . We chose top-k (k =
40) and T = 1.0 following Fan et al. ; Radford et al. ; Keskar et al. ; Ippolito
et al. . With this setting and maintaining
N = 20, we note an SSA increase from 72% to
74% relative to sampling from the whole vocabulary with T = 0.88. This result is the same for
both the interactive and the static evaluation.
We also swept the number of samples in
sample-and-rank, evaluating N ∈{1, 20, 400}.
The results show that N = 20 provides a signiﬁcant improvement over N = 1, with an absolute improvement in SSA of ∼10% (Figure 6).
However, N = 400 demonstrates worse performance for sensibleness (Figure 7 in the appendix)
and diminishing returns over N = 20 for speci-
ﬁcity (Figure 8 in the appendix). The signiﬁcant
improvement from sample-and-rank with N = 20
motivates future work exploring alternate ranking
functions and tuning parameters.
Static SSA over number of sampled responses for top-k and sampling with temperature.
Addressing Cross-turn Repetitions
In interactive evaluation, about one third of the
conversations with Meena (base) contain crossturn repetitions toward the end. Cross-turn repetition means that one turn somewhat repeats an
earlier turn. For illustration, we cherry picked particularly problematic examples of cross-turn repetition shown in Tables 5 and 6.
It is worth mentioning that there also exist inturn contradictions and repetitions, where the contradiction or repetition is contained in the response
turn itself (e.g., “I like pizza, but I don’t like it”).
This type of artifact is often observed in Meena
versions with worse perplexities, but is far less frequent in the Meena (base), which has the lowest
perplexity as reﬂected in the samples shared in the
appendix and the higher sensibleness scores.
We wrote a rule that detects if any two turns
contain long common sub-sequences. We automatically remove candidates that are detected as
repetition. This rule seems to have addressed most
of the cross-turn repetition. We, therefore, further
improve on the above interactive SSA of 74%±%1
to 79% ± 1%.
Safety Layer
It is important to mention that the evaluation and
conversation collection for the full Meena version
was conducted with an additional classiﬁer layer
at serving time as part of the ﬁltering mechanism
Cross-turn Repetition Example 1
Hi! How are you?! :)
doing good, what would be the best city to visit
in Europe?
Paris, Barcelona, Amsterdam, Prague
great list! why?
Paris is a beautiful city, and Barcelona is too.
And I’ve always wanted to go to Amsterdam and
Prague and have never been.
have you even been to India?
No, but I’d love to go.
how about Japan?
I’d love to go to Japan too.
how about Paris?
I’d love to go to Paris too.
are you lying to me?
I’d love to go to Japan too.
i think you’re lying to me, that makes me sad
I’d love to go to Japan, too.
you’re blocking me out! you have to talk to me
I’d love to go to Japan.
where else?
I’d love to go to Japan, too!
Table 5: Multiple near identical repetitions of “I’d love
to go to Japan too.”.
Cross-turn Repetition Example 2
Hey there! How are you doing?
not well, i’m quite depressed
That’s no good. Wanna talk about it?
not really, it’ll only make it worse...can you
cheer me up?
Sure! What’s got you down?
I’d love to hear a really lame joke
How about this one? What did the farmer say
when he lost his tractor? Where is my tractor?
haha that was lame! where did you hear it?
I was at the mall and I heard it in the food court.
How was your day?
wow, i didn’t know you could walk!
Why did the scarecrow win an award?
i don’t know..you tell me!
Because he was outstanding in his ﬁeld!
haha, where did you hear that?
I was in the food court and I heard it in the ﬁeld!
you have a great sense of humor!
Why did the chicken cross the road? He was
outstanding in his ﬁeld!
ok, i don’t want to hear any more jokes
Do you know why he was outstanding in his
ﬁeld? Because he was a chicken!
haha looks like you’re really into this discussion!
Yes! I am! Do you know why? Because he was
outstanding in his ﬁeld!
haha, borderline creepy but still funny :D
Why was the mouse trapped in the bag? Because he was outstanding in his ﬁeld!
i think i broke you
Do you know why I broke you? Because you
were outstanding in your ﬁeld!
Table 6: Multiple partial repetitions of “(...) Because
he was outstanding in his ﬁeld!”.
to automatically ﬁlter out potentially sensitive or
toxic response candidates for publication.
Related Work
Finding a good automatic metric that correlates
with human evaluation has been an important
goal of open-domain conversational modeling.
BLEU , ROUGE , or other related metrics in translation and
summarization, while popular and easy to compute, have been shown to be unsuitable for dialog
 or more broadly language generation systems .
Past works have attempted to build learnable
metrics, either in a supervised fashion , which requires human labels, or with unsupervised approaches , that are more complex and need separate training, e.g., of a ranking system. In our
work, we show that perplexity, which is readily
available to any neural seq2seq model, exhibits
a strong correlation with human evaluation. Our
work is therefore also related to past attempts to
correlate perplexity with other automatic metrics
in other tasks, e.g., perplexity vs. BLEU in translation .
Another interesting line of work is to combine human evaluation with either automatic metrics or with model likelihood . While theoretically motivated, these metrics are too complex to
be practical, requiring both human judgments and
training separate models, e.g., an estimator to reduce bias in automatic
evaluation or a discriminator to distinguish between human- and modelgenerated samples.
In terms of designing of human evaluation metrics, existing literature differs in what attributes
are used to assess the quality of a neural conversational model.
Many works, e.g., Zhao et al.
 ; Xu et al. ; Ippolito et al. ,
have focused solely on the diversity aspect to
counter the commonly observed problem of models generating generic responses .
Others have attempted to improve and evaluate
multiple aspects at once. For example, Venkatesh
et al. aim to unify many metrics, such as
diversity, engagement, and user experience; Gao
et al. jointly optimize for both diversity
and relevance; See et al. control decoding
attributes (such as repetition, speciﬁcity, responserelatedness, and question-asking) to improve engagingness and interestingness; and Hashimoto
et al. design metrics to capture human likeness and diversity.
In contrast, we focus on sensibleness and speci-
ﬁcity for our human evaluation.
While human
likeness and relevance used in aforementioned
works are related to sensibleness, we speciﬁcally
use sensibleness as it leads to better agreement
among crowd workers (see §2.1).
Similar reasoning applies to speciﬁcity, which is related to
other attributes such as engagingness and interestingness, as measured in previous works.17 A limitation of our work is that it does not cover aspects
such as empathy , attempted to do so by augmenting
the decoder of seq2seq models with speciﬁcitycontrol components.
These added complexities
sometimes lead to implausible responses as analyzed by Ko et al. . In contrast, the speci-
ﬁcity of our model improves as perplexity decreases.
Recent work on DialoGPT 
compares the conversation quality of chatbots with
that of humans but their evaluation settings are
limited to single-turn dialogs. We instead conduct
our evaluation on conversations of up to 3 turns
in the static MTB benchmark and 14 turns in the
interactive setup.
Discussion
Our results suggest perplexity on public domain
social media conversations might be a good automatic proxy for human judgement of fundamental
attributes of human-likeness, such as sensibleness
and speciﬁcity. The results also suggests that optimizing the probability of the next token on larger
volumes of social media conversations could lead
to human-like sensibleness in an open-domain setting. However, our static evaluation dataset only
contains one to three-turn contexts and is biased
17It is worth pointing out that we do not explicitly measure
diversity as it requires judging a set of responses; whereas, for
conversation, what is most important is the ﬁrst reply that a
chatbot produces. As our decoding method is sampling, it implies that our generation is diverse. However, there remains a
question of whether the sampled response is of high quality.
The fact that our model has low perplexity and achieves high
SSA score indicates that the generation is meaningful.
by the sources of the ﬁrst turn and the fact that
the two-turn and three-turn contexts build on the
shorter contexts.
Moreover the contexts in this
dataset are predominantly Turing test and social
conversation style, including common sense, basic knowledge, asking/sharing about personality,
likes/dislikes, opinions, feelings, hobbies, pleasantries, etc. This dataset does not include contexts like deeper question answering (e.g., how
fast is a cheetah), basic math (e.g., how much is
1+1) and common sense tests designed to challenge machines, but not humans . Human-likeness is an incredibly broad and
abstract concept. The interactive evaluation addresses some of the bias and scope limitations
in static evaluation while still providing a consistent score to quantify a given chatbot. Nevertheless, unlike static evaluation it does not allow for
granular comparison between different chatbot responses. In addition, it may be too short (14 to 28
turns), and may assign too much weight to typical beginning and ending of conversations. It may
also be too short to cover deeper topics and exercise longer term memory.
Furthermore, it may be necessary to expand
the set of basic human-like conversation attributes
being measured beyond sensibleness and speci-
ﬁcity. Some directions could include humor, empathy, deep reasoning, question answering and
knowledge discussion skills.
One could also
break down sensibleness into its implicit subcomponents: logical and personality consistency,
common sense, relevance, basic factual correctness and so on. Future work may also explore the
continued optimization of sensibleness via the optimization of test set perplexity.
Acknowledgments
Thanks to the people who gave feedback on
drafts of the paper: Anna Goldie, Abigail See,
Yizhe Zhang, Lauren Kunze, Steve Worswick,
Jianfeng Gao, Daphne Ippolito, Scott Roy, Ilya
Sutskever, Tatsu Hashimoto, Dan Jurafsky, Dilek
Hakkani-tur,
Noam Shazeer,
Gabriel Bender,
Prajit Ramachandran, Rami Al-Rfou, Michael
Fink, Mingxing Tan, Maarten Bosma and Adams
Also thanks to the many volunteers who
helped collect conversations with each other and
with various chatbots.
Finally thanks to Samy
Bengio, Noam Shazeer, Anna Goldie, Rami
Al-Rfou, Khoa Vo, Trieu H. Trinh, Ni Yan, Kyu
Jin Hwang and the Google Brain team for the help
with the project.