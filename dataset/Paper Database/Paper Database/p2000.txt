Neural Approaches to Conversational AI
Question Answering, Task-Oriented Dialogues and Social Chatbots
Jianfeng Gao
Microsoft Research
 
Michel Galley
Microsoft Research
 
Google Brain
 
The present paper surveys neural approaches to conversational AI that have been
developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and
(3) chatbots. For each category, we present a review of state-of-the-art neural
approaches, draw the connection between them and traditional approaches, and
discuss the progress that has been made and challenges still being faced, using
speciﬁc systems and models as case studies.1
1We are grateful to the anonymous reviewers, Chris Brockett, Asli Celikyilmaz, Yu Cheng, Bill Dolan,
Pascale Fung, Zhe Gan, Sungjin Lee, Jinchao Li, Xiujun Li, Bing Liu, Andrea Madotto, Rangan Majumder,
Alexandros Papangelis, Olivier Pietquin, Chris Quirk, Alan Ritter, Paul Smolensky, Alessandro Sordoni, Yang
Song, Hisami Suzuki, Wei Wei, Tal Weiss, Kun Yuan, and Yizhe Zhang for their helpful comments and suggestions on earlier versions of this paper.
 
Introduction
Who Should Read this Paper?
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
Dialogue: What Kinds of Problems? . . . . . . . . . . . . . . . . . . . . . . . . .
A Uniﬁed View: Dialogue as Optimal Decision Making . . . . . . . . . . . . . . .
The Transition of NLP to Neural Approaches
. . . . . . . . . . . . . . . . . . . .
Machine Learning Background
Machine Learning Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Deep Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Two Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reinforcement Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Basic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Question Answering and Machine Reading Comprehension
Knowledge Base
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Semantic Parsing for KB-QA . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Embedding-based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Multi-Step Reasoning on KB . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Symbolic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Neural Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reinforcement Learning based Methods . . . . . . . . . . . . . . . . . . .
Conversational KB-QA Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Machine Reading for Text-QA . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Neural MRC Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Conversational Text-QA Agents
. . . . . . . . . . . . . . . . . . . . . . . . . . .
Task-oriented Dialogue Systems
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Evaluation and User Simulation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Simulation-Based Evaluation . . . . . . . . . . . . . . . . . . . . . . . . .
Human-based Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . .
Other Evaluation Techniques . . . . . . . . . . . . . . . . . . . . . . . . .
Natural Language Understanding and Dialogue State Tracking . . . . . . . . . . .
Natural Language Understanding
. . . . . . . . . . . . . . . . . . . . . .
Dialogue State Tracking . . . . . . . . . . . . . . . . . . . . . . . . . . .
Dialogue Policy Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Deep RL for Policy Optimization
. . . . . . . . . . . . . . . . . . . . . .
Efﬁcient Exploration and Domain Extension
. . . . . . . . . . . . . . . .
Composite-task Dialogues . . . . . . . . . . . . . . . . . . . . . . . . . .
Multi-domain Dialogues . . . . . . . . . . . . . . . . . . . . . . . . . . .
Integration of Planning and Learning
. . . . . . . . . . . . . . . . . . . .
Reward Function Learning . . . . . . . . . . . . . . . . . . . . . . . . . .
Natural Language Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
End-to-end Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Further Remarks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Fully Data-Driven Conversation Models and Social Bots
End-to-End Conversation Models . . . . . . . . . . . . . . . . . . . . . . . . . . .
The LSTM Model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The HRED Model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Attention Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Pointer-Network Models . . . . . . . . . . . . . . . . . . . . . . . . . . .
Challenges and Remedies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Response Blandness
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Speaker Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Word Repetitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Further Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Grounded Conversation Models
. . . . . . . . . . . . . . . . . . . . . . . . . . .
Beyond Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Open Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Conversational AI in Industry
Question Answering Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Satori QA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Customer Support Agents
. . . . . . . . . . . . . . . . . . . . . . . . . .
Task-oriented Dialogue Systems (Virtual Assistants)
. . . . . . . . . . . . . . . .
Chatbots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Conclusions and Research Trends
Introduction
Developing an intelligent dialogue system1 that not only emulates human conversation, but also
answers questions on topics ranging from latest news about a movie star to Einstein’s theory of relativity, and fulﬁlls complex tasks such as travel planning, has been one of the longest running goals
in AI. The goal has remained elusive until recently. We are now observing promising results both in
academia sindustry, as large amounts of conversational data become available for training, and the
breakthroughs in deep learning (DL) and reinforcement learning (RL) are applied to conversational
Conversational AI is fundamental to natural user interfaces. It is a rapidly growing ﬁeld, attracting
many researchers in the Natural Language Processing (NLP), Information Retrieval (IR) and Machine Learning (ML) communities. For example, SIGIR 2018 has created a new track of Artiﬁcial
Intelligence, Semantics, and Dialog to bridge research in AI and IR, especially targeting Question
Answering (QA), deep semantics and dialogue with intelligent agents.
Recent years have seen the rise of a small industry of tutorials and survey papers on deep learning
and dialogue systems. Yih et al. ; Gao reviewed deep learning approaches for
a wide range of IR and NLP tasks, including dialogues. Chen et al. presented a tutorial on
dialogues, with a focus on task-oriented agents. Serban et al. surveyed public dialogue
datasets that can be used to develop conversational agents. Chen et al. reviewed popular
deep neural network models for dialogues, focusing on supervised learning approaches. The present
work substantially expands the scope of Chen et al. ; Serban et al. by going beyond
data and supervised learning to provide what we believe is the ﬁrst survey of neural approaches to
conversational AI, targeting NLP and IR audiences.2 Its contributions are:
• We provide a comprehensive survey of the neural approaches to conversational AI that
have been developed in the last few years, covering QA, task-oriented and social bots with
a uniﬁed view of optimal decision making.
• We draw connections between modern neural approaches and traditional approaches, allowing us to better understand why and how the research has evolved and to shed light on
how we can move forward.
• We present state-of-the-art approaches to training dialogue agents using both supervised
and reinforcement learning.
1“Dialogue systems” and “conversational AI” are often used interchangeably in the scientiﬁc literature. The
difference is reﬂective of different traditions. The former term is more general in that a dialogue system might
be purely rule-based rather than AI-based.
2One important topic of conversational AI that we do not cover is Spoken Language Understanding (SLU).
SLU systems are designed to extract the meaning from speech utterances and their application are vast, ranging
from voice search in mobile devices to meeting summarization. The present work does encompass many
Spoken Dialogue Systems – for example Young et al. – but does not focus on components related to
speech. We refer readers to Tur and De Mori for a survey of SLU.
• We sketch out the landscape of conversational systems developed in the research community and released in industry, demonstrating via case studies the progress that has been
made and the challenges that we are still facing.
Who Should Read this Paper?
This paper is based on tutorials given at the SIGIR and ACL conferences in 2018 , with the IR and NLP communities as the primary target audience. However, audiences
with other backgrounds (such as machine learning) will also ﬁnd it an accessible introduction to
conversational AI with numerous pointers, especially to recently developed neural approaches.
We hope that this paper will prove a valuable resource for students, researchers, and software developers. It provides a uniﬁed view, as well as a detailed presentation of the important ideas and
insights needed to understand and create modern dialogue agents that will be instrumental to making
world knowledge and services accessible to millions of users in ways that seem natural and intuitive.
This survey is structured as follows:
• The rest of this chapter introduces dialogue tasks and presents a uniﬁed view in which
open-domain dialogue is formulated as an optimal decision making process.
• Chapter 2 introduces basic mathematical tools and machine learning concepts, and reviews
recent progress in the deep learning and reinforcement learning techniques that are fundamental to developing neural dialogue agents.
• Chapter 3 describes question answering (QA) agents, focusing on neural models for
knowledge-base QA and machine reading comprehension (MRC).
• Chapter 4 describes task-oriented dialogue agents, focusing on applying deep reinforcement learning to dialogue management.
• Chapter 5 describes social chatbots, focusing on fully data-driven neural approaches to
end-to-end generation of conversational responses.
• Chapter 6 gives a brief review of several conversational systems in industry.
• Chapter 7 concludes the paper with a discussion of research trends.
Dialogue: What Kinds of Problems?
Fig. 1.1 shows a human-agent dialogue during the process of making a business decision. The
example illustrates the kinds of problems a dialogue system is expected to solve:
• question answering: the agent needs to provide concise, direct answers to user queries
based on rich knowledge drawn from various data sources including text collections such
as Web documents and pre-compiled knowledge bases such as sales and marketing datasets,
as the example shown in Turns 3 to 5 in Fig. 1.1.
• task completion: the agent needs to accomplish user tasks ranging from restaurant reservation to meeting scheduling (e.g., Turns 6 to 7 in Fig. 1.1), and to business trip planning.
• social chat: the agent needs to converse seamlessly and appropriately with users — like a
human as in the Turing test — and provide useful recommendations (e.g., Turns 1 to 2 in
Fig. 1.1).
One may envision that the above dialogue can be collectively accomplished by a set of agents, also
known as bots, each of which is designed for solving a particular type of task, e.g., QA bots, taskcompletion bots, social chatbots. These bots can be grouped into two categories, task-oriented and
chitchat, depending on whether the dialogue is conducted to assist users to achieve speciﬁc tasks,
e.g., obtain an answer to a query or have a meeting scheduled.
Most of the popular personal assistants in today’s market, such as Amazon Alexa, Apple Siri, Google
Home, and Microsoft Cortana, are task-oriented bots. These can only handle relatively simple tasks,
such as reporting weather and requesting songs. An example of a chitchat dialogue bot is Microsoft
Figure 1.1: A human-agent dialogue during the process of making a business decision. (usr: user,
agt: agent) The dialogue consists of multiple segments of different types. Turns 1 and 2 are a social
chat segment. Turns 3 to 5 are a QA segment. Turns 6 and 7 are a task-completion segment.
Figure 1.2: Two architectures of dialogue systems for (Top) traditional task-oriented dialogue and
(Bottom) fully data-driven dialogue.
XiaoIce. Building a dialogue agent to fulﬁll complex tasks as in Fig. 1.1 remains one of the most
fundamental challenges for the IR and NLP communities, and AI in general.
A typical task-oriented dialogue agent is composed of four modules, as illustrated in Fig. 1.2 (Top):
(1) a Natural Language Understanding (NLU) module for identifying user intents and extracting
associated information; (2) a state tracker for tracking the dialogue state that captures all essential
information in the conversation so far; (3) a dialogue policy that selects the next action based on the
current state; and (4) a Natural Language Generation (NLG) module for converting agent actions to
natural language responses. In recent years there has been a trend towards developing fully datadriven systems by unifying these modules using a deep neural network that maps the user input to
the agent output directly, as shown in Fig. 1.2 (Bottom).
Most task-oriented bots are implemented using a modular system, where the bot often has access
to an external database on which to inquire about information to accomplish the task . Social chatbots, on the other hand, are often implemented using a
unitary (non-modular) system. Since the primary goal of social chatbots is to be AI companions to
humans with an emotional connection rather than completing speciﬁc tasks, they are often developed
to mimic human conversations by training DNN-based response generation models on large amounts
of human-human conversational data . Only recently have researchers begun to explore how to ground the chitchat in
world knowledge and images so as to make
the conversation more contentful and interesting.
A Uniﬁed View: Dialogue as Optimal Decision Making
The example dialogue in Fig. 1.1 can be formulated as a decision making process. It has a natural
hierarchy: a top-level process selects what agent to activate for a particular subtask (e.g., answering
a question, scheduling a meeting, providing a recommendation or just having a casual chat), and
a low-level process, controlled by the selected agent, chooses primitive actions to complete the
Such hierarchical decision making processes can be cast in the mathematical framework of options
over Markov Decision Processes (MDPs) , where options generalize primitive
actions to higher-level actions. In a traditional MDP setting, an agent chooses a primitive action at
each time step. With options, the agent can choose a “multi-step” action which for example could
be a sequence of primitive actions for completing a subtask.
If we view each option as an action, both top- and low-level processes can be naturally captured by
the reinforcement learning framework. The dialogue agent navigates in a MDP, interacting with its
environment over a sequence of discrete steps. At each step, the agent observes the current state, and
chooses an action according to a policy. The agent then receives a reward and observes a new state,
continuing the cycle until the episode terminates. The goal of dialogue learning is to ﬁnd optimal
policies to maximize expected rewards. Table 1.1 formulates an sample of dialogue agents using
this uniﬁed view of RL, where the state-action spaces characterize the complexity of the problems,
and the rewards are the objective functions to be optimized.
The uniﬁed view of hierarchical MDPs has already been applied to guide the development of some
large-scale open-domain dialogue systems. Recent examples include Sounding Board 3, a social
chatbot that won the 2017 Amazon Alexa Prize, and Microsoft XiaoIce 4, arguably the most popular
social chatbot that has attracted more than 660 million users worldwide since its release in 2014.
Both systems use a hierarchical dialogue manager: a master (top-level) that manages the overall
conversation process, and a collection of skills (low-level) that handle different types of conversation
segments (subtasks).
The reward functions in Table 1.1, which seem contradictory in CPS (e.g., we need to minimize
CPS for efﬁcient task completion but maximize CPS for improving user engagement), suggest that
we have to balance the long-term and short-term gains when developing a dialogue system. For
example, XiaoIce is a social chatbot optimized for user engagement, but is also equipped with more
than 230 skills, most of which are QA and task-oriented. XiaoIce is optimized for expected CPS
which corresponds a long-term, rather than a short-term, engagement. Although incorporating many
task-oriented and QA skills can reduce CPS in the short term since these skills help users accomplish
tasks more efﬁciently by minimizing CPS, these new skills establish XiaoIce as an efﬁcient and
trustworthy personal assistant, thus strengthening the emotional bond with human users in the long
Although RL provides a uniﬁed ML framework for building dialogue agents, applying RL requires
training the agents by interacting with real users, which can be expensive in many domains. Hence,
in practice, we often use a hybrid approach that combines the strengths of different ML methods.
For example, we might use imitation and/or supervised learning methods (if there is a large amount
of human-human conversational corpus) to obtain a reasonably good agent before applying RL to
3 
4 
Table 1.1: Reinforcement Learning for Dialogue. CPS stands for Conversation-turns Per Session,
and is deﬁned as the average number of conversation-turns between the bot and the user in a conversational session.
understanding of
user query intent
clariﬁcation
or answers
relevance of answer,
task-oriented
understanding of
dialogue-act and
slot/value
task success rate,
conversation history
and user intent
user engagement,
measured in CPS
top-level bot
understanding of
user top-level intent
user engagement,
measured in CPS
Figure 1.3: Traditional NLP Component Stack. Figure credit: Bird et al. .
continue improving it. In the paper, we will survey these ML approaches and their use for training
dialogue systems.
The Transition of NLP to Neural Approaches
Neural approaches are now transforming the ﬁeld of NLP and IR, where symbolic approaches have
been dominating for decades.
NLP applications differ from other data processing systems in their use of language knowledge of
various levels, including phonology, morphology, syntax, semantics and discourse . Historically, much of the NLP ﬁeld has organized itself around the architecture
of Fig. 1.3, with researchers aligning their work with one component task, such as morphological
analysis or parsing. These tasks can be viewed as resolving (or realizing) natural language ambiguity
(or diversity) at different levels by mapping (or generating) a natural language sentence to (or from)
a series of human-deﬁned, unambiguous, symbolic representations, such as Part-Of-Speech (POS)
tags, context free grammar, ﬁrst-order predicate calculus. With the rise of data-driven and statistical
approaches, these components have remained and have been adapted as a rich source of engineered
features to be fed into a variety of machine learning models .
Neural approaches do not rely on any human-deﬁned symbolic representations but learn in a taskspeciﬁc neural space where task-speciﬁc knowledge is implicitly represented as semantic concepts
using low-dimensional continuous vectors. As Fig. 1.4 illustrates, neural methods in NLP tasks (e.g.,
machine reading comprehension and dialogue) often consist of three steps: (1) encoding symbolic
Figure 1.4: Symbolic and Neural Computation.
user input and knowledge into their neural semantic representations, where semantically related or
similar concepts are represented as vectors that are close to each other; (2) reasoning in the neural
space to generate a system response based on input and system state; and (3) decoding the system
response into a natural language output in a symbolic space. Encoding, reasoning and decoding are
implemented using neural networks of different architectures, all of which may be stacked into a
deep neural network trained in an end-to-end fashion via back propagation.
End-to-end training results in tighter coupling between the end application and the neural network
architecture, lessening the need for traditional NLP component boundaries like morphological analysis and parsing. This drastically ﬂattens the technology stack of Fig. 1.3, and substantially reduces
the need for feature engineering. Instead, the focus has shifted to carefully tailoring the increasingly
complex architecture of neural networks to the end application.
Although neural approaches have already been widely adopted in many AI tasks, including image
processing, speech recognition and machine translation , their impact
on conversational AI has come somewhat more slowly. Only recently have we begun to observe
neural approaches establish state-of-the-art results on an array of conversation benchmarks for both
component tasks and end applications and, in the process, sweep aside the traditional componentbased boundaries that have deﬁned research areas for decades. This symbolic-to-neural shift is
also reshaping the conversational AI landscape by opening up new tasks and user experiences that
were not possible with older techniques. One reason for this is that neural approaches provide a
consistent representation for many modalities, capturing linguistic and non-linguistic ) features in the same modeling framework.
There are also works on hybrid methods that combine the strengths of both neural and symbolic
approaches e.g., . As summarized in Fig. 1.4, neural approaches
can be trained in an end-to-end fashion and are robust to paraphrase alternations, but are weak in
terms of execution efﬁciency and explicit interpretability. Symbolic approaches, on the other hand,
are difﬁcult to train and sensitive to paraphrase alternations, but are more interpretable and efﬁcient
in execution.
Machine Learning Background
This chapter presents a brief review of the deep learning and reinforcement learning technologies
that are most relevant to conversational AI in later chapters.
Machine Learning Basics
Mitchell deﬁnes machine learning broadly to include any computer program that improves
its performance at some task T, measured by P, through experiences E.
Dialogue, as summarized in Table 1.1, is a well-deﬁned learning problem with T, P, and E speciﬁed
as follows:
• T: perform conversations with a user to fulﬁll the user’s goal.
• P: cumulative reward deﬁned in Table 1.1.
• E: a set of dialogues, each of which is a sequence of user-agent interactions.
As a simple example, a single-turn QA dialogue agent might improve its performance as measured
by accuracy or relevance of its generated answers at the QA task, through experiences of humanlabeled question-answer pairs.
A common recipe of building an ML agent using supervised learning (SL) consists of a dataset, a
model, a cost function (a.k.a. loss function) and an optimization procedure.
• The dataset consists of (x, y∗) pairs, where for each input x, there is a ground-truth output
y∗. In QA, x consists of an input question and the documents from which an answer is
generated, and y∗is the desired answer provided by a knowledgeable external supervisor.
• The model is typically of the form y = f(x; θ), where f is a function (e.g., a neural
network) parameterized by θ that maps input x to output y.
• The cost function is of the form L(y∗, f(x; θ)). L(.) is often designed as a smooth function
of error, and is differentiable w.r.t. θ. A commonly used cost function that meets these
criteria is the mean squared error (MSE), deﬁned as
i −f(xi; θ))2 .
• The optimization can be viewed as a search algorithm to identify the best θ that minimize
L(.). Given that L is differentiable, the most widely used optimization procedure for deep
learning is mini-batch Stochastic Gradient Descent (SGD) which updates θ after each batch
i , f(xi; θ)) ,
where N is the batch size and α the learning rate.
Common Supervised Learning Metrics.
Once a model is trained, it can be tested on a hold-out
dataset to have an estimate of its generalization performance. Suppose the model is f(·; θ), and the
hold-out set contains N data points: D = {(x1, y∗
1), (x2, y∗
2), . . . , (xN, y∗
The ﬁrst metric is the aforementioned mean squared error that is appropriate for regression problems
i is considered real-values):
MSE(f) := 1
i −f(xi; θ))2 .
For classiﬁcation problems, y∗
i takes values from a ﬁnite set viewed as categories. For simplicity,
i ∈{+1, −1} here, so that an example (xi, y∗
i ) is called positive (or negative) if y∗
(or −1). The following metrics are often used:
• Accuracy: the fraction of examples for which f predicts correctly:
ACCURACY(f) := 1
1(f(xi; θ) = y∗
where 1(E) is 1 if expression E is true and 0 otherwise.
• Precision: the fraction of correct predictions among examples that are predicted by f to be
PRECISION(f) :=
i=1 1(f(xi; θ) = y∗
i=1 1(f(xi; θ) = +1)
• Recall: the fraction of positive examples that are correctly predicted by f:
RECALL(f) :=
i=1 1(f(xi; θ) = y∗
• F1 Score: the harmonic mean of precision and recall:
F1(f) := 2 × ACCURACY(f) × RECALL(f)
ACCURACY(f) + RECALL(f)
Other metrics are also widely used, especially for complex tasks beyond binary classiﬁcation, such
as the BLEU score .
Reinforcement Learning.
The above SL recipe applies to prediction tasks on a ﬁxed dataset.
However, in interactive problems such as dialogues1, it can be challenging to obtain examples of
desired behaviors that are both correct and representative of all the states in which the agent has
to act. In unexplored territories, the agent has to learn how to act by interacting with an unknown
environment on its own. This learning paradigm is known as reinforcement learning (RL), where
there is a feedback loop between the agent and the external environment. In other words, while SL
learns from previous experiences provided by a knowledgeable external supervisor, RL learns by
experiencing on its own. RL differs from SL in several important respects 
• Exploration-exploitation tradeoff. In RL, the agent needs to collect reward signals from
the environment. This raises the question of which experimentation strategy results in more
effective learning. The agent has to exploit what it already knows in order to obtain high
rewards, while having to explore unknown states and actions in order to make better action
selections in the future.
1As shown in Table 1.1, dialogue learning is formulated as RL where the agent learns a policy π that in each
dialogue turn chooses an appropriate action a from the set A, based on dialogue state s, so as to achieve the
greatest cumulative reward.
• Delayed reward and temporal credit assignment. In RL, training information is not
available in the form of (x, y∗) as in SL. Instead, the environment provides only delayed
rewards as the agent executes a sequence of actions. For example, we do not know whether
a dialogue succeeds in completing a task until the end of the session. The agent, therefore,
has to determine which of the actions in its sequence are to be credited with producing the
eventual reward, a problem known as temporal credit assignment.
• Partially observed states. In many RL problems, the observation perceived from the environment at each step, e.g., user input in each dialogue turn, provides only partial information about the entire state of the environment based on which the agent selects the next
action. Neural approaches learn a deep neural network to represent the state by encoding
all information observed at the current and past steps, e.g., all the previous dialogue turns
and the retrieval results from external databases.
A central challenge in both SL and RL is generalization, the ability to perform well on unseen
inputs. Many learning theories and algorithms have been proposed to address the challenge with
some success by, e.g., seeking a good tradeoff between the amount of available training data and
the model capacity to avoid underﬁtting and overﬁtting. Compared to previous techniques, neural
approaches provide a potentially more effective solution by leveraging the representation learning
power of deep neural networks, as we will review in the next section.
Deep Learning
Deep learning (DL) involves training neural networks, which in their original form consisted of
a single layer (i.e., the perceptron) . The perceptron is incapable of learning
even simple functions such as the logical XOR, so subsequent work explored the use of “deep”
architectures, which added hidden layers between input and output , a form of neural network that is commonly called the multi-layer perceptron (MLP),
or deep neural networks (DNNs). This section introduces some commonly used DNNs for NLP and
IR. Interested readers are referred to Goodfellow et al. for a comprehensive discussion.
Foundations
Consider a text classiﬁcation problem: labeling a text string (e.g., a document or a query) by a domain name such as “sport” and “politics”. As illustrated in Fig. 2.1 (Left), a classical ML algorithm
ﬁrst maps a text string to a vector representation x using a set of hand-engineered features (e.g.,
word and character n-grams, entities, and phrases etc.), then learns a linear classiﬁer with a softmax
layer to compute the distribution of the domain labels y = f(x; W), where W is a matrix learned
from training data using SGD to minimize the misclassiﬁcation error. The design effort is focused
mainly on feature engineering.
Instead of using hand-designed features for x, DL approaches jointly optimize the feature representation and classiﬁcation using a DNN, as exempliﬁed in Fig. 2.1 (Right). We see that the DNN
consists of two halves. The top half can be viewed as a linear classiﬁer, similar to that in the classical ML model in Fig. 2.1 (Left), except that its input vector h is not based on hand-engineered
features but is learned using the bottom half of the DNN, which can be viewed as a feature generator optimized jointly with the classiﬁer in an end-to-end fashion. Unlike classical ML, the effort
of designing a DL classiﬁer is mainly on optimizing DNN architectures for effective representation
For NLP tasks, depending on the type of linguistic structures that we hope to capture in the text, we
may apply different types of neural network (NN) layer structures, such as convolutional layers for
local word dependencies and recurrent layers for global word sequences. These layers can be combined and stacked to form a deep architecture to capture different semantic and context information
at different abstract levels. Several widely used NN layers are described below:
Word Embedding Layers.
In a symbolic space each word is represented as a one-hot vector
whose dimensionality n is the size of a pre-deﬁned vocabulary. The vocabulary is often large; e.g.,
n > 100K. We apply a word embedding model, which is parameterized by a linear projection
matrix We ∈Rn×m, to map each one-hot vector to a m-dimensional real-valued vector (m ≪n)
Figure 2.1: Flowcharts of classic machine learning (Left) and deep learning (Right). A convolutional
neural network is used as an example for deep learning.
in a neural space where the embedding vectors of the words that are more semantically similar are
closer to each other.
Fully Connected Layers.
They perform linear projections as W⊺x.2 We can stack multiple fully
connected layers to form a deep feed-forward NN (FFNN) by introducing a nonlinear activation
function g after each linear projection. If we view a text as a Bag-Of-Words (BOW) and let x be the
sum of the embedding vectors of all words in the text, a deep FFNN can extract highly nonlinear
features to represent hidden semantic topics of the text at different layers, e.g., h(1) = g
at the ﬁrst layer, and h(2) = g
 W(2)⊺h(1)
at the second layer, and so on, where W’s are trainable
Convolutional-Pooling Layers.
An example of convolutional neural networks (CNNs) is shown
in Fig. 2.1 (Right). A convolutional layer forms a local feature vector, denoted ui, of word wi in two
steps. It ﬁrst generates a contextual vector ci by concatenating the word embedding vectors of wi and
its surrounding words deﬁned by a ﬁxed-length window. It then performs a projection to obtain ui =
c ci), where Wc is a trainable matrix and g is an activation function. Then, a pooling layer
combines the outputs ui, i = 1...L into a single global feature vector h. For example, in Fig. 2.1,
the max pooling operation is applied over each “time” i of the sequence of the vectors computed
by the convolutional layer to obtain h, where each element is computed as hj = max1≤i≤L ui,j.
Another popular pooling function is average pooling.
Recurrent Layers.
An example of recurrent neural networks (RNNs) is shown in Fig. 2.2. RNNs
are commonly used for sentence embedding where we view a text as a sequence of words rather
than a BOW. They map the text to a dense and low-dimensional semantic vector by sequentially
and recurrently processing each word, and mapping the subsequence up to the current word into a
low-dimensional vector as hi = RNN(xi, hi−1) := g (W⊺
rhi−1), where xi is the word
embedding of the i-th word in the text, Wih and Wr are trainable matrices, and hi is the semantic
representation of the word sequence up to the i-th word.
2We often omit the bias terms for simplifying notations in this paper.
Figure 2.2: An example of recurrent neural networks.
Figure 2.3: The architecture of DSSM.
Two Examples
This section gives a brief description of two examples of DNN models, designed for the ranking and
text generation tasks, respectively. They are composed of the NN layers described in the last section.
DSSM for Ranking.
In a ranking task, given an input query x, we want to rank all its candidate
answers y ∈Y, based on a similarity scoring function sim(x, y). The task is fundamental to many
IR and NLP applications, such as query-document ranking, answer selection in QA, and dialogue
response selection.
DSSM stands for Deep Structured Semantic Models , or more
generally, Deep Semantic Similarity Model . DSSM is a deep learning model for
measuring the semantic similarity of a pair of inputs (x, y) 3. As illustrated in Fig. 2.3, a DSSM
consists of a pair of DNNs, f1 and f2, which map inputs x and y into corresponding vectors in a
common low-dimensional semantic space. Then the similarity of x and y is measured by the cosine
distance of the two vectors. f1 and f2 can be of different architectures depending on x and y. For
3DSSM can be applied to a wide range of tasks depending on the deﬁnition of (x, y). For example, (x, y)
is a query-document pair for Web search ranking , a document pair
in recommendation , a question-answer pair in QA , a sentence pair of
different languages in machine translation , and an image-text pair in image captioning and so on.
Figure 2.4: The architecture of seq2seq.
example, to compute the similarity of an image-text pair, f1 can be a deep convolutional NN and f2
Let θ be the parameters of f1 and f2. θ is learned to identify the most effective feature representations
of x and y, optimized directly for end tasks. In other words, we learn a hidden semantic space,
parameterized by θ, where the semantics of distance between vectors in the space is deﬁned by the
task or, more speciﬁcally, the training data of the task. For example, in Web document ranking, the
distance measures the query-document relevance, and θ is optimized using a pair-wise rank loss.
Consider a query x and two candidate documents y+ and y−, where y+ is more relevant than y−to
x. Let simθ(x, y) be the similarity of x and y in the semantic space parameterized by θ as
simθ(x, y) = cos(f1(x), f2(y)).
We want to maximize ∆= simθ(x, y+) −simθ(x, y−). We do so by optimizing a smooth loss
L(∆; θ) = log (1 + exp (−γ∆)) ,
where γ is a scaling factor, using SGD of Eqn. 2.1.
Seq2Seq for Text Generation.
In a text generation task, given an input text x, we want to generate
an output text y. This task is fundamental to applications such as machine translation and dialogue
response generation.
Seq2seq stands for the sequence-to-sequence architecture , which is also
known as the encoder-decoder architecture . Seq2Seq is typically implemented
based on sequence models such as RNNs or gated RNNs. Gate RNNs, such as Long-Short Term
Memory (LSTM) and the networks based on Gated Recurrent
Unit (GRU) , are the extensions of RNN in Fig. 2.2, and are often more effective
in capturing long-term dependencies due to the use of gated cells that have paths through time that
have derivatives neither vanishing nor exploding. We will illustrate in detail how LSTM is applied
to end-to-end conversation models in Sec. 5.1.
Seq2seq deﬁnes the probability of generating y conditioned on x as P(y|x) 4. As illustrated in
Fig. 2.4, a seq2seq model consists of (1) an input RNN or encoder f1 that encodes input sequence x
into context vector c, usually as a simple function of its ﬁnal hidden state; and (2) an output RNN or
decoder f2 that generates output sequence y conditioned on c. x and y can be of different lengths.
The two RNNs, parameterized by θ, are trained jointly to minimize the loss function over all the
pairs of (x, y) in training data
log −Pθ(yi|xi) .
Reinforcement Learning
This section reviews reinforcement learning to facilitate discussions in later chapters. For a comprehensive treatment of this topic, interested readers are referred to existing textbooks and reviews,
such as Sutton and Barto ; Kaelbling et al. ; Bertsekas and Tsitsiklis ; Szepesv´ari
 ; Wiering and van Otterlo ; Li .
4Similar to DSSM, seq2seq can be applied to a variety of generation tasks depending on the deﬁnition of
(x, y). For example, (x, y) is a sentence pair of different languages in machine translation , an image-text pairs in image captioning (where f1 is a CNN),
and message-response pairs in dialogue .
Figure 2.5: Interaction between an RL agent and the external environment.
Foundations
Reinforcement learning (RL) is a learning paradigm where an intelligent agent learns to make optimal decisions by interacting with an initially unknown environment .
Compared to supervised learning, a distinctive challenge in RL is to learn without a teacher (that
is, without supervisory labels). As we will see, this will lead to algorithmic considerations that are
often unique to RL.
As illustrated in Fig. 2.5, the agent-environment interaction is often modeled as a discrete-time
Markov decision process, or MDP , described by a ﬁve-tuple M = ⟨S, A, P, R, γ⟩:
• S is a possibly inﬁnite set of states the environment can be in;
• A is a possibly inﬁnite set of actions the agent can take in a state;
• P(s′|s, a) gives the transition probability of the environment landing in a new state s′ after
action a is taken in state s;
• R(s, a) is the average reward immediately received by the agent after taking action a in
state s; and
• γ ∈(0, 1] is a discount factor.
The intersection can be recorded as a trajectory (s1, a1, r1, . . .), generated as follows: at step t =
1, 2, . . .,
• the agent observes the environment’s current state st ∈S, and takes an action at ∈A;
• the environment transitions to a next-state st+1, distributed according to the transition probabilities P(·|st, at);
• associated with the transition is an immediate reward rt ∈R, whose average is R(st, at).
Omitting the subscript, each step results in a tuple (s, a, r, s′) that is called a transition. The goal
of an RL agent is to maximize the long-term reward by taking optimal actions (to be deﬁned soon).
Its action-selection policy, denoted by π, can be deterministic or stochastic. In either case, we use
a ∼π(s) to denote selection of action by following π in state s. Given a policy π, the value of a
state s is the average discounted long-term reward from that state:
V π(s) := E[r1 + γr2 + γ2r3 + · · · |s1 = s, ai ∼π(si), ∀i ≥1] .
We are interested in optimizing the policy so that V π is maximized for all states. Denote by π∗an
optimal policy, and V ∗its corresponding value function (also known as the optimal value function).
In many cases, it is more convenient to use another form of value function called the Q-function:
Qπ(s, a) := E[r1 + γr2 + γ2r3 + · · · |s1 = s, a1 = a, ai ∼π(si), ∀i > 1] ,
which measures the average discounted long-term reward by ﬁrst selecting a in state s and then following policy π thereafter. The optimal Q-function, corresponding to an optimal policy, is denoted
Basic Algorithms
We now describe two popular classes of algorithms, exempliﬁed by Q-learning and policy gradient,
respectively.
Q-learning.
The ﬁrst family is based on the observation that an optimal policy can be immediately
retrieved if the optimal Q-function is available. Speciﬁcally, the optimal policy can be determined
π∗(s) = arg max
Q∗(s, a) .
Therefore, a large family of RL algorithms focuses on learning Q∗(s, a), and are collectively called
value function-based methods.
In practice, it is expensive to represent Q(s, a) by a table, one entry for each distinct (s, a), when
the problem at hand is large. For instance, the number of states in the game of Go is larger than
2 × 10170 . Hence, we often use compact forms to represent Q. In
particular, we assume the Q-function has a predeﬁned parametric form, parameterized by some
vector θ ∈Rd. An example is linear approximation:
Q(s, a; θ) = φ(s, a)Tθ ,
where φ(s, a) is a d-dimensional hand-coded feature vector for state-action pair (s, a), and θ is the
corresponding coefﬁcient vector to be learned from data. In general, Q(s, a; θ) may take different
parametric forms. For example, in the case of Deep Q-Network (DQN), Q(s, a; θ) takes the form of
deep neural networks, such as multi-layer perceptrons and convolutional networks , recurrent network , etc. More
examples will be seen in later chapters. Furthermore, it is possible to represent the Q-function in
a non-parametric way, using decision trees or Gaussian processes , which is outside of the scope of this introductory section.
To learn the Q-function, we modify the parameter θ using the following update rule, after observing
a state transition (s, a, r, s′):
a′ Q(s′, a′; θ) −Q(s, a; θ)
“temporal difference”
∇θQ(s, a; θ) .
The above update is known as Q-learning , which applies a small change to θ,
controlled by the step-size parameter α and computed from the temporal difference .
While popular, in practice, Q-learning can be quite unstable and requires many samples before
reaching a good approximation of Q∗. Two modiﬁcations are often helpful. The ﬁrst is experience
replay , popularized by Mnih et al. . Instead of using an observed transition to
update θ just once using Eqn. 2.4, one may store it in a replay buffer, and periodically sample
transitions from it to perform Q-learning updates. This way, every transition can be used multiple
times, thus increasing sample efﬁciency. Furthermore, it helps stabilize learning by preventing the
data distribution from changing too quickly over time when updating parameter θ.
The second is a two-network implementation , an instance of the more general
ﬁtted value iteration algorithm . Here, the learner maintains an extra
copy of the Q-function, called the target network, parameterized by θtarget. During learning, θtarget
is ﬁxed and is used to compute temporal difference to update θ. Speciﬁcally, Eqn. 2.4 now becomes:
a′ Q(s′, a′; θtarget) −Q(s, a; θ)
temporal difference with a target network
∇θQ(s, a; θ) .
Periodically, θtarget is updated to be θ, and the process continues.
There have been a number of recent improvements to the basic Q-learning described above, such as
dueling Q-network , double Q-learning , and a provably
convergent SBEED algorithm .
Policy Gradient.
The other family of algorithms tries to optimize the policy directly, without
having to learn the Q-function. Here, the policy itself is directly parameterized by θ ∈Rd, and
π(s; θ) is often a distribution over actions. Given any θ, the policy is naturally evaluated by the
average long-term reward it gets in a trajectory of length H, τ = (s1, a1, r1, . . . , sH, aH, rH):5
γt−1rt|at ∼π(st; θ)
If it is possible to estimate the gradient ∇θJ from sampled trajectories, one can do stochastic gradient ascent6 to maximize J:
θ ←θ + α∇θJ(θ) ,
where α is again a stepsize parameter.
One such algorithm, known as REINFORCE , estimates the gradient as follows.
Let τ be a length-H trajectory generated by π(·; θ); that is, at ∼π(st; θ) for every t. Then, a
stochastic gradient based on this single trajectory is given by
∇θ log π(at|st; θ)
REINFORCE may suffer high variance in practice, as its gradient estimate depends directly on the
sum of rewards along the entire trajectory. Its variance may be reduced by the use of an estimated
value function of the current policy, often referred to as the critic in actor-critic algorithms :
∇θ log π(at|st; θ) ˆQ(st, at, h)
where ˆQ(s, a, h) is an estimated value function for the current policy π(s; θ) that is used to approximate PH
h=t γh−trh in Eqn. 2.7.
ˆQ(s, a, h) may be learned by standard temporal difference
methods (similar to Q-learning), but many variants exist. Moreover, there has been much work on
methods to compute the gradient ∇θJ more effectively than Eqn. 2.8. Interested readers can refer to
a few related works and the references therein for further details .
Exploration
So far we have described basic algorithms for updating either the value function or the policy, when
transitions are given as input. Typically, an RL agent also has to determine how to select actions
to collect desired transitions for learning. Always selecting the action (“exploitation”) that seems
best is problematic, as not selecting a novel action (that is, underrepresented, or even absent, in data
collected so far), known as “exploration”, may result in the risk of not seeing outcomes that are
potentially better. Balancing exploration and exploitation efﬁciently is one of the unique challenges
in reinforcement learning.
A basic exploration strategy is known as ϵ-greedy. The idea is to choose the action that looks best
with high probability (for exploitation), and a random action with small probability (for exploration).
In the case of DQN, suppose θ is the current parameter of the Q-function, then the action-selection
rule for state s is given as follows:
arg maxa Q(st, a; θ)
with probability 1 −ϵ
random action
with probability ϵ .
In many problems this simple approach is effective (although not necessarily optimal). A further
discussion is found in Sec. 4.4.2.
5We describe policy gradient in the simpler bounded-length trajectory case, although it can be extended to
problems when the trajectory length is unbounded .
6Stochastic gradient ascent is simply stochastic gradient descent on the negated objective function.
Question Answering and Machine
Reading Comprehension
Recent years have witnessed an increasing demand for conversational Question Answering (QA)
agents that allow users to query a large-scale Knowledge Base (KB) or a document collection in
natural language. The former is known as KB-QA agents and the latter text-QA agents. KB-QA
agents are more ﬂexible and user-friendly than traditional SQL-like systems in that users can query
a KB interactively without composing complicated SQL-like queries. Text-QA agents are much
easier to use in mobile devices than traditional search engines, such as Bing and Google, in that they
provide concise, direct answers to user queries, as opposed to a ranked list of relevant documents.
It is worth noting that multi-turn, conversational QA is an emerging research topic, and is not as
well-studied as single-turn QA. Many papers reviewed in this chapter are focused on the latter.
However, single-turn QA is an indispensable building block for all sorts of dialogues (e.g., chitchat
and task-oriented), deserving our full attention if we are to develop real-world dialogue systems.
In this chapter, we start with a review of KB and symbolic approaches to KB-QA based on semantic
parsing. We show that a symbolic system is hard to scale because the keyword-matching-based,
query-to-answer inference used by the system is inefﬁcient for a very large KB, and is not robust
to paraphrasing. To address these issues, neural approaches are developed to represent queries and
KB using continuous semantic vectors so that the inference can be performed at the semantic level
in a compact neural space. We also describe the typical architecture of multi-turn, conversational
KB-QA agents, using a movie-on-demand agent as an example, and review several conversational
KB-QA datasets developed recently.
We then discuss neural text-QA agents. The heart of these systems is a neural Machine Reading
Comprehension (MRC) model that generates an answer to an input question based on a (set of) passage(s). After reviewing popular MRC datasets and TREC text-QA open benchmarks, we describe
the technologies developed for state-of-the-art MRC models along two dimensions: (1) the methods
of encoding questions and passages as vectors in a neural space, and (2) the methods of performing
reasoning in the neural space to generate the answer. We also describe the architecture of multi-turn,
conversational text-QA agents, and the way MRC tasks and models are extended to conversational
Knowledge Base
Organizing the world’s facts and storing them in a structured database, large scale Knowledge Bases
(KB) like DBPedia , Freebase and Yago have become important resources for supporting open-domain QA.
A typical KB consists of a collection of subject-predicate-object triples (s, r, t) where s, t ∈E are
entities and r ∈R is a predicate or relation. A KB in this form is often called a Knowledge Graph
Figure 3.1: An example of semantic parsing for KB-QA. (Left) A subgraph of Freebase related to
the TV show Family Guy. (Right) A question, its logical form in λ-calculus and query graph, and
the answer. Figures adapted from Yih et al. .
(KG) due to its graphical representation, i.e., the entities are nodes and the relations the directed
edges that link the nodes.
Fig. 3.1 (Left) shows a small subgraph of Freebase related to the TV show Family Guy. Nodes
include some names, dates and special Compound Value Type (CVT) entities.1 A directed edge
describes the relation between two entities, labeled by a predicate.
Semantic Parsing for KB-QA
Most state-of-the-art symbolic approaches to KB-QA are based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a KB
query. The answers to the question can then be obtained by ﬁnding a set of paths in the KB that
match the query and retrieving the end nodes of these paths .
We take the example used in Yih et al. to illustrate the QA process. Fig. 3.1 (Right) shows
the logical form in λ-calculus and its equivalent graph representation, known as query graph, of
the question “Who ﬁrst voiced Meg on Family Guy?”. Note that the query graph is grounded in
Freebase. The two entities, MegGriffin and FamilyGuy, are represented by two rounded rectangle
nodes. The circle node y means that there should exist an entity describing some casting relations
like the character, actor and the time she started the role. y is grounded in a CVT entity in this case.
The shaded circle node x is also called the answer node, and is used to map entities retrieved by
the query. The diamond node arg min constrains that the answer needs to be the earliest actor for
this role. Running the query graph without the aggregation function against the graph as in Fig. 3.1
(Left) will match both LaceyChabert and MilaKunis. But only LaceyChabert is the correct
answer as she started this role earlier (by checking the from property of the grounded CVT node).
Applying a symbolic KB-QA system to a very large KB is challenging for two reasons:
• Paraphrasing in natural language: This leads to a wide variety of semantically equivalent
ways of stating the same question, and in the KB-QA setting, this may cause mismatches
between the natural language questions and the label names (e.g., predicates) of the nodes
and edges used in the KB. As in the example of Fig. 3.1, we need to measure how likely
the predicate used in the question matches that in Freebase, such as “Who ﬁrst voiced Meg
on Family Guy?” vs. cast-actor. Yih et al. proposed to use a learned DSSM
1CVT is not a real-world entity, but is used to collect multiple ﬁelds of an event or a special relationship.
described in Sec. 2.2.2, which is conceptually an embedding-based method we will review
in Sec. 3.3.
• Search complexity: Searching all possible multi-step (compositional) relation paths that
match complex queries is prohibitively expensive because the number of candidate paths
grows exponentially with the path length. We will review symbolic and neural approaches
to multi-step reasoning in Sec. 3.4.
Embedding-based Methods
To address the paraphrasing problem, embedding-based methods map entities and relations in a KB
to continuous vectors in a neural space; see, e.g., Bordes et al. ; Socher et al. ; Yang
et al. ; Yih et al. . This space can be viewed as a hidden semantic space where various
expressions with the same semantic meaning map to the same continuous vector.
Most KB embedding models are developed for the Knowledge Base Completion (KBC) task: predicting the existence of a triple (s, r, t) that is not seen in the KB. This is a simpler task than KB-QA
since it only needs to predict whether a fact is true or not, and thus does not suffer from the search
complexity problem.
The bilinear model is one of the basic KB embedding models . It
learns a vector xe ∈Rd for each entity e ∈E and a matrix Wr ∈Rd×d for each relation r ∈R.
The model scores how likely a triple (s, r, t) holds using
score(s, r, t; θ) = x⊤
The model parameters θ (i.e., the embedding vectors and matrices) are trained on pair-wise training
samples in a similar way to that of the DSSM described in Sec. 2.2.2. For each positive triple
(s, r, t) in the KB, denoted by x+, we construct a set of negative triples x−by corrupting s, t, or
r. The training objective is to minimize the pair-wise rank loss of Eqn. 2.2, or more commonly the
margin-based loss deﬁned as
γ + score(x−; θ) −score(x+; θ)
where [x]+ := max(0, x), γ is the margin hyperparameter, and D the training set of triples.
These basic KB models have been extended to answer multi-step relation queries, as known as path
queries, e.g., “Where did Tad Lincoln’s parents live?” . A path query consists of an initial anchor entity s (e.g., TadLincoln),
followed by a sequence of relations to be traversed (r1, ..., rk) (e.g., (parents, location)). We
can use vector space compositions to combine the embeddings of individual relations ri into an
embedding of the path (r1, ..., rk). The natural composition of the bilinear model of Eqn. 3.1 is
matrix multiplication. Thus, to answer how likely a path query (q, t) holds, where q = (s, r1, ..., rk),
we would compute
score(q, t) = x⊤
s Wr1...Wrkxt.
These KB embedding methods are shown to have good generalization performance in terms of
validating unseen facts (e.g., triples and path queries) given an existing KB. Interested users are
referred to Nguyen for a detailed survey of embedding models for KBC.
Multi-Step Reasoning on KB
Knowledge Base Reasoning (KBR) is a subtask of KB-QA. As described in Sec. 3.2, KB-QA is
performed in two steps: (1) semantic parsing translates a question into a KB query, then (2) KBR
traverses the query-matched paths in a KB to ﬁnd the answers.
To reason over a KB, for each relation r ∈R, we are interested in learning a set of ﬁrst-order logical
rules in the form of relational paths, π = (r1, ..., rk). For the KBR example in Fig. 3.2, given
the question “What is the citizenship of Obama?”, its translated KB query in the form of subjectpredicate-object triple is (Obama, citizenship, ?). Unless the triple (Obama, citizenship,
Figure 3.2: An example of knowledge base reasoning (KBR). We want to identify the answer node
USA for a KB query (Obama, citizenship, ?). Figure adapted from Shen et al. .
Table 3.1: A sample of relational paths learned by PRA. For each relation, its top-2 PRA paths are
presented, adapted from Lao et al. .
PRA Path # Comment
athlete-plays-for-team
(athlete-plays-in-league, league-players,
athlete-plays-for-team)
# teams with many players in the athlete’s league
(athlete-plays-in-league, league-teams, team-against-team)
# teams that play against many teams in the athlete’s league
stadium-located-in-city
(stadium-home-team,team-home-stadium,stadium-located-in-city)
# city of the stadium with the same team
(latitude-longitude,latitude-longitude-of,
stadium-located-in-city)
# city of the stadium with the same location
team-home-stadium
(team-plays-in-city,city-stadium)
# stadium located in the same city with the query team
(team-member,athlete-plays-for-team,team-home-stadium)
# home stadium of teams which share players with the query
team-plays-in-league
(team-plays-sport,players,athlete-players-in-league)
# the league that the query team’s members belong to
(team-plays-against-team,team-players-in-league)
# the league that query team’s competing team belongs to
USA) is explicitly stored in the KB,2 a multi-step reasoning procedure is needed to deduce the answer
from the paths that contain relevant triples, such as (Obama, born-in, Hawaii) and (Hawaii,
part-of, USA), using the learned relational paths such as (born-in, part-of).
Below, we describe three categories of multi-step KBR methods. They differ in whether reasoning
is performed in a discrete symbolic space or a continuous neural space.
Symbolic Methods
The Path Ranking Algorithm (PRA) is one of the primary
symbolic approaches to learning relational paths in large KBs. PRA uses random walks with restarts
to perform multiple bounded depth-ﬁrst search to ﬁnd relational paths. Table 3.1 shows a sample of
2As pointed out by Nguyen , even very large KBs, such as Freebase and DBpedia, which contain
billions of fact triples about the world, are still far from complete.
Figure 3.3: An overview of the neural methods for KBR .
The KB is embedded in neural space as matrix M that is learned to store compactly the connections
between related triples (e.g., the relations that are semantically similar are stored as a cluster). The
controller is designed to adaptively produce lookup sequences in M and decide when to stop, and
the encoder and decoder are responsible for the mapping between the symbolic and neural spaces.
relational paths learned by PRA. A relational path is a sequence π = (r1, ..., rk). An instance of the
relational path is a sequence of nodes e1, ..., ek+1 such that (ei, ri, ei+1) is a valid triple.
During KBR, given a query q = (s, r, ?), PRA selects the set of relational paths for r, denoted by
Br = {π1, π2, ...}, then traverses the KB according to the query and Br, and scores each candidate
answer t using a linear model
score(q, t) =
λπP(t|s, π) ,
where λπ’s are the learned weights, and P(t|s, π) is the probability of reaching t from s by a random
walk that instantiates the relational path π, also known as a path constrained random walk.
Because PRA operates in a fully discrete space, it does not take into account semantic similarities
among relations. As a result, PRA can easily produce millions of categorically distinct paths even for
a small path length, which not only hurts generalization but makes reasoning prohibitively expensive.
To reduce the number of relational paths that need to be considered in KBR, Lao et al. used
heuristics (e.g., requiring that a path be included in PRA only if it retrieves at least one target entity
in the training data) and added an L1 regularization term in the loss function for training the linear
model of Eqn. 3.3. Gardner et al. proposed a modiﬁcation to PRA that leverages the KB
embedding methods, as described in Sec. 3.3, to collapse and cluster PRA paths according to their
relation embeddings.
Neural Methods
Implicit ReasoNet (IRN) and Neural Logic Programming (Neural LP)
 are proposed to perform multi-step KBR in a neural space and achieve stateof-the-art results on popular benchmarks. The overall architecture of these methods is shown in
Fig. 3.3, which can be viewed as an instance of the neural approaches illustrated in Fig. 1.4 (Right).
In what follows, we use IRN as an example to illustrate how these neural methods work. IRN
consists of four modules: encoder, decoder, shared memory, and controller, as in Fig. 3.3.
Encoder and Decoder
These two modules are task-dependent. Given an input query (s, r, ?),
the encoder maps s and r, respectively, into their embedding vectors and then concatenates the
two vectors to form the initial hidden state vector s0 of the controller. The use of vectors rather than
matrices for relation representations is inspired by the bilinear-diag model , which
restricts the relation representations to the class of diagonal matrices.
The decoder outputs a prediction vector o = tanh(W⊤
o st + bo), a nonlinear projection of state s
at time t, where Wo and bo are the weight matrix and bias vector, respectively. In KBR, we can
map the answer vector o to its answer node (entity) o in the symbolic space based on L1 distance as
o = arg mine∈E ∥o −xe∥1, where xe is the embedding vector of entity e.
Shared Memory
The shared memory M is differentiable, and consists of a list of vectors
{mi}1≤i≤|M| that are randomly initialized and updated through back-propagation in training. M
stores a compact version of KB optimized for the KBR task. That is, each vector represents a
concept (a cluster of relations or entities) and the distance between vectors represents the semantic
relatedness of these concepts. For example, the system may fail to answer the question (Obama,
citizenship, ?) even if it ﬁnds the relevant facts in M, such as (Obama, born-in, Hawaii)
and (Hawaii, part-of, USA), because it does not know that bore-in and citizenship are semantically related relations. In order to correct the error, M needs to be updated using the gradient
to encode the piece of new information by moving the two relation vectors closer to each other in
the neural space.
Controller
The controller is implemented as an RNN. Given initial state s0, it uses attention to
iteratively lookup and fetch information from M to update the state st at time t according to Eqn. 3.4,
until it decides to terminate the reasoning process and calls the decoder to generate the output.
st+1 = g(W⊤
where W’s are learned projection matrices, λ a scaling factor and g a nonlinear activation function.
The reasoning process of IRN can be viewed as a Markov Decision Process (MDP), as illustrated in
Sec. 2.3.1. The step size in the information lookup and fetching sequence of Eqn. 3.4 is not given
by training data, but is decided by the controller on the ﬂy. More complex queries need more steps.
Thus, IRN learns a stochastic policy to get a distribution over termination and prediction actions
by the REINFORCE algorithm , which is described in Sec. 2.3.2 and Eqn. 2.7.
Since all the modules of IRN are differentiable, IRN is an end-to-end differentiable neural model
whose parameters, including the embedded KB matrix M, can be jointly optimized using SGD on
the training samples derived from a KB, as shown in Fig. 3.3.
As outlined in Fig. 1.4, neural methods operate in a continuous neural space, and do not suffer from
the problems associated with symbolic methods. They are robust to paraphrase alternations because
knowledge is implicitly represented by semantic classes via continuous vectors and matrices. They
also do not suffer from the search complexity issue even with complex queries (e.g.path queries)
and a very large KB because they reason over a compact representation of a KB (e.g., the matrix M
in the shared memory in IRN) rather than the KB itself.
One of the major limitations of these methods is the lack of interpretability. Unlike PRA which
traverses the paths in the graph explicitly as Eqn. 3.3, IRN does not follow explicitly any path in
the KB during reasoning but performs lookup operations over the shared memory iteratively using
the RNN controller with attention, each time using the revised internal state s as a query for lookup.
It remains challenging to recover the symbolic representations of queries and paths (or ﬁrst-order
logical rules) from the neural controller. See for some
interesting preliminary results of interpretation of neural methods.
Reinforcement Learning based Methods
DeepPath , MINERVA and M-Walk are
among the state-of-the-art methods that use RL for multi-step reasoning over a KB. They use a
policy-based agent with continuous states based on KB embeddings to traverse the knowledge graph
to identify the answer node (entity) for an input query. The RL-based methods are as robust as the
neural methods due to the use of continuous vectors for state representation, and are as interpretable
as symbolic methods because the agents explicitly traverse the paths in the graph.
We formulate KBR as an MDP deﬁned by the tuple (S, A, R, P), where S is the continuous state
space, A the set of available actions, P the state transition probability matrix, and R the reward
function. Below, we follow M-Walk to describe these components in detail. We denote a KB as
graph G(E, R) which consists a collection of entity nodes E and the relation edges R that link the
nodes. We denote a KB query as q = (e0, r, ?), where e0 and r are the given source node and
relation, respectively, and ? the answer node to be identiﬁed.
Let st denote the state at time t, which encodes information of all traversed nodes up to t,
all the previous selected actions and the initial query q. st can be deﬁned recursively as follows:
s0 := {q, Re0, Ee0},
st = st−1 ∪{at−1, et, Ret, Eet},
where at ∈A is the action selected by the agent at time t, et is the currently visited node, Ret ∈R
is the set of all the edges connected to et, and Eet ∈E is the set of all the nodes connected to et. Note
that in RL-based methods, st is represented as a continuous vector using e.g., a RNN in M-Walk
and MINERVA or a MLP in DeepPath.
Based on st, the agent selects one of the following actions: (1) choosing an edge in Eet
and moving to the next node et+1 ∈E, or (2) terminating the reasoning process and outputting the
current node et as a prediction of the answer node eT .
Transitions
The transitions are deterministic. As shown in Fig. 3.2, once action at is selected, the
next node et+1 and its associated Eet+1 and Ret+1 are known.
We only have the terminal reward of +1 if eT is the correct answer, and 0 otherwise.
Policy Network
The policy πθ(a|s) denotes the probability of selecting action a given state s,
and is implemented as a neural network parameterized by θ. The policy network is optimized to
maximize E[Vθ(s0)], which is the long-term reward of starting from s0 and following the policy πθ
afterwards. In KBR, the policy network can be trained using RL, such as the REINFORCE method,
from the training samples in the form of triples (es, r, et) extracted from a KB. To address the reward
sparsity issue (i.e., the reward is only available at the end of a path), Shen et al. proposed
to use Monte Carlo Tree Search to generate a set of simulated paths with more positive terminal
rewards by exploiting the fact that all the transitions are deterministic for a given knowledge graph.
Conversational KB-QA Agents
All of the KB-QA methods we have described so far are based on single-turn agents which assume
that users can compose in one shot a complicated, compositional natural language query that can
uniquely identify the answer in the KB.
However, in many cases, it is unreasonable to assume that users can construct compositional queries
without prior knowledge of the structure of the KB to be queried. Thus, conversational KB-QA
agents are more desirable because they allow users to query a KB interactively without composing
complicated queries.
A conversational KB-QA agent is useful for many interactive KB-QA tasks such as movie-ondemand, where a user attempts to ﬁnd a movie based on certain attributes of that movie, as illustrated
by the example in Fig. 3.4, where the movie DB can be viewed as an entity-centric KB consisting
of entity-attribute-value triples.
In addition to the core KB-QA engine which typically consists of a semantic parser and a KBR
engine, a conversational KB-QA agent is also equipped with a Dialogue Manager (DM) which
tracks the dialogue state and decides what question to ask to effectively help users navigate the KB
in search of an entity (movie). The high-level architecture of the conversational agent for movieon-demand is illustrated in Fig. 3.5. At each turn, the agent receives a natural language utterance ut
as input, and selects an action at ∈A as output. The action space A consists of a set of questions,
each for requesting the value of an attribute, and an action of informing the user with an ordered
list of retrieved entities. The agent is a typical task-oriented dialogue system of Fig. 1.2 (Top),
consisting of (1) a belief tracker module for resolving coreferences and ellipsis in user utterances
using conversation context, identifying user intents, extracting associated attributes, and tracking the
dialogue state; (2) an interface with the KB to query for relevant results .
Figure 3.5: An overview of a conversational KB-QA agent. Figure credit: Dhingra et al. .
except that we need to form the query based on the dialogue history captured by the belief tracker,
not just the current user utterance, as described in Suhr et al. ); (3) a beliefs summary module
to summarize the state into a vector; and (4) a dialogue policy which selects the next action based on
the dialogue state. The policy can be either programmed or trained on dialogues
 .
Wu et al. presented an Entropy Minimization Dialogue Management (EMDM) strategy. The
agent always asks for the value of the attribute with maximum entropy over the remaining entries
in the KB. EMDM is proved optimal in the absence of language understanding errors. However, it
does not take into account the fact that some questions are easy for users to answer, whereas others
are not. For example, in the movie-on-demand task, the agent could ask users to provide the movie
release ID which is unique to each movie but is often unknown to regular users.
Dhingra et al. proposed KB-InfoBot – a fully neural end-to-end multi-turn dialogue agent for
the movie-on-demand task. The agent is trained entirely from user feedback. It does not suffer from
the problem of EMDM, and always asks users easy-to-answer questions to help search in the KB.
Like all KB-QA agents, KB-InfoBot needs to interact with an external KB to retrieve real-world
knowledge. This is traditionally achieved by issuing a symbolic query to the KB to retrieve entries
based on their attributes. However, such symbolic operations break the differentiability of the system
and prevent end-to-end training of the dialogue agent. KB-InfoBot addresses this limitation by
replacing symbolic queries with an induced posterior distribution over the KB that indicates which
entries the user is interested in. The induction can be achieved using the neural KB-QA methods
described in the previous sections. Experiments show that integrating the induction process with RL
leads to higher task success rate and reward in both simulations and against real users 3.
Recently, several datasets have been developed for building conversational KB-QA agents. Iyyer
et al. collected a Sequential Question Answering (SQA) dataset via crowd sourcing by leveraging WikiTableQuestions ), which contains highly compositional
questions associated with HTML tables from Wikipedia. As shown in the example in Fig. 3.6 (Left),
3It remains to be veriﬁed whether the method can deal with large-scale KBs with millions of entities.
Figure 3.6: The examples from two conversational KB-QA datasets. (Left) An example question
sequence created from a compositional question intent in the SQA dataset. Figure credit: Iyyer et al.
 . (Right) An example dialogue from the CSQA dataset. Figure credit: Saha et al. .
each crowd sourcing task contains a long, complex question originally from WTQ as the question
intent. The workers are asked to compose a sequence of simpler but inter-related questions that lead
to the ﬁnal intent. The answers to the simple questions are subsets of the cells in the table.
Saha et al. presented a dataset consisting of 200K QA dialogues for the task of Complex Sequence Question Answering (CSQA). CSQA combines two sub-tasks: (1) answering factoid questions through complex reasoning over a large-scale KB, and (2) learning to converse through a
sequence of coherent QA pairs. As the example in Fig. 3.6 (Right) shows, CSQA calls for a conversational KB-QA agent that combines many technologies described in this chapter, including (1)
parsing complex natural language queries (Sec. 3.2), (2) using conversation context to resolve coreferences and ellipsis in user utterances like the belief tracker in Fig. 3.5, (3) asking for clariﬁcation
questions for ambiguous queries, like the dialogue manager in Fig. 3.5, and (4) retrieving relevant
paths in the KB to answer questions (Sec. 3.4).
Machine Reading for Text-QA
Machine Reading Comprehension (MRC) is a challenging task: the goal is to have machines read a
(set of) text passage(s) and then answer any question about the passage(s). The MRC model is the
core component of text-QA agents.
The recent big progress on MRC is largely due to the availability of a multitude of large-scale
datasets that the research community has created over various text sources such as Wikipedia
 , SQuAD , WikiHop , DRCD ), news and other articles ,
NewsQA , RACE , ReCoRD ), ﬁctional
stories , CBT , NarrativeQA ), science questions ), and general Web documents , TriviaQA , SearchQA , DuReader ).
This is the MRC dataset released by the Stanford NLP group. It consists of 100K questions posed by crowdworkers on a set of Wikipedia articles. As shown in the example in Fig. 3.7
(Left), the MRC task deﬁned on SQuAD involves a question and a passage, and aims to ﬁnd an
answer span in the passage. For example, in order to answer the question “what causes precipitation to fall?”, one might ﬁrst locate the relevant part of the passage “precipitation ... falls under
gravity”, then reason that “under” refers to a cause (not location), and thus determine the correct
answer: “gravity”. Although the questions with span-based answers are more constrained than the
real-world questions users submit to Web search engines such as Google and Bing, SQuAD provides
a rich diversity of question and answer types and became one of the most widely used MRC datasets
in the research community.
Figure 3.7: The examples from two MRC datasets. (Left) Question-answer pairs for a sample
passage in the SQuAD dataset, adapted from Rajpurkar et al. . Each of the answers is a text
span in the passage. (Right) A question-answer pair for a set of passages in the MS MARCO dataset,
adapted from Nguyen et al. . The answer, if there is one, is human generated.
This is a large scale real-world MRC dataset, released by Microsoft, aiming to address the limitations of other academic datasets. For example, MS MARCO differs from SQuAD
in that (1) SQuAD consists of the questions posed by crowdworkers while MS MARCO is sampled
from the real user queries; (2) SQuAD uses a small set of high quality Wikipedia articles while
MS MARCO is sampled from a large amount of Web documents, (3) MS MARCO includes some
unanswerable queries4 and (4) SQuAD requires identifying an answer span in a passage while MS
MARCO requires generating an answer (if there is one) from multiple passages that may or may not
be relevant to the given question. As a result, MS MARCO is far more challenging, and requires
more sophisticated reading comprehension skills. As shown in the example in Fig. 3.7 (Right),
given the question “will I qualify for OSAP if I’m new in Canada”, one might ﬁrst locate the relevant passage that includes: “you must be a 1 Canadian citizen; 2 permanent resident; or 3 protected
person...” and reason that being new to the country is usually the opposite of being a citizen, permanent resident etc., thus determine the correct answer: “no, you won’t qualify”.
In addition, TREC5 also provides a series of text-QA benchmarks:
The automated QA track.
This is one of the most popular tracks in TREC for many years, up
to year 2007 . It has focused on the task of providing
automatic answers for human questions. The track primarily dealt with factual questions, and the
answers provided by participants were extracted from a corpus of News articles. While the task
evolved to model increasingly realistic information needs, addressing question series, list questions,
and even interactive feedback, a major limitation remained: the questions did not directly come from
real users, in real time.
The LiveQA track.
This track started in 2015 , focusing on answering user
questions in real time. Real user questions, i.e., fresh questions submitted on the Yahoo Answers
(YA) site that have not yet been answered, were sent to the participant systems, which provided
an answer in real time. Returned answers were judged by TREC editors on a 4-level Likert scale.
LiveQA revived this popular QA track which has been frozen for several years, attracting signiﬁcant
attention from the QA research community.
Figure 3.8: Two examples of state of the art neural MRC models. (Left) The Stochastic Answer Net
(SAN) model. Figure credit: Liu et al. . (Right) The BiDirectional Attention Flow (BiDAF)
model. Figure credit: Seo et al. .
Neural MRC Models
The description in this section is based on the state of the art models developed on SQuAD, where
given a question Q = (q1, ..., qI) and a passage P = (p1, ..., pJ), we need to locate an answer span
A = (astart, aend) in P.
In spite of the variety of model structures and attention types , a typical neural MRC model performs reading comprehension in three steps, as outlined in Fig. 1.4: (1) encoding the symbolic representation
of the questions and passages into a set of vectors in a neural space; (2) reasoning in the neural
space to identify the answer vector (e.g., in SQuAD, this is equivalent to ranking and re-ranking the
embedded vectors of all possible text spans in P); and (3) decoding the answer vector into a natural
language output in the symbolic space (e.g., this is equivalent to mapping the answer vector to its
text span in P). Since the decoding module is straightforward for SQuAD models, we will focus on
encoding and reasoning below.
Fig. 3.8 illustrate two examples of neural MRC models. BiDAF is among the most
widely used state of the art MRC baseline models in the research community, and SAN is the best documented MRC model on the SQuAD1.1 leaderboard6 as of Dec. 19, 2017.
Most MRC models encode questions and passages through three layers: a lexicon embedding layer,
a contextual embedding layer, and an attention layer, as reviewed below.
Lexicon Embedding Layer.
This extracts information from Q and P at the word level and normalizes for lexical variants. It typically maps each word to a vector space using a pre-trained word
embedding model, such as word2vec or GloVe , such
that semantically similar words are mapped to the vectors that are close to each other in the neural
space (also see Sec. 2.2.1). Word embedding can be enhanced by concatenating each word embedding vector with other linguistic embeddings such as those derived from characters, Part-Of-Speech
(POS) tags, and named entities etc. Given Q and P, the word embeddings for the tokens in Q is a
matrix Eq ∈Rd×I and tokens in P is Ep ∈Rd×J , where d is the dimension of word embeddings.
4SQuAD v2 also includes unanswerable queries.
5 
6 
Contextual Embedding Layer.
This utilizes contextual cues from surrounding words to reﬁne
the embedding of the words. As a result, the same word might map to different vectors in a neural
space depending on its context, such as “bank of a river” vs. “ bank of America”. This is typically
achieved by using a Bi-directional Long Short-Term Memory (BiLSTM) network,7 an extension of
RNN of Fig. 2.2. As shown in Fig. 3.8, we place two LSTMs in both directions, respectively, and
concatenate the outputs of the two LSTMs. Hence, we obtain a matrix Hq ∈R2d×I as a contextually
aware representation of Q and a matrix Hp ∈R2d×J as a contextually aware representation of P.
ELMo is one of the state of the art contextual embedding models. It is based on
deep BiLSTM. Instead of using only the output layer representations of BiLSTM, ELMo combines
the intermediate layer representations in the BiLSTM, where the combination weights are optimized
on task-speciﬁc training data.
BERT differs from ELMo and BiLSTM in that it is designed to pre-train deep
bidirection representations by jointly conditioning on both left and right context in all layers. The
pre-trained BERT representations can be ﬁne-tuned with just one additional output layer to create
state of the art models for a wide range of NLP tasks, including MRC.
Since an RNN/LSTM is hard to train efﬁciently using parallel computing, Yu et al. presents a
new contextual embedding model which does not require an RNN: Its encoder consists exclusively
of convolution and self-attention, where convolution models local interactions and self-attention
models global interactions. Such a model can be trained an order of magnitude faster than an RNNbased model on GPU clusters.
Attention Layer.
This couples the question and passage vectors and produces a set of queryaware feature vectors for each word in the passage, and generates the working memory M over
which reasoning is performed. This is achieved by summarizing information from both Hq and Hp
via the attention process8 that consists of the following steps:
1. Compute an attention score, which signiﬁes which query words are most relevant to each
passage word: sij = simθs(hq
j) ∈R for each hq
i in Hq, where simθs is the similarity
function e.g., a bilinear model, parameterized by θs.
2. Compute
normalized
exp(sij)/ P
k exp(skj).
3. Summarize information for each passage word via ˆhp
i . Thus, we obtain a
matrix ˆHp ∈R2d×J as the question-aware representation of P.
Next, we form the working memory M in the neural space as M = fθ( ˆHp, Hp), where fθ is a
function of fusing its input matrices, parameterized by θ. fθ can be an arbitrary trainable neural
network. For example, the fusion function in SAN includes a concatenation layer, a self-attention
layer and a BiLSTM layer. BiDAF computes attentions in two directions: from passage to question
ˆHq as well as from question to passage ˆHp. The fusion function in BiDAF includes a layer that
concatenates three matrices Hp, ˆHp and ˆHq, and a two-layer BiLSTM to encode for each word its
contextual information with respect to the entire passage and the query.
MRC models can be grouped into different categories based on how they perform reasoning to
generate the answer. Here, we distinguish single-step models from multi-step models.
7Long Short-Term Memory (LSTM) networks are an extension for recurrent neural networks (RNNs). The
units of an LSTM are used as building units for the layers of a RNN. LSTMs enable RNNs to remember their
inputs over a long period of time because LSTMs contain their information in a gated cell, where gated means
that the cell decides whether to store or delete information based on the importance it assigns to the information.
The use of BiLSTM for contextual embedding is suggested by Melamud et al. ; McCann et al. .
8Interested readers may refer to Table 1 in Huang et al. for a summarized view on the attention
process used in several state of the art MRC models.
Figure 3.9: (Top) A human reader can easily answer the question by reading the passage only once.
(Bottom) A human reader may have to read the passage multiple times to answer the question.
Single-Step Reasoning.
A single-step reasoning model matches the question and document only
once and produces the ﬁnal answers. We use the single-step version of SAN9 in Fig. 3.8 (Left) as an
example to describe the reasoning process. We need to ﬁnd the answer span (i.e., the start and end
points) over the working memory M. First, a summarized question vector is formed as
where βi = exp(w⊤hq
k exp(w⊤hq
k), and w is a trainable vector. Then, a bilinear function is
used to obtain the probability distribution of the start index over the entire passage by
p(start) = softmax(hq⊤W(start)M),
where W(start) is a weight matrix. Another bilinear function is used to obtain the probability
distribution of the end index, incorporating the information of the span start obtained by Eqn. 3.7,
p(end) = softmax([hq;
mj]⊤W(end)M),
where the semicolon mark ; indicates the vector or matrix concatenation operator, p(start)
probability of the j-th word in the passage being the start of the answer span, W(end) is a weight
matrix, and mj is the j-th vector of M.
Single-step reasoning is simple yet efﬁcient and the model parameters can be trained using the
classical back-propagation algorithm, thus it is adopted by most of the systems . However, since humans often solve question answering tasks by re-reading and
re-digesting the document multiple times before reaching the ﬁnal answer (this may be based on the
complexity of the questions and documents, as illustrated by the examples in Fig. 3.9), it is natural
to devise an iterative way to ﬁnd answers as multi-step reasoning.
Multi-Step Reasoning.
Multi-step reasoning models are pioneered by Hill et al. ; Dhingra
et al. ; Sordoni et al. ; Kumar et al. , who used a pre-determined ﬁxed number
of reasoning steps. Shen et al. showed that multi-step reasoning outperforms single-step
ones and dynamic multi-step reasoning further outperforms the ﬁxed multi-step ones on two distinct
MRC datasets (SQuAD and MS MARCO). But the dynamic multi-step reasoning models have to be
trained using RL methods, e.g., policy gradient, which are tricky to implement due to the instability
9This is a special version of SAN where the maximum number of reasoning steps T = 1. SAN in Fig. 3.8
(Left) uses T = 3.
issue. SAN combines the strengths of both types of multi-step reasoning models. As shown in
Fig. 3.8 (Left), SAN uses a ﬁxed number of reasoning steps, and generates a
prediction at each step. During decoding, the answer is based on the average of predictions in
all steps. During training, however, SAN drops predictions via stochastic dropout, and generates
the ﬁnal result based on the average of the remaining predictions. Albeit simple, this technique
signiﬁcantly improves the robustness and overall accuracy of the model. Furthermore, SAN can be
trained using back-propagation which is simple and efﬁcient.
Taking SAN as an example, the multi-step reasoning module computes over T memory steps and
outputs the answer span. It is based on an RNN, similar to IRN in Fig. 3.5. It maintains a state vector,
which is updated on each step. At the beginning, the initial state s0 is the summarized question vector
computed by Eqn. 3.6. At time step t ∈{1, 2, . . . , T}, the state is deﬁned by st = RNN(st−1, xt),
where xt contains retrieved information from memory using the previous state vector as a query via
the attention process: M: xt = P
j γjmj and γ = softmax(st−1⊤W(att)M), where W(att) is a
trainable weight matrix. Finally, a bilinear function is used to ﬁnd the start and end points of answer
spans at each reasoning step t, similar to Eqn. 3.7 and 3.8:
= softmax(st
⊤W(start)M),
= softmax([st;
mj]⊤W(end)M),
where p(start)
is the j-th value of the vector p(start)
, indicating the probability of the j-th passage
word being the start of the answer span at reasoning step t.
A neural MRC model can be viewed as a deep neural network that includes all component modules
(e.g., the embedding layers and reasoning engines) which by themselves are also neural networks.
Thus, it can be optimized on training data in an end-to-end fashion via back-propagation and SGD,
as outlined in Fig. 1.4. For SQuAD models, we optimize model parameters θ by minimizing the loss
function deﬁned as the sum of the negative log probabilities of the ground truth answer span start
and end points by the predicted distributions, averaged over all training samples:
where D is the training set, y(start)
and y(end)
are the true start and end of the answer span of the
i-th training sample, respectively, and pk the k-th value of the vector p.
Conversational Text-QA Agents
While all the neural MRC models described in Sec. 3.7 assume a single-turn QA setting, in reality, humans often ask questions in a conversational context . For example, a user
might ask the question “when was California founded?”, and then depending on the received answer,
follow up by “who is its governor?” and “what is the population?”, where both refer to “California” mentioned in the ﬁrst question. This incremental aspect, although making human conversations
succinct, presents new challenges that most state-of-the-art single-turn MRC models do not address
directly, such as referring back to conversational history using coreference and pragmatic reasoning10 .
A conversational text-QA agent uses a similar architecture to Fig. 3.5, except that the Soft-KB
Lookup module is replaced by a text-QA module which consists of a search engine (e.g., Google
10Pragmatic reasoning is deﬁned as “the process of ﬁnding the intended meaning(s) of the given, and it is
suggested that this amounts to the process of inferring the appropriate context(s) in which to interpret the given”
 . The analysis by Jia and Liang ; Chen et al. revealed that state of the art neural MRC
models, e.g., developed on SQuAD, mostly excel at matching questions to local context via lexical matching
and paragraphing, but struggle with questions that require reasoning.
Figure 3.10: The examples from two conversational QA datasets. (Left) A QA dialogue example
in the QuAC dataset. The student, who does not see the passage (section text), asks questions. The
teacher provides answers in the form of text spans and dialogue acts. These acts include (1) whether
the student should ,→, could ¯
,→, or should not ̸,→ask a follow-up; (2) afﬁrmation (Yes / No), and,
when appropriate, (3) No answer. Figure credit: Choi et al. . (Right) A QA dialogue example
in the CoQA dataset. Each dialogue turn contains a question (Qi), an answer (Ai) and a rationale
(Ri) that supports the answer. Figure credit: Reddy et al. .
or Bing) that retrieves relevant passages for a given question, and an MRC model that generates the
answer from the retrieved passages. The MRC model needs to be extended to address the aforementioned challenges in the conversation setting, henceforth referred to as a conversational MRC
Recently, several datasets have been developed for building conversational MRC models. Among
them are CoQA ) and QuAC ), as shown in Fig. 3.10. The task of conversational MRC is
deﬁned as follows. Given a passage P, the conversation history in the form of question-answer pairs
{Q1, A1, Q2, A2, ..., Qi−1, Ai−1} and a question Qi, the MRC model needs to predict the answer
A conversational MRC model extends the models described in Sec. 3.7 in two aspects. First, the
encoding module is extended to encode not only P and Ai but also the conversation history. Second,
the reasoning module is extended to be able to generate an answer (via pragmatic reasoning) that
might not overlap P. For example, Reddy et al. proposed a reasoning module that combines
the text-span MRC model of DrQA and the generative model of PGNet . To generate a free-form answer, DrQA ﬁrst points to the answer evidence in text (e.g.,
R5 in Fig. 3.10 (Right)), and PGNet generates the an answer (e.g., A5) based on the evidence.
Task-oriented Dialogue Systems
This chapter focuses on task-oriented dialogue systems that assist users in solving a task. Different from applications where the user seeks an answer or certain information (previous chapter),
dialogues covered here are often for completing a task, such as making a hotel reservation or booking movie tickets. Furthermore, compared to chatbots (next chapter), these dialogues often have a
speciﬁc goal to achieve, and are typically domain dependent.
While task-oriented dialogue systems have been studied for decades, they have quickly gaining
increasing interest in recent years, both in the research community and in industry. This chapter
focuses on the foundation and algorithmic aspects, while industrial applications are discussed in
Chapter 6. Furthermore, we restrict ourselves to dialogues where user input is in the form of raw
text, not spoken language, but many of the techniques and discussions in this chapter can be adapted
to spoken dialogues systems.
The chapter is organized as follows. It starts with an overview of basic concepts, terminology, and
a typical architecture for task-oriented dialogue systems. Second, it reviews several representative
approaches to dialogue system evaluation. This part is critical in the development cycle of dialogue
systems, but is largely orthogonal to the concrete techniques used to build them. The next three
sections focus on each of three main components in a typical dialogue system, with an emphasis on
recent, neural approaches. Finally, we review several recent works on end-to-end dialogue systems,
which are enabled by recent advances at the intersection of deep learning and reinforcement learning,
with further discussions and pointers provided in the last section.
We start with a brief overview of task-oriented dialogue systems, focusing on aspects that facilitate
later discussions. For more information and historical developments, readers are referred to the
textbook of Jurafsky and Martin .
Slot-ﬁlling Dialogues
Throughout the chapter, we focus on a relatively simple yet important class of dialogues that involve
ﬁlling in values for a predeﬁned set of slots before a task can be successfully completed. These
dialogues, known as slot-ﬁlling or form-ﬁlling, ﬁnd a number of uses in practice. Table 4.1 gives an
example conversation between a user and a dialogue system. To successfully complete a transaction
to book tickets, the system must collect necessary information by conversing with the user.
Slot-ﬁlling dialogue may be applied to many other domains, such as movie (as shown in the example
above), restaurant, air ticket booking, etc. For each domain, a set of slots are deﬁned by domain
experts and are application speciﬁc. For example, in the movie domain, slots like movie name,
theater name, time, date, ticket price, num tickets, etc. are necessary.
Finally, a slot is called informable if the value for this slot can be used to constrain the conversation, such as phone number; a slot is called requestable if the speaker can ask for its value, such
Table 4.1: An example movie-booking dialogue, adapted from Li et al. .
Hello! How may I assist you?
Can I get tickets for Zoolander 2 tomorrow?
Sure! Which city?
I want to watch it in Seattle.
How many tickets do you need?
Two, please. And I’d like to watch at Regal Meridian 16.
9:25 pm is available at Regal Meridian 16. Does it work for you?
Well, yes.
Okay, I was able to book 3 tickets for Zoolander 2 tomorrow at Regal Meridian 16 theater
in Seattle at 9:25 pm.
Thank you.
Thank you. Good bye!
as ticket price. Note that a slot can be both informable and requestable, an example being
movie name.
Dialogue Acts
The interaction between a dialogue agent and a user, as shown in the previous example, mirrors
the interaction between an RL agent and the environment (Fig. 2.5), where a user utterance is the
observation, and the system utterance is the action selected by the dialogue agent. The dialogue acts
theory gives a formal foundation for this intuition .
In this framework, the utterances of a user or agent are considered actions that can change the
(mental) state of both the user and the system, thus the state of the conversation. These actions can
be used to suggest, inform, request certain information, among others. A simple example dialogue
act is greet, which corresponds to natural language sentences like “Hello! How may I assist you?”.
It allows the system to greet the user and start a conversation. Some dialogue acts may have slots or
slot-value pairs as arguments. For example, the following question in the movie-booking example
“How many tickets do you need?”
is to request information about a certain slot:
request(num tickets),
while the following sentence
“I want to watch it in Seattle.”
is to inform the city name:
inform(city=‘‘seattle’’).
In general, dialogue acts are domain speciﬁc. Therefore, the set of dialogue acts in a movie domain,
for instance, will be different from that in the restaurant domain .
Dialogue as Optimal Decision Making
Equipped with dialogue acts, we are ready to model multi-turn conversations between a dialogue
agent and a user as an RL problem. Here, the dialogue system is the RL agent, and the user is the
environment. At every turn of the dialogue,
• the agent keeps track of the dialogue state, based on information revealed so far in the
conversation, and then takes an action; the action may be a response to the user in the form
of dialogue acts, or an internal operation such as a database lookup or an API call;
• the user responds with the next utterance, which will be used by the agent to update its
internal dialogue state in the next turn;
• an immediate reward is computed to measure the quality and/or cost for this turn of conversation.
This process is precisely the agent-environment interaction discussed in Sec. 2.3. We now discuss
how a reward function is determined.
Figure 4.1: An architecture for multi-turn task-oriented dialogues. It consists of the following
modules: NLU (Natural Language Understanding), DM (Dialogue Manager), and NLG (Natural
Language Generation). DM contains two sub-modules, DST (Dialogue State Tracker) and POL
(Dialogue Policy). The dialogue system, indicated by the dashed rectangle, may have access to an
external database (DB).
An appropriate reward function should capture desired features of a dialogue system. In taskoriented dialogues, we would like the system to succeed in helping the user in as few turns as
possible. Therefore, it is natural to give a high reward (say +20) at the end of the conversation if the
task is successfully solved, or a low reward (say −20) otherwise. Furthermore, we may give a small
penalty (say, −1 reward) to every intermediate turn of the conversation, so that the agent is encouraged to make the dialogue as short as possible. The above is of course just a simplistic illustration
of how to set a reward function for task-oriented dialogues, but in practice more sophisticated reward functions may be used, such as those that measure diversity and coherence of the conversation.
Further discussion of the reward function can be found in Sections 4.4.6, 4.2.1 and 5.4.
To build a system, the pipeline architecture depicted in Fig. 4.1 is often used in practice. It consists
of the following modules.
• Natural Language Understanding (NLU): This module takes the user’s raw utterance as
input and converts it to the semantic form of dialogue acts.
• Dialogue Manager (DM): This module is the central controller of the dialogue system. It
often has a Dialogue State Tracking (DST) sub-module that is responsible for keeping track
of the current dialogue state. The other sub-module, the policy, relies on the internal state
provided by DST to select an action. Note that an action can be a response to the user, or
some operation on backend databases (e.g., looking up certain information).
• Natural Language Generation (NLG): If the policy chooses to respond to the user, this
module will convert this action, often a dialogue act, into a natural language form.
Dialogue Manager
There is a huge literature on building (spoken) dialogue managers. A comprehensive survey is out
of the scope of the this chapter. Interested readers are referred to some of the earlier examples , as well as excellent surveys like McTear , Paek and Pieraccini , and
Young et al. for more information. Here, we review a small subset of traditional approaches
from the decision-theoretic view we take in this paper.
Levin et al. viewed conversation as a decision making problem. Walker and Singh
et al. are two early applications of reinforcement learning to manage dialogue systems. While
promising, these approaches assumed that the dialogue state can only take ﬁnitely many possible
values, and is fully observable (that is, the DST is perfect). Both assumptions are often violated in
real-world applications, given ambiguity in user utterance and unavoidable errors in NLU.
To handle uncertainty inherent in dialogue systems, Roy et al. and Williams and Young
 proposed to use Partially Observable Markov Decision Process (POMDP) as a principled
mathematical framework for modeling and optimizing dialogue systems. The idea is to take user
utterances as observations to maintain a posterior distribution of the unobserved dialogue state; the
distribution is sometimes referred to as the “belief state.” Since exact optimization in POMDPs
is computationally intractable, authors have studied approximation techniques and alternative representations such as the information states framework . Still, compared to the neural approaches covered in later sections, these methods often
require more domain knowledge to engineer features and design states.
Another important limitation of traditional approaches is that each module in Fig. 4.1 is often optimized separately. Consequently, when the system does not perform well, it can be challenging to
solve the “credit assignment” problem, namely, to identify which component in the system causes
undesired system response and needs to be improved. Indeed, as argued by McTear , “[t]he
key to a successful dialogue system is the integration of these components into a working system.”
The recent marriage of differentiable neural models and reinforcement learning allows a dialogue
system to be optimized in an end-to-end fashion, potentially leading to higher conversation quality;
see Sec. 4.6 for further discussions and recent works on this topic.
Evaluation and User Simulation
Evaluation has been an important research topic for dialogue systems. Different approaches have
been used, including corpus-based approaches, user simulation, lab user study, actual user study,
etc. We will discuss pros and cons of these various methods, and in practice trade-offs are made to
ﬁnd the best option or a combination of them.
Evaluation Metrics
While individual components in a dialogue system can often be optimized against more well-deﬁned
metrics such as accuracy, precision, recall, F1 and BLEU scores, evaluating a whole dialogue system requires a more holistic view and is more challenging . In the reinforcement-learning framework, it implies that the reward
function has to take multiple aspects of dialogue quality into consideration. In practice, the reward
function is often a weighted linear combination of a subset of the following metrics.
The ﬁrst class of metrics measures task completion success. The most common choice is perhaps
task success rate—the fraction of dialogues that successfully solve the user’s problem (buying the
right movie tickets, ﬁnding proper restaurants, etc.). Effectively, the reward corresponding to this
metric is 0 for every turn, except for the last turn where it is +1 for a successful dialogue and −1
otherwise. Many examples are found in the literature . Other variants have also been used, such as those to measure partial success .
The second class measures cost incurred in a dialogue, such as time elapsed. A simple yet useful
example is the number of turns, which reﬂects the intuition that a more succinct dialogue is preferred
with everything else being equal. The reward is simply −1 per turn, although more complicated
choices exist .
In addition, other aspects of dialogue quality may also be encoded into the reward function, although
this is a relatively under-investigated direction. In the context of chatbots (Chapter 5), coherence, diversity and personal styles have been used to result in more human-like dialogues .
They can be useful for task-oriented dialogues as well. In Sec. 4.4.6, we will review a few recent
works that aim to learn reward functions automatically from data.
Simulation-Based Evaluation
Typically, an RL algorithm needs to interact with a user to learn (Sec. 2.3). But running RL on either
recruited users or actual users can be expensive. A natural way to get around this challenge is to
build a simulated user, with which an RL algorithm can interact at virtually no cost. Essentially, a
simulated user tries to mimic what a real user does in a conversation: it keeps track of the dialogue
state, and converses with an RL dialogue system.
Substantial research has gone into building realistic user simulators . There are many differ-
request_slots:
ticket: UNK
theater: UNK
start_time: UNK
inform_slots:
number_of_people: 3
date: tomorrow
movie_name: batman vs. superman
Figure 4.2: An example user goal in the movie-ticket-booking domain
ent dimensions to categorize a user simulator, such as deterministic vs. stochastic, content-based vs.
collaboration-based, static vs. non-static user goals during the conversations, among others. Here,
we highlight two dimensions, and refer interested users to Schatzmann et al. for further details on creating and evaluating user simulators :
• Along the granularity dimension, the user simulator can operate either at the dialogue-act
level (also known as intention level), or at the utterance level .
• Along the methodology dimension, the user simulator can be implemented using a rulebased approach, or a model-based approach with the model learned from a real conversational corpus.
Agenda-Based Simulation.
As an example, we describe a popular hidden agenda-based user simulator developed by Schatzmann and Young , as instantiated in Li et al. and Ultes
et al. . Each dialogue simulation starts with a randomly generated user goal that is unknown
to the dialogue manager. In general the user goal consists of two parts: the inform-slots contain
a number of slot-value pairs that serve as constraints the user wants to impose on the dialogue; the
request-slots are slots whose values are initially unknown to the user and will be ﬁlled out during
the conversation. Fig. 4.2 shows an example user goal in a movie domain, in which the user is trying
to buy 3 tickets for tomorrow for the movie batman vs.
Furthermore, to make the user goal more realistic, domain-speciﬁc constraints are added, so that
certain slots are required to appear in the user goal. For instance, it makes sense to require a user to
know the number of tickets she wants in the movie domain.
During the course of a dialogue, the simulated user maintains a stack data structure known as user
agenda. Each entry in the agenda corresponds to a pending intention the user aims to achieve, and
their priorities are implicitly determined by the ﬁrst-in-last-out operations of the agenda stack. In
other words, the agenda provides a convenient way of encoding the history of conversation and the
“state-of-mind” of the user. Simulation of a user boils down to how to maintain the agenda after
each turn of the dialogue, when more information is revealed. Machine learning or expert-deﬁned
rules can be used to set parameters in the stack-update process.
Model-based Simulation.
Another approach to building user simulators is entirely based on
data . Here, we describe a
recent example due to El Asri et al. . Similar to the agenda-based approach, the simulator
also starts an episode with a randomly generated user goal and constraints. These are ﬁxed during a
conversation.
In each turn, the user model takes as input a sequence of contexts collected so far in the conversation,
and outputs the next action. Speciﬁcally, the context at a turn of conversation consists of:
• the most recent machine action,
• inconsistency between machine information and user goal,
• constraint status, and
• request status.
With these contexts, an LSTM or other sequence-to-sequence models are used to output the next
user utterance. The model can be learned from human-human dialogue corpora. In practice, it often
works well by combining both rule-based and model-based techniques to create user simulators.
Further Remarks on User Simulation.
While there has been much work on user simulation,
building a human-like simulator remains challenging. In fact, even user simulator evaluation itself
continues to be an ongoing research topic . In practice, it is often observed that dialogue policies that are overﬁtted to a particular
user simulator may not work well when serving another user simulator or real humans . The gap between a user simulator and humans is the major
limitation of user simulation-based dialogue policy optimization.
Some user simulators are publicly available for research purposes. Other than the aforementioned
agenda-based simulators by Li et al. ; Ultes et al. , a large corpus with an evaluation
environment, called AirDialogue (in the ﬂight booking domain), was recently made available . At the IEEE workshop on Spoken Language Technology in 2018, Microsoft organized a dialogue challenge1 of building end-to-end task-oriented dialogue systems by providing an
experiment platform with built-in user simulators in several domains .
Human-based Evaluation
Due to the discrepancy between simulated users and human users, it is often necessary to test a
dialogue system on human users to reliably evaluate its quality. There are roughly two types of
human users.
The ﬁrst is human subjects recruited in a lab study, possibly through crowd-sourcing platforms.
Typically, the participants are asked to test-use a dialogue system to solve a given task (depending on
the domain of the dialogues), so that a collection of dialogues are obtained. Metrics of interest such
as task-completion rate and average turns per dialogue can be measured, as done with a simulated
user. In other cases, a fraction of these subjects are asked to test-use a baseline dialogue system, so
that the two can be compared against various metrics.
Many published studies involving human subjects are of the ﬁrst type . While this approach has beneﬁts over simulation-based
evaluation, it is rather expensive and time-consuming to get a large number of subjects that can
participate for a long time. Consequently, it has the following limitations:
• The small number of subjects prevents detection of statistically signiﬁcant yet numerically
small differences in metrics, often leading to inconclusive results.
• Only a very small number of dialogue systems may be compared.
• It is often impractical to run an RL agent that learns by interacting with these users, except
in relatively simple dialogue applications.
The other type of humans for dialogue system evaluation is actual users ).
They are similar to the ﬁrst type of users, except that they come with their actual tasks to be solved
by conversing with the system. Consequently, metrics evaluated on them are even more reliable
than those computed on recruited human subjects with artiﬁcially generated tasks. Furthermore,
the number of actual users can be much larger, thus resulting in greater ﬂexibility in evaluation. In
this process, many online and ofﬂine evaluation techniques such as A/B-testing and counterfactual
estimation can be used . The major downside of experimenting with actual
users is the risk of negative user experience and disruption of normal services.
1 
Other Evaluation Techniques
Recently, researchers have started to investigate a different approach to evaluation that is inspired by
the self-play technique in RL . This technique is typically used in a
two-player game (such as the game of Go), where both players are controlled by the same RL agent,
possibly initialized differently. By playing the agent against itself, a large amount of trajectories can
be generated at relatively low cost, from which the RL agent can learn a good policy.
Self-play must be adapted to be used for dialogue management, as the two parties involved in a
conversation often play asymmetric roles (unlike in games such as Go). Shah et al. described
such a dialogue self-play procedure, which can generate conversations between a simulated user and
the system agent. Promising results have been observed in negotiation dialogues 
and task-oriented dialogues . It provides
an interesting solution to avoid the evaluation cost of involving human users as well as overﬁtting to
untruthful simulated users.
In practice, it is reasonable to have a hybrid approach to evaluation. One possibility is to start
with simulated users, then validate or ﬁne-tune the dialogue policy on human users ). Furthermore, there are more systematic approaches to using both sources of users for policy
learning (see Sec. 4.4.5).
Natural Language Understanding and Dialogue State Tracking
NLU and DST are two closely related components essential to a dialogue system. They can have
a signiﬁcant impact on the overall system’s performance ). This section
reviews some of the classic and state-of-the-art approaches.
Natural Language Understanding
The NLU module takes user utterance as input, and performs three tasks: domain detection, intent
determination, and slot tagging. An example output for the three tasks is given in Fig. 4.3. Typically,
a pipeline approach is taken, so that the three tasks are solved one after another. Accuracy and F1
score are two of the most common metrics used to evaluate a model’s prediction quality. NLU is a
pre-processing step for later modules in the dialogue system, whose quality has a signiﬁcant impact
on the system’s overall quality .
Among them, the ﬁrst two tasks are often framed as a classiﬁcation problem, which infers the domain or intent (from a predeﬁned set of candidates) based on the current user utterance . Neural approaches to multi-class classiﬁcation have been used in the recent literature and outperformed traditional statistical methods.
Ravuri and Stolcke studied the use of standard recurrent neural networks, and found
them to be more effective. For short sentences where information has to be inferred from the context, Lee and Dernoncourt proposed to use recurrent and convolutional neural networks that
also consider texts prior to the current utterance. Better results were shown on several benchmarks.
The more challenging task of slot tagging is often treated as sequence classiﬁcation, where the
classiﬁer predicts semantic class labels for subsequences of the input utterance . Fig. 4.3 shows an ATIS (Airline Travel Information System) utterance example
in the Inside-Outside-Beginning (IOB) format , where for each word
the model predicts a semantic tag.
Yao et al. and Mesnil et al. applied recurrent neural networks to slot tagging, where
inputs are one-hot encoding of the words in the utterance, and obtained higher accuracy than statistical baselines such as conditional random ﬁelds and support vector machines. Moreover, it is also
shown that a-prior word information can be effectively incorporated into basic recurrent models to
yield further accuracy gains.
As an example, this section describes the use of bidirectional LSTM , or bLSTM in short, in NLU tasks, following Hakkani-T¨ur et al. who also discussed
other models for the same tasks. The model, as shown in Fig. 4.4, uses two sets of LSTM cells
applied to the input sequence (the forward) and the reversed input sequence (the backward). The
concatenated hidden layers of the forward and backward LSTMs are used as input to another neural
Figure 4.3: An example output of NLU, where the utterance (W) is used to predict domain (I),
intent (I), and the slot tagging (S). The IOB representation is used. Figure credit: Hakkani-T¨ur et al.
Figure 4.4: A bLSTM model for joint optimization in NLU. Picture credit: Hakkani-T¨ur et al.
network to compute the output sequence. Mathematically, upon the tth input token, wt, operations
of the forward part of bLSTM are deﬁned by the following set of equations:
it = g(Wwiwt + Whiht−1)
ft = g(Wwfwt + Whfht−1)
ot = g(Wwowt + Whoht−1)
ˆct = tanh(Wwcwt + Whcht−1)
ct = ft ⊙ct−1 + it ⊙ˆct
ht = ot ⊙tanh(ct) ,
where ht−1 is the hidden layer, W⋆the trainable parameters, and g(·) the sigmoid function. As in
standard LSTMs, it, ft and ot are the input, forget, and output gates, respectively. The backward
part is similar, with the input reversed.
To predict the slot tags as shown in Fig. 4.3, the input wt is often a one-hot vector of a word
embedding vector. The output upon input wt is predicted according to the following distribution pt:
pt = softmax(W(f)
where the superscripts, (f) and (b), denote forward and backward parts of the bLSTM, respectively. For tasks like domain and intent classiﬁcation, the output is predicted at the end of the input
sequence, and simpler architectures may be used .
In many situations, the present utterance alone can be ambiguous or lack all necessary information.
Contexts that include information from previous utterances are expected to help improve model
accuracy. Hori et al. treated conversation history as a long sequence of words, with alternating roles (words from user, vs. words from system), and proposed a variant to LSTM with roledependent layers. Chen et al. built on memory networks that learn which part of contextual
information should be attended to, when making slot-tagging predictions. Both models achieved
higher accuracy than context-free models.
Although the three NLU tasks are often studied separately, there are beneﬁts to jointly solving them
(similar to multi-task learning), and over multiple domains, so that it may require fewer labeled
Figure 4.5: Neural Belief Tracker. Figure credit: Mrkˇsi´c et al. .
data when creating NLU models for a new domain .
Another line of interesting work that can lead to substantial reduction of labeling cost in new domains is zero-shot learning, where slots from different domains are represented in a shared latent
semantic space through embedding of the slots’ (text) descriptions . Interested readers are referred to recent tutorials, such as Chen and Gao and Chen
et al. , for more details.
Dialogue State Tracking
In slot-ﬁlling problems, a dialogue state contains all information about what the user is looking for at
the current turn of the conversation. This state is what the dialogue policy takes as input for deciding
what action to take next (Fig. 4.1).
For example, in the restaurant domain, where a user tries to make a reservation, the dialogue state
may consists of the following components :
• The goal constraint for every informable slot, in the form of a value assignment to that slot.
The value can be “don’t care” (if the user has no preference) or “none” (if the user has
not yet speciﬁed the value).
• The subset of requested slots that the user has asked the system to inform.
• The current dialogue search method, taking values by constraint, by alternative
and finished. It encodes how the user is trying to interact with the dialogue system.
Many alternatives have also been used in the literature, such as a compact, binary representation
recently proposed by Kotti et al. , and the StateNet tracker of Ren et al. that is more
scalable with the domain size (number of slots and number of slot values).
In the past, DST can either be created by experts, or obtained from data by statistical learning
algorithms like conditional random ﬁelds . More recently, neural approaches
have started to gain popularity, with applications of deep neural networks 
and recurrent networks as some of the early examples.
A more recent DST model is the Neural Belief Tracker proposed by Mrkˇsi´c et al. , shown in
Fig. 4.5. The model takes three items as input. The ﬁrst two are the last system and user utterances,
each of which is ﬁrst mapped to an internal, vector representation. The authors studied two models
for representation learning, based on multi-layer perceptrons and convolutional neural networks,
both of which take advantage of pre-trained collections of word vectors and output an embedding for
the input utterance. The third input is any slot-value pair that is being tracked by DST. Then, the three
embeddings may interact among themselves for context modeling, to provide further contextual
information from the ﬂow of conversation, and semantic decoding, to decide if the user explicitly
expressed an intent matching the input slot-value pair. Finally, the context modeling and semantic
decoding vectors go through a softmax layer to produce a ﬁnal prediction. The same process is
repeated for all possible candidate slot-value pairs.
A different representation of dialogue states, called belief spans, is explored by Lei et al. 
in the Sequicity framework. A belief span consists of two ﬁelds: one for informable slots and the
other for requestable slots. Each ﬁeld collects values that have been found for respective slots in
the conversation so far. One of the main beneﬁts of belief spans and Sequicity is that it facilitates
the use of neural sequence-to-sequence models to learn dialogue systems, which take the belief
spans as input and output system responses. This greatly simpliﬁes system design and optimization,
compared to more traditional, pipeline approaches (c.f., Sec. 4.6).
Dialogue State Tracking Challenge (DSTC)
is a series of challenges that provide common
testbeds and evaluation measures for dialogue state tracking. Starting from Williams et al. ,
it has successfully attracted many research teams to focus on a wide range of technical problems in
DST . Corpora used by DSTC over the years have covered human-computer and human-human conversations,
different domains such as restaurant and tourist, cross-language learning. More information may be
found in the DSTC website.2
Dialogue Policy Learning
In this section, we will focus on dialogue policy optimization based on reinforcement learning.
Deep RL for Policy Optimization
The dialogue policy may be optimized by many standard reinforcement learning algorithms. There
are two ways to use RL: online and batch. The online approach requires the learner to interact with
users to improve its policy; the batch approach assumes a ﬁxed set of transitions, and optimizes the
policy based on the data only, without interacting with users ; Pietquin et al.
 ). In this chapter, we discuss the online setting which often has batch learning as an internal
step. Many covered topics can be useful in the batch setting. Here, we use the DQN as an example,
following Lipton et al. , to illustrate the basic work ﬂow.
Model: Architecture, Training and Inference.
The DQN’s input is an encoding of the current
dialogue state. One option is to encode it as a feature vector, consisting of: (1) one-hot representations of the dialogue act and slot corresponding to the last user action; (2) the same one-hot
representations of the dialogue act and slot corresponding to the last system action; (3) a bag of slots
corresponding to all previously ﬁlled slots in the conversation so far; (4) the current turn count; and
(5) the number of results from the knowledge base that match the already ﬁlled-in constraints for
informed slots. Denote this input vector by s.
DQN outputs a real-valued vector, whose entries correspond to all possible (dialogue-act, slot) pairs
that can be chosen by the dialogue system. Available prior knowledge can be used to reduce the
number of outputs, if some (dialogue-act, slot) pairs do not make sense for a system, such as
request(price). Denote this output vector by q.
The model may have L ≥1 hidden layers, parameterized by matrices {W1, W2, . . . , WL}, so that
hl = g(Wlhl−1) ,
l = 1, 2, . . . , L −1
q = WLhL−1 ,
where g(·) is an activation function such as ReLU or sigmoid. Note that the last layer does not need
an activation function, and the output q is to approximate Q(s, ·), the Q-values in state s.
To learn parameters in the network, one can use an off-the-shelf reinforcement-learning algorithm
(e.g., Eqn. 2.4 or 2.5 with experience replay); see Sec. 2.3 for the exact update rules and improved
algorithms. Once these parameters are learned, the network induces a greedy action-selection policy
2 
as follows: for a current dialogue state s, use a forward pass on the network to compute q, the Qvalues for all actions. One can pick the action, which is a (dialogue act, slot) pair, that corresponds
to the entry in q with the largest value. Due to the need for exploration, the above greedy action
selection may not be desired; see Sec. 4.4.2 for a discussion on this subject.
Warm-start Policy.
Learning a good policy from scratch often requires many data, but the process
can be signiﬁcantly sped up by restricting the policy search using expert-generated dialogues or teacher advice , or by initializing the policy to be a
reasonable one before switching to online interaction with (simulated) users.
One approach is to use imitation learning (also known as behavioral cloning) to mimic an expertprovided policy. A popular option is to use supervised learning to directly learn the expert’s action
in a state; see Su et al. ; Dhingra et al. ; Williams et al. ; Liu and Lane 
for a few recent examples. Li et al. turned imitation learning into an induced reinforcement
learning problem, and then applied an off-the-shelf RL algorithm to learn the expert’s policy.
Finally, Lipton et al. proposed a simple yet effective alternative known as Replay Buffer
Spiking (RBS) that is particularly suited to DQN. The idea is to pre-ﬁll the experience replay buffer
of DQN with a small number of dialogues generated by running a na¨ıve yet occasionally successful,
rule-based agent. This technique is shown to be essential for DQN in simulated studies.
Other Approaches.
In the above example, a standard multi-layer perceptron is used in the DQN
to approximate the Q-function. It may be replaced by other models, such as a Bayesian version described in the next subsection for efﬁcient exploration, and recurrent networks that can more easily capture information from conversational histories
than expert-designed dialogue states. In another recent example, Chen et al. used graph neural networks to model the Q-function, with nodes in the graph corresponding to slots of the domain.
The nodes may share some of the parameters across multiple slots, therefore increasing learning
Furthermore, one may replace the above value function-based methods by others like policy gradient
(Sec. 2.3.2), as done by Fatemi et al. ; Dhingra et al. ; Strub et al. ; Williams et al.
 ; Liu et al. .
Efﬁcient Exploration and Domain Extension
Without a teacher, an RL agent learns from data collected by interacting with an initially unknown
environment. In general, the agent has to try new actions in novel states, in order to discover potentially better policies. Hence, it has to strike a good trade-off between exploitation (choosing good
actions to maximize reward, based on information collected thus far) and exploration (choosing
novel actions to discover potentially better alternatives), leading to the need for efﬁcient exploration . In the context of dialogue policy learning, the implication is that
the policy learner actively tries new ways to converse with a user, in the hope of discovering a better
policy in the long run .
While exploration in ﬁnite-state RL is relatively well-understood , exploration when parametric models like neural
networks are used is an active research topic . Here, a general-purpose exploration strategy is described, which is
particularly suited for dialogue systems that may evolve over time.
After a task-oriented dialogue system is deployed to serve users, there may be a need over time
to add more intents and/or slots to make the system more versatile. This problem, referred to as
domain extension , makes exploration even more challenging: the agent needs to
explicitly quantify the uncertainty in its parameters for intents/slots, so as to explore new ones more
aggressively while avoiding exploring those that have already been learned. Lipton et al. 
approached the problem using a Bayesian-by-Backprop variant of DQN.
Their model, called BBQ, is identical to DQN, except that it maintains a posterior distribution q over
the network weights w = (w1, w2, . . . , wd). For computational convenience, q is a multivariate
Gaussian distribution with diagonal covariance, parameterized by θ = {(µi, ρi)}d
i=1, where weight
wi has a Gaussian posterior distribution, N(µi, σ2
i ) and σi = log(1 + exp(ρi)). The posterior
information leads to a natural exploration strategy, inspired by Thompson Sampling . When selecting actions, the agent simply draws
a random weight ˜w ∼q, and then selects the action with the highest value output by the network.
Experiments show that BBQ explores more efﬁciently than state-of-the-art baselines for dialogue
domain extension.
The BBQ model is updated as follows. Given observed transitions T = {(s, a, r, s′)}, one uses the
target network (see Sec. 2.3) to compute the target values for each (s, a) in T , resulting in the set
D = {(x, y)}, where x = (s, a) and y may be computed as in DQN. Then, parameter θ is updated to
represent the posterior distribution of weights. Since the exact posterior is not Gaussian any more,
and thus not representable by BBQ, it is approximated as follows: θ is chosen by minimizing the
variational free energy , the KL-divergence between the variational
approximation q(w|θ) and the posterior p(w|D):
argminθ KL[q(w|θ)||p(w|D)]
KL[q(w|θ)||p(w)] −Eq(w|θ)[log p(D|w)]
In other words, the new parameter θ is chosen so that the new Gaussian distribution is closest to the
posterior measured by KL-divergence.
Composite-task Dialogues
In many real-world problems, a task may consist of a set of subtasks that need to be solved collectively. Similarly, dialogues can often be decomposed into a sequence of related sub-dialogues, each
of which focuses on a subtopic . Consider for example a travel planning
dialogue system, which needs to book ﬂights, hotels and car rental in a collective way so as to satisfy
certain cross-subtask constraints known as slot constraints . Slot constraints are
application speciﬁc. In a travel planning problem, one natural constraint is that the outbound ﬂight’s
arrival time should be earlier than the hotel check-in time.
Complex tasks with slot constraints are referred to as composite tasks by Peng et al. . Optimizing the dialogue policy for a composite task is challenging for two reasons. First, the policy has
to handle many slots, as each subtask often corresponds to a domain with its own set of slots, and
the set of slots of a composite-task consists of slots from all subtasks. Furthermore, thanks to slot
constraints, these subtasks cannot be solved independently. Therefore, the state space considered by
a composite-task is much larger. Second, a composite-task dialogue often requires many more turns
to complete. Typical reward functions give a success-or-not reward only at the end of the whole
dialogue. As a result, this reward signal is very sparse and considerably delayed, making policy
optimization much harder.
Cuay´ahuitl et al. proposed to use hierarchical reinforcement learning to optimize a composite
task’s dialogue policy, with tabular versions of the MAXQ and Hierarchical Abstract Machine approaches. While promising, their solutions assume ﬁnite
states, so do not apply directly to larger-scale conversational problems.
More recently, Peng et al. tackled the composite-task dialogue policy learning problem under
the more general options framework , where the task hierarchy has two levels.
As illustrated in Fig. 4.6, a top-level policy πg selects which subtask g to solve, and a low-level
policy πa,g solves the subtask speciﬁed by πg. Assuming predeﬁned subtasks, they extend the DQN
model that results in substantially faster learning speed and superior policies. A similar approach is
taken by Budzianowski et al. , who used Gaussian process RL instead of deep RL for policy
A major assumption in options/subgoal-based hierarchical reinforcement learning is the need for
reasonable options and subgoals. Tang et al. considered the problem of discovering subgoals
from dialogue demonstrations. Inspired by a sequence segmentation approach that is successfully
applied to machine translation , the authors developed the Subgoal Discovery
Network (SDN), which learns to identify “bottleneck” states in successful dialogues. It is shown that
the hierarchical DQN optimized with subgoals discovered by SDN is competitive to expert-designed
Finally, another interesting attempt is made by Casanueva et al. based on Feudal Reinforcement Learning (FRL) . In contrast to the above methods that decompose a
Figure 4.6: A two-level hierarchical dialogue policy. Figure credit: Peng et al. .
Table 4.2: An example of multi-domain dialogue, adapted from Cuay´ahuitl et al. . The ﬁrst
column speciﬁes which domain is triggered in the system, based on user utterances received so far.
“Hi! How can I help you?”
“I’m looking for a hotel in Seattle on January 2nd
for 2 nights.”
“A hotel for 2 nights in Seattle on January 2nd?”
“I found Hilton Seattle.”
“Anything else I can help with?”
“I’m looking for cheap Japanese food in the downtown.”
restaurant
“Did you say cheap Japanese food?”
“I found the following results.”
task into temporally separated subtasks, FRL decomposes a complex decision spatially. In each turn
of a dialogue, the feudal policy ﬁrst decides between information-gathering actions and informationproviding actions, then chooses a primitive action that falls in the corresponding high-level category.
Multi-domain Dialogues
A multi-domain dialogue can converse with a user to have a conversation that may involve more than
one domain . Table 4.2 shows
an example, where the dialogue covers both the hotel and restaurant domains, in addition to a
special meta domain for sub-dialogues that contain domain-independent system and user responses.
Different from composite tasks, sub-dialogues corresponding to different domains in a conversation
are separate tasks, without cross-task slot constraints. Similar to composite-task systems, a multidomain dialogue system needs to keep track of a much larger dialogue state space that has slots
from all domains, so directly applying RL can be inefﬁcient. It thus raises the need to learn reusable policies whose parameters can be shared across multiple domains as long as they are related.
Gaˇsi´c et al. proposed to use a Bayesian Committee Machine (BCM) for efﬁcient multidomain policy learning. During training time, a number of policies are trained on different, potentially small, datasets. The authors used Gaussian processes RL algorithms to optimize those
policies, although they can be replaced by deep learning alternatives. During test time, in each turn
of a dialogue, these policies recommend an action, and all recommendations are aggregated into a
ﬁnal action to be taken by the BCM policy.
Cuay´ahuitl et al. developed another related technique known as NDQN—Network of DQNs,
where each DQN is trained for a specialized skill to converse in a particular sub-dialogue. A meta-
Figure 4.7: Three strategies for optimizing dialogue policies based on reinforcement learning. Figure credit: Peng et al. .
policy controls how to switch between these DQNs, and can also be optimized using (deep) reinforcement learning.
More recently, Papangelis et al. studied another approach in which policies optimized for
difference domains can be shared, through a set of features that describe a domain. It is shown to
be able to handle unseen domains, and thus reduce the need for domain knowledge to design the
Integration of Planning and Learning
As mentioned in Sec. 4.2, optimizing the policy of a task-oriented dialogue against humans is costly,
since it requires many interactions between the dialogue system and humans (left panel of Fig. 4.7).
Simulated users provide an inexpensive alternative to RL-based policy optimization (middle panel
of Fig. 4.7), but may not be a sufﬁciently truthful approximation of human users.
Here, we are concerned with the use of a user model to generate more data to improve sample
complexity in optimizing a dialogue system. Inspired by the Dyna-Q framework ,
Peng et al. proposed Deep Dyna-Q (DDQ) to handle large-scale problems with deep learning
models, as shown by the right panel of Fig. 4.7. Intuitively, DDQ allows interactions with both
human users and simulated users. Training of DDQ consists of three parts:
• direct reinforcement learning: the dialogue system interacts with a real user, collects real
dialogues and improves the policy by either imitation learning or reinforcement learning;
• world model learning: the world model (user simulator) is reﬁned using real dialogues
collected by direct reinforcement learning;
• planning: the dialogue policy is improved against simulated users by reinforcement learning.
Human-in-the-loop experiments show that DDQ is able to efﬁciently improve the dialogue policy
by interacting with real users, which is important for deploying dialogue systems in practice.
One challenge with DDQ is to balance samples from real users (direct reinforcement learning) and
simulated users (planning). Peng et al. used a heuristics that reduces planning steps in later
stage of DDQ when more real user interactions are available. In contrast, Su et al. proposed
the Discriminative Deep Dyna-Q (D3Q) that is inspired by generative adversarial networks. Specifically, it incorporates a discriminator which is trained to differentiate experiences of simulated users
from those of real users. During the planning step, a simulated experience is used for policy training
only when it appears to be a real-user experience according to the discriminator.
Reward Function Learning
The dialogue policy is often optimized to maximize long-term reward when interacting with users.
The reward function is therefore critical to creating high-quality dialogue systems. One possibility is
to have users provide feedback during or at the end of a conversation to rate the quality, but feedback
like this is intrusive and costly. Often, easier-to-measure quantities such as time-elapsed are used to
compute a reward function. Unfortunately, in practice, designing an appropriate reward function is
not always obvious, and substantial domain knowledge is needed (Sec. 4.1). This inspires the use of
machine learning to ﬁnd a good reward function from data which can better correlate with user satisfaction , or is more consistent with expert demonstrations .
Su et al. proposed to rate dialogue success with two neural network models, a recurrent and
a convolutional network. Their approach is found to result in competitive dialogue policies, when
compared to a baseline that uses prior knowledge of user goals. However, these models assume
the availability of labeled data in the form of (dialogue, success-or-not) pairs, in which the successor-not feedback provided by users can be expensive to obtain. To reduce the labeling cost, Su
et al. investigated an active learning approach based on Gaussian processes, which
aims to learn the reward function and policy at the same time while interacting with human users.
Ultes et al. argued that dialogue success only measures one aspect of the dialogue policy’s
quality. Focusing on information-seeking tasks, the authors proposed a new reward estimator based
on interaction quality that balances multiple aspects of the dialogue policy. Later on, Ultes et al.
 used multi-objective RL to automatically learn how to linearly combine multiple metrics of
interest in the deﬁnition of reward function.
Finally, inspired by adversarial training in deep learning, Liu and Lane proposed to view
the reward function as a discriminator that distinguishes dialogues generated by humans from those
by the dialogue policy. Therefore, there are two learning processes in their approach: the reward
function as a discriminator, and the dialogue policy optimized to maximize the reward function.
The authors showed that such an adversarially learned reward function can lead to better dialogue
policies than with hand-designed reward functions.
Natural Language Generation
Natural Language Generation (NLG) is responsible for converting a communication goal, selected
by the dialogue manager, into a natural language form. It is an important component that affects
naturalness of a dialogue system, and thus the user experience.
There exist many approaches to language generation. The most common in practice is perhaps
template- or rule-based ones, where domain experts design a set of templates or rules, and hand-craft
heuristics to select a proper candidate to generate sentences. Even though machine learning can be
used to train certain parts of these systems , the cost to write and maintain templates and rules leads to challenges in adapting to
new domains or different user populations. Furthermore, the quality of these NLG systems is limited
by the quality of hand-crafted templates and rules.
These challenges motivate the study of more data-driven approaches, known as corpus-based methods, that aim to optimize a generation module from corpora . Most such methods are based on supervised learning, while Rieser and Lemon takes a decision-theoretic view and uses reinforcement learning to make a trade-off between sentence length and information revealed.3
In recent years, there is a growing interest in neural approaches to language generation. An elegant
model, known as Semantically Controlled LSTM (SC-LSTM) , is a variant of
LSTM , with an extra component that gives a semantic control
on the language generation results. As shown in Fig. 4.8, a basic SC-LSTM cell has two parts: a
typical LSTM cell (upper part in the ﬁgure) and a sentence planning cell (lower part) for semantic
3Some authors have taken a similar, decision-theoretic point of
view for NLG. Their formulate NLG as a planning problem, as opposed to data-driven or corpus-based methods
being discussed here.
Figure 4.8: A Semantic Controlled LSTM (SC-LSTM) Cell. Picture credit: Wen et al. .
More precisely, the operations upon receiving the tth input token, denoted wt, are deﬁned by the
following set of equations:
it = g(Wwiwt + Whiht−1)
ft = g(Wwfwt + Whfht−1)
ot = g(Wwowt + Whoht−1)
rt = g(Wwrwt + αWhrht−1)
dt = rt ⊙dt−1
ˆct = tanh(Wwcwt + Whcht−1)
ct = ft ⊙ct−1 + it ⊙ˆct + tanh(Wdcdt)
ht = ot ⊙tanh(ct) ,
where ht−1 is the hidden layer, W⋆the trainable parameters, and g(·) the sigmoid function. As in
a standard LSTM cell, it, ft and ot are the input, forget, and output gates, respectively. The extra
component introduced to SC-LSTM is the reading gate rt, which is used to compute a sequence
of dialogue acts {dt} starting from the original dialogue act d0. This sequence is to ensure that
the generated utterance represents the intended meaning, and the reading gate is to control what
information to be retained for future steps. It is in this sense that the gate rt plays the role of
sentence planning . Finally, given the hidden layer ht, the output distribution is
given by a softmax function:
wt+1 ∼softmax(Whoht) .
Wen et al. proposed several improvements to the basic SC-LSTM architecture. One was to
make the model deeper by stacking multiple LSTM cells on top of the structure in Fig. 4.8. Another
was utterance reranking: they trained another instance of SC-LSTM on the reversed input sequence,
similar to bidirectional recurrent networks, and then combined both instances to ﬁnalize reranking.
The basic approach outlined above may be extended in several ways. For example, Wen et al. 
investigated the use of multi-domain learning to reduce the amount of data to train a neural language
generator, and Su et al. proposed a hierarchical approach that leverages linguistic patterns to
further improve generation results. Language generation remains an active research area. The next
chapter will cover more recent works for chitchat conversations, in which many techniques can also
be useful in task-oriented dialogue systems.
End-to-end Learning
Traditionally, components in most dialogue systems are optimized separately. This modularized
approach provides the ﬂexibility that allows each module to be created in a relatively independent
way. However, it often leads to a more complex system design, and improvements in individual
modules do not necessarily translate into improvement of the whole dialogue system. Lemon 
argued for, and empirically demonstrated, the beneﬁt of jointly optimizing dialogue management
and natural language generation, within a reinforcement-learning framework. More recently, with
the increasing popularity of neural models, there have been growing interests in jointly optimizing
multiple components, or even end-to-end learning of a dialogue system.
One beneﬁt of neural models is that they are often differentiable and can be optimized by gradientbased methods like back-propagation . In addition to language understanding, state tracking and policy learning that have been covered in previous sections, speech
recognition & synthesis (for spoken dialogue systems) may be learned by neural models and backpropagation to achieve state-of-the-art performance . In the extreme, if all components in a task-oriented dialogue system (Fig. 4.1)
are differentiable, the whole system becomes a larger differentiable system that can be optimized by
back-propagation against metrics that quantify overall quality of the whole system. This is an advantage compared to traditional approaches that optimize individual components separately. There
are two general classes of approaches to building an end-to-end dialogue system:
Supervised Learning.
The ﬁrst is based on supervised learning, where desired system responses
are ﬁrst collected and then used to train multiple components of a dialogue system in order to maximize prediction accuracy .
Wen et al. introduced a modular neural dialogue system, where most modules are represented
by a neural network. However, their approach relies on non-differentiable knowledge-base lookup
operators, so training of the components is done separately in a supervised manner. This challenge
is addressed by Dhingra et al. who proposed “soft” knowledge-base lookups; see Sec. 3.5 for
more details.
Bordes et al. treated dialogue system learning as the problem of learning a mapping from
dialogue histories to system responses. They show memory networks and supervised embedding
models outperform standard baselines on a number of simulated dialogue tasks. A similar approach
was taken by Madotto et al. in their Mem2Seq model. This model uses mechanisms from
pointer networks so as to incorporate external information from knowledge
Finally, Eric et al. proposed an end-to-end trainable Key-Value Retrieval Network, which is
equipped with an attention-based key-value retrieval mechanism over entries of a KB, and can learn
to extract relevant information from the KB.
Reinforcement Learning.
While supervised learning can produce promising results, they require
training data that may be expensive to obtain. Furthermore, this approach does not allow a dialogue
system to explore different policies that can potentially be better than expert policies that produce
responses for supervised training. This inspire another line of work that uses reinforcement learning
to optimize end-to-end dialogue systems .
Zhao and Esk´enazi proposed a model that takes user utterance as input and outputs a semantic
system action. Their model is a recurrent variant of DQN based on LSTM, which learns to compress
a user utterance sequence to infer an internal state of the dialogue. Compared to classic approaches,
this method is able to jointly optimize the policy as well as language understanding and state tracking
beyond standard supervised learning.
Another approach, taken by Williams et al. , is to use LSTM to avoid the tedious step of state
tracking engineering, and jointly optimize state tracker and the policy. Their model, called Hybrid
Code Networks (HCN), also makes it easy for engineers to incorporate business rules and other
prior knowledge via software and action templates. They show that HCN can be trained end-to-end,
demonstrating much faster learning than several end-to-end techniques.
Strub et al. applied policy gradient to optimize a visually grounded task-oriented dialogue
in the GuessWhat?! game in an end-to-end fashion. In the game, both the user and the dialogue
system have access to an image. The user chooses an object in the image without revealing it, and
the dialogue system is to locate this object by asking the user a sequence of yes-no questions.
Finally, it is possible to combine supervised and reinforcement learning in an end-to-end trainable
system. Liu et al. proposed such a hybrid approach. First, they used supervised learning on
human-human dialogues to pre-train the policy. Second, they used an imitation learning algorithm,
known as DAgger , to ﬁne tune the policy with human teachers who can suggest
correct dialogue actions. In the last step, reinforcement learning was used to continue policy learning
with online user feedback.
Further Remarks
In this chapter, we have surveyed recent neural approaches to task-oriented dialogue systems, focusing on slot-ﬁlling problems. This is a new area with many exciting research opportunities. While
it is out of the scope of the paper to give a full coverage of more general dialogue problems and all
research directions, we brieﬂy describe a small sample of them to conclude this chapter.
Beyond Slot-ﬁlling Dialogues.
Task-oriented dialogues in practice can be much more diverse and
complex than slot-ﬁlling ones. Information-seeking or navigation dialogues are another popular
example that has been mentioned in different contexts , Papangelis et al.
 , and Sec. 3.5). Another direction is to enrich the dialogue context. Rather than text-only or
speech-only ones, our daily dialogues are often multimodal, and involve both verbal and nonverbal
inputs like vision .
Challenges such as how to combine information from multiple modalities to make decisions arise
naturally.
So far, we have looked at dialogues that involve two parties—the user and the dialogue agent, and
the latter is to assist the former. In general, the task can be more complex such as mixed-initiative
dialogues and negotiations . More generally,
there may be multiple parties involved in a conversation, where turn taking becomes more challenging . In such scenarios, it is helpful to take a game-theoretic view,
more general than the MDP view as in single-agent decision making.
Weaker Learning Signals.
In the literature, a dialogue system can be optimized by supervised,
imitation, or reinforcement learning. Some require expert labels/demonstrations, while some require
a reward signal from a (simulated) user. There are other weaker form of learning signals that facilitate dialogue management at scale. A promising direction is to consider preferential input: instead
of having an absolute judgment (either in the form of label or reward) of the policy quality, one only
requires a preferential input that indicates which one of two dialogues is better. Such comparable
feedback is often easier and cheaper to obtain, and can be more reliable than absolute feedback.
Related Areas.
Evaluation remains a major research challenge. Although user simulation can be
useful (Sec. 4.2.2), a more appealing and robust solution is to use real human-human conversation
corpora directly for evaluation. Unfortunately, this problem, known as off-policy evaluation in the
RL literature, is challenging with numerous current research efforts . Such off-policy techniques can ﬁnd
important use in evaluating and optimizing dialogue systems.
Another related line of research is deep reinforcement learning applied to text games , which are in many ways similar to a conversation, except that the
scenarios are predeﬁned by the game designer. Recent advances for solving text games, such as
handling natural-language actions and
interpretable policies may be useful for task-oriented dialogues as well.
Fully Data-Driven Conversation
Models and Social Bots
Researchers have recently begun to explore fully data-driven and end-to-end (E2E) approaches
to conversational response generation, e.g., within the sequence-to-sequence (seq2seq) framework . These models are trained entirely
from data without resorting to any expert knowledge, which means they do not rely on the four
traditional components of dialogue systems noted in Chapter 4. Such end-to-end models have been
particularly successful with social bot (chitchat) scenarios, as social bots rarely require interaction
with the user’s environment, and the lack of external dependencies such as API calls simpliﬁes endto-end training. By contrast, task-completion scenarios typically require such APIs in the form of,
e.g., knowledge base access. The other reason this framework has been successful with chitchat is
that it easily scales to large free-form and open-domain datasets, which means the user can typically
chat on any topic of her liking. While social bots are of signiﬁcant importance in facilitating smooth
interaction between humans and their devices, more recent work also focuses on scenarios going
beyond chitchat, e.g., recommendation.
End-to-End Conversation Models
Most of the earliest end-to-end (E2E) conversation models are inspired by statistical machine translation (SMT) , including neural machine translation . The casting of the conversational response generation task (i.e., predict a response Ti based on the previous dialogue turn Ti−1)
as an SMT problem is a relatively natural one, as one can treat turn Ti−1 as the “foreign sentence”
and turn Ti as its “translation”. This means one can apply any off-the-shelf SMT algorithm to a
conversational dataset to build a response generation system. This was the idea originally proposed
in one of the ﬁrst works on fully data-driven conversational AI , which applied
a phrase-based translation approach to dialogue datasets extracted from Twitter . A different E2E approach was proposed in , but it
relied on IR-based methods rather than machine translation.
While these two papers constituted a paradigm shift, they had several limitations. The most significant one is their representation of the data as (query, response) pairs, which hinders their ability to
generate responses that are contextually appropriate. This is a serious limitation as dialogue turns in
chitchat are often short (e.g., a few word utterance such as “really?”), in which case conversational
models critically need longer contexts to produce plausible responses. This limitation motivated
the work of Sordoni et al. , which proposed an RNN-based approach to conversational response generation (similar to Fig. 2.2) to exploit longer context. Together with the contemporaneous
works , these papers presented the ﬁrst neural approaches
to fully E2E conversation modeling. While these three papers have some distinct properties, they
are all based on RNN architectures, which nowadays are often modeled with a Long Short-Term
Memory (LSTM) model .
The LSTM Model
We give an overview of LSTM-based response generation. LSTM is arguably the most popular
seq2seq model, although alternative models like GRU are often as effective.
LSTM is an extension of the RNN model in Fig. 2.2, and is often more effective at exploiting longterm context.
An LSTM-based response generation system is usually modeled as follows : Given a dialogue history represented as a sequence of words S = {s1, s2, ..., sNs}
(S here stands for source), the LSTM associates each time step k with input, memory, and output
gates, denoted respectively as ik, fk and ok. Ns is the number of words in the source S.1 Then, the
hidden state hk of the LSTM for each time step k is computed as follows:
ik = σ(Wi[hk−1; ek])
fk = σ(Wf[hk−1; ek])
ok = σ(Wo[hk−1; ek])
lk = tanh(Wl[hk−1; ek])
ck = fk ◦ck−1 + ik ◦lk
k = ok ◦tanh(ck)
where matrices Wi, Wf, Wo, Wl belong to Rd×2d, ◦denotes the element-wise product. As it is a
response generation task, each conversational context S is paired with a sequence of output words to
predict: T = {t1, t2, ..., tNt}. Here, Nt is the length of the response and t represents a word token
that is associated with a d-dimensional word embedding et (distinct from the source).
The LSTM model deﬁnes the probability of the next token to predict using the softmax function.
Speciﬁcally, let f(hk−1, eyk) be the softmax activation function of hk−1 and eyk, where hk−1 is
the hidden vector at time k −1. Then, the probability of outputing token T is given by
p(tk|s1, s2, ..., st, t1, t2, ..., tk−1)
exp(f(hk−1, eyk))
y′ exp(f(hk−1, ey′)) .
The HRED Model
While the LSTM model has been shown to be effective in encoding textual contexts up to 500
words , dialogue histories can often be long and there is sometimes a need
to exploit longer-term context. Hierarchical models were designed to address this limitation by
capturing longer context . One popular approach is the Hierarchical Recurrent Encoder-Decoder (HRED) model, originally proposed
in for query suggestion and applied to response generation in , a problem that limits
RNN’s (including LSTM’s) ability to model very long word sequences. Note that, in this particular
work, RNN hidden states are implemented using GRU instead of LSTM.
1The notation distinguishes e and h where ek is the embedding vector for an individual word at time step k,
and hk is the vector computed by the LSTM model at time k by combining ek and hk−1. ck is the cell state
vector at time k, and σ represents the sigmoid function.
Figure 5.1: (a) Recurrent architecture used by models such as RNN, GRU, LSTM, etc. (2) Two-level
hierarchy representative of HRED. Note: To simplify the notation, the ﬁgure represents utterances
of length 3.
Attention Models
The seq2seq framework has been tremendously successful in text generation tasks such as machine
translation, but its encoding of the entire source sequence into a ﬁxed-size vector has certain limitations, especially when dealing with long source sequences. Attention-based models alleviate this limitation by allowing the model to search and condition on
parts of a source sentence that are relevant to predicting the next target word, thus moving away from
a framework that represents the entire source sequence merely as a single ﬁxed-size vector. While
attention models and variants have contributed to
signiﬁcant progress in the state-of-the-art in translation and are very commonly
used in neural machine translation nowadays, attention models have been somewhat less effective in
E2E dialogue modeling. This can probably be explained by the fact that attention models effectively
attempt to “jointly translate and align” , which is a desirable goal in machine
translation as each information piece in the source sequence (foreign sentence) typically needs to
be conveyed in the target (translation) exactly once, but this is less true in dialogue data. Indeed,
in dialogue entire spans of the source may not map to anything in the target and vice-versa.2 Some
speciﬁc attention models for dialogue have been shown to be useful , e.g., to avoid word repetitions (which are discussed further in Sec. 5.2).
Pointer-Network Models
Multiple model extensions of the seq2seq framework improve
the model’s ability to “copy and paste” words between the conversational context and the response.
Compared to other tasks such as translation, this ability is particularly important in dialogue, as the
response often repeats spans of the input (e.g., “good morning” in response to “good morning”)
or uses rare words such as proper nouns, which the model would have difﬁculty generating with
a standard RNN. Originally inspired by the Pointer Network model —which
produces an output sequence consisting of elements from the input sequence—these models hypothesize target words that are either drawn from a ﬁxed-size vocabulary (akin to a seq2seq model) or
selected from the source sequence (akin to a pointer network) using an attention mechanism. An
instance of this model is CopyNet , which was shown to signiﬁcantly improve over
RNNs thanks to its ability to repeat proper nouns and other words of the input.
2Ritter et al. also found that alignment produced by an off-the-shelf word aligner produced alignments of poor quality, and an extension of their work with attention models yield attention scores that did not correspond to meaningful alignments.
Challenges and Remedies
The response generation task faces challenges that are rather speciﬁc to conversation modeling.
Much of the recent research is aimed at addressing the following issues.
Response Blandness
Utterances generated by neural response generation systems are often bland and deﬂective. While
this problem has been noted in other tasks such as image captioning , the problem
is particularly acute in E2E response generation, as commonly used models such as seq2seq tend to
generate uninformative responses such as “I don’t know” or “I’m OK”. Li et al. suggested
that this is due to their training objective, which optimizes the likelihood of the training data according to p(T|S), where S is the source (dialogue history) and T is the target response. The objective
p(T|S) is asymmetrical in T and S, which causes the trained systems to prefer responses T that
unconditionally enjoy high probability, i.e., irrespectively of the context S. For example, such systems often respond “I don’t know” if S is a question, as the response “I don’t know” is plausible for
almost all questions. Li et al. suggested replacing the conditional probability p(T|S) with
mutual information
p(T )p(S) as an objective, since the latter formulation is symmetrical in S and T,
thus giving no incentive for the learner to bias responses T to be particularly bland and deﬂective,
unless such a bias emerges from the training data itself. While this argument may be true in general,
optimizing the mutual information objective ) can be challenging, so Li et al. used that objective at inference time.
More speciﬁcally, given a conversation history S, the goal at inference time is to ﬁnd the best T
according to:3
ˆT = argmaxT
log p(S, T)
log p(T|S) −log p(T)
A hyperparameter λ was introduced to control how much to penalize generic responses, with either
formulations:4
ˆT = argmaxT
log p(T|S) −λ log p(T)
(1 −λ) log p(T|S)
+ λ log p(S|T) −λ log p(S)
(1 −λ) log p(T|S) + λ log p(S|T)
Thus, this weighted MMI objective function can be viewed as representing a tradeoff between
sources given targets (i.e., p(S|T)) and targets given sources (i.e., p(T|S)), which is also a tradeoff
between response appropriateness and lack of blandness. Note, however, that despite this tradeoff, Li et al. have not entirely solved the blandness problem, as this objective is only used
at inference and not training time. This approach ﬁrst generates N-best lists according to p(T|S)
and rescores them with MMI. Since such N-best lists tend to be overall relatively bland due to the
p(T|S) inference criterion (beam search), MMI rescoring often mitigates rather than completely
eliminates the blandness problem.
More recently, researchers have used adversarial training and Generative Adversarial Networks (GAN) , which often
have the effect of reducing blandness. Intuitively, the effect of GAN on blandness can be understood
as follows: adversarial training puts a Generator and Discriminator against each other (hence the
term “adversarial”) using a minimax objective, and the objective for each of them is to make their
counterpart the least effective. The Generator is the response generation system to be deployed,
while the goal of the Discriminator is to be able to identify whether a given response is generated
by a human (i.e., from the training data) or is the output of the Generator. Then, if the Generator
3Recall that log
p(S)p(T ) = log p(T |S)
= log p(T|S) −log p(T)
4The second formulation is derived from:
log p(T) = log p(T|S) + log p(S) −log p(S|T).
always responds “I don’t know” or with other deﬂective responses, the Discriminator would have
little problem distinguishing them from human responses in most of the cases, as most humans do
not respond with “I don’t know” all the time. Therefore, in order to fool the Discriminator, the Generator progressively steers away from such predictable responses. More formally, the optimality of
GAN is achieved when the hypothesis distribution matches the oracle distribution, thus encouraging
the generated responses to spread out to reﬂect the true diversity of real responses. To promote more
diversity, Zhang et al. explicitly optimize a variational lower bound on pairwise mutual information between query and response to encourage generating more informative responses during
training time.
Serban et al. presented a latent Variable Hierarchical Recurrent Encoder-Decoder (VHRED)
model that also aims to generate less bland and more speciﬁc responses. It extends the HRED
model described previously in this chapter, by adding a high-dimensional stochastic latent variable
to the target. This additional latent variable is meant to address the challenge associated with the
shallow generation process. As noted in , this process is problematic from an
inference standpoint because the generation model is forced to produce a high-level structure—i.e.,
an entire response—on a word-by-word basis. This generation process is made easier in the VHRED
model, as the model exploits a high-dimensional latent variable that determines high-level aspects
of the response (topic, names, verb, etc.), so that the other parts of the model can focus on lowerlevel aspects of generation, e.g., ensuring ﬂuency. The VHRED model incidentally helps reducing
blandness as suggested by sample outputs of . Indeed, as the content of the
response is conditioned on the latent variable, the generated response is only bland and devoid of
semantic content if the latent variable determines that the response should be as such. More recently,
Zhang et al. presented a model that also introduces an additional variable (modeled using a
Gaussian kernel layer), which is added to control the level of speciﬁcity of the response, going from
bland to very speciﬁc.
While most response generation systems surveyed earlier in this chapter are generation-based (i.e.,
generating new sentences word-by-word), a more conservative solution to mitigating blandness is
to replace generation-based models with retrieval-based models for response generation , in
which the pool of possible responses is constructed in advance (e.g., pre-existing human responses).
These approaches come at the cost of reduced ﬂexibility: In generation, the set of possible responses
grows exponentially in the number of words, but the set of responses of a retrieval system is ﬁxed,
and as such retrieval systems often do not have any appropriate responses for many conversational
inputs. Despite this limitation, retrieval systems have been widely used in popular commercial
systems, and we survey them in Chapter 6.
Speaker Consistency
It has been shown that the popular seq2seq approach often produces conversations that are incoherent , where the system may for instance contradict what it had just said in the
previous turn (or sometimes even in the same turn). While some of this effect can be attributed to
the limitation of the learning algorithms, Li et al. suggested that the main cause of this inconsistency is probably due to the training data itself. Indeed, conversational datasets (see Sec. 5.5)
feature multiple speakers, which often have different or conﬂicting personas and backgrounds. For
example, to the question “how old are you?”, a seq2seq model may give valid responses such as
“23”, “27”, or “40”, all of which are represented in the training data.
This sets apart the response generation task from more traditional NLP tasks: While models for
other tasks such as machine translation are trained on data that is mostly one-to-one semantically,
conversational data is often one-to-many or many-to-many as the above example implies.5 As oneto-many training instances are akin to noise to any learning algorithm, one needs more expressive
models that exploits a richer input to better account for such diverse responses.
To do this, Li et al. proposed a persona-based response generation system, which is an
extension of the LSTM model of Sec. 5.1.1 that uses speaker embeddings in addition to word embeddings. Intuitively, these two types of embeddings work similarly: while word embeddings form
5Conversational data is also many-to-one, for example with multiple semantically-unrelated inputs that map
to “I don’t know.”
Word embeddings (50k)
Speaker embeddings (70k)
england Rob
skinnyoflynny2
Dreamswalls
kierongillen5
TheCharlieZ
The_Football_Bar
This_Is_Artful
DigitalDan285
Bob_Kelly2
Figure 5.2: Persona-based response generation system. Figure credit: Li et al. 
a latent space in which spacial proximity (i.e., low Euclidean distance) means two words are semantically or functionally close, speaker embeddings also constitute a latent space in which two nearby
speakers tend to converse in the same way, e.g., having similar speaking styles (e.g., British English)
or often talking about the same topic (e.g., sports).
Like word embeddings, speaker embedding parameters are learned jointly with all other parameters
of the model from their one-hot representations. At inference time, one just needs to specify the
one-hot encoding of the desired speaker to produce a response that reﬂects her speaking style. The
global architecture of the model is displayed in Fig. 5.2, which shows that each target hidden state is
conditioned not only on the previous hidden state and the current word embedding (e.g., “England”),
but also on the speaker embedding (e.g., of “Rob”). This model not only helps generate more
personalized responses, but also alleviates the one-to-many modeling problem mentioned earlier.
Other approaches also utilized personalized information. For example, Al-Rfou et al. presented a persona-based response generation model, but geared for retrieval using an extremely large
dataset consisting of 2.1 billion responses. Their retrieval model is implemented as a binary classi-
ﬁer (i.e., good response or not) using a deep neural network. The distinctive feature of their model
is a multi-loss objective, which augments a single-loss model p(R|I, A, C) of the response R, input
I, speaker (“author”) A, and context C, by adding auxiliary losses that, e.g., model the probability
of the response given the author p(R|A). This multi-loss model was shown to be quite helpful , as the multiple losses help cope with the fact that certain traits of the author are
often correlated with the context or input, which makes it difﬁcult to learn good speaker embedding
representation. By adding a loss for p(R|A), the model is able to learn a more distinctive speaker
embedding representation for the author.
More recently, Luan et al. presented an extension of the speaker embedding model of Li et al.
 , which combines a seq2seq model trained on conversational datasets with an autoencoder
trained on non-conversational data, where the seq2seq and autoencoder are combined in a multitask learning setup . The tying of the decoder parameters of both seq2seq and
autoencoder enables Luan et al. to train a response generation system for a given persona
without actually requiring any conversational data available for that persona. This is an advantage
of their approach, as conversational data for a given user or persona might not always be available.
In , the idea of is extended to a social-graph embedding model.
While is not a persona-based response generation model per se, their work
shares some similarities with speaker embedding models such as . Indeed, both
Li et al. and Serban et al. introduced a continuous high-dimensional variable in the
target side of the model in order to bias the response towards information encoded in a vector. In
the case of , that variable is latent, and is trained by maximizing a variational
lower-bound on the log-likelihood. In the case of , the variable (i.e., the speaker
embedding) is technically also latent, although it is a direct function of the one-hot representation
of speaker. might be a good ﬁt when utterance-level information (e.g., speaker ID
or topic) is available. On the other hand, the strength of is that it learns a latent
variable that best “explains” the data, and may learn a representation that is more optimal than the
one based strictly on speaker or topic information.
Word Repetitions
Word or content repetition is a common problem with neural generation tasks other than machine
translation, as has been noted with tasks such as response generation, image captioning, visual story
generation, and general language modeling . While machine translation is a relatively one-to-one task where each piece of information in
the source (e.g., a name) is usually conveyed exactly once in the target, other tasks such as dialogue
or story generation are much less constrained, and a given word or phrase in the source can map
to zero or multiple words or phrases in the target. This effectively makes the response generation
task much more challenging, as generating a given word or phrase doesn’t completely preclude the
need of generating the same word or phrase again. While the attention model helps prevent repetition errors in machine translation as that task is relatively one-to-one,6 the
attention models originally designed for machine translation often do not help reduce word repetitions in dialogue.
In light of the above limitations, Shao et al. proposed a new model that adds self-attention to
the decoder, aiming at improving the generation of longer and coherent responses while incidentally
mitigating the word repetition problem. Target-side attention helps the model more easily keep
track of what information has been generated in the output so far,7 so that the model can more easily
discriminate against unwanted word or phrase repetitions.
Further Challenges
The above issues are signiﬁcant problems that have only been partially solved and that require further investigation. However, a much bigger challenge faced by these E2E systems is response appropriateness. As explained in Chapter 1, one of the most distinctive characteristics of earlier E2E
systems, when compared to traditional dialogue systems, is their lack of grounding. When asked
“what is the weather forecast for tomorrow?”, E2E systems are likely to produce responses such
as “sunny” and “rainy”, without a principled basis for selecting one response or the other, as the
context or input might not even specify a geographical location. Ghazvininejad et al. argued
that seq2seq and similar models are usually quite good at producing responses that have plausible
overall structure, but often struggle when it comes to generating names and facts that connect to the
real world, due to the lack of grounding. In other words, responses are often pragmatically correct
(e.g., a question would usually be followed by an answer, and an apology by a downplay), but the
semantic content of the response is often inappropriate. Hence, recent research in E2E dialogue has
increasingly focused on designing grounded neural conversation models, which we will survey next.
Grounded Conversation Models
Unlike task-oriented dialogue systems, most E2E conversation models are not grounded in the real
world, which prevents these systems from effectively conversing about anything that relates to the
user’s environment. This limitation is also inherited from machine translation, which neither models
nor needs are grounded. Recent approaches to neural response generation address this problem
by grounding systems in the persona of the speaker or addressee , textual knowledge sources such as Foursquare , the user’s or
agent’s visual environment , and affect or emotion of
the user . At a high level, most of these works
have in common the idea of augmenting their context encoder to not only represent the conversation
history, but also some additional input drawn from the user’s environment, such as an image or textual information .
6Ding et al. indeed found that word repetition errors, usually few in machine translation, are often
caused by incorrect attention.
7A seq2seq model can also keep track of what information has been generated so far. However, this becomes
more difﬁcult as contexts and responses become longer, as a seq2seq hidden state is a ﬁxed-size vector.
Consistently the best omakase
CONTEXTUALLY
RELEVANT FACTS
Amazing sushi tasting […]
They were out of kaisui […]
Kusakabe tonight
CONVERSATION HISTORY
Try omakase, the
best in town
Figure 5.3: A neural conversation model grounded in “facts” relevant to the current conversation.
Figure credit: Ghazvininejad et al. 
As an illustrative example of such grounded models, we give a brief overview of Ghazvininejad
et al. , whose underlying model is depicted in Fig. 5.3. The model mainly consists of two
encoders and one decoder. The decoder and the dialogue encoder are similar to those of standard
seq2seq models. The additional encoder is called the facts encoder, which infuses into the model
factual information or so-called facts relevant to the conversation history, e.g., restaurant reviews
(e.g., “amazing sushi tasting”) that pertain to a restaurant that happened to be mentioned in the
conversation history (e.g., “Kusakabe”). While the model in this work was trained and evaluated
with Foursquare reviews, this approach makes no speciﬁc assumption that the grounding consists
of reviews, or that trigger words are restaurants (in fact, some of the trigger words are, e.g., hotels
and museums). To ﬁnd facts that are relevant to the conversation, their system uses an IR system
to retrieve text from a very large collection of facts or world facts (e.g., all Foursquare reviews of
several large cities) using search words extracted from the conversation context. While the dialogue
encoder of this model is a standard LSTM, the facts encoder is an instance of the Memory Network of Chen et al. , which uses an associative memory for modeling the facts relevant to a
particular problem, which in this case is a restaurant mentioned in a conversation.
There are two main beneﬁts to this approach and other similar work on grounded conversation
modeling. First, the approach splits the input of the E2E system into two parts: the input from
the user and the input from her environment. This separation is crucial because it addresses the
limitation of earlier E2E (e.g., seq2seq) models which always respond deterministically to the same
query (e.g.to “what’s the weather forecast for tomorrow?”). By splitting input into two sources
(user and environment), the system can effectively generate different responses to the same user
input depending on what has changed in the real world, without having to retrain the entire system.
Second, this approach is much more sample efﬁcient compared to a standard seq2seq approach.
For an ungrounded system to produce a response like the one in Fig. 5.3, the system would require
that every entity any user might conceivably talk about (e.g., “Kusakabe” restaurant) be seen in
the conversational training data, which is an unrealistic and impractical assumption. While the
amount of language modeling data (i.e., non-conversational data) is abundant and can be used to
train grounded conversation systems (e.g., using Wikipedia, Foursquare), the amount of available
conversational data is typically much more limited. Grounded conversational models don’t have
that limitation, and, e.g., the system of Ghazvininejad et al. can converse about venues that
are not even mentioned in the conversational training data.
Beyond Supervised Learning
There is often a sharp disconnect between conversational training data (human-to-human) and envisioned online scenarios (human-computer). This makes it difﬁcult to optimize conversation models
towards speciﬁc objectives, e.g., maximizing engagement by reducing blandness. Another limitation of the supervised learning setup is their tendency to optimize for an immediate reward (i.e., one
Figure 5.4: Deep reinforcement learning for response generation, pitching the system to optimize
against a user simulator (both systems are E2E generation systems.) Figure credit: Li et al. 
response at a time) rather than a long-term reward. This also partially explains why their responses
are often bland and thus fail to promote long-term user engagement. To address these limitations,
some researchers have explored reinforcement learning (RL) for E2E systems 
which could be augmented with human-in-the-loop architectures . Unlike RL
for task-oriented dialogue, a main challenge that E2E systems are facing is the lack of well-deﬁned
metrics for success (i.e., reward functions), in part because they have to deal with informal genres
such as chitchat, where the user goal is not explicitly speciﬁed.
Li et al. constitutes the ﬁrst attempt to use RL in a fully E2E approach to conversational
response generation. Instead of training the system on human-to-human conversations as in the
supervised setup of , the system of Li et al. is
trained by conversing with a user simulator which mimics human users’ behaviors.
As depicted in Fig. 5.4, human users have to be replaced with a user simulator because it is prohibitively expensive to train an RL system using thousands or tens of thousands of turns of real user
dialogues. In this work, a standard seq2seq model is used as a user simulator. The system is trained
using policy gradient (Sec. 2.3). The objective is to maximize the expected total reward over the
dialogues generated by the user simulator and the agent to be learned. Formally, the objective is
J(θ) = E[R(T1, T2, . . . , TN)]
where R(.) is the reward function, and Ti’s are dialogue turns. The above objective can be optimized
using gradient descent, by factoring the log probability of the conversation and the aggregated reward, which is independent of the model parameters:
∇J(θ) = ∇log p(T1, T2, . . . , TN)R(T1, T2, ..., TN)
p(Ti|Ti−1)R(T1, T2, ..., TN)
where p(Ti|Ti−1) is parameterized the same way as the standard seq2seq model of Sec. 5.1.1, except
that the model here is optimized using RL. The above gradient is often approximated using sampling,
and Li et al. used a single sampled conversation for each parameter update. While the above
policy gradient setup is relatively common in RL, the main challenge in learning dialogue models
is how to devise an effective reward function. Li et al. used a combination of three reward
functions that are designed to mitigate the problems of the supervised seq2seq model, which was
used in their work as initialization parameters. The three reward functions are:
• −p(Dull Response|Ti): Li et al. created a short list of dull responses such as “I
don’t know” selected from the training data. This reward function penalizes those turns Ti
that are likely to lead to any of these dull responses. This is called the ease of answering
reward, as it promotes conversational turns that are not too difﬁcult to respond to, so as
to keep the user engaged in the conversation. For example, the reward function gives a
very low reward to turns whose response is “I don’t know”, as this evasive response indicates that the previous turn was difﬁcult to respond to, which may ultimately terminate the
conversation.
• −log Sigmoid cos(Ti−1, Ti): This information ﬂow reward function ensures that consecutive turns Ti−1 and Ti are not very similar to each other (e.g., “how are you?” followed by
“how are you?”), as Li et al. assumed that conversations with little new information
are often not engaging and therefore more likely to be terminated.
• log p(Ti−1|Ti) + log p(Ti|Ti−1): This meaningfulness reward function was mostly introduced to counterbalance the aforementioned two rewards. For example, the two other reward functions prefer the type of conversations that constantly introduce new information
and change topics so frequently that users ﬁnd them hard to follow. To avoid this, the
meaningfulness reward encourages consecutive turns in a dialogue session to be related to
each other.
Serban et al. presented a comprehensive survey of existing datasets that are useful beyond the
E2E and social bot research. What distinguishes E2E conversation modeling from other NLP and
dialogue tasks is that data is available in very large quantities, thanks in part to social media (e.g.,
Twitter and Reddit). On the other hand, most of this social media data is neither redistributable
nor available through language resource organizations (such as the Linguistic Data Consortium),
which means there are still no established public datasets (either with Twitter or Reddit) for training
and testing response generation systems. Although these social media companies offer API access
to enable researchers to download social media posts in relatively small quantities and then to reconstruct conversations from them, the strict legal terms of the service speciﬁed by these companies
inevitably affect the reproducibility of the research. Most notably, Twitter makes certain tweets
(e.g., retracted tweets or tweets from suspended user) unavailable through the API and requires that
any such previously downloaded tweets be deleted. This makes it difﬁcult to establish any standard
training or test datasets, as these datasets deplete over time.8 Consequently, in most of the papers
cited in this chapter, their authors have created their own (subsets of) conversational data for training
and testing, and then evaluated their systems against baselines and competing systems on these ﬁxed
datasets. Dodge et al. used an existing dataset to deﬁne standard training and test sets, but it
is relatively small. Some of the most notable E2E and chitchat datasets include:
• Twitter: Used since the ﬁrst data-driven response generation systems ,
Twitter data offers a wealth of conversational data that is practically unbounded, as Twitter
produces new data each day that is more than most system developers can handle.9 While
the data itself is made accessible through the Twitter API as individual tweets, its metadata
easily enables the construction of conversation histories, e.g., between two users. This
dataset forms the basis of the DSTC Task 2 competition in 2017 .
• Reddit: Reddit is a social media source that is also practically unbounded, and represents
about 3.2 billion dialogue turns as of July 2018. It was for example used in Al-Rfou et al.
 to build a large-scale response retrieval system. Reddit data is organized by topics
(i.e.“subreddits”), and its responses don’t have a character limit as opposed to Twitter.
• OpenSubtitles: This dataset consists of subtitles made available on the opensubtitles.org
website. It offers captions of many commercial movies, and contains about 8 billion words
as of 2011 in multiple languages .
• Ubuntu: The Ubuntu dataset has also been used extensively for E2E
conversation modeling. It differs from other datasets such as Twitter in that it is less focused
on chitchat but more goal-oriented, as it contains many dialogues that are speciﬁc to the
Ubuntu operating system.
• Persona-Chat dataset: This crowdsourced dataset was developed to
meet the need for conversational data where dialogues exhibit distinct user personas. In
collecting Persona-Chat, every crowdworker was asked to impersonate a given character
8Anecdotally, the authors of Li et al. found that a Twitter dataset from 2013 had lost about 25%
of its tweets by 2015 due to retracted tweets and Twitter account suspensions.
9For example, the latest ofﬁcial statistics from Twitter, dating back from 2014, states that Twitter users
post on average more than 500 million tweets per day: 
2014/the-2014-yearontwitter.html
described using ﬁve facts. Then that worker took part in dialogues while trying to stay in
character. The resulting dataset contains about 160k utterances.
Evaluation
Evaluation is a long-standing research topic for generation tasks such as machine translation and
summarization. E2E dialogue is no different. While it is common to evaluate response generation
systems using human raters , this
type of evaluation is often expensive and researchers often have to resort to automatic metrics for
quantifying day-to-day progress and for performing automatic system optimization. E2E dialogue
research mostly borrowed those metrics from machine translation and summarization, using string
and n-gram matching metrics like BLEU and ROUGE . Proposed more recently, METEOR aims to improve BLEU by identifying
synonyms and paraphrases between the system output and the human reference, and has also been
used to evaluate dialogue. deltaBLEU is an extension of BLEU that exploits
numerical ratings associated with conversational responses.
There has been signiﬁcant debate as to whether such automatic metrics are actually appropriate for
evaluating conversational response generation systems. For example, Liu et al. argued that
they are not appropriate by showing that most of these machine translation metrics correlate poorly
with human judgments. However, their correlation analysis was performed at the sentence level, but
decent sentence-level correlation has long been known to be difﬁcult to achieve even for machine
translation , the task for which the underlying
metrics (e.g., BLEU and METEOR) were speciﬁcally intended.10 In particular, BLEU was designed from the outset to be used as a corpus-level rather than sentence-level
metric, since assessments based on n-gram matches are brittle when computed on a single sentence.
Indeed, the empirical study of Koehn suggested that BLEU is not reliable on test sets consisting of fewer than 600 sentences. Koehn ’s study was on translation, a task that is arguably
simpler than response generation, so the need to move beyond sentence-level correlation is probably even more critical in dialogue. When measured at a corpus- or system-level, correlations are
typically much higher than that at sentence-level , e.g., with Spearman’s ρ
above 0.95 for the best metrics on WMT translation tasks .11 In the
case of dialogue, Galley et al. showed that the correlation of string-based metrics (BLEU and
deltaBLEU) signiﬁcantly increases with the units of measurement bigger than a sentence. Speciﬁcally, their Spearman’s ρ coefﬁcient goes up from 0.1 (essentially no correlation) at sentence-level
to nearly 0.5 when measuring correlation on corpora of 100 responses each.
Recently, Lowe et al. proposed a machine-learned metric for E2E dialogue evaluation. They
presented a variant of the VHRED model that takes context, user input, gold and
system responses as input, and produces a qualitative score between 1 and 5. As VHRED is effective
for modeling conversations, Lowe et al. was able to achieve an impressive Spearman’s ρ correlation of 0.42 at the sentence level. On the other hand, the fact that this metric is trainable leads to
other potential problems such as overﬁtting and “gaming of the metric” ,12
which might explain why previously proposed machine-learned evaluation metrics computed the percentage of times popular metrics are consistent with human ranking at the sentence level, but the
results did not bode well for sentence-level studies: “Many metrics failed to reach [a random] baseline (including most metrics in the out-of-English direction). This indicates that sentence-level evaluation of machine
translation quality is very difﬁcult.”
11In one of the largest scale system-level correlation studies to date, Graham and Baldwin found that
BLEU is relatively competitive against most translation metrics proposed more recently, as they show there “is
currently insufﬁcient evidence for a high proportion of metrics to conclude that they outperform BLEU”. Such
a large scale study remains to be done for dialogue.
12In discussing the potential pitfalls of machine-learned evaluation metrics, Albrecht and Hwa argued
for example that it would be “prudent to defend against the potential of a system gaming a subset of the
features.” In the case of deep learning, this gaming would be reminiscent of making non-random perturbations
to an input to drastically change the network’s predictions, as it was done, e.g., with images in to show how easily deep learning models can be fooled. However, preventing such a gaming is difﬁcult
if the machine-learned metric is to become a standard evaluation, and this would presumably require model
parameters to be publicly available.
et al., 2001; Kulesza and Shieber, 2004; Lita et al., 2005; Albrecht and Hwa, 2007; Gim´enez and
M`arquez, 2008; Pado et al., 2009; Stanojevi´c and Sima’an, 2014, etc.) are not commonly used in
ofﬁcial machine translation benchmarks. The problem of “gameable metrics” is potentially serious,
for example in the frequent cases where automatic evaluation metrics are used directly as training
objectives as unintended “gaming” may occur unbeknownst to the
system developer. If a generation system is optimized directly on a trainable metric, then the system
and the metric become akin to an adversarial pair in GANs , where the
only goal of the generation system (Generator) is to fool the metric (Discriminator). Arguably, such
attempts become easier with trainable metrics as they typically incorporate thousands or millions
of parameters, compared to a relatively parameterless metric like BLEU that is known to be fairly
robust to such exploitation and was shown to be the best metric for direct optimization among other established string-based metrics. To prevent machine-learned metrics from being
gamed, one would need to iteratively train the Generator and Discriminator as in GANs, but most
trainable metrics in the literature do not exploit this iterative process. Adversarial setups proposed
for dialogue and related tasks 
offer solutions to this problem, but it is also well-known that such setups suffer from instability due to the nature of GANs’ minimax formulation. This fragility is potentially
troublesome as the outcome of an automatic evaluation should ideally be stable 
and reproducible over time, e.g., to track progress of E2E dialogue research over the years. All of
this suggests that automatic evaluation for E2E dialogue is far from a solved problem.
Open Benchmarks
Open benchmarks have been the key to achieving progress in many AI tasks such as speech recognition, information retrieval, and machine translation. Although end-to-end conversational AI is a
relatively nascent research problem, some open benchmarks have already been developed:
• Dialog System Technology Challenges (DSTC): In 2017, DSTC proposed for the ﬁrst
time an “End-to-End Conversation Modeling” track,13 which requires systems to be fully
data-driven using Twitter data. Two of the tasks in the subsequent challenge (DSTC7) focus
on grounded conversation scenarios. One is focused on audio-visual scene-aware dialogue
and the other on response generation grounded in external knowledge (e.g., Foursquare and
Wikipedia), with conversations extracted from Reddit.14
• ConvAI Competition: This is a NIPS competition that has been featured so far at two
conferences. It offers prizes in the form of Amazon Mechanical Turk funding. The competition aims at “training and evaluating models for non-goal-oriented dialogue systems”,
and in 2018 uses the Persona-Chat dataset , among other datasets.
• NTCIR STC: This benchmark focuses on conversation “via short texts”.
benchmark focused on retrieval-based methods, and in 2017 was expanded to evaluate
generation-based approaches.
• Alexa Prize: In 2017, Amazon organized an open competition on building “social bots”
that can converse with humans on a range of current events and topics. The competition
enables participants to test their systems with real users (Alexa users), and offers a form of
indirect supervision as users are asked to rate each of their conversations with each of the
Alexa Prize systems. The inaugural prize featured 15 academic teams .15
13 
14 
15These 15 systems are described in the online proceeding:
 
alexaprize/proceedings
Conversational AI in Industry
This chapter pictures the landscape of conversational systems in industry, including task-oriented
systems (e.g., personal assistants), QA systems, and chatbots.
Question Answering Systems
Search engine companies, including Google, Microsoft and Baidu, have incorporated multi-turn
QA capabilities into their search engines to make user experience more conversational, which is
particularly appealing for mobile devices. Since relatively little is publicly known about the internals
of these systems (e.g., Google and Baidu), this section presents a few example commercial QA
systems whose architectures have been at least partially described in public source, including Bing
QA, Satori QA and customer support agents.
Bing QA is an example of the Web-scale text-QA agents. It is an extension of the Microsoft Bing
Web search engine. Instead of returning ten blue links, Bing QA generates a direct answer to a
user query by reading the passages retrieved by the Bing Web search engine using MRC models, as
illustrated in Fig. 6.1.
The Web QA task that Bing QA is dealing with is far more challenging than most of the academic
MRC tasks described in Chapter 3. For example, Web QA and SQuAD differs in:
• Scale and quality of the text collection. SQuAD assumes the answer is a text span in a
passage which is a clean text section from a Wikipedia page. Web QA needs to identify
an answer from billions of Web documents which consist of trillions of noisy passages that
often contain contradictory, wrong, obsolete information due to the dynamic nature of Web
• Runtime latency. In an academic setting, an MRC model might take seconds to read and
re-read documents to generate an answer, while in the Web QA setting the MRC part (e.g.,
in Bing QA) is required to add no more than 10 mini seconds to the entire serving stack.
• User experience. While SQuAD MRC models provide a text span as an answer, Web QA
needs to provide different user experiences depending on different devices where the answer is shown, e.g., a voice answer in a mobile device or a rich answer in a Search Engine
Result Page (SERP). Fig. 6.1 (Right) shows an example of the SERP for the question “what
year did Disney buy lucasﬁlms?”, where Bing QA presents not only the answer as a highlighted text span, but also various supporting evidence and related Web search results (i.e.,
captions of retrieved documents, passages, audios and videos) that are consistent with the
As a result, a commercial Web QA agent such as Bing QA often incorporates a MRC module as a
post-web component on top of its Web search engine stack. An overview of the Bing QA agent is
Figure 6.1: (Left) An overview of the Bing QA architecture. (Right) An example of a search engine
result page of the question “what year did disney buy lucasﬁlms?”. Example graciously provided by
Rangan Majumder.
illustrated in Fig. 6.1 (Left). Given the question “what year did Disney buy lucasﬁlms?”, a set of
candidate documents are retrieved from Web Index via a fast, primary ranker. Then in the Document
Ranking module, a sophisticated document ranker based on boosted trees is used
to assign relevance scores for these documents. The top-ranked relevant documents are presented
in a SERP, with their captions generated from a Query-Focused Captioning module, as shown in
Fig. 6.1 (Right). The Passage Chunking module segments the top documents into a set of candidate
passages, which are further ranked by the Passage Ranking module based on another passage-level
boosted trees ranker . Finally, the MRC module identiﬁes the answer span “2012”
from the top-ranked passages.
Although turning Bing QA into a conversational QA agent of Sec. 3.8 requires the integration of
additional components such as dialogue manager, which is a nontrivial ongoing engineering effort,
Bing QA can already deal with conversational queries (e.g., follow up questions) using a Conversational Query Understanding (CQU) module . As the example in Fig. 6.2, CQU
reformulates a conversational query into a search engine friendly query in two steps: (1) determine
whether a query depends upon the context in the same search session (i.e., previous queries and
answers), and (2) if so, rewrite that query to include the necessary context e.g., replace “its” with
“California” in Q2 and add “Stanford” in Q5 in Fig. 6.2.
Satori QA is an example of the KB-QA agents, as described in Sec. 3.1–3.5. Satori is Microsoft’s
knowledge graph, which is seeded by Freebase, and now is several orders of magnitude larger than
Freebase. Satori QA is a hybrid system that uses both neural approaches and symbolic approaches.
It generates answers to factual questions.
Similar to Web QA, Satori QA has to deal with the issues regarding scalability, noisy content, speed,
etc. One commonly used design strategy of improving system’s robustness and runtime efﬁciency is
to decompose a complex question into a sequence of simpler questions, which can be answered more
easily by a Web-scale KB-QA system, and compute the ﬁnal answer by recomposing the sequence
of answers, as exempliﬁed in Fig. 6.3 .
Customer Support Agents
Several IT companies, including Microsoft and Salesforce, have developed a variety of customer
support agents. These agents are multi-turn conversational KB-QA agents, as described in 3.5.
Figure 6.2: An example query session, where some queries are rewritten to include context information via the CQU module as indicated by the arrows. Examples adapted from Ren et al. .
Figure 6.3: Given a complex question Q, we decompose it to a sequence of simple questions
Q1, Q2, ..., use a Web-scale KB-QA agent to generate for each Qi an answer Ai, from which we
compute the ﬁnal answer A. Figure credit: Talmor and Berant .
Given a user’s description of a problem e.g., “cannot update the personal information of my account”, the agent needs to recommend a pre-compiled solution or ask a human agent to help. The
dialogue often consists of multiple turns as the agent asks the user to clarify the problem while
navigating the knowledge base to ﬁnd the solution. These agents often take both text and voice as
Task-oriented Dialogue Systems (Virtual Assistants)
Commercial task-oriented dialogue systems nowadays often reside in smart phones, smart speakers
and personal computers. They can perform a range of tasks or services for a user, and are sometimes
referred to as virtual assistants or intelligent personal assistants. Some of the example services
are providing weather information, setting alarms, and calling center support. In the US, the most
widely used systems include Apple’s Siri, Google Assistant, Amazon Alexa, and Microsoft Cortana,
among others. Users can interact with them naturally through voice, text or images. To activate a
virtual assistant using voice, a wake word might be used, such as “OK Google.”
Figure 6.4: Architecture of Task Completion Platform. Figure credit: Crook et al. .
There are also a number of fast-growing tools available to facilitate the development of virtual assistants, including Amazon’s Alexa Skills Kit1, IBM’s Watson Assistant2, and similar offerings from
Microsoft and Google, among others. A comprehensive survey is outside of the scope of this section, and not all information of such tools is publicly available. Here, we will give a high-level
description of a sample of them:
• The Task Completion Platform (TCP) of Microsoft is a platform for
creating multi-domain dialogue systems. As shown in Fig. 6.4, TCP follows a similar
structure as in Fig. 4.1, containing language understanding, state tracking, and a policy.
A useful feature of TCP is a task conﬁguration language, TaskForm, which allows the
deﬁnitions of individual tasks to be decoupled from the platform’s overarching dialogue
policy. TCP is used to power many of the multi-turn dialogues supported by the Cortana
personal assistant.
• Another tool from Microsoft is LUIS, a cloud-based API service for natural language understanding3. It provides a suite of pre-built domains and intentions, as well as a convenient
interface for a non-expert to use machine learning to obtain an NLU model by providing
training examples. Once a developer creates and publishes a LUIS app, the app can be used
as a NLU blackbox module by a client dialogue system: the client sends a text utterance to
the app, which will return language understanding results in the JSON format, as illustrated
in Fig. 6.5.
• While LUIS focuses on language understanding, the Azure Bot Service4 allows developers
to build, test, deploy, and manage dialogue systems in one place. It can take advantages of a
suite of intelligent services, including LUIS, image captioning, speech-to-text capabilities,
among others.
• DialogFlow is Google’s development suite for creating dialogue systems on websites, mobile and IoT devices.5 Similar to the above tools, it provides mechanisms to facilitate
development of various modules of a dialogue system, including language understanding
and carrying information over multiple turns. Furthermore, it can deploy a dialogue system
as an action that users can invoke through Google Assistant.
1 
2 
3 
4 
5 
Figure 6.5: Use of LUIS by a client dialogue system. Figure credit: 
com/en-us/azure/cognitive-services/LUIS .
There have been publicly-available conversational systems going back many decades . Those precursors of today’s chatbot systems relied heavily on hand-crafted
rules, and are very different from the data-driven conversational AI systems discussed in Chapter 5.
Nowadays publicly available and commercial chatbot systems are often a combination of statistical
methods and hand-crafted components, where statistical methods provide robustness to conversational systems (e.g., via intent classiﬁers) while rule-based components are often still used in practice, e.g., to handle common chitchat queries (e.g., “tell me a joke”). Examples include personal
assistants like Amazon’s Alexa, Google Assistant, Facebook M, and Microsoft’s Cortana, which
in addition to personal assistant skills are able to handle chitchat user inputs. Other commercial
systems such as XiaoIce,6 Replika, Zo,7 and Ruuh8 focus almost entirely
on chitchat. Since relatively little is publicly known about the internals of main commercial systems (Alexa, Google Assistant, etc.), the rest of this section focuses on commercial systems whose
architecture have been at least partially described in some public source.
One of the earliest such systems is XiaoIce, which was initially released in 2014. XiaoIce is designed
as an AI companion with an emotional connection to satisfy the human need for communication,
affection, and social belonging . The overall architecture of XiaoIce is shown in
Fig. 6.6. It consists of three layers.
• User experience layer: It connects XiaoIce to popular chat platforms (e.g., WeChat, QQ),
and deals with conversations in two communication modes. The full-duplex module handles voice-stream-based conversations where both a user and XiaoIce can talk to each other
simultaneously. The other module deals with message-based conversations where a user
and XiaoIce have to take turns to talk.
• Conversation engine layer: In each dialogue turn, the dialogue state is ﬁrst updated using
the state tracker, and either Core Chat (and a topic) or a dialogue skill is selected by the
dialogue policy to generate a response. A unique component of XiaoIce is the empathetic
computing module, designed to understand not only the content of the user input (e.g.,
topic) but also the empathy aspects (e.g., emotion, intent, opinion on topic, and the user’s
background and general interests), to ensure the generation of an empathetic response that
ﬁts XiaoIce’s persona. Another central module, Core Chat, combines neural generation
techniques (Sec. 5.1) and retrieval-based methods . As Fig. 6.7 show,
XiaoIce is capable of generating socially attractive responses (e.g., having a sense of humor, comforting, etc.), and can determine whether to drive the conversation when, e.g.,
6 
7 
8 
Figure 6.6: XiaoIce system architecture. Figure credit: Zhou et al. 
Figure 6.7: Conversation between a user and XiaoIce. The empathy model provides a context-aware
strategy that can drive the conversation when needed.
the conversation is somewhat stalled, or whether to perform active listening when the user
herself is engaged.9
• Data layer: It consists of a set of databases that store the collected human conversational
data (in text pairs or text-image pairs), non-conversational data and knowledge graphs used
for Core Chat and skills, and the proﬁles of XiaoIce and all the registered users for empathetic computing.
The Replika system for chitchat combines neural generation and retrievalbased methods, and is able to condition responses on images as in . The
neural generation component of Replika is persona-based , as it is trained to mimic
speciﬁc characters. While Replika is a company, the Replika system has been open-sourced10 and
can thus be used as a benchmark for future research.
Alexa Prize systems are social chatbots that are exposed to real users, and as
such anyone with an Alexa device is able to interact with these social bots and give them ratings.
This interaction is triggered with the “Alexa, let’s chat” command, which then triggers a free-form
conversation about any topic selected by either the user or the system. These systems featured
not only fully data-driven approaches, but also more engineered and modularized approaches. For
example, the winning system of the 2017 competition (Sounding Board11) contained a chitchat
9 
10 
11 
component as well as individual “miniskills” enabling the system to handle distinct tasks (e.g., QA)
and topics (e.g., news, sports). Due to the diversity of systems in the Alexa prize, it would be
impractical to overview these systems in this survey, and instead we refer the interested reader to the
Alexa Prize online proceedings .
Conclusions and Research Trends
Conversational AI is a rapidly growing ﬁeld. This paper surveys neural approaches that were recently developed. Some of them have already been widely used in commercial systems.
• Dialogue systems for question answering, task completion, chitchat and recommendation
etc. can be conceptualized using a uniﬁed mathematical framework of optimal decision
process. The neural approaches to AI, developed in the last few years, leverage the recent
breakthrough in RL and DL to signiﬁcantly improve the performance of dialogue agents
across a wide range of tasks and domains.
• A number of commercial dialogue systems allow users to easily access various services and
information via conversation. Most of these systems use hybrid approaches that combine
the strength of symbolic methods and neural models.
• There are two types of QA agents. KB-QA agents allow users to query large-scale knowledge bases via conversation without composing complicated SQL-like queries. Text-QA
agents, equipped with neural MRC models, are becoming more popular than traditional
search engines (e.g., Bing and Google) for the query types to which users expect a concise
direct answer.
• Traditional task-oriented systems use handcrafted dialogue manager modules, or shallow
machine-learning models to optimize the modules separately. Recently, researchers have
begun to explore DL and RL to optimize the system in a more holistic way with less domain
knowledge, and to automate the optimization of systems in a changing environment such
that they can efﬁciently adapt to different tasks, domains and user behaviors.
• Chatbots are important in facilitating smooth and natural interaction between humans and
their electronic devices. More recent work focuses on scenarios beyond chitchat, e.g.,
recommendation. Most state-of-the-art chatbots use fully data-driven and end-to-end generation of conversational responses within the framework of neural machine translation.
We have discussed some of the main challenges in conversational AI, common to Question Answering agents, task-oriented dialogue bots and chatbots.
• Towards a uniﬁed modeling framework for dialogues: Chapter 1 presents a uniﬁed view
where an open-domain dialogue is formulated as an optimal decision process. Although
the view provides a useful design principle, it remains to be proved the effectiveness of
having a uniﬁed modeling framework for system development. Microsoft XiaoIce, initially
designed as a chitchat system based on a retrieval engine, has gradually incorporated many
ML components and skills, including QA, task completion and recommendation, using a
uniﬁed modeling framework based on empathic computing and RL, aiming to maximize
user engagement in the long run, measured by expected conversation-turn per session. We
plan to present the design and development of XiaoIce in a future publication. McCann
et al. presented a platform effort of developing a uniﬁed model to handle various
tasks including QA, dialogue and chitchat.
• Towards fully end-to-end dialogue systems: Recent work combines the beneﬁt of taskoriented dialogue with more end-to-end capabilities. The grounded models discussed in
Sec. 5.3 represent a step towards more goal-oriented conversations, as the ability to interact
with the user’s environment is a key requirement for most goal-oriented dialogue systems.
Grounded conversation modeling discussed in this paper is still preliminary, and future
challenges include enabling API calls in fully data-driven pipelines.
• Dealing with heterogeneous data: Conversational data is often heterogeneous. For example, chitchat data is plentiful but not directly relevant to goal-oriented systems, and
goal-oriented conversational datasets are typically very small. Future research will need
to address the challenge of capitalizing on both, for example in a multi-task setup similar
to Luan et al. . Another research direction is the work of Zhao et al. , which
brought synergies between chitchat and task-oriented data using a “data augmentation”
technique. Their resulting system is not only able to handle chitchat, but also more robust
to goal-oriented dialogues. Another challenge is to better exploit non-conversational data
(e.g., Wikipedia) as part of the training of conversational systems (Ghazvininejad et al.,
• Incorporating EQ (or empathy) into dialogue: This is useful for both chatbots and QA
bots. For example, XiaoIce incorporates an EQ module so as to deliver a more understandable response or recommendation ). Fung et al. 
embedded an empathy module into a dialogue agent to recognize users’ emotion using
multimodality, and generate emotion-aware responses.
• Scalable training for task-oriented dialogues: It is important to fast update a dialogue
agent to handle a changing environment. For example, Lipton et al. proposed an
efﬁcient exploration method to tackle a domain extension setting, where new slots can be
gradually introduced. Chen et al. proposed a zero-shot learning for unseen intents
so that a dialogue agent trained on one domain can detect unseen intents in a new domain
without manually labeled data and without retraining.
• Commonsense knowledge is crucial for any dialogue agents. This is challenging because
common sense knowledge is often not explicitly stored in existing knowledge base. Some
new datasets are developed to foster the research on common sense reasoning, such as
Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) , Winograd Schema Challenge (WSC) and Choice
Of Plausible Alternatives (COPA) .
• Model interpretability: In some cases, a dialogue agent is required not only to give a
recommendation or an answer, but also provide explanations. This is very important in
e.g., business scenarios, where a user cannot make a business decision without justiﬁcation. Shen et al. ; Xiong et al. ; Das et al. combine the interpretability
of symbolic approaches and the robustness of neural approaches and develop an inference
algorithm on KB that not only improves the accuracy in answering questions but also provides explanations why the answer is generated, i.e., the paths in the KB that leads to the
answer node.