Learning Imbalanced Datasets with
Label-Distribution-Aware Margin Loss
Stanford University
 
Stanford University
 
Adrien Gaidon
Toyota Research Institute
 
Nikos Arechiga
Toyota Research Institute
 
Stanford University
 
Deep learning algorithms can fare poorly when the training dataset suffers from
heavy class-imbalance but the testing criterion requires good generalization on less
frequent classes. We design two novel methods to improve performance in such
scenarios. First, we propose a theoretically-principled label-distribution-aware
margin (LDAM) loss motivated by minimizing a margin-based generalization
bound. This loss replaces the standard cross-entropy objective during training
and can be applied with prior strategies for training with class-imbalance such as
re-weighting or re-sampling. Second, we propose a simple, yet effective, training
schedule that defers re-weighting until after the initial stage, allowing the model to
learn an initial representation while avoiding some of the complications associated
with re-weighting or re-sampling. We test our methods on several benchmark
vision tasks including the real-world imbalanced dataset iNaturalist 2018. Our
experiments show that either of these methods alone can already improve over
existing techniques and their combination achieves even better performance gains1.
Introduction
Modern real-world large-scale datasets often have long-tailed label distributions [Van Horn and
Perona, 2017, Krishna et al., 2017, Lin et al., 2014, Everingham et al., 2010, Guo et al., 2016, Thomee
et al., 2015, Liu et al., 2019]. On these datasets, deep neural networks have been found to perform
poorly on less represented classes [He and Garcia, 2008, Van Horn and Perona, 2017, Buda et al.,
2018]. This is particularly detrimental if the testing criterion places more emphasis on minority
classes. For example, accuracy on a uniform label distribution or the minimum accuracy among
all classes are examples of such criteria. These are common scenarios in many applications [Cao
et al., 2018, Merler et al., 2019, Hinnefeld et al., 2018] due to various practical concerns such as
transferability to new domains, fairness, etc.
The two common approaches for learning long-tailed data are re-weighting the losses of the examples
and re-sampling the examples in the SGD mini-batch .
They both devise a training loss that is in expectation closer to the test distribution, and therefore
can achieve better trade-offs between the accuracies of the frequent classes and the minority classes.
However, because we have fundamentally less information about the minority classes and the models
1Code available at 
33rd Conference on Neural Information Processing Systems , Vancouver, Canada.
 
Figure 1: For binary classiﬁcation with a linearly
separable classiﬁer, the margin γi of the i-th class
is deﬁned to be the the minimum distance of the
data in the i-th class to the decision boundary. We
show that the test error with the uniform label
distribution is bounded by a quantity that scales
γ2√n2 . As illustrated here, ﬁxing the
direction of the decision boundary leads to a ﬁxed
γ1 + γ2, but the trade-off between γ1, γ2 can be
optimized by shifting the decision boundary. As
derived in Section 3.1, the optimal trade-off is
where ni is the sample size of the
i-th class.
deployed are often huge, over-ﬁtting to the minority classes appears to be one of the challenges in
improving these methods.
We propose to regularize the minority classes more strongly than the frequent classes so that we
can improve the generalization error of minority classes without sacriﬁcing the model’s ability to ﬁt
the frequent classes. Implementing this general idea requires a data-dependent or label-dependent
regularizer — which in contrast to standard ℓ2 regularization depends not only on the weight matrices
but also on the labels — to differentiate frequent and minority classes. The theoretical understanding
of data-dependent regularizers is sparse 
We explore one of the simplest and most well-understood data-dependent properties: the margins
of the training examples. Encouraging a large margin can be viewed as regularization, as standard
generalization error bounds depend on the inverse of
the minimum margin among all the examples. Motivated by the question of generalization with
respect to minority classes, we instead study the minimum margin per class and obtain per-class and
uniform-label test error bounds.2 Minimizing the obtained bounds gives an optimal trade-off between
the margins of the classes. See Figure 1 for an illustration in the binary classiﬁcation case.
Inspired by the theory, we design a label-distribution-aware loss function that encourages the model
to have the optimal trade-off between per-class margins. The proposed loss extends the existing soft
margin loss [Wang et al., 2018a] by encouraging the minority classes to have larger margins. As a
label-dependent regularization technique, our modiﬁed loss function is orthogonal to the re-weighting
and re-sampling approach. In fact, we also design a deferred re-balancing optimization procedure that
allows us to combine the re-weighting strategy with our loss (or other losses) in a more efﬁcient way.
In summary, our main contributions are (i) we design a label-distribution-aware loss function to
encourage larger margins for minority classes, (ii) we propose a simple deferred re-balancing optimization procedure to apply re-weighting more effectively, and (iii) our practical implementation shows
signiﬁcant improvements on several benchmark vision tasks, such as artiﬁcially imbalanced CIFAR
and Tiny ImageNet [tin], and the real-world large-scale imbalanced dataset iNaturalist’18 [Van Horn
et al., 2018].
Related Works
Most existing algorithms for learning imbalanced datasets can be divided in to two categories:
re-sampling and re-weighting.
Re-sampling. There are two types of re-sampling techniques: over-sampling the minority classes and under-sampling the frequent classes The downside of under-sampling is
2The same technique can also be used for other test label distribution as long as the test label distribution is
known. See Section C.5 for some experimental results.
that it discards a large portion of the data and thus is not feasible when data imbalance is extreme.
Over-sampling is effective in a lot of cases but can lead to over-ﬁtting of the minority classes [Chawla
et al., 2002, Cui et al., 2019]. Stronger data augmentation for minority classes can help alleviate the
over-ﬁtting [Chawla et al., 2002, Zou et al., 2018].
Re-weighting. Cost-sensitive re-weighting assigns (adaptive) weights for different classes or even
different samples. The vanilla scheme re-weights classes proportionally to the inverse of their
frequency [Huang et al., 2016, 2019, Wang et al., 2017]. Re-weighting methods tend to make
the optimization of deep models difﬁcult under extreme data imbalanced settings and large-scale
scenarios [Huang et al., 2016, 2019]. Cui et al. observe that re-weighting by inverse class
frequency yields poor performance on frequent classes, and thus propose re-weighting by the inverse
effective number of samples. This is the main prior work that we empirically compare with.
Another line of work assigns weights to each sample based on their individual properties. Focal loss
[Lin et al., 2017] down-weights the well-classiﬁed examples; Li et al. suggests an improved
technique which down-weights examples with either very small gradients or large gradients because
examples with small gradients are well-classiﬁed and those with large gradients tend to be outliers.
In a recent work [Byrd and Lipton, 2019], Byrd and Lipton study the effect of importance weighting
and show that empirically importance weighting does not have a signiﬁcant effect when no regularization is applied, which is consistent with the theoretical prediction in [Soudry et al., 2018] that
logistical regression without regularization converges to the max margin solution. In our work, we
explicitly encourage rare classes to have higher margin, and therefore we don’t converge to a max
margin solution. Moreover, in our experiments, we apply non-trivial ℓ2-regularization to achieve the
best generalization performance. We also found deferred re-weighting (or deferred re-sampling) are
more effective than re-weighting and re-sampling from the beginning of the training.
In contrast, and orthogonally to these papers above, our main technique aims to improve the generalization of the minority classes by applying additional regularization that is orthogonal to the
re-weighting scheme. We also propose a deferred re-balancing optimization procedure to improve
the optimization and generalization of a generic re-weighting scheme.
Margin loss. The hinge loss is often used to obtain a “max-margin” classiﬁer, most notably in
SVMs [Suykens and Vandewalle, 1999]. Recently, Large-Margin Softmax [Liu et al., 2016], Angular
Softmax [Liu et al., 2017a], and Additive Margin Softmax [Wang et al., 2018a] have been proposed
to minimize intra-class variation in predictions and enlarge the inter-class margin by incorporating
the idea of angular margin. In contrast to the class-independent margins in these papers, our approach
encourages bigger margins for minority classes. Uneven margins for imbalanced datasets are also
proposed and studied in [Li et al., 2002] and the recent work [Khan et al., 2019, Li et al., 2019]. Our
theory put this idea on a more theoretical footing by providing a concrete formula for the desired
margins of the classes alongside good empirical progress.
Label shift in domain adaptation. The problem of learning imbalanced datasets can be also viewed
as a label shift problem in transfer learning or domain adaptation . In a typical label shift formulation, the
difﬁculty is to detect and estimate the label shift, and after estimating the label shift, re-weighting
or re-sampling is applied. We are addressing a largely different question: can we do better than
re-weighting or re-sampling when the label shift is known? In fact, our algorithms can be used to
replace the re-weighting steps of some of the recent interesting work on detecting and correcting
label shift [Lipton et al., 2018, Azizzadenesheli et al., 2019].
Distributionally robust optimization (DRO) is another technique for domain adaptation However, the
formulation assumes no knowledge of the target label distribution beyond a bound on the amount
of shift, which makes the problem very challenging. We here assume the knowledge of the test
label distribution, using which we design efﬁcient methods that can scale easily to large-scale vision
datasets with signiﬁcant improvements.
Meta-learning. Meta-learning is also used in improving the performance on imbalanced datasets
or the few shot learning settings. We refer the readers to [Wang et al., 2017, Shu et al., 2019, Wang
et al., 2018b] and the references therein. So far, we generally believe that our approaches that modify
the losses are more computationally efﬁcient than meta-learning based approaches.
Main Approach
Theoretical Motivations
Problem setup and notations.
We assume the input space is Rd and the label space is {1, . . . , k}.
Let x denote the input and y denote the corresponding label. We assume that the class-conditional
distribution P(x | y) is the same at training and test time. Let Pj denote the class-conditional
distribution, i.e. Pj = P(x | y = j). We will use Pbal to denote the balanced test distribution which
ﬁrst samples a class uniformly and then samples data from Pj.
For a model f : Rd →Rk that outputs k logits, we use Lbal[f] to denote the standard 0-1 test error
on the balanced data distribution:
(x,y)∼Pbal[f(x)y < max
ℓ̸=y f(x)ℓ]
Similarly, the error Lj for class j is deﬁned as Lj[f] = Pr(x,y)∼Pj[f(x)y < maxℓ̸=y f(x)ℓ].
Suppose we have a training dataset {(xi, yi)}n
i=1. Let nj be the number of examples in class j. Let
Sj = {i : yi = j} denote the example indices corresponding to class j.
Deﬁne the margin of an example (x, y) as
γ(x, y) = f(x)y −max
j̸=y f(x)j
Deﬁne the training margin for class j as:
i∈Sj γ(xi, yi)
We consider the separable cases (meaning that all the training examples are classiﬁed correctly)
because neural networks are often over-parameterized and can ﬁt the training data well. We also
note that the minimum margin of all the classes, γmin = min{γ1, . . . , γk}, is the classical notion of
training margin studied in the past [Koltchinskii et al., 2002].
Fine-grained generalization error bounds.
Let F be the family of hypothesis class. Let C(F) be
some proper complexity measure of the hypothesis class F. There is a large body of recent work on
measuring the complexity of neural networks , and our discussion below is orthogonal to the precise choices.
When the training distribution and the test distribution are the same, the typical generalization error
bounds scale in C(F)/√n. That is, in our case, if the test distribution is also imbalanced as the
training distribution, then
imbalanced test error ≲
Note that the bound is oblivious to the label distribution, and only involves the minimum margin
across all examples and the total number of data points. We extend such bounds to the setting
with balanced test distribution by considering the margin of each class. As we will see, the more
ﬁne-grained bound below allows us to design new training loss function that is customized to the
imbalanced dataset.
Theorem 1 (Informal and simpliﬁed version of Theorem 2). With high probability (1 −n−5) over
the randomness of the training data, the error Lj for class j is bounded by
where we use ≲to hide constant factors. As a direct consequence,
Lbal[f] ≲1
Class-distribution-aware margin trade-off.
The generalization error bound (4) for each class
suggests that if we wish to improve the generalization of minority classes (those with small nj’s),
we should aim to enforce bigger margins γj’s for them. However, enforcing bigger margins for
minority classes may hurt the margins of the frequent classes. What is the optimal trade-off between
the margins of the classes? An answer for the general case may be difﬁcult, but fortunately we can
obtain the optimal trade-off for the binary classiﬁcation problem.
With k = 2 classes, we aim to optimize the balanced generalization error bound provided in (5),
which can be simpliﬁed to (by removing the low order term log n
√nj and the common factor C(F))
At the ﬁrst sight, because γ1 and γ2 are complicated functions of the weight matrices, it appears
difﬁcult to understand the optimal margins. However, we can ﬁgure out the relative scales between
γ1 and γ2. Suppose γ1, γ2 > 0 minimize the equation above, we observe that any γ′
1 = γ1 −δ and
2 = γ2 + δ (for δ ∈(−γ2, γ1)) can be realized by the same weight matrices with a shifted bias term
(See Figure 1 for an illustration). Therefore, for γ1, γ2 to be optimal, they should satisfy
(γ1 −δ)√n1
(γ2 + δ)√n2
The equation above implies that
, and γ2 =
for some constant C. Please see a detailed derivation in the Section A.
Fast rate vs slow rate, and the implication on the choice of margins. The bound in Theorem 1
may not necessarily be tight. The generalization bounds that scale in 1/√n (or 1/√ni here with
imbalanced classes) are generally referred to the “slow rate” and those that scale in 1/n are referred
to the “fast rate”. With deep neural networks and when the model is sufﬁciently big enough, it is
possible that some of these bounds can be improved to the fast rate. See [Wei and Ma, 2019] for
some recent development. In those cases, we can derive the optimal trade-off of the margin to be
Label-Distribution-Aware Margin Loss
Inspired by the trade-off between the class margins in Section 3.1 for two classes, we propose to
enforce a class-dependent margin for multiple classes of the form
We will design a soft margin loss function to encourage the network to have the margins above. Let
(x, y) be an example and f be a model. For simplicity, we use zj = f(x)j to denote the j-th output
of the model for the j-th class.
The most natural choice would be a multi-class extension of the hinge loss:
LLDAM-HG((x, y); f) = max(max
j̸=y {zj} −zy + ∆y, 0)
where ∆j =
for j ∈{1, . . . , k}
Here C is a hyper-parameter to be tuned. In order to tune the margin more easily, we effectively
normalize the logits (the input to the loss function) by normalizing last hidden activation to ℓ2 norm
1, and normalizing the weight vectors of the last fully-connected layer to ℓ2 norm 1, following
the previous work [Wang et al., 2018a]. Empirically, the non-smoothness of hinge loss may pose
difﬁculties for optimization. The smooth relaxation of the hinge loss is the following cross-entropy
loss with enforced margins:
LLDAM((x, y); f) = −log
ezy−∆y + P
where ∆j =
for j ∈{1, . . . , k}
In the previous work [Liu et al., 2016, 2017a, Wang et al., 2018a] where the training set is usually
balanced, the margin ∆y is chosen to be a label independent constant C, whereas our margin depends
on the label distribution.
Remark: Attentive readers may ﬁnd the loss LLDAM somewhat reminiscent of the re-weighting
because in the binary classiﬁcation case — where the model outputs a single real number which is
passed through a sigmoid to be converted into a probability, — both the two approaches change the
gradient of an example by a scalar factor. However, we remark two key differences: the scalar factor
introduced by the re-weighting only depends on the class, whereas the scalar introduced by LLDAM
also depends on the output of the model; for multiclass classiﬁcation problems, the proposed loss
LLDAM affects the gradient of the example in a more involved way than only introducing a scalar
factor. Moreover, recent work has shown that, under separable assumptions, the logistical loss, with
weak regularization [Wei et al., 2018] or without regularization [Soudry et al., 2018], gives the max
margin solution, which is in turn not effected by any re-weighting by its deﬁnition. This further
suggests that the loss LLDAM and the re-weighting may complement each other, as we have seen in
the experiments. (Re-weighting would affect the margin in the non-separable data case, which is left
for future work.)
Deferred Re-balancing Optimization Schedule
Cost-sensitive re-weighting and re-sampling are two well-known and successful strategies to cope
with imbalanced datasets because, in expectation, they effectively make the imbalanced training
distribution closer to the uniform test distribution. The known issues with applying these techniques
are (a) re-sampling the examples in minority classes often causes heavy over-ﬁtting to the minority
classes when the model is a deep neural network, as pointed out in prior work , and (b) weighting up the minority classes’ losses can cause difﬁculties and instability in
optimization, especially when the classes are extremely imbalanced [Cui et al., 2019, Huang et al.,
2016]. In fact, Cui et al. develop a novel and sophisticated learning rate schedule to cope
with the optimization difﬁculty.
We observe empirically that re-weighting and re-sampling are both inferior to the vanilla empirical
risk minimization (ERM) algorithm (where all training examples have the same weight) before
annealing the learning rate in the following sense. The features produced before annealing the
learning rate by re-weighting and re-sampling are worse than those produced by ERM. (See Figure 6
for an ablation study of the feature quality performed by training linear classiﬁers on top of the
features on a large balanced dataset.)
Inspired by this, we develop a deferred re-balancing training procedure (Algorithm 1), which ﬁrst
trains using vanilla ERM with the LDAM loss before annealing the learning rate, and then deploys a
re-weighted LDAM loss with a smaller learning rate. Empirically, the ﬁrst stage of training leads
to a good initialization for the second stage of training with re-weighted losses. Because the loss is
non-convex and the learning rate in the second stage is relatively small, the second stage does not
move the weights very far. Interestingly, with our LDAM loss and deferred re-balancing training,
the vanilla re-weighting scheme (which re-weights by the inverse of the number of examples in each
class) works as well as the re-weighting scheme introduced in prior work [Cui et al., 2019]. We also
found that with our re-weighting scheme and LDAM, we are less sensitive to early stopping than [Cui
et al., 2019].
Algorithm 1 Deferred Re-balancing Optimization with LDAM Loss
Require: Dataset D = {(xi, yi)}n
i=1. A parameterized model fθ
1: Initialize the model parameters θ randomly
2: for t = 1 to T0 do
B ←SampleMiniBatch(D, m)
▷a mini-batch of m examples
(x,y)∈B LLDAM((x, y); fθ)
fθ ←fθ −α∇θL(fθ)
▷one SGD step
Optional: α ←α/τ
▷anneal learning rate by a factor τ if necessary
8: for t = T0 to T do
B ←SampleMiniBatch(D, m)
▷A mini-batch of m examples
(x,y)∈B n−1
· LLDAM((x, y); fθ)
▷standard re-weighting by frequency
(x,y)∈B n−1
▷one SGD step with re-normalized learning rate
Experiments
We evaluate our proposed algorithm on artiﬁcially created versions of IMDB review [Maas et al.,
2011], CIFAR-10, CIFAR-100 [Krizhevsky and Hinton, 2009] and Tiny ImageNet [Russakovsky
et al., 2015, tin] with controllable degrees of data imbalance, as well as a real-world large-scale
imbalanced dataset, iNaturalist 2018 [Van Horn et al., 2018]. Our core algorithm is developed using
PyTorch [Paszke et al., 2017].
Baselines.
We compare our methods with the standard training and several state-of-the-art techniques and their combinations that have been widely adopted to mitigate the issues with training on
imbalanced datasets: (1) Empirical risk minimization (ERM) loss: all the examples have the same
weights; by default, we use standard cross-entropy loss. (2) Re-Weighting (RW): we re-weight each
sample by the inverse of the sample size of its class, and then re-normalize to make the weights
1 on average in the mini-batch. (3) Re-Sampling (RS): each example is sampled with probability
proportional to the inverse sample size of its class. (4) CB [Cui et al., 2019]: the examples are
re-weighted or re-sampled according to the inverse of the effective number of samples in each class,
deﬁned as (1 −βni)/(1 −β), instead of inverse class frequencies. This idea can be combined with
either re-weighting or re-sampling. (5) Focal: we use the recently proposed focal loss [Lin et al.,
2017] as another baseline. (6) SGD schedule: by SGD, we refer to the standard schedule where the
learning rates are decayed a constant factor at certain steps; we use a standard learning rate decay
Our proposed algorithm and variants.
We test combinations of the following techniques proposed by us. (1) DRW and DRS: following the proposed training Algorithm 1, we use the standard
ERM optimization schedule until the last learning rate decay, and then apply re-weighting or resampling for optimization in the second stage. (2) LDAM: the proposed Label-Distribution-Aware
Margin losses as described in Section 3.2.
When two of these methods can be combined, we will concatenate the acronyms with a dash in
between as an abbreviation. The main algorithm we propose is LDAM-DRW. Please refer to Section B
for additional implementation details.
Experimental results on IMDB review dataset
IMDB review dataset consists of 50,000 movie reviews for binary sentiment classiﬁcation [Maas
et al., 2011]. The original dataset contains an evenly distributed number of positive and negative
reviews. We manually created an imbalanced training set by removing 90% of negative reviews. We
train a two-layer bidirectional LSTM with Adam optimizer [Kingma and Ba, 2014]. The results are
reported in Table 1.
Table 1: Top-1 validation errors on imbalanced IMDB review dataset. Our proposed approach
LDAM-DRW outperforms the baselines.
Error on positive reviews
Error on negative reviews
Mean Error
Table 2: Top-1 validation errors of ResNet-32 on imbalanced CIFAR-10 and CIFAR-100. The
combination of our two techniques, LDAM-DRW, achieves the best performance, and each of them
individually are beneﬁcial when combined with other losses or schedules.
Imbalanced CIFAR-10
Imbalanced CIFAR-100
Imbalance Type
long-tailed
long-tailed
Imbalance Ratio
Focal [Lin et al., 2017]
CB RW [Cui et al., 2019]
CB Focal [Cui et al., 2019]
LDAM-HG-DRS
Experimental results on CIFAR
Imbalanced CIFAR-10 and CIFAR-100. The original version of CIFAR-10 and CIFAR-100
contains 50,000 training images and 10,000 validation images of size 32×32 with 10 and 100 classes,
respectively. To create their imbalanced version, we reduce the number of training examples per class
and keep the validation set unchanged. To ensure that our methods apply to a variety of settings,
we consider two types of imbalance: long-tailed imbalance [Cui et al., 2019] and step imbalance
[Buda et al., 2018]. We use imbalance ratio ρ to denote the ratio between sample sizes of the most
frequent and least frequent class, i.e., ρ = maxi{ni}/ mini{ni}. Long-tailed imbalance follows an
exponential decay in sample sizes across different classes. For step imbalance setting, all minority
classes have the same sample size, as do all frequent classes. This gives a clear distinction between
minority classes and frequent classes, which is particularly useful for ablation study. We further
deﬁne the fraction of minority classes as µ. By default we set µ = 0.5 for all experiments.
We report the top-1 validation error of various methods for imbalanced versions of CIFAR-10 and
CIFAR-100 in Table 2. Our proposed approach is LDAM-DRW, but we also include a various
combination of our two techniques with other losses and training schedule for our ablation study.
We ﬁrst show that the proposed label-distribution-aware margin cross-entropy loss is superior to
pure cross-entropy loss and one of its variants tailored for imbalanced data, focal loss, while no
data-rebalance learning schedule is applied. We also demonstrate that our full pipeline outperforms
the previous state-of-the-arts by a large margin. To further demonstrate that the proposed LDAM
loss is essential, we compare it with regularizing by a uniform margin across all classes under the
setting of cross-entropy loss and hinge loss. We use M-DRW to denote the algorithm that uses a
cross-entropy loss with uniform margin [Wang et al., 2018a] to replace LDAM, namely, the ∆j
in equation (13) is chosen to be a tuned constant that does not depend on the class j. Hinge loss
(HG) suffers from optimization issues with 100 classes so we constrain its experiment setting with
CIFAR-10 only.
Table 3: Validation errors on iNaturalist 2018 of various approaches. Our proposed method LDAM-
DRW demonstrates signiﬁcant improvements over the previous state-of-the-arts. We include ERM-
DRW and LDAM-SGD for the ablation study.
CB Focal [Cui et al., 2019]
Figure 2: Per-class top-1 error on CIFAR-10 with
step imbalance (ρ = 100, µ = 0.5). Classes 0-F
to 4-F are frequent classes, and the rest are minority classes. Under this extremely imbalanced
setting RW suffers from under-ﬁtting, while RS
over-ﬁts on minority examples. On the contrary,
the proposed algorithm exhibits great generalization on minority classes while keeping the performance on frequent classes almost unaffected.
This suggests we succeeded in regularizing minority classes more strongly.
Figure 3: Imbalanced training errors (dotted
lines) and balanced test errors (solid lines) on
CIFAR-10 with long-tailed imbalance (ρ = 100).
We anneal decay the learning rate at epoch 160
for all algorithms.
Our DRW schedule uses
ERM before annealing the learning rate and thus
performs worse than RW and RS before that
point, as expected.
However, it outperforms
the others signiﬁcantly after annealing the learning rate.
See Section 4.4 for more analysis.
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Imbalanced but known test label distribution: We also test the performance of an extension of
our algorithm in the setting where the test label distribution is known but not uniform. Please see
Section C.5 for details.
Visual recognition on iNaturalist 2018 and imbalanced Tiny ImageNet
We further verify the effectiveness of our method on large-scale imbalanced datasets. The iNatualist
species classiﬁcation and detection dataset [Van Horn et al., 2018] is a real-world large-scale imbalanced dataset which has 437,513 training images with a total of 8,142 classes in its 2018 version.
We adopt the ofﬁcial training and validation splits for our experiments. The training datasets have a
long-tailed label distribution and the validation set is designed to have a balanced label distribution.
We use ResNet-50 as the backbone network across all experiments for iNaturalist 2018. Table 3
summarizes top-1 validation error for iNaturalist 2018. Notably, our full pipeline is able to outperform
the ERM baseline by 10.86% and previous state-of-the-art by 6.88% in top-1 error. Please refer to
Appendix C.2 for results on imbalanced Tiny ImageNet.
Ablation study
Evaluating generalization on minority classes. To better understand the improvement of our
algorithms, we show per-class errors of different methods in Figure 2 on imbalanced CIFAR-10.
Please see the caption there for discussions.
Evaluating deferred re-balancing schedule. We compare the learning curves of deferred rebalancing schedule with other baselines in Figure 3. In Figure 6 of Section C.3, we further show that
even though ERM in the ﬁrst stage has slightly worse or comparable balanced test error compared to
RW and RS, in fact the features (the last-but-one layer activations) learned by ERM are better than
those by RW and RS. This agrees with our intuition that the second stage of DRW, starting from
better features, adjusts the decision boundary and locally ﬁne-tunes the features.
Conclusion
We propose two methods for training on imbalanced datasets, label-distribution-aware margin loss
(LDAM), and a deferred re-weighting (DRW) training schedule. Our methods achieve signiﬁcantly improved performance on a variety of benchmark vision tasks. Furthermore, we provide a
theoretically-principled justiﬁcation of LDAM by showing that it optimizes a uniform-label generalization error bound. For DRW, we believe that deferring re-weighting lets the model avoid the
drawbacks associated with re-weighting or re-sampling until after it learns a good initial representation (see some analysis in Figure 3 and Figure 6). However, the precise explanation for DRW’s
success is not fully theoretically clear, and we leave this as a direction for future work.
Acknowledgements
Toyota Research Institute ("TRI") provided funds and computational resources
to assist the authors with their research but this article solely reﬂects the opinions and conclusions of
its authors and not TRI or any other Toyota entity. We thank Percy Liang and Michael Xie for helpful
discussions in various stages of this work.