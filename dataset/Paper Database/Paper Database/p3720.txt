Leave-One-Out Cross-Validation Based Model
Selection Criteria for Weighted LS-SVMs
Gavin C. Cawley
School of Computing Sciences
University of East Anglia
Norwich NR4 7TJ
United Kingdom
E-mail: 
Abstract— While the model parameters of many kernel learning methods are given by the solution of a convex optimisation
problem, the selection of good values for the kernel and
regularisation parameters, i.e. model selection, is much less
straight-forward. This paper describes a simple and efﬁcient
approach to model selection for weighted least-squares support
vector machines, and compares a variety of model selection
criteria based on leave-one-out cross-validation. An external
cross-validation procedure is used for performance estimation,
with model selection performed independently in each fold to
avoid selection bias. The best entry based on these methods
was ranked in joint ﬁrst place in the WCCI-2006 performance
prediction challenge, demonstrating the effectiveness of this
I. INTRODUCTION
Kernel learning methods, such as the least-squares support
vector machine (LS-SVM) are attractive because they allow the construction of powerful non-linear classiﬁers, using
only relatively simple mathematical and computational techniques. The model parameters of an LS-SVM are given by the
solution of a system of linear equations, which can be found
efﬁciently via Cholesky factorisation. The generalisation performance of the LS-SVM is however, heavily dependent on
the model selection process, in this case the careful selection
of an appropriate kernel function and good values for the
regularisation and kernel parameters. This paper is concerned
with model selection strategies based on minimisation of the
leave-one-out cross-validation estimate of a range of model
selection criteria, which can be performed very efﬁciently for
this class of kernel learning methods. The aim of the WCCI-
2006 Performance Prediction Challenge (PPC) is to identify
accurate methods for predicting the performance of statistical
classiﬁers on unseen test data, for use in model selection
and model evaluation. The challenge takes place over a
suite of ﬁve benchmark datasets, ADA, GINA, HIVA, NOVA
and SYLVA, each having pre-deﬁned training, validation and
test partitions. The ﬁnal performance assessment is based
on a combination of the Balanced Error Rate (BER) of
the classiﬁer over the test partition and the accuracy of
the predicted balanced error rate generated by the model
selection procedure. A number of features of the performance
prediction challenge warrant serious consideration, and are
discussed in the remainder of this section.
A. The Balanced Error Rate (BER) Criterion
CONFUSION MATRIX FOR TWO-CLASS PATTERN RECOGNITION.
Prediction
The Balanced Error Rate (BER) statistic is the average of
the misclassiﬁcation rates on examples drawn from positive
and negative classes (denoted by C+ and C−respectively),
where a, b, c and d are entries in the confusion matrix for
a two-class pattern recognition problem, shown in Table I.
Clearly the balanced error rate only coincides with the more
traditional misclassiﬁcation rate if there are an equal number
of positive and negative examples, in which case a+b = c+d.
However, the relative class frequencies in the performance
prediction challenge benchmarks are skewed in favour of
the negative class; in the case of the HIVA and SYLVA
benchmarks the ratios are skewed rather heavily in favour of
the negative class. We must therefore tailor our approach to
account for the unequal weight assigned to false-negative and
false-positive errors in the performance assessment criterion.
This can be accomplished via a number of means, including
altering the bias parameters of the classiﬁer or differentially
weighting positive and negative examples during the training
procedure. Both of these approaches are investigated in this
B. Over-Fitting in the Model Selection Process
Let G(θ) represent the true test error of a classiﬁer with
parameters θ, and g(θ|D) an estimate of the true test error
based on a sample of data D. In the context of model
selection, D, might refer to an independent validation set,
or the set of validation partitions arising in (leave-one-out)
cross-validation. The expected error of the estimator can be
broken down into bias and variance components ,
[g(θ; D) −G(θ)]2o
= ED {g(θ; D) −G(θ)}2
[g(θ; D) −ED {g(θ; D)}]2o
where the expectations are taken over all data sets, D,
of ﬁxed size. The ﬁrst term, the squared bias, is low if
on average the difference between the true test error and
estimated error is small, i.e. the bias represents degree to
which the estimated error systematically differs from the true
test error. The second component, the variance, essentially
reﬂects the sensitivity of the estimator to the particular
choice of data over which it is evaluated. Note that the
variance can normally be expected to fall as the size of
the sample of data, D, increases. The leave-one-out crossvalidation estimator is known to be approximately unbiased
 . This is a reassuring, but not essential, property for a
model selection criterion as it suggests that on average the
vector of model parameters minimising the model selection
criterion are approximately the same as those minimising the
true test error. However, the leave-one-out estimator generally
exhibits a higher variance than, for example, the k-fold crossvalidation estimator e.g. . This is an undesirable property
of a model selection criterion as the “optimal” parameters,
θ, will be sensitive to the sample of data used. The relatively
high variance of the leave-one-out estimator has a number of
signiﬁcant implications for the model selection process:
1) Over-ﬁtting of the Model Selection Criterion: During
the model selection process, the parameters θ are iteratively
modiﬁed so as to minimise the value of the model selection
criterion. However, the value of the model selection criterion
can be considered to comprise of two components, a component that is closely related to generalisation performance, and
a component that is sensitive to the characteristics speciﬁc
to the particular sample of data on which it is evaluated. It
seems reasonable to expect that the largest reductions in the
model selection criterion should come from changes in the
parameters that result in a reduction in the true test error.
If the number of parameters to be determined during model
selection is relatively small, the model selection process is
likely to be dominated by changes that genuinely improve
generalisation. On the other hand if the number of parameters
is relatively large, there may be sufﬁcient degrees of freedom
that the model selection becomes sensitive to the particular
sample of data used, i.e. over-ﬁtting will occur. It is therefore
prudent to avoid the selection of a large number of parameters
on the basis of a model selection criterion, unless it is known
to have a low variance.
2) Feature Selection: Many of the challenge datasets
are characterised by a large number of features relative to
the number of training patterns. If feature selection were
performed as part of the model selection process, this would
vastly inﬂate the degrees of freedom available for over-
ﬁtting the model selection criterion, as there is essentially
an extra degree of freedom associated with each feature. In
practice, performing feature selection on the basis of leaveone-out cross-validation criteria often degrades generalisation
performance in the presence of a large number of features due
to over-ﬁtting of the model selection criterion. Fortunately,
least-squares support vector machines generally perform well
in high-dimensional spaces, due to the use of formal regularisation , and so we are able to neglect a feature selection
stage in this study.
3) Model Selection versus Performance Prediction: For
performance evaluation purposes, we require a criterion that
is both unbiased and also exhibits a low variance. Leave-oneout cross-validation based estimators, while approximately
unbiased, are likely to be sub-optimal performance evaluation
criteria due to their high variance1. However, in general, one
should be cautious in using the same criterion for model
selection and performance evaluation; any criterion that has
been directly optimised during the model selection process
is likely to result in a signiﬁcantly optimistic estimate of the
true generalisation performance, due to the variance of the
estimator.
C. Reliability of Validation Set Statistics
The design of the performance prediction challenge benchmarks is such that the test set is approximately ten times
larger than the training set, which in turn is approximately ten
times larger than the validation set, with the ratio of positive
and negative examples being closely matched in all three
partitions of the available data. However, optimising the balanced error rate on the validation set, so as to achieve a good
ranking in the model development stage of the challenge, is a
risky strategy as the variance of the validation set estimate of
the test BER is likely to be high, due to the small size of the
validation set. In essence, this means it might be possible to
“over-ﬁt” the hyper-parameters of the model to the validation
set. This is especially true for the HIVA benchmark, where
the validation set contains only 14 positive examples in
addition to the 370 negative examples. As the balanced error
rate is very sensitive to errors in the minority class, the
validation set BER will be very sensitive to the sampling
of these positive examples. If fourteen “clearly positive”
examples were chosen, the BER will be unrealistically low,
if fourteen “difﬁcult” examples were selected the BER will
be unduly high. As there are only fourteen positive patterns,
either of these scenarios could easily occur, in which case
the validation set BER would be a poor predictor of the
test set BER. In this study, we have therefore chosen to
largely ignore the validation set performance, available from
the challenge web-site, in favour of estimators likely to have
a lower variance. It will be interesting to see, in hindsight,
whether this was a good strategy at the conclusion of the
challenge!
1Leave-one-out cross-validation is typically used in the analysis of very
small datasets, where the relatively high variance of the leave-one-out
estimator is offset by the stability resulting from the greater size of the
training partition than is possible using conventional k-fold cross-validation.
II. METHOD
In this section, we give a brief overview of the Least-
Squares Support Vector Machine (LS-SVM), including a
weighted variant suitable for the performance prediction
challenge, before going on to describe an efﬁcient closedform implementation of the leave-one-out cross-validation
method for least-squares kernel learning methods. This forms
the basis of a family of model selection procedures, based
on the leave-one-out cross-validation estimates of a variety
of model selection criteria.
A. Least-Squares Support Vector Machines
Assume we are given labelled training data, D
{(xi, yi)}ℓ
i=1, where xi ∈X ⊂Rd is a vector of input
features describing the ith example and yi ∈{−1, +1} is
an indicator variable such that yi = −1 if the ith example
is drawn from class C−and yi = +1 is drawn from class
C+. The Least-Squares Support Vector Machine (LS-SVM)
aims to construct a linear model f(x) = w · φ(x) + b in a
ﬁxed feature space, φ : X →F, that is able to distinguish
between examples drawn from C−and C+, such that
if f(x) ≥0
However, rather than specifying the feature space, F directly,
it is implied by a kernel function K : X ×X →R, giving the
inner product between the images of vectors in the feature
space, F, i.e. K(x, x′) = φ(x) · φ(x′). A common kernel
function is the isotropic Radial Basis Function (RBF) kernel
K(x, x′) = exp
−η∥x −x′∥2
where η is a kernel parameter controlling the sensitivity of
the kernel function. Other useful kernels include the linear,
K(x, x′) = x · x′
and polynomial kernels
K(x, x′) = (x · x′ + c)d
where c and d are kernel parameters (d = 2 gives the
quadratic kernel and d = 3 the cubic kernel) in addition
to the Boolean kernel
K(x, x′) = (1 + η)x·x′.
The model parameters (w, b) are given by the minimum of
a regularised least-squares loss function,
[yi −w · φ(xi) −b]2 ,
where µ is a regularisation parameter controlling the biasvariance trade-off . The accuracy of an LS-SVM on test
data is critically dependent on the choice of good values for
the hyper-parameters, in this case µ and η. The search for the
optimal values for such hyper-parameters is a process known
as model selection.
B. Training Algorithm
The regularised least-squares problem (4) can be solved via
a system of linear equations, with a computational complexity
of O(ℓ3) operations, as follows: Minimising (4) can be recast
in the form of a constrained optimisation problem,
subject to
yi = w · φ(xi) + b + εi,
∀i ∈{1, 2, . . . , ℓ}.
The primal Lagrangian for this optimisation problem gives
the unconstrained minimisation problem,
αi {w · φ(xi) + b + εi −yi} ,
where α = (α1, α2, . . . , αℓ) ∈Rℓis a vector of Lagrange
multipliers. The optimality conditions for this problem can
be expressed as follows:
0 =⇒αi = εi
0 =⇒w · φ(xi) + b + εi −yi = 0.
Using (7) and (9) to eliminate w and ε = (ε1, ε2, . . . , εℓ),
from (10), we ﬁnd that
αjφ(xj) · φ(xi) + b + ℓµαi = yi.
Noting that K(x, x′) = φ(x) · φ(x′), the system of linear
equations can be written more concisely in matrix form as
From (7) and noting that K(x, x′) = φ(x)·φ(x′), the output
of the LS-SVM can be written in terms of the dual model
parameters, (α, b), as
αiK(xi, x) + b.
C. Efﬁcient Implementation Via Cholesky Factorisation
A more efﬁcient training algorithm can be obtained, taking
advantage of the special structure of the system of linear
equations . The system of linear equations (12) to be
solved in ﬁtting a least-squares support vector machine can
be written as
where M = K + µℓI. Unfortunately the matrix on the lefthand side is not positive deﬁnite, and so we cannot solve
this system of linear equations directly using the Cholesky
factorisation. However, the ﬁrst row of (13) can be re-written
 α + M −11b
Rearranging (14), we see that α = M −1 (y −1b), using this
result to eliminate α, the second row of (13) can be written
1T M −11b = 1T M −1y
The system of linear equations can then be re-written as
α + M −11b
In this case, the matrix on the left hand side is positivedeﬁnite, as M = K +µℓI is positive-deﬁnite and 1T M −11
is positive since the inverse of a positive deﬁnite matrix is
also positive deﬁnite. The revised system of linear equations
can then be solved as follows: First solve
The model parameters of the least-squares support vector
machine are then given by
α = ν −ηb.
The two systems of linear equations (16) can be solved
efﬁciently using the Cholesky decomposition of M = RT R,
where R is the upper triangular Cholesky factor of M.
D. Weighted Least-Squares Support Vector Machines
For some applications, it may be preferable ﬁnd the model
parameters (w, b) via minimisation of a regularised weighted
least-squares loss function ,
ζi [yi −w · φ(xi) −b]2 ,
where ζ = {ζ1, ζ2, . . . , ζℓ} is a vector of weights associated
with each pattern. The optimal dual model parameters, (α, b)
are then given by the solution of a modiﬁed system of linear
equations,
where W = diag
2 , . . . , ζ−1
. The most common situation in which a weighted loss function is used
is where the proportions of positive and negative examples
in the training data are known not to be representative of
the operational class frequencies. A weighted loss function
is also appropriate if we wish to minimise the balanced
error rate, in order to balance the contribution of the sets of
positive and negative examples to the data misﬁt term of the
regularised loss function. In this case, the weighting factors
should be chosen according to
if ti = +1
where ℓ+ and ℓ−represent the number of positive and negative examples respectively. Note that this is asymptotically
equivalent to re-sampling the data so that there are an equal
number of positive and negative examples (c.f. ).
E. Efﬁcient Leave-One-Out Cross-Validation
The optimal values of the parameters of a Least-Squares
Support Vector Machine are given by the solution of a system
of linear equations (12), the matrix on the left-hand side of
which can be decomposed into block-matrix representation,
as follows: K + µℓI
Let [α(−i); b(−i)] represent the parameters of the leastsquares support vector machine during the ith iteration of
the leave-one-out cross-validation procedure, then in the ﬁrst
iteration, in which the ﬁrst training pattern is excluded,
[y2, . . . , yℓ, 0]T .
The leave-one-out prediction for the ﬁrst training pattern is
then given by,
[y2, . . . , yℓ, 0]T
Considering the last ℓequations in the system of linear
equations (12), it is clear that [c1 C1] [α2, . . . , αℓ, b]T =
[y2, . . . , yℓ, 0]T , and so
1 c1α1 + c1 [α2, . . . , αℓ, b]T .
Noting, from the ﬁrst equation in the system of linear
equations (12), that y1 = c11α1 + cT
1 [α2, . . . , αℓ, b]T , thus
Finally, via the block matrix inversion lemma,
where κ = c11 −cT
1 c, and noting that the system of
linear equations (12) is insensitive to permutations of the
ordering of the equations and of the unknowns, we have that,
= yi −ˆy(−i)
This means that, assuming the system of linear equations
is solved via explicit inversion of C, a leave-one-out crossvalidation estimate of an appropriate model selection criterion
can be evaluated using information already available as a byproduct of training the least-squares support vector machine
on the entire dataset, with only a negligible additional computational expense.
F. Efﬁcient Implementation via Cholesky Factorisation
The coefﬁcients of the kernel expansion, α, can be found
efﬁciently, via Cholesky factorisation, as described in Section II-C. However, in order to perform the efﬁcient leaveone-out cross-validation procedure, we must also determine
the diagonal elements of C−1 in an efﬁcient manner. Using
the block matrix inversion formula, we obtain
 M −1 + M −11S−1
where M = K + µℓI and SM = −1T M −11 = −1T η is
the Schur complement of M. The inverse of the positive
deﬁnite matrix, M, can be computed efﬁciently from its
Cholesky factorisation, via the SYMINV algorithm ,
for example using the LAPACK routine DTRTRI . Let
R = [rij]n
i,j=1 be the lower triangular Cholesky factor of
the positive deﬁnite matrix M, such that M = RRT .
Furthermore, let
S = [sij]n
i,j=1 = R−1,
sij = −sii
represent the (lower triangular) inverse of the Cholesky
factor. The inverse of M is then given by M −1 = ST S.
In the case of efﬁcient leave-one-out cross-validation of
least-squares support vector machines, we are principally
concerned only with the diagonal elements of M −1, given
The computational complexity of the basic training algorithm
is O(ℓ3) operations, being dominated by the evaluation of the
Cholesky factor. However, the computational complexity of
the analytic leave-one-out cross-validation procedure, when
performed as a by-product of the training algorithm, is
only O(ℓ) operations. The computational expense of the
leave-one-out cross-validation procedure therefore becomes
increasingly negligible as the training set becomes larger.
G. Model Selection Criteria
While the optimal model parameters of the LS-SVM are
given by the solution of a simple system of linear equations,
(12) or (17), some form of model selection is required to determine good values for the hyper-parameters, θ = (µ, η) in
order to maximise generalisation performance. The analytic
leave-one-out cross-validation procedure described in the previous section can easily form the basis of an efﬁcient model
selection strategy based on a weighted version of Allen’s
predicted residual sum-of-squares (PRESS) statistic ,
PRESS(θ) =
However the PRESS statistic is best suited to regression problems, and more sophisticated model selection criterion may
be preferable in the context of statistical pattern recognition.
For instance, the leave-one-out cross-validation estimate of
the weighted error rate is given by
ERROR(θ) =
where Ψ{·} is the unit step function,
The leave-one-out estimate of the balanced error rate (BER)
is obtained by setting the weighting coefﬁcients to give equal
weight to the sets of positive and negative examples, that
is according to (18). The leave-one-out balanced error rate
ought to provide a good model selection criterion for the
performance prediction challenge as the balanced error rate
over the test set forms the major component of the ﬁnal ranking criterion. However, while the leave-one-out estimate of
the BER provides a reasonable performance estimate for the
purposes of the challenge, it is not entirely suitable for model
selection purposes, as we would prefer a continuous function
that is more amenable to numerical optimisation routines.
One approach would be to approximate the discontinuous
unit step function by a continuous approximation, such as
the logistic function ,
1 + exp{−γx},
where γ is a parameter governing the accuracy of the approximation. Alternatively, we may opt for an upper bound on the
balanced error rate, obtained by substituting the weighted
hinge loss for the step function,
HINGE(θ) =
or the weighted squared hinge loss,
HINGE2(θ) =
where [x]+ = max{0, x} (see Figure 1). A ﬁnal model selection criterion is concerned only with the quality of the relative
ranking of patterns under leave-one-out cross-validation, via
maximising the area under the receiver operating characteristic (AUC). Equivalently, one could instead minimise the
modiﬁed Wilcoxon-Mann-Whitney statistic,
where again, the smooth approximation to the step function
(20) can be employed to obtain a continuous selection
criterion. The hyper-parameters of the (weighted) LS-SVM,
θ, can then be optimised by minimisation of any of these
model selection criteria via, for example, the Nelder-Mead
simplex method, as implemented by the fminsearch
routines of the MATLAB Optimisation Toolbox.
The hinge and squared hinge loss bounds on the zero-one loss.
III. THE CHALLENGE BENCHMARK DATASETS
Table III shows summary information on each of the ﬁve
challenge benchmark datasets. The class ratios for the HIVA
and SYLVA benchmarks are highly skewed, with a very low
prior probability for the positive class P(C+). The GINA,
HIVA and especially the NOVA benchmarks also have a
very large number of features, given the number of training
examples. Note however that the number of features that have
a non-zero variance over the training set, dnc, is signiﬁcantly
less than the total number of features. Obviously features that
have zero variance over the training set are uninformative
and can safely be omitted from the analysis. Many of the
benchmarks also include a large number of binary features,
with a high degree of sparsity.
SUMMARY OF THE DIMENSIONS AND COMPOSITION OF THE FIVE
CHALLENGE BENCHMARK DATASETS.
The following pre-processing steps were taken for each
benchmark dataset: ADA - logarithmic transform of features
1 and 3, features 4 and 5 discretized via thresholding at
44 and respectively, standardisation of continuous features.
GINA - all features scaled by 255−1. HIVA - no preprocessing required. NOVA - no pre-processing required.
SYLVA - standardisation of continuous features, reduction of
training set using features never associated with the positive
IV. RESULTS
The aim of this study is to evaluate a range of criteria
for leave-one-out cross-validation based model selection of
weighted least-squares support vector machines. A 100-fold
validation approach was used in order to obtain a lowvariance estimator of the true test balanced error rate. In each
of 100 trials, the data are randomly partitioned into a training
set containing approximately 90% of the available data and a
test set containing the remaining patterns. Model selection is
performed independently in each trial via minimisation of a
leave-one-out model selection criterion via the Nelder-Mead
simplex optimisation method . A total of 70 experiments
were performed, based on different combinations of model
selection criteria, kernel function and the use of weighting
factors in the training and/or model selection procedures. The
results of these experiments are shown in Table III. The best
performance on each benchmark are shown in bold.
Table IV shows the weights resulting from a regression
analysis of the data given in Table III. The 100-fold validation
estimates of the test balanced error rate were standardised
to have a zero mean and unit variance. A linear leastsquares model was then used to predict the estimate of the
test balanced error rate using boolean features representing
the choice of model selection criterion (PRESS, HINGE1,
HINGE2, WMW and ERATE), the use of weighting factors
during training and model selection (Training and Selection
respectively) and the choice of kernel function (Linear,
Quadratic, Cubic, Boolean and RBF). The results suggest
that the use of weighting factors in training and/or model
selection does not confer a signiﬁcant advantage and that,
unsurprisingly, the choice of kernel is data dependent. The
choice of model selection criterion also seems data dependent, but that relatively good performance can be achieved
using the simple PRESS statistic, even though this is better
suited to regression problems, obviating the need to employ
a more complex criteria.
Three ﬁnal submissions have been made to the WCCI-
2006 performance prediction challenge website. The ﬁrst,
shown in Table V, consists of models selected for each
benchmark dataset on the basis of the leave-one-out estimate
of the balanced error rate, which is also used as the ﬁnal
performance estimate. In this case, it would be reasonable to
expect that the predicted balanced error rate will be unduly
low as the estimator has also been used as the model selection
criterion.
Table VI shows the second ﬁnal submission. In this
case the ﬁnal model choice is based on the leave-one-out
cross-validation estimate of the balanced error rate, but the
performance estimate is based on an independent 100-fold
validation estimate. This represents, in the author’s opinion,
the best practice methodology as the performance estimate
has not been biased by the model selection process in any
ESTIMATE OF THE TEST BALANCED ERROR RATE BASED ON 100-FOLD VALIDATION FOR THE WCCI-2006 PERFORMANCE PREDICTION CHALLENGE
BENCHMARKS FOR A VARIETY OF LEAVE-ONE-OUT CROSS-VALIDATION BASED MODEL SELECTION CRITERIA.
Experiment
WEIGHTS OBTAINED BY REGRESSION ANALYSIS OF 100-FOLD
VALIDATION ESTIMATE OF THE TEST BALANCED ERROR RATE.
PERFORMANCE OF THE FIRST FINAL SUBMISSION, MODEL CHOICE AND
PERFORMANCE ESTIMATION BASED ON LEAVE-ONE-OUT BER.
Balanced Error
way. Note that the guess error for this method is very much
SECOND FINAL SUBMISSION, MODEL CHOICE VIA LEAVE-ONE-OUT
BER, PERFORMANCE ESTIMATION VIA 100-FOLD VALIDATION BER.
Balanced Error
V. CONCLUSIONS
In this study, we have investigated a variety of model
selection criteria for (weighted) least-squares support vector
machines, based on leave-one-out cross-validation estimators.
A useful conclusion that may be drawn from the results
obtained suggests that the optimal choice of model selection
criterion is data dependent (and we cannot know a-priori
which will perform best) and so it is reasonable to use a
simple, mathematically tractable criterion, such as Allen’s
PRESS statistic. This study generated the joint winning entry
in the challenge, ﬁnishing ﬁrst in terms of average score and
second in terms of average ranking. The best model also
exhibited the second highest area under the receiver operating characteristic on the test set. The study also generated
two individual data set winners (HIVA and NOVA). This
demonstrates that leave-one-out cross-validation provides an
effective means of model selection for least-squares support
vector machines, but that an external means of performance
estimation is required. If performance evaluation is performed
using cross-validation, it is important that the model selection
process is performed separately in each trial in order to avoid
selection bias.
ACKNOWLEDGEMENTS
The author would like to thank the organisers of the
WCCI-2006 performance prediction challenge, the anonymous reviewers for their helpful comments and Nicola Talbot
for her help in typesetting the manuscript.