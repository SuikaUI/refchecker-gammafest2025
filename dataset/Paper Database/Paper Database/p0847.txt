DeepCare: A Deep Dynamic Memory Model for Predictive
Trang Pham, Truyen Tran, Dinh Phung and Svetha Venkatesh
April 12, 2017
Personalized predictive medicine necessitates the modeling of patient illness and care processes,
which inherently have long-term temporal dependencies.
Healthcare observations, recorded in
electronic medical records, are episodic and irregular in time. We introduce DeepCare, an end-toend deep dynamic neural network that reads medical records, stores previous illness history, infers
current illness states and predicts future medical outcomes. At the data level, DeepCare represents
care episodes as vectors in space, models patient health state trajectories through explicit memory
of historical records.
Built on Long Short-Term Memory (LSTM), DeepCare introduces time
parameterizations to handle irregular timed events by moderating the forgetting and consolidation
of memory cells. DeepCare also incorporates medical interventions that change the course of illness
and shape future medical risk. Moving up to the health state level, historical and present health
states are then aggregated through multiscale temporal pooling, before passing through a neural
network that estimates future outcomes. We demonstrate the eﬃcacy of DeepCare for disease
progression modeling, intervention recommendation, and future risk prediction. On two important
cohorts with heavy social and economic burden – diabetes and mental health – the results show
improved modeling and risk prediction accuracy.
Introduction
When a patient is admitted to hospital, there are two commonly asked questions: “what is happening?”
and “what happens next?” The ﬁrst question refers to the diagnosis of the illness, the second is about
prediction of future medical risk . While there are a wide array of diagnostic tools to answer the
ﬁrst question, the technologies are much less advanced in answering the second . Traditionally, this
prognostic question may be answered by experienced clinicians who have seen many patients, or by
clinical prediction models with well-deﬁned and rigorously collected risk factors. But this is expensive
and of limited availability. Modern electronic medical records (EMRs) promise to oﬀer a fast and cheap
alternative. An EMR typically contains the history of hospital encounters, diagnoses and interventions,
lab tests and clinical narratives. The wide adoption of EMRs has led to intensiﬁed research in building
predictive models from this rich data source in the past few years .
Answering to prognostic inquiries necessitates modeling patient-level temporal healthcare processes.
An eﬀective modeling must address four open challenges: (i) Long-term dependencies in healthcare:
the future illness and care may depend critically on historical illness and interventions. For example,
the onset of diabetes at middle age remains a risk factor for the rest of the life; cancers may recur
after years; and a previous surgery may prevent certain future interventions.
(ii) Representation
 
of admission: an admission episode consists of a variable-size discrete set containing diagnoses and
interventions. (iii) Episodic recording and irregular timing: medical records vary greatly in length, are
inherently episodic in nature and irregular in time . The data is episodic because it is only recorded
when the patient visits hospital and is undergone an episode of care. The episode is often tightly
packed in a short period, typically ranging from a day to two weeks. The timing of arrivals is largely
random. (iv) Confounding interactions between disease progression and intervention: medical records
are a mixture of the course of illness, the developmental and the intervening processes. In addition to
addressing these four challenges, a predictive system should be end-to-end and generic so that it can
be deployed on diﬀerent hospital implementations of EMRs. An end-to-end system requires minimal or
no feature engineering to read medical records, infer present illness states and predict future outcomes.
Existing methods are poor in handling such complexity. They inadequately capture variable length
 and ignore the long-term dependencies . Temporal models based on Markovian assumption are
unable to model temporal irregularity and have no memory, and thus can completely forget previous
major illness given an irrelevant episode . Deep learning, which has recently revolutionized cognitive
ﬁelds such as speech recognition, vision and computational linguistics, holds a great potential in
constructing end-to-end systems . However, its promise to healthcare has not been realized [11,
To this end, we introduce DeepCare, an end-to-end deep dynamic memory neural network that
addresses the four challenges.
DeepCare is built on Long Short-Term Memory (LSTM) ,
a recurrent neural network equipped with memory cells to store experiences. At each time-step, the
LSTM reads an input, updates the memory cell, and returns an output. Memory is maintained through
a forget gate that moderates the passing of memory from one time step to another, and is updated
by seeing new input at each time step. The output is determined by the memory and moderated
by an output gate. In DeepCare, the LSTM models the illness trajectory and healthcare processes
of a patient encapsulated in a time-stamped sequence of admissions. The inputs to the LSTM are
information extracted from admissions. The outputs represent illness states at the time of admission.
Memory maintenance enables capturing of long-term dependencies, thus addressing the ﬁrst challenge.
In fact, this capacity has made LSTM an ideal model for a variety of sequential domains .
No LSTM has been used in healthcare, however – one major diﬃculty would be the lack of handling
of set inputs, irregular timing and interventions.
Addressing these three drawbacks, DeepCare modiﬁes LSTM in several ways. For representing
admission, which is a set of discrete elements such as diagnoses and interventions, the solution is to
embed these elements into continuous vector spaces. Vectors of the same type are then pooled into a
single vector. Type-speciﬁc pooled vectors are then concatenated to represent an admission. In that
way, variable-size admissions are embedded in to continuous distributed vector space. The admission
vectors then serve as input features for the LSTM. As the embedding is learnt from data, the model
does not rely on manual feature engineering.
For irregular timing, the forget gate is extended to be a function of irregular time gap between
consecutive time steps.
We introduce two new forgetting mechanisms: monotonic decay and full
time-parameterization.
The decay mimics the natural forgetting when learning a new concept in
human. The parameterization accounts for more complex dynamics of diﬀerent diseases over time. The
resulting model is sparse in time and eﬃcient to compute since only observed records are incorporated,
regardless of the irregular time spacing. Finally, in DeepCare the confounding interaction between
disease progression and interventions is modeled as follows. Interventions inﬂuence the output gate of
current illness states and the forget gate that moderates memory carried into the future. As a result,
the illness states (the output) are moderated by past and current interventions.
Once illness states are outputted by the LSTM layer, they are aggregated through a new timedecayed multiscale pooling strategy. This allows further handling of time-modulated memory. Finally
at the top layer, pooled illness states are passed through a neural network for estimating future
prognosis. In short, computation steps in DeepCare can be summarized as
P (y | u1:n) = P (nnety (pool {LSTM(u1:n)}))
where u1:n is the input sequence of admission observations, y is the outcome of interest (e.g., readmission), nnety denotes estimate of the neural network with respect to outcome y, and P is probabilistic
model of outcomes. Overall, DeepCare is an end-to-end prediction model that relies on no manual
feature engineering, is capable of reading generic medical records, memorizing a long history, inferring
illness states and predicting the future risk.
We demonstrate our DeepCare on answering a crucial part of the holy grail question “what happens
next?”. In particular, we demonstrate our model on disease progression, intervention recommendation
and future risk prediction. Disease progression refers to the next disease occurrence given the medical
history. Intervention recommendation is about predicting a subset of treatment procedures for the
current diagnoses. Future risk may involve readmission or mortality within a predeﬁned period after
discharge. We note in passing that the forecasting of future events may be considerably harder than the
traditional notion of classiﬁcation (e.g., objects/documents categorization) due to inherent uncertainty
in unseen interleaved events. Our experiments are demonstrated on two datasets of very diﬀerent nature
– diabetes (a well-deﬁned chronic condition) and mental health (a diverse mixture of many acute and
chronic conditions). The cohorts were collected from a large regional hospital in the period of 2002 to
2013. We show that DeepCare outperforms state-of-the-art baseline classiﬁcation methods.
To summarize, through introducing DeepCare, we make four modeling contributions: (i) handling
long-term dependencies in healthcare; (ii) a novel representation of variable-size admission as ﬁxedsize continuous vectors; (iii) modeling episodic recording and irregular timing; and (iv) capturing
confounding interactions between disease progression and intervention.
We also contribute to the
healthcare analytics practice by demonstrating the eﬀectiveness of DeepCare on disease progression,
intervention recommendation and medical risk prediction. Finally, we wish to emphasize that although
DeepCare is designed as predictive model targeted to healthcare, DeepCare can be applied to other
temporal domains with similar data characteristics (i.e., long-term dependencies, discrete set inputs,
irregular timing and confounding interventions).
The paper is organized as follows. Section 2 provides background for Electronic Medical Records,
sequential and deep learning for healthcare. Section 3 presents preliminaries for DeepCare model:
Recurrent neural networks, LSTM and learning word representation. DeepCare is described in Section
4 while the experiments and results are reported in Section 5. Finally, Section 6 discusses further and
concludes the paper.
Background
Electronic medical records (EMRs)
An electronic medical record (EMR) is a digital version of patients health information. A wide range
of information can be stored in EMRs, such as detailed records of symptoms, data from monitoring
devices, clinicians’ observations . EMR systems store data accurately, decrease the risk of data
replication and the risk of data lost. EMRs are now widely adopted in developed countries and are
increasingly present in the rest of the world.
It is expected that EMRs in hospital help improve
treatment quality and reduce healthcare costs .
A typical EMR contains information about a sequence of admissions for a patient. There are two
types of admission methods: planned (routine) and unplanned (emergency). Unplanned admission
refers to transfer from the emergency department. EMRs typically store admitted time, discharge
time, lab tests, diagnoses, procedures, medications and clinical narratives. Diagnoses, procedures and
medications stored in EMRs are typically coded in standardized formats. Diagnoses are represented
using WHO’s ICD (International Classiﬁcation of Diseases) coding schemes1. For example, E10 encodes
Type 1 diabetes mellitus, E11 encodes Type 2 diabetes mellitus while F32 indicates depressive episode.
The procedures are typically coded in CPT (Current Procedural Terminology) or ICHI (International
Classiﬁcation of Health Interventions) schemes 2. Medication names can be mapped into the ATC
(Anatomical Therapeutic Chemical) scheme 3.
The wide adoption of EMRs has led to calls for meaningful use . One of the most important
uses is building predictive models . Like most applications of machine learning, the
bottleneck here is manual feature engineering due to the complexity of the data . Our DeepCare
solves this problem by building an end-to-end system where features are learnt automatically from data.
Sequential models for healthcare
Although healthcare is inherently episodic in nature, it has been well-recognized that modeling the
entire illness trajectory is important . Nursing illness trajectory model was popularized by
Strauss and Corbin , but the model is qualitative and imprecise in time . Thus its predictive
power is very limited.
Electronic medical records (EMRs) oﬀer the quantitative alternative with precise timing of events.
However, EMRs are complex – they reﬂect the interleaving between the illness processes and care
processes. The timing is irregular – patients only visit hospital when the illness is beyond a certain
threshold, even though the illness may have been present long before the visit. Existing work that
handles such irregularities includes interval-based extraction , but this method is rather coarse and
does not explicitly model the illness dynamics.
Capturing disease progression has been of great interest , and much eﬀort has been spent
on Markov models and dynamic Bayesian networks . However, healthcare is inherently
non-Markovian due to the long-term dependencies. For example, a routine admission with irrelevant
medical information would destroy the eﬀect of severe illness , especially for chronic conditions. Irregular timing and interventions have not been adequately modeled. Irregular-time Bayesian networks
 oﬀer a promise, but its power has yet to be demonstrated.
Further, assuming discrete states
are ineﬃcient since the information pathway has only log(K) bits for K states. Our work assumes
distributed and continuous states, thus oﬀering much larger state space.
Deep learning for healthcare
Deep learning is currently at the center of a new revolution in making sense of a large volume of data.
It has achieved great successes in cognitive domains such as speech, vision and NLP . To date, deep
learning approach to healthcare has been an unrealized promise, except for several very recent works
1 
2 
3 ddd index/
Figure 1: (Left) A typical Recurrent Neural Network and (Right) an RNN unfolded in time. Each
RNN unit at time step t reads input xt and previous hidden state ht−1 , generates output at and
predicts the label ˜yt.
 , where irregular timing is not property modeled. We observe that is a considerable similarity
between NLP and EMR, where diagnoses and interventions play the role of nouns and modiﬁers,
and an EMR is akin to a sentence. A major diﬀerence is the presence of precise timing in EMR,
as well as the episodic nature. This suggests that it is possible to extend NLP language models to
EMR, provided that irregular timing and episodicity are properly handled. Our DeepCare contributes
along that line. Going down to the genetic basis of health, a recent work called DeepFind uses
convolutional networks to detect regular DNA/RNA motifs. This is unlike DeepCare, where irregular
temporal dynamics are modeled.
Preliminaries
In this section, we brieﬂy review building blocks for DeepCare, which will be described fully in Sec. 4.
Recurrent neural network
A Recurrent Neural Network (RNN) is a neural network repeated over time. In particular, an RNN
allows self-loop connections and shared parameters across diﬀerent time steps. While a feedforward
neural network maps an input vector into an output vector, an RNN maps a sequence into a sequence.
Unlike hidden Markov models, where the states are typically discrete and the transitions between
states are stochastic, RNNs maintain distributed continuous states with deterministic dynamics. The
recurrent connections allow an RNN to memorize previous inputs, and therefore capture longer dependencies than a hidden Markov model does. Since the ﬁrst version of RNN was introduced in the 1980s
 , many varieties of RNN have been proposed such as Time-Delay Neural Networks and Echo
State Network . Here we restrict our discussion to the simple RNN with a single hidden layer as
shown in Fig. 1.
Forward propagation
An RNN unit has three connections: a recurrent connection from the previous hidden state to the
current hidden state (ht−1 →ht), an input-to-hidden-state connection (xt →ht) and a hidden-stateto-output connection (ht →at). At time step t, the model reads the input xt ∈RM and previous
hidden state ht−1 ∈RK to compute the hidden state ht (Eq. 2). Thus ht summarizes information
from all previous inputs x0, x1, ..., xt. The output at ∈Rk (Eq. 3) is generated by a transformation
function of ht, where k is the number of classes in the classiﬁcation tasks. To predict the label ˜yt,
at is then passed through a probabilistic function fprob to compute the vector of probabilities P =
[P (˜yt = 0 | xt, ..., x0) , ..., P (˜yt = k −1 | xt, ..., x0)] (Eq. 4), where P (0 | xt, ..., x0) , ..., P (k −1 | xt, ..., x0) ≥
0 and P (0 | xt, ..., x0) + ... + P (k −1 | xt, ..., x0) = 1. Denote by ai
t the element ith of the vector at.
For two classes, fprob is normally a logistic sigmoid function:
P (˜yt = 1 | xt, ..., x0) = sigmoid
and for multiple classes, fprob is a softmax function:
P (˜yt = i | xt, ..., x0) = softmax
for i = 0, ..., k −1.
The weighted matrices W ∈RK×M, U ∈RK×K and V ∈Rk×K and bias vectors b and c are shared
among all time steps. This allows the model to learn with varied length sequences and produce an
output at each time step as follows:
tanh (b + Wht−1 + Uxt)
At step 0, there is no previous hidden state, h0 is computed as tanh (b + Ux0).
The total loss for a sequence x0, x1, ..., xn and its corresponding labels y0, y1, ..., yn, where y0, y1, ..., yn ∈
[0, 1, .., k −1], would be the sum of the losses over all time steps:
L (y | x) =
Lt (˜yt = yt | xt...x0) = −
log P (˜yt = yt)
Back-propagation
RNNs can be trained to minimize the loss function using gradient descent. The derivatives with respect
to the parameters can be determined by the Back-Propagation Through Time algorithm . This
algorithm obtains the gradients by the chain rule like the standard back-propagation.
Challenge of long-term dependencies
Many experiments have shown that gradient based learning algorithms face diﬃculties in training RNN.
This is because the long term dependencies in long input sequences lead to vanishing or exploding
Figure 2: An LSTM unit that reads input xt and previous output state ht−1 and produces current output
state ht. An unit has a memory cell ct , an input gate it, an output gate ot and a forget gate f t.
gradients . Many approaches have been proposed to solve the problem, such as Leaky Units
 , Nonlinear AutoRegressive models with eXogenous (NARX) and Long-Short Term Memory
(LSTM) . Among them, LSTM has proved to be the most eﬀective for handling very long sequences
 , and thus will be chosen as a building block in our DeepCare.
Long-short term memory
This section reviews Long Short-Term Memory (LSTM) , a modiﬁed version of RNN, to address
the problem of long-term dependencies. Central to an LSTM is a linear self-loop memory cell which
allows gradients to ﬂow through long sequences. The memory cell is gated to moderate the amount of
information ﬂow into or from the cell. LSTMs have been signiﬁcantly successful in many applications,
such as machine translation , handwriting recognition and speech recognition .
Fig. 2 describes an LSTM unit. Instead of a simple RNN unit, an LSTM unit has a memory cell
that has state ct ∈RK at time t. The information ﬂowing through the memory cell is controlled by
three gates: an input gate, a forget gate and an output gate. The input gate it ∈RK controls the
input ﬂowing into the cell, the forget gate f t ∈RK controls the forgetting of the memory cell, and the
output gate ot ∈RK moderates the output ﬂowing from the memory cell. Before describing detailed
formulas, we denote the element-wise sigmoid function of a vector by σ and the element-wise product
of two vectors by ∗.
The three gates are all sigmoidal units which set every element of the gates to a value between 0
σ (Wixt + Uiht−1 + bi)
σ (Wfxt + Ufht−1 + bf)
σ (Woxt + Uoht−1 + bo)
where W{i,f,o}, U{i,f,o}, b{i,f,o} are parameters. The gates control the amount of information passing
through, from full when the gate value is 1, to complete blockage when the value is 0.
At each time step t, the input features are ﬁrst computed by passing input xt ∈RMand the previous
hidden state ht−1 ∈RK through a squashing tanh function:
gt = tanh (Wcxt + Ucht−1 + bc)
The memory cell is updated through partially forgetting the previous memory cell and reading the
moderated input features as follows:
f t ∗ct−1 + it ∗gt
The memory cell sequence is additive, and thus the gradient is also updated in a linear fashion through
the chain rule. This eﬀectively prevents the gradient from vanishing or exploding. The memory cell
plays a crucial role in memorizing past experiences through the learnable forgetting gates f t. If f t →1,
all the past memory is preserved, and new memory keeps updated with new inputs. If f t →0, only
new experience is updated and the system becomes memoryless.
Finally, a hidden output state ht is computed based on the memory ct, gated by the output gate
ot as follows:
ht = ot ∗tanh (ct)
Note that since the system dynamic is deterministic, ht is a function of all previous input: ht =
LSTM(x1:t).
The output states are then used to generate outputs.
We subsequently review two
output types: sequence labeling and sequence classiﬁcation.
LSTM for sequence labeling
The output states ht can be used to generate labels at time t as follows:
P (yt = l | x1:t) = softmax
for label speciﬁc parameters vl.
LSTM for sequence classiﬁcation
LSTMs can be used for sequence classiﬁcation using a simple mean-pooling strategy over all output
states coupled with a diﬀerentiable loss function. For example, in the case of binary outcome y ∈{0, 1},
P (y = 1 | x1:n) = LR (pool {LSTM(x1:n)})
where LR denotes probability estimate of the logistic regression, and pool {h1:n} = 1
Learning word representation
We use “word” to refer to a discrete element within a larger context (e.g., a word in a document, or
a diagnosis in an admission described in Sec. 4.2). Recall that input fed into many machine learning
models is often represented as a ﬁx-length feature vector. For text, bag-of-words are commonly used.
A word w is represented by a one-hot vector vw ∈R|V |, where vw =
w, ..., v|V |
and |V | is the
number of words in the dictionary: vw = [0, ..., 0, 1, 0, ..., 0] (vi
w = 1 if w = i, which implies w is the
word ith in the dictionary, and vi
w = 0, otherwise). Under bag-of-words representation, the vector of a
sentence w0, ..., wn is the sum of its word vectors: u = vw0 +vw1 +...+vwn. However, the bag-of-words
method fails to capture ordering and semantic of the words .
A powerful alternative to bag-of-words is to embed words into continuous distributed representation
in a vector space of M dimensions where M ≪|V | . Every word is map to a unique vector which
is a column in a matrix E ∈RM×|V |. There are several beneﬁts for word embedding. First, the
dimensionality is greatly reduced and does not depend on the appearance of new words. Second, the
semantic of a word is represented in a distributed fashion, that is, there are multiple elements that
encode the word meaning.
Third, manipulation of continuous vectors is much easier with current
algebraic tools such as addition and matrix multiplication, as evidenced in recent works . For
example, the similarity between two words is simply a cosine between two vectors. More importantly,
the embedding matrix E can be learnt from data.
There are various approaches to learn the embedding matrix E. The most popular approach is
perhaps Continuous Bag-of-Words model . For a word wi in a sequence of words, the model uses the
words surrounding wi to predict wi. With an input context size of C, wi−C, ..., wi−1, wi+1, ..., wi+C are
called context words of wi. All the context words are embedded into vectors using embedding matrix
E and then averaged to get the mean vector h
h = Ewi−C + ... + Ewi−1 + Ewi+1 + .. + Ewi+C
where Et is the column tth of the matrix E. The model then generates the output a = ¯Eh, where
¯E ∈R|V |×M and predict the center word wi using softmax function
P (wi | wi−C, ..., wi−1, wi+1, ..., wi+C) = softmax (a)
The parameters E and ¯E are learnt by minimizing the loss function
log P (wi | wi−C, ..., wi−1, wi+1, ..., wi+C)
through back-propagation using stochastic gradient descent.
Another approach to learn the embedding matrix E is language modeling with an RNN . More
formally, given a sequence of words: w0, w1, ..., wt, the objective is maximizing the log probability
log P (wt+1 | wt, ..., w1, w0). Each word wi in the sequence is embedded into vector xi = Ewi and the
sequence x0, x1, ..., xt is the input of an RNN. The model only produces the output at at the step t
(See Sec. 3.1, Eq. 3) and predict the next word using a multiclass classiﬁer with a softmax function
P (wt+1 | wt, ..., w1, w0) = softmax (at)
The loss function is L = 1
t=0 log p(wt+1 | wt, ..., w0). The matrix E and all the parameters of
the RNN model are learnt jointly through back-propagation using gradient descent.
In this section we present our main contribution named DeepCare for modeling illness trajectories and
predicting future outcomes. DeepCare is built upon LSTM to exploit the ability to model long-term
dependencies in sequences. We extend LSTM to address the three major challenges: (i) variable-size
discrete inputs, (ii) confounding interactions between disease progression and intervention, and (iii)
irregular timing.
Model overview
Recall from Sec. 2.1, there are two types of admission methods: planned and unplanned. Let mt be the
admission method at time step t, where mt = 1 indicates unplanned admission and mt = 2 indicates
planned admission. Let ∆t be the elapsed time between the current admission and its previous one.
As illustrated in Fig. 3, DeepCare is a deep dynamic neural network that has three main layers.
The bottom layer is built on LSTM whose memory cells are modiﬁed to handle irregular timing and
interventions. More speciﬁcally, the input is a sequence of admissions. Each admission t contains a set
of diagnosis codes (which is then formulated as a feature vector xt ∈RM), a set of intervention codes
(which is further formulated as a feature vector pt ∈RM), the admission method mt ∈{1,2} and the
elapsed time ∆t ∈R+. Denote by u0, u1, ..., un the input sequence, where ut = [xt, pt, mt, ∆t], the
LSTM computes the corresponding sequence of distributed illness states h0, h1, ..., hn, where ht ∈RK
(See Fig. 4b). The middle layer aggregates illness states through multiscale weighted pooling ¯h =
pool {h0, h1, ..., hn}, where ¯h ∈RsK for s scales.
The top layer is a neural network that takes pooled states and other statistics to estimate the ﬁnal
outcome probability, as summarized in Eq. (1) as
P (y | u0:n) = P (nnety (pool {LSTM(u0:n)}))
The probability P (y | u0:n) depends on the nature of outputs and the choice of statistical structure. For
example, for binary outcome, P (y = 1 | u0:n) is a logistic function; for multiclass outcome, P (y | u0:n)
is a softmax function; and for continuous outcome, P (y | u0:n) is a Gaussian. In what follows, we
describe the ﬁrst two layers in more detail.
Representing variable-size admissions
There are two main types of information recorded in an admission: (i) diagnoses of current condition;
and (ii) interventions. Interventions include procedures and medications. Diagnoses, procedures and
medications are coded using coding schemes which are described in Sec. 2.1.
These schemes are
hierarchical and the vocabularies are of tens of thousands in size. Thus for a problem, a suitable
coding level should be used for balancing between speciﬁcity and robustness.
Our approach is to embed admissions into vectors. Fig. 4a illustrates the embedding method. An
admission is a set of a varied number of codes (diagnoses and interventions). Codes are ﬁrst embedded
into vectors, analogous to word embedding described in Sec. 3.3. We then pool all the present diagnosis
vectors to derive xt ∈RM. Likewise, we derive the pooled intervention vector pt ∈RM. Finally, an
admission embedding is a 2M-dim vector [xt, pt].
Weighted pooling
Neural network
Concatenation
Weighted pooling
Weighted pooling
Figure 3: DeepCare architecture. The bottom layer is Long Short-Term Memory with irregular
timing and interventions ( see also Fig. 4).
Let D be the set of diagnosis codes and I be the set of intervention codes. The two sets are indexed
from 1 to |D| and from 1 to |I|, respectively. Denote diagnosis embedding matrix by A ∈RM×|D| and
intervention embedding matrix by B ∈RM×|I|. Let Aj is the jth column and Aj
i is the element at the
jthcolumn and the ith row of the matrix A. Let xi
t be the ith element of the vector xt and pi
ith element of the vector pt. Each admission t contains h diagnoses: d1, d2, ..., dh ∈{1, 2, .., |D|} and k
interventions: s1, s2, ..., sk ∈{1, 2, ..., |I|}. The admission is pooled by max, sum or mean pooling as
• Max pooling admission (max adm.). The pooling is element-wise as follows:
i , ..., Adh
t = max (Bs1
i , ..., Bsk
for i = 1, ..., M. This is analogous to paying selective attention to the element of the highest
impact among diagnoses and among interventions. It also resembles the usual coding practice
that one diagnosis is picked as the primary reason for admission.
Admission 𝑡
Procedures
Medications
Figure 4: (a) Admission embedding.
A and B are embedding matrices.
Discrete diagnoses and
interventions are embedded into 2 vectors xt and pt. (b) Modiﬁed LSTM unit as a carrier of illness
history. Compared to the original LSTM unit (Fig. 2), the modiﬁed unit models times, admission
methods, diagnoses and intervention
• Normalized sum pooling admission (sum adm.). In healthcare, risk loosely adds up. A patient
with multiple diseases (multiple comorbidities) is more likely to be at risk than those with single
condition. We propose the following normalized sum pooling method:
i + ... + Adh
i + ... + Adh
+ ... + Bsk
+ ... + Bsk
for i = 1, ..., M. The normalization is to reduce the eﬀect of highly variable length.
• Mean pooling admission (mean adm.). In absence of primary conditions, a mean pooling could
be a sensible choice:
xt = Ad1 + Ad2 + ... + Adh
pt = Bs1 + Bs2 + ... + Bsk
Admission as input
Once admission embedding has been derived, diagnosis embedding is used as input for the LSTM. As
interventions are designed to reduce illness, their eﬀect is modeled separately in Sec. 4.3. There are
two main types of admission: planned and unplanned. Unplanned admissions refer to transfer from
emergency attendances, which typically indicates higher risk. Recall from Eqs. (5,8) that the input
gate i control how much new information is updated into memory c. The gate can be modiﬁed to
reﬂect the risk level of admission type as follows:
σ (Wixt + Uiht−1 + bi)
where mt = 1 if the admission method is unplanned, mt = 2 otherwise, and σ is the element-wise
sigmoid function of a vector.
Modeling eﬀect of interventions
The intervention vector (pt) of an admission is modeled as illustrated in Fig. 4b. Since interventions
are designed to cure diseases or reduce patient’s illness, the output gate, which controls the illness
states, is moderated by the current intervention as follows:
ot = σ (Woxt + Uoht−1 + Popt + bo)
where Po is the intervention weight matrix for the output gate and pt is intervention at time step t.
Moreover, interventions may have long-term impacts (e.g., curing disease or introducing toxicity).
This suggests the illness forgetting is moderated by previous intervention
 Wfxt + Ufht−1 + Pfpt−1 + bf
where pt−1is intervention embedded vector at time step t −1 and Pf is the intervention weight matrix
for the forget gate.
Capturing time irregularity
When a patient’s history is modeled by LSTM (Sec. 3.2), the memory cell carries the illness history. But
this memory needs not be constant as illness states change over time. We introduce two mechanisms
of forgetting the memory by modiﬁed the forget gate f t in Eq. 15:
Time decay
There are acute conditions that naturally reduce their eﬀect through time. This suggests a simple
decay modeled in the forget gate f t:
f t ←d (∆t−1:t) f t
where ∆t−1:t is the time passed between step t −1 and step t, and d (∆t−1:t) ∈(0, 1] is a decay
function, i.e., it is monotonically non-increasing in time.
One function we found working well is
d(∆t−1:t) = [log(e + ∆t−1:t)]−1, where ∆t−1:t is measured in days and e ≈2.718 is the the base of the
natural logarithm.
Parametric time
Time decay may not capture all conditions, since some conditions can get worse, and others can be
chronic. This suggests a more ﬂexible parametric forgetting:
Wfxt + Ufht−1 + Qfq∆t−1:t + Pfpt−1 + bf
where q∆t−1:t is a vector derived from the time diﬀerence ∆t−1:t, Qf is the parametric time weight
matrix. For example, we may have: q∆t−1:t =
to model the thirddegree forgetting dynamics. ∆t−1:t is measured in days and is divided by 60, 180 and 365 to prevent
the vector q∆t−1:t from large values.
Prognosis through multiscale pooling and recency attention
Once the illness dynamics have been modeled using the memory LSTM, the next step is to aggregate
the illness states to infer about the future prognosis (Fig. 3). The simplest way is to use mean-pooling,
where ¯h = pool {h0:n} =
t=0 ht. However, this does not reﬂect the attention to recency in
healthcare. Here we introduce a simple attention scheme that weighs recent events more than old
ones: ¯h =
t=t0 rt, where
[mt + log (1 + ∆t:n)]−1
and ∆t:n is the elapsed time between the step t and the current step n, measured in months; mt = 1
if emergency admission, mt = 2 if routine admission. The starting time step t0 is used to control
the length of look-back in the pooling, for example, ∆t0:n ≤12 for one year look-back. Since diseases
progress at diﬀerent rates for diﬀerent patients, we employ multiple look-backs: 12 months, 24 months,
and all available history.
Finally, the three pooled illness states are stacked into a vector:
¯h12, ¯h24, ¯hall
which is then fed to a neural network for inferring about future prognosis.
Model complexity
The number of model parameters are M × |V | + M × K + K × K + K × D, which consists of the
following components:
Parameters in the LSTM layer
• For admission embedding, we use two embedding matrices A and B. We have A + B ∈RM×|V |
• The input gate: Wi ∈RM×K, Ui ∈RK×K and bi ∈RK×1
• The output gate: Wo ∈RM×K, Uo ∈RK×K, Po ∈RK×K and bo ∈RK×1
• The forget gate: Wf ∈RM×K, Uf ∈RK×K, Pf ∈RK×K and bf ∈RK×1. In the case of time
decay there are no other parameters and in the case of parametric time, the forget gate has a
time weight matrix Qf ∈RNtime×K (Ntime = 3 in our implementation)
• The memory cell: Wi ∈RM×K, Ui ∈RK×K and bi ∈RK×1
Parameters in the Neural network layer
• The neural network layer consists of an input-hidden weight matrix Uh1 ∈R3K×D, hidden-output
weight matrix Uh2 ∈RD×2 and two bias vectors c1 ∈RD×1 and c2 ∈R2x1
Once all the illness states are pooled and stacked into vector ¯h, ¯h is then fed to a neural network with
one hidden layer
 Uh¯h + bh
P (y | u1:n)
fprob (zy)
Learning is carried out through minimizing cross-entropy: L = −log P (y | u0:n). For example,
in the case of binary classiﬁcation, y ∈{0, 1}, we use logistic regression to represent P (y | u0:n), i.e.
P (y = 1 | u0:n) = σ (zy). The cross-entropy becomes
L = −y log σ −(1 −y) log (1 −σ)
Despite having a complex structure, DeepCare’s loss function is fully diﬀerentiable, and thus can
be minimized using standard back-propagation. The learning complexity is linear with the number of
parameters. See Alg. 1 for an overview of DeepCare forward pass.
Algorithm 1 DeepCare forward pass
Inputs: Patients’ disease history records
1: for each step t do
[xt, pt] = embedding(d1, ..., dh, s1, ..., sk) (Sec. 4.2)
Compute 3 gates: it (Eq. 13), ot (Eq. 14), f t (Eq. 16 or Eq. 17)
Compute ct (Eq. 9) and ht (Eq. 10)
5: end for
6: Compute ¯h (Sec. 4.5)
7: Compute P(y | u0:n) (Eq. 18,19,20)
8: Compute loss function L (Eq. 21)
Pretraining and regularization
Pretraining with auxiliary tasks
Pretraining can be done by unsupervised learning on unlabeled data . Pretraining has been
proven to be eﬀective because it helps the optimization by initializing weights in a region near a good
local minimum .
In our work we use auxiliary tasks to pretrain the model for future risk
prediction tasks. In our case, auxiliary tasks are predicting diagnoses of the next readmission and
predicting interventions of current admission. These tasks play a role in disease progression tracking
and intervention recommendation.
We use the bottom layer of DeepCare for training auxiliary tasks. As described in Sec. 4.1, the
LSTM layer reads a sequence of admissions u0, u1, ..., un and computes the corresponding sequence
of distributed illness states h0, h1, ..., hn.
At each step t, ht is used to generate labels yt by the
formula given in Eq. (11) where yt can be a set of diagnoses or interventions. After training, the code
embedding matrix is then used to initialize the embedding matrix for training the risk prediction tasks.
The results of next readmission diagnosis prediction and current admission intervention prediction are
reported in Sec. 5.3 and Sec. 5.4.
Regularization
DeepCare may lead to overﬁtting because it introduces three more parameter matrices to the sigmoid
gates to handle interventions and time. Therefore, we use L2-norm and Dropout to prevent overﬁtting.
L2-norm regularization, also called “weight decay”, is used to prevent weight parameters from extreme
A constant λ is introduced to control the magnitude of the regularization.
Dropout is a
regularization method for DNNs. During training, units are deleted with a pre-deﬁned probability
1 −p (dropout ratio) and the remaining parts are trained through back-propagation as usual .
This prevents the co-adaptation between units, and therefore prevents overﬁtting. At the test time,
a single neural net is used without dropout and the outgoing weights of a unit that is retained with
probability p during training are multiplied by p. This combines 2k(k is the number of units) shared
weight networks into a single neural network at test time. Therefore, dropout is also considered as an
ensemble method.
However, the original version of dropout does not work well with RNNs because it may hurt the
dependencies in sequential data during training . Thus, dropout in DeepCare is only introduced
at input layer and neural network layer:
(b) Admission
(c) Length of stay (days)
10 15 20 25 30 35 40
Type 2 diabetes
Hypertension
Tobacco use disorder
Type 2 w/o complication
Long-term use of insulin
Figure 5: Top row: Diabetes cohort statistics (y axis: number of patients; x axis: (a) age, (b) number
of admissions, (c) number of days); Mid row: Progression from pre-diabetes (upper diag. cloud) to
post-diabetes (lower diag. cloud); Bottom row: Top diagnoses.
• Dropout codes: Before pooling the embedding vectors of diagnoses and interventions in each
admission, each of these embedding vectors is deleted with probability 1 −pcode
• Dropout input features: After deriving [xt, pt] as described in Sec. 4.2, each value in these two
vector is dropped with probability 1 −pfeat
• Dropout units in neural network layer: The pooled state z as described in Sec. 4.2 is feed as
the input of the neural network. Dropout is used at input units with probability 1 −pin and at
hidden units with probability 1 −phidd.
Experiments
We model disease progression, intervention recommendation and future risk prediction in two very
diverse cohorts: mental health and diabetes. These diseases diﬀer in causes and progression.
(b) Admission
(c) Length of stay (days)
10 15 20 25 30 35 40
Severe depressive episode
Hypertension
Tobacco use current
Mental/behavioral disorder
due to alcohol use
Personal history of tobacco
use disorder
Figure 6: Top row: Mental health cohort statistics (y axis: number of patients; x axis: (a) age, (b)
number of admissions, (c) number of days); Mid row: Progression from pre-mental diseases (upper
diag. cloud) to post-mental diseases (lower diag. cloud); Bottom row: Top diagnoses.
Data for both cohorts were collected for 12 years from a large regional Australian hospital.
We preprocessed the datasets by removing (i) admissions with incomplete patient information; and (ii)
patients with less than 2 admissions. We deﬁne the vocabulary as the set of diagnosis, procedure and
medication codes. To reduce the vocabulary, we collapse diagnoses that share the ﬁrst 2 characters
into one diagnosis. Likewise, the ﬁrst digits in the procedure block are used.
The diabetes cohort contained more than 12,000 patients (55.5% males, median age 73). Data
statistics are summarized in Fig. 5. After preprocessing, the dataset contained 7,191 patients with
53,208 admissions.
The vocabulary consisted of 243 diagnosis, 773 procedure and 353 medication
codes. The mental health cohort contains more than 11,000 patients (49.4% males, median age 37).
Data statistics are summarized in Fig. 6. After preprocessing, the mental health dataset contained
6,109 patients and 52,049 admissions with the vocabulary of 247 diagnosis, 752 procedure and 319
medication codes. The average age of diabetic patients is much higher than the average age of mental
patients (See Fig 5a and Fig 6a).
Implementation
The training, validation and test sets are created by randomly dividing the dataset into three parts
of 2/3, 1/6, 1/6 data points, respectively. We vary the embedding and hidden dimensions from 5 to
50 but the results are rather robust. We report best results for disease progression and intervention
recommendation tasks with M = 30 and K = 40 and for prediction tasks with M = 10 embedding
dimensions and K = 20 hidden units (M and K are the number of embedding dimensions and hidden
units respectively). Learning is by Stochastic Gradient Descent with the mini-batch of 16 sequences.
The learning rate λ is modiﬁed as follows. We start with λ = 0.01. When the model cannot ﬁnd a
smaller training cost, we wait nwait epochs before updating λ as λ = λ/2. Initially, nwait = 5, and is
subsequently modiﬁed as nwait = min {15, nwait + 2} for each λ update. Learning is terminated after
nepoch = 200 or after learning rate smaller than ϵ = 0.0001.
Disease progression
We ﬁrst verify that the recurrent memory embedded in DeepCare is a realistic model of disease progression. The model predicts the next np diagnoses at each discharge using Eq. (11).
For comparison, we implement two baselines: Markov models and plain RNNs. Markov model is a
stochastic model used to model changing systems. A Markov model consists of a list of possible states,
the possible transitions between those states and the probability of those transitions. The future states
depend only on the present state (Markov assumption). The Markov model has memoryless disease
transition probabilities P
from disease dj to di at time t. Given an admission with disease
subset Dt, the next disease probability is estimated as Q
RNNs are described in Sec. 3.1.
We use Precision at K (Precision@K) to measure the performance of the models. Precision@K
corresponds to the percentage of relevant results in retrieved results. That means if the model predicts np diagnoses of the next readmission and nr diagnoses among of them are relevant the model’s
performance is
Precision@np = nr
Figure 7: (Left) 40 channels of forgetting due to time elapsed. (Right) The forget gates of a patient
in the course of their illness.
Table 1: Precision@np Diagnoses Prediction.
DeepCare (mean adm.)
DeepCare (sum adm.)
DeepCare (max adm.)
Dynamics of forgetting
Fig. 7(left) plots the contribution of time into the forget gate. The contributions for all 40 states
are computed using Qfq∆t as in Eq. (17). There are two distinct patterns: decay and growing. This
suggests that the time-based forgetting has a very small dimensionality, and we will under-parameterize
time using decay only as in Eq. (16), and over-parameterize time using full parameterization as in
Eq. (17). A right balance is interesting to warrant a further investigation. Fig. 7(right) shows the
evolution of the forget gates through the course of illness for a patient.
Diagnoses prediction result
Table 1 reports the Precision@np for diﬀerent values of np. For diabetes cohort, using plain RNN
improves over memoryless Markov model by 8.8% with np = 1 and by 27.7% with npred = 3. This
signiﬁcant improvement demonstrates the role of modeling the dynamics in sequential data. Modeling
irregular timing and interventions in DeepCare gains a further 2% improvement. For mental health
cohort, Markov model is failed to predict next diagnoses with only 9.5% for np = 1. Plain RNN gains
50% improvement in Precision@1, while and DeepCare demonstrates a 2% improvement in Precision@1
Table 2: Precision@np intervention prediction
DeepCare (mean adm.)
DeepCare (sum adm.)
DeepCare (max adm.)
Table 3: Eﬀect of pretraining and regularization for unplanned readmission prediction using DeepCare
for diabetes dataset. The results are reported in F-score (%)
Regularization
Intervention recommendation
Table 2 reports the results of current intervention prediction. For all values of np, RNN consistently
outperforms Markov model by a huge margin for both diabetes and mental health cohort. DeepCare
with sum-pooling outperforms other models in both diabetes and mental health datasets.
Predicting future risk
Next we demonstrate DeepCare on risk prediction. For each patient, a discharge is randomly chosen
as prediction point, from which unplanned readmission and high risk patients within X months will
be predicted. A patient is in high risk at a particular time T if he or she have at least three unplanned
readmissions within X months after time T. We choose X = 12 months for diabetes and X = 3
months for mental health.
For comparison, baselines are SVM and Random Forests running on standard non-temporal features engineering using one-hop representation of diagnoses and intervention codes. Then pooling is
applied to aggregate over all existing admissions for each patient. Two pooling strategies are tested:
max and sum.
Max-pooling is equivalent to the presence-only strategy in , and sum-pooling is
akin to an uniform convolutional kernel in .
This feature engineering strategy is equivalent to
zeros-forgetting – any risk factor occurring in the past is memorized.
Pretraining and Regularization
Table 3 reports the impacts of pretraining and regularization on the results of unplanned readmission
prediction in diabetes dataset using DeepCare model.
Pretraining and regularization improve the
results of all three admission pooling methods. While mean pooling admission is found to perform
well with regularization, max pooling produces best results with pretraining and sum pooling produces
best results with both approaches.
Table 4: Results of unplanned readmission prediction in F-score (%) within 12 months for diabetes
and 3 months for mental health patients (DC is DeepCare, inv. is intervention).
1. SVM (max-pooling)
2. SVM (sum-pooling)
3. Random Forests (max-pooling)
4. Random Forests (sum-pooling)
5. Plain RNN (logist. regress.)
6. LSTM (logit. regress.)
7. DC (nnets + mean adm.)
8. DC ( [inv.+time decay]+recent.multi.pool.+nnets+mean adm.)
9. DC ([inv.+param. time]+recent.multi.pool.+nnets+mean adm.)
Unplanned readmission prediction results
Table 4 reports the F-scores of predicting unplanned readmission. For the diabetes cohort, the best
baseline (non-temporal) is Random Forests with sum pooling has a F-score of 71.4% [Row 4]. Using
plain RNN with simple logistic regression improves over best non-temporal methods by a 3.7% difference in 12-months prediction [Row 5, ref: Sec. (3.1,4.2)]. Replacing RNN units by LSTM units
gains 4.5% improvement [Row 6, ref: Sec. 3.2]. Moving to deep models by using a neural network
as classiﬁer helps with a gain of 5.1% improvement [Row 7, ref: Eq. (1)]. By carefully modeling the
irregular timing, interventions and recency+multiscale pooling, we gain 5.7% improvement [Row 8,
ref: Secs. (4.4–4.5)]. Finally, with parametric time we arrive at 79.0% F-score, a 7.6% improvement
over the best baselines [Row 9, ref: Secs. (4.4)].
For the mental health dataset, the best non-temporal baseline is sum-pooling Random Forest with
result of 67.9%. Plain RNN and LSTM with logistic regression layer gain 2.6% and 3.8% improvements,
respectively.
The best model is DeepCare with parametric time with a gap of 6.8% improvement
compared to sum-pooling Random Forest.
High risk prediction results
In this part, we report the performance of DeepCare on high risk patient prediction task. Figure 8
reports the F-score of high risk prediction. RNN improves the best non-temporal model (sum-pooling
SVM) over 10% F-score for both two cohorts. Max-pooling DeepCare best performs in diabetes dataset
with nearly 60% F-score, while sum-pooling DeepCare wins in mental health cohort with 50.0% F-score.
Discussion and Conclusion
Discussion
DeepCare was partly inspired by human memory . There are three kinds of related memory: semantic, episodic and working memory. Semantic memory stores the meaning of concepts and their
relations. Episodic memory refers to the storage of experiences triggered by an event, for example, wedding or earthquake. Working memory is a system of temporarily loading and processing information
as part of complex cognitive tasks.
(a) Diabetes
(b) Mental health
Figure 8: Result of high risk prediction in F-score (%) within 12 months for diabetes (a) and 3 months
for mental health (b). DC is DeepCare. Mean, sum, max are 3 admission pooling methods
DeepCare makes use of embedding to represent the semantics of diagnoses, interventions and admissions. In theory, this embedding can be estimated independently of the task at hand. Our previous
work learns diagnosis and patient embedding using nonnegative restricted Boltzmann machines
 and known semantic relations and temporal relations . This method uses global contexts,
unlike DeepCare, where only local contexts (e.g., next admission) are considered.
The memory cells in DeepCare are used to store, update, forget and manipulate illness experiences
over time-stamped episodes. The inferred experiences are then pooled to reason about the current
illness states and the future prognosis. Like human memory, healthcare risk also has a recency eﬀect,
that is, more recent events contribute more into the future risk. In DeepCare, two recency mechanisms
are used. First, through forgetting, recent events in DeepCare tend to contribute more to the current
illness states. Second, multiscale pooling as in Sec. 4.5 has weights decayed over time.
DeepCare can be implemented on existing EMR systems. For that more extensive evaluations on
a variety of cohorts, sites and outcomes will be necessary. This oﬀers opportunities for domain adaptations through parameter sharing among multiple cohorts and hospitals. Modeling-wise, DeepCare
can also be extended to predict a sequence of outcomes at speciﬁc timing, in the same spirit as the
sequence to sequence mapping in . Future work also includes more ﬂexibility in time parameterization such as using radial basis expansion and splines. Further, DeepCare is generic so it can be applied
to not only medical data but also other kinds of sequential data which contain long-term dependencies,
sequence of sets, irregular time and interventions.
Conclusion
In this paper we have introduced DeepCare, an end-to-end deep dynamic memory neural network for
personalized healthcare. It frees model designers from manual feature extraction. DeepCare reads
medical records, memorizes illness trajectories and care processes, estimates the present illness states,
and predicts the future risk. Our framework models disease progression, supports intervention recom-
mendation, and provides prognosis from electronic medical records. To achieve precision and predictive
power, DeepCare extends the classic Long Short-Term Memory by (i) embedding variable-size discrete
admissions into vector space, (ii) parameterizing time to enable irregular timing, (iii) incorporating
interventions to reﬂect their targeted inﬂuence in the course of illness and disease progression; (iv)
using multiscale pooling over time; and ﬁnally (v) augmenting a neural network to infer about future
outcomes. We have demonstrated DeepCare on predicting next disease stages, recommending interventions, and estimating unplanned readmission among diabetic and mental health patients. The results
are competitive against current state-of-the-arts. DeepCare opens up a new principled approach to
predictive medicine.