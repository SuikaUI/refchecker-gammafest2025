Using Deep Learning to Detect Price Change
Indications in Financial Markets
Avraam Tsantekidis∗, Nikolaos Passalis∗, Anastasios Tefas∗,
Juho Kanniainen†, Moncef Gabbouj‡ and Alexandros Iosiﬁdis‡§
∗Department of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece
{avraamt, passalis}@csd.auth.gr, 
†Laboratory of Industrial and Information Management, Tampere University of Technology, Tampere, Finland
 
‡Laboratory of Signal Processing, Tampere University of Technology, Tampere, Finland
{moncef.gabbouj, alexandros.iosiﬁdis}@tut.ﬁ
§Department of Engineering, Electrical and Computer Engineering, Aarhus University, Denmark
 
Abstract—Forecasting ﬁnancial time-series has long been
among the most challenging problems in ﬁnancial market
analysis. In order to recognize the correct circumstances to
enter or exit the markets investors usually employ statistical
models (or even simple qualitative methods). However, the
inherently noisy and stochastic nature of markets severely
limits the forecasting accuracy of the used models. The
introduction of electronic trading and the availability of
large amounts of data allow for developing novel machine
learning techniques that address some of the difﬁculties
faced by the aforementioned methods. In this work we
propose a deep learning methodology, based on recurrent
neural networks, that can be used for predicting future
price movements from large-scale high-frequency timeseries data on Limit Order Books. The proposed method
is evaluated using a large-scale dataset of limit order book
I. INTRODUCTION
Using mathematical models to gain an advantage in
ﬁnancial markets is the main consideration of the ﬁeld of
quantitative analysis. The main hypothesis of the ﬁeld is
that the utilization time-series of values like the price and
volume of ﬁnancial products produced by the market can
be analyzed with mathematical and statistical models to
extract predictions about the current state of the market
and future changes in metrics, such as the price volatility
and direction of movement. However, these mathematical models rely on handcrafted features and have their
parameters tuned manually by observation, which can
reduce the accuracy of their predictions. Furthermore,
asset price movements in the ﬁnancial markets very
frequently exhibit irrational behaviour since they are
largely inﬂuenced by human activity that mathematical
models fail to capture.
Recently there have been multiple solution to the
aforementioned limitations of handcrafted systems using
machine learning models. Given some input features
machine learning models can be used to predict the
behaviour of various aspects of ﬁnancial markets , ,
 , . This has led several organizations, such as hedge
funds and investment ﬁrms, to create machine learning
models alongside the conventional mathematical models
for conducting their trading operation.
With the introduction of electronic trading and the automation that followed has increased the trading volume
thus producing a immense amount of data that representing the trades happening in exchanges. Exchanges have
been gathering this trading data, creating comprehensive
logs of every transaction, selling them to ﬁnancial institutions that analyze them to discover signals that provide
foresight for changes in the market, which can in turn be
used by algorithms to make the proﬁtably manage investments. However, applying machine learning techniques
on such large-scale data is not a straightforward task.
Being able to utilize the information at this scale can
provide strategies for many different market conditions
but also safeguard from volatile market movements.
The main contribution of this work is the proposal of
a deep learning methodology, based on recurrent neural
networks, that can be used for predicting future midprice movements from large-scale high-frequency limit
order data.
In Section 2 related work on machine learning models
that were applied on ﬁnancial data is brieﬂy presented.
Then, the used large-scale dataset is described in detail
in Section 3. In Section 4 the proposed deep learning
2017 25th European Signal Processing Conference (EUSIPCO)
ISBN 978-0-9928626-7-1 © EURASIP 2017
methodology is introduced, while in Section 5 the experimental evaluation is provided. Finally, conclusions
are drawn and future work is discussed in Section 6.
II. RELATED WORK
Recent Deep Learning methods has been shown to
signiﬁcantly improve upon previous machine learning
techniques in tasks such as speech recognition ,
image captioning , , and question answering .
Deep Learning models, such as Convolutional Neural Networks (CNNs) , and Recurrent Neural Networks (RNNs), e.g., the Long Short-Term Memory Units
(LSTMs) , have greatly contributed in the increase
of performance on these ﬁelds, with ever deeper architectures producing even better results .
In Deep Portfolio Theory , the authors use autoencoders to optimize the performance of a portfolio and
beat several proﬁt benchmarks, such as the biotechnology IBB Index. Similarly in , a Restricted Boltzmann
Machine (RBM) is used to encode monthly closing
prices of stocks and then it is ﬁne-tuned to predict the
direction the price of each stock will move (above or
below the median change). This strategy is compared to
a simple momentum strategy and it is established that
the proposed method achieves signiﬁcant improvements
in annualized returns.
The daily data of the S&P 500 market fund prices and
Google domestic trends of 25 terms like “bankruptcy”
and “insurance” are used as the input to a recurrent
neural network that it is trained to predict the volatility
of the market fund’s price . This method greatly improves upon existing benchmarks, such as autoregressive
GARCH and Lasso techniques.
An application using high frequency limit orderbook
(LOB) data is , where the authors create a set of
handcrafted features, such as price differences, bidask spreads, and price and volume derivatives. Then,
a Support Vector Machine (SVM) is trained to predict
whether the mid-price will move upwards or downward
in the near future using these features. However, only
2000 data points are used for training the SVM in each
training round, limiting the prediction accuracy of the
To the best of our knowledge this is the ﬁrst work that
uses a Limit Order Book data on such a large-scale with
more than 4 million events to train LSTMs for predicting
the price movement of stocks. The method proposed in
this paper is also combined with an intelligent normalization scheme that takes into account the differences in the
price scales between different stocks and time periods,
which is essential for effectively scaling to such largescale data.
III. HIGH FREQUENCY LIMIT ORDER DATA
In ﬁnancial equity markets a limit order is a type of
order to buy or sell a speciﬁc number of shares within
a set price. For example, a sell limit order (ask) of $10
with volume of 100 indicates that the seller wishes to sell
the 100 shares for no less that $10 each. Respectively,
a buy limit order (bid) of $10 it means that the buyer
wishes to buy a speciﬁed amount of shares for no more
than $10 each.
Consequently the orderbook has two sides, the bid
side, containing buy orders with prices pb(t) and volumes vb(t), and the ask side, containing sell orders with
prices pa(t) and volumes va(t). The orders are sorted
on both sides based on the price. On the bid side p(1)
is the is the highest available buy price and on the ask
a (t) is the lowest available sell price.
Whenever a bid order price exceeds an ask order price
b (t) > p(j)
a (t), they “annihilate”, executing the orders
and exchanging the traded assets between the investors.
If there are more than two orders that fulﬁll the price
range requirement the effect chains to them as well.
Since the orders do not usually have the same requested
volume, the order with the greater size remains in the
orderbook with the remaining unfulﬁlled volume.
Several tasks arise from this data ranging from the
prediction of the price trend and the regression of the
future value of a metric, e.g., volatility, to the detection
of anomalous events that cause price jumps, either upwards or downwards. These tasks can lead to interesting
applications, such as protecting the investments when
market condition are unreliable, or taking advantage of
such conditions to create automated trading techniques
for proﬁt.
Methods utilizing this data often use subsampling
techniques, such as the OHLC (Open-High-Low-Close)
resampling , to limit the number of values exist
for each timeframe, e.g., every minute or every day.
Even though the OHLC method preserves the trend
features of the market movements, it removes all the
microstructure information of the markets. Note that it
is difﬁcult to preserve all the information contained in
the LOB data, since orders arrive inconsistently and most
methods require a speciﬁc number of features for each
time step. This is one of the problems RNNs can solve
and take full advantage of the information contained in
the data, since they can natively handle this inconsistent
amount of incoming orders.
2017 25th European Signal Processing Conference (EUSIPCO)
ISBN 978-0-9928626-7-1 © EURASIP 2017
IV. LSTMS FOR FINANCIAL DATA
The input data consists of 10 orders for each side of
the LOB (bid and ask). Each order is described by 2
values, the price and the volume. In total we have 40
values for each timestep. The stock data, provided by
Nasdaq Nordic, come from the Finnish companies Kesko
Oyj, Outokumpu Oyj, Sampo, Rautaruukki and Wartsila
Oyj. The time period used for collecting that data ranges
from the 1st to the 14th June 2010 (only business days
are included), while the data are provided by the Nasdaq
Nordic data feeds .
The dataset is made up of 10 days for 5 different
stocks and the total number of messages is 4.5 million
with equally many separate depths. Since the price and
volume range is much greater than the range of the
values of the activation function of our neural network,
we need to normalize the data before feeding them to
the network. To this end, standardization (z-score) is
employed to normalize the data:
xnorm = x −¯x
where x is the vector of values we want to normalize, ¯x
is the mean value of the data and σ¯x is the standard
deviation of the data. Instead of simply normalizing
all the values together, we take into account the scale
differences between order prices and order volumes
and we use a separate normalizer, with different mean
and standard deviation, for each of them. Also, since
different stocks have different price ranges and drastic
distributions shifts might occur in individual stocks for
different days, the normalization of the current day’s
values uses the mean and standard deviation calculated
using previous day’s data.
We want to predict the direction towards which the
price will change. In this work the term price is used to
refer to the mid-price of a stock, which is deﬁned as the
mean between the best bid price and best ask price at
a (t) + p(1)
This is a virtual value for the price since no order can
happen at that exact price, but predicting its upwards
or downwards movement provides a good estimate of
the price of the future orders. A set of discrete choices
must be constructed from our data to use as targets for
our classiﬁcation model. Simply using pt > pt+k to
determine the direction of the mid price would introduce unmanageable amount of noise, since the smallest
change would be registered as an upward or downward
Note that each consecutive depth sample is only
slightly different from the previous one. Thus the shortterm changes between prices are very small and noisy.
In order to ﬁlter such noise from the extracted labels we
use the following smoothed approach. First, the mean of
the previous k mid-prices, denoted by mb, and the mean
of the next k mid-prices, denoted by ma, are deﬁned as:
where pt is the mid price as described in Equation (2).
Then, a label lt that express the direction of price movement at time t is extracted by comparing the previously
deﬁned quantities (mb and ma):
if mb(t) > ma(t) · (1 + α)
if mb(t) < ma(t) · (1 −α)
where the threshold α is set as the least amount of
change in price that must occur for it to be considered
upward or downward. If the price does not exceed this
limit, the sample will be considered to belong to the
stationary class. Therefore, the resulting label expresses
the current trend we wish to predict. Note that this
process is applied for every time step in our data.
LSTM , is employed to classify our data. The
LSTM solves the problem of vanishing gradients, which
makes virtually impossible for an RNN to learn to
correlate temporally distant events. This is achieved
by protecting its hidden activation using gates between
each of its transaction points with the rest of its layer.
The hidden activation that is protected is called the cell
state. The following equations describe the behavior of
the LSTM model :
ft = σ(Wxfx + Whfht−1 + bf)
it = σ(Wxix + Whiht−1 + bi)
t = tanh(Whcht−1 + Wxcxt + bc)
ct = ftct−1 + itc′
ot = σ(Wocct + Wohht−1 + bo)
ht = otσ(ct)
where ft, it and ot are the activations of the input, forget
and output gates at time-step t, which control how much
2017 25th European Signal Processing Conference (EUSIPCO)
ISBN 978-0-9928626-7-1 © EURASIP 2017
of the input and the previous state will be considered and
how much of the cell state will be included in the hidden
activation of the network. The protected cell activation at
time-step t is denoted by ct, whereas ht is the activation
that will be given to other components of the model.
The parameters of the model are learned by minimizing the categorical cross entropy loss deﬁned as:
yi · log ˆyi
parameters
Wxf, Whf, Wxi, Whi, Whc, Wxc, Woc, Woh, bf, bi,
bc, and bo. The ground truth vector is denoted by y,
while ˆy is the predicted label distribution. The loss
is summed over all samples in each batch. The most
commonly used method to minimize the loss function
deﬁned in Equation (12) and learn the parameters W
of the model is gradient descent :
W′ = W −η · ∂L
where W′ are the parameters of the model after each
gradient descent step and η is the learning rate. In
this work we utilize the Adaptive Moment Estimation
algorithm, known as ADAM , which ensures that
the learning steps are scale invariant with respect to the
parameter gradients.
The input to the LSTM is a sequence of vectors X =
{x0, x1, . . . , xn} that represent the LOB depth at each
time step t. Each xt is fed sequentially to the LSTM
and its output yt expresses the categorical distribution
for the three direction labels (upward, downward and
stationary), as described in Equation (5), for each timestep t.
V. EXPERIMENTAL EVALUATION
In our ﬁrst attempt to train an LSTM network to
predict the mid-price trend direction we noticed a very
interesting pattern in the mean cost per recurrent step,
as shown in Figure 1. The cost is signiﬁcantly higher on
the initial steps before it eventually settles. This happens
because it is not possible for the network to build a
correct internal representation having seen only a few
samples of the depth. To avoid this unnecessary source
of error and noise in the training gradients, we do not
propagate the error for the ﬁrst 100 recurrent steps. These
steps are treated as a ”burn-in” sequence, allowing the
network to observe a portion of the LOB depth timeline
before making an accountable prediction.
Fig. 1: Mean cost per recurrent step of the LSTM
TABLE I: Experimental results for different prediction
horizons k
Mean Recall
Mean Prec.
Prediction Horizon k = 10
Prediction Horizon k = 20
Prediction Horizon k = 50
Experimentally we found out that to avoid over-ﬁtting
the hidden layer of the LSTM should contain 32 to 64
hidden neurons. If more hidden neurons are used, then
the network can easily overﬁt the data, while if less
hidden neurons are used the network under-ﬁts the data
reducing the accuracy of the predictions.
We use an LSTM with 40 hidden neurons followed
by a feed-forward layer with Leaky Rectifying Linear
Units as activation function . We split our dataset as
follows. The ﬁrst 7 days are used to train the network,
while the next 3 days are used as test data. We train
the same model for 3 different prediction horizons k, as
deﬁned in Equations (3) and (4).
To measure the performance of our model we use
Kohen’s kappa , which is used to measure the
concordance between sets of given answers, taking into
consideration the possibility of random agreements happening. We also report the mean recall, precision and F1
score between all 3 classes. Recall is the number true
positive samples divided by the sum of true positives
and false negatives, while precision is the number of
true positive divided by the sum of true positives and
false positives. F1 score is the harmonic mean of the
precision and recall metrics.
2017 25th European Signal Processing Conference (EUSIPCO)
ISBN 978-0-9928626-7-1 © EURASIP 2017
The results of our experiments are shown in Table I.
We compare our results with those of a Linear SVM
model and an MLP model with Leaky Rectiﬁers as
activation function. The SVM model is trained using
stochastic gradient descent since the dataset is too large
to use a closed-form solution. The MLP model uses a
single hidden layer with 128 neurons with Leaky ReLU
activations. The regularization parameter of the SVM
was chosen using cross validation on a split from the
training set. Since both models are sequential, we feed
the concatenation of the previous 100 depth samples as
input and we use as prediction target the price movement
associated with the last depth sample. The proposed
method signiﬁcantly outperforms all the other evaluated
models, especially for short term prediction horizons
(t = 10 and t = 20).
VI. CONCLUSION
In this work we trained an LSTM network on high
frequency LOB data, applying a temporally aware normalization scheme on the volumes and prices of the
LOB depth. The proposed approach was evaluated using
different prediction horizons and it was demonstrated
that it performs signiﬁcantly better than other techniques,
such as Linear SVMs and MLPs, when trying to predict
short term price movements.
There are several interesting future research directions.
First, more data can be used to train the proposed model,
scaling up to a billion training samples, to determine
if using more data leads to better classiﬁcation performance. With more data also increase the ”burn-in”
phase along with the prediction horizon to gauge the
models ability to predict the trend further into the future.
Also, an attention mechanism , , can be introduced to allow the network to capture only the relevant
information and avoid noise. Finally, more advanced
trainable normalization techniques can be used, as it was
established that normalization is essential to ensure that
the learned model will generalize well on unseen data.
ACKNOWLEDGMENT
The research leading to these results has received
funding from the H2020 Project BigDataFinance MSCA-
ITN-ETN 675044 ( Training for
Big Data in Financial Research and Risk Management.
Alexandros Iosiﬁdis was supported from the Academy of
Finland Postdoctoral Research Fellowship (No. 295854).
He joined Aarhus University on August 2017.