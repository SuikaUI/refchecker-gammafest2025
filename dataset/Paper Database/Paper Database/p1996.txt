Adversarial Robustness vs. Model Compression, or Both?
Shaokai Ye1*
Kaidi Xu2∗Sijia Liu3 Jan-Henrik Lambrechts1 Huan Zhang5
Aojun Zhou4 Kaisheng Ma1+ Yanzhi Wang2+ Xue Lin2+
1IIIS, Tsinghua University & IIISCT, China 2Northeastern University, USA
3MIT-IBM Watson AI Lab, IBM Research
4SenseTime Research, China
5University of California, Los Angeles, USA
It is well known that deep neural networks (DNNs) are
vulnerable to adversarial attacks, which are implemented
by adding crafted perturbations onto benign examples.
Min-max robust optimization based adversarial training
can provide a notion of security against adversarial attacks.
However, adversarial robustness requires a signiﬁcantly
larger capacity of the network than that for the natural
training with only benign examples. This paper proposes
a framework of concurrent adversarial training and weight
pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the
dilemma of adversarial training. Furthermore, this work
studies two hypotheses about weight pruning in the conventional setting and ﬁnds that weight pruning is essential for
reducing the network model size in the adversarial setting;
training a small model from scratch even with inherited
initialization from the large model cannot achieve neither
adversarial robustness nor high standard accuracy. Code
is available at 
Robustness-Aware-Pruning-ADMM.
1. Introduction
Deep learning or deep neural networks (DNNs) have
achieved extraordinary performance in many application
domains such as image classiﬁcation , object detection and recognition , natural language processing and medical image analysis . Besides
deployments on the cloud, deep learning has become ubiquitous on embedded systems such as mobile phones, IoT
devices, personal healthcare wearables, autonomous driving , unmanned aerial systems , etc.
It has been well accepted that DNNs are vulnerable to
adversarial attacks , which raises concerns
*Equal contribution
1Institute for Interdisciplinary Information Core Technology
+ Corresponding Authors
of DNNs in security-critical applications and may result in
disastrous consequences. For example, in autonomous driving, a stop sign may be mistaken by a DNN as a speed limit
sign; malware may escape from deep learning based detection; and in authentication using face recognition, unauthorized people may escalate their access rights by fooling the
Adversarial attacks are implemented by generating adversarial examples, i.e., adding sophisticated perturbations
onto benign examples, such that adversarial examples are
classiﬁed by the DNN as target (wrong) labels instead of
the correct labels of the benign examples. The adversary
may have white-box accesses to the DNN where the adversary has full information about the model (e.g., structure and weight parameters) ; or black-box accesses where the adversary can only make queries and observe outputs . The black-box scenarios are of particular interest in the Machine Learning as a Service (MLaaS)
paradigm, speciﬁcally in some cases where DNN models
trained through the cloud platform cannot be downloaded
and are accessed only through the service’s API.
According to , defenses that cause obfuscated gradients may provide a false sense of security and can be
overcome with improved attack techniques such as backward pass differentiable approximation, expectation over
transformation, and reparameterization. Also pointed out
in , adversarial training leveraging min-max robust optimization does not have obfuscated gradients issue
and can be a promising defense mechanism. Since that researchers have begun to notice the issue when designing
new defenses, more defenses have been proposed including adversarial training based ones and others .
Min-max robust optimization based adversarial training can provide a notion of security against all
ﬁrst-order adversaries (i.e., attacks that rely on gradients of
the loss function with respect to the input), by modeling an
universal ﬁrst-order attack through the inner maximization
problem while the outer minimization still representing the
 
training process. However, as noted by , adversarial robustness requires a signiﬁcant larger architectural capacity
of the network than that for the natural training with only
benign examples. For example, we may need to quadruple a DNN model with state-of-the-art standard accuracy
on MNIST for strong adversarial robustness. In addition,
increasing the network capacity may provide a better tradeoff between standard accuracy of an adversarially trained
model and its adversarial robustness .
Therefore, the required large network capacity by adversarial training may limit its use for security-critical scenarios especially in resource constrained application systems.
On the other hand, model compression techniques such as
weight pruning have been essential
for implementing DNNs on resource constrained embedded and IoT systems. Weight pruning explores weight sparsity to prune synapses and neurons without notable accuracy degradation. References theoretically discuss
the relationship between adversarial robustness and weight
sparsity, but do not apply any active defense techniques in
their research. The work concludes that moderate sparsity can help with adversarial robustness in that it increases
the ℓp norm of adversarial examples (although DNNs with
weight sparsity are still vulnerable under attacks).
We are motivated to investigate whether and how weight
sparsity can facilitate an active defense technique i.e., the
adversarial training, by relaxing the network capacity requirement. Figure 1 characterizes the weight distribution of
VGG-16 network on CIFAR dataset. We test on the original size, 1/2 size, and 1/4 size of VGG-16 network for their
standard accuracy and adversarial accuracy. We have following observations: (i) Smaller model size (network capacity) indicates both lower standard accuracy and adversarial accuracy for adversarially trained model. (ii) Adversarially trained model is less sparse (fewer zero weights)
than naturally trained model. Therefore, pre-pruning before
adversarial training is not a feasible solution and it seems
harder to prune an adversarially trained model.
This paper tries to answer the question of whether we
can enjoy both the adversarial robustness and model compression together. Basically, we integrate weight pruning
with the adversarial training to enable security-critical applications in resource constrained systems.
Our Contributions:
We build a framework that
achieves both adversarial robustness and model compression through implementing concurrent weight pruning and
adversarial training.
Speciﬁcally, we use the ADMM
(alternating direction method of multipliers) based pruning in our framework due to its compatibility with
adversarial training. More importantly, the ADMM based
pruning is universal in that it supports both irregular pruning and different kinds of regular pruning, and in this way
we can easily switch between different pruning schemes
for fair comparison. Eventually, our framework tackles the
dilemma of adversarial training.
We also study two hypotheses about weight pruning that
were proposed for the conventional model compression setting and experimentally examine their validness for the adversarial training setting. We ﬁnd that the weight pruning
is essential for reducing the network model size in the adversarial setting, training a small model from scratch even
with inherited initialization from the large model cannot
achieve adversarial robustness and high standard accuracy
at the same time.
With the proposed framework of concurrent adversarial
training and weight pruning, we systematically investigate
the effect of different pruning schemes on adversarial robustness and model compression.
We ﬁnd that irregular
pruning scheme is the best for preserving both standard accuracy and adversarial robustness while pruning the DNN
2. Related Work
2.1. Adversarial Training
Adversarial training uses a min-max robust optimization formulation to capture the notion of security
against adversarial attacks.
It does this by modeling an
universal ﬁrst-order attack through the inner maximization
problem while the outer minimization still represents the
training process.
Speciﬁcally, it solves the optimization
δ∈∆L(θ, x + δ, y)
where pairs of examples x ∈Rd and corresponding labels y ∈[k] follow an underlying data distribution D; δ
is the added adversarial perturbation that belongs to a set
of allowed perturbations ∆⊆Rd for each example x;
θ ∈Rp presents the set of weight parameters to be optimized; and L(θ, x, y) is the loss function, for instance, the
cross-entropy loss for a DNN.
The inner maximization problem is solved by sign-based
projected gradient descent (PGD), which presents a powerful adversary bounded by the ℓ∞-ball around x as:
xt+1 = Πx+∆
 xt + α sgn(∇xL(θ, x, y))
where t is the iteration index, α is the step size, and sgn(·)
returns the sign of a vector. PGD is a variant of IFGSM
attack and can be used with random start to add uniformly distributed noise to model ∆during adversarial
One major drawback of adversarial training is that it
needs a signiﬁcantly larger network capacity for achieving
strong adversarial robustness than for correctly classifying
Weights value
(i)natural train:
standard acc
(ii)adv train:
standard acc
adversarial acc
Weights value
(i)natural train:
standard acc
(ii)adv train:
standard acc
adversarial acc
Weights value
(i)natural train:
standard acc
(ii)adv train:
standard acc
adversarial acc
Figure 1: Weight distribution of VGG-16 network with (a) original size, (b) 1/2 size, and (c) 1/4 size on CIFAR dataset. For each size in one subﬁgure,
the weights are characterized for (i) a naturally trained model and (ii) an adversarially trained model. The standard accuracy and adversarial accuracy are
marked with the legend.
benign examples only . In addition, adversarial training
suffers from a more signiﬁcant overﬁtting issue than the natural training . Later in this paper, we will demonstrate
some intriguing ﬁndings related to the above mentioned observations.
2.2. Weight Pruning
Weight pruning as a model compression technique has
been proposed for facilitating DNN implementations on
resource constrained application systems, as it explores
weight sparsity to prune synapses and consequently neurons without notable accuracy degradation. There are in
general the regular pruning scheme that can preserve the
model’s structure in some sense, and otherwise the irregular pruning scheme. Regular pruning can be further categorized as the ﬁlter pruning scheme and the column pruning
scheme. Filter pruning by the name prunes whole ﬁlters
from a layer. Column pruning prunes weights for all ﬁlters
in a layer, at the same locations. Please note that some references mention channel pruning, which by the name prunes
some channels completely from the ﬁlters. But essentially
channel pruning is equivalent to ﬁlter pruning, because if
some ﬁlters are pruned in a layer, it makes the corresponding channels of next layer invalid .
In this work, we implement and investigate the ﬁlter
pruning, column pruning, and irregular pruning schemes
in the adversarial training setting. Also, with each pruning scheme, we uniformly prune every layer by the same
pruning ratio. For example, when we prune the model size
(network capacity) by a half, it means the size of each layer
is reduced by a half.
There are existing irregular pruning work 
and regular pruning work . In addition, almost all the regular pruning work are actually ﬁlter
pruning, except the work which is the ﬁrst to propose
column pruning and work which can implement column pruning through an ADMM based approach. In this
work, we use the ADMM approach due to its potential for
all the pruning schemes and its compatibility with adversarial training, as shall be demonstrated in the later section.
Researchers have also begun to reﬂect and make some
hypotheses about the weight pruning.
The lottery ticket
hypothesis conjectures that inside the large network,
a subnetwork together with their initialization makes the
pruning particular effective, and together they are termed
as the “winning tickets”. In this hypothesis, the original
initilizaiton of the sub-network (before the large network
pruning) is needed for it to achieve competitive performance
when trained in isolation. In addition, the work concludes that training a predeﬁned target model from scratch
is no worse or even better than applying structured (regular)
pruning on a large over-parameterized model to the same
target model architecture.
However, these hypotheses and ﬁndings are proposed for
the general weight pruning. In this paper, we make some
intriguing observations about weight pruning in the adversarial setting, which are insufﬁciently explained under the
existing hypotheses .
Concurrent
Adversarial
Weight Pruning
In this section, we provide the framework for concurrent adversarial training and weight pruning. We formulate
the problem in a way that lends itself to the application of
ADMM (alternating direction method of multipliers):
δ∈∆L(θ, x + δ, y)
θi = zi, i = 1, . . . , N.
Here θi are the weight parameters in each layer.
is an indicator function to incorporate weight sparsity constraint (different weight pruning schemes can be deﬁned
through the set Si). zi are auxiliary variables that enable
the ADMM based solution.
The ADMM framework is built on the augmented Lagrangian of an equality constrained problem . For problem (3), the augmented Lagrangian form becomes
L({θi}, {zi}, {ui}) = E(x,y)∼D
δ∈∆L(θ, x + δ, y)
i (θi −zi) + ρ
where {ui} are Lagrangian multipliers associated with
equality constraints of problem (3), and ρ > 0 is a given
augmented parameter. Through formation of the augmented
Lagrangian, the ADMM framework decomposes problem
(3) into two subproblems that are solved iteratively:
i } = arg min
{θi} L({θi}, {zk−1
i } = arg min
{zi} L({θk
i }, {zi}, {uk−1
where k is the iteration index. The Lagrangian multipliers
are updated as uk
The ﬁrst subproblem (6) is explicitly given by
δ∈∆L(θ, x + δ, y)
The ﬁrst term in the objective function of (8) is a min-max
problem. Same as solving the adversarial training problem
in Section 2.1, here we can use the PGD adversary (2) with
T iterations and random start for the inner maximization
problem. The inner problem is tractable under an universal ﬁrst-order adversary . The second convex quadratic
term in (8) arises due to the presence of the augmented term
in (5). Given the adversarial perturbation δ, we can apply the stochastic gradient decent algorithm for solving the
overall minimization problem. Due to the non-convexity of
the loss function, the global optimality of the solution is not
guaranteed. However, ADMM could offer a local optimal
solution when ρ is appropriately chosen since the quadratic
term in (8) is strongly convex as ρ > 0, which stabilizes the
convergence of ADMM .
On the other hand, the second subproblem (7) is given
gi(zi) + ρ
Note that gi(·) is the indicator function deﬁned by Si,
thus this subproblem can be solved analytically and optimally . The optimal solution is
= ΠSi(θk+1
where ΠSi(·) is Euclidean projection of θk+1
i onto Si.
Algorithm 1 Concurrent Adversarial Training and Weight
1: Input: dataset D, ADMM iteration number K, PGD
step size α, PGD iteration number T, augmented parameter ρ, and sets Si’s for weight sparsity constraint.
2: Output: weight parameters θ.
3: for k = 1, 2, . . . , K do
Sample batch (x, y) from D
▷Solve Eq (8) over θ
for t = 1, 2, . . . , T do
Solve the inner max by Eq (2)
Apply Adam optimizer on the outer min of Eq (8)
to obtain {θk
Solve Eq (9) using Eq (10) to obtain {zk
12: end for
3.1. Deﬁnitions of Si for Weight Pruning Schemes
This subsection introduces how to use the weight sparsity constraint θi ∈Si to implement different weight pruning scheme. For each weight pruning scheme, we ﬁrst provide the exact form of θi ∈Si constraint and then provide
the explicit form of the solution (10). Before doing that,
we reduce θi back into the four dimensional tensor form
as θi ∈RNi×Ci×Hi×Wi, where Ni, Ci, Hi, and Wi are respectively the number of ﬁlters, the number of channels in
a ﬁlter, the height of a ﬁlter, and the width of a ﬁlter.
Filter pruning
θi ∈Si := {θi | ∥θi∥n=0 ≤αi}.
Here, ∥θi∥n=0 means the number of ﬁlters containing nonzero elements. To obtain the solution (10) with such constraint, we ﬁrstly calculate On = ∥(θk+1
i )n,:,:,:∥2
where ∥· ∥F denotes the Frobenius norm. We then keep αi
largest values in On and set the rest to zeros.
Column pruning
θi ∈Si := {θi | ∥θi∥c,h,w=0 ≤βi}.
Here, ∥θi∥c,h,w=0 means the number of elements at the
same locations in all ﬁlters in the ith layer containing nonzero elements. To obtain the solution (10) with such constraint, ﬁrst we calculate Oc = ∥(θk+1
i ):,c,h,w∥2
then keep βi largest values in Oc and set the rest to zeros.
Irregular pruning
θi ∈Si := {θi | ∥θi∥0 ≤γi}.
In this special case, we only constrain the number of nonzero elements in the ith layer ﬁlters i.e., in θi. To obtain the
solution (10), we keep γi largest magnitude elements in θi
and set the rest to zeros.
Algorithm 1 summarizes the framework of concurrent
adversarial training and weight pruning.
In addition to ADMM based weight pruning, we also
show results of post-pruning in Appendix A, with and without retraining respectively.
4. Weight Pruning in the Adversarial Setting
In this section, we examine the performance of weight
pruning in the adversarial setting. We obtain intriguing results contradictory from those in the conventional
model compression setting. Here we specify the proposed
framework of concurrent adversarial training and weight
pruning by the ﬁlter pruning scheme, which is a common
pruning choice to facilitate the implementation of sparse
neural networks on hardware. Other pruning schemes will
be investigated in the experiment section. In Table 1, we
summarize all the networks tested in the paper with their
model architectures speciﬁed by the width scaling factor w.
4.1. Weight Pruning vs Training from Scratch
An ongoing debate about pruning is whether weight
pruning is actually needed and why not just training a
small network from scratch. To answer this question, the
work performs a large amount of experiments to ﬁnd
that (i) training a large, over-parameterized model is often not necessary to obtain an efﬁcient ﬁnal model, and (ii)
the meaning of weight pruning lies in searching the architecture of the ﬁnal pruned model. In another way, if we
are given with a predeﬁned target model, it makes no difference whether we reach the target model from pruning
a large, over-parameterized model or we train the target
model from scratch. We also remark that the above conclusions from are made while performing regular pruning.
Although the ﬁndings in may hold in the setting of
natural training, the story becomes different in the setting
of adversarial training. Tables 2, 3, and 4 demonstrate the
natural test accuracy / adversarial test accuracy of natural training, adversarial training, and concurrent adversarial
training and weight pruning for different datasets and networks. Let us take Table 2 as an example. When we naturally train a network of size w = 1, we have 98.25% natural
test accuracy and 0% adversarial test accuracy. When we
adversarially train the network of size w = 1, both natural
test accuracy and adversarial test accuracy become 11.35%,
which is still quite low. It demonstrates that the network
of size w = 1 does not have enough capacity for strong
adversarial robustness. In order to promote the adversarial
robustness, we need to adversarially train the network with
size of w = 4 at least. Surprisingly, by leveraging our concurrent adversarial training and weight pruning on the network of size w = 4, we can obtain a much smaller pruned
model with the target size of w = 1 but achieve competitive
natural test accuracy / adversarial test accuracy (96.22% /
89.41%) compared to the adversarially trained model of size
w = 4. To obtain a network of size w = 1 with the highest
natural and adversarial test accuracy, we should apply the
proposed framework on the network of size w = 8. Similar
observations hold for Tables 3 and 4.
In summary, the value of weight pruning is essential in
the adversarial training setting: it is possible to acquire a
network of small model size (by weight pruning) with both
high natural test accuracy and adversarial test accuracy. By
contrast, one may lose the natural and adversarial test accuracy if the adversarial training is directly applied to a smallsize network that is not acquired from weight pruning.
4.2. Pruning to Inherit Winning Ticket or Else?
In the natural training (pruning) setting, the lottery ticket
hypothesis states that the meaning of weight pruning
is in that the small sub-network model can inherit the initialization (the so-called “winning ticket”) from the large
model. Or in another way, the weight pruning is meaningful only in that it provides effective initialization to the ﬁnal
pruned model.
To test whether or not the lottery ticket hypothesis is
valid in the adversarial setting, we perform adversarial
training under the similar experimental setup as . The
natural/adversarial test accuracy results are summarized in
Table 5, where the result in cell w1-w2 (w1 > w2) denotes
the accuracy of an adversarially trained model of size w2 using the inherited initialization from an adversarially trained
model of size w1. No pruning is used in Table 5. For example, cell 4-2 in Table 5 only yields 11.35%/11.35% accuracy. Recall from Table 2 that if we use our proposed framework of concurrent adversarial training and weight pruning
to prune from a model with size 4 to a small model with size
Table 1: Network structures used in our experiments. FC, M, and A mean fully connected layer, max-pooling layer, and average-pooling
layer, respectively. Other numbers denote the numbers of ﬁlters in convolutional layers. We use w to denote the scaling factor of a network.
Each layer is equally scaled with w.
2*w, 4*w, FC(196*w, 64*w), FC(64*w,10)
CIFAR LeNet
6*w, 16*w, FC(400*w, 120*w), FC(120*w, 84*w), FC(84*w,10)
4*w,4*w,M,8*w,8*w,M,16*w,16*w,16*w,M,32*w,32*w,32*w,M,32*w,32*w,32*w,M,A,FC(32*w,10)
CIFAR ResNet
b*w, where b denotes 1/16 of the size of ResNet18 
Table 2: Natural test accuracy/adversarial test accuracy (in %) on MNIST of [column ii] naturally trained model with different size w,
[column iii] adversarially trained model with different size w, [columns iv–vii] concurrent adversarial training and weight pruning from a
large size to a small size.
nat baseline
adv baseline
98.25/0.00
11.35/11.35
98.72/0.00
11.35/11.35
11.35/11.35
99.07/0.00
98.15/91.38
96.22/89.41
97.68/91.77
99.20/0.00
98.85/93.51
97.31/92.16
98.31/93.93
98.87/94.27
99.31/0.00
99.02/94.65
96.19/87.79
98.07/89.95
98.87/94.77
99.01/95.44
of 2, we can have high accuracy 97.68%/91.77% in cell 4-2
of Table 2. Our results suggest that the lottery ticket hypothesis requires additional careful studies in the setting of
adversarial training.
Epoch number
Loss value
km normal nat train
km normal adv train
km normal nat test
km normal adv test
Epoch number
Loss value
km normal nat train
km normal adv train
km normal nat test
km normal adv test
Figure 2: The adversarial training loss of kaiming normal initialization with Adam optimizer trained from scratch on MNIST by
LeNet with size of w = 1 using different random seeds. The left
subﬁgure is the only successful case we found and the right sub-
ﬁgure represents a common case.
Moreover, to further explore the relationship between
initialization and model capacity in adversarial training,
we conduct additional experiment. Seven different initialization methods are compared to train the smallest LeNet
model (w = 1) with 300 epochs using Adam, SGD and
CosineAnnealing on MNIST. We repeat this experiment 10 times with different random seed and report the average accuracy in Table B1. As suggested by Table 2, adversarial training from scratch failed as w = 1, 2. In all studied scenarios, we only ﬁnd two exceptions: a) Adam with
uniform initialization and b) Adam with kaiming normal
in which 1 out of 10 trials succeeds (the losses are drawn
in Figure 2). Even for these exceptions, the corresponding
test accuracy is much worse than that of the smallest model
obtained from concurrent adversarial training and weight
pruning in Table 2. We also ﬁnd that the accuracy 11.35%
corresponds to a saddle point that the adversarial training
meets in most of cases. Our results in Table B1 suggest that
without concurrent adversarial training and weight pruning, it becomes extremely difﬁcult to adversarially train a
small model from scratch even using different initialization
schemes and optimizers.
4.3. Possible Beneﬁt of Over-Parameterization
It is clear from Sec. 4.1 and 4.2 that in the adversarial setting, pruning from a large model is useful, which
yields beneﬁts in both natural test accuracy and adversarial robustness. By contrast, these advantages are not provided by adversarially training a small model from scratch.
Such intriguing results could be explained from the beneﬁt of over-parameterization , which shows that
training neural networks possibly reaches the global solution when the number of parameters is larger than that is
statistically required to ﬁt the training data.
In the similar spirit, in adversarial training setting, the larger, overparameterized models lead to good convergence while adversarially trained small models are stuck at the saddle
points frequently. These two observations have motivated
us to propose a framework that can beneﬁt from larger models during adversarial training and at the same time reduce
the models’ size. As a result, the remained weights preserve
Table 3: Natural test accuracy/adversarial test accuracy (in %) on CIFAR10 by LeNet of [column ii] naturally trained model with
different size w, [column iii] adversarially trained model with different size w, [columns iv–vii] concurrent adversarial training and weight
pruning from a large size to a small size.
nat baseline
adv baseline
74.84/0.01
10.00/10.00
78.41/0.07
55.03/33.29
50.3/31.33
83.36/0.19
65.01/36.30
53.30/32.41
62.77/34.52
85.12/0.55
72.80/37.67
52.27/31.91
62.22/35.42
70.50/37.92
87.22/0.93
74.91/38.65
51.28/31.30
62.10/35.55
70.59/37.93
71.93/39.00
Table 4: Natural test accuracy/adversarial test accuracy (in %) on CIFAR10 by ResNet of [column ii] naturally trained model with
different size w, [column iii] adversarially trained model with different size w, [columns iv–vii] concurrent adversarial training and weight
pruning from a large size to a small size.
nat baseline
adv baseline
84.23/0.00
57.16/34.40
87.05/0.00
71.16/42.45
64.53/37.90
91.93/0.00
77.35/44.99
64.36/37.78
73.21/43.14
93.11/0.00
77.26/47.28
64.52/38.01
73.36/43.17
78.12/45.49
94.80/0.00
82.71/49.31
64.17/37.99
71.80/42.86
78.85/47.19
81.83/48.00
Table 5: Natural test accuracy/adversarial test accuracy (in %)
on MNIST for validating the lottery ticket hypothesis in the adversarial setting.
11.35/11.35
11.35/11.35
11.35/11.35
11.35/11.35
97.36/90.19
98.64/94.66
11.35/11.35
11.35/11.35
98.42/91.63
98.96/95.49
adversarial robustness.
5. Pruning Schemes and Transfer Attacks
In this section,
we examine the performance of
the proposed concurrent adversarial training and weight
ﬁlter/column/irregular pruning) and transfer attacks. The proposed framework is tested on CIFAR10 using VGG-16 and
ResNet-18 networks, as shown in Figure 3.
see, the natural and adversarial test accuracy decrease as the
pruned size decreases. Among different pruning schemes,
the irregular pruning performs the best while the ﬁlter pruning performs the worst in both natural and adversarial test
accuracy. That is because in addition to weight sparsity, ﬁlter pruning imposes the structure constraint, which restricts
the pruning granularity compared to the irregular pruning.
Moreover, irregular pruning preserves the accuracy against
different pruned sizes. The reason is that the weight sparsity is beneﬁcial to mitigate the overﬁtting issue , and
the adversarial training suffers a more signiﬁcant overﬁtting
than the natural training .
In Table C1, we evaluate the performance of our PGD
adversary based robust model against C&W ℓ∞attacks. As
we can see, the concurrent adversarial training and weight
pruning yields the pruned model robust to transfer attacks.
In particular, the pruned model is able to achieve better adversarial test accuracy than that of the original model prior
to pruning (baseline).
Furthermore, we design a cross transfer attack experiment.
Consider the baseline models in Table 2, when
w = 1, 2, the models are not well-trained so we generate
adversarial examples by PGD attack from baseline model
with w = 4, 8, 16 and apply them to test the pruned models. In Table 6, the results show that even the worst case
in each pruned model, the adversarial test accuracy is also
higher than that of the pruned models in Table 2. The results
imply that the model is most vulnerable against adversarial
examples generated by itself, regardless of the size of the
6. Supplementary Details of Experiment Setup
We use LeNet for MNIST, and LeNet, VGG-16 and
ResNet-18 for CIFAR10. The LeNet models used here follow the work . Batch normalization (BN) is applied in
VGG-16 and ResNet-18. More details about the network
structures are listed in Table 1.
To solve the inner max problem in (1), we set PGD adversary iterations as 40 and 10, step size α as 0.01 and
Table 6: Adversarial test accuracy (in %) on MNIST against transfer attack from baseline models (row) when w ∈{4, 8, 16} to pruned
models (column) m −n which means pruned from original model with w = m to small model with w = n.
Accuracy(%)
filter/natural test
column/natural test
irregular/natural test
filter/adversarial test
column/adversarial test
irregular/adversarial test
(a) VGG-16
Accuracy(%)
filter/natural test
column/natural test
irregular/natural test
filter/adversarial test
column/adversarial test
irregular/adversarial test
(b) ResNet-18
Figure 3: Natural and adversarial test accuracy of the proposed
framework of concurrent adversarial training and weight pruning
on CIFAR10. Filter, column, and irregular pruning schemes are
applied in the proposed framework respectively. Weight pruning
is performed from size of w = 16 to sizes of w = 1, 2, 4, 8.
The solid lines denote natural accuracy when pruning from the
size of w = 16 to different sizes, and the dashed lines denote the
corresponding adversarial accuracy.
2/255, the ℓ∞bound as 0.3 and 8/255 for MNIST and
CIFAR respectively, and all pixel values are normalized in
 . We use Adam with learning rate 1 × 10−4 to train
our LeNet for 83 epochs as suggested by the released code
of . During pruning we set ρ = 1 × 10−3 and K = 30
for Algorithm 1. Moreover, there are controversial on the
baselines of CIFAR and we do the following to ensure our
baselines are strong enough:
1. We follow the suggestions by to train our models
with a larger learning rate 0.1 as initial learning rate.
2. We train all models in CIFAR with 300 epochs and
divide the learning rate by 10 times at epoch 80 and
epoch 150 following the .
3. Liu et al. suggests that models trained from
scratch need fair training time to compare with pruned
models. Therefore, we double the training time if the
loss is still descent at the end of the training.
4. Since there is always a trade off between natural accuracy and adversarial accuracy, we report accuracy
when the models achieve the lowest average loss for
both natural and adversarial images on test dataset.
Hence, we believe that in our setting, we have fair baselines
for training from scratch.
7. Conclusion
Min-max robust optimization based adversarial training
can provide a notion of security against adversarial attacks.
However, adversarial robustness requires a signiﬁcant larger
capacity of the network than that for the natural training
with only benign examples. This paper proposes a framework of concurrent adversarial training and weight pruning
that enables model compression while still preserving the
adversarial robustness and essentially tackles the dilemma
of adversarial training. Furthermore, this work studies two
hypotheses about weight pruning in the conventional setting and ﬁnds that weight pruning is essential for reducing
the network model size in the adversarial setting, and that
training a small model from scratch even with inherited initialization from the large model cannot achieve adversarial robustness and high standard accuracy at the same time.
This work also systematically investigates the effect of different pruning schemes on adversarial robustness and model
compression.
Acknowledgments
This work is partly supported by the National Science
Foundation CNS-1932351, Institute for Interdisciplinary
Information Core Technology (IIISCT) and Zhongguancun
Haihua Institute for Frontier Information Technology.