Understanding data augmentation for classiﬁcation:
when to warp?
Sebastien C. Wong
Defence Science and Technology
SA, Australia
Email: 
Australian Defence Force
SA, Australia
Victor Stamatescu
and Mark D. McDonnell
Computational Learning Systems Laboratory
Information Technology and Mathematical Sciences
University of South Australia
Mawson Lakes
SA, Australia
©2016 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works. DOI: ¡DOI No. TBD¿
Abstract—In this paper we investigate the beneﬁt of augmenting data with synthetically created samples when training a machine learning classiﬁer. Two approaches for creating additional
training samples are data warping, which generates additional
samples through transformations applied in the data-space,
and synthetic over-sampling, which creates additional samples
in feature-space. We experimentally evaluate the beneﬁts of
data augmentation for a convolutional backpropagation-trained
neural network, a convolutional support vector machine and
a convolutional extreme learning machine classiﬁer, using the
standard MNIST handwritten digit dataset. We found that while
it is possible to perform generic augmentation in feature-space, if
plausible transforms for the data are known then augmentation in
data-space provides a greater beneﬁt for improving performance
and reducing overﬁtting.
I. INTRODUCTION
The competition winning classiﬁers described in , ,
 , all utilized techniques to artiﬁcially increase the number of training examples. Previous research to systematically
understand the beneﬁts and limitations of data augmentation
demonstrate that data augmentation can act as a regularizer in
preventing overﬁtting in neural networks , and improve
performance in imbalanced class problems . In this paper
we empirically investigate the beneﬁts and limitations of
data augmentation on three machine learning classiﬁers, and
attempt to answer the question of how and when to apply data
augmentation?
To enable comparison of data augmentation for a convolutional neural network (CNN), a convolutional support vector
machine (CSVM) , and a convolutional extreme learning
machine (CELM) classiﬁer, we use the well known MNIST
handwritten digit dataset .
II. PREVIOUS WORK
This section provides a brief review of previous works
that have used augmented training data to improve classiﬁer
performance.
The concept of data warping (for neural networks) can
possibly be attributed to in creating a model of character
distortions that could occur in the printing (or handwriting)
process, the model described deformations and defects needed
to generate synthetic character examples. The term warping
was coined by , in the context of producing randomly
warped stroke data for handwriting character classiﬁcation.
The warped character stroke data was used to balance the
amount of training examples for each character class in order
to reduce the bias in the classiﬁer to favor more frequently
presented training examples. This approach was extended by
 to improve the performance of a standard backpropagationtrained neural network, and achieve record performance of 0.4% error rate on the MNIST handwritten digit
database using a convolutional neural network. The warped
training data was created by applying both afﬁne transformations (translation, shearing, rotation) and elastic distortions to
images of existing character examples. The elastic distortions
were generated using a scaled normalized random displacement ﬁeld in image space, and are stated to correspond to
uncontrolled oscillations of the hand muscles. This approach
was further extended to create a simple deep neural network
that was only trained on warped data , where new warped
training data was created for each epoch of the backpropagation algorithm. This allowed for a neural network with a large
number of parameters that overcame the issues of over-ﬁtting
and diminished convergence by the backpropagation algorithm
during training, which achieved a state of the art 
error rate of 0.35% on the MNIST database.
The problem of class imbalance, where real-world datasets
often only contain a small percentage of “interesting” or target
class examples, was addressed by the use of a Synthetic Minority Over-Sampling Technique (SMOTE) . In SMOTE the
synthetic examples are created in feature-space from randomly
selected pairs of real world feature-examples from the minority
class. The advantage of synthetic over-sampling compared to
the data warping approach, is that synthetic examples are
created in feature-space, and thus the SMOTE algorithm is
application independent.
Both the SMOTE and data warping approaches modify
real world examples to create augmented data. An alternative
would be to generate synthetic imagery. This approach was
investigated by , in creating synthetic images of building
roofs to augment real image data. By visualizing the data in
feature-space they found that the distributions of artiﬁcially
generated images and real images (that were the basis of the
Convolutional
and Pooling
Classi ication
Experimental architecture. Stage 1 was kept the same for all
experiments, while stage 2 consisted of either a backpropagation-trained
neural network, support vector machine or extreme learning machine classiﬁer.
synthetic examples) were displaced. The authors termed this
problem the synthetic gap. To overcome this issue they trained
a sparse auto-encoder simultaneously with real and synthetic
images, such that the learnt features minimized the synthetic
The issue of the difference between real and synthetic
examples is discussed by , in using a classiﬁer that was
trained with augmented data only. The paper argues that to
maximize the probability of correct classiﬁcation any test
sample should undergo the same warping process as the
training samples.
III. METHOD AND DATA
The literature suggests that it is possible to augment training
data in data-space or feature-space. To understand the beneﬁts
of each approach the experimental architecture (for each
machine learning classiﬁer) was structured into two stages,
such that there was a clear distinction between data-space
and feature-space, illustrated in Figure 1. Each system under
evaluation had the same convolutional and pooling layer that
was used to generate features for the classiﬁcation engine.
The weights for this convolutional and pooling layer were
kept ﬁxed to ensure that the same features were used for all
experiments with the same dataset.
A. Datasets
The MNIST database was used for the experiments.
This database consists of a training set of 60,000 labeled 28
by 28 pixel grayscale images of handwritten digits and a test
set of 10,000 labelled images. There are 10 classes, the digits
An important question to be answered is how does using
synthetic data augmentation compare with collecting more real
data? To answer this question a set of baseline performance
ﬁgures for each of the classiﬁers were created by reducing the
amount of data available in training. The quantity of samples
from each class was kept equal to remove any impact from
class imbalance, which reduced the total number of available
training samples to 50,000. These results can then be compared
with the performance of adding an equivalent amount of
Data warping using elastic deformations . Original MNIST digits
compared to warped digits for α = 1.2 (left) and α = 8 pixels (right).
augmented data. Here the augmented data was generated from
a very small pool of 500 real training samples from each class.
B. Augmentation in data-space
For image data it is possible to create plausible transformations of existing samples that preserves label information, with
the validation of label integrity being performed by a human
observer (can a human still recognize the object). One of the
signiﬁcant improvements in performance of classiﬁers on the
MNIST database was through the introduction of elastic deformations , in addition to the existing afﬁne transformations,
for data augmentation. The elastic deformation was performed
by deﬁning a normalized random displacement ﬁeld u(x, y)
that for each pixel location (x, y) in an image speciﬁes a unit
displacement vector, such that Rw = Ro + αu, where Rw
and Ro describe the location of the pixels in the original and
warped images respectively. The strength of the displacement
in pixels is given by α. The smoothness of the displacement
ﬁeld is controlled by the parameter σ, which is the standard
deviation of the Gaussian that is convolved with matrices of
uniformly distributed random values that form the x and y
dimensions of the displacement ﬁeld u.
For the MNIST data set we found that large deformations,
with the displacement α ≥8 pixels, could occasionally result
in characters that were difﬁcult to recognize by a human
observer, that is label information was not preserved. This
loss of label integrity was caused by shifting a critical part
of the character outside of image boundary, or by introducing
a “kink” that rendered the character illegible, as illustrated in
Figure 2. We empirically set α = 1.2 pixels with σ = 20 based
on performance of the (quick to train) CELM algorithm.
The samples needed for elastic warping were generated offline and the same data applied to each of the experiments.
C. Augmentation in feature-space
For some machine learning problems it may not be easy
to validate that an arbitrary transformation of an existing raw
data sample has indeed preserved label information. For these
problems the features may have been hand crafted to extract
salient information on which the classiﬁer performs learning.
The Synthetic Minority Over-Sampling Technique (SMOTE)
 was inspired by the use of data-warping to reduce class
imbalance in the handwritten digit problem. But by being
applied in feature-space it was proposed by the authors to
be domain independent, and has since been used extensively
in medical research where datasets with a signiﬁcant minority
class are common. For SMOTE a new synthetic sample is
created by selecting a random point in feature-space along a
line intersecting k randomly chosen samples of the same class.
A more recent derivative of SMOTE is the Density Based
SMOTE (DBSMOTE) algorithm , which generates the
new synthetic samples within a distance eps of the cluster
centre for the class (following original notation). As the
DBSMOTE algorithm generates its synthetic samples around
the center of each class, it could be expected that these
synthetic samples may not contribute to reducing classiﬁer
overﬁtting. Indeed we found that using DBSMOTE actually
increased the amount of overﬁtting.
For the MNIST data set we used k = 2 random samples, for
both SMOTE and DBSMOTE, and distance threshold eps ≤4
for DBSMOTE.
The samples needed for SMOTE and DBSMOTE were
generated online, as these algorithms are naturally embedded
within the classiﬁcation architecture, which is described in the
next section.
IV. EXPERIMENTAL SYSTEM PARAMETERS
As previously stated, the two-stage architecture illustrated
in Figure 1 was used for each experiment.
A. Stage-1 convolution and pooling.
For stage-1, to create a local receptive ﬁeld, we adopt the
method and parameters described in . Conceptually, the
28 × 28 input image is convolved with a W × W ﬁlter, where
W = 7 and the stride length is 1. This results in an intermediate output feature layer with dimensions (28 −W −1)2. Next
LP-pooling is applied with a pool size of q × q, where q = 8,
which results in a down-sampled feature layer with dimensions
22 × 22. This is repeated for L ﬁlters, where L = 96 and
corresponds to the ﬁrst layer of pre-trained OverFeat ﬁlters
B. Stage-2 classiﬁcation.
For stage-2, we evaluated the following classiﬁcation
schemes: backpropagation-trained neural network, support
vector machine, and extreme learning machine. The weights
of the convolutional layer were held constant.
The neural network classiﬁer was a standard multi-layer
perceptron neural network with sigmoidal activation function. For simplicity of parameters, and to keep the neural
architecture similar between classiﬁers, a single hidden layer
with 1600 neurons was used. Learning was performed using
backpropagation on the the hidden neurons only (and no
changes were made to the convolutional layer). The number
of epochs (one batch application of backpropagation on all
training samples) was ﬁxed at 2000.
The multi-class support vector machine classiﬁer, which
used the 1-vs-all L2 loss function, was based on the code
and parameters settings of .
The extreme learning machine classiﬁer, which used least
squares regression of random weight projection neurons, was
based on the code and parameter settings of . For consistency with the CNN, a single hidden layer with 1600 neurons
V. EXPERIMENTS AND RESULTS
The ﬁrst step in understanding the beneﬁt of data augmentation was to vary the amount of real data samples available to
train a classiﬁer. Our hypothesis is that for a given number of
samples (of either real or synthetic data) the beneﬁt provided
by real samples provides an upper-bound that the performance
of a classiﬁer can be improved by an equivalent number of
synthetic samples. We denote this experiment as our baseline,
where training error indicates the percentage error rate (error
%) of the classiﬁer when evaluated against the samples used
to train the classiﬁer, and test error is the error % on the
predeﬁned MNIST test set of 10,000 samples.
Each experiment is repeated three times to account for
random variation. The primary variation in the CNN is the
initialization of the weights and the order in which the
training samples are presented to the classiﬁer. The variation
in the CELM output is due to the random weight projection.
The CSVM implementation is deterministic, leading to no
performance variation for multiple runs.
A. Baseline Results
The performance of the baseline system, illustrating the
relative performance of the CNN, SVM and ELM is shown in
Figure 3. Each of the classiﬁers have similar test error % for
the amount of training data used. The primary observation
is that as the number of real samples are increased (from
500 to 5000 per class) the gap between training error and
test error decreases, which indicates a reduction in overﬁtting.
The amount of improvement in performance varies across the
classiﬁers. This conﬁrms that the primary beneﬁt of increasing
the number of samples is to reduce overﬁtting (given that all
other classiﬁer parameters are held constant).
Another interesting observation is that the CNN beneﬁts
the most from additional training samples, in that increasing
the number of samples improving both the training error and
the test error. The increase in the amount of data results
in more iterations of the backpropagation algorithm for the
same number of epochs. The CELM varies the most in error
% for a given amount of data, due to the random weight
projection neurons in the hidden layer. The CELM also has
good performance for small amounts of training data. The
CSVM also exhibited similar behaviour to the CELM, with the
best test error % for 500 samples per class, but performance
did not improve as much as the other classiﬁers as the number
of samples were increased.
training samples per class
Baseline performance of CNN, CSVM and CELM on MNIST
using real data. The dashed lines indicate training error %, and the solid
lines indicate test error %. Lower test error % indicates better performance.
Reducing the difference between training and test error % is indicative of
reducing overﬁtting.
B. CNN Results
The performance of the CNN is shown in Figure 4, which
illustrates how error % varies as the number of samples
are increased. Again this shows that increasing the number
of samples resulted in a performance improvement. Most
notably the test error % decreased steadily. The CNN results
were consistent, with multiple runs of the same experiment
producing similar error %.
Augmentation in data-space using elastic deformations gave
a better improvement in error % than augmentation in featurespace. Interestingly, while test error % decreased, training
error % also decreased, and thus the gap between test and
training was roughly maintained.
Augmentation in feature-space using SMOTE showed
marginally promising results. Using SMOTE, the test error %
continued to decrease as the number of samples were increased
all the way to 50,000 samples.
Augmentation in feature-space using DBSMOTE resulted
in a slight improvement on test error %. Another interesting
observation is that the training error % for SMOTE and
DBSMOTE followed very similar curves.
C. Convolutional SVM Results
The performance of the CSVM is shown in Figure 5, which
illustrates how error % varies as the number of samples are
increased. Perhaps the most interesting result is that increasing
the number of synthetic samples using DBSMOTE caused the
performance to degrade, i.e., the error % increased as more
samples were used. Augmentation in feature-space using the
DBSMOTE algorithm was not effective in reducing overﬁtting
in the SVM classiﬁer. However, this result is not surprising as
the DBSMOTE promotes towards the centre of class clusters.
By creating synthetic samples that are “more of the same”
training samples per class
CNN performance on MNIST. The dashed lines indicate training
error %, and the solid lines indicate test error %.
training samples per class
CSVM performance on MNIST. The dashed lines indicate training
error %, and the solid lines indicate test error %.
DBSMOTE encourages overﬁtting of the training data. Augmentation in data-space using the SMOTE algorithm provided
little to no improvement in performance. Even augmentation
in data-space using elastic warping only provided a modest
improvement in test error %, and this only occurred at very
large amounts of data augmentation. However, it should be
noted that the gap between training error % and test error
% did decrease steadily as the amount of augmentation was
increased, thus indicating a reduction in overﬁtting.
D. Convolutional ELM Results
The performance of the CELM is shown in Figure 6. This
Figure illustrates how the training error and test error varies
as the number of samples are increased, which again shows
that performance typically improves as the amount of synthetic
samples are increased. However, this improvement is less than
that given by increasing the number of real samples.
training samples per class
CELM performance on MNIST. The dashed lines indicate training
error %, and the solid lines indicate test error %.
Augmentation in data-space using elastic deformations gave
the best results, which were slightly worse than having additional real training samples. However the trend is not linear, as
there was a marked improvement in error % for 1000 training
samples (500 real samples and 500 augmented samples) per
class, before the gain in error % ﬂatten-off once again.
Augmentation in feature-space using SMOTE seemed
promising when increasing the amount of samples from 500
to 1000 samples per class. However, increasing the amount
of synthetic samples further resulted in a decrease in performance. Also while test error % did initially decrease with
the use of SMOTE, there was no corresponding reduction
in training accuracy, which does occur when increasing the
number of real samples. Thus, the gap between training and
testing performance remained large.
Augmentation in feature-space using DBSMOTE had a
slightly negative impact on test error %.
VI. DISCUSSION
The experiments conducted used a classiﬁcation architecture
that was neatly divided into two stages: a feature generation
stage and a classiﬁcation stage. The purpose of this was to
investigate if it is better to conduct data augmentation in dataspace or feature-space?
For the experiments conducted on handwritten digits recognition, it was clearly better to perform augmentation in dataspace using elastic distortions. We expect this to hold true for
other classiﬁcation tasks, if label preserving transforms are
known and can be applied.
When label preserving transforms are not avaliable, the
SMOTE algorithm can be used to perform data augmentation
in feature-space, and provide some beneﬁt to a CNN or
CELM classiﬁer. Our results suggest the DBSMOTE algorithm should not be used for data augmentation. However,
a classiﬁcation architecture that is neatly divided into dataspace and feature-space is an artiﬁcial construct. The modern
trend is to construct architectures with deep layered hierarchies
of feature transformations with more complex features built
upon simpler features. Nevertheless, these results should also
provide insight into performing data augmentation for more
modern architectures. A good overview of label preserving
transformations for image data for a more modern classﬁcation
architecture is given by .
Another research question that we sought to answer, is how
much data is enough? For most classiﬁcation systems, more
data is better. And more real data is better than more synthetic
data. In none of the experiments did the performance of the
system trained on synthetic data outperform the system trained
on real data. Thus, the performance (test error %) that can be
achieved by augmenting classiﬁer training with synthetic data,
is likely to be bounded by training on the equivalment amount
of real data.
For the CNN, adding more synthetic data, using SMOTE
and elastic warping, consistently reduced the testing error %.
The experiments did not reach the limits of the amount of
synthetic data that could be added before error % no longer
For the CSVM adding more synthetic data, using DB-
SMOTE, caused classiﬁcation performance to degrade.
For the CELM more synthetic data was not always better for
performance. For the combination of the CELM with SMOTE
there was a peak improvement provided by data augmentation
at 1000 training samples per class, while increasing the number of samples further using the SMOTE technique resulted
in a decrease in performance.
When comparing the three classiﬁcation algorithms, the
algorithm demonstrated
from dataaugmentation than the CELM, which showed less beneﬁt than
the CNN algorithm.
VII. CONCLUSION
This paper demonstrates the beneﬁts and pitfalls of data
augmentation in improving the performance of classiﬁcation
systems. Data augmentation can be performed in data-space
or feature-space. We found that it was better to perform
data augmentation in data-space, as long as label preserving
transforms are known. The highly cited SMOTE algorithm can
be used to perform data augmentation in feature-space. This is
a more robust solution than the DBSMOTE algorithm, which
can increase overﬁtting due to the algorithm creating new
samples close to existing cluster centers. The improvement
in error % provided by data augmentation was bounded by
the equivalent amount of real-data.