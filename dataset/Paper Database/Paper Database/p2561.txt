Learning for Control from Multiple Demonstrations
Adam Coates
 
Pieter Abbeel
 
Andrew Y. Ng
 
Stanford University CS Department, 353 Serra Mall, Stanford, CA 94305 USA
We consider the problem of learning to follow
a desired trajectory when given a small number of demonstrations from a sub-optimal expert. We present an algorithm that (i) extracts the—initially unknown—desired trajectory from the sub-optimal expert’s demonstrations and (ii) learns a local model suitable for control along the learned trajectory.
We apply our algorithm to the problem of
autonomous helicopter ﬂight.
In all cases,
the autonomous helicopter’s performance exceeds that of our expert helicopter pilot’s
demonstrations.
Even stronger, our results
signiﬁcantly extend the state-of-the-art in autonomous helicopter aerobatics. In particular, our results include the ﬁrst autonomous
tic-tocs, loops and hurricane, vastly superior
performance on previously performed aerobatic maneuvers (such as in-place ﬂips and
rolls), and a complete airshow, which requires
autonomous transitions between these and
various other maneuvers.
1. Introduction
Many tasks in robotics can be described as a trajectory
that the robot should follow. Unfortunately, specifying the desired trajectory and building an appropriate
model for the robot dynamics along that trajectory are
often non-trivial tasks. For example, when asked to
describe the trajectory that a helicopter should follow
to perform an aerobatic ﬂip, one would have to specify a trajectory that (i) corresponds to the aerobatic
ﬂip task, and (ii) is consistent with the helicopter’s dynamics. The latter requires (iii) an accurate helicopter
dynamics model for all of the ﬂight regimes encountered in the vicinity of the trajectory. These coupled
tasks are non-trivial for systems with complex dynamics, such as helicopters. Failing to adequately address
these points leads to a signiﬁcantly more diﬃcult con-
Appearing in Proceedings of the 25 th International Conference on Machine Learning, Helsinki, Finland, 2008. Copyright 2008 by the author(s)/owner(s).
trol problem.
In the apprenticeship learning setting, where an expert is available, rather than relying on a handengineered target trajectory, one can instead have the
expert demonstrate the desired trajectory. The expert
demonstration yields both a desired trajectory for the
robot to follow, as well as data to build a dynamics
model in the vicinity of this trajectory. Unfortunately,
perfect demonstrations can be hard (if not impossible)
to obtain. However, repeated expert demonstrations
are often suboptimal in diﬀerent ways, suggesting that
a large number of suboptimal expert demonstrations
could implicitly encode the ideal trajectory the suboptimal expert is trying to demonstrate.
In this paper we propose an algorithm that approximately extracts this implicitly encoded optimal demonstration from multiple suboptimal expert
demonstrations, and then builds a model of the dynamics in the vicinity of this trajectory suitable for
high-performance control. In doing so, the algorithm
learns a target trajectory and a model that allows the
robot to not only mimic the behavior of the expert but
even perform signiﬁcantly better.
Properly extracting the underlying ideal trajectory
from a set of suboptimal trajectories requires a signiﬁcantly more sophisticated approach than merely averaging the states observed at each time-step. A simple
arithmetic average of the states would result in a trajectory that does not even obey the constraints of the
dynamics model. Also, in practice, each of the demonstrations will occur at diﬀerent rates so that attempting to combine states from the same time-step in each
trajectory will not work properly.
We propose a generative model that describes the expert demonstrations as noisy observations of the unobserved, intended target trajectory, where each demonstration is possibly warped along the time axis. We
present an EM algorithm—which uses a (extended)
Kalman smoother and an eﬃcient dynamic programming algorithm to perform the E-step—to both infer
the unobserved, intended target trajectory and a timealignment of all the demonstrations. The time-aligned
demonstrations provide the appropriate data to learn
Learning for Control from Multiple Demonstrations
good local models in the vicinity of the trajectory—
such trajectory-speciﬁc local models tend to greatly
improve control performance.
Our algorithm allows one to easily incorporate prior
knowledge to further improve the quality of the learned
trajectory. For example, for a helicopter performing
in-place ﬂips, it is known that the helicopter can be
roughly centered around the same position over the
entire sequence of ﬂips. Our algorithm incorporates
this prior knowledge, and successfully factors out the
position drift in the expert demonstrations.
We apply our algorithm to learn trajectories and dynamics models for aerobatic ﬂight with a remote controlled helicopter. Our experimental results show that
(i) our algorithm successfully extracts a good trajectory from the multiple sub-optimal demonstrations,
and (ii) the resulting ﬂight performance signiﬁcantly
extends the state of the art in aerobatic helicopter
ﬂight . Most
importantly, our resulting controllers are the ﬁrst to
perform as well, and often even better, than our expert pilot.
We posted movies of our autonomous helicopter ﬂights
 
The remainder of this paper is organized as follows:
Section 2 presents our generative model for (multiple) suboptimal demonstrations; Section 3 describes
our trajectory learning algorithm in detail; Section 4
describes our local model learning algorithm; Section 5
describes our helicopter platform and experimental results; Section 6 discusses related work.
2. Generative Model
2.1. Basic Generative Model
We are given M demonstration trajectories of length
N k, for k = 0..M −1. Each trajectory is a sequence
of states, sk
j , and control inputs, uk
j , composed into a
single state vector:
, for j = 0..N k −1, k = 0..M −1.
Our goal is to estimate a “hidden” target trajectory of
length T, denoted similarly:
, for t = 0..T −1.
We use the following notation: y = {yk
j | j = 0..N k −
1, k = 0..M −1}, z = {zt | t = 0..T −1}, and similarly
for other indexed variables.
The generative model for the ideal trajectory is given
by an initial state distribution z0 ∼N(µ0, Σ0) and an
approximate model of the dynamics
zt+1 = f(zt) + ω(z)
∼N(0, Σ(z)).
The dynamics model does not need to be particularly
accurate—in our experiments, we use a single generic
model learned from a large corpus of data that is not
speciﬁc to the trajectory we want to perform. In our
experiments (Section 5) we provide some concrete examples showing how accurately the generic model captures the true dynamics for our helicopter.1
Our generative model represents each demonstration
as a set of independent “observations” of the hidden,
ideal trajectory z. Speciﬁcally, our model assumes
∼N(0, Σ(y)).
j is the time index in the hidden trajectory to
which the observation yk
j is mapped. The noise term in
the observation equation captures both inaccuracy in
estimating the observed trajectories from sensor data,
as well as errors in the maneuver that are the result of
the human pilot’s imperfect demonstration.2
The time indices τ k
j are unobserved, and our model
assumes the following distribution with parameters dk
To accommodate small, gradual shifts in time between
the hidden and observed trajectories, our model assumes the observed trajectories are subsampled versions of the hidden trajectory.
We found that having a hidden trajectory length equal to twice the
average length of the demonstrations, i.e., T
k=1 N k), gives suﬃcient resolution.
Figure 1 depicts the graphical model corresponding to
our basic generative model. Note that each observation yk
j depends on the hidden trajectory’s state at
j , which means that for τ k
j unobserved, yk
j depends on all states in the hidden trajectory that it
could be associated with.
2.2. Extensions to the Generative Model
Thus far we have assumed that the expert demonstrations are misaligned copies of the ideal trajectory
1The state transition model also predicts the controls
as a function of the previous state and controls. In our
experiments we predict u⋆
t plus Gaussian noise.
2Even though our observations, y, are correlated over
time with each other due to the dynamics governing the observed trajectory, our model assumes that the observations
j are independent for all j = 0..N k −1 and k = 0..M −1.
Learning for Control from Multiple Demonstrations
Figure 1. Graphical model representing our trajectory assumptions. (Shaded nodes are observed.)
merely corrupted by Gaussian noise.
Listgarten et
al. have used this same basic generative model (for
the case where f(·) is the identity function) to align
speech signals and biological data . We now augment the basic
model to account for other sources of error which are
important for modeling and control.
2.2.1. Learning Local Model Parameters
For many systems, we can substantially improve our
modeling accuracy by using a time-varying model ft(·)
that is speciﬁc to the vicinity of the intended trajectory
at each time t. We express ft as our “crude” model,
f, augmented with a bias term3, β⋆
zt+1 = ft(zt) + ω(z)
≡f(zt) + β⋆
To regularize our model, we assume that β⋆
only slowly over time. We have β⋆
t , Σ(β)).
We incorporate the bias into our observation model
by computing the observed bias βk
for each of the observed state transitions, and modeling this as a direct observation of the “true” model
bias corrupted by Gaussian noise. The result of this
modiﬁcation is that the ideal trajectory must not only
look similar to the demonstration trajectories, but it
must also obey a dynamics model which includes those
errors consistently observed in the demonstrations.
2.2.2. Factoring out Demonstration Drift
It is often diﬃcult, even for an expert pilot, during
aerobatic maneuvers to keep the helicopter centered
around a ﬁxed position.
The recorded position trajectory will often drift around unintentionally. Since
these position errors are highly correlated, they are
not explained well by the Gaussian noise term in our
observation model.
To capture such slow drift in the demonstrated trajec-
3Our generative model can incorporate richer local
models. We discuss our choice of merely using biases in our
generative trajectory model in more detail in Section 4.
tories, we augment the latent trajectory’s state with a
“drift” vector δk
t for each time t and each demonstrated
trajectory k. We model the drift as a zero-mean random walk with (relatively) small variance. The state
observations are now noisy measurements of zt + δk
rather than merely zt.
2.2.3. Incorporating Prior Knowledge
Even though it might be hard to specify the complete
ideal trajectory in state space, we might still have prior
knowledge about the trajectory. Hence, we introduce
additional observations ρt = ρ(zt) corresponding to
our prior knowledge about the ideal trajectory at time
t. The function ρ(zt) computes some features of the
hidden state zt and our expert supplies the value ρt
that this feature should take.
For example, for the
case of a helicopter performing an in-place ﬂip, we use
an observation that corresponds to our expert pilot’s
knowledge that the helicopter should stay at a ﬁxed
position while it is ﬂipping. We assume that these observations may be corrupted by Gaussian noise, where
the variance of the noise expresses our conﬁdence in
the accuracy of the expert’s advice. In the case of the
ﬂip, the variance expresses our knowledge that it is,
in fact, impossible to ﬂip perfectly in-place and that
the actual position of the helicopter may vary slightly
from the position given by the expert.
Incorporating prior knowledge of this kind can greatly
enhance the learned ideal trajectory.
We give more
detailed examples in Section 5.
2.2.4. Model Summary
In summary, we have the following generative model:
f(zt) + β⋆
ρ(zt) + ω(ρ)
are zero mean Gaussian
random variables with respective covariance matrices
Σ(z), Σ(β), Σ(δ), Σ(ρ), Σ(y).
The transition probabilities for τ k
j are deﬁned by Eqs. (3, 4) with parameters
3 (collectively denoted d).
3. Trajectory Learning Algorithm
Our learning algorithm automatically ﬁnds the timealignment indexes τ, the time-index transition probabilities d, and the covariance matrices Σ(·) by (approximately) maximizing the joint likelihood of the
observed trajectories y and the observed prior knowl-
Learning for Control from Multiple Demonstrations
edge about the ideal trajectory ρ, while marginalizing
out over the unobserved, intended trajectory z. Concretely, our algorithm (approximately) solves
τ,Σ(·),d log P(y, ρ, τ ; Σ(·), d).
Then, once our algorithm has found τ, d, Σ(·), it ﬁnds
the most likely hidden trajectory, namely the trajectory z that maximizes the joint likelihood of the observed trajectories y and the observed prior knowledge
about the ideal trajectory ρ for the learned parameters
τ, d, Σ(·).4
The joint optimization in Eq. (11) is diﬃcult because
(as can be seen in Figure 1) the lack of knowledge of
the time-alignment index variables τ introduces a very
large set of dependencies between all the variables.
However, when τ is known, the optimization problem
in Eq. (11) greatly simpliﬁes thanks to context speciﬁc independencies . When τ
is ﬁxed, we obtain a model such as the one shown in
Figure 2. In this model we can directly estimate the
multinomial parameters d in closed form; and we have
a standard HMM parameter learning problem for the
covariances Σ(·), which can be solved using the EM algorithm —often referred to as
Baum-Welch in the context of HMMs. Concretely, for
our setting, the EM algorithm’s E-step computes the
pairwise marginals over sequential hidden state variables by running a (extended) Kalman smoother; the
M-step then uses these marginals to update the covariances Σ(·).
Figure 2. Example of graphical model when τ is known.
(Shaded nodes are observed.)
To also optimize over the time-indexing variables τ,
we propose an alternating optimization procedure. For
4Note maximizing over the hidden trajectory and the
covariance parameters simultaneously introduces undesirable local maxima: the likelihood score would be highest
(namely inﬁnity) for a hidden trajectory with a sequence
of states exactly corresponding to the (crude) dynamics
model f(·) and state-transition covariance matrices equal
to all-zeros as long as the observation covariances are nonzero. Hence we marginalize out the hidden trajectory to
ﬁnd τ , d, Σ(·).
ﬁxed Σ(·) and d, and for ﬁxed z, we can ﬁnd the optimal time-indexing variables τ using dynamic programming over the time-index assignments for each demonstration independently. The dynamic programming algorithm to ﬁnd τ is known in the speech recognition
literature as dynamic time warping and in the biological sequence alignment literature as the Needleman-Wunsch algorithm . The ﬁxed z we use, is the one that
maximizes the likelihood of the observations for the
current setting of parameters τ, d, Σ(·).5
In practice, rather than alternating between complete
optimizations over Σ(·), d and τ, we only partially optimize over Σ(·), running only one iteration of the EM
algorithm.
We provide the complete details of our algorithm in
the full paper .
4. Local Model Learning
For complex dynamical systems, the state zt used
in the dynamics model often does not correspond to
the “complete state” of the system, since the latter
could involve large numbers of previous states or unobserved variables that make modeling diﬃcult.6 However, when we only seek to model the system dynamics
along a speciﬁc trajectory, knowledge of both zt and
how far we are along that trajectory is often suﬃcient
to accurately predict the next state zt+1.
Once the alignments between the demonstrations are
computed by our trajectory learning algorithm, we can
use the time aligned demonstration data to learn a sequence of trajectory-speciﬁc models. The time indices
of the aligned demonstrations now accurately associate
the demonstration data points with locations along the
learned trajectory, allowing us to build models for the
state at time t using the appropriate corresponding
data from the demonstration trajectories.7
5Fixing z means the dynamic time warping step only
approximately optimizes the original objective. Unfortunately, without ﬁxing z, the independencies required to
obtain an eﬃcient dynamic programming algorithm do not
In practice we ﬁnd our approximation works very
6This is particularly true for helicopters. Whereas the
state of the helicopter is very crudely captured by the 12D
rigid-body state representation we use for our controllers,
the “true” physical state of the system includes, among
others, the airﬂow around the helicopter, the rotor head
speed, and the actuator dynamics.
7We could learn the richer local model within the trajectory alignment algorithm, updating the dynamics model
during the M-step.
We chose not to do so since these
models are more computationally expensive to estimate.
The richer models have minimal inﬂuence on the alignment
because the biases capture the average model error—the
richer models capture the derivatives around it. Given the
limited inﬂuence on the alignment, we chose to save computational time and only estimate the richer models after
Learning for Control from Multiple Demonstrations
Figure 3. Our XCell Tempest autonomous helicopter.
To construct an accurate nonlinear model to predict
zt+1 from zt, using the aligned data, one could use locally weighted linear regression ,
where a linear model is learned based on a weighted
dataset. Data points from our aligned demonstrations
that are nearer to the current time index along the
trajectory, t, and nearer the current state, zt, would
be weighted more highly than data far away. While
this allows us to build a more accurate model from
our time-aligned data, the weighted regression must
be done online, since the weights depend on the current state, zt. For performance reasons8 this may often
be impractical. Thus, we weight data only based on
the time index, and learn a parametric model in the remaining variables (which, in our experiments, has the
same form as the global “crude” model, f(·)). Concretely, when estimating the model for the dynamics
at time t, we weight a data point at time t′ by:9
W(t′) = exp
where σ is a bandwidth parameter. Typical values for
σ are between one and two seconds in our experiments.
Since the weights for the data points now only depend
on the time index, we can precompute all models ft(·)
along the entire trajectory. The ability to precompute
the models is a feature crucial to our control algorithm,
which relies heavily on fast simulation.
5. Experimental Results
5.1. Experimental Setup
To test our algorithm, we had our expert helicopter
pilot ﬂy our XCell Tempest helicopter (Figure 3),
alignment.
8During real-time control execution,
our model is
queried roughly 52000 times per second. Even with KDtree or cover-tree data structures a full locally weighted
model would be much too slow.
9In practice, the data points along a short segment of
the trajectory lie in a low-dimensional subspace of the state
space. This sometimes leads to an ill-conditioned parameter estimation problem.
To mitigate this problem, we
regularize our models toward the “crude” model f(·).
which can perform professional, competition-level maneuvers.10
We collected multiple demonstrations from our expert
for a variety of aerobatic trajectories: continuous inplace ﬂips and rolls, a continuous tail-down “tic toc,”
and an airshow, which consists of the following maneuvers in rapid sequence: split-S, snap roll, stall-turn,
loop, loop with pirouette, stall-turn with pirouette,
“hurricane” (fast backward funnel), knife-edge, ﬂips
and rolls, tic-toc and inverted hover.
The (crude) helicopter dynamics f(·) is constructed
using the method of Abbeel et al.
 .11
helicopter dynamics model predicts linear and angular
accelerations as a function of current state and inputs.
The next state is then obtained by integrating forward
in time using the standard rigid-body equations.
In the trajectory learning algorithm, we have bias
t for each of the predicted accelerations. We
use the state-drift variables, δk
t , for position only.
For the ﬂips, rolls, and tic-tocs we incorporated our
prior knowledge that the helicopter should stay in
place. We added a measurement of the form:
0 = p(zt) + ω(ρ0),
ω(ρ0) ∼N(0, Σ(ρ0))
where p(·) is a function that returns the position coordinates of zt, and Σ(ρ0) is a diagonal covariance matrix. This measurement—which is a direct observation
of the pilot’s intended trajectory—is similar to advice
given to a novice human pilot to describe the desired
maneuver: A good ﬂip, roll, or tic-toc trajectory stays
close to the same position.
We also used additional advice in the airshow to indicate that the vertical loops, stall-turns and split-S
should all lie in a single vertical plane; that the hurricanes should lie in a horizontal plane and that a good
knife-edge stays in a vertical plane. These measurements take the form:
c = N ⊤p(zt) + ω(ρ1),
ω(ρ1) ∼N(0, Σ(ρ1))
where, again, p(zt) returns the position coordinates of
zt. N is a vector normal to the plane of the maneuver, c is a constant, and Σ(ρ1) is a diagonal covariance
10We instrumented the helicopter with a Microstrain
3DM-GX1 orientation sensor. A ground-based camera system measures the helicopter’s position. A Kalman ﬁlter
uses these measurements to track the helicopter’s position,
velocity, orientation and angular rate.
11The model of Abbeel et al. naturally generalizes to any orientation of the helicopter regardless of the
ﬂight regime from which data is collected.
Hence, even
without collecting data from aerobatic ﬂight, we can reasonably attempt to use such a model for aerobatic ﬂying,
though we expect it to be relatively inaccurate.
Learning for Control from Multiple Demonstrations
Figure 4. Colored lines: demonstrations. Black dotted line: trajectory inferred by our algorithm. (See text for details.)
5.2. Trajectory Learning Results
Figure 4(a) shows the horizontal and vertical position
of the helicopter during the two loops ﬂown during
the airshow.
The colored lines show the expert pilot’s demonstrations. The black dotted line shows the
inferred ideal path produced by our algorithm. The
loops are more rounded and more consistent in the inferred ideal path. We did not incorporate any prior
knowledge to this extent.
Figure 4(b) shows a topdown view of the same demonstrations and inferred
trajectory. The prior successfully encouraged the inferred trajectory to lie in a vertical plane, while obeying the system dynamics.
Figure 4(c) shows one of the bias terms, namely the
model prediction errors for the Z-axis acceleration of
the helicopter computed from the demonstrations, before time-alignment. Figure 4(d) shows the result after
alignment (in color) as well as the inferred acceleration
error (black dotted). We see that the unaligned bias
measurements allude to errors approximately in the -
1G to -2G range for the ﬁrst 40 seconds of the airshow
(a period that involves high-G maneuvering that is not
predicted accurately by the “crude” model). However,
only the aligned biases precisely show the magnitudes
and locations of these errors along the trajectory. The
alignment allows us to build our ideal trajectory based
upon a much more accurate model that is tailored to
match the dynamics observed in the demonstrations.
Results for other maneuvers and state variables are
similar. At the URL provided in the introduction we
posted movies which simultaneously replay the diﬀerent demonstrations, before alignment and after alignment. The movies visualize the alignment results in
many state dimensions simultaneously.
5.3. Flight Results
After constructing the idealized trajectory and models
using our algorithm, we attempted to ﬂy the trajectory
on the actual helicopter.
Our helicopter uses a receding-horizon diﬀerential dynamic programming (DDP) controller . DDP approximately solves general continuous state-space optimal control problems by taking
advantage of the fact that optimal control problems
with linear dynamics and a quadratic reward function
(known as linear quadratic regulator (LQR) problems)
can be solved eﬃciently. It is well-known that the solution to the (time-varying, ﬁnite horizon) LQR problem is a sequence of linear feedback controllers.
short, DDP iteratively approximates the general control problem with LQR problems until convergence, resulting in a sequence of linear feedback controllers that
are approximately optimal. In the receding-horizon algorithm, we not only run DDP initially to design the
sequence of controllers, but also re-run DDP during
control execution at every time step and recompute
the optimal controller over a ﬁxed-length time interval
(the horizon), assuming the precomputed controller
and cost-to-go are correct after this horizon.
As described in Section 4, our algorithm outputs a
sequence of learned local parametric models, each of
the form described by Abbeel et al.
implementation linearizes these models on the ﬂy with
a 2 second horizon (at 20Hz). Our reward function
penalizes error from the target trajectory, s⋆
t , as well
as deviation from the desired controls, u⋆
t , and the
desired control velocities, u⋆
First we compare our results with the previous stateof-the-art in aerobatic helicopter ﬂight, namely the inplace rolls and ﬂips of Abbeel et al.
work used hand-speciﬁed target trajectories and a single nonlinear model for the entire trajectory.
Figure 5(a) shows the Y-Z position12 and the collective (thrust) control inputs for the in-place rolls for
both their controller and ours. Our controller achieves
(i) better position performance (standard deviation of
approximately 2.3 meters in the Y-Z plane, compared
to about 4.6 meters and (ii) lower overall collective
control values (which roughly represents the amount
of energy being used to ﬂy the maneuver).
Similarly, Figure 5(b) shows the X-Z position and the
collective control inputs for the in-place ﬂips for both
controllers.
Like for the rolls, we see that our controller signiﬁcantly outperforms that of Abbeel et al.
 , both in position accuracy and in control energy
12These are the position coordinates projected into a
plane orthogonal to the axis of rotation.
Learning for Control from Multiple Demonstrations
Altitude (m)
North Position (m)
Collective Input
Altitude (m)
East Position (m)
Collective Input
North Position (m)
Altitude (m)
Figure 5. Flight results. (a),(b) Solid black: our results. Dashed red: Abbeel et al. . (c) Dotted black: autonomous
tic-toc. Solid colored: expert demonstrations. (See text for details.)
Besides ﬂips and rolls, we also performed autonomous
“tic tocs”—widely considered to be an even more challenging aerobatic maneuver. During the (tail-down)
tic-toc maneuver the helicopter pitches quickly backward and forward in-place with the tail pointed toward
the ground (resembling an inverted clock pendulum).
The complex relationship between pitch angle, horizontal motion, vertical motion, and thrust makes it extremely diﬃcult to create a feasible tic-toc trajectory
by hand. Our attempts to use such a hand-coded trajectory with the DDP algorithm from failed repeatedly.
By contrast, our algorithm
readily yields an excellent feasible trajectory that was
successfully ﬂown on the ﬁrst attempt.
Figure 5(c)
shows the expert trajectories (in color), and the autonomously ﬂown tic-toc (black dotted).
Our controller signiﬁcantly outperforms the expert’s demonstrations.
We also applied our algorithm to successfully ﬂy a
complete aerobatic airshow, which consists of the following maneuvers in rapid sequence: split-S, snap roll,
stall-turn, loop, loop with pirouette, stall-turn with
pirouette, “hurricane” (fast backward funnel), knifeedge, ﬂips and rolls, tic-toc and inverted hover.
The trajectory-speciﬁc local model learning typically
captures the dynamics well enough to ﬂy all the aforementioned maneuvers reliably.
Since our computer
controller ﬂies the trajectory very consistently, however, this allows us to repeatedly acquire data from
the same vicinity of the target trajectory on the real
helicopter. Similar to Abbeel et al. , we incorporate this ﬂight data into our model learning, allowing
us to improve ﬂight accuracy even further. For example, during the ﬁrst autonomous airshow our controller
achieves an RMS position error of 3.29 meters, and this
procedure improved performance to 1.75 meters RMS
position error.
Videos of all our ﬂights are available at:
 
6. Related Work
Although no prior works span our entire setting of
learning for control from multiple demonstrations,
there are separate pieces of work that relate to various components of our approach.
Atkeson and Schaal use multiple demonstrations to learn a model for a robot arm, and then ﬁnd an
optimal controller in their simulator, initializing their
optimal control algorithm with one of the demonstrations.
The work of Calinon et al. considered learning
trajectories and constraints from demonstrations for
robotic tasks. There, they do not consider the system’s
dynamics or provide a clear mechanism for the inclusion of prior knowledge. Our formulation presents a
principled, joint optimization which takes into account
the multiple demonstrations, as well as the (complex)
system dynamics and prior knowledge. While Calinon
et al. also use some form of dynamic time warping, they do not try to optimize a joint objective capturing both the system dynamics and time-warping.
Among others, An et al. and, more recently,
Abbeel et al.
 have exploited the idea of
trajectory-indexed model learning for control. However, contrary to our setting, their algorithms do not
time align nor coherently integrate data from multiple
trajectories.
While the work by Listgarten et al. does not consider robotic control and model learning, they also consider the problem of multiple continuous time series alignment with
a hidden time series.
Our work also has strong similarities with recent work
on inverse reinforcement learning, which extracts a reward function (rather than a trajectory) from the expert demonstrations. See, e.g., Ng and Russell ;
Abbeel and Ng ; Ratliﬀet al. ; Neu and
Szepesvari ; Ramachandran and Amir ;
Syed and Schapire .
Learning for Control from Multiple Demonstrations
Most prior work on autonomous helicopter ﬂight only
considers the ﬂight-regime close to hover.
are three notable exceptions.
The aerobatic work
of Gavrilets et al. comprises three maneuvers:
split-S, snap-roll, and stall-turn, which we also include
during the ﬁrst 10 seconds of our airshow for comparison. They record pilot demonstrations, and then
hand-engineer a sequence of desired angular rates and
velocities, as well as transition points. Ng et al. 
have their autonomous helicopter perform sustained
inverted hover. We compared the performance of our
system with the work of Abbeel et al. , by far
the most advanced autonomous aerobatics results to
date, in Section 5.
7. Conclusion
We presented an algorithm that takes advantage of
multiple suboptimal trajectory demonstrations to (i)
extract (an estimate of) the ideal demonstration, (ii)
learn a local model along this trajectory. Our algorithm is generally applicable for learning trajectories
and dynamics models along trajectories from multiple demonstrations.
We showed the eﬀectiveness of
our algorithm for control by applying it to the challenging problem of autonomous helicopter aerobatics.
The ideal target trajectory and the local models output by our trajectory learning algorithm enable our
controllers to signiﬁcantly outperform the prior state
of the art.
Acknowledgments
We thank Garett Oku for piloting and building our
helicopter. Adam Coates is supported by a Stanford
Graduate Fellowship. This work was also supported
in part by the DARPA Learning Locomotion program
under contract number FA8650-05-C-7261.