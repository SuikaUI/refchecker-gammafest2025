Adaptive Sparseness for Supervised Learning
Ma´rio A.T. Figueiredo, Senior Member, IEEE
Abstract—The goal of supervised learning is to infer a functional mapping based on a set of training examples. To achieve good
generalization, it is necessary to control the “complexity” of the learned function. In Bayesian approaches, this is done by adopting a
prior for the parameters of the function being learned. We propose a Bayesian approach to supervised learning, which leads to sparse
solutions; that is, in which irrelevant parameters are automatically set exactly to zero. Other ways to obtain sparse classifiers (such as
Laplacian priors, support vector machines) involve (hyper)parameters which control the degree of sparseness of the resulting
classifiers; these parameters have to be somehow adjusted/estimated from the training data. In contrast, our approach does not
involve any (hyper)parameters to be adjusted or estimated. This is achieved by a hierarchical-Bayes interpretation of the Laplacian
prior, which is then modified by the adoption of a Jeffreys’ noninformative hyperprior. Implementation is carried out by an expectationmaximization (EM) algorithm. Experiments with several benchmark data sets show that the proposed approach yields state-of-the-art
performance. In particular, our method outperforms SVMs and performs competitively with the best alternative techniques, although it
involves no tuning or adjustment of sparseness-controlling hyperparameters.
Index Terms—Supervised learning, classification, regression, sparseness, feature selection, kernel methods, expectationmaximization algorithm.
INTRODUCTION: THE LEARNING PROBLEM
UPERVISED learning can be formalized as the problem of
inferring a function y ¼ fðxÞ, based on a training set
D ¼ fðx1; y1Þ; . . . ; ðxn; ynÞg. Usually, the inputs are d-dimensional vectors, xi = ½xi; 1, . . . ; xi; dT 2
IRd. When y is
continuous (e.g., y 2 IR), we are in the context of regression,
whereas in classification problems, y is of categorical nature
(e.g., binary, y 2 f1; 1g). The obtained function is evaluated
by how well it generalizes, i.e., how accurately it performs on
new data assumed to follow the same distribution as the
training data. Usually, the function fðÞ is assumed to have a
fixed structure and to depend on a set of parameters  (say,
the coefficients in a linear classifier, or the weights in a feedforward neural network); in this case, we write y ¼ fðx; Þ,
and the goal becomes to estimate  from the training data.
It is known that, to achieve good generalization, it is
necessary to control the complexity of the learned function. If
it is too complex, it may follow irrelevant properties of the
particular data set on which it is trained (what is usually
called overfitting). Conversely, an overly simple function
may not be rich enough to capture the the true underlying
relationship (underfitting). This is a well-known problem
which has been addressed with a variety of formal tools
(see, e.g., , , , , , and references therein).
OVERVIEW OF METHODS AND THE PROPOSED
Discriminative versus Generative Learning
Supervised learning can be formulated using either a
generative approach or a discriminative approach. Basically,
the generative approach involves estimating the joint
probability density function pðx; yÞ from D. In classification,
this is usually done by learning the so-called classconditional densities, pðxjyÞ, and the probability of each
class, pðyÞ, . In regression, this can be done, for example,
by representing the joint density using a kernel method or a
Gaussian mixture (see and references therein). From this
joint probability function estimate, optimal Bayesian decision rules can be derived by the standard Bayesian decision
theory machinery .
In the discriminative approach, the focus is on learning the
function fðx; Þ directly from the training set. Well known
discriminative approaches include linear and generalized
linear models, k-nearest neighbor classifiers, tree classifiers
 , feed-forward neural networks , support vector
machines (SVM), and other kernel-based methods , ,
 . Although, usually computationally more demanding,
discriminative approaches tend to perform better, especially
with small training data sets (see ).The approach described
in this paper falls in the discriminative category.
Bayesian Discriminative Learning with
Gaussian Priors
A Bayesian approach to complexity control in discriminative learning consists in using a prior pðjÞ favoring
simplicity, or smoothness, in some sense, of the function to be
learned (where  is a vector of hyperparameters) . The
usual choice, namely for analytical and computational
tractability, is a zero-mean Gaussian prior, which appears
under different guises, like ridge regression , or weight
decay , . Gaussian priors are also at the heart of the
nonparametric Gaussian processes approach , , ,
which has roots in earlier spline models and regularized radial basis functions .
The main disadvantage of Gaussian priors is that they do
not control the structural complexity of the learned function.
That is, if one of the components of  (say, a given coefficient
of a linear classifier) happens to be irrelevant, it will not be set
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
SEPTEMBER 2003
. The author is with the Institute of Telecommunications and the
Department of Electrical and Computer Engineering, Instituto Superior
Te´cnico, 1049-001 Lisboa, Portugal. E-mail: .
Manuscript received 13 Mar. 2002; revised 26 Sept. 2002; accepted 20 Mar.
Recommended for acceptance by A. Del Bimbo.
For information on obtaining reprints of this article, please send e-mail to:
 , and reference IEEECS Log Number 116078.
0162-8828/03/$17.00  2003 IEEE
Published by the IEEE Computer Society
exactly to zero. Any structural simplification will have to be
based on additional tests.
Sparseness
A sparse estimate of  is defined as one in which irrelevant or
redundant components are exactly zero. Sparseness is
desirable in supervised learning for several reasons,
Sparseness leads to a structural simplification of the
estimated function.
Obtaining a sparse estimate corresponds to performing feature/variable selection.
In kernel-based methods, the generalization ability
improves with the degree of sparseness (a key idea
behind SVM , ).
One of the possible ways to achieve sparse estimates is to
use a zero-mean Laplacian (rather than Gaussian) prior,
expf jijg ¼ exp  kk1
where kk1 ¼ P
i jij denotes the l1 norm, and  is the
parameter of this density. The sparseness-inducing nature
of the Laplacian prior (or, equivalently, of the l1 penalty
from a regularization point of view) is well-known and has
been exploited in several areas , , .
SVMs are another approach to supervised learning
leading to sparse structures. Both in approaches based on
Laplacian priors and in SVMs, there are hyperparameters
(e.g.,  in (1)) controlling the degree of sparseness of the
obtained estimates. These parameters are commonly adjusted by crossvalidation methods which may be very time
consuming.
Proposed Approach
We propose a Bayesian approach to sparse regression and
classification whose main advantage is the absence of
parameters controlling the degree of sparseness. This is
achieved with the following building blocks:
a hierarchical-Bayes interpretation of the Laplacian
prior as a normal/independent distribution (as proposed for robust regression in );
a Jeffreys’ noninformative second-level hyperprior
(in the same spirit as ), which expresses scaleinvariance and, more importantly, is parameterfree ;
an expectation-maximization (EM) algorithm which
yields a maximum a posteriori (MAP) estimate of 
(and of the observation noise variance, in the case of
regression).
Experimental evaluation of the proposed method (Sections 5.1 and 5.2), both with synthetic and real data, show
that our method performs competitively with (often better
than) the state-of-the-art methods (such as SVM).
Related Approaches
Our method is formally and conceptually related to the
automatic relevance determination (ARD) concept , ,
which underlies the recently proposed relevance vector
machine (RVM) , . The RVM exhibits state-of-the-art
performance: It beats SVMs, both in terms of accuracy and
sparseness , . However, our approach does not rely
on a type-II maximum likelihood approximation (as in
ARD and RVM); rather, our modeling assumptions lead to a
marginal a posteriori probability function on  whose mode
is located by a very simple EM algorithm. A more detailed
explanation of the difference between our approach and the
RVM will be presented in Section 3.5.
Organization of the Paper
In Section 3, after reviewing linear regression and Laplacian
priors, we introduce our approach. Section 4 extends the
formulation to classification problems by using a (probit)
generalized linear model. Experimental results are reported
in Section 5. Finally, Section 6 concludes the paper by
presenting some concluding remarks, including a brief
discussion of the limitations of the approach and possible
directions of future research.
REGRESSION
Introduction: Linear Regression with
Gaussian Prior
In this paper, we consider regression functions that are
linear with respect to the parameter vector  (whose
dimensionality we will denote by k), i.e.,
ihiðxÞ ¼ ThðxÞ;
where hðxÞ ¼ ½h1ðxÞ; . . . ; hkðxÞT
is a vector of k fixed
functions of the input, often called features. Functions of
this form include the following well-known formulations:
Linear regression, in which hðxÞ ¼ ½1; x1; . . . ; xdT; in
this case, k ¼ d þ 1.
Nonlinear regression via a set of k fixed basis
functions, where hðxÞ ¼ ½1ðxÞ; . . . ; kðxÞT; usually,
1ðxÞ ¼ 1.
Kernel regression, hðxÞ = ½1; Kðx; x1Þ; . . . , Kðx; xnÞT,
where Kðx; xiÞ is some (symmetric) kernel function
 (as in SVM and RVM regression); in this case
k ¼ n þ 1.
We follow the standard assumption that the output
variables in the training set were contaminated by additive
white Gaussian noise, yi ¼ fðxi; Þ þ wi, for i ¼ 1; . . . ; n,
where ½w1; . . . ; wn is a set of independent zero-mean
Gaussian samples with variance 2. With y ¼ ½y1; . . . ; ynT,
the likelihood function is then
pðyjÞ ¼ N ðyjH; 2IÞ;
where N ðvj; CÞ stands for a Gaussian density of mean 
and covariance C, evaluated at v, H is the so-called design
matrix, and I is an identity matrix. The element ði; jÞ of H,
denoted Hij, is given by Hij ¼ hjðxiÞ.
With a zero-mean Gaussian prior for , with covariance
A, pðjAÞ ¼ N ðj0; AÞ, the posterior pðjyÞ is still Gaussian
with the mode given by the well-known expression
b ¼ ð2A1 þ HTHÞ1HTy:
This is, of course, the maximum a posteriori (MAP) estimate
of  under this Gaussian prior. When A is proportional to
FIGUEIREDO: ADAPTIVE SPARSENESS FOR SUPERVISED LEARNING
identity, say A ¼ 2I, this is called ridge regression .
When  ! 1, we obtain b ¼ ðHTHÞ1HTy, the well-known
ordinary least squares estimate, which may have undesirably
large variance when HTH is an ill-conditioned matrix.
Regression with a Laplacian Prior
To favor a sparse estimate, a Laplacian prior can be adopted
for ; that is
2 expf jijg ¼
expf kk1g:
In this case, the posterior probability density function pðjyÞ
is no longer Gaussian. The MAP estimate of  is no longer a
linear function of y; it is given by
b ¼ arg min
 fkH  yk2
2 þ 2 2 kk1g;
where kvk2
i denotes the squared Euclidean (l2) norm.
In regression, this criterion was named the LASSO (least
absolute shrinkage and selection operator) in . To understand
why the l1 norm induces sparseness, notice that moving a
vector away from the coordinate axes increases the l1 norm
more than the l2 norm. For example, k½1; 0Tk2 = k½1=
Tk2 = 1, while k½1; 0Tk1 = 1 < k½1=
The particular case where H is an orthogonal matrix
gives us some further insight on the l1 penalty. In this case,
since HTH ¼ I, (3) can be solved separately for each i,
bi ¼ arg min
i  2iðHTyÞi þ 2 2  jijg
¼ sgnððHTyÞiÞ jðHTyÞij  2 
where ðHTyÞi denotes the ith component of HTy, ðÞþ is the
positive part operator (defined as ðaÞþ ¼ a, if a  0, and
ðaÞþ ¼ 0, if a < 0), and sgnðÞ is the sign function. Observe
that, when the absolute value of ðHTyÞi is below a
threshold, the estimate bi is exactly zero; otherwise, the
estimate is obtained by subtracting a constant (equal to the
threshold) from the absolute value of ðHTyÞi. This rule is
called the soft threshold (see Fig. 1), and is widely used in
wavelet-based signal/image estimation , .
A Hierarchical-Bayes View of the
Laplacian Prior
Let us now consider that each i has a zero-mean Gaussian
prior pðijiÞ ¼ N ðij0; iÞ, with its own variance i, and that
each i has an exponential (hyper)prior
pðijÞ ¼ 
for i  0:
It is now possible to integrate out i,
pðijiÞpðijÞ di ¼
obtaining a Laplacian density. This shows that the
Laplacian prior is equivalent to a two-level hierarchical-
Bayes model: zero-mean Gaussian priors with independent,
exponentially distributed variances. This equivalence has
been previously exploited to derive EM algorithms for
robust regression under Laplacian noise models . A
related equivalence was also considered in .
Sparse Regression via EM
The hierarchical decomposition of the Laplacian prior allows
using the expectation-maximization (EM) algorithm to
implement the LASSO criterion in (3). This is done simply
by regarding  ¼ ½1; . . . ; k as hidden/missing data. If we could
observe , the complete log-posterior log pð; 2jy; Þ could be
easily obtained; in fact,
pð; 2jy; Þ / pðyj; 2Þ pðjÞ pð2Þ;
where both pðyj; 2Þ and pðjÞ are Gaussian densities (thus,
theproductcanbecarriedoutanalytically),andwesetpð2Þ ¼
“constant” (of course, we could also adopt a conjugate
inverse-Gamma prior for 2, but, for large n, the influence of
the prior on the estimate of 2 is very small). In this case, it
would be easy to obtain the MAP (or any other) estimate of 
and 2. Since we do not observe , we may resort to the EM
algorithm, which produces a sequence of estimates, bðtÞ and
2ðtÞ, for t ¼ 1; 2; 3; . . . , by applying two alternating steps:
E-step. Computes the expected value (with respect to the
missing variables ) of the complete log-posterior, given
the current parameter estimates and the observed data;
this is usually called the Q-function
Qð; 2j bðtÞ; b2
log pð; 2j y; Þ pðj bðtÞ; b2
2ðtÞ; yÞ d:
M-step. Updates the parameter estimates by maximizing
the Q-function with respect to parameters:
ðbðtþ1Þ; b2
2ðtþ1ÞÞ ¼ arg max
;2 Qð; 2j bðtÞ; b2
The EM algorithm converges to a local maximum of the
a posteriori probability density function pð; 2jyÞ /
pðyj; 2Þ pðjÞ, without explicitly using the marginal prior
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
SEPTEMBER 2003
Fig. 1. Solid line: Estimation rule produced by the EM Algorithm with the
Jeffrey’s hyperprior, in the orthogonal H case. Dashed line: Hardthreshold rule. Dotted line: Soft-threshold rule (obtained with a Laplacian
pðjÞ, which is not Gaussian, but simply using the
conditional Gaussian prior pðjÞ in each iteration.
Let us now find the particular expressions for the
Q-function and for the parameter updates, under the
modelling assumptions above expressed, which we now
summarize:
pðyj; 2Þ ¼ N ðyjH; 2 IÞ
pð2Þ / 00constant00;
N ðij0; iÞ ¼ N j0; ðÞ
for j  0;
where ðÞ  diagð1
1 ; . . . ; 1
k Þ is a diagonal matrix with
the inverse variances of all the i’s. Moreover, y is the
observed data, and  is missing data.
First, applying logarithms to (6), since pð2Þ is flat,
log pð; 2jy; Þ / log pðyj ; 2Þ þ log pðjÞ
/ n log 2  ky  Hk2
 TðÞ:
Second, since the complete log-posterior is linear with
respect to ðÞ (see (7)), and its other two terms do not
depend on , the E-step reduces to computing the
conditional expectation of ðÞ, given y and the
current estimates b2
2ðtÞ and bðtÞ, which we denote as
VðtÞ ¼ E½ðÞj y; b2
2ðtÞ; bðtÞ
¼ diag E½1
1 j y; b2
2ðtÞ; bðtÞ; . . . ; E½1
k j y; b2
2ðtÞ; bðtÞ
Now, observe that pðij y; ; 2Þ ¼ pðij iÞ because,
given i, i does not depend on y, 2, or j, for j 6¼ i.
So, pðij y; b2
2ðtÞ; bðtÞÞ / pðbi;ðtÞj iÞ pðiÞ.
Since pðbi;ðtÞj iÞ ¼ N ðbi;ðtÞj 0; iÞ and pðiÞ is the
exponential hyperprior given by (5), elementary
integration yields
i j y; b2
2ðtÞ; bðtÞ
i N ðbi;ðtÞj 0; iÞ 
0 N ðbi;ðtÞj 0; iÞ 
VðtÞ ¼  diag jb1;ðtÞj1; . . . ; jbk;ðtÞj1
The Q-function is obtained by plugging VðtÞ in the
place of ðÞ in (7),
Qð; 2j bðtÞ; b2
 n log 2  ky  Hk2
 TVðtÞ:
Finally, the M-step (estimate update equations)
maximizing
Qð; 2j bðtÞ; b2
respect to 2 and , yielding
2ðtþ1Þ ¼ arg max
n log 2  ky  Hk2
¼ ky  HbðtÞk2
bðtþ1Þ ¼ arg max
 ky  Hk2
2ðtþ1ÞVðtÞ þ HTH
Summarizing, the E-step is implemented by (10), while (12)
and (13) constitute the M-step. This EM algorithm is not the
most computationally efficient way to solve (3); see, e.g., the
methods proposed in , . However, it is very simple
to implement and serves our main goal which is to open the
way to the adoption of different hyperpriors.
Comparison with the RVM
We are now in position to explain the difference between
this EM approach and the relevance vector machine (RVM)
 ; for simplicity, we will consider that 2 is known and
omit it from the notation. In the RVM, the modeling
assumptions are similar: each i has a zero-mean Gaussian
prior pðijiÞ ¼ N ðij0; iÞ, with its own variance i. However, rather than integrating out the (hyper)parameters
 ¼ ½1; . . . ; k, explicitly, or implicitly via the EM algorithm, a maximum likelihood estimate b is obtained from
the marginal likelihood pðyjÞ, and then plugged into the
a posteriori density pðjb; yÞ. In the Bayesian literature, this
is known as an empirical Bayes or type-II maximum likelihood
approach .
Getting Rid of the Hyperparameter:
The Jeffreys’ Prior
One question remains: How to adjust , which controls the
degree of sparseness of b? Our proposal is to remove  from
the model, by replacing the exponential hyperprior on each
i (here, generically denoted as ) by a noninformative
Jeffreys hyperprior:
This prior expresses ignorance with respect to scale (see ,
 ) and, most importantly, it is parameter-free. To see why
this prior is scale-invariant, suppose we change the measurement units (scale) in which  is expressed; this defines a new
variable 0 ¼ K, where K is the constant expressing the
change of units/scale. Then, by applying the rule for the
change of variable in a probability density function to
FIGUEIREDO: ADAPTIVE SPARSENESS FOR SUPERVISED LEARNING
pðÞ ¼ 1=, we retain the same prior, pð0Þ / 1=0. Notice that
this prior is not normalizable (its integral is not finite); it is a
so-called improper prior . Of course, this prior no longer
leads to a Laplacian prior on , but to some other prior (see
 for details). As will be shown experimentally, this prior
strongly induces sparseness and leads to a very good
performance.
Computationally, this choice leads to a minor modification of the EM algorithm described above: Matrix VðtÞ is
now given by
VðtÞ ¼ diagðjb1;ðtÞj2; . . . ; jbk;ðtÞj2Þ;
notice the absence of any free parameter in this E-step.
Finally, to get some insight into the behavior of this
scheme, we consider the case in which H is an orthogonal
matrix. In this case, all the equations in the EM algorithm
decouple, and the final estimate of each coefficient bi is only a
function of the corresponding ðHTyÞi. In Fig. 1, we plot bi as a
function of ðHTyÞi, obtained by the EM algorithm initialized
with bi;ð0Þ ¼ ðHTyÞi. For comparison purposes, we also plot
the well-known hard and soft threshold estimation rules .
Observe how the resulting estimation rule is between those
two rules; for large values of ðHTyÞi, the estimate is very close
to ðHTyÞi (equal in the limit), approaching the behavior of the
hard rule; as ðHTyÞi gets smaller, the estimate becomes
progressively shrunken, approaching the behavior of the soft
rule; when ðHTyÞi is below a threshold, the estimate is exactly
zero (as in the hard and soft rules), this being the reason why
sparseness is achieved.
An Important Implementation Detail
Since several elements of b are expected to go to zero, it is
not convenient to deal with VðtÞ, as given by (15), because
that would imply handling arbitrarily large numbers.
However, it turns out that we can rewrite the M-step
bðtþ1Þ¼ UðtÞð b2
2ðtþ1ÞI þ UðtÞHTHUðtÞÞ1UðtÞHTy;
UðtÞ  diagðjb1;ðtÞj; . . . ; jbk;ðtÞjÞ;
thus avoiding the inversion of the elements of bðtÞ. Moreover, it is not necessary to obtain the inverse matrix, but
simply to solve the corresponding linear system, whose
dimension is only the number of nonzero elements in UðtÞ.
CLASSIFICATION
Introduction: Generalized Linear Models
In classification problems, the formulation is somewhat
more complicated due to the categorical nature of the
output variable. A standard approach is to consider a
generalized linear model (GLM) . For a two-class problem
(y 2 f1; 1g), a GLM models the probability that an
observation x belongs to, say, class 1, as given by a
nonlinearity applied to the output of a linear regression
function. Formally, let this nonlinearity (called the link
function) be denoted as : IR ! ½0; 1; then, the GLM is
defined as
Pðy ¼ 1jxÞ ¼ ðThðxÞÞ;
where hðxÞ was defined in Section 3.1. In logistic regression
 , the link function is
ðzÞ ¼ ð1 þ expðzÞÞ1;
shown in Fig. 2. An important feature of the GLM approach
is that it yields class probabilities, rather than a hard
classifier; it can thus be used to obtain optimal classifiers
under different cost functions. For example, if the cost
function is simply the misclassification error, then the
classifier is obtained by thresholding ðThðxÞÞ at 1=2.
The Probit
Although the most common link is the logistic function, here
we adopt the probit model ðzÞ ¼ ðzÞ, where ðzÞ is the
standard Gaussian cumulative distribution function (cdf)
N ðxj0; 1Þ dx:
The rescaled probit ð3 z 2Þ is plotted in Fig. 2, together
with the logistic function; notice that they are almost
indistinguishable . Of course, both the logistic and
probit functions can be rescaled (horizontally), but this scale
is implicitly absorbed by .
Learning a Probit Classifier via EM
The fundamental reason behind our choice of the probit
model is its simple interpretation in terms of hidden
variables . Consider a hidden variable z ¼ ThðxÞ þ w,
where w is a zero-mean unit-variance Gaussian noise
sample pðwÞ ¼ N ðwj0; 1Þ. Then, if the classification rule is
y ¼ 1 if z  0, and y ¼ 1 if z < 0, we obtain the probit
Pðy ¼ 1jxÞ ¼ PðThðxÞ þ w  0Þ ¼ ðThðxÞÞ:
Given training data D ¼ fðx1; y1Þ; . . . ; ðxn; ynÞg, consider
the corresponding vector of hidden/missing variables
z ¼ ½z1; . . . ; znT, where
zi ¼ ThðxiÞ þ wi;
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
SEPTEMBER 2003
Fig. 2. The logistic and (rescaled) probit link functions.
of course, zi is not observed because we also do not know wi.
However, if we had z, we would have a simple linear
regression likelihood with unit noise variance: pðzjÞ =
N ðzjH; IÞ. This observation suggests the use of the EM
algorithm to estimate , by treating z as missing data.
To promote sparseness, we will adopt the same hierarchical prior on  that we have used for regression: pðijiÞ ¼
N ðij0; iÞ and pðiÞ / 1=i (the Jeffreys prior). The complete
log posterior (with the hidden vectors  and z) is
log pðj y; ; zÞ / log pð; y; ; zÞ
/ pðzjÞ pðj Þ pðÞ pðyj zÞ
/ THTH  2THTz  TðÞ;
after dropping all terms that do not depend on . Notice that
this is similar to (7), with two differences: the noise variance is
1;themissing zplaystherole of observationsin theregression
equations. The expected value of ðÞ is, of course, similar to
the regression case, since it only depends on ; accordingly,
we consider the same diagonal matrix defined in (16). In
addition, we also need E½zjbðtÞ; y (notice that the complete
log-posterior is linear with respect to z), which can be
expressed in closed form, for each zi, as
si;ðtÞ  E½zijbðtÞ; y
ðtÞhðxiÞ þ
ðtÞhðxiÞj0;1Þ
ðtÞhðxiÞ 
ðtÞhðxiÞj0;1Þ
if yi ¼ 1:
These expressions are easily derived after noticing that zi is
(conditionally) Gaussian with mean bT
ðtÞhðxiÞ, but lefttruncated at zero if yi ¼ 1, and right-truncated at zero if
yi ¼ 1. With sðtÞ  ½s1;ðtÞ; . . . ; sn;ðtÞT, the M-step is similar to
the regression case:
bðtþ1Þ¼ UðtÞðI þ UðtÞHTHUðtÞÞ1UðtÞHTsðtÞ;
with sðtÞ playing the role of observed data.
Summarizing, for the classification case, the E-step
corresponds to (16) and (20), while the M-step is carried
out by (21).
EXPERIMENTAL RESULTS
Regression
5.1.1 Variable Selection in Linear Regression
Our first example illustrates the use of the proposed
method for variable selection in standard linear regression;
that is, when the regression equation is y ¼ Tx.
We consider a sequence of 20  vectors, all 20dimensional, with an increasing number (from 1 to 20) of
nonzero components. Specifically, ð1Þ = ½3; 0; 0; . . . ; 0, ð2Þ
= ½3; 3; 0; . . . ; 0, . . . , ð20Þ = ½3; 3; 3; . . . ; 3. For each ðiÞ, i ¼ 1,
. . . ; 20, we obtain 100 random (50  20) design matrices,
following the procedure in , and for each of these, we
obtain 50 data-points with unit noise variance. Finally, for
each ðiÞ, we obtain the 100 estimates of ðiÞ (one for each
design matrix). Fig. 3 shows the mean number of estimated
nonzero components, as a function of the true number. Our
method exhibits a very good ability to find the correct
number of nonzero components in , in an adaptive
We next consider two of the experimental conditions that
were studied in : ðaÞ ¼ ½3; 1:5; 0; 0; 2; 0; 0; 0, with  ¼ 3,
and ðbÞ ¼ ½5; 0; 0; 0; 0; 0; 0; 0, with  ¼ 2. In both cases, the
(20  8) design matrices are generated as described in .
We have compared the relative modeling error (defined as
ME ¼ E½kHb  Hk2) improvement (with respect to ordinary least squares) of our solution against several methods
studied in : LASSO with  adjusted by cross-validation
(CV) and generalized crossvalidation (GCV), and subset selection (also based on crossvalidation; see for details). As
shown in Table 1, our method performs comparably with the
best method for each case, although it involves no tuning or
adjustment of parameters, and is computationally simpler
and faster.
5.1.2 Kernel Regression
We now study the performance of our method in kernel
regression; that is, with
y ¼ fðx; Þ ¼ 0 þ
i Kðx; xiÞ;
where Kðx; xiÞ is a kernel function. This type of representation is used in support vector machine (SVM) regression 
and in relevance vector machine (RVM) regression , . In
all the examples, we use Gaussian kernels, i.e.,
FIGUEIREDO: ADAPTIVE SPARSENESS FOR SUPERVISED LEARNING
Fig. 3. Mean number of nonzero components in b versus the number of
nonzero components in  (the dotted line is the identity).
Relative (%) Improvement in Modeling Error of Several Methods
Kðx; xiÞ ¼ exp  kx  xik2
where is a parameter which controls the kernel width.
We begin by considering the synthetic example studied
in and , where the true function is y ¼ sinðxÞ=x (see
Fig. 4). To compare our results to the RVM and the
variational RVM (VRVM), we ran the algorithm on
25 generations of the noisy data. The results are summarized in Table 2 (which also includes the SVM results
reported in ). Finally, we have also applied our method
to the well-known Boston housing data set (using 20 random
partitions of the full data set into 481 training samples and
25 test samples); Table 2 shows the results, again, versus
SVM, RVM, and VRVM regression (as reported in ). In
these tests, our method performs better than RVM, VRVM,
and SVM regression, although it does not require any
adjustment of a sparseness-related parameter; of course, the
results depend on the choice of kernel width (as do the
RVM and SVM results).
Classification
5.2.1 Feature Selection for Linear and Quadratic
Classifiers
For linear, quadratic, or higher order classifiers, our method
may be seen as a feature selection criterion embedded into
the learning algorithm. To illustrate this, consider two
Gaussian classes, both with covariance matrices equal to
identity, and means
; 0; 0; . . . ; 0
zﬄﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄﬄ{
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
d dimensions
and 2 ¼ 1. The optimal Bayes error rate is given by
ð1j0; 1Þ ’ 0:1587, independently of d. Of course, the
optimal classifier for this data is linear and only uses the first
two dimensions of the data. We first trained our classifier,
using both linear and quadratic functions, i.e., the functions
iðxÞ (see Section 3.1) include all the d components, their
squares, and all the pair-wise products, in a total number of
d þ dðd þ 1Þ=2 features. We also trained a standard Bayesian
plug-in classifier obtained by estimating the mean and
covariance of each class. Both classifiers were trained on 100
samples per class, and then tested on independent test sets of
size 1,000 (500 per class). Fig. 5 shows the resulting average
(over 90 repetitions) test set error rate as a function of d. Fig. 6
reports a similar experiment, now with linear classifiers
learned from training sets with 50 samples per class. These
results show that the proposed method exhibits a much
smaller performance degradation as more irrelevant features
are included, when compared with the common plug-in
classifier.
5.2.2 Kernel Classifiers
The final experiments address kernel-based classifiers. As in
the regression case, we use Gaussian kernels (see (22)). Our
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
SEPTEMBER 2003
Mean Root Squared Errors and Mean Number of Kernels for the
“sinðxÞ=x” Function and the Boston Housing Examples
Fig. 5. Average (90 repetitions) test error rates of quadratic classifiers
versus dimensionality.
Fig. 4. Kernel regression example; dotted line true function y ¼ sinðxÞ=x.
Dots: 50 noisy observations ( ¼ 0:1). Solid line: the estimated function.
Circles: data points corresponding to the kernels with nonzero weight,
the “support points.”
first experiment uses Ripley’s synthetic data set,1 in which
each class is a bimodal mixture of two Gaussians; the optimal
error rate for this problem is 0.08 . Fig. 7 shows 100 points
from thetrainingsetandtheresulting classificationboundary
learned byouralgorithm. The five trainingsamplesneeded to
support the selected kernels (like support vectors in SVM) are
marked by small squares. Table 3 shows the average test set
error(and thefinal numberof kernels)of20 classifierslearned
from 20 random subsets of size 100 from the original
250 training samples. For comparison, the table also includes
results reported in for RVM, VRVM (variational RVM),
and SVM classifiers. Our method is comparable to RVM and
VRVM and clearly better than SVM (specially in terms of
sparseness) on this data set. To allow the comparisons, we
chose ¼ 0:5, which is the value used in .
Foradditionaltests,weusedthreewell-knownbenchmark
real-data problems: the Pima indians diabetes,2 where the goal
is to decide whether a subject has diabetes or not, based on
seven measured variables; the Leptograpsus crabs3 where the
problem consists in determinng the sex of crabs, based on five
geometric measurements; and the Wisconsin breast cancer4
(WBC), where the tast is to produce a benign/malignant
diagnosis from a set of 30 numerical features. In the Pima case,
we have 200 predefined training samples and 332 test
samples. For the crabs problem, there are 80 (also predefined)
training samples and 120 test samples. For the WBC problem,
there is a total of 569 samples; the results reported were
obtained by averaging over 30 random partitions with
300 training samples and 269 test samples (as in ). Prior
to applying our algorithm, all the inputs are normalized to
zero mean and unit variance, as is customary in kernel-based
methods. The kernel width was set to ¼ 4, for the Pima and
crabs problems, and to ¼ 12 for the WBC. Table 4 reports the
numbers of errors achieved by the proposed method and by
several other state-of-the-art techniques. On the Pima and
crabs data sets, our algorithm outperforms all the other
techniques. On the WBC data set, our method performs
nearly as well as the best available alternative. The running
time of our learning algorithm (implemented in MATLAB) is
less than 1 second for the crabs data set, and about 2 seconds
for the Pima and WBC problems. Finally, we note that the
kernel classifiers obtained with our algorithm use only 6, 5,
and 5 kernels (they are very sparse), for the Pima, crabs, and
WBC data sets, respectively. Compare this with the 110 support vectors (more than half the training set) selected by the
SVM (reported in ) on the Pima data set.
Like the SVM, our learning algorithm can be extended to
k-class problems (with k > 2) by learning k binary classifiers
(each class versus the others). There are other ways of
extending binary classifiers to multiclass problems, but we
willnotfocusonthisissuehere.Totesttheperformanceofour
FIGUEIREDO: ADAPTIVE SPARSENESS FOR SUPERVISED LEARNING
Fig. 7. Classification boundary for Ripley’s data. The data points
corresponding to the selected kernels are marked by squares.
Mean Test Error and Number of Kernels on
Ripley’s Synthetic Data Set
Numbers of Test Errors on Three Data Sets Studied
1. Available at www.stats.ox.ac.uk/pub/PRNN/.
2. Available at www.stats.ox.ac.uk/pub/PRNN/.
3. Available at www.stats.ox.ac.uk/pub/PRNN/.
4. Available at www.ics.uci.edu/~mlearn/MLSummary.html.
Fig. 6. Average (90 repetitions) test error rates of linear classifiers
versus dimensionality.
method on a multiclass problem, we used the forensic glass
data set,5 which is a 6-class problem with nine features.
Following , the classification error rate was estimated
using 10-fold cross-validation. The results in Table 5 show
that ourmethod outperforms thebest method referred in ,
a Gaussian process (GP) classifier implemented by MCMC.
The GP-MCMC classifier requires about 24 hours of
computer time , while ours is learned in a few seconds.
CONCLUDING REMARKS
We have introduced a new sparseness-inducing prior
related to the Laplacian prior. Its key feature is the absence
of hyperparameters to be adjusted or estimated, achieved
through the use of a (parameter-free) Jeffreys’ prior.
Experiments with several publicly available benchmark
data sets, both for regression and classification, have shown
state-of-the-art performance. In particular, on the data sets
considered, our approach outperforms support vector
machines and Gaussian process classifiers, although it
involves no tuning or adjusting of sparseness-controlling
hyperparameters.
In its current form, the kernel version of our approach is
not applicable to large-scale problems (that is, with large
training sets, like in handwritten digit classification), which
were thus not considered in this paper. This fact is due to
the need (in the M-step) to solve a linear system of
dimension n þ 1, (where n is the number of training points),
whose computational requirements scale with the third
power of n. This computational issue is a topic of current
interest to researchers in kernel-based methods (e.g., ),
and we also intend to focus on it. Another well-known
limitation of kernel methods is the need to adjust the kernel
parameter(s) (e.g., the Gaussian kernel width in (22)). Of
course, the results of our method strongly depend on an
adequate choice of these parameters, and our formulation
does not contribute to the solution of this problem.
ACKNOWLEDGMENTS
Earlier versions of this work were presented in and .
This work was supported by the Foundation for Science and
Technology, Portuguese Ministry of Science and Technology, under project POSI/33143/SRI/2000.