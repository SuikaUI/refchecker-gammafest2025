Meta-learning for time series forecasting and forecast
combination
Christiane Lemke and Bogdan Gabrys
Smart Technologies Research Centre, School of Design, Engineering and Computing, Bournemouth
University, Poole House, Talbot Campus, Poole, BH12 5BB, UK, Phone: +44 1202 595298, Fax: +44 1202
595314, email: [clemke, bgabrys]@bournemouth.ac.uk
In research of time series forecasting, a lot of uncertainty is still related to the task of
selecting an appropriate forecasting method for a problem. It is not only the individual algorithms that are available in great quantities; combination approaches have been
equally popular in the last decades. Alone the question of whether to choose the most
promising individual method or a combination is not straightforward to answer. Usually, expert knowledge is needed to make an informed decision, however, in many cases
this is not feasible due to lack of resources like time, money and manpower. This work
identiﬁes an extensive feature set describing both the time series and the pool of individual forecasting methods. The applicability of diﬀerent meta-learning approaches
are investigated, ﬁrst to gain knowledge on which model works best in which situation,
later to improve forecasting performance. Results show the superiority of a rankingbased combination of methods over simple model selection approaches.
Key words: Forecasting, Forecast combination, Time series, Time series features,
Meta-learning, Diversity
1. Introduction
Time series forecasting has been a very active area of research since the 1950’s,
with research on the combination of time series forecasts starting a few years later.
During this time, many empirical studies on forecasting performance have been conducted to assess performance of the continuously growing numbers of available algorithms, for example in and . These studies however fail to provide consistent
 
November 6, 2009
results as to which actual method performs best, which is not surprising considering the
variety in investigated time series forecasting problems. Robert J. Hyndman described
the future challenges for time series prediction in the following words: ”Now it is
time to identify why some methods work well and others do not”.
But what is it that determines the success or failure of a forecasting model? The
well-known no-free-lunch theorem, for example described in , states that there
are no algorithms that generally perform better or worse than random when looking
at all possible data sets. This implies, that no assumptions on the performance of an
algorithm can be made if nothing is known about the problem that it is applied to. Of
course, there will be speciﬁc problems for which one algorithm performs better than
another in practice. In accordance to this, this work investigates approaches to relax the
assumption that nothing is known about a problem by automatically extracting domain
knowledge from a data, linking it to well-performing methods and drawing conclusions
for a similar set of time series.
Traditionally, experts visually inspect time series characteristics and ﬁt models according to their judgement. This work investigates an automatic approach, since a
thorough time series analysis by humans is often not feasible in practical applications
that process a large number of time series in very limited time.
A classic and straightforward classiﬁcation for time series has been given by Pegels
 . Time series can thus have patterns that show diﬀerent seasonal eﬀects and trends,
both of which can be additive, multiplicative or non-existent. Gardner extended
this classiﬁcation by including damped trends. Time series analysis in order to ﬁnd
an appropriate ARIMA model has been discussed since the seminal paper of Box and
Jenkins . Guidelines are summarised in and rely heavily on visually examining
autocorrelation and partial autocorrelation values of a series.
The idea of using characteristics of univariate time series to select an appropriate
forecasting model has been pursued since the 1990’s. The ﬁrst systems were rule based
and built on a mix of judgemental and quantitative methods. Collopy and Armstrong
use time series features to generate 99 rules for weighting four diﬀerent models;
features were obtained judgementally, by both visually inspecting the time series and
using domain knowledge. Adya et al. later modify this system and reduced the neces-
sary human input , yet did not abandon expert intervention completely. Vokurka
et al. extract features automatically to weight between three individual models
and a combination in a rule-base that was built automatically, but required manual review of the outputs. Completely automatic systems have been proposed in , where
a generated rule base selects between six forecasting methods. Discriminant analysis
to select between three forecasting methods using 26 features is used in .
The phrase ”meta-learning” in the context of time series was ﬁrst used in and
represents a new term for describing the process of automatically acquiring knowledge for time series forecasting model selection that was adopted from the general
machine learning community. Two case studies are presented in : In the ﬁrst one,
a C4.5 decision tree is used to link six features to the performance of two forecasting
methods; in the second one, the NOEMON approach is used for ranking three
methods. The most recent and comprehensive treatment of the subject can be found
in , where time series are clustered according to their data characteristics and rules
generated judgementally as well as using a decision tree. The approach is then extended to determine weights for a combination of individual models based on data
Meta-learning
Model pool
18 (judgemental)
rule base (judgemental)
smoothing (Holt and
Brown), random walk, linear regression
Vokurka et al.
rule base (partly
automatic)
exp. smoothing (single and
Gardner), structural and a
combination of the three
smoothing (Holt and
Winter), adaptive ﬁltering,
three ”hybrids” of the previous
discriminant
exp. smoothing (single and
Holt-Winter), structural
Adya et al. 
26 (mainly
automatic)
rule base (judgemental)
smoothing (Holt and
Brown), random walk, linear regression
Prudencio and
Ludermir 
exp. smoothing, neural network / random walk, Holt’s
smoothing, auto-regressive
decision tree
random walk,
smoothing,
ARIMA, neural network
Table 1: Time series model selection - overview of literature
characteristics. Table 1 summarises some facts about the related work presented here
for better overview of approaches and methods used. The calculation of features and
meta-learning method listed are implemented automatically if not otherwise stated.
Some time series features presented in this work are similar to the ones used in
literature, but new and diﬀerent features are introduced extending previous work published in . In particular, features concerning the diversity of the pool of algorithms
are included, which is facilitated by adding a number of popular forecast combination
algorithms to the feature pool. In addition to the original question of which model
to select, this work also tries to ﬁnd evidence for features being useful for guiding
the choice of whether to pick an individual model or a combination. In an initial exploratory experiment, decision trees are generated to ﬁnd evidence for the existence
of a link between time series characteristics and the performance of models. Leaving
aside the requirement for interpretable rules and recommendations, four meta-learning
techniques are compared in another empirical experiment, assessing potential performance improvements.
The paper is structured as follows: Section two will present the underlying empirical experiments and results. Section three begins treating the model selection problem
as a classiﬁcation task and describes an extensive number of time series characteristics
which are necessary to link performances of algorithms to the nature of the time series.
The diﬀerent experiments using meta-learning techniques are evaluated in section four.
2. Performance of forecasting and forecast combination methods
This part of this work presents empirical experiments that provide the basis for
further meta-learning analysis. Individual predictors are diverse, but are kept relatively
simple and, more importantly, automatic. These methods perform often just as well as
more complex methods , are more eﬃcient in terms of computational requirements
and also more likely to be employed in practical applications, especially when no expert
advice is available.
2.1. Data sets
Two data sets both consisting of 111 time series have been used in this study; they
were obtained from the NN3 and NN5 neural network forecasting competi-
tions. NN3 data includes monthly empirical business time series with 52 to 126 observations, while the NN5 series are daily time series from cash machine withdrawals with
735 observations each. The competition task was to predict the next 18 or 56 observations, respectively. While NN3 data did not need speciﬁc preprocessing, NN5 data
included some missing values, which were substituted by taking the mean of the value
of the corresponding weekday of the previous and the following week. The last 18 or
56 values of each series were not used for training the models to enable out-of-sample
error evaluation.
2.2. Forecasting Methods
Available forecasting algorithms can be roughly divided into a few groups. Simple
approaches are often surprisingly robust and popular, for example those based on exponential smoothing , . Statisticians and econometricians tend to rely on complex
ARIMA models and their derivatives . The machine learning community mainly
looks at neural networks, either using Multi-Layer-Perceptrons with time-lagged time
series observations as inputs as, for example, in and , or recurrent networks
with a memory, see, for example, . As not all of the algorithms provide native
multi-step-ahead forecasting, some of them are implemented using two approaches:
An iterative approach, where the last prediction is fed back to the model to obtain the
next forecast, or a direct approach, where n diﬀerent predictors are trained for each of
the 1 to n steps ahead problem. The selection of models used in this work is presented
in the next paragraphs.
2.2.1. Simple forecasting models
Many algorithms for forecasting time series are considered simple, yet they are
usually very popular and can be surprisingly powerful. In the latest extensive M3 competitions , an exponential smoothing approach was considered a good match for the
most successful complex method while providing a better trade-oﬀbetween prediction
accuracy and computational complexity. Simple methods used for this experimental
study are listed below, where ˆyt+1 denotes the one-step-ahead prediction and yi the
observation of the time series at time i.
• For the moving average, the arithmetic mean of the last k observations according to equation 1 is calculated. An appropriate time window is found by gridsearching k-values from 1 to 20 and choosing the k with the lowest mean squared
error on a validation set prior to the test set.
• Single exponential smoothing is the simplest representative of smoothing methods and it is calculated by adjusting the previous forecast by the error it produced.
The parameter α controls the extent of the adjustment and is determined again
by minimising errors on a validation set that was separated from the training set.
ˆyt+1 = αyt + (1 −α)ˆyt
• Taylor’s exponential smoothing is a more recently introduced exponential smoothing algorithm with a trend dampened by a factor φ, using a multiplicative approach and a growth rate R . It is given by equation 3, where h is the number
of periods ahead to be forecasted and lt denotes the estimated level of the series
at time t. The parameters α and β are smoothing constants taking values between
zero and one, which are again determined by grid search.
αyt + (1 −α)(lt−1rφ
β(lt/lt−1) + (1 −β)rφ
• Polynomial regression ﬁts a polynomial to the time series by regressing time
series indices against time series values. In this experiment, a suitable order
of the polynome between two and six is grid-searched and the resulting curve
extrapolated into the future; equation 4 shows the example of a regression of
order three, where ωi are parameters estimated using the training set and t is the
current time index.
ˆyt = ω0 + ω1t + ω2t2 + ω3t3
• The Theta-model was introduced in . It decomposes series into short and
long term components by applying a coeﬃcient θ to the second order diﬀerences
of the time series, thus modifying the curvature of the time series. Here, it is
employed using formulas given by . The general equation for a theta-curve
ˆyt+1(θ) = ˆat + ˆbtt + θyt.
Values ˆat and ˆbt are constants determined according to and t is again the
time index. In the original setup in , two curves are used and the obtained
forecasts averaged. The ﬁrst forecast is calculated using θ = 0 which results in a
linear regression problem where the linear part of formula 5 is extrapolated into
the future. The second forecast for θ = 2 is calculated using formula
ˆyt+1(2) = α
(1 −α)iyt−i(2) + (1 −α)ny1(2),
which is a single exponential smoothing applied to series yt(2)
2.2.2. Automatic Box-Jenkins Models
Autoregressive integrated moving average models (ARIMA) according to are
a complex tool of modelling and forecasting time series. They are described by the
notation ARIMA (p,d,q) and consist of the following three parts:
• AR(p) denotes the autoregressive part of order p. Autoregression deﬁnes a regression yt = ω0 + ω1yt−1 + ...ωpyt−p + ϵt where the target variable depends on p
time-lagged values of itself weighted by weights ωi.
• I(d) deﬁnes the degree of diﬀerencing involved. Diﬀerencing is a method of
removing non-stationarity in a time series by calculating the change between
each observation. The ﬁrst diﬀerence of a time series is thus given by y′
• MA(q) indicates the order of the moving average part of the model, which is
given as a moving average of the error series ϵ. It can be described as a regression
against the past error values of the series yt = ω0 + ω1ϵt−1 + ...ωqϵt−q + ϵt
The identiﬁcation of an appropriate ARIMA model for a speciﬁc time series is
not straightforward and usually involves expert knowledge and intervention. Two
automatic approaches have been implemented for this study:
• The original time series as well as two series representing its ﬁrst and second differences are submitted to the automatic ARMA selection process of a MATLAB
toolbox published in , subsequently choosing the approach that produced the
lowest MSE on the validation set. The maximum number of time lags used is
an input parameter of the toolbox and has been set to two, which is suﬃcient in
practice according to . Furthermore, the same authors state, that it is almost
never necessary to generate more than second-order diﬀerences of a time series,
because data usually only involves nonstationarity of the ﬁrst or second level.
• An alternative automatic approach for ARMA-modelling is included in the State-
Space-Models Toolbox . In this case, an appropriate order for p and q was
chosen using the Akaike’s Information criterion (AIC), while a suitable order of
diﬀerencing was determined by the log-likelihood of a model given the corresponding diﬀerenced series.
2.2.3. Structural time series
The structural approach formulates a time series as a number of directly interpretable components such as trends or cycles. Structural models ﬁt into the statistical
framework of state space models, which allows usage of well established algorithms
like the Kalman ﬁlter and Kalman smoother . The State-Space-Models Toolbox
 has been used for implementation of a local level model with a dummy seasonal
component of either twelve or seven, depending on the data set used. A basic local
level consists of a random walk with noise as described by the following formulas:
µt + ϵt, ϵt ∼NID(0, σ2
µt−1 + ηt, ηt ∼NID(0, σ2
where ϵt and ηt are normally and independently distributed error terms. Variable
µt represents a stochastic trend component in the more general structural time series
deﬁnition, for the random walk it simply denotes last time series observations.
2.2.4. Computational Intelligence models
Looking at the pool of available methods belonging to computational intelligence
models, it is neural networks that have most frequently and successfully been used
for forecasting purposes. An extensive summary of work done in this area can be
found in , which is somewhat outdated but still very relevant in terms of guidelines
given. According to these, a feed-forward neural network was implemented. It has
one hidden layer with twelve neurons, training is carried out with a backpropagation
algorithm with momentum. Input variables were the latest observations up to a lag of
seven or twelve to catch weekly or yearly seasonality depending on the data set used.
Ten neural networks have been trained and their predictions averaged to obtain the ﬁnal
forecasts.
Furthermore, a recurrent neural network of the Elman-type has been employed.
There seem to be no general guidelines in literature about the architecture of a recurrent
network for time series forecasting, but one thing seems to be a common agreement:
They need more hidden nodes than the feedforward neural network because the temporal relationship has to be modelled as well. In these experiments, the number of hidden
nodes was set to 24 (double the amount of hidden nodes for the feedforward network).
2.3. Forecast Combinations
Combinations of forecasts are motivated by the fact that all models of a real world
data generation process are likely to be misspeciﬁed and picking only one of the available models is risky, especially if the data and consequently the performance of models
change over time. It is a reliable method of decreasing model risk and aims at improving forecast accuracy by exploiting the diﬀerent strengths of various models while also
compensating their weaknesses. The following methods have been implemented for
the experiments here:
• Using simple average, all available forecasts are averaged, which has proven to
be a successful and robust method ( ).
• The simple average with trimming averages individual forecasts as well, but
without taking the worst performing 20% of the models into account. This is in
accordance with guidelines given in , where 10-30% are recommended.
• In the variance-based model, weights for a linear combination of forecasts are
determined using past forecasting performance according to .
• The outperformance method was proposed in and determines weights
based on the number of times a method performed best in the past.
• In variance-based pooling, past performance is used to group forecasts into two
or three clusters by a k-means algorithm as suggested by Aiolﬁand Timmermann
in . Forecasts of the historically better performing cluster are then averaged
to obtain a ﬁnal forecast.
Literature in the area of nonlinear forecast combination is quite sparse, which is
probably due to the lack of evidence of success as stated in . Only linear combinations are considered here.
2.4. Results
The following tables present result averages of the individual and combined methods for the two data sets. The symmetric mean absolute percentage error (SMAPE)
has been used for evaluating and comparing methods. It is a relative error measure that
enables comparisons between diﬀerent series and also comparison with the results of
the forecasting competitions that provided the data sets. It is given by
(yt + ˆyt)/2 ∗100,
where n denotes the forecasting horizon. The standard deviation for each method
is given to provide a measure of variability across the diﬀerent time series.
Concerning individual methods in the NN3 competition in table 2, the feedforward
neural networks perform quite well, together with the direct moving average. For the
NN5 competitions, results in general are considerably worse, which can be attributed to
the the longer period to be forecasted, where errors can accumulate quite quickly. The
1. Iterated moving average
2. Iterated single exponential smoothing
3. Iterated Taylor smoothing
4. Direct regression
5. Iterated Theta
6. Direct Theta
7. ARIMA v1
8. ARIMA v2
9. Structural model
10. Iterated neural network
11. Iterated elman neural network
12. Direct moving average
13. Direct single exponential smoothing
14. Direct Taylor
15. Direct neural network
Table 2: Performances averaged per data set and standard deviations for forecasting methods, best SMAPE
printed in bold.
local level model with seasonality is the clear-cut winner here, closely followed by the
direct neural network. ARIMA models suﬀer from outliers indicated by high standard
deviation values and can only be ﬁtted well in some cases. The three-cluster pooling
approach outperforms the best individual method in both data sets, as can be observed
in table 3 and has also been shown in previous work . If this approach would have
been submitted to the original NN3 competition, it would have ranked sixth out of 26
participants. The result for the NN5 competition looks less convincing, where it would
have ranked 12th out of 20 participants. This shows that the longer series might require
more complex algorithms due to their length and complexity.
1. simple average
2. simple trimmed average
3. outperformance
4. variance-based
5. pooling (2)
6. pooling (3)
7. regression
Table 3: Performances averaged per data set and standard deviations for combination methods, best SMAPE
printed in bold.
method number
number of time series for which a method performed best
number of time series for which a method performed best
method number
Figure 1: Histogram showing the number of times a particular method performs best for the NN3 data, left:
individual methods, right: combinations
A look at the histograms of the best performing methods in ﬁgures 1 and 2 show
some interesting facts as well: While the best performing individual methods are well
spread on the NN3 data set, it is almost only the structural model and the direct neural
network that perform best for NN5. Simple average combinations consequently perform badly on the NN5 data set, as there are many predictors that comparatively do
not perform well. The regression combination does not have an outstanding average
performance, but performs best for the largest number of the individual series. This
illustrates why it might be beneﬁcial to identify conditions in which one or the other
method is more likely to perform well, which will be investigated in the next sections.
number of time series for which a method performed best
method number
method number
number of time series for which a method performed best
Figure 2: Histogram showing the number of times a particular method performs best for the NN5 data, left:
individual methods, right: combinations
3. Creating a feature pool
In the next step, the question of which individual forecasting method or combination to choose for which time series will be treated as a classiﬁcation problem. This
requires the extraction of a number of features from the available time series, which
will be discussed in this section. Finding suitable time series features for classiﬁcation
is not straightforward, as time series analysis is a complex area of research in itself
 . The features used in this work were selected for their automatic detectability and
their diversity and, as a group, aim to describe the nature of a time series as accurately
as possible. Features describing diversity of the pool of individual methods have been
added to provide information possibly relevant to combining approaches. This section
introduces diﬀerent groups of features before summarising them in a table within each
paragraph.
3.1. General statistics
For calculation of some of the descriptive statistics, the original time series has
been detrended using a polynomial regression of up to order three. The residuals of this
regression e are then subjected to a Durbin-Watson test that checks their autocorrelation
with the formula
t(et −et−1)2
General descriptive statistics in the feature set are standard deviation, skewness
and kurtosis of the detrended series as well as its length. The quotient of the standard
deviation of the original and the detrended series is calculated to provide a measure of
how much of the variability of the series can be accounted for with the trend curve.
Turning points and step changes are adapted from , to capture oscillating behaviour and structural breaks, respectively. A turning point for series with observations yi = {y1...yt} is given if yi is a local maximum or minimum for its two closest
neighbours. A step change is counted whenever
yi −{y1...yi−1}
> 2σ(y1...yi−1), where
{y1...yt−1} is the mean and σ(y1...yi−1) the standard deviation of the series up to point
i−1. Both measures are divided by the number of observations to ensure comparability.
Two measures of interest have been published in : The deterministic component of a time series measure is measured by representing a time series as a number of
delay vectors of embedding dimension m, denoted by yt = [yt−1...yt−m]. Delay vectors
are grouped according to their similarity, so that the variances of the targets provides
an inverse indication of predictability. Furthermore, nonlinearity is estimated by generating surrogate time series as the realisation of the null hypothesis that the series is
linear. If the delay vector representations of original and surrogate series are signiﬁcantly diﬀerent, the time series is considered to be nonlinear.
General statistics
abbreviation
description
standard deviation of detrended series
skewness of series
kurtosis of series
length of series
standard deviation(series)/standard deviation(detrended series)
durbin-watson statistic of regression residuals
turning points
step changes
predictability measure
nonlinearity measure
largest Lyapunov exponent
Table 4: Feature Pool - general statistics
The largest Lyapunov exponent is a measure for the separation rate of state-space
trajectories that were initially close to each other and quantiﬁes chaos in a time series.
The average of the Lyapunov exponents calculated using software provided in was
added to the feature set.
3.2. Frequency domain
A number of features have been extracted from the fast fourier transform of the
detrended series. The frequencies at which the three maximum values of the power
spectrum occur are intended to give an indication of seasons and cycles, the maximum
value of the power spectrum should give an indication of the general strength of the
strongest seasonal or cyclic component. The number of peaks in the power spectrum
that have a value of at least 60% of the maximum value quantify how many strong
recurring components the time series has.
Frequency domain
abbreviation
description
power spectrum frequencies of three biggest values
power spectrum: maximal value
number of peaks not lower than 60% of the maximum
Table 5: Feature pool - frequency domain
3.3. Autocorrelations
Autocorrelation and partial autocorrelation give indications on stationarity and seasonality of a time series; both of the measures have been included for the lags one and
two. Furthermore, domain knowledge on seasonality is exploited by including partial
autocorrelation of lag 12 for the NN3 data set which consists of monthly data and the
partial autocorrelation of lag 7 for the NN5 data, which consists of weekly time series.
Autocorrelations
abbreviation
description
autocorrelations at lags one and two
partial autocorrelations at lags one and two
seasonality measure, pacf for NN5, pacf for NN3
Table 6: Feature pool - autocorrelations
3.4. Diversity features
When dealing with combinations of forecasts, it is crucial to look at characteristics
of the available individual forecasts. It is desirable to have a diverse pool of individual
predictors, ideally with the strengths of one forecast compensating weaknesses of another. Additionally, if there is one extremely superior forecast in the pool of available
methods, it is unlikely that it will be outperformed in a combination with other forecasts. Diversity is a well known concept in ensemble learning, which is a term normally
used to describe strategies for training a number of models sharing the same functional
approach. A few concepts have been successful in this area, for example bagging ,
boosting or negative correlation learning . However, since the predictors used
here are also diverse in their functional approach and most of them cannot be ”trained”
in a machine-learning fashion, these concepts are not applicable.
The standard ways to look at diversity for a number of methods is examining correlation coeﬃcients. The feature pool here includes mean and standard deviation of
the error correlation coeﬃcients of the forecast pool. Other diversity measures have
mainly been discussed in the context of classiﬁcation tasks, for example in and
 . One of the few publications dealing with diversity in a regression context is ,
where an error function e for training regression ensembles is introduced following the
(ˆyi −y)2 −κ ∗1
(ˆyi −¯ˆy)2,
where ˆyi denotes the prediction of the ith of M models, y the actual observation and
¯ˆy the mean of the output of all ensemble members. It has been shown, that the ﬁrst
term of this equation contains the bias and variance error terms, while the second error
term includes the covariance of the ensemble members in addition to these. Hence,
parameter κ controls the extent of the covariance impact on the error. To exploit these
ﬁndings in form of time series features, two values have been added to the feature set:
The proposed error measure in its original form and the quotient of the mean error
(ﬁrst term of equation) and the variability between the members in the method pool
(second term). In this way, the trade oﬀbetween individual accuracy and diversity can
be measured.
Diversity features
abbreviation
description
mean(SMAPEs)-mean(SMAPEs deviation from average SMAPE)
mean(SMAPEs)/mean(SMAPEs deviation from average SMAPE)
mean(correlation coeﬃcients in the method pool)
std(correlation coeﬃcients in the method pool)
number of methods in top performing cluster
distance top performing cluster to second best
Table 7: Feature pool - diversity
The clustering combination method inspires a diﬀerent approach on quantifying
diversity. A k-means clustering algorithm is used to group individual forecasts in three
groups. The number of methods in the top performing cluster is then taken as a feature,
that will identify if there are few or many equally well performing methods, or even
just a single one. Additionally, the distance of the mean of the top performing cluster
to the mean of the second best is added, to put the two performances into relation.
4. Meta-learning
According to , the goal of meta learning is to ”..understand how learning itself can become ﬂexible according to the domain or task under study.” There have
been diﬀerent more detailed interpretations of the term in the literature; in this work,
meta-learning is referred to as the process of linking knowledge on the performance
of so-called base-learners to the characteristics of the problem . One diﬀerence
to general perception of meta-learning is that the base-learners used here are not necessarily machine learning algorithms, but include other approaches as well. Many
meta-learning approaches are available as reviewed in ; this section presents three
diﬀerent experiments: In the ﬁrst step, decision trees using meta-features described
in the previous section have been built as a machine learning method giving readable
results. The second experiment compares a number of approaches and evaluates possible performance improvements. In the last part of this section, performance of one
approach is tested under competition conditions for the NN5 data set.
4.1. Experiment one - decision trees for data exploration
Decision trees were built using the Matlab statistics toolbox, choosing the minimumcost-tree after a ten-fold crossvalidation. Features were determined on the whole time
series, as the nature of this work is exploratory. Using all available methods as class
labels did not yield interpretable results, which is why, concentrating on the best performing approaches in the underlying experiments, the classiﬁcation problem has been
reduced to three diﬀerent questions:
• When does an individual method perform better, when a combination, when does
it not matter? (three class labels)
• When should a structural model, a direct neural network, a pooling combination
or a regression combination be used? (four class labels)
• How to decide between pooling and regression for the combination models? (two
class labels)
Results given in this section do not claim to be universally applicable, they merely
provide an insight on the existence of rules for the speciﬁc data set used. However, if
the meta-features describe the series well and a series with similar characteristics can
be found, it is probable that the guidelines are generalisable, but there is, of course, no
guarantee.
In the ﬁgures given in the following results, the leaf to the left of a node represents
the data that fulﬁls its condition, the leaf to the right hand side represents data that does
not. The numbers following the methods in the leafs denote the number of times this
particular method performed best on the data subset. The ﬁrst classiﬁcation problem
concerned the use of combinations and individual methods; the generated tree can be
seen in Figure 5.
Figure 3: Decision tree 1
The ﬁrst node of the tree divides the series into more and less chaotic series, indicated by the Lyapunov exponent. The majority of the less chaotic series to the left of the
tree are best treated with an individual model, combination models are only suggested
for series where the ﬁrst and second best performing cluster do not have a huge perfor-
mance diﬀerence. Looking at the right part of the tree, three more diversity measures
appear in the nodes together with the kurtosis and the turning point measure. Results
include, amongst others, the suggestion that individual models should not be used for
series with a high kurtosis, and for these a combination method is likely to perform
well if the number of well performing methods is high or correlation coeﬃcients are
similar, otherwise, they tend to perform just as well as an individual model. The whole
tree misclassiﬁes 53 of the 222 data instances, which equals to a rate of 23% as opposed to a misclassiﬁcation rate of 49% when picking the class with the most training
examples (individual method).
The same experiment was run only using features from the diversity feature set,
to see how especially these aﬀect combination performance. Additionally, the class
label ”undecided” was replaced with combinations: As they have the reputation for
being less risky, it seems like a straightforward approach to pick them whenever their
performance is equal or better in comparison to individual methods.
Figure 4: Decision tree 2
In the top node of the tree given in ﬁgure 4, instances with a smaller variability
in correlation coeﬃcients (diversiﬁcation measure four) are sent to the left side of the
tree, where combination methods work best. This is counter-intuitive, as a bigger diversity in individual forecasts should favour combinations, it however illustrates the
fact that diversity has to be traded oﬀwith individual accuracy and is not beneﬁcial
for combinations at all times. In the right part of the tree, most of the instances fall
in rightmost leaf, which suggests individual approaches for individual pools with more
than two methods in the top performing cluster and a big distance between this cluster
and the second best. Diversity measure two, the quantiﬁcation of the actual trade-oﬀ
between individual accuracy and diversity is another node of the subtree, claiming that
if diversity is big in relation to individual accuracy (small value of the measure), an
individual method should be used, and combinations otherwise. The misclassiﬁcation
rate of the tree is 26%, which is worse compared to the previous tree, indicating that the
diversity features do not work optimally on their own. However, it is still considerably
better than the rate of 49% that is obtained when just picking individual methods.
The second classiﬁcation problem deals with the decision between the two best
individual (structural model, neural network) and the two best combination models
(pooling, regression), producing the tree shown in Figure 5.
Figure 5: Decision tree 3
The ﬁrst node sends instances with three or fewer methods per cluster to a leaf suggesting to use a structural model. This shows, that the structural model will most likely
be among the top three methods of the top performing cluster with such a superior
performance that neither the neural network nor the combinations can beat. Structural
models are also recommended for higher autocorrelation values at lag two. Pooling
performs well in the middle subtree, if the series only shows a low seasonality or if
there are many well performing methods. Neural networks are suggested for in a con-
ﬁguration where other predictors have a higher individual accuracy compared to their
diversity. This tree misclassiﬁes 37% of the instances compared to 50% that would be
misclassiﬁed using the most frequently appearing class label.
The third problem investigated is how to decide between using a pooling or a regression combination approach, the corresponding tree can be found in Figure 6. The
features selected here diﬀer from the ones selected in the previous experiment, and the
tree is not easily interpretable. However, it misclassiﬁes only 24% compared to 44%
that would be misclassiﬁed in only using the pooling approach.
Figure 6: Decision tree 4
4.2. Experiment two - comparing meta-learning approaches
In this experiment, a number of machine learning algorithms for meta learning
have been tested adopting the leave-one-out methodology that has, for example, been
applied in . Of the 222 series, only 221 are used as a training set before the remaining one is used to test the resulting meta-model. This process is repeated 222 times,
until every series has been the test series once. To move away from the exploratory
nature of the previous experiments, features for the test series were now only calculated using the training set, which means that the last 18 or 56 observations were held
back from the series. For the diversity features of the test series, only the validation set
forecasts were used.
Classic machine learning approaches have been tested using this methodology,
dealing with the classiﬁcation problem of linking time series features to the class label
of one of the most promising forecasting algorithms including the structural model,
the direct neural network, variance-based pooling and the regression combination as
investigated in the second problem of the last section. Three algorithms have been
implemented:
• A feedforward neural network with one hidden layer and 30 hidden nodes,
• a decision tree obtained by picking the minimum cost tree after a ten-fold cross
validation and
• a support vector machine with a radial basis function as kernel function.
Only selecting one model that is applied to a problem as in the three more traditional meta-learning approaches presented above has the obvious limitation of bearing
a certain risk, even if one of the selected algorithms is a combination of predictors
as in the case of our experiments. A newer approach that facilitates combinations on
a higher level, the meta learning level, is presented in and applied to time series
forecasting in . It allows taking relations of individual performances into account
by providing a ranking of methods for a particular problem. The problem space is divided using clustering on a distance measure that was calculated using the time series
features. Details of this so-called zoomed ranking and our implementation of it follow.
In the ﬁrst step of the zoomed ranking algorithm, distances in the set of time series
are calculated. With the normalised features fx, where x is the meta-attribute number,
the distance of two series is given by the unweighted L1 norm:
dist(si, s j) =
fx,si −fx,sj
maxk,i( fx,sk) −mink,i( fx,sk)
The distances are then clustered using the k-means algorithm, and the series in the
cluster closest to the test series are identiﬁed for further inspection. The ranking is
then generated by a variation of the Adjust Ratio of Ratios (ARR), which is applied
in a classiﬁcation context in the original paper and extended by a penalty for time
intensity in , however, in this experiment, the time dimension was discarded and
the SMAPE measure was used instead of classiﬁer success rates to adapt the ranking
to regression problems. The pairwise ARR for models mp and mq on series si is
ARRsimp,mq =
A high ARR indicates that model p performs better than model q. To aggregate all
rankings over the selected series and the pairwise ranking to one number per method,
the following formula is used:
si ARRsimp,mq
The method with the best ranking then gets selected, or, in an alternative approach,
the rankings are then used to calculate convex weights for the four algorithms considered in this experiment. One of the open questions using this approach is determining
the number of clusters to use for the k-means algorithm. However, trying diﬀerent values for the number of clusters, it becomes clear that the impact on the performance is
small as can be seen in ﬁgure 7, so that it is safe to set the number arbitrarily, within
Figure 7: Average performance in relation to number of clusters
Performance results of all of the approaches presented in this section can be found
in table 4.2. Experiments were run on the whole data set, but results are given separately for the NN3 and the NN5 competition, to allow better comparison with the
performances given in section two. However, it has to be stated that the performances
given in the table cannot be compared to the competition results, as the whole time
series were used in the training set for building the models. The ranking approach
combining four methods outperforms all other meta-learning approaches and also improves upon the best individual predictors. It can thus be seen, that combinations of
models outperformed model selection on a meta-learning level, which underlines the
need for approaches that provide a ranking of models as opposed to just recommending
one of the available approaches.
neural network
decision tree
support vector machine
zoomed ranking, best method
zoomed ranking, combination
Table 8: Performances applying diﬀerent meta-learning techniques
4.3. Experiment 3 - simulating NN5 competition conditions
For the last experiments, time series observations that were not yet available at the
time of the competition were used for training the meta-models. In this experiment,
the zoomed ranking approach presented in the previous section was evaluated on features that were calculated excluding the test data, hence the obtained forecast could
have participated in the competition. This caused problems for the NN3 data set, as the
necessary validation periods for the combination approaches would reduce the observations available for individual model building to only 15 for the shortest of the series,
which is too few for some of the methods to work. This experiment therefore only
considers the NN5 competition.
Applying the zoomed ranking approach, the resulting out-of-sample SMAPE of
23.8 is similar to the performance in the previous experiment, showing that the approach was successful also in competition conditions and improving the twelfth rank
of the best individual method to rank nine of twenty competitors.
5. Conclusions
This work investigated meta-learning for time series prediction with the aim to link
problem-speciﬁc knowledge to well performing forecasting methods and apply them
in similar situations. Initially, an extensive pool of features for describing the nature of
time series was identiﬁed. Along with features that have been used in previous publications, several new ones have been added, for example a measure for nonlinearity and
predictability and characteristics of the frequency spectrum. Furthermore, measures
have not only been calculated for the time series themselves, but also for describing
the behaviour of the pool of available individual forecasting methods. In that way, the
following characteristics could be quantiﬁed: diversity in the method pool, the tradeoﬀbetween individual accuracy and diversity, the size of the group of best performing
individual methods and the distance to the group of second best performing methods.
Decision trees have been built to gain knowledge which of the chosen features are
important for method selection. Some time series characteristics that lead to good results in previous work did not seem to be signiﬁcant for the time series and methods
used here. However, the newly introduced diversity measures gave some interesting insights, quantifying some intuitive perceptions on mechanisms that make a combination
more successful. One of the lessons that has been illustrated quite well is that diversity
alone is not the key to a successful combination of methods, it is individual accuracy
as well. Other easily interpretable results for the given data set include that individual
methods in general work better on less chaotic time series and the pooling approach in
particular works well if there are many well performing methods with a good ratio of
accuracy and diversity in the method pool. It is not claimed that all of these guidelines
are universally applicable to all data sets, however, it was shown that they do exist and
can be successfully exploited for meta-learning experiments as illustrated in the other
empirical experiments.
Furthermore, neural networks, decision trees and support vector machines have
been implemented, building meta-models in a leave-one-out cross-validation methodology, which did not lead to convincing results. A ranking approach to determine
combination weights was however able to clearly improve upon the performance of the
best individual predictors, showing that a combination strategy can outperform a model
selection one even on the meta-level. The last experiment was designed in a way that
the result could have taken part in the NN5 competition and again clearly outperformed
all individual predictors.
An interesting approach for a deeper understanding of the connection between the
nature of a time series and mechanisms that work best for forecasting could be the
clustering of series in a self-organising map as pursued in . Future work will also
be concerned with extending the data set used and identifying and implementing further
ranking algorithms as they have proven to be a very promising strategy in this work.