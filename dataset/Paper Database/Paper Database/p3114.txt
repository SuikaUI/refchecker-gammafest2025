Proceedings of The 11th International Natural Language Generation Conference, pages 322–328,
Tilburg, The Netherlands, November 5-8, 2018. c⃝2018 Association for Computational Linguistics
Findings of the E2E NLG Challenge
Ondˇrej Duˇsek, Jekaterina Novikova and Verena Rieser
The Interaction Lab, School of Mathematical and Computer Sciences
Heriot-Watt University
Edinburgh, Scotland, UK
{o.dusek, j.novikova, v.t.rieser}@hw.ac.uk
This paper summarises the experimental
setup and results of the ﬁrst shared task
on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems. Recent end-to-end generation systems are promising since they reduce the
need for data annotation. However, they
are currently limited to small, delexicalised datasets.
The E2E NLG shared
task aims to assess whether these novel approaches can generate better-quality output by learning from a dataset containing higher lexical richness, syntactic complexity and diverse discourse phenomena. We compare 62 systems submitted
by 17 institutions, covering a wide range
of approaches, including machine learning architectures – with the majority implementing sequence-to-sequence models
(seq2seq) – as well as systems based on
grammatical rules and templates.
Introduction
This paper summarises the ﬁrst shared task
on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems (SDSs).
Shared tasks have become an established way of
pushing research boundaries in the ﬁeld of natural language processing, with NLG benchmarking tasks running since 2007 (Belz and Gatt,
This task is novel in that it poses new
challenges for recent end-to-end, data-driven NLG
systems for SDSs which jointly learn sentence
planning and surface realisation and do not require costly semantic alignment between meaning representations (MRs) and the corresponding
natural language reference texts, e.g. , SF Hotels/Restaurants , or RoboCup , whereas the E2E shared task is
based on a new crowdsourced dataset of 50k instances in the restaurant domain, which is about
10 times larger and also more complex than previous datasets. For the shared challenge, we received 62 system submissions by 17 institutions
from 11 countries, with about 1/3 of these submissions coming from industry.
We assess the
submitted systems by comparing them to a challenging baseline using automatic as well as human
evaluation.
We consider this level of participation an unexpected success, which underlines the
timeliness of this task.2 While there are previous
studies comparing a limited number of end-to-end
NLG approaches , this is the
ﬁrst research to evaluate novel end-to-end generation at scale and using human assessment.
The E2E NLG dataset
Data Collection Procedure
In order to maximise the chances for data-driven
end-to-end systems to produce high quality output, we aim to provide training data in high quality
and large quantity. To collect data in large enough
quantity, we use crowdsourcing with automatic
1Note that as opposed to the “classical” deﬁnition of NLG
 , generation
for dialogue systems does not involve content selection and
its sentence planning stage may be less complex.
2In comparison, the well established Conference in Machine Translation WMT’17 received
submissions from 31 institutions to a total of 8 tasks .
name[The Wrestlers],
priceRange[cheap],
customerRating[low]
The wrestlers offers competitive prices,
but isn’t highly rated by
customers.
Figure 1: Example of an MR-reference pair.
Example value
verbatim string The Eagle, ...
dictionary
restaurant, pub, ...
familyFriendly
priceRange
dictionary
cheap, expensive, ...
dictionary
French, Italian, ...
verbatim string Zizzi, Cafe Adriatic, ...
dictionary
riverside, city center, ...
customerRating dictionary
1 of 5 (low), 4 of 5 (high), ...
Table 1: Domain ontology of the E2E dataset.
quality checks. We use MRs consisting of an unordered set of attributes and their values and collect multiple corresponding natural language texts
(references) – utterances consisting of one or several sentences. An example MR-reference pair is
shown in Figure 1, Table 1 lists all the attributes in
our domain.
In contrast to previous work , we use different modalities of meaning representation for data collection: textual/logical and
pictorial MRs. The textual/logical MRs (see Figure 1) take the form of a sequence with attributevalue pairs provided in a random order. The pictorial MRs (see Figure 2) are semi-automatically
generated pictures with a combination of icons
corresponding to the appropriate attributes. The
icons are located on a background showing a map
of a city, thus allowing to represent the meaning of
attributes area and near (cf. Table 1).
In a pre-study , we
showed that pictorial MRs provide similar collection speed and utterance length, but are less
likely to prime the crowd workers in their lexical choices. Utterances produced using pictorial
MRs were considered to be more informative, natural and better phrased. However, while pictorial
MRs provide more variety in the utterances, this
also introduces noise. Therefore, we decided to
use pictorial MRs to collect 20% of the dataset.
Our crowd workers were asked to verbalise all
information from the MR; however, they were not
Figure 2: An example pictorial MR.
E2E data part
References
training set
development set
full dataset
Table 2: Total number of MRs and human references in the E2E data sections.
penalised for skipping an attribute. This makes the
dataset more challenging, as NLG systems need to
account for noise in training data. On the other
hand, the systems are helped by having multiple
human references per MR at their disposal.
Data Statistics
The resulting dataset contains over 50k references for 6k distinct MRs (cf.
Table 2), which is 10 times bigger than previous sets in comparable domains (BAGEL, SF Hotels/Restaurants, RoboCup). The dataset contains
more human references per MR (8.27 on average),
which should make it more suitable for data-driven
approaches. However, it is also more challenging
as it uses a larger number of sentences in references (up to 6 compared to 1–2 in other sets) and
more attributes in MRs.
For the E2E challenge, we split the data into
training, development and test sets (in a roughly
82-9-9 ratio). MRs in the test set are all previously
unseen, i.e. none of them overlaps with training/development sets, even if restaurant names are
removed. MRs for the test set were only released
to participants two weeks before the challenge
submission deadline on October 31, 2017. Participants had no access to test reference texts. The
whole dataset is now freely available at the E2E
NLG Challenge website at:
 
InteractionLab/E2E/
METEOR ROUGE-L CIDEr
norm. avg.
♥TGEN baseline : seq2seq with MR classiﬁer reranking
♥SLUG : seq2seq-based ensemble (LSTM/CNN encoders, LSTM
decoder), heuristic slot aligner reranking, data augmentation
♥TNT1 : TGEN with data augmentation
♥NLE : fully lexicalised character-based seq2seq with MR
classiﬁcation reranking
♥TNT2 : TGEN with data augmentation
♥HARV : fully lexicalised seq2seq with copy mechanism,
coverage penalty reranking, diverse ensembling
♥ZHANG : fully lexicalised seq2seq over subword units, attention
♥GONG : TGEN ﬁne-tuned using reinforcement learning
♥TR1 : seq2seq with stronger delexicalization (incl. priceRange
and customerRating)
♦SHEFF1 : 2-level linear classiﬁers deciding on next slot/token,
trained using LOLS, training data ﬁltering
♣DANGNT : rule-based two-step approach, selecting phrases
for each slot + lexicalising
♥SLUG-ALT : SLUG trained only using complex
sentences from the training data
♦ZHAW2 : semantically conditioned LSTM RNN language
model + controlling the ﬁrst generated word
♠TUDA : handcrafted templates
♦ZHAW1 : ZHAW2 with MR classiﬁcation loss + reranking
♥ADAPT : seq2seq with preprocessing that enriches the MR with
desired target words
♥CHEN : fully lexicalised seq2seq with copy mechanism and attention
♠FORGE3 : templates mined from training data
♥SHEFF2 : vanilla seq2seq
♠TR2 : templates mined from training data
♣FORGE1 : grammar-based
Table 3: A list of primary systems in the E2E NLG challenge, with word-overlap metric scores.
System architectures are coded with colours and symbols: ♥seq2seq, ♦other data-driven, ♣rule-based, ♠template-based. Unless noted otherwise, all data-driven systems use partial delexicalisation (with name and near attributes replaced by placeholders
during generation), template- and rule-based systems delexicalise all attributes. In addition to word-overlap metrics (see Section 4.1), we show the average of all metrics’ values normalised into the 0-1 range, and use this to sort the list. Any values
higher than the baseline are marked in bold.
Systems in the Competition
The interest in the E2E Challenge has by far exceeded our expectations. We received a total of
62 submitted systems by 17 institutions (about 1/3
from industry). In accordance with ethical considerations for NLP shared tasks , we allowed researchers to withdraw
or anonymise their results if their system performs
in the lower 50% of submissions.
Two groups
from industry withdrew their submissions and one
group asked to be anonymised after obtaining automatic evaluation results.
We asked each of the remaining teams to identify 1-2 primary systems, which resulted in 20 systems by 14 groups. Each primary system is described in a short technical paper (available on
the challenge website) and was evaluated both by
automatic metrics and human judges (see Section 4).
We compare the primary systems to a
baseline based on the TGEN generator . An overview of all primary systems is given in Table 3, including the main features of their architectures. A more detailed description and comparison of systems will be given
in .
Evaluation Results
Word-overlap Metrics
Following previous shared tasks in related ﬁelds
 , we selected a range of metrics measuring word-overlap
between system output and references, including
BLEU, NIST, METEOR, ROUGE-L, and CIDEr.
Table 3 summarises the primary system scores.
The TGEN baseline is very strong in terms of
word-overlap metrics: No primary system is able
♥SLUG-ALT (late)
Naturalness
♥SLUG-ALT (late)
Table 4: TrueSkill measurements of quality (left) and naturalness (right).
Signiﬁcance cluster number, TrueSkill value, range of ranks where the system falls in 95% of cases or more, system name.
Signiﬁcance clusters are separated by a dotted line. Systems are colour-coded by architecture as in Table 3.
to beat it in terms of all metrics – only SLUG
comes very close.
Several other systems beat
TGEN in one of the metrics but not in others.3
Overall, seq2seq-based systems show the best
word-based metric values, followed by SHEFF1,
a data-driven system based on imitation learning.
Template-based and rule-based systems mostly
score at the bottom of the list.
Results of Human Evaluation
However, the human evaluation study provides a
different picture. Rank-based Magnitude Estimation (RankME) was used
for evaluation, where crowd workers compared
outputs of 5 systems for the same MR and assigned scores on a continuous scale. We evaluated
output naturalness and overall quality in separate
tasks; for naturalness evaluation, the source MR
was not shown to workers. We collected 4,239 5way rankings for naturalness and 2,979 for quality,
comparing 9.5 systems per MR on average.
The ﬁnal evaluation results were produced using the TrueSkill algorithm , with partial ordering into
signiﬁcance clusters computed using bootstrap resampling . For both criteria, this resulted in 5
3Note, however, that several secondary system submissions perform better than the primary ones (and the baseline)
with respect to word-overlap metrics.
clusters of systems with signiﬁcantly different performance and showed a clear winner: SHEFF2 for
naturalness and SLUG for quality. The 2nd clusters are quite large for both criteria – they contain
13 and 11 systems, respectively, and both include
the baseline TGEN system.
The results indicate that seq2seq systems dominate in terms of naturalness of their outputs, while
most systems of other architectures score lower.
The bottom cluster is ﬁlled with template-based
The results for quality are, however,
more mixed in terms of architectures, with none
of them clearly prevailing. Here, seq2seq systems
with reranking based on checking output correctness score high while seq2seq systems with no
such mechanism occupy the bottom two clusters.
Conclusion
This paper presents the ﬁrst shared task on end-toend NLG. The aim of this challenge was to assess
the capabilities of recent end-to-end, fully datadriven NLG systems, which can be trained from
pairs of input MRs and texts, without the need for
ﬁne-grained semantic alignments. We created a
novel dataset for the challenge, which is an orderof-magnitude bigger than any previous publicly
available dataset for task-oriented NLG. We received 62 system submissions by 17 participating
institutions, with a wide range of architectures,
from seq2seq-based models to simple templates.
We evaluated all the entries in terms of ﬁve different automatic metrics; 20 primary submissions (as
identiﬁed by the 14 remaining participants) underwent crowdsourced human evaluation of naturalness and overall quality of their outputs.
We consider the SLUG system , a seq2seq-based ensemble system with a
reranker, as the overall winner of the E2E NLG
challenge.
SLUG scores best in human evaluations of quality, it is placed in the 2nd-best cluster of systems in terms of naturalness and reaches
high automatic scores.
While the SHEFF2 system , a vanilla seq2seq setup,
won in terms of naturalness, it scores poorly on
overall quality – it placed in the last cluster. The
TGEN baseline system turned out hard to beat: It
ranked highest on average in word-overlap-based
automatic metrics and placed in the 2nd cluster in
both quality and naturalness.
The results in general show the seq2seq architecture as very capable, but requiring reranking
to reach high-quality results. On the other hand,
while rule-based approaches are not able to beat
data-driven systems in terms of automatic metrics,
they often perform comparably or better in human
evaluations.
We are preparing a detailed analysis of the results and a release of all system outputs with user ratings on the challenge
website.4 We plan to use this data for experiments
in automatic NLG output quality estimation , where the
large amount of data obtained in this challenge allows a wider range of experiments than previously
Acknowledgements
This research received funding from the EPSRC
projects DILiGENt (EP/M005429/1) and MaDrIgAL (EP/N017536/1). The Titan Xp used for this
research was donated by the NVIDIA Corporation.