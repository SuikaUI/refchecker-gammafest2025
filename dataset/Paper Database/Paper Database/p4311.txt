Explaining Data-Driven Document Classiﬁcations*
David Martens
Faculty of Applied Economics, University of Antwerp, Belgium
Foster Provost
Department of Information, Operations and Management Sciences, Stern School of Business, New York University, NY
Many document classiﬁcation applications require human understanding of the reasons for data-driven classiﬁcation decisions: by managers, client-facing employees, and the technical team. Predictive models treat
documents as data to be classiﬁed, and document data are characterized by very high dimensionality, often
with tens of thousands to millions of variables (words). Unfortunately, due to the high dimensionality, understanding the decisions made by document classiﬁers is very difﬁcult. This paper begins by extending the most
relevant prior theoretical model of explanations for intelligent systems to account for some missing elements.
The main theoretical contribution of the work is the deﬁnition of a new sort of explanation as a minimal set
of words (terms, more generally), such that removing all words within this set from the document changes
the predicted class from the class of interest. We present an algorithm to ﬁnd such explanations, as well as a
framework to assess such an algorithm’s performance. We demonstrate the value of the new approach with a
case study from a real-world document classiﬁcation task: classifying web pages as containing objectionable
content, with the goal of allowing advertisers to choose not to have their ads appear there. A second empirical demonstration on news-story topic classiﬁcation uses the 20 Newsgroups benchmark dataset. The results
show the explanations to be concise and document-speciﬁc, and to be capable of providing better understanding of the exact reasons for the classiﬁcation decisions, of the workings of the classiﬁcation models, and of the
business application itself. We also illustrate how explaining documents’ classiﬁcations can help to improve
data quality and model performance.
Key words: Document Classiﬁcation, Instance Level Explanation, Text mining, Comprehensibility
Introduction
Document classiﬁcation systems classify text documents automatically, based on the words,
phrases, and word combinations therein (hereafter, “words”). Business applications of document
* Please cite as David Martens and Foster Provost. Explaining Data-Driven Document Classiﬁcations, MIS Quarterly
To Appear.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
classiﬁcation are becoming increasingly widespread, especially with the introduction of low-cost
micro-outsourcing systems for annotating training corpora. Prevalent applications include sentiment analysis , spam identiﬁcation , web page classiﬁcation , legal document classiﬁcation , medical document
triage , and document classiﬁcation for topical web search , just to name a few. Classiﬁcation models are built from labeled data sets that encode
the frequencies of the words in the documents. Importantly for this paper, and different from many
data mining applications, the document classiﬁcation data representation has very high dimensionality, with the number of words and phrases typically ranging from tens of thousands to millions.
The main contribution of this paper is to examine in detail an important aspect of the business
application of document classiﬁcation that has received little attention in the research literature.
Speciﬁcally, organizations often need to understand the exact reasons why classiﬁcation models
make particular decisions. The need comes from various perspectives, including those of managers,
customer-facing employees, and the technical team. To understand these needs more deeply, in the
next section we extend an existing theoretical model from the Information Systems (IS) literature
to include these various perspectives.
As a concrete illustration, consider an application currently receiving substantial interest in online advertising: keeping ads off of objectionable web content . Having
invested substantially in their brands, ﬁrms cite the potential to appear adjacent to nasty content as
the primary reason they do not spend more on on-line advertising. To help reduce the risk, document classiﬁers are applied to web pages along various dimensions of objectionability, including
adult content, hate speech, violence, drugs, bomb-making, and many others. However, because the
on-line advertising ecosystem supports the economic interests of both advertisers and content publishers, black-box models are insufﬁcient. Managers cannot put models into production that might
block advertising from substantial numbers of non-objectionable pages, without understanding the
risks of incorporating them into the product offering. Customer-facing employees need to explain
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
why particular pages were deemed objectionable by the models. And the technical team needs
to understand the exact reasons for the classiﬁcations made, so that they can address errors and
continuously improve the models.
Popular techniques to build document classiﬁcation models include naive Bayes, linear and nonlinear support vector machines (SVMs), classiﬁcation-tree based methods ), K-nearest neighbor and
many others . Because of the massive dimensionality, even for linear and treebased models, it is very difﬁcult to understand exactly how a given model classiﬁes documents. It
is essentially impossible for a non-linear SVM or an ensemble of trees. Understanding the classi-
ﬁcations requires concise explanations, which here we deﬁne as explanations that refer to only a
very small fraction of the total vocabulary, in contrast to existing explanation approaches which in
most cases include large fractions of the vocabulary.
Understanding particular classiﬁcations also provides other important beneﬁts. Not only can we
get improved understanding of the classiﬁcation model, the explanations also can provide a novel
lens into the complexity of the business domain. For example, in Explanation 1 (shown below;
described fully in Section 3.3), the word ‘welcome’ as an indication of adult content initially seems
strange. Upon investigation/reﬂection we understand that in some cases an adult website’s ﬁrst
page contains a phrase similar to ‘Welcome to ... By continuing you conﬁrm you are an adult and
agree with our policy’. The explanation brings this complexity to light.
We introduce this problem, tying it in to the existing literature on explanations for decision
systems and extending the relevant theory to account for modern, data-driven modeling. In line
with this theory, we then introduce the ﬁrst (to our knowledge) technique that directly addresses the
explanation of the decisions made by document classiﬁers. The technique focuses on explaining
why a document is classiﬁed as a speciﬁc class of interest (e.g., “objectionable content” or “hate
speech”). Finally, we present a case study based on data from a real application to the business
problem of safe advertising discussed above, and an empirical follow-up study on benchmark data
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
sets (from news classiﬁcation). These studies demonstrate that the methods can be effective, and
also ﬂush out additional important issues in explaining document classiﬁcations, such as the need
for hyper-explanations.
Explanation 1: An example explanation why a web page is classiﬁed as having adult content.
If words (welcome ﬁction erotic enter bdsm adult) are removed then class changes from adult to non-adult.
Explanations and Statistical Classiﬁcation Models
Explaining the decisions made by intelligent decision systems has received both practical and
research attention for decades, and a complete review is well beyond the scope of this paper.
Nonetheless, there are important results from prior work that help to frame, motivate, and explain
the speciﬁc gap in the current state of the art that this paper addresses.
Model-based decision systems and instance-speciﬁc explanations
Starting as early as the celebrated MYCIN project in the 1970s studying intelligent systems for
infectious disease diagnosis , the ability for intelligent systems
to explain their decisions was understood to be necessary for effective use of such systems and
therefore was studied explicitly. The document classiﬁcation systems that are the subject of this
paper are an instance of decision systems (DSs): systems that either (i) support and improve human
decision making ), or (ii)
make decisions automatically. The focal application of this paper’s case study falls in the second
category: billions of attempts to place advertisements are made each day, and each decision is
made in a couple dozen milliseconds. Model-based decision systems have seen a steep increase in
development and use over the past two decades . We focus on models
produced by large-scale automated statistical predictive modeling systems , for which generating explanations can be particularly problematic.
Different applications impose different requirements for understanding. Consider three different application scenarios, both to add clarity in what follows, and so that we can rule out one of
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
them. First, in some applications it is important to understand every decision that the DS may possibly make. For example, for many applications of credit scoring regulatory
requirements stipulate that every decision be justiﬁable, and often this is required in advance of
the ofﬁcial “acceptance” and implementation of the system. Similarly, one could easily see that a
medical decision system may need to be completely transparent in this respect. The present paper,
about individual case-speciﬁc explanations, is not intended to apply to systems such as these.1
In contrast, consider applications where one needs to explain the speciﬁc reasons for some subset of the individual decisions , discussed below). Our case study falls into this category. Often, this need
for individual case explanations arises because particular decisions need to be justiﬁed after the
fact, because (for example) a customer questions the decision or a developer is examining model
performance on historical cases. Furthermore, to reveal problems with the classiﬁcation of documents it may be more efﬁcient for an analyst to study concise explanations than the documents
themselves. Alternatively, a developer may be exploring decision-making performance by giving
the system a set of theoretical test cases. In both scenarios, it is necessary for the system to provide
explanations for speciﬁc individual cases.2 Other examples in the second scenario include fraud
detection , many cases of targeted marketing, and all of the document
classiﬁcation applications listed in the ﬁrst paragraph of this paper.
In a third application scenario, every decision that the system actually makes must be understood. This often is the case with a classical decision-support system, where the system is aiding a
human decision maker, for example for forecasting or auditing . For such systems, again, it is necessary to have individual case-speciﬁc explanations.
1 The current prevailing interpretation of this requirement for complete transparency argues for a globally comprehensible predictive model. Indeed, in credit scoring generally the only models that are accepted are linear models with a
small number of well-understood, intuitive variables. Such models are chosen even when non-linear alternatives are
shown to give better predictive performance .
2 Individual case-speciﬁc explanations may also be sufﬁcient in many applications. For this paper it is only important
that they be necessary.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Cognitive perspectives on model explanations
Gregor and Benbasat provide a survey of empirical work on explanations from intelligent
systems. They ﬁnd that explanations are important to users when there is some speciﬁc reason and
anticipated beneﬁt, when an anomaly is perceived, or when there is an aim of learning.
Their theoretical analysis brings to the fore three ideas that are critical for our context. First,
they introduce the reasons for explanations: to resolve perceived anomalies, a need to better grasp
the inner workings of the intelligent system, or the desire for long-term learning. Second, they
describe the type of explanations that should be provided: they emphasize the need not just for
general explanations of the model, but for explanations that are context-speciﬁc. Third, Gregor and
Benbasat emphasize the need for “justiﬁcation”-type explanations, which provide a justiﬁcation
for moving from the grounds to the claims, in contrast to rule-trace explanations. In statistical
predictive modeling, the “rule trace” often entails simply the application of a mathematical function
to the case data, with the result being a score representing the likelihood of the case belonging
to the class of interest, with no justiﬁcation of why. There is little existing work on methods for
explaining modern statistical models extracted from data that satisfy these latter two criteria, and
none (to our knowledge) that provide such explanations for the very high-dimensional models that
are the focus of this paper.
An important subtlety that is not brought out explicitly by Gregor and Benbasat, but which
is quite important in our contemporary context is the difference between (i) an explanation as
intended to help the user to understand how the world works, and thereby help with acceptance
of the system, and (ii) an explanation of how the model works. The latter case can be further
subdivided into (a) how the model works in general, and (b) how the model works on a particular
instance. The explanation thereby either can help with acceptance, or can focus attention on the
need for improving the model. When the model reﬂects reality well, then this also will support (i).
Kayande et al.’s 3-gap framework
In order to examine more carefully why explanations are needed and their impact on decision
model understanding, long-term learning, and improved decision making, we turn to the recent
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
work by Kayande et al. . This work focuses on the same context as we do in our case study,
speciﬁcally where data are voluminous, the link between decisions and outcomes is probabilistic,
and the decisions are repetitive. They presume that it is highly unlikely that decision makers can
consistently outperform model-based DSs in such contexts.
Prior work has suggested that when users do not understand the workings of the DS model,
they will be skeptical and reluctant to use the model, even if the model is known to improve decision performance, see, e.g., Umanath and Vessey , Limayem and De Sanctis , Lilien
et al. , Arnold et al. , Kayande et al. . Further, decision makers need impetus to change their decision strategies , as well as guidance in making
decisions . Kayande et al. introduce a “3-gap” framework (Figure 1) for understanding the use of explanations to improve decision making by aligning three different “models”: the
user’s model, the system’s model, and reality. Their results show that guidance toward improved
understanding of decisions combined with feedback on the potential improvement achievable by
the model induce decision makers to align their mental models more closely with the decision
model, leading to deep learning. This alignment reduces the corresponding gap (Gap 1), which in
turn improves user evaluations of the DS. It is intuitive to argue that this then improves acceptance
and increases use of the system. Under the authors’ assumption that the DS’s model is objectively
better than the decision maker’s (large Gap 3 compared to Gap 2), this then would lead to improved
decision-making performance, cf., Todd and Benbasat . Expectancy theory suggests that
this will lead to higher usage and acceptance of the DS model, as users will be more motivated to
actually use the DS if they believe that a greater usage will lead to better performance (De Sanctis
An extended gap framework
The framework of Kayande et al. is incomplete in two important ways, which we now will address
in turn. First, Kayande et al. do not address the use of explanations (or other feedback) to improve
the DS model. Technically this incompleteness is not an incompleteness in their 3-gap framework,
Martens and Provost: Explaining Data-Driven Document Classiﬁcations



3-Gap framework by Kayande et al. .
because improving the model ﬁts as closing Gap 2. Indeed, the authors note speciﬁcally that “to
provide high-quality decision support, the gap between the DSS model and the true model must
be small (Gap 2).” However, in the paper, Kayande et al. focus their attention on closing Gap 1
between the user’s mental model and the DS model. They justify this with the explicit assumption
“that the DSS model is of high objective quality (small Gap 2) and that it is of better quality than
the user’s mental model (large Gap 3).” Even when the model’s performance generally is much
better than the user’s, in many applications there still are plenty of cases where the user is correct
when the model is wrong. True mistakes of the model, when noticed by a user, can jeopardize user
trust and acceptance.
More generally, we need research that focuses on a user-centric theoretical understanding of the
production of explanations with a primary goal of improving data-driven models based on feedback
and iterative development. This is important because as model-based systems increasingly are built
by mining models from large data, users may have much less conﬁdence in the model’s reasoning
than with hand-crafted knowledge-based systems. There are likely to be many cases where the
decisions are erroneous due either to biases in the process, or to overﬁtting the training data . As pointed out by Gregor and Benbasat , a user will want an explanation when
she perceives an anomaly. The resultant explanation may help the user to learn about how the world
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
(a) Proposed
highlighting
between the users’ models and reality. Understanding
document classiﬁcations can close these gaps, helping
users to understand the world better, thereby improving
acceptance of the system.
  
(b) Proposed
highlighting
between the users’ models and the DS’s model. These
gaps can be closed in either direction: improving users’
understanding of how the DS model works, or helping to
improve the DS model. Improving the DS model, in turn,
helps close the vertical gap between the DS model and
7-gap extension to Kayande et al.’s 3-gap framework, showing that (i) explanations can
close more than just the gap between the user’s mental model and the DS model, and (ii) the
extension of a single user to three relevant user roles: client, manager and developer.
works , and thereby improve acceptance. However, it alternatively may lead
to the identiﬁcation of a ﬂaw in the model, and lead to a development effort focused on improving
the model. At a higher level, this ability for the users and the developers to collaborate on ﬁxing
problems with the system’s decision-making may also improve user acceptance, because the user
sees herself as an active, integral part of the system development, rather than a passive recipient
of explanations as to why she is wrong about the world. Therefore, our ﬁrst extension to the 3-gap
framework is that explanations can be used to improve the model—closing Gap 2 (and Gap 1) in
the other direction—as well as to improve user understanding.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
This leads us to the second important incompleteness in the framework of Kayande et al. The
3-gap framework considers a single, monolithic “user” of the decision system. We contend that to
better understand the uses of explanations in the context of practices within contemporary organizations, we need to differentiate between different roles of people who interact with the decision
system.3 In order to understand how explanations are or should be used, there are at least three
different roles that are important to distinguish: developers, managers, and customers.
Figures 2a and 2b present a 7-gap extension to Kayande et al.’s framework. The extended framework makes three novel contributions. First, it clariﬁes the bidirectional nature of the gap closing
that can be achieved via explanations: explanations can lead to changes in user mental models;
they also can lead to changes in the DS model. Second, the extended framework divides out three
different user roles. Each different role has different needs and uses for explanations, as will be
illustrated in the context of our case study. Third, the extended framework distinguishes between
two quite different sorts of user understanding, which both are important: understanding reality
better, and understanding the DS model better.
More speciﬁcally, Figure 2a illustrates how the extended model breaks apart the closing of the
gap between the different user roles and reality. In each case, explanations can give the user better
understanding of the domain. However, although customers, managers, and developers all need to
accept the DS model, “acceptance” means different things for each. In our case study application
of web page classiﬁcation for safe advertising, explanations of why ads are blocked on certain
pages can increase a customer’s understanding of the sorts of pages on which her ads are being
shown (a difﬁcult task in modern online display advertising). If these include hate speech pages
on user-generated content sites, this may substantially increase the user’s acceptance of the need
in the ﬁrst place for the DS. Managers seeing explanations of blocked pages can better understand
the landscape of objectionable content, in order to better market the service. Developers can better
3 We discuss different roles rather than different sorts of people, because in some contexts the same person may play
more than one of the roles.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
understand the need for focused data collection, in order to ensure adequate training data for the
classiﬁcation problems faced . In sum, assuming (as do Kayande et al.) that the DS model is relatively close to reality, a better understanding
of the domain should improve: acceptance by customers and managers, marketing and sales by
managers, and efﬁciency and efﬁcacy of developers.
Figure 2b highlights the gaps between the users’ mental models and the DS model. The arrows
moving from the mental models toward the DS model break apart different sorts of understanding
that underlie the gap closing that explanations may provide, inherent in the treatment by Kayande
et al. In the case of data-driven statistical models, all the different user roles may need to achieve
some level of understanding of the decision system, in order to improve acceptance (in line with
prior research). At the top of the ﬁgure, clients/customers may need to have the speciﬁc decisions of
the system justiﬁed. As represented by the middle gap, managers need to understand the workings
of the DS model: customer-relationship managers need to deal with customer queries regarding
how decisions are made. Even in applications for which black-box systems are deployed routinely,
such as fraud detection , managers still need to have conﬁdence in the
operation of the system (middle gap) and may need to explain to customers reasons for particular
classiﬁcations when errors are made. Operations managers need to “sign off” on models being
placed into production, and prefer to understand how the model makes its decisions, rather than
just to trust the technical/data science team. Development managers need to understand speciﬁc
decisions when they are called into question by customers or business-side employees. Finally,
(bottom gap) the data science developers themselves need to understand the reasons for decisions
in order to be able to debug/improve the models (discussed next). Holistic views of a model and
aggregate statistics across a “test set” may not give sufﬁcient guidance as to what exactly is wrong
and how the model can and should be improved.
The dashed arrows (emanating from the DS model) represent gap-closing in the other direction,
by improving the DS model. The explanation methods introduced in this paper can have a substantial impact on improving document classiﬁcation models from the users’ perspectives. Despite
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
the stated goals of early research on data mining and knowledge discovery ,
very little work has addressed support for the process of building acceptable models, especially
in business situations where various parties must be satisﬁed with the results. Recently, there is
increasing research focus on using advanced statistical models that mimic a certain behavior in
the real world, without understanding the meaning of that behavior . The design we
introduce provides support for such understanding. The DS model can move closer to the mental
models of people playing each of the different user roles, to the extent that they were correct on the
speciﬁc ﬂaws that were improved upon. Presumably these gap closings also would improve acceptance. Possibly equally important for acceptance would be the increase in the users’ perception
that the model can be improved when necessary.
Note that, when improved, the model is likely also to move closer to reality (the vertical, dashed
arrow). We say “is likely to” because since there is a gap between each user’s mental model and
reality, it may be that moving the model closer to the mental model of some user actually moves
it further away from reality. We will not examine that possibility in this paper.4 The extended
gap model also highlights the existence of the vertical gaps between user roles. Closing these
gaps also is important to DS development , Barki and
Hartwick ). For example, to avoid conﬂicts managers and developers should have similar
mental models. Producing good explanations may address these gaps indirectly, as closing the gaps
between the user roles and reality and between the user roles and the DS model may act naturally
to close these vertical gaps between user mental models. We do not address these vertical gaps
directly in this paper.
4 We have omitted the possibility that reality can move closer to the DS model in our treatment. However, this is not
necessarily out of the question. The “true” classiﬁcations of documents are subjective in certain domains, and it may
be that a broadly used classiﬁcation system changes the accepted subjective class deﬁnitions. Further, in dynamic
domains the production of documents may co-evolve with system development and usage. Authors may write documents differently based on their knowledge of the algorithms used to ﬁnd or process them. Such issues are beyond the
scope of this paper.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Explaining Documents’ Classiﬁcations
Prior research has examined two different sorts of “explanation” procedures for understanding
predictive models: global explanation and instance-level explanation . Global explanations provide improved understanding of
the complete model, and its performance over the entire space of possible instances. Instance-level
explanations provide explanations for the model’s classiﬁcation of an individual instance.
In the previous section we presented reasons for preferring instance-level explanations over
global explanations, drawing on prior IS research. We now will present additional reasons why
existing methods are not ideal (or not suitable) for explaining classiﬁcations of documents in particular, and then we will present a new approach that addresses the drawbacks.
Key Aspects of Document Classiﬁcation
We focus on textual document classiﬁcation, where a score is produced representing the predicted
likelihood (or strength of belief) of the document belonging to some discrete class or category,
based on the values of a large number of independent variables representing the words.5 There
are several ways in which document classiﬁcation differs from traditional data mining for common applications such as credit scoring, medical diagnosis, fraud detection, churn prediction and
response modeling. First, the data instances have less structure. Technically, one can engineer a
feature-vector representation from the sequence or bag of words, but this leads us to our second
main difference. In a feature-vector representation of a document data set, the number of variables
is often orders of magnitude larger than in the “standard” classiﬁcation problems presented above.
Thirdly, the values of the variables in a text mining data set denote the presence, frequency of
occurrence, or some positively weighted frequency of occurrence of the corresponding word (see
5 Technically, text document classiﬁcation applications generally use “terms” that include not only individual words,
but phrases, metadata terms, n-grams, etc. For this paper, we call all these “words.” Cases where the terms are not
comprehensible to a human present a limitation of our approach.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
These three aspects of document classiﬁcation all are critical for the explanation of classiﬁer
decisions. The ﬁrst two combine to render existing explanation approaches relatively useless (as
we discuss in detail next). The third, however, presents the basis for the design of the solution
we propose. Speciﬁcally, with all such document classiﬁcation representations, removing words
always corresponds to reducing the value of the corresponding variable or setting it to zero.
A few technical details of document classiﬁcation are important here. All non-textual symbols,
such as punctuation, are removed from each document, unless they are speciﬁcally included for
their semantic relationship to the classiﬁcation task. For a set of n documents and a vocabulary
of m words, an n × m dataset is created with the value tfij on row i and column j denoting the
frequency of word j in document i (“term frequency”). As such, each document is described by a
sparse numerical row vector. As most of the words available in the vocabulary will not be present
in any given document, most values will be zero, and a sparse representation typically is used.
Often a weighting scheme is applied to the frequencies, where the weights reﬂect the importance of
the word for the speciﬁc application . A commonly used data-driven weighting
scheme is tﬁdf: xij = tfij × idfj where the weight of a word is the “inverse document frequency,”
which describes how uncommon the word is: id f(wj) = log(n/nj) with nj the number of documents that contain word wj.
Classiﬁcation models are built using a training set of “labeled” documents, meaning we know
the value of the “target” variable being predicted/estimated. The resultant classiﬁcation model, or
classiﬁer, maps any document to one of the predeﬁned classes, and more speciﬁcally generally
maps it to a score representing the likelihood of belonging to the class; this score is compared
to a threshold for classiﬁcation. Based on an independent test set, the performance of the model
can be assessed by comparing the true labels with the predicted labels. Note that Latent Semantic
Analysis (LSA) is sometimes used for indexing and information retrieval
 ). Its clustering over the identiﬁed concepts can provide improved
understanding, but is different from making or explaining prediction models based on labeled data.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Global explanations
The most common approach to understanding a predictive model is to examine the coefﬁcients of
a linear model. Unfortunately such an approach is impracticable for a model with 104 to 106 variables. For such applications, the most common approach for a linear model is to list the variables
(words in our case) with the highest weights. To understand more complex models such as neural
networks and non-linear support-vector machines (SVMs) , the principal approach is rule extraction: rules or trees are extracted that mimic the black box as closely as
possible . The motivation for using rule extraction
is to combine the desirable predictive behavior of non-linear techniques with the comprehensibility of decision trees and rules. Previous benchmarking studies have revealed that when it comes
to predictive accuracy, non-linear methods often outperform traditional statistical methods such
as multiple regression, logistic regression, naive Bayesian and linear discriminant analysis , Lessmann et al. ). For some applications however, e.g., medical
diagnosis and credit scoring, a clear explanation of how the decision is reached by models is a
crucial business requirement and sometimes a regulatory requirement.
These rule extraction approaches are not suitable for our present problem for several reasons.
Not all classiﬁcations are explained by these rule extraction approaches (as we will demonstrate
for the most common approach). For some instances that seem to be explained by the rules, more
reﬁned (and therefore more accurate) explanations exist. In addition, often one is only interested
in the explanation of the classiﬁcation of a single data instance. For example, because it has been
brought to a manager’s attention because it has been misclassiﬁed or simply because additional
information is required for this case (to address a perceived anomaly, or for other learning).
In addition, global explanations do not provide much insight for document classiﬁcation anyway, because of the massive dimensionality. For a classiﬁcation tree to remain readable it can not
include thousands of variables (or nodes). Similarly, listing all these thousands of words with their
corresponding weights for a linear model will not provide much insight into individual decisions.
Considering our running example of web page classiﬁcation for safe advertising, what we want to
know is ‘Why did the model classify this particular web page as containing objectionable content?’
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Instance-level explanations
Over the past few years, instance explanation methods have been introduced that explain the predictions for individual instances . Generally, these methods provide a realvalued score to each of the variables that indicates to what extent it contributes to the instance’s
classiﬁcation. This deﬁnition of an explanation as a vector with a real-valued contribution for each
of the variables makes sense for many classiﬁcation problems, which often have relatively few variables ). For document classiﬁcation, however, due to the high-dimensionality of the data,
this sort of explanation is not ideal, and possibly not useful at all. Considering our safe-advertising
problem, an explanation for a web page’s classiﬁcation as a vector with thousands of non-zero
values can hardly be considered comprehensible. Although the words with the highest contributions will have the biggest impact on the classiﬁcation, we still don’t know which (combination of)
words actually led to any given classiﬁcation.
Aside from the unsuitable format of these previous explanations, previous instance-based explanation approaches are unable to handle high-dimensional data computationally. The sample-based
approximation method of ˇStrumbelj and Kononenko is reported to be able to handle up to
about 200 variables, even there requiring hours of computation time. The authors acknowledge that
for such data sets other approaches should be introduced:
Arguably, providing a comprehensible explanation involving a hundred or more features is a
problem in its own right and even inherently transparent models become less comprehensible
with such a large number of features .
Because of this inability to deal with the high-dimensionality of text mining data sets, as well as
the explanation format as a real-valued vector, these methods are not applicable for explaining
documents’ classiﬁcations.
In focusing on document classiﬁcation, we take advantage of three main observations to deﬁne
a slightly different problem from that addressed by prior work, that will address the motivating
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
business needs and that we will be able to solve efﬁciently. The ﬁrst observation is that in many
document classiﬁcation problems there really are two quite different explanation problems. We
often are interested speciﬁcally in one of them: why documents were classiﬁed as a particular focal
class (a “class of interest”). Considering our web page classiﬁcation setting, we will focus primarily
on explaining why a page has received (rightly or wrongly) a “positive” classiﬁcation of containing
objectionable content. The asymmetry is due to the negative class being a default class: if there
is no evidence of the class of interest (or of any of the classes of interest), then the document is
classiﬁed as the default class. In this paper we will not treat in detail the other explanation problem.
The question of why a particular page has not received a positive classiﬁcation can be important
as well, but reﬂection tells us that it is indeed a very different problem. Often the answer is “the
page did not exhibit any of the countless possible combinations of evidence that would have led
the model to deem it objectionable.” The problem here generally is “how do I ﬁx the model given
that I believe it has made an error on this document.” This is a fundamentally different problem and
thereby should require a very different solution—for example, an interactive solution where users
try to explain to the system why the page should be a positive, for example using dual supervision
 , or a relevance feedback/active learning system where chosen cases
are labeled and then the system is retrained . These are important problems,
but are beyond the scope of this paper.
The second important observation is that in contrast to the individual variables in many predictive modeling tasks, individual words can be quite comprehensible. Thus for us an explanation will
be a set of words present in the document such that removing all occurrences of these words results
in a different classiﬁcation (deﬁned precisely below). The innate comprehensibility of the words
often will immediately give deep intuitive understanding of the explanation. As we will see, when
it does not it can indicate problems with the model.
The third observation is that in document classiﬁcation, removing all occurrences of a word
always sets the corresponding variable’s value to zero. This will allow us to formulate an optimization problem for which we can ﬁnd solutions fast.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Explaining the Classiﬁcation of Documents
As discussed above, the question we address is ‘Why is this document classiﬁed as a non-default
class?’ To answer this question the technique(s) we introduce will provide an explanation as a set
of words present in the document such that removing these words causes a change in the class.
Only when all the words in the explanation are removed does the class change (the set is minimal).
To deﬁne the explanation formally (see Deﬁnition 1) we need to recall that a document D ∈D
is a bag (multiset) of words. Let WD be the corresponding set of words. We presume that classi-
ﬁcations are based on a classiﬁer CM, which is a function from documents to classes. Later, our
heuristic algorithm will presume that CM incorporates at least one scoring function fCM; classiﬁcations will be based on scores exceeding thresholds (in the binary case), or choosing the class with
the highest score (in the multiclass case). The majority of classiﬁcation algorithms operate in this
way, including all that we discuss in this paper.
DEFINITION 1. Given a document D consisting of mD unique words WD from the vocabulary
of m words: WD = {wi,i = 1,2,...,mD}, which is classiﬁed by classiﬁer CM : D →{1,2,...,k}
as class c. We deﬁne an explanation for document D’s classiﬁcation as a set E of words such that
removing all words in E from the document leads CM to produce a different classiﬁcation. Further,
an explanation E is minimal in the sense that removing any subset of E does not yield a change in
class. Speciﬁcally:
E is an explanation for CM(D) ⇐⇒
1. E ⊆WD (the words are in the document),
2. CM(D\E) ̸= c (the class changes), and
3. ∄E′ ⊂E : CM(D\E′) ̸= c (E is minimal).
D\E denotes the result of removing the words in E from document D.
Deﬁnition 1 is speciﬁcally tailored to document classiﬁcation. It provides intuitive explanations in
terms of words present in the document, and we will be able to produce such explanations even
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
in the massively dimensional input spaces typical of document classiﬁcation. More speciﬁcally,
Deﬁnition 1 differs from those of prior approaches in that the explanation is a set of words rather
than a vector. It also deﬁnes the size of the explanation as the cardinality of E. Our empirical
analysis will reveal that explanations typically are quite small (often about a dozen words) as
compared to the size of the vocabulary, and as such the technique is able to effectively transform
the high-dimensional input space to a low-dimensional explanation. This is of crucial importance
in order to provide explanations that address the business problems at hand, such as a manager’s
or a customer’s need to understand a classiﬁer’s decision, obtaining better understanding of the
domain, or improving the document classiﬁcation model’s performance.
The goal of the present approach seems to align with that of inverse classiﬁcation . However, the explanation format, the speciﬁc optimization problem, and the search
algorithms are quite different. First, for document classiﬁcation, we should only consider reducing
the values for the corresponding variables. Increasing the value of variables does not make sense.
Second, we don’t need to decide on step sizes for changes in the values, as removing the occurrences of a word corresponds to setting the value to zero. In the optimization routine of inverse
classiﬁcation, the search problem is exactly to ﬁnd the minimal distance for each dimension. The
optimization is completely different for explanations of documents’ classiﬁcation, as we will discuss next. Third, applying inverse classiﬁcation approaches to document classiﬁcation generally is
not feasible, due to the huge dimensionality of these data sets. Our approach takes advantage of the
sparseness of document representations, and only needs to consider those words actually present
in the document. Finally, we provide a general framework to obtain explanations independent of
the classiﬁcation technique.
The desire to be model-independent is important and worth discussing further. Some ﬁrms use
different model types for different document classiﬁcation problems. For document classiﬁcation, complicated non-linear models are often used, such as non-linear SVMs or
boosted trees . These models are incomprehensible globally. Explaining the individual decisions made by such models to a client, manager, or subject-matter expert is a
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
natural application of our approach. When a linear model is being used, one could argue simply to
list the top k words that appear in the document with the highest positive weights as an explanation
for the class (assuming we are explaining class 1 versus class 0). The choice of k can be set to 10
for example. A more suitable choice for k would follow our deﬁnition and be the minimal number
of top words such that removing these k words leads to a class change. This is exactly what our
approach would provide with a linear model. Finally, although they are often cited as producing
comprehensible models, classiﬁcation trees for document classiﬁcation do not provide the sort of
explanations we need (as in Deﬁnition 1): they do not explain what words actually are responsible
for the classiﬁcation. All words from the root to the speciﬁc leaf for this document may be important for the classiﬁcation, but some of these words are likely not present in the document (the path
branched on the absence of the word) and we do not know which (minimal) set of words actually
is responsible for the given classiﬁcation.
Finally, note the link with K- (different from the k above) Nearest Neighbor (KNN) approaches.
If such a technique is used as classiﬁcation method, see, e.g. D’Silva et al. , Han et al.
 , showing these K nearest neighbors and their classes “explains” why the model had chosen
that classiﬁcation. This technical “explanation” notwithstanding, the comprehensibility of such
classiﬁcation models is disputable. What is it exactly about the present document that makes it most
similar to a set of documents that yield the predicted class? The KNN technique does not tell me: if
the document had been slightly different would it simply be closer to a different set of documents
that yields the same predicted class? Below we discuss how showing the nearest neighbor(s) as an
explanation for the classiﬁcation made by any type of model can be used as secondary support for
an explanation, for example, showing training data that may have been mislabeled and led a model
to make erroneous classiﬁcations (see Hyperexplanation 3). This can help us to improve a model
if the explanation reveals an error.
Finding Document Classiﬁcation Explanations
The discussion above allows us to understand the problem more precisely from an optimization
perspective. Unlike the settings in prior work, here we are looking for the shortest paths in the space
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
deﬁned by word presence, based on the effect on the surface deﬁned by the document classiﬁcation
model, which is in a space deﬁned by more sophisticated word-based features (e.g., frequency or
tﬁdf, as described above). Conceptually, given a document vocabulary with m words, consider a
mask vector µ to be a binary vector of length m, with each element of the vector corresponding to
one word in the vocabulary. An explanation E can be represented by a mask vector µE with µE(i) =
1 ⇐⇒wi ∈E (otherwise, µE(i) = 0). Recall that the size of the explanation is the cardinality of
E, which becomes the L1-norm of µE. Then D\E is the Hadamard product of the feature vector of
document D (which may comprise frequencies or tﬁdf values) with the one’s complement of µE.
Thus, ﬁnding a minimal explanation corresponds to ﬁnding a mask vector µE such thatCM(D\E) ̸=
CM(D) but if any bit of µE is set to zero to form E′, CM(D\E′) = CM(D).
To our knowledge, this sort of explanation for document classiﬁcation has not previously been
formalized or examined carefully, so before presenting algorithms for producing document explanations, we should discuss the possible objectives precisely.
Objectives and Performance Metrics
Although Deﬁnition 1 is quite concise, the objectives for an algorithm searching for such explanations can vary greatly. A user may want to: (1) Find one or more minimum-sized explanation:
an explanation such that no other explanation of smaller size exists. (2) Find all minimal explanations. (3) Find all explanations of size smaller than a given k. (4) Find l explanations, as quickly
as possible (l = 1 may be a common objective). (5) Find as many explanations as possible within
a ﬁxed time period. Combinations of such objectives may also be of interest. To allow the evaluation of different explanation procedures for these objectives, we must deﬁne a set of performance
Search effectiveness:
6 Note that explanation accuracy is not a major concern: as an explanation by deﬁnition should change the predicted
class, it is straightforward to ensure that explanations produced always are correct. What is important with regards to
the usefulness of an explanation (or set of explanations) is how complex the explanation is, and how long it took for
the algorithm to ﬁnd the explanation.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
1. PE: Percentage of test instances explained (%)
Explanation complexity:
2. AWS: Average number of words in the smallest explanation (number)
Problem complexity:
3. ANS: Average number of smallest explanations given (number)
4. ANT: Average number of total explanations given (number)
Computational complexity:
5. ADF: Average duration to ﬁnd ﬁrst explanation (seconds)
6. ADA: Average duration to ﬁnd all explanations (seconds)
These performance metrics describe the behavior of a document explanation algorithm. In a
separate analysis, one can also employ a domain expert to verify the explanations. An interesting
question that is beyond the scope of this paper is: if the explanations are counterintuitive, does that
reﬂect on the explanation-ﬁnding method? Or only on the underlying classiﬁcation model that is
being explained? We will show that some explanations reveal the overﬁtting of the training data by
the modeling procedure, which often is not revealed by traditional machine learning evaluations
that examine summary statistics (error rate, area under the ROC curve, etc.).
Complete Enumeration of Explanations of Increasing Size
A straightforward approach to producing explanations is to conduct a complete search through
the space of all candidate word combinations, starting with one word, and increasing the number
of words until an explanation is found. The candidate word combinations are all combinations of
words in the document (rather than in the vocabulary), for which a subset of the words was not
already found to be an explanation. This approach starts by checking whether removing any one
word w from the document would cause a change in the class label. If so, we add the explaining
rule ‘if word w is removed then the class changes’. We check this for all of the words that are
present in the document. For a document with mD words, this requires mD evaluations of the classi-
ﬁer. If the class does not change based on one word only, the case of several words being removed
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
simultaneously will be considered. First, the algorithm considers all word combinations of size 2,
then 3 and so on. For combinations of 2 words, the algorithm makes mD × (mD −1) evaluations,
for all combination of 3 words mD×(mD−1)×(mD−2) evaluations, and more generally for combinations of k words we need mD!/(mD −k)! = O(mk
D) evaluations. This complete search scales
exponentially with the number of words in the document. Therefore, it is inpracticable for all but
the smallest documents. It could be used for small documents, such as explaining the classiﬁcations
of search queries, sentiment predictions for Twitter posts, or classiﬁcations based on non-standard
documents such as ad targeting classiﬁcation based on collections of visited URLs. Note that if the
goal of the search is to ﬁnd an explanation, the complete search is almost certain not to exhaustively search the space. If a short explanation exists, then the complete search may be quite fast for
such short documents. However, as the search will be impracticable for most document settings,
including the domains of our experiments, we will not consider complete search further.
Explaining Documents’ Classiﬁcations: A Heuristic-search Approach
As the number of potential explanations scales exponentially with the number of features, complete search is impracticable for most real document classiﬁcation problems. We now introduce a
heuristic search approach, formally described in Algorithm 1. It is designed speciﬁcally to ﬁnd one
or more minimal solutions in reasonable time. However, it is not guaranteed to ﬁnd all minimal
solutions or the shortest solution. (We will see below that it indeed is optimal in a certain, important
setting.) The approach is based on two notions:
1. Heuristic search guided by local improvement: We assume that the underlying classiﬁcation model will always be able to provide a probability estimate or score7 in addition to a categorical class assignment. We will denote this score function for classiﬁer CM by fCM(·). The algorithm
starts by listing all potential explanations of one word, and calculating the class and score change
7 No explicit mapping to is necessary; a score that ranks by likelihood of class membership is sufﬁcient. The
scores for different classes must be comparable in the multiclass case, so in practice scores often are scaled to . For
example, support-vector machines’ output scores are often scaled to (0,1) by passing them through a simple logistic
regression .
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Algorithm 1 SEDC: Search for Explanations for Document Classiﬁcation (via Best-ﬁrst Search
with Pruning)
WD = {wi,i = 1,2,...,mD} % Document D to classify, with mD words
CM : D →{1,2,...,k} % Trained classiﬁer CM with scoring function fCM
max iteration = 30 % Maximum number of iterations
Explanatory list of rule R
1: c = CM(D) % The class predicted by the trained classiﬁer
2: p = fCM(D) % Corresponding probability or score
% The explanatory list that is gradually constructed
4: combinations to expand on = {}
5: P combinations to expand on = {}
6: for i = 1 →mD do
cnew = CM(D\wi) % The class predicted by the trained classiﬁer if word wi did not appear in the document
pnew = fCM(D\wi) % The probability or score predicted by the trained classiﬁer if word wi did not appear in
the document
if cnew ̸= c then
R = R ∪‘if word wi is removed then class changes’
combinations to expand on = combinations to expand on∪wi
P combinations to expand on = P combinations to expand on∪pnew
15: end for
16: for iteration = 1 →max iteration do
combo = word combination in combinations to expand on for which
(p−p combinations to expand on) is maximal % The best ﬁrst
combo set = create all expansions of combo with one word
combo set2 = remove combinations containing already found explanations of R from combo set % The pruning
for all combos Co in combo set2 do
cnew = CM(D \Co) % The class predicted by the trained classiﬁer if the words in Co did not appear in the
pnew = fCM(D\Co) % The probability or score predicted by the trained classiﬁer if the words in Co did not
appear in the document
if cnew ̸= c then
R = R ∪‘if words Co are removed then class changes’
combinations to expand on = combinations to expand on∪Co
P combinations to expand on = P combinations to expand on∪pnew
30: end for
for each. The algorithm proceeds as a straightforward heuristic best-ﬁrst search. Speciﬁcally, at
each step in the search, given the current set of word combinations denoting partial explanations,
the algorithm next will expand the partial explanation for which the output score changes the most
in the direction of class change. Expanding the partial explanation entails creating a set of new,
candidate explanations, comprising all combinations with one additional word from the document
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
(that is not yet included in the partial explanation).
2. Search-space pruning: For each explanation with l words that is found, we do not need to
check combinations of size l + 1 with these same words, hence we can prune these branches of
the search tree. For example, if the words ‘hate’ and ‘furious’ provide an explanation, we are not
interested in explanations of three words that include these two words, such as ‘hate’, ‘furious’ and
‘never’. This search problem generally (including the complete search solution) is an instance of
unordered-set search. Unordered-set search is described in detail by Webb (and references
therein), including optimizations that speed up the search substantially, while still allowing various
guarantees, including this sort of search-space pruning. The pruning is somewhat different from
the search-space pruning in similar set-enumeration algorithms, such as the Apriori association
rule mining algorithm , in that it is based on set subsumption rather
than coverage statistics.
For the case of a linear classiﬁer with a binary feature representation, we might explain the classiﬁcation by looking at the words with the highest weights that appear in the document. However,
we would still want to know which words exactly are responsible for the classiﬁcation. SEDC
produces minimum-size explanations for linear models, which we discuss further next. Assuming
again a class 1 versus class 0 prediction for document i, SEDC ranks all words appearing in the
document according to the product βjxij, where βj is the linear model coefﬁcient. An explanation
of smallest size is the one with the top-ranked words, as chosen by SEDC’s heuristic search.
LEMMA 1. For document representations based on linear binary-classiﬁcation models
fCM(D) = β0 +∑βjxij with binary (presence/absence) features, the smallest explanation found by
SEDC will be a minimum-size explanation. More speciﬁcally, for E1,E2 explanations, if E1 is the
smallest explanation found by SEDC, |E1| = k ⇒∄E2 : |E2| < k. Furthermore, the ﬁrst explanation
found by SEDC will be of size k.
Proof (by contradiction):
If no explanation exists, then the theorem holds vacuously. Assume
there exists at least one explanation. In the linear model, let the (additive) contribution wij to the
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
output score for word j of document i be the linear model weight βj corresponding to binary
word-presence feature xb
ij for those words that are present in document i (and zero otherwise).
Assume w.l.o.g. that the classiﬁcation threshold is placed at fCM(D) = 0. SEDC will compose
the ﬁrst candidate explanation E∗by ﬁrst selecting the largest wij such that the word is present
in the document, xb
ij = 1, and adding word j to the explanation. SEDC will then add to E∗the
word with the next-largest such wij, and so on until fCM(E∗) ≤0. Thus, the ﬁrst explanation E1 by
construction will consist of the k highest-weight words that are present in the document.
Now assume that there exists another explanation E2 such that |E2| < k; being an explanation,
fCM(E2) ≤0. Recall that explanations are minimal, so ∄S ⊊E1 : fCM(S) ≤0. Thus E2 must have at
least one element e ̸∈E1. Let ∑E denote the sum of the weights corresponding to the words in an
explanation E. For a linear model based on the (binary) presence/absence of words, fCM(X\Y) =
fCM(X) −∑Y. As noted above, E1 comprises by construction the k words with the largest wij,
so ∀wij ∈E1,∀we /∈E1 : wij ≥we. Therefore, ∃S ⊊: E1,∑S > ∑E2, which means that ∃S ⊊E1 :
fCM(D\S) ≤fCM(D\E2). But ∀S ⊊E1 : fCM(D\S) > 0 and thus fCM(D\E2) > 0. Therefore, E2 is
not an explanation, a contradiction.
This optimality applies as well to monotonic transformations over the output of the linear model,
as with the common logistic transform used to turn linear output scores into probability estimates.
The optimality also applies more generally for linear models based on numeric word-based features, such as frequencies, tﬁdf scores, etc., as detailed in the following theorem.
THEOREM 1. For document representations based on linear models fCM(D) = β0+∑βjxij with
numeric word-based features, such as frequencies or tﬁdf scores, that take on positive values when
the word is present and zero when the word is absent, the smallest explanation found by SEDC
will be a minimum-size explanation. More speciﬁcally, for E1,E2 explanations, if E1 is the smallest
explanation found by SEDC, |E1| = k ⇒∄E2 : |E2| < k. Furthermore, the ﬁrst explanation found
by SEDC will be of size k.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Decompose each non-negative word feature xij into the product xb
ijdij of a binary word
presence/absence feature xb
ij and a document-speciﬁc non-negative weight dij. The corresponding
term in the linear model βjxij then becomes βjdijxb
ij. The proof then follows the previous proof
directly, except with the additive contribution of each word being wij = βjdij.
For non-linear models no such optimal solutions are guaranteed, in the sense that smaller explanations could exist. For multiclass classiﬁcation problems optimal solutions are also not guaranteed
if one decomposes the problem into several binary classiﬁcation problems (as in a one-versus-rest
or one-versus-one approach), since the ﬁnal classiﬁcation of data instances now depends on several
models with their own weights. This motivates our next optimization, applying local search on the
obtained explanations.
SEDC Augmented with Local Search
The SEDC algorithm has two potential issues when applied to non-linear models, addressed by
two optimizations. Firstly (and most importantly), seeing that the prediction space is non-linear
in the words, the obtained explanations might not contain a minimal subset of words, required by
Deﬁnition 1 (requirement 3; E is minimal). It could be that removing a word from the explanation
E still provides an explanation E′, hence: there exists an explanation E′ ⊂E : CM(D\E′) ̸= c. To
address this concern, we extend the previously deﬁned heuristic search procedure with a limited
local search post-processing phase applied to the obtained explanations. This method will prune
the explanation if necessary, by verifying whether removing a word (or word combination) from
an obtained explanation E also provides an explanation E′. If that is the case E is replaced by the
smaller explanation E′, containing a subset of the words of E. This guarantees minimality of the
explanations (though in the empirical studies we never observed the need for such pruning).
The second issue with SEDC for non-linear models is that potentially smaller explanations exist
(with different words, making it different from the above optimization) than those obtained. More
formally, there might exist an explanation E′, where E′ \E ̸= /0 (E′ has some word(s) that E does
not), |E′| < |E| (explanation E′ is smaller than E), CM(D\E′) ̸= c (E′ also deﬁnes an explanation).
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
To investigate the extent of this potential issue, we deﬁne a second local search approach that
is applied to the explanations found by the heuristic search method (with optimizations). For each
explanation, we replace two words by another word of the document, not yet in the explanation.
Next, we attempt replacing three words of the explanation by two words of the document, not yet
in the explanation, and so on. This yields a very large number of potential combinations to check:
replacing a set of k words of an explanation for a document with mD words yields
combinations.8 To deal with this huge number of new word combinations to check, we limit ourselves
in our experiments up to k = 5 words, and a maximum of 5,000 combinations. If more exist, no
attempt to optimize is undertaken. Within our empirical results, this local search addition provided
an improvement of one word for only very few explanations (less than 1%), while requiring much
more time (up to two hours per explanation, even with the limitation on the number of combinations). Seeing that the additional local search is so computationally expensive compared to the
heurstic search procedure (with negligible improvements in explanation size), the results in the
next section are provided without the local search.
SEDC with Branch-and-bound
As described in Section 4.1, there are various objectives one might have when ﬁnding explanations
for document classiﬁcations. In the important case where one wants the shortest explanation, or
the set of shortest explanations, the SEDC search can be improved by keeping track of the current
shortest explanation found, and pruning from the search space all longer explanations (a simple
branch-and-bound search), which can result in massive portions of the search space being discarded
en masse once a ﬁrst explanation has been found.9
8 To indicate how large these values can be, for k = 3 and mD = 100 we have 147,440 combinations; for k = 5 and
mD = 500 we have 255,244,687,600 combinations.
9 Unfortunately, for the general problem one cannot give non-trivial upper and lower bounds on explanation size given
a partial explanation. For particular types of models, this may be possible, yielding more sophisticated brand-andbound searches.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Empirical Analysis
We now present an empirical case study on the problem of classifying web
pages as containing adult content. A follow-up analysis is presented in Appendix A based on a
suite of text classiﬁcation problems (the 20 Newsgroups) widely used in the research literature.
Explaining Web Pages’ Classiﬁcations for Safe Advertising
The case study is based on data obtained from a ﬁrm that focuses on helping advertisers to avoid
inappropriate adjacencies between on-line advertisements and web content, similar to our motivating example above. Speciﬁcally, the analysis is based on a data set of 25,706 web pages, labeled as
either having adult content or not. The web pages are described by tﬁdf scores over a vocabulary
chosen by the ﬁrm, including a total of 73,730 unique words. No stemming was conducted. The
data set is balanced by class, with half of the pages containing adult content and half non-adult
content. For this data set, the class labels were obtained from a variety of sources used in practice,
including Amazon’s Mechanical Turk. Given the variety of labeling sources, the quality of the
labeling might be questioned . Interestingly, the explanations indeed reveal that
certain web pages are wrongly classiﬁed. No meta-data, links, or information on images is being
used for this study; the inclusion of such data could improve the model further, but the focus of
this paper is on textual document classiﬁcation.
For this analysis, we built SVM document classiﬁcation models with linear and RBF kernel
functions.10 The linear model is correct on 96.2% of the test instances, with a sensitivity (percentage of non-adult web pages correctly classiﬁed) of 97.0%, and a speciﬁcity (percentage of adult
web pages correctly classiﬁed) of 95.6%. The non-linear RBF kernel model has an accuracy of
93.3%, with a sensitivity of 89.0% and a speciﬁcity of 96.5%.
10 Using the LIBLINEAR and LIBSVM packages , with 90% of the data used
as training data, the remaining 10% as test data. SEDC was coded in Matlab and is available upon request. Experiments
were run on an Intel Core 2 Quad (3 GHz) PC with 8GB RAM.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Global explanations As discussed above, rule extraction is the most researched and
applied model explanation methodology. Trying to comprehend the SVM model, a tree can be
extracted by applying the C4.5 tree induction technique on the aforementioned safe
advertising data set with class labels changed to SVM predicted labels. Unfortunately, we could
not get C4.5 to generate a small tree that models either SVM model (with linear or RBF kernel)
with high-ﬁdelity. A tree with 327 nodes models the classiﬁer with a ﬁdelity of only 87%. Pruning
the tree further reduces the size, but further decreases ﬁdelity.
As discussed above, an alternative method for comprehending the function of a linear document
classiﬁer is to examine the weights on the word features, as these indicate the effect that each
word has on the ﬁnal output score. As with the distinction between Lemma 1 and Theorem 1, we
need to keep in mind that in a preprocessing step the data set is encoded in tﬁdf format. Hence
for actual document explanations, the frequency is vital.11 Figure 3 shows the weight sizes of all
the words in the vocabulary; the weights are ranked smallest-to-largest, left-to-right. Clearly many
words show a high indication of adult content, while many others show a clear counter-indication
of adult content. Looking deeper, Table 1 shows the highest (positive) weight words, as well as the
words that give the highest mutual information (with the positive class) and information gain. We
additionally list the top words when taking into account the idf weights, viz., based on the weights
of the words multiplied with the corresponding idf values. The ﬁnal column shows the words most
frequently occurring in the explanations, which will be elaborated on below.
From Table 1 we see that most indicative words for adult content ranked highly using the mutual
information criterion are very rare, unintuitive words. It may be possible to engineer a better
information-based criterion, for example countering this overﬁtting behavior by requiring a minimal frequency of the top ranked words, but later results will show why such efforts ultimately are
destined to fail to provide a comprehensive explanation. The top words provided by the other rankings on the other hand are quite intuitive. As stated before, even initially not-so-obvious words as
11 The inverse document frequency is constant across documents, and could be incorporated in the model weights to
facilitate global explanation.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
The size of the weights for all 73,730 words, ranked left-to-right according to increasing
‘welcome’, ‘enter’ or ‘age’ make sense once we realize that many positive examples are entrance
pages of adult sites, which inform a visitor about the content of the website and require veriﬁcation
of age. Nevertheless, as we will see next, explanation of individual decisions simply requires too
many individual words. Consider that we would have to produce a list of over 700 of the highestweight words just to include ‘porn’ and over 10,000 to include ‘xxx’.
Given the intuitiveness of the top-weighted words, we should consider how well a short list of
such words really explains the behavior of the model. Does the explanation of a web page typically
consist of (some of) the top-100 or so words? It turns out that the content of web pages varies
tremendously, even within individual categories. For “adult content”, even though some strongly
discriminative words exist, the model classiﬁes most web pages as being adult content for other
reasons. This is demonstrated by Figure 4, which plots the percentage of the classiﬁcations of the
test instances that would be explained by considering the top-k words (horizontal axis) by weight
(with and without idf correction), mutual information and information gain. Speciﬁcally, if an
explanation in the sense of Deﬁnition 1 can be formed by any subset of the set of top-k words, then
the document is deemed explained. So for example, if an explanation would be ‘if words (welcome
enter) are removed then class changes’, that explanation would be counted when k ≥2.
We see from Figure 4 that we would need thousands of these top words before being able to explain a
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Percentage Explained
Percentage Explained with top k words only
Ranked on frequency in explanations
Ranked on w
Ranked on w with idf correction
Ranked on MI
Ranked on IG
Percentage of 100 adult-classiﬁed test instances explained when considering only the top k
words, ranked according to the frequency of occurrence in the explanations, the weights (w), the
weights with idf correction, mutual information (MI) and information gain (IG).
Ranking based on
Mutual Information
Information Gain Size of weight Size of weight
Frequency of word occurring
with idf correction in the explanations
primarykey
permanently
webplayerrequiredgeos
permanently
compuserve
vnesfrsgphplitgrmxnlkrause advertise
copyrightc
videocategoryids
prostitution
latestwebplayerversion
isyoutubepermalink
amateurbasecom
Global explanation of the model by listing the top words providing evidence for the adult class. Five
rankings are considered: based on mutual information, information gain, weights of the words, weights with idf
correction (weight multiplied with word idf), and frequencies of the words occurring in the explanations.
large percentage of the individual documents, as shown by the line with words ranked on the weight. More
precisely, more than two thousand top-weight words (3% of the vocabulary) are needed before even half
of the documents are explained. Using the ranking based on mutual information requires even more words.
This suggests either (i) that many, many words are necessary for individual explanations, or (ii) the words
in the individual explanations vary tremendously. The latter conclusion is also supported by the fact that the
document-term matrix is very sparse even when the documents belong to the same topic. This motivates
the use of an instance-level explanation algorithm not only for obtaining understanding of the individual
decisions, but also for understanding the model overall.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
When we rank the words according to how often they occur in explanations, we obtain the line with
the maximal area underneath. For the 100 classiﬁed instances, a total of 810 unique words are used in all
the explanations (where we consider maximum 10 minimal explanations for a single data instance). This
already suggests a wide variety of words are present in the explanations. The instance-based explanations
can be aggregated to a global explanation by listing the words that occur most frequently in the explanations, as shown in the ﬁnal column of Table 1, which provides yet another beneﬁt of the instance-level
explanations. We will not explore this further, as it is peripheral to the main focus of this paper.
Instance-level explanations None of the previously published instance-level explanation methods are able to handle many thousands of variables, so they can not be applied to this domain. We’ll show
now that SEDC is effective, and fast as well, where we initially focus on the linear classiﬁcation model.
Explanation 2 shows several typical explanations for classiﬁcations of test documents. We show the ﬁrst
three explanations of test instances with explanations that are appropriate for publication. These explanations demonstrate several things. First, they directly address suggestion (i) just above: in fact, documents
generally do not need many, many words to be explained. They also provide evidence supporting suggestion (ii): the words in the individual explanations are quite different, including explanations in different
languages.
We can examine the size of explanations more systematically by referring to the explanation performance
metrics introduced in Section 4.1. The top-left plot in Figure 5 shows the percentage of the test cases
explained (PE) when an explanation is limited to a maximum number of words (on the horizontal axis). We
see that almost all the documents have an explanation comprising fewer than three dozen words, and more
than half have an explanation with fewer than two dozen words. In other words, each explanation is very
concise, as it uses only about 0.01% of the words in the vocabulary. Note that even explanations containing
dozens of words can easily give an understanding of why the classiﬁer classiﬁed the document as the class of
interest, as is discussed and shown in Section 5.2, below. Figure 5 also shows that, not too surprisingly, the
number of words in the smallest explanation (AWS plot) and the (smallest and total) number of explanations
(ANS, ANT plots) both grow as we allow larger and larger explanations.12
12 In the experiments, we limit ourselves to searching for 10 explanations: if 10 or more explanations have been found,
no further word expansions/iterations are attempted.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
AWS ANS ANT ADF ADA
Explanation performance metrics for the false positives (FP) versus true positives (TP) of the linear
model, allowing up to 30 words in an explanation. Shown are percentage explained (PE), average number of
explanations given (ANT), average number of words in the smallest explanation (AWS), average duration to ﬁnd
the ﬁrst explanation (ADF) and average duration to ﬁnd all explanations (ADA).
Explanation 2:
Some explanations why a web page is classiﬁed as having adult content for web pages of the test set.
Explaining document 13 (class 1) with 61 features and class 1 ...
Iteration 7 (from score 0.228905 to -0.00155753): If words (submissive pass hardcore check bondage adult
ac) are removed then class changes from 1 to -1 (1 sec)
Iteration 7 (from score 0.228905 to -0.00329069): If words (submissive pass hardcore check bondage adult
access) are removed then class changes from 1 to -1 (1 sec)
Iteration 7 (from score 0.228905 to -0.00182021): If words (submissive pass hardcore check bondage all
adult) are removed then class changes from 1 to -1 (1 sec)
Explaining document 30 (class 1) with 89 features and class 1 ...
Iteration 4 (from score 0.894514 to -0.0108126): If words (searches nude domain adult) are removed then
class changes from 1 to -1 (1 sec)
Iteration 6 (from score 0.894514 to -0.000234276): If words (searches men lesbian domain and adult) are
removed then class changes from 1 to -1 (1 sec)
Iteration 6 (from score 0.894514 to -0.00225592): If words (searches men lesbian domain appraisal adult)
are removed then class changes from 1 to -1 (1 sec)
Explaining document 32 (class 1) with 51 features and class 1 ...
Iteration 8 (from score 0.803053 to -0.0153803): If words (viejas sitios sexo mujeres maduras gratis
desnudas de) are removed then class changes from 1 to -1 (1 sec)
Translation: old mature women sex sites free naked of
Iteration 9 (from score 0.803053 to -7.04005e-005): If words (viejas sitios mujeres maduras gratis desnudas
de contiene abuelas) are removed then class changes from 1 to -1 (1 sec)
Translation: old mature women free sites containing nude grandmothers
Iteration 9 (from score 0.803053 to -0.00304367): If words (viejas sitios mujeres maduras gratis desnudas
de contiene adicto) are removed then class changes from 1 to -1 (1 sec)
Translation: old sites free naked mature women contains addict
Explaining document 35 (class 1) with 36 features and class 1 ...
Iteration 6 (from score 1.04836 to -0.00848977): If words (welcome ﬁction erotic enter bdsm adult) are
removed then class changes from 1 to -1 (0 sec)
Iteration 6 (from score 1.04836 to -0.10084): If words (welcome ﬁction erotica erotic bdsm adult) are
removed then class changes from 1 to -1 (1 sec)
Iteration 6 (from score 1.04836 to -0.0649064): If words (welcome kinky ﬁction erotic bdsm adult) are
removed then class changes from 1 to -1 (1 sec)
Table 2, presents the differences between the false and true positives (for the default threshold of 0).
Interestingly, we ﬁnd higher coverage, as well as more and smaller explanations for the web pages wrongly
classiﬁed as adult (false positives, FP) versus those correctly classiﬁed as adult (true positives, TP). Seeing that FPs are classiﬁcations we are particularly interested in explaining ), this suggests that the overall explanation metrics yield conser-
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
vative estimates of practical performance for this case study.
More interestingly, examining these performance metrics gives a view into how the classiﬁcation model
is functioning in this application domain. Speciﬁcally, the plots show that document explanation sizes vary
quite smoothly and that there seem to be many different explanations for documents. The former observation
suggests that the strength of the individual evidence varies widely: some cases are classiﬁed by aggregating
many weak pieces of evidence, others by a few strong pieces of evidence (and some, presumably by a
combination of strong and weak). The latter observation suggests substantial redundancy in the evidence
available for classiﬁcation.
Figure 5 also shows that for this particular problem, explanations can be produced fairly quickly using
SEDC. This problem is of moderate size; real-world document classiﬁcation problems can be much larger,
in terms of documents for training, documents to be classiﬁed, and the vocabulary. A brief word about
scaling up can be found in Appendix B.
To validate the applicability of the explanation method for non-linear models, an SVM model with a
radial basis function (RBF) kernel (a popular non-linear model) was used as well.
Table 3 shows SEDC’s performance on both linear SVM and non-linear radial-basis function (RBF)
kernel SVM models, when allowing up to 30 words in an explanation. The percentage explained is about the
same for the linear and non-linear model, with interestingly the non-linear model requiring slightly fewer
words per explanation (AWS). A large difference is observed in the time needed to obtain an explanation:
whereas for the linear model it takes on average four seconds to ﬁnd an explanation, for the RBF model
it takes almost three minutes. A deeper investigation into the reasons for the speed differences shows that
processing the non-linear models takes longer not because of the backtracking in the search. Rather, the
non-linear models simply run much slower, which has a crucial affect due to the repeated applications
of the scoring function. Therefore, faster implementations of the non-linear models could produce faster
explanation performance. Please note that explanation times on the orders of minutes are not necessarily a
cause for concern, depending on the context of application. In many of the application scenarios discussed
above, explanation methods would be reserved for periodic development use or for tactical use when a
concern arises over a particular case. 13
13 Also, recall that these experiments were conducted mainly in Matlab on a desktop PC. Further speed improvements
could easily be obtained with faster software implementations or with the high-performance computing systems typically used by organizations that build text classiﬁers from massive data. Importantly, once again, the complexity is
independent of the size of the vocabulary. Further, unordered-set search is highly parallelizable.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
AWS ANS ANT ADF ADA
SEDC Linear SVM 84%
SEDC B&B Linear SVM 84%
SEDC Non-linear RBF SVM 82%
SEDC B&B Non-linear RBF SVM 82%
Explanation performance for SEDC and SEDC with branch-and-bound (B&B), for SVMs with a linear
kernel and a radial basis function (RBF) kernel SVM. SEDC was allowed up to 30 words in an explanation.
Number of words
Number of words
Number of words
Number of words
Number of words
Number of words
Explanation performance metrics in terms of maximal number of words allowed in an
explanation. Both the performance and the complexity increase with the number of words. Next to
the average metrics, the 10th and 90th percentiles are also shown (dotted lines).
Hyper-explanations
Conducting the case studies brought to the fore some additional issues regarding explaining documents
classiﬁcations. Speciﬁcally, a procedure for producing explanations of document classiﬁcations may provide no explanation at all. Why not? A document’s explanation may be non-intuitive. Then what? There are
several classes of reasons for these behaviors, which we group into hyper-explanations. Many of these are
speciﬁcally helpful for the task of improving the decision system’s model (cf., Section 2).
Hyper-explanations for the lack of an explanation. We distinguish between cases where the
predicted class is the default class (hyper-explanation 1), and those where the predicted class is the nondefault class (hyper-explanation 2).
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Hyper-explanation 1a: no evidence present. The default class is predicted and no evidence for either
class is present. For example, this would be the case when all words in the document have zero weights in
the model or no words present are actually used in the model.
Technically, this case falls outside the scope of this paper’s development, since we are speciﬁcally considering explaining why a document is classiﬁed as a non-default class. Nevertheless, this may be a practically
important situation that cannot simply be ignored. For example, this case may have been brought to a manager’s or developer’s attention as a “false negative error”, i.e., it should have been classiﬁed as a positive
example. In this case the hyper-explanation explains exactly why the case was classiﬁed as being negative
(there was no model-relevant evidence) and can be a solid starting point for a management/technical discussion about what to do about it. For example, it may be clear that the model’s vocabulary needs to be
Hyper-explanation 1b: no evidence of non-default class present. The default class is predicted and
only evidence in support of the default class is present. This is a minor variation to Hyper-explanation 1a,
and the discussion above applies regarding explaining false negatives and providing a starting point for
discussions of corrective actions.
Hyper-explanation 1c: evidence for default class outweighs evidence for the non-default class. A
more interesting and complex situation is when, in weighing evidence, the model’s decision simply comes
out on the side of the default class. In this case an immediate reaction may be to apply the explanation
procedure to generate explanations of why the case was classiﬁed as being default (i.e., if these words were
removed, the class would change to positive). However, when the case truly is of the “uninteresting” class,
the explanations returned would likely be fairly meaningless, e.g., “if you remove all the content words on
the page except the ’offending words’ (e.g., the words with positive weights), the classiﬁer would classify
the page as an offensive page.” However, applying the procedure may be very helpful for explaining false
negatives, because it would show the words that the model feels trump the positive-class-indicative words
on the page (e.g., if you remove the medical terminology on the page, the classiﬁer would then rate the page
as being adult). This again could provide a solid foundation for the process of improving the classiﬁers.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Explanation 3:
Explanations of web pages misclassiﬁed as non-adult (false negatives), which indicate which words
the model feels trump the positive-class-indicative words.
Explaining document 10 (class 1) with 31 features and class -1 (score -0.126867)...
Iteration 4 (from score -0.126867 to 0.00460739): If words (policy gear found blog) are removed then class
changes from -1 to 1 (0 sec)
Explaining document 13 (class 1) with 50 features and class -1 (score -0.123585)...
Iteration 4 (from score -0.123585 to 0.000689515): If words (sorry miscellaneous found about) are removed
then class changes from -1 to 1 (0 sec)
Explaining document 11 (class 1) with 198 features and class -1 (score -0.142504)...
Iteration 2 (from score -0.142504 to 0.00313354): If words (watch bikini) are removed then class changes
from -1 to 1 (1 sec)
Explaining document 31 (class 1) with 22 features and class -1 (score -0.0507037)...
Iteration 4 (from score -0.0507037 to 0.00396628): If words (search handjobs bonus big) are removed then
class changes from -1 to 1 (0 sec)
Within our safe advertizing application, an explanation for all 46 false negatives is found, indicating
that indeed adult words are present but these are outweighed by the non-adult, negative words. Example
explanations of such false negatives are given in Explanation 3. For some words like ‘blog’ it seems logical
to have received a large non-adult/negative weight. The word ‘bikini’ seemingly ought to receive a nonadult weight as well, as swimsuit sites are generally not considered to be adult content by raters. However,
some pages mix nudes with celebrities in bikinis (for example). If not enough of these are in the training
set, it potentially would cause ‘bikini’ to lead to a false negative. Many other words however can be found
in the explanations that do seem to be adult-related (such as ‘handjobs’), and as such should receive a
positive weight. All the words are great candidates for human feedback to indicate which of these words
actually are adult related and potentially update the model’s weights ) or review the labeling quality of the web pages with the word. The
words occurring most in these explanations of false negatives (when considering only the ﬁrst explanation)
are ‘found’, ‘blog’ and ‘policy’. The seemingly-adult related words are not found when examining the words
with most negative weights, again supporting the need to look at explanations separately, on an instance
Hyper-explanation 2: too much evidence of non-default class present. No explanation is provided
because, although a non-default class is predicted, there are so many words in support of this class that one
needs to remove almost all of them before the class will change. The situations when this will occur fall
along a spectrum between two fundamentally different reasons:
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
cunnilingus
Number of words removed
Evolution of score for explanations
freepornpicsfreeporncom
Highest starting score
Most words in explanation
Average words in explanation
Score evolution when removing words from the three selected documents: the one with
highest starting score, the one with the most words in an explanation and a document with average
number of words in an explanation. The class changes to non-adult when the score falls below zero.
1. There are very many words each providing weak evidence in support of the class. Thus, the explanation exceeds the bound given to the algorithm, or the algorithm does not return a result in a timely
fashion. Figure 6 shows the words of the explanations for three documents and how the scores change as
the words are removed. The middle line, for the explanation with the most words, shows that if the number
of allowed words is below 40, no explanation is found. This lack of explanation can be explained by this
hyper-explanation, as too many adult-related words are present for a short explanation to be found.
2. There are very many words each providing strong evidence. In this case, the procedure may not be
able to get the score below the threshold with a small explanation, because there is just so much evidence
for the class. The full upper line with the highest starting score in Figure 6 shows such an example: when
allowing fewer than 15 words in an explanation, the score remains above the threshold and no explanation
can be given.
This lack of base-level explanation can be mitigated (partially) by presenting “the best” partial explanation as the search advances.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Hyper-explanations for non-intuitive explanations. Explanations are always correct in the
technical sense: removing the words by deﬁnition changes the class. However, it is possible that the explanation clashes with the user’s intuition, creating a perceived anomaly that should be explained. Several
reasons exist for this:
The data instance is misclassiﬁed. The explanations of some of the web pages that are misclassiﬁed
by the SVM model are listed in Explanation 4 (only the ﬁrst explanation is shown). For these pages the
predicted class is adult, while the human-provided class label is non-adult (false positives). These three
explanations indicate strongly that the web pages actually contain adult content and the human-provided
label seems wrong. On the other hand, in other cases, explanations indicate that their web pages seem to
be non-adult and hence are probably misclassiﬁed. Examples are given in Explanation 5.14 Such explanations provide very useful support for interactive model development, as the technical/business team can ﬁx
training data or incorporate background knowledge to counter the misclassiﬁcation.
The data instance is correctly classiﬁed, but the explanation just does not make sense to the business
users/developers. This case is particularly problematic for any automated explanation procedure, since
providing explanations that “make sense” requires somehow codifying in an operationally useful way the
background knowledge of the domain, as well as common sense, which to our knowledge is (far) beyond
current capabilities (and certainly beyond the scope of this paper). Nevertheless, we still can provide a quite
useful hyper-explanation in the speciﬁc and common setting where the document classiﬁcation model had
been built from a training set of labeled instances (as in our case study). Speciﬁcally:
Hyper-explanation 3: Show similar training instance. For a case with a counter-intuitive explanation,
we can show “similar” training instances with the same class. The similarity metric in principle should
roughly match that used by the induction technique that produced the classiﬁer. Such a nearest-neighbor
approach can aid understanding in two ways. (1) If the training classiﬁcations of the similar examples do
make sense, then the user can understand why the focal example was classiﬁed as it was. (2) If the training classiﬁcations do not make sense (e.g., they are wrong), then this hyper-explanation provides precise
14 Our models are limited by the data set obtained for the case study. By our understanding, models built for this
application from orders-of-magnitude larger data sets are considerably more accurate; nonetheless, they still make
both false-positive and false-negative errors, and the general principles illustrated here apply.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
guidance to the data science team for improving the training,15 and thereby the model.
Consider document 8. Explanation 5 suggests strongly that it contains non-adult content, even though
the model classiﬁes it as adult. The web page most similar to document 8 is also classiﬁed as adult and
has 44 (out of 57) words which are the same, which are listed in Explanation 6. This is a web page with
a variety of topics, and probably a listing of links to other websites. This sort of web page needs further,
expert investigation for use in training (and evaluating) models for safe advertising. It could be that labelers
have not properly examined the entire web site; it may be that there indeed is adult content in images that
our text-based analysis does not consider; it may be that these sites simply are misclassiﬁed, or it may be
that in order to classify such pages correctly, the data science team needs to construct speciﬁcally tailored
feature to deal with the ambiguity.
Explanation 4:
Explanations of web pages misclassiﬁed as adult (false positives), which indicate that the model is
right and the class should have been adult (class 1).
Explaining document 1 (class -1) with 180 features and class 1 (score 1.50123)...
Iteration 35 (from score 1.50123 to -0.00308141): If words (you years web warning usc these sites site
sexual sex section porn over offended nudity nude models material male links if hosting hardcore gay free
explicit exit enter contains comic club are age adults adult) are removed then class changes from 1 to -1 (53
Explaining document 2 (class -1) with 106 features and class 1 (score 0.811327)...
Iteration 24 (from score 0.811327 to -0.00127533): If words (you web warning under und these site porn
over offended nude nature material links illegal if here exit enter blonde are age adults adult) are removed
then class changes from 1 to -1 (15 sec)
Explaining document 3 (class -1) with 281 features and class 1 (score 0.644614)...
Iteration 15 (from score 0.644614 to -0.00131314): If words (you sex prostitution over massage inside
hundreds here girls click breasts bar) are removed then class changes from 1 to -1 (29 sec)
Explanation 5:
Explanations of truly misclassiﬁed web pages (false positives).
Explaining document 8 (class -1) with 57 features and class 1 (score 0.467374)...
Iteration 7 (from score 0.467374 to -0.0021664): If words (welcome searches jpg investments index fund
domain) are removed then class changes from 1 to -1 (3 sec)
Explaining document 16 (class -1) with 101 features and class 1 (score 0.409314)...
Iteration 8 (from score 0.409314 to -0.000867436): If words (welcome und sites searches domain de b
airline) are removed then class changes from 1 to -1 (5 sec)
Explaining document 32 (class -1) with 66 features and class 1 (score 0.124456)...
Iteration 2 (from score 0.124456 to -0.00837441): If words (searches airline) are removed then class changes
from 1 to -1 (0 sec)
15 Data cleaning is a very important aspect of the data mining process that has received relatively little treatment in
the research literature. One of the main data cleaning activities in classiﬁer induction is “ﬁxing” labels on mislabeled
training data.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
Explanation 6:
Hyper-explanation 3 showing the words of the web page most similar to document 8. This most similar
web page is classiﬁed as adult, providing a hyper-explanation of why document 8 is also classiﬁed
(incorrectly) as adult.
and, articles, at, buy, capital, check, china, commitment, dat, ﬁle, ﬁles, for, free, fund, funds, high, hot, in,
index, instructionalwwwehowcom, international, internet, investing, investment, investments, jpg, listings,
mutual, out, performance, project, related, results, return, searches, social, sponsored, temporary, tiff, to,
trading, vietnam, web, welcome.
Discussion and Limitations
In this paper, we followed the guidelines set forth by Hevner et al. for designing, executing and
evaluating research within design science to explain documents’ classiﬁcations. We presented a search algorithm (SEDC) for ﬁnding such explanations and empirically evaluated the algorithm two different document
classiﬁcation domains.
An unexpected result of the case study was the need for various sorts of hyper-explanations. Several
of these are the result of the document classiﬁcation models being statistical models learned from data,
and thus are subject to the main challenges of machine learning: overﬁtting, underﬁtting, and errors in the
data. When classiﬁcation errors are introduced due to these pathologies, even instance-level explanations
may be inadequate (e.g., missing) or unintuitive. Hyperexplanations are needed for deep understanding, for
example, showing training cases that likely led to the current model behavior.
As discussed in the introduction, we believe that instance-level explanation methods such as SEDC can
have a substantial impact in improving the process of building document classiﬁcation models. The ﬁeld
needs more research addressing support for the process of building acceptable models, especially in business situations where various parties must be satisﬁed with the results. Indeed, recent developments in
machine learning and data mining arguably have moved us further away from the needed transparency, with
the strong research emphasis on and seeming success of techniques resulting in complex models, such as
boosting, non-linear SVMs, feature hashing, etc. Managers and developers need to be able to interact to
agree that a classiﬁcation system is behaving appropriately.
More speciﬁcally, systems like SEDC may become a critical component of the iterative process for
improving document classiﬁcation models. As the case study and the newsgroup study showed, SEDC can
identify data quality issues and model deﬁciencies. These deﬁciencies can be resolved via various mechanisms, leading to improved models directly or, alternatively, to improved data quality, which ultimately
should lead to better model performance and decision making.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
This paper has not provided a rigorous study of the insight provided by the explanations. The case studies
show that the method is capable of providing improved understanding of the inner workings of the classiﬁer,
and better understanding of the domain of application. It would be fascinating future work to examine the
changes in the decision makers’ judgment after having been presented with such explanations.
In this paper we have focused speciﬁcally on document classiﬁcation. We conjecture that these techniques
also will be quite useful in other high-dimensional classiﬁcation problems, which are becoming increasingly
important to modern business. For example, it may not be obvious, but classifying web users based on the
web pages they visit could be cast in the same framework as document classiﬁcation.
Each user can be represented by a set of webpage URLs from an extremely large set (billions). Users are
classiﬁed by models over this vocabulary. Understanding their classiﬁcations is directly analogous to the
problem addressed in this paper. Similarly, the problem of classifying bank customers for targeted marketing
based on the parties with which they transact also can be formulated similary.
The “documents” are the customers and the “words” are the payment receivers. In both of these additional
domains, being able to understand the individual classiﬁcations would have the same beneﬁts shown in
the extended gap model. However, the technique would not necessarily apply to every high-dimensional
classiﬁcation problem. It is necessary that the individual dimensions (and small subsets thereof) can be
interpretable. So, in the aforementioned web-user classiﬁcation example, if the URLs were irreversibly
hashed for privacy reasons, prior to forming the classiﬁcation model, then the techniques introduced in this
paper would not provide useful explanations.
Conclusion
The business problem this paper addresses is to enhance the understanding of a document classiﬁcation
model such that (1) the manager using it understands how decisions are being made, (2) the customers
affected by the decisions can be advised why a certain action regarding them is taken, and (3) the data
science/development team can improve the model iteratively. Further, (4) document classiﬁcation explanations can provide better understanding of the business domain. The 7-gap extension to Kayande’s 3-gap
framework formalizes these different roles, and shows how explanations can reduce the corresponding gaps
between the users’ mental model(s) and the decision system in both directions, and also can reduce the gap
between the decision system and reality, as the developers use the explanations to help improve the model.
Martens and Provost: Explaining Data-Driven Document Classiﬁcations
We found that global explanations in the form of a decision tree or a list of the most indicative words
do not provide a satisfactory solution. Moreover, previously proposed explanation methods on the datainstance level are not able to deal the huge dimensionality of document classiﬁcation problems. With the
technical constraints of high-dimensional data in mind, we addressed this business problem by creating an
explanation as a “necessary” set of words: a minimal set such that after removal the current classiﬁcation
would no longer be made. The presented search algorithm (SEDC) for ﬁnding such explanations is optimal
for linear binary-classiﬁcation models, and heuristic for non-linear models.
In terms of effectiveness, the results show that the explanations are quite concise and comprehensible,
comprising a few to a few dozen words (a very small portion of the overall vocabulary). The words in the
explanations vary greatly across the explanations, even with words in different languages, which supports
the claim that existing global explanations are inadequate for such document classiﬁcation domains.
We hope that this new sort of instance-level explanation for document classiﬁcation will provide an
immediately useful method across a wide variety of business (and scientiﬁc, medical, and legal) applications where document classiﬁcations are critical. We also hope we have made the case that thinking about
explanations in this way opens up a large number of new research problems and opportunities for improving
the state of the art in building and using data-driven document classiﬁcation systems.
Acknowledgments
Thanks to the anonymous reviewers and editors for very constructive comments, which substantially
improved the paper. We extend our gratitude to AdSafe Media and Josh Attenberg for many discussions
into the problem of safe advertising. This particular data set was not necessarily used in the development of
any production model used for safe advertising. Foster Provost also thanks NEC for a Faculty Fellowship.