Manipulating Machine Learning: Poisoning Attacks
and Countermeasures for Regression Learning
Matthew Jagielski∗, Alina Oprea∗, Battista Biggio †, Chang Liu‡, Cristina Nita-Rotaru∗, and Bo Li‡
∗Northeastern University, Boston, MA
†Univ. Cagliary, Cagliary, Italy
‡UC Berkeley, Berkeley, CA
Abstract—As machine learning becomes widely used for automated decisions, attackers have strong incentives to manipulate
the results and models generated by machine learning algorithms.
In this paper, we perform the ﬁrst systematic study of poisoning
attacks and their countermeasures for linear regression models. In poisoning attacks, attackers deliberately inﬂuence the
training data to manipulate the results of a predictive model.
We propose a theoretically-grounded optimization framework
speciﬁcally designed for linear regression and demonstrate its
effectiveness on a range of datasets and models. We also introduce
a fast statistical attack that requires limited knowledge of the
training process. Finally, we design a new principled defense
method that is highly resilient against all poisoning attacks. We
provide formal guarantees about its convergence and an upper
bound on the effect of poisoning attacks when the defense is
deployed. We evaluate extensively our attacks and defenses on
three realistic datasets from health care, loan assessment, and
real estate domains.
I. INTRODUCTION
As more applications with large societal impact rely on
machine learning for automated decisions, several concerns
have emerged about potential vulnerabilities introduced by machine learning algorithms. Sophisticated attackers have strong
incentives to manipulate the results and models generated
by machine learning algorithms to achieve their objectives.
For instance, attackers can deliberately inﬂuence the training
dataset to manipulate the results of a predictive model (in
poisoning attacks , – , , , ), cause misclassiﬁcation of new data in the testing phase (in evasion
attacks , , , , , , ) or infer private
information on training data (in privacy attacks , ,
 ). Several experts from academia and industry highlighted
the importance of considering these vulnerabilities in designing machine learning systems in a recent hearing held by the
Senate Subcommittee on Space, Science, and Competitiveness
entitled “The Dawn of AI” . The ﬁeld of adversarial
machine learning studies the effect of such attacks against
machine learning models and aims to design robust defense
algorithms . A comprehensive survey can be found in .
We consider the setting of poisoning attacks here, in which
attackers inject a small number of corrupted points in the
training process. Such poisoning attacks have been practically
demonstrated in worm signature generation , , spam
ﬁlters , DoS attack detection , PDF malware classiﬁcation , handwritten digit recognition , and sentiment
analysis . We argue that these attacks become easier to
mount today as many machine learning models need to be
updated regularly to account for continuously-generated data.
Such scenarios require online training, in which machine
learning models are updated based on new incoming training
data. For instance, in cyber-security analytics, new Indicators
of Compromise (IoC) rise due to the natural evolution of
malicious threats, resulting in updates to machine learning
models for threat detection . These IoCs are collected
from online platforms like VirusTotal, in which attackers can
also submit IoCs of their choice. In personalized medicine,
it is envisioned that patient treatment is adjusted in realtime by analyzing information crowdsourced from multiple
participants . By controlling a few devices, attackers can
submit fake information (e.g., sensor measurements), which
is then used for training models applied to a large set of patients. Defending against such poisoning attacks is challenging
with current techniques. Methods from robust statistics (e.g,
 , ) are resilient against noise but perform poorly on
adversarially-poisoned data, and methods for sanitization of
training data operate under restrictive adversarial models .
One fundamental class of supervised learning is linear
regression. Regression is widely used for prediction in many
settings (e.g., insurance or loan risk estimation, personalized
medicine, market analysis). In a regression task a numerical
response variable is predicted using a number of predictor
variables, by learning a model that minimizes a loss function.
Regression is powerful as it can also be used for classiﬁcation
tasks by mapping numerical predicted values into class labels.
Assessing the real impact of adversarial manipulation of
training data in linear regression, as well as determining how
to design learning algorithms resilient under strong adversarial
models is not yet well understood.
In this paper, we conduct the ﬁrst systematic study of
poisoning attacks and their countermeasures for linear regression models. We make the following contributions: (1)
we are the ﬁrst to consider the problem of poisoning linear
regression under different adversarial models; (2) starting
from an existing baseline poisoning attack for classiﬁcation,
we propose a theoretically-grounded optimization framework
speciﬁcally tuned for regression models; (3) we design a
fast statistical attack that requires minimal knowledge on
the learning process; (4) we propose a principled defense
algorithm with signiﬁcantly increased robustness than known
methods against a large class of attacks; (5) we extensively
2018 IEEE Symposium on Security and Privacy
© 2018, Matthew Jagielski. Under license to IEEE.
DOI 10.1109/SP.2018.00057
evaluate our attacks and defenses on four regression models
(OLS, LASSO, ridge, and elastic net), and on several datasets
from different domains, including health care, loan assessment,
and real estate. We elaborate our contributions below.
• On the attack dimension, we are the ﬁrst to consider the
problem of poisoning attacks against linear regression models.
Compared to classiﬁcation poisoning, in linear regression the
response variables are continuous and their values also can be
selected by the attacker. First, we adapt an existing poisoning
attack for classiﬁcation into a baseline regression attack.
Second, we design an optimization framework for regression
poisoning in which the initialization strategy, the objective
function, and the optimization variables can be selected to
maximize the attack’s impact on a particular model and
dataset. Third, we introduce a fast statistical attack that is
motivated by our theoretical analysis and insights. We ﬁnd
that optimization-based attacks are in general more effective
than statistical-based techniques, at the expense of higher
computational overhead and more information required by the
adversary on the training process.
• On the defense axis, we propose a principled approach to
constructing a defense algorithm called TRIM, which provides
high robustness and resilience against a large class of poisoning attacks. The TRIM method estimates the regression
parameters iteratively, while using a trimmed loss function
to remove points with large residuals. After few iterations,
TRIM is able to isolate most of the poisoning points and
learn a robust regression model. TRIM performs signiﬁcantly
better and is much more effective in providing robustness
compared to known methods from robust statistics (Huber 
and RANSAC ), typically designed to provide resilience
against noise and outliers. In contrast to these methods, TRIM
is resilient to poisoned points with similar distribution as the
training set. TRIM also outperforms other robust regression
algorithms designed for adversarial settings (e.g., Chen et
al. and RONI ). We provide theoretical guarantees
on the convergence of the algorithm and an upper bound on
the model Mean Squared Error (MSE) generated when a ﬁxed
percentage of poisoned data is included in the training set.
• We evaluate our novel attacks and defenses extensively on
four linear regression models and three datasets from health
care, loan assessment, and real estate domains. First, we
demonstrate the signiﬁcant improvement of our attacks over
the baseline attack of Xiao et al. in poisoning all models and
datasets. For instance, the MSEs of our attacks are increased
by a factor of 6.83 compared to the Xiao et al. attack, and
a factor of 155.7 compared to unpoisoned regression models.
In a case study health application, we ﬁnd that our attacks
can cause devastating consequences. The optimization attack
causes 75% of patients’ Warfarin medicine dosages to change
by an average of 93.49%, while one tenth of these patients
have their dosages changed by 358.89%. Second, we show
that our defense TRIM is also signiﬁcantly more robust than
existing methods against all the attacks we developed. TRIM
achieves MSEs within 1% of the unpoisoned model MSEs.
TRIM achieves MSEs much lower than existing methods,
improving Huber by a factor of 1295.45, RANSAC by a factor
of 75, and RONI by a factor of 71.13.
Outline. We start by providing background on regression
learning, as well as introducing our system and adversarial
model in Section II. We describe the baseline attack adapted
from Xiao et al. , and our new poisoning attacks in
Section III. Subsequently, we introduce our novel defense
algorithm TRIM in Section IV. Section V includes a detailed
experimental analysis of our attacks and defenses, as well as
comparison with previous methods. Finally, we present related
work in Section VI and conclude in Section VII.
II. SYSTEM AND ADVERSARIAL MODEL
Linear regression is at the basis of machine learning .
It is widely studied and applied in many applications due to
its efﬁciency, simplicity of use, and effectiveness. Other more
advanced learning methods (e.g., logistic regression, SVM,
neural networks) can be seen as generalizations or extensions
of linear regression. We systematically study the effect of
poisoning attacks and their defenses for linear regression. We
believe that our understanding of the resilience of this fundamental class of learning model to adversaries will enable future
research on other classes of supervised learning methods.
Problem deﬁnition. Our system model is a supervised setting
consisting of a training phase and a testing phase as shown
in Figure 1 on the left (“Ideal world”). The learning process
includes a data pre-processing stage that performs data cleaning and normalization, after which the training data can be
represented, without loss of generality, as Dtr = {(xi, yi)}n
where xi ∈ d are d-dimensional numerical predictor
variables (or feature vectors) and yi ∈ are numerical
response variables, for i ∈{1, . . . , n}. After that, the learning
algorithm is applied to generate the regression model at the end
of the training phase. In the testing phase, the model is applied
to new data after pre-processing, and a numerical predicted
value is generated using the regression model learned in
training. Our model thus captures a standard multi-dimensional
regression setting applicable to different prediction tasks.
In linear regression, the model output at the end of the
training stage is a linear function f(x, θ) = w⊤x + b, which
predicts the value of y at x. This function is parametrized
by a vector θ = (w, b) ∈Rd+1 consisting of the feature
weights w ∈Rd and the bias b ∈R. Note that regression is
substantially different from classiﬁcation, as the y values are
numerical, rather than being a set of indices (each denoting a
different class from a predetermined set). The parameters of
f are chosen to minimize a quadratic loss function:
L(Dtr, θ) = 1
i=1 (f(xi, θ) −yi)2
MSE(Dtr,θ)
where the Mean Squared Error MSE(Dtr, θ) measures the
error in the predicted values assigned by f(·, θ) to the training
samples in Dtr as the sum of squared residuals, Ω(w) is
a regularization term penalizing large weight values, and λ
Fig. 1: System architecture.
is the so-called regularization parameter. Regularization is
used to prevent overﬁtting, i.e., to preserve the ability of
the learning algorithm to generalize well on unseen (testing)
data. For regression problems, this capability, i.e., the expected
performance of the trained function f on unseen data, is
typically assessed by measuring the MSE on a separate test
set. Popular linear regression methods differ mainly in the
choice of the regularization term. In particular, we consider
four models in this paper:
1) Ordinary Least Squares (OLS), for which Ω(w) = 0
(i.e., no regularization is used);
2) Ridge regression, which uses ℓ2-norm regularization
3) LASSO, which uses ℓ1-norm regularization Ω(w) =
4) Elastic-net regression, which uses a combination of ℓ1norm and ℓ2-norm regularization Ω(w) = ρ∥w∥1+(1−
2, where ρ ∈(0, 1) is a conﬁgurable parameter,
commonly set to 0.5 (as we do in this work).
When designing a poisoning attack, we consider two metrics for quantifying the effectiveness of the attack. First,
we measure the success rate of the poisoning attack by the
difference in testing set MSE of the corrupted model compared
to the legitimate model (trained without poisoning). Second,
we consider the running time of the attack.
A. Adversarial model
We provide here a detailed adversarial model for poisoning
attacks against regression algorithms, inspired from previous
work in , , , . The model consists of deﬁning
the adversary’s goal, knowledge of the attacked system, and
capability of manipulating the training data, to eventually
deﬁne an optimal poisoning attack strategy.
Adversary’s Goal. The goal of the attacker is to corrupt
the learning model generated in the training phase, so that
predictions on new data will be modiﬁed in the testing phase.
The attack is considered a poisoning availability attack, if
its goal is to affect prediction results indiscriminately, i.e., to
cause a denial of service. It is instead referred to as a poisoning
integrity attack, if the goal is to cause speciﬁc mis-predictions
at test time, while preserving the predictions on the other test
samples. This is a similar setting to that of backdoor poisoning
attacks recently reported in classiﬁcation settings , .
Adversary’s Knowledge. We assume here two distinct attack
scenarios, referred to as white-box and black-box attacks in
the following. In white-box attacks, the attacker is assumed
to know the training data Dtr, the feature values x, the
learning algorithm L, and even the trained parameters θ.
These attacks have been widely considered in previous work,
although mainly against classiﬁcation algorithms , ,
 . In black-box attacks, the attacker has no knowledge
of the training set Dtr but can collect a substitute data set
tr. The feature set and learning algorithm are known, while
the trained parameters are not. However, the latter can be
estimated by optimizing L on the substitute data set D′
setting is useful to evaluate the transferability of poisoning
attacks across different training sets, as discussed in , .
Adversary’s Capability. In poisoning attacks, the attacker
injects poisoning points into the training set before the regression model is trained (see the right side of Figure 1 labeled
“Adversarial world”). The attacker’s capability is normally
limited by upper bounding the number p of poisoning points
that can be injected into the training data, whose feature values
and response variables are arbitrarily set by the attacker within
a speciﬁed range (typically, the range covered by the training
data, i.e., in our case) , . The total number
of points in the poisoned training set is thus N = n + p,
with n being the number of pristine training samples. We
then deﬁne the ratio α = p/n, and the poisoning rate as the
actual fraction of the training set controlled by the attacker,
i.e., n/N = α/(1 + α). In previous work, poisoning rates
higher than 20% have been only rarely considered, as the
attacker is typically assumed to be able to control only a small
fraction of the training data. This is motivated by application
scenarios such as crowdsourcing and network trafﬁc analysis,
in which attackers can only reasonably control a small fraction
of participants and network packets, respectively. Moreover,
learning a sufﬁciently-accurate regression function in the presence of higher poisoning rates would be an ill-posed task, if
not infeasible at all , , , , , .
Poisoning Attack Strategy. All the aforementioned poisoning
attack scenarios, encompassing availability and integrity violations under white-box or black-box knowledge assumptions,
can be formalized as a bilevel optimization problem , .
For white-box attacks, this can be written as:
p ∈arg minθ L(Dtr ∪Dp, θ) .
The outer optimization amounts to selecting the poisoning
points Dp to maximize a loss function W on an untainted
data set D′ (e.g., a validation set which does not contain any
poisoning points), while the inner optimization corresponds to
retraining the regression algorithm on a poisoned training set
including Dp. It should be clear that θ⋆
p depends implicitly on
the set Dp of poisoning attack samples through the solution of
the inner optimization problem. In poisoning integrity attacks,
the attacker’s loss W can be evaluated only on the points of
interest (for which the attacker aims to cause mis-predictions at
test time), while in poisoning availability attacks it is computed
on an untainted set of data points, indiscriminately. In the
black-box setting, the poisoned regression parameters θ⋆
estimated using the substitute training data D′
tr instead of Dtr.
In the remainder of this work, we only focus on poisoning
availability attacks against regression learning, and on defending against them, as those have been mainly investigated in the
literature of poisoning attacks. We highlight anyway again that
poisoning integrity attacks can be implemented using the same
technical derivation presented in this work, and leave a more
detailed investigation of their effectiveness to future work.
III. ATTACK METHODOLOGY
previously-proposed
gradient-based optimization approaches to solving Problem (2)-(3) in classiﬁcation settings. In Sect. III-A, we discuss
how to adapt them to the case of regression learning, and
propose novel strategies to further improve their effectiveness.
Notably, since these attacks have been originally proposed
in the context of classiﬁcation problems, the class label of
the attack sample is arbitrarily initialized and then kept ﬁxed
during the optimization procedure (recall that y is a categorical
variable in classiﬁcation). As we will demonstrate in the
remainder of this work, a signiﬁcant improvement we propose
here to the current attack derivation is to simultaneously optimize the response variable of each poisoning point along with
its feature values. We subsequently highlight some theoretical
insights on how each poisoning sample is updated during
the gradient-based optimization process. This will lead us
to develop a much faster attack, presented in Sect. III-B,
which only leverages some statistical properties of the data
and requires minimal black-box access to the targeted model.
A. Optimization-based Poisoning Attacks
Previous work has considered solving Problem (2)-(3) by
iteratively optimizing one poisoning sample at a time through
gradient ascent , , , . An exemplary algorithm
is given as Algorithm 1. We denote with xc the feature
vector of the attack point being optimized, and with yc its
response variable (categorical for classiﬁcation problems). In
particular, in each iteration, the algorithm optimizes all points
in Dp, by updating their feature vectors one at a time. As
reported in , the vector xc can be updated through a line
search along the direction of the gradient ∇xcW of the outer
objective W (evaluated at the current poisoned solution) with
respect to the poisoning point xc (cf. line 7 in Algorithm 1).
Note that this update step should also enforce xc to lie within
the feasible domain (e.g., xc ∈ d), which can be typically
achieved through simple projection operators , , .
The algorithm terminates when no sensible change in the outer
objective W is observed.
Gradient Computation. The aforementioned algorithm is essentially a standard gradient-ascent algorithm with line search.
Algorithm 1 Poisoning Attack Algorithm
Input: D = Dtr (white-box) or D′
tr (black-box), D′, L, W,
the initial poisoning attack samples D(0)
= (xc, yc)p
small positive constant ε.
1: i ←0 (iteration counter)
2: θ(i) ←arg minθ L(D ∪D(i)
w(i) ←W(D′, θ(i))
θ(i+1) ←θ(i)
for c = 1, . . . , p do
←line search
xc(i), ∇xcW(D′, θ(i+1))
θ(i+1) ←arg minθ L(D ∪D(i+1)
w(i+1) ←W(D′, θ(i+1))
11: until |w(i) −w(i−1)| < ε
Output: the ﬁnal poisoning attack samples Dp ←D(i)
The challenging part is understanding how to compute the
required gradient ∇xcW(D′, θ), as this has to capture the
implicit dependency of the parameters θ of the inner problem
on the poisoning point xc. Indeed, assuming that W does not
depend directly on xc, but only through θ, we can compute
∇xcW(D′, θ) using the chain rule as:
∇xcW = ∇xcθ(xc)⊤· ∇θW ,
where we have made explicit that θ depends on xc. While the
second term is simply the derivative of the outer objective with
respect to the regression parameters, the ﬁrst one captures the
dependency of the solution θ of the learning problem on xc.
We focus now on the computation of the term ∇xcθ(xc).
While for bilevel optimization problems in which the inner
problem is not convex (e.g., when the learning algorithm is
a neural network) this requires efﬁcient numerical approximations , when the inner learning problem is convex, the
gradient of interest can be computed in closed form. The underlying trick is to replace the inner learning problem (Eq. 3)
with its Karush-Kuhn-Tucker (KKT) equilibrium conditions,
i.e., ∇θL(D′
tr ∪Dp, θ) = 0, and require such conditions to
remain valid while updating xc , , , . To this
end, we simply impose that their derivative with respect to xc
remains at equilibrium, i.e., ∇xc (∇θL(D′
tr ∪Dp, θ)) = 0.
Now, it is clear that the function L depends explicitly on
xc in its ﬁrst argument, and implicitly through the regression
parameters θ. Thus, differentiating again with the chain rule,
one yields the following linear system:
∇xc∇θL + ∇xcθ⊤· ∇2
Finally, solving for ∇xcθ, one yields:
For the speciﬁc form of L given in Eq. (1), it is not difﬁcult
to see that the aforementioned derivative becomes equal to
that reported in (except for a factor of 2 arising from a
different deﬁnition of the quadratic loss).
  Σ + λg
i xi, and M = xcw⊤+
(f(xc) −yc) Id. As in , the term g is zero for OLS and
LASSO, the identity matrix Id for ridge regression, and (1 −
ρ)Id for the elastic net.
Objective Functions. In previous work, the main objective
used for W has been typically a loss function computed on
an untainted validation set Dval = {(x′
i=1 , ,
 . Notably, only Xiao et al. have used a regularized
loss function computed on the training data (excluding the
poisoning points) as a proxy to estimate the generalization
error on unseen data. The rationale was to avoid the attacker
to collect an additional set of points. In our experiments, we
consider both possibilities, always using the MSE as the loss
Wtr(Dtr, θ) = 1
i=1 (f(xi, θ) −yi)2 + λΩ(w) ,
Wval(Dval, θ) = 1
The complete gradient ∇xcW (Eq. 4) for these two objectives
can thus be computed by multiplying Eq. (7) respectively to:
i=1(f(xi) −yi)xi + λ ∂Ω
i=1(f(xi) −yi)
j=1(f(xj) −yj)xj
j=1(f(xj) −yj)
Initialization strategies. We discuss here how to select the
initial set Dp of poisoning points to be passed as input to the
gradient-based optimization algorithm (Algorithm 1). Previous
work on poisoning attacks has only dealt with classiﬁcation
problems , , , . For this reason, the initialization
strategy used in all previously-proposed approaches has been
to randomly clone a subset of the training data and ﬂip their
labels. Dealing with regression opens up different avenues. We
therefore consider two initialization strategies in this work. In
both cases, we still select a set of points at random from the
training set Dtr, but then we set the new response value yc of
each poisoning point in one of two ways: (i) setting yc = 1−y,
and (ii) setting yc = round(1 −y), where round rounds to
the nearest 0 or 1 value (recall that the response variables are
in ). We call the ﬁrst technique Inverse Flipping (InvFlip)
and the second Boundary Flipping (BFlip). Worth remarking,
we experimented with many techniques for selecting the
feature values before running gradient descent, and found that
surprisingly they do not have signiﬁcant improvement over a
simple uniform random choice. We thus report results only for
the two aforementioned initialization strategies.
Baseline Gradient Descent (BGD) Attack. We are now in
a position to deﬁne a baseline attack against which we will
compare our improved attacks. In particular, as no poisoning
attack has ever been considered in regression settings, we
deﬁne as the baseline poisoning attack an adaptation from
the attack by Xiao et al. . In particular, as in Xiao et
al. , we select Wtr as the outer objective. To simulate label
ﬂips in the context of regression, we initialize the response
variables of the poisoning points with the InvFlip strategy.
We nevertheless test all the remaining three combinations of
initialization strategies and outer objectives in our experiments.
Response Variable Optimization. This work is the ﬁrst to
consider poisoning attacks in regression settings. Within this
context, it is worth remarking that response variables take on
continuous values rather than categorical ones. Based on this
observation, we propose here the ﬁrst poisoning attack that
jointly optimizes the feature values xc of poisoning attacks
and their associated response variable yc. To this end, we
extend the previous gradient-based attack by considering the
optimization of zc = (xc, yc) instead of only considering xc.
This means that all previous equations remain valid provided
that we substitute ∇zc to ∇xc. This clearly requires expanding
∇xcθ by also considering derivatives with respect to yc:
and, accordingly, modify Eq. (7) as
  Σ + λg
The derivatives given in Eqs. (10)-(12) remain clearly unchanged, and can be pre-multiplied by Eq. (14) to obtain
the ﬁnal gradient ∇zcW. Algorithm 1 can still be used to
implement this attack, provided that both xc and yc are
updated along the gradient ∇zcW (cf. Algorithm 1, line 7).
Theoretical Insights. We discuss here some theoretical insights on the bilevel optimization of Eqs. (2)-(3), which
will help us to derive the basis behind the statistical attack
introduced in the next section. To this end, let us ﬁrst consider
as the outer objective a non-regularized version of Wtr, which
can be obtained by setting λ = 0 in Eq. (8). As we will see,
in this case it is possible to compute simpliﬁed closed forms
for the required gradients. Let us further consider another
objective denoted with W′
tr, which, instead of optimizing the
loss, optimizes the difference in predictions from the original,
unpoisoned model θ′:
i=1(f(xi, θ) −f(xi, θ′))2.
In Appendix A, we show that Wtr and W′
tr are interchangeable for our bilevel optimization problem. In particular,
differentiating W′
tr with respect to zc = (xc, yc) gives:
n(f(xc, θ) −f(xc, θ′))(w0 −2w)⊤
n(f(xc, θ) −f(xc, θ′)).
The update rules deﬁned by these gradients have nice interpretation. We see that ∂W′
∂yc will update yc to be further away
from the original line than it was in the previous iteration.
This is intuitive, as a higher distance from the line will push
the line further in that direction. The update for xc is slightly
more difﬁcult to understand, but by separating (w0−2w) into
(−w)+(w0−w), we see that the xc value is being updated in
two directions summed together. The ﬁrst is perpendicularly
away from the regression line (like the yc update step, the
poison point should be as far as possible from the regression
line). The second is parallel to the difference between the
original regression line and the poisoned regression line (it
should keep pushing in the direction it has been going). This
gives us an intuition for how the poisoning points are being
updated, and what an optimal poisoning point looks like.
B. Statistical-based Poisoning Attack (StatP)
Motivated by the aforementioned theoretical insights, we
design a fast statistical attack that produces poisoned points
with similar distribution as the training data. In StatP, we
simply sample from a multivariate normal distribution with
the mean and covariance estimated from the training data.
Once we have generated these points, we round the feature
values to the corners, exploiting the observation that the most
effective poisoning points are near corners. Finally, we select
the response variable’s value at the boundary (either 0 or 1)
to maximize the loss.
Note that, importantly, the StatP attack requires only blackbox access to the model, as it needs to query the model to
ﬁnd the response variable (before performing the boundary
rounding). It also needs minimal information to be able to
sample points from the training set distribution. In particular,
StatP requires an estimate of the mean and co-variance of
the training data. However, StatP is agnostic to the exact
regression algorithm, its parameters, and training set. Thus,
it requires much less information on the training process than
the optimization-based attacks. It is signiﬁcantly faster than
optimization-based attacks, though slightly less effective.
IV. DEFENSE ALGORITHMS
In this section, we describe existing defense proposals
against poisoning attacks, and explain why they may not be
effective under adversarial corruption in the training data. Then
we present a new approach called TRIM, speciﬁcally designed
to increase robustness against a range of poisoning attacks.
A. Existing defense proposals
Existing defense proposals can be classiﬁed into two categories: noise-resilient regression algorithms and adversariallyresilient defenses. We discuss these approaches below.
Noise-resilient regression. Robust regression has been extensively studied in statistics as a method to provide resilience
against noise and outliers , , , . The main idea
behind these approaches is to identify and remove outliers
from a dataset. For example, Huber uses an outlier-robust
loss function. RANSAC iteratively trains a model to ﬁt a
Fig. 2: Several iterations of the TRIM algorithm. Initial
poisoned data is in blue in top left graph. The top right graph
shows in red the initial randomly selected points removed
from the optimization objective. In the following two iterations
(bottom left and right graphs) the set of high-residual points
is reﬁned and the model becomes more robust.
subset of samples selected at random, and identiﬁes a training
sample as an outlier if the error when ﬁtting the model to the
sample is higher than a threshold.
While these methods provide robustness guarantees against
noise and outliers, an adversary can still generate poisoning
data that affects the trained model. In particular, an attacker
can generate poisoning points that are very similar to the true
data distribution (these are called inliers), but can still mislead
the model. Our new attacks discussed in Section III generate
poisoning data points which are akin to the pristine ones.
For example, in StatP the poisoned points are chosen from a
distribution that is similar to that of the training data (has the
same mean and co-variance). It turns out that these existing
regression methods are not robust against inlier attack points
chosen to maximally mislead the estimated regression model.
Adversarially-resilient
regression.
Previously
adversarially-resilient regression algorithms typically provide
guarantees under strong assumptions about data and noise
distribution. For instance, Chen et al. , assume that
the feature set matrix satisﬁes XT X = I and data has sub-
Gaussian distribution. Feng et al. assume that the data
and noise satisfy the sub-Gaussian assumption. Liu et al. 
design robust linear regression algorithms robust under the
assumption that the feature matrix has low rank and can be
projected to a lower dimensional space. All these methods
have provable robustness guarantees, but the assumptions on
which they rely are not usually satisﬁed in practice.
B. TRIM algorithm
In this section, we propose a novel defense algorithm
called TRIM with the goal of training a regression model
with poisoned data. At an intuitive level, rather than simply removing outliers from the training set, TRIM takes a
Algorithm 2 [TRIM algorithm]
1: Input: Training data D = Dtr
∪Dp with |D| = N;
number of attack points p = α · n.
2: Output: θ.
3: I(0) ←a random subset with size n of {1, ..., N}
4: θ(0) ←arg minθ L(I(0), θ) /* Initial estimation of θ*/
5: i ←0 /* Iteration count */
I(i) ←subset of size n that min. L(DI(i), θ(i−1))
θ(i) ←arg minθ L(DI(i), θ) /* Current estimator */
R(i) = L(DI(i), θ(i)) /* Current loss */
11: until i > 1 ∧R(i) = R(i−1) /* Convergence condition*/
12: return θ(i) /* Final estimator */.
principled approach. TRIM iteratively estimates the regression
parameters, while at the same time training on a subset of
points with lowest residuals in each iteration. In essence,
TRIM uses a trimmed loss function computed on a different
subset of residuals in each iteration. Our method is inspired
by techniques from robust statistics that use trimmed versions
of the loss function for robustness. Our main contribution
is to apply trimmed optimization techniques for regularized
linear regression in adversarial settings, and demonstrate their
effectiveness compared to other defenses on a range of models
and real-world datasets.
As in Section II, assume that the original training set is Dtr
of size n, the attacker injects p = α · n poisoned samples
Dp, and the poisoned training set D = Dtr ∪Dp is of size
N = (1 + α)n. We require that α < 1 to ensure that the
majority of training data is pristine (unpoisoned).
Our main observation is the following: we can train a linear
regression model only using a subset of training points of
size n. In the ideal case, we would like to identify all p
poisoning points and train the regression model based on the
remaining n legitimate points. However, the true distribution
of the legitimate training data is clearly unknown, and it is
thus difﬁcult to separate legitimate and attack points precisely.
To alleviate this, our proposed defense tries to identify a
set of training points with lowest residuals relative to the
regression model (these might include attack points as well,
but only those that are “close” to the legitimate points and
do not contribute much to poisoning the model). In essence,
our TRIM algorithm provides a solution to the following
optimization problem:
θ,I L(DI, θ)
s.t. I ⊂[1, . . . , N] ∧|I| = n .
We use the notation DI
to indicate the data samples
{(xi, yi) ∈D}i∈I. Thus, we optimize the parameter θ of
the regression model and the subset I of points with smallest
residuals at the same time. It turns out though that solving
this optimization problem efﬁciently is quite challenging. A
simple algorithm that enumerates all subsets I of size n of
the training set is computationally inefﬁcient. On the other
hand, if the true model parameters θ = (w, b) were known,
then we could simply select points in set I that have lowest
residual relative to θ. However, what makes this optimization
problem difﬁcult to solve is the fact that θ is not known, and
we do not make any assumptions on the true data distribution
or the attack points.
To address these issues, our TRIM algorithm learns parameter θ and distinguishes points with lowest residuals from
training set alternately. We employ an iterative algorithm
inspired by techniques such as alternating minimization or
expectation maximization . At the beginning of iteration
i, we have an estimate of parameter θ(i). We use this estimate
as a discriminator to identify all inliers, whose residual values
are the n smallest ones. We do not consider points with large
residuals (as they increase MSE), but use only the inliers
to estimate a new parameter θ(i+1). This process terminates
when the estimation converges and the loss function reaches a
minimum. The detailed algorithm is presented in Algorithm 2.
A graphical representation of three iterations of our algorithm
is given in Figure 2. As observed in the ﬁgure, the algorithm
iteratively ﬁnds the direction of the regression model that ﬁts
the true data distribution, and identiﬁes points that are outliers.
We provide provable guarantees on the convergence of
Algorithm 2 and the estimation accuracy of the regression
model it outputs. First, Algorithm 2 is guaranteed to converge
and thus it terminates in ﬁnite number of iterations, as stated
in the following theorem.
Theorem 1. Algorithm 2 terminates in a ﬁnite number of
iterations.
We do not explicitly provide a bound on the number of iterations needed for convergence, but it is always upper bounded
. However, our empirical evaluation demonstrates that
Algorithm 2 converges within few dozens of iterations at most.
We are next interested in analyzing the quality of the
estimated model computed from Algorithm 2 (adversarial
world) and how it relates to the pristine data (ideal world).
However, relating these two models directly is challenging
due to the iterative minimization used by Algorithm 2. We
overcome this by observing that Algorithm 2 ﬁnds a local
minimum to the optimization problem from (17). There is no
efﬁcient algorithm for solving (17) that guarantees the solution
to be the global minimum of the optimization problem.
It turns out that we can provide a guarantee about the
global minimum ˆθ of (17) on poisoned data (under worstcase adversaries) in relation to the parameter θ⋆learned by
the original model on pristine data. In particular, Theorem 2
shows that ˆθ “ﬁts well” to at least (1 −α) · n pristine data
samples. Notably, it does not require any assumptions on how
poisoned data is generated, thus it provides guarantees under
worst-case adversaries.
Theorem 2. Let Dtr denote the original training data, ˆθ the
global optimum for (17), and θ⋆= arg minθ L(Dtr, θ) the
estimator in the ideal world on pristine data. Assuming α < 1,
there exist a subset D′ ⊂Dtr of (1−α)·n pristine data samples
MSE(D′, ˆθ) ≤
L(Dtr, θ⋆) .
Note that the above theorem is stated without any assumptions on the training data distribution. This is one of the
main difference from prior work , , which assume
the knowledge of the mean and covariance of the legitimate
data. In practice, such information on training data is typically
unavailable. Moreover, an adaptive attacker can also inject
poisoning samples to modify the mean and covariance of
training data. Thus, our results are stronger than prior work in
relying on fewer assumptions.
We now give an intuitive explanation about the above
theorem, especially inequality (18). Since Dtr is assumed
to be the pristine dataset, and D′ is a subset of Dtr of
size (1 −α)n, we know all data in D′ is also pristine
(not corrupted by the adversary). Therefore, the stationary
assumption on pristine data distribution, which underpins all
machine learning algorithms, guarantees that MSE(Dtr, θ) is
close to MSE(D′, θ) regardless of the choices of θ and D′,
as long as α is small enough.
Next, we explain the left-hand side of inequality (18).
This is the MSE of a subset of pristine samples D′ using
ˆθ computed by the TRIM algorithm in the adversarial world.
Based on the discussion above, the left-hand side is close to the
MSE of the pristine data Dtr using the adversarially learned
estimator ˆθ. Thus, inequality (18) essentially provides an upper
bound on the worst-case MSE using the estimator ˆθ output by
Algorithm 2 from the poisoned data.
To understand what upper bound Theorem 2 guarantees, we
need to understand the right-hand side of inequality (18). We
use OLS regression (without regularization) as an example to
explain the intuition of the right-hand side. In OLS we have
L(Dtr, θ⋆) = MSE(Dtr, θ⋆), which is the MSE using the
“best-case” estimator computed in the ideal world. Therefore,
the right-hand side of inequality (18) is proportional to the
ideal world MSE, with a factor of (1+
1−α). When α ≤20%,
we notice that this factor is at most 1.25×.
Therefore, informally, Theorem 2 essentially guarantees
that, the ratio of the worst-case MSE by solving (18) computed
in the adversarial world over best-case MSE computed in ideal
world for a linear model is at most 1.25. Note that since
Algorithm 2 may not always ﬁnd the global minimum of (17),
we empirically examine this ratio of the worst-case to bestcase MSEs. Our empirical evaluation shows that in most of
our experiments, this ratio for TRIM is less than 1.01×, which
is much smaller than all existing defenses.
For other models whose loss function includes the regularizer term (Lasso, ridge, and elastic net), the right-hand side
of (18) includes the same term as well. This may allow the
blowup of the worst-case MSE in the adversarial world with
respect to the best-case MSE to be larger; however, we are
not aware of any technique to trigger this worst-case scenario,
and our empirical evaluation shows that the blowup is typically
less than 1% as mentioned above.
The proofs of Theorem 1 and 2 can be found in Appendix B.
V. EXPERIMENTAL EVALUATION
We implemented our attack and defense algorithms in
Python, using the numpy and sklearn packages. Our code is
available at We ran our
experiments on four 32 core Intel(R) Xeon(R) CPU E5-2440
v2 @ 1.90GHz machines. We parallelize our optimizationbased attack implementations to take advantage of the multicore capabilities. We use the standard cross-validation method
to split the datasets into 1/3 for training, 1/3 for testing, and
1/3 for validation, and report results as averages over 5 runs.
We use two main metrics for evaluating our algorithms: MSE
for the effectiveness of the attacks and defenses, and running
time for their cost.
We describe the datasets we used for our experiments in
Section V-A. We then systematically analyze the performance
of the new attacks and compare them against the baseline
attack algorithm in Section V-B. Finally, we present the results
of our new TRIM algorithm and compare it with previous
methods from robust statistics in Section V-C.
A. Datasets
We used three public regression datasets in our experimental
evaluation. We present some details and statistics about each
of them below.
Health care dataset. This dataset includes 5700 patients,
where the goal is to predict the dosage of anticoagulant drug
Warfarin using demographic information, indication for Warfarin use, individual VKORC1 and CYP2C9 genotypic data,
and use of other medications affected by related VKORC1
and CYP2C9 polymorphisms . As is standard practice for
studies using this dataset (see ), we only select patients
with INR values between 2 and 3. The INR is a ratio that
represents the amount of time it takes for blood to clot, with a
therapeutic range of 2-3 for most patients taking Warfarin. The
dataset includes 67 features, resulting in 167 features after onehot encoding categorical features and normalizing numerical
features as above.
Loan dataset. This dataset contains information regarding
loans made on the Lending Club peer-to-peer lending platform
 . The predictor variables describe the loan attributes,
including information such as total loan size, interest rate,
and amount of principal paid off, as well as the borrower’s
information, such as number of lines of credit, and state of
residence. The response variable is the interest rate of a loan.
Categorical features, such as the purpose of the loan, are onehot encoded, and numerical features are normalized into .
The dataset contains 887,383 loans, with 75 features before
pre-processing, and 89 after. Due to its large scale, we sampled
a set of 5000 records for our poisoning attacks.
House pricing dataset. This dataset is used to predict house
sale prices as a function of predictor variables such as square
footage, number of rooms, and location . In total, it
TABLE I: Best performing optimization attack OptP for Ridge
and LASSO regression.
includes 1460 houses and 81 features. We preprocess by onehot encoding all categorical features and normalize numerical
features, resulting in 275 total features.
B. New poisoning attacks
In this section, we perform experiments on the three regression datasets (health care, loan, and house pricing) to evaluate
the newly proposed attacks, and compare them against the
baseline BGD for four regression models. For each dataset
we select a subset of 1400 records (this is the size of the house
dataset, and we wanted to use the same number of records
for all datasets). We use MSE as the metric for assessing
the effectiveness of an attack, and also measure the attacks’
running times. We vary the poisoning rate between 4% and
20% at intervals of 4% with the goal of inferring the trend in
attack success. More details about hyperparameter setting are
presented in Appendix C.
Figures 3 and 4 show the MSE of each attack for ridge
and LASSO regression. We picked these two models as they
are the most popular linear regression models. We plot the
baseline attack BGD, statistical attack StatP, as well as our
best performing optimization attack (called OptP). Details on
OptP are given in Table I. Additional results for the Contagio
PDF classiﬁcation dataset are given in Appendix C.
Below we pose several research questions to elucidate the
beneﬁts, and in some cases limitations, of these attacks.
1) Question 1: Which optimization strategies are most effective for poisoning regression?: Our results conﬁrm that the
optimization framework we design is effective at poisoning
different models and datasets. Our new optimization attack
OptP improves upon the baseline BGD attack by a factor of
6.83 in the best case. The OptP attack could achieve MSEs
by a factor of 155.7 higher than the original models.
As discussed in Section III, our optimization framework
has several instantiations, depending on: (1) The initialization
strategy (InvFlip or BFlip); (2) The optimization variable (x
or (x, y)); and (3) The objective of the optimization (Wtr
or Wval). For instance, BGD is given by (InvFlip, x, Wtr).
We show that each of these dimensions has an important
effect in generating successful attacks. Table I shows the best
optimization attack for each model and dataset, while Tables II
and III provide examples of different optimization attacks for
LASSO on the loan and house datasets, respectively.
We highlight several interesting observations. First, boundary ﬂip BFlip is the preferred initialization method, with
only one case (LASSO regression on house dataset) in which
InvFlip performs better in combination with optimizing (x, y)
Poisoning rates
TABLE II: MSEs of optimization attacks for LASSO on loan
data. BGD is the ﬁrst row.
Poisoning rates
TABLE III: MSEs of optimization attacks for LASSO on
house data. BGD is the ﬁrst row.
under objective Wval. For instance, in LASSO on house
dataset, BFlip alone can achieve a factor of 3.18 higher MSE
than BGD using InvFlip. In some cases the optimization by
y can achieve higher MSEs even starting with non-optimal
y values as the gradient ascent procedure is very effective
(see for example the attack (InvFlip, (x, y), Wval) in Table III).
However, the combination of optimization by x with InvFlip
initialization (as used by BGD) is outperformed in all cases
by either BFlip or (x, y) optimization.
Second, using both (x, y) as optimization arguments is most
effective compared to simply optimizing by x as in BGD. Due
to the continuous response variables in regression, optimizing
by y plays a large role in making the attacks more effective.
For instance, optimizing by (x, y) with BFlip initialization and
Wval achieves a factor of 6.83 improvement in MSE compared
to BGD on house dataset with LASSO regression.
Third, the choice of the optimization objective is equally
important for each dataset and model. Wval can improve over
Wtr by a factor of 7.09 (on house for LASSO), by 17.5% (on
loan for LASSO), and by 30.4% (on loan for ridge) when the
initialization points and optimization arguments are the same.
Thus, all three dimensions in our optimization framework
are inﬂuential in improving the success of the attack. The
optimal choices are dependent on the data distribution, such
as feature types, sparsity of the data, ratio of records over data
dimension, and data linearity. In particular, we noticed that for
non-linear datasets (such as loan), the original MSE is already
high before the attack and all the attacks that we tested perform
worse than in cases when the legitimate data ﬁts a linear model
(i.e., it is close to the regression hyperplane). The reason may
be that, in the latter case, poisoning samples may be shifted
farther away from the legitimate data (i.e., from the regression
hyperplane), and thus have a greater impact than in the former
case, when the legitimate data is already more evenly and nonlinearly distributed in feature space. Nevertheless, our attacks
are able to successfully poison a range of models and datasets.
2) Question 2: How do optimization and statistical attacks compare in effectiveness and performance?: In general,
(a) Health Care Dataset
(b) Loan Dataset
(c) House Price Dataset
Fig. 3: MSE of attacks on ridge regression on the three datasets. Our new optimization (OptP) and statistical (StatP) attacks
are more effective than the baseline. OptP is best optimization attack according to Table I.
(a) Health Care Dataset
(b) Loan Dataset
(c) House Price Dataset
Fig. 4: MSE of attacks on LASSO on the three datasets. As for ridge, we ﬁnd that StatP and OptP are able to poison the
dataset very effectively, outperforming the baseline (BGD). OptP is best optimization attack according to Table I.
optimization-based attacks (BGD and OptP) outperform the
statistical-based attack StatP in effectiveness. This is not
surprising to us, as StatP uses much less information about the
training process to determine the attack points. Interestingly,
we have one case (LASSO regression on loan dataset) in which
StatP outperforms the best optimization attack OptP by 11%.
There are also two instances on ridge regression (health and
loan datasets) in which StatP and OptP perform similarly.
These cases show that StatP is a reasonable attack when the
attacker has limited knowledge about the learning system.
The running time of optimization attacks is proportional
to the number of iterations required for convergence. On
the highest-dimensional dataset, house prices, we observe
OptP taking about 337 seconds to complete for ridge and
408 seconds for LASSO. On the loan dataset, OptP ﬁnishes
LASSO poisoning in 160 seconds on average. As expected,
the statistical attack is extremely fast, with running times on
the order of a tenth of a second on the house dataset and a
hundredth of a second on the loan dataset to generate the same
number of points as OptP. Therefore, our attacks exhibit clear
tradeoffs between effectiveness and running times, with optimization attacks being more effective than statistical attacks,
at the expense of higher computational overhead.
3) Question 3: What is the potential damage of poisoning
in real applications?: We are interested in understanding the
effect of poisoning attacks in real applications, and perform a
case study on the health-care dataset. Speciﬁcally, we translate
the MSE results obtained with our attacks into application
speciﬁc parameters. In the health care application, the goal
is to predict medicine dosage for the anticoagulant drug
Warfarin. In Table IV, we show ﬁrst statistics on the medicine
dosage predicted by the original regression models (without
poisoning), and then the absolute difference in the amount of
dosage prescribed after the OptP poisoning attack. We ﬁnd
that all linear regression models are vulnerable to poisoning,
with 75% of patients having their dosage changed by 93.49%,
and half of patients having their dosage changed by 139.31%
on LASSO. For 10% of patients, the increase in MSE is
devastating to a maximum of 359% achieved for LASSO
regression. These results are for 20% poisoning rate, but it
turns out that the attacks are also effective at smaller poisoning
rates. For instance, at 8% poisoning rate, the change in dosage
is 75.06% for half of patients.
Thus, the results demonstrate the effectiveness of our new
poisoning attacks that induce signiﬁcant changes to the dosage
of most patients with a small percentage of poisoned points
added by the attacker.
4) Question 4: What are the transferability properties of
our attacks?: Our transferability analysis for poisoning attacks
is based on the black-box scenario discussed in Sect. II, in
TABLE IV: Initial dosage distribution (mg/wk) and percentage
difference between original and predicted dosage after OptP
attack at 20% poisoning rate (health care dataset).
which the attacker uses a substitute training set D′
tr to craft
the poisoning samples, and then tests them against the targeted
model (trained on Dtr). Our results, averaged on 5 runs,
are detailed in Table V, which presents the ratio between
transferred and original attacks. Note that the effectiveness
of transferred attacks is very similar to that of the original
attacks, with some outliers on the house dataset. For instance,
the statistical attack StatP achieves transferred MSEs within
11.4% of the original ones. The transferred OptP attacks have
lower MSEs by 3% than the original attack on LASSO. At
the same time, transferred attacks could also improve the
effectiveness of the original attacks: by 30% for ridge, and
78% for LASSO. We conclude that, interestingly, our most
effective poisoning attacks (OptP and StatP) tend to have
good transferability properties. There are some exceptions
(ridge on house dataset), which deserve further investigation
in future work. In most cases the MSEs obtained when using
a different training set for both attacks is comparable to MSEs
obtained when the attack is mounted on the actual training set.
Summary of poisoning attack results.
• We introduce a new optimization framework for poisoning regression, which manages to improve upon BGD by
a factor of 6.83. The best OptP attack selects the initialization strategy, optimization argument, and optimization
objective to achieve maximum MSEs.
• We ﬁnd that our statistical-based attack (StatP) works
reasonably well in poisoning all datasets and models, is
efﬁcient in running time, and needs minimal information
on the model. Our optimization-based attack OptP takes
longer to run, needs more information on the model, but
can be more effective in poisoning than StatP if properly
conﬁgured.
• In a health care case study, we ﬁnd that our OptP attack
can cause half of patients’ Warfarin dosages to change
by an average of 139.31%. One tenth of these patients
can have their dosages changed by 359%, demonstrating
the devastating consequences of poisoning.
• We ﬁnd that both our statistical and optimization attacks
have good transferability properties, and still perform
well with minimal difference in accuracy, when applied
to different training sets.
C. Defense algorithms
In this section, we evaluate our proposed TRIM defense and
other existing defenses from the literature (Huber, RANSAC,
TABLE V: Transferability of OptP and StatP attacks. Presented are the ratio of the MSE obtained with transferred
attacks over original attacks. Values below 1 represent original
attacks outperforming transferred attacks, while values above
1 represent transferred attacks outperforming original attacks.
Chen, and RONI) against the best performing optimization
attacks from the previous section (OptP). We test two wellknown methods from robust statistics: Huber regression 
and RANSAC , available as implementations in Python’s
sklearn package. Huber regression modiﬁes the loss function
from the standard MSE to reduce the impact of outliers. It does
this by using quadratic terms in the loss function for points
with small residuals and linear terms for points with large
residuals. The threshold where linear terms start being used is
tuned by a parameter ϵ > 1, which we set by selecting the best
of 5 different values: {1.1, 1.25, 1.35, 1.5, 2}. RANSAC builds
a model on a random sample of the dataset, and computes the
number of points that are outliers from that model. If there are
too many outliers, the model is rejected and a new model is
computed on a different random dataset sample. The size of
the initial random sample is a parameter that requires tuning
- we select 5 different values, linearly interpolating from 25
to the total number of clean data, and select the value which
has the lowest MSE. If the number of outliers is smaller than
the number of poisoning points, we retain the model.
We also compare against our own implementation of the
robust regression method by Chen et al. from the machine
learning community, and the RONI method from the security
community . Chen picks the features of highest inﬂuence
using an outlier resilient dot product computation. We vary
the number of features selected by Chen (the only parameter
in the algorithm) between 1 and 9 and pick the best results.
We ﬁnd that Chen has highly variable performance, having
MSE increases of up to a factor of 63,087 over the no defense
models, and we decided to not include it in our graphs. The
poor performance of Chen is due to the strong assumptions
of the technique (sub-Gaussian feature distribution and covariance matrix XT X = I.), that are not met by our real world
datasets. While we were able to remove the assumption that
all features had unit variance through robust scaling (using
the robust dot product provided by their work), removing
the covariance terms would require a robust matrix inversion,
which we consider beyond the scope of our work.
RONI (Reject On Negative Impact) was proposed in the
context of spam ﬁlters and attempts to identify outliers by
observing the performance of a model trained with and without
each point. If the performance degrades too much on a sampled
validation set (which may itself contain outliers), the point is
(a) Health Care Dataset
(b) Loan Dataset
(c) House Price Dataset
Fig. 5: MSE of defenses on ridge on the three datasets. We exclude Chen from the graphs due to its large variability. Defenses
are evaluated against the OptP attack. The only defense that consistently performs well in these situations is our proposed
TRIM defense, with RANSAC, Huber, and RONI actually performing worse than the undefended model in some cases.
(a) Health Care Dataset
(b) Loan Dataset
(c) House Price Dataset
Fig. 6: MSE of defenses on LASSO. We exclude Chen from the graphs due to its large variability. Defenses are evaluated
against the most effective attack OptP. As with ridge, the only defense that consistently performs well is our TRIM defense.
identiﬁed as an outlier and not included in the model. This
method has some success in the spam scenario due to the
ability of an adversary to send a single spam email with all
words in dictionary, but is not applicable in other settings in
which the impact of each point is small. We set the size of
the validation set to 50, and pick the best points on average
from 5 trials, as in the original paper. The size of the training
dataset is selected from the same values as RANSAC’s initial
sample size.
We show in Figures 5 and 6 MSEs for ridge and LASSO
regression for the original model (no defense), the TRIM algorithm, as well as the Huber, RANSAC, and RONI methods.
We pose three research questions next:
1) Question 1: Are known methods effective at defending
against poisoning attacks?: As seen in Figures 5 and 6,
existing techniques (Huber regression, RANSAC, and RONI),
are not consistently effective at defending against our presented attacks. For instance, for ridge models, the OptP attack
increases MSE over unpoisoned models by a factor of 60.22
(on the house dataset). Rather than decreasing the MSE, Huber
regression in fact increases the MSE over undefended ridge
models by a factor of 3.28. RONI also increases the MSE of
undefended models by 18.11%. RANSAC is able to reduce
MSE, but it is still greater by a factor of 4.66 than that of the
original model. The reason for this poor performance is that
robust statistics methods are designed to remove or reduce
the effect of outliers from the data, while RONI can only
identify outliers with high impact on the trained models. Our
attacks generate inlier points that have similar distribution as
the training data, making these previous defenses ineffective.
2) Question 2: What is the robustness of the new defense
TRIM compared to known methods?: Our TRIM technique
is much more effective at defending against all attacks than
the existing techniques are. For ridge and LASSO regression,
TRIM’s MSE is within 1% of the original models in all cases.
Interestingly, on the house price dataset the MSE of TRIM is
lower by 6.42% compared to unpoisoned models for LASSO
regression. TRIM achieves MSEs much lower than existing
methods, improving Huber by a factor of 1295.45, RANSAC
by a factor of 75, and RONI by a factor of 71.13. This demonstrates that the TRIM technique is a signiﬁcant improvement
over prior work at defending against these poisoning attacks.
3) Question 3: What is the performance of various defense
algorithms?: All of the defenses we evaluated ran in a
reasonable amount of time, but TRIM is the fastest. For
example, on the house dataset, TRIM took an average of 0.02
seconds, RANSAC took an average of 0.33 seconds, Huber
took an average of 7.86 seconds, RONI took an average of
15.69 seconds and Chen took an average of 0.83 seconds. On
the health care dataset, TRIM took an average of 0.02 seconds,
RANSAC took an average of 0.30 seconds, Huber took an
average of 0.37 seconds, RONI took an average of 14.80
seconds, and Chen took an average of 0.66 seconds. There
is some variance depending on the dataset and the number of
iterations to convergence, but TRIM is consistently faster than
other methods.
Summary of defense results.
• We ﬁnd that previous defenses (RANSAC, Huber, Chen,
and RONI) do not work very well against our poisoning
attacks. As seen in Figures 5-6, previous defenses can in
some cases increase the MSEs over unpoisoned models.
• Our proposed defense, TRIM, works very well and
signiﬁcantly improves the MSEs compared to existing
defenses. For all attacks, models, and datasets, the MSEs
of TRIM are within 1% of the unpoisoned model MSEs.
In some cases TRIM achieves lower MSEs than those of
unpoisoned models (by 6.42%).
• All of the defenses we tested ran reasonably quickly.
TRIM was the fastest, running in an average of 0.02
seconds on the house price dataset.
VI. RELATED WORK
The security of machine learning has received a lot of
attention in different communities (e.g., , , , ,
 . Different types of attacks against machine learning algorithms have been designed and analyzed, including evasion
attacks (e.g., , , , , , , ), and privacy
attacks (e.g., , , ). In poisoning attacks the attacker
manipulates or injects malicious data during training to cause
either availability attacks (inducing an effect on the trained
model) or targeted attacks (inducing an effect on speciﬁc data
points) , , , , .
In the security community, practical poisoning attacks have
been demonstrated in worm signature generation , ,
spam ﬁlters , network trafﬁc analysis systems for detection of DoS attacks , sentiment analysis on social networks , crowdsourcing , and health-care . In supervised learning settings, Newsome et al. have proposed red
herring attacks that add spurious words (features) to reduce the
maliciousness score of an instance. These attacks work against
conjunctive and Bayes learners for worm signature generation.
Perdisci et al. practically demonstrate how an attacker
can inject noise in the form of suspicious ﬂows to mislead
worm signature classiﬁcation. Nelson et al. present both
availability and targeted poisoning attacks against the public
SpamBayes spam classiﬁer. Venkataraman et al. analyze
the theoretical limits of poisoning attacks against signature
generation algorithms by proving bounds on false positives
and false negatives for certain adversarial capabilities.
In unsupervised settings, Rubinstein et al. examined
how an attacker can systematically inject trafﬁc to mislead
a PCA anomaly detection system for DoS attacks. Kloft
and Laskov demonstrated boiling frog attacks on centroid anomaly detection that involve incremental contamination of systems using retraining. Theoretical online centroid
anomaly detection analysis has been discussed in . Ciocarlie et al. discuss sanitization methods against timebased anomaly detectors in which multiple micro-models are
built and compared over time to identify poisoned data. The
assumption in their system is that the attacker only controls
data generated during a limited time window.
In the machine learning and statistics communities, earliest
treatments consider the robustness of learning to noise, including the extension of the PAC model by Kearns and Li ,
as well as work on robust statistics , , , . In
adversarial settings, robust methods for dealing with arbitrary
corruptions of data have been proposed in the context of
linear regression , high-dimensional sparse regression ,
logistic regression , and linear regression with low rank
feature matrix . These methods are based on assumptions
on training data such as sub-Gaussian distribution, independent
features, and low-rank feature space. Biggio et al. pioneered the research of optimizing poisoning attacks for kernelbased learning algorithms such as SVM. Similar techniques
were later generalized to optimize data poisoning attacks for
several other important learning algorithms, such as feature
selection for classiﬁcation , topic modeling , autoregressive models , collaborative ﬁltering , and simple
neural network architectures .
VII. CONCLUSIONS
We perform the ﬁrst systematic study on poisoning attacks
and their countermeasures for linear regression models. We
propose a new optimization framework for poisoning attacks
and a fast statistical attack that requires minimal knowledge
of the training process. We also take a principled approach
in designing a new robust defense algorithm that largely outperforms existing robust regression methods. We extensively
evaluate our proposed attack and defense algorithms on several
datasets from health care, loan assessment, and real estate
domains. We demonstrate the real implications of poisoning
attacks in a case study health application. We ﬁnally believe
that our work will inspire future research towards developing
more secure learning algorithms against poisoning attacks.
ACKNOWLEDGEMENTS
We thank Ambra Demontis for conﬁrming the attack results
on ridge regression, and Tina Eliassi-Rad, Jonathan Ullman,
and Huy Le Nguyen for discussing poisoning attacks. We also
thank the anonymous reviewers for all the extensive feedback
received during the review process.
This work was supported in part by FORCES (Foundations
Of Resilient CybEr-Physical Systems), which receives support
from the National Science Foundation (NSF award numbers
CNS-1238959, CNS-1238962, CNS-1239054, CNS-1239166),
DARPA under grant no. FA8750-17-2-0091, Berkeley Deep
Drive, and Center for Long-Term Cybersecurity.
This work was also partly supported by the EU H2020
project ALOHA, under the European Union’s Horizon 2020
research and innovation programme (grant no. 780788).