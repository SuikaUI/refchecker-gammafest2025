Online Learning for Ofï¬‚oading and
Autoscaling in Energy Harvesting Mobile Edge
Jie Xu, Member, IEEE, Lixing Chen, Student Member, IEEE,
Shaolei Ren, Member, IEEE
Mobile edge computing (a.k.a. fog computing) has recently emerged to enable in-situ processing
of delay-sensitive applications at the edge of mobile networks. Providing grid power supply in support
of mobile edge computing, however, is costly and even infeasible (in certain rugged or under-developed
areas), thus mandating on-site renewable energy as a major or even sole power supply in increasingly
many scenarios. Nonetheless, the high intermittency and unpredictability of renewable energy make
it very challenging to deliver a high quality of service to users in energy harvesting mobile edge
computing systems. In this paper, we address the challenge of incorporating renewables into mobile
edge computing and propose an efï¬cient reinforcement learning-based resource management algorithm,
which learns on-the-ï¬‚y the optimal policy of dynamic workload ofï¬‚oading (to the centralized cloud)
and edge server provisioning to minimize the long-term system cost (including both service delay and
operational cost). Our online learning algorithm uses a decomposition of the (ofï¬‚ine) value iteration
and (online) reinforcement learning, thus achieving a signiï¬cant improvement of learning rate and runtime performance when compared to standard reinforcement learning algorithms such as Q-learning.
We prove the convergence of the proposed algorithm and analytically show that the learned policy has
a simple monotone structure amenable to practical implementation. Our simulation results validate the
efï¬cacy of our algorithm, which signiï¬cantly improves the edge computing performance compared to
ï¬xed or myopic optimization schemes and conventional reinforcement learning algorithms.
J. Xu and L. Chen are with the Department of Electrical and Computer Engineering, University of Miami. Email:
 , .
S. Ren is with the Department of Electrical and Computer Engineering, University of California, Riverside. Email:
 
Index Terms
Mobile edge computing, energy harvesting, online learning.
I. INTRODUCTION
In the era of mobile computing and Internet of Things, a tremendous amount of data is
generated from massively distributed sources, requiring timely processing to extract its maximum
value. Further, many emerging applications, such as mobile gaming and augmented reality, are
delay sensitive and have resulted in an increasingly high computing demand that frequently
exceeds what mobile devices can deliver. Although cloud computing enables convenient access
to a centralized pool of conï¬gurable computing resources, moving all the distributed data and
computing-intensive applications to clouds (which are often physically located in remote megascale data centers) is simply out of the question, as it would not only pose an extremely
heavy burden on todayâ€™s already-congested backbone networks but also result in (sometimes
intolerable) large transmission latencies that degrade the quality of service â€“ .
As a remedy to the above limitations, mobile edge computing (MEC) â€“ (a.k.a., fog
computing ) has recently emerged to enable in-situ processing of (some) workloads locally
at the network edge without moving them to the cloud. In MEC, network edge devices, such
as base stations, access points and routers, are endowed with cloud-like computing and storage
capabilities to serve usersâ€™ requests as a substitute of clouds, while signiï¬cantly reducing the
transmission latency as they are placed in close proximity to end users and data sources. In this
paper, we consider base station as the default edge device and refer to the combination of an
edge device and the associated edge servers as an edge system.
Effective operation of MEC is contingent upon efï¬cient power provisioning for the edge
system. However, providing reliable and stable grid power supply in remote areas and hazardous locations can be extremely costly and even infeasible since construction and operation of
transmission lines are often prohibitive, and grid-tied servers can violate environmental quality
regulations in rural areas that are ecologically sensitive . For instance, in many developing
countries, the majority of base stations have to be powered by continuously operating diesel
generators because the electric grid is too unreliable . In view of the signiï¬cant carbon
footprint of grid power as well as soaring electricity prices, off-grid renewable energy harvested
from ambient vibrations, heat, wind and/or solar radiation is embraced as a major or even
Fig. 1. Architecture of a renewable-powered edge computing system. The photo shows a solar- and wind-powered base station
deployed by Alcatel Lucent in Turkey. (Source: 
sole power supply for edge systems in the ï¬eld, thanks to the recent advancements of energy
harvesting techniques , .
Despite the clear advantages, the high intermittency and unpredictability of renewable energy
creates tremendous new challenges for fully reaping the beneï¬ts of MEC. Although batteries are
often installed as an energy buffer, the computing capacity of an edge system is still signiï¬cantly
limited at any moment in time. As a result, although processing computation tasks at the edge
reduces the transmission latency, a considerable processing time may occur when little power
supply is available. This gives rise to an important trade-off between transmission delay and
processing delay, which is jointly determined by the edge systemâ€™s ofï¬‚oading policy (i.e. how
much workload is ofï¬‚oaded to the cloud) and autoscaling policy (i.e. how many servers are
dynamically provisioned or activated). The problem is further complicated due to the temporal
correlation â€” provisioning more servers and processing more workloads at the edge system
in the current time means that fewer servers can be provisioned and fewer workloads can be
processed locally in the future due to the limited and time-varying renewable energy supply.
Figure 1 illustrates the considered system architecture.
In this paper, we address the challenge of incorporating renewable energy into MEC and
propose an efï¬cient reinforcement learning-based resource management algorithm, which learns
on-the-ï¬‚y the optimal policy of dynamic workload ofï¬‚oading (to the centralized cloud) and
edge server provisioning to minimize the long-term system cost (including both service delay
and operational cost). Our main contributions are summarized as follows:
â€¢ We formulate the joint ofï¬‚oading and edge server provisioning problem as a Markov
decision process (MDP) by taking into account various unique aspects of the considered
energy harvesting MEC system. The ofï¬‚oading and edge server provisioning decisions are
jointly made according to the information of computation workload, core network congestion
state, available battery power and anticipated renewable power arrival. By formulating the
MDP, the edge system resource management is carried out in a foresighted way by taking
future system dynamics into account, thereby optimizing the long-term system performance.
â€¢ We develop a novel post-decision state (PDS) based learning algorithm that learns the
optimal joint ofï¬‚oading and autoscaling policy on-the-ï¬‚y. It is well-known that MDP suffers
from the so-called â€œcurse of dimensionalityâ€ problem when the state space is large .
The proposed PDS-based learning algorithm exploits the special structure of state transitions of the considered energy harvesting MEC system to conquer this challenge, thereby
signiï¬cantly improving both the learning convergence speed and the run-time performance
compared with conventional online reinforcement learning algorithms such as Q-learning
 . The key to achieving this performance improvement is a decomposition of the (ofï¬‚ine)
value iteration and (online) reinforcement learning that allows many components of the
algorithm to be learned in a batch manner.
â€¢ We prove the convergence of the proposed PDS-based learning algorithm and analytically
characterize the structure of the resulting optimal policy. The optimal policy is proven to
have a simple monotone structure: the power demand for the optimal joint ofï¬‚oading and
autoscaling policy is non-decreasing in the amount of available battery power. This result
enables easy implementation of the proposed algorithm in practical MEC applications.
â€¢ Extensive simulations are carried out to verify our analytical results and evaluate the
performance of the proposed algorithm. The results conï¬rm that our method can signiï¬cantly
improve the performance of the energy harvesting MEC system.
The rest of this paper is organized as follows. Section II discusses related works. Section III
describes the system model. Section IV formulates the MDP problem. Section V develops the
PDS-based learning algorithm. Section VI proves the convergence of the proposed algorithm
and characterizes the structure of the optimal policy. Section VII evaluates the proposed method
via systematic simulations. Section VIII concludes the paper.
II. RELATED WORK
Mobile edge computing (MEC) has received an increasing amount of attention in recent years.
The concept of MEC was proposed in 2014 as a new platform that provides IT and cloudcomputing capabilities within the radio access network in close proximity to mobile subscribers
 . Initially, MEC refers to the use of BSs for ofï¬‚oading computation tasks from mobile
devices. Recently, the deï¬nition of edge devices gets broader, encompassing any devices that
have computing resources along the path between data sources and cloud data centers . Fog
computing is a related concept that refers to the same computing paradigm. The areas of Fog
computing and MEC are overlapping and the terminologies are frequently used interchangeably.
There exist signiï¬cant disparities between MEC and mobile cloud computing (MCC). Compared
with MCC, MEC has the advantages of achieving lower latency, saving energy, supporting
context-aware computing, and enhancing privacy and security for mobile applications . A
central theme of many prior studies is ofï¬‚oading policy on the user side, i.e., what/when/how to
ofï¬‚oad a userâ€™s workload from its device to the edge system or cloud (see and references
therein). Depending on the type of tasks, ofï¬‚oading can be either binary or partial. Our work
focuses on edge-side ofï¬‚oading and autoscaling, and hence is complementary to these studies
on user-side ofï¬‚oading.
MEC servers are small-scale data centers and consume substantially less energy than the
conventional cloud mega-scale data center . However, as MEC servers become more widely
deployed, the system-wide energy consumption becomes a big concern. Therefore, innovative
techniques for achieving green MEC is in much need. Off-grid renewable energy, such as solar
radiation and wind energy, has recently emerged as a viable and promising power source for
various IT systems thanks to the recent advancement of energy harvesting techniques , .
Compared with traditional grid energy which is normally generated by coal-ï¬red power plants,
employing renewable energy signiï¬cantly reduces the amount of carbon emission. Moreover, the
use of renewable energy sources eliminates the need of human intervention, which is difï¬cult if
not impossible for certain types of application scenarios where the devices are hard and dangerous
to reach. However, designing green MEC powered by renewable energy is much more challenging
compared to green communication systems â€“ or green data center networks ,
 , since the radio and computation resources have to be jointly managed, whereas prior
research typically only considers one of the two decisions. For example, autoscaling (a.k.a.,
right-sizing) in data centers dynamically controls the number of active servers, but the
control knob of ofï¬‚oading to the cloud is not available in the context of data centers. For energy
harvesting mobile devices, a dynamic computation ofï¬‚oading policy was proposed in using
Lyapunov optimization techniques based on both information of the wireless channel and
energy. The focus of the present paper is on energy harvesting MEC systems and our solution
is based on reinforcement learning.
Another study relevant to our work is , which also studies workload allocation/ofï¬‚oading
in a cloud-fog computing system. However, unlike our energy harvesting edge system, this
paper considers a grid-powered system and focuses on a one-shot static optimization without
addressing the temporal correlation among the ofï¬‚oading decisions across time (due to intermittent renewable energy and limited battery capacity). The present paper develops a foresighted
resource management algorithm for energy harvesting MEC, which can operate optimally in timevarying and unknown environments by formulating a Markov decision process problem. To cope
with unknown time-varying system dynamics, our learning algorithm employs a decomposition
of (ofï¬‚ine) value iteration and (online) reinforcement learning based on factoring the system
dynamics into an a priori known and an a priori unknown component. A key advantage of
our proposed algorithm is that it exploits the partial information of the edge computing system
and the structure of the resource management problem, and thus it converges much faster than
conventional reinforcement learning algorithms such as Q-learning .
To the best knowledge of the authors, the conference version of this paper was the
ï¬rst to study resource management for energy harvesting MEC servers (see related discussions
in a recent comprehensive survey paper ). The present paper extents our ï¬ndings in .
Speciï¬cally, in addition to developing an efï¬cient learning algorithm for the optimal ofï¬‚oading
and autoscaling policy, we analytically characterize the structure of the learned optimal resource
management policy and carry out extensive simulations to evaluate its performance.
III. SYSTEM MODEL
As a major deployment method of mobile edge computing , we consider an edge system
consisting of a base station and a set of edge servers, which are physically co-located and share
the same power supply in the cell site.
MAIN NOTATIONS AND THEIR MEANINGS
total workload arrival rate
amount of locally processed workload
number of active servers
wireless access and transmission delay cost
local processing delay cost
ofï¬‚oading delay cost
total delay cost
backup power supply cost
edge system operation power consumption
edge system computing power consumption
total energy consumption
harvested green energy
environment state
battery level
backhaul network congestion state
system state
post-decision system state
normal value function
post-decision value function
A. Workload model
We consider a discrete-time model by dividing the operating period into time slots of equal
length indexed by t = 0, 1, ..., each of which has a duration that matches the timescale at which
the edge device can adjust its computing capacity (i.e. number of active servers). We use x âˆˆL
to represent a location coordinate in the service area L. Let Î»(x, t) represent the workload arrival
rate in location x, and Î¸(x, t) be the wireless transmission rate between the base station and
location x. Thus Î»(t) = P
xâˆˆL Î»(x, t) âˆˆ[0, Î»max] is the total workload arrival rate at the edge
system, where Î»max is the maximum possible arrival rate. The system decides the amount of
workload Âµ(t) â‰¤Î»(t) that will be processed locally. The remaining workload Î½(t) â‰œÎ»(t)âˆ’Âµ(t)
will be ofï¬‚oaded to the cloud for processing. The edge system also decides at the beginning
of the time slot the number of active servers, denoted by m(t) âˆˆ[0, M] â‰œM. These servers
are used to process the local workload Âµ(t). Since changing the number of servers during job
execution are difï¬cult and in many cases impossible, we only allow determining the number of
servers at the beginning of each time slot but not within the slot.
B. Delay cost model
The average utilization of the base station is Ï(t) = P
x Î»(x, t)/Î¸(x, t), which results in a total
wireless access and transmission delay of cwi(t) = P
x Î»(x, t)/[Î¸(x, t)(1âˆ’Ï(t))] by following the
literature and modeling the base station as a queueing system . Next we model the workload
processing delay incurred at the edge servers.
For the local processed workload, the delay cost clo(t) is mainly processing delay due to the
limited computing capacity at the local edge servers. The transmission delay from the edge device
to the local servers is negligible due to physical co-location. To quantify the delay performance
of services, such as average delay and tail delay (e.g. 95th-percentile latency), without restricting
our model to any particular performance metric, we use the general notion of clo(m(t), Âµ(t))
to represent the delay performance of interest during time slot t. As a concrete example, we
can model the service process at a server instance as an M/G/1 queue and use the average
response time (multiplied by the arrival rate) to represent the delay cost, which can be expressed
as clo(m(t), Âµ(t)) =
m(t)Â·Îºâˆ’Âµ(t), where Îº is the service rate of each server.
For the ofï¬‚oaded workload, the delay cost coff(t) is mainly transmission delay due to network
round trip time (RTT), which varies depending on the network congestion state. For modeling
simplicity, the service delay at the cloud side is also absorbed into the network congestion state.
Thus, we model the network congestion state, denoted by h(t), as an exogenous parameter and
express it in terms of the RTT (plus cloud service delay) for simplicity. The delay cost is thus
coff(h(t), Î»(t), Âµ(t)) = (Î»(t) âˆ’Âµ(t))h(t). The total delay cost is therefore
cdelay(h(t), Î»(t), m(t), Âµ(t))
=clo(m(t), Âµ(t)) + coff(h(t), Î»(t), Âµ(t)) + cwi(Î»(t))
C. Power model
We interchangeably use power and energy, since energy consumption during each time slot
is the product of (average) power and the duration of each time slot that is held constant in our
model. The total power demand of the edge system in a time slot consists of two parts: ï¬rst,
basic operation and transmission power demand by edge devices (base station in our study); and
second, computing power demand by edge servers. The ï¬rst part is independent of the ofï¬‚oading
or the autoscaling policy, which is modeled as dop(Î»(t)) = dsta + ddyn(Î»(t)) where dsta is the
static power consumption and ddyn(Î»(t)) is the dynamic power consumption depending on the
amount of total workload. The computing power demand depends on the number of active servers
as well as the locally processed workload. We use a generic function dcom(m(t), Âµ(t)), which is
increasing in m(t) and Âµ(t), to denote the computing power demand. The total power demand
in time slot t is therefore
d(Î»(t), m(t), Âµ(t)) = dop(Î»(t)) + dcom(m(t), Âµ(t))
To model the uncertainty of the green power supply, we assume that the green power budget,
denoted by g(t), is realized after the ofï¬‚oading and autoscaling decisions are made. Therefore,
the decisions cannot utilize the exact information of g(t). However, we assume that there is an
environment state e(t) which the system can observe and it encodes valuable information of how
much green energy budget is anticipated in the current time slot. For instance, daytime in a good
weather usually implies high solar power budget. Speciï¬cally, we model g(t) as an i.i.d. random
variable given e(t), which obeys a conditional probability distribution Pg(g(t)|e(t)). Note that
the environment state e(t) itself may not be i.i.d.
D. Battery model
Batteries are used to balance the power supply and demand. In a solar+wind system, photovoltaic modules and wind turbines can combine their output to power the edge system and
charge the batteries . When their combined efforts are insufï¬cient, batteries take over to ensure
steady operation of the edge system. We denote the battery state at the beginning of time slot t
by b(t) âˆˆ[0, B] â‰œB (in units of power) where B is the battery capacity. For system protection
reasons, the battery unit has to be disconnected from the load once its terminal voltage is below
a certain threshold for charging. We map b(t) = 0 to this threshold voltage to ensure basic
operation of the system.
Since green power budget is unpredictable and hence unknown at the beginning of time slot
t, the edge system uses a conservative policy which satisï¬es dcom(m(t), Âµ(t)) â‰¤max{b(t) âˆ’
dop(Î»(t)), 0} to avoid activating backup power supply by making ofï¬‚oading and autoscaling
decisions.
 
 

   
 

Fig. 2. Battery state dynamics. Case 1: current battery cannot support basic operation and thus, backup power supply is invoked.
Case 2: current battery can support basic operation.
â€¢ When dop(Î»(t)) â‰¥b(t), dcom(Î»(t), m(t), Âµ(t)) must be zero, which means that the edge
system ofï¬‚oads all workload to the cloud if the existing battery level cannot even support
the basic operation and transmission in the current slot. Moreover, the backup power supply
(e.g. diesel generator) will be used to maintain basic operation for the slot. The cost due
to activating the backup power supply is cbak(t) = Ï† Â· dop(Î»(t)) where Ï† > 0 is a large
constant representing the large cost due to using the backup power supply. The next time
slot battery state then evolves to b(t + 1) = b(t) + g(t).
â€¢ When dop(Î»(t)) â‰¤b(t), the edge system may process part of the workload Âµ(t) â‰¤Î»(t)
at the local servers, but the power demand must satisfy dcom(Î»(t), m(t), Âµ(t)) â‰¤b(t) âˆ’
dop(Î»(t)). Depending on the realized green power g(t) and the computing power demand
dcom(Î»(t), m(t), Âµ(t)), the battery is recharged or discharged accordingly:
â€“ If g(t) â‰¥d(Î»(t), m(t), Âµ(t)), then the surplus g(t) âˆ’d(Î»(t), m(t), Âµ(t)) is stored in the
battery until reaching its capacity B:
b(t + 1) = max{b(t) + g(t) âˆ’d(Î»(t), m(t), Âµ(t)), B}
â€“ If g(t) < d(Î»(t), m(t), Âµ(t)), then the battery has to be discharged to cover the energy
deï¬cit d(Î»(t), m(t), Âµ(t)) âˆ’g(t).
b(t + 1) = b(t) + g(t) âˆ’d(Î»(t), m(t), Âµ(t))
For simplicity, we will assume that there is no power loss either in recharging or
discharging the batteries, noting that this can be easily generalized. We also assume
that the batteries are not leaky. We model the battery depreciation cost in a time slot,
denoted by cbattery(t), using the amount of discharged power in this time slot since the
lifetime discharging is often limited. Speciï¬cally,
cbattery(t) = Ï‰ Â· max{d(Î»(t), m(t), Âµ(t)) âˆ’g(t), 0}
where Ï‰ > 0 is the normalized unit depreciation cost.
IV. PROBLEM FORMULATION
In this section, we formulate the dynamic ofï¬‚oading and autoscaling problem as an online
learning problem, in order to minimize the system cost. The system state is described by a tuple
s(t) â‰œ(Î»(t), e(t), h(t), b(t)), which is observable at the beginning of each time slot. Among
the four state elements, the workload arrival rate Î»(t), the environment state e(t), the backbone
network state h(t) are exogenous states which are independent of the ofï¬‚oading and autoscaling
actions while the battery state b(t) evolves according the to ofï¬‚oading and autoscaling actions
as well as the renewable power realization. To make the stochastic control problem tractable,
they are assumed to have ï¬nite value spaces and Î»(t), e(t), h(t) evolve as ï¬nite-state Markov
chains. Speciï¬cally, let PÎ»(Î»(t + 1)|Î»(t)), Pe(e(t + 1)|e(t)) and Ph(h(t + 1)|h(t)) denote the
transition matrices for Î»(t), e(t) and h(t), respectively. Similar assumptions have been made in
existing literature, e.g. , . Importantly, all these probability distributions are unknown a
priori to the edge system.
The stochastic control problem now can be cast into an MDP, which consists of four elements:
the state space S, the action space A, the state transition probabilities Ps(s(t+1)|s(t), a(t)), âˆ€s, sâ€² âˆˆ
S, a âˆˆA, and the cost function c(s, a), âˆ€s, a. We have already deï¬ned the state space. Next we
introduce the other elements as follows.
Actions. Although the actual actions taken by the edge system are Î½(t) (ofï¬‚oading) and m(t)
(autoscaling) in each time slot t, we will consider an intermediate action in the MDP formulation,
which is the computing power demand in each time slot t, denoted by a(t) âˆˆA where A is a
ï¬nite value space [0, 1, ...., B]. We will see in a moment how to determine the optimal ofï¬‚oading
and autoscaling actions based on this. As mentioned before, to maintain basic operation in the
worst case, we require that a(t) â‰¤max{b(t) âˆ’dop(Î»(t)), 0}. Thus, this condition determines the
feasible action set in each time slot.
State transitions. Given the current state s(t), the computing power demand a(t) and the
realized green power budget g(t), the buffer state in the next time slot is
b(t + 1) = [b(t) + g(t)]B
0 , if dop(Î»(t)) > b(t)
b(t + 1) = [b(t) âˆ’dop(Î»(t)) âˆ’a(t) + g(t)]B
0 , otherwise
where [Â·]B
0 denotes max{min{Â·, B}, 0}. The system then evolves into the next time slot t + 1
with the new state s(t + 1). The transition probability from s(t) to s(t + 1), given a(t), can be
expressed as follows
P(s(t + 1)|s(t), a(t))
=PÎ»(Î»(t + 1)|Î»(t))Pe(e(t + 1)|e(t))Ph(h(t + 1)|h(t))
Pg(g(t)|e(t))1{Î¶(t)}
where 1{Â·} is the indicator function and Î¶(t) denotes the event deï¬ned by (5). Notice that the
state transition only depends on a(t) but not the ofï¬‚oading or the autoscaling action. This is
why we can focus on the computing power demand action a(t) for the foresighted optimization
Cost function. The total system cost is the sum of the delay cost, the battery depreciation
cost and the backup power supply cost. If dop(Î»(t)) > b(t), then the cost is simply
Ëœc(t) = cdelay(h(t), Î»(t), 0, 0) + cbak(Î»(t))
since we must have m(t) = 0 and Âµ(t) = 0. Otherwise, the realized cost given the realized green
power budget g(t) is
Ëœc(t) = cdelay(h(t), Î»(t), m(t), Âµ(t)) + Ï‰ Â· [a(t) âˆ’g(t)]âˆ
Since the state transition does not depend on Âµ(t) or m(t), they can be optimized given s(t)
and a(t) by solving the following myopic optimization problem
cdelay(h, Î», m, Âµ) s.t. d(m, Âµ) = a
Let mâˆ—(s, a) and Âµâˆ—(s, a) denote the optimal solution and câˆ—
delay(s, a) the optimal value given s
and a. Therefore, the minimum cost in time slot t given s and a is
Ëœc(t) = câˆ—
delay(s(t), a(t)) + Ï‰ Â· [a(t) âˆ’g(t)]âˆ
The expected cost is thus
c(s(t), a(t)) = câˆ—
delay(s(t), a(t)) + Eg(t)|e(t)Ï‰ Â· [a(t) âˆ’g(t)]âˆ
Policy. The edge systemâ€™s computing power demand policy (which implies the joint ofï¬‚oading
and autoscaling policy) in the MDP is a mapping Ï€ : Î› Ã— E Ã— H Ã— B â†’A. We focus
on optimizing the policy to minimize the edge systemâ€™s expected long-term cost, which is
deï¬ned as the expectation of the discounted sum of the edge deviceâ€™s one-slot cost: CÏ€(s(0)) =
Î´tc(s(t), a(t))|s(0)
where Î´ < 1 is a constant discount factor, which models the fact
that a higher weight is put on the current cost than the future cost. The expectation is taken over
the distribution of the green power budget, the workload arrival, the environment state and the
network congestion state. It is known that in MDP, this problem is equivalent to the following
optimization
Ï€ CÏ€(s), âˆ€s âˆˆS
Let Câˆ—(s) be the optimal discounted sum cost starting with state s. It is well-known that Ï€âˆ—and
Câˆ—(s) can be obtained by recursively solving the following set of Bellman equations
Câˆ—(s) = min
c(s, a) + Î´
P(sâ€²|s, a)Câˆ—(sâ€²)
In the next section, we solve this problem using the idea of dynamic programming and online
V. POST-DECISION STATE BASED ONLINE LEARNING
If all the probability distributions were known a priori, then the optimal policy could be
solved using traditional algorithms for solving Bellman equations, e.g. the value iteration and
the policy iteration , in an ofï¬‚ine manner. In the considered problem, all these probability
distributions are unknown a priori and hence, these algorithms are not feasible. In this section,
we propose an online reinforcement learning algorithm to derive the optimal policy Ï€âˆ—on-the-ï¬‚y.
Our solution is based on the idea of post-decision state (PDS), which exploits the partially known
information about the system dynamics and allows the edge system to integrate this information
into its learning process to speed up learning. Compared with conventional online reinforcement
learning algorithms, e.g. Q-learning, the proposed PDS based learning algorithm signiï¬cantly
improves its convergence speed and run-time performance.
In this rest of this section, we ï¬rst deï¬ne PDS, and then describe the proposed algorithm.
Finally, we prove the convergence of the proposed algorithm.
 
             
                     
Fig. 3. Illustration of Post-Decision State
A. Post-Decision State
We ï¬rst introduce the notion of PDS, which is the most critical idea of our proposed algorithm.
In our problem, PDS is the intermediate system state after the edge system takes the computing
power demand action a(t) but before the green power budget g(t) is realized. Figure 3 illustrates
the relationship between a normal state s(t) and its PDS Ëœs(t). Speciï¬cally, the PDS in time slot
t, denoted by Ëœs(t) â‰œ(ËœÎ»(t), Ëœe(t), Ëœh(t),Ëœb(t)), is deï¬ned as
ËœÎ»(t) = Î»(t),
Ëœe(t) = e(t),
Ëœh(t) = h(t)
b(t), if dop(Î»(t)) > b(t)
max{b(t) âˆ’dop(Î»(t)) âˆ’a(t), 0}, otherwise
As we can see, the post-decision workload state ËœÎ»(t), post-decision environment state Ëœe(t)
and post-decision network congestion state Ëœh(t) remain the same because the computing power
demand action a(t) does not have a direct impact on these elements of the system state. The only
element of the system state that may change is the battery state b(t). However, it is important
to notice that the post-decision battery state Ëœb(t) is only a virtual state but not the real battery
state. Given the deï¬nition of PDS, we further deï¬ne the post-decision value function V âˆ—(Ëœs) as
P(sâ€²|Ëœs)Câˆ—(sâ€²), âˆ€s
where the transition P(sâ€²|Ëœs) between PDS and the next system state is now independent of the
ËœP(s|Ëœs) =PÎ»(Î»|ËœÎ»)Pe(e|Ëœe)Ph(h|Ëœh)
Pg(g|Ëœe)1{b = min{Ëœb + g, B}}
For the ease of exposition, we refer to s as the â€œnormalâ€ state and Câˆ—(s) as the â€œnormalâ€ value
(cost) function, in order to differentiate with their post-decision counterparts.
By comparing (11) and (14), it is obvious that there is a deterministic mapping from the
normal value function Câˆ—(s) to the post-state value function V âˆ—(Ëœs) as follows by
Câˆ—(s) = min
aâˆˆA(c(s, a) + Î´V âˆ—(Ëœs))
The above equation shows that the normal value function Câˆ—(s) in each time slot is obtained from
the corresponding post-decision value function V âˆ—(Ëœs) in the same time slot, where Ëœs = (Î», e, h, b)
if dop(Î»)) > b and Ëœs = (Î», e, h, max{b âˆ’dop(Î») âˆ’a, 0}) if dop(Î») â‰¤b, by performing the
minimization over the action a.
The advantages of using the PDS and post-decision value function are summarized as follows.
(1) In the normal state based Bellmanâ€™s equation set (11), the expectation over the possible
workload arrival Î», the environment state e, network congestion state h, and green power budget
g has to be performed before the minimization over the possible energy demand actions a.
Therefore, performing the minimization requires the knowledge of these dynamics. In contrast,
in the PDS based Bellman equations (14), the expectation operation is separated from the
minimization operation. If we can learn and approximate the post-decision value function V âˆ—(Ëœs),
then the minimization can be solved without any prior knowledge of the system dynamics.
(2) Given the energy demand action a, the PDS decomposes the system dynamics into an a
priori unknown component, i.e. Î», e, h and g whose evolution is independent of a, and an a priori
known component, i.e. the battery state evolution is partially determined by a. Importantly, Î», e,
h and g are also independent of the battery state b. This fact enables us to develop a batch update
scheme on the post-decision value functions, which can signiï¬cantly improve the convergence
speed of the proposed PDS based reinforcement learning.
B. The algorithm
The algorithm maintains and updates a set of variables in each time slot. These variables are
â€¢ The one slot cost estimate Ë†ct(s, a), âˆ€(s, a) âˆˆS Ã— A.
â€¢ The post-decision value function estimate Ë†V t(Ëœs), âˆ€Ëœs âˆˆS.
â€¢ The normal value function estimates Ë†Ct(s), âˆ€s âˆˆS.
The superscript t is used to denote the estimates at the beginning of the time slot t. If these
estimates are accurate, i.e. Ë†ct(s, a) = c(s, a), Ë†V t(Ëœs) = V âˆ—(Ëœs) and Ë†Ct(s) = Câˆ—(s), then the optimal
power demand policy is readily obtained by solving (16). Our goal is to learn these variables
over time using the realizations of the system states and costs. The algorithm works as follows:
(In each time slot t)
Step 1: Determine the empirically optimal computing power demand by solving
a(t) = min
a (Ë†ct(s(t), a) + Î´ Ë†V t(Ëœs(t)))
using the current estimates Ë†ct(s(t), a) and Ë†V t(Ëœs(t)), where for each a, Ëœs(t) represents the corresponding PDS. Given this power demand, the corresponding optimal ofï¬‚oading and autoscaling
actions are determined as Âµ(t) = Âµâˆ—(s(t), a(t)) and m(t) = mâˆ—(s(t), a(t)) based on the solution
After the green power g(t) is harvested and hence the current slot cost Ëœc(t) is realized according
to (9), the battery state evolves to b(t + 1) according to (5).
Steps 2 through 4 update the estimates.
Step 2: Batch update Ë†ct(s, a) for any action a and any state s = (Î», e, h, b) such that e is
the same as the current slot environment state e(t) using the realized green power budget g(t)
according to
Ë†ct+1(s, a) = (1 âˆ’Ït)Ë†ct(s, a) + Ïtc(s, a, g(t))
where Ït is the learning rate factor that satisï¬es Pâˆ
t=0 Ït = âˆand
(Ït)2 < âˆ. For all other
action-state pair, Ë†ct+1(s, a) = Ë†ct(s, a). We can do this batch update because the green power
budget g(t) depends only on the environment state e(t) but not on other states or actions.
Step 3: Batch update the normal value function estimate for any state s = (Î», e, h, b) such
that e = e(t) according to
Ë†Ct+1(s) = min
aâˆˆA(Ë†ct+1(s, a) + Î´ Ë†V t(Ëœs))
The normal value function estimates for the remaining states are unchanged. The reason why
we can do this batch update is the same as that in Step 2.
Step 4: Batch update the post-decision value function estimate for any Ëœs âˆˆËœS such that
ËœÎ» = ËœÎ»(t), Ëœe = Ëœe(t) and Ëœh = Ëœh(t) according to
Ë†V t+1(Ëœs) = (1 âˆ’Î±t) Ë†V t(Ëœs) + Î±t Ë†Ct+1(s)
where s = (Î», e, h, b) satisï¬es Î» = Î»(t+1), e = e(t+1), h = h(t+1) and b = min{Ëœb+g(t), B}.
In this way, we update not only the currently visited PDS Ëœs(t) but all PDS with common ËœÎ»(t),
Ëœe(t) and Ëœh(t). This is because the temporal transition of Î», e, h is independent of of the battery
state b and the green power budget realization follows the same distribution since the environment
state e is the same for these states.
Algorithm 1 Online Learning for Joint Ofï¬‚oading and Autoscaling
1: Initialize: Ë†c0(s, a) = 0, âˆ€(s, a), Ë†V 0(Ëœs) = 0, âˆ€Ëœs, Ë†C0(s) = 0, âˆ€s.
2: for every time slot t do
Observe current state s(t)
Determine power demand a(t) by solving (17)
Determine ofï¬‚oading and autoscaling actions Âµ(t) and m(t) by solving (8)
Compute the post-decision state Ëœst according to (12) and (13)
(The green power budget g(t) is realized.)
Batch update Ë†ct(s, a) according to (18)
Batch update Ë†Ct(s) according to (19)
Batch update Ë†V t(Ëœ(s)) according to (20)
11: end for
VI. ALGORITHM ANALYSIS
A. Convergence of the PDS learning algorithm
We ï¬rst prove the convergence of our algorithm.
Theorem 1. The PDS based online learning algorithm converges to the optimal post-decision
value function V âˆ—(Ëœs), âˆ€Ëœs when the sequence of learning rates Ït satisï¬es Pâˆ
t=0 Ït = âˆand
(Ït)2 < âˆ.
Proof. The proof follows . For each PDS Ëœs, we deï¬ne a function on its value function as
FËœs(V ) = min
aâˆˆA(c(s, a) + Î´V (Ëœs))
where s and a are such that, Î» = ËœÎ», e = Ëœe, h = Ëœh and b = Ëœb âˆ’a. Thus, for any value of V (Ëœs),
FËœs(V ) maps to a real number. Based on this, we deï¬ne F : R|S| â†’R|S| be a mapping which
collects FËœs for all Ëœs âˆˆS. It is proven in that the convergence of our proposed algorithm is
equivalent to the convergence of the associated ordinary differential equation (O.D.E.):
Ë™V = F(V ) âˆ’V
Since the map F : R|S| â†’R|S| is a maximum norm Î´-contraction, the asymptotic stability of
the unique equilibrium point of the above O.D.E. is guaranteed . This unique equilibrium
point corresponds to the optimal post-decision value function V âˆ—(Ëœs), âˆ€Ëœs âˆˆS.
Because Câˆ—(s), âˆ€s is a deterministic function of V âˆ—(Ëœs), âˆ€Ëœs, it is straightforward that the PDS
based online learning algorithm also converges to Câˆ—(s), âˆ€s. Therefore, we prove that the edge
system is able to learn the optimal power demand policy and hence the optimal ofï¬‚oading and
autoscaling policies using the proposed algorithm.
B. Structure of the Optimal Policy
Next, we characterize the structure of the optimal policy. First, we show that the one-slot cost
function is convex in the power demand action.
Lemma 1. Assume that both clo(m, Âµ) and dcom(m, Âµ) are jointly convex in (m, Âµ) for any given
s, then the one-slot cost function c(s, a) is convex in a for any given s.
Proof. Recall that c(s, a) = câˆ—
delay(s, a) + Eg|eÏ‰ Â· max{a âˆ’g, 0}. Since a âˆ’g is linear in a, and
the maximum of convex functions is still convex, it is obvious that max{a âˆ’g, 0} is convex in
a. Since the expectation is just a weighted sum of convex functions, Eg|eÏ‰ Â· max{a âˆ’g, 0} is
also convex in a. Now, if we can prove câˆ—
delay(s, a) is convex in a, then the lemma is proved.
Recall that câˆ—
delay(s, a) is the solution to
cdelay(h, Î», m, Âµ) s.t. d(m, Âµ) = a
for the given s. Since cdelay(h, Î», m, Âµ) = clo(m, Âµ)+coff(Âµ(t))+cwi, clo(m, Âµ) is jointly convex
in (m, Âµ) and coff(Âµ(t)) is linear in Âµ, cdelay(h, Î», m, Âµ) is also jointly convex in (m, Âµ). Similarly,
since d(m, Âµ) = dop + dcom(m, Âµ) and dcom(m, Âµ) is jointly convex in (m, Âµ), d(m, Âµ) is also
jointly convex in (m, Âµ).
Consider two power demand actions a1, a2, let (mâˆ—
1) and (mâˆ—
2) be the corresponding
optimal joint ofï¬‚oading and autoscaling actions. Clearly, we should have the constraint be binding
for the optimal solution, i.e. d(mâˆ—
1 and d(mâˆ—
2. Now, we have, âˆ€Î» âˆˆ(0, 1),
delay(a1) + (1 âˆ’Î»)Î»câˆ—
=Î»cdelay(mâˆ—
1) + (1 âˆ’Î»)Î»cdelay(mâˆ—
â‰¥cdelay(Î»(mâˆ—
1) + (1 âˆ’Î»)(mâˆ—
2)) â‰œcdelay(mÎ», ÂµÎ»)
where we deï¬ne (mÎ», ÂµÎ») â‰œÎ»(mâˆ—
1)+(1âˆ’Î»)(mâˆ—
2). Let Ëœa = d(mÎ», ÂµÎ») be the corresponding
required power demand. Further, we let (mâˆ—, Âµâˆ—) be the optimal ofï¬‚oading and autoscaling action
for this Ëœa. Clearly, cdelay(mÎ», ÂµÎ») â‰¥cdelay(mâˆ—, Âµâˆ—). Due to the convexity of d(m, Âµ), we have
Ëœa = d(mÎ», ÂµÎ») = d(Î»(mâˆ—
1) + (1 âˆ’Î»)(mâˆ—
1) + (1 âˆ’Î»)d(mâˆ—
=Î»a1 + (1 âˆ’Î»)a2 â‰œaÎ»
Therefore, cdelay(mâˆ—, Âµâˆ—) â‰¥cdelay(mâˆ—
Î»), where (mâˆ—
Î») are the optimal ofï¬‚oading and autoscaling actions for aÎ». Therefore, we have
delay(a1) + (1 âˆ’Î»)Î»câˆ—
â‰¥cdelay(mâˆ—
delay(Î»a1 + (1 âˆ’Î»)a2)
This completes the proof of this lemma.
Lemma 2 characterizes the shape of the optimal value function and post-state value function.
Lemma 2. Assume that c(s, a) is convex in a for any given s, then both V âˆ—(s) and Câˆ—(s) are
non-increasing and convex in b for any given Î», e, h.
Proof. We ï¬rst prove Câˆ—(s) is non-increasing. The optimal value functions satisfy
Câˆ—(s) = min
c(s, a) + Î´
P(sâ€²|s, a)Câˆ—(sâ€²)
Consider two states s and sâ€² that differs only in b and bâ€², and assume b < bâ€². Let aâˆ—be the
optimal action for s. Now consider an action aâ€² = bâ€² âˆ’b + aâˆ—> aâˆ—for state sâ€². It is obvious
that the second term on the right-hand side is identical in both cases since the transitions of
Î», e, h are independent of the battery state, and because bâ€² âˆ’aâ€² = b âˆ’aâˆ—, the battery state
transition is also the same. Because c(s, a) is non-increasing in a, we have c(sâ€², aâ€²) â‰¤c(s, aâˆ—).
Therefore, by choosing aâ€² for sâ€², we have C(sâ€², aâ€²) â‰¤Câˆ—(s). Realizing Câˆ—(sâ€²) â‰¤C(sâ€², aâ€²) due
to the minimization operation, we have Câˆ—(sâ€²) â‰¤Câˆ—(s), thus proving Câˆ—(s) is non-increasing.
Next, we prove that Câˆ—(s) is convex by induction. The optimal value functions Câˆ—(s), âˆ€s can
be solved by the value iteration algorithm
Cn+1(s) = min
c(s, a) + Î´
P(sâ€²|s, a)Cn(sâ€²)
where the subscript n and n + 1 represent the n-th iteration and (n + 1)-th iteration. It is well
known that valuation iteration converges to the optimal solution, i.e. as n â†’âˆ, Cn(s) â†’
Câˆ—(s), âˆ€s, starting from any initial value function C0(s), âˆ€s. We initialize C0(s) to be convex
in b for any given Î», e, h. It is easy to see that V âˆ—(s) is also non-increasing since it is simply a
weighted average of a bunch of value functions.
Suppose Cn(s), âˆ€s are convex in b for any given Î», e, h. Consider two battery states b < bâ€²
and let the corresponding optimal action be aâˆ—and aâ€²âˆ—. Then we have
Cn+1(b) = c(aâˆ—) + Î´Vn(b âˆ’aâˆ—)
Cn+1(bâ€²) = c(aâ€²âˆ—) + Î´Vn(bâ€² âˆ’aâ€²âˆ—)
In the above equations, we omitted the state elements Î», e, h. Since we have assumed that
Cn(b), âˆ€s are convex in b. In addition, Ë†b = min{b + g, B} is a concave function in b. Thus, by
applying the results of composition (i.e. if f is concave and g is convex and non-increasing,
then h(x) = g(f(x)) is convex.), we have Cn(min{b + g, B}), âˆ€s also convex. Therefore Vn(b)
is also convex in b since it is a weighted sum of convex functions.
Now, combining (29) and (30) and using the convexity of Vn(b) we have
Î»Cn+1(b) + (1 âˆ’Î»)Cn+1(bâ€²)
=Î»c(aâˆ—) + (1 âˆ’Î»)c(aâ€²âˆ—)
+ Î´[Î»Vn(b âˆ’aâˆ—) + (1 âˆ’Î»)Vn(bâ€² âˆ’aâ€²âˆ—)]
â‰¥c(aÎ») + Î´Vn(bÎ» âˆ’aÎ») â‰¥Cn+1(bÎ»)
where bÎ» = Î»b + (1 âˆ’Î»)bâ€² and aÎ» = Î»aâˆ—+ (1 âˆ’Î»)aâ€²âˆ—. This proves that Cn+1(b) is also convex
Now, we are ready to prove the structural result of the optimal power demand policy.
Theorem 2. Assume that both clo(m, Âµ) and dcom(m, Âµ) are jointly convex in (m, Âµ) for any given
s, then the optimal power demand policy is monotonically non-decreasing in b for any given
Î», e, h. That is, âˆ€s, sâ€² such that Î» = Î»â€², e = eâ€², h = hâ€² and b â‰¤bâ€², then we have Ï€âˆ—(s) â‰¤Ï€(sâ€²).
Proof. We aim to prove that C(b, a) is subadditive in the battery state b and the power demand
action a for any given Î», e, h. This is to prove that âˆ€b â‰¤bâ€² and a â‰¤aâ€², we have
C(b, aâ€²) âˆ’C(b, a) â‰¥C(bâ€², aâ€²) âˆ’C(bâ€², a)
If the above is true, then we can apply (Section 4.7) to show that Ï€(s) is non-decreasing
Equation (32) is equivalent to
V âˆ—(b âˆ’aâ€²) âˆ’V âˆ—(b âˆ’a) â‰¥V âˆ—(bâ€² âˆ’aâ€²) âˆ’V âˆ—(bâ€² âˆ’a)
Let b âˆ’a = Ë†b, bâ€² âˆ’a = Ë†bâ€² and âˆ†= aâ€² âˆ’a. Then the above becomes
V âˆ—(Ë†b âˆ’âˆ†) âˆ’V âˆ—(Ë†b) â‰¥V âˆ—(Ë†bâ€² âˆ’âˆ†) âˆ’V âˆ—(Ë†bâ€²)
which is true due to the fact that V âˆ—(b) is non-increasing and convex when clo(m, Âµ) and
dcom(m, Âµ) are jointly convex in (m, Âµ) for any given s (by Lemma 1 and Lemma 2). This
completes the proof.
VII. SIMULATION
A. Simulation Setup
We consider each time slot as 15 minutes. The workload arrival space is set as Î› ={10
units/sec, 20 units/sec, ..., 100 units/sec}. The network congestion space is H ={20 ms/unit,
30 ms/unit, ..., 60 ms/unit}. The environment state space is E ={Low, Medium, High}. For
each environment state, the green power will be realized according to a normal distrinution
with different means: g(t|e = Low) âˆ¼N (200W, 102), g(t|e = Medium) âˆ¼N (400W, 102),
g(t|e = High) âˆ¼N (600W, 102). The battery capacity is set as B =2 kWh. The base station
static power consumption is dsta = 300W. The maximum number of activated edge server is
M = 15. The power consumption of each edge server is 150W. The maximum service rate of
each edge server is 20 units/sec. Other important parameters are set as follows: the normalized
unit depreciation cost Ï‰ = 0.01, the cost coefï¬cient of backup power supply Ï† = 0.15.
Time Average Cost
PDS learning (proposed)
Q-learning
Myopic optimization
Fixed 0.4 kW
Fixed 1.0 kW
Fig. 4. Run-time performance comparison
The proposed PDS-based learning algorithm is compared with three benchmark schemes:
â€¢ Q-learning : Q-learning is a famous model-free reinforcement learning technique for
solving MDP problems. It has been proven that for any ï¬nite MDP problem, Q-learning
eventually ï¬nds an optimal policy.
â€¢ Myopic optimization: this scheme ignores the temporal correlation between the system
states and the decisions, and minimizes the cost function given the state in the current time
slot by utilizing all available battery energy.
â€¢ Fixed power: this scheme uses a ï¬xed computation power (whenever possible) for edge
computing in each time slot.
B. Run-time Performance Comparison
Figure 4 compares the run-time performance of our scheme with the three benchmark schemes
for 10000 time slots. As can be seen, the proposed PDS-based learning algorithm incurs a
signiï¬cantly lower cost than all benchmark schemes.
â€¢ Myopic optimization incurs a large time-average cost since it ignores the temporal correlation of decision making and frequently is forced to activate the backup power in the
subsequent time slots.
â€¢ The ï¬xed power scheme has tremendously different performance depending on the ï¬xed
value used, which implies that it is sensitive to system parameters. In Figure 4, two ï¬xed
values (1.0kW and 0.4kW) are shown for illustrative purposes, where 1.0kW is the best ï¬xed
value found by our extensive simulations. Since the system dynamics is unknown a priori
Battery level (kWh)
Computing power demand (kW)
PDS policy ( = 10, h = 100 ms, e = Low)
PDS policy ( = 20, h = 150 ms, e = Medium)
Myopic policy ( = 10, h = 100 ms, e = Low)
Myopic policy ( = 20, h = 150 ms, e = Medium)
Fig. 5. Computing power demand policy
and may change over time, using a ï¬xed computing power scheme may cause signiï¬cant
performance loss.
â€¢ The performance of Q-learning is much worse than our proposed PDS learning since it
converges very slowly due to the large state space. Although there is a trend of declining in
the time average cost, even after 10000 time slots, there is still a considerable performance
gap compared with our scheme. On the other hand, our proposed PDS scheme converges
very quickly.
C. Learned Optimal Policy
Figure 5 further explains why the proposed algorithm outperforms the myopic solution by
showing the learned optimal policy. When the workload demand is low and the network is not
congested, the policy learned by the proposed algorithm tends to be conservative in using local
computing power if the battery level is low. In this way, more power can be saved for future
when the workload is high and the network congestion state degrades, thereby reducing the
long-term system cost. On the other hand, the myopic policy ignores this temporal correlation,
it activates local servers to process workload even if the battery level is not so high. As a result,
even though it achieves slight improvement in the current slot, it wastes power for potentially
reducing signiï¬cant cost in the future. Figure 5 also validates our theoretical results in Theorem
2 on the structure of the optimal policy: The optimal power demand increases in the battery
Insufficient
Battery level (kWh)
Distribution
PDS policy (proposed)
Q-learning
Myopic optimization
Fixed (0.4 kW)
Fixed (1 kW)
(a) Battery state distribution
Battery level (kWh)
Distribution
Insufficient
PDS learning (proposed)
Q-learning
Myopic optimization
Fixed 0.4 kW
Fixed 1.0 kW
(b) Battery state distribution (ï¬tted)
Fig. 6. Battery state distributions
D. Battery State Distribution
Figure 6(a) shows the distributions of the battery state over the simulated 10000 time slots
in one representative simulation run for the various schemes, and Figure 6(b) shows the ï¬tted
curves (polynomial ï¬t) for better inspection. As can be seen, Myopic optimization results in a
large portion of time when the system is in the insufï¬cient battery zone, incurring signiï¬cant
backup power costs. If a too small ï¬xed power demand is used (e.g. 0.4kW), the battery state
may spend a considerable amount of time in the high battery level zone (i.e. 0.7 â€“ 1kWh).
This implies that much of the green power cannot be harvested and hence is wasted due to the
limited battery capacity constraint. Moreover, using a smaller ï¬xed power for computation does
not guarantee that it has a smaller chance to get into the insufï¬cient battery zone. This is because
when the battery state is slightly higher than the level that can support basic operation, using
a smaller ï¬xed power can easily make battery state drop to the insufï¬cient battery zone in the
subsequent time slot whereas if a larger ï¬xed power scheme is used, the system will decide to
ofï¬‚oad all workload to the cloud without using the local battery power. Although a proper ï¬xed
power demand is able to strike a decent balance, it does not well adapt to the changing system
dynamics. The proposed PDS-based algorithm achieves the highest harvesting energy efï¬ciency
by keeping the battery at a relatively low state while above the insufï¬cient level.
E. Cost Composition
Figure 7 shows the cost compositions of the PDS-based algorithm and Myopic optimization
in 10000 time slots. It can be observed that the proposed PDS-based algorithm signiï¬cantly
(a) Run-time cost composition (PDS)
(b) Run-time cost composition (Myopic)
Fig. 7. Run-time cost composition
MyopicQ-learning 0.4kW
Time Average Cost
Backup power cost
Delay cost
Fig. 8. Time-average cost composition (T = 104)
cuts the back-up power cost by taking conservative action at low battery states which avoids the
usage of backup power. By contrast, Myopic optimization frequently leads the battery state into
the insufï¬cient zone as shown in ï¬gure 6 and results in signiï¬cant backup power costs. Figure
8 presents the composition of time-average cost at the end of simulation. It can be observed that
the PDS-based algorithm and Q-leaning reduce the total cost by considering the future system
dynamics and incur low backup power cost accounting for 12.6% and 16.7% of the total cost,
respectively. These fractions are much lower than those of the remaining schemes which do not
consider the long-term system performance.
F. Optimal Ofï¬‚oading Strategy
Finally, we visualize the optimal ofï¬‚oading strategy in Figure 9 given a ï¬xed number of active
servers under various system states. As the network becomes more congested and there are more
(a) Optimal ofï¬‚oading policy (m = 4)
(b) Optimal ofï¬‚oading policy (m = 10)
Fig. 9. Optimal ofï¬‚oading policy
active edge servers, the optimal strategy chooses to process more workload at the local edge
system. However, as the workload arrival rate increases, the amount of workload that can be
processed at the local edge system is saturated after certain workload arrival rate.
VIII. CONCLUSION
In this paper, we studied the joint ofï¬‚oading and autoscaling problem in energy harvesting
MEC systems. We found that foresightedness and adaptivity are keys to reliable and efï¬cient operation of renewable-powered MEC. To enable fast learning in the presence of a priori unknown
system parameters, a PDS-based reinforcement learning algorithm was developed to learn the
optimal ofï¬‚oading and autoscaling policy by exploiting the special structure of the considered
problem. Our simulations showed that the proposed scheme can signiï¬cantly improve the edge
computing performance even if it is powered by intermittent and unpredictable renewable energy.
Future work includes investigating large-scale edge computing systems powered by renewable
energy, e.g. green power-aware geographical load balancing.