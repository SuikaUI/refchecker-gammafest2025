This document is downloaded from DR‑NTU ( 
Nanyang Technological University, Singapore.
Cross‑position activity recognition with stratified
transfer learning
Chen, Yiqiang; Wang, Jindong; Huang, Meiyu; Yu, Han
Chen, Y., Wang, J., Huang, M., & Yu, H.  . Cross‑position activity recognition with
stratified transfer learning. Pervasive and Mobile Computing, 57, 1‑13.
doi:10.1016/j.pmcj.2019.04.004
 
 
© 2019 Elsevier B.V. All rights reserved. This paper was published in Pervasive and Mobile
Computing and is made available with permission of Elsevier B.V.
Downloaded on 27 Mar 2025 02:00:38 SGT
Cross-position Activity Recognition with Stratiﬁed
Transfer Learning
Yiqiang Chenab∗, Jindong Wangab, Meiyu Huangc, Han Yud
aBeijing Key Lab of Mobile Computing and Pervasive Devices, Institute of Computing
Technology, Chinese Academy of Sciences
bUniversity of Chinese Academy of Sciences
cQian Xuesen Lab of Space Technology, China Academy of Space Technology
dSchool of Computer Science and Engineering, Nanyang Technological University
Human activity recognition (HAR) aims to recognize the activities of daily living by utilizing the sensors attached to diﬀerent body parts. The great success
of HAR has been achieved by training machine learning models using suﬃcient
labeled activity data.
However, when the labeled data from a certain body
position (i.e. target domain) is missing, how to leverage the data from other
positions (i.e. source domain) to help recognize the activities of this position?
This problem can be divided into two steps. Firstly, when there are several
source domains available, it is often diﬃcult to select the most similar source
domain to the target domain. Secondly, with the selected source domain, we
need to perform accurate knowledge transfer between domains in order to recognize the activities on the target domain. Existing methods only learn the global
distance between domains while ignoring the local property. In this paper, we
propose a Stratiﬁed Transfer Learning (STL) framework to perform both source
domain selection and activity transfer. STL is based on our proposed Stratiﬁed
distance to capture the local property of domains. STL consists of two components: 1) Stratiﬁed Domain Selection (STL-SDS), which can select the most
similar source domain to the target domain; and 2) Stratiﬁed Activity Transfer
(STL-SAT), which is able to perform accurate knowledge transfer. Extensive
experiments on three public activity recognition datasets demonstrate the superiority of STL. Furthermore, we extensively investigate the performance of
transfer learning across diﬀerent degrees of similarities and activity levels between domains. We also discuss the potential applications of STL in other ﬁelds
of pervasive computing for future research.
Activity Recognition, Transfer Learning, Domain Adaptation,
Pervasive Computing
∗Corresponding author. Email address: .
 
November 15, 2018
 
1. Introduction
Human activity recognition (HAR) aims to seek high-level knowledge from
the low-level sensor inputs . For example, we can detect if a person is walking
or running using the on-body sensors such as the smartphone or the wristband.
HAR has been widely used to applications such as smart care , wireless sensing , adaptive systems , and smart home sensing .
Activities are of great importance to a person’s health status.
person is performing some activities, each of his body parts has certain activity
Thus, sensors can be attached on some body positions to collect
activity data which can be used to build machine learning models in order to
recognize their activities. The combination of sensor signals from diﬀerent body
positions can be used to reﬂect meaningful knowledge such as a person’s detailed
health conditions and working states . Unfortunately, it is nontrivial to
designing wearing styles for a wearable device.
On the one hand, it is not
bearable to equip all the body positions with sensors which makes the activities
not natural. Therefore, we can only attach sensors on limited body positions.
On the other hand, it is impossible to perform HAR if the labels on some body
positions are missing, since the activity patterns on speciﬁc body positions are
signiﬁcant to recognize certain information.
Figure 1: An example of cross-position activity recognition. Firstly, the activity signals on
diﬀerent body parts are often diﬀerent. Secondly, if the labels of a certain part are missing
(the red pentacle), how to leverage the well-labeled activity data on other body parts (the
blue dots) to acquire its labels?
Figure 1 illustrates this situation.
Assume this person is suﬀering from
Small Vessel Disease (SVD) , which is a severe brain disease heavily related
to activities. However, we cannot equip his all body with sensors to acquire
the labels since this will make his activities unnatural. We can only label the
activities on certain body parts in reality. If the doctor wants to see his activity
information on the arm (the red pentacle, we call it the target domain), which
only contains sensor readings instead of labels, how to utilize the information
on other parts (such as torso or leg, we call them the source domains) to help
obtain the labels on the target domain? This is referred to as the cross-position
activity recognition (CPAR). In this paper, we mainly focus on recognizing daily
activities using sensors from a single body position.
The problem of CPAR is extremely challenging. Firstly, we do not know
which body part is the most similar to the target position since the sensor signals
are not independent, but highly correlated because of the shared body structures
and functions. If we use all the body parts as the source domain, there is likely to
be negative transfer because some body parts may be dissimilar. Secondly,
we only have the raw activity data on the target domain without the actual
activity labels, making it infeasible to measure the similarities between diﬀerent
body positions.
Thirdly, even when we know the similar body parts to the
target domain, it is still diﬃcult to build a good machine learning model using
both the source and the target domains. The reason is that signals from diﬀerent
domains are following diﬀerent distributions, which means there are distribution
discrepancies between them. However, traditional machine learning models are
built by assuming that all signals follow the same distribution. Fourthly, when
it comes to multiple persons, the sensor readings are more diﬀerent compared to
diﬀerent body parts on one person. This makes the problem more challenging.
To tackle the above challenges, several transfer learning methods have been
proposed . The key is to learn and reduce the distribution divergence (distance) between two domains. With the distance, we can perform source domain
selection as well as knowledge transfer. Based on this principle, existing methods can be summarized into two categories: exploiting the correlations between
features , or transforming both the source and the target domains into
a new shared feature space .
Existing approaches tend to reduce the global distance by projecting all
samples in both domains into a single subspace. However, they fail to consider
the local property within classes . The global distance may result in loss of
domain local property such as the source label information and the similarities
within the same class. Therefore, it will generate a negative impact on the source
selection as well as the transfer learning process. It is necessary to exploit the
local property of classes to overcome the limitation of global distance learning.
In this paper, we propose a Stratiﬁed Transfer Learning (STL) framework to tackle the challenges of both source domain selection and knowledge
transfer in CPAR. The term stratiﬁed comes from the notion of spliting at different levels and then combining.
We adopt the well-established assumption
that data samples within the same class should lay on the same subspace, even
if they come from diﬀerent domains . Thus, stratiﬁed refers to transformed
This has motivated us to propose the concept of Stratiﬁed distance (SD) in comparison to traditional Global distance (GD). STL has two
components regarding the challenges in CPAR: a Stratiﬁed Domain Selection (STL-SDS) algorithm to select the most similar source domain to the
target domain, and a Stratiﬁed Activity Transfer (STL-SAT) method to
perform activity knowledge transfer between diﬀerent body parts. Both STL-
SDS and STL-SAT are able to exploit the local property of domains, thus they
can achieve promising results in CPAR. Comprehensive experiments on three
large public activity recognition datasets (i.e. OPPORTUNITY, PAMAP2, and
UCI DSADS) demonstrate that STL-SDS is better than the existing global distance approaches used in selecting source domains. STL-SAT outperforms ﬁve
state-of-the-art methods with a signiﬁcant improvement of 7.7% in classiﬁcation
accuracy with improved F1 score.
Contributions. Our contributions are four-fold:
1) We propose the Stratiﬁed Transfer Learning (STL) framework for source
domain selection and knowledge transfer in CPAR. STL is the ﬁrst attempt
to exploit the Stratiﬁed distance (SD) in order to capture the local property
between domains.
SD is a general distance that can be applied to diﬀerent
transfer learning applications.
2) We propose the Stratiﬁed Domain Selection (STL-SDS) algorithm to accurately select the most similar source domain to the target domain. Experiments
demonstrate the superiority of STL-SDS compared to the traditional global
distance measure.
3) We propose the Stratiﬁed Activity Transfer (STL-SAT) method to perform knowledge transfer for cross-position activity recognition.
Experiments
demonstrate signiﬁcantly improved accuracy achieved by STL-SAT compared
to ﬁve state-of-the-art methods.
4) We extensively investigate the performance of cross-position activity recognition on diﬀerent degrees of position similarity and diﬀerent levels of activities.
And we additionally discuss the potential of STL in other pervasive computing
applications, providing experience for future research.
This paper is an extended version of our PerCom paper , where we proposed a stratiﬁed transfer learning algorithm for activity transfer. That algorithm is regarded as STL-SAT in this paper. Beyond that, we further extend the
idea of stratiﬁed learning and propose the stratiﬁed distance as the similarity
measurement for domains. Based on this, we propose the STL-SDS algorithm
for source domain selection. On the top, we unite STL-SDS and STL-SAT into
a framework for CPAR and conduct extensive experiments to evaluate their
performance.
The rest of this paper is organized as follows. Section 2 reviews the related
work. Section 3 introduces the proposed stratiﬁed transfer learning framework.
In Section 4, we present experimental evaluation and analysis on public datasets.
In Section 5, we discuss the potential of the framework in other real applications.
Finally, the conclusions and future work are presented in Section 6.
2. Related Work
2.1. Activity Recognition
Human Activity recognition has been a popular research topic in pervasive
computing for its competence in learning profound high-level knowledge
about human activity from raw sensor inputs. Several survey articles have elaborated on the recent advance of activity recognition using conventional machine
learning and deep learning approaches.
Conventional machine learning approaches have made tremendous progress
on HAR by adopting machine learning algorithms such as similarity-based approach , active learning , crowdsourcing , and other semi-supervised
methods . Those methods typically treat HAR as a standard time series
classiﬁcation problem. And they tend to solve it by subsequently performing
preprocessing procedures, feature extraction, model building, and activity inference.
However, they all assume that the training and test data are with
the same distribution. As for CDAR where the training and the test data are
from diﬀerent feature distributions, those conventional methods are prune to
under-ﬁtting since their generalization ability will be undermined .
Deep learning based HAR achieves the state-of-the-art performance
than conventional machine learning approaches. The reason is that deep learning is capable of automatically extracting high-level features from the raw sensor
readings . Therefore, the features are likely to be more domain-invariant and
tend to perform better for cross-domain tasks. A recent work evaluated deep
models for cross-domain HAR , which provides experience on this area. In
this paper, we mainly focus on the traditional approaches.
2.2. Transfer Learning
Transfer learning has been applied in many applications such as Wi-Fi localization , natural language processing , and visual object recognition .
According to the literature survey , transfer learning can be categorized into
3 types: instance-based, parameter-based, and feature-based methods.
Instance-based methods perform knowledge transfer mainly through instance
re-weighting techniques . Parameter-based methods ﬁrst train a
model using the labeled source domain, then perform clustering on the target
domain. Our framework belongs to the feature based category, which brings
the features of source and target domain into the same subspace where the
data distributions can be the same. A fruitful line of work has been done in
this area . STL diﬀers from existing feature-based methods in the
following aspects:
Exploit the correlations between features. proposed structural
correspondence learning (SCL) to generatively learn the relation of features. 
applied a feature-level transfer model to learn the dependence between domains,
then trained a domain-adapted classiﬁer. Instead of modeling the relationship
of domain features, SAT transforms the domain data into a new subspace, which
does not depend on the domain knowledge in modeling features.
Transform domains into new feature space. proposed maximum
mean discrepancy embedding (MMDE) to learn latent features in the reproducing kernel Hilbert space (RKHS). MMDE requires solving a semideﬁnite
programming (SDP) problem, which is computationally prohibitive. extended MMDE by Transfer Component Analysis (TCA), which learns a kernel
in RKHS. adopted a similar idea. learns target predictive function with
a low variance. sampled the domain features by viewing the data in a Grassmann manifold to obtain subspaces. exploited the low-dimensional structure to integrate the domains according to geodesic ﬂow kernel (GFK). Long
et al. proposed joint distribution adaptation (JDA) based on minimizing joint
distribution between domains, while SAT focuses on the marginal distribution.
 proposed transfer kernel learning (TKL), which learned a domain-invariant
kernel in RKHS. studied the conditional transfer components between domains. Methods in these literature tend to learn some common representations
in the new feature space, then a global domain shift can be achieved. However,
the diﬀerence between individual classes is ignored.
2.2.1. Source Domain Selection
The work ﬁrst proposed a source-selection free transfer learning approach. They choose the source samples that are close to the target samples
using the Laplacian Eigenmap. The work followed this idea in the text classiﬁcation. However, both of them only focused on the sample selection, while
our STL-SDS focuses on the selection of the whole domain. Collier et al. 
investigated the transfer performance of diﬀerent layers of a neural network in
a grid search manner. But they did not perform source selection. The work 
developed a relation network, which can be used to evaluate the distance between diﬀerent image samples. Yet they still focused on the global distance.
Authors in proposed a greedy multi-source selection algorithm. This selection algorithm could iteratively select the best K source domains and then
perform transfer learning based on this selection. However, their method still
relies on the similarity calculated by the global distance. Our STL-SDS is the
ﬁrst work to perform source domain selection using the local property.
2.2.2. Transfer Learning based Activity Recognition
Some existing work also focused on transfer learning based HAR . Among
existing work, Zhao et al. proposed a transfer learning method TransEMDT 
using decision trees, but it ignored the intra-class similarity within classes. 
proposed the TransAct framework, which is a boosting-based method and ignores the feature transformation procedure. Thus it is not feasible in most activity cases. Feuz et al. proposed a heterogeneous transfer learning method
for HAR, but it only learns a global domain shift. A more recent work of Wang
et al. performs activity recognition using deep transfer learning, which also
focused on the global distance between domains. To the best of our knowledge,
our STL framework is the ﬁrst attempt towards learning a stratiﬁed distance to
capture the local property of domains.
3. Stratiﬁed Transfer Learning
In this section, we introduce the Stratiﬁed Transfer Learning (STL) framework for cross-position activity recognition.
3.1. Problem Deﬁnition
The goal of cross-position activity recognition (CPAR) is to predict the activities of one body part (i.e. the target domain) using existing labeled data
from another body part (i.e. the source domain). Formally speaking, we can use
i=1 to denote the labeled source domain, and Dt = {(xt
denotes the target domain. Note that yt
j is missing, i.e. it is the goal of CPAR.
In CPAR, we assume the sensor modalities and the activity categories are equal
in all body parts, thus the feature space X s = X t and label space Ys = Yt.
Here, d denotes the feature dimension. We use C to denote the total categories
of both domains. The diﬀerent data distributions on diﬀerent body parts means
that the marginal and conditional distributions between domains are diﬀerent.
Therefore, P(xs) ̸= P(xt), and Q(ys|xs) ̸= Q(yt|xt).
Note that we only focus on the setting where the source and the target domains are sharing the same label spaces (i.e. Ys = Yt) since this setting is rather
common in transfer learning research and many previous work . This
is likely to be a class imbalance problem in theory when some particular classes
are with extremely few samples. However, since our setting is based on the common activities of daily living where there exist enough samples for each class,
we will leave class imbalance problem as a future work.
This problem is extremely challenging. We neither know which body part
is the most similar to the target domain, nor we know how to build a machine
learning model to deal with the diﬀerent distributions. Accordingly, we separate it into two steps: 1) Determine the most similar source domain to the
target domain. 2) Transfer the knowledge from the source domain to the target
3.2. Motivation
The key to successful activity transfer learning is to utilize the similarity
between the source and the target domain . It is not diﬃcult to directly
measure the similarity between domains using existing distance functions. For
instance, we can adopt the Euclidean distance to compute the sample-wise distance and then average them; or we can use the Kullback-Leibler divergence 
to calculate the probability distance between domains.
Unfortunately, these
computing approaches are only calculating the global distance between domains
since they are applied to the whole domain. The global distance may result
in loss of domain local property such as the source label information and the
similarities within the same class. Therefore, it will generate a negative impact
on the source selection as well as the transfer learning process.
In order to capture the local property of domains, we adopt the assumption : the data samples from the same class should lay on the same subspace,
even if they belong to diﬀerent domains. By following this assumption, we propose the stratiﬁed distance to capture the local property of domains. The stratiﬁed distance refers to the class-wise distance between two diﬀerent domains.
For example, if there are 5 classes in two domains: Ds = (D(1)
s , . . . , D(5)
(a) Original target
(b) Global dist.
(c) Stratiﬁed dist.
Figure 2: Traditional global distance (GD) and proposed stratiﬁed distance (SD).
Dt = (D(1)
t , . . . , D(5)
t ), then the stratiﬁed distance (SD) should be calculated as:
where Dist(·) denotes some distance function such as the Euclidean distance.
As a comparison, the global distance (GD) can be represented as:
GD = Dist(Ds, Dt).
Figure 2 brieﬂy illustrates the results of global and our stratiﬁed distance
using two classes of the same target domain . It indicates that SD could
not only help to learn good classiﬁcation function, but helps to obtain tighter
within-class distance.
3.3. The STL Framework
We can exploit the proposed Stratiﬁed distance (SD) measure to perform
source domain selection and activity transfer in CPAR. Technically, there are
two critical challenges ahead. Firstly, SD is based on the labels of both domains,
while the target domain has no labels. Secondly, even if we could obtain the
labels for the target domain, it is nontrivial to solve Eq. 4 since we do not know
how to choose the Dist(·) function.
In this paper, we propose the Stratiﬁed Transfer Learning (STL) framework to address both of these two challenges.
Firstly, STL uses a popular
majority voting technique to acquire the labels for the target domain. Secondly,
STL exploits an eﬀective distance measure called Maximum Mean Discrepancy
(MMD) to calculate the SD distance and perform Intra-class transfer. Finally,
STL could perform source domain selection and activity transfer based on the
calculated distance. Figure 3 illustrates the main idea of the STL framework.
3.3.1. Majority voting
We can easily obtain the pseudo labels for the target domain based on a
weak classier such as 1-nearest-neighbor (1NN) trained on the source domain
by following existing methods . However, we consider that since there is a
large distribution discrepancy between two domains, a simple 1NN classiﬁer may
not work well. Therefore, we propose to use the majority voting technique to
Stratified
Domain Selection
Stratified
Activity Transfer
Activity labels
Figure 3: Main idea of the Stratiﬁed Transfer Learning (STL) framework. There are two steps:
(1) Stratiﬁed Domain Selection (STL-SDS), which can select the most similar source domains
to the target domains. (2) Stratiﬁed Activity Transfer (STL-SAT), which can perform activity
recognition on the target domain based on transfer learning. Both STL-SDS and STL-SAT
are well exploiting the idea of our proposed stratiﬁed distance.
exploit the knowledge from the crowd . The idea is that one certain classiﬁer
may be less reliable, so we assemble several diﬀerent classiﬁers to obtain more
reliable pseudo labels.
To this end, STL makes use of some base classiﬁers
learned on Ds to collaboratively learn the labels for Dt.
Let Aj(j = 1, 2, · · · , n2) denotes the ﬁnal result of majority voting on xtj,
and ft(j) denotes the prediction of the j-th sample by the t-th classiﬁer ft(·),
majority(ft(j), t)
if majority holds
where t ∈{1, 2, · · · } denotes the index of the classiﬁer.
The majority voting technique generally ensembles all the classiﬁers learned
in the source domain.
The condition “majority holds” refers to any potential scheme that helps to generate a better solution such as simple voting and
weighted voting. Speciﬁcally, Aj could be deﬁned as a) if most classiﬁers have
the same results on a sample, we take its label, else we label it ‘-1’; b) same as
a) with voting weights to classiﬁers; c) the stacking of some base classiﬁers. In
theory, the classiﬁers can be of any type.
Formally, we call the samples with pseudo labels in the target domain Candidates, which is denoted as Xcan. For those samples with label ‘-1’, we call
them Residuals and denote as Xres.
Using majority voting, STL can generate reliable pseudo labels for the target domain. These pseudo labels will act as evidence of the following transfer
learning algorithms.
The eﬀectiveness of this technique will be evaluated in
later sections.
3.3.2. Intra-class transfer
In this step, STL exploits the local property of domains to further transform
each class of the source and target domains into the same subspace. Since the
properties within each class are more similar, the Intra-class transfer technique
will guarantee that the transformed domains have the minimal distance. In this
step, we only consider the candidates from the target domain for their reliable
pseudo labels.
Initially, Ds and Xcan are divided into C groups according to their (pseudo)
labels, where C is the total number of classes. Then, feature transformation
is performed within each class of both domains. Finally, the results of distinct
subspaces are merged.
In order to achieve intra-class transfer, we need to calculate the distance
between each class. Since the target domain has no labels, we use the pseudo
labels from majority voting.
For the candidates and the source domain, we
calculate their intra-class distance using the stratiﬁed distance:
where D(c)
can denote the samples from class c in the source and candidates, respectively.
This distance can easily be calculated using existing metrics such as Euclidean distance and Kullback-Leibler (KL) divergence . However, the Euclidean distance and KL divergence are too general for activity recognition problem, which ignores the label information of the source domain. Moreover, the
KL divergence needs to ﬁrst estimate the probability of both domains, which
is trivial and not suitable since the target domain has no labels. Therefore,
we need to calculate the similarity between domains while considering the label
information on the source domain.
In order to calculate the function Dist(·) in Eq. 1, we adopt Maximum Mean
Discrepancy (MMD) as the measurement. MMD is a nonparametric method
to measure the divergence between two distinct distributions and it has been
widely applied to many transfer learning methods . The MMD distance
between two domains can be formally computed as:
D(Ds, Xcan) =
where H denotes reproducing kernel Hilbert space (RKHS). n(c)
can|. Here φ(·) denotes some feature map to map the original samples to
RKHS. The reason we do not use the original data is that the features are often
distorted in the original feature space, and it can be more eﬃcient to perform
knowledge transfer in RKHS .
Therefore, given a predeﬁned mapping function φ(·), we can compute the
intra-class distance between two domains.
Remark: The majority voting and intra-class transfer are common steps in
STL. In the next sections, we will elaborate on how to use the STL framework
to perform source domain selection and activity transfer.
3.4. Stratiﬁed Domain Selection
Given a set of body parts with label information, we have to determine the
body part that has the most similar property to the target body part. In this
paper, based on the STL framework, we propose the Stratiﬁed Domain Selection
(STL-SDS) algorithm to handle this challenge. Based on the proposed Stratiﬁed
distance, STL-SDS well exploits the local property of diﬀerent domains as well
as the supervised information on the source domain.
We adopt a greedy technique in STL-SDS. We know that the most similar
body part to the target is the one with the most similar structure and body
functions. Therefore, we use the distance to reﬂect their similarity. We calculate
the stratiﬁed distance according to Eq. 5 between each source domain and the
target domain and select the one with the minimal distance.
Unfortunately, it is non-trivial to solve Eq. 5 directly since the mapping
function φ(·) is to be determined. Simply use a certain function will ruin the
local property of domains. Thus, we turn to some kernel methods. We deﬁne
a kernel matrix K ∈R(n1+n2)×(n1+n2), which can be constructed by the inner
product of the mapping:
Kij = ⟨φ(xs
j are samples from either Ds or Xcan.
Therefore, we can easily calculate this equation without losing the local
property by giving φ(·) a certain implementation.
In our work, we use the
Radical Basis Funcation (RBF), which can be deﬁned as:
where σ is the kernel bandwidth.
The complete learning process of STL-SDS is in Algorithm 1.
Algorithm 1 STL-SDS: Stratiﬁed Domain Selection
A list of source domains Ds1, · · · , DsK, target domain Dt.
The source domain that has the minimal distance to Dt.
1: Initialize a source domain set S = {};
2: Perform majority voting on Dt to acquire Xcan;
3: Obtain the pseudo labels yt for Dt;
4: for i = 1 to K do
Compute the distance SDi between Dsi and Xcan using Eq. 5;
Add SDi to S;
7: end for
8: return Index j of source domain that SDj = min{S}.
3.5. Stratiﬁed Activity Transfer
After source domain selection, we can obtain the most similar body part
to the target domain. The next step is to design an accurate transfer learning
algorithm to perform activity transfer. In this paper, we propose a Stratiﬁed
Activity Transfer (STL-SAT) method for activity recognition. STL-SAT is also
based on our stratiﬁed distance, and it can simultaneously transform the individual classes of the source and target domains into the same subspaces by
exploiting the local property of domains. After feature learning, STL can learn
the labels for the candidates. Finally, STL-SAT will perform a second annotation to obtain the labels for the residuals. The process of STL-SAT can be seen
in Figure 4.
Intra-class
Figure 4: Main idea of Stratiﬁed Activity Transfer (STL-SAT). There are three steps: (1) Candidates generating to generate pseudo labels for the target domain; (2) Perform intra-class
transfer between source domain and candidates; (3) Perform activity transfer using the transferred data.
The feature transformation is also based on the intra-class transfer technique
in Eq. 5. However, diﬀerent from STL-SDS where we use a certain mapping
function such as RBF for feature mapping, we learn this mapping φ(·) in this
step. The reason is that STL-SDS only needs one particular feature map to
avoid the feature distortion, while STL-SAT will ﬁnd the optimal feature map
with the minimal domain distance.
The learning of this feature map also starts with Eq. 5. For eﬃcient learning,
we introduce a feature transformation matrix W ∈R(n1+n2)×m to transform
the samples of both domains from the original space to the RKHS. Here m ≪
d denotes the dimension after feature transformation. Thus, learning φ(·) is
equal to learning the matrix W. Then, by applying kernel tricks, Eq. 5 can be
eventually formulated as the following trace optimization problem:
tr(W⊤KLcKW) + λtr(W⊤W)
W⊤KHKW = I
There are two terms in the objective function of Eq. 8.
The ﬁrst term
c=1 tr(W⊤KLcKW)) denotes the MMD distance of each class between source
and target domain, and the second one (λtr(W⊤W)) denotes the regularization term to ensure the problem is well-deﬁned with λ the trade-oﬀparameter.
The constraint in Eq. 8 is used to guarantee that the transformed data (W⊤K)
will still preserve some structure property of the original data. Ins+nt is the
Algorithm 2 STL-SAT: Stratiﬁed Activity Transfer
Source domain Ds, target domain Dt, dimension m.
The labels for the target domain: {yt}.
Perform majority voting on Dt using Eq. 3 to get {Xcan, eycan} and Xres;
2: Construct kernel matrix K according to Eq. 6 using Xsrc and Xcan, and
compute the intra-class MMD matrix Lc using Eq. 9;
Solve the eigen-decomposition problem in Eq. 11 and take the m smallest
eigen-vectors to obtain the transformation matrix W;
Transform the same classes of Xs and Xcan into the same subspaces using
W, and then merge them;
Perform second annotation to get {ˆycan} and {ˆyres};
Construct kernel matrix K and compute the intra-class MMD matrix Lc
using Eq. 9;
8: until Convergence
return {yt}.
identical matrix, and H = Ins+nt −1/(ns + nt)11⊤is the centering matrix. For
notational brevity, we will drop the subscript for Ins+nt in the sequel. Lc is the
intra-class MMD matrix, which can be constructed as:
xi, xj ∈D(c)
xi, xj ∈X(c)
s , xj ∈X(c)
can, xj ∈D(c)
Learning algorithm: Acquiring the solution of Eq. 8 is non-trivial. To this
end, we adopt Lagrange method as most of the existing work did . We
denote Φ the Lagrange multiplier, then the Lagrange function can be derived
+ λtr(W⊤W)
 (I −W⊤KHK⊤W)Φ
Setting the derivative ∂L/∂W = 0, Eq. 10 can be ﬁnally formalized as an
generalized eigen-decomposition problem
W = KHK⊤WΦ
Solving Eq. 11 refers to solve this generalized eigen-decomposition problem
and take the m smallest eigenvectors to construct W. W can transform both
domains into the same subspace with minimum domain distance while preserving their properties. Since the knowledge transfer pertains to each class, we call
this step intra-class transfer, and that is where the name stratiﬁed originates
from. After this step, the source and target domains belonging to the same class
are simultaneously transformed into the same subspaces.
After solving the above optimization problem, it is easy to get more reliable predictions (ˆycan) of the candidates.
Speciﬁcally, we train a standard
classiﬁer using {[W⊤K]1:n1,:, ys} and apply prediction on [W⊤K]n1+1:n2,:. Finally, the labels of residuals can be obtained by training classiﬁer on instances
{Xcan, ˆycan}.
The labels of candidates can be correspondingly close to the
ground truth by annotating twice since the domains are now in the same subspace after intra-class transfer.
The overall process of STL-SAT is described in Algorithm 2.
Remark: It should be noted that STL-SAT could achieve a better prediction
if we use the result of second annotation as the initial state and run intra-class
transfer iteratively.
This EM-like algorithm is empirically eﬀective and will
be validated in the following experiments. Additionally, we only use majority
voting in the ﬁrst round of the iteration, then the proposed SAT could iteratively
reﬁne the labels for the target domain by only using its previous results.
4. Experimental Evaluation
In this section, we evaluate the performances of STL framework (STL-SDS
and STL-SAT) via extensive experiments on public activity recognition datasets.
4.1. Datasets and Preprocessing
Three large public datasets are adopted in experiments. Table 1 provides
a brief introduction to them.
In the following, we brieﬂy introduce those
datasets, and more information can be found in their original papers.
PORTUNITY dataset (OPP) is composed of 4 subjects executing diﬀerent
levels of activities with sensors tied to more than 5 body parts. PAMAP2 dataset
(PAMAP) is collected by 9 subjects performing 18 activities with sensors
on 3 body parts. UCI daily and sports dataset (DSADS) consists of 19
activities collected from 8 subjects wearing body-worn sensors on 5 body parts.
Accelerometer, gyroscope, and magnetometer are all used in these datasets.
Table 1: Statistical information of three public datasets for activity recognition
Back (B), Right Upper Arm (RUA), Right Left Arm (RLA), Left Upper Arm (LUA), Left Lower Arm (LLA)
Hand (H), Chest(C), Ankle (A)
Torso (T), Right Arm (RA), Left Arm (LA), Right Leg (RL), Left Leg (LL)
Figure 5 illustrates the positions we investigated in three datasets. In our
experiments, we use the data from all three sensors in each body part since
most information can be retained in this way. For one sensor, we combine the
data from 3 axes together using a =
x2 + y2 + z2.
Then, we exploit the
sliding window technique to extract features (window length is 5s). The feature
extraction procedure is mainly executed according to existing work .
total, 27 features from both time and frequency domains are extracted for a
single sensor. Since there are three sensors (i.e. accelerometer, gyroscope, and
magnetometer) on one body part, we extracted 81 features from one position.
Table 2 shows the features we extracted for each dataset.
Left Upper
Left Lower
Right Upper
Right Lower
OPPORTUNITY
Figure 5: Diﬀerent positions on OPPOPTUNITY, PAMAP2 and DSADS.
In order to better exploit these three datasets, we perform two aspects of
cross-position activity recognition: Within Dataset and Cross dataset. Within
Dataset refers to perform CPAR inside a particular dataset. For instance, we
can learn the labels for the Right Arm (RA) of the DSADS dataset by using
labeled data from the other four body parts. In contrast, Cross dataset uses the
labeled body parts from another diﬀerent dataset as the source domains. In our
experiments, we use DSADS and OPP datasets for Within dataset since they
contain more body parts than PAMAP. We use all classes for each dataset in
this aspect. For Cross Dataset, we use the body parts from DSADS and OPP
as the target domain, and use body parts from PAMAP as the source domain
since there are more samples in PAMAP than other two datasets (Table 1).
Note that there are diﬀerent activities in three datasets. Thus we extract 4
common classes: Walking, Sitting, Lying, and Standing.
4.2. Evaluation of Stratiﬁed Domain Selection
We perform source domain selection using the proposed STL-SDS (i.e. the
stratiﬁed distance (SD)) algorithm on both the Within Dataset and Cross
Dataset aspects.
The comparison method is the global distance (GD) using
It is worth noting that there is no eﬀective evaluation metric for this domain selection problem. We can never quantitatively know the actual distance
between two activity domains. Thus, we evaluate the similarity based on classiﬁcation accuracy using the same classiﬁer following existing work . For
instance, for source domains A and B and target domain C, we respectively
train a linear SVM classiﬁer using A or B and apply prediction on C. The accuracy acts as the ground truth for domain similarity: higher accuracy means
shorter distance. Then, the source domain with the highest accuracy is the right
source for the target domain. Therefore, we can obtain the ‘ground truth’ for
the source domain selection.
Table 2: Features extracted per sensor on each body part
Description
Average value of samples in window
Standard deviation
The value with the largest frequency
Maximum minus minimum
Mean crossing rate
Rate of times signal crossing mean value
Direct component
Spectrum peak position
First 5 peaks after FFT
Frequencies corresponding to 5 peaks
Square of norm
Four shape features
Mean, STD, skewness, kurtosis
Four amplitude features
Mean, STD, skewness, kurtosis
Figure 6 shows the average source domain selection accuracy of the global
and stratiﬁed distance. It is obvious that our proposed SD distance achieves
better performance than the traditional global distance.
Note that in both
Within Dataset and Cross Dataset scenarios, SD distance outperforms GD. Furthermore, for the even challenging Cross Dataset distance where the similarity
between the source and the target domains are little, SD could signiﬁcantly
outperform the traditional GD distance. This indicates the eﬀectiveness of our
proposed STL-SDS algorithm.
Within Dataset
Cross Dataset
Accuracy (%)
Global distance
Stratified distance
Ground truth
Figure 6: The comparison between global and stratiﬁed distance as the similarity measurement
for domain selection.
Now we dig deeper into the results. Table 3 shows the selected source domains of GD and SD, and their accuracies are in Table 4. We also report the
Table 3: Comparison of Global (G) and Stratiﬁed (S) distance on source domain selection.
The symbol item denotes that the item is not consistent with the ground truth.
Target Dataset
Target Position
Table 4: Accuracy (%) of Global (GD) and Stratiﬁed (SD) distance on source domain selection.
Target Dataset
Target Position
Target as source
accuracy when target itself is as the source domain for comparison (which is
the ideal state that can never be satisﬁed). From these tables, we observe: 1)
The proposed Stratiﬁed distance can select better source domain than the traditional global distance with close performance with the ground truth. 2) The
situation when the target domain as the source domain can achieve the best
performance. However, it is only the ideal state since there are always the label
scarcity problems. It indicates that CPAR is a challenging task since both of
the Within Dataset and Cross Dataset aspects only achieve worse performance,
which ensures the necessity of transfer learning algorithm. 2) Generally speaking, the most similar body parts to a side Arm or Leg is its other side. This
observation is much easier to understand as common sense. 3) Torso (or Back /
Chest) is the most similar body part to other body parts such as Arms and Legs.
This is probably because the Torso is physically connected to Arms and Legs,
leading to similar moving patterns. Therefore, most of the target domains can
leverage the labeled information from Torso to build models. 4) It is important
to notice that although SD achieves good results, its performance is not 100%
right. This indicates that it is extremely diﬃcult to perform source selection.
We expect to increase the performance of SD in future research.
4.3. Evaluation of Stratiﬁed Activity Transfer
We evaluate the performance of STL-SAT in both Within Dataset and Cross
Dataset aspects. The state-of-the-art comparison methods include:
• PCA: Principal component analysis .
• KPCA: Kernel principal component analysis .
• TCA: Transfer component analysis .
• GFK: Geodesic ﬂow kernel .
• TKL: Transfer kernel learning .
PCA and KPCA are classic dimensionality reduction methods, while TCA,
GFK, and TKL are representative transfer learning approaches. The codes of
PCA and KPCA are provided in Matlab. The codes of TCA, GFK, and TKL
can be obtained online 1.
We construct several CPAR tasks according to each scenario and use the
labels for the target domain only for testing by following the common setting in
existing work . Other than TKL, all other methods require dimensionality reduction. Therefore, they were tested using the same dimension. After that,
a classiﬁer with the same parameter is learned using the source domain and then
the target domain can be labeled. To be more speciﬁc, we use the random forest
classiﬁer (#Tree = 30) as the ﬁnal classiﬁer for all the 6 methods. For majority
voting in STL-SAT, we simply use SVM (C = 100), kNN (k = 3), and random
forest (#Tree = 30) as the base classiﬁers. Other parameters are searched to
achieve their optimal performance. The #iteration is set to be T = 10 for SAT.
Other parameters of STL-SAT are set λ = 1.0, d = 30, kernel = linear. It is
noticeable that we randomly shuﬄe the experimental data 5 times in order to
gain robust results.
Classiﬁcation accuracy on the target domain is adopted as the evaluation
metric, which is widely used in existing transfer learning methods 
Accuracy = |x : x ∈Dt ∧ˆy(x) = y(x)|
|x : x ∈Dt|
where y(x) and ˆy(x) are the truth and predicted labels, respectively.
Additionally, we also use the F1 score as another measurement of the results.
F1 score can be calculated as
where P, R are the precision and recall, respectively.
We run STL-SAT and other methods on all tasks and report the classiﬁcation
accuracy in TABLE 5. The F1 scores of all the methods are in Table 6. For
brevity, we only report the results of A →B since its result is close to B →
It is obvious that SAT signiﬁcantly outperforms other methods in most
cases (with a remarkable improvement of 7.7% over the best baseline GFK).
Compared to traditional dimensionality reduction methods (PCA and KPCA),
STL-SAT improves the accuracy by 10% ∼20%, which implies that SAT is
better than typical dimensionality reduction methods. Compared to transfer
learning methods (TCA, GFK, and TKL), STL-SAT still shows an improvement
of 5% ∼15%. Therefore, STL-SAT is more eﬀective than all the comparison
methods in most cases.
The performance of TKL is the worst, because of the instability of the transfer kernel. TCA only learns a global domain shift, thus the similarity within
1 
Table 5: Classiﬁcation accuracy (%) of STL-SAT and other comparison methods.
Within Dataset
Cross Dataset
PAMAP →OPP
Table 6: F1 score of STL-SAT and other comparison methods.
Within Dataset
Cross Dataset
PAMAP →OPP
classes is not fully exploited. The performance of GFK is second to SAT, even
if GFK also learns a global domain shift. Because the geodesic distance in highdimensional space is capable of preserving the intra properties of domains. The
diﬀerences between STL-SAT and GFK are: 1) STL-SAT outperforms GFK
in most cases with signiﬁcant improvement; 2) STL-SAT strongly outperforms
GFK in Within Dataset category, indicating that SAT is more robust in recognizing diﬀerent levels of activities. For SAT, it performs intra-class knowledge
transfer after generating pseudo labels for candidates. Thus, better performance
can be achieved by exploiting the local property of classes.
Figure 7 shows the classiﬁcation accuracy of diﬀerent methods of each person
on RA →LA task of the DSADS dataset. It clearly indicates that STL-SAT
achieves the best performance on most persons. On other datasets, the results
are the same. We further did a case study by showing the confusion matrices
of all methods for task RUA →LUA on OPP dataset in Table 7. The confusion matrix helps to analyze which class is easier to classify. From the results,
we can observe that in CPAR, all the classes are easy to be wrongly classi-
ﬁed since there is distribution divergence between domains. Of all the classes,
Walking and Standing are two classes that are more easily to be misclassiﬁed.
In this situation, our proposed STL-SAT could signiﬁcantly outperform other
comparison methods. In other situations, STL-SAT can still obtain compara-
Accuracy (%)
Figure 7: Classiﬁcation accuracy of diﬀerent methods of each person on RA →LA task.
ble performance. Overall, STL-SAT achieves the best classiﬁcation accuracy.
On other tasks, the results are almost the same. It indicates the superiority
of STL-SAT. We also noticed that on some persons (P6, P7, and P8), the performance of STL-SAT is worse. This may be because the sensor data of these
three persons are a little mixed during the activity transition.
4.4. Further Analysis
In this section, we conduct further experiments to discover more insights in
cross-position activity recognition.
4.4.1. Performance on Diﬀerent Degrees of Similarities
Since the similarity between positions is extremely important to CPAR, we
want to explore the performance of transfer learning methods in more ﬁnegrained similarity degrees. Generally speaking, positions in Within Dataset are
with more similarity than Cross Dataset. But in one dataset, can we ﬁnd more
degrees of similarities? It seems that in one person, Right Arm is more similar to
Left Arm than to Torso. In order to explore this, we average the performance of
all the methods in Figure 8(a). We use Highly Likely to denote the similar body
parts such as RA →LA, Likely to denote the dissimilar body parts such as RA
→T, and Less Likely to denote the Within Dataset for notational consistency.
For all the methods, the accuracy drops as the domain similarity becomes
Additionally, the performance of STL-SAT is the best in all scenarios.
In diﬀerent degrees of similarities, the change of classiﬁcation accuracy of each
method follows the same tendency. Speciﬁcally, the performance of all the methods is the best between similar body parts for the same person (e.g. RA →LA).
The performance becomes worse for diﬀerent body parts (e.g. RA →T). This
is because Right Arm (RA) is more similar to Left Arm (LA) than to Torso (T).
For a diﬀerent person, all methods produce the worst results because diﬀerent
people have exactly diﬀerent body structure and moving patterns (e.g. T →T
Table 7: Confusion matrix of STL-SAT and other comparison methods on RUA →LUA of
OPP dataset. Other tasks are following the same tendency. In the table, symbols a, b, c, and
d refer to four common activities: Walking, Lying, Standing, and Sitting
across datasets). At the same degree of similarity, the results are also diﬀerent.
For example, the performance of RUA →T is better than RLA →T in the same
dataset. Because there is more similarity between Right Upper Arm (RUA) and
Torso (T) than between Right Lower Arm (RLA) and Torso (T). Other positions
also share a similar discovery.
All the experimental results indicate that the similarity between the source
and target domain is important for cross-domain learning. In real life, other
factors such as age and hobby also help deﬁne the similarity of activities. For
other cross-domain tasks (image classiﬁcation etc.), ﬁnding the relevant domain
is also important. For example, the most similar image set for a dog is probably
a cat, since those two kinds of animals have similar body structures, moving
patterns, and living environments.
4.4.2. Performance on Diﬀerent Levels of Activities
The previous experiments are mainly on the activity of daily living, which is
mostly body movements and sports activities. In real life, there are more than
this level of activities. For instance, Having Coﬀee is much higher than Walking
since it requires more environmental information. Diﬀerent levels of activities
imply the diﬀerent depths of activity granularities. In this section, we extensively investigate the performance of CPAR on diﬀerent levels of activities. By
Highly likely
Less likely
Levels of similarity
Accuracy (%)
(a) Diﬀerent degrees of similarities
Levels of activitiy
Accuracy (%)
(b) Diﬀerent levels of activities of 3 persons
Figure 8: Classiﬁcation accuracy of (a) diﬀerent degrees of similarities and (b) diﬀerent levels
of activities in transfer learning.
taking advantage of the diverse activity classes in OPP dataset , we analyze
the transfer learning performance on low-level (OPP-LL), middle-level (OPP-
ML), and high-level (OPP-HL) activities. The results are presented in Figure 8(b). It indicates the best performance is achieved at middle-level activities,
while it suﬀers from low-level and even worse at high-level activities.
Low-level activities such as Walking and middle-level activities such as Closing are mostly contributed by the atomic movements of body parts and they are
likely to achieve better transfer results than the high-levels. On the other hand,
high-level activities such as Coﬀee Time not only involve basic body movements
but also contain contextual information like ambient or objects, which is diﬃcult
to capture only by the body parts. Since the bridge of successful cross-position
transfer learning is the similarity of body parts, it is not ideal to achieve good
transfer performance by only using body parts. The reason why results on OPP-
ML are better than OPP-LL is that activities of OPP-ML are more ﬁne-grained
than OPP-LL, making it more capable of capturing the similarities between the
body parts.
4.5. Eﬀectiveness Analysis
In this section, we verify the eﬀectiveness of STL-SAT in several aspects since
it is more complicated than STL-SDS. The core idea of STL-SAT is intra-class
transfer, where the pseudo labels of the candidates are acting as the evidence of
transfer learning. It is intuitive to ask the following three questions. Firstly, Can
the conﬁdence of the pseudo labels have an inﬂuence on the algorithm? It seems
that STL-SAT will not achieve good performance if there is not enough reliable
candidates available.
Secondly, Can the choice of majority voting classiﬁers
aﬀect the framework? If we use diﬀerent classiﬁers, the performance is likely to
vary. Thirdly, Is the performance of STL-SAT robust to parameter selection?
There are several parameters in STL-SAT: trade-oﬀparameter λ and dimension
Candidate usage (%)
Accuracy (%)
(a) Candidate usage
# Iteration
Accuracy (%)
(b) Iteration
# Dimension
Accuracy (%)
(c) Parameter sensitivity
Figure 9: Detailed results: a) Candidates usage of STL-SAT for RA →LA on DSADS dataset;
b) Convergence of 1NN-SAT and STL-SAT; c) Parameter sensitivity
m. Will they aﬀect the performance? In this part, we answer these questions
through the following experiments.
1) The conﬁdence of the candidates: We control the percentage of the
candidates from 10% to 100% in every trial and make the rest belong to the
residual part. Then we test the performance of STL-SAT. We test on the task
LA →RA on DSADS and compare with other 2 methods (PCA and TCA). The
result is shown in Figure 9(a). For simplicity, we only compare STL-SAT with
PCA and TCA, since they are both classic dimensionality reduction methods.
From the results, we can observe that the performance of STL-SAT is increasing
along with the increment of candidates percentage. More importantly, STL-
SAT outperforms the other two methods with less than 40% of candidates. It
reveals that SAT does not largely rely on the conﬁdence of the candidates and
can achieve good performance even with fewer candidates.
On the other hand, we also noticed that the accuracy of the majority voting classiﬁers may signiﬁcantly inﬂuence the results of STL. Luckily, a previous
work has veriﬁed that the accuracy of the voting classiﬁers will not heavily
inﬂuence the ﬁnal results. In their experiments, they chose diﬀerent kinds of
classiﬁers to generate pseudo labels. The results of diﬀerent classiﬁers are showing that the ﬁnal results will not heavily rely on the power of the classiﬁers.
2) Majority voting classiﬁers and iteration: To test the eﬀectiveness
of majority voting classiﬁers, we choose 1 nearest neighbor (1NN) as the base
classiﬁer of majority voting. Then we run SAT iteratively. The results are shown
in Figure 9(b), where ‘1NN-SAT’ and ‘SAT’ denotes the result of STL-SAT
using 1NN and random forest as the base majority voting classiﬁer, respectively.
From those results, we can observe: 1) STL-SAT can iteratively improve the
classiﬁcation accuracy even with some weak majority voting classiﬁer. 1NN-
SAT achieves slightly worse than STLSAT, indicating that STL-SAT is rather
robust to the base classiﬁers. Since more powerful classiﬁers would lead to better
performance, we strongly suggest using more reliable classiﬁers for majority
voting in real problems. 2) STL-SAT can reach a quick convergence within fewer
than 10 iterations. This indicates that STL-SAT can be eﬃciently trained.
3) Parameter sensitivity. STL-SAT involves two parameters: the dimension m, and trade-oﬀparameter λ. In this experiment, we empirically evaluate
the sensitivity of m. The evaluation of λ follows the same tendencies. We set
m ∈{10, 20, 30, 40} and test the performance of SAT and other dimensionality
reduction methods. As shown in Figure 9(c), STL-SAT achieves the best accuracy under diﬀerent dimensions. Meanwhile, the accuracy of STL-SAT almost
does not change with the decrement of m. The results reveal that STL-SAT is
much more eﬀective and robust than other methods under diﬀerent dimensions.
Therefore, STL-SAT can be easily applied to many cross-domain tasks which
require robust performance w.r.t. diﬀerent dimensions.
5. Potentials of STL in Pervasive Computing
We studied cross-domain activity recognition through the proposed STL
framework and evaluated its performance on cross-position HAR tasks. There
are more applications in pervasive computing that STL could be used for. In
this section, we discuss the potential of STL in other applications.
1) Activity recognition.
The results of activity recognition can be diﬀerent according to diﬀerent devices, users, and wearing positions.
it possible to perform cross-device/user/position activity recognition with high
accuracy. In case cross-domain learning is needed, ﬁnding and measuring the
similarity between the device/user/position is critical.
2) Localization.
In WiFi localization, the WiFi signal changes with the
time, sensor, and environment, causing the distributions to diﬀer. Therefore, it
is necessary to perform cross-domain localization. When applying SAT to this
situation, it is also important to capture the similarity of signals according to
time/sensor/environment.
3) Gesture recognition. Due to the diﬀerences in hand structure and movement patterns, the current gesture recognition models cannot generalize well.
In this case, STL can be a good option. Meanwhile, special attention needs to
be paid to the divergence between the diﬀerent characteristics of the subjects.
4) Other context-aware applications. Other applications include smart home
sensing, intelligent city planning, healthcare, and human-computer interaction.
They are also context-aware applications. Most of the models built for pervasive
computing are only speciﬁc to certain contexts.
Transfer learning makes it
possible to transfer the knowledge between related contexts, of which STL can
achieve the best performance. However, when recognizing high-level contexts
such as Coﬀee Time, it is rather important to consider the relationship between
diﬀerent contexts in order to leverage their similarities. The research in this
area is still on the go.
5) Suggestions for using STL. Firstly, before using STL, it is critical to
extracting useful feature representations from the original data. Secondly, it is
better to use strong classiﬁers for majority voting to improve the convergence
Thirdly, the intra-class transfer step of STL-SAT can be deployed in
parallel if we compute the transformation matrix of each class.
6. Conclusions and Future Work
The label scarcity problem is very common in activity recognition. Transfer
learning addresses this issue by leveraging labeled data from auxiliary domains
to annotate the target domain.
In this paper, we propose a novel and general Stratiﬁed Transfer Learning (STL) framework for cross-domain learning in
pervasive computing. STL can address both the source domain selection (by
STL-SDS) and knowledge transfer (by STL-SAT) problems in CPAR. Compared
to existing approaches which only learn a global distance, STL can exploit the
local property of each class between diﬀerent domains. Experiments on three
large public datasets demonstrate the signiﬁcant superiority of STL over ﬁve
state-of-the-art methods. We extensively analyze the performance of transfer
learning under diﬀerent degrees of similarities and diﬀerent levels of activities.
In the future, we plan to extend STL in the following research directions:
One natural idea is to extend STL in the deep neural networks, which can
perform end-to-end learning for activity recognition.
The biggest diﬀerence
between this paper and the deep version is we can exploit deep networks to
automatically extract features and then perform domain selection and activity
All these steps can be operated in one neural network.
network, our focus will be designing eﬀective backward strategies for STL.
Heterogeneous activity recognition.
Currently, STL can be applied in a
homogeneous situation where the source and the target domains are sharing
the same features. In a heterogeneous situation, the features for diﬀerent domains may be diﬀerent. For instance, if the source domain contains activity
information of the accelerometer and the target domain has information of the
gyroscope, we need to develop heterogeneous activity recognition algorithms to
deal with this situation. One natural idea is to use autoencoder to unitedly
represent the diﬀerent kinds of features. Thus, domains can share the same
features at the high level of the autoencoder.
Acknowledgments
This work is supported in part by National Key R & D Plan of China
(No.2017YFB1002802), NSFC (No.61572471,61472399), and Beijing Municipal
Science & Technology Commission (No.Z171100000117017).
References