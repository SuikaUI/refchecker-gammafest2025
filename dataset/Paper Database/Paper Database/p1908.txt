American Political Science Review
Vol. 89, No. 3
September 1995
WHAT TO DO (AND NOT TO DO) WITH TIME-SERIES
CROSS-SECTION
University
of California,
California
of Technology
examine some issues in the estimation of time-series cross-section models, calling into
question the conclusions of many published studies, particularly in thefield of comparative
political economy. We show that the generalized least squares approach of Parks produces
standard errors that lead to extreme overconfidence, often underestimating variability by 50% or
more. We also provide an alternative estimator of the standard errors that is correct when the error
structures show complications found in this type of model. Monte Carlo analysis shows that these
"panel-corrected standard errors" perform well. The utility of our approach is demonstrated via a
reanalysis of one "social democratic corporatist" model.
e shall show that a commonly used tech-
nique for the analysis of time-series cross-
section (TSCS) data produces incorrect re-
sults. Our result either invalidates or calls into
question the findings of at least five articles published
in the American Political Science Review, as well as a like
number in other leading journals in political science
and sociology. Table 1 provides an incomplete list of
relevant articles whose conclusions are based on the
use of this problematic technique. All of these articles
use an application of the generalized least squares
(GLS) method first described by Parks , a
method designed to deal with some common prob-
lems that occur in TSCS data. We show that the Parks
method produces dramatically inaccurate standard
errors when used for the type of data commonly
analyzed by students of comparative politics. We
then offer a new method that is both easier to
implement and produces accurate standard errors.
Time-series cross-section data are characterized by
having repeated observations on fixed units, such as
states or nations. The number of units analyzed
would typically range from about 10 to 100, with each
unit observed over a relatively long time period (often
20 to 50 years). Both the temporal and spatial prop-
erties of TSCS data make the use of ordinary least
squares (OLS) problematic. In particular, models for
TSCS data often allow for temporally and spatially
correlated errors, as well as for heteroscedasticity.
Parks proposed a method for dealing with these
problems based on GLS.1 The use of this method can
lead to dramatic underestimates of parameter vari-
ability in common research situations.
Why the severe problems with the Parks method?
Is it not just an application of well-known GLS? While
GLS has optimal properties for TSCS data, it assumes
that we have knowledge about the error process that,
in practice, we never have. Thus analysts use not
GLS, but feasible generalized least squares (FGLS). It
is "feasible" because it uses an estimate of the error
process, avoiding the GLS assumption that the error
process is known. The FGLS formula for standard
errors, however, assumes that the error process is
known, not estimated. In many applications this is
not a problem because the error process has few
enough parameters that they can be well estimated.
Such is not the case for TSCS models, where the error
process has a large number of parameters. This
oversight causes estimates of the standard errors of
the estimated coefficients to understate their true
variability. We provide a measure of how much the
Parks standard errors understate true sampling vari-
ability, that is, how much the Parks method falsely
inflates confidence in the findings of TSCS studies.
Unfortunately, it is not possible to provide analytic
formulae for the degree of overconfidence introduced
by the Parks method. Instead, we provide evidence
from Monte Carlo experiments using simulated data
to assess the performance of the various estimators.
This evidence clearly shows the overconfidence in-
duced by the Parks method. The Parks estimator may
understate variability by between 50% and 300% in
practical research situations. It is this extreme overcon-
fidence that leads us either to overturn or to cast
doubt on the findings of many analyses based on the
Parks method.
Having demonstrated the problems of the Parks
method, we instead advocate a simpler method for
estimating TSCS models. It is well known that even
though OLS estimates of TSCS model parameters
may not be optimal, they often perform well in
practical research situations. It is also well known
that the OLS estimates of standard errors may be
highly inaccurate in such situations. We therefore
propose to retain OLS parameter estimates but re-
place the OLS standard errors with panel-corrected
standard errors. Monte Carlo analysis shows that these
new estimates of sampling variability are very accu-
rate, even in the presence of complicated panel error
structures.
We shall detail the problems of the Parks method,
laying out the structure of TSCS models and showing
why OLS is. problematic. In order to understand
Parks' solution and why it is problematic, it is neces-
American Political Science Review
Vol. 89, No. 3
Articles Using the Parks Method
Number of Units Is Less Than Number of Time Points
Hicks & Swank 1992
Political determinants of welfare spending in advanced
industrial societies
Hicks 1994bC
Political and union determinants of economic growth in
advanced industrial societies
Pampel 1993c
Political and social determinants of fertility rates in
advanced industrial society
Schneider & Ingraham 1984
Political determinants of social program expansion in
advanced industrial societies
Su, Kamlet & Mowery 1993d
5, 7, 10,11
Political determinants of U.S. budget by category
Swank 1992
Political determinants of tax policy in advanced industrial
Number of Units Exceeds Number of Time Pointse
Alvarez, Garrett & Lange
Political and union determinants of economic growth in
advanced industrial societies
Friedland & Sanders 1985c
Political determinants of economic growth in advanced
industrial societies
Giles and Hertz 1994cf
Party registration and race in Louisiana parishes
20, 42, 62
Influence of government spending on cross-national
economic growth
Pampel & Williamson 1988C
Political determinants of welfare spending in advanced
industrial societies
Scholz, Twombly & Headrick
Political determinants of enforcement of regulatory
standards in New York counties
Scholz & Wei 1986
Regulatory enforcement in the states
Policy implementation in the states
aNumber of cross-sectional units.
'Number of time periods.
'Used Parks in addition to other methods; estimates using other methods not discussed in this article.
dEstimated with seemingly unrelated regressions; same as Parks correction for contemporaneously correlated errors.
eParks method cannot produce results.
fArticle indicates use of Park. Private communication with the authors indicates that Park was not in fact used.
sary first to consider some properties of FGLS. The
Parks method is then laid out. This method provides
an estimation strategy that takes into account both
cross-sectional and temporal complications of the
data. These two components
of the method are
presented and assessed separately. These assess-
ments show that the Parks standard errors are likely
to lead to extreme overconfidence for typical TSCS
data, with the correction for cross-sectional complica-
tions being much more problematic than the correc-
tion for temporal complications.
We shall then present our proposed method for
estimating TSCS models. We argue that this method,
which combines ordinary least squares parameter
panel-corrected
should perform well. As with the Parks method, a full
assessment of this method for the types of data
encountered in research situations is only possible via
Monte Carlo analysis.
We then present this Monte Carlo evidence. Our
results demonstrate the extreme overconfidence in-
duced by the Parks standard errors. Our simulations
show that while the Parks correction for cross-sec-
tional complication causes much of the problem, the
correction for temporal complications is also problem-
atic. In addition, the Monte Carlo evidence shows
that panel-corrected standard errors perform ex-
tremely well, even in the presence of complicated
panel error structures. The Monte Carlo evidence also
shows that OLS parameter estimates are themselves,
at worst, not much inferior to the Parks parameter
estimates. Thus the costs of the inaccurate Parks
standard errors are in no sense paid for by the
superiority of the Parks estimator of the model pa-
Finally, we use our proposed method to reanalyze
Hicks and Swank's results obtained with the
Parks method. We show that the strengths of many
of their findings about the political causes of govern-
ment spending in advanced industrial societies are an
artifact of their use of the Parks method. We also
briefly reconsider the findings of other studies that
used that method. The conclusion presents a unified
method for analyzing TSCS data.2
THE PARKS METHOD AND ITS FLAWS
Our analysis is limited to what Stimson called
temporally dominated TSCS models, where a limited
Time-Series Cross-Section Data
September 1995
number of units are observed for a relatively long
period of time.3 The critical assumption of TSCS
models is that of "pooling"; that is, all units are
characterized by the same regression equation at all
points in time. Given this assumption, we can write
the generic TSCS model as
yilt =- xi~tt + Eilt; i =1, ... , N; t =1,..,T(1
where xit is a vector of one or more (k) exogenous
variables and observations are indexed by both unit
(i) and time (t).4 We shall denote the matrix of
independent variables for all observations as X and
the vector of observations on the dependent variable
as Y. We assume that the data are stacked by unit.5
We denote the NT x NT covariance matrix of the
errors with typical element E(E. tE- ) by 0.
models can be difficult to estimate because the error
process of such models may be more complicated
than is typical of either time-series or cross-sectional
models. Different assumptions about this error pro-
cess lead to different preferred methods of estimation.
Ordinary Least Squares Is Problematic
for Time-Series Cross-Section Data
Ordinary least squares is optimal (best linear unbi-
ased) for TSCS models if the errors are assumed to be
generated in an uncomplicated ("spherical") manner.
In particular, for OLS to be optimal it is necessary to
assume that all the error processes have the same
variance (homoscedasticity) and that all of the error
processes are independent of each other. The latter
assumption can be broken down into the assumption
that errors for a particular unit at one time are
unrelated to errors for that unit at all other times (no
serial correlation) and that errors for one unit are
unrelated to the errors for every other unit (no spatial
correlation). Under these assumptions TSCS models
should be estimated by OLS and OLS standard errors
are correct. Most analysts, however, are not willing to
accept the assumption of spherical errors for TSCS
Ordinary least squares is not optimal in the pres-
ence of nonspherical errors, in the sense that there
will be other estimators that make more efficient use
of the data. More seriously, if the errors are not
spherical, there is no guarantee that the OLS stan-
dard errors will be correct. We use the term correct
standard errors to indicate that we have accurate
estimates of the variability of parameter estimates.
Correct standard errors allow for the correct compu-
tation of confidence intervals and statistical tests.
Incorrect standard errors will lead us to be either too
confident or insufficiently confident about whether
our findings might merely be statistical artifacts.
It is, of course, always possible that the errors of
any regression model may be nonspherical. The
problem is, however, much more acute for TSCS
models. In particular, we might expect TSCS errors to
be contemporaneously correlated in that large errors
for unit i at time t will often be associated with large
errors for unit j at time t. This is likely in the
cross-national context, where the economies of, say,
the Netherlands and Belgium are linked. It is also a
likely problem in other TSCS contexts, such as the
study of disaggregated budgets, where large errors in
one budget category may be associated with large
errors in other categories in the same year. These
contemporaneous correlations may differ by unit. For
example, the errors in the Scandinavian economies
may be linked together but remain independent of
errors in the southern European countries.
We might also expect the errors in TSCS models to
show "panel heteroscedasticity,"
where the vari-
ances of the error process differ from unit to unit. The
errors of a cross-national panel study, for example,
may show panel heteroscedasticity because the scale
of the dependent variable, such as the level of gov-
ernment spending, may differ between countries.
The assumption of panel heteroscedasticity is more
stringent than just cross-sectional heteroscedasticity
because we continue to assume that the error vari-
ances within each unit do not differ over time; this
assumed structure allows for certain estimation strat-
egies not available in the nonpanel case.
Finally, it is possible that the errors may show
temporal dependence. The most typical assumption
is that the errors show first-order serial correlation.
Some analysts assume that the degree of serial corre-
lation differs from unit to unit, while others assume it
is constant across units.
Time-series cross-section analysts do put some
structure on the assumed error process. In particular,
they assume that for any given unit, the error vari-
ance is constant, so that the only source of heterosce-
dasticity is differing error variances across units.
Analysts also assume that all spatial correlation both
is contemporary and does not vary with time. The
temporal dependence exhibited by the errors is also
assumed to be time-invariant and may also be invari-
ant across units. All of these assumptions allow
analysts to attempt to improve on OLS for TSCS data.
Since these assumptions are all based on the panel
nature of the data, we call them the panel error
assumptions.6
Feasible Generalized Least Squares
Equation 1 can be estimated by generalized least
squares regardless of any complexities of the error
process, so long as the covariance matrix of those
errors, 0, is known (up to a scale factor). Given that
assumption, GLS is fully efficient and yields consis-
tent estimates of the standard errors . Generalized least squares
works by transforming equation 1 with a general
error covariance matrix to another linear equation
where the error covariance matrix is suitable for OLS
estimation (spherical). The GLS estimates of p3 are
(XiiV1X)-1X'0lY
American Political Science Review
Vol. 89, No. 3
with estimated covariance matrix
(X'Q-lX)-l.
The problem is that the covariance matrix of the
errors, Q, is never known in practice (even up to a
scale factor). Thus an estimate of 0, 0, is used in
expressions 2 and 3. This procedure, FGLS, provides
consistent estimates of f3 if 0 is estimated by residu-
als computed from consistent estimates of /8. Ordi-
nary least squares provides such consistent esti-
mates. We denote the FGLS estimates of 13 by 13. The
application of FGLS to TSCS models with panel errors
was first described by Parks .
Feasible generalized least squares performs well in
large samples. In the limit, it is equivalent to full
maximum likelihood, and so has all the optimal
asymptotic
properties
likelihood
 . We know little about the finite
sample properties of FGLS other than that it yields
unbiased estimators under very general conditions
that are usually met in practice . The
better the estimate of 0, of course, the better the
FGLS estimator; obviously, estimation of 0 will im-
prove as NT increases in relationship to the number
of parameters in 0 that must be estimated.
It is difficult to assess the performance of FGLS in
finite samples. There are by now many Monte Carlo
studies showing that FGLS may be less efficient than
its OLS counterpart, especially in very small samples.
Our interest is in how well FGLS estimates of vari-
ability (expression 3) perform in finite samples. It is
known that FGLS standard errors underestimate true
variability, at least for normal errors . There are, unfortunately, no analytic
results that indicate whether this underestimate is of
importance to applied researchers, nor, in particular,
are there any analytic results about the performance
of the Parks estimates of variability for TSCS models.
While we will assess this variability with Monte Carlo
experiments, we can get some hint about this vari-
ability by closer examination of the Park method.
The Parks Method
The Parks method is FGLS for TSCS models where
the errors show panel heteroscedasticity, contempo-
raneous correlation, and unit specific serial correla-
tion. The correction for contemporaneous correlation
of the errors automatically corrects for any panel
heteroscedasticity,
so we need only consider the
corrections for contemporaneous and serial correla-
tion of the errors here.
The Parks method consists of two sequential FGLS
transformations, first eliminating serial correlation
of the errors then eliminating contemporaneous cor-
relation of the errors. This is done by initially esti-
mating equation 1 by OLS. The residuals from this
estimation are used to estimate the unit-specific serial
correlation of the errors, which are then used to
transform the model into one with serially indepen-
dent errors.8 Residuals from this estimation are then
used to estimate the contemporaneous correlation of
the errors, and the data is once again transformed to
allow for OLS estimation with now spherical errors.9
We can therefore consider the consequences of the
two corrections separately.
Correctingfor Contemporaneous Correlation of the Errors.
We first consider the Parks correction for contempo-
raneously correlated errors. The TSCS model with
contemporaneously correlated errors is then exactly
expression 1 with the variance covariance matrix of
the errors having zeros for all noncontemporaneous
observations and free parameters allowing for con-
temporaneous pairwise correlation of the errors and
panel heteroscedasticity. We can write this compactly
as f) = I 0
IT, where I is the N x N matrix of
contemporaneous covariances, with typical element
Et(fi EjtE- t).While
these parameters differ among
pairs of units, they do not vary by time. Feasible
generalized least squares, therefore, requires estimat-
ing all the pairwise contemporaneous covariances.
The matrix of all these estimates is denoted X. There
are N x (N + 1)/2 contemporaneous covariances; each
of these is estimated using NT observations.
The Parks correction for contemporaneously corre-
lated errors cannot be used unless T is at least as big
as N .11 But even when T is greater
than N, so that FGLS can be used, estimation of
standard errors is problematic unless T is consider-
ably larger than N. Each element of the matrix of
contemporaneous covariances of the errors is esti-
mated using, on average, 2T/N observations. Many
cross-national panel studies have ratios of T to N very
close to 1, so covariances are being estimated with
only slightly more than two observations per esti-
mate! Studies on the political economy of advanced
industrial nations seldom have T to N ratios that
exceed 3; thus the elements of the covariance matrix
of the errors are estimated with, on average, six
observations. Theory does not tell us how inaccurate
the Parks method is in these cases, but we should be
prepared to see highly overconfident Parks standard
errors in the typical cross-national panel case. We
shall provide Monte Carlo evidence of this.
Correctingfor Serially Correlated Errors. The Parks cor-
rection for serially correlated errors assumes the
errors follow a unit-specific first-order autoregressive
(AR1) process
Eit = PiEiot-1
where the vis are (mean zero) variables indepen-
dently distributed across time. Some analysts impose
the additional assumption that the pi are homoge-
neous across units, that is, pi = p. Ordinary least
squares residuals are used to estimate either the
common p or the pi; this estimate is then used to
transform the data, using the well-known
Winsten transformation .
The FGLS correction for a single p requires estimat-
ing one extra, unaccounted-for parameter. This is
unlikely to cause FGLS standard errors to estimate
variability inaccurately in the typical cross-national
Time-Series Cross-Section Data
September 1995
panel situation. The FGLS correction for unit-specific
serially correlated errors, used by Parks, is likely to
cause more serious underestimates of variability. The
essence of the problem is that each pi is estimated
using an autoregression based on only T observa-
tions. It is well known that such estimates are biased
downward . As a consequence, the
Parks estimates, which correct based on these inac-
curate autoregressions, may be inferior to OLS esti-
mates. The underestimates of the pi, when combined
with trending data, can cause the Parks estimates of
standard errors to misestimate variability substantially.
The assumption of unit-specific serial correlations
also seems odd at a theoretical level. Time-series
cross-section analysts assume that the "interesting"
parameters of the model, f, do not vary across units;
this assumption of pooling is at the heart of TSCS
analysis. Why should we expect the "nuisance" p to
not show similar pooling? p can be interpreted as how
long it takes for prior shocks to be removed from the
system. Why should this "memory" be the only
model parameter that varies from unit to unit?
The choice whether to correct for serially correlated
errors assuming either heterogeneous
or homoge-
neous p depends on the small sample properties of
the two types of estimators. While we would expect
the unit-specific serial correlation correction to lead to
more inaccurate estimates of variability, it is also
possible that allowing for variation among the pi
might improve overall estimation. We can only assess
the small sample performance of the two corrections
for serial correlation through Monte Carlo experimen-
tation. Before looking at the results of those experi-
ments, we first consider a new method for estimating
the variability of OLS estimators. We can then com-
pare the performance of the Parks estimator with our
new method.
ORDINARY LEAST SQUARES
WITH PANEL-CORRECTED
STANDARD ERRORS
If the errors in equation 1 meet one or more of the
panel error assumptions, then OLS estimates of 13 will
be consistent but inefficient; the degree of inefficiency
depends on the data and the exact form of the error
process. The OLS standard errors will also be inaccu-
rate,"2 but they can be corrected so that they provide
accurate estimates of the variability of the OLS esti-
mates of 13. This correction takes into account the
contemporaneous correlation of the errors (and per-
force heteroscedasticity). Any serial correlation of the
errors must be eliminated before the panel-corrected
standard errors are calculated. The correction for
contemporaneous
correlation of the errors is only
possible because we have repeated information on
the contemporaneous correlation of the errors; our
proposed method does not work outside the TSCS
The correct formula for the sampling variability of
the OLS estimates is given by the square roots of the
diagonal terms of
Cov(f3) = (X'X)- {X'OX}(X'X)-.
If the errors obey the spherical assumption, this
simplifies to the usual OLS formula, where the OLS
standard errors are the square roots of the diagonal
terms of o-2 (X'X)-1, where o-2 is the usual OLS
estimator of the common error variance, o2. If the
errors obey the panel structure, then this formula
provides incorrect standard errors. Expression 5,
however, can still be used, in combination with that
panel structure of the errors, to provide accurate,
panel-corrected standard errors (PCSEs).13
For panel models with contemporaneously corre-
lated and panel heteroscedastic errors, 0 is an NT x
NT block diagonal matrix with an N x N matrix of
contemporaneous covariances, l, along the diagonal.
To estimate equation 5, we need an estimate of l.
Since the OLS estimates of expression 1 are consis-
tent, we can use the OLS residuals from that estima-
tion to provide a consistent estimate of l. Let eist be
the OLS residual for unit i at time t. We can estimate
a typical element of l: by
Et=l eittej,t
with the estimate l: being comprised of all these
elements."4 We then use this to form the estimator fi
by creating a block diagonal matrix with the l matri-
ces along the diagonal.15 As the number of time
points increases, l: becomes an increasingly better
estimator of l. We cannot, however, assess the finite
sample performance of PCSEs by analytic methods.
Thus we shall have to evaluate them with the same
Monte Carlo experiments we use to evaluate Parks.
MONTE CARLO ANALYSIS
We have argued that the Parks method may not
perform well in correcting for a variety of TSCS
complications and, in particular, may lead to substan-
tial underestimates of variability in finite samples. We
have also argued that using OLS with PCSEs is a
reasonable estimation strategy. We designed a series
of Monte Carlo experiments to assess Parks, PCSEs,
and OLS in the TSCS context. As argued, we can
consider the two components of the Parks GLS "cor-
rection" separately.
Design of the Experiments
All experiments used simulated data that were gen-
erated to mimic some property of TSCS data. The
setup of each simulation was similar. For a given N
and T, observations on an independent variable, xi't,
(i = 1,...,
N; t = 1,...,
T), were generated as
random draws from a zero-mean normal distribution.
Experiments were run with various combination of N
American Political Science Review
Vol. 89, No. 3
and T chosen to reflect values typically found in
cross-national panel models (see Table 1).
For each experiment, we generated, using GAUSS,
one thousand replicates of the NT error terms (4l, i =
t = 1, ...,T; l = 1, ...,1000)
according to
the model being studied. (A superscript (1) denotes a
specific replicate.) The errors were always generated
as zero-mean NT-variate normals, with standard de-
viations chosen so that estimated coefficients were
roughly twice their standard errors. We chose the
covariance structure of the errors to mimic the prop-
erty of TSCS models being examined in a given set of
experiments; this structure is discussed in the context
of each set of experiments.
The xit were fixed over these one thousand repli-
cations. The one thousand replicates of Ej,t-and the
fixed xi -were used to generate one thousand repli-
cates of yit, using
Y) = 830 + f3,xi,t + E Y);i = 1, ...,
T; 1= 1, ...
where both ,80 and Al were fixed at 10. We report only
statistics concerning the estimation of I81.
The OLS estimate of ,1 for replication I is referred
to as 13(l); the Parks estimate for that replication is /3().
We are concerned with the performance of the esti-
mated standard errors. An accurate measure of the
sampling variability of each estimator is the standard
deviation of the one thousand
f30'%s or 73(')s. The
quality of the OLS or Parks estimates of variability
can then be assessed by comparing the root mean
square average of the one thousand estimated stan-
dard errors with the corresponding standard devia-
tion of the one thousand estimates. The measure of
accuracy we focus on, overconfidence, is the percentage
by which, say, FGLS understates variability; that is
Overconfidence = 100
Overconfidence of 200%, for example, indicates that
the true sampling variability of an estimator is, on
average, twice the reported estimate of that variabil-
An alternate measure of the accuracy of standard
errors is the true "level" of reported "95% confidence
intervals." The "95% confidence interval" for each of
the one thousand replications was computed using
the estimated standard errors. We can then see how
often these intervals contained the true value of /3.
Insofar as this proportion is under 95%, estimated
variability leads to overconfidence.
We were also interested in the relative efficiency of
Parks and OLS. Since the true value of 3,B is known,
the root mean square error of the OLS and Parks
estimates of 91- can be calculated. The relative effi-
ciency of OLS as compared to Parks is given by
1l,OOO (A(3() -
Efficiency = 100
Efficiency greater than 100% indicates that OLS is
superior, in mean square error terms, to Parks.
Overconfidence of the Parks Correction
Correctingfor Contemporaneously Correlated Errors. The
first set of experiments was designed to evaluate the
overconfidence of standard errors produced by the
Parks FGLS transformation to eliminate contempora-
neous correlation of the errors. This overconfidence is
simply a function of N and T and does not depend on
the actual contemporaneous correlation of the errors.
We therefore generated the data as simply as possi-
ble, with the errors drawn as independent normals.
The errors, in other words, were generated so as to
appear spherical.16
The striking results of this experiment are reported
in Table 2. In the worst cases, where T is exactly N,
Parks is overconfident by 400% for N = T = 10 and by
600% for N = T = 20. Reported "95% confidence
intervals" contain the true value of f3 only 30% of the
time when N = T = 10; this falls to 20% when N =
The Parks-induced overconfidence remains a seri-
ous problem as T grows in relation to N. Even in the
most favorable case for Parks (N = 10, T = 40), its
standard errors are 30% overconfident and reported
95% confidence intervals contain the true I3 only 87%
of the time. In what is perhaps the most typical
configuration, with N = 15 and T = 30, FGLS errors
are more than 50% overconfident and reported 95%o
confidence intervals contain the true value of f3 only
about 80% of the time. Readers confronted with
analysis using the Parks correction for contempora-
neous correlation of the errors in such a situation
should substitute 2.6 for the conventional critical
t-ratio of 1.68. These results show that TSCS research-
ers should simply avoid the Parks correction for
contemporaneously correlated errors unless the ratio
of the T to N is well above three, a situation not
normally seen.
Correcting for Serially Correlated Errors. The Parks
method first eliminates serial correlation of the errors
by transforming the data, allowing for a separate
transformation for each unit. We have argued that
this may induce substantial overconfidence, a prob-
lem that could be eliminated by assuming that the
serial correlation of the errors followed an identical
process for all units. In order to test this claim of
overconfidence, as well as to compare the relative
efficiency of unit-specific versus common error pro-
cesses, we needed to design a more complex set of
experiments. The added complexity is necessary be-
cause the performance of the estimators varies with
the degree of trend in the data.
The experiments generated data where the simu-
lated errors were generated as ARi processes with
Time-Series Cross-Section Data
September 1995
Overconfidence of Parks Correction for
Contemporaneously Correlated Errors
OVERCONFIDENCE
aNumber of cross-sectional units.
bNumber of time points.
cOverconfidence = 100-
V /211=00 (s -e
dPercentage of 95% confidence intervals containing 13*
unit-specific pi. More precisely, the errors were gen-
erated according to expression 4, where the pi were
set to Pi for the first half of the units and P2 for the
second half; the separation between these two values
was varied experimentally. The degree of trend in the
independent variable was also experimentally manip-
ulated. Thus the independent variables were gener-
ated according to xi t = 5xi tl + Ai to where the At are
zero-mean, independently distributed standard nor-
mal variates. Values of 8 near 1 indicate strongly
trending data. The experiments then proceeded as
Results of the experiments are presented in Table 3.
Experiments with different values of N showed that
the relative performance of the two estimators was
sensitive only to the value of T; we therefore only
show the results for N = 15. In addition, since the
results so strongly favor estimation with a common p,
we present only the results least favorable to that
The experiments show that standard errors assum-
ing a common p were never far off. Even in the worst
cases, where half the units had p = .9 and half had
p = .3, common p standard errors were never more
than 20% too low; more typically, the common p
estimator of variability was within 5% of the true
The situation was very different for the unit-spe-
cific pi estimator. The overconfidence of this estimator
can be substantial; this overconfidence varies with T,
8 and p. The most important determinant of overcon-
fidence is 8, that is, the degree of trend in the
independent variable. When 8 = .9, the Parks stan-
dard errors understate variability by almost 100% for
T less than 30 and by about 30% for T above 30.
Overconfidence caused by correcting for unit-specific
serial correlation disappears only when the data
trend mildly and T is at least 30.
Efficiency considerations clinch the case for estima-
tion with a common p. When T is under 20, estima-
tion assuming a common p is always more efficient
than estimation assuming a varying p, even when the
errors were generated with two very different pi.
Even when T gets as large as 40, estimation assuming
a common p remains more efficient than estimation
assuming a varying p, unless the errors were gener-
ated with two extremely different pi. Even in this
extreme case, estimation assuming varying pi is less
than 20% more efficient than estimation assuming a
common p. Such extreme cases are unlikely to arise in
practice. These experiments clearly show that TSCS
analysts should correct for serially correlated errors
assuming a common p.18
Thus, while the Parks (unit-specific) correction for
serial correlation causes fewer problems in terms of
overconfidence than does the Parks correction for
contemporaneously correlated errors, it does induce
substantial overconfidence.
Where the data trend
strongly, as they do in many political economy stud-
ies, this overconfidence can be as much as 100%.
Ordinary Least Squares with
Panel-corrected Standard Errors
The overconfidence induced by the Parks standard
errors makes it unusable except in the rarest of
research situations. We proposed a simpler estima-
tor, OLS with PCSEs. How does the proposed esti-
mator perform in the Monte Carlo experiments? We
first assess the accuracy of PCSEs and then compare
the efficiency of OLS estimators with those produced
by the Parks method.19
The Accuracy of Panel-corrected Standard Errors. OLS
standard errors are accurate in the presence of either
panel heteroscedasticity or contemporaneous correla-
tion of the errors if the terms in the error covariance
matrix, Q, are not related to the squares and cross
products of the independent
variables. Since we
wished to study the performance of PCSEs when
OLS standard errors were incorrect, we designed
experiments
contemporaneously
correlated
and/or panel-heteroscedastic
error structures that
were related to the panel structure of the indepen-
dent variables.
For each value of t, the N-vector xit (i = 1, ..
was generated as a draw from a zero-mean N-variate
normal distribution. Varying degrees of heterosce-
dasticity were simulated by setting the variance of the
first half of the units to 1 while the variance of the
second half of the units was experimentally manipu-
lated. The covariance matrix of this multivariate dis-
tribution was constructed so that all pairs of units
were equally correlated, with the degree of correla-
American Political Science Review
Vol. 89, No. 3
Comparison of Feasible Generalized Least Squares Corrections for Common and Varying p
OVERCONFIDENCE (%)
EFFICIENCY
aNumber of time points; number of units fixed at 15.
bXizt = bX,.tt1 + At_
CSerial correlation of errors of first half of units.
dSerial correlation of errors of second half of units.
eOverconfidence = 100
fEfficiency = 100
Over 100% indicates superiority of common p estimator.
tion also experimentally manipulated.20 Errors were
then generated so that the variances and covariances
of the errors were proportional to the corresponding
variances and covariances of the independent vari-
able. The errors could therefore show panel hetero-
scedasticity and contemporaneous correlation, either
alone or in combination.
Table 4 shows a few key results from these exper-
iments; a more complete table is in our companion
article. These experiments set N = 15, vary T, and
allow for various combinations of heteroscedasticity21
and contemporaneous correlation of the errors.
Panel-corrected standard errors performed excel-
lently in these experiments. They were always within
10% of true variability, even under conditions of
extremely high heteroscedasticity and contemporane-
ous correlation of the errors.22 In a typical research
situation, we would expect PCSEs to be off by only a
very few percentage points.
Of equal importance, in the case of homoscedastic-
ity and contemporaneously
independent
where OLS standard errors are accurate, PCSEs per-
formed exactly as well as the OLS standard errors.
But (as expected) as the errors became less spherical,
the performance of the OLS standard errors de-
clined.23 Thus PCSEs dominate OLS standard errors;
when PCSEs are not necessary, they perform as well
as the OLS standard errors, and when OLS standard
errors perform poorly, PCSEs still perform well. Since
PCSEs are not difficult to compute, they should
replace OLS standard errors for TSCS data.24
The Relative Efficiency of OLS and Parks. Panel-cor-
rected standard errors perform well and are more
accurate than Parks standard errors. Parks, however,
was designed to take account of the panel error
structure and hence be more efficient than OLS. The
combination of OLS and PCSEs can only be clearly
recommended if the OLS estimates of the parameters
of equation 1 are, at worst, not much less efficient
than the Parks estimates. We therefore designed
experiments to compare the relative efficiency of OLS
and Parks.
These experiments generated the errors completely
independently
of the independent variable. 5The
experiments generated the errors to show contempo-
raneous correlation of the errors. As before, for ease
of exposition we generated the errors so that all units
showed the same level of contemporaneous correla-
tion of those errors.
Results of these experiments are in Table 5. Each
entry in the table represents the relative efficiency of
OLS as compared to Parks, with, for example, the
first entry of 102 indicating that OLS is 2% more
Time-Series Cross-Section Data
September 1995
Ordinary Least Squares and Panel-corrected
Standard Errors
CONFIDENCE
HETEROSCE-
DASTICITYb
CORRELATION
aNumber of time points; number of units fixed at 15.
'Standard deviation of 1/oh, normalized.
cOverconfidence
z~=?010(s-_e,(w~('))
accurate (in terms of the square root of mean squared
error) in estimating f31 than is Parks.
Ordinary least squares is, as expected, more effi-
cient than Parks when the errors are uncorrelated
(spherical). But even when the average correlation of
the errors rises to .25, OLS remains slightly more
efficient than Parks. Parks becomes more efficient
than OLS when average contemporaneous correla-
tions rise to .50, but this advantage is noticeable only
when the number of time points is at least double the
number of units. Even here, the efficiency advantage
of Parks over OLS is under 20%. Only when the
average contemporaneous correlation of the errors
rises to .75 is the advantage of Parks marked, and
then only when T is twice N.
Researchers can use the OLS residuals to compute
the average contemporaneous correlation of the re-
siduals.26 Researchers should find OLS acceptable
unless the average contemporaneous correlation is at
least .50 and the time sample is quite long. We have
done this calculation for a variety of TSCS data sets
that were sent to us, and none of them met this
condition. It is, of course, possible that some TSCS
data might show extremely high contemporaneous
correlation of the errors. For such data, researchers
should consider alternatives to OLS, although the
inaccurate standard errors of the Parks method
would not make it the alternative of choice. A better
strategy would be to model the cause of the high unit
correlations directly, allowing whatever is causing
unit errors to covary to be a variable in equation 1.
But the need to do so should occur very rarely.
REANALYSIS
Hicks and Swank
We now use our methodology to reanalyze the study
of Hicks and Swank and to draw some conclu-
sions about a variety of other analyses. The underly-
ing model assessed by Hicks and Swank, the social
democratic corporatist model, has played an important
role in the recent study of comparative political
economy.27 The Hicks and Swank study was chosen
both because it exemplifies the issues we have been
discussing
and because the authors were kind
enough to make their data available. Hicks and
Swank used the Parks procedure as implemented in
the computer package SHAZAM; reanalysis was
done using RATS. We had no difficulty replicating
the original Hicks and Swank results using RATS; all
of the differences between our findings and theirs are
due to changes in methodology.
Hicks and Swank are interested in the political and
economic determinants of welfare spending in 18
advanced industrial societies for the 23-year period
1960-82. Here we reanalyze their "short model"
containing only variables that pass a "jackknife" test
 .28 Their dependent
variable is welfare spending as a proportion of gross
domestic product. They use a variety of political,
institutional, and economic independent variables.
The political variables are electoral turnout and nine
measures reflecting the strength of various parties:
the strength of the Left, Center, and Right in the
Relative Efficiency of Ordinary Least Squares
Compared to Parks (%)
CONTEMPORANEOUS
CORRELATION
OF THE ERRORS
Note: Efficiency = 100 Adz 1,011 _07
Over 100% indicates superiority of OLS.
aNumber of cross-sectional units.
bNumber of time points.
American Political Science Review
Vol. 89, No. 3
Estimates of the Hicks and Swank Model of Social Security Spending in 18 Advanced Industrial Societies,
PARKS-ARlb
Government
Opposition
Left corporatism
State centralization
Bureaucratic traditionalism
Center-Left
Left-Center
Left-Right
Price level (x100)
Aged share of population
Note: The dependent variable is SOCIAL SECURITY
SPENDING AS % OF GDP.
a'ficks and Swank estimates multiplied by 100.
bFrom H-icks and Swank 1992, table 3, col. 4, correcting for both contemporaneously correlated and unit-specific ARi errors.
CCorrecting for AR1 errors (common p).
dComputed using PCSEs.
'Average of unit-specific p, for Parks-AR1, single p for OLS-ARL.
government; three similar measures for opposition
party strength; and three interaction terms between
government and opposition (center governments and
left opposition, left governments and center opposi-
tion, and left governments and right opposition). A
factor analysis of institutional variables yields three
additional explanatory factor scores: left corporatism,
state centralization, and bureaucratic patrimonialism.
Finally, the model includes a variety of economic and
social controls: the natural log of gross domestic
product, the rate of inflation, the proportion of the
population that is elderly, and a post-1973 OPEC
oil-shock dummy. The model is linear in parameters
and variables are measured so that all effects in the
model are contemporaneous; dynamics are captured
by (unit-specific) serially correlated errors. The model
does not contain country dummy variables.
The estimates reported by Hicks and Swank, com-
puting with the Parks method allowing for country-
specific pi are in Table 6, columns 1-3, labeled "Parks-
AR1." Their t-ratios are impressive, with 13 of 17
coefficients having t-ratios over four. However, our
analysis shows that the Hicks and Swank standard
errors may understate variability by a factor of three.
We reestimated the Hicks and Swank equation
using OLS after correcting for serial correlation of the
errors, assuming a common p. Results of this estima-
tion are in Table 6, columns 4-7. Both the usual
standard errors (OLS se) and PCSEs are reported.29
The t-ratios reported were computed using PCSEs.
The Hicks and Swank data, transformed to elimi-
nate serially correlated errors, showed both hetero-
scedasticity (standardized measure of .37) and con-
temporaneous correlation of the errors (average of
.25). Under these conditions, PCSEs and OLS stan-
dard errors should differ. They typically did differ but
only by about 10%. While we base our conclusions on
the PCSEs, we would have made similar findings
using OLS standard errors.30
Panel-corrected standard errors are, as predicted,
roughly three times the standard errors obtained by
Hicks and Swank.31 The Hicks and Swank data,
when estimated with corrected standard errors, are
not consistent with many of their conclusions. Hicks
and Swank find that their "results support expecta-
tions, strongly rejecting most null hypotheses" . Our reanalysis finds that of the thirteen political
and institutional variables in the short model, only
four show t-ratios exceeding 2.0. Hicks and Swank
find that the "evidence for positive voter turnout
effects is pervasive and robust" . Our
reanalysis finds this effect to be marginally statisti-
cally insignificant. Hicks and Swank find that "sig-
nificant positive estimates for social democratic cor-
Time-Series Cross-Section Data
September 1995
poratism, state administrative/political
capabilities
and traditional political legacies [are] everywhere
strong and robust" . We find the social
democratic corporatist effect to be marginally statisti-
cally insignificant, the state administrative/political
capabilities effect to be moderately statistically signif-
icant and only the traditional bureaucratic legacies
effect to be strongly statistically significant. Hicks and
Swank find that "overall, strong evidence emerges
for strategic interactions among parties" .
We find that of the three interaction effects that pass
the jackknife test, two are marginally significant and
one is moderately so. Like Hicks and Swank, we do
find the economic and social controls to have a
generally strong impact on welfare spending, but
those are not the variables of interest. While it is
possible to argue about the credibility of findings that
are at the margin of statistical significance, our re-
analysis clearly casts doubt on the strength of the
Hicks and Swank conclusions.
Other Studies
It is impossible to use the Parks method if the length
of the time frame, T, is smaller than the number of
Several published
therefore,
present results that are either logically impossible to
obtain or are completely a function of numerical
inaccuracies. We have shown elsewhere that Alvarez,
Garrett, and Lange's original results on the
interaction of the strength of labor, party control, and
economic outcome were simply artifacts of numerical
inaccuracies in an old, unsupported, SAS procedure.
Reanalysis using the methods recommended here
supported their principal finding about economic
growth but not their findings on unemployment or
inflation . Several other articles listed
in the lower half of Table 1 report having used the
Parks method with N > T.33 It is possible that these
impossible results were obtained using the problem-
atic routine. But while we cannot know how the
results reported in these articles were achieved, we
can be sure that they cannot have been achieved by
appropriate use of the Parks method.
Turning to studies where the Parks method can in
theory be useful, we reanalyzed Swank's study
of the impact of politics on tax policy in 16 Organiza-
tion for Economic Cooperation and Development
nations observed over 20 years. With a sample this
size, our simulations indicate that Swank's estimated
standard errors should be overconfident by a factor of
at least three; our reanalysis bore this out. The
reanalysis, using OLS and PCSEs, showed that few of
his coefficients, and none of his political coefficients,
were significantly different from zero.
We did not reanalyze data sets used in the other
Parks analyses (upper half of Table 1) and so can only
use the Monte Carlo results to assess the inaccuracy
of their standard errors. The findings of Hicks 
about the short-run political causes of economic
growth and unemployment, which are based on
Parks analyses, are very problematic.34 With obser-
vations on 17 countries over only 18 years, reported
standard errors for these analyses should be overcon-
fident by over 600%. Thus, while he reports findings
with impressive t-ratios that are often near 10, correct
computation of standard errors would lead to few if
any rejections of the null hypothesis that none of the
social democratic corporatist variables affect eco-
nomic performance. Hicks concludes that "in the
short run of year-to-year economic fluctuations, the
social democratic corporatist (SDC) theory of eco-
nomic performance is distinctly upheld for the case of
economic growth" (p. 208). This is almost certainly an
artifact of computing overconfident standard errors.
Su, Kamlet, and Mowery estimated models
for disaggregated middle-class and defense budget
categories with contemporaneously
correlated er-
rors.5 Their most surprising finding was that party
had a statistically significant effect on middle-class
spending programs. This finding is based on 10
subprograms observed over 26 years. Our simula-
tions indicate that their "significant" t-ratio of 2.5 is
on the cusp of being significant at the .05 level.
Pampel's model relating fertility to cohort
size and other socioeconomic variables worked with a
larger time frame (36 years). His estimated standard
errors are about 40% overconfident due to the correc-
tion for contemporaneously correlated errors, plus an
additional factor for using unit-specific pi to correct
for serial correlation. The size of the latter factor is not
known, because it is a function of the unknown trend
in his data. Thus he should have used a critical value
for his t-tests of between 2.5 and 3. Even with this
higher critical value, it appears that many of his
findings would remain statistically significant, al-
though some of his findings of significant interactions
between cohort size and socioeconomic variables
would be overturned by the use of a more realistic
critical value.
Finally, Schneider and Ingraham have an
even longer time frame (59 years). With such a time
frame the Parks standard errors are only slightly
overconfident; none of the findings of that paper
would be overturned by using more accurate stan-
dard errors. It is only with time frames as long as this
study that the Parks method might prove useful.
Such long time frames are exceedingly rare in the
political science literature.
CONCLUSIONS
In his recent discussion of the analysis of TSCS data
in comparative political economy, Hicks notes that
"we should, pending information on the small sam-
ple properties of standard errors and t statistics in the
Parks-Kmenta model, be wary of downward bias in
standard errors and upward bias in t-statistics to the
extent that N(N - 1)/2 approaches NT" .
The present article answers Hicks' request and shows
clearly that the downward bias in standard errors
makes the Parks technique unusable unless there are
substantially more time points (T) than there are
American Political Science Review
Vol. 89, No. 3
cross-sectional units (N). In particular, the Parks
technique is extremely misleading for the types of
TSCS data typically analyzed by political scientists.
As a consequence, the substantive conclusions of
many articles that use the Parks method to estimate
TSCS models are, at best, open to doubt. We coun-
terbalance this negative conclusion by providing a
simple methodology for analyzing TSCS data.
Time-series cross-section analysts should proceed
by first examining the temporal properties of their
data. This can be done, as we argue elsewhere, with
lagged dependent variables or, as is typically done in
the cross-national panel literature and as we do here,
by transforming the data to eliminate serial correla-
tion of the errors (Beck and Katz n.d.). If researchers
choose the latter route, there is no doubt that they
should transform based on an estimate of the com-
mon serial correlation; researchers correcting for se-
rial correlation only should follow a similar strategy.
Once the dynamics are accounted for or trans-
formed away, TSCS analysts can estimate model
parameters by OLS. Our Monte Carlo evidence
shows that this will seldom lead to a substantial loss
of efficiency for the types of TSCS data typically
analyzed by political scientists.
Standard errors
should be calculated using PCSEs. Our Monte Carlo
evidence shows that there can be no harm from using
PCSEs, while, in some circumstances, they may be
considerably more accurate than the usual OLS stan-
dard errors. The combination of OLS with PCSEs
allows for accurate estimation of variability in the
presence of panel error structures without inducing
the severe problems caused by the Parks method.
Researchers who worry that their data may fall into
one of the extreme cases of heteroscedasticity or
contemporaneous correlation of the errors can check
for these problems by examining the structure of the
OLS residuals. Only if these problems are severe, and
only if sample sizes are large enough, should re-
searchers contemplate a more complicated FGLS es-
timation strategy. Those contemplating such a strat-
egy must trade-off the potential advantages of FGLS
against the disadvantages of inaccurate standard er-
rors. We have not seen a TSCS data set that makes it
necessary even to consider this trade-off.
Many will take comfort in our finding that the
workhorse of political methodology, OLS, is superior
to the more complicated GLS approach to the analysis
of TSCS data. We do not argue that complicated
methods will always have problems, or that OLS is in
general superior to complicated methods. It is, how-
ever, critical that we learn to assess the properties of
complicated estimation strategies, and in particular
that we study these properties for the types of data
actually analyzed, rather than for large samples ob-
served only in "asymptopia." This is particularly true
for assessments of variability, about which we often
have little or no intuition. Ordinary least squares,
with corrected standard errors, will not always prove
to be superior to more complicated techniques, but it
clearly is superior to the Parks method for analyzing
the types of time-series cross-section data that are
used by students of comparative politics.
We would like to thank Michael Alvarez, Geoffrey Garrett,
Peter Lange, Alexander Hicks, and Duane Swank for gener-
ously providing their data for replication purposes. William
Greene, Gary King, and Glenn Sueyoshi deserve more than
the usual thanks for helping us to figure out both what we
were doing and how to communicate it. We also thank
Michael Alvarez, Charles Franklin, Ronald Gallant, Elizabeth
Gerber, Sung Hahm, William Heller, Mark Kamlet, Brian
Loynd, Glenn Mitchell, Chris Mooney, Jimmy Sanders,
Renee Smith, James Stimson, and Michael Thies for helpful
comments and conversations. We are grateful to Peter Wil-
liams for providing new LATEX styles. Katz thanks the Na-
tional Science Foundation for a graduate fellowship that
funded his work on this project while he was at the University
of California, San Diego. Earlier versions were delivered at the
1993 annual meetings of the American Political Science Asso-
ciation in Washington, the Political Methodology Group in
Tallahassee, and the Midwest Political Science Association in
Chicago. All computer codes and data related to this article
may be obtained via ftp to weber.ucsd.edu.
1. This method was popularized in Kmenta's text, so it is
sometimes referred to as Parks-Kmenta and sometimes attrib-
uted only to Kmenta . We call it "Parks"
throughout this article.
2. We have written a companion article, Beck and Katz
n.d., that treats the dynamic issues of TSCS estimation in
detail; it also examines estimators that correct for panel
heteroscedasticity only.
3. We assume that the reader is familiar with the basics of
TSCS models, as laid out in Stimson 1985 and more fully in
Hsaio 1986. Stimson distinguished models that are cross-
sectionally dominated from ones that are not. The former
(e.g., the National Election Studies Panels) have observations
on thousands of units observed a few times. While TSCS
models are formally equivalent to such panel designs, the
problems faced by TSCS modelers are very different from
those faced by electoral panel modelers. The present article
does not consider issues that arise in cross-sectionally domi-
nated designs. While we typically call the designs we study
TSCS models, in a few places it is more convenient to refer to
our designs as panels; the two terms are used interchangeably
here. Earlier versions of this article used the term cross-national
panel. While this term correctly connotes the size of cross-
section we consider, we do not wish in any way to limit the
range of applications of our method to cross-national data.
4. The exogenous
variables may contain unit-specific
dummy variables, allowing intercepts to vary by unit. Such a
model is called a fixed effects model. Fixed effects present no
special problems for TSCS models, because the number of
unit-specific dummy variables required is not large. We do not
consider random effects models, which are heavily used in
cross-sectionally dominated panel models but are not important
for TSCS work. We show in Beck and Katz n.d. that it is easy to
include lagged dependent variables in expression 1.
5. That is, the data are ordered so that the second obser-
vation is the observation on unit 1 for the second time period
and, in general, the observation following unit i for time
period t is the observation for unit i for time period t + 1 (or,
following the last observation on unit i, it is the first observa-
tion on unit i + .1).
6. We can state the various "panel error" assumptions
symbolically as:
Panel Heteroscedasticity. E(ei~) ? E(eg,), but E(ei~) = E(e'i), so
we can write E(e,)) =
Contemporaneously Correlated Errors. E(Ei ,tet) = E(Eitt',t') ? 0,
but E(El t~jt') = 0, 50 we can write E(E t~jt) = aiwith
other covariances being zero.
Time-Series Cross-Section Data
September 1995
Unit-specific Serially Correlated Errors. Est = Ptt-
+ vit, where
the v are incoming "shocks" that are temporally indepen-
dent, identically distributed, zero-mean random variables.
Common Serially Correlated Errors. Ett = PE,,t_1 + vts, where the
vt are incoming "shocks."
7. The FGLS method to correct for only panel heterosce-
dasticity is panel-weighted least squares, which is incorpo-
rated in Stimson's GLS-ARMA technique. The FGLS
correction for panel heteroscedasticity is not subject to the
extreme problems of the correction for contemporaneously
correlated errors so we do not further consider it here. Those
contemplating using this technique should see Beck and Katz
8. The same method may be used with a common serial
correlation process, but the Parks method allows for unit-
specific serial correlations.
9. Analysts modeling temporal dynamics with a lagged
dependent variable need only use the second transformation.
Other analysts use only the first stage of the Parks method,
correcting only for unit-specific serial correlation. As we shall
see, the use of either transformation alone leads to serious
underestimates of variability.
10. 0 is the Kronecker product. For example, if N = 2 and
T = 3, the variance covariance matrix of the errors is
11. Feasible generalized least squares requires that l be
invertible. It is an N x N matrix, but its rank is, at most, the
lesser of T and N. Thus it is not invertible unless T is at least
as large as N.
12. The OLS standard errors will not be consistent. The
degree of inaccuracy is a complicated function of the relation-
ship between the X'X matrix and the variances and covari-
ances of the error process. If these are only slightly related
then the OLS standard errors will only be slightly incorrect.
This is shown clearly for the cross-sectional case by White
 . One consequence of this is that it not possible to infer
the overconfidence of OLS standard errors as a simple func-
tion of sample size.
13. We use this term to distinguish our proposed estimate
of variability from White's heteroscedasticity-consistent
standard errors. Many standard packages, such as RATS or
SHAZAM compute robust standard errors based on White's
method; none of these take into account the panel structure of
the errors, even within a panel estimation module. Our
Monte Carlo results, presented in Beck and Katz n.d., show
that researchers should not use robust standard errors calcu-
lated by typical computer packages to approximate PCSEs. It
should also be stressed that our use of the term panel-corrected
indicates that the standard errors computed will not be
inaccurate due to the panel structure of the data. Other
pathologies,
such as a time-varying error structure, may
indeed cause PCSEs to be inaccurate. In this regard, PCSEs
are no better, but no worse, than any other estimator pro-
posed for TSCS data.
14. We are using the maximum likelihood, rather than the
unbiased, estimate of l. The unbiased estimate would be
obtained by dividing by T - k. It is not clear which method is
superior. In our simulations, we use only one independent
variable, so the difference between the two estimators is
15. In symbols, if E denotes the T x N matrix of the OLS
residuals, we can estimate l by
and hence estimate fl by
where 0 is the Kronecker product. Panel-corrected standard
errors are thus computed by taking the square root of the
diagonal elements of
0(9 IT)X(X X)
16. We examined the overconfidence of the Parks standard
errors in many more complicated contexts. Our findings
about the overconfidence of the Parks standard errors in those
experiments are identical, to within the small sampling error
of the experiments, to what is reported in Table 2.
17. As in all our experiments, we are assessing the estima-
tion of 81 and its variability.
18. Some TSCS analysts do not use the Parks method but
do correct for serially correlated errors using unit-specific pi
 . The simulation
results apply equally to this method. Correcting for unit-
specific serial correlation leads to substantial overconfidence,
whether used alone or as part of the Parks method.
19. We deal only with the performance of these estimators
assuming that serial correlation has already been eliminated.
20. The contemporaneous correlation of the errors matrix
was assumed to take this simple form for ease of exposition.
None of the experimental results are artifacts of this simple
form, since the various estimation techniques allow for freely
varying contemporaneous correlations.
21. We have seen no textbook measure of the degree of
panel heteroscedasticity. It is measured for Table 4 by the
standard deviation of the normalized weights that would be
used in panel-weighted least squares. The variance of the jth
unit is of. Let wi = i/-o. We define standardized heteroscedastic-
ity as the standard deviation of the w1/fv.
22. This result is based on both Table 4 and results reported
in Beck and Katz n.d.
23. Again, we designed the experiments where OLS per-
formance would be degraded. In practical situations the OLS
standard errors may perform well.
24. The code to compute PCSEs in RATS is available via ftp
from weber.ucsd.edu.
25. The inefficiency of OLS in the presence of nonspherical
errors does not depend on the relationship of the error
process and the independent variables.
26. This statistic is computed in our RATS program.
27. This importance is best seen in the variety of papers in
Janoski and Hicks 1994.
28. For definition and operationalization of these variables,
see Hicks and Swank 1992, 663-64. We received data only on
variables contained in the short model. Results from this
model are similar to other results reported by Hicks and
29. Panel-corrected standard errors were computed after
the data were transformed to eliminate serial correlation.
30. This is not an argument for using OLS standard errors.
It is difficult to know, in advance, whether OLS standard
errors are accurate, while we are confident about the accuracy
of PCSEs. The only way we know that OLS standard errors
are accurate for this reanalysis is that they are close to the
estimated PCSEs. Since there is no reason for TSCS research-
ers to compute possibly misleading OLS standard errors, we
focus on the PCSEs here.
31. We also estimated the Hicks and Swank model using
our methods but allowing for unit-specific pa. These estimates
(not reported here) had, as predicted, PCSEs that were
between 25% and 50% smaller than the PCSEs computed
under the assumption of a common p. If we compare PCSEs
to Parks standard errors when both correct for serially corre-
lated errors similarly, we find that the PCSEs are, as pre-
dicted, about two-and-a-half times larger than the corre-
sponding Parks standard errors.
32. Hicks and Swank appear to equate a strong effect with
American Political Science Review
Vol. 89, No. 3
one having a large t-ratio. We therefore assess their findings
using this standard.
33. As noted in that table, some of these studies used Parks
in addition to other estimation methods. We have no findings
on the conclusions based on other methods.
34. His analysis of the long run does not use Parks and so
is not discussed here.
35. This study uses a "seemingly unrelated regressions"
methodology which is identical to the FGLS correction for
contemporaneously correlated errors.