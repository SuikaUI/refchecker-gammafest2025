Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3901–3910
Brussels, Belgium, October 31 - November 4, 2018. c⃝2018 Association for Computational Linguistics
Paragraph-level Neural Question Generation
with Maxout Pointer and Gated Self-attention Networks
Xiaochuan Ni
Yuanyuan Ding
Microsoft AI & Research
Sunnyvale, California
{yaozhao, xiaon, yuand, qke}@microsoft.com
Question generation, the task of automatically
creating questions that can be answered by a
certain span of text within a given passage,
is important for question-answering and conversational systems in digital assistants such
as Alexa, Cortana, Google Assistant and Siri.
Recent sequence to sequence neural models
have outperformed previous rule-based systems. Existing models mainly focused on using one or two sentences as the input. Long
text has posed challenges for sequence to sequence neural models in question generation
– worse performances were reported if using the whole paragraph (with multiple sentences) as the input.
In reality, however, it
often requires the whole paragraph as context
in order to generate high quality questions.
In this paper, we propose a maxout pointer
mechanism with gated self-attention encoder
to address the challenges of processing long
text inputs for question generation.
sentence-level inputs, our model outperforms
previous approaches with either sentence-level
or paragraph-level inputs.
Furthermore, our
model can effectively utilize paragraphs as inputs, pushing the state-of-the-art result from
13.9 to 16.3 (BLEU 4).
Introduction
Question generation (QG), aiming at creating
questions from natural language text, e.g. a sentence or paragraph, is an important area in natural
language processing (NLP). It is receiving increasing interests in recent years from both industrial
and academic communities, due to the booming
of Question-and-Answer (QnA) and conversation
systems, including Alexa, Cortana, Google Assistant and Siri, the advancement of QnA or machine
comprehension technologies together with the releases of datasets like SQuAD and MS MARCO ,
and the success of language generation technologies for tasks like machine translation and text summarization 
in NLP. A conversational system can be proactive
by asking the user questions ,
while a QnA system can beneﬁt from a large scale
question-answering corpus which can be created
by an automated QG system .
Education is another key application where QG
can help with reading comprehension .
In NLP, QG has been mainly tackled by two approaches: 1) rule-based approach, e.g. 2) neural QG approach: end-toend training a neural network using the sequence
to sequence (also called encoder-decoder) framework, e.g. . In this paper, we
adopt the second approach.
More speciﬁcally, we focus on an answer-aware
QG problem, which takes a passage and an answer
as inputs, and generates a question that targets the
given answer.
It is also assumed the answer is
comprised of certain spans of the text from the
given passage. This is the exact setting of SQuAD,
and similar problems have been addressed in, e.g.
 pointed out that about 20% questions
in SQuAD require paragraph-level information to
be asked and using the whole paragraph can improve QG performance on those questions. However, a paragraph can contain irrelevant information w.r.t. the answer for generating the question.
The challenge is thus how to effectively utilize relevant information at paragraph-level for QG. Existing neural QG works conducted on SQuAD use
1 Paragraph: carolina suffered a major setback when thomas davis , an 11year veteran who had already overcome three acl tears in his career , went
down with a broken arm in the nfc championship game . despite this , he
insisted he would still ﬁnd a way to play in the super bowl . his prediction
turned out to be accurate
Human generated: what game did thomas davis say he would play in ,
despite breaking a bone earlier on ?
Sentence-level QG: what sports game did spielberg decide to play in ?
Paragraph-level QG: what competition did thomas davis think he would
2 Paragraph: walt disney and his brother roy contacted goldenson at the
end of 1953 for abc to agree to ﬁnance part of the disneyland project in
exchange for producing a television program for the network . walt wanted
abc to invest $ 500,000 and accrued a guarantee of $ 4.5 million in additional loans , a third of the budget intended for the park . around 1954 ,
abc agreed to ﬁnance disneyland in exchange for the right to broadcast a
new sunday night program , disneyland , which debuted on the network on
october 27 , 1954 as the ﬁrst of many anthology television programs that
disney would broadcast over the course of the next 50 years
Human generated: how much did walt disney want abc to invest in disneyland ?
Sentence-level QG: how much money did walt wanted to invest ?
Paragraph-level QG: how much money did walt wanted to invest in 1953
3 Paragraph: following the peterloo massacre of 1819 , poet percy shelley
wrote the political poem the mask of anarchy later that year , that begins
with the images of what he thought to be the unjust forms of authority of
his timeand then imagines the stirrings of a new form of social action . it is
perhaps the ﬁrst modern [ vague ] statement of the principle of nonviolent
protest . a version was taken up by the author henry david thoreau in his
essay civil disobedience , and later by gandhi in his doctrine of satyagraha
. gandhi ’s satyagraha was partially inﬂuenced and inspired by shelley ’s
nonviolence in protest and political action . in particular , it is known that
gandhi would often quote shelley ’s masque of anarchy to vast audiences
during the campaign for a free india
Human generated: his poem is considered the ﬁrst kind of what type of
Sentence-level QG: what is the principle of the protest ?
Paragraph-level QG: what type of protest did percy shelley write ?
4 Paragraph: the victoria and albert museum ( often abbreviated as the
v & a ) , london , is the world ’s largest museum of decorative arts
and design , housing a permanent collection of over 4.5 million objects
. it was founded in 1852 and named after queen victoria and prince albert.
the v & a is located in the brompton district of the royal borough of kensington and chelsea , in an area that has become known as “ albertopolis ”
because of its association with prince albert , the albert memorial and the
major cultural institutions with which he was associated . these include the
natural history museum , the science museum and the royal albert hall . the
museum is a non-departmental public body sponsored by the department
for culture , media and sport . like other national british museums , entrance to the museum has been free since 2001
Human generated: when was the victoria and albert museum founded ?
Sentence-level QG: when was prince albert and prince albert founded ?
Paragraph-level QG: when was the victoria and albert museum founded ?
Figure 1: Examples where paragraph-level information is required to ask right questions.
Sentences contain answers are in italic font, while
answers are underscored. QG results are generated by model s2s-a-ct-mp-gsa.
only a sentence as context, e.g. ; when applied
to paragraph-level context we
observed large gaps compared to state-of-the-art
results achieved by using sentence-level context.
In this paper, we extend previous sequence to
sequence attention model with a maxout pointer
mechanism and a gated self-attention encoder
which outperforms existing neural QG approaches
with either sentence or paragraph as inputs. Furthermore, with paragraph-level inputs, it outperforms the results of previous approaches with
sentence-level inputs, improving state-of-the-art
result from 13.9 to 16.3 (BLEU 4). This is the
ﬁrst model that demonstrates large improvement
with paragraph as input over sentence as input.
In addition, our model is more concise compared
to most of existing ones, e.g. . Techniques like incorporating
rich features and policy gradient are orthogonal to ours and can be leveraged for further
performance improvement in the future.
Problem Deﬁnition
We use P and A to represent input passage and answer respectively, and use Q to represent the generated question. ”Passage” in this section can represent either a sentence or a paragraph. The task is
to ﬁnd ¯Q that:
¯Q = argmax
Prob(Q|P, A)
where passage is comprised of sequence of
words:P = {xt}M
t=1, answer A must be sub spans
of the passage. Words generated in Q = {yt}K
are either from the input passage, {xt}M
t=1, or from
a vocabulary V .
Figure 2 illustrates the end-to-end structure of
our model proposed in this paper.
Passage and Answer Encoding
Different types of encoders are designed for various domains . We are agnostic to the form
of the encoder and simply use recurrent neural network (RNN) to present the encoding process:
ut = RNNE(ut−1, [et, mt])
Answer Tagging. In Eq. 1, ut represents the
RNN hidden state at time step t, et is the word
embedding representation of word xt in passage
P. mt is the meta-word representation of whether
word xt is in or outside the answer. [a, b] represents the concatenation of vector a and b. We call
this approach answer tagging which is similar to
the techniques in . For applications, it is essential to be able to
generate question that is coherent to an answer.
Encoding Passage
Passage-Answer
Representation
Self Attention
Feature Fusion
Final Passage-Answer Representation
Partial Decoding
Decoding Attention
Copy Scores
Generative Scores
Generative Probability
with Padding for OOV
Copy Probability
Distrubution
Figure 2: End-to-end diagram for the model with answer tagging, gated self-attention and maxout pointer
mechanism.
If RNNE is bi-directional, u is the concatenated representation of the forward and backward
passes: U = {[−→
Gated Self-attention. Our gated self-attention
mechanism is designed to aggregate information
from the whole passage and embed intra-passage
dependency to reﬁne the encoded passage-answer
representation at every time step. It has two steps:
1) taking encoded passage-answer representation
u as input and conducting matching against itself
to compute self matching representation; 2) combining the input with self
matching representation using a feature fusion
gate .
ast = softmax(UTWsut)
st = U · ast
Step 1. In Eq. 2, Ws is a trainable weight matrix. In Eq. 3, st is the weighted sum of all words’
encoded representation in passage based on their
corresponding matching strength to current word
at t. s = {st}M
t=1 is the ﬁnal self matching representation.
ft = tanh(Wf[ut, st])
gt = sigmoid(Wg[ut, st])
ˆut = gt ⊙ft + (1 −gt) ⊙ut
Step 2. The self matching representation st is
combined with original passage-answer representation ut as the new self matching enhanced representation ft, Eq. 4. A learnable gate vector gt,
Eq. 5, chooses the information between the original passage-answer representation and the new
self matching enhanced representation to form the
ﬁnal encoded passage-answer representation ˆut,
Eq. 6, where ⊙is the element-wise multiplication.
Decoding with Attention and Maxout
In the decoding stage, the decoder is another RNN
that generates words sequentially conditioned on
the encoded input representation and the previously decoded words.
dt = RNND(dt−1, yt−1)
p(yt|{y<t}) = softmax(WV dt)
In Eq. 7, dt represents the hidden state of the RNN
at time t where d0 is passed from the ﬁnal hidden
state of the encoder. yt stands for the word generated at time t. The bold font yt is used to represent yt’s corresponding word embedding representation. In Eq. 8, ﬁrst an afﬁne layer projects dt
to a space with vocabulary-size dimensions, then
a softmax layer computes a probability distribution over all words in a ﬁxed vocabulary V . WV
is a trainable weight matrix.
Attention.
Attention mechanism has been used
to improve sequence to sequence models’ performance and has became a default setting for many
applications.
We use Luong attention mechanism to compute
raw attention scores rt, Eq. 9. An attention layer,
Eq. 12 is applied above the concatenation of decoder state dt and the attention context vector ct
and its output is used as the new decoder state.
rt = ˆUTWadt
adt = softmax(rt)
ct = ˆU · adt
ˆdt = tanh(Wb[dt, ct])
Copy/Pointer.
Copy mechanism or pointer network was introduced to allow both copying words from input via pointing, and generating
words from a predeﬁned vocabulary during decoding.
Similar to , our pointer mechanism directly leverages raw attention scores rt =
k=1 over the input sequence which has a vocabulary of χ. Words at every time step (a pointer)
are treated as unique copy targets and the ﬁnal
score on one word is calculated as the sum of all
scores pointing to the same word, Eq. 13, where
xk and yt stand for word vocabulary indices of the
kth word in input and the tth word in decoded
sequence respectively. The scores of the nonoccurence words are set to negative inﬁnity which
will be masked out by the downstream softmax
sccopy(yt) =
k,where xk=yt
We then concatenate sccopy
with the generative scores (from Eq.
8 before softmax),
], which has dimension: |V | + |χ|.
Then we perform softmax on the concatenated
vectors and sum up the probabilities pointing to
same words.
Taking softmax on the concatenated score vector enforces copy and generative
modes to compete with each other due to the
shared normalization denominator. Another popular solution is to do softmax independently to
the scores from each mode, and then combine their
output probabilities with a dynamic weight which
is generated by a trainable network . We have tested both and didn’t ﬁnd signiﬁcant difference in terms of accuracy on our QG
task. We choose the former copy approach mainly
because it doesn’t add extra trainable parameters.
Maxout Pointer. Despite the outstanding performance of existing copy/pointer mechanisms,
we observed that repeated occurrence of words
in the input sequence tends to cause repetitions
in output sequence, especially when the input sequence is long, e.g. a paragraph. This issue exacerbates the repetition problem which has already
been commonly observed in sequence to sequence
models . In this
paper, we propose a new maxout pointer mechanism to address this issue and improve the metrics
for QG task. Related works have explored MLP maxout with dropout.
Instead of combining all the scores to calculate the probability, We limit the magnitude of
scores of repeated words to their maximum value,
as shown in Eq. 14. The rest remains the same as
in the previous copy mechanism.
sccopy(yt) =
k,where xk=yt rt,k,
Experiments
In our experiments, we study the proposed model
on the QG task on SQuAD 
and MS MARCO dataset,
demonstrate the performance of proposed components on both sentence and paragraph inputs, and
compare the model with existing approaches.
The SQuAD dataset contains 536 Wikipedia articles and more than 100K questions posed about
the articles by crowd-workers. Answers are also
s2s-a-at-cp
s2s-a-at-mcp
s2s-a-at-mcp-gsa
Table 1: Performance of our models on Split1 with both sentence-level input and paragraph-level input.
Sen. means sentence, while Par. means paragraph.
provided to the questions, which are spans of tokens in the articles.
Following , our experiments are conducted
using the accessible part of SQuAD: train and development (dev*) sets. To be able to directly compare with their works, we adopt two types of data
split: 1) Split1: similar to , we use
dev* set as test set, and split train set into train and
dev sets randomly with ratio 90%-10%. The split
is done at article level. However, we keep all samples instead of only keeping the sentence-question
pairs that have at least one non-stop-word in common (with 6.7% pairs dropped) as in . This makes our dataset harder for training
and evaluation. 2) Split2: similar to , we split dev* set into dev and test sets randomly with ratio 50%-50%. The split is done at
sentence level.
MS MARCO datasets contains 100,000 queries
with corresponding answers and passages.
questions are sampled from real anonymized user
queries and context passages are extracted from
real web documents. We picked a subset of MS
MARCO data where answers are sub-spans within
the passages, and use dev set as test set (7k), and
split train set with ratio 90%-10% into train (51k)
and dev (6k) sets.
Implementation Details
We used 2 layers LSTM as the RNN cell for
both encoding and decoding. For encoding, bidirectional LSTM was used. The cell hidden size
was 600. Dropout with probability 0.3 was applied
between vertical LSTM stacks.
For word embedding, we used pre-trained GloVe word vectors
with 300 dimensions ,
and froze them during training. Dimension of answer tagging meta-word embedding was 3. Both
encoder and decoder shared the same vocabulary
of the most frequent 45k GloVe words. For optimization, we used SGD with momentum . Learning rate was initially
set to 0.1 and halved since epoch 8 at every 2
epochs afterwards.
Models were totally trained
with 20 epochs. The mini-batch size for parameter update was 64. After training, we looked at
the 4 models with lowest perplexities and selected
the one which used the most number of epochs as
ﬁnal model. During decoding for prediction, we
used beam search with the beam size of 10, and
stopped decoding when every beam in the stack
generates the EOS token.
Evaluation
We conduct automatic evaluation with metrics:
BLEU 1, BLEU 2, BLEU 3, BLEU 4 , METEOR and ROUGE-L , and use evaluation package released by 
to compute them.
Results and Analysis
Comparison of Techniques
Table 1 shows evaluation results for different
models on SQuAD Split1.
Results with both
sentence-level and paragraph-level inputs are included. Similar results also have been observed
on SQuAD Split2. The deﬁnitions of the models
under comparison are:
s2s: basic sequence to sequence model
s2s-a: s2s + attention mechanism
s2s-a-at: s2s-a + answer tagging
s2s-a-at-cp: s2s-a-at + copy mechanism
s2s-a-at-mp: s2s-a-at + maxout pointer mechanism
s2s-a-at-mp-gsa: s2s-a-at-mp + gated self-attention
Attention Mechanism
s2s-a vs. s2s: we can see attention brings in large
improvement on both sentence and paragraph inputs. The lower performance on paragraph indicates the challenge of encoding paragraph-level
information.
Answer Tagging
s2s-a-at vs. s2s-a: answer tagging dramatically
boosts the performance, which conﬁrms the importance of answer-aware QG: to generate good
question, we need to control/learn which part of
the context the generated question is asking about.
More importantly, answer tagging clearly reduces
the gap between sentence and paragraph inputs,
which could be explained with: by providing guidance on answer words, we can make the model
learn to neglect noise when processing a long context.
Copy Mechanism
s2s-a-at-cp vs. s2s-a-at: as expected, copy mechanism further improves the performance on the QG
task. pointed out most of the
sentence-question pairs in SQuAD have over 50%
overlaps in non-stop-words. Our results prove that
sequence to sequence models with copy mechanism can very well learn when to generate a word
and when to copy one from input on such QG task.
More interestingly, the performance is lower when
paragraph is given as input than sentence as input.
The gap, again, reveals the challenge of leveraging
longer context. We found that, when paragraph is
given, the model tends to generate more repetitive
words, and those words (often entities/concepts)
usually appear multiple times in the context, Figure 3. The repetition issue can also be seen for
sentence input, but it is more severe for paragraph.
Maxout Pointer
s2s-a-at-mp vs. s2s-a-at-cp: Maxout pointer is designed to resolve the repetition issue brought by
the basic copy mechanism, for example Figure 3.
The maxout pointer mechanism outperforms the
basic copy mechanism in all metrics. Moreover,
the effectiveness of maxout pointer is more significant when paragraph is given as the model input,
as it reverses the performance gap between models
Paragraph: a problem is regarded as inherently difﬁcult if its solution requires
signiﬁcant resources , whatever the algorithm used . the theory formalizes
this intuition , by introducing mathematical models of computation to study
these problems and quantifying the amount of resources needed to solve them
, such as time and storage . other complexity measures are also used , such
as the amount of communication ( used in communication complexity ) , the
number of gates in a circuit ( used in circuit complexity ) and the number of
processors ( used in parallel computing ) . one of the roles of computational
complexity theory is to determine the practical limits on what computers can
and can not do
Human generated: what unit is measured to determine circuit complexity ?
Basic copy QG: what is an example of a circuit complexity in complexity
complexity ?
Maxout Pointer QG: what is another name for circuit complexity ?
Figure 3: Example for maxout pointer vs. basic
copy/pointer.
s2s-a s2s-a-at s2s-aat-cp
s2s-aat-mp
s2s-a-atmp-gsa
Paragraph(left)
Sentence(right)
Figure 4: Word duplication rates of QG on Split1,
duplication rate on human generated question is
trained with sentence and paragraph inputs, Table
To demonstrate that maxout pointer reduces
repetitions in generated questions, we present the
word duplication rates in generated questions for
various models in Figure 4. Word duplication rate
was computed by counting the number of words
which appear more than once, and then taking a
ratio of them over the total word counts. As shown
in Figure 4, both the attention mechanism and the
basic copy mechanism introduce more repetitions,
although they improve overall accuracy according
to Table 1. For models trained with paragraph inputs, where the duplication rates are much higher,
maxout pointer reduces the duplication rates to
half of their values in the basic copy and to the
same level as model trained with sentence inputs.
Such repetition issue was also observed in other
sequence to sequence applications, e.g. who proposed a coverage model and coverage loss to resolve this issue. We implemented
and tested their approach on our QG task. Even
though the duplication ratio dropped as expected,
we observed a slight decline in the accuracy when
coverage loss was added.
prediction
ampionship
prediction
Ground-truth: what game did thomas davis say he would play in , despite breaking a bone earlier on ?
QG: what competition did thomas davis think he would play in ?
Figure 5: Self-attention alignments map: each row
represents an alignment vector of self-attention.
Gated Self-attention
s2s-a-at-mp-gsa vs.
s2s-a-at-mp:
the results
demonstrate the effectiveness of gated selfattention, in particular, when working with paragraph inputs. This is the ﬁrst time, as we know,
taking paragraph as input is better than sentence
for neural QG tasks. The observation is consistent
across all metrics. Gated self-attention helps re-
ﬁne encoded context by fusing important information with the context’s self representation properly,
especially when the context is long.
To better understand how gated self-attention
works, we visualize the self alignment vectors at
each time step of the encoded sequence for one example, in Figure 5. This example corresponds to
the example 1 in Figure 1. We can see the alignments distribution concentrates near the answer
sequence and the most relevant context: ”thomas
davis” in this example. Such alignments in turn
would help to promote the most valuable information for decoding.
Beam Search
Beam search is commonly used for decoding for
predictions. Existing neural QG works, e.g. , evaluated their
models with beam search decoding. However, so
far, none of them have reported the comparison between beam search decoding with greedy decoding. In this paper, we give such comparison for our
best model: s2s-a-at-mp-gsa with both sentence
Beam Search
Table 2: Comparison of beam search and greedy
decodings for model s2s-a-ct-mp-gsa on Split1.
and paragraph inputs in Table 2. We can clearly
see beam search decoding boosts all metrics for
both sentence and paragraph inputs. The effectiveness of beam search has also been demonstrated
for other tasks, like neural machine translation in
 .
Comparison with Existing Neural
Question Generation Works
On SQuAD dataset, we compare the BLEU, ME-
TEOR, ROUGE-L scores of our best model, s2sa-at-mp-gsa, with the numbers in the existing
works in Table 3. Comparison with and is conducted on
SQuAD data Split1, while comparison with and is conducted
on data SQuAD split2. Because 
had results on both splits, we compare with both of
On MS MARCO dataset, we compare the
BLEU 4 scores reported by in
Our model with maxout pointer and gated selfattention achieves the state-of-the-art results in
QG. Note that all those existing works in SQuAD
encoded only sentence-level information, the results from our model surpass them on the same
sentence input while achieving much higher numbers when working with paragraph.
Case Study
In Figure 1, we present some examples for
which paragraph-level information is needed to
ask good/correct questions. Generated questions
from model s2s-a-ct-mp-gsa are also presented for
both sentence and paragraph inputs. Those examples demonstrate that generated questions contain
richer information when paragraphs are provided
 
 
ours, sentence
ours, paragraph
 
 
ours, sentence
ours, paragraph
Table 3: Comparison of results on SQuAD dataset.
 
 
ours, sentence
ours, paragraph
Table 4: Comparison of results on MSMARCO
instead of sentences.
In example 1 and 3, name ”thomas davis” and
”percy shelley” appear in the paragraphs not the
sentences contain the answers.
In example 2, paragraph-level QG can generate richer description ”in 1953” from the paragraph, although human generated is ”disneyland”.
Both generated questions lack one relative important piece of information ”want abc to invest”.
In example 4, paragraph-level QG correctly
identiﬁes ”it” is referring to the museum which is
out of the sentence.
Related Work
QG has been mainly tackled with two types of approaches. One is built on top of heuristic rules that
creates questions with manually constructed template and ranks the generated results, e.g. . Those approaches heavily
depend on human effort, which makes them hard
to scale up to many domains. The other one, which
is becoming increasingly popular, is to train an
end-to-end neural network from scratch by using
sequence to sequence or encoder-decoder framework, e.g. . The second one is
more related to us, so we will focus on describing
those approaches.
 pioneered the work of automatic QG using an end-to-end trainable sequence
to sequence neural model. Automatic and human
evaluation results showed that their system outperformed the previous rule-based systems . However, in
their study, there was no control about which part
of the passage the generated question was asking
Answer-aware sequence to sequence neural QG
systems encoded answer location information using an annotation vector corresponding to the answer word positions. utilized rich features of the passage
including answer positions. deployed a two-stage neural model that detects key phrases and subsequently generates questions conditioned on them.
 
combined supervised and reinforcement learning
in the training of their model using policy gradient techniques to maximize several rewards that
measure question quality. Instead of using an annotation vector to tag the answer locations, proposed a uniﬁed framework for
QG and question answering by encoding both the
answer and the passage with a multi-perspective
matching mechanism.
 proposed
joint models to address QG and question answering together. conducted QG for
improving question answering. Due to the mixed
objectives including question answering, their approaches’ performance on QG were lower than the
state-of-the-art results.
Conclusion and Future Work
In this paper, we proposed a new sequence to
sequence network which contains a gated self-
attention encoder and a maxout pointer decoder
to address the answer-aware QG problem for long
text input.
We demonstrated the model can effectively utilize paragraph-level context, and outperformed the results with sentence-level context. The new model exceeded state-of-the-art approaches with either paragraph or sentence inputs.
We would like to discuss some potential challenges when applying the QG model in practice.
1) Answer spans aren’t provided as input. One
straight-forward method is to extract entities or
noun phrases and use them as potential answer
spans. A neural entity selection model can also be
leveraged to extract good answer candidates to improve the precision as proposed in . 2) An input passage does not contain
any eligible answers. In such case, we do not expect the model to output valid questions. We could
remove questions with low generation probability,
while a better approach could be running entity selection or quality detection model before question
generation step to eliminate ineligible passages. 3)
An answer could be shared by different questions.
We could output multiple questions using beam
search. However, beam search does not guarantee to output diversiﬁed candidates.
need to explicitly model diversity among candidates during generation, for example, leveraging
the approach described in .
Our future work lands in the following directions:
incorporate rich features, such as POS
and entity, in input passages; directly optimize
sequence-level metrics with policy gradient; relax the constraint on answer to accept abstractive
answers; jointly model question generation and
question answering; ask multiple questions simultaneously with diverse perspectives.