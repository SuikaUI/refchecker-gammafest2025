Learning Memory-guided Normality for Anomaly Detection
Hyunjong Park∗
Jongyoun Noh∗
Bumsub Ham†
School of Electrical and Electronic Engineering, Yonsei University
We address the problem of anomaly detection, that is,
detecting anomalous events in a video sequence. Anomaly
detection methods based on convolutional neural networks (CNNs) typically leverage proxy tasks, such as reconstructing input video frames, to learn models describing normality without seeing anomalous samples at training time, and quantify the extent of abnormalities using the
reconstruction error at test time. The main drawbacks of
these approaches are that they do not consider the diversity
of normal patterns explicitly, and the powerful representation capacity of CNNs allows to reconstruct abnormal video
frames. To address this problem, we present an unsupervised learning approach to anomaly detection that considers the diversity of normal patterns explicitly, while lessening the representation capacity of CNNs. To this end, we
propose to use a memory module with a new update scheme
where items in the memory record prototypical patterns
of normal data. We also present novel feature compactness and separateness losses to train the memory, boosting
the discriminative power of both memory items and deeply
learned features from normal data. Experimental results on
standard benchmarks demonstrate the effectiveness and ef-
ﬁciency of our approach, which outperforms the state of the
1. Introduction
The problem of detecting abnormal events in video sequences, e.g., vehicles on sidewalks, has attracted significant attention over the last decade, which is particularly
important for surveillance and fault detection systems. It
is extremely challenging for a number of reasons: First,
anomalous events are determined differently according to
circumstances. Namely, the same activity could be normal
or abnormal (e.g., holding a knife in the kitchen or in the
park). Manually annotating anomalous events is in this context labor intensive. Second, collecting anomalous datasets
requires a lot of effort, as anomalous events rarely happen
in real-life situations. Anomaly detection is thus typically
deemed to be an unsupervised learning problem, aiming at
∗Equal contribution. †Corresponding author.
Figure 1: Distributions of features and memory items of our model
on CUHK Avenue . The features and items are shown in points
and stars, respectively. The points with the same color are mapped
to the same item. The items in the memory capture diverse and
prototypical patterns of normal data. The features are highly discriminative and similar image patches are clustered well. (Best
viewed in color.)
learning a model describing normality without anomalous
samples. At test time, events and activities not described by
the model are then considered as anomalies.
There are many attempts to model normality in video sequences using unsupervised learning approaches. At training time, given normal video frames as inputs, they typically extract feature representations and try to reconstruct
the inputs again. The video frames of large reconstruction
errors are then treated as anomalies at test time. This assumes that abnormal samples are not reconstructed well, as
the models have never seen them during training. Recent
methods based on convolutional neural networks (CNNs)
exploit an autoencoder (AE) .
The powerful representation capacity of CNNs allows to extract better feature representations.
The CNN features from abnormal
frames, on the other hand, are likely to be reconstructed
by combining those of normal ones . In this case,
abnormal frames have low reconstruction errors, often ocarXiv:2003.13228v1 [cs.CV] 30 Mar 2020
curring when a majority of the abnormal frames are normal (e.g., pedestrians in a park). In order to lessen the capacity of CNNs, a video prediction framework is introduced that minimizes the difference between a predicted
future frame and its ground truth. The drawback of these
methods is that they do not detect anomalies directly . They instead leverage proxy tasks for anomaly
detection, e.g., reconstructing input frames or predicting future frames , to extract general feature representations rather than normal patterns. To overcome this
problem, Deep SVDD exploits the one-class classi-
ﬁcation objective to map normal data into a hypersphere.
Speciﬁcally, it minimizes the volume of the hypersphere
such that normal samples are mapped closely to the center
of the sphere. Although a single center of the sphere represents a universal characteristic of normal data, this does not
consider various patterns of normal samples.
We present in this paper an unsupervised learning approach to anomaly detection in video sequences considering the diversity of normal patterns.
We assume that
a single prototypical feature is not enough to represent
various patterns of normal data. That is, multiple prototypes (i.e., modes or centroids of features) exist in the feature space of normal video frames (Fig. 1). To implement
this idea, we propose a memory module for anomaly detection, where individual items in the memory correspond
to prototypical features of normal patterns. We represent
video frames using the prototypical features in the memory items, lessening the capacity of CNNs. To reduce the
intra-class variations of CNN features, we propose a feature compactness loss, mapping the features of a normal
video frame to the nearest item in the memory and encouraging them to be close. Simply updating memory items
and extracting CNN features alternatively in turn give a degenerate solution, where all items are similar and thus all
features are mapped closely in the embedding space. To
address this problem, we propose a feature separateness
loss. It minimizes the distance between each feature and
its nearest item, while maximizing the discrepancy between
the feature and the second nearest one, separating individual items in the memory and enhancing the discriminative
power of the features and memory items. We also introduce
an update strategy to prevent the memory from recording
features of anomalous samples at test time. To this end,
we propose a weighted regular score measuring how many
anomalies exist within a video frame, such that the items
are updated only when the frame is determined as a normal one. Experimental results on standard benchmarks, including UCSD Ped2 , CUHK Avenue and ShanghaiTech , demonstrate the effectiveness and efﬁciency
of our approach, outperforming the state of the art.
The main contributions of this paper can be summarized
as follows:
• We propose to use multiple prototypes to represent the
diverse patterns of normal video frames for unsupervised
anomaly detection. To this end, we introduce a memory
module recording prototypical patterns of normal data on
the items in the memory.
• We propose feature compactness and separateness losses
to train the memory, ensuring the diversity and discriminative power of the memory items. We also present a
new update scheme of the memory, when both normal
and abnormal samples exist at test time.
• We achieve a new state of the art on standard benchmarks
for unsupervised anomaly detection in video sequences.
We also provide an extensive experimental analysis with
ablation studies.
Our code and models are available online: https://
cvlab.yonsei.ac.kr/projects/MNAD.
2. Related work
Anomaly detection.
Many works formulate anomaly detection as an unsupervised learning problem, where anomalous data are not available at training time.
They typically adopt reconstructive or discriminative approaches to
learn models describing normality.
Reconstructive models encode normal patterns using representation learning
methods such as an AE , a sparse dictionary learning , and a generative model . Discriminative
models characterize the statistical distributions of normal
samples and obtain decision boundaries around the normal
instances e.g., using Markov random ﬁeld (MRF) , a
mixture of dynamic textures (MDT) , Gaussian regression , and one-class classiﬁcation . These
approaches, however, often fail to capture the complex distributions of high-dimensional data such as images and
videos .
CNNs have allowed remarkable advances in anomaly
detection over the last decade. Many anomaly detection
methods leverage reconstructive models exploiting feature representations from e.g., a convolutional
AE (Conv-AE) , a 3D Conv-AE , a recurrent neural network (RNN) , and a generative adversarial network (GAN) . Although CNN-based methods outperform classical approaches by large margins, they
even reconstruct anomalous samples with a combination of
normal ones, mainly due to the representation capacity of
CNNs. This problem can be alleviated by using predictive
or discriminative models . The work of assumes that anomalous frames in video sequences are unpredictable, and trains a network for predicting future frames
rather than the input itself . It achieves a remarkable
performance gain over reconstructive models, but at the
cost of runtime for estimating optical ﬂow between video
frames. It also requires ground-truth optical ﬂow to train a
sub-network for computing ﬂow ﬁelds. Deep SVDD 
Memory Module
Output #!"
Figure 2: Overview of our framework for reconstructing a video
frame. Our model mainly consists of three parts: an encoder, a
memory module, and a decoder. The encoder extracts a query
map qt of size H ×W ×C from an input video frame It at time t.
The memory module performs reading and updating items pm of
size 1×1×C using queries qk
t of size 1×1×C, where the numbers
of items and queries are M and K, respectively, and K = H×W.
The query map qt is concatenated with the aggregated (i.e., read)
items ˆpt. The decoder then inputs them to reconstruct the video
frame ˆIt. For the prediction task, we input four successive video
frames to predict the ﬁfth one. (Best viewed in color.)
leverages CNNs as mapping functions that transform normal data into the center of the hypersphere, whereas forcing anomalous samples to fall outside the sphere, using the
one-class classiﬁcation objective. Our method also lessens
the representation capacity of CNNs but using a different
way. We reconstruct or predict a video frame with a combination of items in the memory, rather than using CNN
features directly from an encoder, while considering various patterns of normal data. In case of future frame prediction, our model does not require computing optical ﬂow, and
thus it is much faster than the current method . Deep-
Cascade detects various normal patches using cascaded
deep networks. In contrast, our method leverages memory items to record the normal pattern explicitly even in
test sequences. Concurrent to our method, Gong et al. introduce a memory-augmented autoencoder (MemAE) for
anomaly detection . It also uses CNN features but using a 3D Conv-AE to retrieve relevant memory items that
record normal patterns, where the items are updated during training only. Unlike this approach, our model better
records diverse and discriminative normal patterns by separating memory items explicitly using feature compactness
and separateness losses, enabling using a small number of
items compared to MemAE (10 vs 2,000 for MemAE). We
also update the memory at test time, while discriminating
anomalies simultaneously, suggesting that our model also
memorizes normal patterns of test data.
Memory networks.
There are a number of attempts to
capture long-term dependencies in sequential data. Long
short-term memory (LSTM) addresses this problem
using local memory cells, where hidden states of the network record information in the past partially. The memorization performance is, however, limited, as the size of
the cell is typically small and the knowledge in the hidden state is compressed. To overcome the limitation, memory networks have recently been introduced. It uses
a global memory that can be read and written to, and performs a memorization task better than classical approaches.
The memory networks, however, require layer-wise supervision to learn models, making it hard to train them using
standard backpropagation. More recent works use continuous memory representations or key-value pairs 
to read/write memories, allowing to train the memory networks end-to-end. Several works adopt the memory networks for computer vision tasks including visual question
answering , one-shot learning , image generation , and video summarization . Our work also
exploits a memory module but for anomaly detection with a
different memory updating strategy. We record various patterns of normal data to individual items in the memory, and
consider each item as a prototypical feature.
3. Approach
We show in Fig. 2 an overview of our framework. We
reconstruct input frames or predict future ones for unsupervised anomaly detection. Following , we input four successive video frames to predict the ﬁfth one for the prediction task. As the prediction can be considered as a reconstruction of the future frame using previous ones, we use
almost the same network architecture with the same losses
for both tasks. We describe hereafter our approach for the
reconstruction task in detail.
Our model mainly consists of three components: an encoder, a memory module, and a decoder. The encoder inputs
a normal video frame and extracts query features. The features are then used to retrieve prototypical normal patterns
in the memory items and to update the memory. We feed
the query features and memory items aggregated (i.e., read)
to the decoder for reconstructing the input video frame. We
train our model using reconstruction, feature compactness,
and feature separateness losses end-to-end. At test time, we
use a weighted regular score in order to prevent the memory
from being updated by abnormal video frames. We com-
Figure 3: Illustration of reading and updating the memory. To read
items in the memory, we compute matching probabilities wk,m
in (1) between the query qk
t and items (p1, . . . pM), and apply a weighted average of the items with the probabilities to obtain the feature ˆpk
t . To update the items, we compute another
matching probabilities vk,m
in (4) between the item pm and the
queries (q1
t, . . . qK
t ). We then compute a weighted average of the
queries in the set U m
with the corresponding probabilities, and
add it to the initial item pm in (3). c: cosine similarities; s: a softmax function; w: a weighted average; n: max normalization; U m
a set of indices for the m-th memory item. See text for details.
(Best viewed in color.)
pute the discrepancies between the input frame and its reconstruction and the distances between the query feature
and the nearest item in the memory to quantify the extent
of abnormalities in a video frame.
3.1. Network architecture
Encoder and decoder
We exploit the U-Net architecture , widely used for the
tasks of reconstruction and future frame prediction , to
extract feature representations from input video frames and
to reconstruct the frames from the features. Differently, we
remove the last batch normalization and ReLU layers in the encoder, as the ReLU cuts off negative values,
restricting diverse feature representations. We instead add
an L2 normalization layer to make the features have a common scale. Skip connections in the U-Net architecture may
not be able to extract useful features from the video frames
especially for the reconstruction task, and our model may
learn to copy the inputs for the reconstruction. We thus remove the skip connections for the reconstruction task, while
retaining them for predicting future frames. We denote by It
and qt a video frame and a corresponding feature (i.e., a
query) from the encoder at time t, respectively. The encoder inputs the video frame It and gives the query map qt
of size H × W × C, where H, W, C are height, width,
and the number of channels, respectively. We denote by
t ∈RC (k = 1, . . . K), where K = H × W, individual
queries of size 1 × 1 × C in the query map qt. The queries
are then inputted to the memory module to read the items
in the memory or to update the items, such that they record
prototypical normal patterns. The detailed descriptions of
the memory module are presented in the following section.
The decoder inputs the queries and retrieved memory items
and reconstructs the video frame ˆIt.
The memory module contains M items recording various
prototypical patterns of normal data. We denote by pm ∈
RC (m = 1, . . . , M) the item in the memory. The memory
performs reading and updating the items (Fig. 3).
To read the items, we compute the cosine similarity
between each query qk
t and all memory items pm, resulting
in a 2-dimensional correlation map of size M × K. We
then apply a softmax function along a vertical direction, and
obtain matching probabilities wk,m
as follows:
exp((pm)T qk
m′=1 exp((pm′)T qk
For each query qk
t , we read the memory by a weighted average of the items pm with the corresponding weights wk,m
and obtain the feature ˆpk
t ∈RC as follows:
Using all items instead of the closest item allows our
model to understand diverse normal patterns, taking into
account the overall normal characteristics. That is, we represent the query qk
t with a combination of the items pm
in the memory. We apply the reading operator to individual queries, and obtain a transformed feature map ˆpt ∈
RH×W ×C (i.e., aggregated items). We concatenate it with
the query map qt along the channel dimension, and input
them to the decoder. This enables the decoder to reconstruct the input frame using normal patterns in the items,
lessening the representation capacity of CNNs, while understanding the normality.
For each memory item, we select all queries declared that the item is the nearest one, using the matching
probabilities in (1). Note that multiple queries can be assigned to a single item in the memory. See, for example,
Fig. 5 in Sec. 4.3. We denote by U m
the set of indices for
the corresponding queries for the m-th item in the memory.
We update the item using the queries indexed by the set U m
only as follows:
pm ←f(pm +
where f(·) is the L2 norm. By using a weighted average
of the queries, rather than summing them up, we can concentrate more on the queries near the item. To this end, we
compute matching probabilities vk,m
similar to (1) but by
applying the softmax function to the correlation map of size
M × K along a horizontal direction as
exp((pm)T qk
k′=1 exp((pm)T qk′
and renormalize it to consider the queries indexed by the
as follows:
We update memory items recording prototypical features
at both training and test time, since normal patterns in training and test sets may be different and they could vary with
various factors, e.g., illumination and occlusion. As both
normal and abnormal frames are available at test time, we
propose to use a weighted regular score to prevent the memory items from recording patterns in the abnormal frames.
Given a video frame It, we use the weighted reconstruction
error between It and ˆIt as the regular score Et:
Wij(ˆIt, It)∥ˆIij
where the weight function Wij(·) is
Wij(ˆIt, It) =
1 −exp(−||ˆI
i,j 1 −exp(−||ˆI
and i and j are spatial indices. When the score Et is higher
than a threshold γ, we regard the frame It as an abnormal
sample, and do not use it for updating memory items. Note
that we use this score only when updating the memory. The
weight function allows to focus more on the regions of large
reconstruction errors, as abnormal activities typically appear within small parts of the video frame.
3.2. Training loss
We exploit the video frames as a supervisory signal to
discriminate normal and abnormal samples. To train our
model, we use reconstruction, feature compactness, and
feature separateness losses (Lrec, Lcompact and Lseparate,
respectively), balanced by the parameters λc and λs as follows:
L = Lrec + λcLcompact + λsLseparate.
Reconstruction loss.
The reconstruction loss makes the
video frame reconstructed from the decoder similar to its
ground truth by penalizing the intensity differences. Specifically, we minimize the L2 distance between the decoder
output ˆIt and the ground truth It:
∥ˆIt −It∥2,
where we denote T by the total length of a video sequence.
We set the ﬁrst time step to 1 and 5 for reconstruction and
prediction tasks, respectively.
Feature compactness loss.
The feature compactness loss
encourages the queries to be close to the nearest item in
the memory, reducing intra-class variations. It penalizes the
discrepancies between them in terms of the L2 norm as:
Lcompact =
where p is an index of the nearest item for the query qk
deﬁned as,
p = argmax
Note that the feature compactness loss and the center
loss are similar, as the memory item pp corresponds the
center of deep features in the center loss. They are different
in that the item in (10) is retrieved from the memory, and it
is updated without any supervisory signals, while the cluster
center in the center loss is computed directly using the features learned from ground-truth class labels. Note also that
our method can be considered as an unsupervised learning
of joint clustering and feature representations. In this task,
degenerate solutions are likely to be obtained . As
will be seen in our experiments, training our model using
the feature compactness loss only makes all items similar,
and thus all queries are mapped closely in the embedding
space, losing the capability of recording diverse normal patterns.
Feature separateness loss.
Similar queries should be allocated to the same item in order to reduce the number of
items and the memory size. The feature compactness loss
in (10) makes all queries and memory items close to each
other, as we extract the features (i.e., queries) and update
the items alternatively, resulting that all items are similar.
The items in the memory, however, should be far enough
apart from each other to consider various patterns of normal data. To prevent this problem while obtaining compact
feature representations, we propose a feature separateness
loss, deﬁned with a margin of α as follows:
Lseparate =
t −pp∥2−∥qk
t −pn∥2+α]+, (12)
where we set the query qk
t , its nearest item pp and the second nearest item pn as an anchor, and positive and hard
negative samples, respectively. We denote by n an index of
the second nearest item for the query qk
n = argmax
Note that this is different from the typical use of the triplet
loss that requires ground-truth positive and negative samples for the anchor. Our loss encourages the query and the
second nearest item to be distant, while the query and the
nearest one to be nearby. This has the effect of placing the
items far away. As a result, the feature separateness loss
allows to update the item nearest to the query, whereas discarding the inﬂuence of the second nearest item, separating
all items in the memory and enhancing the discriminative
3.3. Abnormality score
We quantify the extent of normalities or abnormalities in
a video frame at test time. We assume that the queries obtained from a normal video frame are similar to the memory
items, as they record prototypical patterns of normal data.
We compute the L2 distance between each query and the
nearest item as follows:
D(qt, p) = 1
We also exploit the memory items implicitly to compute
the abnormality score.
We measure how well the video
frame is reconstructed using the memory items. This assumes that anomalous patterns in the video frame are not
reconstructed by the memory items. Following , we
compute the PSNR between the input video frame and its
reonstruction:
P(ˆIt, It) = 10 log10
∥ˆIt −It∥2
where N is the number of pixels in the video frame. When
the frame It is abnormal, we obtain a low value of PSNR
and vice versa. Following , we normalize each error in (14) and (15) in the range of by a min-max normalization . We deﬁne the ﬁnal abnormality score St
for each video frame as the sum of two metrics, balanced
by the parameter λ, as follows:
St = λ(1 −g(P(ˆIt, It))) + (1 −λ)g(D(qt, p)),
where we denote by g(·) the min-max normalization 
over whole video frames, e.g.,
g(D(qt, p)) =
D(qt, p) −mint(D(qt, p)
maxt(D(qt, p)) −mint(D(qt, p)).
4. Experiments
4.1. Implementation details
We evaluate our method on three benchmark
datasets and compare the performance with the state of the
art. 1) The UCSD Ped2 dataset contains 16 training
and 12 test videos with 12 irregular events, including riding a bike and driving a vehicle. 2) The CUHK Avenue
dataset consists of 16 training and 21 test videos with
47 abnormal events such as running and throwing stuff. 3)
The ShanghaiTech dataset contains 330 training and
107 test videos of 13 scenes. It is the largest dataset among
existing benchmarks for anomaly detection.
We resize each video frame to the size of 256
× 256 and normalize it to the range of [-1, 1]. We set the
height H and the width W of the query feature map, and
the numbers of feature channels C and memory items M
to 32, 32, 512 and 10, respectively. We use the Adam optimizer with β1 = 0.9 and β2 = 0.999, with a batch size
of 4 for 60, 60, and 10 epochs on UCSD Ped2 , CUHK
Avenue , and ShanghaiTech , respectively. We set
initial learning rates to 2e-5 and 2e-4, respectively, for reconstruction and prediction tasks, and decay them using a
cosine annealing method . For the reconstruction task,
we use a grid search to set hyper-parameters on the test split
of UCSD Ped1 : λc = 0.01, λs = 0.01, λ = 0.7, α = 1
and γ = 0.015. We use different parameters for the prediction task similarly chosen using a grid search: λc = 0.1,
λs = 0.1, λ = 0.6, α = 1 and γ = 0.01. All models are
trained end-to-end using PyTorch , taking about 1, 15
and 36 hours for UCSD Ped2, CUHK Avenue, and ShanghaiTech, respectively, with an Nvidia GTX TITAN Xp.
4.2. Results
Comparison with the state of the art.
We compare in
Table 1 our models with the state of the art for anomaly
detection on UCSD Ped2 , CUHK Avenue , and
ShanghaiTech .
Following the experimental protocol in , we measure the average area under
curve (AUC) by computing the area under the receiver operation characteristics (ROC) with varying threshold values
for abnormality scores. We report the AUC performance of
our models using memory modules for the tasks of frame reconstruction and future frame prediction. For comparison,
we also provide the performance without the memory module. The sufﬁces ‘-R’ and ‘-P’ indicate the reconstruction
and prediction tasks, respectively.
From the table, we observe three things: (1) Our model
with the prediction task (Ours-P w/ Mem.) gives the best
results on UCSD Ped2 and CUHK Avenue, achieving the
average AUC of 97.0% and 88.5%, respectively.
demonstrates the effectiveness of our approach to exploiting a memory module for anomaly detection.
our method is outperformed by Frame-Pred on Shang-
Table 1: Quantitative comparison with the state of the art for
anomaly detection. We measure the average AUC (%) on UCSD
Ped2 , CUHK Avenue , and ShanghaiTech . Numbers
in bold indicate the best performance and underscored ones are the
second best.
Ped2 Avenue Shanghai 
MPPCA 
MPPC+SFA 
Unmasking 
MT-FRCN 
ConvAE 
StackRNN 
AbnormalGAN 
MemAE w/o Mem. 
MemAE w/ Mem. 
Ours-R w/o Mem.
Ours-R w/ Mem.
Frame-Pred 
Ours-P w/o Mem.
Ours-P w/ Mem.
haiTech, it uses additional modules for estimating optical
ﬂow, which requires more network parameters and groundtruth ﬂow ﬁelds. Moreover, Frame-Pred leverages an adversarial learning framework, taking lots of effort to train
the network.
On the contrary, our model uses a simple
AE for extracting features and predicting the future frame,
and thus it is much faster than Frame-Pred (67 fps vs. 25
fps). This suggests that our model offers a good compromise in terms of AUC and runtime; (2) Our model with the
reconstruction task (Ours-R w/ Mem.) shows the competitive performance compared to other reconstructive methods
on UCSD Ped2, and outperforms them on other datasets,
except MemAE . Note that MemAE exploits 3D convolutions with 2,000 memory items of size 256. On the
contrary, our model uses 2D convolutions and it requires 10
items of size 512. It is thus computationally much cheaper
than MemAE: 67 fps for our model vs. 45 fps for MemAE;
(3) Our memory module boosts the AUC performance signiﬁcantly regardless of the tasks on all datasets. For example, the AUC gains are 2.7%, 4.0%, and 3.7% on UCSD
Ped2, CUHK Avenue, and ShanghaiTech, respectively, for
the prediction task. This indicates that the memory module
is generic and it can be added to other anomaly detection
With an Nvidia GTX TITAN Xp, our current
implementation takes on average 0.015 seconds to determine abnormality for an image of size 256 × 256 on UCSD
Ped2 . Namely, we achieve 67 fps for anomaly detection, which is much faster than other state-of-the-art meth-
Figure 4: Qualitative results for future frame prediction on (top
to bottom) UCSD Ped2 , CUHK Avenue , and ShanghaiTech : input frames (left); prediction error (middle); abnormal regions (right). We can see that our model localizes the
regions of abnormal events. Best viewed in color.
ods based on CNNs, e.g., 20 fps for Unmasking , 50 fps
for StackRNN , 25 fps for Frame-Pred , and 45 fps
for MemAE with the same setting as ours.
Qualitative results.
We show in Fig. 4 qualitative results of our model for future frame prediction on UCSD
Ped2 , CUHK Avenue , and ShanghaiTech . It
shows input frames, prediction error, and abnormal regions
overlaid to the frame. For visualizing the anomalies, we
compute pixel-wise abnormality scores similar to (16). We
then mark the regions whose abnormality scores are larger
than the average value within the frame. We can see that 1)
normal regions are predicted well, while abnormal regions
are not, and 2) abnormal events, such as the appearance of
vehicle, jumping and ﬁght on UCSD Ped2, CUHK Avenue,
and ShanghaiTech, respectively, are highlighted.
4.3. Discussions
Ablation study.
We show an ablation analysis on different components of our models in Table 2. We report the
AUC performance for the variants of our models for reconstruction and prediction tasks on UCSD Ped2 . As the
AUC performance of both tasks shows a similar trend, we
describe the results for the frame reconstruction in detail.
We train the baseline model in the ﬁrst row with the reconstruction loss, and use PSNR only to compute abnormality scores. From the second row, we can see that our model
with the memory module gives better results. The third row
shows that the AUC performance even drops when the feature compactness loss is additionally used, as the memory
Table 2: Quantitative comparison for variants of our model. We
measure the average AUC (%) on UCSD Ped2 .
Lcompact Lseparate
Query features
Memory items
Query features
Memory items
Figure 5: Visualization of matching probabilities in (1) learned
with (left) and without (right) the feature separateness loss (blue:
low, yellow: high). We randomly select 10 query features for the
purpose of visualization. Best viewed in color.
items are not discriminative. The last row demonstrates that
the feature separateness loss boosts the performance drastically. It provides the AUC gain of 3.8%, which is quite
signiﬁcant. The last four rows indicate that 1) feature compactness and separateness losses are complementary, 2) updating the memory item using Et with normal frames only
at test time largely boosts the AUC performance, and 3) our
abnormality score St, using both PSNR and memory items,
quantiﬁes the extent of anomalies better than the one based
on PSNR only.
Memory items.
We visualize in Fig. 5 matching probabilities in (1) from the model trained with/without the feature separateness loss for the reconstruction task on UCSD
Ped2 . We observe that each query is highly activated
on a few items with the separateness loss, demonstrating
that the items and queries are highly discriminative, allowing the sparse access of the memory. This also indicates that
abnormal samples are not likely to be reconstructed with a
combination of memory items.
Feature distribution.
We visualize in Fig. 6 the distribution of query features for the reconstruction task, randomly
chosen from UCSD Ped2 , learned with and without the
feature separateness loss. We can see that our model trained
Figure 6: t-SNE visualization for query features and memory items.
We randomly sample 10K query features, learned
with (left) and without (right) the feature separateness loss, from
UCSD Ped2 . The features and memory items are shown in
points and stars, respectively. The points with the same color are
mapped to the same item. The feature separateness loss enables
separating the items, recording the diverse prototypes of normal
data. Best viewed in color.
without the separateness loss loses the discriminability of
memory items, and thus all features are mapped closely in
the embedding space. The separateness loss allows to separate individual items in the memory, suggesting that it enhances the discriminative power of query features and memory items signiﬁcantly. We can also see that our model gives
compact feature representations.
Reconstruction with motion cues.
Following , we use
multiple frames for the reconstruction task. Speciﬁcally,
we input sixteen successive video frames to reconstruct the
ninth one. This achieves AUC of 91.0% for UCSD Ped2,
providing the AUC gain of 0.8% but requiring more network parameters (∼4MB).
5. Conclusion
We have introduced an unsupervised learning approach
to anomaly detection in video sequences that exploits multiple prototypes to consider the various patterns of normal
To this end, we have suggested to use a memory
module to record the prototypical patterns to the items in
the memory. We have shown that training the memory using feature compactness and separateness losses separates
the items, enabling the sparse access of the memory. We
have also presented a new memory update scheme when
both normal and abnormal samples exist, which boosts the
performance of anomaly detection signiﬁcantly. Extensive
experimental evaluations on standard benchmarks demonstrate the our model outperforms the state of the art.
Acknowledgments.
This research was partly supported
by Samsung Electronics Company, Ltd., Device Solutions
under Grant, Deep Learning based Anomaly Detection,
20182020, and R&D program for Advanced Integratedintelligence for Identiﬁcation (AIID) through the National
Research Foundation of KOREA(NRF) funded by Ministry
of Science and ICT (NRF-2018M3E3A1057289).