Client-server multi-task learning from distributed
Francesco Dinuzzo∗, Gianluigi Pillonetto†,and Giuseppe De Nicolao‡
A client-server architecture to simultaneously solve multiple learning
tasks from distributed datasets is described. In such architecture, each
client is associated with an individual learning task and the associated
dataset of examples. The goal of the architecture is to perform information
fusion from multiple datasets while preserving privacy of individual data.
The role of the server is to collect data in real-time from the clients and
codify the information in a common database. The information coded
in this database can be used by all the clients to solve their individual
learning task, so that each client can exploit the informative content of
all the datasets without actually having access to private data of others.
The proposed algorithmic framework, based on regularization theory and
kernel methods, uses a suitable class of “mixed eﬀect” kernels. The new
method is illustrated through a simulated music recommendation system.
Introduction
The solution of learning tasks by joint analysis of multiple datasets is receiving
increasing attention in diﬀerent ﬁelds and under various perspectives. Indeed,
the information provided by data for a speciﬁc task may serve as a domainspeciﬁc inductive bias for the others.
Combining datasets to solve multiple
learning tasks is an approach known in the machine learning literature as multitask learning or learning to learn .
In this context,
the analysis of the inductive transfer process and the investigation of general
methodologies for the simultaneous learning of multiple tasks are important topics of research. Many theoretical and experimental results support the intuition
that, when relationships exist between the tasks, simultaneous learning performs
better than separate (single-task) learning .
Theoretical results include the extension to the multi-task setting of generalization bounds and the notion of VC-dimension and a methodology for
learning multiple tasks exploiting unlabeled data (the so-called semi-supervised
setting) .
∗Francesco Dinuzzo is with Department of Mathematics, University of Pavia, and Risk and
Security Study Center, Istituto Universitario di Studi Superiori (IUSS), Pavia, Italy e-mail:
 .
†Gianluigi Pillonetto is with Department of Information Engineering, University of Padova,
Padova, Italy, e-mail: 
‡Giuseppe De Nicolao is with Department of Computer Engineering and Systems Science,
University of Pavia, Pavia, Italy, e-mail: 
 
Importance of combining datasets is especially evident in biomedicine. In
pharmacological experiments, few training examples are typically available for a
speciﬁc subject due to technological and ethical constraints . This makes
hard to formulate and quantify models from experimental data. To obviate this
problem, the so-called population method has been studied and applied with success since the seventies in pharmacology . Population methods are
based on the knowledge that subjects, albeit diﬀerent, belong to a population
of similar individuals, so that data collected in one subject may be informative
with respect to the others . Such population approaches belongs to the
family of so-called mixed-eﬀect statistical methods.
In these methods, clinical measurements from diﬀerent subjects are combined to simultaneously learn
individual features of physiological responses to drug administration . Population methods have been applied with success also in other biomedical contexts
such as medical imaging and bioinformatics . Classical approaches postulate ﬁnite-dimensional nonlinear dynamical systems whose unknown parameters
can be determined by means of optimization algorithms . Other
strategies include Bayesian estimation with stochastic simulation 
and nonparametric population methods .
Information fusion from diﬀerent but related datasets is widespread also in
econometrics and marketing analysis, where the goal is to learn user preferences by analyzing both user-speciﬁc information and information from related
users, see e.g. . The so-called conjoint analysis aims to determine
the features of a product that mostly inﬂuence customer’s decisions.
web, collaborative approaches to estimate user preferences have become standard methodologies in many commercial systems and social networks, under the
name of collaborative ﬁltering or recommender systems, see e.g. . Pioneering
collaborative ﬁltering systems include Tapestry , GroupLens , ReferralWeb , PHOAKS . More recently, the collaborative ﬁltering problem
has been attacked with machine learning methodologies such as Bayesian networks , MCMC algorithms , mixture models , dependency networks
 , maximum margin matrix factorization .
Coming back to the machine learning literature, in the single-task context
much attention has been given in the last years to non-parametric techniques
such as kernel methods and Gaussian processes . These approaches are
powerful and theoretically sound, having their mathematical foundations in regularization theory for inverse problems, statistical learning theory and Bayesian
estimation . The ﬂexibility of kernel engineering allows
for the estimation of functions deﬁned on generic sets from arbitrary sources of
data. These methodologies have been recently extended to the multi-task setting. In , a general framework to solve multi-task learning problems using
kernel methods and regularization has been proposed, relying on the theory of
reproducing kernel Hilbert spaces (RKHS) of vector-valued functions .
In many applications (e-commerce, social network data processing, recommender systems), real-time processing of examples is required. On-line multitask learning schemes ﬁnd their natural application in data mining problems
involving very large datasets, and are therefore required to scale well with the
number of tasks and examples. In , an on-line task-wise algorithm to solve
multi-task regression problems has been proposed. The learning problem is formulated in the context of on-line Bayesian estimation, see e.g. , within
which Gaussian processes with suitable covariance functions are used to char-
acterize a non-parametric mixed-eﬀect model. One of the key features of the
algorithm is the capability to exploit shared inputs between the tasks in order
to reduce computational complexity. However, the algorithm in has a centralized structure in which tasks are sequentially analyzed, and is not able to
address neither architectural issues regarding the ﬂux of information nor privacy
protection.
In this paper, multi-task learning from distributed datasets is addressed
using a client-server architecture. In our scheme, clients are in a one-to-one
correspondence with tasks and their individual database of examples. The role
of the server is to collect examples from diﬀerent clients in order to summarize
their informative content. When a new example associated with any task becomes available, the server executes an on-line update algorithm. While in 
diﬀerent tasks are sequentially analyzed, the architecture presented in this paper can process examples coming in any order from diﬀerent learning tasks. The
summarized information is stored in a disclosed database whose content is available for download enabling each client to compute its own estimate exploiting
the informative content of all the other datasets. Particular attention is paid
to conﬁdentiality issues, especially valuable in commercial and recommender
systems, see e.g. . First, we require that each speciﬁc client cannot access other clients data. In addition, individual datasets cannot be reconstructed
from the disclosed database. Two kind of clients are considered: active and
passive ones. An active client sends its data to the server, thus contributing to
the collaborative estimate. A passive client only downloads information from
the disclosed database without sending its data. A regularization problem with
a parametric bias term is considered in which a mixed-eﬀect kernel is used to
exploit relationships between the tasks. Albeit speciﬁc, the mixed-eﬀect nonparametric model is quite ﬂexible, and its usefulness has been demonstrated in
several works .
The paper is organized as follows. Multi-task learning with regularized kernel
methods is presented in section 2, in which a class of mixed-eﬀect kernels is also
introduced. In section 3, an eﬃcient centralized oﬀ-line algorithm for multitask learning is described that solves the regularization problem of section 2.
In section 4, a rather general client-server architecture is described, which is
able to eﬃciently solve online multi-task learning from distributed datasets.
The server-side algorithm is derived and discussed in subsection 4.1, while the
client-side algorithm for both active and passive clients is derived in subsection
4.2. In section 5, a simulated music recommendation system is employed to
test performances of our algorithm. Conclusions (section 6) end the paper. The
Appendix contains technical lemmas and proofs.
Notational preliminaries
• X denotes a generic set with cardinality |X|.
• A vector is an element of a ∈Xn (an object with one index). Vectors are
denoted by lowercase bold characters. Vector components are denoted by
the corresponding non-bold letter with a subscript (e.g. ai denotes the
i-th component of a).
• A matrix is an element of A ∈Xn×m (an object with two indices). Matrices are denoted by uppercase bold characters. Matrix entries are denoted
by the corresponding non-bold letter with two subscript (e.g. Aij denotes
the entry of place (i, j) of A).
• Vectors y ∈Rn are associated with column matrices, unless otherwise
• For all n ∈N, let [n] := {1, 2, . . . , n}.
• Let I denote the identity matrix of suitable dimension.
• Let ei ∈Rn denote the i-th element of the canonical basis of Rn (all zeros
with 1 in position i):
• An (n, p) index vector is an object k ∈[n]p.
• Given a vector a ∈Xn and an (n, p) index vector k, let
• Given a matrix A ∈Xn×m and two index vectors k1 e k2, that are (n, p1)
and (m, p2), respectively, let
A(k1, k2) :=
• Finally, let
A(:, k2) := A([n], k2),
A(k1, :) := A(k1, [m]).
Notice that vectors, as deﬁned in this paper, are not necessarily elements of
a vector space. The deﬁnition of “vector” adopted in this paper is similar to
that used in standard object-oriented programming languages such as C++.
Problem formulation
Let m ∈N denote the total number of tasks. For the task j, a vector of ℓj
input-output pairs Sj ∈(X × R)ℓj is available:
  (x1j, y1j)
(xℓjj, yℓjj) 
sampled from a distribution Pj on X × R. The aim of a multi-task regression
problem is learning m functions fj : X →R, such that expected errors with
respect to some loss function L
L(y, fj(x))dPj
are small.
Task-labeling is a simple technique to reduce multi-task learning problems to
single-task ones. Task-labels are integers ti that identify a speciﬁc task, say ti ∈
[m]. The overall dataset can be viewed as a set of triples S ∈([m] × X × R)ℓ,
where ℓ:= Pm
j=1 ℓj is the overall number of examples:
  (t1, x1, y1)
(tℓ, xℓ, yℓ) 
Thus, we can learn a single scalar-valued function deﬁned over an input space
enlarged with the task-labels f : X × [m] →R. The correspondence between
the dataset Sj and S is recovered through an (ℓ, ℓj) index vector kj such that
Let H denote an RKHS of functions deﬁned on the enlarged input space
X × [m] with kernel K, and B denote a d-dimensional bias-space. Solving the
multi-task learning problem by regularized kernel methods amounts to ﬁnding
ˆf ∈H + B, such that
ˆf = arg min
Vi(yi, f(xi, ti)) + λ
where Vi : R × R →R are suitable loss functions, λ ≥0 is the regularization
parameter and PH is the projection operator into H. In this paper, the focus is
on the mixed eﬀect kernels, with the following structure:
K(x1, t1, x2, t2) = αK(x1, x2)
T (t1, t2) eKj(x1, x2).
Kernels K and eKj are deﬁned on X × X and can possibly be all distinct. On
the other hand, Kj
T are “selector” kernels deﬁned on [m] × [m] as
T (t1, t2) =
t1 = t2 = j;
otherwhise
Kernels Kj
T are not strictly positive. Assume that B is spanned by functions
Of course, this is the same of using {ψi}d
1 when α ̸= 0.
weighting the basis functions by α is convenient to recover the separate approach
(α = 0) by continuity. Usually, the dimension d is relatively low. A common
choice might be d = 1 with ψ1 = α, that is B is simply the space of constant
functions, useful to make the learning method translation-invariant.
Under rather general hypotheses for the loss function V , the representer
theorem, see e.g. , gives the following expression for the optimum ˆf:
ˆf(x, t) =α
aiK(xi, x) +
T (ti, t) eKj(xi, x).
The estimate ˆfj is deﬁned to be the function obtained by plugging the corresponding task-label t = j in the previous expression. As a consequence of the
structure of K, the expression of ˆfj decouples into two parts:
ˆfj(x) := ˆf(x, j) = ¯f(x) + ˜fj(x),
aiK(xi, x) +
˜fj(x) = (1 −α)
ai eKj(xi, x).
Function ¯f is independent of j and can be regarded as a sort of average task,
whereas ˜fj is a non-parametric individual shift. The value α is related to the
“shrinking” of the individual estimates toward the average task. When α = 1,
the same function is learned for all the tasks, as if all examples referred to an
unique task (pooled approach). On the other hand, when α = 0, all the tasks
are learned independently (separate approach), as if tasks were not related at
Throughout this paper, the problem is specialized to the case of (weighted)
squared loss functions
Vi(y, f(x, t)) =
(y −f(x, t))2,
where w ∈Rℓ
+ denote a weight vector. For squared loss functions, coeﬃcient
vectors a and b can be obtained by solving the linear system 
W = diag(w),
αK + (1 −α)
I(:, kj) eKj(kj, kj)I(kj, :)
Kij = K(xi, xj),
ij = eKk(xi, xj),
Ψij = ψj(xi).
For α = 0, vector b is not well determined. The linear system can be also solved
via back-ﬁtting on the residual generated by the parametric bias estimate:
ΨT (K + λW)−1 Ψ
ΨT (K + λW)−1 y,
(K + λW) a
Complexity reduction
In many applications of multi-task learning, some or all of the input data xi are
shared between the tasks so that the number of diﬀerent basis functions appearing in the expansion (3) may be considerably less than ℓ. As explained below,
this feature can be exploited to derive eﬃcient incremental online algorithms
for multi-task learning. Introduce the vector of unique inputs
˘xi ̸= ˘xj,
where n < ℓdenote the number of unique inputs. For each task j, a new (n, ℓj)
index vector hj can be deﬁned such that
xij = ˘xhj
aj := a(kj),
yj := y(kj),
wj := w(kj).
The information contained in the index vectors hj is equivalently represented
through a binary matrix P ∈{0, 1}ℓ×n, such that
otherwise.
We have the following decompositions:
˘a := PT a,
K = P ˘KPT ,
˘K := LDLT ,
where L ∈Rn×r, D ∈Rr×r are suitable rank-r factors, D is diagonal, and
LDLT ∈Rn×n.
˘K ∈Rn×n is a kernel matrix associated with the condensed
input set ˘x: ˘Kij = K(˘xi, ˘xj), ˘Ψ ∈Rn×d, ˘Ψij = ψj(˘xi). If K is strictly positive,
we can assume r = n and L can be taken as a lower triangular matrix, see e.g.
Solution (3) can be rewritten in a compact form:
˘aiK(˘xi, x) +
i eKj(˘xhj
Introduce the following “compatibility condition” between kernel K and the
bias space B.
Assumption 1 There exists M ∈Rr×d such that
Assumption 1 is automatically satisﬁed in the no-bias case or when K is strictly
The next result shows that coeﬃcients a and b can be obtained by solving
a system of linear equations involving only “small-sized” matrices so that complexity depends on the number of unique inputs rather then the total number
of examples.
Algorithm 1 Centralized oﬀ-line algorithm.
2: for j = 1 : m do
(1 −α) eKj(kj, kj) + λW(kj, kj)
R ←R + I([ℓ], kj)RjI(kj, [ℓ])
5: end for
6: if α ̸= 0 then
Compute factors L, D, M
˘y ←LT PT Ry
 D−1 + αLT PT RPL
b ←Solution to
 MT (D −H)M
b = MT H˘y
a ←R [y −αPLH (˘y + Mb)]
14: end if
Theorem 1 Let Assumption 1 hold. Coeﬃcient vectors a and b can be evaluated through Algorithm 1. For α = 0, b is undetermined.
Algorithm 1 is an oﬀ-line (centralized) procedure whose computational complexity scales with O(n3m + d3). In the following section, a client-server on-line
version of Algorithm 1 will be derived that preserves this complexity bound.
Typically, this is much better than O
, the worst-case complexity of
directly solving (4).
A client-server online algorithm
Now, we are ready to describe the structure of the client-server algorithm. It
is assumed that each client is associated with a diﬀerent task. The role of the
server is twofold:
1. Collecting triples (xi, yi, wi) (input-output-weight) from the clients and
updating on-line all matrices and coeﬃcients needed to compute estimates
for all the tasks.
2. Publishing suﬃcient information so that any client (task) j can independently compute its estimate ˆfj, possibly without sending data to the
On the other hand, each client j can perform two kind of operations:
1. Sending triples (xi, yi, wi) to the server.
2. Receiving information from the server suﬃcient to compute its own estimate ˆfj.
It is required that each client can neither access other clients data nor reconstruct their individual estimates. We have the following scheme:
• Undisclosed Information: hj, yj, wj, Rj, for j ∈[m].
• Disclosed Information: ˘x, ˘y, H.
Disclosed database
Undisclosed Database
Figure 1: The client-server scheme.
Server side
In order to formulate the algorithm in compact form, it is useful to introduce
the functions “ﬁnd”, “ker” and “bias”. Let
A(x) := {i : xi = x} .
For any p, q ∈N, x ∈X, x ∈Xp, y ∈Xq, let
ﬁnd : X × Xp
A(x) ̸= Ø.
ker(·, ·; K) : Xp × Xq
ker (x, y; K)ij
K (xi, yj)
bias (x)ij
The complete computational scheme is reported in Algorithm 2. The initialization is deﬁned by resorting to empty matrices whose manipulation rules
can be found in . In particular, hj, yj, wj, Rj, D, L, M, ˘x, ˘y, H are all
initialized to empty matrix. In this respect, it is assumed that functions “ker”
and “bias” return an empty matrix as output, when applied to empty matrices.
Algorithm 2 is mainly based on the use of matrix factorizations and matrix
manipulation lemmas in the Appendix. The rest of this subsection is an extensive proof devoted to show that Algorithm 2 correctly updates all the relevant
quantities when a new triple (xi, yi, wi) becomes available from task j. Three
cases are possible:
1. The input xi is already among the inputs of task j.
2. The input xi is not among the inputs of task j, but can be found in the
common database ˘x.
Algorithm 2 Server: receive (xi, yi, wi) from client j and update the database.
1: s = ﬁnd (xi, ˘x)
2: if (s = n + 1) then
 xi, ˘x; K
ψ ←bias(xi),
r ←Solution to LDr = k([n −1]),
β ←kn −rT Dr,
β−1  ψ −rT DM
14: end if
15: p = ﬁnd
 xi, ˘x(hj)
16: if (p = ℓj + 1) then
ℓj ←ℓj + 1
ek ←(1 −α) · ker
xi, ˘x(hj); eKj
Rjek([ℓj −1])
λwi −uT ek
µ ←γuT yj,
u ←Rj(:, p),
wi (yi −yj
p)/(wi −wj
p) + γuT yj,
32: end if
33: Rj ←Rj + γuuT
34: v ←LT (:, hj)u
35: z ←Hv,
36: ˘y ←˘y + µv,
37: H ←H −
(αγ)−1+vT z.
3. The input xi is new.
Case 1: repetition within inputs of task j
The input xi has been found in ˘x(hj), so that it is also present in ˘x. Thus, we
s ̸= n + 1,
p ̸= ℓj + 1,
and the ﬂow of Algorithm 2 can be equivalently reorganized as in Algorithm 3.
Algorithm 3 Server (Case 1).
1: s = ﬁnd (xi, ˘x)
2: p = ﬁnd
 xi, ˘x(hj)
wi (yi −yj
6: u ←Rj(:, p),
7: Rj ←Rj + γuuT
p)/(wi −wj
p) + γuT yj,
9: v ←LT (:, hj)u
10: ˘y ←˘y + µv,
11: z ←Hv,
12: H ←H −
(αγ)−1+vT z.
Let r denote the number of triples of the type (x, yi, wi) belonging to task
j. These data can be replaced by a single triple (x, y, w) without changing the
output of the algorithm. Let
The part of the empirical risk regarding these data can be rewritten as
(yi −fj(x))2
 fj(x)2 −2fj(x)y
=(y −fj(x))2
where A is a constant independent of f. To recursively update w and y when a
repetition is detected, notice that
yr+1 = wr+1
By applying these formulas to the p-th data of task j, lines 3, 4 of Algorithm 3
are obtained. To check that Rj is correctly updated by lines 5, 6, 7 of Algorithm
3 just observe that, taking into account the deﬁnition of Rj and applying Lemma
1, we have:
(1 −α) eKj(hj, hj) + λW(hj, hj) −λepeT
= Rj + γuuT .
Consider now the update of ˘y. Since yj has already been updated, the previous
yj is given by
where the variation ∆yj
p can be expressed as
Recalling the deﬁnition of ˘y in Algorithm 1, and line 7 of Algorithm 3, we have
I(:, hk)Rkyk + I(:, hj)
 Rj + γuuT 
By adding and subtracting ∆yj
pep, using the deﬁnition of µ in line 8,
 Rj + γuuT 
= Rj  yj −∆yj
= Rj  yj −∆yj
˘y ←˘y + µLT (:, hj)u
By deﬁning v as in line 9 of Algorithm 3, the update of line 10 is obtained.
Finally, we show that H is correctly updated. Let
F := αLT PT RPL.
Then, from the deﬁnition of H it follows that
In view of lines 7,9 of Algorithm 3,
F ←F + αγvvT ,
 H−1 + αγvvT −1 .
By Lemma 1, lines 11, 12 are obtained.
Case 2: repetition in ˘x.
Since xi belongs to ˘x but not to ˘x(hj), we have
s ̸= n + 1,
p = ℓj + 1.
The ﬂow of Algorithm 2 can be organized as in Algorithm 4.
Algorithm 4 Server (Case 2)
1: s = ﬁnd (xi, ˘x)
2: p = ﬁnd
 xi, ˘x(hj)
3: ℓj ←ℓj + 1
7: ek ←(1 −α) · ker
xi, ˘x(hj); eKj
Rjek([ℓj −1])
λwi −uT ek
11: v ←LT (:, hj)u
12: µ ←γuT yj,
13: ˘y ←˘y + µv,
14: z ←Hv,
15: H ←H −
(αγ)−1+vT z.
First, vectors hj, yj and wj must be properly enlarged as in lines 3-6. Recalling
the deﬁnition of Rj, we have:
ek([ℓj −1])
ek([ℓj −1])T
ekℓj + λwi
The update for Rj in lines 7-10 is obtained by applying Lemma 2 with A =
Consider now the update of ˘y. Recall that hj and yj have already been
updated. By the deﬁnition of ˘y and in view of line 10 of Algorithm 4, we have
I(:, hk)Rkyk
+ LT I(:, hj)
= ˘y + γ(uT yj)LT I(:, hj)u.
The update in lines 11-13 immediately follows. Finally, the update in lines 14-15
for H is obtained by applying Lemma 2 as in Case 1.
Case 3: xi is a new input.
Algorithm 5 Server (Case 3)
1: n ←n + 1
 xi, ˘x; K
4: r ←Solution to LDr = k([n −1]),
5: β ←kn −rT Dr,
6: ψ ←bias(xi),
β−1  ψ −rT DM
12: Call Algorithm 4.
Since xi is a new input, we have
s = n + 1,
p = ℓj + 1.
The ﬂow of Algorithm 2 can be reorganized as in Algorithm 5. The ﬁnal part
of Algorithm 5 coincides with Algorithm 4. However, the case of new input also
requires updating factors D and L and matrix M. Assume that K is strictly
positive so that D is diagonal and L is lower triangular. If K is not strictly
positive, other kinds of decompositions can be used. In particular, for the linear
kernel K(x1, x2) = ⟨x1, x2⟩over Rr, D and L can be taken, respectively, equal
to the identity and ˘x. Recalling that ˘K = LDLT , we have
k([n −1])T
k([n −1])T
with r and β as in lines 4-5.
Concerning M, recall from Assumption 1 that
The relation must remain true by substituting the updated quantities. Indeed,
after the update in lines 6-9, we have
β−1  ψ −rT DM
rT DM + ψ −rT DM
Finally, it is easy to see that updates for ˘y and H are similar to that of previous
Case 2, once the enlargements in lines 10-11 are made.
Client side
To obtain coeﬃcients a by Algorithm 1, access to undisclosed data hj, yj, Rj is
required. Nevertheless, as shown next, each client can compute its own estimate
ˆfj without having access to the undisclosed data. It is not even necessary to
know the overall number m of tasks, nor their “individual kernels” eKj: all the
required information is contained in the disclosed quantities ˘x, ˘y and H. From
the client point of view, knowledge of ˘x is equivalent to the knowledge of ˘K and
˘Ψ. In turn, also L, D and M can be computed using the factorization (7) and
the deﬁnition of M in Assumption 1. As mentioned in the introduction, two
kind of clients are considered.
• An active client j sends its own data to the server. This kind of client
can request both the disclosed information and its individual coeﬃcients
aj (Algorithm 6).
• A passive client j does not send its data. In this case, the server is not able
to compute aj. This kind of client can only request the disclosed information, and must run a local version of the server to obtain aj (Algorithm
The following Theorem ensures that vector ˘a can be computed by knowing only
disclosed data.
Theorem 2 Given ˘x, ˘y and H, the condensed coeﬃcients vector ˘a can be computed by solving the linear system
DLT ˘a = H (˘y + Mb) −DMb.
Once the disclosed data and vector ˘a have been obtained, each client still
needs the individual coeﬃcients vector aj in order to perform predictions for its
own task. While an active client can simply receive such vector from the server,
a passive client must compute it independently. Interestingly, it turns out that
aj can be computed by knowing only disclosed data together with private data
of task j. Indeed, line 11 of Algorithm 1 decouples with respect to the diﬀerent
yj −αL(hj, :) (z + HMb)
This is the key feature that allows a passive client to perform predictions without
disclosing its private data and exploiting the information contained in all the
other datasets.
Algorithm 6 (Active client j) Receive ˘x, ˘y, H and aj and evaluate ˘a, b
1: for i = 1 : n do
 ˘xi, ˘x([i]); K
r ←Solution to LDr = k([i −1]),
β ←ki −rT Dr,
ψ ←bias(xi),
β−1  ψ −rT DM
9: end for
10: z ←H˘y
11: b ←Solution to
 MT (D −H)M
12: ˘a ←Solution to
 DLT  ˘a = z + (H −D)Mb.
Illustrative example: music recommendation
In this section, the proposed algorithm is applied to a simulated music recommendation problem, in order to predict preferences of several virtual users
with respect to a set of artists. Artist data were obtained from the May 2005
AudioScrobbler Database dump 1 which is the last dump released by Audio-
Scrobbler/LastFM under Creative Commons license.
LastFM is an internet
radio that provides individualized broadcasts based on user preferences. The
database dump includes users playcounts and artists names so that it is possible
to rank artists according to global number of playcounts. After sorting artists
according to decreasing playcounts, 489 top ranking artists were selected. The
1 
Algorithm 7 (Passive client j) Receive ˘x, ˘y and H and evaluate ˘a, b and aj
1: for i = 1 : n do
 ˘xi, ˘x([i]); K
r ←Solution to LDr = k([i −1]),
β ←ki −rT Dr,
ψ ←bias(xi),
β−1  ψ −rT DM
9: end for
10: for i = 1 : ℓj do
Run a local version of Algorithm 2 with (xij, yij, wij).
12: end for
13: z ←H˘y
14: b ←Solution to
 MT (D −H)M
15: ˘a ←Solution to
 DLT  ˘a = z + (H −D)Mb.
16: aj ←Rj 
yj −αL(hj, :) (z + HMb)
input space X is therefore a set of 489 artists, i.e.
X = {Bob Marley, Madonna, Michael Jackson, ...} .
The tasks are associated with user preference functions. More precisely, normalized preferences of user j over the entire set of artists are expressed by functions
sj : X → deﬁned as
1 + e−fj(xi)/2 .
where fj : X →R are the tasks to be learnt.
The simulated music recommendation system relies on music type classiﬁcation expressed by means of tags (rock, pop, ...). In particular, the 19 main
tags of LastFM 2 were considered. The i-th artist is associated with a vector
zi ∈ 19 of 19 tags, whose values were obtained by querying the LastFM
site on September 22, 2008. In Figure 2, the list of the tags considered in this
experiment, together with an example of artist’s tagging are reported. Vectors
zi have been normalized to lie on the unit hyper-sphere, i.e. ∥zi∥2 = 1. The
input space data (artists together with their normalized tags) are available for
download 3.
Tag information was used to build a mixed-eﬀect kernel over X. More precisely, K is a Gaussian RBF kernel and eKk = eK are linear kernels:
K(xi(zi), xj(zj)) = e1−∥zi−zj∥/2 = ezT
eK(xi(zi), xj(zj)) = zT
2 
3 
The above kernels were employed to generate synthetic users.
“average user” was generated by drawing a function f : X →R from a Gaussian
process with zero mean and auto-covariance K. Then, m = 3000 virtual user’s
preferences were generated as
fj = 0.25f + 0.75 efj,
where efj are drawn from a Gaussian process with zero mean and auto-covariance
eK. For the j-th virtual user, ℓj = 5 artists xij were uniformly randomly sampled
from the input space X, and corresponding noisy outputs yij generated as
yij = fj(xij) + ϵij,
where ϵij are i.i.d.
Gaussian errors with zero mean and standard deviation
σ = 0.01. The learned preference function ˆsj is
fj(xi)/2 ,
where ˆfj is estimated using the algorithm described in the paper. Performances
are evaluated by both the average root mean squared error
(sj(xi) −ˆsj(xi))2,
and the average number of hits over the top 20 ranked artists, deﬁned as
TOP20HITS = 1
hits20j := |top20(sj) ∩top20(ˆsj)| ,
where top20 : H →X20 returns the sorted vector of 20 inputs with highest
scores, measured by a function s : X → , s ∈H.
Learning was performed for 15 values of the shrinking parameter α linearly
spaced in and 15 values of the regularization parameter λ logarithmically spaced in the interval
10−7, 100
, see Figure 3. The multi-task approach,
i.e. 0 < α < 1 outperforms both the separate (α = 0) and pooled (α = 1)
approaches. Interestingly, performances remain fairly stable for a range of values of α.
Figure 4 shows the distribution of hits20j over the 3000 users in
correspondence with values of α∗and λ∗achieving the optimal RMSE.
Although α∗= 0.0714 and λ∗= 3.1623 · 10−4 were selected so as to minimize
the RMSE, remarkably good performances are obtained also with respect to
the TOP20HITS score which is 8.3583 (meaning that on the average 8.3583
artists among the top-20 are correctly retrieved). Finally, true and estimated
top-20 hits are reported for the average user (Figure 5) and two representative
users (Figure 6). Artists of the true top-20 that are correctly retrieved in the
estimated top-20 are reported in bold-face.
Concerning the computational burden, it is worth observing that without
exploiting the presence of repeated inputs and the mixed-eﬀect structure of the
kernel, the complexity of a naive approach would be of the order of the cube of
electronic
indie rock
Figure 2: Example of artist tagging
the overall number of examples, that is (5 · 3000)3. Conversely, the complexity
of the approach proposed in the paper scales with n3m, where n is the number
of unique inputs and m the number of tasks (in our example, n is bounded by
the cardinality of the input set |X| = 489, and m = 3000).
Conclusions
Recent studies have highlighted the potentialities of kernel methods applied to
multi-task learning, but their eﬀective implementation involve the solution of
architectural and complexity issues. In this paper, emphasis is posed on the
architecture with reference to learning from distributed datasets. For a general
class of kernels with a “mixed-eﬀect” structure it is shown that the optimal
solution can be given a collaborative client-server architecture that enjoys favorable computational and conﬁdentiality properties. By interacting with the
server, each client can solve its own estimation task while taking advantage of
all the data from the other clients without having any direct access to them.
Client’s privacy is preserved, since both active and passive clients are allowed
by the architecture.
The former are those that agree to send their data to
the server while the latter only exploit information from the server without
disclosing their private data. The proposed architecture has several potential
applications ranging from biomedical data analysis (where privacy issues are
Figure 3: Average TOP20HITS and RMSE against α and λ.
Optimal hits
Number of users
Figure 4: Distribution of hits20j in correspondence with α∗and λ∗achieving
optimal RMSE.
20)Lynyrd Skynyrd
18)Jimi Hendrix
17)Led Zeppelin
16)The Who
15)Leonard Cohen
14)The Velvet Underground
13)The Doors
12)Grateful Dead
11)Bob Dylan
10)Creedence Clearwater Revival
9)Ryan Adams
8)The Rolling Stones
7)Fleetwood Mac
6)The Kinks
5)Neil Young
4)Simon & Garfunkel
3)John Lennon
2)The Beatles
1)Elvis Presley
(Average user) True Top20
20)The Velvet Underground
19)Ryan Adams
18)Led Zeppelin
17)Neil Young
16)Leonard Cohen
15)The Beatles
14)Lynyrd Skynyrd
13)Jimi Hendrix
12)The Kinks
10)Fleetwood Mac
9)John Lennon
8)The Rolling Stones
7)Simon & Garfunkel
6)Grateful Dead
4)Bob Dylan
3)Creedence Clearwater Revival
2)The Doors
1)Elvis Presley
(Average user) Estimated Top20
Figure 5: True and estimated Top20 for the “average user”.
20)Elton John
19)Kylie Minogue
18)Sarah McLachlan
17)Ashlee Simpson
16)Kelly Clarkson
15)Robbie Williams
14)Black Eyed Peas
13)Madonna
12)Air Supply
11)Lindsay Lohan
8)Gwen Stefani
7)Hilary Duff
4)Britney Spears
3)Herman van Veen
2)Michael Jackson
1)The Beach Boys
True Top20
20)Jurassic 5
19)Roxette
16)Die Fantastischen Vier
15)Sarah McLachlan
14)Ashlee Simpson
13)Elton John
12)Kelly Clarkson
11)Robbie Williams
10)Gwen Stefani
9)The Beach Boys
7)Lindsay Lohan
5)Air Supply
4)Hilary Duff
3)Britney Spears
2)Michael Jackson
1)Herman van Veen
Estimated Top20
20)The Smiths
19)The White Stripes
18)Lynyrd Skynyrd
17)Led Zeppelin
16)Helloween
14)Judas Priest
13)Aerosmith
12)Talking Heads
11)Jimi Hendrix
10)Ozzy Osbourne
9)Guns N’ Roses
8)The Cure
7)Van Halen
5)Leonard Cohen
4)W.A.S.P.
3)Joy Division
2)Dire Straits
1)Eric Clapton
True Top20
20)Black Sabbath
19)Deep Purple
18)The Rolling Stones
17)The Doors
16)Creedence Clearwater Revival
15)Grateful Dead
14)Joy Division
12)The Smiths
10)Helloween
9)Aerosmith
8)Judas Priest
7)Guns N’ Roses
6)Dire Straits
5)The Cure
4)Ozzy Osbourne
3)Van Halen
1)W.A.S.P.
Estimated Top20
Figure 6: True and estimated Top20 for two representative users.
crucial) to web data mining. An illustrative example is given by the simulated
music recommendation system discussed in the paper.
Acknowledgments
This work has been partially supported by MIUR Project Artiﬁcial pancreas:
physiological models, control algorithms and clinical test and PRIN Project
Metodi e algoritmi innovativi per la stima Bayesiana e l’identiﬁcazione e il controllo adattativo e distribuito. The authors would like to thank Pietro De Nicolao
for preprocessing data used in the numerical example.
Recall the following two lemmas on matrix inversions, see e.g. .
Lemma 1 (Sherman-Morrison-Woodbury) Let A ∈Rn×n, B ∈Rm×m
be two nonsingular matrix, U ∈Rn×m, V ∈Rm×n such that (A + UBV) is
nonsingular. Then, matrix
 B−1 + VA−1U
is nonsingular, and
(A + UBV)−1 = A−1 −A−1UE−1VA−1.
Lemma 2 (Schur) Suppose that matrix
∈R(n+m)×(n+m)
is nonsingular, with B ∈Rn×n. Then,
 CT B−1C −D
is nonsingular and
 B−1 −B−1CECT B−1
Proof of Theorem 1 Let Rj, R be deﬁned as in line 1-5 of Algorithm 1, and
observe that
R−1 = (1 −α)
I(:, kj) eKj(kj, kj)I(kj, :) + λW
Consider the back-ﬁtting formulation (5), (6) of the linear system (4).
Lemma 1, we have:
(K + λW)−1 =
 αK + R−1−1
 αPLDLT PT + R−1−1
= R −αRPLHLT PT R.
Let F := αLT PT RPL, and observe that
DFH = HFD = D −H.
In the following, we exploit the following relationship:
αLT PT (K + λW)−1 PL
=αLT PT  R −αRPLHLT PT R
Consider the case α ̸= 0. Then, in view of the previous relationship, recalling
that Ψ = P ˘Ψ = PLDM, we have
αΨT (K + λW)−1 Ψ
=αMT DT LT PT (K + λW)−1 PLDM
=MT D (F −FHF) DM
=MT (D −DFH) FDM
=MT (D −H)M,
ΨT (K + λW)−1 y
=MT DLT PT  R −αRPLHLT PT R
=MT (D −DFH) LT PT Ry
Then, line 10 of Algorithm 1 follows from (5). Observe that
(K + λW) a = αKa + R−1a.
Then, from (6) we have
 DLT PT a + DMb
DLT PT a + DMb
=DLT PT (K + λW)−1 (y −αPLDMb) + DMb
= (D −DFH)
 LT PT Ry −FDMb
=HLT PT Ry −HFDMb + DMb
=H (˘y + Mb) .
Hence, we obtain line 11 of Algorithm 1. Finally, for α = 0, we have H = D so
that the thesis follows.
Proof of Theorem 2 Let F := λLT PT TPL. Recalling the expression of a, ˘y,
H in Algorithm 1, we have
DLT ˘a = DLT PT a
= DLT PT R [y −λPLH (˘y + Mb)]
= D [˘y −FH (˘y + Mb)]
= (D −DFH) ˘y −DFHMb
= H˘y −(D −H) Mb
= H (˘y + Mb) −DMb.