Recurrent Convolutional Neural Networks for Text Classiﬁcation
Siwei Lai, Liheng Xu, Kang Liu, Jun Zhao
National Laboratory of Pattern Recognition (NLPR)
Institute of Automation, Chinese Academy of Sciences, China
{swlai, lhxu, kliu, jzhao}@nlpr.ia.ac.cn
Text classiﬁcation is a foundational task in many NLP
applications. Traditional text classiﬁers often rely on
many human-designed features, such as dictionaries,
knowledge bases and special tree kernels. In contrast
to traditional methods, we introduce a recurrent convolutional neural network for text classiﬁcation without human-designed features. In our model, we apply
a recurrent structure to capture contextual information
as far as possible when learning word representations,
which may introduce considerably less noise compared
to traditional window-based neural networks. We also
employ a max-pooling layer that automatically judges
which words play key roles in text classiﬁcation to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms
the state-of-the-art methods on several datasets, particularly on document-level datasets.
Introduction
Text classiﬁcation is an essential component in many applications, such as web searching, information ﬁltering, and
sentiment analysis . Therefore, it
has attracted considerable attention from many researchers.
A key problem in text classiﬁcation is feature representation, which is commonly based on the bag-of-words
(BoW) model, where unigrams, bigrams, n-grams or some
exquisitely designed patterns are typically extracted as features. Furthermore, several feature selection methods, such
as frequency, MI , pLSA , LDA , are applied
to select more discriminative features. Nevertheless, traditional feature representation methods often ignore the contextual information or word order in texts and remain unsatisfactory for capturing the semantics of the words. For
example, in the sentence “A sunset stroll along the South
Bank affords an array of stunning vantage points.”, when
we analyze the word “Bank” (unigram), we may not know
whether it means a ﬁnancial institution or the land beside
a river. In addition, the phrase “South Bank” (bigram), particularly considering the two uppercase letters, may mislead
Copyright c⃝2015, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
people who are not particularly knowledgeable about London to take it as a ﬁnancial institution. After we obtain the
greater context “stroll along the South Bank” (5-gram), we
can easily distinguish the meaning. Although high-order ngrams and more complex features ) are designed to capture more contextual
information and word orders, they still have the data sparsity
problem, which heavily affects the classiﬁcation accuracy.
Recently, the rapid development of pre-trained word embedding and deep neural networks has brought new inspiration to various NLP tasks. Word embedding is a distributed
representation of words and greatly alleviates the data sparsity problem . Mikolov, Yih, and Zweig
 shows that pre-trained word embeddings can capture meaningful syntactic and semantic regularities. With the
help of word embedding, some composition-based methods
are proposed to capture the semantic representation of texts.
Socher et al. proposed the Recursive Neural Network (RecursiveNN) that has been proven
to be efﬁcient in terms of constructing sentence representations. However, the RecursiveNN captures the semantics of
a sentence via a tree structure. Its performance heavily depends on the performance of the textual tree construction.
Moreover, constructing such a textual tree exhibits a time
complexity of at least O(n2), where n is the length of the
text. This would be too time-consuming when the model
meets a long sentence or a document. Furthermore, the relationship between two sentences can hardly be represented
by a tree structure. Therefore, RecursiveNN is unsuitable for
modeling long sentences or documents.
Another model, which only exhibits a time complexity
O(n), is the Recurrent Neural Network (RecurrentNN).
This model analyzes a text word by word and stores the semantics of all the previous text in a ﬁxed-sized hidden layer
 . The advantage of RecurrentNN is the ability
to better capture the contextual information. This could be
beneﬁcial to capture semantics of long texts. However, the
RecurrentNN is a biased model, where later words are more
dominant than earlier words. Thus, it could reduce the effectiveness when it is used to capture the semantics of a whole
document, because key components could appear anywhere
in a document rather than at the end.
To tackle the bias problem, the Convolutional Neural
Network (CNN), an unbiased model is introduced to NLP
Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence
tasks, which can fairly determine discriminative phrases in
a text with a max-pooling layer. Thus, the CNN may better
capture the semantic of texts compared to recursive or recurrent neural networks. The time complexity of the CNN
is also O(n). However, previous studies on CNNs tends to
use simple convolutional kernels such as a ﬁxed window
 .
When using such kernels, it is difﬁcult to determine the
window size: small window sizes may result in the loss of
some critical information, whereas large windows result in
an enormous parameter space (which could be difﬁcult to
train). Therefore, it raises a question: can we learn more contextual information than conventional window-based neural
networks and represent the semantic of texts more precisely
for text classiﬁcation.
To address the limitation of the above models, we propose a Recurrent Convolutional Neural Network (RCNN)
and apply it to the task of text classiﬁcation. First, we apply a bi-directional recurrent structure, which may introduce
considerably less noise compared to a traditional windowbased neural network, to capture the contextual information
to the greatest extent possible when learning word representations. Moreover, the model can reserve a larger range
of the word ordering when learning representations of texts.
Second, we employ a max-pooling layer that automatically
judges which features play key roles in text classiﬁcation, to
capture the key component in the texts. By combining the recurrent structure and max-pooling layer, our model utilizes
the advantage of both recurrent neural models and convolutional neural models. Furthermore, our model exhibits a
time complexity of O(n), which is linearly correlated with
the length of the text length.
We compare our model with previous state-of-the-art approaches using four different types of tasks in English and
Chinese. The classiﬁcation taxonomy contains topic classiﬁcation, sentiment classiﬁcation and writing style classi-
ﬁcation. The experiments demonstrate that our model outperforms previous state-of-the-art approaches in three of the
four commonly used datasets.
Related Work
Text Classiﬁcation
Traditional text classiﬁcation works mainly focus on three
topics: feature engineering, feature selection and using different types of machine learning algorithms. For feature engineering, the most widely used feature is the bag-of-words
feature. In addition, some more complex features have been
designed, such as part-of-speech tags, noun phrases and tree kernels . Feature
selection aims at deleting noisy features and improving the
classiﬁcation performance. The most common feature selection method is removing the stop words (e.g., “the”). Advanced approaches use information gain, mutual information , or L1 regularization to select useful features. Machine learning algorithms
often use classiﬁers such as logistic regression (LR), naive
Bayes (NB), and support vector machine (SVM). However,
these methods have the data sparsity problem.
Deep neural networks
Recently, deep neural networks and representation learning have led to new ideas for solving the data
sparsity problem, and many neural models for learning word
representations have been proposed . The neural representation of a word is called word embedding and is a realvalued vector. The word embedding enables us to measure
word relatedness by simply using the distance between two
embedding vectors.
With the pre-trained word embeddings, neural networks
demonstrate their great performance in many NLP tasks.
Socher et al. use semi-supervised recursive autoencoders to predict the sentiment of a sentence. Socher et al.
 proposed a method for paraphrase detection also
with recurrent neural network. Socher et al. introduced recursive neural tensor network to analyse sentiment
of phrases and sentences. Mikolov uses recurrent
neural network to build language models. Kalchbrenner and
Blunsom proposed a novel recurrent network for dialogue act classiﬁcation. Collobert et al. introduce
convolutional neural network for semantic role labeling.
We propose a deep neural model to capture the semantics
of the text. Figure 1 shows the network structure of our
model. The input of the network is a document D, which
is a sequence of words w1, w2 . . . wn. The output of the network contains class elements. We use p(k|D, θ) to denote
the probability of the document being class k, where θ is the
parameters in the network.
Word Representation Learning
We combine a word and its context to present a word. The
contexts help us to obtain a more precise word meaning.
In our model, we use a recurrent structure, which is a bidirectional recurrent neural network, to capture the contexts.
We deﬁne cl(wi) as the left context of word wi and
cr(wi) as the right context of word wi. Both cl(wi) and
cr(wi) are dense vectors with |c| real value elements. The
left-side context cl(wi) of word wi is calculated using Equation (1), where e(wi−1) is the word embedding of word
wi−1, which is a dense vector with |e| real value elements.
cl(wi−1) is the left-side context of the previous word wi−1.
The left-side context for the ﬁrst word in any document uses
the same shared parameters cl(w1). W (l) is a matrix that
transforms the hidden layer (context) into the next hidden
layer. W (sl) is a matrix that is used to combine the semantic
of the current word with the next word’s left context. f is a
non-linear activation function. The right-side context cr(wi)
is calculated in a similar manner, as shown in Equation (2).
The right-side contexts of the last word in a document share
the parameters cr(wn).
cl(wi) = f(W (l)cl(wi−1) + W (sl)e(wi−1))
cr(wi) = f(W (r)cr(wi+1) + W (sr)e(wi+1))
recurrent structure (convolutional layer)
max-pooling layer output layer
left context
right context
word embedding
Figure 1: The structure of the recurrent convolutional neural network. This ﬁgure is a partial example of the sentence “A
sunset stroll along the South Bank affords an array of stunning vantage points”, and the subscript denotes the position of the
corresponding word in the original sentence.
As shown in Equations (1) and (2), the context vector
captures the semantics of all left- and right-side contexts.
For example, in Figure 1, cl(w7) encodes the semantics of
the left-side context “stroll along the South” along with all
previous texts in the sentence, and cr(w7) encodes the semantics of the right-side context “affords an ...”. Then, we
deﬁne the representation of word wi in Equation (3), which
is the concatenation of the left-side context vector cl(wi),
the word embedding e(wi) and the right-side context vector
cr(wi). In this manner, using this contextual information,
our model may be better able to disambiguate the meaning
of the word wi compared to conventional neural models that
only use a ﬁxed window (i.e., they only use partial information about texts).
xi = [cl(wi); e(wi); cr(wi)]
The recurrent structure can obtain all cl in a forward scan
of the text and cr in a backward scan of the text. The time
complexity is O(n). After we obtain the representation xi of
the word wi, we apply a linear transformation together with
the tanh activation function to xi and send the result to the
next layer.
W (2)xi + b(2)
is a latent semantic vector, in which each semantic
factor will be analyzed to determine the most useful factor
for representing the text.
Text Representation Learning
The convolutional neural network in our model is designed
to represent the text. From the perspective of convolutional
neural networks, the recurrent structure we previously mentioned is the convolutional layer.
When all of the representations of words are calculated,
we apply a max-pooling layer.
The max function is an element-wise function. The k-th element of y(3) is the maximum in the k-th elements of y(2)
The pooling layer converts texts with various lengths into
a ﬁxed-length vector. With the pooling layer, we can capture
the information throughout the entire text. There are other
types of pooling layers, such as average pooling layers . We do not use average pooling here because only a few words and their combination are useful for
capturing the meaning of the document. The max-pooling
layer attempts to ﬁnd the most important latent semantic factors in the document. The pooling layer utilizes the output of
the recurrent structure as the input. The time complexity of
the pooling layer is O(n). The overall model is a cascade of
the recurrent structure and a max-pooling layer, therefore,
the time complexity of our model is still O(n).
The last part of our model is an output layer. Similar to
traditional neural networks, it is deﬁned as
y(4) = W (4)y(3) + b(4)
Finally, the softmax function is applied to y(4). It can convert the output numbers into probabilities.
Training Network parameters
We deﬁne all of the parameters to be trained as θ.
θ = {E, b(2), b(4), cl(w1), cr(wn), W (2),
W (4), W (l), W (r), W (sl), W (sr)}
Speciﬁcally, the parameters are word embeddings E ∈
R|e|×|V |, the bias vectors b(2) ∈RH, b(4) ∈RO, the initial
contexts cl(w1), cr(wn) ∈R|c| and the transformation matrixes W (2) ∈RH×(|e|+2|c|), W (4) ∈RO×H, W (l), W (r) ∈
R|c|×|c|, W (sl), W (sr) ∈R|e|×|c| , where |V | is the number
of words in the vocabulary, H is the hidden layer size, and
O is the number of document types.
The training target of the network is used to maximize the
log-likelihood with respect to θ:
log p(classD|D, θ)
where D is the training document set and classD is the correct class of document D.
We use stochastic gradient descent to optimize the training target. In each step, we randomly select an
example (D, classD) and make a gradient step.
θ ←θ + α∂log p(classD|D, θ)
where α is the learning rate.
We use one trick that is widely used when training neural networks with stochastic gradient descent in the training phase. We initialize all of the parameters in the neural
network from a uniform distribution. The magnitude of the
maximum or minimum equals the square root of the “fanin” . The number is the network node
of the previous layer in our model. The learning rate for that
layer is divided by “fan-in”.
Pre-training Word Embedding
Word embedding is a
distributed representation of a word. Distributed representation is suitable for the input of neural networks. Traditional
representations, such as one-hot representation, will lead to
the curse of dimensionality . Recent research 
shows that neural networks can converge to a better local
minima with a suitable unsupervised pre-training procedure.
In this work, we use the Skip-gram model to pre-train the
word embedding. this model is the state-of-the-art in many
NLP tasks . The Skipgram model trains the embeddings of words w1, w2 . . . wT
by maximizing the average log probability
−c≤j≤c,j̸=0
log p(wt+j|wt)
p(wb|wa) =
 e′(wb)T e(wa)
e′(wk)T e(wa)
where |V | is the vocabulary of the unlabeled text. e′(wi)
is another embedding for wi. We use the embedding e because some speed-up approaches ) will be used here, and e′ is not
calculated in practice.
Experiments
To demonstrate the effectiveness of the proposed method,
we perform the experiments using the following four
datasets: 20Newsgroups, Fudan Set, ACL Anthology Network, and Sentiment Treebank. Table 1 provides detailed
information about each dataset.
Train/Dev/Test
7520/836/5563
8823/981/9832
146257/28565/28157
8544/1101/2210
Table 1: A summary of the datasets, including the number of
classes, the number of train/dev/test set entries, the average
text length and the language of the dataset.
20Newsgroups1
This dataset contains messages from
twenty newsgroups. We use the bydate version and select
four major categories (comp, politics, rec, and religion) followed by Hingmire et al. .
Fudan set2
The Fudan University document classiﬁcation
set is a Chinese document classiﬁcation set that consists of
20 classes, including art, education, and energy.
ACL Anthology Network3
This dataset contains scientiﬁc documents published by the ACL and by related organizations. It is annotated by Post and Bergsma with the
ﬁve most common native languages of the authors: English,
Japanese, German, Chinese, and French.
Stanford Sentiment Treebank4
The dataset contains
movie reviews parsed and labeled by Socher et al. .
The labels are Very Negative, Negative, Neutral, Positive,
and Very Positive.
Experiment Settings
We preprocess the dataset as follows. For English documents, we use the Stanford Tokenizer5 to obtain the tokens.
For Chinese documents, we use ICTCLAS6 to segment the
words. We do not remove any stop words or symbols in the
texts. All four datasets have been previously separated into
training and testing sets. The ACL and SST datasets have
a pre-deﬁned training, development and testing separation.
For the other two datasets, we split 10% of the training set
into a development set and keep the remaining 90% as the
real training set. The evaluation metric of the 20Newsgroups
is the Macro-F1 measure followed by the state-of-the-art
work. The other three datasets use accuracy as the metric.
The hyper-parameter settings of the neural networks may
depend on the dataset being used. We choose one set of commonly used hyper-parameters following previous studies
 .
Moreover, we set the learning rate of the stochastic gradient descent α as 0.01, the hidden layer size as H = 100,
the vector size of the word embedding as |e| = 50 and the
size of the context vector as |c| = 50. We train word em-
1qwone.com/˜jason/20Newsgroups/
2www.datatang.com/data/44139 and 43543
3old-site.clsp.jhu.edu/˜sbergsma/Stylo/
4nlp.stanford.edu/sentiment/
5nlp.stanford.edu/software/tokenizer.shtml
6ictclas.nlpir.org
Bigram + LR
Bigram + SVM
Average Embedding
ClassifyLDA-EM 
Labeled-LDA 
CFG 
C&J 
RecursiveNN 
RNTN 
Paragraph-Vector 
Table 2: Test set results for the datasets. The top, middle, and bottom parts are the baselines, the state-of-the-art results and the
results of our model, respectively. The state-of-the-art results are reported by the corresponding essays.
beddings using the default parameter in word2vec7 with the
Skip-gram algorithm. We use Wikipedia dumps in both English and Chinese to train the word embedding.
Comparison of Methods
We compare our method with widely used text classiﬁcation
methods and the state-of-the-art approaches for each dataset.
Bag of Words/Bigrams + LR/SVM
Wang and Manning
 proposed several strong baselines for text classiﬁcation. These baselines mainly use machine learning algorithms with unigram and bigrams as features. We use logistic regression (LR) and SVM8, respectively. The weight of
each feature is the term frequency.
Average Embedding + LR
This baseline uses the
weighted average of the word embeddings and subsequently
applies a softmax layer. The weight for each word is its tfidf value. Huang et al. also used this strategy as the
global context in their task. Klementiev, Titov, and Bhattarai
 used this in crosslingual document classiﬁcation.
LDA-based approaches achieve good performance
in terms of capturing the semantics of texts in several classiﬁcation tasks. We select two methods as the methods for
comparison: ClassifyLDA-EM and
Labeled-LDA .
Tree Kernels
Post and Bergsma used various tree
kernels as features. It is the state-of-the-art work in the ACL
native language classiﬁcation task. We list two major methods for comparison: the context-free grammar (CFG) produced by the Berkeley parser and the reranking feature set of Charniak and Johnson (C&J).
7code.google.com/p/word2vec
8www.csie.ntu.edu.tw/˜cjlin/liblinear
RecursiveNN
We select two recursive-based methods for
comparison with the proposed approach: the Recursive Neural Network (RecursiveNN) and its
improved version, the Recursive Neural Tensor Networks
(RNTNs) .
We also select a convolutional neural network for comparison. Its convolution kernel
simply concatenates the word embeddings in a pre-deﬁned
window. Formally,
xi = [e(wi−⌊win/2⌋); . . . ; e(wi); . . . ; e(wi+⌊win/2⌋)].
Results and Discussion
The experimental results are shown in Table 2.
• When we compare neural network approaches (RecursiveNN, CNN, and RCNN) to the widely used traditional
methods (e.g., BoW+LR), the experimental results show
that the neural network approaches outperform the traditional methods for all four datasets. It proves that neural network based approach can effective compose the semantic representation of texts. Neural networks can capture more contextual information of features compared
with traditional methods based on BoW model, and may
suffer from the data sparsity problem less.
• When comparing CNNs and RCNNs to RecursiveNNs using the SST dataset, we can see that the convolution-based
approaches achieve better results. This illustrates that the
convolution-based framework is more suitable for constructing the semantic representation of texts compared
with previous neural networks. We believe the main reason is that CNN can select more discriminative features
through the max-pooling layer and capture contextual information through convolutional layer. By contrast, RecursiveNN can only capture contextual information using semantic composition under the constructed textual
tree, which heavily depends on the performance of tree
construction. Moreover, compared to the recursive-based
approaches, which require O(n2) time to construct the
representations of sentences, our model exhibits a lower
time complexity of O(n). In practice, the training time of
the RNTN as reported in Socher et al. is approximately 3-5 hours. Training the RCNN on the SST dataset
only takes several minutes using a single-thread machine.
• In all datasets expect ACL and SST dataset, RCNN outperforms the state-of-the-art methods. In the ACL dataset,
RCNN has a competitive result with the best baseline. We
reduce the error rate by 33% for the 20News dataset and
by 19% for the Fudan set with the best baselines. The results prove the effectiveness of the proposed method.
• We compare our RCNN to well-designed feature sets in
the ACL dataset. The experimental results show that the
RCNN outperforms the CFG feature set and obtains a result that is competitive with the C&J feature set. We believe that the RCNN can capture long-distance patterns,
which are also introduced by tree kernels. Despite the
competitive results, the RCNN does not require handcrafted feature sets, which means that it might be useful
in low-resource languages.
• We also compare the RCNN to the CNN and ﬁnd that
the RCNN outperforms the CNN in all cases. We believe
that the reason is the recurrent structure in the RCNN captures contextual information better than window-based
structure in CNNs. This results demonstrate the effectiveness of the proposed method. To illustrate this point more
clearly, we propose a detailed analysis in the next subsection.
Contextual Information
In this subsection, we investigate the ability of the recurrent structure in our model for
capturing contextual information in further detail. The difference between CNNs and RCNNs is that they use different
structure for capturing contextual information. CNNs use a
ﬁxed window of words as contextual information, whereas
RCNNs use the recurrent structure to capture a wide range
of contextual information. The performance of a CNN is in-
ﬂuenced by the window size. A small window may result
in a loss of some long-distance patterns, whereas large windows will lead to data sparsity. Furthermore, a large number
of parameters are more difﬁcult to train.
We consider all odd window sizes from 1 to 19 to train and
test the CNN model. For example, when the window size is
one, the CNN only uses the word embedding [e(wi)] to represent the word. When the window size is three, the CNN
uses [e(wi−1); e(wi); e(wi+1)] to represent word wi. The
test scores for these various window sizes are shown in Figure 2. Because of space limitations, we only show the classi-
ﬁcation results for the 20Newsgroups dataset. In this ﬁgure,
we can observe that the RCNN outperforms the CNN for all
window sizes. It illustrate that the RCNN could capture contextual information with a recurrent structure that does not
rely on the window size. The RCNN outperforms windowbased CNNs because the recurrent structure can preserve
longer contextual information and introduces less noise.
Learned Keywords
To investigate how our model constructs the representations of texts, we list the most impor-
Context window size
ClassifyLDA-EM
Figure 2: Macro-F1 curve for how context window size in-
ﬂuences the performance of the 20Newsgroups classiﬁcation
well worth the; a wonderful movie; even stinging at;
and invigorating ﬁlm; and ingenious entertainment;
and enjoy .; ’s sweetest movie
A dreadful live-action; Extremely boring .; is n’t a;
’s painful .; Extremely dumb .; an awfully derivative;
’s weaker than; incredibly dull .; very bad sign;
an amazing performance; most visually stunning;
wonderful all-ages triumph; a wonderful movie
for worst movie; A lousy movie; a complete failure;
most painfully marginal; very bad sign
Table 3: Comparison of positive and negative features extracted by the RCNN and the RNTN
tant words in the test set in Table 3. The most important
words are the information most frequently selected in the
max-pooling layer. Because the word representation in our
model is a word together with its context, the context may
contain the entire text. We only present the center word
and its neighboring trigram. For comparison, we also list
the most positive/negative trigram phrases extracted by the
RNTN .
In contrast to the most positive and most negative phrases
in RNTN, our model does not rely on a syntactic parser,
therefore, the presented n-grams are not typically “phrases”.
The results demonstrate that the most important words for
positive sentiment are words such as “worth”, “sweetest”,
and “wonderful”, and those for negative sentiment are words
such as “awfully”, “bad”, and “boring”.
Conclusion
We introduced recurrent convolutional neural networks to
text classiﬁcation. Our model captures contextual information with the recurrent structure and constructs the representation of text using a convolutional neural network. The experiment demonstrates that our model outperforms CNN and
RecursiveNN using four different text classiﬁcation datasets.
Acknowledgments
The authors would like to thank the anonymous reviewers for the constructive comments. This work was sponsored by the National Basic Research Program of China (No.
2014CB340503) and the National Natural Science Foundation of China (No. 61202329, 61272332).