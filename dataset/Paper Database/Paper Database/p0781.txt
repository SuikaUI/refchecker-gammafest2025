Resampling-Based Ensemble Methods
for Online Class Imbalance Learning
Shuo Wang, Member, IEEE, Leandro L. Minku, Member, IEEE, and Xin Yao, Fellow, IEEE
Abstract—Online class imbalance learning is a new learning problem that combines the challenges of both online learning and
class imbalance learning. It deals with data streams having very skewed class distributions. This type of problems commonly exists
in real-world applications, such as fault diagnosis of real-time control monitoring systems and intrusion detection in computer networks.
In our earlier work, we deﬁned class imbalance online, and proposed two learning algorithms OOB and UOB that build an ensemble
model overcoming class imbalance in real time through resampling and time-decayed metrics. In this paper, we further improve the
resampling strategy inside OOB and UOB, and look into their performance in both static and dynamic data streams. We give the ﬁrst
comprehensive analysis of class imbalance in data streams, in terms of data distributions, imbalance rates and changes in class
imbalance status. We ﬁnd that UOB is better at recognizing minority-class examples in static data streams, and OOB is more robust
against dynamic changes in class imbalance status. The data distribution is a major factor affecting their performance. Based on
the insight gained, we then propose two new ensemble methods that maintain both OOB and UOB with adaptive weights
for ﬁnal predictions, called WEOB1 and WEOB2. They are shown to possess the strength of OOB and UOB with good accuracy
and robustness.
Index Terms—Class imbalance, resampling, online learning, ensemble learning, Bagging
INTRODUCTION
NLINE class imbalance learning is an emerging topic that
is attracting growing attention. It aims to tackle the combined issue of online learning and class imbalance learning . Different from incremental learning that processes
data in batches, online learning here means learning from
data examples “one-by-one” without storing and reprocessing observed examples . Class imbalance learning handles
a type of classiﬁcation problems where some classes of data
are heavily underrepresented compared to other classes.
With both problems, online class imbalance learning deals
with data streams where data arrive continuously and the
class distribution is imbalanced. Although online learning
and class imbalance learning have been well studied in the literature individually, the combined problem has not been discussed much. It is commonly seen in real world applications,
such as intrusion detection in computer networks and fault
diagnosis of control monitoring systems .
When both issues of online learning and class imbalance
exist, new challenges and interesting research questions
arise, with regards to the prediction accuracy on the minority class and adaptivity to dynamic environments. The difﬁculty of learning from imbalanced data is caused by the
relatively or absolutely underrepresented class that cannot
draw equal attention to the learning algorithm compared to
the majority class. It often leads to very speciﬁc classiﬁcation
rules or missing rules for the minority class without much
generalization ability for future prediction . This problem
is exaggerated when data arrive in an online fashion. First,
we cannot get a whole picture of data to evaluate the
imbalance status. An online deﬁnition of class imbalance is
necessary to describe the current imbalance degree. Second,
the imbalance status can change over time. Therefore, the
online model needs to be kept updated for good performance on the current minority class without damaging the
performance on the current majority class.
As one of the earliest studies that focus on online class
imbalance learning, our recent work proposed an online
deﬁnition of class imbalance through two indicators (i.e.
time-decayed class size and recall), and a class imbalance
detection method to report the real-time class imbalance status in the data stream . Based on the status information,
we proposed two online ensemble learning methods—
Oversamplingbased Online Bagging (OOB) and Undersampling-based Online Bagging (UOB) , which can adjust the
learning bias from the majority to the minority class effectively and adaptively through resampling.
However, because the resampling rate in OOB and UOB
does not consider the size ratio between classes, there exists
an issue that the resampling rate is not consistent with the
imbalance degree in data and varies with the number of classes. Besides, no existing work has studied the fundamental
issues of class imbalance in online cases and the adaptivity
of online learners to deal with dynamic data streams with a
varying imbalance rate (IR) so far. Most work focuses on
online learning with concept drifts that involve classiﬁcation
boundary shifts. This paper will give the ﬁrst comprehensive
analysis of class imbalance in data streams.
First, we improve the resampling setting strategies in
OOB and UOB. Second, we look into the performance of
The authors are with the Centre of Excellence for Research in Computational Intelligence and Applications (CERCIA), School of Computer
Science, The University of Birmingham, Edgbaston, Birmingham B15
2TT, United Kingdom.
E-mail: {S.Wang, L.L.Minku, X.Yao}@cs.bham.ac.uk.
Manuscript received 18 Aug. 2013; revised 13 June 2014; accepted 10 July
2014. Date of publication 4 Aug. 2014; date of current version 27 Mar. 2015.
Recommended for acceptance by S. Chawla.
For information on obtaining reprints of this article, please send e-mail to:
 , and reference the Digital Object Identiﬁer below.
Digital Object Identiﬁer no. 10.1109/TKDE.2014.2345380
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,
This work is licensed under a Creative Commons Attribution 3.0 License. For more information, see 
improved OOB and UOB in both static and dynamic data
streams, aiming for a deep understanding of the roles of
resampling and time-decayed metrics. We also give a statistical analysis of how their performance is affected by datarelated factors and algorithm settings through factorial
ANOVA. Generally speaking, our experiments show the
beneﬁts of using resampling and time-decayed metrics in
the improved OOB and UOB. Particularly, UOB is better at
recognizing minority-class examples in static data streams,
and OOB is more robust against dynamic changes in class
imbalance status. The data distribution is a major factor
affecting their performance. Based on the achieved results,
for better accuracy and robustness under dynamic scenarios, we propose two ensemble strategies that maintain both
OOB and UOB with adaptive weight adjustment, called
WEOB1 and WEOB2. They are shown to successfully combine the strength of OOB and UOB. WEOB2 outperforms
WEOB1 in terms of G-mean.
LEARNING IMBALANCED DATA STREAMS
In this section, we deﬁne class imbalance under online scenarios, introduce the two classiﬁcation methods (i.e. OOB
and UOB), and review the research progress in learning from
imbalanced data streams. They form the basis of this paper.
Deﬁning Class Imbalance
To handle class imbalance online, we ﬁrst need to deﬁne it
by answering the following three questions: 1) is the data
stream currently imbalanced? 2) Which classes belong to
the minority/majority? 3) What is the imbalance rate currently? We answered the questions by deﬁning two online
indicators—time-decayed class size and recall calculated for
each class . Different from the traditional way of considering all observed examples so far equally, they are updated
incrementally by using a time decay (forgetting) factor to
emphasize the current status of data and weaken the effect
of old data.
Suppose a sequence of examples xt; yt
Þ arriving one at a
time. xt is a p-dimensional vector belonging to an input
space X observed at time t, and yt is the corresponding label
belonging to the label set Y ¼ c1; . . . ; cN
g. For any class
ck 2 Y , the class size indicates the occurrence probability
(percentage) of examples belonging to ck. If the label set contains only two classes, then the size of the minority class is
referred to as the imbalance rate of the data stream, reﬂecting how imbalanced the data stream is at the current
moment. The recall of class ck indicates the classiﬁcation
accuracy on this class. To reﬂect the current characteristics
of the data stream, these two indicators are incrementally
updated at each time step. When a new example xt arrives,
the size of each class, denoted by wðtÞ
k , is updated by :
k ¼ uwðt1Þ
þ ð1  uÞ½ðxt; ckÞ;
ðk ¼ 1; . . . ; NÞ;
 ¼ 1 if the true class label of xt is ck, otherwise
Þ is a pre-deﬁned time decay factor, which
forces older data to affect the class percentage less along
with time through the exponential smoothing. Thus, wðtÞ
adjusted more based on new data.
If xt’s real label yt is ci, the recall of class ci, denoted by
i , is updated by :
¼ u0Rðt1Þ
For the recall of the other classes, denoted by RðtÞ
j (j 6¼ i), it is
updated by:
ðj ¼ 1; . . . ; N; j 6¼ iÞ:
In Eq. (2), u0 0 < u0 < 1
Þ is the time decay factor for emphasizing the learner’s performance at the current moment.
 is equal to 1 if x is correctly classiﬁed, and 0 otherwise. u ¼ 0:9 and u0 ¼ 0:9 were shown to be a reasonable setting to balance the responding speed and the estimation
variance in our experiments .
wðtÞ and RðtÞ values are then used by a class imbalance
information
whether the data stream should be regarded as imbalanced
and which classes should be treated as the minority class. It
examines two conditions for any two classes ci and cj:
wi  wj > d1 (0 < d1 < 1)
Ri  Rj > d2 (0 < d2 < 1).
If both conditions are satisﬁed, then class cj is sent to the
minority class label set Ymin and class ci is sent to the majority class label set Ymaj. If Ymin and Ymaj are not empty, it
means that the data stream is imbalanced. This can then be
used to invoke the class imbalance techniques running in
the online model to tackle the imbalanced distribution.
Recall is used as one criterion of estimating class imbalance
status, because it has been agreed that the imbalance rate is
not the only factor that causes the classiﬁcation difﬁculty .
Online Solutions OOB and UOB
With the information from the class imbalance detection
method, we proposed OOB and UOB to learn imbalanced
data streams . They integrate resampling into ensemble
algorithm Online Bagging (OB) . Resampling is one of the
simplest and most effective techniques of tackling class
imbalance , . It works at the data level independently
of the learning algorithm. Two major types of resampling
are oversampling—increasing the number of minority-class
examples, and undersampling—reducing the number of
majority-class examples. Online Bagging is an online
ensemble learning algorithm that extends the ofﬂine Bagging . It builds multiple base classiﬁers and each classi-
ﬁer is trained K times by using the current training
example, where K follows the Poisson  ¼ 1
Þ distribution.
Poisson distribution is used, because the Binomial distribution K in ofﬂine Bagging tends to the Poisson(1) distribution
when the bootstrap sample size becomes inﬁnity.
Once Ymin and Ymaj (the output of the class imbalance
detection method) are not empty, oversampling or undersampling embedded in Online Bagging will be triggered
to either increase the chance of training minority-class
examples (in OOB) or reduce the chance of training
majority-class examples (in UOB). Their training procedures are given in Table 1.
Resampling in OOB and UOB is performed through the
distribution
WANG ET AL.: RESAMPLING-BASED ENSEMBLE METHODS FOR ONLINE CLASS IMBALANCE LEARNING
imbalance. In OOB, 1 is set to 1=w tð Þ
k ; 2 is set to 1. In UOB,
1 is set to 1, 2 is set to xð1  wðtÞ
k Þ. If the new training
example belongs to the minority class, OOB increases value
K, which decides how many times to use this example for
training. Similarly, if it belongs to the majority class, UOB
decreases K. The advantages of OOB and UOB are: 1)
resampling is algorithm-independent, which allows any
type of online classiﬁers to be used; 2) time-decayed class
size used in OOB and UOB dynamically estimates imbalance status without storing old data or using windows, and
adaptively decides the resampling rate at each time step; 3)
like other ensemble methods, they combine the predictions
from multiple classiﬁers, which are expected to be more
accurate than a single classiﬁer.
Existing Research
Most existing algorithms dealing with imbalanced data
streams require processing data in batches/chunks (incremental learning), such as MuSeRA and REA proposed by Chen et al., and Learn++.CDS and Learn++.NIE
 proposed by Ditzler and Polikar. Among limited class
imbalance solutions strictly for online processing, Nguyen
et al. ﬁrst proposed an algorithm to deal with imbalanced
data streams through random undersampling . The
majority class examples have a lower probability to be
selected for training. It assumes that the information of
which class belongs to the minority/majority is known and
the imbalance rate does not change over time. Besides, it
requires a training set to initialize the classiﬁcation model
before learning. Minku and Yao proposed to use undersampling and oversampling to deal with class imbalance in
online learning by changing the parameter corresponding to
Online Bagging’s sampling rate. However, the sampling
parameters need to be set prior to learning and cannot be
adjusted to changing imbalance rates. Very recently, two perceptron-based methods RLSACP and WOS-ELM 
were proposed, which assign different misclassiﬁcation costs
to classes to adjust the weights between perceptrons. The
error committed on the minority class suffers a higher cost.
RLSACP adopts a window-based strategy to update misclassiﬁcation costs based on the number of examples in each
class at a pre-deﬁned speed. WOS-ELM requires a validation
set to adjust misclassiﬁcation costs based on classiﬁcation
performance, which however may not be available in many
real-world applications. They were tested in static scenarios
with a ﬁxed imbalance rate and shown to be effective.
IMPROVED OOB AND UOB
In the current OOB and UOB methods, the choice of  for
one class only depends on the size of this class. Therefore,
this training strategy can be applied to multi-class cases
directly, having more than one minority or majority class.
However, when the data stream becomes balanced,  will
not be equal to 1. It means that the resampling keeps running even when the data is balanced, if the class imbalance
detection method is not applied as the trigger. For example,
given a balanced 2-class problem, where w tð Þ
is equal to
0.5,  for one class will be set to 2 by OOB; given a balanced
5-class problem, where w tð Þ
is equal to 0.2,  for one class
will be set to 5 by OOB. On the one hand, the strategy of setting  is not consistent with the imbalance degree, and
varies with the number of classes. On the other hand, it is
necessary to use the class imbalance detection method, to
inform OOB and UOB of when resampling should be
To overcome this issue, we improve OOB and UOB with
a better parameter setting strategy in this section.  in the
improved versions is determined by the size ratio between
classes. The pseudo-code of improved OOB and UOB at
each training step t is given in Table 2, assuming there are
two possible classes Y ¼ þ1; 1
g (denoted by ‘þ’ and ‘’
for simplicity).
If the size of the positive class is smaller than the size of
the negative class at the current moment,  for the positive
class will be set to w tð Þ
þ for oversampling in OOB;  for
the negative class will be set to w tð Þ
 for undersampling
in UOB. With the same Bagging strategy as the traditional
Online Bagging , improved OOB and UOB only sweep
through each training example once. They need O M
to keep the online model up-to-date, when the base classi-
ﬁers are updated sequentially.
When data become balanced, these methods will be
reduced to OB automatically. For any 2-class problems,
therefore, it is not necessary to apply the class imbalance
detection method any more for a hard partition of class
labels into minority and majority label sets. For multi-class
problems, however, this hard partition is still necessary for
OOB and UOB Training Procedures
Input: label sets Ymin and Ymaj, an ensemble with M base
learners, and current training example xt; yt
for each base learner fm (m ¼ 1; 2; . . . ; M) do
if yt 2 Ymin
set K  Poisson 1
set K  Poisson 2
update fm K times
Improved OOB and UOB
Input: an ensemble with M base learners, current training
example xt; yt
Þ, and current class size w tð Þ ¼ ðw tð Þ
þ ; w tð Þ
for each base learner fm (m ¼ 1; 2; . . . ; M) do
if yt ¼ þ1 and
þ < w tð Þ
þ > w tð Þ
set K  Poissonðw tð Þ
else if yt ¼ 1 and
 < w tð Þ
 > w tð Þ
set K  Poissonðw tð Þ
set K  Poisson 1
update fm K times
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,
choosing w tð Þ
min and w tð Þ
maj, where w tð Þ
min (w tð Þ
maj) stands for the
class size of the smaller (larger) class. For example, for a
data stream with 4 classes
c1; c2; c3; c4
g, their proportions
are 0.05, 0.15, 0.25, 0.55 respectively. Class c3 is a majority
class relative to c1, but a minority class relative to c4. Since
we do not want to overlook the performance of any minority class, the class imbalance detection method will label c3
as minority. This information is then used by OOB and
UOB to treat this class. Without recognizing this situation, it
would be difﬁcult to determine the way of sampling. This
paper focuses on 2-class imbalanced data streams, so we do
not use the class imbalance detection method to trigger
resampling in improved OOB and UOB here. How it helps
the classiﬁcation in multi-class cases will be studied as our
next-step work. All the following analysis will be based on
the improved OOB and UOB. We will simply use “OOB”
and “UOB” to indicate the improved ones for brevity.
CLASS IMBALANCE ANALYSIS IN STATIC DATA
This section studies OOB and UOB in static data streams
without any changes. We focus on the fundamental issue of
class imbalance and look into the following questions under
different imbalanced scenarios: 1) to what extent does
resampling in OOB and UOB help to deal with class imbalance online? 2) How do they perform in comparison with
other state-of-the-art algorithms? 3) How are they affected
by different types of class imbalance and classiﬁers? For the
ﬁrst question, we compare OOB and UOB with OB , to
show the effectiveness of resampling. For the second question, we compare OOB and UOB with two recently proposed learning algorithms, RLSACP and WOS-ELM
 , which also aim to tackle online imbalanced data. For
the third question, we perform a mixed (split-plot) factorial
analysis of variance (ANOVA) to analyse the impact of
three factors, including the data distribution and imbalance
rate in data, and the base classiﬁer in OOB and UOB.
Data Description and Experimental Settings
This section describes the static imbalanced data used in the
experiment, including 12 artiﬁcial data streams and two
real-world data streams, and explains the algorithm settings
and experimental designs for a clear understanding in the
following analysis.
To facilitate a deep understanding and accurate analysis,
artiﬁcial data sets are generated in order to obtain desired
types of imbalanced data streams. We produce 12 two-class
data streams with different distributions and imbalance
rates. The imbalance rate, i.e. the occurrence probability of
the minority class, is a direct factor that affects any online
learner’s performance. A smaller rate means a smaller
chance to collect the minority-class examples, and thus a
harder case for classiﬁcation. During the online processing,
since it is not possible to get the whole picture of data, the
frequency of minority-class examples and data sequence
become more important in online learning than in ofﬂine
learning. In addition to IR, complex data distributions have
been shown to be a major factor causing degradation of
classiﬁcation performance in ofﬂine class imbalance learning, such as small sub-concepts of the minority class with
very few examples , and the overlapping between classes , . Particularly, authors in distinguished and
analysed four types of data distributions in the minority
class—safe, borderline, outliers and rare examples. Safe
examples are located in the homogenous regions populated
by the examples from one class only; borderline examples
are scattered in the boundary regions between classes,
where the examples from both classes overlap; rare examples and outliers are singular examples located deeper in the
regions dominated by the majority class. Borderline, rare
and outlier data sets were found to be the real source of difﬁculties in real-world data sets, which could also be the case
in online applications .
In our experiment, we consider four imbalance levels (i.e.
5, 10, 20 and 30 percent) and three minority-class distributions (i.e. safe, borderline, rare/outlier). Rare examples and
outliers are merged into one category, due to the observation that they always appear together . Each data stream
has 1,000 examples (namely 1,000 time steps). Each example
is formed of two numeric attributes and a class label that
can be þ1 or 1. The positive class is ﬁxed to be the minority class. Each class follows the multivariate Gaussian distribution. We vary the mean and covariance matrix settings of
the Gaussian distributions to obtain different distributions.
For each type of distribution, the data set contains more
than 50 percent of corresponding category of examples,
measured by the 5-nearest neighbour method . In the
borderline case, for instance, more than 50 percent minority-class examples meet the “borderline” deﬁnition.
For an understanding under practical scenarios, we
include two real-world data from fault detection applications—Gearbox and Smart Building . The task of
Gearbox is to detect faults in a running gearbox using
accelerometer data and information about bearing geometry. Smart Building aims to identify sensor faults in smart
buildings . Both contain two classes, and the faulty
class is the minority. Without loss of generality, we let
the minority class be the positive class þ1, and let the
majority class be the negative class 1. One data stream
is generated from each data source. We limit the length of
the data stream to 1,000 examples and ﬁx IR at 10 percent.
In the experiment of comparing OOB, UOB and OB, each
method builds an ensemble model composed of 50 Hoeffding trees . Hoeffding tree is an incremental, anytime
decision tree induction algorithm capable of learning from
high-speed data streams, supported mathematically by the
Hoeffding bound and implemented by the massive online
analysis (MOA) tool with its default settings . It was
shown to be an effective base classiﬁer for the original OOB
and UOB , .
When comparing OOB and UOB with RLSACP and
WOS-ELM, we use the multilayer perceptron (MLP) classi-
ﬁer as the base learner of OOB and UOB, considering that
RLSACP and WOS-ELM are both perceptron-based algorithms. The number of neurons in the hidden layer of MLP
is set to the number of attributes in data, which is also the
number of perceptrons in RLSACP and WOS-ELM. The
WANG ET AL.: RESAMPLING-BASED ENSEMBLE METHODS FOR ONLINE CLASS IMBALANCE LEARNING
error cost of classes is updated at every 500 time steps in
RLSACP. The error cost in WOS-ELM is predeﬁned based
on the real imbalance rate in the data stream. OOB and
UOB are composed of 50 MLPs. For a clear role of resampling, OOB with a single MLP is also included in the comparison as a benchmark, since RLSACP and WOS-ELM only
involve one neural network and they emphasize the minority class by increasing its misclassiﬁcation cost.
The decay factor for updating class size wðtÞ
and UOB is set to 0.9, based on our preliminary experiments . Every discussed method is repeated 100 times
on each data stream. The average prequential performance is recorded at each time step. Prequential test is a
popular performance evaluation strategy in online learning, in which each individual example is used to test the
model before it is used for training, and from this the performance measures can be incrementally updated. For
example, the prequential accuracy at time step t can be
calculated by 
acc tð Þ ¼
accx tð Þ;
Þ þ accx tð Þacc t1
otherwise,
where accx is 0 if the prediction of the current training
example x before its learning is wrong and 1 if it is correct.
In our prequential test, we choose G-mean and minorityclass recall to be the evaluation metrics. They are two most
commonly used evaluation criteria in the class imbalance
learning literature, as they are insensitive to the imbalance
rate. Recall is deﬁned as the classiﬁcation accuracy on a single class. Minority(positive)-class recall (Recp) and majority
(negative)-class recall (Recn) can be obtained through the following formulas: Recp ¼ TP=P and Recn ¼ TN=N, where
TP is the number of true positives, TN is the number of true
negatives, and P and N are the total numbers of positive and
negative examples observed so far. G-mean is deﬁned as the
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Recp  Recn
for the two-class case. Recall helps us to analyse the performance within classes, but does not reﬂect any
performance on other classes. G-mean is an overall performance metric. It helps us to understand how well the performance is balanced among classes.
When comparing any two learning algorithms, we use
the Wilcoxon Sign Rank test with Holm-Bonferroni corrections to show the difference statistically, at the overall level
of signiﬁcance of 0.05. Holm-Bonferroni corrections are performed to counteract the problem of multiple comparisons.
Role of Resampling
We compare the ﬁnal-step minority-class recall and G-mean
produced from tree-based OOB, UOB and OB. They show
the ﬁnal prequential performance step after the online
model has gone through all the examples, averaged over
multiple runs.
Tables 3 and 4 present their means and standard deviations over the 100 runs. We can see that UOB achieves the
The Final-Step Minority-Class Recall and Statistical
Test Results from Tree-Based OOB, UOB and OB
Distribute
0.970  0.002
0.973  0.001
0.969  0.001
0.981  0.003
0.964  0.002
0.964  0.004
0.912  0.007
0.936  0.005
0.905  0.007
0.840  0.000
0.917  0.013
0.876  0.019
borderline
0.636  0.013
0.774  0.007
0.462  0.015
0.577  0.007
0.811  0.007
0.266  0.046
0.424  0.018
0.868  0.011
0.026  0.016
0.225  0.035
0.831  0.019
0.000  0.000
rare/outlier
0.197  0.016
0.662  0.033
0.033  0.004
0.395  0.015
0.755  0.014
0.142  0.015
0.195  0.024
0.699  0.014
0.195  0.024
0.310  0.010
0.519  0.021
0.008  0.009
0.045  0.009
0.446  0.041
0.000  0.000
Smart Building
0.430  0.004
0.764  0.011
0.234  0.103
The Final-Step G-Mean and Statistical Test Results
from Tree-Based OOB, UOB and OB
Distribute
0.984  0.001
0.978  0.001
0.983  0.001
0.981  0.001
0.971  0.001
0.975  0.002
0.953  0.003
0.959  0.002
0.951  0.003
0.916  0.000
0.954  0.006
0.936  0.010
borderline
0.705  0.005
0.735  0.003
0.631  0.009
0.712  0.003
0.786  0.003
0.500  0.045
0.636  0.013
0.859  0.004
0.154  0.048
0.470  0.036
0.872  0.008
0.000  0.000
rare/outlier
0.405  0.014
0.532  0.016
0.181  0.013
0.565  0.009
0.651  0.011
0.374  0.020
0.435  0.027
0.697  0.007
0.435  0.027
0.547  0.008
0.641  0.011
0.058  0.069
0.206  0.023
0.498  0.013
0.000  0.000
Smart Building
0.609  0.003
0.689  0.008
0.472  0.108
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,
highest minority-class recall and G-mean in almost all cases,
and OB gets the lowest. Both oversampling in OOB and
undersampling in UOB improve the prediction accuracy on
the minority class and the overall performance greatly compared to OB. UOB is more effective. These observations are
further conﬁrmed by our statistical test, between UOB and
OOB/OB. Twenty eight pairs of comparisons are involved
for each performance metric. The resulting p-values are
included in brackets in the tables. Those in bold italics suggest a signiﬁcant difference between UOB and the method
in the corresponding column.
Moreover, the three models show quite high performance in the cases with a ‘safe’ distribution, but the performance gets much worse in the other cases. Particularly, OB
suffers a greater performance reduction. It means that
‘borderline’ and ‘rare/outlier’ are harder distributions than
‘safe’ in online learning. A smaller IR does not necessarily
cause worse performance from the results here. More
details about the impact of data distribution and IR will be
given in Section 4.4.
Comparison with Other Algorithms
This section compares MLP-based OOB and UOB with two
state-of-the-art methods RLSACP and WOS-ELM. OOB
with a single MLP is included as a benchmark, denoted by
OOBsg. Tables 5 and 6 present their ﬁnal-step minority-class
recall and G-mean respectively. We can observe that UOB is
the winner of minority-class recall in most cases, and OOB
is the winner of G-mean. The Wilcoxon Sign Rank test is
thus carried out between UOB and every other method for
minority-class recall and between OOB and every other
method for G-mean. There are 56 pairs of comparison for
each metric. P-values are included in the tables.
Although undersampling in MLP-based UOB improves
minority-class recall greatly, we notice that the majorityclass recall is dragged down too much, which explains why
its overall performance is worse than OOB. RLSACP and
WOS-ELM outperform OOB in terms of minority-class
recall in some cases. However, because their majority-class
recall is decreased more than the increase of minority-class
recall, their G-mean does not beat OOB’s. Besides, we notice
The Final-Step G-Mean and Statistical Test Results from MLP-Based OOB, OOBsg, UOB, RLSACP and WOS-ELM
Distribute
0.972  0.001
0.926  0.007 (0.00000)
0.493  0.345 (0.00000)
0.065  0.066 (0.00000)
0.963  0.006 (0.00000)
0.970  0.001
0.907  0.010 (0.00000)
0.548  0.365 (0.00000)
0.036  0.077 (0.00000)
0.960  0.008 (0.00000)
0.957  0.002
0.842  0.025 (0.00000)
0.593  0.304 (0.00000)
0.146  0.135 (0.00000)
0.955  0.005 (0.00240)
0.933  0.000
0.776  0.040 (0.00000)
0.123  0.105 (0.00000)
0.160  0.181 (0.00000)
0.936  0.013 (0.02430)
borderline
0.586  0.007
0.515  0.022 (0.00000)
0.515  0.079 (0.00000)
0.231  0.137 (0.00000)
0.580  0.026 (0.16150)
0.537  0.010
0.426  0.050 (0.00000)
0.475  0.086 (0.00008)
0.348  0.138 (0.00000)
0.537  0.041 (0.37770)
0.500  0.009
0.374  0.096 (0.00000)
0.467  0.123 (0.02940)
0.189  0.113 (0.00000)
0.509  0.042 (0.09680)
0.104  0.061
0.447  0.060 (0.00000)
0.183  0.055 (0.00000)
0.306  0.172 (0.00000)
0.149  0.093 (0.00001)
rare/outlier
0.399  0.016
0.463  0.026 (0.00000)
0.513  0.021 (0.00000)
0.476  0.056 (0.00000)
0.458  0.039 (0.00000)
0.561  0.007
0.425  0.060 (0.00000)
0.482  0.093 (0.00000)
0.254  0.228 (0.00000)
0.558  0.022 (0.93770)
0.598  0.011
0.447  0.094 (0.00000)
0.516  0.153 (0.09300)
0.163  0.170 (0.00000)
0.605  0.030 (0.03150)
0.416  0.004
0.450  0.081 (0.00000)
0.316  0.047 (0.00000)
0.162  0.208 (0.00000)
0.446  0.045 (0.00000)
0.077  0.047
0.459  0.055 (0.00000)
0.189  0.063 (0.00000)
0.289  0.022 (0.00000)
0.198  0.088 (0.00000)
Smart Building
0.243  0.027
0.485  0.020 (0.00000)
0.220  0.081 (0.00033)
0.527  0.004 (0.00000)
0.295  0.082 (0.00000)
The Final-Step Minority-Class Recall and Statistical Test Results from MLP-Based OOB, OOBsg, UOB, RLSACP and WOS-ELM
Distribute
0.973  0.004 (0.00000)
0.986  0.001
0.490  0.344 (0.00000)
0.332  0.466 (0.00000)
0.959  0.016 (0.00000)
0.979  0.003 (0.00000)
0.992  0.007
0.551  0.373 (0.00000)
0.283  0.443 (0.00000)
0.960  0.026 (0.00000)
0.923  0.005 (0.00620)
0.900  0.048
0.593  0.302 (0.00000)
0.586  0.466 (0.97450)
0.920  0.010 (0.0231)
0.880  0.000 (0.00000)
0.741  0.120
0.027  0.025 (0.00000)
0.414  0.449 (0.02530)
0.884  0.025 (0.00000)
borderline
0.488  0.016 (0.00000)
0.828  0.036
0.512  0.020 (0.00000)
0.194  0.286 (0.00000)
0.515  0.074 (0.00000)
0.369  0.017 (0.00000)
0.854  0.055
0.535  0.028 (0.00000)
0.520  0.341 (0.00002)
0.392  0.079 (0.00000)
0.293  0.011 (0.00000)
0.806  0.106
0.481  0.064 (0.00000)
0.296  0.398 (0.00000)
0.315  0.060 (0.00000)
0.015  0.008 (0.00000)
0.456  0.212
0.039  0.017 (0.00000)
0.417  0.348 (0.21640)
0.032  0.026 (0.00000)
rare/outlier
0.196  0.019 (0.00000)
0.727  0.047
0.493  0.115 (0.00000)
0.559  0.149 (0.00000)
0.303  0.072 (0.00000)
0.370  0.011 (0.00000)
0.853  0.052
0.468  0.015 (0.00000)
0.234  0.266 (0.00000)
0.391  0.040 (0.00000)
0.390  0.015 (0.00000)
0.839  0.088
0.521  0.144 (0.00000)
0.363  0.419 (0.00000)
0.408  0.044 (0.00000)
0.181  0.004 (0.00000)
0.479  0.200
0.111  0.030 (0.00000)
0.164  0.270 (0.00000)
0.212  0.045 (0.00000)
0.008  0.005 (0.00000)
0.697  0.110
0.042  0.023 (0.00000)
0.888  0.022 (0.00000)
0.049  0.037 (0.00000)
Smart Building
0.065  0.014 (0.00000)
0.552  0.075
0.161  0.280 (0.00000)
0.484  0.011 (0.00000)
0.109  0.058 (0.00000)
WANG ET AL.: RESAMPLING-BASED ENSEMBLE METHODS FOR ONLINE CLASS IMBALANCE LEARNING
that WOS-ELM presents very large performance variance in
many cases. This is caused by the fact that the minority-class
overemphasized
majority-class
performance is sacriﬁced greatly in some runs. A possible
explanation for this phenomenon is that the extreme learning machine (ELM) used in WOS-ELM was found to be sensitive to outliers in data and lacks robustness sometimes
 . Especially after the minority class is further emphasized by a higher weight in WOS- ELM, the empirical risk
minimization principle of ELM is more likely to lead to
great bias towards the minority class.
OOB and OOBsg have similar performance. OOBsg shows
better G-mean than RLSACP and WOS-ELM in most cases.
This observation conﬁrms that resampling is the main reason for OOB and UOB outperforming RLSACP and WOS-
ELM, rather than the ensemble of classiﬁers.
Factorial Analysis of Data-Related Factors and
Base Classiﬁers
We have looked into the effectiveness of OOB and UOB
through algorithm comparisons. Based on the results so
far, we perform a mixed (split-plot) factorial analysis of
variance , to study statistically whether and how their
performance is affected by types of class imbalance and
base classiﬁers. Three factors are analysed: data distributions, IR and base classiﬁers. There are three different levels (settings) for ‘distribution’ (safe, borderline, rare/
outlier) and four different levels for IR (5, 10, 20 and 30
percent). We consider two levels (types) of base classi-
ﬁers—decision tree and MLP. A mixed design is necessary,
because the distribution and IR are between-subjects factors (their levels vary with the data being used), and the
base classiﬁer is a within-subjects factor (its levels vary
within the data). The factorial design allows the effects of a
factor to be estimated at several levels of the other factors.
The effects of the factors on the ﬁnal-step G-mean is discussed. The metric under observation is also called
“response” in ANOVA.
The ANOVA results are presented in Table 7, including
p-value and eta-squared (h2). A p-value smaller than 0.05
indicates a signiﬁcant difference by rejecting the null
hypothesis under the signiﬁcance level of 5 percent. h2 is a
measure in the range of 0; 1
 describing the effect size. The
larger the h2, the greater the effect of the factor. It is worth
mentioning here that h2 is calculated separately for betweensubjects and within-subjects factors . For the cases with
only between-subjects factors and the cases involving
within-subjects factors, the sum of h2 of the factor effects and
the associated standard errors is equal to 1 respectively.
Table 7 shows the factor effects on G-mean. All the p-values are much smaller than 0.05, suggesting that all the three
factors and their interactions have a signiﬁcant impact on
G-mean. According to h2, the effect of data distribution
(0.842 for OOB and 0.941 for UOB) is much larger than the
effects of IR (0.063 for OOB and 0.001 for UOB) and the base
classiﬁer (0.201 for OOB and 0.692 for UOB). The effect of IR
on UOB is much smaller than that on OOB, suggesting that
UOB is less sensitive to the imbalance rate. The effect of
base classiﬁers on OOB is much smaller than that on UOB,
suggesting that OOB is less sensitive to the choice of base
classiﬁers. Generally speaking, the minority-class distribution is a major factor affecting the performance of OOB and
UOB, compared to which their performance is quite robust
to the imbalance rate.
CLASS IMBALANCE ANALYSIS IN DYNAMIC
DATA STREAMS
This section focuses on the dynamic feature of online class
imbalance. Different from existing concept drift methods
that mainly aim for changes in class-conditional probability density functions, we look into the performance of
OOB and UOB when tackling data streams with imbalance status changes (i.e. changes in class prior probabilities). In other words, IR is changing over time. It happens
in real-world problems. For example, given the task of
detecting faults in a running engineering system, the
faulty class is usually the minority class that rarely occurs.
The faults can become more and more frequent over time,
if the damaged condition gets worse; or the faults are not
likely to happen from some moment, because the faulty
system is repaired. Dynamic data are more challenging
than static ones, because the online model needs to sense
the change and adjust its learning bias quickly to maintain
its performance. Different types of changes in imbalance
status are designed and studied here, varying in changing
speed and severity.
The adaptivity and robustness of OOB and UOB are
studied by answering the following questions: 1) how does
the time-decayed metric used in OOB and UOB help to
handle the imbalance change? 2) How do OOB and UOB
perform in comparison with other state-of-the-art algorithms under dynamic scenarios? 3) How is their performance affected by the decay factor? For the ﬁrst question,
OOB and UOB are compared with those applying the traditional method of updating the class size. For the second
question, OOB and UOB are compared with RLSACP and
repeated measure design is performed to analyse the
impact of the decay factor.
Factor Effects of Data Distributions (abbr. Dis), Imbalance
Rates (abbr. IR) and Base Classiﬁers (abbr. BC), and Their
Interaction Effects on G-mean from OOB and UOB
Between-Subjects Effects
Within-Subjects Effects
The symbol “*” indicates the interaction between two factors.
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,
Data Description and Experimental Settings
In this section, we ﬁrst describe the dynamic imbalanced
data used in the experiment, covering various types of
changes in terms of changing severity and speed. Then, we
give the algorithm settings and experimental designs.
Three data sources are used to produce dynamic data
streams—Gaussian data, Gearbox and Smart Building. By
using the same data generation method as described in
Section 4.1, we produce two-class Gaussian data streams
formed of two numeric attributes and a class label that can
be þ1 or 1. Each class follows the multivariate Gaussian
distribution. Some overlapping between classes is enabled
for a certain level of data complexity. Gearbox and Smart
Building are fault detection data from the real-world applications used in the previous section, containing a faulty class
(þ1) and a nonfaulty class (1). We limit the length of each
data stream to 1,000 examples. Different from static data
streams, the dynamic ones involve a class imbalance change
right after time step 500. During the ﬁrst 500 time steps, IR is
ﬁxed to 10 percent and the positive class is the minority.
From time step 501, a change occurs at a certain speed and
severity. Four data streams are generated from each data
source. Each type is a combination of two changing severity
levels and two changing speeds. The changing severity can
be either high or low; the changing speed can be either
abrupt or gradual. The concrete change settings are:
High severity. The negative class becomes the minority with IR 10 percent in the new status.
Low severity. The data stream becomes balanced (i.e.
IR = 50%) in the new status.
Abrupt change. The length of the changing period is 0.
The new status completely takes over the data
stream from time step 501.
Gradual change. The change lasts for 300 time steps.
During the changing period, the probability of examples being drawn from the old joint distribution
decreases linearly.
When studying how the time-decayed class size wðtÞ
and UOB helps the classiﬁcation in dynamic data streams,
we replace wðtÞ
k with the class size updated in the traditional
way, which considers all observed examples so far equally.
They are denoted by OOBtr and UOBtr and compared with
OOB and UOB. All the methods here build an ensemble
model composed of 50 Hoeffding trees.
When discussing the adaptivity of OOB and UOB in
comparison with RLSACP and WOS-ELM, MLP is used as
the base classiﬁer of OOB and UOB as before. Both OOB
and UOB consist of 50 MLPs. OOB with a single MLP
(OOBsg) is included as the benchmark. The window size for
updating the error cost of classes is shortened to 50 time
steps in RLSACP, to encourage a faster response to the
change. The error cost in the original WOS-ELM was
updated based on a separate validation data set in the original article , which however may not be available in
many real-world problems, or expire over time. To allow
the same level of adaptivity as OOB and UOB, the timedecayed class sizes are used to set the costs in WOS-ELM.
Speciﬁcally, the error cost of the majority class is always
equal to 1; the error cost of the minority class is set to
min, where wðtÞ
maj and wðtÞ
min are the time-decayed sizes
of the current majority and minority classes respectively.
The same as in Section 4, the prequential recall and Gmean are tracked to observe the performance before and
after the change. For a clear and accurate understanding of
OOB and UOB learning dynamic data, we divide the prequential evaluation process into three stages, old-status
stage, changing stage and new-status stage, by resetting the
performance metrics to 0 between the stages. In more details,
for the data with an abrupt change (where there is no changing stage), recall and G-mean are reset after time step 500,
when the change starts and ends; for the data with a gradual
change (where the changing stage is 300 time-step long),
recall and G-mean are reset after time step 500 and 800. This
is to ensure that the performance observed after the change
is not affected by the performance before the change. Prequential performance curves will be presented to help visualize the performance behaviors at each time step and the
impacts of changes. For a quantitative understanding, we
will provide the average prequential performance covering
all the time steps during the new-status stage. It can better
reﬂect the inﬂuence of the status change than the ﬁnal-step
performance. The Wilcoxon Sign Rank test with Holm-Bonferroni corrections is applied for statistical analysis.
Role of the Time-Decayed Metric
In this section, we aim to ﬁnd out: 1) how different types of
imbalance changes affect the performance of OOB and
UOB; 2) whether and how the time-decayed class size in
OOB and UOB facilitates learning in dynamic data streams,
through the comparison with OOBtr and UOBtr. To understand the impact of change on each class, we monitor the
behaviours of prequential recall of minority and majority
classes individually along with time. Fig. 1 presents the
recall curves produced from the data streams using data
source Smart Building.
For the abrupt change with high severity, before the
change happens, UOB and UOBtr have better minority(positive)-class recall than OOB and OOBtr, which conﬁrms that
undersampling is a more aggressive technique of emphasizing the minority class than oversampling. After time step
500 when this class becomes the majority abruptly, all methods present a rapid growth in recall, especially for OOBtr
and UOBtr. The growth is caused by the frequent arrival of
positive-class examples; OOBtr and UOBtr increase faster
than OOB and UOB, because the imbalance status obtained
by using the traditional method still shows that this class is
the minority within a period of time after the change. So,
the wrong resampling technique is applied, which emphasizes this class even when it has become the majority. The
time-decayed class size in OOB and UOB reﬂects the correct
imbalance status quickly, and thus OOB and UOB adjust
their focus on the correct class after the change.
In terms of negative-class recall, when this class turns
from the majority to the minority, all the methods present a
more or less reduced recall; OOB is the most resistant. One
WANG ET AL.: RESAMPLING-BASED ENSEMBLE METHODS FOR ONLINE CLASS IMBALANCE LEARNING
reason for the reduction is the performance trade-off
between classes. As the positive class becomes the majority
with a great performance improvement, the performance of
the other class would be compromised to some extent. UOB
suffers from a greater reduction than OOB right after the
change, because undersampling makes the online model
learn less knowledge about this class before the change.
Even so, the recall from UOB recovers faster than OOBtr
and UOBtr. When OOBtr and UOBtr still treat this class as
the majority after the change, UOB has already realized its
minority status and applied undersampling to the other
class. The robustness of OOB and the adaptivity of UOB
prove the beneﬁts of using time-decayed class size and the
importance of considering real-time status of data streams.
The other three types of changes present similar results.
The positive-class recall presents a fast increase, and the
negative-class recall is decreased, after the change occurs.
Among the four methods, UOB is the best at recognizing
the minority-class examples before the change; OOB is the
most resistant method to changes; UOB is affected more by
the changes than OOB on the old majority class, but its performance recovers faster than OOBtr and UOBtr. For the
space consideration, the plots from Gaussian and Gearbox
were omitted, from which similar results are obtained.
To understand how the overall performance is affected,
we compare average G-mean over the new-status stage.
Table 8 shows its means and standard deviations from 100
runs for the Smart Building data. P-values from the Wilcoxon Sign Rank test between OOB and every other method
are included in brackets, among which the ones in bold
italics suggest a signiﬁcant difference. We can see that
OOBtr and UOBtr are signiﬁcantly worse than OOB and
UOB, because of the incorrectly detected imbalance rate.
Although the above results show that UOB suffers from a
performance reduction on the old majority class, its G-mean
still outperforms OOB in some cases, because of its fast performance recovery on this class through undersampling
using the correctly estimated imbalance rate.
In conclusion, when there is an imbalance status change
in the data stream, OOB is the least affected. A performance
drop on the old majority class occurs to UOB after the
change, but its performance recovers rapidly. It is necessary
to use the time-decayed metric for choosing the correct
resampling technique and estimating the imbalance status.
Comparison with Other Algorithms
This section discusses the performance of MLP-based OOB
and UOB on the above dynamic data, in comparison with
RLSACP and WOS-ELM. OOB containing a single MLP
(OOBsg) is included as the benchmark. Average G-mean over
the new-status stage and the p-values produced from the Wilcoxon Sign Rank test between OOB and the others are shown
in Table 9. “NaN” p-value in the table means that the two
groups of samples for the statistical test are exactly the same.
Fig. 1. Prequential recall curves of positive (left) and negative (right)
classes from OOB, UOB, OOBtr and UOBtr on Smart Building data with
four types of changes.
Average G-Mean during the New-Status Stage and Statistical Test Results from Tree-Based OOB, UOB,
OOBtr and UOBtr on Smart Building
0.693  0.004
0.763  0.012 (0.00000)
0.389  0.018 (0.00000)
0.662  0.016 (0.00000)
0.609  0.001
0.462  0.012 (0.00000)
0.000  0.000 (0.00000)
0.208  0.123 (0.00000)
0.675  0.002
0.701  0.006 (0.00000)
0.446  0.025 (0.00000)
0.639  0.006 (0.00000)
0.679  0.001
0.683  0.020 (0.29420)
0.000  0.000 (0.00000)
0.511  0.022 (0.00000)
(Change type: H, high severity; L, low severity; A, abrupt; G, gradual).
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,
The results show that MLP-based OOB and UOB are
quite competitive. In Gaussian data, UOB shows worse Gmean than OOB, because the imbalance change causes
larger recall degradation on the old majority class. In Gearbox and Smart Building data, OOB produces zero G-mean
in two cases, which is caused by zero negative-class recall.
In these two cases where the negative class turns from the
majority to the minority gradually, because OOB is not so
aggressive as UOB at recognizing minority-class examples,
it could not adjust the learning bias to the new minority
class quickly enough. That is not observed in tree-based
OOB, as the decision tree was shown to be a more stable
base classiﬁer than MLP based on the results in Section 4.
OOB and UOB outperform RLSACP and WOS-ELM in most
cases. Although WOS-ELM applies the same time-decayed
metric for updating misclassiﬁcation costs as in OOB and
UOB, its G-mean remains zero in many cases, caused by the
zero recall of the old minority class. This class is not well
learnt before the change, and its recall value remains low
after the change because no emphasis is given to this class.
Resampling is simply a better class imbalance strategy than
adjusting class costs through the comparison with OOBsg,
which tallies with our observation in the previous section.
Factorial Analysis of the Decay Factor
Section 5.2 has shown the key role of using the time-decayed
class size wðtÞ
for deciding the resampling rate in OOB and
UOB. According to our preliminary experiments , the
decay factor u in its deﬁnition formula (Eq. (1)) is set to 0.9 in
the above experiments. Too small values of u (<0:8) emphasize the current performance too much, which can affect the
performance stability considerably. On the contrary, too
large values (>0:95) can deteriorate the adaptivity of online
models to dynamic changes. Within a reasonable setting
range, this section studies the impact of the decay factor on
the performance of OOB and UOB statistically. One-way
ANOVA using a repeated measure design is conducted.
We choose three levels (settings) for the factor u—0.8, 0.9
and 0.95. DT-based OOB and UOB are applied to the Smart
Building data with an abrupt change in high severity, under
each setting of u. The response in this ANOVA is the average
prequential G-mean during the new-status stage of class
imbalance. It is worth nothing that the data groups under
different factor levels violate the assumption of sphericity in
ANOVA based on Mauchly’s test, i.e. the level of dependence between pairs of groups is not equal. Therefore,
Greenhouse-Geisser correction is used . The ANOVA
and performance results are shown in Table 10.
According to the p-values and effect size h2, we can see
that the decay factor has a signiﬁcant impact on both OOB
and UOB (p-val < 0.05). The effect on OOB is much higher
than the effect on UOB. For OOB, a relatively smaller u seems
to be beneﬁcial to its performance, implying that adapting to
the change sooner may help OOB to respond to the new
minority class better. UOB is less sensitive to the setting of u.
ENSEMBLES OF OOB AND UOB
UOB has been shown to be a better choice than OOB in
terms of minority-class recall and G-mean. However, it has
some weaknesses when the majority class in the data stream
turns into the minority class. OOB has been shown to be
more robust against changes. To combine the strength of
OOB and UOB, this section proposes new methods based
on the idea of ensembles, which train and maintain both
OOB and UOB. A weight is maintained for each of them,
adjusted adaptively according to their current performance
measured by G-mean. Their combined weighted vote will
decide the ﬁnal prediction.
Recalling the deﬁnition of time-decayed recall RðtÞ in Section 2.1, we use RðtÞ to calculate real-time G-mean for deciding the weights of online learners. It is adaptive to dynamic
changes through the time decay factor, and reﬂects the current performance better than the prequential recall. However, we ﬁnd that RðtÞ presents a large variance between time
steps, which may result in misleading and unstable weights.
Average G-Mean during the New-Status Stage and Statistical Test Results from MLP-Based OOB, OOBsg, UOB,
RLSACP and WOS-ELM
0.532  0.034
0.019  0.013 (0.00000)
0.366  0.096 (0.00000)
0.090  0.125 (0.00000)
0.544  0.049 (0.07080)
0.360  0.002
0.359  0.011 (0.00440)
0.388  0.089 (0.04490)
0.037  0.095 (0.00000)
0.350  0.010 (0.00000)
0.795  0.003
0.521  0.025 (0.00000)
0.439  0.210 (0.00000)
0.022  0.074 (0.00000)
0.778  0.013 (0.00000)
0.811  0.001
0.758  0.015 (0.00000)
0.613  0.118 (0.00000)
0.015  0.056 (0.00000)
0.813  0.010 (0.00160)
0.293  0.009
0.201  0.032 (0.00000)
0.438  0.012 (0.00000)
0.000  0.000 (0.00000)
0.261  0.113 (0.54110)
0.000  0.000
0.364  0.119 (0.00000)
0.140  0.126 (0.00000)
0.000  0.000 (NaN)
0.161  0.177 (0.00000)
0.452  0.014
0.479  0.012 (0.00000)
0.378  0.031 (0.00000)
0.000  0.000 (0.00000)
0.430  0.056 (0.01470)
0.408  0.032
0.432  0.023 (0.00000)
0.055  0.095 (0.00000)
0.000  0.000 (0.00000)
0.373  0.080 (0.00078)
Smart Building
0.276  0.028
0.261  0.052 (0.04740)
0.135  0.156 (0.00000)
0.123  0.008 (0.00000)
0.229  0.131 (0.03530)
0.000  0.000
0.140  0.091 (0.12090)
0.157  0.188 (0.00000)
0.000  0.000 (NaN)
0.063  0.094 (0.00000)
0.451  0.019
0.435  0.016 (0.00000)
0.228  0.144 (0.00000)
0.000  0.000 (0.00000)
0.441  0.044 (0.23750)
0.443  0.023
0.485  0.018 (0.00000)
0.080  0.078 (0.00000)
0.000  0.000 (0.00000)
0.374  0.112 (0.00000)
(Change type: H, high severity; L, low severity; A, abrupt; G, gradual).
ANOVA Results and Average G-Mean from OOB and
UOB After the Change
Average G-mean
WANG ET AL.: RESAMPLING-BASED ENSEMBLE METHODS FOR ONLINE CLASS IMBALANCE LEARNING
So, we introduce the simple moving average (SMA) technique to smooth out the short-term ﬂuctuations of RðtÞ
curves. Given l values of RðtÞ, smoothed recall SRðtÞ at time
step t is deﬁned as,
SR tð Þ ¼ R tð Þ þ    þ R t l=2
Þ þ    þ R tl
SR tð Þ looks back l time steps for the smoother recall. Using
SRðtÞ, we next introduce the weighted ensemble methods of
OOB and UOB, tested on the real-world data in comparison
with individual OOB and UOB.
Weighted Ensemble of OOB and UOB
By calculating SMA of recall of both classes, we can obtain
current G-mean. It is used to determine the weights of OOB
and UOB. Their weighted ensemble, denoted as WEOB, is
expected to be both accurate and robust in dynamic environments, as it adopts the better strategy (OOB or UOB) for
different situations. Two weight adjusting strategies are
proposed and compared here. Suppose OOB has G-mean
value go and UOB has G-mean value gu at the current
moment. Let ao and au denote the weights of OOB and UOB
respectively. In the ﬁrst strategy, normalized G-mean values
of OOB and UOB are used as their weights:
In the second strategy, the weights can only be binary
values (0 or 1):
ao ¼ 1; au ¼ 0
if go  gu;
ao ¼ 0; au ¼ 1
if go < gu:
In other words, the ﬁnal prediction will solely depend on
the online model with the higher G-mean. Let’s denote these
two strategies as WEOB1 and WEOB2. WEOB1 follows the
traditional idea of ensemble. From the statistical point of
view, combining the outputs of several classiﬁers by averaging can reduce the risk of an unfortunate selection of a
poorly performing classiﬁers and thus provide stable and
accurate performance , although the averaging may or
may not beat the performance of the best classiﬁers in the
ensemble (used by WEOB2). Therefore, it is worth looking
into both strategies.
Data Description and Experimental Settings
To compare WEOB1 and WEOB2 with OOB and UOB, we
generate two data streams from real-world applications
Gearbox and Smart Building. Each data stream contains
5,000 time steps. At every 500 time steps, our data generation algorithm randomly chooses one class to be the
minority class, and ﬁxes the imbalance rate randomly
chosen from set {5, 10, 20 percent}. By doing so, there
could be an abrupt change in the class imbalance status
after every 500 examples have arrived. The detailed information of the resulting data is summarized in Table 11,
including which class belongs to the minority during
which period of time.
WEOB1 and WEOB2 combine one OOB and one UOB
composed of 50 base classiﬁers respectively. All methods
use Hoeffding tree as the base classiﬁer. l is set to 51 for calculating SR tð Þ. All the other parameter settings remain the
same. Prequential recall and G-mean are recorded at each
time step. The performance metrics are reset to 0 after every
500 time steps for a clear observation.
Experimental Analysis
Prequential recall curves are shown in Fig. 2. Each plot
compares WEOB1, WEOB2, OOB and UOB. We can see
that, during the static periods of 500 time steps between
the performance resettings, when OOB and UOB present
a signiﬁcant performance difference, WEOB1 and WEOB2
always locate in between OOB and UOB. When UOB suffers from a recall reduction caused by the class imbalance
change, WEOB1 and WEOB2 are shown to be less affected
by the change.
With respect to the overall performance, Table 12 compares the average prequential G-mean during the 500 time
steps after each status change, including its mean and
standard deviation over 100 runs, and the p-values from
Wilcoxon Sign Rank test performed between WEOB2 and
every other method. The comparative results from this
table tally with our results on recall. WEOB1 and WEOB2
are better than or locate in between single OOB and UOB
in nine out of 10 cases. On the Gearbox data, WEOB2 is
competitive with UOB, and outperforms the other two in
most cases signiﬁcantly; on the Smart Building data,
WEOB2 seems to be slightly worse than OOB, and better
than the other two in most cases. Overall, WEOB2
Data Stream Description
Class label
Time steps of being minority
1-500, 1501-2000, 3001-4500
501-1500, 2001-3000, 4501-5000
Smart Building
1-500, 1501-2000, 3001-4000
501-1500, 2001-3000, 4001-5000
Fig. 2. Prequential recall of positive (left) and negative (right) classes.
IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,
outperforms WEOB1 signiﬁcantly in six out of 10 cases, in
terms of G-mean.
Generally speaking, WEOB1 and WEOB2 successfully
combine the strength of OOB and UOB. They achieve high
performance in terms of recall and G-mean, and show good
robustness to changes in the class imbalance status. Particularly, WEOB2 shows better G-mean than WEOB1.
CONCLUSIONS
As one of the earliest works focusing on online class
imbalance learning, this paper improved and studied in
depth two ensemble learning methods OOB and UOB
using resampling and the time-decayed metric to overcome class imbalance online. Four major contributions
have been made.
First, the original OOB and UOB were improved with a
better resampling strategy, in which the sampling rate is
consistent with the imbalance degree in the data stream.
Second, they were analysed in a group of static data streams
varying in data distributions and imbalance rates. Both
oversampling in OOB and undersampling in UOB were
shown to facilitate the recognition of minority-class examples with improved minority-class recall and G-mean. They
outperformed
algorithms
RLSACP and WOS-ELM for learning imbalanced
data streams. To ﬁnd out which fundamental factors affect
the performance of OOB and UOB the most, three factors
were investigated through factorial ANOVA, which are
data distributions, imbalance rates and types of base classi-
ﬁers. All of them had signiﬁcant impacts on G-mean. The
data distribution was found to be the most inﬂuential factor.
Decision tree was a better base classiﬁer than MLP. Among
all discussed models, tree-based UOB showed the best
minority-class recall and G-mean on the static data streams.
Third, to examine the adaptivity and robustness, OOB and
UOB were studied in dynamic data streams involving class
imbalance changes with different speed and severity. The
time-decayed
class size
played an important role
responding to the change correctly and timely. However, it
was found that UOB suffers from a great performance
reduction right after the change, while OOB is more robust
against this situation. Finally, to combine the strength of
OOB and UOB, we proposed two weighted ensemble methods—WEOB1 and WEOB2. Both showed better accuracy
than OOB and better robustness than UOB. WEOB2 outperformed WEOB1 in terms of G-mean.
In the future, we would like to extend our work to multiclass cases. Second, we would like to study our methods in
data streams with concept drifts. Third, the work presented
in this paper relied heavily on computational experiments.
More theoretical studies will be given.
ACKNOWLEDGMENTS
This work was supported by two European Commission
FP7 Grants (Nos. 270428 and 257906) and an EPSRC Grant
(No. EP/J017515/1). Xin Yao was supported by a Royal
Society Wolfson Research Merit Award.