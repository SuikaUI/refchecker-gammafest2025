A Bayesian Approach for combining ensembles
of GP classiﬁers
C. De Stefano1, F. Fontanella1, G. Folino2 and A. Scotto di Freca1
1 Universit`a di Cassino
Via G. Di Biasio, 43 02043 Cassino (FR) – Italy
{destefano,fontanella,a.scotto}@unicas.it
2 ICAR-CNR Istituto di Calcolo e Reti ad Alte Prestazioni
Via P. Bucci 87036 Rende (CS)–Italy
 
Abstract. Recently, ensemble techniques have also attracted the attention of Genetic Programing (GP) researchers. The goal is to further improve GP classiﬁcation performances. Among the ensemble techniques,
also bagging and boosting have been taken into account. These techniques improve classiﬁcation accuracy by combining the responses of different classiﬁers by using a majority vote rule. However, it is really hard
to ensure that classiﬁers in the ensemble be appropriately diverse, so as
to avoid correlated errors. Our approach tries to cope with this problem, designing a framework for eﬀectively combine GP-based ensemble
by means of a Bayesian Network. The proposed system uses two diﬀerent
approaches. The ﬁrst one applies a boosting technique to a GP–based
classiﬁcation algorithm in order to generate an eﬀective decision trees ensemble. The second module uses a Bayesian network for combining the
responses provided by such ensemble and select the most appropriate
decision trees. The Bayesian network is learned by means of a speciﬁcally devised Evolutionary algorithm. Preliminary experimental results
conﬁrmed the eﬀectiveness of the proposed approach.
Introduction
In the last years, in order to further improve classiﬁcation performance, ensemble techniques have been taken into account in the Genetic Programming
(GP) ﬁeld . The GP approach uses the evolutionary computation paradigm to
evolve computer programs, according to a user-deﬁned ﬁtness function. When
dealing with classiﬁcation problems, GP–based techniques exhibited very interesting performance . In this context, the decision tree data structure is
typically adopted since it allows to eﬀectively arrange in a tree-structured plans
the set of attributes chosen for pattern representation. Successful examples of
ensemble techniques applied to GP can be found in . In , bagging and
boosting techniques have been used for evolving ensembles of decision trees. In 
a novel GP–based classiﬁcation system, named Cellular GP for data Classiﬁcation (CGPC), has been presented. Such approach, inspired by cellular automata
model, enables a ﬁne-grained parallel implementation of GP. In , an extension
of CGPC based on the use of two ensemble techniques is presented: the ﬁrst
technique is the Breiman’s bagging algorithm , while the second one is the
AdaBoost.M2 boosting algorithm by Freund and Schapire . The experimental results presented in these papers show that CGPC represents an eﬀective
classiﬁcation algorithm, whose performance have been further improved using
ensemble techniques. In this framework, it is generally agreed that a key issue is
to ensure that classiﬁers in the ensemble be appropriately diverse, so as to avoid
correlated errors . In fact, as the number of classiﬁers increases, it may happen that a correct classiﬁcation provided by some classiﬁers is overturned by the
convergence of other classiﬁers on the same wrong decision. This event is much
more likely in case of highly correlated classiﬁers and may reduce the performance obtainable with any combination strategy.Classiﬁer diversity for bagging
and boosting have been experimentally investigated in . The results have
shown that these techniques do not ensure to obtain suﬃciently diverse classi-
ﬁers. As regards boosting, in it has been observed that while at ﬁrst steps
highly diverse classiﬁers are obtained, as the boosting process proceeds classiﬁer
diversity strongly decreases.
In a previous work an attempt to solve this problem has been made by
reformulating the classiﬁer combination problem as a pattern recognition one,
in which the pattern is represented by the set of class labels provided by the
classiﬁers when classifying a sample. Following this approach, the role of the
combiner is that of estimating the conditional probability of each class, given
the set of labels provided by the classiﬁers for each sample of a training set.
In this way, it is possible to automatically derive the combining rule through
the estimation of the conditional probability of each class. It it also possible to
identify redundant classiﬁers, i.e. classiﬁers whose outputs do not inﬂuence the
output of the combiner: the behavior of such classiﬁers is very similar to that of
other classiﬁers in the ensemble and they may be eliminated without aﬀecting
the overall performance of the combiner, thus overcoming the main drawback of
the combining methods discussed above. In a Bayesian Network (BN) has
been used to automatically infer the joint probability distributions between the
outputs of the classiﬁers and the classes. The BN learning has been performed
by means of an evolutionary algorithm using a direct encoding scheme of the BN
structure. Such encoding scheme is based on a speciﬁcally devised data structure,
called Multilist, which allows an easy and eﬀective implementation of the genetic
operators.
In this paper we present a new classiﬁcation system that merges the two
aforementioned approaches. We have combined the BoostCGPC algorithm ,
which produces a high performing ensemble of decision tree classiﬁers, with the
BN based approach to classiﬁer combination. Our system tries to exploit the
advantages provided by both techniques and allows to identify the minimum
number of independent classiﬁers able to recognize the data at hand.
In order to assess the eﬀectiveness of the proposed system, several experiments have been performed. More speciﬁcally, four data sets, having diﬀerent sizes, number of attributes and classes have been considered. The results
have been compared with those obtained by the BoostCGPC approach using a
weighted majority vote combining rule.
The remainder of the paper is organized as follows. In Section 2 the BN based
combining technique is presented. In Section 3 the architecture of the proposed
system is described. In Section 4 the experimental results are illustrated, while
discussion and some concluding remarks are eventually left to Section 5.
Bayesian Networks for Combining Classiﬁers
The problem of combining the responses provided by a set of classiﬁers can be
also faced considering the joint probability p (c, e1, ..., eL), where ei represents the
response of the i–th classiﬁer and c is the class to be assigned to the sample taken
into account. The problem of computing the joint probability p (c, e1, ..., eL) may
be eﬀectively solved by using a Bayesian Network (BN). In particular, in , a
BN has been used for for combining the responses of more classiﬁers in a multi
expert system.
A BN is a probabilistic graphical model that allows the representation of
a joint probability distribution of a set of random variables through a Direct
Acyclic Graph (DAG). The nodes of the graph correspond to variables, while
the arcs characterize the statistical dependencies among them. An arrow from
a node i to a node j has the meaning that j is conditionally dependent on i,
and we can refer to i as a parent of j. For each node, a conditional probability
quantiﬁes the eﬀect that the parents have on that node. Once the statistical
dependencies among variables have been estimated and encoded in the DAG
structure, each node ei is associated with a conditional probability function
exhibiting the following property:
p( ei | paei, ndei) = p( ei | paei )
where paei indicates the set of nodes which are parents of node ei, and ndei
indicates all the remaining nodes. This property allows the description of the
joint probability of a set of variables {c, e1, . . . , eL} as follows:
p (c, e1, . . . , eL) = p ( c | pac )
p ( ei | paei )
It is worth noticing that the node c may be parent of one or more nodes of
the DAG. Therefore, it may be useful to divide the ei nodes of the DAG in two
groups: the ﬁrst one, denoted as Ec, contains the nodes having the node c among
their parents, and the second one, denoted as Ec, the remaining ones. With this
assumption, Eq. (2) can be rewritten as:
p (c, e1, . . . , eL) = p ( c | pac )
p ( ei | paei )
p ( ei | paei )
Fig. 1. An example of a BN. The DAG structure induces the factorization of the joint
probability p(c, e1, e2, e3, e4, e5) = p(e3|c)p(c|e4, e5)p(e5|e1, e2)p(e1)p(e2)p(e4). In this
case Ec = {e3}, Ec = {e1, e2, e4, e5} and hence bc = arg max
c∈C p(e3|c)p(c|e4, e5).
Since the third term in Eq. (3) does not depend on c, Eq. (6) assumes the form:
bc = arg max
c∈C p(c, e1, . . . , eL) = arg max
c∈C p ( c | pac )
p ( ei | paei )
For instance, the BN reported in Fig. 1 considers only the responses of the experts e3, e4 and e5, while the experts e1 and e2 are not taken into account. Thus,
this approach allows to detect a reduced set of relevant experts, namely the ones
connected to node c, whose responses are actually used by the combiner to provide the ﬁnal output, while the set Ec of experts, which do not add information
to the choice of bc, are discarded.
Using a BN for combining the responses of more classiﬁers requires that
both the network structure, which determines the statistical dependencies among
variables, and the parameters of the probability distributions be learned from
a training set of examples. The structural learning, is aimed at capturing the
relation between the variables, and hence the structure of the DAG. It can be
seen as an optimization problem which requires the deﬁnition of a search strategy in the space of graph structures, and a scoring function for evaluating the
eﬀectiveness of candidate solutions. A typical scoring functions is the posterior
probability of the structure given the training data. More formally, if D and
Sh denote the training set and the structure of a candidate BN, respectively,
the scoring function to be maximized is the likelihood of D given the structure
Sh. Once the DAG structure Sh has been determined, the parameters of the
conditional probability distributions are computed from training data.
The exhaustive search of the BN structure which maximizes the scoring function is a NP-hard problem in the number of variables. This is the reason why
standard algorithms search for suboptimal solutions by maximizing at each step
a local scoring function which takes into account only the local topology of the
DAG. Moving from these considerations, we have proposed an alternative approach in which the structure of the BN is learned by means of an Evolutionary
Fig. 2. A multilist (right) and the encoded DAG’s(left).
algorithm, using a direct encoding scheme. The algorithm is based on a specifically devised data structure for encoding DAG, called multilist (ML), which
consists of two basic lists. The ﬁrst one, called main list, contains all the nodes
of the DAG arranged in such a way that source nodes occupy the ﬁrst positions,
and sink node, the last ones. Moreover, nodes having both incoming and outgoing arcs are inserted in the main list after their parents. To each node of the
main list is associated a second list called sublist, representing the outgoing connections among that node and the other nodes in the DAG. More speciﬁcally, if
si is the sublist associated to the i−th element of the main list, then it contains
information about the outgoing arcs possibly connecting the i −th element and
the other elements following it in the main list, ordered according to the position
of such elements. Since an arc may be present or not, each element of a sublist
contains a binary information: 1 if the arc exists, 0 otherwise (see ﬁgure 2).
This ML data structure allows an easy and eﬀective implementation of genetic
operators. Moreover, since the above deﬁnition ensures that a ML intrinsically
represents a DAG structure, the application of such operators always produces
valid oﬀspring.
As regards the genetic operators, we have deﬁned two mutation operators
which can modify a ML in two diﬀerent ways. The m mutation changes a ML
by swapping two elements of the main list, whereas the s mutation adds and/or
deletes one or more arcs in a sub list.
The m–mutation performs a permutation on the elements of the main list, but
leaves unchanged the connection topology of the ML. This mutation consists of
two steps:
(i) randomly pick two elements in the main list and swap their positions.
(ii) modify sublist elements in such a way to restore the connection topology as
it was before the step (i).
It is worth noticing that the m–mutation generates a new ordering of the variables, which modiﬁes the directions of the existing arcs in the DAG, but preserves
dependencies between variables. If we consider the DAG in ﬁgure 2, for instance,
the swap between the second and the fourth node in the main list changes only
the directions of the arcs connecting the couples of nodes (1, 5) and (5, 2). This
operator is applied according to a predeﬁned probability value pm.
The s–mutation, instead, modiﬁes the values of the sublist elements. For each
element of the sublists, ps represents the probability of changing its value from
0 to 1, or vice versa. Thus the eﬀect of this operator is that of adding or deleting arcs in the DAG. Such an operation is applied with probability ps. Further
details about ML data structure and the genetic operators can be found in .
The evolutionary algorithm starts by randomly generating an initial population of P individuals. Afterward, the ﬁtness of each individual is evaluated
by computing the scoring function. At each generation, the best e individuals
are selected and copied in the new population in order to implement an elitist
strategy. Then, the tournament is used to select (P −e) individuals and the m
and s mutation operators are applied to each selected individual according to
the probabilities pm and ps, respectively. Finally these individuals are added to
the new population. This process is repeated for ng generations.
System Architecture
The proposed system consists of two main modules: the ﬁrst one builds an ensemble of decision tree classiﬁers (experts); the second one implements the combining rule that produces the ﬁnal classiﬁcation result of the whole system.
The ﬁrst module, called BoostCGPC, builds decision tree using a Genetic
Programming (GP) technique , which is an evolutionary computation-based
technique able to evolve computer programs according to a user-deﬁned ﬁtness
function. The output ensemble is learned by implementing a modiﬁed version
of the algorithm AdaBoost.M2 . Such an implementation allows to run the
algorithm on distributed memory parallel computer, making the system able to
deal with large data sets. Further details about this algorithm can be found in
The second module uses the approach described in the previous section for
combining the responses provided by the classiﬁers making up the ensemble built
in the ﬁrst module. More speciﬁcally, let us denote with N the number of classes
to be discriminated, with L the number of decision tree classiﬁers included the
ensemble and with E = {e1, . . . , eL} the set of responses provided by such classiﬁers for a given input sample. Let us assume that such responses constitute the
input to the combiner module. In this module, the combining technique operates
as a “higher level” classiﬁer, working on a L-dimensional discrete-valued feature
space, which is trained by using a supervised learning procedure. This procedure requires to observe both the “true” class label c, and the set of responses
provided by the classiﬁers for each sample of a training set, in order to estimate
the conditional probability p(c|e1, . . . , eL). Once this conditional probability has
been learned, the combiner evaluates the most probable class bc of an unknown
input sample, given the expert observations, as follows:
bc = arg max
c∈C p (c|e1, ..., eL)
where C is the set of classes. Considering the deﬁnition of conditional probability
and omitting the terms not depending on the variable c to be maximized, Eq.
(5) can be rewritten as:
bc = arg max
c∈C p (c, e1, ..., eL).
that involves only the joint probabilities p (c, e1, ..., eL). This problem may be effectively solved by using a Bayesian Network, according to the approach outlined
in the previous section.
Note that the devised system recognizes unknown samples using a two–step
procedure: (i) the feature values describing the unknown sample are provided to
each of the ensemble classiﬁers built by the BoostCGPC module; (ii) the set of
responses produced is given in input to the BN module. Such module labels the
sample with the most likely class, among those of the problem at hand, given
the responses collected by the ﬁrst module 3. Note that, for some samples, the
BN is not able to assign them a label. This case occurs when two or even more
classes are equally likely. In this case, the unknown sample is labeled using the
majority vote rule, applied to the ﬁrst module responses.
Experimental Results
The proposed approach has been tested on four data sets: Census, Segment,
Adult and Phoneme. The size and class distribution of these data sets are described in Table 1. They present diﬀerent characteristics in the number and type
(continuous and nominal) of attributes, two classes versus multiple classes and
number of samples. In particular, Census and Adult, are real large data set containing census data collected by the U.S. Census Bureau. The Segment contains
image data. Finally, the Phoneme data set contains data distinguishing between
nasal and oral vowels. For each data set, two statistically independent sets of
equal size, have been built randomly splitting the samples of each class. The ﬁrst
set has been used for training, while the second set for the test.
All the experiments were performed on a Linux cluster with 16 Itanium2
1.4GHz nodes each having 2 GBytes of main memory and connected by a Myrinet
3 Note that the second step does not require any further computation with respect to
the Majority Voting rule. In fact, it only needs to read tables storing class probabilities.
Table 1. The data sets used in the experiments
datasets attr. samples classes
high performance network. As regards the boostGCPC algorithm, it has been run
on ﬁve nodes, using standard GP parameters and a population of 100 individuals
for node. The original training set has been partitioned among the nodes and
respectively 5 and 10 rounds of boosting, with 100 generations for round, have
been used to produce respectively 25 and 50 classiﬁers on 5 nodes. It is worth
to remember the algorithm produce a diﬀerent classiﬁer for each round on each
All results were obtained by averaging over 30 runs. For each run of the
BoostCGPC module, a run of the BN module has been carried out. Each BN run
has been performed by using the responses, on the whole training set, provided
by the classiﬁers learned in the corresponding BoostCGPC run. The results on
the test set has been obtained by ﬁrst submitting each sample to the learned
decision trees ensemble. The ensemble responses have been then provided to the
learned BN. Finally, the BN output label has been compared with the true one
of that sample.
The results achieved by the our approach (hereafter BN-BoostCGPC) have
been compared with those obtained by BoostCGPC approach, which uses the
Weighted Majority rule for combining the ensemble responses. The comparison
results are shown in Tab. 2. The second column shows the ensembles (25 or
50 classiﬁers), while the columns 3 and 6 shows the training set errors of the
BoostCGPC and BN-BoostCGPC, respectively. Similarly, the columns 4 and 7
show the test set errors of the BoostCGPC and BN-BoostCGPC, respectively.
The columns 5 and 8 contain the number of classiﬁers actually used by both
approaches. It is worth noticing that for the BoostCGPC approach such number
equals the number of classiﬁer making up the ensemble (25 or 50). The BN-
BoostCGPC, instead, uses only the classiﬁers that are directly connected to the
output node in the DAG.
In order to statistically validate the comparison results, we performed the two–
tailed t–test(α = 0.05) over the 30 carried out runs. The values in bold in the test
set error columns highlight, for each ensemble, the results which are signiﬁcantly
better according to the two–tailed t–test. The proposed approach achieves better
performance on the majority of the considered ensembles while, in the remaining
cases, the performance are comparable. It is also worth noticing that the most
signiﬁcant improvements have been obtained on Adult and Census data sets,
which are the largest ones among those considered. This result is due to the
fact that larger data sets allow the BN learning process to better estimate the
conditional probabilities to be modeled. Finally, is worth to remark that the
results of our system are always achieved by using only a small number of the
available classiﬁers.
Conclusions and Future work
We presented a novel approach for improving the performance of derivation tree
ensembles, learned by means of a boosted GP algorithm. The approach consists
of two modules, the ﬁrst one uses a boosted GP algorithm to generate ensembles
Table 2. Comparison results.
Datasets ens.
BN-BoostCGPC
Train Test # sel. Train Test
15.90 16.94
15.85 16.28
16.88 18.23
16.55 16.99
11.82 12.69
10.82 11.68
10.39 12.06
10.34 11.99
Phoneme 25
16.41 18.87
17.70 19.23
16.90 20.04
17.23 19.51
of decision trees. The second module, instead, employs Bayesian networks to
eﬀectively combine the responses provided by the ensemble decision trees.
The experimental results have shown that the proposed system further improves the performance achieved by using the boosted GP algorithm. Moreover,
such performances are obtained by using a reduced number of classiﬁers. Finally, the presented approach seems to be particularly suited to deal with very
large data sets. Future work will include testing on several ensembles, having a
diﬀerent number of classiﬁers. Furthermore, larger data sets will be taken into
account, to further investigate the capability of the presented system to deal
with very large data sets.