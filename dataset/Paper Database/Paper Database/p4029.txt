Proceedings of NAACL-HLT 2016, pages 300–309,
San Diego, California, June 12-17, 2016. c⃝2016 Association for Computational Linguistics
Joint Event Extraction via Recurrent Neural Networks
Thien Huu Nguyen, Kyunghyun Cho and Ralph Grishman
Computer Science Department, New York University, New York, NY 10003, USA
 , , 
Event extraction is a particularly challenging
problem in information extraction. The stateof-the-art models for this problem have either applied convolutional neural networks in
a pipelined framework or
followed the joint architecture via structured
prediction with rich local and global features
 . The former is able to learn
hidden feature representations automatically
from data based on the continuous and generalized representations of words. The latter,
on the other hand, is capable of mitigating the
error propagation problem of the pipelined approach and exploiting the inter-dependencies
between event triggers and argument roles via
discrete structures. In this work, we propose
to do event extraction in a joint framework
with bidirectional recurrent neural networks,
thereby beneﬁting from the advantages of the
two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for
the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.
Introduction
We address the problem of event extraction (EE):
identifying event triggers of speciﬁed types and their
arguments in text. Triggers are often single verbs
or normalizations that evoke some events of interest
while arguments are the entities participating into
such events.
This is an important and challenging task of information extraction in natural language processing (NLP), as the same event might
be present in various expressions, and an expression
might expresses different events in different contexts.
There are two main approaches to EE: (i) the joint
approach that predicts event triggers and arguments
for sentences simultaneously as a structured prediction problem, and (ii) the pipelined approach that
ﬁrst performs trigger prediction and then identiﬁes
arguments in separate stages.
The most successful joint system for EE is based on the structured perceptron algorithm with a large set of local and global features1. These features are designed to capture the
discrete structures that are intuitively helpful for EE
using the NLP toolkits (e.g., part of speech tags, dependency and constituent tags). The advantages of
such a joint system are twofold: (i) mitigating the error propagation from the upstream component (trigger identiﬁcation) to the downstream classiﬁer (argument identiﬁcation), and (ii) beneﬁting from the
the inter-dependencies among event triggers and argument roles via global features. For example, consider the following sentence ) in the ACE 2005 dataset:
In Baghdad, a cameraman died when an American tank ﬁred on the Palestine hotel.
In this sentence, died and ﬁred are the event triggers for the events of types Die and Attack, respectively. In the pipelined approach, it is often simple
for the argument classiﬁers to realize that camera-
1Local features encapsulate the characteristics for the individual tasks (i.e, trigger and argument role labeling) while
global features target the dependencies between triggers and arguments and are only available in the joint approach.
man is the Target argument of the Die event due to
the proximity between cameraman and died in the
sentence. However, as cameraman is far away from
ﬁred, the argument classiﬁers in the pipelined approach might fail to recognize cameraman as the
Target argument for the event Attack with their local features. The joint approach can overcome this
issue by relying on the global features to encode the
fact that a Victim argument for the Die event is often
the Target argument for the Attack event in the same
Despite the advantages presented above, the joint
system by Li et al. suffers from the lack of
generalization over the unseen words/features and
the inability to extract the underlying structures for
EE (due to its discrete representation from the handcrafted feature set) .
The most successful pipelined system for EE to
date addresses these drawbacks
of the joint system by Li et al.
 via dynamic multi-pooling convolutional neural networks
(DMCNN). In this system, words are represented by
the continuous representations and features are automatically learnt from data by the DM-
CNN, thereby alleviating the unseen word/feature
problem and extracting more effective features for
the given dataset. However, as the system by Chen
et al. is pipelined, it still suffers from the
inherent limitations of error propagation and failure
to exploit the inter-dependencies between event triggers and argument roles . Finally, we
notice that the discrete features, shown to be helpful
in the previous studies for EE , are
not considered in Chen et al. .
Guided by these characteristics of the EE systems by Li et al. and Chen et al. ,
in this work, we propose to solve the EE problem
with the joint approach via recurrent neural networks (RNNs) augmented with the discrete features, thus inheriting all the beneﬁts from both systems as well as overcoming their inherent issues. To
the best of our knowledge, this is the ﬁrst work to
employ neural networks to do joint EE.
Our model involves two RNNs that run over the
sentences in both forward and reverse directions to
learn a richer representation for the sentences. This
representation is then utilized to predict event triggers and argument roles jointly. In order to capture
the inter-dependencies between triggers and argument roles, we introduce memory vectors/matrices
to store the prediction information during the course
of labeling the sentences.
We systematically explore various memory vector/matrices as well as different methods to learn
word representations for the joint model. The experimental results show that our system achieves
the state-of-the-art performance on the widely used
ACE 2005 dataset.
Event Extraction Task
We focus on the EE task of the Automatic Context
Extraction (ACE) evaluation2. ACE deﬁnes an event
as something that happens or leads to some change
of state. We employ the following terminology:
• Event mention: a phrase or sentence in which
an event occurs, including one trigger and an
arbitrary number of arguments.
• Event trigger: the main word that most clearly
expresses an event occurrence.
• Event argument: an entity mention, temporal
expression or value (e.g. Job-Title) that servers
as a participant or attribute with a speciﬁc role
in an event mention.
ACE annotates 8 types and 33 subtypes (e.g., Attack, Die, Start-Position) for event mentions that
also correspond to the types and subtypes of the
event triggers. Each event subtype has its own set
of roles to be ﬁlled by the event arguments. For instance, the roles for the Die event include Place, Victim and Time. The total number of roles for all the
event subtypes is 36.
Given an English text document, an event extraction system needs to recognize event triggers with
speciﬁc subtypes and their corresponding arguments
with the roles for each sentence. Following the previous work , we assume that the argument candidates (i.e, the entity mentions, temporal
expressions and values) are provided (by the ACE
annotation) to the event extraction systems.
2 
We formalize the EE task as follow.
w1w2 . . . wn be a sentence where n is the sentence
length and wi is the i-th token.
Also, let E =
e1, e2, . . . , ek be the entity mentions3 in this sentence (k is the number of the entity mentions and can
be zero). Each entity mention comes with the offsets
of the head and the entity type. We further assume
that i1, i2, . . . , ik be the indexes of the last words of
the mention heads for e1, e2, . . . , ek, respectively.
In EE, for every token wi in the sentence, we need
to predict the event subtype (if any) for it. If wi is a
trigger word for some event of interest, we then need
to predict the roles (if any) that each entity mention
ej plays in such event.
The joint model for event extraction in this work
consists of two phases: (i) the encoding phase that
applies recurrent neural networks to induce a more
abstract representation of the sentence, and (ii) the
prediction phase that uses the new representation
to perform event trigger and argument role identi-
ﬁcation simultaneously for W. Figure 1 shows an
overview of the model.
Sentence Encoding
In the encoding phase, we ﬁrst transform each token wi into a real-valued vector xi using the concatenation of the following three vectors:
1. The word embedding vector of wi: This is obtained by looking up a pre-trained word embedding
table D .
2. The real-valued embedding vector for the entity type of wi: This vector is motivated from the
prior work and generated by looking up the entity type embedding table
(initialized randomly) for the entity type of wi. Note
that we also employ the BIO annotation schema to
assign entity type labels to each token in the sentences using the heads of the entity mentions as do
Nguyen and Grishman .
The binary vector whose dimensions correspond to the possible relations between words in the
dependency trees. The value at each dimension of
3From now on, when mentioning entity mentions, we always refer to the ACE entity mentions, times and values.
this vector is set to 1 only if there exists one edge
of the corresponding relation connected to wi in the
dependency tree of W. This vector represents the
dependency features that are shown to be helpful in
the previous research .
Note that we do not use the relative position features, unlike the prior work on neural networks for
EE . The reason is we predict the whole sentences
for triggers and argument roles jointly, thus having
no ﬁxed positions for anchoring in the sentences.
The transformation from the token wi to the
vector xi essentially converts the input sentence
W into a sequence of real-valued vectors X
(x1, x2, . . . , xn), to be used by recurrent neural networks to learn a more effective representation.
Recurrent Neural Networks
(x1, x2, . . . , xn).
At each step i, we compute
the hidden vector αi based on the current input
vector xi and the previous hidden vector αi−1,
non-linear
transformation
= Φ(xi, αi−1).
This recurrent computation is done over X to generate the hidden
(α1, α2, . . . , αn),
RNN(x1, x2, . . . , xn) = (α1, α2, . . . , αn).
An important characteristics of the recurrent
mechanism is that it adaptively accumulates the
context information from position 1 to i into the
hidden vector αi, making αi a rich representation.
However, αi is not sufﬁcient for the event
trigger and argument predictions at position i as
such predictions might need to rely on the context information in the future (i.e, from position i
In order to address this issue, we run a
second RNN in the reverse direction from Xn to
X1 to generate the second hidden vector sequence:
RNN(xn, xn−1, . . . , x1) = (α′
n−1, . . . , α′
i summarizes the context information from
position n to i.
Eventually, we obtain the new
representation (h1, h2, . . . , hn) for X by concatenating the hidden vectors in (α1, α2, . . . , αn) and
n−1, . . . , α′
1): hi = [αi, α′
i]. Note that hi essentially encapsulates the context information over
the whole sentence (from 1 to n) with a greater focus on position i.
Regarding the non-linear function, the simplest
Prediction
Prediction
Vectors/Matrices
embeddings
entity type
embeddings
depdendecy
tree relations
input sentence
indexes of trigger
and entity mention
candidates
local argument
feature generator
 
memory matrices
word embedding
feature representations
prediction outputs
Bidirectional
local context
vector extraction
entity mention "man"
entity mention "Baghdad"
Figure 1: The joint EE model for the input sentence “a man died when a tank ﬁred in Baghdad” with local context
window d = 1. We only demonstrate the memory matrices Garg/trg
in this ﬁgure. Green corresponds to the trigger
candidate “died” at the current step while violet and red are for the entity mentions “man” and “Baghdad” respectively.
form of Φ in the literature considers it as a one-layer
feed-forward neural network.
Unfortunately, this
function is prone to the “vanishing gradient” problem , making it challenging to
train RNNs properly. This problem can be alleviated
by long-short term memory units (LSTM) . In this
work, we use a variant of LSTM; called the Gated
Recurrent Units (GRU) from Cho et al.
GRU has been shown to achieve comparable performance .
Prediction
In order to jointly predict triggers and argument
roles for W, we maintain a binary memory vector
for triggers, and binary memory matrices Garg
and Garg/trg
for arguments (at each time i). These
vector/matrices are set to zeros initially (i = 0) and
updated during the prediction process for W.
bidirectional
representation
h1, h2, . . . , hn in the encoding phase and the
initialized memory vector/matrices, the joint prediction procedure loops over n tokens in the sentence
(from 1 to n). At each time step i, we perform the
following three stages in order:
(i) trigger prediction for wi.
(ii) argument role prediction for all the entity mentions e1, e2, . . . , ek with respect to the current
(iii) compute Gtrg
and Garg/trg
for the current step using the previous memory vector/matrices Gtrg
i−1 and Garg/trg
i−1 , and the
prediction output in the earlier stages.
The output of this process would be the predicted trigger subtype ti for wi, the predicted argument roles ai1, ai2, . . . , aik and the memory vector/matrices Gtrg
and Garg/trg
for the current
step. Note that ti should be the event subtype if wi is
a trigger word for some event of interest, or “Other”
in the other cases. aij, in constrast, should be the
argument role of the entity mention ej with respect
to wi if wi is a trigger word and ej is an argument
of the corresponding event, otherwise aij is set to
“Other” (j = 1 to k).
Trigger Prediction
In the trigger prediction stage for the current token wi, we ﬁrst compute the feature representation
vector Rtrg
for wi using the concatenation of the following three vectors:
• hi: the hidden vector to encapsulate the global
context of the input sentence.
i : the local context vector for wi. Ltrg
generated by concatenating the vectors of the
words in a context window d of wi:
= [D[wi−d], . . . , D[wi], . . . , D[wi+d]].
i−1: the memory vector from the previous
The representation vector Rtrg
= [hi, Ltrg
is then fed into a feed-forward neural network F trg
with a softmax layer in the end to compute the probability distribution P trg
i;t over the possible trigger subtypes: P trg
i;t = P trg
i (l = t) = F trg
i ) where t is
a trigger subtype. Finally, we compute the predicted
type ti for wi by: ti = argmaxt . Bij is then computed by feeding Vij
into a feed-forward neural network F binary for
further abstraction: Bij = F binary(Vij).
i−1[j] and Garg/trg
i−1 [j]: the memory vectors for
ej that are extracted out of the memory matrices Garg
i−1 and Garg/trg
from the previous step.
feedforward
softmax layer in the end to transform Rarg
[hi, hij, Larg
ij , Bij, Garg
i−1[j], Garg/trg
i−1 [j]] into the probability distribution P trg
ij;a over the possible argument
roles: P arg
ij;a = P arg
ij (l = a) = F arg
ij ) where
a is an argument role.
Eventually, the predicted
argument role for wi and ej is aij = argmaxa . These
features include the shortest dependency paths, the
entity types, subtypes, etc.
The Memory Vector/Matrices
An important characteristics of EE is the existence of the dependencies between trigger labels and
argument roles within the same sentences (Li et al.,
In this work, we encode these dependencies into the memory vectors/matrices Gtrg
(i = 0 to n) and use them as features in
the trigger and argument prediction explicitly (as
shown in the representation vectors Rtrg
above). We classify the dependencies into the following three categories:
1. The dependencies among trigger subtypes:
are captured by the memory vectors Gtrg
{0, 1}nT for i = 0, . . . , n, and nT is the number
of the possible trigger subtypes). At time i, Gtrg
indicates which event subtypes have been recognized
before i. We obtain Gtrg
i−1 and the trigger
prediction output ti at time i: Gtrg
i [t] = 1 if t = ti
i−1[t] otherwise.
A motivation for such dependencies is that if a
Die event appears somewhere in the sentences, the
possibility for the later occurrence of an Attack event
would be likely.
The dependencies among argument roles:
are encoded by the memory matrix Garg
{0, 1}k×nA for i = 0, . . . , n, and nA is the number
of the possible argument roles). At time i, Garg
summarizes the argument roles that the entity mentions
has played with some event in the past. In particular,
i [j][a] = 1 if and only if ej has the role of a with
some event before time i. Garg
is computed from
i−1, and the prediction outputs ti and ai1, . . . , aik
at time i: Garg
i [j][a] = 1 if ti ̸= “Other” and a = aij,
i−1[j][a] otherwise (for j = 1 to k).
3. The dependencies between argument roles
and trigger subtypes: are encoded by the memory
matrix Garg/trg
∈{0, 1}k×nT for i = 0 to n).
At time i, Garg/trg
speciﬁes which entity mentions
have been identiﬁed as arguments for which event
subtypes before. In particular, Garg/trg
[j][t] = 1 if and
only if ej has been detected as an argument for some
event of subtype t before i. Garg/trg
is computed from
and the trigger prediction output ti at time i:
[j][t] = 1 if ti ̸= “Other” and t = ti, and
i−1 [j][t] otherwise (for all j = 1 to k).
Denote the given trigger subtypes and argument
roles for W in the training time as T = t∗
2, . . . , t∗
and A = (a∗
i=1,n. We train the network by minimizing the joint negative log-likelihood function C
for triggers and argument roles:
C(T, A, X, E) = −log P(T, A|X, E)
= −log P(T|X, E) −log P(A|T, X, E)
I(ti ̸= “Other”)
where I is the indicator function.
We apply the stochastic gradient descent algorithm with mini-batches and the AdaDelta update
rule . The gradients are computed using back-propagation. During training, besides the
weight matrices, we also optimize the word and entity type embedding tables to achieve the optimal
states. Finally, we rescale the weights whose Frobenius norms exceed a hyperparameter .
Word Representation
Following the prior work , we pre-train word embeddings from a large corpus and employ them to
initialize the word embedding table.
One of the
models to train word embeddings have been proposed in Mikolov et al. that introduce two log-linear models, i.e the continuous bagof-words model (CBOW) and the continuous skipgram model (SKIP-GRAM). The CBOW model attempts to predict the current word based on the average of the context word vectors while the SKIP-
GRAM model aims to predict the surrounding words
in a sentence given the current word. In this work,
besides the CBOW and SKIP-GRAM models, we
examine a concatenation-based variant of CBOW
(C-CBOW) to train word embeddings and compare
the three models to understand their effectiveness for
EE. The objective of C-CBOW is to predict the target word using the concatenation of the vectors of
the words surrounding it.
Experiments
Resources, Parameters and Dataset
For all the experiments below, in the encoding
phase, we use 50 dimensions for the entity type embeddings, 300 dimensions for the word embeddings
and 300 units in the hidden layers for the RNNs.
Regarding the prediction phase, we employ the
context window of 2 for the local features, and the
feed-forward neural networks with one hidden layer
for F trg, F arg and F binary (the size of the hidden layers are 600, 600 and 300 respectively).
Finally, for training, we use the mini-batch size =
50 and the parameter for the Frobenius norms = 3.
These parameter values are either inherited from
the prior research or selected according to the validation data.
We pre-train the word embeddings from the
English Gigaword corpus utilizing the word2vec
toolkit4 (modiﬁed to add the C-CBOW model). Following Baroni et al. , we employ the context
window of 5, the subsampling of the frequent words
set to 1e-05 and 10 negative samples.
We evaluate the model with the ACE 2005 corpus.
For the purpose of comparison, we use the same data
split as the previous work . This data
split includes 40 newswire articles (672 sentences)
for the test set, 30 other documents (836 sentences)
for the development set and 529 remaining documents (14,849 sentences) for the training set. Also,
4 
we follow the criteria of the previous work to judge the correctness
of the predicted event mentions.
Memory Vector/Matrices
This section evaluates the effectiveness of the memory vector and matrices presented in Section 3.2.3.
In particular, we test the joint model on different
cases where the memory vector for triggers Gtrg and
the memory matrices for arguments Garg/trg and Garg
are included or excluded from the model. As there
are 4 different ways to combine Garg/trg and Garg for
argument labeling and two options to to employ Gtrg
or not for trigger labeling, we have 8 systems for
comparison in total. Table 1 reports the identiﬁcation and classiﬁcation performance (F1 scores) for
triggers and argument roles on the development set.
Note that we are using the word embeddings trained
with the C-CBOW technique in this section.
Garg/trg+Garg
Table 1: Performance of the Memory Vector/Matrices
on the development set. No means not using the memory
vector/matrices.
We observe that the memory vector Gtrg is not
helpful for the joint model as it worsens both trigger and argument role performance (considering the
same choice of the memory matrices Garg/trg and
Garg (i.e, the same row in the table) and except in
the row with Garg/trg + Garg).
The clearest trend is that Garg/trg is very effective
in improving the performance of argument labeling.
This is true in both the inclusion and exclusion of
Gtrg. Garg and its combination with Garg/trg, on the
other hand, have negative effect on this task. Finally,
Garg/trg and Garg do not contribute much to the trigger labeling performance in general (except in the
case where Gt, Garg/trg and Garg are all applied).
These observations suggest that the dependencies
among trigger subtypes and among argument roles
are not strong enough to be helpful for the joint
model in this dataset. This is in contrast to the dependencies between argument roles and trigger subtypes that improve the joint model signiﬁcantly.
The best system corresponds to the application of
the memory matrix Garg/trg and will be used in all the
experiments below.
Word Embedding Evaluation
We investigate different techniques to obtain the pretrained word embeddings for initialization in the
joint model of EE. Table 2 presents the performance
(for both triggers and argument roles) on the development set when the CBOW, SKIP-GRAM and C-
CBOW techniques are utilized to obtain word embeddings from the same corpus. We also report the
performance of the joint model when it is initialized
with the Word2Vec word embeddings from Mikolov
et al. (trained with the Skip-gram
model on Google News) (WORD2VEC). Finally,
for comparison, the performance of the random
word embeddings (RANDOM) is also included. All
of these word embeddings are updated during the
training of the model.
Word Embeddings
Table 2: Performance of the Word Embedding Techniques.
The ﬁrst observation from the table is that RAN-
DOM is not good enough to initialize the word embeddings for joint EE and we need to borrow some
pre-trained word embeddings for this purpose. Second, SKIP-GRAM, WORD2VEC and CBOW have
comparable performance on trigger labeling while
the argument labeling performance of SKIP-GRAM
and WORD2VEC is much better than that of CBOW
for the joint EE model. Third and most importantly,
among the compared word embeddings, it is clear
that C-CBOW signiﬁcantly outperforms all the others. We believe that the better performance of C-
CBOW stems from its concatenation of the multiple context word vectors, thus providing more information to learn better word embeddings than SKIP-
GRAM and WORD2VEC. In addition, the concate-
Trigger Identiﬁcation
Identiﬁcation (%)
+ Classiﬁcation (%)
Identiﬁcation (%)
Li’s basline
Liao’s cross-event†
Hong’s cross-entity†
Li’s structure
Table 3: Overall Performance on the Blind Test Data. “†” designates the systems that employ the evidences beyond
sentence level.
nation mechanism essentially helps to assign different weights to different context words, thereby being more ﬂexible than CBOW that applies a single
weight for all the context words.
From now on, for consistency, C-CBOW would
be utilized in all the following experiments.
Comparison to the State of the art
The state-of-the-art systems for EE on the ACE 2005
dataset have been the pipelined system with dynamic multi-pooling convolutional neural networks
by Chen et al. (DMCNN) and the joint system with structured prediction and various discrete
local and global features by Li et al. (Li’s
structure).
Note that the pipelined system in Chen et al.
 is also the best-reported system based on
neural networks for EE. Table 3 compares these
state-of-the-art systems with the joint RNN-based
model in this work (denoted by JRNN). For completeness, we also report the performance of the following representative systems:
1) Li’s baseline: This is the pipelined system with
local features by Li et al. .
2) Liao’s cross event: is the system by Liao and
Grishman with the document-level information.
3) Hong’s cross-entity : This
system exploits the cross-entity inference, and is
also the best-reported pipelined system with discrete
features in the literature.
From the table, we see that JRNN achieves the
best F1 scores (for both trigger and argument labeling) among all of the compared models.
is signiﬁcant with the argument role labeling performance (an
improvement of 1.8% and 2.7% for trigger and argument role labeling respectively), we can conﬁrm
the effectiveness of RNNs to learn effective feature
representations for EE.
Sentences with Multiple Events
In order to further prove the effectiveness of JRNN,
especially for those sentences with multiple events,
we divide the test data into two parts according to
the number of events in the sentences (i.e, single
event and multiple events) and evaluate the performance separately, following Chen et al. . Table 4 shows the performance (F1 scores) of JRNN,
DMCNN and two other baseline systems, named
Embeddings+T and CNN in Chen et al. .
Embeddings+T uses word embeddings and the traditional sentence-level features in 
while CNN is similar to DMCNN, except that it applies the standard pooling mechanism instead of the
dynamic multi-pooling method .
The most important observation from the table is
that JRNN signiﬁcantly outperforms all the other
methods with large margins when the input sentences contain more than one events (i.e, the row labeled with 1/N in the table). In particular, JRNN
is 13.9% better than DMCNN on trigger labeling
while the corresponding improvement for argument
role labeling is 6.5%, thereby further suggesting the
beneﬁt of JRNN with the memory features. Regard-
Embedding+T
Embedding+T
Table 4: System Performance on Single Event Sentences
(1/1) and Multiple Event Sentences (1/N).
ing the performance on the single event sentences,
JRNN is still the best system on trigger labeling although it is worse than DMCNN on argument role
labeling. This can be partly explained by the fact
that DMCNN includes the position embedding features for arguments and the memory matrix Garg/trg
in JRNN is not functioning in this single event case.
Related Work
Early research on event extraction has primarily focused on local sentence-level representations in a
pipelined architecture . After that, higher level features has been investigated to improve the performance . Besides, some recent research has proposed joint models for EE, including the methods based on Markov
Logic Networks , structured
perceptron , and dual
decomposition ).
The application of neural networks to EE is very
recent. In particular, Nguyen and Grishman 
study domain adaptation and event detection via
CNNs while Chen et al.
 apply dynamic
multi-pooling CNNs for EE in a pipelined framework. However, none of these work utilizes RNNs
to perform joint EE as we do in this work.
Conclusion
We present a joint model to do EE based on bidirectional RNN to overcome the limitation of the previous models for this task. We introduce the memory
matrix that can effectively capture the dependencies
between argument roles and trigger subtypes. We
demonstrate that the concatenation-based variant of
the CBOW word embeddings is very helpful for the
joint model. The proposed joint model is empirically shown to be effective on the sentences with
multiple events as well as yields the state-of-the-art
performance on the ACE 2005 dataset. In the future, we plan to apply this joint model on the event
argument extraction task of the KBP evaluation as
well as extend it to other joint tasks such as mention
detection together with relation extraction etc.