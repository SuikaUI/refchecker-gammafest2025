Towards Human-Machine Cooperation:
Self-supervised Sample Mining for Object Detection
Keze Wang1,2
Xiaopeng Yan1
Dongyu Zhang1∗
Lei Zhang2
Liang Lin1,3
1Sun Yat-sen University
2The Hong Kong Polytechnic University
3SenseTime Research
 ; ; 
 ; 
Though quite challenging, leveraging large-scale unlabeled or partially labeled images in a cost-effective way
has increasingly attracted interests for its great importance
to computer vision. To tackle this problem, many Active
Learning (AL) methods have been developed.
these methods mainly deﬁne their sample selection criteria within a single image context, leading to the suboptimal robustness and impractical solution for large-scale object detection. In this paper, aiming to remedy the drawbacks of existing AL methods, we present a principled
Self-supervised Sample Mining (SSM) process accounting
for the real challenges in object detection.
Speciﬁcally,
our SSM process concentrates on automatically discovering
and pseudo-labeling reliable region proposals for enhancing the object detector via the introduced cross image validation, i.e., pasting these proposals into different labeled
images to comprehensively measure their values under different image contexts. By resorting to the SSM process, we
propose a new AL framework for gradually incorporating
unlabeled or partially labeled data into the model learning
while minimizing the annotating effort of users. Extensive
experiments on two public benchmarks clearly demonstrate
our proposed framework can achieve the comparable performance to the state-of-the-art methods with signiﬁcantly
fewer annotations.
1. Introduction
In the past decade, object detection has gained incredible
improvements both in accuracy and efﬁciency, beneﬁting
from the remarkable success of deep Convolutional Neural
Nets (CNNs) . Through producing candidate
object regions of input images, object detection is converted
into the region classiﬁcation task, e.g., R-CNN . Recently, more powerful neural network architectures such as
∗Corresponding author is Dongyu Zhang.
ResNet have further pushed the object detection performance into new records. Behind these successes, massive
data collection and annotation such as MS-COCO are
indispensable yet quite expensive. Under such a circumstance, there is an increasing demand of leveraging largescale unlabeled data to promote the detection performance
in an incremental learning manner. However, to achieve this
goal, there remain two technical issues: i) Object annotation
for training is usually labor-intensive. Compared with other
visual recognition task (e.g., image classiﬁcation), annotating object requests to provide both the category label and
bounding box of an object. In order to reduce the burden of
active users, it is highly required to develop human-machine
cooperation based approaches to self-annotate most of the
unlabeled data; ii) Picking out the training samples that are
advantageous to boost the detection performance is a nontrivial task. As ﬁgured out in , existing detection
benchmarks usually contain an overwhelming number of
“easy” examples and a small number of “hard” ones (i.e.,
informative samples with various illuminations, deformations, occlusions and other intra-class variations). Utilizing
these “hard” samples is a key to train the model more effectively and efﬁciently. However, as pointed out in ,
due to following a long-tail distribution, these examples are
quite uncommon. Hence, it is a sophisticated task to ﬁnd
“hard” yet informative samples.
To address the aforementioned issues, we investigate
sample mining techniques to incrementally improve object
detectors with minimal user effort. Recently, Active Learning (AL) proposed to progressively select and annotate most informative unlabeled samples for
user annotation to boost the model. Hence, the sample selection criteria play a crucial role in AL, and are typically
deﬁned according to the prediction certainty (conﬁdence)
or other informative criteria like diversity of samples. Recently, many AL methods have been developed
for training deep convolutional neural networks (CNNs).
However, their sample selection criteria are dominantly performed under a single sample context. This make them sen-
The following publication K. Wang, X. Yan, D. Zhang, L. Zhang and L. Lin, "Towards Human-Machine Cooperation: Self-Supervised Sample Mining for
Object Detection," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 2018, pp. 1605-1613 is available
at 
This is the Pre-Published Version.
© 2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to
servers or lists, or reuse of any copyrighted component of this work in other works.
Sheep (0.9)
user annotating?
Labeled Images
Cross Image Validation
Prediction Consistency
Pseudo-labeled samples of
high-consistency
Annotated samples of
low-consistency
Figure 1. The pipeline of the proposed framework with Self-supervised Sample Mining (SSM) process for object detection. Our framework
includes stages of high-consistency sample pseudo-labeling via the SSM and low-consistency sample selecting via the AL, where the
arrows represent the work-ﬂow, the full lines denote data ﬂow in each mini-batch based training iteration, and the dash lines represent
data are processed intermittently. As shown, our framework presents a rational pipeline for improving object detection from unlabeled and
partially labeled images by automatically distinguishing high-consistency region proposals, which can be easily and faithfully recognized
by computers after the cross image validation, and low-consistency ones, which can be labeled by active users in an interactive manner.
sitive to the bias of classiﬁers and the unbalance of samples.
Attempting to overcome the above-mentioned drawback
of the existing AL approaches, this paper develops a Selfsupervised Sample Mining (SSM) process for automatically mining valuable samples in terms of boosting performance of object detectors. Our developed SSM process
is motivated by the recently popular self-supervised learning technique. In stead of designing to learn a
optimal visual representation as , we concentrate on
developing a rational pipeline of using the self-supervision
to signiﬁcantly decrease the amount of user annotations for
improving detection performance.
In the proposed SSM
process, given the region proposals from unlabeled or partially labeled images, we evaluate their estimation consistency by performing the cross image validation, i.e., pasting them into different annotated images to validate its prediction consistency by the up-to-date object detector. Note
that, to avoid ambiguity, the images for validation are randomly picked out from the labeled samples that do not belong to the estimated category of the under-processing proposal. Through a simple ranking mechanism, those incorrectly pseudo-labeled proposals have a large chance to be
ﬁltered due to the challenges inside various image contexts.
In this way, the bias of classiﬁers and unbalance of samples can be effectively alleviated. Then, we propose to provisionally assign disposable pseudo-labels to the ones with
high estimation consistency, and retrain the detectors within
each mini-batch iteration.
Since the pseudo-annotations
may still contain errors, small amount of user interactions
is necessary to keep our SSM under well control.
By resorting to our SSM, we further propose a novel incremental learning framework to gradually incorporate unlabeled samples to enhance object detectors, as illustrated
in Fig. 1. In our framework, inspired by the recently proposed techniques: Curriculum Learning (CL) and Selfpaced Learning (SPL) , we formulate the combining of the SSM and the AL as a concise optimization problem. Speciﬁcally, the SSM or AL process in our framework
can jointly collaborate with each other. This is done by
imposing a set of latent variables to progressively include
samples into training. These variables determine whether a
sample should be selected for pseudo-labeling or annotating. Meanwhile, the misleading of pseudo-labeled errors
can be suppressed since the sample selection criterion is
progressively optimized together with the batch-based incremental learning. In fact, the ambiguity of incorrectly
annotated samples by users can also be eliminated, thanks
to the correction of the majority pseudo-labeled samples.
Hence, our SSM can further improve the robustness of classiﬁers against noisy samples/outliers in the pursuit of detection accuracy.
The main contributions of this work are two-fold. First,
we propose a novel self-supervised process for automatically discovering and pseudo-labeling reliable region proposals via the cross image validation, which is compatible
with the mini-batch based training for large-scale practical scenarios. Second, through fusing the proposed SSM
process with the AL, we propose a principled framework
with a concise optimization formulation and an alternative optimization algorithm. Extensive experiments demonstrate that our proposed framework can not only achieve a
clear performance gain by mining additional unlabeled data,
but also outperform the dominantly state-of-the-art methods
with signiﬁcantly fewer annotations.
2. Related Work
Active Learning. This branch of works mainly focuses
on the sample selection strategy, i.e., how to pick out the
most informative unlabeled samples for annotation. One
of the most common strategies is the certainty-based selection , in which the certainties are measured according to the prediction conﬁdence on new unlabeled samples. The diversity of the selected instance over the unlabeled data has been also considered in . Recently, Elhamifar et al. proposed to consider both the uncertainty
and diversity measure via convex programming. Freytag
et al. presented a concept that generalizes previous
methods based on the expected model change and incorporates the underlying data distribution. Vijayanarasimhan et
al. proposed a novel active learning approach in crowdsourcing settings for live learning of object detectors, in
which the system autonomously identiﬁes the most uncertain instances via a hashing based solution. Rhee et al. 
proposed to improve object detection performance by leveraging a collaborative sampling strategy, which integrates
the uncertainty and diversity criteria from the AL and the
feature similarity measurement of semi-supervised learning philosophy. However, these mentioned AL approaches
usually emphasize those low-conﬁdence samples (e.g., uncertain or diverse samples) while ignoring the rest majority of high-conﬁdence samples. More recently, attempting
to leverage these ignored samples, several works 
have been proposed to progressively select the minority
of most informative samples and pseudo-label the majority of high prediction conﬁdence samples for network ﬁnetuning. Though these approaches have achieved promising
performances, they have limitations due to that their deﬁned
hyper-parameters for pseudo-labeling is empirically set and
updated within a single image context. Furthermore, these
methods do not support mini-batch based training. Therefore, none of them has successfully proved their capability
on handling large-scale object detection task.
Self-paced Learning. Inspired by the cognitive principle of humans/animals, Bengio et al. initialized the
concept of curriculum learning (CL), in which a model is
learned by gradually including samples into training from
easy to complex. To make it more implementable, Kumar
et al. substantially prompted this learning philosophy
by formulating the CL principle as a concise optimization
model named self-paced learning (SPL). Recently, several
works provided more comprehensive understanding of the learning insight underlying CL/SPL, and
formulated the learning model as a general optimization
Based on this framework, multiple SPL variants have been proposed for object detection. Lee
et al. introduced a self-paced approach to focus on
the easiest instances ﬁrst, and progressively expands its
repertoire to include more complex objects. Sangineto et
al. presented a self-paced learning protocol for object
detection that iteratively selects the most reliable images
and boxes according to class-speciﬁc conﬁdence levels and
inter-classiﬁer competitions. Dong et al. proposed an
object detection framework that uses only a few bounding
box labels per category by consistently alternating between
detector amelioration and reliable sample selection.
Self-supervised Learning. Aiming at training the feature representation without additional manual labeling, selfsupervised learning (SSL) has ﬁrst been introduced in 
for vowel class recognition, and further extended for object
extraction in . Recently, plenty of SSL methods 
have been proposed, e.g., Wang et al. proposed to employ visual tracking to provide the supervision for learning visual representations among thousands of unlabeled
Doersch et al. investigated multiple selfsupervised methods to encourage the network to factorize
the information in its representation. Different from these
methods that focus on learning an optimal visual representation, our SSM intends to use the self-supervision to mine
valuable information from unlabeled and partially labeled
3. Self-supervised Sample Mining
3.1. Formulation
In the context of object detection, suppose that we have
n region proposals from m object categories. Denote the
training sample set as D = {xi}n
i=1 ⊂Rd, where xi is the
i-th region proposal generated from the training images. We
have m detectors/classiﬁers (including the background) for
recognizing each proposal by the one-vs-rest strategy. Correspondingly, we denote the label set of xi as yi = {yi}m
where y(j)
corresponds to the label of xi for the j-th object
category. i.e., if y(j)
= 1, this means that xi is categorized as an instance of the j-th object category. We should
give two necessary remarks on our problem setting. One
is that most of sample labels Y = {yi}n
i=1 are unknown
and need to be completed in the learning process. The initially annotated images are denoted by I. The other remark
is that the data {xi}n
i=1 might possibly be fed into the system in an incremental way. This means that the data scale
might be consistently growing. The whole loss function for
our proposed framework with SSM process is formulated as
Loss = Lloc(W) + LAL
cls(W) + LSSM
cls (W, V),
where Lloc(W) denotes the bounding box regression loss
deﬁned in .
cls(W) and LSSM
cls (W, V) imply the
classiﬁcation loss for the AL and SSM processes, respectively.
We deﬁne the AL process as LAL
j=1 ℓj(xi, W), where ΩI denotes the labeled proposals from the currently annotated image I (I
∈I). ℓj(xi, W) means the softmax loss of the proposal
xi in the j-th classiﬁer:
ℓj(xi, W) = −
log φj(xi; W)+
log(1 −φj(xi; W))
where W represents the parameter of the CNN for all m
categories (including background), φj(xi; W) denotes the
probability of belonging to the j-th category for each region
proposal xi.
To adaptively select xi for pseudo-labeling to update its
yi, our SSM process introduces a set of latent weight variables, i.e., V = {v(j)}m
j=1 = {[v(j)
1 , · · · , v(j)
is formulated as:
cls (W, V) =
ℓj(xi, W) + R(xi, v(j)
+ 1| ≤2, y(j)
where ΩI denotes the unlabeled proposals from the unlabeled or partially labeled image I. The regularization function R(·) penalizes the sample weights linearly in terms of
the loss. In this paper, we utilize the hard weighting regularizer due to its well adaptability to complex scenarios. The
hard weighting regularizer is deﬁned as:
R(xi, v(j)
, W) = −f(xi, W)v(j)
Finally, we can directly calculate v(j)
ℓj(xi, W) ≤f(xi, W),
otherwise.
In contrast to the existing works that rely on only
an empirical hyper-parameter for each category to control
the loss tolerance, we exploit a sample-dependent manner,
i.e., the cross image validation f(·), to include samples into
training. Therefore, our model can be considered as a selfsupervised self-paced learning framework. As illustrated in
Fig. 2, the cross image validation is regarded as the estimation consistency of reliable region proposals. Speciﬁcally,
f(·) is deﬁned as:
f(xi, W) =
 BI(xi), BI(xp)
φj(xp; W),
I represents the labeled region proposals from the
annotated image I without j-th category objects for consistency evaluation. λ denotes the pace parameter. BI(xi)
denotes the bounding box of the proposal xi in the selected
image I, while IoU
 BI(xi), BI(xp)
implies the intersection of union between two bounding boxes BI(xi) and
Unlabeled Image
j-th category
Pasted Image
(predicted)
Figure 2. The illustration of the proposed cross image validation.
The proposal xi from the unlabeled image, predicted to belong to
the j-th category, is randomly pasted into a certain annotated image without j-th category objects for consistency evaluation. The
red bounding boxes BI(xp) and BI(xq) denote the newly generate region proposals xp and xq from the newly pasted image
by the up-to-date classiﬁer, respectively. As shown, the unlabeled
and annotated images have entirely different context information
(denoted in different color).
BI(xp). Note that, γ represents the threshold parameter
to identify whether these two bounding boxes correspond
to the same object, and it is set to 0.5 in all experiments according to the evaluation protocol of object detection. 1(·)
is the indicator function. If the proposal xp generated by the
up-to-date detector from the newly pasted image includes
the same object as xi, i.e., IoU
 BI(xi), BI(xp)
we calculate its estimation consistency value φj(xp; W),
which denotes the possibility of xp being the j-th category
during the cross image validation.
3.2. Alternative Optimization Strategy
The alternative minimization is readily employed to
solve this optimization. Speciﬁcally, the algorithm is designed by alternatively updating the sample importance
weight set V via the SSM process, the label set Y via
pseudo-labeling and the AL process, and the network parameters W via standard backpropagation. In the following, we introduce the details of these optimization steps.
The convergence of this algorithm to the practical implementation of our framework will also be discussed.
Updating V: Fixing the {Y, X, W}, we can directly
calculate f(xi, W) via Eqn. (5), and further obtain V via
Updating Y: After obtaining V, we calculate the consistency score si for sample selection as:
j∗= arg max
j∈[m] φj(xp; W),
φj∗(xp; W).
where j∗represents the predicted category by the current
detector with highest conﬁdence. Note that, to reduce the
computation cost, we only randomly pick out at most N
annotated images for evaluating the consistency of the proposal xi. In all the experiments, we empirically set N = 5
for the trade-off between the accuracy and efﬁciency. Given
i=1, we rank all unlabeled samples in a descending order for each classiﬁer as , and pick out top-k nonzero ones at most for each object category. H is regarded
as high-consistency samples with assigned pseudo-labels.
Speciﬁcally, these important samples for m categories are
deﬁned as H = [H1, ..., Hj, ..., Hm](|Hj| ≤k), where k
is an empirical parameter to control the number of selected
samples for each category.
Fixing {W, V, {xi}H
i=1}, we optimize yi of Eqn. (2)
which corresponds to solve the following problem for each
high-consistency sample i ∈H with its important weight
vector vi ̸= 0:
yi∈{−1,1}m,i∈H
ℓj(xi, W), s.t.
+ 1| ≤2, (7)
where vi is ﬁxed, and can be treated as constant. The constraint Pm
+ 1| ≤2 largely excludes all samples for
pseudo-labeling except under the following two conditions:
i) when y(j)
is predicted to be positive by one classiﬁer but
all other classiﬁers produce negative predictions, or ii) when
all classiﬁers predict y(j)
to be negative, i.e., xi is rejected
by all classiﬁers and identiﬁed as belonging to an undeﬁned
object category. These are the rational cases for practical
object detection in large-scale scenarios. Note that we optimize Y by exhaustively attempting to assign -1 or 1 to
each sample for all m categories to minimize Eqn. (7). The
computational cost of this process is acceptable because we
only need to take m + 1 attempts. Through this fashion,
Eqn. (7) always has a clear solution by enforcing pseudolabels on those top-ranked high-consistency sample set H.
This is exactly the mechanism underlying a re-ranking technique . Compared with the previous methods ,
our framework can effectively suppress the error accumulation during the incremental pseudo-labeling via the following two advantages: (i) The cross image validation can
provide more accurate and robust estimations under various
challenging image contexts; (ii) All the pseudo-labels are
disposable. They will be discarded after each mini-batch iteration. These advantages are beneﬁcial for the detector to
avoid being misled by the accumulate errors.
Low-consistency Sample Annotating: After pseudolabeling high-consistency samples in such a self-supervised
manner, we employ the AL process to update the annotated
image set I by providing more informative guidance based
on human knowledge. The AL process aims to select most
informative unlabeled samples and to label them as either
positive or negative by requesting user annotation. Our selection criteria are based on the classical uncertainty-based
strategy .
Speciﬁcally, we collect those samples
with quite small si after performing the cross image validation. Then, we utilize the current classiﬁers to predict
their labels. Those predicted as more than two positive la-
Algorithm 1 Alternative Optimization Strategy
Input: Input dataset {xi}n
Output: Output model parameters {W}
1: Initialize network parameters W with pre-trained
CNN, initially annotated samples I, sample weight set
V and the corresponding consistency score set S;
2: while true do
for all mini-batch = 1, ..., T do
Update V and S by the SSM process via Eqn. (4)
and Eqn. (6), respectively;
Update H by the re-ranking;
Update {yi}i∈H by pseudo-labeling via Eqn. (7);
Update W by standard backpropagation Eqn.(8);
Update low-consistency sample set U;
if U is not empty do
Update the annotated region proposals {ΩI}I∈I
with {yi}i∈U by the AL;
15: end while
16: return W;
bels (i.e., predicted as the corresponding object category)
actually represent these samples making the current classiﬁers ambiguous. We thus adopt them as so called “lowconsistency” samples and randomly add z of them into lowconsistency sample set U for further manually annotation by
active users. Actually, other similar criterion can be utilized
in this step. We employed this simple strategy just due to its
intuitive rationality and efﬁciency.
Updating W: Fixing {D, V, Y}, the original model in
Eqn. (1) can be approximated as:
|H ∪{ΩI}I∈I|
i∈H∪{ΩI}I∈I
ℓj(xi, W) + Lloc(W).
Thus, we can update the network parameters W by standard
backpropagation. Note that, we do not consider the regularization function R(·), which is introduced to regularize the
latent variable set V.
The entire algorithm can then be summarized into Algorithm 1. It is easy to see that this algorithm ﬁnely accords
with the pipeline of our proposed framework in Fig. 1.
Convergence Analysis: Our framework can guarantee
the convergence to a local optimum based on its implementation. The reason is three-fold: (i) the objective function Eqn. (2) w.r.t V is convex; (ii) network ﬁne-tuning in
Eqn. (8) via backpropagation can converge to a local optimal; (iii) as the model becomes mature, the AL process
stops when no low-consistency samples are found.
4. Experiments
To justify the effectiveness of our proposed SSM process and framework, we have conducted a number of experiments on the public VOC 2007/2012 benchmarks ,
whose data are usually divided into two-fold: trainval and
test. To evaluate the model performance on the VOC 2007
benchmark, we regard the VOC 2007 trainval as the initial annotated samples, and consider the VOC 2012 trainval data as unlabeled data that need to be mined. Therefore, the active user annotating process equals to fetch the
VOC 2012 annotations. As for the VOC 2012 benchmark,
we employ the VOC 2007 trainval and test set for initialization. Moreover, we regard the large-scale object detection dataset COCO as the ‘secondary’ unlabeled data.
In other words, we will perform sample mining on it only
when all the VOC 2012 trainval annotations have been used.
As for the annotation key ‘annotated’ and ‘pseudo’, the ﬁrst
one represents the proportion of the user annotations appended/fetched during the training over the pre-given annotations , which are used for
initializing the object detectors. ‘pseudo’ implies the percentage of pseudo-labeled object proposals from unlabeled
data over the pre-given annotations. The lower ‘annotated’
value is, the less user efforts for annotating are required.
The higher ‘pseudo’ value is, the more pseudo-labeled samples are obtained. Hence, when achieving the same performance, a superior method should have lower ‘annotated’
but higher ‘pseudo’ values.
We adopt the PASCAL Challenge protocol, i.e., a correct detection should has more than 0.5 IoU with the ground
truth bounding-box, and exploit the mean Average Precision
(mAP) as for the evaluation metric. In all experiments, we
set parameters {λ, k, N, γ, z} = {0.9, 500, 5, 0.5, 100}. The
ﬁne-tuning manner for the RFCN pipeline is the same as ,
except that we treat the COCO trainval set as unlabeled data
for mining rather than pre-train the network. In the testing
phase, we use a single scale of 600 pixels as input except
for the VOC 2012 test set, which we use a multi-scale test
as . All the experiments are conducted on a desktop
with Intel CPU 3.4GHz and four Titan Xp GPUs. The testing time of our framework is 120 millisecond/image, and
our training time is 620 millisecond/image. As for the base
line method (i.e., RFCN), its testing time is 120 millisecond/image and training time is 150 millisecond/image.
In order to demonstrate that our proposed framework is
general to different network architecture and object recognition framework, we have incorporated our framework
into the Fast R-CNN (FRCN) pipeline with AlexNet 
and the new state-of-the-art RFCN with ResNet-
We denote these variants as “FRCN+Ours”
and “RFCN+Ours”, respectively. To validate the superior
performance of the proposed framework, we have compared it with the CEAL and K-EM approaches.
Percentage of the used annotations
10% 20% 30% 40%
Average mAP over 5 trials
FRCN+Entropy
FRCN+Density
Figure 3. Quantitative comparison of detection performance
(mAP) on the PASCAL VOC 2007 test set
Note that, since the CEAL is designed for image classiﬁcation and is not mini-batch friendly, we extended it for
object detection by alternatively performing sample selection and ﬁne-tuning the CNN. Since the source code of K-
EM is not publicly available, we have obtained the
results from their paper directly.
We denote these
methods as “FRCN+CEAL,” and “FRCN+K-EM”, respectively. Moreover, we have also included ﬁve baseline methods that ignore the pseudo-labeling of unlabeled samples:
“FRCN+RAND”, which randomly selects region proposals for user annotations, FRCN+EMC (Expected-Model-
Change) , FRCN+Entropy , FRCN+ELC (Expected-
Labeling-Change) and FRCN+Density .
fair comparison, we initialize all the methods by providing only 5% annotations, and allow FRCN+Ours and
FRCN+CEAL to mine unlabeled samples only from the
VOC 2007 train/val set. Moreover, we repeat the testing by
ﬁve trails to report the average mAP with standard variance
to present a comprehensive evaluation.
4.1. Results and Comparisons
The Fig. 3 demonstrates the comparison of detection
performance using the Fast-RCNN (FRCN) pipeline with
AlexNet on the VOC 2007 test set. As illustrated in
Fig. 3, our proposed framework FRCN+Ours consistently
outperforms all the compared methods by clear margins under all annotation settings. Speciﬁcally, FRCN+Ours can
achieve the equivalent of a fully supervised performance
(i.e., FRCN with 100% user annotations) when fetching
only approximately 30% user annotations, while most of
the compared methods require nearly 60%. These results
indicate the superior performance of our framework.
To demonstrate the feasibility and great potential of our
framework, we have conducted amount of experiments to
ﬁne-tune RFCN with ResNet-101 (well pre-trained on ImageNet) on the VOC 2007/2012 trainval set by using our
framework and the compared baseline RFCN+RAND. The
compared results on the VOC 2007 and 2012 benchmarks
are illustrated in Tab. 1 (a)(b), respectively. By controlling the number of training iterations, RFCN+Ours and
RFCN+RAND can fetch different amounts of annotations
Table 1. Test set mAP for VOC 2007/2012 under the RFCN 
pipeline. The entries with the best APs with each sub-table are
bold-faced. Annotation key: ‘annotated’ denotes the proportion of
the user annotations appended/fetched from the VOC 2012 trainval set during the training over the initial annotations , which are used for initializing the object detectors; ‘pseudo’ implies the percentage of pseudo-labeled object
proposals from unlabeled data over the pre-given annotations.
initial test annotated pseudo
1000% 80.6±0.2
from the VOC 2012 trainval set.
As one can see from Tab. 1 (a)(b), both RFCN+RAND
and RFCN+Ours gradually obtain increased detection accuracy when the number of annotations increases.
framework consistently outperforms the compared baseline
RFCN+RAND under all appending conditions on both the
VOC 2007 and 2012 benchmarks by a clear margin. Moreover, our framework surpasses RFCN+RAND by nearly
1.5% (80.6% vs 79.1%) on the VOC 2007 benchmark
and 1.3% (78.1% vs 76.8%) on the VOC 2012 benchmark
with signiﬁcantly small variations when sufﬁcient pseudolabeled object proposals are provided. This validates the
effectiveness of our framework. Some examples of the selected high-consistency and low-consistency region proposals via the cross image validation are depicted in Fig. 5.
Proportion of the pseudo-labeled annotations
Average mAP over 5 trials
RFCN+Flip&Rescale
RFCN+Rescale
Figure 4. Quantitative comparison of average mAP on the VOC
2007 test set
4.2. Ablation Study
To validate the contribution of each component inside
our framework, we have also conducted sufﬁcient experiments for empirical analysis. The variant of our framework that discarding the active learning process is denoted as “FRCN+SSM” / “RFCN+SSM”. Similarly, these
pipelines with Active Learning (AL), Self-paced Learning
(SPL) are denoted by “RFCN+AL”, “RFCN+SPL”, respectively. RFCN+AL adaptively collects low-conﬁdence proposals to request the annotations and stops when no lowconﬁdence samples are found. RFCN+SPL is implemented
according to .
As illustrated in Tab. 1 (c)(d), given the same amount
of annotations during initialization, RFCN+SSM performs
signiﬁcantly better than RFCN+SPL on both VOC 2007 and
the VOC 2012 test set. Speciﬁcally, RFCN+SSM achieves
a nearly 2% performance improvement (76.7% vs 74.7%)
with small variations over RFCN+SPL by pseudo-labeling
the same amount of high-consistency region proposals for
training on the VOC 2007 benchmark. A consistent performance gain of approximately 1.2% (72.1% vs 70.9%) is obtained on the VOC 2012 benchmark by RFCN+SSM. These
results validate the signiﬁcant contribution of the proposed
SSM process on mining reliable region proposals for improving object detection.
We have also compared our self-supervised sample mining (SSM) process with three baseline methods under the
AL process disabled setting. RFCN+Flip implies horizontally ﬂipping images for validation, while RFCN+Rescale
represents randomly rescaling an image from 50% to 200%
of its original size. RFCN+Flip&Rescale means the fusion
of them. As shown in Fig. 4, RFCN+SSM consistently outperforms all the competing methods by clear margins at all
pseudo-labeling proportions. Moreover, compared to these
baselines, RFCN+SSM obtains a slighter performance drop
(caused by the accumulated pseudo-labeling errors) after
reaching its peak performance. This also proves the effectiveness of our SSM for suppressing the error accumulation.
Tab. 1 (c)(d) also demonstrates that RFCN+AL consistently outperforms the baseline RFCN and RFCN+SSM.
Though the improvements are minor compared to the best
(a) High-consistent Region Proposals
(b) Low-consistent Region Proposals
Horse (0.8)
Person (0.9)
Person (0.9)
Person (0.9)
Person (0.9)
Figure 5. Selected (a) high-consistent and (b) low-consistent region proposals with pseudo-labels in yellow via cross image validation. The
ﬁrst column lists the region proposal with predicted label and high prediction conﬁdence from the unlabeled images of the PASCAL VOC
2012 dataset. The region proposal is randomly put into the validation images for object detection. The predicted bounding box that has
more than 0.5 IoU with this proposal is illustrated in red. The corresponding prediction conﬁdence (ranging from 0 to 1) for being the
pseudo-labeled category is also in red. It is obvious from (a) that those high-consistent proposals are of high quality for network ﬁne-tuning.
As one can see from (b), the conﬁdence of those bounding boxes on the validation images is low-consistent due to inaccurate bounding
box (ﬁrst two rows) or incorrect pseudo-labels (last two rows)
performance of the RFCN+SSM, our proposed AL stage is
still beneﬁcial for promoting object detection. This slight
improvement occurs because the informative samples with
great potential for improving performance follow a long-tail
distribution, as reported in . Therefore, it is necessary
to employ abundant training samples by asking active users
to provide labels or ﬁnding other assistance. Fortunately,
our proposed high-consistency sample pseudo-labeling via
the SSM process is an effective way to address this issue.
The results of weaker initialization are listed in Tab. 2.
As shown, our RFCN+SSM achieves a consistent performance gain of about 2% over the original RFCN. Compared to RFCN+RAND, RFCN+Ours obtains about 1.5%
higher average mAP with much lower variances. However,
compared to RFCN+Ours, FRCN+Ours in Fig. 3 obtains a
higher mAP gain. The reason is FRCN and RFCN use different algorithms (Selective Search vs. Region Proposal Network ) to generate object proposals. Since
our objective is to mine samples from these proposals, our
model obtains various performance boosts based on the
quality of proposals. This shows the generality and effectiveness of our model over different proposals.
Acknowledgment
This work was supported in part by the Hong Kong
Polytechnic Universitys Joint Supervision Scheme with the
Chinese Mainland, Taiwan and Macao Universities (Grant
This work was also supported by HK
RGC General Research Fund (PolyU 152135/16E), in part
Table 2. Test set mAP for VOC 2007 under the RFCN pipeline
with ResNet-101.
initial annotated
initial annotated
by Guangdong “Climbing Program” Special Funds under
Grant pdjhb0010, in part by National Natural Science Foundation of China (NSFC) under Grant U1611461 and Grant
61702565, and in part by Science and Technology Planning
Project of Guangdong Province of No.2017B010116001.
5. Conclusion
In this paper, we have introduced a principled Selfsupervised Sample Mining (SSM) process, and justiﬁed
its effectiveness in mining valuable information from unlabeled or partially labeled data to boost object detection. We
further involve this process in the AL pipeline with a concise formulation, which is developed for retraining object
detectors via faithfully pseudo-labeled high-consistency object proposals after our proposed cross image validation.
The proposed SSM process contributes to effectively improve the detection accuracy and the robustness against
noisy samples. Meanwhile, the rest samples, being low consistency (high uncertainty) by the current detectors, can be
handled by the AL, which beneﬁts to generate reliable and
diverse samples gradually. In the future, we will apply our
SSM to improve other speciﬁc visual detection task with
unlabeled web images/videos.