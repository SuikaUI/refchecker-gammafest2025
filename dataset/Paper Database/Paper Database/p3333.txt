Received June 21, 2019, accepted July 2, 2019, date of publication July 8, 2019, date of current version July 26, 2019.
Digital Object Identifier 10.1109/ACCESS.2019.2927433
Brain Tumor Segmentation Using Multi-Cascaded
Convolutional Neural Networks and
Conditional Random Field
1,2, (Member, IEEE), QINGHAI GAN1, YUAN ZHANG1, SHUHUA DENG
1, FEN XIAO1,
WEI HUANG4, CHUNHONG CAO
1, AND XIEPING GAO
1,3, (Member, IEEE)
1Key Laboratory of Intelligent Computing and Information Processing, Ministry of Education, Xiangtan University, Xiangtan 411105, China
2Postdoctoral Research Station for Mechanics, Xiangtan University, Xiangtan 411105, China
3College of Software and Communication Engineering, Xiangnan University, Chenzhou 423043, China
4Department of Radiology, The First Hospital of Changsha, Changsha 410005, China
Corresponding authors: Chunhong Cao ( ) and Xieping Gao ( )
This work was supported in part by the National Natural Science Foundation of China under Grant 61802328 and Grant 61771415, in part
by the Natural Science Foundation of Hunan Province in China under Grant 2019JJ50606, in part by the Cernet Innovation Project under
Grant NGII20170702, and in part by the Scientiﬁc Research Fund of Hunan Provincial Education Department of China under
Grant 17A211.
ABSTRACT Accurate segmentation of brain tumor is an indispensable component for cancer diagnosis
and treatment. In this paper, we propose a novel brain tumor segmentation method based on multicascaded convolutional neural network (MCCNN) and fully connected conditional random ﬁelds (CRFs).
The segmentation process mainly includes the following two steps. First, we design a multi-cascaded
network architecture by combining the intermediate results of several connected components to take the
local dependencies of labels into account and make use of multi-scale features for the coarse segmentation.
Second, we apply CRFs to consider the spatial contextual information and eliminate some spurious outputs
for the ﬁne segmentation. In addition, we use image patches obtained from axial, coronal, and sagittal views
to respectively train three segmentation models, and then combine them to obtain the ﬁnal segmentation
result. The validity of the proposed method is evaluated on three publicly available databases. The experimental results show that our method achieves competitive performance compared with the state-of-the-art
approaches.
INDEX TERMS Brain tumor segmentation, convolutional neural network, multi-cascaded convolutional
neural network, conditional random ﬁeld, multi-modality.
I. INTRODUCTION
Brain tumors are one of the common diseases of the nervous
system and have great harm to human health, and even lead
to death. Glioma is one of the intracranial tumors with the
highest mortality and morbidity . It is usually divided into
high-grade glioma (HGG) and low-grade glioma (LGG), and
the average life expectancy of patients who have evolved into
HGG is about two years. Many imaging techniques have been
employed for investigating brain tumors such as magnetic resonance imaging (MRI), computed tomography (CT), positron
emission tomography (PET), and single-photon emission
computed tomography (SPECT). Among them, MRI has
The associate editor coordinating the review of this manuscript and
approving it for publication was Wei Liu.
become the main imaging technique for the diagnosis and
treatment of glioma, since its advantages of good soft tissue contrast, multi-parameter, imaging in arbitrary direction,
non-invasive imaging, etc. In addition, multiple modalities,
e.g., T1-weighted (T1), T1-weighted with contrast enhancement (T1c), T2-weighted (T2), and Fluid Attenuated Inversion Recovery (FLAIR) can be obtained through MRI .
Different MRI modalities focus on different detailed information of images and describe the characteristics of brain tumors
from different aspects. The accurate segmentation of brain
tumors is of great signiﬁcance for medical diagnosis, surgical
planning, and treatment planning. In particular, it is crucial
to separate tumor tissues, e.g., necrosis, edema, enhancing
core, and non-enhancing core from normal brain tissues
including the gray matter (GM), the white matter (WM), and
VOLUME 7, 2019
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see 
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
the cerebrospinal ﬂuid (CSF). However, it is an extremely
challenging problem to segment them accurately, mainly due
to the following reasons. First, the shape, location, appearance, and size of gliomas can vary greatly from patient to
patient . Second, gliomas usually invade surrounding tissues, making the boundary blurred . Third, the image distortion and noise caused by different factors such as imaging
devices or acquisition protocols further increase the difﬁculty.
Manual segmentation is a tedious, time-consuming task,
and if the person delineating the region of interest (ROI) is
not a well-trained technologist, it will usually yields poor
segmentation results , . Therefore, automatic segmentation or semi-automatic segmentation attracts the attention
of researchers. The existing automatic segmentation or semiautomatic segmentation methods can be roughly classiﬁed
as either generative models based methods or discriminative
models based methods . The generative models rely on
prior knowledge of the brain anatomy, and their learning
process is relatively complex. Prastawa et al. proposed a
typical generative model of MR brain images, which mainly
based on probabilistic image atlas. Other methods that require
prior knowledge can be found in – . On the contrary,
discriminative models often rely on low level image features.
They usually employ a discriminative classiﬁer to transform
local features into class probability, which is more suitable
for multi-category identiﬁcation problems. Typical discriminative models include conditional random ﬁelds – ,
random forests – , support vector machines , decision forests , etc.
In recent years, the deep learning based automatic segmentation methods have shown great advantages in medical
image analysis , . CNN is one of the most popular
deep learning models, which can extract features that are
favorable for classiﬁcation from the original data. Compared
with the traditional machine learning methods, CNN has
more powerful capability of feature learning and characterization, and can reduce the complexity of model due to
the nature of weight sharing. Based on the advantages of
deep learning, it has attracted an increasing attention in brain
tumor segmentation – . Typically, Pereira et al. 
proposed to design different CNN architectures with deeper
layer for different grades of glioma samples using small
3 × 3 kernels. Urban et al. proposed a two-path CNN
model in which one channel extracts local detail features and
the other extracts global context features. Most of the above
brain tumor segmentation methods built CNN upon image
patches. These patch-based methods classify each image
patch into different classes, ignoring the spatial consistency of
the entire image. They usually assumed that the label for each
voxel is independent and did not take the local dependencies
of labels into account. To consider the local dependencies of
labels, Havaei et al. developed a cascaded architecture
(InputCascadeCNN), which can obtain a better segmentation
performance. Furthermore, they adopted a two-stage training
strategy to solve the imbalance of label distributions.
Recently, the advantages of the multi-scale features
from CNNs have been demonstrated in segmentation tasks
 , . Generally, there are two ways to extract multiscale features . One way is to use the feature maps
from different levels of a network to represent multi-scale
features . In , Hu et al. proposed a multi-scale CNNs
for the retinal vessel segmentation. Multi-scale images at different stages of the CNNs were fused to obtain the probability
maps of the retinal vessels. The other is to pass different
scaled versions of the input image using the same network . In , the multi-scale features were extracted by
Kamnitsas et al. using two patches with different sizes.
In , Havaei et al. employed an InputCascadeCNN architecture to capture multi-scale features. While in this paper,
we propose a new network architecture from InputCascadeCNN to obtain more discriminative multi-scale features
through multi-cascaded networks.
It is undeniable that CNN has achieved remarkable
achievement. But for a neural network with ﬁxed architecture and parameters, its learning ability is still limited, and
some useful information may be ignored, especially for the
3D information of the MRI data. Some researchers adopt
3D-CNN models , , or 2.5D models to deal
with 3D images. Myronenko et al. proposed a 3D semantic segmentation network for brain tumor segmentation based
on encoder-decoder architecture. Wang et al. proposed a
hierarchical segmentation system, which converted the multiclass segmentation into three binary segmentation tasks. They
also trained the segmentation models from the axial, sagittal,
and coronal views respectively. During the testing stage,
they averaged the softmax outputs in these three views to
obtain the ﬁnal results. Although these methods achieve
excellent performance, they increase computational complexity and memory consumption. Hence, some mathematical
models, e.g., Markov Random Fields (MRFs) or Conditional
Random Fields (CRFs), are usually introduced to consider
the spatial contextual information , , , .
Kamnitsas et al. adopted 3D CRFs for post-processing,
which improved the segmentation results, but the conﬁguration of the 3D CRFs was a laborious process. Zhao et al. 
formulated CRFs as neural networks and trained them with
fully convolutional neural networks (FCNNs), their training
process was cumbersome and computationally expensive.
As a trade-off, we employ fully connected CRFs as a ﬁne segmentation step to take the appearance and spatial consistency
of the segmentation results into account.
In this paper, a novel coarse-to-ﬁne segmentation framework for brain tumor segmentation is proposed by combining
MCCNN and fully connected CRFs. Firstly, we propose a
multi-cascaded network architecture for coarse segmentation
to consider the dependencies between neighbouring pixels
and the complementary information at different layers and
scales. Then, considering that spatial contextual information of 3D MR images is very important for brain tumor
segmentation and it is not fully considered by CNN, and
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
FIGURE 1. Flowchart of the proposed method.
we introduce the fully connected CRFs to further reﬁne the
probability map obtained by CNN. Finally, to fully consider
the spatial information of 3D medical images and amplify
the training samples, we train the segmentation models from
the axial, sagittal, and coronal views respectively. The segmentation results of these three models are then fused to
obtain the ﬁnal result using a voting based fusion strategy as
described in . The evaluation of our method is performed
on the imaging data provided by the Brain Tumor Segmentation Challenge (BRATS) 2013, BRATS 2015, and BRATS
2018 databases , – . The experimental results show
that the proposed method not only yields competitive performance but also is computationally efﬁcient compared with
the state-of-the-art approaches. We also investigate the effects
of different modalities on the performance of our model. The
experimental results show that our model can achieve competitive performance based on three modalities, i.e., FLAIR,
T1c and T2, compared with the model based on all four
modalities , , , , – , , , and
can also reduce the cost of computational complexity and
data acquisition. Besides, our model can segment the brain
images slice by slice in the testing stage, which is much
faster than the segmentation method based on the image
patches. The contributions of this study can be summarized as
1) We extend InputCascadeCNN to MCCNN architecture
to obtain multi-scale features through multi-cascaded
networks on brain MR images. The discriminative
features are efﬁciently extracted by passing multiple
scaled versions of the input image through three parallel
sub-networks. In each sub-network, the high-level and
low-level information are utilized jointly by fusing the
features of the front-layer. In addition, different cascaded forms are employed to take the dependencies of
labels and the neighborhood information of the central
pixel into account to achieve a smoother boundary.
2) We propose an efﬁcient coarse-to-ﬁne segmentation
framework for brain tumor segmentation. Our framework mainly consists of a coarse segmentation and a ﬁne
segmentation. The former uses the proposed MCCNN
to obtain a probability map by considering multi-scale
features on the multi-modal brain images. The latter
employs a fully connected CRFs to reﬁne the coarse
segmentation by smoothing tumor edges and eliminating
false positives.
II. PROPOSED METHOD
As shown in Fig. 1, the proposed method includes following steps, i.e., pre-processing, coarse segmentation based on
MCCNN, ﬁne segmentation based on fully connected CRFs,
post-processing, and fusion of the segmentation results of
three different views. The details of each step are presented
A. PRE-PROCESSING
Since MRI scans are affected by the bias ﬁeld distortion ,
which makes the intensity of the same tissues to vary across
the image. So we adopt the N4ITK method proposed by
Tustison et al. to correct the bias ﬁeld of MRI data.
In addition, it will also vary even if the images of the same
patient are obtained in the same scanner at different time
points, or in the case of pathology . Therefore, in order
to reduce the contrast and intensity ranges between patients
and acquisitions, we also employ the method proposed by
Nyúl et al. to perform the intensity normalization on each
modality. The pre-processing steps are as follows.
• Step 1. Remove the 1% highest and lowest intensities.
• Step 2. Apply an N4ITK bias correction to T1c modality.
• Step 3. Perform the intensity normalization on each
• Step 4. Normalize the data within each input channel
by subtracting the mean of all pixels from each pixel in
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
FIGURE 2. The baseline network architectures.
FIGURE 3. The proposed simple cascaded architectures.
the image of each channel and dividing by the standard
deviation.
B. COARSE SEGMENTATION USING OUR
MCCNN ARCHITECTURE
CNN is a very powerful method in the ﬁeld of image segmentation . The basic architecture of CNN includes input
layer, hidden layer, and output layer . The value of output
can be obtained from the forward propagation of CNN, and
the bias and weight can be adjusted by back-propagation.
The classic CNN consists of three parts: convolution layers,
pooling layers, and fully connected layers . There are
usually a multitude of parameters that need to be trained in
a deep learning model of CNN. In order to ensure that there
are enough training samples and consider the complexity of
training, our MCCNN is trained using image patches, which
are extracted randomly from slice of different perspectives
i.e., axial, coronal, and sagittal views in 3D brain MRI data.
Overall, the glioma images are roughly segmented in this
process, that is, the entire glioma contour is obtained as completely and accurately as possible. For this purpose, we conduct a large number of experiments to design and select the
network. Firstly, we propose two basic CNN architectures
without cascade as baseline connected components. Then,
we explore the performance of simple cascade architectures
and the multi-cascaded architectures. Details of these models
are as follows.
The basic network architectures are shown in Fig. 2. The
two-path convolutional neural network (TCNN), as shown
in Fig. 2(a), is based on Havaei et al. , and we improve
it to obtain more accurate tumor contour. At the same time,
we design a deep single-path convolutional neural network
(SCNN) with front-layer information as alternative comparison, which is shown in Fig. 2(b).
1) SIMPLE CONCATENATION
As shown in Fig. 3, in this study we propose three simple
cascaded network architectures that concatenate the output
of the ﬁrst CNN at the input of the subsequent CNN, respectively named TTCNN, SSCNN, and TSCNN. In the TTCNN
architecture, as shown in Fig. 3(a), two basic two-path networks are adopted. The ﬁrst network processes larger image
patches to obtain more comprehensive context information.
The output of the ﬁrst network is concatenated to the input of
the second network, which guides the training of the second
network. The basic network consists of two channels, which
extract useful image features by using different kernels. One
channel with larger kernel of size 13 × 13 is used to extract
global contextual information, while another with smaller
kernels of size 7 × 7 and 3 × 3 are used to extract local
details. The global channel takes three planes of size 33 × 33
and generates the feature maps of size 21 × 21. The local
channel also takes patches of size 33 × 33 and generates
the feature maps of size 21 × 21. These feature maps are
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
FIGURE 4. The proposed MCCNN architectures.
combined using the concatenating ability of the convolution
As shown in Fig. 3(b), the proposed SSCNN architecture employs two deep single-path networks with front-layer
information. We herein use smaller 5 × 5 and 3 × 3 receptive
ﬁelds to take more details into account. It integrates highlevel information with low-level information through fusing
the front-layer features to utilize the features of different
layers for more accurate segmentation. In addition, larger
ﬁlters extract more local features, whereas smaller ﬁlters
focus more on edge detection. Therefore, smaller kernels are
used in this structure to take more edge information into
account for a more accurate and complete tumor contour.
The proposed TSCNN architecture is a combination of a
two-path network and a deep single-path network as shown
in Fig. 3(c). Similar to the previous two architectures, the output of the ﬁrst half of TSCNN is cascaded at the input of
the second half. The ﬁrst network of this architecture uses
larger kernels to learn more global features, while the second
network has smaller kernels to consider the local details.
2) MCCNN ARCHITECTURES
Fig. 4 shows the proposed MCCNN, which combined multiple networks to form a multi-cascaded network architecture. The InputCascadeCNN architecture proposed by
Havaei et al. has achieved state-of-the-art segmentation
performance in brain glioma segmentation. We extend and
improve this architecture by taking two cascaded forms. One
form is cascading at the input of the bottom network, and the
output of the ﬁrst network is treated as an additional image
channel to guide the training of the second network, which is
called simple concatenation. The other is to add a cascaded
network before the feature map obtained by simple concatenation being fed to the ﬁnal classiﬁcation layer, thereby forming a multi-cascaded CNN architecture. Since the neurons
of the soft-max output layer are cascaded directly with the
previous output, the network learning is more inclined to
assume that the central pixel label should be similar to its
surroundings, so that smoother boundaries can be produced.
In this work, we investigate two multi-cascaded network
architectures, named MCCNN1 and MCCNN2, respectively.
As shown in Fig. 4(a), MCCNN1 uses the TSCNN
architecture as a simple concatenation. In this architecture,
we adopt a two-path network to extract the features of larger
image patches, and employ a deep single-path network to
learn the features of smaller image patches. In addition,
another single-path network is added to detect tumor edge
information, which helps to extract contextual information
and more local details.
In MCCNN2, as shown in Fig. 4(b), on the basis of the
SSCNN architecture, we crop the larger image patches, which
are used as input to another single-path network to identify
local features of the image, e.g., the edge information of
brain glioma. The intermediate results obtained are connected
in the previous layer of the ﬁnal output. The size of the
cropping patches can be adjusted according to the speciﬁc
requirements. By this manner, it can not only consider the
dependence of labels, but also enhance the performance of
glioma segmentation task by combining different cascaded
forms and fusing the multi-scale information.
C. FINE SEGMENTATION USING FULLY CONNECTED CRFS
Although the form of cascading has been used to consider
the local dependencies of labels, it is not enough for medical
images. This is mainly because anatomical structures have
complex shapes that are difﬁcult to model . Further,
the temporal or spatial correlation of MRI data also plays
important role in the segmentation, which needs to be considered in the method. Therefore, it is necessary to further reﬁne
the probability map obtained by CNN. In recent years, CRFs
have been widely used in many medical image applications,
since they show good performance for modeling the complex
dependencies in spatial data. For the segmentation of brain
tumors, CRFs can be used not only to model the relationship
between image pixel features and its label, but also to model
the dependencies between neighborhood pixel features and
their labels , . As mentioned before, Zhao et al. 
applied CRFs to image segmentation by formulating it as neural networks. However, the training process of their method
is cumbersome and computationally expensive. By contrast,
in this paper we employ the CRFs as a ﬁne segmentation to
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
FIGURE 5. Examples of the brain MRI data from the BRATS 2013 dataset. From left to right: Flair, T1, T1c,
T2, and the ground truth. Each color represents a tumor class: red—necrosis, green—edema,
blue—non-enhancing, and yellow—enhancing tumor.
reﬁne the segmentation of MCCNN. It not only improves
the segmentation accuracy of MCCNN, but also has lower
computational complexity.
For a 2D image I, which consists of pixels {Ii|i
1, 2, . . . , N} and its corresponding label conﬁguration
{Yl|l = 1, 2, . . . , C}, the image segmentation problem is
transformed into an optimization problem. We train a suitable
model by minimizing the energy function of CRFs1 ,
which can be modeled as
9p(yi, yj),
where u, p ∈{1, 2, . . . , C} are segmentation labels, and i,
j ∈{1, 2, . . . , N} are certain pixels of image I. The unary
potential, 9u(yi)
−log P(yi|I), is the negative loglikelihood, where P(yi|I) is the probability obtained by CNN
for pixel i. While the pairwise potential measures the inﬂuence between any pair of pixels , which is deﬁned as
9p(yi, yj) = µ(yi, yj)
w(m)k(m)(fi, fj),
where M = 2 is the number of Gaussian kernel and w(m)
denotes a weight for the m-th Gaussian kernel. µ(yi, yj) =
[yi ̸= yj] is the label compatibility function . k(1) represents the appearance kernel, which tries to assign the same
class labels to nearby pixels with similar intensity. k(2) represents the smoothness kernel, aiming to remove small isolated
regions. They are respectively deﬁned by
k(1)(fi, fj) = exp(−|si −sj|
k(2)(fi, fj) = exp , necrosis (label 1), edema (label 2), non-enhancing
(label 3), and enhancing tumor (label 4). Examples of the
brain MRI data along with the ground truth are shown
in Fig. 5.
The BRATS 2015 database contains 274 training MR
images, which are composed of 220 HGG cases and 54 LGG
cases. Please note that the BRATS 2015 dataset includes
imaging data of the BRATS 2013, where the ground truth
was manually annotated. The rest produced by voted average
of the segmentation results of the top ranked methods in
2 
3 
4 
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
the BRATS 2012 and BRATS 2013. These automatically
generated annotations ultimately manually modiﬁed by experienced raters.
The BRATS 2018 database contains training dataset
of 285 glioma patients, comprising of 210 HGG cases and
75 LGG cases. The validation dataset contains 66 patient
subjects of unknown grade. The ground truths of training
dataset were provided by the BRATS organizers.
B. EVALUATION METRICS
According to the protocol in the BRATS databases ,
the tumor structures are divided into three sub-regions for
each patient, which are deﬁned as follows:
(including
sub-tumoral classes, i.e, label 1, 2, 3, and 4).
b. The core tumor region (including complete tumor region
but excluded ‘‘edema’’ region, i.e., label 1, 3, and 4).
c. The enhancing tumor region (only including ‘‘enhancing’’ region, i.e., label 4).
For each tumor region, we use Dice Similarity Coefﬁcient
(DSC) as well as Positive Predictive Value (PPV) and Sensitivity for the evaluation. DSC provides a measurement criterion for overlapping areas between manual delineated brain
tumor regions and our segmentation results. It is deﬁned as
FP + 2TP + FN ,
where TP, FP, and FN denote the numbers of true positive,
false positive, and false negative measurements, respectively.
PPV is used to evaluate the number of TP and FP, which is
In addition, Sensitivity is a useful measure of the number
of TP and FN and it is given by
Sensitivity =
C. EXPERIMENTAL SCHEME AND
IMPLEMENTATION DETAILS
In order to verify the effectiveness of the proposed method,
a series of comparative experiments are conducted on the
BRATS 2013 database. Firstly, we perform an experiment
to evaluate the segmentation performance of the proposed
two MCCNN architectures in comparison with two basic
networks described previously and the simple concatenation
forms between them. Secondly, to verify the validity of the
proposed coarse-to-ﬁne segmentation framework, we compare the segmentation performance of MCCNN with and
without used CRFs. In addition, to investigate the effect of
each modality on the proposed method, we carry out an
experiment to compare the performance of our models based
on input data of four imaging modalities (Flair, T1, T1c,
and T2) and three imaging modalities (any three of these four
imaging modalities). It should be noted that all the experiments above are conducted on the images extracted from
axial view. Moreover, in order to consider the information
from different views, we evaluate the segmentation results of
three different perspectives and demonstrate the effectiveness
of fusing them. Finally, we compare the proposed method
with some state-of-the-art approaches on the BRATS 2013,
BRATS 2015, and BRATS 2018 databases.
In the experiments, the training process consists of two
stages. In the ﬁrst stage, the coarse segmentation is carried out
through the proposed MCCNN network, and the probability
maps of three perspectives are obtained. In the second stage,
reﬁned segmentation results are obtained through the fully
connected CRFs. The hyper-parameters of the different network architectures (kernel and pooling size for each layer and
the number of feature maps) are found using the validation
set, consisting of two HGG cases, which is shown in Fig. 4.
Other hyper-parameters of the model are also tuned with
the validation set. The ﬁnal results of our experiments are
obtained using ReLU as activation function in convolution
layers. The learning rate is set by α = 0.005. Momentum is 0.9 and weight decay is 10−3. Moreover, we use
Dropout and L2 regularization to prevent overﬁtting and
use the Stochastic Gradient Descent (SGD) algorithm
for training the network. Besides, the parameters of CRFs
in our experiments are obtained by grid search. The optimal
performance in our experiments is obtained when θα = 10,
θβ = 50, and θγ = 3. The linear combination weights are set
by w(1) = 3 and w(2) = 3.
Since the true distribution of each label in the brain is
highly unbalanced, in which the majority of voxels are
healthy (98%), and non-healthy voxels account for a very
small proportion (2%). This makes neural network training algorithms such as SGD perform poorly. Therefore,
we adopt the similar two-phase training procedure to
train the network. In the ﬁrst phase, we construct a training data with uniform distribution of all labels. The model
iterates 50–75 epochs using the equal distributed instances.
In the second phase, unbalanced training data is used. The
model iterates through 5 epochs using the instances with true
distribution.
All models are trained and tested with keras on a computing server with four Intel I5-4590 CPUs and eight NVIDIA
GTX1080 GPUs graphics cards. Each program takes up only
one CPU and GPU at a time.
IV. RESULTS AND DISCUSSION
A. PERFORMANCE COMPARISON OF THE PROPOSED
NETWORK ARCHITECTURES
It is generally accepted that the learning ability of the
network will improve by increasing the complexity of the
network. In this study, we provide a comparison based on
different network architectures, i.e., basic CNN architectures (TCNN and SCNN), simple concatenation architectures
(TTCNN, SSCNN and TSCNN), and multi-cascaded architectures (MCCNN1 and MCCNN2). These architectures are
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
TABLE 1. Performance of the basic network models and variations on the BRATS 2013 testing dataset.
TABLE 2. Evaluation results of the proposed coarse-to-fine segmentation framework on the BRATS 2013 testing dataset.
illustrated in Figs. 2-4 respectively. It is need to note that
the proposed multi-cascaded architectures consist of three
sub-networks, because going deeper will not improve the
performance.
Table 1 presents the comparison of quantitative results on
brain axial view images using different network architectures. All the above results are obtained without any postprocessing. As described in Section II-B, at this stage we
need to obtain the contour that contains the entire tumor
as much as possible. This is the higher the value of DSC
and Sensitivity, the better the subsequent reﬁnement results.
From Table 1, it shows that the segmentation results of
simple concatenation and multi-cascaded architectures have
outstanding performance on these two indicators. Especially,
SSCNN and MCCNN2 achieve similar performance in DSC,
but the latter has higher Sensitivity than the former. Moreover,
MCCNN2 achieves the best performance for the overall.
From Fig. 4 (b), each sub-network of this architecture takes
the front-layer information into account, and uses multiple
concatenation forms to seamlessly integrate the dependencies
of labels and multi-scale features into our network, thus utilizing the complementary information of features at different
layers and scales. Besides, this architecture can process each
sub-network in parallel. Experiments demonstrate that this
architecture achieves comparable segmentation results while
guaranteeing the time efﬁciency. Therefore, MCCNN2 is
selected as the ﬁnal recommended multi-cascaded network
architecture. In the following part, we will use MCCNN to
represent this network architecture.
B. EVALUATION OF THE EFFECTIVENESS
OF THE PROPOSED COARSE-TO-FINE
SEGMENTATION FRAMEWORK
Table 2 shows the evaluation results of MCCNN with
and without used fully connected CRFs on the BRATS
2013 dataset. From Table 2, we can ﬁnd that the results
of MCCNN + CRFs are obviously higher than those of
MCCNN except the Sensitivity of the complete tumor is
slightly lower. Therefore, we can demonstrate that the coarse
segmentation using MCCNN achieves higher Sensitivity, and
the ﬁne segmentation using CRFs can effectively reﬁne the
segmentation results. Furthermore, Fig. 6 shows the visual
effects of the segmentation results. The results in the third
row after the reﬁnement using CRFs are signiﬁcantly better
than those in the second row, not only obtains more accurate
tumor areas, but also effectively removes the false positive
In summary, in our coarse-to-ﬁne segmentation framework, the tumor proﬁle with higher accuracy is obtained by
using the proposed MCCNN due to the advantages of multiple cascaded combinations and multi-scale feature fusion.
And we use CRFs to reﬁne the segmentation results by considering the spatial and appearance consistency of segmentation results. Both the quantitative results and visual effects
demonstrate the performance of the proposed coarse-to-ﬁne
segmentation framework.
C. EVALUATION OF THE IMPACT OF EACH MODALITY
As can be seen from Fig. 5, different modalities in MRI data
emphasize different information. In order to explore the effect
of each modality on training, we establish segmentation models respectively using all four imaging modalities, i.e., Flair,
T1, T1c, and T2, and the combination of any three imaging
modalities. Comparisons are presented in Table 3.
In general, different modalities have different features.
Since the T1c sequence strengthens the features of tumor
boundary, making the boundary clear and easy to distinguish . The information reﬂected by this sequence is effective for the identiﬁcation and classiﬁcation of core tumor and
enhancing tumor regions. Therefore, in the case of removing
the T1c sequence, the model cannot learn robust features
of the tumor edge, resulting in poor segmentation results.
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
FIGURE 6. Visual effects on the BRATS 2013 dataset. (a) and (b) show the segmentation results of the
45-th and 75-th slice of the axial view of case 0301. (c) and (d) show the segmentation results of 50-th
and 70-th slice of the axial view of case 0310. Colors in the results represent different tumor classes:
red—necrosis, green—edema, blue—non-enhancing, and yellow—enhancing tumor.
TABLE 3. Performance comparison of the segmentation models trained using different imaging modalities on the BRATS 2013 testing dataset.
The T1 sequence is helpful for the differentiation of normal
tissues, but it weakens the characteristics of the tumor. Therefore, the model based on the T1 sequence is more favorable
for the segmentation of normal tissues, that is, the value of
PPV is relatively higher, accompanied by lower Sensitivity.
The signals of water molecules are inhibited in the Flair
sequence, which helps to distinguish between edema regions
and CSF . Therefore, the Flair sequence has a signiﬁcant
effect on the segmentation of the complete tumor region
as well as different tumor sub-regions. The T2 sequence is
mainly used to delineate the edema area, and strengthens
the signal of this area, which can provide useful information
for the training of our model. It can be seen that the overall
segmentation accuracy of our model decreased in the absence
of the T2 sequence.
In summary, the Flair, T1c, and T2 modalities are indispensable, and the information they emphasized are very
useful and irreplaceable for model training. Although the
T1 sequence also plays an important role in normal tissue segmentation, it is not prominent for our segmentation
task. In addition, we ﬁnd that the models achieve similar
segmentation performance based on all four modalities or three modalities, i.e., Flair, T1c, and T2, while the
latter has fewer training parameters and lower computational complexity. Therefore, we choose the Flair, T1c, and
T2 modalities as our ﬁnal multi-modal MRI data to conduct
the experiments.
D. EVALUATION OF THE VALIDITY OF FUSING THE
SEGMENTATION RESULTS OBTAINED BY
THREE PERSPECTIVES
Considering that more robust information can be extracted
from different perspectives, we separately train three segmentation models using patches obtained from three different
views, i.e., axial, coronal, and sagittal views. These three
models are used to segment the brain images to produce
three segmentation results, which are then fused to obtain
the ﬁnal segmentation result. The quantitative results of each
stage from three perspectives and their fusion results are
shown in Table 4. It shows that fusing the segmentation
results usually yields better segmentation performance at
any stage. It is worth noting that after the CRFs, the fusion
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
TABLE 4. Evaluations of segmentation results obtained in three different views on the BRATS 2013 testing dataset.
FIGURE 7. Visual effects from three views on case 0301 of the BRATS 2013 dataset. (a) shows raw
images. (b)-(d) represent the segmentation results from axial, coronal, and sigattal views, respectively.
(e) shows the fusion results of three views. For a better visual comparison, these segmentation results
are converted into the same view. In (b)-(e), each row from top to bottom shows: segmentation results of
MCCNN, segmentation results of MCCNN + CRFs, and segmentation results of MCCNN + CRFs +
post-processing. Colors in the results represent different tumor classes: red—necrosis, green—edema,
blue—non-enhancing, and yellow—enhancing tumor.
results are greatly improved without the post-processing compared with the negligible improvement after applying the
post-processing.
To better illustrate the effectiveness of the fusion segmentation results, we show the visual effects of each segmentation
process in Fig. 7. From Table 4 and Fig. 7, we can ﬁnd that
the segmentation results of the axial view are optimal in the
coarse segmentation stage. In the ﬁne and ﬁnal segmentation
stages with post-processing, the coronal view is the best.
And in any stage, the segmentation results from sagittal view
are the worst. Overall, the best performance is achieved by
combining these three different perspectives.
E. COMPARISON WITH THE STATE-OF-THE-ART METHODS
1) RESULTS ON THE BRATS 2013 DATABASE
As listed in Table 5, we summarize the evaluation results
of the top ranked participants, as well as the results of
our method and other six state-of-the-art approaches. In the
ranking of the testing dataset, Pereira et al.’s method 
ranked the ﬁrst, Zhao et al.’s method ranked the second,
and our method ranked the third. As seen from Table 5,
none of approaches can produce optimal segmentation results
for all sub-tumoral regions. However, we can ﬁnd that the
proposed method achieves forefront and competitive performance compared to the state-of-the-art approaches. In particular, our method achieves the best values for DSC of
the complete tumor and Sensitivity of the enhancing tumor
Our method is inspired by Havaei et al.’s approach,
in which they proposed a cascaded network architecture for
brain tumor segmentation. In this paper, we propose a novel
multi-cascaded network architecture by utilizing multiple
concatenation forms, in which the higher accuracy tumor
proﬁle and smoother boundaries can be obtained. We herein
provide a one-to-one comparison with the results of the proposed method and those of Havaei et al.’s approach .
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
TABLE 5. Comparison of the proposed method with other state-of-the-art methods on the BRATS 2013 testing dataset.
TABLE 6. Comparison of the proposed method with other state-of-the-art approaches on the BRATS 2015 dataset.
From Table 5, we can observe that our results are signiﬁcantly
better than their method except for a slightly lower PPV value
of the complete tumor region.
In addition, we have noticed that Zhao et al. proposed
a brain tumor segmentation method of combining FCNNs and
CRFs. Different from them, in this study we employ CRFs as
a ﬁne segmentation process, which can achieve competitive
performance while saving time overhead. As reported, they
have a training duration of about 12 days, whereas ours is
about 3 days on the shared GPU server. From Table 5, our
results are slightly lower than Zhao et al.’s in some indicators
because we only use a simple post-processing procedure and
they carry out a series of complex post-processing operations. Besides, although their post-processing method is highperformance, it is also highly hand-crafted and may not easily
be transferred to other related segmentation tasks. Furthermore, in their study they published segmentation results without post-processing on the BRATS 2013 testing dataset, for
complete, core, and enhancing tumor regions, their results are
0.85, 0.83, 0.74 for DSC, 0.85, 0.84, 0.68 for PPV, and 0.88,
0.82, 0.82 for Sensitivity, respectively. While our results are
0.88, 0.81, 0.76 for DSC, 0.86, 0.81, 0.69 for PPV, and 0.90,
0.84, 0.86 for Sensitivity, which are better than Zhao et al.’s.
2) RESULTS ON THE BRATS 2015 DATABASE
To illustrate the robustness of our method, we also use the proposed method to segment the 3D MR images provided by the
BRATS 2015. Similar to Chen et al. and Zhan et al. 
did in their works, we randomly selected 20% MRI scans
as testing sets, and the remaining scans are used as training
sets. As shown in Table 6, we herein present a comparison of
the proposed method with other approaches in terms of DSC,
PPV, and Sensitivity. It can be seen that the proposed method
achieves better results on the overall performance, especially
the DSC scores exceed all the methods in Table 6. In addition,
the boxplots of the results are shown in Fig. 8. Obviously, our
method performs well on all images in the testing sets except
for some outliers.
3) RESULTS ON THE BRATS 2018 DATABASE
To further evaluate the effectiveness of the proposed method,
we conduct an experiment on the BRATS 2018 dataset. The
data provided by BRATS 2018 differs from the data provided during the previous BRATS challenges. Speciﬁcally,
the tumor sub-regions consist of the enhancing tumor (ET),
the whole tumor (WT), and the tumor core (TC). The labels
in this dataset include label 1 for necrosis and non-enhancing,
label 2 for edema, label 4 for enhancing tumor. The segmentation performance of our algorithm on the validation
dataset can be assessed by DSC, Hausdorff Distance (HD),
Speciﬁcity, and Sensitivity, which are calculated through the
online evaluation system. Table 7 shows the quantitative performance of the proposed method and other six state-of-theart approaches on 66 validation unseen cases. Particularly,
the method proposed by Myronenko ranked ﬁrst at
the BRATS 2018 challenge. Their approach achieved excellent segmentation performance by integrating 10 models and
adopting test-time augmentation (TTA) strategy, but their
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
FIGURE 8. Boxplots of the results on the BRATS 2015 dataset using the proposed method. Red indicates the mean, blue indicates the median,
and green indicates the outliers.
FIGURE 9. Scatter diagram of the results on the BRATS 2018 dataset using the proposed method.
models were quite complex and required high computing
resources.
Relatively speaking, our model is not so complicated,
and promising performance can be obtained on the BRATS
2013, 2015, and 2018 databases. In addition, Fig. 9 gives
a scatter diagram of the validation set. It shows that our
algorithm performs well for most brain images. Since the data
of BRATS 2018 differ signiﬁcantly and the class distribution
is highly imbalanced, thus there are a few outliers that cause
the average score to decrease.
More importantly, our method is mainly aimed at improving the method of Havaei et al. and the improvements
are reﬂected in the network architecture and the coarse-to-
ﬁne segmentation framework. The experimental results have
VOLUME 7, 2019
K. Hu et al.: Brain Tumor Segmentation Using Multi-Cascaded Convolutional Neural Networks and Conditional Random Field
TABLE 7. Comparison of the proposed method with other state-of-the-art approaches on the BRATS 2018 dataset.
TABLE 8. Comparison of computational complexity of different methods for one subject patient.
shown the superior of the proposed method when compared
with Havaei et al.’s method.
F. SEGMENTATION TIME
Thanks to the convolutional nature of the models and the use
of an efﬁcient GPU implementation, the proposed segmentation system can segment the brain image slice by slice in
the testing stage. In general, on the shared GPU server, our
method took 1.5-3 minutes to segment an entire brain for the
three views. A comparison of the testing time for the different segmentation methods is shown in Table 8. The results
demonstrate that our method not only has stable segmentation
results, but also has relatively low computational complexity.
V. CONCLUSION
In this paper, we ﬁrstly proposed a novel MCCNN architecture to extract more discriminative multi-scale features
for brain tumor segmentation. Especially, we considered
different network frameworks and studied their impact on
the segmentation performance and found that high performance can be achieved by stacking three CNNs to model the
dependencies of labels and exploiting the nature of fully
convolution. Secondly, we presented a coarse-to-ﬁne segmentation framework by combining MCCNN and fully connected CRFs. In this framework, we ﬁrst performed a coarse
segmentation using MCCNN in order to obtain the tumor contour. Then, we used CRFs to reﬁne the segmentation results
by minimizing an energy function. In particular, we train
three segmentation models using 2D patches obtained from
different perspectives (axial, coronal, and sagittal views),
respectively, and then to obtain the ﬁnal segmentation results
using a voting based fusion strategy. Finally, the experimental
results have demonstrated the performance of the proposed
method compared with some state-of-the-art approaches. Our
experimental results also show that different modalities have
different effects on the performance. In addition, we found
that the Flair and T1c sequences play an essential role in
the training of a segmentation model, and the segmentation
model based on Flair, T1c, and T2 modalities obtained similar
segmentation results and lower computation time compared
to the segmentation model based on all four modalities.
Overall, our method provided promising performance on
the BRATS 2013, 2015, and 2018 datasets compared to the
state-of-the-art approaches. It took about 1.5-3 minutes to
segment an entire brain, which is faster than most patch-based
methods. However, our model has a decrease in performance
when the data differing signiﬁcantly. Maybe 2D CNN still
cannot fully utilize 3D information of MRI data . Our
experimental results have shown that the fusion of information from different views can improve the performance of
tumor segmentation, but not each single view can achieve
good segmentation results. Therefore, our future research is
to consider the interaction information of images from more
perspectives and effectively integrate them into a 3D CNN for
brain tumor segmentation.
ACKNOWLEDGMENT
The authors would like to thank Dr. Mohammad Havaei from
Université de Sherbrooke for some helpful discussions about
the experiment. The authors would also like to thank the
anonymous reviewers for their insightful comments, which
have greatly helped to improve the quality of this paper.