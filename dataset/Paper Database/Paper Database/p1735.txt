HAL Id: hal-00613637
 
Submitted on 2 Sep 2012
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
A Generalized Forward-Backward Splitting
Hugo Raguet, Jalal M. Fadili, Gabriel Peyré
To cite this version:
Hugo Raguet, Jalal M. Fadili, Gabriel Peyré.
A Generalized Forward-Backward Splitting.
Journal on Imaging Sciences, 2013, 6 (13), pp.1199-1226. ￿10.1137/120872802￿. ￿hal-00613637v5￿
A GENERALIZED FORWARD-BACKWARD SPLITTING
HUGO RAGUET∗, JALAL FADILI†, AND GABRIEL PEYRÉ∗
Abstract. This paper introduces a generalized forward-backward splitting algorithm for ﬁnding
a zero of a sum of maximal monotone operators B + Pn
i=1 Ai, where B is cocoercive. It involves the
computation of B in an explicit (forward) step and of the parallel computation of the resolvents of
the Ai’s in a subsequent implicit (backward) step. We prove its convergence in inﬁnite dimension,
and robustness to summable errors on the computed operators in the explicit and implicit steps. In
particular, this allows eﬃcient minimization of the sum of convex functions f + Pn
i=1 gi, where f
has a Lipschitz-continuous gradient and each gi is simple in the sense that its proximity operator is
easy to compute. The resulting method makes use of the regularity of f in the forward step, and
the proximity operators of the gi’s are applied in parallel in the backward step. While the forwardbackward algorithm cannot deal with more than n = 1 non-smooth function, we generalize it to
the case of arbitrary n. Examples on inverse problems in imaging demonstrate the advantage of the
proposed methods in comparison to other splitting algorithms.
Key words.
Forward-backward algorithm, monotone operator splitting, non-smooth convex
optimization, proximal splitting, image processing, sparsity.
AMS subject classiﬁcations.
1. Introduction. Throughout this paper, H denotes a real Hilbert space endowed with scalar product ⟨· | ·⟩and associated norm || · ||, Id is the identity operator
on H, and n is a positive integer.
1.1. Structured Monotone Inclusion and Minimization Problems. We
consider the following monotone inclusion problem
zer (B + Pn
= {x ∈H | 0 ∈Bx + Pn
where B : H 7→H is cocoercive, and for all i, Ai : H 7→2H is a maximal monotone
set-valued map. While such inclusion problems arise in various ﬁelds, our main motivation is to solve convex minimization problems. Indeed, it is well-known that the
subdiﬀerential ∂gi of a function gi ∈Γ0(H) is a maximal monotone map; Γ0(H) being the class of lower semicontinuous, proper, convex function from H to ]−∞, +∞].
If moreover f ∈Γ0(H) is diﬀerentiable with a Lipschitz continuous gradient, then
Baillon-Haddad’s theorem asserts that ∇f is cocoercive. Deﬁning F
the set of minimizers of F veriﬁes
argmin F = zer (∇f + Pn
provided that the following conditions hold
(H1) argmin F ̸= ∅,
(H2) (0, . . . , 0) ∈sri{(x −y1, . . . , x −yn)
x ∈H and ∀i, yi ∈dom gi},
where dom g
g(x) < +∞} denotes the domain of a function g and sri C
denotes the strong relative interior of a non-empty convex subset C of H . Therefore,
identifying B with ∇f and the Ai’s with the ∂gi’s, solving (1.1) allows to solve
= f(x) + Pn
i=1gi(x)} .
1CEREMADE, CNRS-Université Paris-Dauphine, Place du Maréchal De Lattre De Tassigny,
75775 Paris Cedex 16, France ( ).
2GREYC, CNRS-ENSICAEN-Université de Caen, 6, Bd du Maréchal Juin, 14050 Caen Cedex,
France ( )
H. Raguet, J. Fadili, G. Peyré
The structured monotone inclusion problem (1.1) is fairly general, and a wide
range of iterative algorithms to solve it take advantage of the speciﬁc properties of
the operators involved in the summand. As we will see, one crucial property is the
possibility to compute the resolvent of a maximal monotone operator A, denoted JA.
It is deﬁned as (see Section 4.1 for details)
x ∈y + Ay .
For a given x ∈H, computing JAx is in itself a monotone inclusion problem, but it
turns out that it can be solved explicitly for many operators, e.g. the action of the
resolvent can be easily computed in closed form. Our interest is in splitting methods to
solve (1.1): iterative algorithms that evaluate individually the operator B (cocoercive)
and the resolvents JAi, at various points of H, but not the resolvents of sums.
The next section recall several important previous works on splitting algorithms,
focusing on their application to convex optimization.
1.2. Splitting Methods for Minimization Problems. If g is a function in
Γ0(H), the resolvent of its subdiﬀerential, J∂g, can be shown (see Section 4.1) to be
equal to the Moreau’s proximity operator of g , deﬁned for all x ∈H as
2||x −y||2 + g(y) .
Again, this can be solved explicitly for many functions; such functions are dubbed
Another important property of some part of a functional to be minimized is
diﬀerentiability. Recalling (1.2), the forward-backward algorithm applies if f is diﬀerentiable with a Lipschitz continuous gradient, and n ≡1 with g1 simple. This scheme
consists in performing alternatively a gradient-descent (corresponding to an explicit
step on the function f) followed by a proximal step (corresponding to an implicit step
on the function g1). Such a scheme can be understood as a generalization of the projected gradient method. This algorithm, which ﬁnds its roots in numerical analysis
for PDE’s, has been well studied for solving monotone inclusion and convex optimization problems . Accelerated multistep versions or convex
optimization have been proposed , that enjoy a faster convergence rate of
O(1/k2) on the objective F in the general case, where k is the iteration counter.
Other splitting methods do not require any smoothness on any part of the composite functional F. The Douglas-Rachford scheme was originally developed to ﬁnd
the zeros of the sum of two linear operators , and then two non-linear operators in
 or two maximal monotone operators in , see also . This scheme applies
to minimizing g1 + g2, provided that g1 and g2 are simple. The backward-backward
algorithm can be used to minimize F = g1 +g2 when the functions involved are the indicator functions of non-empty closed convex sets, or involve Moreau
envelopes. Interestingly, if one of the functions g1 or g2 is a Moreau envelope and the
other is simple, the backward-backward algorithm amounts to a forward-backward
If L is a bounded injective linear operator, it is possible to minimize F = g1◦L+g2
by applying these splitting schemes on the Fenchel-Rockafellar dual problem. It was
shown that applying the Douglas-Rachford scheme leads to the alternating direction
method of multipliers (ADMM) . For non-necessarily injective L
and g2 strongly convex with a Lipschitz continuous gradient, the forward-backward
Generalized Forward-Backward Splitting
algorithm can be applied to the Fenchel-Rockafellar dual . Dealing with an
arbitrary bounded linear operator L can be achieved using primal-dual methods motivated by the classical Kuhn-Tucker theory. Starting from methods to solve saddle
function problems such as the Arrow-Hurwicz method and its modiﬁcation , the
extragradient method , this problem has received a lot of attention more recently
 .
It is also possible to extend the Douglas-Rachford algorithm to an arbitrary number n > 2 of simple functions. Inspired by the method of partial inverses [71, Section 5], most methods rely either explicitly or implicitly on introducing auxiliary variables and bringing back the original problem to the case n ≡2 in the product space
Hn. Doing so yields iterative schemes in which one performs independent parallel
proximal steps on each of the simple functions and then computes the next iterate by
essentially averaging the results. Variants have been proposed in , and in who
describe a general projective framework that does not reduce the problem to the case
n ≡2. These extensions however do not apply to the forward-backward scheme that
can only handle n ≡1. It is at the heart of this paper to present such an extension.
Recently proposed methods extend existing splitting schemes to handle the sum
of any number of n ≥2 composite functions of the form gi = hi ◦Li, where the
hi’s are simple and the Li’s are bounded linear operators. Let us denote Li
adjoint operator of Li. If Li satisﬁes LiLi
∗= ν Id for any ν > 0 (it is a so-called
tight frame), hi ◦Li is simple as soon as hi is simple and Li
∗is easy to compute .
This case thus reduces to the previously reviewed ones. If Li is not a tight frame
but (Id +Li
∗Li) or (Id +LiLi
∗) is easily invertible, it is again possible to reduce
the problem to the previous cases by introducing as many auxiliary variables as the
number of Li’s each belonging to the range of Li. Note however that, if solved with
the Douglas-Rachford algorithm on the product space, the auxiliary variables are
also duplicated, which would increase signiﬁcantly the dimensionality of the problem.
Some dedicated parallel implementations were speciﬁcally designed for the case where
∗Li) or (P
∗) is (easily) invertible, see for instance . If the Li’s
satisfy none of the above properties, it is still possible to call on primal-dual methods,
either by writing F = Pn
i=1 hi ◦Li = g ◦L with L(x) = (Li(x))i and g
i hi(xi), see for instance ; or on the product space F
i hi (Lixi) +
 , where ιS is the indicator function of the closed convex set S deﬁned
in Section 4.2.
In spite of the wide range of already existing proximal splitting methods, none
seems satisfying to address explicitly the case where n > 1 and f is smooth but not
necessarily simple.
A workaround that has been proposed previously used nested
algorithms to compute the proximity operator of P
i gi within sub-iterations, see for
instance ; this leads to practical as well as theoretical diﬃculties to select the
number of sub-iterations. More recently, proposed an algorithm for minimizing
F = f + g under linear constraints. We show in Section 2.3 how this can be adapted
to address the general problem (1.2) while achieving full splitting of the proximity
operators of the gi’s and using the gradient of f. In preparing a ﬁrst draft of this
manuscript, we became aware that other authors have independently and
concurrently developed primal-dual algorithms to solve problems that encompass the
one we consider here. These approaches and algorithms are however diﬀerent from
ours in many important ways. This will be discussed in detail in Section 2.3 especially
in relation to . We also report a suite of numerical experiments in Section 3 which
suggest that our primal algorithm is more adapted for imaging problems of the form
H. Raguet, J. Fadili, G. Peyré
1.3. Applications in Image Processing. Many imaging applications require
solving ill-posed inverse problems to recover high quality images from low-dimensional
and noisy observations. These challenging problems necessitate the use of regularization through prior models to capture the geometry of natural signals, images or
videos. Numerical solution of inverse problems can be achieved through minimization
of objective functionals, with respect to a high-dimensional variable, that takes into
account both a ﬁdelity term to the observations and regularization terms reﬂecting
the priors. Clearly, such functionals are composite by construction, hence ﬁtting in
the framework of (1.2). Section 3 details several examples of such inverse problems.
In many situations, this leads to the optimization of a convex functional that
can be split into the sum of convex smooth and nonsmooth terms.
The smooth
part of the objective is in some cases a data ﬁdelity term and reﬂects some speciﬁc
knowledge about the forward model, i.e. the noise and the measurement/degradation
operator. This is for instance the case if the operator is linear and the noise is additive
Gaussian, in which case the data ﬁdelity is a quadratic function. The most successful
regularizations that have been advocated are nonsmooth, which typically allow to
preserve sharp and intricate structures in the recovered data. Among such priors,
sparsity-promoting ones have become popular, e.g. the ℓ1-norm of coeﬃcients in a
wisely chosen dictionary , or total variation (TV) prior . To better model the
data, composite priors can be constructed by summing several suitable regularizations,
see for instance the morphological diversity framework . The proximity operator
of the ℓ1-norm penalization is a simple soft-thresholding , whereas the use of
complex or mixed regularization priors justiﬁes the splitting of nonsmooth terms in
several simpler functions (see Section 3 for concrete examples).
The composite structure of convex optimization problems raising when solving
inverse problems in the form of a sum of simple and/or smooth functions involving linear operators explains the popularity of proximal splitting schemes in imaging science.
Depending on the structure of the objective functional as detailed in the previous section, one can resort to the appropriate splitting algorithm. For instance, the forwardbackward algorithm and its modiﬁcations has become popular for sparse regularization
with a smooth data ﬁdelity, see for instance . The Douglas-
Rachford and its parallelized extensions were also used in a variety of inverse problems
implying only nonsmooth functions, see for instance .
The ADMM (which is nothing but Douglas-Rachford on the dual) was also applied to
some linear inverse problems in . Primal-dual schemes are among the
most ﬂexible schemes to handle more complicated priors. The interested reader may
refer to [72, Chapter 7] and for extensive reviews.
1.4. Contributions and Paper Organization. This paper introduces a novel
generalized forward-backward (GFB) algorithm to solve the monotone inclusion (1.1).
The algorithm achieves full splitting where all operators are used separately: an
explicit step for B (single-valued) and a parallelized implicit step through the resolvent
of the Ai’s.
We prove convergence of the algorithm even when summable errors
may contaminate the iterations. To the best of our knowledge, it is among the ﬁrst
algorithms to tackle the case where n > 1 (see Section 2.3 for relation to other
works). Although our numerical results are reported only on imaging applications,
the algorithm may prove useful for many other applications such as machine learning,
statistical estimation or optimal control.
Section 2 presents the algorithm and states our main theoretical result, before
Generalized Forward-Backward Splitting
commenting on some relevant aspects and on alternatives in the literature. Numerical examples are reported in Section 3 to show the usefulness of this approach for
imaging problems. The convergence proof is deferred to Section 4, after recalling some
preliminary results on monotone operator theory.
2. Generalized Forward-Backward Splitting.
2.1. The Algorithmic Scheme. We consider problem (1.1) where all operators
are maximal monotone, B is β-cocoercive with β ∈]0, +∞[, i.e.
β||Bx −By||2 ≤⟨Bx −By | x −y⟩,
and for all i and all γ > 0, JγAi (the resolvent of γAi) is easy to compute.
proposed generalized forward-backward algorithm is detailed in Algorithm 1.
Algorithm 1 A Generalized Forward-Backward Algorithm for solving (1.1).
β ∈]0, +∞[ is a cocoercivity constant of B.
(zi)i∈J1,nK ∈Hn,
(wi)i∈J1,nK ∈]0, 1]n s.t. Pn
i=1 wi = 1,
γ ∈]0, 2β[,
Initialization
Main iteration
for i ∈J1, nK do
zi ←zi + λk
 2x −zi −γBx
until convergence ;
To state our main theorem that ensures the convergence of the algorithm and its
robustness to summable errors, for each i let ε1,k,i be the error at iteration k when
computing J γ
wiAi, and let ε2,k be the error at iteration k when computing B. An
inexact GFB algorithm generates sequences (zi,k)k∈N, i ∈J1, nK and (xk)k∈N, such
that for all i ∈J1, nK and k ∈N,
zi,k+1 = zi,k + λk
 2xk −zi,k −γk (Bxk + ε2,k)
+ ε1,k,i −xk
Theorem 2.1. Suppose that zer (B + Pn
i=1Ai) ̸= ∅. Suppose that the following
assumptions are satisﬁed:
(i) 0 < infk∈N λk ≤supk∈N λk < min
k=0 ||ε2,k|| < +∞, and for all i, P+∞
k=0 ||ε1,k,i|| < +∞.
Then the sequence (xk)k∈N deﬁned in (2.2) converges weakly towards a solution of
Moreover, if ∀k ∈N, λk ≤1, then the convergence is strong if either B
is uniformly monotone, or "n
Ai is uniformly monotone.
The latter is true
for instance if ∀i ∈J1, nK, Ai is uniformly monotone with its modulus ϕ being also
subadditive or convex.
H. Raguet, J. Fadili, G. Peyré
The deﬁnition of uniform monotonicity and the function ϕ is provided in Section 4.1.
The following corollary specializes Theorem 1 to the case of convex optimization
problems of the form (1.2).
Corollary 2.2. Suppose that ∇f is Lipschitz continuous with constant 1/β, and
that (H1)-(H2) are veriﬁed. Substitute, in Algorithm 1 and in (2.2), B with ∇f and Ai
with ∂gi (i.e. J γ
wiAi with prox γ
wigi). Then under assumptions (i)-(ii) of Theorem 2.1,
the sequence (xk)k∈N converges weakly towards a minimizer of (1.2). Moreover, if
∀k ∈N, λk ≤1, then (xk)k∈N converges strongly to the unique minimizer of (1.2)
if either f is uniformly convex, or "n
∂gi is uniformly monotone. The latter is
true for instance if ∀i ∈J1, nK, gi is uniformly convex with its modulus ϕ being also
subadditive or convex.
The proofs are detailed in Section 4.
Remark 2.1. Recall that a function f ∈Γ0(H) is uniformly convex of modulus
ϕ : [0, +∞[→[0, +∞] if ϕ is a non-decreasing function that vanishes only at 0, such
that for all x and y in dom f, the following holds
∀ρ ∈]0, 1[, f(ρx + (1 −ρ)y) + ρ(1 −ρ)ϕ(||x −y||) ≤ρf(x) + (1 −ρ)f(y) .
The formulation of Algorithm 1 is general, but it can be simpliﬁed for practical
In particular, the auxiliary variables zi can all be initialized to 0, the
weights wi set equally to 1/n, and for simplicity the relaxation parameters λk can
be set to 1, constant along iterations. This is what has been done in the numerical
experiments described in Section 3.
2.2. Special instances. Our GFB algorithm can be viewed as a hybrid splitting
algorithm whose special instances turn out to be classical splitting methods; namely
the forward-backward and Douglas-Rachford algorithms.
Relaxed Forward-Backward. For n ≡1, the core update operator (2.1) of Algorithm 1 specializes to
x ←x + λk (JγA (x −γBx) −x) ,
so that xk given by (2.2) follows the iterations of the relaxed forward-backward algorithm [20, Section 6]. In this case, convergence can be ensured with step-size γ
varying along iterations, see discussion in Remark 4.3.
For convex minimization
problems, known results on convergence rate analysis (on the objective in general)
and accelerated versions of the forward-backward algorithm might be
inspiring to study those of our GFB (for the case where β > 0).
Relaxed Douglas-Rachford. If we set B ≡0, the update of the auxiliary variables
in (2.1) becomes
zi ←zi + λk
so that (zi,k)i given by (2.2) follow the iterations of the relaxed Douglas-Rachford
algorithm on the product space Hn for solving 0 ∈P
i Aix . The convergence
statements of Theorem 4.17-(a)-(c) hold by replacing the conditions on the relaxation parameters by ∀k ∈N, λk ∈]0, 2[ and P
k∈N λk(2 −λk) = +∞; this extends
Remark 4.1 to α = 1
2, by Proposition 4.12 (see Section 4.5).
Generalized Forward-Backward Splitting
Resolvents of the sum of monotone operators. Our GFB scheme provides yet
another way for computing the resolvent of the sum of maximal monotone operators
(Ai)i. Given a point y ∈ran (Id + P
i Ai), set in (1.1) B : x 7→x −y and β ≡1.
It would be interesting to compare this algorithm with the Douglas-Rachford and
Dykstra-based variants . This is left to a future work.
2.3. Relation to other works.
Relation to . The authors in [59, Section 5.3, (51)] describe an instance of the
“block-decomposition” hybrid proximal extragradient (HPE) for minimizing F = f +g
under linear constraints. (1.2) can be cast in an equivalent linearly constraint convex
programming
z=(zi)i∈Hn f (P
PS⊥(z) = 0 ,
where PS⊥is the orthogonal projector on the subspace S⊥def
= {z = (zi)i ∈Hn | P
i wizi = 0}.
As PS⊥is self-adjoint, z is an optimal solution if and only if there exists v = (vi)i ∈Hn
i + (∂gi(zi)/wi)i + PS⊥(v)
PS⊥(z) = 0 ,
and the minimizer of F is given by x = P
Let ς ∈]0, 1] and γ = ς
1+4ς2β2 . Transposed to our setting, their iterations are
presented in Algorithm 2.
Algorithm 2 Iterations of Block-Decomposition HPE .
for i ∈J1, nK do
zi ←prox γ
zi −γ∇f(x) + γ (vi −u)
for i ∈J1, nK do
vi ←vi −γzi + γx;
until convergence ;
The update of the zi’s in this iteration bears similarities with the one in Algorithm 1,
where the γ’s play analogous roles. Nonetheless, the two algorithms are diﬀerent. For
instance, our algorithm solves the primal problem while theirs solves both the primal
and dual problems. In addition, the objective in is to study complexity, hence
the diﬀerent set of assumptions.
In preparing a revised draft of this manuscript, it came to our attention that an
other adaptation of the block-decomposition HPE, exploiting the speciﬁc properties
of the linear constraints PS⊥(z) = 0 and changing the metric, leads to the iterations
(2.2) with ∀k ∈N, λk = 1, i.e. no under- nor over-relaxation. This could be an other
framework to study convergence properties of GFB.
Relation to . These authors independently developed another algorithm to
solve a general class of problems that covers (1.1). They rely on the classical Kuhn-
Tucker theory and propose a primal-dual splitting algorithm for solving monotone
H. Raguet, J. Fadili, G. Peyré
inclusions involving a mixture of sums, linear compositions, and parallel sums (infconvolution in convex optimization) of set-valued and Lipschitz operators. More precisely, the authors exploit the fact that the primal and dual problems have a similar
structure, cast the problem as ﬁnding a zero of the sum of a Lipschitz continuous
monotone map with a maximal monotone operator whose resolvent is easily computable. They solve the corresponding monotone inclusion using an inexact version
of Tseng’s forward-backward-forward splitting algorithm .
Removing the parallel sum, taking the linear operators as the identity in [26,
(1.1)], and assuming that the Lipschitz part is also cocoercive, one recovers problem
(1.1). For the sake of simplicity and space saving we do not reproduce here in full their
algorithm. However, adapted to the optimization problem minx∈H f(x)+P
i gi(Lix),
where each Li is a bounded linear operator, their scheme is presented in Algorithm 3
(gi∗is the Legendre-Fenchel conjugate of gi).
Algorithm 3 Iterations of Primal-Dual Algorithm of .
Choose a sequence (γk)k∈N in [ϵ, (1 −ϵ)/ζ], where ζ
i ||Li||2 and ϵ ∈
]0, 1/(1 + ζ)[.
y ←x −γk (∇f(x) + Pn
for i ∈J1, nK do
zi ←vi + γkLi (x);
vi ←vi −zi + proxγkgi∗(zi) + γkLi (y);
 ∇f (y) + Pn
∗ proxγkgi∗(zi)
until convergence ;
Recall that the proximity operator of gi∗can be easily deduced from that of gi using
Moreau’s identity. Taking Li = Id in Algorithm 3 solves (1.2). While we solve the
primal problem, their algorithm solves both the primal and dual ones. Note however
that it requires two calls to the gradient of f per iteration.
3. Numerical experiments. This section exempliﬁes the applicability of our
GFB splitting algorithm on image processing problems by solving some regularized
inverse problems. The problems are selected so that other splitting algorithms can be
applied as well and compared fairly. The parameters involved were manually selected
for each compared algorithm to achieve its best performance, for instance in terms of
energy decay. In the following, Id denotes the identity operator on the appropriate
space to be understood from the context, N is a positive integer and I ≡RN×N is
the set of images of size N × N pixels.
3.1. Variational Image Restoration. We consider a class of inverse problem regularizations, where one wants to recover an (unknown) high resolution image
y0 ∈I from noisy low resolution observations y = Φy0 + w ∈I. We report results
using several ill-posed linear operators Φ : I →I, and focus our attention on convolution and masking operator, and a combination of these operators. In the numerical
experiments, the noise vector w ∈I is a realization of an additive white Gaussian
noise of variance σ2
The restored image by0 = W bx is obtained by optimizing the coeﬃcients bx ∈H in
a redundant wavelet frame , where W : H →I is the wavelet synthesis operator.
Generalized Forward-Backward Splitting
The wavelet atoms are normalized so that W is a Parseval tight frame, i.e. it satisﬁes
In this setting, the coeﬃcients are vectors x ∈H ≡IJ where the
redundancy J = 3J0 +1 depends on the number of resolutions levels J0 of the wavelet
transform.
The general variational problem for the recovery reads
x∈H{F(x) ≡1
2||y −ΦWx||2 + µ||x||B
1,2 + ν||Wx||TV} .
The ﬁrst term in the summand is the data-ﬁdelity term, which is taken to be a squared
ℓ2-norm to reﬂect the additive white Gaussianity of the noise. The second and third
terms are regularizations, enforcing priors assumed to be satisﬁed by the original
image. The ﬁrst regularization is a ℓ1/ℓ2-norm by blocks, inducing structured sparsity
on the solution. The second regularization is a discrete total variation semi-norm,
inducing sparsity on the gradient of the restored image. The scalars µ and ν are
weights – so-called regularization parameters – to balance between each terms of the
energy F. We now detail the properties of each of these three terms.
3.1.1. Data-Fidelity
2||y −ΦWx||2. For the inpainting inverse problem, one
considers a masking operator
otherwise ,
where Ωis a set of pixels, taking into account missing or defective sensors; we will
denote ρ = |Ω|/N 2 the ratio of missing pixels. For the deblurring inverse problem, we
consider a convolution with a discrete Gaussian ﬁlter of width σK, K : y 7→GσK ∗y,
normalized to a unit mass. In the following, Φ will be either M, K or the composition
of both MK.
Denoting L
= ΦW, the ﬁdelity term thus reads f(x) = 1
2||y −Lx||2. The function
f corresponds to the smooth term in (1.2). Its gradient ∇f : x 7→L∗(Lx −y) is
Lipschitz continuous with constant β−1 ≤||ΦW||2 = 1.
For any γ > 0, the proximity operator of f reads
proxγf(x) = (Id +γL∗L)-1 (x + γL∗y) .
The vector L∗y can be precomputed, but inverting Id +γL∗L may be in general
computationally demanding. For inpainting or deblurring alone, as W is associated
to a Parseval tight frame, the Sherman-Morrison-Woodbury formula gives
(Id +γL∗L)-1 = Id −L∗(Id +γLL∗)-1L
= Id −W ∗Φ∗(Id +γΦΦ∗)-1ΦW .
Since M (resp. K) is a diagonal operator in the pixel domain (resp. Fourier domain),
(3.3) can be computed in O(N 2) (resp.
O(N 2 log N)) operations.
However, the
composite case L ≡MKW is more involved. A possible workaround1 is to introduce
an auxiliary variable, replacing f : H →R by ˜f : H × I →]−∞, +∞] deﬁned by
˜f(x, u) = 1
2||y −Mu||2 + ιCKW (x, u) = g1(u) + g2(x, u) ,
1In this special case where Φ = MK, an alternative would be to reapply the inversion lemma to
(3.3). But this does not work in general unlike the auxiliary variables approach.
H. Raguet, J. Fadili, G. Peyré
= {(x, u) ∈H × I | u = KWx}, and ιC is the indicator function of the
closed convex set C, i.e.
ιC(v) = 0 if v ∈C, and +∞otherwise.
Only then,
proxγg1 can be computed from (3.2), and proxγg2 is the orthogonal projection on
ker([Id, −KW]) , which involves a similar inversion as in (3.3).
3.1.2. Regularization µ||x||B
1,2. Sparsity-promoting regularizations with a synthesis-type prior over wavelet (or other transformed) coeﬃcients are popular to solve
a wide range of inverse problems . Figure 3.1(a) shows an example of orthogonal wavelet coeﬃcients of a natural image where most of the coeﬃcients have small
amplitude. A way to enforce this “sparsity” is to include in (3.1) the ℓ1-norm of the
coeﬃcients ||x||1 = P
The presence of edges or textures creates structured local dependencies in the
wavelet coeﬃcients of natural images. A way to take into account those dependencies
is to replace the absolute value of the coeﬃcients in the ℓ1-norm by the ℓ2-norm of
groups (or blocks) of coeﬃcients . This is known as the mixed
ℓ1/ℓ2-norm, deﬁned here as
µb||xb|| =
where p indexes the coeﬃcients, the blocks b are sets of indices, the block-structure B is
a collection of blocks and xb
= (xp)p∈b is a subvector of x indexed by b. The positive
scalars µb are weights tuning the inﬂuence of each block. (3.5) deﬁnes a norm on H as
soon as B covers the whole space, i.e. ∀p ∈J1, NK2×J1, JK, ∃b ∈B : p ∈b and µb > 0.
Note that for B ≡S
p {p} and µ{p} ≡1 for all p, it reduces to the ℓ1-norm.
We mentioned in the introduction that the proximal operator of the ℓ1-norm is
the coeﬃcient-wise soft-thresholding. Similarly, it is easy to show that whenever B is
a disjoint partition where the blocks are non-overlapping, i.e. ∀b, b′ ∈B, b ∩b′ = ∅,
the proximity operator of || · ||B
1,2 is the block-wise soft-thresholding
proxµ||·||B
 Θµb·µ(xb)
if ||xb|| < τ ,
otherwise ,
and the coeﬃcients xp not covered by B are left unaltered.
Non-overlapping block structures break the translation invariance that is underlying most traditional image models.
To restore this invariance, one can consider
overlapping blocks, as illustrated in Figure 3.1(c). Computing prox||·||B
1,2 in this case
is not as simple as for the non-overlapping case, because the blocks cannot be treated
independently. For tree-structured blocks (i.e. b ∩b′ ̸= ∅⇒b ⊂b′ or b′ ⊂b),
 proposes a method involving the computation of a min-cost ﬂow. This could be
computationally expensive and do not address the general case anyway.
it is always possible to decompose the block structure as a ﬁnite union of nonoverlapping sub-structures B = S
i Bi. The resulting term can ﬁnally be split into
b∈B ||xb|| = P
b∈Bi ||xb|| = P
1,2, where each || · ||Bi
1,2 is simple.
In our numerical experiments where H ≡IJ, coeﬃcients within each resolution
level (from 1 to J0) and each subband are grouped according to all possible square
spatial blocks of size S × S; which can be decomposed into S2 non-overlapping block
structures.
Generalized Forward-Backward Splitting
(a) ||x||1 = P
(b) ||x||B
b∈B ||xb||
(c) ||x||B
1,2 = ||x||B1
1,2 + ||x||B2
Figure 3.1: Illustration of the block ℓ1/ℓ2-norm.
(a) sparsity of the image in an
orthogonal wavelet decomposition (gray pixels corresponds to low coeﬃcients); (b) a
non-overlapping block structure; (c) splitting of a more complex overlapping block
structure into two non-overlapping layers.
3.1.3. Regularization ν||Wx||TV. The second regularization favors piecewisesmooth images, by inducing sparsity on its gradient . The total variation seminorm can be viewed as a speciﬁc instance of ℓ1/ℓ2-norm, ||y||TV = || grad y||BTV
1,2 , with
(V ∗y, H ∗y)
|| (v, h) ||BTV
where the image gradient is computed by ﬁnite diﬀerences through convolution with
a vertical ﬁlter V and a horizontal ﬁlter H, and BTV is clearly non-overlapping. For
some special gradient ﬁlters, the modiﬁed TV semi-norm can be split into simple
functions, see for instance . However, we consider more conventional ﬁlters
centered in the upper-left corner. Introducing an auxiliary variable as advocated in
(3.4), the main diﬃculty remains to invert the operator (Id +γ grad ◦grad∗), where
grad∗is the adjoint of the gradient (i.e. −the divergence operator). Under appropriate boundary conditions, this can be done in the Fourier domain in O(N 2 log(N))
operations.
3.2. Resolution with Splitting Methods.
3.2.1. Tested Algorithms. We now give the details of the diﬀerent splitting
strategies required to apply the three tested algorithms to (3.1).
Generalized Forward-Backward ( GFB). The problem is rewritten under the form
2||y −MKWx||2 + µ
1,2 + ν||u||BTV
1,2 + ιCgrad ◦W (x, u) ,
with f(x) ≡1
2||y −MKWx||2 and n ≡S2 + 2. The indicator function ιCgrad ◦W is
deﬁned similarly as in (3.4). In Algorithm 1, we set equal weights wi ≡1/n, a constant
gradient step-size γ ≡1.8β and a constant relaxation parameter to λ ≡1.
H. Raguet, J. Fadili, G. Peyré
Relaxed Douglas-Rachford ( DR). Here the problem is split as
2||y −M u1||2 + ιCKW (x, u1) + µ
1,2 + ν||u2||BTV
1,2 + ιCgrad ◦W (x, u2) ,
and solved with Algorithm 1, where f ≡0 and n ≡S2+4. As mentioned in Section 2.2,
this corresponds to a relaxed version of the Douglas-Rachford algorithm.
experiments, the best results were obtained for γ ≡1/n.
Primal-Dual Chambolle-Pock ( ChPo). A way to avoid operator inversions is to
rewrite the original problem as
 MKWx, x, . . . , x, grad ◦Wx
 u1, x1, . . . , xS2, g
2||y −u1||2 + µ PS2
i=1 ||xi||Bi
1,2 + ν||g||BTV
The operator Λ is a concatenation of linear operators and its adjoint is easy to compute, and g is simple, being a separable sum of simple functions. Note that this is
not the only splitting possible. For instance, one can write the problem on a product
(xi)i∈HιS((xi)i) + P
i gi(Λixi), where gi is each of the functions in g above,
and Λi is each of the linear operators in Λ.
To solve this, we here use the primal-dual relaxed Arrow-Hurwicz algorithm described in . According to the notations in that paper, we set the parameters σ ≡1,
σ(1+S2+8) and θ ≡1.
Block-Decomposition Hybrid Proximal Extragradient ( HPE). We split the problem written in (3.6) according to (2.3), and set equals weights wi ≡1/n. According
to Section 2.3, we set the parameter ς ≡0.9.
Primal-Dual Combettes-Pesquet ( CoPe). Finally, the problem takes its simplest
2||y −MKWx||2 + µ
1,2 + ν|| grad ◦Wx||BTV
As long as ν ≡0 (no TV-regularization), this is exactly (3.6); we apply Algorithm 3
where Li ≡Id for all i and γ ≡0.9/(1 + S).
However with TV-regularization,
we avoid the introduction of the auxiliary variable u with LS2+1 ≡grad ◦W and
γ ≡0.9/(1 +
3.2.2. Results. All experiments were performed on discrete images of width
N ≡256, with values in the range .
The additive white Gaussian noise has
standard-deviation σw ≡2.5·10−2. The reconstruction operator W uses separable bidimensional Daubechies wavelets with 2 vanishing moments. It is implemented such
Generalized Forward-Backward Splitting
that each atom has norm 2−j, with j ∈J1, J0K and where J0 is the coarsest resolution
level. Accordingly, we set the weights µb in the ℓ1/ℓ2-norm to 2−j at the resolution
level j of the coeﬃcients in block b. We use J0 ≡4, resulting in a dictionary with
redundancy J = 3J0 + 1 = 13. All algorithms are implemented in Matlab2.
Results are presented in Figures 3.2, 3.3, 3.4 and 3.5, Only one image is shown
here, but we obtained very similar results on other natural images (see 
ceremade.dauphine.fr/~raguet/gfb/). For each problem, the ﬁve algorithms were
run 1000 iterations (initialized at zero), while monitoring their objective functional
values F along iterations. Fmin is ﬁxed as the minimum value reached over the ﬁve
algorithms (in our experiments, this was always that of GFB), and evolution of the
objectives compared to Fmin is displayed for the ﬁrst 100 iterations.
Because the
computational complexity of an iteration may vary between algorithms, computation
times for 100 iterations (no parallel implementation) are given beside the curves.
Below the objective decay graph, one can ﬁnd from left to right the original image,
the degraded image and the restored image after 100 iterations of generalized forwardbackward. Degraded and restored images quality are given in term of signal-to-noise
ratio (SNR).
Comparison to algorithms that do not use the (gradient) explicit step ( ChPo,
DR). For the ﬁrst three experiments, there is no total variation regularization. In the
deblurring task (Figure 3.2), blocks of size 2×2 are used. GFB is slightly faster than
the others while the iteration cost of ChPo is much higher for this problem. When
increasing the block size (inpainting, Figure 3.3, size 4×4) computation times tend to
be similar but the decay of the objective provided by GFB is clearly faster than that
of other algorithms. The advantage of using the gradient information becomes even
more salient in the composite case (i.e. Φ ≡MK): in Figure 3.4, DR performs hardly
better than ChPo. Indeed, in contrast to the previous cases (see Section 3.1.1), f
is not simple anymore and the introduction of the auxiliary variable decreases the
eﬃciency of each iteration of DR. This phenomenon is further illustrated in the last
case, where the total variation is added, introducing another auxiliary variable.
Comparison to algorithms that use the (gradient) explicit step ( HPE, CoPe). In
the ﬁrst experiment where n is small, the iterations of the suggested block-decomposition HPE and CoPe are almost as eﬃcient as those of GFB but take more time
to compute, especially for CoPe that needs two calls to ∇f. Recall however that
HPE and CoPe solve both the primal and dual problems. In the second setting,
HPE and CoPe are hardly better than DR. They perform better in the composite
setting (i.e. Φ ≡MK), but require more computational time than GFB. In the last
setting, iterations of CoPe are still not as eﬃcient as those of GFB, despite the
higher computational load due to the composition by the linear operator grad ◦W.
Finally, let us note that in the composite case, the SNR of the restored image
is greater when using both regularizations rather than one or the other separately.
Moreover, we observed that it yields restorations more robust to variations of the
parameters µ and ν. Those arguments seem to be in favor of mixed regularizations.
4. Convergence Proofs. This section is dedicated to the proof of convergence
of the GFB. We ﬁrst recall some essential deﬁnitions and properties of monotone
operator theory that are necessary to our exposition. The interested reader may refer
to for a comprehensive treatment. As we will deal with maximal monotone
2The codes for reproducing the experiments, as well as results on other images, are available at
 
H. Raguet, J. Fadili, G. Peyré
(a) log(F −Fmin) vs. iteration #
(b) computing time
(c) LaBoute y0
(d) y = Ky0 + w, 19.63 dB
(e) by0 = W bx, 22.45 dB
Figure 3.2: Deblurring: σ = 2; µ = 1.3 · 10−3; S = 2; ν = 0.
(a) log(F −Fmin) vs. iteration #
(b) computing time
(c) LaBoute y0
(d) y = My0 + w, 1.54 dB
(e) by0 = W bx, 21.66 dB
Figure 3.3: Inpainting: ρ = 0.7; µ = 2.6 · 10−3; S = 4; ν = 0.
Generalized Forward-Backward Splitting
(a) log(F −Fmin) vs. iteration #
(b) computing time
(c) LaBoute y0
(d) y = MKy0 + w, 3.93 dB
(e) by0 = W bx, 20.77 dB
Figure 3.4: Composite: σ = 2; ρ = 0.4; µ = 1.0 · 10−3; S = 4; ν = 0.
(a) log(F −Fmin) vs. iteration #
(b) computing time
(c) LaBoute y0
(d) y = MKy0 + w, 3.93 dB
(e) by0 = W bx, 22.48 dB
Figure 3.5: Composite: σ = 2; ρ = 0.4; µ = 5.0 · 10−4; S = 4; ν = 5.0 · 10−3.
H. Raguet, J. Fadili, G. Peyré
operator splitting, we then introduce speciﬁc notations on the product space Hn. Finally, the proof itself is detailed in two steps. First, we derive an equivalent ﬁxed point
equation satisﬁed by any solution of (1.1). From this, we draw an algorithmic scheme
(equivalent to GFB) and establish its convergence properties and its robustness to
summable errors.
4.1. Deﬁnitions and Properties. In the following, A : H →2H is a set-valued
operator and T : dom T = H →H is a full-domain (see below), single-valued operator.
Id denotes the identity operator on H.
Definition 4.1 (Graph, inverse, domain, range and zeros).
The graph of A
is the set gra A
(x, y) ∈H2
. The inverse of A is the operator whose
graph is gra A-1
(x, y) ∈H2
(y, x) ∈gra A
The domain of A is dom A
{x ∈H | Ax ̸= ∅}. The range of A is ran A
= {y ∈H | ∃x ∈H : y ∈Ax}, and its
zeros set is zer A
= {x ∈H | 0 ∈Ax} = A-1 (0).
Definition 4.2 (Resolvent and reﬂection operators). The resolvent of A is the
operator JA
-1. The reﬂection operator associated to JA is the operator
= 2JA −Id.
Definition 4.3 (Maximal monotone operator). A is monotone if
(u ∈Ax and v ∈Ay) ⇒⟨u −v | x −y⟩≥0 .
It is moreover maximal monotone if its graph is not strictly contained in the graph of
any other monotone operator.
Definition 4.4 (Uniformly monotone operator). A is uniformly monotone of
modulus ϕ : [0, +∞[→[0, +∞] if ϕ is a non-decreasing function that vanishes only at
0, such that
(u ∈Ax and v ∈Ay) ⇒⟨u −v | x −y⟩≥ϕ(||x −y||) .
Definition 4.5 (Non-expansive and α-averaged operators). T is non-expansive
||Tx −Ty|| ≤||x −y|| .
For α ∈]0, 1[, T is α-averaged if there exists R non-expansive such that T = (1 −
α) Id +αR. We denote A(α) the class of α-averaged operators on H. In particular,
is the class of ﬁrmly non-expansive operators.
Definition 4.6 (cocoercive operator).
For β ∈]0, +∞[, T is β-cocoercive if
The following lemma gives some useful characterizations of ﬁrmly non-expansive
operators.
Lemma 4.7. The following statements are equivalent:
(i) T is ﬁrmly non-expansive;
(ii) 2T −Id is non-expansive;
(iii) ∀x, y ∈H, ||Tx −Ty||2 ≤⟨Tx −Ty | x −y⟩;
(iv) T is the resolvent of a maximal monotone operator A, i.e. T = JA.
Proof. (i) ⇔(ii), T ∈A
⇔T = Id +R
for some R non-expansive. (i) ⇔(iii),
see . (i) ⇔(iv), see .
Note that with (iii), one retrieves the characterization of the cocoercivity given in
Section 2.1. It follows by the Cauchy-Schwarz inequality that β-cocoercivity implies
Generalized Forward-Backward Splitting
1/β-Lipschitz continuity, but the converse is not true in general. It turns however to
be the case for gradients of convex functionals. We summarize here some properties
of the subdiﬀerential.
Lemma 4.8.
Let f : H →R be a convex diﬀerentiable function, with 1/β-
Lipschitz continuous gradient, β ∈]0, +∞[, and let g ∈Γ0(H). Then,
(i) β∇f ∈A
, i.e. is ﬁrmly non-expansive;
(ii) ∂g is maximal monotone;
(iii) The resolvent of ∂g is the proximity operator of g, i.e. proxg = J∂g.
Proof. (i) This is Baillon-Haddad theorem . (ii) See . (iii) See .
4.2. Product Space. Let (wi)i∈J1,nK ∈]0, 1]n such that Pn
i=1 wi = 1.
consider H
= Hn endowed with the scalar product ⟨⟨· || ·⟩⟩, deﬁned as
∀x = (xi)i , y = (yi)i ∈H,
⟨⟨x || y⟩⟩=
wi ⟨xi | yi⟩
and with the corresponding norm ||·||. S ⊂H denotes the non-empty closed convex set
deﬁned by S
= {x = (xi)i ∈H | x1 = x2 = · · · = xn}, whose orthogonal complement
is the closed linear subspace S⊥. We denote by Id the identity operator on H, and
we deﬁne the canonical isometry
C : H →S, x 7→(x, . . . , x) .
ιS : H →]−∞, +∞] and NS : H →2H are respectively the indicator function and
the normal cone of S, that is
otherwise ,
otherwise .
Since S is non-empty closed and convex, it is straightforward to see that NS is
maximal monotone.
We also introduce the following concatenated operators. Fix B and the Ai’s in
problem (1.1). Given γ = (γi)i∈J1,nK ∈]0, +∞[n, we deﬁne
γ•A : H →2H, x = (xi)i 7→
i=1 γiAi(xi) ,
i.e. its graph is
i=1 gra γiAi
(x, y) ∈H2
x = (xi)i , y = (yi)i , and ∀i, yi ∈γiAixi
and B : H →H, x = (xi)i 7→(Bxi)i. Using the maximal monotonicity of the Ai’s
and the β-cocoercivity of B, it is an easy exercise to establish that γ•A is maximal
monotone and B is β-cocoercive on H.
4.3. Fixed Point Equation. Now that we have all necessary material, let us
characterize solutions of (1.1).
Proposition 4.9. Let (wi)i∈J1,nK ∈]0, 1]n. For any γ > 0, x ∈H is a solution
of (1.1) if and only if there exists (zi)i∈J1,nK ∈Hn such that
 ∀i, zi = R γ
wiAi(2x −zi −γBx) −γBx ,
H. Raguet, J. Fadili, G. Peyré
Proof. set γ > 0, we have the equivalence
Aix ⇔∃(zi)i ∈Hn :
 ∀i, wi (x −zi −γBx) ∈γAix ,
wi (x −zi −γBx) ∈γAix ⇔(2x −zi −γBx) −x ∈γ
(by Lemma 4.7 (iv)) ⇔x = J γ
wiAi(2x −zi −γBx)
⇔2x −(2x −zi) = 2J γ
wiAi(2x −zi −γBx)
−(2x −zi −γBx) −γBx
wiAi(2x −zi −γBx) −γBx .
From now on, to lighten the notation, we denote PS
= JNS and RS
Before formulating our ﬁxed point equation, we need the following preparatory lemma.
Lemma 4.10. For all z = (zi)i ∈H, b = (b)i ∈S, and γ = (γi)i ∈]0, +∞[n,
(i) PS is the orthogonal projector on S, and PSz = C
(ii) RS (z −b) = RSz −b;
(iii) Rγ•Az =
 RγiAi(zi)
(i). From Lemma 4.8 (iii), we have for z ∈H,
PS(z) = argminy∈S ||z −y||
= projS(z) .
Now, argminy∈S ||z −y||2 = C
 argminy∈H
i wi||zi −y||2
, where the unique minimizer of P
i wi||zi −y||2 is the barycenter of (zi)i, i.e. P
(ii). PS is obviously linear, and so is RS. Since b ∈S, RSb = b and the result
(iii). This is a consequence of the separability of γ•A in terms of the components
of z implying that Jγ•Az = (JγiAizi)i. The result follows from the deﬁnition of Rγ•A.
In the sequel, we denote the set of ﬁxed points of an operator T : H →H by
= {z ∈H | T z = z}.
Proposition 4.11. (zi)i∈J1,nK ∈Hn satisﬁes (4.1) if and only if z = (zi)i is a
ﬁxed point of the following operator
Rγ•ARS + Id
Proof. Using Lemma 4.10 in (4.1), we have C(x) = PSz, C(Bx) = BPS(z) and
RS −γBPS = RS[Id −γBPS]. Altogether, this yields,
z satisﬁes (4.1) ⇔z = Rγ•ARS
⇔2z = Rγ•ARS
Rγ•ARS + Id
Generalized Forward-Backward Splitting
4.4. Properties of the Fixed Point Operator. Expression (4.2) gives us the
operator on which is based our GFB scheme. We ﬁrst study the properties of this
operator that will play a crucial role in the convergence proof.
Proposition 4.12. For any γ ∈]0, +∞[n, deﬁne
2 [Rγ•ARS + Id] z .
Then, T1,γ is ﬁrmly non-expansive, i.e. T1,γ ∈A
Proof. From Lemma 4.7 (i)⇔(ii), RγiAi and RS are non-expansive, and so is Rγ•A
in view of Lemma 4.10 (iii). Finally, as a composition of non-expansive operators,
Rγ•ARS is also non-expansive, and the proof is complete by deﬁnition of A
Proposition 4.13. For any γ ∈]0, 2β[, deﬁne
[Id −γBPS] z .
Then, T2,γ ∈A
Proof. By hypothesis, βB ∈A
and so is βB. Then, from Lemma 4.7 (iii), we
have for any x, y ∈H
||βBPSx −βBPSy||2 ≤⟨⟨βBPSx −βBPSy || PSx −PSy⟩⟩
= ⟨⟨βPSBPSx −βPSBPSy || x −y⟩⟩
= ⟨⟨βBPSx −βBPSy || x −y⟩⟩,
where we derive the ﬁrst equality from the fact that PS is self-adjoint (Lemma 4.10 (i)),
and the second equality using that for all x ∈H, BPSx ∈S. From Lemma 4.7 (iii)⇔(i),
we establish that βBPS ∈A
. We conclude using [20, Lemma 2.3].
Proposition 4.14. For all γ ∈]0, +∞[n and γ ∈]0, 2β[, T1,γT2,γ ∈A(α), with
Proof. As T1,γ and T2,γ are α-averaged operators by Proposition 4.12 and Proposition 4.13, it follows from [20, Lemma 2.2 (iii)] that their composition is also α-averaged
with the given value of α.
The following proposition deﬁnes a maximal monotone operator A′
γ which will be
useful for characterizing the operator T1,γ.
Proposition 4.15.
For all γ ∈]0, +∞[n there exists a maximal monotone
operator A′
γ such that T1,γ = JA′γ. Moreover for all γ > 0,
y = T1,γT2,γz ⇔z −y −γBPSz ∈A′
In particular,
ﬁx T1,γT2,γ = zer
Proof. The existence of A′
γ is ensured by Proposition 4.12 and Lemma 4.7 (iv).
Then for z ∈H,
y = T1,γT2,γz ⇔y =
-1 Id −γBPS
⇔z −γBPSz −y ∈A′
H. Raguet, J. Fadili, G. Peyré
Taking y = z proves the second statement.
Now, let us examine the properties of A′
Proposition 4.16. For all γ ∈]0, +∞[n and (u, y) ∈H2
γy ⇔uS −y⊥∈γ•A
where we denote for any v ∈H, vS def
= PS (v) and v⊥def
Proof. First of all, by deﬁnition of T1,γ we have
2 [(2Jγ•A −Id) (2PS −Id) + Id]
2 [2Jγ•A(PS −PS⊥) −(PS −PS⊥) + PS + PS⊥]
= Jγ•A(PS −PS⊥) + PS⊥.
Therefore,
γy ⇔T1,γ (u + y) = y
(by (4.8)) ⇔Jγ•A
(u + y)S −(u + y)⊥
= y −(u + y)⊥= yS −u⊥
⇔(u + y)S −(u + y)⊥−yS + u⊥∈γ•A
⇔uS −y⊥∈γ•A
4.5. Convergence. We are now ready to state the main convergence result of
our relaxed and inexact GFB splitting algorithm (2.2) to solve (1.1).
Theorem 4.17. Let γ ∈]0, 2β[, and set γ =
i ∈]0, +∞[n,
let (λk)k∈N be a sequence in
set z0 ∈H, and for every k ∈N, set
zk+1 = zk + λk
 T2,γzk + ε2,k
+ ε1,k −zk
where T1,γ (resp. T2,γ) is deﬁned in (4.3) (resp. in (4.4)), and ε1,k, ε2,k ∈H. If
(ii) 0 < infk∈N λk ≤supk∈N λk < min
k=0 ||ε1,k|| < +∞and P+∞
k=0 ||ε2,k|| < +∞.
are satisﬁed, then
 T1,γT2,γzk −zk
k∈N converges strongly to 0;
(b) (zk)k∈N converges weakly to a point z ∈ﬁx T1,γT2,γ;
k∈N converges weakly to x
i wizi ∈zer
(d) Moreover, if ∀k ∈N, λk ≤1, (xk)k∈N converges strongly in each of the
following cases:
(1) B is uniformly monotone.
Ai is uniformly monotone. For instance, this is true if ∀i ∈
J1, nK, Ai is uniformly monotone with the same modulus ϕ being also
subadditive or convex.
Generalized Forward-Backward Splitting
Proof. (a). Denoting T
= T1,γT2,γ, we have for all k ∈N,
zk+1 = zk + λk
 T zk + εk −zk
 T2,γzk + ε2,k
+ ε1,k. Proposition 4.12 shows that
is in particular non-expansive, so that ||εk|| ≤||ε2,k|| + ||ε1,k||, and we
deduce from (iii) that P+∞
k=0 ||εk|| < +∞. Moreover, by Proposition 4.14, T ∈A(α)
with α = max
. In particular, T is non-expansive and thus ﬁx T is closed
and convex. Now, for k ∈N, set Tk
= Id + λk (T −Id), the iterations (4.10) can be
zk+1 = Tkzk + λkεk .
Since (ii) provides for all k ∈N, αk
= λkα < 1, [20, Lemma 2.2 (i)] shows that Tk ∈
A(αk), and (4.11) is thus a particular instance of [20, Algorithm 4.1]. Also, it is clear
that for all k ∈N, ﬁx Tk = ﬁx T . Thus with Proposition 4.9 and Proposition 4.11, (i)
provides T
k∈N ﬁx Tk = ﬁx T ̸= ∅. According to (ii), infk∈N λk > 0 and supk∈N αk < 1,
so we deduce from [20, Theorem 3.1 and Remark 3.4] that
and that (zk)k∈N is quasi-Fejér monotone with respect to ﬁx T . By deﬁnition of Tk,
(4.12) gives P
< +∞, which in turn implies T zk −zk −→0
since infk∈N λk > 0.
(b). T being non-expansive, it follows from the demiclosed principle [6, Corollary 4.18] that any weak cluster point of (zk)k∈N belongs to ﬁx T , so that [6, Theorem 5.5] provides weak convergence towards z ∈ﬁx T .
For any y ∈H, ⟨y | xk −x⟩= ⟨y | P
i wi(zi,k −zi)⟩= P
i wi ⟨y | zi,k −zi⟩=
⟨⟨C(y) || zk −z⟩⟩. So, (b) provides weak convergence of (xk)k∈N towards x, which is a
zero of B + P
i Ai by Proposition 4.9.
(d). If moreover ∀k ∈N, λk ≤1, in view of Proposition 4.12 and Proposition 4.13,
(4.9) is immediately a particular instance of [20, Algorithm 4.1]. In particular, [20,
Theorem 3.1 and Remark 3.4] provides
(Id −T2,γ)zk −(Id −T2,γ)z
BPSzk −BPSz
(d)(1). Now, if B is uniformly monotone, then we have for all k ∈N,
⟨⟨BPSzk −BPSz || zk −z⟩⟩= P
iwi(zi,k −zi)
 ||xk −x||
From (b) and (4.13), we deduce that the right-hand side of the last inequality converges
to 0. In view of the properties of ϕ, we obtain strong convergence of (xk)k∈N towards
H. Raguet, J. Fadili, G. Peyré
(d)(2). Let u = −γBPSz and ∀k ∈N,
yk = T1,γT2,γzk
uk = (zk −yk) −γBPSzk .
We then have
≤||yk −zk|| + γ
BPSzk −BPSz
It then follows from (a) and (4.13) that uk converges strongly to u. On the other
hand, by Proposition 4.15, we have
Therefore, applying Proposition 4.16 to the pairs (z, u) and (yk, uk), and using the
fact that "n
Ai is uniformly monotone, we obtain
(uS −z⊥) −(uS
(zS −u⊥) −(yS
(zS −u⊥) −(yS
for some non-decreasing function ϕ : [0, +∞[→[0, +∞] that vanishes only at 0. We
(uS −z⊥) −(uS
(zS −u⊥) −(yS
k ) −(z⊥−y⊥
k ) −(u⊥−u⊥
= ⟨u −uk | z −yk⟩.
(zS −u⊥) −(yS
k ) −(u⊥−u⊥
since ϕ ◦√· : [0, +∞[→[0, +∞] is non-decreasing. Altogether, we arrive at
≤⟨u −uk | z −yk⟩.
By (a) and (b), yk converges weakly to z and we have shown that uk converges
strongly to u. This proves that ⟨u −uk | z −yk⟩→0 and therefore yS
k converges
strongly to zS = x in view of the properties of ϕ. The latter in conjunction with (a)
implies that zS
k = xk converges strongly to x.
It remains to show the special cases implying uniform monotonicity in (d)(2).
Indeed, if ∀i ∈J1, nK, Ai is uniformly monotone with the same modulus ϕ which is
Generalized Forward-Backward Splitting
also convex, then for (x, u) ∈gra γ•A and (y, v) ∈gra γ•A,
⟨⟨u −v || x −y⟩⟩=
wi ⟨ui −vi | xi −yi⟩≥
wiγiϕ (||xi −yi||)
i γi < ∞) ≥inf
wiϕ (||xi −yi||)
(ϕ is convex) ≥inf
wi||xi −yi||
(ϕ is non-decreasing) ≥inf
||xi −yi||2
(ϕ is non-decreasing and wi ∈]0, 1]) ≥inf
wi||xi −yi||2
i wi ||x −y||
The proof for the case where ϕ is subadditive follows the same lines using subadditivity instead of convexity in the inequalities, and replacing infi γi by γ (since by
deﬁnition γiwi = γ) and infi wi by 1.
Remark 4.1. For statements (a)-(c), assumptions (ii) can be weakened. More
precisely, (ii) can be replaced by P
k∈N λk(1−αλk) = +∞where α = max
and (iii) by P
t∈N λk(||ε1,k|| + ||ε2,k||) < +∞. The proof would follow the same lines
as [20, Lemma 5.1].
Remark 4.2 (Strong Convergence). We have proved strong convergence of the
sequence (xk)k∈N, but we did not elaborate on strong convergence of (zk)k∈N. It turns
out that the sequence (zk)k∈N is indeed quasi-Fejér monotone with respect to ﬁx T .
Thus, if int (ﬁx T ) ̸= ∅, [20, Lemma 2.8(iv)] provides strong convergence of (zk)k∈N,
and therefore of (xk)k∈N. An alternative suﬃcient condition is that A′
γ is demiregular;
see [4, Deﬁnition 2.3] and also [27, Condition 3.2] in the case of convex optimization.
Demiregularity occurs for instance if the operator has a boundedly relatively compact
domain (the intersection of its closure with any closed ball is compact); see [4, Proposition 2.4]. However, this condition is rather abstract and it is not easy to translate
it in terms of the properties of the individual Ai’s when n > 1.
Remark 4.3 (Non-stationary GFB). Convergence of the non-stationary version
of our inexact GFB splitting algorithm, i.e. for a varying sequence (γk)k∈N, can also
be established. More precisely, it can be shown that the statements of Theorem 4.17
hold under the additional assumption that 0 < γ ≤γk ≤γ < 2β and (γk −γ)k∈N
is absolutely summable where γ ∈[γ, γ]. The key idea underlying the proof consists
in viewing the non-stationary method as a perturbed version of the stationary method
with an additional error term (beside those previously considered in the implicit and
explicit steps), and to ensure that this error is also summable; see the initial work of
 in this direction. This absolute summability assumption on (γk −γ)k∈N can
be dropped for n ≡1, in which case we recover the forward-backward algorithm.
Finally, let us explicit the relationship between Theorem 4.17 and the claims of
Section 2.1.
Proof of Theorem 2.1.
It is straightforward to see that the vector whose coordinates are the sequences (zi,k)k∈N deﬁned in (2.2) follows iterations (4.9), with
H. Raguet, J. Fadili, G. Peyré
ε1,k = (ε1,k,i)i and ε2,k = C (−γε2,k), which are of course summable under the
required assumptions. Applying Theorem 4.17, the claimed convergence properties
Proof of Corollary 2.2. Under (H1)-(H2), [6, Theorem 16.2 and Theorem 16.37(i)]
provides that argmin(f+P
i gi) = zer (∇f + P
i ∂gi) ̸= ∅. Furthermore, in Lemma 4.8(i)
provides that ∇f is β-cocoercive and Lemma 4.8(iii) shows that J γ
wiAi corresponds to
wigi. Hence, weak convergence of (xk)k∈N towards a minimizer of (1.2) follows
from Theorem 4.17 (c). The proof of strong convergence is a consequence of Theorem 4.17 (d) together with the fact that uniform convexity of a function in Γ0(H)
implies uniform monotonicity of its subdiﬀerential .
5. Conclusion. We have introduced in this paper a novel splitting method for
ﬁnding a zero of a sum of an arbitrary number of maximal monotone operator. It
takes advantage of either the cocoercivity, or the possibility to compute the resolvent
of each operator separately. We provided theoretical guarantees on the convergence of
the algorithm and its robustness to summable errors. We emphasized the corresponding novel primal proximal splitting method for minimizing convex functionals that are
the sum of a smooth term and several simple functions. It generalizes some existing
schemes and enlarges the class of problems that can be solved eﬃciently with proximal splitting methods. Numerical experiments on convex optimization for inverse
problems show evidence of the advantages of our approach for large-scale imaging