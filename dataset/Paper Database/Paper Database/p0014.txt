Sanity Checks for Saliency Maps
Julius Adebayo∗, Justin Gilmer♯, Michael Muelly♯, Ian Goodfellow♯, Moritz Hardt♯†, Been Kim♯
 , {gilmer,muelly,goodfellow,mrtz,beenkim}@google.com
♯Google Brain
†University of California Berkeley
Saliency methods have emerged as a popular tool to highlight features in an input
deemed relevant for the prediction of a learned model. Several saliency methods
have been proposed, often guided by visual appeal on image data. In this work, we
propose an actionable methodology to evaluate what kinds of explanations a given
method can and cannot provide. We ﬁnd that reliance, solely, on visual assessment
can be misleading. Through extensive experiments we show that some existing
saliency methods are independent both of the model and of the data generating
process. Consequently, methods that fail the proposed tests are inadequate for
tasks that are sensitive to either data or model, such as, ﬁnding outliers in the data,
explaining the relationship between inputs and outputs that the model learned,
and debugging the model. We interpret our ﬁndings through an analogy with
edge detection in images, a technique that requires neither training data nor model.
Theory in the case of a linear model and a single-layer convolutional neural network
supports our experimental ﬁndings2.
Introduction
As machine learning grows in complexity and impact, much hope rests on explanation methods as
tools to elucidate important aspects of learned models . Explanations could potentially help
satisfy regulatory requirements , help practitioners debug their model , and perhaps, reveal
bias or other unintended effects learned by a model . Saliency methods3 are an increasingly
popular class of tools designed to highlight relevant features in an input, typically, an image. Despite
much excitement, and signiﬁcant recent contribution , the valuable effort of explaining machine
learning models faces a methodological challenge: the difﬁculty of assessing the scope and quality
of model explanations. A paucity of principled guidelines confound the practitioner when deciding
between an abundance of competing methods.
We propose an actionable methodology based on randomization tests to evaluate the adequacy
of explanation approaches. We instantiate our analysis on several saliency methods for image
classiﬁcation with neural networks; however, our methodology applies in generality to any explanation
approach. Critically, our proposed randomization tests are easy to implement, and can help assess the
suitability of an explanation method for a given task at hand.
In a broad experimental sweep, we apply our methodology to numerous existing saliency methods,
model architectures, and data sets. To our surprise, some widely deployed saliency methods are
independent of both the data the model was trained on, and the model parameters. Consequently,
∗Work done during the Google AI Residency Program.
2All code to replicate our ﬁndings will be available here: 
3We refer here to the broad category of visualization and attribution methods aimed at interpreting trained
models. These methods are often used for interpreting deep neural networks particularly on image data.
32nd Conference on Neural Information Processing Systems , Montréal, Canada.
 
SmoothGrad
Integrated
Integrated
SmoothGrad
Figure 1: Saliency maps for some common methods compared to an edge detector. Saliency
masks for 3 different inputs for an Inception v3 model trained on ImageNet. We see that an edge
detector produces outputs that are strikingly similar to the outputs of some saliency methods. In
fact, edge detectors can also produce masks that highlight features which coincide with what appears
to be relevant to a model’s class prediction. Interestingly, we ﬁnd that the methods that are most
similar to an edge detector, i.e., Guided Backprop and its variants, show minimal sensitivity to our
randomization tests.
these methods are incapable of assisting with tasks that depend on the model, such as debugging the
model, or tasks that depend on the relationships between inputs and outputs present in the data.
To illustrate the point, Figure 1 compares the output of standard saliency methods with those of an
edge detector. The edge detector does not depend on model or training data, and yet produces results
that bear visual similarity with saliency maps. This goes to show that visual inspection is a poor
guide in judging whether an explanation is sensitive to the underlying model and data.
Our methodology derives from the idea of a statistical randomization test, comparing the natural
experiment with an artiﬁcially randomized experiment. We focus on two instantiations of our general
framework: a model parameter randomization test, and a data randomization test.
The model parameter randomization test compares the output of a saliency method on a trained
model with the output of the saliency method on a randomly initialized untrained network of the
same architecture. If the saliency method depends on the learned parameters of the model, we should
expect its output to differ substantially between the two cases. Should the outputs be similar, however,
we can infer that the saliency map is insensitive to properties of the model, in this case, the model
parameters. In particular, the output of the saliency map would not be helpful for tasks such as model
debugging that inevitably depend on the model.
The data randomization test compares a given saliency method applied to a model trained on a
labeled data set with the method applied to the same model architecture but trained on a copy of the
data set in which we randomly permuted all labels. If a saliency method depends on the labeling of
the data, we should again expect its outputs to differ signiﬁcantly in the two cases. An insensitivity to
the permuted labels, however, reveals that the method does not depend on the relationship between
instances (e.g. images) and labels that exists in the original data.
Speaking more broadly, any explanation method admits a set of invariances, i.e., transformations
of data and model that do not change the output of the method. If we discover an invariance that is
incompatible with the requirements of the task at hand, we can safely reject the method. As such, our
tests can be thought of as sanity checks to perform before deploying a method in practice.
Our contributions
We propose two concrete, easy to implement tests for assessing the scope and quality of
explanation methods: the model parameter randomization test, and the data randomization test. Both
tests applies broadly to explanation methods.
2. We conduct extensive experiments with several explanation methods across data sets and model
architectures, and ﬁnd, consistently, that some of the methods tested are independent of both the
model parameters and the labeling of the data that the model was trained on.
3. Of the methods tested, Gradients & GradCAM pass the sanity checks, while Guided BackProp &
Guided GradCAM are invariant to higher layer parameters; hence, fail.
4. Consequently, our ﬁndings imply that the saliency methods that fail our proposed tests are incapable
of supporting tasks that require explanations that are faithful to the model or the data generating
5. We interpret our ﬁndings through a series of analyses of linear models and a simple 1-layer
convolutional sum-pooling architecture, as well as a comparison with edge detectors.
Methods and Related Work
In our formal setup, an input is a vector x ∈Rd. A model describes a function S : Rd →RC,
where C is the number of classes in the classiﬁcation problem. An explanation method provides an
explanation map E : Rd →Rd that maps inputs to objects of the same shape.
We now brieﬂy describe some of the explanation methods we examine. The supplementary materials
contain an in-depth overview of these methods. Our goal is not to exhaustively evaluate all prior
explanation methods, but rather to highlight how our methods apply to several cases of interest.
The gradient explanation for an input x is Egrad(x) = ∂S
∂x . The gradient quantiﬁes how
much a change in each input dimension would a change the predictions S(x) in a small neighborhood
around the input.
Gradient ⊙Input. Another form of explanation is the element-wise product of the input and the
gradient, denoted x ⊙∂S
∂x , which can address “gradient saturation”, and reduce visual diffusion .
Integrated Gradients (IG) also addresses gradient saturation by summing over scaled versions of
the input . IG for an input x is deﬁned as EIG(x) = (x −¯x) ×
∂S(¯x+α(x−¯x))
dα, where ¯x is
a “baseline input” that represents the absence of a feature in the original input x.
Guided Backpropagation (GBP) builds on the “DeConvNet” explanation method and
corresponds to the gradient explanation where negative gradient entries are set to zero while backpropagating through a ReLU unit.
Guided GradCAM. Introduced by Selvaraju et al. , GradCAM explanations correspond to the
gradient of the class score (logit) with respect to the feature map of the last convolutional unit of a
DNN. For pixel level granularity GradCAM, can be combined with Guided Backpropagation through
an element-wise product.
SmoothGrad (SG) seeks to alleviate noise and visual diffusion for saliency maps by
averaging over explanations of noisy copies of an input. For a given explanation map E, SmoothGrad
is deﬁned as Esg(x) = 1
i=1 E(x + gi), where noise vectors gi ∼N(0, σ2) are drawn i.i.d. from
a normal distribution.
Related Work
Other Methods & Similarities.
Aside gradient-based approaches, other methods ‘learn’ an explanation per sample for a model . More recently, Ancona et al. showed that
for ReLU networks (with zero baseline and no biases) the ϵ-LRP and DeepLift (Rescale) explanation
methods are equivalent to the input ⊙gradient. Similarly, Lundberg and Lee proposed SHAP
explanations which approximate the shapley value and unify several existing methods.
Fragility.
Ghorbani et al. and Kindermans et al. both present attacks against saliency
methods; showing that it is possible to manipulate derived explanations in unintended ways. Nie
et al. theoretically assessed backpropagation based methods and found that Guided BackProp
and DeconvNet, under certain conditions, are invariant to network reparamaterizations, particularly
random Gaussian initialization. Speciﬁcally, they show that Guided BackProp and DeconvNet both
seem to be performing partial input recovery. Our ﬁndings are similar for Guided BackProp and
its variants. Further, our work differs in that we propose actionable sanity checks for assessing
explanation approaches. Along similar lines, Mahendran and Vedaldi also showed that some
backpropagation-based saliency methods can often lack neuron discriminativity.
Current assessment methods.
Both Samek et al. and Montavon et al. proposed an input
perturbation procedure for assessing the quality of saliency methods. Dabkowski and Gal 
proposed an entropy based metric to quantify the amount of relevant information an explanation
mask captures. Performance of a saliency map on an object localization task has also been used for
assessing saliency methods. Montavon et al. discuss explanation continuity and selectivity as
measures of assessment.
Randomization.
Our label randomization test was inspired by the work of Zhang et al. ,
although we use the test for an entirely different purpose.
Visualization & Similarity Metrics
We discuss our visualization approach and overview the set of metrics used in assessing similarity
between two explanations.
Visualization.
We visualize saliency maps in two ways. In the ﬁrst case, absolute-value (ABS), we
take absolute values of a normalized map. For the second case, diverging visualization, we leave the
map as is, and use different colors to show positive and negative importance.
Similarity Metrics.
For quantitative comparison, we rely on the following metrics: Spearman rank
correlation with absolute value (absolute value), Spearman rank correlation without absolute value
(diverging), the structural similarity index (SSIM), and the Pearson correlation of the histogram of
gradients (HOGs) derived from two maps. We compute the SSIM and HOGs similarity metric on
ImageNet examples without absolute values4. SSIM and Pearson correlation of HOGs have been
used in literature to remove duplicate images and quantify image similarity. Ultimately, quantifying
human visual perception is still an active area of research.
Model Parameter Randomization Test
The parameter settings of a model encode what the model has learned from the data during training.
In particular, model parameters have a strong effect on test performance of the model. Consequently,
for a saliency method to be useful for debugging a model, it ought to be sensitive to model parameters.
As an illustrative example, consider a linear function of the form f(x) = w1x1 + w2x2 with input
x ∈R2. A gradient-based explanation for the model’s behavior for input x is given by the parameter
values (w1, w2), which correspond to the sensitivity of the function to each of the coordinates.
Changes in the model parameters therefore change the explanation.
Our proposed model parameter randomization test assesses an explanation method’s sensitivity
to model parameters. We conduct two kinds of randomization. First we randomly re-initialize
all weights of the model both completely and in a cascading fashion. Second, we independently
randomize a single layer at a time while keeping all others ﬁxed. In both cases, we compare the
resulting explanation from a network with random weights to the one obtained with the model’s
original weights.
Cascading Randomization
Overview. In the cascading randomization, we randomize the weights of a model starting from the
top layer, successively, all the way to the bottom layer. This procedure destroys the learned weights
from the top layers to the bottom ones. Figure 2 shows masks, for several saliency methods, for an
example input for the cascading randomization on an Inception v3 model trained on ImageNet. In
Figure 4, we show the two Spearman (absolute value and no-absolute value) metrics across different
data sets and architectures. Finally, in Figure 5, we show the SSIM and HOGs similarity metrics.
The gradient shows sensitivity while Guided Backprop is invariant to higher layer weights.
We ﬁnd that the gradient map is, indeed, sensitive to model parameter randomization. Similarly,
GradCAM is sensitive to model weights if the randomization is downstream of the last convolutional
layer. However, Guided Backprop (along with Guided GradCAM) is invariant to higher layer weights.
4We refer readers to the appendix for a discussion on calibration of these metrics.
SmoothGrad
Gradient Input
Back-propagation
Integrated Gradients
Integrated Gradients-SG
conv2d_1a_3x3
conv2d_2a_3x3
conv2d_2b_3x3
conv2d_4a_3x3
conv2d_3b_1x1
Original Explanation
Guided GradCAM
Cascading randomization
from top to bottom layers
Original Image
Figure 2: Cascading randomization on Inception v3 (ImageNet). Figure shows the original explanations (ﬁrst column) for the Junco bird as well as the label for each explanation type. Progression
from left to right indicates complete randomization of network weights (and other trainable variables)
up to that ‘block’ inclusive. We show images for 17 blocks of randomization. Coordinate (Gradient,
mixed_7b) shows the gradient explanation for the network in which the top layers starting from Logits
up to mixed_7b have been reinitialized. The last column corresponds to a network with completely
reinitialized weights. See Appendix for more examples.
Masks derived from Guided Backprop remain visually and quantitatively similar to masks of a trained
model until lower layer weights (those closest to the input) are randomized.5
The danger of the visual assessment. On visual inspection, we ﬁnd that gradient⊙input and
integrated gradients show visual similarity to the original mask. In fact, from Figure 2, it is still
possible to make out the structure of the bird even after multiple blocks of randomization. This visual
similarity is reﬂected in the SSIM comparison (Figure 5), and the rank correlation with absolute value
(Figure 4-Top). However, re-initialization disrupts the sign of the map, so that the spearman rank
correlation without absolute values goes to zero (Figure 4-Bottom) almost as soon as the top layers
are randomized. The observed visual perception versus ranking dichotomy indicates that naive visual
inspection of the masks, in this setting, does not distinguish networks of similar structure but widely
differing parameters. We explain the source of this phenomenon in our discussion section.
Independent Randomization
Overview. As a different form of the model parameter randomization test, we now conduct an independent layer-by-layer randomization with the goal of isolating the dependence of the explanations
by layer. This approach allows us to exhaustively assess the dependence of saliency masks on lower
versus higher layer weights. More concretely, for each layer, we ﬁx the weights of other layers to
their original values, and randomize one layer at a time.
Results. Figure 3 shows the evolution of different masks as each layer of Inception v3 is independently
randomized. We observe a correspondence between the results from the cascading and independent
layer randomization experiments: Guided Backprop (along with Guided GradCAM) show invariance
to higher layer weights. However, once the lower layer convolutional weights are randomized, the
Guided Backprop masks changes, although the resulting mask is still dominated by the input structure.
5A previous version of this work noted that Guided Backprop was entirely invariant; however, this is not this
SmoothGrad
Gradient Input
Back-propagation
Integrated Gradients
Integrated Gradients-SG
conv2d_1a_3x3
conv2d_2a_3x3
conv2d_2b_3x3
conv2d_4a_3x3
conv2d_3b_1x1
Original Explanation
Guided GradCAM
Independent ‘layer’ randomization
Original Image
Figure 3: Independent randomization on Inception v3 (ImageNet). Similar to Figure 2, however
each ‘layer’/‘block’ is randomized independently, i.e., the rest of the weights are kept at the pre-trained
values, while only each layer/block is randomized. Masks derived from these partially randomized
networks are shown here. We observe, again, that Guided Backprop is sensitive to only the lower
layer weights.
Data Randomization Test
The feasibility of accurate prediction hinges on the relationship between instances (e.g., images)
and labels encoded by the data. If we artiﬁcially break this relationship by randomizing the labels,
no predictive model can do better than random guessing. Our data randomization test evaluates the
sensitivity of an explanation method to the relationship between instances and labels. An explanation
method insensitive to randomizing labels cannot possibly explain mechanisms that depend on the
relationship between instances and labels present in the data generating process. For example, if an
explanation did not change after we randomly assigned diagnoses to CT scans, then evidently it did
not explain anything about the relationship between a CT scan and the correct diagnosis in the ﬁrst
place (see for an application of Guided BackProp as part of a pipepline for shadow detection in
2D Ultrasound).
In our data randomization test, we permute the training labels and train a model on the randomized
training data. A model achieving high training accuracy on the randomized training data is forced to
memorize the randomized labels without being able to exploit the original structure in the data. As it
turns out, state-of-the art deep neural networks can easily ﬁt random labels as was shown in Zhang
et al. .
In our experiments, we permute the training labels for each model and data set pair, and train the
model to greater than 95% training set accuracy. Note that the test accuracy is never better than
randomly guessing a label (up to sampling error). For each resulting model, we then compute
explanations on the same test bed of inputs for a model trained with true labels and the corresponding
model trained on randomly permuted labels.
Gradient is sensitive. We ﬁnd, again, that gradients, and its smoothgrad variant, undergo substantial changes. We also observe that GradCAM masks undergo changes that result in masks with
disconnected patches.
Sole reliance on the visual inspection can be misleading. For Guided BackProp, we observe a
visual change; however, we ﬁnd that the masks still highlight portions of the input that would seem
plausible, given correspondence with the input, on naive visual inspection. For example, from the
diverging masks (Figure 6-Right), we see that the Guided BackProp mask still assigns positive
relevance across most of the digit for the network trained on random labels.
Inception v3 - Imagenet
CNN - Fashion MNIST
MLP - MNIST
Correlation
Correlation
Figure 4: Cascading Randomization. Successive re-initialization of weights starting from top layers
for Inception v3 on ImageNet, CNN on Fashion MNIST, and MLP on MNIST. In all plots, y axis
is the rank correlation between original explanation and the randomized explanation derived for
randomization up to that layer/block, while the x axis corresponds to the layers/blocks of the DNN
starting from the output layer. The black dashed line indicates where successive randomization of
the network begins, which is at the top layer. Top: Spearman Rank correlation with absolute values,
Bottom: Spearman Rank correlation without absolute values.
SSIM: Inception v3 - ImageNet
HOGs Similarity: Inception v3 - ImageNet
Figure 5: Cascading Randomization. Figure showing SSIM and HOGs similarity between original
input masks and the masks generated as the Inception v3 is randomized in a cascading manner.
For gradient⊙input and integrated gradients, we also observe visual changes in the masks obtained,
particularly, in the sign of the attributions. Despite this, the input structure is still clearly prevalent in
the masks. The effect observed is particularly prominent for sparse inputs like MNIST where most of
the input is zero; however, we observe similar effects for Fashion MNIST (see Appendix), which is
less sparse. With visual inspection alone, it is not inconceivable that an analyst could confuse the
integrated gradient and gradient⊙input masks derived from a network trained on random labels as
legitimate. We clarify these ﬁndings and address implications in the discussion section.
CNN - MNIST
SmoothGrad
Integrated
Integrated
Gradients-SG
SmoothGrad
Integrated
Integrated
Gradients-SG
Rank Correlation - Abs
Rank Correlation - No Abs
Absolute-Value Visualization
Diverging Visualization
Figure 6: Explanation for a true model vs. model trained on random labels. Top Left: Absolutevalue visualization of masks for digit 0 from the MNIST test set for a CNN. Top Right: Saliency
masks for digit 0 from the MNIST test set for a CNN shown in diverging color. Bottom Left:
Spearman rank correlation (with absolute values) bar graph for saliency methods. We compare the
similarity of explanations derived from a model trained on random labels, and one trained on real
labels. Bottom Right: Spearman rank correlation (without absolute values) bar graph for saliency
methods for MLP. See appendix for corresponding ﬁgures for CNN, and MLP on Fashion MNIST.
Discussion
We now take a step back to interpret our ﬁndings. First, we discuss the inﬂuence of the model
architecture on explanations derived from NNs. Second, we consider methods that approximate an
element-wise producet of the input and the gradient, as several local explanations do . We
show, empirically, that the input “structure” dominates the gradient, especially for sparse inputs.
Third, we explain the observed behavior of the gradient explanation with an appeal to linear models.
We then consider a single 1-layer convolution with sum-pooling architecture, and show that saliency
explanations for this model mostly capture edges. Finally, we return to the edge detector and make
comparisons between methods that fail our sanity checks and an edge detector.
The role of model architecture as a prior
The architecture of a deep neural network has an important effect on the representation derived from
the network. A number of results speak to the strength of randomly initialized models as classiﬁcation
priors . Moreover, randomly initialized networks trained on a single input can perform tasks
like denoising, super-resolution, and in-painting without additional training data. These prior
works speak to the fact that randomly initialized networks correspond to non-trivial representations.
Explanations that do not depend on model parameters or training data might still depend on the
model architecture and thus provide some useful information about the prior incorporated in the
model architecture. However, in this case, the explanation method should only be used for tasks
where we believe that knowledge of the model architecture on its own is sufﬁcient for giving useful
explanations.
Element-wise input-gradient products
A number of methods, e.g., ϵ-LRP, DeepLift, and integrated gradients, approximate the element-wise
product of the input and the gradient (on a piecewise linear function like ReLU). To gain further
insight into our ﬁndings, we can look at what happens to the input-gradient product E(x) = x⊙∂S
the input is kept ﬁxed, but the gradient is randomized. To do so, we conduct the following experiment.
For an input x, sample two normal random vectors u, v (we consider both the truncated normal and
uniform distributions) and consider the element-wise product of x with u and v, respectively, i.e.,
x ⊙u, and x ⊙v. We then look at the similarity, for all the metrics considered, between x ⊙u and
x ⊙v as noise increases. We conduct this experiment on Fashion MNIST and ImageNet samples.
We observe that the input does indeed dominate the product (see Figure 19 in Appendix). We also
observe that the input dominance persists even as the noisy gradient vectors change drastically. This
experiment indicates that methods that approximate the “input-times-gradient” mostly return the
input, in cases where the gradients look visually noisy as they tend to do.
Analysis for simple models
SmoothGrad
Gray Scale
Figure 7: Explanations derived for the
1-layer Sum-Pooling Convolution architecture. We show gradient, SmoothGrad,
Integrated Gradients and Guided Back-
Prop explanations.
To better understand our ﬁndings, we analyze the output of
the saliency methods tested on two simple models: a linear
model and a 1-layer sum pooling convolutional network.
We ﬁnd that the output of the saliency methods, on a
linear model, returns a coefﬁcient that intuitively measures
the sensitivity of the model with respect to that variable.
However, these methods applied to a random convolution
seem to result in visual artifacts that are akin to an edge
Linear Model.
Consider a linear model f : Rd →R
deﬁned as f(x) = w · x where w ∈Rd are the model
weights. For gradients we have Egrad(x) = ∂(w·x)
Similarly for SmoothGrad we have Esg(x) = w (the gradient is independent of the input, so averaging gradients over
noisy inputs yields the same model weight). Integrated
Gradients reduces to “gradient ⊙input” for this case:
EIG(x) = (x −¯x) ⊙
∂f(¯x + α(x −¯x))
= (x −¯x) ⊙
wαdα = (x −¯x) ⊙w/2 .
Consequently, we see that the application of the basic gradient method to a linear model will pass our
sanity check. Gradients on a random model will return an image of white noise, while integrated
gradients will return a noisy version of the input image. We did not consider Guided Backprop and
GradCAM here because both methods are not deﬁned for the linear model considered above.
1 Layer Sum-Pool Conv Model.
We now show that the application of these same methods to a
1-layer convolutional network may result in visual artifacts that can be misleading unless further
analysis is done. Consider a single-layer convolutional network applied to a grey-scale image
x ∈Rn×n. Let w ∈R3×3 denote the 3 × 3 convolutional ﬁlter, indexed as wij for i, j ∈{−1, 0, 1}.
We denote by w ∗x ∈Rn×n the output of the convolution operation on the image x. Then the output
of this network can be written as l(x) =
σ(w ∗x)ij , where σ is the ReLU non-linearity
applied point-wise. In particular, this network applies a single 3x3 convolutional ﬁlter to the input
image, then applies a ReLU non-linearity and ﬁnally sum-pools over the entire convolutional layer
for the output. This is a similar architecture to the one considered in . As shown in Figure 7, we
see that different saliency methods do act like edge detectors. This suggests that the convolutional
structure of the network is responsible for the edge detecting behavior of some of these saliency
To understand why saliency methods applied to this simple architecture visually appear to be edge
detectors, we consider the closed form of the gradient
∂xij l(x). Let aij = 1 {(w ∗x)ij ≥0} indicate
the activation pattern of the ReLU units in the convolutional layer. Then for i, j ∈[2, n −1] we have
σ′((w ∗x)i+k,j+l)wkl =
ai+k,j+lwkl
(Recall that σ′(x) = 0 if x < 0 and 1 otherwise). This implies that the 3 × 3 activation pattern local
to pixel xij uniquely determines
∂xij . It is now clear why edges will be visible in the produced
saliency mask — regions in the image corresponding to an “edge” will have a distinct activation
pattern from surrounding pixels. In contrast, pixel regions of the image which are more uniform will
all have the same activation pattern, and thus the same value of
∂xij l(x). Perhaps a similar principle
applies for stacked convolutional layers.
The case of edge detectors.
An edge detector, roughly speaking, is a classical tool to highlight sharp transitions in an image.
Notably, edge detectors are typically untrained and do not depend on any predictive model. They are
solely a function of the given input image. As some of the saliency methods we saw, edge detection
is invariant under model and data transformations.
In Figure 1 we saw that edge detectors produce images that are strikingly similar to the outputs of
some saliency methods. In fact, edge detectors can also produce pictures that highlight features which
coincide with what appears to be relevant to a model’s class prediction. However, here the human
observer is at risk of conﬁrmation bias when interpreting the highlighted edges as an explanation of
the class prediction. In Figure 37 (Appendix), we show a qualitative comparison of saliency maps of
an input image with the same input image multiplied element-wise by the output of an edge detector.
The result indeed looks strikingly similar, illustrating that saliency methods mostly use the edges of
the image.
While edge detection is a fundamental and useful image processing technique, it is typically not
thought of as an explanation method, simply because it involves no model or training data. In light of
our ﬁndings, it is not unreasonable to interpret some saliency methods as implicitly implementing
unsupervised image processing techniques, akin to edge detection, segmentation, or denoising. To
differentiate such methods from model-sensitive explanations, visual inspection is insufﬁcient.
Conclusion and future work
The goal of our experimental method is to give researchers guidance in assessing the scope of model
explanation methods. We envision these methods to serve as sanity checks in the design of new model
explanations. Our results show that visual inspection of explanations alone can favor methods that
may provide compelling pictures, but lack sensitivity to the model and the data generating process.
Invariances in explanation methods give a concrete way to rule out the adequacy of the method for
certain tasks. We primarily focused on invariance under model randomization, and label randomization. Many other transformations are worth investigating and can shed light on various methods
we did and did not evaluate. Along these lines, we hope that our paper is a stepping stone towards a
more rigorous evaluation of new explanation methods, rather than a verdict on existing methods.
Acknowledgments
We thank the Google PAIR team for open source implementation of the methods used in this work.
We thank Martin Wattenberg and other members of the Google Brain team for critical feedback
and helpful discussions that helped improved the work. Lastly, we thank anonymous reviewers for
feedback that helped improve the manuscript. We are also grateful to Leon Sixt for pointing out a
bug in our Guided Backprop experiments in an earlier version of this work.