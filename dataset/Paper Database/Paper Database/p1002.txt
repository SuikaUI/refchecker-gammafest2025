The Link-Prediction Problem for Social Networks∗
David Liben-Nowell
Department of Computer Science
Carleton College
Northﬁeld, MN 55057 USA
 
Jon Kleinberg
Department of Computer Science
Cornell University
Ithaca, NY 14853 USA
 
Given a snapshot of a social network, can we infer which new interactions among its members
are likely to occur in the near future? We formalize this question as the link-prediction problem,
and we develop approaches to link prediction based on measures for analyzing the “proximity”
of nodes in a network. Experiments on large co-authorship networks suggest that information
about future interactions can be extracted from network topology alone, and that fairly subtle
measures for detecting node proximity can outperform more direct measures.
Introduction
As part of the recent surge of research on large, complex networks and their properties, a considerable amount of attention has been devoted to the computational analysis of social networks—
structures whose nodes represent people or other entities embedded in a social context, and whose
edges represent interaction, collaboration, or inﬂuence between entities. Natural examples of social
networks include the set of all scientists in a particular discipline, with edges joining pairs who have
co-authored papers; the set of all employees in a large company, with edges joining pairs working
on a common project; or a collection of business leaders, with edges joining pairs who have served
together on a corporate board of directors. The increased availability of large, detailed datasets
encoding such networks has stimulated extensive study of their basic properties, and the identi-
ﬁcation of recurring structural features. (See, for example, the work of Watts and Strogatz ,
Watts , Grossman , Newman , and Adamic and Adar , or, for a thorough recent
survey, Newman .)
Social networks are highly dynamic objects; they grow and change quickly over time through
the addition of new edges, signifying the appearance of new interactions in the underlying social
structure. Identifying the mechanisms by which they evolve is a fundamental question that is still
not well understood, and it forms the motivation for our work here. We deﬁne and study a basic
computational problem underlying social-network evolution, the link-prediction problem: Given a
snapshot of a social network at time t, we seek to accurately predict the edges that will be added
to the network during the interval from time t to a given future time t′.
∗Appears in Journal of the American Society for Information Science and Technology, 58(7):1019–1031, May 2007.
An abbreviated preliminary version of this paper appears in Proceedings of the Twelfth Annual ACM International
Conference on Information and Knowledge Management (CIKM’03), November 2003, pp. 556–559.
In eﬀect, the link-prediction problem asks: to what extent can the evolution of a social network
be modeled using features intrinsic to the network itself? Consider a co-authorship network among
scientists, for example. There are many reasons exogenous to the network why two scientists who
have never written a paper together will do so in the next few years: for example, they may happen
to become geographically close when one of them changes institutions. Such collaborations can be
hard to predict. But one also senses that a large number of new collaborations are hinted at by
the topology of the network: two scientists who are “close” in the network will have colleagues in
common, and will travel in similar circles; this social proximity suggests that they themselves are
more likely to collaborate in the near future. Our goal is to make this intuitive notion precise, and to
understand which measures of “proximity” in a network lead to the most accurate link predictions.
We ﬁnd that a number of proximity measures lead to predictions that outperform chance by factors
of forty to ﬁfty, indicating that the network topology does indeed contain latent information from
which to infer future interactions. Moreover, certain fairly subtle measures—involving inﬁnite sums
over paths in the network—often outperform more direct measures, such as shortest-path distances
and numbers of shared neighbors.
We believe that a primary contribution of the present paper is in the area of network-evolution
models. While there has been a proliferation of such models in recent years—see, for example, the
work of Jin et al. , Barabasi et al. , and Davidsen et al. for recent work on collaboration
networks, or the survey of Newman —these models have generally been evaluated only by asking
whether they reproduce certain global structural features observed in real networks. As a result,
it has been diﬃcult to evaluate and compare diﬀerent approaches on a principled footing. Link
prediction, on the other hand, oﬀers a very natural basis for such evaluations: a network model is
useful to the extent that it can support meaningful inferences from observed network data. One sees
a related approach in recent work of Newman , who considers the correlation between certain
network-growth models and data on the appearance of edges of co-authorship networks.
In addition to its role as a basic question in social-network evolution, the link-prediction problem
could be relevant to a number of interesting current applications of social networks. Increasingly, for
example, researchers in artiﬁcial intelligence and data mining have argued that a large organization,
such as a company, can beneﬁt from the interactions within the informal social network among
its members; these ties serve to supplement the oﬃcial hierarchy imposed by the organization
itself . Eﬀective methods for link prediction could be used to analyze such a social network
to suggest promising interactions or collaborations that have not yet been identiﬁed within the
organization. In a diﬀerent vein, research in security has recently begun to emphasize the role of
social-network analysis, largely motivated by the problem of monitoring terrorist networks; link
prediction in this context allows one to conjecture that particular individuals are working together
even though their interaction has not been directly observed .
The link-prediction problem is also related to the problem of inferring missing links from an
observed network: in a number of domains, one constructs a network of interactions based on
observable data and then tries to infer additional links that, while not directly visible, are likely
to exist . This line of work diﬀers from our problem formulation in that it works with
a static snapshot of a network, rather than considering network evolution; it also tends to take
into account speciﬁc attributes of the nodes in the network, rather than evaluating the power of
prediction methods that are based purely on the graph structure.
We turn to a description of our experimental setup in Section 2.
Our primary focus is on
understanding the relative eﬀectiveness of network-proximity measures adapted from techniques
in graph theory, computer science, and the social sciences, and we review a large number of such
techniques in Section 3. Finally, we discuss the results of our experiments in Section 4.
Data and Experimental Setup
Suppose that we have a social network G = ⟨V, E⟩in which each edge e = ⟨u, v⟩∈E represents
an interaction between u and v that took place at a particular time t(e).
We record multiple
interactions between u and v as parallel edges, with potentially diﬀerent time-stamps. For two
times t < t′, let G[t, t′] denote the subgraph of G consisting of all edges with a time-stamp between t
and t′. Here, then, is a concrete formulation of the link-prediction problem. We choose four times
0 < t1 < t′
1 and give an algorithm access to the network G[t0, t′
0]; it must then output a list
of edges not present in G[t0, t′
0] that are predicted to appear in the network G[t1, t′
1]. We refer to
0] as the training interval and [t1, t′
1] as the test interval.
Of course, social networks grow through the addition of nodes as well as edges, and it is not
sensible to seek predictions for edges whose endpoints are not present in the training interval. Thus,
in evaluating link-prediction methods, we will generally use two parameters κtraining and κtest, and
deﬁne the set Core to consist of all nodes that are incident to at least κtraining edges in G[t0, t′
and at least κtest edges in G[t1, t′
1]. We will then evaluate how accurately the new edges between
elements of Core can be predicted.
We now describe our experimental setup more speciﬁcally. We work with ﬁve co-authorship
networks G, obtained from the author lists of papers contained in ﬁve sections of the physics e-
Print arXiv, www.arxiv.org. (See Figure 1 for statistics on the sizes of each of these ﬁve networks.)
Some heuristics were used to deal with occasional syntactic anomalies, and authors were identiﬁed
by ﬁrst initial and last name, a process that introduces a small amount of noise due to multiple
authors with the same identiﬁer . The errors introduced by this process appear to be minor.
Now consider any one of these ﬁve graphs. We deﬁne the training interval to be the three years
from 1994 through 1996, and the test interval to be the three years from 1997 through 1999. We
denote the subgraph G on the training interval by Gcollab := ⟨A, Eold⟩and use Enew to
denote the set of edges ⟨u, v⟩such that u, v ∈A, and u, v co-author a paper during the test interval
but not the training interval—these are the new interactions we are seeking to predict. In our
experiments on the arXiv, we can identify which authors are active throughout the entire period
on the basis of the number of papers published and not on the number of coauthors. Thus here we
deﬁne the set Core to consist of all authors who have written at least κtraining := 3 papers during
the training period and at least κtest := 3 papers during the test period.
Evaluating a link predictor.
Each link predictor p that we consider outputs a ranked list Lp of
pairs in A×A−Eold; these are predicted new collaborations, in decreasing order of conﬁdence. For
our evaluation, we focus on the set Core, so we deﬁne E∗
new := Enew ∩(Core×Core) and n := |E∗
Our performance measure for predictor p is then determined as follows: from the ranked list Lp,
we take the ﬁrst n pairs that are in Core × Core, and determine the size of the intersection of this
set of pairs with the set E∗
1A collaboration is an ordered pair of authors who have written at least one paper together during the training
period. This number is odd in the cond-mat dataset because in that arXiv section there were three (an odd number)
instances of “self-collaboration”—where two authors of the same paper have the same ﬁrst initial and last name; the
two researchers are therefore conﬂated into a single node x, and a collaboration between x and x is recorded. These
training period
collaborations1
Figure 1: The ﬁve sections of the arXiv from which co-authorship networks were constructed:
astro-ph (astrophysics), cond-mat (condensed matter), gr-qc (general relativity and quantum
cosmology), hep-ph (high energy physics—phenomenology), and hep-th (high energy physics—
theory). The set Core is the subset of the authors who have written at least κtraining = 3 papers
during the training period and κtest = 3 papers during the test period .
The sets Eold and Enew denote undirected edges between Core authors that ﬁrst appear during the
training and test periods, respectively.
Methods for Link Prediction
In this section, we survey an array of methods for link prediction.
All the methods assign a
connection weight score(x, y) to pairs of nodes ⟨x, y⟩, based on the input graph Gcollab, and then
produce a ranked list in decreasing order of score(x, y). Thus, they can be viewed as computing a
measure of proximity or “similarity” between nodes x and y, relative to the network topology. In
general, the methods are adapted from techniques used in graph theory and social-network analysis;
in a number of cases, these techniques were not designed to measure node-to-node similarity and
hence need to be modiﬁed for this purpose. Figure 2 summarizes most of these measures; below we
discuss them in more detail. We note that some of these measures are designed only for connected
graphs; because each graph Gcollab that we consider has a giant component—a single component
containing most of the nodes—it is natural to restrict the predictions for these measures to this
component.
Perhaps the most basic approach is to rank pairs ⟨x, y⟩by the length of their shortest path in
Gcollab. Such a measure follows the notion that collaboration networks are “small worlds,” in which
individuals are related through short chains . (In keeping with the notion that we rank pairs
in decreasing order of score(x, y), we deﬁne score(x, y) here to be the negative of the shortest path
length.) Pairs with shortest-path distance equal to one are joined by an edge in Gcollab, and hence
they belong to the training edge set Eold. For all of our graphs Gcollab, there are well more than n
pairs at shortest-path distance two, so our shortest-path predictor simply selects a random subset
of these distance-two pairs.
Methods based on node neighborhoods.
For a node x, let Γ(x) denote the set of neighbors
of x in Gcollab. A number of approaches are based on the idea that two nodes x and y are more
likely to form a link in the future if their sets of neighbors Γ(x) and Γ(y) have large overlap; this
approach follows the natural intuition that such nodes x and y represent authors who have many
colleagues in common and hence who are more likely to come into contact themselves. Jin et al. 
self-collaborations are examples of the rare errors that are introduced because multiple authors are mapped to the
same identiﬁer.
graph distance
(negated) length of shortest path between x and y
common neighbors
|Γ(x) ∩Γ(y)|
Jaccard’s coeﬃcient
|Γ(x)∩Γ(y)|
|Γ(x)∪Γ(y)|
Adamic/Adar
z∈Γ(x)∩Γ(y)
log |Γ(z)|
preferential attachment
|Γ(x)| · |Γ(y)|
ℓ=1 βℓ· |paths⟨ℓ⟩
where paths⟨ℓ⟩
x,y := {paths of length exactly ℓfrom x to y}
weighted: paths⟨1⟩
x,y := number of collaborations between x, y.
unweighted: paths⟨1⟩
x,y := 1 iﬀx and y collaborate.
hitting time
stationary-normed
−Hx,y · πy
commute time
−(Hx,y + Hy,x)
stationary-normed
−(Hx,y · πy + Hy,x · πx)
where Hx,y
expected time for random walk from x to reach y
stationary-distribution weight of y
(proportion of time the random walk is at node y)
rooted PageRankα
stationary distribution weight of y under the following random walk:
with probability α, jump to x.
with probability 1 −α, go to random neighbor of current node.
b∈Γ(y) score(a,b)
|Γ(x)|·|Γ(y)|
Figure 2: Values for score(x, y) under various predictors; each predicts pairs ⟨x, y⟩in descending
order of score(x, y). The set Γ(x) consists of the neighbors of the node x in Gcollab.
and Davidsen et al. have deﬁned abstract models for network growth using this principle, in
which an edge ⟨x, y⟩is more likely to form if edges ⟨x, z⟩and ⟨z, y⟩are already present for some z.
• Common neighbors. The most direct implementation of this idea for link prediction is to deﬁne
score(x, y) := |Γ(x) ∩Γ(y)|, the number of neighbors that x and y have in common. Newman 
has computed this quantity in the context of collaboration networks, verifying a correlation between
the number of common neighbors of x and y at time t and the probability that they will collaborate
in the future.
• Jaccard’s coeﬃcient and Adamic/Adar. The Jaccard coeﬃcient—a commonly used similarity
metric in information retrieval —measures the probability that both x and y have a feature f,
for a randomly selected feature f that either x or y has. If we take “features” here to be neighbors
in Gcollab, this approach leads to the measure score(x, y) := |Γ(x) ∩Γ(y)|/|Γ(x) ∪Γ(y)|.
Adamic and Adar consider a similar measure, in the context of deciding when two personal
home pages are strongly “related.” To do this, they compute features of the pages and deﬁne the
similarity between two pages to be
z : feature shared by x, y
log(frequency(z)).
This quantity reﬁnes the simple counting of common features by weighting rarer features more
heavily. This idea suggests the measure score(x, y) := P
z∈Γ(x)∩Γ(y)
log |Γ(z)|.
• Preferential attachment has received considerable attention as a model of the growth of networks . The basic premise is that the probability that a new edge has node x as an endpoint
is proportional to |Γ(x)|, the current number of neighbors of x. Newman and Barabasi et al. 
have further proposed, on the basis of empirical evidence, that the probability of co-authorship
of x and y is correlated with the product of the number of collaborators of x and y. This proposal
corresponds to the measure score(x, y) := |Γ(x)| · |Γ(y)|.
Methods based on the ensemble of all paths.
A number of methods reﬁne the notion of
shortest-path distance by implicitly considering the ensemble of all paths between two nodes.
• Katz deﬁnes a measure that directly sums over this collection of paths, exponentially
damped by length to count short paths more heavily. This notion leads to the measure
score(x, y) :=
βℓ· |paths⟨ℓ⟩
where paths⟨ℓ⟩
x,y is the set of all length-ℓpaths from x to y, and β > 0 is a parameter of the predictor.
(A very small β yields predictions much like common neighbors, because paths of length three or
more contribute very little to the summation.) One can verify that the matrix of scores is given by
(I −βM)−1 −I, where M is the adjacency matrix of the graph. We consider two variants of this
Katz measure: (1) unweighted, in which paths⟨1⟩
x,y = 1 if x and y have collaborated and 0 otherwise,
and (2) weighted, in which paths⟨1⟩
x,y is the number of times that x and y have collaborated.
• Hitting time, PageRank, and variants.
A random walk on Gcollab starts at a node x and
iteratively moves to a neighbor of x chosen uniformly at random from the set Γ(x). The hitting
time Hx,y from x to y is the expected number of steps required for a random walk starting at x
to reach y. Because the hitting time is not in general symmetric, it is also natural to consider the
commute time Cx,y := Hx,y + Hy,x. Both of these measures serve as natural proximity measures
and hence (negated) can be used as score(x, y).
One diﬃculty with hitting time as a measure of proximity is that Hx,y is quite small whenever y
is a node with a large stationary probability πy, regardless of the identity of x. To counterbalance this
phenomenon, we also consider normalized versions of the hitting and commute times, by deﬁning
score(x, y) := −Hx,y · πy or score(x, y) := −(Hx,y · πy + Hy,x · πx).
Another diﬃculty with these measures is their sensitive dependence to parts of the graph far
away from x and y, even when x and y are connected by very short paths. A way of counteracting
this dependence is to allow the random walk from x to y to periodically “reset,” returning to x with a
ﬁxed probability α at each step; in this way, distant parts of the graph will almost never be explored.
Random resets form the basis of the PageRank measure for Web pages , and we can adapt it for
link prediction as follows. Deﬁne score(x, y) under the rooted PageRank measure with parameter
α ∈ to be the stationary probability of y in a random walk that returns to x with probability α
each step, moving to a random neighbor with probability 1 −α. Similar approaches have been
considered for personalized PageRank, in which one wishes to rank web pages based both on their
overall importance, the core of PageRank, and their relevance to a particular topic or individual,
by biasing the random resets towards topically relevant or bookmarked pages .
• SimRank is a ﬁxed point of the following recursive deﬁnition: two nodes are similar to the
extent that they are joined to similar neighbors. Numerically, this quantity is speciﬁed by deﬁning
similarity(x, x) := 1 and
similarity(x, y) := γ ·
b∈Γ(y) similarity(a, b)
|Γ(x)| · |Γ(y)|
for a parameter γ ∈ . We then deﬁne score(x, y) := similarity(x, y). SimRank can also be
interpreted in terms of a random walk on the collaboration graph: it is the expected value of γ ℓ,
where ℓis a random variable giving the time at which random walks started from x and y ﬁrst
Higher-level approaches.
We now discuss three “meta-approaches” that can be used in conjunction with any of the methods discussed above.
• Low-rank approximation. Because the adjacency matrix M can be used to represent the graph
Gcollab, all of our link-prediction methods have an equivalent formulation in terms of this matrix M.
In some cases, this correspondence was noted explicitly above (for example in the case of the Katz
similarity score), but in many other cases the matrix formulation is also quite natural. For example,
the common-neighbors method consists simply of mapping each node x to its row r(x) in M, and
then deﬁning score(x, y) to be the inner product of the rows r(x) and r(y).
A common general technique when analyzing the structure of a large matrix M is to choose
a relatively small number k and compute the rank-k matrix Mk that best approximates M with
respect to any of a number of standard matrix norms. This computation can be done eﬃciently
using the singular-value decomposition, and it forms the core of methods like latent semantic
analysis in information retrieval . Intuitively, working with Mk rather than M can be viewed as
a type of “noise-reduction” technique that generates most of the structure in the matrix but with
a greatly simpliﬁed representation.
In our experiments, we investigate three applications of low-rank approximation: (i) ranking by
the Katz measure, in which we use Mk rather than M in the underlying formula; (ii) ranking by
common neighbors, in which we score by inner products of rows in Mk rather than M; and—most
simply of all—(iii) deﬁning score(x, y) to be the ⟨x, y⟩entry in the matrix Mk.
• Unseen bigrams. Link prediction is akin to the problem of estimating frequencies for unseen bigrams in language modeling—pairs of words that co-occur in a test corpus, but not in the
corresponding training corpus (see, e.g., the work of Essen and Steinbiss ).
Following ideas
proposed in that literature [23, for example], we can augment our estimates for score(x, y) using
values of score(z, y) for nodes z that are “similar” to x. Speciﬁcally, we adapt this approach to the
link-prediction problem as follows. Suppose we have values score(x, y) computed under one of the
measures above. Let S⟨δ⟩
denote the δ nodes most related to x under score(x, ·), for a parameter
δ ∈Z+. We then deﬁne enhanced scores in terms of the nodes in this set:
unweighted(x, y)
{z : z ∈Γ(y) ∩S⟨δ⟩
weighted(x, y)
z∈Γ(y)∩S⟨δ⟩
score(x, z).
• Clustering. One might seek to improve on the quality of a predictor by deleting the more
“tenuous” edges in Gcollab through a clustering procedure, and then running the predictor on the
resulting “cleaned-up” subgraph. Consider a measure computing values for score(x, y). We compute
score(u, v) for all edges in Eold, and delete the (1 −ρ) fraction of these edges for which the score is
lowest, for a parameter ρ ∈ . We now recompute score(x, y) for all pairs ⟨x, y⟩on this subgraph;
in this way we determine node proximities using only edges for which the proximity measure itself
has the most conﬁdence.
Results and Discussion
As discussed in Section 1, many collaborations form (or fail to form) for reasons outside the scope
of the network; thus the raw performance of our predictors is relatively low. To more meaningfully
represent predictor quality, we use as our baseline a random predictor, which simply predicts randomly selected pairs of authors who did not collaborate in the training interval. The probability
that a random prediction is correct is just the ratio between |Enew|, the number of possible correct
predictions, and
−|Eold|, the number of possible predictions that can be made. (Any pair
chosen from the set Core of core authors is a legal prediction unless they had already collaborated,
which occurs for |Eold| pairs.) A random prediction is correct with probability between 0.15%
(cond-mat) and 0.48% (astro-ph).
Figures 3 and 4 show each predictor’s performance on each arXiv section, in terms of the factor
improvement over random. One can use standard tail inequalities (see the text of Motwani and
Raghavan , for example) to show that the probability of a random predictor’s performance
exceeding its expectation by a factor of ﬁve is very small: this probability ranges from about 0.004
for gr-qc to about 10−48 for astro-ph. Thus almost every predictor performs signiﬁcantly better
than random predictions on every dataset.
Figures 5, 6, and 7 show the average relative performance of several diﬀerent predictors versus
three baseline predictors—the random predictor, the graph-distance predictor, and the commonneighbors predictor.
There is no single clear winner among the techniques, but we see that a
number of methods signiﬁcantly outperform the random predictor, suggesting that there is indeed
useful information contained in the network topology alone. The Katz measure and its variants
based on clustering and low-rank approximation perform consistently well; on three of the ﬁve
arXiv sections, a variant of Katz achieves the best performance. Some of the very simple measures
also perform surprisingly well, including common neighbors and the Adamic/Adar measure.
Similarities among the predictors and the datasets.
Not surprisingly, there is signiﬁcant
overlap in the predictions made by the various methods.
In Figure 8, we show the number of
common predictions made by ten of the most successful measures on the cond-mat graph. We see
that Katz, low-rank inner product, and Adamic/Adar are quite similar in their predictions, as are
(to a somewhat lesser extent) rooted PageRank, SimRank, and Jaccard. Hitting time is remarkably
unlike any of the other nine in its predictions, despite its reasonable performance. The number of
common correct predictions shows qualitatively similar behavior; see Figure 9. It would interesting
to understand the generality of these overlap phenomena, especially because certain of the large
overlaps do not seem to follow obviously from the deﬁnitions of the measures.
It is harder to quantify the diﬀerences among the datasets, but their relationship is a very
interesting issue as well. One perspective is provided by the methods based on low-rank approxi-
probability that a random prediction is correct
graph distance (all distance-two pairs)
common neighbors
preferential attachment
Adamic/Adar
hitting time
hitting time—normed by stationary distribution
commute time
commute time—normed by stationary distribution
rooted PageRank
Katz (weighted)
β = 0.0005
Katz (unweighted)
β = 0.0005
Figure 3: Performance of various predictors on the link-prediction task deﬁned in Section 2. For each
predictor and each arXiv section, the given number speciﬁes the factor improvement over random
prediction. Two predictors in particular are used as baselines for comparison: graph distance and
common neighbors. (See Section 3 for deﬁnitions.) Italicized entries have performance at least as
good as the graph-distance predictor; bold entries are at least as good as the common-neighbors
predictor. See also Figure 4.
probability that a random prediction is correct
graph distance (all distance-two pairs)
common neighbors
Low-rank approximation:
rank = 1024
Inner product
rank = 256
Low-rank approximation:
rank = 1024
Matrix entry
rank = 256
Low-rank approximation:
rank = 1024
Katz (β = 0.005)
rank = 256
unseen bigrams
common neighbors, δ = 8
(weighted)
common neighbors, δ = 16
Katz (β = 0.005), δ = 8
Katz (β = 0.005), δ = 16
unseen bigrams
common neighbors, δ = 8
(unweighted)
common neighbors, δ = 16
Katz (β = 0.005), δ = 8
Katz (β = 0.005), δ = 16
clustering:
Katz (β1 = 0.001, β2 = 0.1)
Figure 4: Performance of various meta-approaches on the link-prediction task deﬁned in Section 2.
As before, for each predictor and each arXiv section, the given number speciﬁes the factor improvement over random predictions. See Figure 3.
Relative performance ratio versus random predictions
random predictor
Adamic/Adar
weighted Katz∗
Katz clustering∗
low-rank inner product∗
common neighbors
rooted PageRank∗
unseen bigrams∗
graph distance
hitting time
Figure 5: Relative average performance of various predictors versus random predictions. The value
shown is the average ratio over the ﬁve datasets of the given predictor’s performance versus the
random predictor’s performance. The error bars indicate the minimum and maximum of this ratio
over the ﬁve datasets. The parameters for the starred predictors are as follows: (1) for weighted
Katz, β = 0.005; (2) for Katz clustering, β1 = 0.001, ρ = 0.15, β2 = 0.1; (3) for low-rank inner
product, rank = 256; (4) for rooted Pagerank, α = 0.15; (5) for unseen bigrams, unweighted
common neighbors with δ = 8; and (6) for SimRank, γ = 0.8.
Relative performance ratio versus graph-distance predictor
graph-distance predictor
Adamic/Adar
low-rank inner product
weighted Katz
common neighbors
Katz clustering
rooted PageRank
unseen bigrams
hitting time
Figure 6: Relative average performance of various predictors versus the graph-distance predictor.
The plotted value shows the average taken over the ﬁve datasets of the ratio of the performance
of the given predictor versus the graph-distance predictor; the error bars indicate the range of this
ratio over the ﬁve datasets. All parameter settings are as in Figure 5.
Relative performance ratio versus common-neighbors predictor
common-neighbors predictor
Adamic/Adar
Katz clustering
low-rank inner product
weighted Katz
rooted PageRank
unseen bigrams
graph distance
hitting time
Figure 7: Relative average performance of various predictors versus the common-neighbors predictor, as in Figure 6. Error bars display the range of the performance ratio of the given predictor
versus common neighbors over the ﬁve datasets; the displayed value gives the average ratio. Parameter settings are as in Figure 5.
Adamic/Adar
Katz clustering
common neighbors
hitting time
Jaccard’s coeﬃcient
weighted Katz
low-rank inner product
rooted Pagerank
unseen bigrams
Adamic/Adar
Katz clustering
common neighbors
hitting time
Jaccard’s coeﬃcient
weighted Katz
low-rank inner product
rooted Pagerank
unseen bigrams
Figure 8: The number of common predictions made by various predictors on the cond-mat dataset,
out of 1150 predictions. Parameter settings are as in Figure 5.
mation: on four of the datasets, their performance tends to be best at an intermediate rank, while
on gr-qc they perform best at rank 1. (See Figure 10, for example, for a plot of the change in the
performance of the low-rank matrix-entry predictor as the rank of the approximation varies.) This
fact suggests a sense in which the collaborations in gr-qc have a much “simpler” structure than
in the other four. One also observes the apparent importance of node degree in the hep-ph collaborations: the preferential-attachment predictor—which considers only the number (and not the
identity) of a scientist’s co-authors—does uncharacteristically well on this dataset, outperforming
the basic graph-distance predictor. Finally, it would be interesting to make precise a sense in which
astro-ph is a “diﬃcult” dataset, given the low performance of all methods relative to random, and
the fact that none beats simple ranking by common neighbors. We will explore this issue further
below when we consider collaboration data drawn from other ﬁelds.
Because almost all of our experiments were carried out on social networks formed via the
collaborations of physicists, it is diﬃcult to draw broad conclusions about link prediction in social
networks in general. The culture of physicists and of physics collaboration (see, e.g., the work of
Katz and Martin ) plays a role in the quality of our results. The considerations discussed above
suggest that there are some important diﬀerences even within physics (depending on the subﬁeld),
and an important area for future study is to understand how other social networks diﬀer from the
ones that we study here.
Small worlds.
It is reassuring that even the basic graph-distance predictor handily outperforms
random predictions, but this measure has severe limitations. Extensive research has been devoted
Adamic/Adar
Katz clustering
common neighbors
hitting time
Jaccard’s coeﬃcient
weighted Katz
low-rank inner product
rooted Pagerank
unseen bigrams
Adamic/Adar
Katz clustering
common neighbors
hitting time
Jaccard’s coeﬃcient
weighted Katz
low-rank inner product
rooted Pagerank
unseen bigrams
Figure 9: The number of correct common predictions made by various predictors on the cond-mat
dataset, out of 1150 predictions. The diagonal entries indicate the number of correct predictions
for each predictor. Parameter settings are as in Figure 5.
Relative performance ratio versus random predictions
Figure 10: Relative performance of the low-rank matrix-entry predictor for various ranks on each
arXiv section. For each arXiv section, the performance of this predictor, measured by the factor of
improvement over random predictions, is shown for ranks 1, 4, 16, 64, 256, and 1024. Notice that
for all arXiv sections save gr-qc, predictor performance is maximized by an intermediate rank; for
that data set, performance continues to improve as the rank decreases all the way to rank one.
to understanding the so-called small-world problem in collaboration networks—i.e., accounting for
the existence of short paths connecting virtually every pair of scientists .
This property is
normally viewed as a vital fact about the scientiﬁc community (new ideas spread quickly, and every
discipline interacts with—and gains from—other ﬁelds) but in the context of our prediction task,
we come to a diﬀerent conclusion: the small-world problem is really a problem. The shortest path
between two scientists in wholly unrelated disciplines is often very short (and very tenuous). To
take one particular but not atypical example, the developmental psychologist Jean Piaget has as
small an Erd˝os Number—three —as most mathematicians and computer scientists. Overall, the
basic graph-distance predictor is not competitive with most of the other approaches studied; our
most successful link predictors can be viewed as using measures of proximity that are robust to the
few edges that result from rare collaborations between ﬁelds.
Restricting to distance three.
The small-world problem suggests that there are many pairs of
authors separated by a graph distance of two who will not collaborate, but we also observe the dual
problem: many pairs who collaborate are at distance greater than two. Between 71% (hep-ph) and
83% (cond-mat) of new edges form between pairs at distance three or greater; see Figure 11.
Because most new collaborations are not at distance two, we are also interested in how well
our predictors perform when we disregard all distance-two pairs. Clearly, nodes at distance greater
than two have no neighbors in common, and hence this task essentially rules out the use of methods
based on common neighbors. The performance of the other measures is shown in Figure 12. The
graph-distance predictor (i.e., predicting all distance-three pairs) performs between about three
and nine times random and is consistently beaten by virtually all of the predictors: SimRank,
rooted PageRank, Katz, and the low-rank and unseen-bigram techniques. The unweighted Katz
and unseen-bigram predictors have the best performance (as high as about 30 times random, on
gr-qc), followed closely by weighted Katz, SimRank, and rooted PageRank.
The breadth of the data.
We also have considered three other datasets: (1) the proceedings of two conferences in theoretical computer science, Symposium on the Theory of Computing (STOC) and Foundations of Computer Science (FOCS), (2) the papers found in the Citeseer
(www.citeseer.com) online database, which ﬁnds papers by crawling the web for any ﬁles in
postscript format, and (3) all ﬁve of the arXiv sections merged into one. Consider the performance
of the common-neighbors predictor compared to random on these datasets:
arXiv sections
combined arXiv sections
Performance versus random swells dramatically as the topical focus of our data set widens. That is,
when we consider a more diverse collection of scientists, it is fundamentally easier to group scientists
into ﬁelds of study (and therefore outperform the random predictor, which will usually make guesses
between ﬁelds). When we consider a suﬃciently narrow set of researchers—e.g., STOC/FOCS—
almost any author can collaborate with almost any other author, and there seems to a strong
random component to new collaborations. (In extensive experiments on the STOC/FOCS data,
we could not beat random guessing by a factor of more than about seven.) It is an interesting
challenge to formalize the sense in which the STOC/FOCS collaborations are truly intractable to
predict—i.e., to what extent information about new collaborations is simply not present in the old
collaboration data.
Proportion of distance-two pairs that form an edge:
Proportion of new edges that are between distance-two pairs:
# pairs at distance two
# new collaborations at distance two
# new collaborations
Figure 11: Relationship between new collaborations and graph distance.
Future directions.
While the predictors that we have discussed perform reasonably well, even
the best (Katz clustering on gr-qc) is correct on only about 16% of its predictions.
clearly much room for improvement in performance on this task, and ﬁnding ways to take better
advantage of the information in the training data is an interesting open question. Another issue is
to improve the eﬃciency of the proximity-based methods on very large networks; fast algorithms
for approximating the distribution of node-to-node distances may be one approach .
The graph Gcollab is a lossy representation of the data; we can also consider a bipartite collaboration graph Bcollab, with a vertex for every author and paper, and an edge connecting each
paper to each of its authors. The bipartite graph contains more information than Gcollab, so we
may hope that predictors can use it to improve performance. The size of Bcollab is much larger
than Gcollab, making experiments prohibitive, but we have tried using the SimRank and Katz predictors on smaller datasets (gr-qc, or shorter training periods). Their performance does not seem
to improve, but perhaps other predictors can fruitfully exploit the additional information in Bcollab.
Similarly, our experiments treat all training-period collaborations equally.
Perhaps one can
improve performance by treating more recent collaborations as more important than older ones.
One could also tune the parameters of the Katz predictor, e.g., by dividing the training set into
temporal segments, training β on the beginning, and then using the end of the training set to make
ﬁnal predictions.
One might also try to use additional information, such as the titles of papers or the institutional
aﬃliations of the authors, to identify the speciﬁc research area or geographic location of each
scientist, and then use areas/locations to predict collaborations. In the ﬁeld of bibliometrics, for
graph distance (all distance-three pairs)
preferential attachment
hitting time
hitting time—normed by stationary distribution
commute time
commute time—normed by stationary distribution
rooted PageRank
Katz (weighted)
β = 0.0005
Katz (unweighted)
β = 0.0005
Low-rank approximation:
rank = 1024
Inner product
rank = 256
Low-rank approximation:
rank = 1024
Matrix entry
rank = 256
Low-rank approximation:
rank = 1024
Katz (β = 0.005)
rank = 256
unseen bigrams
common neighbors, δ = 8
(weighted)
common neighbors, δ = 16
Katz (β = 0.005), δ = 8
Katz (β = 0.005), δ = 16
unseen bigrams
common neighbors, δ = 8
(unweighted)
common neighbors, δ = 16
Katz (β = 0.005), δ = 8
Katz (β = 0.005), δ = 16
clustering:
Katz (β1 = 0.001, β2 = 0.1)
Figure 12: The distance-three task: performance of predictors only on edges in Enew for which the
endpoints were at distance three or more in Gcollab. Methods based on common neighbors are not
appropriate for this task. See Section 4.
example, Katz , Melin and Persson , and Ding, Foo, and Chowdhury , among others,
have observed institutional and geographic correlations in collaboration; a natural further direction
would be to attempt to use geographic location, for instance, as a component of a predictor. To
some extent, such geographic information, or indeed any other relevant properties of the nodes, is
latently present in the graph Gcollab—precisely because such factors have already played a role in
the formation of old edges in the training set. However, direct access to such information may well
confer additional predictive power, and it is an interesting open question to better understand the
strength of such information in link prediction.
Finally, there has been relevant work in the machine-learning community on estimating distribution support : given samples from an unknown probability distribution P, we must ﬁnd a
“simple” set S so that Prx∼P[x /∈S] < ε. We can view training-period collaborations as samples
drawn from a probability distribution on pairs of scientists; our goal is to approximate the set of
pairs that have positive probability of collaborating. There has also been some potentially relevant
work in machine learning on classiﬁcation when the training set consists only of a relatively small
set of positively labeled examples and a large set of unlabeled examples, with no labeled negative
examples . It is an open question whether these techniques can be fruitfully applied to the
link-prediction problem.
Acknowledgements
We thank Jon Herzog, Tommi Jaakkola, David Karger, Lillian Lee, Frank McSherry, Mike Schneider, Grant Wang, and Robert Wehr for helpful discussions and comments on earlier drafts of this
paper. We thank Paul Ginsparg for generously providing the bibliographic data from the arXiv.
The ﬁrst author was supported in part by an NSF Graduate Research Fellowship. The second
author was supported in part by a David and Lucile Packard Foundation Fellowship and NSF ITR
Grant IIS-0081334.