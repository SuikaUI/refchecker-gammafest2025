HAL Id: hal-01960659
 
Submitted on 8 Jan 2019
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Continuous space translation models with neural
Le Hai Son, Alexandre Allauzen, François Yvon
To cite this version:
Le Hai Son, Alexandre Allauzen, François Yvon. Continuous space translation models with neural networks. Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Jun 2012, Montréal, Canada. ￿hal-01960659￿
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48,
Montr´eal, Canada, June 3-8, 2012. c⃝2012 Association for Computational Linguistics
Continuous Space Translation Models with Neural Networks
Le Hai Son and Alexandre Allauzen and Franc¸ois Yvon
Univ. Paris-Sud, France and LIMSI/CNRS
rue John von Neumann, 91403 Orsay cedex, France
 
The use of conventional maximum likelihood
estimates hinders the performance of existing
phrase-based translation models. For lack of
sufﬁcient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in
lieu of standard discrete representations. In
order to handle a large set of translation units,
these representations and the associated estimates are jointly computed using a multi-layer
neural network with a SOUL architecture. In
small scale and large scale English to French
experiments, we show that the resulting models can effectively be trained and used on top
of a n-gram translation system, delivering signiﬁcant improvements in performance.
Introduction
The phrase-based approach to statistical machine
translation (SMT) is based on the following inference rule, which, given a source sentence s, selects
the target sentence t and the underlying alignment a
maximizing the following term:
P(t, a|s) =
λkfk(s, t, a)
where K feature functions (fk) are weighted by a
set of coefﬁcients (λk), and Z is a normalizing factor. The phrase-based approach differs from other
approaches by the hidden variables of the translation
process: the segmentation of a parallel sentence pair
into phrase pairs and the associated phrase alignments.
This formulation was introduced in as an extension of the word based models , then later motivated within
a discriminative framework .
One motivation for integrating more feature functions was to improve the estimation of the translation
model P(t|s), which was initially based on relative
frequencies, thus yielding poor estimates.
This is because the units of phrase-based models are phrase pairs, made of a source and a target phrase; such units are viewed as the events of
discrete random variables. The resulting representations of phrases (or words) thus entirely ignore the
morphological, syntactic and semantic relationships
that exist among those units in both languages. This
lack of structure hinders the generalization power of
the model and reduces its ability to adapt to other
domains. Another consequence is that phrase-based
models usually consider a very restricted context1.
This is a general issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as,
for instance, using smoothing techniques , or working with linguistically enriched, or more abstract, representations. In statistical language modeling, another line of research considers numerical representations, trained automatically through the use of neural network , or no context at all, for the standard approach of .
 ). An inﬂuential proposal,
in this respect, is the work of 
on continuous space language models. In this approach, n-gram probabilities are estimated using a
continuous representation of words in lieu of standard discrete representations. Experimental results,
reported for instance in show signiﬁcant improvements in speech recognition applications. Recently, this model has been extended in
several promising ways . In the context of SMT,
Schwenk et al. is the ﬁrst attempt to estimate translation probabilities in a continuous space.
However, because of the proposed neural architecture, the authors only consider a very restricted set
of translation units, and therefore report only a slight
impact on translation performance. The recent proposal of seems especially relevant,
as it is able, through the use of class-based models,
to handle arbitrarily large vocabularies and opens the
way to enhanced neural translation models.
In this paper, we explore various neural architectures for translation models and consider three different ways to factor the joint probability P(s, t)
differing by the units (respectively phrase pairs,
phrases or words) that are projected in continuous
While these decompositions are theoretically straightforward, they were not considered in
the past because of data sparsity issues and of the
resulting weaknesses of conventional maximum likelihood estimates. Our main contribution is then to
show that such joint distributions can be efﬁciently
computed by neural networks, even for very large
context sizes; and that their use yields signiﬁcant
performance improvements. These models are evaluated in a n-best rescoring step using the framework
of n-gram based systems, within which they integrate easily. Note, however that they could be used
with any phrase-based system.
The rest of this paper is organized as follows. We
ﬁrst recollect, in Section 2, the n-gram based approach, and discuss various implementations of this
framework. We then describe, in Section 3, the neural architecture developed and explain how it can be
made to handle large vocabulary tasks as well as language models over bilingual units. We ﬁnally report, in Section 4, experimental results obtained on
a large-scale English to French translation task.
Variations on the n-gram approach
Even though n-gram translation models can be
integrated within standard phrase-based systems
 , the n-gram based framework provides a more convenient way to introduce
our work and has also been used to build the baseline systems used in our experiments.
In the ngram based approach , translation is divided in two steps: a source reordering
step and a translation step.
Source reordering is
based on a set of learned rewrite rules that nondeterministically reorder the input words so as to
match the target order thereby generating a lattice
of possible reorderings. Translation then amounts
to ﬁnding the most likely path in this lattice using a
n-gram translation model 2 of bilingual units.
The standard n-gram translation model
n-gram translation models (TMs) rely on a speciﬁc decomposition of the joint probability P(s, t),
where s is a sequence of I reordered source words
(s1, ..., sI) and t contains J target words (t1, ..., tJ).
This sentence pair is further assumed to be decomposed into a sequence of L bilingual units
called tuples deﬁning a joint segmentation: (s, t) =
u1, ..., uL. In the approach of ,
this segmentation is a by-product of source reordering, and ultimately derives from initial word and
phrase alignments.
In this framework, the basic
translation units are tuples, which are the analogous
of phrase pairs, and represent a matching u = (s, t)
between a source s and a target t phrase (see Figure 1). Using the n-gram assumption, the joint probability of a segmented sentence pair decomposes as:
P(ui|ui−1, ..., ui−n+1)
A ﬁrst issue with this model is that the elementary
units are bilingual pairs, which means that the underlying vocabulary, hence the number of parameters,
can be quite large, even for small translation tasks.
Due to data sparsity issues, such models are bound
2Like in the standard phrase-based approach, the translation
process also involves additional feature functions that are presented below.
to face severe estimation problems. Another problem with (2) is that the source and target sides play
symmetric roles, whereas the source side is known,
and the target side must be predicted.
A factored n-gram translation model
To overcome some of these issues, the n-gram probability in equation (2) can be factored by decomposing tuples in two (source and target) parts :
P(ui|ui−1, ..., ui−n+1) =
P(ti|si, si−1, ti−1, ..., si−n+1, ti−n+1)
× P(si|si−1, ti−1..., si−n+1, ti−n+1)
Decomposition (3) involves two models: the ﬁrst
term represents a TM, the second term is best viewed
as a reordering model. In this formulation, the TM
only predicts the target phrase, given its source and
target contexts.
Another beneﬁt of this formulation is that the elementary events now correspond either to source or
to target phrases, but never to pairs of such phrases.
The underlying vocabulary is thus obtained as the
union, rather than the cross product, of phrase inventories. Finally note that the n-gram probability
P(ui|ui−1, ..., ui−n+1) could also factor as:
P(si|ti, si−1, ti−1, ..., si−n+1, ti−n+1)
× P(ti|si−1, ti−1, ..., si−n+1, ti−n+1)
A word factored translation model
A more radical way to address the data sparsity issues is to take (source and target) words as the basic
units of the n-gram TM. This may seem to be a step
backwards, since the transition from word to phrase-based models 
is considered as one of the main recent improvement
in MT. One important motivation for considering
phrases rather than words was to capture local context in translation and reordering. It should then be
stressed that the decomposition of phrases in words
is only re-introduced here as a way to mitigate the
parameter estimation problems.
Translation units
are still pairs of phrases, derived from a bilingual
segmentation in tuples synchronizing the source and
target n-gram streams, as deﬁned by equation (3).
In fact, the estimation policy described in section 3
will actually allow us to design n-gram models with
longer contexts than is typically possible in the conventional n-gram approach.
i denote the kth word of source tuple si.
Considering again the example of Figure 1, s1
to the source word nobel, s4
11 is to the source word
paix, and similarly t2
11 is the target word peace. We
ﬁnally denote hn−1(tk
i ) the sequence made of the
n −1 words preceding tk
i in the target sentence: in
Figure 1, h3(t2
11) thus refers to the three word context receive the nobel associated with the target word
peace. Using these notations, equation (3) is rewritten as:
i |hn−1(tk
i ), hn−1(s1
i |hn−1(t1
i ), hn−1(sk
This decomposition relies on the n-gram assumption, this time at the word level.
Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these windows remain synchronized by the tuple segmentation. Moreover, the context is not limited to the current phrase, and continues to include words in adjacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P(s, t) using a 3- gram model is
 nobel|[receive, the], [la, paix]
 peace|[the, nobel], [la, paix]
Likewise, the contribution of the source phrase
s11 =nobel, de, la, paix is:
 nobel|[receive, the], [recevoir,le]
 de|[receive, the], [le,nobel]
 la|[receive, the], [nobel, de]
 paix|[receive, the], [de,la]
A beneﬁt of this new formulation is that the involved
vocabularies only contain words, and are thus much
smaller. These models are thus less bound to be affected by data sparsity issues. While the TM deﬁned
by equation (5) derives from equation (3), a variation
can be equivalently derived from equation (4).
s̅9: recevoir
t̅9: receive
s̅11: nobel de la paix
t̅11: nobel peace
s̅12: prix
t̅12: prize
prix nobel de la paix
org : ....
Figure 1: Extract of a French-English sentence pair segmented in bilingual units. The original (org) French sentence
appears at the top of the ﬁgure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
The SOUL model
In the previous section, we deﬁned three different
n-gram translation models, based respectively on
equations (2), (3) and (5). As discussed above, a
major issue with such models is to reliably estimate
their parameters, the numbers of which grow exponentially with the order of the model. This problem
is aggravated in natural language processing, due to
well known data sparsity issues. In this work, we
take advantage of the recent proposal of : using a speciﬁc neural network architecture
(the Structured OUtput Layer model), it becomes
possible to handle large vocabulary language modeling tasks, a solution that we adapt here to MT.
Language modeling in a continuous space
Let V be a ﬁnite vocabulary, n-gram language models (LMs) deﬁne distributions over ﬁnite sequences
of tokens (typically words) wL
1 in V+ as follows:
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difﬁcult, especially in NLP applications where V
typically contains dozens of thousands words.
In spite of the simplifying n-gram assumption, maximum likelihood estimation remains unreliable and tends to underestimate the probability of very rare n-grams.
Smoothing techniques, such as Kneser-Ney and Witten-Bell backoff schemes for an
empirical overview, and for a Bayesian
interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events.
One of the most successful alternative to date is to
use distributed word representations , where distributionally similar words are represented as neighbors in a continuous space. This
turns n-grams distributions into smooth functions
of the word representations. These representations
and the associated estimates are jointly computed
in a multi-layer neural network architecture. Figure 2 provides a partial representation of this kind
of model and helps ﬁguring out their principles. To
compute the probability P(wi|wi−1
i−n+1), the n −1
context words are projected in the same continuous space using a shared matrix R; these continuous
word representations are then concatenated to build
a single vector that represents the context; after a
non-linear transformation, the probability distribution is computed using a softmax layer.
The major difﬁculty with the neural network approach remains the complexity of inference and
training, which largely depends on the size of the
output vocabulary (i.e. the number of words that
have to be predicted). One practical solution is to restrict the output vocabulary to a short-list composed
of the most frequent words . However, the usual size of the short-list is under 20k,
which does not seem sufﬁcient to faithfully represent the translation models of section 2.
Principles of SOUL
To circumvent this problem, Structured Output
Layer (SOUL) LMs are introduced in , the
SOUL model combines the neural network approach
with a class-based LM . Struc-
turing the output layer and using word class information makes the estimation of distributions over the
entire vocabulary computationally feasible.
To meet this goal, the output vocabulary is structured as a clustering tree, where each word belongs
to only one class and its associated sub-classes. If
wi denotes the ith word in a sentence, the sequence
c1:D(wi) = c1, . . . , cD encodes the path for word wi
in the clustering tree, with D being the depth of the
tree, cd(wi) a class or sub-class assigned to wi, and
cD(wi) being the leaf associated with wi (the word
itself). The probability of wi given its history h can
then be computed as:
P(wi|h) =P(c1(wi)|h)
P(cd(wi)|h, c1:d−1).
There is a softmax function at each level of the tree
and each word ends up forming its own class (a leaf).
The SOUL model, represented on Figure 2, is thus
the same as for the standard model up to the output
layer. The main difference lies in the output structure which involves several layers with a softmax activation function. The ﬁrst (class layer) estimates
the class probability P(c1(wi)|h), while other output sub-class layers estimate the sub-class probabilities P(cd(wi)|h, c1:d−1). Finally, the word layers estimate the word probabilities P(cD(wi)|h, c1:D−1).
Words in the short-list remain special, since each of
them represents a (ﬁnal) class.
Training a SOUL model can be achieved by maximizing the log-likelihood of the parameters on some
training corpus.
Following ,
this optimization is performed by stochastic backpropagation. Details of the training procedure can
be found in .
Neural network architectures are also interesting
as they can easily handle larger contexts than typical
n-gram models. In the SOUL architecture, enlarging
the context mainly consists in increasing the size of
the projection layer, which corresponds to a simple
look-up operation. Increasing the context length at
the input layer thus only causes a linear growth in
complexity in the worst case .
shared input space
input layer
hidden layers
Figure 2: The architecture of a SOUL Neural Network
language model in the case of a 4-gram model.
Translation modeling with SOUL
The SOUL architecture was used successfully to
deliver (monolingual) LMs probabilities for speech
recognition and machine translation applications.
using this architecture, it is possible to estimate ngram distributions for any kind of discrete random
variables, such as a phrase or a tuple. The SOUL architecture can thus be readily used as a replacement
for the standard n-gram TM described in section 2.1.
This is because all the random variables are events
over the same set of tuples.
Adopting this architecture for the other n-gram
TM respectively described by equations (3) and (5)
is more tricky, as they involve two different languages and thus two different vocabularies: the predicted unit is a target phrase (resp. word), whereas
the context is made of both source and target phrases
(resp. words). A subsequent modiﬁcation of the
SOUL architecture was thus performed to make up
for “mixed” contexts: rather than projecting all the
context words or phrases into the same continuous
space (using the matrix R, see Figure 2), we used
two different projection matrices, one for each language. The input layer is thus composed of two vectors in two different spaces; these two representations are then combined through the hidden layer,
the other layers remaining unchanged.
Experimental Results
We now turn to an experimental comparison of the
models introduced in Section 2. We ﬁrst describe
the tasks and data that were used, before presenting
our n-gram based system and baseline set-up. Our
results are ﬁnally presented and discussed.
Let us ﬁrst emphasize that the design and integration of a SOUL model for large SMT tasks is
far from easy, given the computational cost of computing n-gram probabilities, a task that is performed
repeatedly during the search of the best translation.
Our solution was to resort to a two pass approach:
the ﬁrst pass uses a conventional back-off n-gram
model to produce a k-best list (the k most likely
translations); in the second pass, the probability of
a m-gram SOUL model is computed for each hypothesis, added as a new feature and the k-best list
is accordingly reordered3. In all the following experiments, we used a ﬁxed context size for SOUL of
m = 10, and used k = 300.
Tasks and corpora
The two tasks considered in our experiments
are adapted from the text translation track of
IWSLT 2011 from English to French (the ”TED”
talk task): a small data scenario where the only
training data is a small in-domain corpus; and a large
scale condition using all the available training data.
In this article, we only provide a short overview of
the task; all the necessary details regarding this evaluation campaign are on the ofﬁcial website4.
The in-domain training data consists of 107, 058
sentence pairs, whereas for the large scale task, all
the data available for the WMT 2011 evaluation5 are
added. For the latter task, the available parallel data
includes a large Web corpus, referred to as the GigaWord parallel corpus. This corpus is very noisy
and is accordingly ﬁltered using a simple perplexity
criterion as explained in . The
total amount of training data is approximately 11.5
million sentence pairs for the bilingual part, and
about 2.5 billion of words for the monolingual part.
As the provided development data was quite small,
3The probability estimated with the SOUL model is added as
a new feature to the score of an hypothesis given by Equation 1.
The coefﬁcients are retuned before the reranking step.
4iwslt2011.org
5www.statmt.org/wmt11/
Vocabulary size
Small task
Large task
Phrase factored
Word factored
Table 1: Vocabulary sizes for the English to French tasks
obtained with various SOUL translation (TM) models.
For the factored models, sizes are indicated for both
source (src) and target (trg) sides.
development and test set were inverted, and we ﬁnally used a development set of 1,664 sentences, and
a test set of 934 sentences. The table 1 provides the
sizes of the different vocabularies. The n-gram TMs
are estimated over a training corpus composed of tuple sequences. Tuples are extracted from the wordaligned parallel data (using MGIZA++6 with default
settings) in such a way that a unique segmentation
of the bilingual corpus is achieved, allowing to directly estimate bilingual n-gram models for details).
n-gram based translation system
The n-gram based system used here is based on an
open source implementation described in . In a nutshell, the TM is implemented as
a stochastic ﬁnite-state transducer trained using a ngram model of (source, target) pairs as described in
section 2.1. Training this model requires to reorder
source sentences so as to match the target word order. This is performed by a non-deterministic ﬁnitestate reordering model, which uses part-of-speech
information generated by the TreeTagger to generalize reordering patterns beyond lexical regularities.
In addition to the TM, fourteen feature functions
are included: a target-language model; four lexicon models; six lexicalized reordering models ; a distance-based
distortion model; and ﬁnally a word-bonus model
and a tuple-bonus model.
The four lexicon models are similar to the ones used in standard phrasebased systems: two scores correspond to the relative frequencies of the tuples and two lexical weights
are estimated from the automatically generated word
6geek.kyloo.net/software
alignments. The weights associated to feature functions are optimally combined using the Minimum
Error Rate Training (MERT) . All the
results in BLEU are obtained as an average of 4 optimization runs7.
For the small task, the target LM is a standard
4-gram model estimated with the Kneser-Ney discounting scheme interpolated with lower order models , while for the large task, the target LM is obtained by linear interpolation of several 4-gram models for details). As for
the TM, all the available parallel corpora were simply pooled together to train a 3-gram model. Results
obtained with this large-scale system were found to
be comparable to some of the best ofﬁcial submissions.
Small task evaluation
Table 2 summarizes the results obtained with the
baseline and different SOUL models, TMs and a
target LM. The ﬁrst comparison concerns the standard n-gram TM, deﬁned by equation (2), when estimated conventionally or as a SOUL model. Adding
the latter model yields a slight BLEU improvement
of 0.5 point over the baseline. When the SOUL TM
is phrased factored as deﬁned in equation (3) the
gain is of 0.9 BLEU point instead. This difference
can be explained by the smaller vocabularies used
in the latter model, and its improved robustness to
data sparsity issues. Additional gains are obtained
with the word factored TM deﬁned by equation (5):
a BLEU improvement of 0.8 point over the phrase
factored TM and of 1.7 point over the baseline are
respectively achieved. We assume that the observed
improvements can be explained by the joint effect of
a better smoothing and a longer context.
The comparison with the condition where we only
use a SOUL target LM is interesting as well. Here,
the use of the word factored TM still yields to a 0.6
BLEU improvement. This result shows that there
is an actual beneﬁt in smoothing the TM estimates,
rather than only focus on the LM estimates.
Table 3 reports a comparison among the different components and variations of the word
7The standard deviations are below 0.1 and thus omitted in
the reported results.
Adding a SOUL model
Standard TM
Phrase factored TM
Word factored TM
Table 2: Results for the small English to French task obtained with the baseline system and with various SOUL
translation (TM) or target language (LM) models.
Adding a SOUL model
i |hn−1(tk
i ), hn−1(s1
i |hn−1(t1
i ), hn−1(sk
+ the combination of both
i |hn−1(sk
i ), hn−1(t1
i |hn−1(s1
i ), hn−1(tk
+ the combination of both
Table 3: Comparison of the different components and
variations of the word factored translation model.
factored TM. In the upper part of the table,
the model deﬁned by equation (5) is evaluated
component by component:
ﬁrst the translation
i |hn−1(tk
i ), hn−1(s1
, then its distortion counterpart P
i |hn−1(t1
i ), hn−1(sk
and ﬁnally their combination, which yields the joint probability of the sentence pair. Here, we observe that
the best improvement is obtained with the translation term, which is 0.7 BLEU point better than the
latter term. Moreover, the use of just a translation
term only yields a BLEU score equal to the one obtained with the SOUL target LM, and its combination with the distortion term is decisive to attain the
additional gain of 0.6 BLEU point. The lower part of
the table provides the same comparison, but for the
variation of the word factored TM. Besides a similar
trend, we observe that this variation delivers slightly
lower results. This can be explained by the restricted
context used by the translation term which no longer
includes the current source phrase or word.
Adding a word factored SOUL TM
+ in-domain TM
+ out-of-domain TM
+ out-of-domain adapted TM
Adding a SOUL LM
+ out-of-domain adapted LM
Table 4: Results for the large English to French translation task obtained by adding various SOUL translation
and language models (see text for details).
Large task evaluation
For the large-scale setting, the training material increases drastically with the use of the additional outof-domain data for the baseline models. Results are
summarized in Table 4. The ﬁrst observation is the
large increase of BLEU (+2.4 points) for the baseline system over the small-scale baseline. For this
task, only the word factored TM is evaluated since
it signiﬁcantly outperforms the others on the small
task (see section 4.3).
In a ﬁrst scenario, we use a word factored TM,
trained only on the small in-domain corpus. Even
though the training corpus of the baseline TM is one
hundred times larger than this small in-domain data,
adding the SOUL TM still yields a BLEU increase
of 1.2 point8. In a second scenario, we increase the
training corpus for the SOUL, and include parts of
the out-of-domain data (the WMT part). The resulting BLEU score is here slightly worse than the one
obtained with just the in-domain TM, yet delivering
improved results with the respect to the baseline.
In a last attempt, we amended the training regime
of the neural network. In a ﬁst step, we trained conventionally a SOUL model using the same out-ofdomain parallel data as before. We then adapted this
model by running ﬁve additional epochs of the backpropagation algorithm using the in-domain data. Using this adapted model yielded our best results to
date with a BLEU improvement of 1.6 points over
the baseline results. Moreover, the gains obtained
using this simple domain adaptation strategy are re-
8Note that the in-domain data was already included in the
training corpus of the baseline TM.
spectively of +0.4 and +0.8 BLEU, as compared
with the small in-domain model and the large outof-domain model. These results show that the SOUL
TM can scale efﬁciently and that its structure is well
suited for domain adaptation.
Related work
To the best of our knowledge, the ﬁrst work on machine translation in continuous spaces is , where the authors introduced the model
referred here to as the the standard n-gram translation model in Section 2.1. This model is an extension of the continuous space language model
of , the basic unit is the tuple
(or equivalently the phrase pair). The resulting vocabulary being too large to be handled by neural networks without a structured output layer, the authors
had thus to restrict the set of the predicted units to a
8k short-list . Moreover, in , the authors propose a tighter integration of a
continuous space model with a n-gram approach but
only for the target LM.
A different approach, described in , divides the problem in two parts: ﬁrst the
continuous representation is obtained by an adaptation of the Latent Semantic Analysis; then a Gaussian mixture model is learned using this continuous representation and included in a hidden Markov
model. One problem with this approach is the separation between the training of the continuous representation on the one hand, and the training of the
translation model on the other hand. In comparison,
in our approach, the representation and the prediction are learned in a joined fashion.
Other ways to address the data sparsity issues
faced by translation model were also proposed in the
literature. Smoothing is obviously one possibility
 . Another is to use factored language models, introduced in , then adapted for translation models in . Such approaches require to use external linguistic analysis
tools which are error prone; moreover, they did not
seem to bring clear improvements, even when translating into morphologically rich languages.
Conclusion
In this paper, we have presented possible ways to use
a neural network architecture as a translation model.
A ﬁrst contribution was to produce the ﬁrst largescale neural translation model, implemented here in
the framework of the n-gram based models, taking advantage of a speciﬁc hierarchical architecture
(SOUL). By considering several decompositions of
the joint probability of a sentence pair, several bilingual translation models were presented and discussed. As it turned out, using a factorization which
clearly distinguishes the source and target sides, and
only involves word probabilities, proved an effective remedy to data sparsity issues and provided signiﬁcant improvements over the baseline. Moreover,
this approach was also experimented within the systems we submitted to the shared translation task of
the seventh workshop on statistical machine translation . These experimentations in a
large scale setup and for different language pair corroborate the improvements reported in this article.
We also investigated various training regimes for
these models in a cross domain adaptation setting.
Our results show that adapting an out-of-domain
SOUL TM is both an effective and very fast way to
perform bilingual model adaptation. Adding up all
these novelties ﬁnally brought us a 1.6 BLEU point
improvement over the baseline. Even though our
experiments were carried out only within the framework of n-gram based MT systems, using such models in other systems is straightforward. Future work
will thus aim at introducing them into conventional
phrase-based systems, such as Moses (Koehn et al.,
Given that Moses only implicitly uses ngram based information, adding SOUL translation
models is expected to be even more helpful.
Acknowledgments
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Programme.