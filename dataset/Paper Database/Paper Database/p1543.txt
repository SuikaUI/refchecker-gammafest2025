AutoInt: Automatic Feature Interaction Learning via
Self-Attentive Neural Networks
Weiping Song∗
Department of Computer Science,
School of EECS, Peking University
 
Chence Shi
Department of Computer Science,
School of EECS, Peking University
 
Zhiping Xiao
Department of Computer Science,
University of California, Los Angeles
 
Zhijian Duan, Yewen Xu
Department of Computer Science,
School of EECS, Peking University
{zjduan,xuyewen}@pku.edu.cn
Ming Zhang†
Department of Computer Science,
School of EECS, Peking University
 
Jian Tang†
Mila-Quebec AI Institute,
HEC Montreal & CIFAR AI Chair
 
Click-through rate (CTR) prediction, which aims to predict the
probability of a user clicking on an ad or an item, is critical to many
online applications such as online advertising and recommender
systems. The problem is very challenging since (1) the input features
(e.g., the user id, user age, item id, item category) are usually sparse
and high-dimensional, and (2) an effective prediction relies on highorder combinatorial features (a.k.a. cross features), which are very
time-consuming to hand-craft by domain experts and are impossible
to be enumerated. Therefore, there have been efforts in finding lowdimensional representations of the sparse and high-dimensional
raw features and their meaningful combinations.
In this paper, we propose an effective and efficient method called
the AutoInt to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general,
which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multihead self-attentive neural network with residual connections is
proposed to explicitly model the feature interactions in the lowdimensional space. With different layers of the multi-head selfattentive neural networks, different orders of feature combinations
of input features can be modeled. The whole model can be efficiently
fit on large-scale raw data in an end-to-end fashion. Experimental
results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches
for prediction but also offers good explainability. Code is available
at: 
∗Part of this work was performed when the first author was visiting Mila.
†Corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
CIKM ’19, November 3–7, 2019, Beijing, China
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6976-3/19/11...$15.00
 
CCS CONCEPTS
• Information systems →Recommender systems; • Computing methodologies →Neural networks; Learning latent representations;
High-order feature interactions, Self attention, CTR prediction,
Explainable recommendation
ACM Reference Format:
Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming
Zhang, and Jian Tang. 2019. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks. In The 28th ACM International
Conference on Information and Knowledge Management (CIKM ’19), November 3–7, 2019, Beijing, China. ACM, New York, NY, USA, 10 pages.
 
INTRODUCTION
Predicting the probabilities of users clicking on ads or items (a.k.a.,
click-through rate prediction) is a critical problem for many applications such as online advertising and recommender systems . The performance of the prediction has a direct impact on
the final revenue of the business providers. Due to its importance,
it has attracted growing interest in both academia and industry
communities.
Machine learning has been playing a key role in click-through
rate prediction, which is usually formulated as supervised learning with user profiles and item attributes as input features. The
problem is very challenging for several reasons. First, the input features are extremely sparse and high-dimensional .
In real-world applications, a considerable percentage of user’s demographics and item’s attributes are usually discrete and/or categorical. To make supervised learning methods applicable, these
features are first converted to a one-hot encoding vector, which
can easily result in features with millions of dimensions. Taking
the well-known CTR prediction data Criteo1 as an example, the
feature dimension is approximately 30 million with sparsity over
99.99%. With such sparse and high-dimensional input features, the
machine learning models are easily overfitted. Second, as shown in
extensive literature , high-order feature interactions2
1 
2In this paper, we will use “combinatorial feature” and “feature interaction” interchangeably as they are both used in the literature .
 
are crucial for a good performance. For example, it is reasonable
to recommend Mario Bros., a famous video game, to David, who
is a ten-year-old boy. In this case, the third-order combinatorial
feature <Gender=Male, Age=10, ProductCategory=VideoGame> is
very informative for prediction. However, finding such meaningful
high-order combinatorial features heavily relies on domain experts.
Moreover, it is almost impossible to hand-craft all the meaningful
combinations . One may ask that we can enumerate all the
possible high-order features and let machine learning models select the meaningful ones. However, enumerating all the possible
high-order features will exponentially increase the dimension and
sparsity of the input features, leading to a more serious problem
of model overfitting. Therefore, there has been extensive efforts
in the communities in finding low-dimensional representations of
the sparse and high-dimensional input features and meanwhile
modeling different orders of feature combinations.
For example, Factorization Machines (FM) , which combine
polynomial regression models with factorization techniques, are
developed to model feature interactions and have been proved
effective for various tasks . However, limited by its polynomial fitting time, it is only effective for modeling low-order feature
interactions and impractical to capture high-order feature interactions. Recently, many works based on deep neural
networks have been proposed to model the high-order feature interactions. Specifically, multiple layers of non-linear neural networks
are usually used to capture the high-order feature interactions.
However, such kinds of methods suffer from two limitations. First,
fully-connected neural networks have been shown inefficient in
learning multiplicative feature interactions . Second, since these
models learn the feature interactions in an implicit way, they lack
good explanation on which feature combinations are meaningful.
Therefore, we are looking for an approach that is able to explicitly
model different orders of feature combinations, represent the entire
features into low-dimensional spaces, and meanwhile offer good
model explainability.
In this paper, we propose such an approach based on the multihead self-attention mechanism . Our proposed approach learns
effective low-dimensional representations of the sparse and highdimensional input features and is applicable to both the categorical
and/or numerical input features. Specifically, both the categorical
and numerical features are first embedded into low-dimensional
spaces, which reduces the dimension of the input features and
meanwhile allows different types of features to interact with each
other via vector arithmetic (e.g., summation and inner product).
Afterwards, we propose a novel interacting layer to promote the
interactions between different features. Within each interacting
layer, each feature is allowed to interact with all the other features
and is able to automatically identify relevant features to form meaningful higher-order features via the multi-head attention mechanism . Moreover, the multi-head mechanism projects a feature
into multiple subspaces, and hence it can capture different feature
interactions in different subspaces. Such an interacting layer models
the one-step interaction between the features. By stacking multiple
interacting layers, we are able to model different orders of feature
interactions. In practice, the residual connection is added to
the interacting layer, which allows combining different orders of
feature combinations. We use the attention mechanism for measuring the correlations between features, which offers good model
explainability.
To summarize, in this paper we make the following contributions:
• We propose to study the problem of explicitly learning highorder feature interactions and meanwhile finding models
with good explainability for the problem.
• We propose a novel approach based on self-attentive neural network, which can automatically learn high-order feature interactions and efficiently handle large-scale highdimensional sparse data.
• We conducted extensive experiments on several real-world
data sets. Experimental results on the task of CTR prediction show that our proposed approach not only outperforms
existing state-of-the-art approaches for prediction but also
offers good model explainability.
Our work is organized as follows. In Section 2, we summarize
the related work. Section 3 formally defines our problem. Section
4 presents the proposed approach to learn feature interactions. In
Section 5, we present the experimental results and detailed analysis.
We conclude this paper and point out the future work in Section 6.
RELATED WORK
Our work is relevant to three lines of work: 1) Click-through rate
prediction in recommender systems and online advertising, 2) techniques for learning feature interactions, and 3) self-attention mechanism and residual networks in the literature of deep learning.
Click-through Rate Prediction
Predicting click-through rates is important to many Internet companies, and various systems have been developed by different companies . For example, Google developed the
Wide&Deep learning system for recommender systems, which
combines the advantages of both the linear shallow models and
deep models. The system achieves remarkable performance in APP
recommendation. The problem also receives a lot of attention in
the academic communities. For example, Shan et al. proposed a
context-aware CTR prediction method which factorized three-way
<user, ad, context> tensor. Oentaryo et al. developed hierarchical importance-aware factorization machine to model dynamic
impacts of ads.
Learning Feature Interactions
Learning feature interactions is a fundamental problem and therefore extensively studied in the literature. A well-known example is Factorization Machines (FM) , which were proposed to
mainly capture the first- and second-order feature interactions
and have been proved effective for many tasks in recommender
systems . Afterwards, different variants of factorization machines have been proposed. For example, Field-aware Factorization
Machines (FFM) modeled fine-grained interactions between
features from different fields. GBFM and AFM considered
the importance of different second-order feature interactions. However, all these approaches focus on modeling low-order feature
interactions.
There are some recent works that model high-order feature interactions. For example, NFM stacked deep neural networks on
top of the output of the second-order feature interactions to model
higher-order features. Similarly, PNN , FNN , DeepCrossing , Wide&Deep and DeepFM utilized feed-forward
neural networks to model high-order feature interactions. However, all these approaches learn the high-order feature interactions
in an implicit way and therefore lack good model explainability.
On the contrary, there are three lines of works that learn feature interactions in an explicit fashion. First, Deep&Cross 
and xDeepFM took outer product of features at the bit- and
vector-wise level respectively. Although they perform explicit feature interactions, it is not trivial to explain which combinations are
useful. Second, some tree-based methods combined the
power of embedding-based models and tree-based models but had
to break training procedure into multiple stages. Third, HOFM 
proposed efficient training algorithms for high-order factorization
machines. However, HOFM requires too many parameters and only
its low-order (usually less than 5) form can be practically used. Different from existing work, we explicitly model feature interactions
with attention mechanism in an end-to-end manner, and probe the
learned feature combinations via visualization.
Attention and Residual Networks
Our proposed model makes use of the latest techniques in the literature of deep learning: attention and residual networks .
Attention is first proposed in the context of neural machine translation and has been proved effective in a variety of tasks such
as question answering , text summarization , and recommender systems . Vaswani et al. further proposed
multi-head self-attention to model complicated dependencies between words in machine translation.
Residual networks achieved state-of-the-art performance
in the ImageNet contest. Since the residual connection, which can
be simply formalized as y = F(x) + x, encourages gradient flow
through interval layers, it becomes a popular network structure for
training very deep neural networks.
PROBLEM DEFINITION
We first formally define the problem of click-through rate (CTR)
prediction as follows:
DEFINITION 1. (CTR Prediction) Let x ∈Rn denotes the concatenation of user u’s features and item v’s features, where categorical features are represented with one-hot encoding, and n is the
dimension of concatenated features. The problem of click-through
rate prediction aims to predict the probability of user u clicking on
item v according to the feature vector x.
A straightforward solution for CTR prediction is to treat x as the
input features and deploy the off-the-shelf classifiers such as logistic
regression. However, since the original feature vector x is very
sparse and high-dimensional, the model will be easily overfitted.
Therefore, it is desirable to represent the raw input features in lowdimensional continuous spaces. Moreover, as shown in existing
literature, it is crucial to utilize the higher-order combinatorial
features to yield good prediction performance .
Multi-head
Self-Attention
Interacting
Output Layer: Estimated CTR
Input Layer: sparse feature X
Feature field 1
Feature field M
Figure 1: Overview of our proposed model AutoInt. The details of embedding layer and interacting layer are illustrated
in Figure 2 and Figure 3 respectively.
Specifically, we define the high-order combinatorial features as
DEFINITION 2. (p-order Combinatorial Feature) Given input
feature vector x ∈Rn, a p-order combinatorial feature is defined
as д(xi1, ...,xip ) , where each feature comes from a distinct field, p
is the number of involved feature fields, and д(·) is a non-additive
combination function, such as multiplication and outer product . For example, xi1 × xi2 is a second-order combinatorial
feature involving xi1 and xi2.
Traditionally, meaningful high-order combinatorial features are
hand-crafted by domain experts. However, this is very time-consuming
and hard to generalize to other domains. Besides, it is almost impossible to hand-craft all meaningful high-order features. Therefore,
we aim to develop an approach that is able to automatically discover
the meaningful high-order combinatorial features and meanwhile
map all these features into low-dimensional continuous spaces.
Formally, we define our problem as follows:
DEFINITION 3. (Problem Definition) Given an input feature
vector x ∈Rn for click-through rate prediction, our goal is to learn
a low-dimensional representation of x, which models the high-order
combinatorial features.
AUTOINT: AUTOMATIC FEATURE
INTERACTION LEARNING
In this section, we first give an overview of the proposed approach
AutoInt, which can automatically learn feature interactions for CTR
prediction. Next, we present a comprehensive description of how
to learn a low-dimensional representation that models high-order
combinatorial features without manual feature engineering.
The goal of our approach is to map the original sparse and highdimensional feature vector into low-dimensional spaces and meanwhile model the high-order feature interactions. As shown in Figure 1, our proposed method takes the sparse feature vector x as
input, followed by an embedding layer that projects all features
Input layer
Figure 2: Illustration of input and embedding layer, where
both categorical and numerical fields are represented by lowdimensional dense vectors.
(i.e., both categorical and numerical features) into the same lowdimensional space. Next, we feed embeddings of all fields into a
novel interacting layer, which is implemented as a multi-head selfattentive neural network. For each interacting layer, high-order
features are combined through the attention mechanism, and different kinds of combinations can be evaluated with the multi-head
mechanisms, which map the features into different subspaces. By
stacking multiple interacting layers, different orders of combinatorial features can be modeled.
The output of the final interacting layer is the low-dimensional
representation of the input feature, which models the high-order
combinatorial features and is further used for estimating the clickthrough rate through a sigmoid function. Next, we introduce the
details of our proposed method.
Input Layer
We first represent user’s profiles and item’s attributes as a sparse
vector, which is the concatenation of all fields. Specifically,
x = [x1; x2; ...; xM],
where M is the number of total feature fields, and xi is the feature
representation of the i-th field. xi is a one-hot vector if the i-th field
is categorical (e.g., x1 in Figure 2). xi is a scalar value if the i-th
field is numerical (e.g., xM in Figure 2).
Embedding Layer
Since the feature representations of the categorical features are very
sparse and high-dimensional, a common way is to represent them
into low-dimensional spaces (e.g., word embeddings). Specifically,
we represent each categorical feature with a low-dimensional vector,
ei = Vixi,
where Vi is an embedding matrix for field i, and xi is an one-hot
vector. Often times categorical features can be multi-valued, i.e., xi
is a multi-hot vector. Take movie watching prediction as an example,
there could be a feature field Genre which describes the types of
a movie and it may be multi-valued (e.g., Drama and Romance
for movie “Titanic”). To be compatible with multi-valued inputs,
we further modify the Equation 2 and represent the multi-valued
feature field as the average of corresponding feature embedding
where q is the number of values that a sample has for i-th field and
xi is the multi-hot vector representation for this field.
To allow the interaction between categorical and numerical features, we also represent the numerical features in the same lowdimensional feature space. Specifically, we represent the numerical
feature as
em = vmxm,
where vm is an embedding vector for field m, and xm is a scalar
By doing this, the output of the embedding layer would be a concatenation of multiple embedding vectors, as presented in Figure 2.
Interacting Layer
Once the numerical and categorical features live in the same lowdimensional space, we move to model high-order combinatorial
features in the space. The key problem is to determine which features should be combined to form meaningful high-order features.
Traditionally, this is accomplished by domain experts who create
meaningful combinations based on their knowledge. In this paper, we tackle this problem with a novel method, the multi-head
self-attention mechanism .
Multi-head self-attentive network has recently achieved
remarkable performance in modeling complicated relations. For
example, it shows superiority for modeling arbitrary word dependency in machine translation and sentence embedding ,
and has been successfully applied to capturing node similarities
in graph embedding . Here we extend this latest technique to
model the correlations between different feature fields.
Specifically, we adopt the key-value attention mechanism to
determine which feature combinations are meaningful. Taking the
feature m as an example, next we explain how to identify multiple
meaningful high-order features involving feature m. We first define
the correlation between feature m and feature k under a specific
attention head h as follows:
exp(ψ (h)(em, ek))
l=1 exp(ψ (h)(em, el))
ψ (h)(em, ek) = ⟨W(h)
Queryem, W(h)
whereψ (h)(·, ·) is an attention function which defines the similarity
between the feature m and k. It can be defined as a neural network
or as simple as inner product, i.e., ⟨·, ·⟩. In this work, we use inner
product due to its simplicity and effectiveness. W(h)
Query, W(h)
Rd′×d in Equation 5 are transformation matrices which map the
original embedding space Rd into a new space Rd′. Next, we update
the representation of feature m in subspace h via combining all
relevant features guided by coefficients α(h)
where W(h)
Value ∈Rd′×d.
Since ee(h)
m ∈Rd′ is a combination of feature m and its relevant
features (under head h), it represents a new combinatorial feature
learned by our method. Furthermore, a feature is also likely to be
involved in different combinatorial features, and we achieve this by
using multiple heads, which create different subspaces and learn
distinct feature interactions separately. We collect combinatorial
features learned in all subspaces as follows:
Figure 3: The architecture of interacting layer. Combinatorial features are conditioned on attention weights, i.e., α(h)
eem = ee(1)
m ⊕· · · ⊕ee(H)
where ⊕is the concatenation operator, and H is the number of total
To preserve previously learned combinatorial features, including
raw individual (i.e., first-order) features, we add standard residual
connections in our network. Formally,
= ReLU(eem + WResem),
where WRes ∈Rd′H×d is the projection matrix in case of dimension mismatching , and ReLU(z) = max(0,z) is a non-linear
activation function.
With such an interacting layer, the representation of each feature
em will be updated into a new feature representation eRes
is a representation of high-order features. We can stack multiple
such layers with the output of the previous interacting layer as the
input of the next interacting layer. By doing this, we can model
arbitrary-order combinatorial features.
Output Layer
The output of the interacting layer is a set of feature vectors {eRes
which includes raw individual features reserved by residual block
and combinatorial features learned via the multi-head self-attention
mechanism. For final CTR prediction, we simply concatenate all of
them and then apply a non-linear projection as follows:
ˆy = σ(wT(eRes
⊕· · · ⊕eRes
where w ∈Rd′HM is a column projection vector which linearly
combines concatenated features,b is the bias, andσ(x) = 1/(1+e−x)
transforms the values to users clicking probabilities.
Our loss function is Log loss, which is defined as follows:
Loдloss = −1
(yj log(ˆyj) + (1 −yj) log(1 −ˆyj)),
where yj and ˆyj are ground truth of user clicks and estimated
CTR respectively, j indexes the training samples, and N is the total
number of training samples. The parameters to learn in our model
are {Vi, vm, W(h)
Query, W(h)
Value, WRes, w,b}, which are updated
via minimizing the total Logloss using gradient descent.
Analysis Of AutoInt
Modeling Arbitrary Order Combinatorial Features. Given feature interaction operator defined by Equation 5 - 8, we now analyze
how low-order and high-order combinatorial features are modeled
in our proposed model.
For simplicity, let’s assume there are four feature fields (i.e., M=4)
denoted byx1,x2,x3 andx4 respectively. Within the first interacting
layer, each individual feature interacts with any other features
through attention mechanism (i.e. Equation 5) and therefore a set
of second-order feature combinations such asд(x1,x2),д(x2,x3) and
д(x3,x4) are captured with distinct correlation weights, where the
non-additive property of interaction function д(·) (in DEFINITION
2) can be ensured by the non-linearity of activation function ReLU(·).
Ideally, combinatorial features that involve x1 can be encoded into
the updated representation of the first feature field eRes
same can be derived for other feature fields, all second-order feature
interactions can be encoded in the output of the first interacting
layer, where attention weights distill useful feature combinations.
Next, we prove that higher-order feature interactions can be
modeled within the second interacting layer. Given the representation of the first feature field eRes
and the representation of the third
feature field eRes
generated by the first interacting layer, third-order
combinatorial features that involve x1, x2 and x3 can be modeled
by allowing eRes
to attend on eRes
because eRes
contains the interaction д(x1,x2) and eRes
contains the individual feature x3 (from
residual connection). Moreover, the maximum order of combinatorial features grows exponentially with respect to the number of
interacting layers. For example, fourth-order feature interaction
д(x1,x2,x3,x4) can be captured by the combination of eRes
which contain the second-order interactions д(x1,x2) and д(x3,x4)
respectively. Therefore a few interacting layers will suffice to model
high-order feature interactions.
Based on above analysis, we can see that AutoInt learns feature
interactions with attention mechanism in a hierarchical manner,
i.e., from low-order to high-order, and all low-order feature interactions are carried by residual connections. This is promising and
reasonable because learning hierarchical representation has proven
quite effective in computer vision and speech processing with deep
neural networks .
Space Complexity. The embedding layer, which is a shared component in neural network-based methods , contains nd
parameters, where n is the dimension of sparse representation of input feature and d is the embedding size. As an interacting layer contains following weight matrices: {W(h)
Query, W(h)
Value, WRes},
the number of parameters in an L-layer network is L×(3dd′+d′Hd),
which is independent of the number of feature fields M. Finally,
there are d′HM + 1 parameters in the output layer. As far as interacting layers are concerned, the space complexity is O(Ldd′H).
Note that H and d′ are usually small (e.g., H = 2 and d′ = 32 in our
experiments), which makes the interacting layer memory-efficient.
Time Complexity. Within each interacting layer, the computation
cost is two-fold. First, calculating attention weights for one head
takes O(Mdd′ + M2d′) time. Afterwards, forming combinatorial
features under one head also takes O(Mdd′ + M2d′) time. Because
we have H heads, it takes O(MHd′(M + d)) time altogether. It is
therefore efficient because H,d andd′ are usually small. We provide
running time of AutoInt in Section 5.2.
EXPERIMENT
In this section, we move forward to evaluate the effectiveness of
our proposed approach. We aim to answer the following questions:
RQ1 How does our proposed AutoInt perform on the problem of
CTR prediction? Is it efficient for large-scale sparse and
high-dimensional data?
RQ2 What are the influences of different model configurations?
RQ3 What are the dependency structures between different
features? Is our proposed model explainable?
RQ4 Will integrating implicit feature interactions further
improve the performance?
We first describe the experimental settings before answering these
questions.
Experiment Setup
Data Sets. We use four public real-world data sets. The
statistics of the data sets are summarized in Table 1. Criteo3 This is
a benchmark dataset for CTR prediction, which has 45 million users’
clicking records on displayed ads. It contains 26 categorical feature
fields and 13 numerical feature fields. Avazu4 This dataset contains
users’ mobile behaviors including whether a displayed mobile ad
is clicked by a user or not. It has 23 feature fields spanning from
user/device features to ad attributes. KDD125 This data set was
released by KDDCup 2012, which originally aimed to predict the
number of clicks. Since our work focuses on CTR prediction rather
than the exact number of clicks, we treat this problem as a binary
classification problem (1 for clicks>0, 0 for without click), which is
similar to FFM . MovieLens-1M6 This dataset contains users’
ratings on movies. During binarization, we treat samples with a
rating less than 3 as negative samples because a low score indicates
that the user does not like the movie. We treat samples with a rating
greater than 3 as positive samples and remove neutral samples, i.e.,
a rating equal to 3.
Data Preparation First, we remove the infrequent features (appearing in less than threshold instances) and treat them as a single
feature “<unknown>”, where threshold is set to {10, 5, 10} for Criteo,
Avazu and KDD12 data sets respectively. Second, since numerical
features may have large variance and hurt machine learning algorithms, we normalize numerical values by transforming a value
z to loд2(z) if z > 2, which is proposed by the winner of Criteo
Competition7. Third, we randomly select 80% of all samples for
training and randomly split the rest into validation and test sets of
equal size.
Evaluation Metrics. We use two popular metrics to evaluate the performance of all methods.
3 
4 
5 
6 
7 
Table 1: Statistics of evaluation data sets.
#Features (Sparse)
45,840,617
40,428,967
149,639,105
MovieLens-1M
AUC Area Under the ROC Curve (AUC) measures the probability
that a CTR predictor will assign a higher score to a randomly chosen
positive item than a randomly chosen negative item. A higher AUC
indicates a better performance.
Logloss Since all models attempt to minimize the Logloss defined
by Equation 10, we use it as a straightforward metric.
It is noticeable that a slightly higher AUC or lower Logloss at
0.001-level is regarded significant for CTR prediction task, which
has also been pointed out in existing works .
Competing Models. We compare the proposed approach
with three classes of previous models. (A) the linear approach that
only uses individual features. (B) factorization machines-based
methods that take into account second-order combinatorial features.
(C) techniques that can capture high-order feature interactions. We
associate the model classes with model names accordingly.
LR (A). LR only models the linear combination of raw features.
FM (B). FM uses factorization techniques to model secondorder feature interactions.
AFM (B). AFM is one of the state-of-the-art models that
capture second-order feature interactions. It extends FM by using
attention mechanism to distinguish the different importance of
second-order combinatorial features.
DeepCrossing (C). DeepCrossing utilizes deep fully-connected
neural networks with residual connections to learn non-linear feature interactions in an implicit fashion.
NFM (C). NFM stacks deep neural networks on top of
second-order feature interaction layer. High-order feature interactions are implicitly captured by the nonlinearity of neural networks.
CrossNet (C). Cross Network, which is the core of Deep&Cross
model, takes outer product of concatenated feature vector at the
bit-wise level to model feature interactions explicitly.
CIN (C). Compressed Interaction Network, which is the
core of xDeepFM model, takes outer product of stacked feature
matrix at vector-wise level.
HOFM (C). HOFM proposes efficient kernel-based algorithms
for training high-order factorization machines. Follow settings
in Blondel et al. and He and Chua , we build a third-order
factorization machine using public implementation.
We will compare with the full models of CrossNet and CIN, i.e.,
Deep&Cross and xDeepFM, under the setting of joint training with
plain DNN later (i.e., Section 5.5).
Implementation Details. All methods are implemented in
TensorFlow . For AutoInt and all baseline methods, we empirically
set embedding dimension d to 16 and batch size to 1024. AutoInt
has three interacting layers and the number of hidden units d′ is
32 in default setting. Within each interacting layer, the number of
Table 2: Effectiveness Comparison of Different Algorithms. We highlight that our proposed model almost outperforms all
baselines across four data sets and both metrics. Further analysis is provided in Section 5.2.
Model Class
MovieLens-1M
First-order
Second-order
High-order
DeepCrossing 
CrossNet 
AutoInt (ours)
AutoInt outperforms the strongest baseline w.r.t. Criteo, KDD12 and MovieLens-1M data at the: ** 0.01 and * 0.05 level, unpaired t-test.
Run time per epoch
(a) Criteo
Run time per epoch
Run time per epoch
Run time per epoch
(d) MovieLens-1M
Figure 4: Efficiency Comparison of Different Algorithms in terms of Run Time. “DC” and “CN” are DeepCrossing and CrossNet
for short, respectively. Since HOFM cannot be fit on one GPU card for the KDD12 dataset, extra communication cost makes it
most time-consuming. Further analysis is presented in Section 5.2.
attention head is two8. To prevent overfitting, we use grid search
to select dropout rate from {0.1 - 0.9} for MovieLens-1M data
set, and we found dropout is not necessary for other three large
data sets. For baseline methods, we use one hidden layer of size 200
on top of Bi-Interaction layer for NFM as recommended by their
paper. For CN and CIN, we use three interaction layers following
AutoInt. DeepCrossing has four feed-forward layers and the number
of hidden units is 100, because it performs poorly when using
three neural layers. Once all network structures are fixed, we also
apply grid search to baseline methods for optimal hype-parameters.
Finally, we use Adam to optimize all deep neural network-based
Quantitative Results (RQ1)
Evaluation of Effectiveness
We summarize the results averaged over 10 different runs into Table 2. We have the following observations: (1) FM and AFM, which
explore second-order feature interactions, consistently outperform
LR by a large margin on all datasets, which indicates that individual features are insufficient in CTR prediction. (2) An interesting
observation is the inferiority of some models which capture highorder feature interactions. For example, although DeepCrossing
8We also tried different number of attention heads. The performance of using one
head is inferior to that of two heads, and the improvement of further increasing head
number is not significant.
and NFM use the deep neural network as a core component to
learning high-order feature interactions, they do not guarantee
improvement over FM and AFM. This may attribute to the fact
that they learn feature interactions in an implicit fashion. On the
contrary, CIN does it explicitly and outperforms low-order models
consistently. (3) HOFM significantly outperforms FM on Criteo and
MovieLens-1M datasets, which indicates that modeling third-order
feature interactions can be beneficial to prediction performance. (4)
AutoInt achieves the best performance overall baseline methods on
three of four real-world data sets. On Avazu data set, CIN performs
a little better than AutoInt in AUC evaluation, but we get lower
Logloss. Note that our proposed AutoInt shares the same structures
as DeepCrossing except the feature interacting layer, which indicates using the attention mechanism to learn explicit combinatorial
features is crucial.
Evaluation of Model Efficiency
We present the runtime results of different algorithms on four data
sets in Figure 4. Unsurprisingly, LR is the most efficient algorithm
due to its simplicity. FM and NFM perform similarly in terms of
runtime because NFM only stacks a single feed-forward hidden
layer on top of the second-order interaction layer. Among all listed
methods, CIN, which achieves the best performance for prediction
among all the baselines, is much more time-consuming due to its
complicated crossing layer. This may make it impractical in the
Table 3: Efficiency Comparison of Different Algorithms in
terms of Model Size on Criteo data set. “DC” and “CN”
are DeepCrossing and CrossNet for short, respectively. The
counted parameters exclude the embedding layer.
Table 4: Ablation study comparing the performance of AutoInt with and without residual connections. AutoIntw/ is
the complete model while the AutoIntw/o is the model without residual connection.
AutoIntw/o
AutoIntw/o
AutoIntw/o
MovieLens-1M
AutoIntw/o
industrial scenarios. Note that AutoInt is sufficiently efficient, which
is comparable to the efficient algorithms DeepCrossing and NFM.
We also compare the sizes of different models (i.e., the number
of parameters) as another criterion for efficiency evaluation. As
shown in Table 3, comparing to the best model CIN in the baseline
models, the number of parameters in AutoInt is much smaller.
To summarize, our proposed AutoInt achieves the best performance among all the compared models. Compared to the most
competitive baseline model CIN, AutoInt requires much fewer parameters and is much more efficient during online inference.
Analysis (RQ2)
To further validate and gain deep insights into the proposed model,
we conduct ablation study and compare several variants of AutoInt.
Influence of Residual Structure. The standard AutoInt utilizes residual connections, which carry through all learned combinatorial features and therefore allow modeling very high-order
combinations. To justify the contribution of residual units, we tease
apart them from our standard model and keep other structures as
they are. As presented in Table 4, we observe that the performance
decrease on all datasets if residual connections are removed. Specifically, the full model outperforms the variant by a large margin
on the KDD12 and MovieLens-1M data, which indicates residual
connections are crucial to model high-order feature interactions in
our proposed method.
Influence of Network Depths. Our model learns high-order
feature combinations by stacking multiple interacting layers (introduced in Section 4). Therefore, we are interested in how the
performance change w.r.t. the number of interacting layers, i.e.,
the order of combinatorial features. Note that when there is no
interacting layer (i.e., Number of layers equals zero), our model
Number of layers
MovieLens-1M
Number of layers
MovieLens0M
(b) Logloss
Figure 5: Performance w.r.t. the number of interacting layers. Results on Criteo and Avazu data sets are similar and
hence omitted.
#Dimension
MovieLens-1M
#Dimension
MovieLens-1M
(b) Logloss
Figure 6: Performance w.r.t. number of embedding dimensions. Results on Criteo and Avazu data sets are similar and
hence omitted.
takes the weighted sum of raw individual features as input, i.e., no
combinatorial features are considered.
The results are summarized in Figure 5. We can see that if one
interacting layer is used, i.e., feature interactions are taken into
account, the performance increase dramatically on both data sets,
showing that combinatorial features are very informative for prediction. As the number of interacting layers further increases, i.e.,
higher-order combinatorial features are taken into account, the
performance of the model further increases. When the number
of layers reaches three, the performance becomes stable, showing
that adding extremely high-order features are not informative for
prediction.
Influence of Different Dimensions. Next, we investigate the
performance w.r.t. the parameter d, which is the output dimension
of the embedding layer. On the KDD12 dataset, we can see that the
performance continuously increase as we increase the dimension
size since larger models are used for prediction. The results are
different on the MovieLens-1M dataset. When the dimension size
reaches 24, the performance begins to decrease. The reason is that
this data set is small, and the model is overfitted when too many
parameters are used.
Explainable Recommendations (RQ3)
A good recommender system can not only provide good recommendations but also offer good explainability. Therefore, in this part,
we present how our AutoInt is able to explain the recommendation
results. We take the MovieLens-1M dataset as an example.
Let’s look at a recommendation result suggested by our algorithm, i.e., a user likes an item. Figure 7 (a) presents the correlations
between different fields of input features, which are obtained by
Table 5: Results of Integrating Implicit Feature Interactions. We indicate the base model behind each method. The last two
columns are average changes of AUC and Logloss compared to corresponding base models (“+”: increase, “-”: decrease).
MovieLens-1M
Avg. Changes
Wide&Deep (LR)
DeepFM (FM)
Deep&Cross (CN)
xDeepFM (CIN)
AutoInt+ (ours)
AutoInt+ outperforms the strongest baseline w.r.t. each data at the: ** 0.01 and * 0.05 level, unpaired t-test.
(a) Label=1, Predicted CTR=0.89
(b) Overall feature interactions
Figure 7: Heat maps of attention weights for both caseand global-level feature interactions on MovieLens-1M. The
axises represent feature fields <Gender, Age, Occupation, Zipcode, RequestTime, RealeaseTime, Genre>. We highlight some
learned combinatorial features in rectangles.
the attention score. We can see that AutoInt is able to identify
the meaningful combinatorial feature <Gender=Male, Age=[18-24),
MovieGenre=Action&Triller> (i.e., red dotted rectangle). This is very
reasonable since young men are very likely to prefer action&triller
We are also interested in what the correlations between different
feature fields in the data are. Therefore, we measure the correlations
between the feature fields according to their average attention score
in the entire data. The correlations between different fields are summarized into Figure 7 (b). We can see that <Gender, Genre>, <Age,
Genre>, <RequestTime, ReleaseTime> and <Gender, Age, Genre> (i.e.,
solid green region) are strongly correlated, which are the explainable rules for recommendation in this domain.
Integrating Implicit Interactions (RQ4)
Feed-forward neural networks are capable of modeling implicit feature interactions and have been widely integrated into existing CTR
prediction methods . To investigate whether integrating
implicit feature interactions further improves the performance, we
combine AutoInt with a two-layer feed-forward neural network by
joint training. We name the joint model AutoInt+ and compare it
with the following algorithms:
• Wide&Deep . Wide&Deep integrates the outputs of logistic
regression and feed-forward neural networks.
• DeepFM . DeepFM combines trainditional second-order factorization machines and feed-forward neural network, with a
shared embedding layer.
• Deep&Cross . Deep&Cross is the extension of CrossNet by
integrating feed-forward neural networks.
• xDeepFM . xDeepFM is the extension of CIN by integrating
feed-forward neural networks.
Table 5 presents the averaged results (over 10 runs) of jointtraining models. We have the following observations: 1) The performance of our method improves by joint training with feed-forward
neural networks on all datasets. This indicates that integrating implicit feature interactions indeed boosts the predictive ability of our
proposed model. However, as can be seen from last two columns,
the magnitude of performance improvement is fairly small compared to other models, showing that our individual model AutoInt
is quite powerful. 2) After integrating implicit feature interactions,
AutoInt+ outperforms all competitive methods, and achieves new
state-of-the-art performances on used CTR prediction data sets.
CONCLUSION AND FUTURE WORK
In this work, we propose a novel CTR prediction model based on
self-attention mechanism, which can automatically learn high-order
feature interactions in an explicit fashion. The key to our method
is the newly-introduced interacting layer, which allows each feature to interact with the others and to determine the relevance
through learning. Experimental results on four real-world data sets
demonstrate the effectiveness and efficiency of our proposed model.
Besides, we provide good model explainability via visualizing the
learned combinatorial features. When integrating with implicit
feature interactions captured by feed-forward neural networks,
we achieve better offline AUC and Logloss scores compared to the
previous state-of-the-art methods.
For future work , we are interested in incorporating contextual
information into our method and improving its performance for online recommender systems. Besides, we also plan to extend AutoInt
for general machine learning tasks, such as regression, classification
and ranking.
ACKNOWLEDGEMENT
The authors would like to thank all the anonymous reviewers for
their insightful comments. We thank Xiao Xiao and Jianbo Dong for
the discussion on recommendation mechanism in China University
MOOC platform. We also thank Meng Qu for reviewing the initial
version of this paper. Weiping Song and Ming Zhang are supported
by National Key Research and Development Program of China with
Grant No. SQ2018AAA010010, Beijing Municipal Commission of
Science and Technology under Grant No. Z181100008918005 as well
as the National Natural Science Foundation of China (NSFC Grant
Nos.61772039 and 91646202). Weiping Song is also supported by
Chinese Scholarship Council. Jian Tang is supported by the Natural
Sciences and Engineering Research Council of Canada, as well as
the Canada CIFAR AI Chair Program.