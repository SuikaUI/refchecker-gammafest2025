Communicated by Klaus-Robert M¨uller
SVDD-Based Pattern Denoising
Jooyoung Park
 
Daesung Kang
 
Department of Control and Instrumentation Engineering, Korea University,
Jochiwon, Chungnam, 339-700, Korea
Jongho Kim
 
Mechatronics and Manufacturing Technology Center, Samsung Electronics Co., Ltd.,
Suwon, Gyeonggi, 443-742, Korea
James T. Kwok
 
Ivor W. Tsang
 
Department of Computer Science and Engineering, Hong Kong University of Science
and Technology, Clear Water Bay, Hong Kong
The support vector data description (SVDD) is one of the best-known
one-class support vector learning methods, in which one tries the strategy
of using balls deﬁned on the feature space in order to distinguish a
set of normal data from all other possible abnormal objects. The major
concern of this letter is to extend the main idea of SVDD to pattern
denoising. Combining the geodesic projection to the spherical decision
boundary resulting from the SVDD, together with solving the preimage
problem, we propose a new method for pattern denoising. We ﬁrst solve
SVDD for the training data and then for each noisy test pattern, obtain its
denoised feature by moving its feature vector along the geodesic on the
manifold to the nearest decision boundary of the SVDD ball. Finally we
ﬁnd the location of the denoised pattern by obtaining the pre-image of the
denoised feature. The applicability of the proposed method is illustrated
by a number of toy and real-world data sets.
1 Introduction
Recently, the support vector learning method has become a viable tool in
the area of intelligent systems . Among the important application areas for support
Neural Computation 19, 1919–1938 
C⃝2007 Massachusetts Institute of Technology
J. Park et al.
vector learning, we have the one-class classiﬁcation problems . In one-class classiﬁcation problems, we are given
only the training data for the normal class, and after the training phase
is ﬁnished, we are required to decide whether each test vector belongs
to the normal or the abnormal class. One-class classiﬁcation problems
are often called outlier detection problems or novelty detection problems. Obvious examples of this class include fault detection for machines
and the intrusion detection system for computers (Sch¨olkopf & Smola,
One of the best-known support vector learning methods for the oneclass problems is the SVDD (support vector data description) . In the SVDD, balls are used for expressing the region
for the normal class. Among the methods having the same purpose with
the SVDD are the so-called one-class SVM of Sch¨olkopf and others , the linear programming method of Campbell and Bennet , the information-bottleneckprinciple-based optimization approach of Crammer and Chechik ,
and the single-class minimax probability machine of Lanckriet et al.
 . Since balls on the input domain can express only a limited
class of regions, the SVDD in general enhances its expressing power
by utilizing balls on the feature space instead of the balls on the input
In this letter, we extend the main idea of the SVDD toward the use for
the problem of pattern denoising . Combining the movement to the spherical decision
boundary resulting from the SVDD together with a solver for the preimage
problem, we propose a new method for pattern denoising that consists of
the following steps. First, we solve the SVDD for the training data consisting
of the prototype patterns. Second, for each noisy test pattern, we obtain its
denoised feature by moving its feature vector along the geodesic to the
spherical decision boundary of the SVDD ball on the feature space. Finally
in the third step, we recover the location of the denoised pattern by obtaining
the preimage of the denoised feature following the strategy of Kwok and
Tsang .
The remaining parts of this letter are organized as follows. In section 2,
preliminaries are provided regarding the SVDD. Our main results on pattern denoising based on the SVDD are presented in section 3. In section
4, the applicability of the proposed method is illustrated by a number of
toy and real-world data sets. Finally, in section 5, concluding remarks are
SVDD-Based Pattern Denoising
2 Preliminaries
The SVDD method, which approximates the support of objects belonging to
normal class, is derived as follows . Consider a
ball B with center a ∈Rd and radius R, and the training data set Dconsisting
of objects xi ∈Rd, i = 1, . . . , N. The main idea of SVDD is to ﬁnd a ball that
can achieve two conﬂicting goals (it should be as small as possible and
contain as many training data as possible) simultaneously by solving
min L0(R2, a, ξ) = R2 + C
∥xi −a∥2 ≤R2 + ξi, ξi ≥0,
i = 1, . . . , N.
Here, the slack variable ξi represents the penalty associated with the deviation of the ith training pattern outside the ball, and C is a trade-off constant
controlling the relative importance of each term. The dual problem of equation 2.1 is
αi⟨xi, xi⟩−
αi α j⟨xi, x j⟩
αi = 1, αi ∈[0, C],
i = 1, . . . , N.
In order to express more complex decision regions in Rd, one can use the
so-called feature map φ : Rd →F and balls deﬁned on the feature space F.
Proceeding similar to the above and utilizing the kernel trick ⟨φ(x), φ(z)⟩=
k(x, z), one can ﬁnd the corresponding feature space SVDD ball BF in F.
Moreover, from the Kuhn-Tucker condition, its center can be expressed as
and its radius RF can be computed by utilizing the distance between aF
and any support vector x on the ball boundary:
F = k(x, x) −2
αik(xi, x) +
αiα jk(xi, x j).
In this letter, we always use the gaussian kernel k(x, z) = exp(−∥x −z∥2/s2),
and so k(x, x) = 1 for each x ∈Rd. Finally, note that in this case, the SVDD
J. Park et al.
formulation is equivalent to
αiα jk(xi, x j)
αi = 1, αi ∈[0, C],
i = 1, . . . , N,
and the resulting criterion for the normality is
F −∥φ(x) −aF ∥2 ≥0.
3 Main Results
In SVDD, the objective is to ﬁnd the support of the normal objects; anything
outside the support is viewed as abnormal. In the feature space, the support
is expressed by a reasonably small ball containing a reasonably large portion
of the φ(xi)’s. The main idea of this letter is to utilize the ball-shaped support
on the feature space for correcting test inputs distorted by noise. More
precisely, with the trade-off constant C set appropriately,1 we can ﬁnd a
region where the normal objects without noise generally reside. When an
object (which was originally normal) is given as a test input x in a distorted
form, the network resulting from the SVDD is supposed to judge that the
distorted object x does not belong to the normal class. The role of the SVDD
has been conventional up to this point, and the problem of curing the
distortion might be thought of as beyond the scope of the SVDD.
In this letter, we go one step further and move the feature vector φ(x)
of the distorted test input x to the point Qφ(x) lying on the surface of the
SVDD ball BF so that it can be tailored enough to be normal (see Figure 1).
Given that all the points in the input space are mapped to a manifold in
the kernel-induced feature space , the movement is along
the geodesic on this manifold, and so the point Qφ(x) can be considered
as the geodesic projection of φ(x) onto the SVDD ball. Of course, since
the movement starts from the distorted feature φ(x), there are plenty of
reasons to believe that the tailored feature Qφ(x) still contains essential
information about the original pattern. We claim that the tailored feature
Qφ(x) is the denoised version of the feature vector φ(x). Pertinent to this
claim is the discussion of Ben-Hur, Horn, Siegelmann, and Vapnik 
on the support vector clustering, in which the SVDD is shown to be a very
efﬁcient tool for clustering since the SVDD ball, when mapped back to
1 In our experiments for noisy handwritten digits, C = 1/(N × 0.2) was used for the
purpose of denoising.
SVDD-Based Pattern Denoising
Figure 1: Basic idea for ﬁnding the denoised feature vector Qφ(x) by moving
along the geodesic.
the input space, can separate into several components, each enclosing a
separate cluster of normal data points, and can generate cluster boundaries
of arbitrary shapes. These arguments, together with an additional step for
ﬁnding the preimage of Qφ(x), comprise our proposal for a new denoising
strategy. In the following, we present the proposed method more precisely
with mathematical details.
The proposed method consists of three steps. First, we solve the SVDD,
equation 2.5, for the given prototype patterns D
△= {xi ∈Rd|i = 1, . . . , N}.
As a result, we ﬁnd the optimal αi’s along with aF and R2
F obtained via
equations 2.3 and 2.4. Second, we consider each test pattern x. When the
decision function fF of equation 2.6 yields a nonnegative value for x, the
test input is accepted as normal, and the denoising process is bypassed
with Qφ(x) set equal to φ(x). Otherwise, the test input x is considered to
be abnormal and distorted by noise. To recover the denoised pattern, we
move its feature vector φ(x) along the geodesic deﬁned on the manifold
in the feature space, toward the SVDD ball BF up to the point where it
touches the ball. In principle, any kernel can be used here. However, as we
will show, closed-form solutions can be obtained when stationary kernels
(such as the gaussian kernel) are used.2 In this case, it is obvious that all the
points are mapped onto the surface of a ball in the feature space, and we
can see from Figure 2 that the point Qφ(x) is the ultimate destination of this
movement. For readers’ convenience, we also include a three-dimensional
drawing (see Figure 3) to clarify Figure 2.
In the following, the proposed method will be presented only for the
gaussian kernel, where all the points are mapped to the unit ball in the feature space. Extension to other stationary kernels is straightforward. In order
to ﬁnd Qφ(x), it is necessary to solve the following series of subproblems:
2 A stationary kernel k(x, x′) is a kernel that depends on only x −x′.
J. Park et al.
Figure 2: Proposed denoising procedure when a stationary kernel is used.
r To ﬁnd the separating hyperplane HF . From Figure 2, it is clear that
for the SVDD problems utilizing stationary kernels, the center aF of
the SVDD ball has the same direction with the weight vector of the
separating hyperplane HF . In particular, when the gaussian kernel is
used, the hyperplane HF can be represented by
2⟨aF , φ(x)⟩= 1 + ∥aF ∥2 −R2
Further information needed for identifying the location of Qφ(x) includes the vectors βaF , δaF , and the distance γ shown in Figure 2.
r To ﬁnd vector βaF . As shown in Figure 2, the vector βaF lies on the
hyperplane HF . Thus, it should satisfy equation 3.1—that is,
2⟨aF , βaF ⟩= 1 + ∥aF ∥2 −R2
Therefore, we have
β = 1 + ∥aF ∥2 −R2
= 1 + αTKα −R2
SVDD-Based Pattern Denoising
Figure 3: Denoised feature vector Qφ(x) shown in a (hypothetical) threedimensional feature space. Here, since the chosen kernel is stationary, the projected feature vector Qφ(x), as well as the feature vector φ(x), should lie on a
ball centered at the origin of the feature space. Also note that the location of
Qφ(x) should be at the boundary of the intersection of the ball surface and the
SVDD ball, which is colored black.
△= [α1 · · · αN]T, and K is the kernel matrix with entries Ki j =
k(xi, x j).
r To ﬁnd distance γ . Since Qφ(x) is on the surface of the unit ball, we
have ∥Qφ(x)∥2 = 1. Also from the Pythagorean theorem, ∥βaF ∥2 +
γ 2 = ∥Qφ(x)∥2 holds. Hence, we have
1 −β2∥aF ∥2 =
1 −β2αTKα.
r To ﬁnd vector δaF . Since Pφ(x)
△= φ(x) + δaF should lie on the hyperplane HF , it should satisfy equation 3.1. Thus, the following holds:
2⟨aF , φ(x) + δaF ⟩= 1 + ∥aF ∥2 −R2
Hence, we have
δ = 1 + ∥aF ∥2 −R2
F −2⟨aF , φ(x)⟩
= 1 + αTKα −R2
△= [k(x, x1), . . . , k(x, xN)]T.
J. Park et al.
r To ﬁnd the denoised feature vector Qφ(x). From Figure 2, we see that
Qφ(x) = βaF +
∥φ(x) + (δ −β)aF ∥(φ(x) + (δ −β)aF ).
Note that with
∥φ(x) + (δ −β)aF ∥
∥φ(x) + (δ −β)aF ∥,
the above expression for Qφ(x) can be further simpliﬁed into
Qφ(x) = λ1φ(x) + λ2aF ,
where λ1 and λ2 can be computed from
1 + 2(δ −β)kxα + (δ −β)2αTKα
1 + 2(δ −β)kxα + (δ −β)2αTKα
Obviously, the movement from the feature φ(x) to Qφ(x) is along the
geodesic to the noise-free normal class and thus can be interpreted as performing denoising in the feature space. With this interpretation in mind,
the feature vector Qφ(x) will be called the denoised feature of x in this letter.
In the third and ﬁnal steps, we try to ﬁnd the preimage of the denoised feature Qφ(x). If the inverse map φ−1 : F →Rd is well deﬁned and available,
this ﬁnal step attempting to get the denoised pattern via ˆx = φ−1(Qφ(x))
will be trivial. However, the exact preimage typically does not exist . Thus, we need to seek an approximate solution instead. For
this, we follow the strategy of Kwok and Tsang , which uses a simple relationship between feature-space distance and input-space distance
 together with the MDS (multi-dimensional scaling) . Using the kernel trick and the simple relation, equation 3.10,
we see that ⟨Qφ(x), φ(xi)⟩can be easily computed as follows:
⟨Qφ(x), φ(xi)⟩= λ1k(xi, x) + λ2
α jk(xi, x j).
Thus, the feature space distance between Qφ(x) and φ(xi) can be obtained
by plugging equation 3.13 into
˜d2(Qφ(x), φ(xi))
△= ∥Qφ(x) −φ(xi)∥2
= 2 −2⟨Qφ(x), φ(xi)⟩.
SVDD-Based Pattern Denoising
Now, note that for the gaussian kernel, the following simple relationship holds true between d(xi, x j)
△= ∥xi −x j∥and ˜d(φ(xi), φ(x j))
△= ∥φ(xi) −
φ(x j)∥ :
˜d2(φ(xi), φ(x j)) = ∥φ(xi) −φ(x j)∥2
= 2 −2k(xi, x j)
= 2 −2 exp(−∥xi −x j∥2/s2)
= 2 −2 exp(−d2(xi, x j)/s2).
Since the feature space distance ˜d2(Qφ(x), φ(xi)) is now available from equation 3.14 for each training pattern xi, we can easily obtain the corresponding
input space distance between the desired approximate preimage ˆx of Qφ(x)
and each xi. Generally, the distances with neighbors are the most important
in determining the location of any point. Hence, here we consider only the
squared input space distances between Qφ(x) and its n nearest neighbors
{φ(x(1)), . . . , φ(x(n))} ⊂DF , and deﬁne
2, . . . , d2
where di is the input space distance between the desired preimage of Qφ(x)
and x(i). In MDS , one attempts to ﬁnd a representation
of the objects that preserves the dissimilarities between each pair of them.
Thus, we can use the MDS idea to embed Qφ(x) back to the input space.
For this, we ﬁrst take the average of the training data {x(1), . . . , x(n)} ⊂D to
get their centroid ¯x = (1/n) n
i=1 x(i), and construct the d × n matrix,
△= [x(1), x(2), . . . , x(n)].
Here, we note that by deﬁning the n × n centering matrix H
n , where In
△= diag[1, . . . , 1] ∈Rn×n and 1n
△= [1, . . . , 1]T ∈Rn×1, the
matrix XH centers the x(i)’s at their centroid:
XH = [x(1) −¯x, . . . , x(n) −¯x].
The next step is to deﬁne a coordinate system in the column space of
XH. When XH is of rank q, we can obtain the SVD (singular value
J. Park et al.
decomposition) of the d × n matrix XH as
XH = [U1U2]
where U1 = [e1, . . . , eq] is the d × q matrix with orthonormal columns ei,
1 = [z1, . . . , zn] is a q × n matrix with columns zi being the
projections of x(i) −¯x onto the e j’s. Note that
∥x(i) −¯x∥2 = ∥zi∥2,
i = 1, . . . , n,
and collect these into an n-dimensional vector:
△= [∥z1∥2, . . . , ∥zn∥2]T.
The location of the preimage ˆx is obtained by requiring d2(ˆx, x(i)), i =
1, . . . , n to be as close to those values in equation 3.16 as possible; thus,
we need to solve the LS (least squares) problem to ﬁnd ˆx:
d2(ˆx, x(i)) ≃d2
i = 1, . . . , n.
Now following the steps of Kwok and Tsang and Gower ,
ˆz ∈Rn×1 deﬁned by ˆx −¯x = U1ˆz can be shown to satisfy
Therefore, by transforming equation 3.23 back to the original coordinated
system in the input space, the location of the recovered denoised pattern
turns out to be
ˆx = U1ˆz + ¯x.
4 Experiments
In this section, we compare the performance of the proposed method with
other denoising methods on toy and real-world data sets. For simplicity, we
denote the proposed method by SVDD.
SVDD-Based Pattern Denoising
4.1 Toy Data Set. We ﬁrst use a toy example to illustrate the proposed
method and compare its reconstruction performance with PCA. The setup is
similar to that in Mika et al. . Eleven clusters of samples are generated
by ﬁrst choosing 11 independent sources randomly in [−1, 1]10 and then
drawing samples uniformly from translations of [−σ0, σ0]10 centered at each
source. For each source, 30 points are generated to form the training data
and 5 points to form the clean test data. Normally distributed noise, with
variance σ 2
o in each component, is then added to each clean test data point
to form the corrupted test data.
We carried out SVDD (with C =
N×0.6) and PCA for the training set,
and then performed reconstructions of each corrupted test point using
both the proposed SVDD-based method (with neighborhood size n = 10)
and the standard PCA method.3 The procedure was repeated for different
numbers of principal components in PCA and for different values of σ0.
For the width s of the gaussian kernel, we used s2 = 2 × 10 × σ 2
0 as in Mika
et al. . From the simulations, we found out that when the input space
dimensionality d is low (as in this example, where d = 10), applying the
proposed method iteratively (i.e., recursively applying the denoising to the
previous denoised results) can improve the performance.
We compared the results of our method (with 100 iterations) to those of
the PCA-based method using the mean squared distance (MSE), which is
∥tk −ˆtk∥2,
where M is the number of test patterns, tk is the kth clean test pattern, and
ˆtk is the denoised result for the kth noisy test pattern. Table 1 shows the
ratio of MSEPC A/MSESVDD. Note that ratios larger than one indicate that
the proposed SVDD-based method performs better compared to the other
Simulations were also performed for a two-dimensional version of the
toy example (see Figure 4a), and the denoised results were shown in
Figures 4b and 4c. For PCA, we used only one eigenvector (if two eigenvectors were used, the result is just a change of basis and thus not useful).
The observed MSE values for the reconstructions using the proposed and
PCA-based methods were 0.0192 and 0.1902, respectively.
From Table 1 and Figures 4b and 4c, one can see that in the considered
examples, the proposed method yielded better performance than the PCAbased method. The reason seems to be that here, the examples basically deal
3 The corresponding Matlab program is posted online at 
ac lab/pro01.html.
J. Park et al.
Table 1: Comparison of MSE Ratios After Reconstructing the Corrupted Test
Points in R10.
Note: Performance ratios, MSEPC A/MSESVDD, being larger than one, indicate how
much better SVDD did compared to PCA for different choices of σ0, and different
numbers of principal components (#EV) in reconstruction using PCA.
Figure 4: A two-dimensional version of the toy example (with σ0 = 0.15) and
its denoised results. Lines join each corrupted point (denoted +) with its
reconstruction (denoted o). For SVDD, s2 = 2 × 2 × σ 2
0 and C = 1/(N × 0.6).
(a) Training data (denoted •) and corrupted test data (denoted +). (b) Reconstruction using the proposed method (with 100 iterations) along with the resultant SVDD balls. (c) Reconstruction using the PCA-based method, where one
principal component was used.
SVDD-Based Pattern Denoising
Figure 5: Sample USPS digit images. (a) Clean. (b) With gaussian noise (σ 2 =
0.3). (c) With gaussian noise (σ 2 = 0.6). (d) With salt-and-pepper noise (p = 0.3).
(e) With salt-and-pepper noise (p = 0.6).
with clustering-type tasks, so any reconstruction method directly utilizing
projection onto low-dimensional linear manifolds would be inefﬁcient.
4.2 Handwritten Digit Data. In this section, we report the denoising results on the USPS digit database ( which
consists of 16 × 16 handwritten digits of 0 to 9. We ﬁrst normalized each
feature value to the range . For each digit, we randomly chose 60 examples to form the training set and 100 examples as the test set (see Figure 5).
Two types of additive noise were added to the test set.
The ﬁrst is the gaussian noise N(0, σ 2) with variance σ 2, and the second
is the so-called salt-and-pepper noise with noise level p, where p/2 is the
probability that a pixel ﬂips to black or white. Denoising was applied to
each digit separately. The width s of the gaussian kernel is set to
∥xi −x j∥2,
the average squared distance between training patterns. Here, the value of
C was set to the effect that the support for the normal class resulting from
the SVDD may cover approximately 80% (=100% −20%) of the training
data. Finally, in the third step, we used n = 10 neighbors to recover the
denoised pattern ˆx by solving the preimage problem.
J. Park et al.
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
Figure 6: SNRs of the denoised USPS images. (Top) Gaussian noise with variance
σ 2. (a) σ 2 = 0.6. (b) σ 2 = 0.5. (c) σ 2 = 0.4. (Bottom) Salt-and-pepper noise with
noise level p. (d) p = 0.6. (e) p = 0.5. (f) p = 0.4.
The proposed approach is compared with the following standard methods:
r Kernel PCA denoising, using the preimage ﬁnding method in Mika
et al. 
r Kernel PCA denoising, using the preimage ﬁnding method in Kwok
and Tsang 
r Standard (linear) PCA
r Wavelet denoising (using the Wavelet Toolbox in Matlab).
For wavelet denoising, the image is ﬁrst decomposed into wavelet coef-
ﬁcients using the discrete wavelet transform . These wavelet
coefﬁcients are then compared with a given threshold value, and those
that are close to zero are shrunk so as to remove the effect of noise in the
data. The denoised image is then reconstructed from the shrunken wavelet
coefﬁcients by using the inverse discrete wavelet transform. The choice
of the threshold value can be important to denoising performance. In the
experiments, we use two standard methods to determine the threshold:
VisuShrink and SureShrink .
Moreover, the Symlet6 wavelet basis, with two levels of decomposition,
is used. The methods of Mika et al. and Kwok and Tsang 
are both based on kernel PCA and require the number of eigenvectors as a
SVDD-Based Pattern Denoising
Figure 7: Sample denoised USPS images. (Top two rows) Gaussian noise
(σ 2 = 0.6). (a) SVDD. (b) KPCA . (c) KPCA . (d) PCA. (e) Wavelet (VisuShrink). (f) Wavelet (SureShrink). (Bottom two
rows) Salt-and-pepper noise (p = 0.6). (g) SVDD. (h) KPCA .
(i) KPCA . (j) PCA. (k) Wavelet (VisuShrink). (l) Wavelet
(SureShrink).
predetermined parameter. In the experiments, the number of principal components is varied from 5 to 60 (the maximum number of PCA components
that can be obtained on this data set). For SVDD, we set C =
Nν with ν set
to 0.2. For denoising using MDS and SVDD, 10 nearest neighbors are used
to perform preimaging.
J. Park et al.
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
Figure 8: SNRs of the denoised USPS images when 300 samples are chosen
from each digit for training. (Top) Gaussian noise with variance σ 2. (a) σ 2 = 0.6.
(b) σ 2 = 0.5. (c) σ 2 = 0.4. (Bottom) Salt-and-pepper noise with noise level p.
(d) p = 0.6. (e) p = 0.5. (f) p = 0.4.
To quantitatively evaluate the denoised performance, we used the average signal-to-noise ratio (SNR) over the test set images, where the SNR is
var(clean image)
var(clean image – new image)
in decibel (dB). Figure 6 shows the (average) SNR values obtained for
the various methods. SVDD always achieves the best performance. When
more PCs are used, the performance of denoising using kernel PCA increases, while the performance of PCA ﬁrst increases and then decreases
as some noisy PCs are included, which corrupts the resultant images. Note
that one advantage of the wavelet denoising methods is that they do not
require training. But a subsequent disadvantage is that they cannot utilize
the training set, and so both do not perform well here. Samples of the denoised images that correspond to the best setting of each method are shown
in Figure 7.
As the performance of denoising using kernel PCA appears improving
with the number of PCs, we also experimented with a larger data set so
that even more PCs can be used. Here, we followed the same experimental
setup except that 300 (instead of 60) examples were randomly chosen from
SVDD-Based Pattern Denoising
SNR (in dB)
weighting factor for s2
SNR (in dB)
SNR (in dB)
SNR (in dB)
weighting factor for s2
SNR (in dB)
SNR (in dB)
Figure 9: SNR results of the proposed method on varying each of ν, width of the
gaussian kernel (s) and the neighborhood size for MDS (n). (Top) Gaussian noise
with different σ 2’s. (a) Varying ν. (b) Varying s as a factor of s0 in equation 4.2.
(c) Varying n. (Bottom) Salt-and-pepper noise with different p’s. (d) Varying ν.
(e) Varying s as a factor of s0 in equation 4.2. (f) Varying n.
each digit to form the training set. Figure 8 shows the SNR values for the
various methods. On this larger data set, denoising using kernel PCA does
perform better than the others when a suitable number of PCs are chosen.
This demonstrates that the proposed denoising procedure is comparatively
more effective on small training sets.
In order to investigate the robustness of the proposed method, we also
performed experiments using the 60-example training set for a wide range
of ν, width of the gaussian kernel (s), and the neighborhood size for MDS
(n). Results are reported in Figure 9. The proposed method shows robust
performance around the range of parameters used.
In the previous experiments, denoising was applied to each digit separately, which means one must know what the digit is before applying
denoising. To investigate how well the proposed method denoises when
the true digit is unknown, we follow the same setup but combine all
the digits (with a total of 600 digits) for training. Results are shown in
Figures 10 and 11. From the visual inspection, one can see that its performance is slightly inferior to that of the separate digit case. Again,
SVDD is still the best, though kernel PCA using the preimage method in
Kwok and Tsang sometimes achieves better results as more PCs are
J. Park et al.
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
SNR (in dB)
KPCA (Mika et. al.)
KPCA (Kwok & Tsang)
Wavelet (VisuShrink)
Wavelet (SureShrink)
Figure 10: SNRs of the denoised USPS images. Here, the 10 digits are combined
during training. (Top) Gaussian noise with variance σ 2’s. (a) σ 2 = 0.6. (b) σ 2 =
0.5. (c) σ 2 = 0.4. (Bottom) Salt-and-pepper noise with noise level p’s. (d) p = 0.6.
(e) p = 0.5. (f) p = 0.4.
Figure 11: Sample denoised USPS images. The 10 digits are combined during
training. Recall that wavelet denoising does not use the training set, and so their
de-noising results are the same as those in Figure 7 and are not shown here.
(Top) Gaussian noise (with σ 2 = 0.6). (a) SVDD. (b) KPCA .
(c) KPCA . (d) PCA. (Bottom) Salt-and-pepper noise (with
p = 0.6). (e) SVDD. (f) KPCA . (g) KPCA .
SVDD-Based Pattern Denoising
5 Conclusion
We have addressed the problem of pattern denoising based on the SVDD.
Along with a brief review over the SVDD, we presented a new denoising
method that uses the SVDD, the geodesic projection of the noisy point to
the surface of the SVDD ball in the feature space, and a method for ﬁnding
the preimage of the denoised feature vectors. Work yet to be done includes
more extensive comparative studies, which will reveal the strengths and
weaknesses of the proposed method, and reﬁnement of the method for
better denoising.