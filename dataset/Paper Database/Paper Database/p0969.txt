A General Survey on Attention Mechanisms in
Deep Learning
Gianni Brauwers and Flavius Frasincar
Abstract—Attention is an important mechanism that can be employed for a variety of deep learning models across many different
domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The
various attention mechanisms are explained by means of a framework consisting of a general attention model, uniform notation, and a
comprehensive taxonomy of attention mechanisms. Furthermore, the various measures for evaluating attention models are reviewed,
and methods to characterize the structure of attention models based on the proposed framework are discussed. Last, future work in
the ﬁeld of attention models is considered.
Index Terms—Attention models, deep learning, introductory and survey, neural nets, supervised learning
INTRODUCTION
HE idea of mimicking human attention ﬁrst arose in the
ﬁeld of computer vision , in an attempt to reduce
the computational complexity of image processing while
improving performance by introducing a model that would
only focus on speciﬁc regions of images instead of the entire
picture. Although, the true starting point of the attention
mechanisms we know today is often attributed to originate
in the ﬁeld of natural language processing . Bahdanau et
al. implement attention in a machine translation model to
address certain issues with the structure of recurrent neural
networks. After Bahdanau et al. emphasized the advantages of attention, the attention techniques were reﬁned 
and quickly became popular for a variety of tasks, such as
text classiﬁcation , , image captioning , , sentiment
analysis , , and speech recognition , , .
Attention has become a popular technique in deep learning for several reasons. Firstly, models that incorporate
attention mechanisms attain state-of-the-art results for all
of the previously mentioned tasks, and many others. Furthermore, most attention mechanisms can be trained jointly
with a base model, such as a recurrent neural network or
a convolutional neural network using regular backpropagation . Additionally, attention introduces a certain type
of interpretation into neural network models that are
generally known to be highly complicated to interpret.
Moreover, the popularity of attention mechanisms was additionally boosted after the introduction of the Transformer
model that further proved how effective attention can
be. Attention was originally introduced as an extension to
recurrent neural networks . However, the Transformer
model proposed in poses a major development in attention research as it demonstrates that the attention mechanism is sufﬁcient to build a state-of-the-art model. This
means that disadvantages, such as the fact that recurrent
neural networks are particularly difﬁcult to parallelize, can
G. Brauwers and F. Frasincar are with the Erasmus School of Economics,
Erasmus University Rotterdam, 3000 DR, Rotterdam, the Netherlands (email: {frasincar, brauwers}@ese.eur.nl).
Manuscript received July 6, 2020; revised June 21, 2021; Corresponding
author: F. Frasincar
be circumvented. As was the case for the introduction
of the original attention mechanism , the Transformer
model was created for machine translation, but was quickly
adopted to be used for other tasks, such as image processing
 , video processing , and recommender systems .
The purpose of this survey is to explain the general
form of attention, and provide a comprehensive overview
of attention techniques in deep learning. Other surveys have
already been published on the subject of attention models.
For example, in , a survey is presented on attention in
computer vision, provides an overview of attention in
graph models, and , , are all surveys on attention
in natural language processing. This paper partly builds
on the information presented in the previously mentioned
surveys. Yet, we provide our own signiﬁcant contributions.
The main difference between this survey and the previously
mentioned ones is that the other surveys generally focus
on attention models within a certain domain. This survey,
however, provides a cross-domain overview of attention
techniques. We discuss the attention techniques in a general
way, allowing them to be understood and applied in a
variety of domains. Furthermore, we found the taxonomies
presented in previous surveys to be lacking the depth and
structure needed to properly distinguish the various attention mechanisms. Additionally, certain signiﬁcant attention
techniques have not yet been properly discussed in previous surveys, while other presented attention mechanisms
seem to be lacking either technical details or intuitive explanations. Therefore, in this paper, we present important
attention techniques by means of a single framework using
a uniform notation, a combination of both technical and intuitive explanations for each presented attention technique,
and a comprehensive taxonomy of attention mechanisms.
The structure of this paper is as follows. Section 2 introduces a general attention model that provides the reader
with a basic understanding of the properties of attention
and how it can be applied. One of the main contributions
of this paper is the taxonomy of attention techniques presented in Section 3. In this section, attention mechanisms
are explained and categorized according to the presented
 
Feature Model
Query Model
Attention Model
Output Model
Fig. 1. An illustration of the general structure of the task model.
taxonomy. Section 4 provides an overview of performance
measures and methods for evaluating attention models.
Furthermore, the taxonomy is used to evaluate the structure
of various attention models. Lastly, in Section 5, we give our
conclusions and suggestions for further research.
GENERAL ATTENTION MODEL
This section presents a general form of attention with corresponding notation. The notation introduced here is based
on the notation that was introduced in and popularized
in . The framework presented in this section is used
throughout the rest of this paper.
To implement a general attention model, it is necessary
to ﬁrst describe the general characteristics of a model that
can employ attention. First of all, we will refer to the
complete model as the task model, of which the structure is
presented in Fig. 1. This model simply takes an input, carries
out the speciﬁed task, and produces the desired output. For
example, the task model can be a language model that takes
as input a piece of text, and produces as output a summary
of the contents, a classiﬁcation of the sentiment, or the text
translated word for word to another language. Alternatively,
the task model can take an image, and produce a caption
or segmentation for that image. The task model consists of
four submodels: the feature model, the query model, the
attention model, and the output model. In Subsection 2.1,
the feature model and query model are discussed, which
are used to prepare the input for the attention calculation.
In Subsection 2.2, the attention model and output model are
discussed, which are concerned with producing the output.
Attention Input
Suppose the task model takes as input the matrix X ∈
Rdx×nx, where dx represents the size of the input vectors
and nx represents the amount of input vectors. The columns
in this matrix can represent the words in a sentence,
the pixels in an image, the characteristics of an acoustic
sequence, or any other collection of inputs. The feature
model is then employed to extract the nf feature vectors
f1, . . . , fnf ∈Rdf from X, where df represents the size of
the feature vectors. The feature model can be a recurrent
neural network (RNN), a convolutional neural network
(CNN), a simple embedding layer, a linear transformation
of the original data, or no transformation at all. Essentially,
the feature model consists of all the steps that transform the
original input X into the feature vectors f1, . . . , fnf that the
attention model will attend to.
Fig. 2. The inner mechanisms of the general attention module.
To determine which vectors to attend to, the attention
model requires the query q ∈Rdq, where dq indicates the
size of the query vector. This query is extracted by the
query model, and is generally designed based on the type
of output that is desired of the model. A query tells the
attention model which feature vectors to attend to. It can be
interpreted literally as a query, or a question. For example,
for the task of image captioning, suppose that one uses a
decoder RNN model to produce the output caption based
on feature vectors obtained from the image by a CNN. At
each prediction step, the hidden state of the RNN model can
be used as a query to attend to the CNN feature vectors. In
each step, the query is a question in the sense that it asks for
the necessary information from the feature vectors based on
the current prediction context.
Attention Output
The feature vectors and query are used as input for the
attention model. This model consists of a single, or a
collection of general attention modules. An overview of a
general attention module is presented in Fig. 2. The input of
the general attention module is the query q ∈Rdq, and the
matrix of feature vectors F = [f1, . . . , fnf ] ∈Rdf ×nf . Two
separate matrices are extracted from the matrix F : the keys
matrix K = [k1, . . . , knf ] ∈Rdk×nf , and the values matrix
V = [v1, . . . , vnf ] ∈Rdv×nf , where dk and dv indicate,
respectively, the dimensions of the key vectors (columns
of K) and value vectors (columns of V ). The general way
of obtaining these matrices is through a linear transformation of F using the weight matrices WK ∈Rdk×df and
WV ∈Rdv×df , for K and V , respectively. The calculations
of K and V are presented in (1). Both weight matrices can
be learned during training or predeﬁned by the researcher.
For example, one can choose to deﬁne both WK and WV
as equal to the identity matrix to retain the original feature
vectors. Other ways of deﬁning the keys and the values are
also possible, such as using completely separate inputs for
the keys and values. The only constraint to be obeyed is that
the number of columns in K and V remains the same.
The goal of the attention module is to produce a
weighted average of the value vectors in V . The weights
used to produce this output are obtained via an attention
scoring and alignment step. The query q and the keys
matrix K are used to calculate the vector of attention scores
e = [e1, . . . , enf ] ∈Rnf . This is done via the score function
score(), as illustrated in (2).
1×1 = score( q
As discussed before, the query symbolizes a request for information. The attention score el represents how important
the information contained in the key vector kl is according
to the query. If the dimensions of the query and key vectors
are the same, an example of a score function would be to
take the dot-product of the vectors. The different types of
score functions are further discussed in Section 3.2.1.
Next, the attention scores are processed further through
an alignment layer. The attention scores can generally have
a wide range outside of . However, since the goal is to
produce a weighted average, the scores are redistributed via
an alignment function align() as deﬁned in (3).
1×1 = align( el
where al ∈R1 is the attention weight corresponding to
the lth value vector. One example of an alignment function
would be to use a softmax function, but the various other
alignment types are discussed in Section 3.2.2. The attention
weights provide a rather intuitive interpretation for the
attention module. Each weight is a direct indication of how
important each feature vector is relative to the others for
this particular problem. This can provide us with a more
in-depth understanding of the model behaviour, and the relations between inputs and outputs. The vector of attention
weights a = [a1, . . . , anf ] ∈Rnf is used to produce the
context vector c ∈Rdv by calculating a weighted average of
the columns of the values matrix V , as shown in (4).
As illustrated in Fig. 1, the context vector is then used in
the output model to create the output ˆy. This output model
translates the context vector into an output prediction. For
example, it could be a simple softmax layer that takes as
input the context vector c, as shown in (5).
dy×1 = softmax( Wc
where dy is the number of output choices or classes, and
Wc ∈Rdy×dv and bc ∈Rdy are trainable weights.
Attention Applications
Attention is a rather general mechanism that can be used in
a wide variety of problem domains. Consider the task of machine translation using an RNN model. Also, consider the
problem of image classiﬁcation using a basic CNN model.
While an RNN produces a sequence of hidden state vectors,
a CNN creates feature maps, where each region in the image
is represented by a feature vector. The RNN hidden states
are organized sequentially, while the CNN feature maps
are organized spatially. Yet, attention can still be applied
in both situations, since the attention mechanism does not
inherently depend on the organization of the feature vectors.
This characteristic makes attention easy to implement in a
wide variety of models in different domains.
Another domain where attention can be applied is audio
processing , . Acoustic sequences can be represented
by a sequence of feature vectors that relate to certain time
periods of the audio sample. These vectors could simply
be the raw input audio, or they can be extracted via, for
example, an RNN or CNN. Video processing is another
domain where attention can be applied intuitively , .
Video data consists of sequences of images, so attention can
be applied to the individual images, as well as the entire
sequence. Recommender systems often incorporate a user’s
interaction history to produce recommendations. Feature
vectors can be extracted based on, for example, the id’s
or other characteristics of the products the user interacted
with, and attention can be applied to them . Attention
can generally also be applied to many problems that use
a time series as input, be it medical , ﬁnancial , or
anything else, as long as feature vectors can be extracted.
The fact that attention does not rely on the organization
of the feature vectors allows it to be applied to various
problems that each use data with different structures, as
illustrated by the previous domain examples. Yet, this can be
taken even further by applying attention to data where there
is irregular structure. For example, protein structures, city
trafﬁc ﬂows, and communication networks cannot always
be represented using neatly structured organizations, such
as sequences, like time series, or grids, like images. In such
cases, the different aspects of the data are often represented
as nodes in a graph. These nodes can be represented by
feature vectors, meaning that attention can be applied in
domains that use graph-structured data as well , .
In general, attention can be applied to any problem for
which a set of feature vectors can be deﬁned or extracted.
As such, the general attention model presented in Fig. 2 is
applicable to a wide range of domains. The problem, however, is that there is a large variety of different applications
and extensions of the general attention module. As such,
in Section 3, a comprehensive overview is provided of a
collection of different attention mechanisms.
ATTENTION TAXONOMY
There are many different types of attention mechanisms
and extensions, and a model can use different combinations of these attention techniques. As such, we propose
a taxonomy that can be used to classify different types of
attention mechanisms. Fig. 3 provides a visual overview of
the different categories and subcategories that the attention
mechanisms can be organized in. The three major categories
are based on whether an attention technique is designed
to handle speciﬁc types of feature vectors (feature-related),
speciﬁc types of model queries (query-related), or whether
it is simply a general mechanism that is related to neither
the feature model, nor the query model (general). Further
explanations of these categories and their subcategories are
provided in the following subsections. Each mechanism
discussed in this section is either a modiﬁcation to the
existing inner mechanisms of the general attention module
presented in Section 2, or an extension of it.
The presented taxonomy can also be used to analyze
the architecture of attention models. Namely, the major
categories and their subcategories can be interpreted as
Attention Mechanisms
Feature-Related
Query-Related
Multiplicity
Representations
Multiplicity
Dimensionality
Singular Features Attention
Coarse-Grained Co-Attention
Fine-Grained Co-Attention
Multi-Grained Co-Attention
Rotatory Attention
Single-Level Attention
Attention-via-Attention
Hierarchical Attention
Single-Representational Attention
Multi-Representational Attention
Additive Scoring
Multiplicative Scoring
Scaled Multiplicative Scoring
General Scoring
Biased General Scoring
Activated General Scoring
Similarity Scoring
Global/Soft Alignment
Hard Alignment
Local Alignment
Reinforced Alignment
Single-Dimensional Attention
Multi-Dimensional Attention
Basic Queries
Specialized Queries
Self-Attentive Queries
Alternating Co-Attention
Interactive Co-Attention
Parallel Co-Attention
Singular Query Attention
Multi-Head Attention
Multi-Hop Attention
Capsule-Based Attention
Fig. 3. A taxonomy of attention mechanisms.
Description
Matrix of size df × nf containing the feature vectors
f1, . . . , fnf ∈Rdf as columns. These feature vectors
are extracted by the feature model.
Matrix of size dk × nf containing the key vectors
k1, . . . , knf ∈Rdk as columns. These vectors are used
to calculate the attention scores.
Matrix of size dv × nf containing the value vectors
v1, . . . , vnf ∈Rdv as columns. These vectors are used
to calculate the context vector.
Weights matrix of size dk × df used to create the K
matrix from the F matrix.
Weights matrix of size dv × df used to create the V
matrix from the F matrix.
Query vector of size dq. This vector essentially represents a question, and is used to calculate the attention
Context vector of size dv. This vector is the output of the
attention model.
Score vector of size dnf containing the attention scores
e1, . . . , enf ∈R1. These are used to calculate the attention weights.
Attention weights vector of size dnf containing the attention weights a1, . . . , anf ∈R1. These are the weights
used in the calculation of the context vector.
orthogonal dimensions of an attention model. An attention
model can consist of a combination of techniques taken
from any or all categories. Some characteristics, such as
the scoring and alignment functions, are generally required
for any attention model. Other mechanisms, such as multihead attention or co-attention are not necessary in every
situation. Lastly, in Table 1, an overview of used notation
with corresponding descriptions is provided.
Feature-Related Attention Mechanisms
Based on a particular set of input data, a feature model
extracts feature vectors so that the attention model can
attend to these various vectors. These features may have
speciﬁc structures that require special attention mechanisms
to handle them. These mechanisms can be categorized to
deal with one of the following feature characteristics: the
multiplicity of features, the levels of features, or the representations of features.
Multiplicity of Features
For most tasks, a model only processes a single input, such
as an image, a sentence, or an acoustic sequence. We refer
to such a mechanism as singular features attention. Other
models are designed to use attention based on multiple
inputs to allow one to introduce more information into the
model that can be exploited in various ways. However, this
does imply the presence of multiple feature matrices that
require special attention mechanisms to be fully used. For
example, introduces a concept named co-attention to
allow the proposed visual question answering (VQA) model
to jointly attend to both an image and a question.
Co-attention mechanisms can generally be split up
into two groups : coarse-grained co-attention and
ﬁne-grained co-attention. The difference between the two
groups is the way attention scores are calculated based on
the two feature matrices. Coarse-grained attention mechanisms use a compact representation of one feature matrix
as a query when attending to the other feature vectors.
Fine-grained co-attention, on the other hand, uses all feature
vectors of one input as queries. As such, no information is
lost, which is why these mechanisms are called ﬁne-grained.
As an example of coarse-grained co-attention, proposes an alternating co-attention mechanism that uses the
context vector (which is a compact representation) from one
attention module as the query for the other module, and
vice versa. Alternating co-attention is presented in Fig. 4.
Given a set of two input matrices X(1) and X(2), features
are extracted by a feature model to produce the feature
matrices F (1) ∈Rd(1)
and F (2) ∈Rd(2)
f , where d(1)
Fig. 4. An illustration of alternating co-attention.
represent, respectively, the dimension of the feature
vectors extracted from the ﬁrst and second inputs, while
represent, respectively, the amount of feature
vectors extracted from the ﬁrst and second inputs. In ,
co-attention is used for VQA, so the two input matrices are
the image data and the question data, for which the feature
model for the image consists of a CNN model, and the
feature model for the question consists of word embeddings,
a convolutional layer, a pooling layer, and an LSTM model.
Firstly, attention is calculated for the ﬁrst set of features F (1)
without the use of a query (Attention Module1 in Fig. 4). In
 , an adjusted additive attention score function is used for
this attention mechanism. The general form of the regular
additive score function can be seen in (6).
where act() is a non-linear activation function, and w ∈
Rdw, W1 ∈Rdw×dq, W2 ∈Rdw×dk, and b ∈Rdw are
trainable weights matrices, for which dw is a predeﬁned
dimension of the weight matrices. A variant of this score
function adapted to be calculated without a query for the
application at hand can be seen in (7).
× act(W (1)
where w(1) ∈Rdw, W (1) ∈Rdw×d(1)
k , and b(1) ∈Rdw are
trainable weight matrices for Attention Module1, k(1)
is the lth column of the keys matrix K(1) that was
obtained from F (1) via a linear transformation (see (1)), for
which dw is a prespeciﬁed dimension of the weight matrices
is a prespeciﬁed dimension of the key vectors.
Perhaps one may wonder why the query is absent when
calculating attention in this manner. Essentially, the query in
this attention model is learned alongside the other trainable
parameters. As such, the query can be interpreted as a
general question: ”Which feature vectors contain the most
important information?”. This is also known as a selfattentive mechanism, since attention is calculated based
only on the feature vectors themselves. Self-attention is
explained in more detail in Subsection 3.3.1.
The scores are combined with an alignment function
(see (3)), such as the softmax function, to create attention
weights used to calculate the context vector c(0) ∈Rd(1)
(see (4)). This context vector is not used as the output of
the attention model, but rather as a query for calculating
the context vector c(2) ∈Rd(2)
v , based on the second feature
matrix F (2), where d(2)
is the dimension of the value vectors
obtained from F (2) via a linear transformation (see (1)). For
Fig. 5. An illustration of interactive co-attention.
this module (Attention Module2 in Fig. 4), attention scores
are calculated using another score function with c0 as query
input, as presented in (8). Any function can be used in this
situation, but an additive function is used in .
= score( c(0)
These attention scores are then used to calculate attention
weights using, for example, a softmax function as alignment
function, after which the context vector c(2) can be derived
as a weighted average of the second set of value vectors.
Finally, the context vector c(2) is used as a query for the ﬁrst
attention module, which will produce the context vector
c(1) for the ﬁrst feature matrix F (1). Attention scores are
calculated according to (9). In , the same function and
weight matrices as seen in (7) are used, but with an added
query making it the same as the general additive score
function (see (6)). The rest of the attention calculation is
similar as before.
= score( c(2)
The produced context vectors c(1) and c(2) are concatenated
and used for prediction in the output model. Alternating
co-attention inherently contains a form of sequentiality due
to the fact that context vectors need to be calculated one
after another. This may come with a computational disadvantage since it is not possible to parallelize. Instead of
using a sequential mechanism like alternating co-attention,
 proposes the interactive co-attention mechanism that
can calculate attention on both feature matrices in parallel,
as depicted in Fig. 5. Instead of using the context vectors as
queries, unweighted averages of the key vectors are used as
queries. The calculation of the average keys are provided in
(10), and the calculation of the attention scores are shown
in (11). Any score function can be used in this case, but an
additive score function is used in .
= score( ¯k(2)
= score( ¯k(1)
From the attention scores, attention weights are created via
an alignment function, and are used to produce the context
vectors c(1) and c(2).
While coarse-grained co-attention mechanisms use a
compact representation of one input to use as a query when
calculating attention for another input, ﬁne-grained coattention considers every element of each input individually
when calculating attention scores. In this case, the query
becomes a matrix. An example of ﬁne-grained co-attention
is parallel co-attention . Similarly to interactive coattention, parallel co-attention calculates attention on the
two feature matrices at the same time, as shown in Fig. 6. We
start by evaluating the keys matrices K(1) ∈Rd(1)
K(2) ∈Rd(2)
that are obtained by linearly transforming
the feature matrices F (1) and F (2), where d(1)
prespeciﬁed dimensions of the keys. The idea is to use the
keys matrix from one input as the query for calculating
attention on the other input. However, since K(1) and K(2)
have completely different dimensions, an afﬁnity matrix
is calculated that is used to essentially
translate one keys matrix to the space of the other keys.
In , A is calculated as shown in (12).
= act( K(1)T
where WA ∈Rd(1)
is a trainable weights matrix and
act() is an activation function for which the tanh() function
is used in . proposes a different way of calculating
this matrix, i.e., one can use (13) to calculate each individual
element Ai,j of the matrix A.
× concat(k(1)
where wA ∈R3dk denotes a trainable vector of weights,
concat() denotes vector concatenation, and ◦denotes
element-wise multiplication, also known as the Hadamard
product. Note that the keys of each keys matrix in this
case must have the same dimension dk for the elementwise multiplication to work. The afﬁnity matrix can be
interpreted as a similarity matrix for the columns of the two
keys matrices, and helps translate, for example, image keys
to the same space as the keys of the words in a sentence,
and vice versa. The vectors of attention scores e(1) and e(2)
can be calculated using an altered version of the additive
score function as presented in (14) and (15). The previous
attention score examples in this survey all used a score function to calculate each attention score for each value vector
individually. However, (14) and (15) are used to calculate
the complete vector of all attention scores. Essentially, the
attention scores are calculated in an aggregated form.
W2 ∈Rdw×d(2)
are trainable weight matrices, for which
dw is a prespeciﬁed dimension of the weight matrices. Note
that tanh() is used in for the activation function, and the
feature matrices are used as the key matrices. In that case,
the afﬁnity matrix A can be seen as a translator between
feature spaces. As mentioned before, the afﬁnity matrix is
essentially a similarity matrix for the key vectors of the two
Fig. 6. An illustration of parallel co-attention.
inputs. In , this fact is used to propose a different way
of determining attention scores. Namely, one could take
the maximum similarity value in a row or column as the
attention score, as shown in (16).
j=1,...,n(2)
i=1,...,n(1)
Next, the attention scores are used to calculate attention
weights using an alignment function, so that two context
vectors c(1) and c(2) can be derived as weighted averages of
the value vectors that are obtained from linearly transforming the features. For the alignment function, proposes
to use a softmax function, and the value vectors are simply
set equal to the feature vectors. The resulting context vectors
can be either concatenated or added together.
Finally, coarse-grained and ﬁne-grained co-attention can
be combined to create an even more complex co-attention
mechanism. proposes the multi-grained co-attention
mechanism that calculates both coarse-grained and ﬁnegrained co-attention for two inputs. Each mechanism produces one context vector per input. The four resulting
context vectors are concatenated and used in the output
model for prediction.
A mechanism separate from co-attention that still uses
multiple inputs is the rotatory attention mechanism .
This technique is typically used in a text sentiment analysis
setting where there are three inputs involved: the phrase for
which the sentiment needs to be determined (target phrase),
the text before the target phrase (left context), and the text after the target phrase (right context). The words in these three
inputs are all encoded by the feature model, producing the
following feature matrices: F t = [f t
1, . . . , f t
F l = [f l
1, . . . , f l
f , and F r = [f r
1 , . . . , f r
f , for the target phrase words, left context words,
and right context words, respectively, where dt
f represent the dimensions of the feature vectors for the
corresponding inputs, and nt
f represent the
number of feature vectors for the corresponding inputs. The
feature model used in consists of word embeddings
and separate Bi-LSTM models for the target phrase, the left
context, and the right context. This means that the feature
vectors are in fact the hidden state vectors obtained from the
Bi-LSTM models. Using these features, the idea is to extract
a single vector r from the inputs such that a softmax layer
can be used for classiﬁcation. As such, we are now faced
with two challenges: how to represent the inputs as a single
vector, and how to incorporate the information from the left
and right context into that vector. proposes to use the
rotatory attention mechanism for this purpose.
Firstly, a single target phrase representation is created
by using a pooling layer that takes the average over the
columns of F t, as shown in (17).
rt is then used as a query to create a context vector out of
the left and right contexts, separately. For example, for the
left context, the key vectors kl
1, . . . , kl
k and value
vectors vl
1, . . . , vl
v are extracted from the left context
feature vectors f l
1, . . . , f l
f , similarly as before, where
v are the dimensions of the key and value vectors,
respectively. Note that proposes to use the original
feature vectors as keys and values, meaning that the linear
transformation consists of a multiplication by an identity
matrix. Next, the scores are calculated using (18).
1×1 = score( rt
For the score function, proposes to use an activated
general score function with a tanh activation function.
The attention scores can be combined with an alignment
function and the corresponding value vectors to produce
the context vector rl ∈Rdl
v. The alignment function used in
 takes the form of a softmax function. An analogous procedure can be performed to obtain the representation of the
right context, rr. These two context representations can then
be used to create new representations of the target phrase,
again, using attention. Firstly, the key vectors kt
1, . . . , kt
k and value vectors vt
1, . . . , vt
v are extracted
from the target phrase feature vectors f t
1, . . . , f t
similarly as before, using a linear transformation, where dt
v are the dimensions of the key and value vectors,
respectively. Note, again, that the original feature vectors as
keys and values in . The attention scores for the leftaware target representation are then calculated using (19).
= score( rl
The attention scores can be combined with an alignment
function and the corresponding value vectors to produce
the context vector rlt ∈Rdt
v. For this attention calculation,
 proposes to use the same score and alignment functions
as before. The right-aware target representation rrt can be
calculated in a similar manner. Finally, to obtain the full
representation vector r that is used to determine the classiﬁcation, the vectors rl, rr, rlt, and rrt are concatenated
together, as shown in (20).
v)×1 = concat( rl
To summarize, rotatory attention uses the target phrase
to compute new representations for the left and right context
using attention, and then uses these left and right representations to calculate new representations for the target
phrase. The ﬁrst step is designed to capture the words
in the left and right contexts that are most important to
the target phrase. The second step is there to capture the
most important information in the actual target phrase itself.
Essentially, the mechanism rotates attention between the
target and the contexts to improve the representations.
There are many applications where combining information from different inputs into a single model can be highly
beneﬁcial. For example, in the ﬁeld of medical data, there
are often many different types of data available, such as
various scans or documents, that can provide different types
of information. In , a co-attention mechanism is used
for automatic medical report generation to attend to both
images and semantic tags simultaneously. Similarly, in ,
a co-attention model is proposed that combines general demographics features and patient medical history features to
predict future health information. Additionally, an ablation
study is used in to show that the co-attention part of
the model speciﬁcally improves performance. A ﬁeld where
multi-feature attention has been extensively explored is the
domain of recommender systems. For example, in , a coattention network is proposed that attends to both product
reviews and the reviews a user has written. In , a model
is proposed for video recommendation that attends to both
user features and video features. Co-attention techniques
have also been used in combination with graph networks for
the purpose of, for example, reading comprehension across
multiple documents and fake news detection . In
comparison to co-attention, rotatory attention has typically
been explored only in the ﬁeld of sentiment analysis, which
is most likely due to the speciﬁc structure of the data that
is necessary to use this technique. An implementation of
rotatory attention is proposed in for sentiment analysis,
where the mechanism is extended by repeating the attention
rotation to iteratively further improve the representations.
Feature Levels
The previously discussed attention mechanisms process
data at a single level. We refer to these attention techniques
as single-level attention mechanisms. However, some data
types can be analyzed and represented on multiple levels.
For example, when analyzing documents, one can analyze
the document at the sentence level, word level, or even
the character level. When representations or embeddings
of all these levels are available, one can exploit the extra
levels of information. For example, one could choose to
perform translation based on either just the characters, or
just the words of the sentence. However, in , a technique
named attention-via-attention is introduced that allows one
to incorporate information from both the character, and the
word levels. The idea is to predict the sentence translation
character-by-character, while also incorporating information
from a word-level attention module.
To begin with, a feature model (consisting of, for example, word embeddings and RNNs) is used to encode
the input sentence into both a character-level feature matrix F (c) ∈Rd(c)
f , and a word-level feature matrix
F (w) ∈Rd(w)
, where d(c)
represent, respectively, the dimension of the embeddings of the characters,
and the number of characters, while d(w)
the same but at the word level. It is crucial for this method
that each level in the data can be represented or embedded.
When attempting to predict a character in the translated
Fig. 7. An illustration of attention-via-attention.
sentence, a query q(c) ∈Rdq is created by the query model
(like a character-level RNN), where dq is the dimension of
the query vectors. As illustrated in Fig. 7, the query is used
to calculate attention on the word-level feature vectors F (w).
This generates the context vector c(w) ∈Rd(w)
, where d(w)
represents the dimension of the value vectors for the wordlevel attention module. This context vector summarizes
which words contain the most important information for
predicting the next character. If we know which words are
most important, then it becomes easier to identify which
characters in the input sentence are most important. Thus,
the next step is to attend to the character-level features in
F (c), with an additional query input: the word-level context
vector c(w). The actual query input for the attention model
will therefore be the concatenation of the query q(c) and the
word context vector c(w). The output of this character-level
attention module is the context vector c(c). The complete
context output of the attention model is the concatenation
of the word-level, and character-level context vectors.
The attention-via-attention technique uses representations for each level. However, accurate representations may
not always be available for each level of the data, or it
may be desirable to let the model create the representations
during the process by building them from lower level representations. A technique referred to as hierarchical attention
 can be used in this situation. Hierarchical attention is
another technique that allows one to apply attention on
different levels of the data. Yet, the exact mechanisms work
quite differently compared to attention-via-attention. The
idea is to start at the lowest level, and then create representations, or summaries, of the next level using attention.
This process is repeated till the highest level is reached. To
make this a little clearer, suppose one attempts to create
a model for document classiﬁcation, similarly to the implementation from . We analyze a document containing
nS sentences, with the sth sentence containing ns words,
for s = 1, . . . , nS. One could use attention based on just
the collection of words to classify the document. However,
a signiﬁcant amount of important context is then left out
of the analysis, since the model will consider all words
as a single long sentence, and will therefore not consider
the context within the separate sentences. Instead, one can
use the hierarchical structure of a document (words form
sentences, and sentences form the document).
Fig. 8 illustrates the structure of hierarchical attention.
For each sentence in the document, a sentence representation c(s) ∈Rd(S)
is produced, for s = 1, . . . , nS, where d(S)
is the dimension of the value vectors used in the attention
model for sentence representations (Attention ModuleS in
Fig. 8. An illustration of hierarchical attention.
Fig. 8). The representation is a context vector from an
attention module that essentially summarizes the sentence.
Each sentence is ﬁrst put through a feature model to extract
the feature matrix F (s) ∈Rd(S)
×ns, for s = 1, . . . , nS, where
represents the dimension of the feature vector for each
word, and ns represents the amount of words in sentence s.
For extra clariﬁcation, the columns of F (s) are feature vectors that correspond to the words in sentence s. As shown in
Fig. 8, each feature matrix F (s) is used as input for an attention model, which produces the context vector c(s), for each
s = 1, . . . , nS. No queries are used in this step, so it can be
considered a self-attentive mechanism. The context vectors
are essentially summaries of the words in the sentences. The
matrix of context vectors C = [c(1), . . . , c(nS)] ∈Rd(S)
is constructed by grouping all the obtained context vectors
together as columns. Finally, attention is calculated using C
as feature input, producing the representation of the entire
document in the context vector c(D) ∈Rd(D)
, where d(D)
the dimension of the value vectors in the attention model
for document representation (Attention ModuleD in Fig. 8).
This context vector can be used to classify the document,
since it is essentially a summary of all the sentences (and
therefore also the words) in the document.
Multi-level models can be used in a variety of tasks.
For example, in , hierarchical attention is used in a
recommender system to model user preferences at the longterm level and the short-term level. Similarly, proposes
a hierarchical model for recommending social media images
based on user preferences. Hierarchical attention has also
been successfully applied in other domains. For example,
 proposes to use hierarchical attention in a video action
recognition model to capture motion information at the the
long-term level and the short-term level. Furthermore, 
proposes a hierarchical attention model for cross-domain
sentiment classiﬁcation. In , a hierarchical attention
model for chatbot response generation is proposed. Lastly,
using image data, proposes a hierarchical attention
model for crowd counting.
Feature Representations
In a basic attention model, a single embedding or representation model is used to produce feature representations for the model to attend to. This is referred to as
single-representational attention. Yet, one may also opt
to incorporate multiple representations into the model. In
 , it is argued that allowing a model access to multiple
embeddings can allow one to create even higher quality
representations. Similarly, incorporates multiple representations of the same book (textual, syntactic, semantic,
visual etc.) into the feature model. Feature representations
are an important part of the attention model, but attention
can also be an important part of the feature model. The
idea is to create a new representation by taking a weighted
average of multiple representations, where the weights are
determined via attention. This technique is referred to as
multi-representational attention, and allows one to create
so-called meta-embeddings. Suppose one wants to create a
meta-embedding for a word x for which E embeddings
x(e1), . . . , x(eE) are available. Each embedding x(ei) is of
size dei, for i = 1, . . . , E. Since not all embeddings are of the
same size, a transformation is performed to normalize the
embedding dimensions. Using embedding-speciﬁc weight
parameters, each embedding x(ei) is transformed into the
size-normalized embedding x(ti) ∈Rdt, where dt is the size
of every transformed word embedding, as shown in (21).
dt×1 = Wei
dei ×1 + bei
Rdt×dei , and bei
Rdt are trainable,
embedding-speciﬁc weights matrices. The ﬁnal embedding
x(e) ∈Rdt is a weighted average of the previously calculated transformed representations, as shown in (22).
1×1 × x(ti)
The ﬁnal representation x(e) can be interpreted as the
context vector from an attention model, meaning that the
weights a1, . . . , aE
∈R1 are attention weights. Attention can be calculated as normally, where the columns of
the features matrix F are the transformed representations
x(t1), . . . , x(tE). The query in this case can be ignored since
it is constant in all cases. Essentially, the query is “Which
representations are the most important?” in every situation.
As such, this is a self-attentive mechanism.
interesting
applications
multirepresentational attention are limited. One example of the
application of this technique is found in , where a multirepresentational attention mechanism has been applied to
generate multi-lingual meta-embeddings. Another example
is , where a multi-representational text classiﬁcation
model is proposed that incorporates different representations of the same text. For example, the proposed model uses
embeddings from part-of-speech tagging, named entity recognizers, and character-level and word-level embeddings.
General Attention Mechanisms
This major category consists of attention mechanisms that
can be applied in any type of attention model. The structure
of this component can be broken down into the following
sub-aspects: the attention score function, the attention alignment, and attention dimensionality.
Attention Scoring
The attention score function is a crucial component in how
attention is calculated. Various approaches have been developed that each have their own advantages and disadvantages. An overview of these functions is provided in Table 2.
Overview of score function (score(q, kl)) forms.
Parameters
(Concatenate) 
wT × act(W1 × q + W2 × kl) + b)
W1 ∈Rdw×dq
W2 ∈Rdw×dk
Multiplicative
(Dot-Product) 
Scaled Multiplicative 
General 
Biased General 
l × (W × q + b)
Activated General 
l × W × q + b)
W ∈Rdk×dq,
Similarity 
similarity(q, kl)
Each row of Table 2 presents a possible form for the function
score(q, kl), as seen in (23), where q is the query vector, and
kl is the lth column of K. Note that the score functions
presented in this section can be more efﬁciently calculated
in matrix form using K instead of each column separately.
Nevertheless, the score functions are presented using kl to
more clearly illustrate the relation between a key and query.
1×1 = score( q
Due to their simplicity, the most popular choices for the
score function are the concatenate score function and the
multiplicative score function . The multiplicative score
function has the advantage of being computationally inexpensive due to highly optimized vector operations. However, the multiplicative function may produce non-optimal
results when the dimension dk is too large . When dk
is large, the dot-product between q and kl can grow large
in magnitude. To illustrate this, in , an example is used
where the elements of q and kl are all normally distributed
with a mean equal to zero, and a variance equal to one.
Then, the dot-product of the vectors has a variance of dk.
A higher variance means a higher chance of numbers that
are large in magnitude. When the softmax function of the
alignment step is then applied using these large numbers,
the gradient will become very small, meaning the model
will have trouble converging . To adjust for this, 
proposes to scale the multiplicative function by the factor
√dk , producing the scaled multiplicative score function.
In , the multiplicative score function is extended by
introducing a weights matrix W . This form, referred to
as the general score function, allows for an extra transformation of kl. The biased general score function is
a further extension of the general function that introduces
a bias weight vector b. A ﬁnal extension on this function
named the activated general score function is introduced in
 , and includes the use of both a bias weight b, and an
activation function act().
The previously presented score functions are all based on
determining a type of similarity between the key vector and
the query vector. As such, more typical similarity measures,
such as the Euclidean (L2) distance and cosine similarity,
can also be implemented . These scoring methods are
summarized under the similarity score function which is
represented by the similarity() function.
There typically is no common usage across domains
regarding score functions. The choice of score function for
a particular task is most often based on empirical experiments. However, there are exceptions when, for example,
efﬁciency is vital. In models where this is the case, the multiplicative or scaled multiplicative score functions are typically the best choice. An example of this is the Transformer
model, which is generally computationally expensive.
Attention Alignment
The attention alignment is the step after the attention scoring. This alignment process directly determines which parts
of the input data the model will attend to. The alignment
function is denoted as align() and has various forms. The
align() function takes as input the previously calculated
attention score vector e and calculates for each element el of
e the attention weight al. These attention weights can then
be used to create the context vector c by taking a weighted
average of the value vectors v1, . . . , vnf :
The most popular alignment method to calculate these
weights is a simple softmax function, as depicted in (25).
1×1 = align( el
j=1 exp(ej).
This alignment method is often referred to as soft alignment
in computer vision settings , or global alignment for sequence data . Nevertheless, both these terms represent the
same function and can be interpreted similarly. Soft/global
alignment can be interpreted as the model attending to
all feature vectors. For example, the model attends to all
regions in an image, or all words in a sentence. Even though
the attention model generally does focus more on speciﬁc
parts of the input, every part of the input will receive at least
some amount of attention due to the nature of the softmax
function. Furthermore, an advantage of the softmax function
is that it introduces a probabilistic interpretation to the input
vectors. This allows one to easily analyze which parts of the
input are important to the output predictions.
In contrast to soft/global alignment, other methods aim
to achieve a more focused form of alignment. For example,
hard alignment , also known as hard attention or nondeterministic attention, is an alignment type that forces
the attention model to focus on exactly one feature vector.
Firstly, this method implements the softmax function in the
exact same way as global alignment. However, the outputs
a1, . . . , anf are not used as weights for the context vector
calculation. Instead, these values are used as probabilities
to draw the choice of the one value vector from. A value
m ∈R1 is drawn from a multinomial distribution with
a1, . . . , anf as parameters for the probabilities. Then, the
context vector is simply deﬁned as follows:
Hard alignment is typically more efﬁcient at inference
compared to soft alignment. On the other hand, the main
disadvantage of hard attention is that, due to the stochastic
alignment of attention, the training of the model cannot
be done via the regular backpropagation method. Instead,
simulation and sampling, or reinforcement learning 
are required to calculate the gradient at the hard attention
layer. As such, soft/global attention is generally preferred.
However, a compromise can be made in certain situations.
Local alignment is a method that implements a softmax
distribution, similarly to soft/global alignment. But, the
softmax distribution is calculated based only on a subset
of the inputs. This method is generally used in combination
with sequence data. One has to specify a variable p ∈R1
that determines the position of the region. Feature vectors
close to p will be attended to by the model, and vectors
too far from p will be ignored. The size of the subset
will be determined by the variable D ∈R1. Summarizing,
the attention model will apply a softmax function on the
attention scores in the subset [p −D, p + D]. In other words,
a window is placed on the input and soft/global attention
is calculated within that window:
= align( el
j=p−D exp(ej)
The question that remains is how to determine the location
parameter p. The ﬁrst method is referred to as monotonic
alignment. This straightforward method entails simply setting the location parameter equal to the location of the
prediction in the output sequence. Another method of determining the position of the region is referred to as predictive
alignment. As the name entails, the model attempts to
actually predict the location of interest in the sequence:
1×1 × sigmoid(wT
× tanh( Wp
where S ∈R1 is the length of the input sequence, and
wp ∈Rdp and Wp ∈Rdp×dq are both trainable weights
parameters. The sigmoid function multiplied by S makes
sure that p is in the range [0, S]. Additionally, in , it is
recommended to add an additional term to the alignment
function to favor alignment around p:
1×1 = align( el
nf ×1)exp(−(l −p)2)
where σ ∈R1 is empirically set equal to D
2 according to
 . Another proposed method for compromising between
soft and hard alignment is reinforced alignment . Similarly to local alignment, a subset of the feature vectors is
determined, for which soft alignment is calculated. However, instead of using a window to determine the subset,
reinforced alignment uses a reinforcement learning agent
 , similarly to hard alignment, to choose the subset of
feature vectors. The attention calculation based on these
chosen feature vectors is the same as regular soft alignment.
Soft alignment is often regarded as the standard alignment function for attention models in practically every domain. Yet, the other alignment methods have also seen interesting uses in various domains. For example, hard attention
is used in for the task of visual question answering.
In , both soft and hard attention are used in a graph
attention model for multi-agent game abstraction. Similarly,
in , both global and local alignment are used for review
rating predictions. Reinforced alignment has been employed
in combination with a co-attention structure in for the
task of aspect sentiment classiﬁcation. In , reinforced
alignment is used for the task of person re-identiﬁcation
using surveillance images.
Attention Dimensionality
All previous model speciﬁcations of attention use a scalar
weight al for each value vector vl. This technique is referred
to as single-dimensional attention. However, instead of
determining a single attention score and weight for the
entire vector, proposes to calculate weights for every
single feature in those vectors separately. This technique
is referred to as multi-dimensional attention, since the
attention weights now become higher dimensional vectors.
The idea is that the model no longer has to attend to entire
vectors, but it can instead pick and choose speciﬁc elements
from those vectors. More speciﬁcally, attention is calculated
for each dimension. As such, the model must create a vector
of attention weights al ∈Rdv for each value vector vl ∈Rdv.
The context vector can then be calculated by summing
the element-wise multiplications (◦) of the value vectors
v1, . . . , vnf ∈Rdv and the corresponding attention weight
vectors a1, . . . , anf ∈Rdv, as follows:
However, since one needs to create attention weight vectors,
this technique requires adjusted attention score and weight
calculations. For example, the concatenate score function
found in Table 2 can be adjusted by changing the w ∈Rdw
weights vector to the weight matrix Wd ∈Rdw×dv:
dv×1 = W T
dw×1). (31)
This new score function produces the attention score vectors
e1, . . . , enf ∈Rdv. These score vectors can be combined
into a matrix of scores e = [e1, . . . , enf ] ∈Rdv×nf . To
produce multi-dimensional attention weights, the alignment
function stays the same, but it is applied for each feature
across the attention score columns. To illustrate, when implementing soft attention, the attention weight produced
from the ith element of score vector el is deﬁned as follows:
1×1 = align(el,i
j=1 exp(ej,i),
where el,i represents the ith element of score vector el,
and al,i is the ith element of the attention weights vector
al. Finally, these attention weight vectors can be used to
compute the context vector as presented in (30).
Multi-dimensional attention is a very general mechanism that can be applied in practically every attention
model, but actual applications of the technique have been
relatively sparse. One application example is , where
multi-dimensional attention is used in a model for named
entity recognition based on text and visual context from
multimedia posts. In , multi-dimensional attention is
used in a model for answer selection in community question
answering. In , the U-net model for medical image segmentation is extended with a multi-dimensional attention
mechanism. Similarly, in , the Transformer model is
extended with the multi-dimensional attention mechanism
for the task of dialogue response generation. In , multidimensional attention is used to extend graph attention
networks for dialogue state tracking. Lastly, for the task
of next-item recommendation, proposes a model that
incorporates multi-dimensional attention.
Query-Related Attention Mechanisms
Queries are an important part of any attention model, since
they directly determine which information is extracted from
the feature vectors. These queries are based on the desired
output of the task model, and can be interpreted as literal
questions. Some queries have speciﬁc characteristics that
require speciﬁc types of mechanisms to process them. As
such, this category encapsulates the attention mechanisms
that deal with speciﬁc types of query characteristics. The
mechanisms in this category deal with one of the two
following query characteristics: the type of queries or the
multiplicity of queries.
Type of Queries
Different attention models employ attention for different
purposes, meaning that distinct query types are necessary.
There are basic queries, which are queries that are typically
straightforward to deﬁne based on the data and model. For
example, the hidden state for one prediction in an RNN
is often used as the query for the next prediction. One
could also use a vector of auxiliary variables as query. For
example, when doing medical image classiﬁcation, general
patient characteristics can be incorporated into a query.
Some attention mechanisms, such as co-attention, rotatory attention, and attention-over-attention, use specialized
queries. For example, rotatory attention uses the context
vector from another attention module as query, while interactive co-attention uses an averaged keys vector based
on another input. Another case one can consider is when
attention is calculated based purely on the feature vectors.
This concept has been mentioned before and is referred to
as self-attention or intra-attention . We say that the
models use self-attentive queries. There are two ways of interpreting such queries. Firstly, one can say that the query is
constant. For example, document classiﬁcation requires only
a single classiﬁcation as the output of the model. As such,
the query is always the same, namely: “What is the class
of the document?”. The query can be ignored and attention
can be calculated based only on the features themselves.
Score functions can be adjusted for this by making the query
vector a vector of constants or removing it entirely:
Additionally, one can also interpret self-attention as learning
the query along the way, meaning that the query can be
deﬁned as a trainable vector of weights. For example, the
dot-product score function may take the following form:
where q ∈Rdk is a trainable vector of weights. One could
also interpret vector b ∈Rdw as the query in (33). Another
use of self-attention is to uncover the relations between
the feature vectors f1, . . . , fnf . These relations can then
be used as additional information to incorporate into new
representations of the feature vectors. With basic attention
mechanisms, the keys matrix K, and the values matrix V
are extracted from the features matrix F , while the query
q is produced separately. For this type of self-attention, the
query vectors are extracted in a similar process as the keys
and values, via a transformation matrix of trainable weights
WQ ∈Rdq×df . We deﬁne the matrix Q = [q1, . . . , qnf ] ∈
Rdq×nf , which can be obtained as follows:
Each column of Q can be used as the query for the attention model. When attention is calculated using a query q, the
resulting context vector c will summarize the information
in the feature vectors that is important to the query. Since
the query, or a column of Q, is now also a feature vector
representation, the context vector contains the information
of all feature vectors that are important to that speciﬁc
feature vector. In other words, the context vectors capture
the relations between the feature vectors. For example, selfattention allows one to extract the relations between words:
which verbs refer to which nouns, which pronouns refer to
which nouns, etc. For images, self-attention can be used to
determine which image regions relate to each other.
While self-attention is placed in the query-related category, it is also very much related to the feature model.
Namely, self-attention is a technique that is often used in
the feature model to create improved representations of the
feature vectors. For example, the Transformer model for
language processing , and the Transformer model for
image processing , both use multiple rounds of (multihead) self-attention to improve the representation of the
feature vectors. The relations captured by the self-attention
mechanism are incorporated into new representations. A
simple method of determining such a new representation
is to simply set the feature vectors equal to the acquired
self-attention context vectors , as presented in (36).
where f (new) is the updated feature vector. Another possibility is to add the context vectors to the previous feature
vectors with an additional normalization layer :
df ×1 = Normalize(f (old)
where f (old) is the previous feature vector, and Normalize()
is a normalization layer . Using such techniques, selfattention has been used to create improved word or sentence
embeddings that enhance model accuracy .
Self-attention is arguably one of the more important
types of attention, partly due to its vital role in the highly
popular Transformer model. Self-attention is a very general
mechanism and can be applied to practically any problem.
As such, self-attention has been extensively explored in
many different ﬁelds in both Transformer-based architectures and other types of models. For example, in , selfattention is explored for image recognition tasks, and results
indicate that the technique may have substantial advantages
with regards to robustness and generalization. In , selfattention is used in a generative adversarial network (GAN)
 to determine which regions of the input image to focus
on when generating the regions of a new image. In , selfattention is used to design a state-of-the-art medical image
segmentation model. Naturally, self-attention can also be
used for video processing. In , a self-attention model
is proposed for the purpose of video summarization that
reaches state-of-the-art results. In other ﬁelds, like audio
processing, self-attention has been explored as well. In ,
self-attention is used to create a speech recognition model.
Self-attention has also been explored in overlapping domains. For example, in , the self-attention Transformer
architecture is used to create a model that can recognize
phrases from audio and by lip-reading from a video. For
the problem of next item recommendation, proposes
a Transformer model that explicitly captures item-item relations using self-attention. Self-attention also has applications in any natural language processing ﬁelds. For example,
in , self-attention is used for sentiment analysis. Selfattention is also highly popular for graph models. For
example, self-attention is explored in for the purpose
of representation learning in communication networks and
rating networks. Additionally, the ﬁrst attention model for
graph networks was based on self-attention .
Multiplicity of Queries
In previous examples, the attention model generally used
a single query for a prediction. We say that such models
use singular query attention. However, there are attention
architectures that allow the model to compute attention
using multiple queries. Note that this is different from, for
example, an RNN that may involve multiple queries to
produce a sequence of predictions. Namely, such a model
still requires only a single query per prediction.
One example of a technique that incorporates multiple
queries is multi-head attention , as presented in Fig.
9. Multi-head attention works by implementing multiple
attention modules in parallel by utilizing multiple different
versions of the same query. The idea is to linearly transform
the query q using different weight matrices. Each newly
formed query essentially asks for a different type of relevant
information, allowing the attention model to introduce more
information into the context vector calculation. An attention
model implements d ≥1 heads with each attention head
having its own query vector, keys matrix, and values matrix:
q(j), K(j) and V (j), for j = 1, . . . , d. The query q(j) is
obtained by linearly transforming the original query q,
while the matrices K(j) and V (j) are obtained through
linear transformations of F . As such, each attention head
has its own learnable weights matrices W (j)
for these transformations. The calculation of the query,
keys, and values for the jth head are deﬁned as follows:
dq×1 = W (j)
Thus, each head creates its own representations of the
query q, and the input matrix F . Each head can therefore
Fig. 9. An illustration of multi-head attention.
learn to focus on different parts of the inputs, allowing
the model to attend to more information. For example,
when training a machine translation model, one attention
head can learn to focus on which nouns (e.g., student, car,
apple) do certain verbs (e.g., walking, driving, buying) refer
to, while another attention head learns to focus on which
nouns refer to certain pronouns (e.g., he, she, it) . Each
head will also create its own vector of attention scores
e(j) = [e(j)
1 , . . . , e(j)
nf ] ∈Rnf , and a corresponding vector
of attention weights a(j) = [a(j)
1 , . . . , a(j)
nf ] ∈Rnf . As can
be expected, each attention model produces its own context
vector c(j) ∈Rdv, as follows:
1×1 × v(j)
The goal is still to create a single context vector as output
of the attention model. As such, the context vectors produced by the individual attention heads are concatenated
into a single vector. Afterwards, a linear transformation is
applied using the weight matrix WO ∈Rdc×dvd to make
sure the resulting context vector c ∈Rdc has the desired
dimension. This calculation is presented in (40). The dimension dc can be pre-speciﬁed by, for example, setting it equal
to dv, so that the context vector dimension is unchanged.
dc×dvd × concat(c(1)
dv×1, ..., c(d)
Multi-head attention processes multiple attention modules in parallel, but attention modules can also be implemented sequentially to iteratively adjust the context vectors. Each of these attention modules are referred to as
“repetitions” or “rounds” of attention. Such attention architectures are referred to as multi-hop attention models,
also known as multi-step attention models. An important note to consider is the fact that multi-hop attention
is a mechanism that has been proposed in various forms
throughout various works. While the mechanism always
involves multiple rounds of attention, the multi-hop implementation proposed in differs from the mechanism
proposed in or . Another interesting example is
 , where a “multi-hop” attention model is proposed that
would actually be considered alternating co-attention in this
survey, as explained in Subsection 3.1.1.
We present a general form of multi-hop attention that
is largely a generalization of the techniques introduced in
 and . Fig. 10 provides an example implementation
of a multi-hop attention mechanism. The general idea is
to iteratively transform the query, and use the query to
transform the context vector, such that the model can extract
different information in each step. Remember that a query
Fig. 10. An example illustration of multi-hop attention. Solid arrows
represent the base multi-hop model structure, while dotted arrows represent optional connections.
is similar to a literal question. As such, one can interpret
the transformed queries as asking the same question in a
different manner or from a different perspective, similarly
to the queries in multi-head attention. The query that was
previously denoted by q is now referred to as the initial
query, and is denoted by q(0). At hop s, the current query
q(s) is transformed into a new query representation q(s+1),
possibly using the current context vector c(s) as another
input, and some transformation function transform():
= transform(q(s)
speciﬁc form of
the transformation function
transform(), proposes to use a mechanism similar to
self-attention. Essentially, the queries used by the question
answer matching model proposed in were originally
based on a set of feature vectors extracted from a question.
 also deﬁnes the original query q(0) as the unweighted
average of these feature vectors. At each hop s, attention
can be calculated on these feature vectors using the previous
query q(s) as the query in this process. The resulting context
vector of this calculation is the next query vector. Using
the context vector c(s) instead of q(s) as the query for this
process is also a possibility, which is similar to the LCR-
Rot-hop model proposed in and the multi-step model
proposed in . Such a connection is represented by the
dotted arrows in Fig. 10. The transformation mechanism
uses either the q(s) or the context vector c(s) as query, but a
combination via concatenation is also possible.
Each query representation is used as input for the attention module to compute attention on the columns of the
feature matrix F , as seen previously. One main difference,
however, is that the context vector c(s) is also used as input,
so that the actual query input for the attention model is
the concatenation of c(s) and q(s+1). The adjusted attention
score function is presented in (42). Note that the initial
context vector c(0) is predeﬁned. One way of doing this is
by setting it equal to the unweighted average of the value
vectors v1, . . . , vnf ∈Rdv extracted from F .
1×1 = score(concat(q(s+1)
dq×1 , c(s)
An alignment function and the value vectors are then used
to produce the next context vector c(s+1). One must note
that in , the weights used in each iteration are the same
weights, meaning that the number of parameters do not
scale with the number of repetitions. Yet, using multiple
hops with different weight matrices can also be viable, as
shown by the Transformer model and in . It may be
difﬁcult to grasp why c(s) is part of the query input for the
attention model. Essentially, this technique is closely related
to self-attention in the sense that, in each iteration, a new
context representation is created from the feature vectors
and the context vector. The essence of this mechanism is
that one wants to iteratively alter the query and the context
vector, while attending to the feature vectors. In the process,
the new representations of the context vector absorb more
different kinds of information. This is also the main difference between this type of attention and multi-head attention. Multi-head attention creates multiple context vectors
from multiple queries and combines them to create a ﬁnal
context vector as output. Multi-hop attention iteratively
reﬁnes the context vector by incorporating information from
the different queries. This does have the disadvantage of
having to calculate attention sequentially.
Interestingly, due to the variations in which multi-hop
attention has been proposed, some consider the Transformer
model’s encoder and decoder to consist of several singlehop attention mechanisms instead of being a multihop model. However, in the context of this survey, we
consider the Transformer model to be an alternative form
of the multi-hop mechanism, as the features matrix F is not
directly reused in each step. Instead, F is only used as an
input for the ﬁrst hop, and is transformed via self-attention
into a new representation. The self-attention mechanism
uses each feature vector in F as a query, resulting in a matrix
of context vectors as output of each attention hop. The
intermediate context vectors are turned into matrices and
represent iterative transformations of the matrix F , which
are used in the consecutive steps. Thus, the Transformer
model iteratively reﬁnes the features matrix F by extracting
and incorporating new information.
When dealing with a classiﬁcation task, another idea is
to use a different query for each class. This is the basic
principle behind capsule-based attention , as inspired
by the capsule networks . Suppose we have the feature
vectors f1, . . . , fnf ∈Rdf , and suppose there are are dy
classes that the model can predict. Then, a capsule-based
attention model deﬁnes a capsule for each of the dy classes
that each take as input the feature vectors. Each capsule
consists of, in order, an attention module, a probability
module, and a reconstruction module, which are depicted in
Fig. 11. The attention modules all use self-attentive queries,
so each module learns its own query: ”Which feature vectors
are important to identify this class?”. In , a self-attentive
multiplicative score function is used for this purpose:
where ec,l ∈R1 is the attention score for vector l in capsule
c, and qc ∈Rdk is a trainable query for capsule c, for
c = 1, . . . , dy. Each attention module then uses an alignment
function, and uses the produced attention weights to determine a context vector cc ∈Rdv. Next, the context vector
cc is fed through a probability layer consisting of a linear
transformation with a sigmoid activation function:
= sigmoid(wT
Moduledy-1
Probability
Probability
Probability
Moduledy-1
Probability
Reconstruction
Reconstruction
Reconstruction
Moduledy-1
Reconstruction
Fig. 11. An illustration of capsule-based attention.
where wc ∈Rdv and bc ∈R1 are trainable capsule-speciﬁc
weights parameters, and pc ∈R1 is the predicted probability
that the correct class is class c. The ﬁnal layer is the reconstruction module that creates a class vector representation.
This representation rc ∈Rdv is determined by simply
multiplying the context vector cc by the probability pc:
The capsule representation is used when training the model.
First of all, the model is trained to predict the probabilities
p1, . . . , pdy as accurately as possible compared to the true
values. Secondly, via a joint loss function, the model is also
trained to accurately construct the capsule representations
r1, . . . , rdy. A features representation f ∈Rdf is deﬁned
which is simply the unweighted average of the original feature vectors. The idea is to train the model such that vector
representations from capsules that are not the correct class
differ signiﬁcantly from f while the representation from the
correct capsule is very similar to f. A dot-product between
the capsule representations and the features representation
is used in as a measure of the distance between the vectors. Note that dv must equal df in this case, otherwise the
vectors would have incompatible dimensions. Interestingly,
since attention is calculated for each class individually, one
can track which speciﬁc feature vectors are important for
which speciﬁc class. In , this idea is used to discover
which words correspond to which sentiment class.
The number of tasks that can make use of multiple
queries is substantial, due to how general the mechanisms
are. As such, the techniques described in this section have
been extensively explored in various domains. For example, multi-head attention has been used for speaker recognition based on audio spectrograms . In , multihead attention is used for recommendation of news articles. Additionally, multi-head attention can be beneﬁcial
for graph attention models as well . As for multi-hop
attention, quite a few papers have been mentioned before,
but there are still many other interesting examples. For
example, in , a multi-hop attention model is proposed
for medication recommendation. Furthermore, practically
every Transformer model makes use of both multi-head
and multi-hop attention. The Transformer model has been
extensively explored in various domains. For example, in
 , a Transformer model is implemented for image cap-
tioning. In , Transformers are explored for medical image segmentation. In , a Transformer model is used for
emotion recognition in text messages. A last example of
an application of Transformers is , which proposes a
Transformer model for recommender systems. In comparison with multi-head and multi-hop attention, capsule-based
attention is arguably the least popular of the mechanisms
discussed for the multiplicity of queries. One example is
 , where an attention-based capsule network is proposed
that also includes a multi-hop attention mechanism for the
purpose of visual question answering. Another example is
 , where capsule-based attention is used for aspect-level
sentiment analysis of restaurant reviews.
The multiplicity of queries is a particularly interesting
category due to the Transformer model , which combines a form of multi-hop and multi-head attention. Due
to the initial success of the Transformer model, many improvements and iterations of the model have been produced
that typically aim to improve the predictive performance,
the computational efﬁciency, or both. For example, the
Transformer-XL is an extension of the original Transformer that uses a recurrence mechanism to not be limited
by a context window when processing the outputs. This
allows the model to learn signiﬁcantly longer dependencies
while also being computationally more efﬁcient during the
evaluation phase. Another extension of the Transformer is
known as the Reformer model . This model is signiﬁcantly more efﬁcient computationally, by means of localitysensitive hashing, and memory-wise, by means of reversible
residual layers. Such computational improvements are vital,
since one of the main disadvantages of the Transformer
model is the sheer computational cost due to the complexity
of the model scaling quadratically with the amount of input
feature vectors. The Linformer model manages to
reduce the complexity of the model to scale linearly, while
achieving similar performance as the Transformer model.
This is achieved by approximating the attention weights
using a low-rank matrix. The Lite-Transformer model proposed in achieves similar results by implementing
two branches within the Transformer block that specialize
in capturing global and local information. Another interesting Transformer architecture is the Synthesizer .
This model replaces the pairwise self-attention mechanism
with “synthetic” attention weights. Interestingly, the performance of this model is relatively close to the original
Transformer, meaning that the necessity of the pairwise
self-attention mechanism of the Transformer model may
be questionable. For a more comprehensive overview of
Transformer architectures, we refer to .
EVALUATION OF ATTENTION MODELS
In this section, we present various types of evaluation for
attention models. Firstly, one can evaluate the structure of
attention models using the taxonomy presented in Section 3.
For such an analysis, we consider the attention mechanism
categories (see Fig. 3) as orthogonal dimensions of a model.
The structure of a model can be analyzed by determining
which mechanism a model uses for each category. Table 3
provides an overview of attention models found in the literature with a corresponding analysis based on the attention
mechanisms the models implement.
Secondly, we discuss various techniques for evaluating
the performance of attention models. The performance of attention models can be evaluated using extrinsic or intrinsic
performance measures, which are discussed in Subsections
4.1 and 4.2, respectively.
Extrinsic Evaluation
In general, the performance of an attention model is measured using extrinsic performance measures. For example,
performance measures typically used in the ﬁeld of natural
language processing are the BLEU , METEOR ,
and Perplexity metrics. In the ﬁeld of audio processing,
the Word Error Rate and Phoneme Error Rate are
generally employed. For general classiﬁcation tasks, error
rates, precision, and recall are generally used. For computer
vision tasks, the PSNR , SSIM , or IoU metrics
are used. Using these performance measures, an attention
model can either be compared to other state-of-the-art models, or an ablation study can be performed. If possible, the
importance of the attention mechanism can be tested by
replacing it with another mechanism and observing whether
the overall performance of the model decreases , .
An example of this is replacing the weighted average used
to produce the context vector with a simple unweighted
average and observing whether there is a decrease in overall model performance . This ablation method can be
used to evaluate whether the attention weights can actually
distinguish important from irrelevant information.
Intrinsic Evaluation
Attention models can also be evaluated using attentionspeciﬁc intrinsic performance measures. In , the attention weights are formally evaluated via the Alignment
Error Rate (AER) to measure the accuracy of the attention
weights with respect to annotated attention vectors. 
incorporates this idea into an attention model by supervising the attention mechanism using gold attention vectors.
A joint loss function consisting of the regular task-speciﬁc
loss and the attention weights loss function is constructed
for this purpose. The gold attention vectors are based on
annotated text data sets where keywords are hand-labelled.
However, since attention is inspired by human attention,
one could evaluate attention models by comparing them to
the attention behaviour of humans.
Evaluation via Human Attention
In , the concept of attention correctness is proposed,
which is a quantitative intrinsic performance metric that
evaluates the quality of the attention mechanism based on
actual human attention behaviour. Firstly, the calculation
of this metric requires data that includes the attention behaviour of a human. For example, a data set containing images with the corresponding regions that a human focuses
on when performing a certain task, such as image captioning. The collection of regions focused on by the human is
referred to as the ground truth region. Suppose an attention
model attends to the nf feature vectors f1, . . . , fnf ∈Rdf .
Feature vector fi corresponds to region Ri of the given
Attention models analyzed based on the proposed taxonomy. A plus sign (+) between two mechanisms indicates that both techniques were
combined in the same model, while a comma (,) indicates that both mechanisms were tested in the same paper, but not necessarily as a
combination in the same model.
Feature-Related
Query-Related
Multiplicity
Representations
Dimensionality
Multiplicity
Bahdanau et al. 
Single-Level
Single-Representational
Single-Dimensional
Luong et al. 
Single-Level
Single-Representational
Multiplicative,
Global, Local
Single-Dimensional
Xu et al. 
Single-Level
Single-Representational
Soft, Hard
Single-Dimensional
Lu et al. 
Co-attention
Hierarchical
Single-Representational
Single-Dimensional
Specialized
Yang et al. 
Hierarchical
Single-Representational
Single-Dimensional
Self-Attentive
Li et al. 
Hierarchical
Single-Representational
Single-Dimensional
Self-Attentive
Vaswani et al. 
Single-Level
Single-Representational
Multiplicative
Single-Dimensional
Self-Attentive +
Multi-Head +
Wallaart and Frasincar 
Single-Level
Single-Representational
Single-Dimensional
Specialized
Kiela et al. 
Single-Level
Multi-Representational
Single-Dimensional
Self-Attentive
Shen et al. 
Single-Level
Single-Representational
Multi-Dimensional
Self-Attentive
Zhang et al. 
Single-Level
Single-Representational
Multiplicative
Single-Dimensional
Self-Attentive
Li et al. 
Co-attention
Single-Level
Single-Representational
Multiplicative
Single-Dimensional
Self-Attentive +
Specialized
Yu et al. 
Co-attention
Single-Level
Single-Representational
Multiplicative
Single-Dimensional
Self-Attentive +
Specialized
Multi-Head
Wang et al. 
Co-attention
Single-Level
Single-Representational
Reinforced
Single-Dimensional
Specialized
Oktay et al. 
Single-Level
Single-Representational
Multi-Dimensional
Self-Attentive +
Specialized
Winata et al. 
Single-Level
Multi-Representational
Single-Dimensional
Self-Attentive
Multi-Head
Wang et al. 
Single-Level
Single-Representational
Multiplicative
Single-Dimensional
Self-Attentive
Capsule-Based
image, for i = 1, . . . , nf. We deﬁne the set G as the set
of regions that belong to the ground truth region, such that
Ri ∈G if Ri is part of the ground truth region. The attention
model calculates the attention weights a1, . . . , anf ∈R1 via
the usual attention process. The Attention Correctness (AC)
metric can then be calculated using (46).
Thus, this metric is equal to the sum of the attention weights
for the ground truth regions. Since the attention weights
sum up to 1 due to, for example, a softmax alignment
function, the AC value will be a value between 0 and 1.
If the model attends to only the ground truth regions, then
AC is equal to 1, and if the attention model does not attend
to any of the ground truth regions, AC will be equal to 0.
In , a rank correlation metric is used to compare
the generated attention weights to the attention behaviour
of humans. The conclusion of this work is that attention
maps generated by standard attention models generally do
not correspond to human attention. Attention models often
focus on much larger regions or multiple small non-adjacent
regions. As such, a technique to improve attention models is
to allow the model to learn from human attention patterns
via a joint loss of the regular loss function and an attention
weight loss function based on the human gaze behaviour,
similarly to how annotated attention vectors are used in
 to supervise the attention mechanism. proposes
to use human attention data to supervise the attention
mechanism in such a manner. Similarly, a state-of-the-art
video captioning model is proposed in that learns from
human gaze data to improve the attention mechanism.
Manual Evaluation
A method that is often used to evaluate attention models is
the manual inspection of attention weights. As previously
mentioned, the attention weights are a direct indication of
which parts of the data the attention model ﬁnds most
important. Therefore, observing which parts of the inputs
the model focuses on can be helpful in determining if the
model is behaving correctly. This allows for some interpretation of the behaviour of models that are typically known
to be black boxes. However, rather than checking if the
model focuses on the most important parts of the data, some
use the attention weights to determine which parts of the
data are most important. This would imply that attention
models provide a type of explanation, which is a subject
of contention among researchers. Particularly, in , extensive experiments are conducted for various natural language processing tasks to investigate the relation between
attention weights and important information to determine
whether attention can actually provide meaningful explanations. In this paper titled “Attention is not Explanation”,
it is found that attention weights do not tend to correlate
with important features. Additionally, the authors are able
to replace the produced attention weights with completely
different values while keeping the model output the same.
These so-called “adversarial” attention distributions show
that an attention model may focus on completely different
information and still come to the same conclusions, which
makes interpretation difﬁcult. Yet, in another paper titled
“Attention is not not Explanation” , the claim that
attention is not explanation is questioned by challenging
the assumptions of the previous work. It is found that
the adversarial attention distributions do not perform as
reliably well as the learned attention weights, indicating that
it was not proved that attention is not viable for explanation.
In general, the conclusion regarding the interpretability
of attention models is that researchers must be extremely
careful when drawing conclusions based on attention patterns. For example, problems with an attention model can be
diagnosed via the attention weights if the model is found to
focus on the incorrect parts of the data, if such information
is available. Yet, conversely, attention weights may only be
used to obtain plausible explanations for why certain parts
of the data are focused on, rather than concluding that those
parts are signiﬁcant to the problem . However, one
should still be cautious as the viability of such approaches
can depend on the model architecture .
CONCLUSION
In this survey, we have provided an overview of recent
research on attention models in deep learning. Attention
mechanisms have been a prominent development for deep
learning models as they have shown to improve model performance signiﬁcantly, producing state-of-the-art results for
various tasks in several ﬁelds of research. We have presented
a comprehensive taxonomy that can be used to categorize
and explain the diverse number of attention mechanisms
proposed in the literature. The organization of the taxonomy
was motivated based on the structure of a task model that
consists of a feature model, an attention model, a query
model, and an output model. Furthermore, the attention
mechanisms have been discussed using a framework based
on queries, keys, and values. Last, we have shown how
one can use extrinsic and intrinsic measures to evaluate the
performance of attention models, and how one can use the
taxonomy to analyze the structure of attention models.
The attention mechanism is typically relatively simple
to understand and implement and can lead to signiﬁcant
improvements in performance. As such, it is no surprise that
this is a highly active ﬁeld of research with new attention
mechanisms and models being developed constantly. Not
only are new mechanisms consistently being developed,
but there is also still ample opportunity for the exploration
of existing mechanisms for new tasks. For example, multidimensional attention is a technique that shows promising results and is general enough to be implemented in
almost any attention model. However, it has not seen much
application in current works. Similarly, multi-head attention is a technique that can be efﬁciently parallelized
and implemented in practically any attention model. Yet,
it is mostly seen only in Transformer-based architectures.
Lastly, similarly to how combines rotatory attention
with multi-hop attention, combining multi-dimensional attention, multi-head attention, capsule-based attention, or
any of the other mechanisms presented in this survey may
produce new state-of-the-art results for the various ﬁelds of
research mentioned in this survey.
This survey has mainly focused on attention mechanisms for supervised models, since these comprise the
largest proportion of the attention models in the literature.
In comparison to the total amount of research that has been
done on attention models, research on attention models
for semi-supervised learning , or unsupervised
learning , has received limited attention and has
only become active recently. Attention may play a more
signiﬁcant role for such tasks in the future as obtaining
large amounts of labeled data is a difﬁcult task. Yet, as
larger and more detailed data sets become available, the
research on attention models can advance even further. For
example, we mentioned the fact that attention weights can
be trained directly based on hand-annotated data or
actual human attention behaviour , . As new data
sets are released, future research may focus on developing
attention models that can incorporate those types of data.
While attention is intuitively easy to understand, there
still is a substantial lack of theoretical support for attention.
As such, we expect more theoretical studies to additionally
contribute to the understanding of the attention mechanisms in complex deep learning systems. Nevertheless, the
practical advantages of attention models are clear. Since
attention models provide signiﬁcant performance improvements in a variety of ﬁelds, and as there are ample opportunities for more advancements, we foresee that these models
will still receive signiﬁcant attention in the time to come.