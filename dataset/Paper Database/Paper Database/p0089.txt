Fine-Grained Attention Mechanism for Neural
Machine Translation
Heeyoul Choi
Handong Global University
 
Kyunghyun Cho
New York University
 
Yoshua Bengio
University of Montreal
CIFAR Senior Fellow
 
Neural machine translation (NMT) has been a new paradigm in machine translation, and the attention mechanism has become the dominant approach with the
state-of-the-art records in many language pairs. While there are variants of the attention mechanism, all of them use only temporal attention where one scalar value
is assigned to one context vector corresponding to a source word. In this paper,
we propose a ﬁne-grained (or 2D) attention mechanism where each dimension of
a context vector will receive a separate attention score. In experiments with the
task of En-De and En-Fi translation, the ﬁne-grained attention method improves
the translation quality in terms of BLEU score. In addition, our alignment analysis
reveals how the ﬁne-grained attention mechanism exploits the internal structure of
context vectors.
Introduction
Neural machine translation (NMT), which is an end-to-end approach to machine translation Kalchbrenner and Blunsom ; Sutskever et al. ; Bahdanau et al. , has widely become
adopted in machine translation research, as evidenced by its success in a recent WMT’16 translation task Sennrich et al. ; Chung et al. . The attention-based approach, proposed by
Bahdanau et al. , has become the dominant approach among others, which has resulted in
state-of-the-art translation qualities on, for instance, En-Fr Jean et al. , En-De Jean et al.
 ; Sennrich et al. , En-Zh Shen et al. , En-Ru Chung et al. and En-
Cz Chung et al. ; Luong and Manning . These recent successes are largely due to
better handling a large target vocabulary Jean et al. ; Sennrich et al. ; Chung et al.
 ; Luong and Manning , incorporating a target-side monolingual corpus Sennrich et al.
 ; Gulcehre et al. and advancing the attention mechanism Luong et al. ; Cohn
et al. ; Tu et al. .
We notice that all the variants of the attention mechanism, including the original one by Bahdanau
et al. , are temporal in that it assigns a scalar attention score for each context vector, which
corresponds to a source symbol. In other words, all the dimensions of a context vector are treated
equally. This is true not only for machine translation, but also for other tasks on which the attentionbased task was evaluated. For instance, the attention-based neural caption generation by Xu et al.
 assigns a scalar attention score for each context vector, which corresponds to a spatial location in an input image, treating all the dimensions of the context vector equally. See Cho et al.
 for more of such examples.
 
On the other hand, in Choi et al. , it was shown that word embedding vectors have more than
one notions of similarities by analyzing the local chart of the manifold that word embedding vectors
reside. Also, by contextualization of word embedding, each dimension of the word embedding
vectors could play different role according to the context, which, in turn, led to better translation
qualities in terms of the BLEU scores.
Inspired by the contextualization of word embedding, in this paper, we propose to extend the attention mechanism so that each dimension of a context vector will receive a separate attention score.
This enables ﬁner-grained attention, meaning that the attention mechanism may choose to focus
on one of many possible interpretations of a single word encoded in the high-dimensional context
vector Choi et al. ; Van der Maaten and Hinton . This is done by letting the attention mechanism output as many scores as there are dimensions in a context vectors, contrary to the
existing variants of attention mechanism which returns a single scalar per context vector.
We evaluate and compare the proposed ﬁne-grained attention mechanism on the tasks of En-De and
En-Fi translation. The experiments reveal that the ﬁne-grained attention mechanism improves the
translation quality up to +1.4 BLEU. Our qualitative analysis found that the ﬁne-grained attention
mechanism indeed exploits the internal structure of each context vector.
Background: Attention-based Neural Machine Translation
The attention-based neural machine translation (NMT) from Bahdanau et al. computes a
conditional distribution over translations given a source sentence X = (wx
2, . . . , wx
2, . . . , wy
This is done by a neural network that consists of an encoder, a decoder and the attention mechanism.
The encoder is often implemented as a bidirectional recurrent neural network (RNN) that reads the
source sentence word-by-word. Before being read by the encoder, each source word wx
t is projected
onto a continuous vector space:
xt = Ex[·, wx
where Ex[·, wx
t -th column vector of Ex ∈RE×|V |, a source word embedding matrix, where
E and |V | are the word embedding dimension and the vocabulary size, respectively.
The resulting sequence of word embedding vectors is then read by the bidirectional encoder recurrent
network which consists of forward and reverse recurrent networks. The forward recurrent network
reads the sequence in the left-to-right order while the reverse network reads it right-to-left:
h t = −→φ (−→
h t−1, xt),
h t = ←−φ (←−
h t+1, xt),
where the initial hidden states −→
h 0 and ←−
h T +1 are initialized as all-zero vectors or trained as parameters. The hidden states from the forward and reverse recurrent networks are concatenated at each
time step t to form an annotation vector h:
This concatenation results in a context C that is a tuple of annotation vectors:
C = {h1, h2, . . . , hT } .
The recurrent activation functions −→φ and ←−φ are in most cases either long short-term memory units
 ) or gated recurrent units ).
The decoder consists of a recurrent network and the attention mechanism. The recurrent network is
a unidirectional language model to compute the conditional distribution over the next target word
given all the previous target words and the source sentence:
By multiplying this conditional probability for all the words in the target, we recover the distribution
over the full target translation in Eq. (1).
The decoder recurrent network maintains an internal hidden state zt′. At each time step t′, it ﬁrst
uses the attention mechanism to select, or weight, the annotation vectors in the context tuple C.
The attention mechanism, which is a feedforward neural network, takes as input both the previous
decoder hidden state, and one of the annotation vectors, and returns a relevant score et′,t:
et′,t = fAtt(zt′−1, ht),
which is referred to as score function Luong et al. ; Chung et al. . The function fAtt
can be implemented by fully connected neural networks with a single hidden layer where tanh()
can be applied as activation function. These relevance scores are normalized to be positive and sum
exp(et′,t)
k=1 exp(et′,k)
We use the normalized scores to compute the weighted sum of the annotation vectors
which will be used by the decoder recurrent network to update its own hidden state by
zt′ = φz(zt′−1, yt′−1, ct′).
Similarly to the encoder, φz is implemented as either an LSTM or GRU. yt′−1 is a target-side word
embedding vector obtained by
yt′−1 = Ey[·, wy
similarly to Eq. (2).
The probability of each word i in the target vocabulary V ′ is computed by
<t′, X) = φ (W y
i zt′ + ci) ,
i is the i-th row vector of W y ∈R|V |×dim(zt′) and ci is the bias.
The NMT model is usually trained to maximize the log-probability of the correct translation given
a source sentence using a large training parallel corpus. This is done by stochastic gradient descent,
where the gradient of the log-likelihood is efﬁciently computed by the backpropagation algorithm.
Variants of Attention Mechanism
Since the original attention mechanism was proposed as in Eq. (3) Bahdanau et al. , there
have been several variants Luong et al. .
Luong et al. presented a few variants of the attention mechanism on the sequence-to-sequence
model Sutskever et al. . Although their work cannot be directly compared to the attention
model in Bahdanau et al. , they introduced a few variants for score function of attention
model–content based and location based score functions. Their score functions still assign a single
value for the context vector ht as in Eq. (3).
Another variant is to add the target word embedding as input for the score function Jean et al.
 ; Chung et al. as follows:
et′,t = fAttY(zt′−1, ht, yt′−1),
and the score is normalized as before, which leads to αt′,t, and fAttY can be a fully connected neural
network as Eq. (3) with different input size. This method provides the score function additional
information from the previous word. In training, teacher forced true target words can be used, while
in test the previously generated word is used. In this variant, still a single score value is given to the
context vector ht.
Figure 1: (a) The conventional attention mechanism and (b) The proposed ﬁne-grained attention
mechanism. Note that P
t αt′,t = 1 in the conventional method, and P
t′,t = 1 for all dimension
d in the proposed method.
Fine-Grained Attention Mechanism
All the existing variants of attention mechanism assign a single scalar score for each context vector
ht. We however notice that it is not necessary to assign a single score to the context at a time, and
that it may be beneﬁcial to assign a score for each dimension of the context vector, as each dimension
represents a different perspective into the captured internal structure. In Choi et al. , it was
shown that each dimension in word embedding could have different meaning and the context could
enrich the meaning of each dimension in different ways. The insight in this paper is similar to
Choi et al. , except two points: (1) focusing on the encoded representation rather than word
embedding, and (2) using 2 dimensional attention rather than the context of the given sentence.
We therefore propose to extend the score function fAtt in Eq. (3) to return a set of scores corresponding to the dimensions of the context vector ht. That is,
t′,t = f d
AttY2D(zt′−1, ht, yt′−1),
t′,t is the score assigned to the d-th dimension of the t-th context vector ht at time t′. Here,
fAttY2D is a fully connected neural network where the number of output node is d. These dimensionspeciﬁc scores are further normalized dimension-wise such that
k=1 exp(ed
The context vectors are then combined by
αt′,t ⊙ht,
where αt′,t is
t′,t, . . . , αdim(ht)
, and ⊙an element-wise multiplication.
We contrast the conventional attention mechanism against the proposed ﬁne-grained attention mechanism in Fig. 1.
Experimental Settings
Tasks and Corpora
We evaluate the proposed ﬁne-grained attention mechanism on two translation tasks; (1) En-De and
(2) En-Fi. For each language pair, we use all the parallel corpora available from WMT’151 for
training, which results in 4.5M and 2M sentence pairs for En-De and En-Fi, respectively. In the
case of En-De, we preprocessed the parallel corpora following Jean et al. and ended up with
100M words on the English side. For En-Fi, we did not use any preprocessing routine other than
simple tokenization.
Instead of space-separated tokens, we use 30k subwords extracted by byte pair encoding (BPE),
as suggested in Sennrich et al. . When computing the translation quality using BLEU, we
un-BPE the resulting translations, but leave them tokenized.
Decoding and Evaluation
Once a model is trained, we use a simple forward beam search with width set to 12 to ﬁnd a translation that approximately maximizes log p(Y |X) from Eq. (1). The decoded translation is then
un-BPE’d and evaluated against a reference sentence by BLEU (in practice, BLEU is computed
over a set of sentences.) We use newstest2013 and newstest2015 as the validation and test sets for
En-De, and newsdev2015 and newstest2015 for En-Fi.
We use the attention-based neural translation model from Bahdanau et al. as a baseline,
except for replacing the gated recurrent unit (GRU) with the long short-term memory unit (LSTM).
The vocabulary size is 30K for both source and target languages, the dimension of word embedding
is 620 for both languages, the number of hidden nodes for both encoder and decoder is 1K, and the
dimension of hidden nodes for the alignment model is 2K.
Based on the above model conﬁguration, we test a variant of this baseline model, in which we feed
the previously decoded symbol yt−1 directly to the attention score function fAtt from Eq. (3) (AttY).
These models are compared against the model with the proposed ﬁne-grained model (AttY2D).
We further test adding a recently proposed technique, which treats each dimension of word embedding differently based on the context. This looks similar to our ﬁne-grained attention in a sense that
each dimension of the representation is treated in different ways. We evaluate the contextualization (Context) proposed by Choi et al. . The contextualization enriches the word embedding
vector by incorporating the context information:
where NNθ is a feedforward neural network parametrized by θ. We closely follow Choi et al. .
All the models were trained using Adam Kingma and Ba until the BLEU score on the validation set stopped improving. For computing the validation score during training, we use greedy
search instead of beam search in order to minimize the computational overhead. That is 1 for the
beam search. As in Bahdanau et al. , we trained our model with the sentences of length up to
Experiments
Quantitative Analysis
We present the translation qualities of all the models on both En-De and En-Fi in Table 1. We
observe up to +1.4 BLEU when the proposed ﬁne-grained attention mechanism is used instead of
1 
Beam Width
17.57 (17.62)
20.78 (19.72)
6.07 (7.18)
7.83 (8.35)
19.15 (18.82)
21.41 (20.60)
7.38 (8.02)
8.91 (9.20)
20.49 (19.42)
22.50 (20.83)
8.33 (8.75)
9.32 (9.41)
+Context(C)
19.13 (18.81)
22.13 (21.01)
7.47 (7.93)
8.84 (9.18)
20.96 (20.06)
23.25 (21.35)
8.67 (9.18)
10.01 (9.95)
22.37 (20.56)
23.74 (22.13)
9.02 (9.63)
10.20 (10.90)
Table 1: BLEU scores on the test sets for En-De and En-Fi with two different beam widths. The
scores on the development sets are in the parentheses. The baseline is the vanilla NMT model from
Bahdanau et al. with LSTM and BPE.
the conventional attention mechanism (Baseline vs Baseline+AttY vs Baseline+AttY2D) on the
both language pairs. These results clearly conﬁrm the importance of treating each dimension of the
context vector separately.
With the contextualization (+Context or +C in the table), we observe the same pattern of improvements by the proposed method. Although the contextualization alone improves BLEU by up to +1.8
compared to the baseline, the ﬁne-grained attention boost up the BLEU score by additional +1.4.
The improvements in accuracy require additional time as well as larger model size. The model size
increases 3.5% relatively from +AttY to +AttY2D, and 3.4% from +C+AttY to +C+AttY2D. The
translation times are summarized in Table. 2, which shows the proposed model needs extra time
(from 4.5% to 14% relatively).
Baseline+AttY
Baseline+AttY2D
2,902 (+14.0%)
1,786 (+9.5%)
Baseline+C+AttY
Baseline+C+AttY2D
2,894 (+4.5%)
1,718 (+5.7%)
Table 2: Elapsed time (in seconds) for translation of test ﬁles. The test ﬁle ‘newstest2015’ for
En-De has 2,169 sentences and ‘newstest2015’ for En-Fi has 1,370 sentences. The numbers in the
parenthesis indicate the additional times for AttY2D compared to the corresponding AttY models.
Alignment Analysis
Unlike the conventional attention mechanism, the proposed ﬁne-grained one returns a 3–D tensor
t′,t representing the relationship between the triplet of a source symbol xt, a target symbol yt′ and
a dimension of the corresponding context vector cd
t . This makes it challenging to visualize the result
of the ﬁne-grained attention mechanism, especially because the dimensionality of the context vector
is often larger 
Instead, we ﬁrst visualize the alignment averaged over the dimensions of a context vector:
This computes the strength of alignment between source and target symbols, and should be comparable to the alignment matrix from the conventional attention mechanism.
In Fig. 2, we visualize the alignment found by (left) the original model from Bahdanau et al. ,
(middle) the modiﬁcation in which the previously decoded target symbol is fed directly to the conventional attention mechanism (AttY), and (right) the averaged alignment At,t′ from the proposed
ﬁne-grained attention mechanism. There is a clear similarity among these three alternatives, but we
observe a more clear, focused alignment in the case of the proposed ﬁne-grained attention model.
Figure 2: Attention assignments with different attention models in the En-De translation: (a) the
vanilla attention model (Att), (b) with target words yt′−1 (AttY), and (c) the proposed attention
model (AttY2D).
Second, we visualize the alignment averaged over the target:
This matrix is expected to reveal the dimensions of a context vector per source symbol that are
relevant for translating it without necessarily specifying the aligned target symbol(s).
Figure 3: Attention assignments with the ﬁne-grained attention model. Due to the limit of the space,
only the ﬁrst 50 dimensions are presented. The vertical and the horizontal axes indicate the source
sub-words and the 50 dimensions of the context vector ht, respectively.
In Fig. 3, we can see very sparse representation where each source word receives different pattern
of attentions on different dimensions.
We can further inspect the alignment tensor αd
t′,t by visualizing the d′-th slice of the tensor. Fig. 4
shows 6 example dimensions, where different dimensions focus on different perspective of translation. Some dimensions represent syntactic information, while others do semantic one. Also, syntactic information is handled in different dimensions, according to the word type, like article (‘a’ and
‘the’), preposition (‘to’ and ‘of’), noun (‘strategy’, ‘election’ and ‘Obama’), and adjective (‘Republican’ and ‘re-@@’). As semantic information, Fig. 4(f) shows a strong pattern of attention on the
words ‘Republican’, ’strategy’, ‘election’ and ‘Obama’, which seem to mean ‘politics’. Although
we present one example of attention matrix, we observed the same patterns with other examples.
Conclusions
In this paper, we proposed a ﬁne-grained (or 2D) attention mechanism for neural machine translation. The experiments on En-De and En-Fi show that the proposed attention method improves the
translation quality signiﬁcantly. When the method was applied with the previous technique, contextualization, which was based on the similar idea, the performance was further improved. With
alignment analysis, the ﬁne-grained attention method revealed that the different dimensions of context play different roles in neural machine translation.
Figure 4: Attention assignments in examplary dimensions with the ﬁne-grained attention model:
attentions are focused on (a) article (‘a’ and ‘the’), (b) preposition (‘to’ and ‘of’), (c) noun (‘strategy’, ‘election’ and ‘Obama’), (d) the alignments, (e) adjective (‘Republican’ and ‘re-@@’), and (f)
semantics words representing politics (‘Republican’, ‘strategy’, ‘election’ and ’Obama’).
We ﬁnd it an interesting future work to test the ﬁne-grained attention with other NMT models
like character-level models or multi-layered encode/decode models Ling et al. ; Chung et al.
 . Also, the ﬁne-grained attention mechanism can be applied to different tasks like speech
recognition.
Acknowledgments
The authors would like to thank the developers of Theano Bastien et al. . This research was
supported by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education (2017R1D1A1B03033341). Also, we acknowledge
the support of the following agencies for research funding and computing support: NSERC, Calcul Qu´ebec, Compute Canada, the Canada Research Chairs, CIFAR and Samsung. KC thanks the
support by Facebook and Google .