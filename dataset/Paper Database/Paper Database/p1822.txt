This article appeared in a journal published by Elsevier. The attached
copy is furnished to the author for internal non-commercial research
and education use, including for instruction at the authors institution
and sharing with colleagues.
Other uses, including reproduction and distribution, or selling or
licensing copies, or posting to personal, institutional or third party
websites are prohibited.
In most cases authors are permitted to post their version of the
article (e.g. in Word or Tex form) to their personal website or
institutional repository. Authors requiring further information
regarding Elsevier’s archiving and manuscript policies are
encouraged to visit:
 
Author's personal copy
Stochastic Processes and their Applications 119 2249–2276
www.elsevier.com/locate/spa
Microstructure noise in the continuous case: The
pre-averaging approach$
Jean Jacoda,∗, Yingying Lib, Per A. Myklandb, Mark Podolskijc, Mathias
a Institut de Math´ematiques de Jussieu, CNRS UMR 7586 and Universit´e Pierre et Marie Curie, 175 rue du Chevaleret,
75013 Paris, France
b Department of Statistics, The University of Chicago, Chicago, IL 60637, USA
c CREATES, School of Economics and Management, University of Aarhus, Building 1322, DK-8000 Aarhus C,
Danmark, Germany
d Ruhr-Universit¨at Bochum, Fakult¨at f¨ur Mathematik, 44780 Bochum, Germany
Received 14 December 2007; received in revised form 8 August 2008; accepted 3 November 2008
Available online 24 November 2008
This paper presents a generalized pre-averaging approach for estimating the integrated volatility, in the
presence of noise. This approach also provides consistent estimators of other powers of volatility — in
particular, it gives feasible ways to consistently estimate the asymptotic variance of the estimator of the
integrated volatility. We show that our approach, which possesses an intuitive transparency, can generate
rate optimal estimators (with convergence rate n−1/4).
c⃝2008 Elsevier B.V. All rights reserved.
MSC: primary 60G44; 62M09; 62M10; secondary 60G42; 62G20
Keywords: Consistency; Continuity; Discrete observation; Itˆo process; Leverage effect; Pre-averaging; Quarticity;
Realized volatility; Stable convergence
$ Financial support from the Stevanovich Center for Financial Mathematics at the University of Chicago, from the
National Science Foundation under grants DMS 06-04758 and SES 06-31605, from CREATES funded by the Danish
Research Foundation, and from Deutsche Forschungsgemeinschaft through SFB 475, is gratefully acknowledged.
∗Corresponding author.
E-mail addresses: (J. Jacod), (Y. Li), 
(P.A. Mykland), (M. Podolskij), (M. Vetter).
0304-4149/$ - see front matter c⃝2008 Elsevier B.V. All rights reserved.
doi:10.1016/j.spa.2008.11.004
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
1. Introduction
The recent years have seen a revolution in the statistics of high frequency data. On the one
hand, such data is increasingly available and needs to be analyzed. This is particularly the case
for market prices of stocks, currencies, and other ﬁnancial instruments. On the other hand, the
technology for the analysis of such data has grown rapidly. The emblematic problem is the
question of how to estimate daily volatility for ﬁnancial prices (in stochastic process terms, the
quadratic variation of log prices).
The early theory was developed in the context of stochastic calculus, before the ﬁnancial
application was apparent. The sum of squared increments of the process was shown to be
consistent for the quadratic variation in . A limit theory was then developed in , and
later in .
Meanwhile, these concepts were introduced to econometrics in . A limit theory was
developed in . Further early econometric literature includes, in particular, . The
setting of conﬁdence intervals using bootstrapping has been considered by Goncalves and
Meddahi and Kalnina and Linton .
The direct application to data of results from stochastic calculus have, however, run into the
problem of microstructure. No-arbitrage based characterizations of securities prices (as in )
suggest that these must normally be semimartingales. Econometric evidence, however, suggests
that there is additional noise in the prices. This goes back to Roll and Hasbrouck . In
the nonparametric setting, the deviation from semimartigales is most clearly seen through the
signature plots of Andersen et al. , see also the discussion in .
Statistical and econometric research has for this reason gravitated towards the concept that
the price (and log price) semimartingale is latent rather than observed. Research goes back to the
work on rounding by Jacod and Delattre and Jacod . Additive noise is studied in ,
and a consistent estimator in the nonparametric setting is found in . Issues of bias-variance
tradeoff are discussed in . In the nonparametric case, rate optimal estimators are found in
 . A development for low frequency data is given in .
There are currently three main approaches to estimation in the nonparametric case: linear
combination of realized volatilities obtained by subsampling , and linear combination of
autocovariances . The purpose of this paper is to give more insight to the third approach
of pre-averaging, which was introduced in Podolskij and Vetter for i.i.d. noise and for
non overlapping intervals. The idea is as follows. We suppose that the (say) log securities price
Xt is a continuous semimartingale (of the form (2.1)). The observations are recorded prices at
transaction times ti = i∆n, and what is observed is not Xti , but rather Zti , given by
Zti = Xti + ϵti .
The noise ϵti can be independent of the X process, or have a more complex structure, involving
for example some rounding. The idea is now that if one averages K of these Zti ’s, one is closer
to the latent process. Deﬁne ˘Zti as the average of Zti+ j , j = 0, . . . , K −1. The variance of the
noise in ˘Zi is now reduced by a factor of about 1/K. If one calculates the realized volatility on
the basis of ˘Z0, ˘Zt1, ˘Zt2, . . ., the estimate is therefore closer to being based on the true underlying
semimartingale. The scheme is particularly appealing since it is obviously robust to a wide variety
of structures of the noise ϵ.
The paper provides a way of implementing this idea. There are several issues that have to be
tackled in the process. First of all, the pre-averaging brings in a particular dependence structure
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
which necessitate an adjustive constant in front of the sum of squared increments of the averages.
Second, while the local averaging does reduce the impact of the noise ϵ, it does not completely
eliminate the bias. The pre-averaged realized volatility therefore has to be adjusted by an additive
term to eliminate the remaining error. Third, one would not wish to only average over differences
from non-overlapping intervals, but rather use a moving window. Fourth, the estimator can be
generalized by the use of a general weight function. Our ﬁnal estimator is thus on the form
(3.6), where we note that the special case of simple averaging is given in the example following
Theorem 3.1. Note that in the notation of that example, kn = 2K. Finally, the method used here
is amenable to much more general noise models than the most usual i.i.d. noise, independent of
the process X: see Section 2.
Like the subsampling and the autocovariance methods, the pre-averaging approach, when
well implemented, gives rise to rate optimal estimators (the convergence rate being Op(n−1/4)).
This result, along with a central limit theorem for the estimator, is given as our main result
Theorem 3.1.
What is the use of a third approach to the estimation problem, when there already are two that
provide good convergence? There are at least three advantages of the pre-averaging procedure:
(i) Transparency. It is natural to think of the latent process Xt as the average of observations
in a small interval. Without this assumption, identiﬁability problems may arise, as documented
in . Our procedure implements estimation directly based on this assumption. Also, as noted
after the deﬁnition (3.6), the entire randomness in the estimator is, to ﬁrst order, concentrated in
a single sum of squares.
(ii) Estimation of other powers of volatility. The pre-averaging approach also provides
straightforward consistent estimators of quarticity, thereby moving all the existing estimators
closer to the feasible setting of conﬁdence intervals. See for results in the case of independent
(iii) Edge effects. The three classes of estimators are similar also in that they are based on a
weight or kernel function. To some approximation, one can rewrite all subsampling estimators as
autocovariance estimators, and vice versa. The estimators in this paper can be rewritten, again to
ﬁrst order, as a class of subsampling or autocovariance estimators, cf. Remark 1. The difference
between the three classes of estimators (and what is concealed by the term “to ﬁrst order”) lies in
the treatment of edge effects. The potential impact of such effects can be considerable, cf. Bandi
and Russell . In some cases, the edge effects can even affect asymptotic properties. Because
of the intuitive nature of our estimator, edge effects are less likely to be a problem, and they
certainly do not interfere with the asymptotic results.
The plan of the paper is as follows. The mathematical model is deﬁned in Section 2, and
results are stated in Section 3. Section 4 provides a simulation study. The proofs are in Section 5.
2. The setting
We have a 1-dimensional underlying continuous process X = (Xt)t≥0, and observation times
i∆n for all i = 0, 1, . . . , k, . . .. We are in the context of high frequency data, that is we are
interested in the situation where the time lag ∆n is “small”, meaning that we look at asymptotic
properties as ∆n →0. The process X is observed with an error: that is, at stage n and instead of
the values Xn
i = Xi∆n for i ≥0, we observe real variables Zn
i , which are somehow related to
i , in a way which is explained below.
Our aim is to estimate the integrated volatility of the process X, over a ﬁxed time
interval [0, t], on the basis of the observations Zn
i for i = 0, 1, . . . , [t/∆n]. For this, we
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
need some assumptions on X and on the “noise”, and to begin with we need X to be
a continuous Itˆo semimartingale, so that the volatility is well deﬁned. Being a continuous
Itˆo semimartingale means that the process X is deﬁned on some ﬁltered probability space
(Ω(0), F(0), (F(0)
)t≥0, P(0)) and takes the form
where W = (Wt) is a standard Wiener process and b = (bt) and σ = (σt) are adapted processes,
such that the above integrals make sense. In fact, we will need some, relatively weak, assumptions
on these processes, which are gathered in the following assumption:
Assumption (H). We have (2.1) with two process b and σ which are adapted and c`adl`ag
(= “right-continuous with left limits” in time).
In this paper, we are interested in the estimation of the integrated volatility, that is the process
Next we turn to the description of the “noise”. Loosely speaking, we assume that,
conditionally on the whole process X, and for any given n, the observed values Zn
independent, each one having a (conditional) law which possibly depends on the time and on
the outcome ω, in an “adapted” way, and with conditional expectations Xn
Mathematically speaking, this can be realized as follows: for any t ≥0 we have a transition
probability Qt(ω(0), dz) from (Ω(0), F(0)
) into R, which satisﬁes
z Qt(ω(0), dz) = Xt(ω(0)).
We endow the space Ω(1) = R[0,∞) with the product Borel σ-ﬁeld F(1) and with the probability
Q(ω(0), dω(1)) which is the product ⊗t≥0 Qt(ω(0), .). We also call (Zt)t≥0 the “canonical
process” on (Ω(1), F(1)) and the ﬁltration F(1)
= σ(Zs : s ≤t). Then we consider the ﬁltered
probability space (Ω, F, (Ft)t≥0, P) deﬁned as follows:
Ω= Ω(0) × Ω(1),
F = F(0) × F(1),
Ft = ∩s>t F(0)
P(dω(0), dω(1)) = P(0)(dω(0)) Q(ω(0), dω(1)).
Any variable or process which is deﬁned on either Ω(0) or Ω(1) can be considered in the usual
way as a variable or a process on Ω. By standard properties of extensions of spaces, W is a
Wiener process on (Ω, F, (Ft)t≥0, P), and Eq. (2.1) holds on this extended space as well.
In fact, here again we need a little bit more than what precedes:
Assumption (K). We have (2.3) and further the process
αt(ω(0)) =
z2 Qt(ω(0), dz) −Xt(ω(0))2 = E((Zt)2 | F(0))(ω(0)) −Xt(ω(0))2
is c`adl`ag (necessarily (F(0)
)-adapted), and
z8 Qt(ω(0), dz)
is a locally bounded process.
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Taking the 8th moment in (2.6) is certainly not optimal, but this condition is in fact quite mild
(we need in any case the second moment to be locally bounded). The really strong requirement
above is the condition (2.3) on the noise. This assumption, together with the product structure of
the measure Q, summarize three properties at once:
(1) If Yt =
z Qt(dz) denotes the conditional expectation of the observed value Zt at time t,
conditionally on F(0) (the σ-ﬁeld generated by the latent process and all possible covariates),
then Y is a semimartingale.
(2) Moreover, we have Yt = Xt (unbiasedness).
(3) Finally, knowing F(0), the errors are independent at different times.
If (1) holds and (2) fails, our procedure provides an estimator for the integrated volatility of Yt
instead of Xt and, as explained in , it is probably impossible to make inference on the process
X itself, unless, of course, further speciﬁcation of the model is assumed. If (1) fails, our procedure
just breaks down, and we doubt that there might exist in general a method for retrieving the
volatility of X (see Example 3). As for (3), without any doubt it could be substantially weakened,
to allow for some (weak enough) dependency (cf. the discussion in ), but the independence
assumption simpliﬁes things quite a lot !
Example 1. If Zn
i , where the sequence (εn
i )i≥0 is i.i.d. centered with ﬁnite 8th
moment and independent of X, then (K) is obviously satisﬁed.
Example 2. Let Zn
i = γ ⌊(Xn
i )/γ ⌋for some γ > 0 and (εn
i ) as in the previous example.
This amounts to having an additive i.i.d. noise and then taking the rounded-off value with lag
γ , for example γ = 1 cent. Then as soon as the εn
i are uniform over ⌊0, γ ⌋, or more generally
uniform over [−2iγ, (2i + 1)γ ] for some integer i, (K) is satisﬁed. Another example of model
involving rounding with (K) satisﬁed is given in model 2 of Section 4. If the εn
i have a C2 density,
with further a ﬁnite 8th moment and a support containing an interval of length γ , then (K) is not
satisﬁed in general but the process Y introduced above is of the form Y = f (X) for a C2 function
f , and so everything goes through if we replace X by Y below.
Example 3. Let Zn
i /γ ⌋for some γ > 0 (“pure rounding”). Then the errors Zn
are independent, conditionally on X, but (K) is not satisﬁed, and the process Y is not a
semimartingale, and is not even c`adl`ag: so nothing of what follows applies. In fact in this case, if
we observe the whole process Zt = γ ⌊Xt/γ ⌋over some interval [0, T ], we can derive the local
t for t ∈[0, T ] of the process X at each level x = iγ for i ∈Z, but nothing else, and in
particular we cannot infer the values of the process Ct.
3. The results
We need ﬁrst some notation. We choose a sequence kn of integers and a number θ ∈(0, ∞)
satisfying
∆n = θ + o(∆1/4
(for example kn = [θ/√∆n]). We also choose a function g on , which satisﬁes
g is continuous, piecewise C1 with a piecewise Lipschitz derivative g′,
g(0) = g(1) = 0,
g(s)2ds > 0.
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
We associate with g the following numbers and functions on R+:
i = g(i/kn),
s ∈ 7→φ1(s) =
g′(u)g′(u −s) du,
g(u)g(u −s) du
s > 1 7→φ1(s) = 0,
i, j = 1, 2 ⇒Φi j =
φi(s)φ j(s) ds,
ψi = φi(0).
Next, with any process V = (Vt)t≥0 we associate the following random variables
i+ j V = −
(the two versions of V n
i are identical because g(0) = g(1) = 0).
Recall that in our setting, we do not observe the process X, but the process Z only, and at
times i∆n. So our estimator should be based on the values Zn
i only, and we propose to take
[t/∆n]−kn+1
i )2 −ψ1∆n
The last term above is here to remove the bias due to the noise, but apart from that it plays no
role in the central limit theorem given below.
As we will see, these estimators are asymptotically consistent and mixed normal, and in
order to use this asymptotic result we need an estimator for the asymptotic conditional variance.
Among many possible choices, here is an estimator:
[t/∆n]−kn+1
! [t/∆n]−2kn+1
! [t/∆n]−2
Theorem 3.1. Assume (H) and (K). For any ﬁxed t > 0 the sequence
t −Ct) converges
stably in law to a limiting variable deﬁned on an extension of the original space, and which is of
where B is a standard Wiener process independent of F and γt is the square-root of
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
and therefore, for any t > 0, the sequence
Cn −C) converges stably in law to an
N(0, 1) variable, independent of F.
Example. The simplest function g is probably
g0(x) = x ∧(1 −x).
In this case we have
and also, with kn even, we have
Remark 1. Our estimators are in fact essentially the same as the kernel estimators in . With
our notation the “ﬂat top” estimators of that paper are
[t/∆n]−kn+1
kn≤i≤[t/∆n]−kn+1, 1≤j≤kn
i+ j Z + ∆n
where k is some (smooth enough) weight function on having k(0) = 1 and k(1) = 0, and
also k′(0) = k′(1) = 0. Then we see that
∆n)) −ψ1∆n
i Z)2 + border terms,
provided we take k(s) = φ2(s)/ψ2, so there is a one-to-one correspondence between the weight
functions g and k. The “border terms” are terms arising near 0 and t, because the two sums in
the deﬁnition of K n
t do not involve exactly the same increments of Z. These border terms turn
out to be of order ∆1/4, the same order than b
t −Ct, although they are asymptotically unbiased
(but usually not asymptotically mixed normal). This explains why our CLT is somehow simpler
than the equivalent results in Barndorff-Nielsen et al. .
Remark 2. Suppose that Yt = σ Wt and that αt = α, where σ and α are positive constants, and
that t = 1. In this case there is an efﬁcient parametric bound for the asymptotic variance for
estimating σ 2 in presence of i.i.d. noise, which is 8σ 3√α, see e.g. Gloter and Jacod . On
the other hand, the concrete estimators given in or or , in the i.i.d. additive noise
case again, have an asymptotic variance ranging from 8.01 σ 3√α (and even from the optimal
variance 8 σ 3√α in , although in a slightly different setting) to 26 σ 3√α, upon using an
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
“optimal” choice of θ in (3.1). To compare with these results, here the “optimal” asymptotic
variance in the simple case (3.11), obtained for θ = 4.777 √α /σ, is 8.545 σ 3√α, quite close
to the efﬁcient bound. Note that other than (3.11) and more appropriate choices of g would give
lower asymptotic variances.
Remark 3. In practice, going down from 8.545 to 8 as in the previous remark is rather irrelevant,
in front of the fact that we do not know how to choose θ in an optimal way (this is the drawback
of all previously quoted papers as well, and especially for the efﬁcient estimator of Gloter and
Jacod ). More: since σ = σt and α = αt are usually random and time dependent, there is
no “optimal” choice of the number θ; one should rather take a θ = θt which is time-varying and
data-dependent, and based on “local” estimates of σt and αt.
We have not pursued this matter in this paper. However, simple compromises can be reached
as follows: we usually have an idea of the “average” sizes αave and σave of αt and σt: in this case
one may take θ close to 4.8 √αave/σave. Or, better, we can estimate
t αtdt and
t dt from the data (see the next remark for an estimator of the ﬁrst of these quantities, the
two others can be similarly estimated using the convergence (5.65) in the proof section. To get
these estimates, one uses a preliminary value for θ). Then we plug these estimators in Eq. (3.9)
to get an efﬁcient value of θ.
Remark 4. Note that a consistent estimator of the quarticity can be constructed using a different
linear combination of the three parts in (3.7). More speciﬁcally, we have
[t/∆n]−kn+1
[t/∆n]−2kn+1
see Section 5 for details.
Remark 5. Let us emphasize once more that, even though the structure of the noise considered
here is quite more general than in the existing literature, it still leaves out the case where the noise
is correlated, conditionally on X. It is likely that in the weakly correlated case similar results
(with different asymptotic variances) are true, but it would seriously complicate the analysis.
Another point is worth mentioning: in contrast with some of the existing literature on i.i.d.
noise and most of the literature on rounding, we do not consider a noise level going to 0 as the
frequency increases. If it were the case, the same analysis would apply, but the rate (and the
choice of kn in (3.1), to begin with) would be different.
4. Simulation results
In this section, we examine the performance of our estimator.
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Sample size “n” for four selected stocks from the New York stock exchange: Ford (F), Microsoft (MSFT), Intel (INTC),
and Pﬁzer (PFE), and one week in April 2005.
Fig. 1. One observed sample path generated by model 1.
4.1. Simulation design
We study the case when the weight function is taken to be g(x) = x ∧(1 −x). We simulate
data for one day (t ∈ ), and assume the data is observed once every second (n = 23 400).
The X processes and the market microstructure noise processes are generated from the models
below. 25 000 iterations were run for each model. The parameter values are chosen to be close to
those used in .
The actual values of n will vary from stock to stock and day to day. To get an idea of the range
for commonly traded stock, we report some values in Table 1. Our choice to use n = 23 400 is
meant to represent a fairly heavily traded stock.
Model 1 — the case of constant volatility & additive noise.
Xt = X0 + σ Wt,
Parameters used: σ = 0.2/
i ∼i.i.d. N(0, 0.00052).
The observed sample path looks as in Fig. 1:
Model 2 — the case of constant volatility & rounding plus error.
Xt = X0 + σ Wt,
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Fig. 2. One observed sample path generated by model 2.
i is a Bernoulli variable (probabilities pn
i and 1−pn
i of taking
values 1 and 0), with pn
This model is similar to the two-stage contamination model studied in Li and Mykland ,
where the observed log prices Zn
i ’s are the logarithm of the rounded contaminated prices.
Basically this model amounts to randomly assigning one of the two nearest rounding grids of
Xt to its corresponding observation (with probability pt the upper grid log(γ ⌈exp(Xt)
the lower grid log(γ ⌊exp(Xt)
⌋)). It’s easy to check that for this model, the Assumption (K) is
Parameters used: σ = 0.2/
252, X0 = log(8), γ = 0.01. Note that the initial value X0 is
chosen so that the averaged size of error αave is close to that in the simulations of model 1 and
The observed log price process looks like in Fig. 2.
Model 3 — the case of stochastic volatility & additive noise. The Heston model is used
to generate the stochastic volatility process.
dXt = (µ −νt/2)dt + σtdBt,
dνt = κ(α −νt)dt + γ ν1/2
where νt = σ 2
t and we assume Corr(B, W) = ρ.
Parameters used: µ = 0.05/252, κ = 5/252, α = 0.04/252, γ = 0.05/252, ρ = −0.5 and
i ∼i.i.d. N(0, 0.00052).
4.2. Simulation results
Some initial simulations (not recorded here) showed that our estimator is fairly robust to the
choice of kn, in other words, it performs reasonably well for a large range of kn. Since θ comes
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Relative bias in the estimators ˆCnt and Γ nt , for the three models.
Relative small-sample bias Avg[( ˆCnt −C)/C]
Relative bias in the variance estimator Avg[(Γ nt −
0 γ 2s ds)/
0 γ 2s ds]
Fig. 3. Histogram of ˆCn,adj
, for Model 1.
from asymptotic statistics, it doesn’t give precise instruction about kn for small samples. On the
other hand, when computing the true asymptotic variance
s ds, the θ we should use is really
√∆n. We decided to ﬁrstly ﬁx kn to be close to the one suggested by the optimal θ, and then
re-deﬁne θ to be kn
√∆n for further computations. In all our simulations, we used kn = 51,
which corresponds to a θ ≈1/3.
Table 2 reports the performance of the estimator ˆCn
t and the variance estimator Γ n
As we will see later, the results in Model 1 (see Fig. 3), where ˆCn
t are essentially
normal distributed, show the importance of a correction of these estimators, when dealing with
small sample sizes. We propose to replace the parameters ψi and Φi j by their ﬁnite sample
analogues, which are deﬁned as follows:
1 ( j))2 −1
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Bias in the estimators ˆCn,adj
and Γ n,adj
, for the three models.
Relative small-sample bias Avg[( ˆCn,adj
Relative bias in the variance estimator Avg[(Γ n,adj
0 γ 2s ds)/
0 γ 2s ds]
2 ( j))2 −1
As it can be seen in the proof, these parameters are the “correct” ones, but each of them
converges at a smaller order than n−1
4 and can therefore be replaced in the central limit theorem.
Nevertheless, for small sizes of kn the difference between each of the parameters and its limit
turns out to be substantial. A second adjustment regards the sums appearing in the estimators.
The numbers of summands are implicitly assumed to be ⌊t/∆n⌋rather than ⌊t/∆n⌋−kn + 2,
say. This doesn’t matter in the limit, but it is reasonable to scale each sum by ⌊t/∆n⌋divided
by the actual number of summands to obtain better results. However, this adjustment is of minor
importance. The last step is a ﬁnite sample bias correction due to the fact that P⌊t/∆n⌋
converges to Ct. Therefore, the latter term in ˆCn
t gives a small negative bias, which we dispose
of by another scaling factor. Summarized, the new statistics can be deﬁned as follows:
(⌊t/∆n⌋−kn + 2)θψkn
⌊t/∆n⌋−kn+1
2 )4(⌊t/∆n⌋−kn + 2)
⌊t/∆n⌋−kn+1
4∆n ⌊t/∆n⌋
θ3 (⌊t/∆n⌋−kn + 2)
⌊t/∆n⌋−kn+1
θ3(⌊t/∆n⌋−2)
2 )2 −2Φkn
Table 3 reports the performance of the adjusted estimator ˆCn,adj
and the variance estimator
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Comparisons of quantiles of Nnt , Nn,adj
with N(0, 1).
Model 1 Nnt
Model 1 Nn,adj
Model 2 Nnt
Model 2 Nn,adj
Model 3 Nnt
Model 3 Nn,adj
Comparisons of quantiles of N0nt , N0n,adj
with N(0, 1).
Model 1 N0nt
Model 1 N0n,adj
Model 2 N0nt
Model 2 N0n,adj
Model 3 N0nt
Model 3 N0n,adj
We test the normality of the statistics N n
and N n,adj
quantiles are compared with the N(0, 1) quantiles (see Table 4):
One of the reasons that the above quantiles don’t look good enough is that there is a (small)
positive correlation between the estimator ˆCn
t . One can adjust this effect by using a ﬁrst
order Taylor expansion of Γ: expanding Γ n
t or Γ n,adj
around the theoretical asymptotic variance
t −(Γ0 −Γ n
:= N n,adj
−(Γ0 −Γ n,adj
)( ˆCn,adj
The quantiles of N0n
t and N0n,adj
are compared with the N(0, 1) quantiles (see Table 5):
We see from the above simulation results that our estimator works quite well for these models.
We note that our theoretical results do not require the often made assumption that the rounding
threshold go to zero, and this is reﬂected in the simulation when comparing results of model 2
with those of model 1. Comparing results of model 3 with those of model 1, we see that our
approach works well for the stochastic volatility model (see Figs. 4 and 5).
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Fig. 4. Normal Q–Q plot of Nn,adj
for Model 1.
Fig. 5. Normal Q–Q plot of N0n,adj
for Model 1.
5. The proof
To begin with, we introduce a strengthened version of our assumptions (H) and (K):
Assumption (L). We have (H) and (K), and further the processes b, σ,
z8 Qt(dz) and X itself
are bounded (uniformly in (ω, t)) (then α is also bounded).
Then a standard localization procedure explained in details in for example shows that
for proving Theorem 3.1 it is no restriction to assume that (L) holds. Below, we assume these
stronger assumptions without further mention.
There are two separate parts in the proof. One consists in replacing in (3.6) the observed
process Z by the unobserved X, at the cost of additional terms which involve the quadratic mean
error process α of (2.5). The other part amounts to a central limit theorem for the sums of the
variables (Xn
i )2. This is not completely standard because (Xn
i )2 and (Xn
j)2 are strongly dependent
when |i −j| < kn, since they involve some common variables Xn
l , whereas kn →∞. So for this
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
we split the sum P[t/∆n]−kn+1
i )2 into “big” blocks of length pkn, with p eventually going
to ∞, separated by “small” blocks of length kn, which are eventually negligible but ensure the
conditional independence between the big blocks which we need for the central limit theorem.
Obviously, this scheme asks for somehow involved notation, which we present all together in
the next subsection.
5.1. Some notation
First, K denotes a constant which changes from line to line and may depend on the bounds of
the various processes in (L), and also on supn k2
n∆n (recall (3.1)), and is written Kr if it depends
on an additional parameter r. We also write Ou(x) for a (possibly random) quantity smaller than
K x for some constant K as above.
In the following, and unless otherwise stated, p ≥1 denotes an integer and q > 0 a real. For
each n we introduce the function
j 1(( j−1)∆n, j∆n](s),
which vanishes for s > (kn −1)∆n and s ≤0 and is bounded uniformly in n. We then introduce
the processes
X(n, s)t =
bugn(u −s)du +
σugn(u −s)dWu
C(n, s)t =
u gn(u −s)2 du.
These processes vanish for t ≤s, and are constant in time for t ≥s + (kn −1)∆n, and
i = X(n, i∆n)(i+kn)∆n,
i+ jC = C(n, i∆n)(i+kn)∆n.
Next, we set
(note the differences in the deﬁnition of ζ(V, p)n
i when V = Z or V = X or V = W). Moreover
for any process V we set
ζ ′(V, p)n
( j,m): i≤j<m≤i+pkn−1
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Next we consider the discrete time ﬁltrations Fn
j∆n−(that is, generated by all
j∆n-measurable variables plus all variables Zs for s < j∆n and F′n
= F(0) ⊗F(1)
j(p+1)kn and G′(p)n
j(p+1)kn+pkn, for j ∈N, and we introduce the variables
j = E(η(p)n
j(p+1)kn+pkn,
j = E(η′(p)n
j | G′(p)n
Then jn(p, t) =
−1 is the maximal number of pairs of “blocks” of respective sizes
pkn and kn that can be accommodated without using data after time t, and we set
With the notation in(p, t) = ( jn(p, t) + 1)(p + 1)kn, we also have three “residual” processes:
[t/∆n]−kn+1
[t/∆n]−kn+1
[t/∆n]−kn+1
The key point of all this notation is the following identity, valid for all p ≥1:
t −Ct = M(p)n
t + M′(p)n
t + F′(p)n
We end this subsection with some miscellaneous notation:
s,t∈[i∆n,(i+(p+2)kn)∆n]
(|bs −bt| + |σs −σt| + |αs −αt|)
sφi(s)φ j(s) ds.
5.2. Estimates for the Wiener process
This subsection is devoted to proving the following result about the Wiener process:
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Lemma 5.1. We have
E((ζ(W, p)n
i ) = 4(pΦ22 + Ξ22)k4
i )4 + Ou(p2χ(p)n
 ζ ′(W, p)n
= (pΦ12 + Ξ12)k3
n∆n + Ou(p∆−1/4
Proof. (1) Since g(0) = g(1) = 0 we have
0 g′(s)ds = 0. We introduce the process
g′(s)Wt+sds = −
g′(s −t)Wsds = −
g′(s)(Wt+s −Wt)ds, (5.19)
which is stationary centered Gaussian with covariance E(UtUt+s) = φ2(s), as given by (3.4).
The scaling property of W and (3.5) and g(0) = g(1) = 0 imply that
W(i+ j)/kn
Then (3.2) and the fact that E(supu∈[0,s] |Wt+u −Wt|q) ≤Kqsq/2, plus a standard approximation
of an integral by Riemann sums, yield
kn∆nUi/kn + Rn
i |q) ≤Kq∆q/2
where the last estimate holds for all q > 0. Then in view of (3.1) we get for j ≥i:
2 + Ou(∆5/4
At this stage, (5.18) is obvious.
(2) We have
i )2 = (σ n
i )4Vn(i, p)2 + V ′
n(i, p)2 −2(σ n
i )2Vn(i, p)V ′
Vn(i, p) =
On the one hand, we deduce from (5.3) that if i ≤j ≤i + (p + 1)kn,
j = ψ2kn∆n(σ n
i )2 + Ou(∆n +
Then obviously
n(i, p) = ψ2(σ n
n∆n + Ou(p
∆n + p β(p)n
On the other hand, another application of (5.20) and of the approximation of an integral by
Riemann sums, plus the fact that E(supu∈[0,s] |Ut+u −Ut|q) ≤Kqsq (this easily follows from
(5.19)), yield for any p ≥1:
Vn(i, p) L= k2
(Us)2ds + R(p)n
i |q) ≤Kq pq∆q/4
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Since E(UtUt+s) = φ2(s), that for p ≥2 the variable U p =
0 (Us)2ds satisﬁes
E(U p) = pψ2,
2 + 4pΦ22 + 4Ξ22.
Then (5.25) yields
E(Vn(i, p) | Fn
n∆nψ2 + Ou(p∆1/4
E(Vn(i, p)2 | Fn
i ) = (p2ψ2
2 + 4pΦ22 + 4 Ξ22)k4
n + Ou(p2∆1/4
Combining (5.24) and (5.26) with (5.22), we immediately get (5.17).
5.3. Estimates for the process X
Here we give estimates on the process X. The Assumption (L) implies that for all s, t ≥0 and
u,v∈[t,t+s]
|Xu −Xv|q | Ft
|E(Xt+s −Xt | Ft)| ≤Ks.
Then, since |hn
j| ≤K/kn and Pkn−1
j = 0 for the second inequality below, we have
i+1X|q | Fn
An elementary consequence is the following set of inequalities (use also |cn
i | ≤K√∆n for the
ﬁrst one):
Here and below, as mentioned before, the constant K p depends on p, and it typically goes to ∞
as p →∞(in this particular instance, we have K p = K p4); what is important is that it does not
depend on n, nor on i.
(5.29) is not enough, and we need more precise estimates on ζ(X, p)n
i and ζ ′(X, p)n
in the following two lemmas.
Lemma 5.2. We have
E(ζ(X, p)n
Proof. Observe that, similar to (5.27),
|X(n, s)t|q | Fs
|E(X(n, s)t | Fs)| ≤K
Let us deﬁne the processes
M(n, s)t = 2
X(n, s)u σu gn(u −s) dWu,
B(n, s)t = 2
X(n, s)u bu gn(u −s)du.
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Then M(n, s) is a martingale, and by Itˆo’s formula X(n, s)2 = B(n, s) + C(n, s) + M(n, s).
Hence, since E(χ(1)n
i ) ≤χ(p)n
i when i ≤j ≤i + (p + 1)kn, (5.30) is implied by
E(B(n, j∆n)(i+kn)∆n | Fn
For this we write B(n, i∆n)(i+kn)∆n = Un + Vn, where
Z ( j+kn)∆n
X(n, j∆n)u gn(u −j∆n) du,
Z ( j+kn)∆n
X(n, j∆n)u (bu −bn
j) gn(u −i∆n) du.
On the one hand, the second part of (5.31) yields that
≤K∆n ≤K∆3/4
the other hand, we have |Vn| ≤K√∆n β(1)n
i supt≥0 |X(n, j∆n)t|, hence the ﬁrst part of (5.31)
and Cauchy–Schwarz inequality yield E(|Vn| | Fn
j ) ≤K∆3/4
j, and the result follows.
Lemma 5.3. We have
−4(pΦ22 + Ξ22) k4
 ζ ′(p, X)n
−(pΦ12 + Ξ12)k3
Proof. The method is rather different from the previous lemma, and based upon the property that
for i∆n ≤t ≤s ≤(i + (p + 2)kn)∆n we have
u,v∈[t,t+s]
|Xu −Xv −σt(Wu −Wv)|q | Fn
≤K p,qsq/2 
sq/2 + E((β(p)n
We deduce that for i ≤j,l ≤i + (p + 2)kn we have
≤K p,q∆q/4
+ E((β(p)n
j ) and |hn
j| ≤K/kn, by using H¨older inequality and (5.28) we
get for s a positive integer
≤K p,q,s ∆sq/4
+ E((β(p)n
By (5.7), this for s = 1 and q = 2, plus (5.28) and Cauchy–Schwarz inequality, yield
ζ ′(X, p)n
i )2ζ ′(W, p)n
In a similar way, and in view of (5.6), we apply (5.34) with s = 2 and q = 2 to get
i −ζ(W, p)n
≤K p (χ(p)n
which yields (use (5.29) and Cauchy–Schwarz inequality):
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
i )2 −(ζ(W, p)n
≤K p χ(p)n
At this stage, the result readily follows from Lemma 5.1.
5.4. Estimates for the process Z
Now we turn to the observed process Z, and relate the moments of the variables Zn
conditional on F(0), with the corresponding powers of Xn
j. To begin with, and since |hn
and α is bounded, and by the rate of approximation of the integral of a piecewise Lipschitz
function by Riemann sums, the following properties are obvious:
| j −i| ≥kn
i ≤j ≤m ≤i + (p + 1)kn
+ Ou(p∆n +
( j,m): i≤j<m≤i+pkn−1
j,m)2 = (αn
i )2(pΦ11 + Ξ11) + Ou
∆n + pβ(p)n
Next, we give estimates for the F(0)-conditional expectations of various functions of Z.
Because of the F(0)-conditional independence of the variables Zt −Xt for different values of t,
and because of (2.3), the conditional expectation E((Zt −Xt)(Zs −Xs) | F(0) ⊗F(1)
s−) vanishes
if s < t and equals αt if s = t. Then, recalling (2.5) and (5.4),
More generally, E
i+ jm) | Fn
= 0 as soon as there is one jm which
is different from all the others, and moreover |hn
j| ≤K√∆n, whereas the moments (2.6) are
bounded for q ≤8. Then if we write (Zn
i )q as the sum of Π q
i+ jm) over
all choices of integers jl between 0 and kn −1, we see that for r, q integers we have
if q + r = 3
i, j)2 + Ou(∆3/2
if q = r = 2
if q + r = 8.
Now, if we expand the ﬁrst members of (5.38), and in view of (5.36) and (5.5) and of
i | ≤K√∆n, we deduce from (5.37) and (5.38) that for j ≥i:
i )2 + Ou 2249–2276
Then obviously this, combined with (5.28) and (5.30), yields
E(ζ(Z, p)n
i ) = ζ(X, p)n
E((ζ(Z, p)n
E(ζ(Z, p)n
and also, in view of (5.36),
E((ζ(Z, p)n
i )2 | F′n
i ) = (ζ(X, p)n
i ζ ′(X, p)n
i )2(pΦ11 + Ξ11)
∆n + β(p)n
Then, using (5.28) again and (5.32) and H¨older inequality, we get
E((ζ(Z, p)n
i ) −4(pΦ22 + Ξ22)k4
i )2(pΦ12 + Ξ12)k2
i )2(pΦ11 + Ξ11)
≤K p χ(p)n
We need some other estimates. Exactly as for (5.39) one sees that
i )4 | F′n
i )4 + 6(Xn
i )8 | F′n
and (using the boundedness of X)
= ζ ′′(X)n
E((ζ ′′(Z)n
i )2 | F′n
Therefore, using (5.28), (5.29), (5.36), (5.21) and (5.34) with s = 2, we obtain
i )4 −6∆n(σ n
E(ζ ′′(Z)n
Finally, the following is obtained in the same way, but it is much simpler:
i+1Z)2 | F′n
i+1X)2 + αn
i+3Z)2 | Fn
i+3Z)4 | Fn
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
5.5. Proof of the theorem
We begin the proof of Theorem 3.1 with an auxiliary technical result.
Lemma 5.4. For any p ≥1 we have
j(p+1)kn)2 | Fn
j(p+1)kn)2
Proof. We have jn(p, t) ≤K pt/√∆n. Then the ﬁrst expression in (5.47) is smaller than a
constant times the square-root of the second expression, and thus for (5.47) it sufﬁces to prove
j(p+1)kn)2
Let ε > 0 and denote by N(ε)t the number of jumps of any of the three processes b, σ or
α, with size bigger than ε, over the interval [0, t], and set ρ(ε, t, η) to be the supremum of
|bs −br| + |σs −σr| + |αs −αr| over all pairs (s,r) such that s ≤r ≤s + η ≤t and such that
the interval (s,r] contains no jump of b, σ or α of size bigger than ε. Then obviously, since all
three processes b, σ, α are bounded,
j(p+1)kn)2 ≤
∧(K pt) + K p t ρ(ε, t, (p + 1)kn∆n)2.
Moreover lim supη→0 ρ(ε, t, η) ≤3ε. Then Fatou’s lemma yields that the lim sup of the left
side of (5.48) is smaller than K ptε2, and the result follows.
The proof of the ﬁrst part of the theorem is based on the identity (5.14), valid for all integers
p ≥1. The right side of this decomposition contains two “main” terms M(p)n
t and M′(p)n
all others are taken care of in Lemmas 5.5 and 5.6 below:
Lemma 5.5. For any ﬁxed p ≥1 we have:
Proof. In view of (5.9) and (5.10), the proof of both (5.49) and (5.50) is a trivial consequence
of (5.40) and of Lemmas 5.2 and 5.4. Since the right side of (5.11) contains at most K p/√∆n
summands, each one having expectation less than K∆1/2
by the last part of (5.39) and (5.28),
we immediately get (5.51).
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
In view of (5.3), and with the notation an = Pkn−1
j )2, we see that
[t/∆n]−kn+1
[t/∆n]−kn+1
j=1∨(l+kn−1−[t/∆n])
[t/∆n]−kn+2
l C + Ou(1).
It follows that b
+ Ou(√∆n). Since by Riemann approximation we have
an = knψ2 + Ou(1), we readily deduce (5.52) from (3.1).
Lemma 5.6. For any ﬁxed p ≥1 we have ∆−1/4
Proof. Let ζ n
i Z)2 −(αn
i ). We get by (5.28) and (5.46), and for 1 ≤i ≤j −2:
i ) = E((∆n
i X)2) = Ou(∆n),
j ) = E((∆n
j X)2) = Ou(∆2
and also E(|ζ n
i |2) ≤K. Then obviously E
≤K/∆n, and it follows that
Gn := ψ1∆3/4
It is then enough to prove that
−→0. Observe that by an elementary
calculation,
t + Gn = Un + Vn, where
! in(p,t)−1
in(p,t)+kn−2
l=i+1−in(p,t)
On the one hand, since αt is bounded and |hn
l | ≤K√∆n it is obvious that |Vn| ≤K∆1/4
the other hand, Pkn−1
kn + O(∆n), whereas Pin(p,t)−1
i ≤K/∆n, so by (3.1)), we
see that Un →0 pointwise. Then it ﬁnishes the proof.
Now we study the main terms M(p)n
t and M′(p)n
t in (5.14). Those terms are (discretised)
sums of martingale differences (note that η(p)n
j and η′(p)n
j are measurable with respect to
j+1 and G′(p)n
j+1 respectively).
By Doob’s inequality we have
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
Now, (5.41) for p = 1 and the boundedness of χ(1)n
i imply E(|η′(p)n
j|2) ≤K∆n, and thus
(recall jn(p, t) ≤Kt/p√∆n):
Lemma 5.7. For any ﬁxed p ≥2, the sequence
M(p)n of processes converges stably in
γ (p)sdBs,
where B is like in Theorem 3.1 and γ (p)t is the square root of
p + 1 Φ22 +
p + 1Φ12 +
p + 1 Φ11 +
Proof. (1) In view of a standard limit theorem for triangular arrays of martingale differences, it
sufﬁces to prove the following three convergences:
j)2 | G(p)n
j) −(η(p)n
j)4 | G(p)n
j ∆(N, p)n
where ∆(V, p)n
j = Vj(p+1)kn∆n −V( j−1)(p+2)kn∆n for any process V , and where (5.58) should
hold for all bounded martingales N which are orthogonal to W, and also for N = W. The last
property is as stated as in . However, a look at the proof in shows that it is enough to
have it for N = W, and for all N in a set N of bounded martingales which are orthogonal to W
and such that the family (N∞: N ∈N) is total in the space L1(Ω, F, P). A suitable such set N
will be described later.
(2) Since jn(p, t) ≤Kt/p√∆n, (5.57) trivially follows from (5.40), whereas (5.56) is an
immediate consequence of (5.41) and of a Riemann sums argument.
(3) The proof of (5.58) is much more involved, and we begin by proving that
j = E(ζ(W, p)n
j(p+1)kn ∆(N, p)n
We have ζ(W, p)n
i )2Vn(i, p) −V ′
n(i, p) (see after (5.22)), and we set
j = E(Vn( j(p + 1)kn, p) ∆(N, p)n
n( j(p + 1)kn, p) ∆(N, p)n
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
When N = W, the variable δn
j is the F j(p+1)kn∆n-conditional expectation of an odd function
of the increments of the process W after time j(p + 2)kn∆n, hence it vanishes. Suppose now
that N is a bounded martingale, orthogonal to W. By Itˆo’s formula we see that (W n
j)2 is the sum
of a constant (depending on n) and of a martingale which is a stochastic integral with respect to
W, on the interval [ j∆n, ( j +kn)∆n]. Hence δn
j is the sum of a constant plus a martingale which
is a stochastic integral with respect to W, on the interval [ j(p + 1)kn∆n, ( j + 1)(p + 1)kn∆n].
Then the orthogonality of N and W implies δn
j = 0 again. Hence in both cases we have δn
j(p+1)kn)2δn
i , (5.59) will follow if we prove
For this we use (5.23). Since N is a martingale, we deduce (using Cauchy–Schwarz inequality)
j | ≤K p χ(p)n
E(∆(F, p)n
where F = ⟨N, N⟩(the predictable bracket of N). Then the expected value of the left side of
(5.60) is smaller than the square-root of
j(p+1)kn)2
and we conclude by Lemma 5.4.
(4) In this step we prove that
j = E(ζ(X, p)n
j(p+2)kn ∆(N, p)n
Then by Cauchy–Schwarz inequality and (5.35) we see that |a′n
j | satisﬁes the same estimate
j | in (5.61). Hence we deduce (5.62) from (5.59) like in the previous step.
(5) It remains to deduce (5.58) from (5.62), and for this we have to specify the set N.
This set N is the union of N 0 and N 1, where N 0 is the set of all bounded martingales
on (Ω(0), F(0), (F(0)
), P(0)), orthogonal to W, and N 1 is the set of all martingales having
N∞= f (Zt1, . . . , Ztq), where f is any Borel bounded on Rq and t1 < · · · < tq and q ≥1.
When N is either W or is in N 0, then by (5.40) the left sides of (5.58) and of (5.62) agree,
so in this case (5.58) holds. Next, suppose that N is in N 1, associated with the integer q and
the function f as above. In view of (2.4) it is easy to check that N takes the following form (by
convention t0 = 0 and tq+1 = ∞):
tl ≤t < tl+1 ⇒Nt = M(l; Zt1, . . . , Ztl)t
for l = 0, . . . , q, and where M(l; z1, . . . , zl) is a version of the martingale
M(l; z1, . . . , zl)t = E(0)
Qtr (dzr) f (z1, . . . , zl, zl+1, . . . , zq) | F(0)
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
(with obvious conventions when l = 0 and l = q), which is measurable in (z1, . . . , zl, ω(0)).
E((ζ(Z, p) j(p+1)kn −ζ(X, p) j(p+1)kn) ∆(N, p)n
by (5.40) when the interval ( j(p+1)kn∆n, ( j(p+1)+1)kn∆n] contains no point tl. Furthermore,
the left side of (5.63) is always smaller in absolute value than K p (use (5.29) and (5.40) and
the boundedness of N). Since we have only q intervals ( j(p + 2)kn∆n, ( j(p + 1) + 2)kn∆n]
containing points tl, at most, we deduce from this fact and from (5.63) that
j ∆(N, p)n
and (5.58) readily follows from (5.62).
Now we can proceed to the proof of the ﬁrst claim of Theorem 3.1. We have
t + V (p)n
t + F′(p)n
On the one hand, Lemmas 5.5 and 5.6 and (5.53) yield
p→∞lim sup
for all ε > 0. On the other hand, we ﬁx the Brownian motion B, independent of F. Since
γ (p)t(ω) converges pointwise to γt(ω) and stays bounded by (5.55), it is obvious that Y(p)t
Yt (recall (3.8) and (5.54) for Y and Y(p)). Then the result follows from (5.54) in a standard way.
It remains to prove (3.10). We set for r = 1, 2, 3:
i∈I (r,n,t)
I (r, n, t) =
{0, 1, . . . , [t/∆n] −kn + 1}
{0, 1, . . . , [t/∆n] −2kn + 1}
{0, 1, . . . , [t/∆n] −3}
i = ∆nζ ′′(Z)n
(Note the different summations ranges I (r, n, t), which ensure that we take into account all
variables ζ(r)n
i which are observable up to time t, and not more.)
Then a simple computation shows that (3.10) and (3.14) are implied by
−→Γ(r)t :=
Author's personal copy
J. Jacod et al. / Stochastic Processes and their Applications 119 2249–2276
for r = 1, 2, 3, where
γ (1)t = 3θ2ψ2
t + 6ψ1ψ2σ 2
γ (2)t = 2θ2ψ2σ 2
t αt + 2ψ1α2
γ (3)t = 4α2
We set u′(r)n
i = E(u(r)n
i ), and we denote by Γ ′(r)n
t for r = 1, 2, 3 the processes deﬁned
by (5.64), with u(r)n
i substituted with u′(r)n
i . Then we have Γ ′(r)n
−→Γ(r)t for r = 1, 2, 3:
this is a trivial consequence of (5.44)–(5.46) and of an approximation of an integral by Riemann
sums. Hence it remains to prove that Γ(r)n
t −Γ ′(r)n
−→0, a result obviously implied by the
following convergence:
i, j∈I (r,n,t)
v(r, n, i, j) →0,
where v(r, n, i, j) =
We have |v(r, n, i, j)| ≤K∆2
n by (5.42) for r = 1, by (5.43) for r = 2 and by (5.46) for r = 3.
Further v(1, n, i, j) = 0 when | j −i| ≥kn, and v(2, n, i, j) = 0 when | j −i| ≥2kn, and
v(3, n, i, j) = 0 when | j −i| ≥5, so (5.66) holds in all cases.