Semantic Image Inpainting with Deep Generative Models
Raymond A. Yeh‚àó, Chen Chen‚àó, Teck Yian Lim,
Alexander G. Schwing, Mark Hasegawa-Johnson, Minh N. Do
University of Illinois at Urbana-Champaign
{yeh17, cchen156, tlim11, aschwing, jhasegaw, minhdo}@illinois.edu
Semantic image inpainting is a challenging task where
large missing regions have to be Ô¨Ålled based on the available visual data. Existing methods which extract information from only a single image generally produce unsatisfactory results due to the lack of high level context. In this paper, we propose a novel method for semantic image inpainting, which generates the missing content by conditioning
on the available data. Given a trained generative model,
we search for the closest encoding of the corrupted image
in the latent image manifold using our context and prior
losses. This encoding is then passed through the generative
model to infer the missing content. In our method, inference is possible irrespective of how the missing content is
structured, while the state-of-the-art learning based method
requires speciÔ¨Åc information about the holes in the training
phase. Experiments on three datasets show that our method
successfully predicts information in large missing regions
and achieves pixel-level photorealism, signiÔ¨Åcantly outperforming the state-of-the-art methods.
1. Introduction
Semantic inpainting refers to the task of inferring arbitrary large missing regions in images based on image semantics. Since prediction of high-level context is required,
this task is signiÔ¨Åcantly more difÔ¨Åcult than classical inpainting or image completion which is often more concerned
with correcting spurious data corruption or removing entire
objects. Numerous applications such as restoration of damaged paintings or image editing beneÔ¨Åt from accurate
semantic inpainting methods if large regions are missing.
However, inpainting becomes increasingly more difÔ¨Åcult if
large regions are missing or if scenes are complex.
Classical inpainting methods are often based on either
local or non-local information to recover the image. Most
existing methods are designed for single image inpainting.
‚àóAuthors contributed equally.
Figure 1. Semantic inpainting results by TV, LR, PM and our
method. Holes are marked by black color.
Hence they are based on the information available in the
input image, and exploit image priors to address the illposed-ness. For example, total variation (TV) based approaches take into account the smoothness property
of natural images, which is useful to Ô¨Åll small missing regions or remove spurious noise. Holes in textured images
can be Ô¨Ålled by Ô¨Ånding a similar texture from the same image . Prior knowledge, such as statistics of patch offsets , planarity or low rank (LR) can greatly
improve the result as well. PatchMatch (PM) searches
for similar patches in the available part of the image and
quickly became one of the most successful inpainting methods due to its high quality and efÔ¨Åciency. However, all single image inpainting methods require appropriate information to be contained in the input image, e.g., similar pixels,
structures, or patches. This assumption is hard to satisfy, if
the missing region is large and possibly of arbitrary shape.
Consequently, in this case, these methods are unable to recover the missing information. Fig. 1 shows some challenging examples with large missing regions, where local
 
methods fail to recover the nose and eyes.
In order to address inpainting in the case of large missing
regions, non-local methods try to predict the missing pixels
using external data. Hays and Efros proposed to cut
and paste a semantically similar patch from a huge database.
Internet based retrieval can be used to replace a target region
of a scene . Both methods require exact matching from
the database or Internet, and fail easily when the test scene
is signiÔ¨Åcantly different from any database image. Unlike
previous hand-crafted matching and editing, learning based
methods have shown promising results . After an image dictionary or a neural network is learned, the
training set is no longer required for inference. Oftentimes,
these learning-based methods are designed for small holes
or for removing small text in the image.
Instead of Ô¨Ålling small holes in the image, we are interested in the more difÔ¨Åcult task of semantic inpainting .
It aims to predict the detailed content of a large region based
on the context of surrounding pixels. A seminal approach
for semantic inpainting, and closest to our work is the Context Encoder (CE) by Pathak et al. .
Given a mask
indicating missing regions, a neural network is trained to
encode the context information and predict the unavailable
content. However, the CE only takes advantage of the structure of holes during training but not during inference. Hence
it results in blurry or unrealistic images especially when
missing regions have arbitrary shapes.
In this paper, we propose a novel method for semantic image inpainting. We consider semantic inpainting as
a constrained image generation problem and take advantage of the recent advances in generative modeling. After
a deep generative model, i.e., in our case an adversarial network , is trained, we search for an encoding of the
corrupted image that is ‚Äúclosest‚Äù to the image in the latent
space. The encoding is then used to reconstruct the image
using the generator. We deÔ¨Åne ‚Äúclosest‚Äù by a weighted context loss to condition on the corrupted image, and a prior
loss to penalizes unrealistic images. Compared to the CE,
one of the major advantages of our method is that it does
not require the masks for training and can be applied for
arbitrarily structured missing regions during inference. We
evaluate our method on three datasets: CelebA , SVHN
 and Stanford Cars , with different forms of missing
regions. Results demonstrate that on challenging semantic
inpainting tasks our method can obtain much more realistic
images than the state of the art techniques.
2. Related Work
A large body of literature exists for image inpainting, and
due to space limitations we are unable to discuss all of it in
detail. Seminal work in that direction includes the aforementioned works and references therein. Since our method
is based on generative models and deep neural nets, we will
Figure 2. Images generated by a VAE and a DCGAN. First row:
samples from a VAE. Second row: samples from a DCGAN.
review the technically related learning based work in the
following.
Generative Adversarial Networks (GANs) are a framework for training generative parametric models, and have
been shown to produce high quality images . This
framework trains two networks, a generator, G, and a discriminator D. G maps a random vector z, sampled from a
prior distribution pZ, to the image space while D maps an
input image to a likelihood. The purpose of G is to generate
realistic images, while D plays an adversarial role, discriminating between the image generated from G, and the real
image sampled from the data distribution pdata.
The G and D networks are trained by optimizing the loss
D V (G, D) =Eh‚àºpdata(h)[log(D(h))]+
Ez‚àºpZ(z)[log(1 ‚àíD(G(z))],
where h is the sample from the pdata distribution; z is a
random encoding on the latent space.
With some user interaction, GANs have been applied in
interactive image editing . However, GANs can not be
directly applied to the inpainting task, because they produce
an entirely unrelated image with high probability, unless
constrained by the provided corrupted image.
Autoencoders and Variational Autoencoders (VAEs) 
have become a popular approach to learning of complex
distributions in an unsupervised setting. A variety of VAE
Ô¨Çavors exist, e.g., extensions to attribute-based image editing tasks . Compared to GANs, VAEs tend to generate
overly smooth images, which is not preferred for inpainting
tasks. Fig. 2 shows some examples generated by a VAE and
a Deep Convolutional GAN (DCGAN) . Note that the
DCGAN generates much sharper images. Jointly training
VAEs with an adveserial loss prevents the smoothness ,
but may lead to artifacts.
The Context Encoder (CE) can be also viewed as an
autoencoder conditioned on the corrupted images.
It produces impressive reconstruction results when the
structure of holes is Ô¨Åxed during both training and inference, e.g., Ô¨Åxed in the center, but is less effective for arbitrarily structured regions.
ùêøùëúùë†ùë†= ùêøùëùùíõ+ ùêøùëêùíõ
Figure 3. The proposed framework for inpainting. (a) Given a GAN model trained on real images, we iteratively update z to Ô¨Ånd the closest
mapping on the latent image manifold, based on the desinged loss functions. (b) Manifold traversing when iteratively updating z using
back-propagation. z(0) is random initialed; z(k) denotes the result in k-th iteration; and ÀÜz denotes the Ô¨Ånal solution.
Back-propagation to the input data is employed in our
approach to Ô¨Ånd the encoding which is close to the provided
but corrupted image.
In earlier work, back-propagation
to augment data has been used for texture synthesis and
style transfer . Google‚Äôs DeepDream uses backpropagation to create dreamlike images . Additionally,
back-propagation has also been used to visualize and understand the learned features in a trained network, by ‚Äúinverting‚Äù the network through updating the gradient at the
input layer .
Similar to our method, all
these back-propagation based methods require speciÔ¨Åcally
designed loss functions for the particular tasks.
3. Semantic Inpainting by Constrained Image
Generation
To Ô¨Åll large missing regions in images, our method for
image inpainting utilizes the generator G and the discriminator D, both of which are trained with uncorrupted data.
After training, the generator G is able to take a point z
drawn from pZ and generate an image mimicking samples
from pdata. We hypothesize that if G is efÔ¨Åcient in its representation then an image that is not from pdata (e.g., corrupted data) should not lie on the learned encoding manifold, z. Therefore, we aim to recover the encoding ÀÜz ‚Äúclosest‚Äù to the corrupted image while being constrained to the
manifold, as illustrated in Fig. 3; we visualize the latent
manifold, using t-SNE on the 2-dimensional space, and
the intermediate results in the optimization steps of Ô¨Ånding
ÀÜz. After ÀÜz is obtained, we can generate the missing content
by using the trained generative model G.
More speciÔ¨Åcally, we formulate the process of Ô¨Ånding ÀÜz
as an optimization problem. Let y be the corrupted image
and M be the binary mask with size equal to the image,
to indicate the missing parts. An example of y and M is
shown in Fig. 3 (a).
Using this notation we deÔ¨Åne the ‚Äúclosest‚Äù encoding ÀÜz
ÀÜz = arg min
z {Lc(z|y, M) + Lp(z)},
where Lc denotes the context loss, which constrains the
generated image given the input corrupted image y and the
hole mask M; Lp denotes the prior loss, which penalizes
unrealistic images. The details of the proposed loss function will be discussed in the following sections.
Besides the proposed method, one may also consider using D to update y by maximizing D(y), similar to backpropagation in DeepDream or neural style transfer .
However, the corrupted data y is neither drawn from a
real image distribution nor the generated image distribution.
Therefore, maximizing D(y) may lead to a solution that is
far away from the latent image manifold, which may hence
lead to results with poor quality.
3.1. Importance Weighted Context Loss
To Ô¨Åll large missing regions, our method takes advantage
of the remaining available data. We designed the context
loss to capture such information. A convenient choice for
the context loss is simply the ‚Ñì2 norm between the generated sample G(z) and the uncorrupted portion of the input
image y. However, such a loss treats each pixel equally,
which is not desired. Consider the case where the center
block is missing: a large portion of the loss will be from
pixel locations that are far away from the hole, such as the
background behind the face. Therefore, in order to Ô¨Ånd the
correct encoding, we should pay signiÔ¨Åcantly more attention to the missing region that is close to the hole.
To achieve this goal, we propose a context loss with the
hypothesis that the importance of an uncorrupted pixel is
positively correlated with the number of corrupted pixels
surrounding it. A pixel that is very far away from any holes
plays very little role in the inpainting process. We capture
this intuition with the importance weighting term, W,
if Mi Ã∏= 0
where i is the pixel index, Wi denotes the importance
weight at pixel location i, N(i) refers to the set of neighbors of pixel i in a local window, and |N(i)| denotes the
cardinality of N(i). We use a window size of 7 in all experiments.
Empirically, we also found the ‚Ñì1-norm to perform
slightly better than the ‚Ñì2-norm in our framework. Taking it
all together, we deÔ¨Åne the conextual loss to be a weighted
‚Ñì1-norm difference between the recovered image and the
uncorrupted portion, deÔ¨Åned as follows,
Lc(z|y, M) = ‚à•W ‚äô(G(z) ‚àíy)‚à•1.
Here, ‚äôdenotes the element-wise multiplication.
3.2. Prior Loss
The prior loss refers to a class of penalties based on
high-level image feature representations instead of pixelwise differences. In this work, the prior loss encourages the
recovered image to be similar to the samples drawn from
the training set. Our prior loss is different from the one
deÔ¨Åned in which uses features from pre-trained neural
Our prior loss penalizes unrealistic images. Recall that
in GANs, the discriminator, D, is trained to differentiate
generated images from real images. Therefore, we choose
the prior loss to be identical to the GAN loss for training the
discriminator D, i.e.,
Lp(z) = Œª log(1 ‚àíD(G(z))).
Here, Œª is a parameter to balance between the two losses. z
is updated to fool D and make the corresponding generated
image more realistic. Without Lp, the mapping from y to
z may converge to a perceptually implausible result. We
illustrate this by showing the unstable examples where we
optimized with and without Lp in Fig. 4.
Ours w/o Lp
Figure 4. Inpainting with and without the prior loss.
3.3. Inpainting
With the deÔ¨Åned prior and context losses at hand, the
corrupted image can be mapped to the closest z in the latent
representation space, which we denote ÀÜz. z is randomly
initialized and updated using back-propagation on the total
loss given in Eq. (2). Fig. 3 (b) shows for one example that
z is approaching the desired solution on the latent image
After generating G(ÀÜz), the inpainting result can be easily obtained by overlaying the uncorrupted pixels from the
input. However, we found that the predicted pixels may not
exactly preserve the same intensities of the surrounding pixels, although the content is correct and well aligned. Poisson blending is used to reconstruct our Ô¨Ånal results.
The key idea is to keep the gradients of G(ÀÜz) to preserve
image details while shifting the color to match the color in
the input image y. Our Ô¨Ånal solution, ÀÜx, can be obtained
ÀÜx = arg min
x ‚à•‚àáx ‚àí‚àáG(ÀÜz)‚à•2
s.t. xi = yi for Mi = 1,
where ‚àáis the gradient operator. The minimization problem contains a quadratic term, which has a unique solution
 . Fig. 5 shows two examples where we can Ô¨Ånd visible
seams without blending.
Figure 5. Inpainting with and without blending.
3.4. Implementation Details
In general, our contribution is orthogonal to speciÔ¨Åc
GAN architectures and our method can take advantage of
any generative model G. We used the DCGAN model architecture from Radford et al. in the experiments. The
generative model, G, takes a random 100 dimensional vector drawn from a uniform distribution between [‚àí1, 1] and
generates a 64√ó64√ó3 image. The discriminator model, D,
is structured essentially in reverse order. The input layer is
an image of dimension 64 √ó 64 √ó 3, followed by a series of
convolution layers where the image dimension is half, and
the number of channels is double the size of the previous
layer, and the output layer is a two class softmax.
For training the DCGAN model, we follow the training
procedure in and use Adam for optimization. We
choose Œª = 0.003 in all our experiments. We also perform data augmentation of random horizontal Ô¨Çipping on
the training images. In the inpainting stage, we need to Ô¨Ånd
ÀÜz in the latent space using back-propagation. We use Adam
for optimization and restrict z to [‚àí1, 1] in each iteration,
which we observe to produce more stable results. We terminate the back-propagation after 1500 iterations. We use
the identical setting for all testing datasets and masks.
4. Experiments
In the following section we evaluate results qualitatively
and quantitatively, more comparisons are provided in the
supplementary material.
4.1. Datasets and Masks
We evaluate our method on three dataset: the CelebFaces
Attributes Dataset (CelebA) , the Street View House
Numbers (SVHN) and the Stanford Cars Dataset .
The CelebA contains 202, 599 face images with coarse
alignment .
We remove approximately 2000 images
from the dataset for testing. The images are cropped at the
center to 64 √ó 64, which contain faces with various viewpoints and expressions.
The SVHN dataset contains a total of 99,289 RGB images of cropped house numbers. The images are resized to
64 √ó 64 to Ô¨Åt the DCGAN model architecture. We used
the provided training and testing split. The numbers in the
images are not aligned and have different backgrounds.
The Stanford Cars dataset contains 16,185 images of 196
classes of cars. Similar as the CelebA dataset, we do not use
any attributes or labels for both training and testing. The
cars are cropped based on the provided bounding boxes and
resized to 64 √ó 64. As before, we use the provided training
and test set partition.
We test four different shapes of masks: 1) central block
masks; 2) random pattern masks in Fig. 1, with approximately 25% missing; 3) 80% missing complete random masks; 4) half missing masks (randomly horizontal or
vertical).
4.2. Visual Comparisons
Comparisons with TV and LR inpainting. We compare
our method with local inpainting methods. As we already
Figure 6. Comparisons with local inpainting methods TV and LR
inpainting on examples with random 80% missing.
Figure 7. Comparisons with nearest patch retrieval.
showed in Fig. 1, local methods generally fail for large
missing regions. We compare our method with TV inpainting and LR inpainting on images with small random holes. The test images and results are shown in Fig. 6.
Due to a large number of missing points, TV and LR based
methods cannot recover enough image details, resulting in
very blurry and noisy images. PM cannot be applied to
this case due to insufÔ¨Åcient available patches.
Comparisons with NN inpainting. Next we compare our
method with nearest neighbor (NN) Ô¨Ålling from the training
dataset, which is a key component in retrieval based methods . Examples are shown in Fig. 7, where the misalignment of skin texture, eyebrows, eyes and hair can be
clearly observed by using the nearest patches in Euclidean
distance. Although people can use different features for retrieval, the inherit misalignment problem cannot be easily
solved . Instead, our results are obtained automatically
without any registration.
Comparisons with CE. In the remainder, we compare our
Table 1. The PSNR values (dB) on the test sets. Left/right results
are by CE /ours.
Masks/Dataset
result with those obtained from the CE , the state-ofthe-art method for semantic inpainting. It is important to
note that the masks is required to train the CE. For a fair
comparison, we use all the test masks in the training phase
for the CE. However, there are inÔ¨Ånite shapes and missing
ratios for the inpainting task. To achieve satisfactory results
one may need to re-train the CE. In contrast, our method can
be applied to arbitrary masks without re-training the network, which is according to our opinion a huge advantage
when considering inpainting applications.
Figs. 8 and 9 show the results on the CelebA dataset with
four types of masks. Despite some small artifacts, the CE
performs best with central masks. This is due to the fact that
the hole is always Ô¨Åxed during both training and testing in
this case, and the CE can easily learn to Ô¨Åll the hole from the
context. However, random missing data, is much more dif-
Ô¨Åcult for the CE to learn. In addition, the CE does not use
the mask for inference but pre-Ô¨Åll the hole with the mean
color. It may mistakenly treat some uncorrupted pixels with
similar color as unknown. We could observe that the CE has
more artifacts and blurry results when the hole is at random
positions. In many cases, our results are as realistic as the
real images. Results on SVHN and car datasets are shown
in Figs. 10 and 11, and our method generally produces visually more appealing results than the CE since the images
are sharper and contain fewer artifacts.
4.3. Quantitative Comparisons
It is important to note that semantic inpainting is not trying to reconstruct the ground-truth image. The goal is to Ô¨Åll
the hole with realistic content. Even the ground-truth image is one of many possibilities. However, readers may be
interested in quantitative results, often reported by classical
inpainting approaches. Following previous work, we compare the PSNR values of our results and those by the CE.
The real images from the dataset are used as groundtruth
reference. Table 1 provides the results on the three datasets.
The CE has higher PSNR values in most cases except for
the random masks, as they are trained to minimize the mean
square error. Similar results are obtained using SSIM 
instead of PSNR. These results conÔ¨Çict with the aforementioned visual comparisons, where our results generally yield
to better perceptual quality.
We investigate this claim by carefully investigating the
errors of the results. Fig. 12 shows the results of one exam-
Figure 8. Comparisons with CE on the CelebA dataset.
ple and the corresponding error images. Judging from the
Ô¨Ågure, our result looks artifact-free and very realistic, while
the result obtained from the CE has visible artifacts in the
reconstructed region. However, the PSNR value of CE is
1.73dB higher than ours. The error image shows that our
result has large errors in hair area, because we generate a
hairstyle which is different from the real image. This indi-
Figure 9. Comparisons with CE on the CelebA dataset.
cates that quantitative result do not represent well the real
performance of different methods when the ground-truth is
not unique. Similar observations can be found in recent
super-resolution works , where better visual results
corresponds to lower PSNR values.
For random holes, both methods achieve much higher
PSNR, even with 80% missing pixels.
In this case, our
Figure 10. Comparisons with CE on the SVHN dataset.
method outperforms the CE. This is because uncorrupted
pixels are spread across the entire image, and the Ô¨Çexibility
of the reconstruction is strongly restricted; therefore PSNR
is more meaningful in this setting which is more similar to
the one considered in classical inpainting works.
Figure 11. Comparisons with CE on the car dataset.
4.4. Discussion
While the results are promising, the limitation of our
method is also obvious. Indeed, its prediction performance
strongly relies on the generative model and the training procedure. Some failure examples are shown in Fig. 13, where
our method cannot Ô¨Ånd the correct ÀÜz in the latent image
CE Error √ó 2
Ours Error √ó 2
Figure 12. The error images for one example. The PSNR for context encoder and ours are 24.71 dB and 22.98 dB, respectively.
The errors are ampliÔ¨Åed for display purpose.
Figure 13. Some failure examples.
The current GAN model in this paper works
well for relatively simple structures like faces, but is too
small to represent complex scenes in the world. Conveniently, stronger generative models, improve our method in
a straight-forward way.
5. Conclusion
In this paper, we proposed a novel method for semantic
inpainting. Compared to existing methods based on local
image priors or patches, the proposed method learns the representation of training data, and can therefore predict meaningful content for corrupted images. Compared to CE, our
method often obtains images with sharper edges which look
much more realistic. Experimental results demonstrated its
superior performance on challenging image inpainting examples.
Acknowledgments:
This work is supported in part by
IBM-ILLINOIS Center for Cognitive Computing Systems
Research (C3SR) - a research collaboration as part of the
IBM Cognitive Horizons Network. This work is supported
by NVIDIA Corporation with the donation of a GPU.