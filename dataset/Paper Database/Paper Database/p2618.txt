Vision Research 38 2507–2519
2D observers for human 3D object recognition?
Zili Liu a,*, Daniel Kersten b
a NEC Research Institute, 4 Independence Way, Princeton, NJ 08540, USA
b Department of Psychology, Uni6ersity of Minnesota, Minneapolis, MN 55455, USA
Received 5 February 1997; received in revised form 20 January 1998
In human object recognition, converging evidence has shown that subjects’ performance depends on their familiarity with an
object’s appearance. The extent of such dependence is a function of the inter-object similarity. The more similar the objects are,
the stronger this dependence will be and the more dominant the two-dimensional (2D) image-based information will be. However,
the degree to which three-dimensional (3D) model-based information is used remains an area of strong debate. Previously the
authors showed that all models with independent 2D templates that allowed 2D rotations in the image plane cannot account for
human performance in discriminating novel object views . Here the authors derive an analytic formulation of a Bayesian model
that gives rise to the best possible performance under 2D afﬁne transformations and demonstrate that this model cannot account
for human performance in 3D object discrimination. Relative to this model, human statistical efﬁciency is higher for novel views
than for learned views, suggesting that human observers have used some 3D structural information. © 1998 Elsevier Science Ltd.
All rights reserved.
Keywords: Afﬁne transformation; Object recognition; Object representation; Ideal observer; Template matching
1. Introduction
A basic component in three-dimensional (3D) object
recognition is a process that matches the input stimulus
to stored object representations in memory. The search
for a match, when viewpoint invariant features (e.g.
color or material) are absent, must be based on the
object shape. A major challenge for object recognition
is to understand how potential matches are veriﬁed
despite shape variations in the image due to rotations in
viewpoint.
Empirical evidence has shown that human object
recognition strongly depends on familiar views, a result
particularly pronounced for structurally similar objects
 . These studies leave open, however, the question
of how much 3D information contributes to object
recognition. In contrast, empirical evidence in support
of 3D model-based recognition suggests that object
recognition is viewpoint dependent only when major
object components disappear and new components
come into view, for structurally dissimilar objects 1.
The studies do not, however, resolve the possibility that
since the objects are dissimilar, a two-dimensional (2D)
based qualitative representation already sufﬁces to distinguish an object from the rest within a large range of
viewpoint change.
1.1. View-approximation models
To clarify what the authors mean by 2D versus 3D
information, let us consider one class of models for
shape-based recognition, which the authors refer to as
view-approximation models. (The authors postpone
consideration of the more powerful view-combination
models to the Discussion. See for a general discussion of various classes of object recognition models.)
View-approximation models assume that views are arbitrary samples, whose only link is a common label (e.g.
the name of the object). These views have come to be
associated with each other through experience. Thus,
such models are inherently viewpoint dependent. For
example, assume that an object is represented by two
independent views. The task is to decide whether a
novel view belongs to the object. The strong version of
view-approximation maintains that in order to recog-
* Corresponding
 .
1 But see ; .
0042-6989/98/$19.00 © 1998 Elsevier Science Ltd. All rights reserved.
PII: S0042-6989(98)00063-7
Z. Liu, D. Kersten / Vision Research 38 2507–2519
nize a novel view, a similarity measure is calculated
independently between this view and each of the two
stored views . Recognition is a function of these
measurements. The simplest function is the nearest
neighbor scheme, where a match is based on the closest
view in memory. A more sophisticated scheme is the
Bayes classiﬁer that combines the evidence over the
collection of views optimally.
A more ﬂexible version of view-approximation is to
allow, in addition to combinations of the similarities,
transformations on each stored view. For example, a
novel 2D view can be translated and rotated in the 2D
image plane before matching with each of the stored
2D views. showed that human observers exceeded
even the optimal model that used this strategy (which
the authors referred to as a ‘2D/2D ideal observer’ (2D
model/2D input)). Thus, the results excluded both the
strong and more ﬂexible models above. The authors did
not, however, exclude view-approximation models with
even more ﬂexible transformations. One example of the
2D/2D observer class is to allow 2D afﬁne transformations to each of the templates before similarity computations. A 2D afﬁne transformation is any linear
transformation that includes translation, rotation, scaling and stretching in the image plane, which the authors
transformation exactly characterizes 3D rotations of
2D planar objects under orthographic projection (see
 for a summary2) and approximates, in a small range,
depth rotations of 3D objects . The primary purpose of this paper is to test whether 2D afﬁne transformations account for human performance for 3D object
recognition.
1.2. Distinguishing models experimentally: the ideal
obser6er approach
The authors approach is to ﬁrst construct a 2D afﬁne
model that gives rise to the best possible performance,
which the authors call the 2D afﬁne ideal observer. The
authors then test whether this ideal observer accounts
for human performance or not. If not, the authors can
reject this ideal observer and all the models suboptimal
to it, as models for human object recognition.
In the following, the authors ﬁrst derive the 2D afﬁne
ideal observer. For quantitative comparison, the authors describe three additional models that have been
proposed in the literature. First, the authors introduce
the model by that matches two point sets using 2D
afﬁne transformations. This model provides a particularly simple approximation to the 2D afﬁne ideal observer. Second, the authors introduce a model by 
that recognizes a 2D image of a set of 3D points from
a single 2D template. Third, in order to compare with
the Generalized Radial Basis Functions (GRBF) model
in , the authors present an improved GRBF model
that adjusts the variance of its radial basis (Gaussian)
functions to search for the best result. Finally, the
authors compare human performance with these models in a 3D object discrimination task . The task
requires observers to discriminate which of two objects
is more similar to a learned object. The task provides a
straightforward way of measuring the efﬁciency of the
human matching process for novel object views. The
authors use wire objects as the stimuli because they are
the simplest objects that obey the assumptions of these
2. The computational models
In order to provide a clear context of what the
computational models are supposed to do, the authors
brieﬂy describe the task that both the human observers
and models face . The objects are bent wires whose
vertex feature points are assumed visible from all viewing angles with known correspondence (i.e. the feature
points are labeled). An image of an object is represented by the (x, y) coordinates of its feature points.
The object (termed prototype) is ﬁrst learned from a
number of its images. Then a pair of objects are
generated from this prototype by adding independent
3D positional Gaussian noise at the feature points. One
object is called the target, whose Gaussian noise has a
ﬁxed variance. The other is called the distractor, whose
variance is always larger. The task is to choose from the
two an object that is more similar, in Euclidean distance of the feature points, to the prototype object.
2.1. The 2D afﬁne ideal obser6er
Here the authors summarize the derivation of the
Bayesian 2D afﬁne ideal observer (details in Appendix).
Let us ﬁrst consider the case of only one 2D template.
Assume that a template T and an input stimulus image
S are represented as:
A 2D afﬁne transformation to the template T is
2 When a planar object is rotated in depth under orthographic
projection, the object is scaled in the image plane along the direction
perpendicular to the rotational axis. A 2D afﬁne transformation can
also scale a 2D image along one direction. That is why 2D afﬁne
transformation can exactly characterize any 3D rotation of a 2D
planar object.
Z. Liu, D. Kersten / Vision Research 38 2507–2519
with XT={a, b, c, d, tx, ty}  (−
). The authors
assume that the stimulus image S is obtained by ﬁrst
applying a 2D afﬁne transformation to the template T,
then adding independent Gaussian noise N(0, sI2n),
where I2n is a 2n×2n identity matrix. Therefore the
probability P(S
T, X)=P(N=S−(A T+Tr)) Hence,
T, X) P(X) dX
(2ps2)2n/2& dXP(X) exp
where P(X) is the prior probability distribution of XT.
Assume that
(2pg2)3 exp
−(X−X0)T(X−X0)
=(1,0,0,1,0,0),
which means that the prior probability distribution of a
2D afﬁne transformation to a template is a Gaussian
centered at the identity transformation. Given that the
six variables
(a,b,c,d,tx,ty)=XT(−
are independent of each other, the authors obtain the
following by integration:
(2ps2)n−3g6(n+g−2)det(Q%) exp:
var(yS)+x¯ 2+y¯ 2
ng2+1−2s2/n;
−2g−2−tr(K*TQ(Q%)−1QK*)
QK*=QK+g−2I2
Note that under the assumption of the Gaussian
prior probability distribution P(X)=N(X0, gI6), g is
the only free parameter. When g0, the prior becomes
a d-function and no transformation is allowed to the
template T. Only the template T, not even its 2D
rotations in the image plane are allowed in the matching process. Since this might be over-restrictive, the
authors assume that the 2D rotations of the templates
are automatically available to the ideal observer. Therefore, both g and the number of 2D rotations of the
template will be explored to search for the optimal
performance. The authors also assume that the ideal
observer knows the sequence of the feature points, but
not which is the head and which the tail, so both
possibilities will be considered.
2.2. 2D afﬁne nearest neighbor model
In the above derivation, the prior probability of the
2D afﬁne transformations is assumed to be Gaussian
centered at each of the learned templates and their 2D
rotations. Although the authors will search for the
optimal performance with this prior, it is informative to
know the ideal observer’s performance when it only
uses the 2D afﬁne transformation that brings the stimulus and template to the closest possible match. In other
words, the authors will consider the nearest neighbor
solution for the problem, for which have an analytic derivation.
Their model assumes that the stimulus and template
are represented by 2D point features of known correspondence. The similarity measure between S and T is
deﬁned by the smallest Euclidean distance between the
two 2×n matrices (xi, yi)i=1
after both images are
normalized to the same scale (a point that will be
returned to). Image S can undergo an arbitrary 2D
similarity
transformation
(rotation,
translation
scaling) and image T an arbitrary 2D afﬁne transformation. They showed that the smallest squared Euclidean
distance D2 between the two images is:
D2(S, T)=1−tr(S+S ·TTT)
where tr[·] is the trace of a matrix, S+ =ST(SST)−1 is
the pseudo-inverse of S and
2=tr[TTT].
Only the Euclidean distance D, not the probability, is
deﬁned between two images in this nearest neighbor
model. Thus, when there are multiple templates, either
the summation of the D2 themselves, or the summation
of exp(−D2/2s2) can be used for the similarity measure. The authors will use both in this paper and report
the one that gives rise to the better performance.
2.3. GRBF model
The authors also simulate an improved version of the
GRBF model originally presented in . In , the
model stored a set of 2D images {Ti} of the prototype
object. When a pair of stimulus images {S1, S2} were
presented, the model chose the image with a larger
probability
evaluation
Z. Liu, D. Kersten / Vision Research 38 2507–2519
where {ci} were obtained optimally when the learned
templates themselves were used as input stimuli.
Although s is the right number to use for the learned
views, it is not necessarily the best choice for the novel
views, since the model only approximates a novel view
using weighted Gaussian summations from the learned
views. In this paper, the authors will search for the
optimal value of s for the novel views by hand picking
that gives rise to the best performance, for each individual object.
2.4. The 3D/2D polynomial model
An important theoretical question in object recognition is the amount of available information in images,
from which the 3D structure of an object can be
determined. This is the so called shape-from-views
problem. The approach dates back to the classic work
of the four-points-three-views theorem of structurefrom-motion, in which showed that three images of
four non-coplanar labeled points under orthographic
projection determine the 3D structure of the four points
(with a depth reversion ambiguity). The authors now
brieﬂy review the state of the art of the shape-fromviews problem before introducing the model that is
closely related to the current study. To begin with, 
further showed that if a fourth image is available, it can
be veriﬁed as coming from the same object or not.
While this assumes that the object structure is rigid, 
showed that the rigidity of a labeled e-point non-planar
structure can be veriﬁed from three images by checking
a 6×n matrix. If the matrix has a full rank, then the
structure is non-rigid, otherwise it is.
When only two images S and T are available, the
authors can write a matrix (assuming no translation):
If rank (M)=3, then S and T are from the same
object and the object can undergo arbitrary 3D afﬁne
transformations. If rank (M)\3, then the two images
are not from the same object . In the recognition scheme in , for example, the two stored images
serve as the basis for recognition. When a third image
is available, it can be veriﬁed as coming from the same
object (the third image can be obtained by applying a
certain afﬁne transformation to the object before orthographic projection) or not.
Bennett et al. have proposed a speciﬁc
implementation for object veriﬁcation with two images
(one stored image T, one input image S). This is
equivalent to checking whether the two images are
consistent with an object that can undergo arbitrary 3D
afﬁne transformations. This model is appealing since its
implementation is simple (only a polynomial calculation), it is speciﬁcally proposed as a candidate model
for human object recognition and its proposed Gaussian noise model is closely related to the study in this
It starts with four points in each image, one point is
at the origin (0, 0) to handle the translation. The images
S and T belong to the same (3D afﬁne) object if and
R=determinantÃ
where di, j=xi,Sxj,S+yi,Syj,S−xi,Txj,T−yi,Tyj,T. Similar
to the theorem in , this recognition polynomial does
not specify how the polynomial changes its value when
the feature points are perturbed with noise. Bennett et
al. suggest using R2 as a measure of the
goodness of ﬁt between the two images. When an image
has more than four points, the polynomial is divided
into subsets of four points each and that the overall
similarity is the summation: R(x1,·)2+R(x2,·)2+…
When an object has multiple stored templates, the
above summation will also be across these templates.
3. Experimental methods
The experimental paradigm is described in detail in
 . The authors review its basics here. In a training
phase, a subject ﬁrst learned a 3D prototype wire object
from 11 viewpoints under orthographic projection with
monocular viewing. For the subsequent testing phase,
two objects were created by adding independent 3D
positional Gaussian noise to the vertices of the learned
prototype. The variance of the noise added to one
object, called the target, was ﬁxed and that added to the
other, the distractor, was always larger. The two objects
were presented to the subject from the same viewpoint.
The task in the testing phase was to pick the object that
was more similar in shape and size to the learned
prototype3.
distractor
varied using a staircase procedure in order to ﬁnd
the observer’s threshold at the 75% correct. The smaller
3 In order to deﬁne a proper probability measure so that an ideal
observer can be provably optimal in the task, the authors deﬁne
image similarity as the Euclidean distance between their vertex coordinates. Therefore, the more two images differ in size, the less similar
they are. An ideal observer exploits this, therefore it is only fair to
allow human subjects the same.
Z. Liu, D. Kersten / Vision Research 38 2507–2519
this threshold is, the better the performance will be.
Two conditions were randomly intermixed from trial to
trial: learned views—the two objects were presented
from one of the 11 learned viewpoints; and novel
views—the objects were presented from an arbitrary
viewpoint in 3D rotation. The thresholds for these two
conditions were tracked in parallel.
Four classes of objects were used. They were, in the
increasing order of object regularity (Fig. 1): Balls—
ﬁve balls randomly arranged in 3D; Irregular—the ﬁve
balls were connected by four cylinders into a chain;
Symmetric—the above irregular object were bilaterally
symmetric; and V-Shaped—the two cylinders on each
side of the above symmetric object were collinear, so
the object itself became planar and symmetric. When
independent noise was added to perturb the positions
of the balls, the cylinders connecting them were adjusted accordingly. So the V-Shaped objects were no
longer perfectly planar, symmetric and collinear, nor
were the symmetric objects precisely symmetric. There
were three objects in each class. Three naive subjects
participated in the experiment.
The four models described above were given the
same task as the subjects. Each object’s image was
represented by an ordered sequence of the (x, y) coordinates of the wire vertices. Only the direction of the
ordered sequence was assumed unknown, which is
equivalent to a reﬂection ambiguity in correspondence
between the feature points. The standard deviation of
the Gaussian noise added to the target object was
assumed known by the four models. The authors simulated the discrimination performance of the four models, using the same objects as seen by the human
observers.
4. Predictions
For the Balls, Irregular and Symmetric objects, the
authors expect that both human and the 2D afﬁne ideal
observer will perform better for the learned than for the
novel views. In fact, for the learned views, the 2D afﬁne
ideal observer is the true ideal observer and human
observers are necessarily less efﬁcient due to internal
noise. The question is, are they relatively more efﬁcient
for the novel views? In other words, are humans relatively better for the novel views than for the learned
views as compared with the 2D afﬁne ideal observer? A
‘yes’ answer implies that humans generalize from the
learned to novel views better than the 2D afﬁne ideal
observer does. It further implies the 2D afﬁne ideal
observer cannot completely account for the human
performance.
The authors are particularly interested in the Irregular and Symmetric objects and any differences between
them. For the Balls objects, the authors expect that
human subjects’ performance will be poor. For the
V-Shaped objects, the 2D afﬁne ideal is the true ideal
observer in the sense that it accurately models 3D
viewpoint variations for planar objects (see footnote 2).
These objects therefore serve as a control to verify that
the ideal observer is doing the right thing.
If the performance of human observers relative to
that of the 2D afﬁne ideal observer (deﬁned as the
statistical efﬁciency ) is better for the novel views
than for the learned views, then humans must have
used a better recognition strategy than the 2D afﬁne
ideal. The reason is that the afﬁne model only approximates the learned views, since the objects are not
planar. If humans use a strategy of 2D afﬁne transformations with independent 2D templates, then their
performance relative to the 2D afﬁne ideal for the novel
views must be less than or equal to that for the learned
Due to the internal noise in the human visual system,
the statistical efﬁciency for the learned views will be
necessarily below 100%. Therefore, the statistical efﬁciency for the novel views may also be below 100%,
even when it is greater than for the learned views. As
long as the efﬁciency is higher for the novel views than
for the learned views, the human observers have either
employed a 2D transformation to the templates more
complex than 2D afﬁne transformations, or have not
treated the templates as independent but effectively
Fig. 1. Samples of the experimental stimuli. Top to bottom: Balls,
Irregular, Symmetric and V-Shaped. 2507–2519
Fig. 2. Performance of the 2D afﬁne ideal observer as a function of g—the standard deviation of the Gaussian prior probability distribution,
without additional rotational copies of any template (m=1). The authors plot the Balls and Irregular objects together since they are treated the
same by the model. The error bars are the standard errors (some are too small to be visible).
combined them to reconstruct the (partial) 3D structure of the object (view-combination, ).
The authors will employ a conservative (worst case)
test for the 2D afﬁne ideal and the GRBF model in
the sense that the authors will select parameters that
give rise to the best performance for the novel views.
The models’ performance for the learned views will
be obtained with the parameters optimal for the
novel views. (The parameters optimal for the novel
views usually do not yield the best performance for
the learned views.) In this way, the statistical efﬁciency for the novel views will be the lowest possible.
This makes it more difﬁcult to satisfy the hypothesis
that the statistical efﬁciency for the novel views is
higher than for the learned views. Consequently, if
such a hypothesis is supported from the data, it will
be evidence that the human observers use more 3D
knowledge than implicit in the 2D afﬁne transformations. The evidence will be strong in the sense that
the best performance for the novel views is obtained
by the experimenters, rather than by the models
themselves. This is because it is difﬁcult for the models to automatically search for the best performance
when the two viewing conditions are randomly intermixed and no feedback is provided.
Finally, the polynomial model predicts the
same performance for the learned and novel views.
This is because once the learned template is stored as
the coefﬁcients for the polynomial, the polynomial’s
mean value and variance are completely determined
by the (x, y) coordinates of the feature points in the
input image, learned and novel views alike. The variance associated with coding these (x, y) values can be
assumed equal. Therefore, the model predicts that the
human performance is viewpoint independent. It cannot account for human performance if human performance is different for the learned and novel views.
5. Results
5.1. The 2D afﬁne ideal obser6er
Simulations were conducted to carry out the task for
each of the 12 objects, learned and novel views respectively, with 2000 trials for each condition. These simulations were conducted for different g values and different
numbers of 2D rotated copies (m) of each template
(g=0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 10, 20, 30, 40, 50, 60,
70, 80, 90, 100; m=1, 3, 5, 7, 9, 10, 11, 13, 15, 17, 19,
20, 40, 60, 80). It turns out that the smallest m (=1) and
sufﬁciently large g values give rise to the best performance for the novel views (Figs. 2 and 3). The authors
selected the model’s best performance for the novel views
(Balls: m=1, g=10; Irregular and Symmetric: m=1,
g=10; V-Shaped: m=1,g=100). Fig. 4 shows the
statistical efﬁciency of the human observers relative to
this 2D afﬁne ideal observer (its derivation is in ). The
authors conducted the Wilcoxon order test for the
18 pairs of matched comparisons between the learned
and novel views, for the Irregular and Symmetric objects
(three objects each, three observers). The authors found
that the efﬁciency for the novel views is statistically
higher than for the learned views (PB0.02; T=38,
N=18, z=2.07). This suggests that 2D afﬁne transformations cannot account for the human performance.
There is a signiﬁcant difference in statistical efﬁciency
across the four types of objects (F(3,6)=18.25, PB
0.002). Of particular interest is whether there is a
difference between the Irregular and Symmetric objects.
The efﬁciency for the Symmetric objects is higher than
for the Irregular objects (t(2)=4.10, PB0.03). This
suggests that the subjects may indeed have exploited
symmetry in the task. This implies that subjects may take
advantage of symmetry in 3D, since the 2D image of a
novel view of a Symmetric object is almost always
asymmetric.
Z. Liu, D. Kersten / Vision Research 38 2507–2519
Fig. 3. Performance of the 2D afﬁne ideal observer as a function of g, for both m=1 and 5 For clarity, the error bars are not shown.
In summary, the results that the human observers
were more efﬁcient for the novel than for the learned
views and for the Symmetric than for the Irregular
objects imply that the 2D afﬁne ideal observer may not
account for human performance. Three-dimensional
structural information may have been exploited by the
human visual system.
5.2. The remaining models
Fig. 5 shows the performance of the human observers, the 2D afﬁne ideal observer, the 2D afﬁne nearest
neighbor model (WW in short), the GRBF model
 and the 3D/2D polynomial model . The performance of the 2D afﬁne nearest neighbor model was
obtained by taking the summation of exp(−D2(Ti,
S)/2s2), rather than D2(Ti, S) directly4. The, suboptimal performance of the 2D afﬁne nearest neighbor
model is in part due to the fact that the model normalizes the size of each image ﬁrst before computing the
Euclidean distance. Thus the size information is not
used at all, whereas in the studys task it is informative.
The larger the noise is, the more likely the size is larger.
This problem, however, does not apply to the rest of
the models.
The authors make the following remarks. (1) The
3D/2D polynomial model’s performance was very sensitive to the correspondence ambiguity, its threshold at
least doubled when the correspondence is wrong. In
contrast, the 2D afﬁne model was much less sensitive to
this ambiguity. This is because in the afﬁne nearest
neighbor model a normalization procedure is built in to
align the two images by 2D linear transformations. In
the 3D/2D polynomial model, however, no normalization procedure is available. When the correspondence is
wrong, the model treats the input image as from a
completely different object. This yields a poor polynomial evaluation and leads to a chance performance for
the model at many instances, which was documented by
the studys simulations. For this reason, the statistical
efﬁciency will be plotted for the 2D afﬁne nearest
neighbor model with the correspondence ambiguity and
the 3D/2D polynomial model with only the exact correspondence. (2) As expected, the 2D afﬁne nearest neighbor model’s threshold performance for the learned
views was better than for the novel views, whereas the
polynomial model’s performance was about the same
for both learned and novel views. (3) The 2D afﬁne
Fig. 4. Statistical efﬁciency of the human observers relative to the 2D
afﬁne ideal observer for the four types of objects. The error bars are
standard errors between the three observers’ scores. (Since the
Wilcoxon analysis between the learned and novel view conditions is
for matched pair comparison for each object and within each observer, the error bars cannot directly reﬂect the variance in the
analysis. This applies to similar analysis below).
4 Using the metric of exp(−D2(Ti, S)/2s2), the average thresholds
for the ﬁrst three object types were 0.46 and 0.90 cm, for the learned
and novel views, respectively. They were 0.37 cm for the V-shaped
object, for both learned and novel views. Using the D2(Ti, S) metric,
they were 1.06, 1.07, 0.37 and 0.37 cm. So the ﬁrst metric yields better
performance.
Z. Liu, D. Kersten / Vision Research 38 2507–2519
Fig. 5. Discrimination threshold of the human observers for the learned and novel views, for the 2D afﬁne ideal observer, the model with the
two way correspondence ambiguity (WW2way), the GRBF model, the 3D/2D polynomial model with the exact correspondence (BHP) and
with the two way ambiguity correspondence (BHP2way). The threshold is deﬁned as the standard deviation of the Gaussian noise added to the
distractor object at 75% correct performance, for learned and novel views, respectively.
nearest neighbor model’s performance for the V-Shaped
objects was identical for the learned and novel views, as
it should.
Fig. 6 shows human observers’ statistical efﬁciency
relative to the 2D afﬁne nearest neighbor model. For
the Irregular and Symmetric objects, the efﬁciency for
the novel views is greater than for the learned views
(Wilcoxon test, Irregular: T=7, z=1.84, PB0.05;
Symmetric: T=0, z=2.66, PB0.005). This means that
the 2D afﬁne nearest neighbor matching cannot account for human data.
Fig. 6 also shows the human efﬁciency relative to the
GRBF model, whose performance was obtained by
hand picking the Gaussian variance that gives rise to
the best performance for the novel views for each
individual object. For all types of objects, the efﬁciency
for the novel views was greater than for the learned
views (Wilcoxon test, Balls: T=8, z=1.72, PB0.05;
Irregular: T=9, z=1.60, PB0.05; Symmetric or V-
Shaped: T=0, z=2.66, PB0.005). This means that
the GRBF model, even when the standard deviation of
its basis functions was allowed to (uniformly) vary to
search for the best performance, still cannot account
for the human performance.
Fig. 7 shows human observers’ statistical efﬁciency
relative to the polynomial model with exact correspondence. The absolute values of the efﬁciencies are high,
but the overall pattern of the efﬁciency is similar to the
3D/2D (3D model/2D input) and 3D/3D (3D model/
3D input) ideal observers (in , ﬁg. 8, p. 561). The
higher efﬁciencies for the learned views than for the
novel views with the Balls, Irregular and Symmetric
objects suggest that the 3D/2D polynomial recognition
model, which predicts equal efﬁciencies, cannot account
for human performance.
6. Discussion
The authors have derived an analytic formulation of
a Bayesian model that gives rise to the best possible
performance under 2D afﬁne transformations. By using
this model’s performance as a benchmark for human
performance, the authors have shown that the 2D afﬁne
ideal observer fails to account for human 3D object
discrimination. Relative to this model, human statistical
efﬁciency is higher for novel views than for learned
views. If the statistical efﬁciencies had been 100% for
both learned and novel views, the authors could have
concluded with absolute certainty that the mechanisms
used by human observers in this task is equivalent to a
2D afﬁne observer. To what extent is it likely that a 2D
afﬁne observer (not ideal) could account for human
performance? Excluding this possibility rests on at least
ﬁve assumptions.
6.1. 2D obser6ers for human 3D object recognition?
First, the conclusion that the observers did not use a
2D afﬁne strategy, based on comparison of efﬁciencies
between the novel and learned views, depends on the
way in which internal noise in the visual system operates. For example, imagine that human observers are
2D afﬁne ideal observer plus additive internal noise N.
Then, according to Eqn. (E9) in , statistical efﬁciency
E is the variance difference between the distractor and
the target for the ideal observer DsI over that for the
human observer DsH=DsI+N. It is reasonable to
assume that N is the same for the novel views (n) and
the learned views (l). Consequently,
Z. Liu, D. Kersten / Vision Research 38 2507–2519
Fig. 6. Statistical efﬁciency of the human observers relative to the 2D afﬁne nearest neighbor model and the GRBF model.
The authors have
This means that additive noise in itself increases the
ratio of the efﬁciency for the novel views over the
learned views. Although the equivalent internal noise
thus derived is inconsistent between learned and novel
views and between object types (Balls: learned views
0.75 cm2, novel views 2.19 cm2; Irregular: 0.22, 0.45;
Symmetric: 0.14, 6.45), this only means that addition is
unlikely the correct noise model. The authors cannot,
however, exclude all possible ways in which internal
noise increases the relative efﬁciency for the novel
The second assumption is that each of the six variables in the afﬁne transformation obeys a Gaussian
prior probability distribution with the same variance. It
is a problem because 2D afﬁne transformation only
approximates a 3D object rotation, therefore no ‘correct’ prior distributions ever exist. The choice is only a
matter of convenience. On the other hand, however, the
authors found that the performance of the 2D afﬁne
ideal observer stabilizes at optimal values so long as the
variance of the Gaussian distributions are sufﬁciently
large. It appears therefore that the speciﬁcs of the prior
probability distribution are not critical.
The third assumption is that subjects did not learn
the novel views during testing. If they did, these views
could be used as additional 2D templates. Such learning would improve their performance for the novel
views more than for the learned views. In contrast, the
2D afﬁne ideal observer has only the ﬁxed set of 2D
templates. The authors considered this possibility in 
by creating an additional 2D template for the 2D/2D
ideal observer (with 2D rotations) after each test trial.
The template was the average of the two test images
and was a close approximation to a view of the prototype object since the two test images were from the
same viewpoint. The incorporation of this learning
improved the 2D/2D ideal observer’s performance, but
the efﬁciencies for the novel views of the Symmetric
and V-Shaped objects were still above 100%. The authors did not simulate the 2D afﬁne ideal observer with
learning in this paper, but given the already small
efﬁciency difference between the novel and learned
views, the authors expect that the 2D afﬁne ideal
observer with learning could match human performance. In fact, however, when the authors tested the
Fig. 7. Statistical efﬁciency of the human observers relative to the
polynomial recognition model.
Z. Liu, D. Kersten / Vision Research 38 2507–2519
third assumption, an analysis of the experimental data
did not show learning by the human subjects .
The fourth assumption is that shaded images did not
contribute an unfair advantage to the human observers.
The human subjects had shading and occlusion information in addition to the vertex positions, whereas the
models have available only the (x, y) vertex coordinates. The authors counted in that the number of
occlusion events was about the same for the learned
and novel views and also argued that the shading
information should be about the same for the learned
and novel views. Therefore, it is unlikely that this
additional image information alone should be responsible for any differential effect between the learned and
novel views. One could argue that a differential effect
can be obtained if subjects use only the vertex coordinates for the learned views and use everything possible
for the novel views. The authors think that this scenario
is possible but unlikely given that the learned and novel
views in the human experiment were randomly intermixed. The authors are currently using silhouette images and thin wire objects to directly address this issue.
A ﬁfth assumption is that the class of objects used in
this study is representative of typical visual tasks that
require ﬁne shape discriminations. The objects were
notably peculiar in their lack of substantial occlusions.
It is true that previous studies used exactly the same
type of objects to argue for a 2D template-based approach , therefore it is best to use the same objects
to test the claim. But it remains a challenge to all
computational studies to address everyday object recognition when occlusion is commonplace. On the other
hand, the way in which an object is represented in the
models, aside from the shading and partial occlusions,
is not crucial for the results. So long as the authors
perturb object vertex positions with Gaussian noise, the
task is deﬁned and the ideal observer performance is
determined, no matter what representation is used. The
choice of vertex (x, y) coordinates was a matter of
mathematical convenience. If the authors represent the
objects in terms of the lengths and relative angles of the
cylinders, the authors would obtain the same ideal
performance.
An additional point can be made from the symmetry
condition. The authors noted that the efﬁciency is
greater for the Symmetric than for the Irregular objects.
Is this because the Symmetric objects are ‘simpler,’ in
the sense that the viewing space is half as much for the
Symmetric objects? This cannot explain why Symmetric
objects have a greater efﬁciency, because the ideal
observer’s viewing space is also halved. In fact, the
essence of ideal observer analysis and the measure of
statistical efﬁciency is to take into account (or to normalize) any differences between different stimuli. Therefore, any efﬁciency difference reﬂects representation
and processing differences in the brain, not in the
stimulus. Therefore, the fact that the efﬁciency is higher
for the Symmetric than for the Irregular objects indicates that subjects used 3D information of object symmetry in recognition.
Taken together, the results strongly suggest that 2D
afﬁne transformations are insufﬁcient to account for
the ability of humans to compensate for viewpoint
changes in this task. What are the alternatives?
6.2. Is 3D structural information used for object
recognition?
The fact that human statistical efﬁciency relative to
the 2D afﬁne ideal observer is greater for the novel than
for the learned views, despite the best efforts to ﬁnd the
lowest efﬁciency possible for the novel views, indicates
that human observers incorporate more knowledge of
the regularities between views than that implicit in 2D
afﬁne transformations. The results also suggest that 2D
afﬁne nearest neighbor matching cannot account for
the human performance. The fact that an ‘ideal’ prior
probability distribution on all possible 2D afﬁne transformations is unknown makes this result valuable in its
own right. The authors can rank in increasing order of
greater power and ﬂexibility in approximating 3D novel
object views from 2D template views, the 2D/2D ideal
observer, the learning 2D/2D ideal observer, the Radial
Basis Functions (RBF) model (in ) and in this paper
the 2D afﬁne nearest neighbor model, the GRBF model
and the 2D afﬁne ideal observer model. The results
suggest that the human observers may use yet a more
sophisticated strategy that incorporates knowledge of
3D structure, perhaps by means of view-combination
Models of view-combination have either explicit or
implicit knowledge that views arise from 3D object
rotations. Three-dimensional constraints are built into
the memory representations. By intelligently combining
stored views, these models can, in principle, ﬁnd nearly
exact matches to novel views with orthographic projection .
Consider ﬁrst an extreme and ideal case in which
there is an explicit 3D model in memory. The most
straightforward identiﬁcation scheme veriﬁes a match
by translating, scaling and rotating an explicit 3D
model of the object in memory, projecting the result in
a 2D image plane and then using a measure of similarity to test for a satisfactory match with the 2D input
(see for example ). Liu et al. referred to
the statistically optimal version of this model as a
3D/2D ideal observer. Despite its intuitive simplicity, a
straightforward implementation of this scheme is in
general not computationally feasible. An elegant solution (view-combination) to the computational difﬁculty
was proposed by , who showed that as few as two
views are sufﬁcient to carry out the veriﬁcation process
Z. Liu, D. Kersten / Vision Research 38 2507–2519
by checking the linear dependence of a third view on
the two views. Recognition here assumes, albeit implicitly, that the object has 3D afﬁne structure. In general,
view-combination models exploit the inherent regularity
in the collection of images resulting from a projection
of an object.
The authors noted that that the statistical efﬁciency is
greater for the Symmetric than for the Irregular objects
implies that the human observers may have used 3D
structural information, since 3D symmetry is inherently
a 3D property. The authors cannot rule out, however,
the possibility that 2D afﬁne transformations account
for substantial, though incomplete, portions of the human efﬁciency. This is illustrated by the fact that the
statistical efﬁciencies for both the learned and novel
views are below 100% and that their difference is no
longer substantial, even though statistically signiﬁcant.
It is important to note that ideal observer analysis is
crucial to the conclusions. When object recognition
performance falls off as an object rotates away from the
learned views, it is difﬁcult to distinguish whether the
result can be accounted for by a view-approximation
model , without an ideal observer analysis. The dependence of human performance on viewpoint might
simply reﬂect the information for the task in the stimulus and not the speciﬁc functional constraints of the
visual system. Until stimulus information is adequately
accounted for, such a problem will remain unsolved.
The ideal observer analysis makes it possible to distinguish these possibilities and suggests that view-approximation is not the whole story.
Acknowledgements
DK was supported by a grant from the National
Science Foundation, contract number SBR-9631682.
We thank Ronen Basri, David Jacobs, David Knill,
Mamassian,
Daphna Weinshall, the anonymous reviewers and in
particular, John Oliensis, for many helpful discussions.
Weinshall pointed out to us the Werman–Weinshall
theorem. Part of this work was presented at the Hong
Kong International Workshop on ‘Theoretical Aspects
of Neural Computation,’ Hong Kong University of
Science and Technology, 1997; European Conference
on Visual Perception (ECVP), Helsinki, Finland, 1997;
‘Neural Information Processing’ (NIPS), Denver, Colorado, 1997; and ‘International Conference on Computer Vision’ (ICCV), Mumbai, India, 1998.
Appendix A. The 2D afﬁne ideal observer
Without loss of generality, the authors consider the
case of only one stored template. Assume that the
template T and the input stimulus image S are represented as:
A 2D afﬁne transformation to the template T is
(a, b, c, d, tx, ty)  (−
If the authors assume that the stimulus image S is
obtained by ﬁrst applying a 2D afﬁne transformation to
the template image T and then adding independent
Gaussian noise N(0, sI2n) to the resultant image, the
authors have
T,A,Tr)=P(N=S−(AT+Tr)).
Let us calculate S−(A T+Tr) ﬁrst. Without loss of
generality, the authors assume that the template image
T is centered at the origin, i.e. Sn
The authors calculate the squared Euclidean distance of
S−(A T+Tr)
2. More speciﬁcally, the squared Euclidean distance is
Tx+aXT+bYT−XS
Ty+cXT+dYT−YS
For the ﬁrst term, given that Sixi
authors have
Tx+aXT+bYT−XS
2−2(aXT · XS+bYT · XS)
The ﬁrst term on the right side of the Eq. (24) is
2=n[(tx−x¯ )2+var(xS)],
n , var(xS)=xS
n −(x¯ )2.
The last two terms in Eq. (24) is a2XT
2abXT · YT−2(aXT · XS+bYT · XS).
squared distance is
n[(tx−x¯ )2+(ty−y¯ )2+var(xS)+var(yS)]
2(a2+c2)+YT
+2(ab+cd)XT · YT
−2(aXT · XS+bYT · XS+cXT · YS+dYT · YS).
The authors write
Z. Liu, D. Kersten / Vision Research 38 2507–2519
Then the authors can write the squared distance as
n[(tx−x¯ )2+(ty−y¯ )2+var(xS)+var(xS)]
Completing the square e.g.
TQK1(6x−K1)TQ(6x−K1)−K1
n[(tx−x¯ )2+(ty−y¯ )2+var(xS)+var(yS)]+6%x
A.1. Uniform prior
Assume that P(X)=C−1, where C is a normalization constant such that
1=& P(X) dX
this effectively assumes that XT
(a, b, c, d, tx, ty) has
a uniform distribution in R6 and that C is necessarily
inﬁnite. So
X, T)P(X) dX
(2ps2)nC& dX
S−A(a,b,c,d)T−Tr(tx,ty)
The integral is:
−n(var(xS)+var(yS))−tr(KTQK)
da%db%dc%dd%dt%xdt%y
2+6%xQ6%x+6%yQ6%y
Now the authors use
(2ps2)N/2&
=(det(M))−1/2,
where x is a length–N vector and M a symmetric
N×N matrix. (Verify this by diagonalizing M and
changing variables, then the integral just becomes a
product of N independent Gaussian integrals.) Finally,
the integral is
nC(2ps2)n−3det(Q)
tr(KTQK)−n(var(xS)+var(yS))
A.2. Gaussian prior
Alternatively, the authors can assume that XT
b, c, d, tx, ty) obeys a Gaussian probability distribution
(2pg2)3 exp
−(X−X0)T(X−X0)
A reasonable assumption about X0 is that this afﬁne
transformation is an identity transformation, with
T=(1, 0, 0, 1, 0, 0). The argument of the integral
becomes proportional to
n[(tx−x¯ )2+(ty−y¯ )2+var(xS)+var(yS)]+
2+(a−1)2+b2+c2+(d−1)2)
+n(x¯ 2+y¯ 2)
+n(var(xS)+var(yS))
*TQ%6x*+6y
*TQ%6y*−K1
*TQ(Q%)−1QK1
*TQ(Q%)−1QK2
Z. Liu, D. Kersten / Vision Research 38 2507–2519
Q+g−2I2,QK*=QK+g−2I2
=6−Q%−1QK*.
Similar arguments as before give
−var(xS)+var(yS)+(x¯ 2+y¯ 2)/(ng2+1)
tr(KTQ(Q%)−1QK)−2g−2
(2pg2)3& da%db%dc%dd%dt%xdt%y
−(n+g−2)(t%x
2)+6%xQ%6%x+6%yQ%6%y
(2ps2)n−3g6(n+g−2)det(Q%)
var(xS)+var(yS)+x¯ 2+y¯ 2
−2g−2−tr(K*TQ(Q%)−1QK*)
this goes to the former expression of Eq. (42).