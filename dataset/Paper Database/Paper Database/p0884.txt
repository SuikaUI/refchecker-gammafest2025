Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355
Brussels, Belgium, November 1, 2018. c⃝2018 Association for Computational Linguistics
GLUE: A Multi-Task Benchmark and Analysis Platform
for Natural Language Understanding
Alex Wang1, Amanpreet Singh1, Julian Michael2, Felix Hill3,
Omer Levy2, and Samuel R. Bowman1
1New York University, New York, NY
2Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA
3DeepMind, London, UK
{alexwang,amanpreet,bowman}@nyu.edu
{julianjm,omerlevy}@cs.washington.edu
 
Human ability to understand language is general, ﬂexible, and robust. In contrast, most NLU
models above the word level are designed for a
speciﬁc task and struggle with out-of-domain data.
If we aspire to develop models with understanding beyond the detection of superﬁcial correspondences between inputs and outputs, then it is critical to develop a uniﬁed model that can execute a
range of linguistic tasks across different domains.
To facilitate research in this direction, we
present the General Language Understanding
Evaluation (GLUE, gluebenchmark.com): a
benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of speciﬁc linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is
plentiful, but for others it is limited or does not
match the genre of the test set. GLUE thus favors
models that can represent linguistic knowledge in
a way that facilitates sample-efﬁcient learning and
effective knowledge-transfer across tasks. While
none of the datasets in GLUE were created from
scratch for the benchmark, four of them feature
privately-held test data, which is used to ensure
that the benchmark is used fairly.
We evaluate baselines that use ELMo , a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve
fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance
over all phenomena tested, with some exceptions.
The GLUE benchmark
GLUE consists of nine
English sentence understanding tasks covering a
broad range of domains, data quantities, and difﬁculties. As the goal of GLUE is to spur development of generalizable NLU systems, we design the
benchmark such that good performance should re-
Single-Sentence Tasks
acceptability
movie reviews
Similarity and Paraphrase Tasks
paraphrase
textual sim.
paraphrase
Inference Tasks
coref./NLI
ﬁction books
Table 1: Task descriptions and statistics. Bold denotes tasks for which there is privately-held test
All tasks are binary classiﬁcation, except
STS-B (regression) and MNLI (three classes).
quire models to share substantial knowledge (e.g.,
trained parameters) across tasks, while maintaining some task-speciﬁc components. Though it is
possible to train a model per task and evaluate the
resulting set of models on this benchmark, we expect that inclusion of several data-scarce tasks will
ultimately render this approach uncompetitive.
The nine tasks include two tasks with singlesentence inputs: Corpus of Linguistic Acceptability and Stanford
Sentiment Treebank 
Three tasks involve detecting semantic similarity:
Microsoft Research Paraphrase Corpus ), Quora Question
Pairs1 (QQP), and Semantic Textual Similarity
Benchmark . The remaining four tasks are formatted as natural language inference (NLI) tasks, such as the Multi-Genre NLI
corpus and Recog-
data.quora.com/First-Quora-Dataset-
Release-Question-Pairs
Single Sentence
Similarity and Paraphrase
Natural Language Inference
Single-task
Multi-task
Skip-Thought
Table 2: Baseline performance on the GLUE tasks. For MNLI, we report accuracy on the matched and
mismatched test sets. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and
Spearman correlation. For CoLA, we report Matthews correlation . For all other tasks
we report accuracy. All values are scaled by 100. A similar table is presented on the online platform.
nizing Textual Entailment , as well
as versions of SQuAD 
and Winograd Schema Challenge recast as NLI (resp. QNLI, WNLI). Table 1
summarizes the tasks. Performance on the benchmark is measured per task as well as in aggregate,
averaging performance across tasks.
Diagnostic Dataset
To understand the types of
knowledge learned by models, GLUE also includes a dataset of hand-crafted examples for
probing trained models. This dataset is designed
to highlight common phenomena, such as the use
of world knowledge, logical operators, and lexical entailments, that models must grasp if they are
to robustly solve the tasks. Each of the 550 examples is an NLI sentence pair tagged with the
phenomena demonstrated. We ensure that the data
is reasonably diverse by producing examples for
a wide variety of linguistic phenomena, and basing our examples on naturally-occurring sentences
from several domains. We validate our data by using the hypothesis-only baseline from Gururangan
et al. and having six NLP researchers manually validate a random sample of the data.
To demonstrate the benchmark in use,
we apply multi-task learning on the training data
of the GLUE tasks, via a model that shares a BiL-
STM between task-speciﬁc classiﬁers.
train models that use the same architecture but are
trained on a single benchmark task. Finally, we
evaluate the following pretrained models: average
bag-of-words using GloVe embeddings (CBoW),
Skip-Thought , InferSent , DisSent , and
GenSen .
Sentence Pair
Quantiﬁers
Double Negation
I have never seen a hummingbird
not ﬂying.
I have never seen a hummingbird.
Active/Passive
Cape sparrows eat seeds, along
with soft plant parts and insects.
Cape sparrows are eaten.
Named Entities
World Knowledge
Musk decided to offer up his personal Tesla roadster.
Musk decided to offer up his personal car.
Table 3: Diagnostic set examples. Systems must
predict the relationship between the sentences, either entailment, neutral, or contradiction when
one sentence is the premise and the other is the
hypothesis, and vice versa. Examples are tagged
with the phenomena demonstrated. We group each
phenomena into one of four broad categories.
We ﬁnd that our models trained directly on the
GLUE tasks generally outperform those that do
not, though all models obtain fairy low absolute
scores. Probing the baselines with the diagnostic data, we ﬁnd that performance on the benchmark correlates with performance on the diagnostic data, and that the best baselines similarly
achieve low absolute performance on the linguistic phenomena included in the diagnostic data.
Conclusion
We present the GLUE benchmark,
consisting of: (i) a suite of nine NLU tasks, built
on established annotated datasets and covering a
diverse range of text genres, dataset sizes, and
difﬁculties; (ii) an online evaluation platform and
leaderboard, based primarily on private test data;
(iii) an expert-constructed analysis dataset. Experiments indicate that solving GLUE is beyond the
capability of current transfer learning methods.