Enhanced Image Classiﬁcation With a Fast-Learning
Shallow Convolutional Neural Network
Mark D. McDonnell and Tony Vladusich
Computational and Theoretical Neuroscience Laboratory, Institute for Telecommunications Research,
School of Information Technology and Mathematical Sciences,
University of South Australia
Mawson Lakes, SA 5095, Australia
Email: 
Abstract—We present a neural network architecture and
training method designed to enable very rapid training and low
implementation complexity. Due to its training speed and very
few tunable parameters, the method has strong potential for
applications requiring frequent retraining or online training. The
approach is characterized by (a) convolutional ﬁlters based on
biologically inspired visual processing ﬁlters, (b) randomly-valued
classiﬁer-stage input weights, (c) use of least squares regression to
train the classiﬁer output weights in a single batch, and (d) linear
classiﬁer-stage output units. We demonstrate the efﬁcacy of the
method by applying it to image classiﬁcation. Our results match
existing state-of-the-art results on the MNIST (0.37% error)
and NORB-small (2.2% error) image classiﬁcation databases,
but with very fast training times compared to standard deep
network approaches. The network’s performance on the Google
Street View House Number (SVHN) (4% error) database is also
competitive with state-of-the art methods.
INTRODUCTION
State-of-the-art performance on many image classiﬁcation
databases has been achieved recently using multilayered (i.e.,
deep) neural networks . Such performance generally relies
on a convolutional feature extraction stage to obtain invariance
to translations, rotations and scale . Training of deep
networks, however, often requires signiﬁcant resources, in
terms of time, memory and computing power (e.g. in the order
of hours on GPU clusters). Tasks that require online learning,
or periodic replacement of all network weights based on fresh
data may thus not be able to beneﬁt from deep learning
techniques. It is desirable, therefore, to seek very rapid training
methods, even if this is potentially at the expense of a small
performance decrease.
Recent work has shown that good performance on image
classiﬁcation tasks can be achieved in ‘shallow’ convolutional
networks—neural architectures containing a single training
layer—provided sufﬁciently many features are extracted .
Perhaps surprisingly, such performance arises even with the
use of entirely random convolutional ﬁlters or ﬁlters based on
randomly selected patches from training images . Although
application of a relatively large numbers of ﬁlters is common
(followed by spatial image smoothing and downsampling),
good classiﬁcation performance can also be obtained with
a sparse feature representation (i.e. relatively few ﬁlters and
minimal downsampling) .
Based on these insights and the goal of devising a fast
training method, we introduce a method for combining several
existing general techniques into what is equivalent to a ﬁve
layer neural network (see Figure 1) with only a single trained
layer (the output layer), and show that the method:
produces state-of-the-art results on well known image
classiﬁcation databases;
is trainable in times in the order of minutes (up
to several hours for large training sets) on standard
desktop/laptop computers;
is sufﬁciently versatile that the same hyper-parameter
sets can be applied to different datasets and still
produce results comparable to dataset-speciﬁc optimisation of hyper-parameters.
The fast training method we use has been developed
independently several times and has gained increasing
recognition in recent years—see for recent reviews
of the different contexts and applications. The network architecture in the classiﬁcation stage is that of a three layer
neural network comprised from an input layer, a hidden layer
of nonlinear units, and a linear output layer. The input weights
are randomly chosen and untrained, and the output weights are
trained in a single batch using least squares regression. Due to
the convexity of the objective function, this method ensures the
output weights are optimally chosen for a given set of random
input weights. The rapid speed of training is due to the fact that
the least squares optimisation problem an be solved using an
O(KM 2) algorithm, where M is the number of hidden units
and K the number of training points .
When applied to pixel-level features, these networks can
be trained as discriminative classiﬁers and produce excellent
results on simple image databases but poor performance on more difﬁcult ones. To our knowledge, however, the
method has not yet been applied to convolutional features.
Therefore, we have devised a network architecture (see
Figure 1) that consists of three key elements that work together to ensure fast learning and good classiﬁcation performance: namely, the use of (a) convolutional feature extraction,
(b) random-valued input weights for classiﬁcation, (c) least
squares training of output weights that feed in to (d) linear
output units. We apply our network to several image classiﬁcation databases, including MNIST , CIFAR-10 , Google
Street View House Numbers (SVHN) and NORB .
The network produces state-of-the-art classiﬁcation results on
MNIST and NORB-small databases and near state-of-the-art
performance on SVHN.
 
These promising results are presented in this paper to
demonstrate the potential beneﬁts of the method; clearly further innovations within the method are required if it is to be
competitive on harder datasets like CIFAR-10, or Imagenet.
We expect that the most likely avenues for improving our
presented results for CIFAR-10, whilst retaining the method’s
core attributes, are (1) to introduce limited training of the Stage
1 ﬁlters by generalizing the method of ; (2) introduction
of training data augmentation. We aim to pursuing these
directions in our future work.
The remainder of the paper is organized as follows. Section II contains a generic description of the network architecture and the algorithms we use for obtaining convolutional
features and classifying inputs based on them. Section III
describes how the generic architecture and training algorithms
are speciﬁcally applied to four well-known benchmark image
classiﬁcation datasets. Next, Section IV describes the results
we obtained for these datasets, and ﬁnally the paper concludes
with discussion and remarks in Section V.
NETWORK ARCHITECTURE AND TRAINING
ALGORITHMS
The overall network is shown in Figure 1. There are
three hidden layers with nonlinear units, and four layers of
weights. The ﬁrst layer of weights is the convolutional ﬁlter
layer. The second layer is a pooling (low pass ﬁltering) and
downsampling layer. The third layer is a random projection
layer. The fourth layer is the only trained layer. The output
layer has linear units.
The network can be conceptually divided into two stages
and two algorithms, that to our knowledge have not previously
been combined. The ﬁrst stage is the convolutional feature
extraction stage, and largely follows that of existing approaches
to image classiﬁcation . The second stage is the
classiﬁer stage, and largely follows the approach of .
We now describe the two stages in detail.
A. Stage 1 Architecture: Convolutional ﬁltering and pooling
The algorithm we apply to extract features from images
(including those with multiple channels) is summarised in
Algorithm 1. Note that the details of the ﬁlters hi,c and
hp described in Algorithm 1 are given in Section III-B, but
here we introduce the size of these two-dimensional ﬁlters as
W ×W and Q×Q. The functions g1(·) and g2(·) are nonlinear
transformations applied termwise to matrix inputs to produce
matrix outputs of the same size. The symbol * represents twodimensional convolution.
This sequence of steps in Algorithm 1 suggest looping
over all images and channels sequentially. However, the following mathematical formulation of the algorithm indicates a
standard layered neural network formulation of this algorithm
is applicable, as shown in Figure 1, and therefore that computation of all features (fk, k = 1, ..K ) can be obtained in
one shot from a K-column matrix containing a batch of K
training points.
The key to this formulation is to note that since convolution
is a linear operator, a matrix can be constructed that when
multiplied by a data matrix produces the same result as
Input : Set of K images, xk, each with C channels
Output: Feature vectors, fk, k = 1, . . . K
foreach xk do
split xk into channels, xk,c, c = 1, . . . C
foreach i = 1, . . . P ﬁlters do
Apply ﬁlter to each channel: yi,k,c ←hi,c ∗xk,c
Apply termwise nonlinearity: zi,k,c ←g1(yi,k,c)
Apply lowpass ﬁlter: wi,k,c ←hp ∗zi,k,c
Apply termwise nonlinearity: si,k,c ←g2(wi,k,c)
Downsample: ˆsi,k,c ←si,k,c
Concatenate channels: ri,k ←[ˆsi,k,1| . . . |ˆsi,k,C]
Normalize: fi,k = ri,k/(ri,k1⊤)
Concatenate over ﬁlters: fk ←[f1,k|f2,k| . . . |fP,k]
Algorithm 1: Convolutional feature detection.
convolution applied to one instance of the data. Hence, for
a total of L features per image, we introduce the following
F be a feature matrix of size L × K;
X be a data matrix with K columns;
WFilter be a concatenation of the CP convolution matrices corresponding to hi,c i = 1, . . . P, c = 1, . . . C;
W0 be a convolution matrix corresponding to hl, that
also down samples by a factor of D;
WPool be a block diagonal matrix containing CP
copies of W0 on the diagonals.
The entire ﬂow described in Algorithm 1 can be written
mathematically as
F = g2(WPool g1(WFilterX)),
where g1(·) and g2(·) are applied term by term to all elements
of their arguments. The matrices WFilter and WPool are sparse
Toeplitz matrices. In practice we would not form them directly,
but instead form one pooling matrix, and one ﬁltering matrix
for each ﬁlter, and sequential apply each ﬁlter to the entire
data matrix, X.
We use a particular form for the nonlinear hidden-unit
functions g1(·) and g2(·) inspired by LP-pooling , which
is of the form g1(u) = up and g2(v) = v
p . For example, with
p = 2 we have
WPool (WFilterX)2.
An intuitive explanation for the use of LP-pooling is as
follows. First, note that each hidden unit receives as input
a linear combination of a patch of the input data, i.e. u in
g1(u) has the form u = PW 2
j=1 hi,c,jxi,j. Hence, squaring u
results in a sum that contains terms proportional to x2
terms proportional to products of each xi,j. Thus, squaring is
a simple way to produce hidden layer responses that depend
on the product of pairs of input data elements, i.e. interaction
terms, and this is important for discriminability. Second, the
Hidden Layer 1
Hidden Layer 2
Hidden Layer 3
Linear Output Layer
Stage 1: Convolutional Filtering and Pooling
Stage 2: Classiﬁcation
Classiﬁcation
L features
D2L features
(D2L × J2)
Overall network architecture. In total there are three hidden layers, plus an input layer and a linear output layer. There are two main stages: a
convolutional ﬁltering and pooling stage, and a classiﬁcation stage. Only the ﬁnal layer of weights, Wout is learnt, and this is achieved in a single batch using
least squares regression. Of the remaining weights matrices, WFilter is speciﬁed and remains ﬁxed, e.g. taken from Overfeat ; WPool describes standard
average pooling and downsampling; and Win is set randomly or by using the method of that speciﬁes the weights by sampling examples of the training
distribution, as described in the text. Other variables shown are as follows: J2 is the number of pixels in an image, L is the number of features extracted per
image, D is a downsampling factor, M is the number of hidden units in the classiﬁer stage and N is the number of classes.
square root transforms the distribution of the hidden-unit
responses; we have observed that in practice, the result of the
square root operation is often a distribution that is closer to
Gaussian than without it, which helps to regularise the least
squares regression method of training the output weights.
However, as will be described shortly, the classiﬁer of
Stage 2 also has a square nonlinearity. Using this nonlinearity,
we have found that classiﬁcation performance is generally
optimised by taking the square root of the input to the random
projection layer. Based on this observation, we do not strictly
use LP-pooling, and instead set
g1(u) = u2,
g2(v) = v0.25.
This effectively combines the implementation of L2-pooling,
and the subsequent square root operation.
B. Stage 2 Architecture: Classiﬁer
The following descriptions are applicable whether or not
raw pixels are treated as features or the input is the features
extracted in stage 1. First, we introduce notation. Let:
Ftrain, of size L × K, contain each length L feature
Ylabel be an indicator matrix of size N × K, which
numerically represents the labels of each training vector, where there are N classes—we set each column
to have a 1 in a single row, corresponding to the label
class for each training vector, and all other entries to
Win, of size M × L be the real-valued input weights
matrix for the classiﬁer stage;
Wout, of size N × M be the real-valued output
weights matrix for the classiﬁer stage;
the function g(·) be the activation function of each
hidden-unit; for example, g(·) may be the logistic
sigmoid, g(z) = 1/(1 + exp(−z)), or a squarer,
g(z) = z2;
Atrain = g(WinFtrain), of size M × K, contain the
hidden-unit activations that occur due to each feature
vector; g(·) is applied termwise to each element in the
matrix WinFtrain.
C. Stage 1 Training: Filters and Pooling
In this paper we do not employ any form of training
for the ﬁlters and pooling matrices. The details of the ﬁlter
weights and form of pooling used for the example classiﬁcation
problems presented in this paper are given Section III.
D. Stage 2 Training: Classiﬁer Weights
The training approach for the classiﬁer is that described by
e.g. . The default situation for these methods is that
the input weights, Win, are generated randomly from a speciﬁc distribution, e.g. standard Gaussian, uniform, or bipolar.
However, it is known that setting these weights non-randomly
based on the training data leads to superior performance . In this paper, we use the method of . The input
weights can also be trained iteratively, if desired, using singlebatch backpropagation .
Given a choice of Win, the output weights matrix is
determined according to
Wout = YlabelA+
train is the size K×M Moore-Penrose pseudo inverse
corresponding to Atrain. This solution is equivalent to least
squares regression applied to an overcomplete set of linear
equations, with an N-dimensional target. It is known to often
be useful to regularise such problems, and instead solve the
following ridge regression problem :
Wout = YlabelA⊤
train(AtrainA⊤
train + cI)−1,
where c is a hyper-parameter and I is the M × M identity
matrix. In practice, it is efﬁcient to avoid explicit calculation
of the inverse in Equation (6) and instead use QR
factorisation to solve the following set of NM linear equations
for the NM unknown variables in Wout:
train = Wout(AtrainA⊤
train + cI).
Above we mentioned two algorithms, and Algorithm 2 is simply to form Atrain and solve Eqn. (7), followed by optimisation
of c using ridge regression. For large M and K > M (which
is typically valid) the runtime bottleneck for this method is
typically the O(KM 2) matrix multiplication required to obtain
the Gram matrix, AtrainA⊤
E. Application to Test Data
For a total of Ktest test images contained in a matrix Xtest,
we ﬁrst obtain a matrix Ftest = g2(WPool g1(WFilterXtest)),
of size L × Ktest, by following Algorithm 1. The output of
the classiﬁer is then the N × Ktest matrix
Ytest = Wout g(WinFtest)
= Wout g(Win g2(WPool g1(WFilterX))).
Note that we can write the response to all test images in terms
of the training data:
Ytest = Ylabel (g(WinFtrain))+ g(WinFtest)
Ftrain = g2(WPool g1(WFilterXtrain))
Ftest = g2(WPool g1(WFilterXtest)).
Thus, since the pseudo-inverse, (·)+, can be obtained from
Equation (6), Equations (10), (11) and (12) constitute a closedform solution for the entire test-data classiﬁcation output, given
speciﬁed matrices, Wﬁlter, Wpool and Win, and hidden-unit
activation functions, g1, g2, and g.
The ﬁnal classiﬁcation decision for each image is obtained
by taking the index of the maximum value of each column of
IMAGE CLASSIFICATION EXPERIMENTS: SPECIFIC
We examined the method’s performance when used as a
classiﬁer of images. Table I lists the attributes of four well
known databases we used. For the two databases comprised
from RGB images, we used C = 4 channels, namely the raw
RGB channels, and a conversion to greyscale. This approach
was shown to be effective for SVHN in .
MNIST 
NORB-small 
2 (stereo)
CIFAR-10 
Image Databases. Note that the NORB-small database
consists of images of size 96 × 96 pixels, but we ﬁrst downsampled all
training and test images to 32 × 32 pixels, as in .
A. Preprocessing
All raw image pixel values were scaled to the interval
 . Due to the use of quadratic nonlinearities and LPpooling, this scaling does not affect performance. The only
other preprocessing done was as follows:
MNIST: None;
NORB-small: downsample from 96×96 to 32×32,
for implementation efﬁciency reasons (this is consistent with some previous work on NORB-small,
e.g. );
SVHN: convert from 3 channels to 4 by adding a
conversion to greyscale from the raw RGB. We found
that local and/or global contrast enhancement only
diminished performance;
CIFAR-10: convert from 3 channels to 4 by adding
a conversion to greyscale from the raw RGB; apply
ZCA whitening to each channel of each image, as
B. Stage 1 Design: Filters and Pooling
Since our objective here was to train only a single layer of
the network, we did not seek to train the network to ﬁnd ﬁlters
optimised for the training set. Instead, for the size W ×W twodimension ﬁlters, hi,c, we considered the following options:
simple rotated bar and corner ﬁlters, and square
uniform centre-surround ﬁlters;
ﬁlters trained on Imagenet and made available in
Overfeat ; we used only the 96 stage-1 ‘accurate’
7×7 ﬁlters;
patches obtained from the central W × W region
of randomly selected training images, with P/N
training images from each class.
The ﬁlters from Overfeat1 are RGB ﬁlters. Hence, for the
databases with RGB images, we applied each channel of
the ﬁlter to the corresponding channel of each image. When
applied to greyscale channels, we converted the Overfeat ﬁlter
to greyscale. For NORB, we applied the same ﬁlter to both
stereo channels. For all ﬁlters, we subtract the mean value
over all W 2 dimensions in each channel, in order to ensure a
mean of zero in each channel.
In implementing the two-dimensional convolution operation required for ﬁltering the raw images using hi,c, we
obtained only the central ‘valid’ region, i.e. for images of size
J ×J, the total dimension of the valid region is (J −W +1)2.
Consequently, the total number of features per image obtained
prior to pooling, from P ﬁlters, and images with C channels
is L = CP(J −W + 1)2.
In previous work, e.g. , the form of the Q × Q twodimension ﬁlter, hp is a normalised Gaussian. Instead, we used
a simple summing ﬁlter, equivalent to a kernel with all entries
equal to the same value, i.e.
hp,u,v = 1
u = 1, . . . Q, v = 1, . . . Q.
In implementing the two-dimensional convolution operation
required for ﬁltering using hp, we obtained the ‘full’ convolutional region, which for images of size J ×J is (J −W +Q)2,
given the ‘valid’ convolution ﬁrst applied using hi,c, as described above.
The remaining part of the pooling step is to downsample
each image dimension by a factor of D, resulting in a total
of ˆL = L/D2 features per image. In choosing D, we experimented with a variety of scales before settling on the value
1Available from 
shown in Table II. We note there exists an interesting tradeoff
between the number of ﬁlters P, and the downsampling factor,
D. For example, in , D = L/2, whereas in D = 1. We
found that, up to a point, smaller D enables a smaller number
of ﬁlters, P, for comparable performance.
The hyper-parameters we used for each dataset are shown
in Table II.
Hyper-parameter
Filter size, W
Pooling size, Q
Downsample factor, D
Stage 1 Hyper-parameters (Convolutional Feature
Extraction).
C. Stage 2 Design: Classiﬁer projection weights
To construct the matrix Win we use the method proposed
by . In this method, each row of the matrix Win is
chosen to be a normalized difference between the data vectors
corresponding to randomly chosen examples from distinct
classes of the training set. This method has previously been
shown to be superior to setting the weights to values chosen
from random distributions .
For the nonlinearity in the classiﬁer stage hidden units,
g(z), the typical choice in other work is a sigmoid.
However, we found it sufﬁcient (and much faster in an implementation) to use the quadratic nonlinearity. This suggests
that good image classiﬁcation is strongly dependent on the
presence of interaction terms—see the discussion about this in
Section II-A.
D. Stage 2 Design: Ridge Regression parameter
With these choices, there remains only two hyperparameters for the Classiﬁer stage: the regression parameter,
c, and the number of hidden-units, M. In our experiments,
we examined classiﬁcation error rates as a function of varying
M. For each M, we can optimize c using cross-validation.
However, we also found that a good generic heuristic for
setting c was
M 2 min(diag(AtrainA⊤
and this reduces the number of hyper-parameters for the
classiﬁcation stage to just one: the number of hidden-units,
E. Stage 1 and 2 Design: Nonlinearities
For the hidden-layer nonlinearities, to reiterate, we use:
g1(u) = u2, g2(v) = v0.25, g(z) = z2.
We examined the performance of the network on classifying the test images in the four chosen databases, as a function
of the number of ﬁlters, P, the downsampling rate D, and the
number of hidden units in the classiﬁer stage, M. We use the
maximum number of channels, C, available in each dataset
(recall from above that we convert RGB images to greyscale,
as a fourth channel).
We considered the three kinds of untuned ﬁlters described
in Section III-B, as well as combinations of them. We did not
exhaustively consider all options, but settled on the Overfeat
ﬁlters as being marginally superior for NORB, SVHN and
CIFAR-10 (in the order of 1% in comparison with other
options), while hand-designed ﬁlters were superior for MNIST,
but only marginally compared to randomly selected patches
from the training data. There is clearly more that can be
investigated to determine whether hand-designed ﬁlters can
match trained ﬁlters when using the method of this paper.
A. Summary of best performance attained
The best performance we achieved is summarised in Table III.
State-of-the-art
0.39% 
NORB-small
2.53% 
1.92% 
9.78% 
TABLE III.
Results for various databases. The state-of-the-art result
listed for MNIST and CIFAR-10 can be improved by augmenting the
training set with distortions and other methods ; we have not
done so here, and report state-of-the-art only for methods not doing so.
B. Trend with increasing M
We now use MNIST as an example to indicate how classi-
ﬁcation performance scales with the number of hidden units in
the classiﬁer stage, M. The remain parameters were W = 7,
D = 3 and P = 43, which included hand-designed ﬁlters
comprised from 20 rotated bars (width of one pixel), 20 rotated
corners (dimension 4 pixels) and 3 centred squares (dimensions
3, 4 and 5 pixels), all with zero mean. The rotations were
of binary ﬁlters and used standard pixel value interpolation.
Figure 2 shows a power law-like decrease in error rate as M
increases, with a linear trend on the log-log axes. The best error
rate shown on this ﬁgure is 0.40%. As shown in Table III,
we have attained a best repeatable rate of 0.37% using 60
ﬁlters and D = 2. When we combined Overfeat ﬁlters with
hand-designed ﬁlters and randomly selected patches from the
training data, we obtained up to 0.32% error on MNIST, but
this was an outlier since it was not repeatedly obtained by
different samples of Win.
C. Indicative training times
For an implementation in Matlab on a PC with 4 cores and
32 GB of RAM, for MNIST (60000 training points) the total
time required to generate all features for all 60000 training
images from one ﬁlter is approximately 2 seconds. The largest
number of ﬁlters we used to date was 384 (96 RGB+greyscale),
and when applied to SVHN (∼600000 training points), the
total run time for feature extraction is then about two hours
(in this case we used batches of size 100000 images).
The runtime we achieve for feature generation beneﬁts
from carrying out convolutions using matrix multiplication applied to large batches simultaneously; if instead we iterate over
all training images individually, but still carry out convolutions
Number of classifier hidden units, M
Test error %
10 repeats
mean of 10 repeats
Example set of error percentage value on the 10000 MNIST test
images, for ten repetitions of the selection Win. The best result shown is 40
errors out of 10000. Increasing M above 6400 saturates in performance.
using matrix multiplication, the time for generating features
approximately doubles. Note also that we employ Matlab’s
sparse matrix data structure functionality to represent WFilter
and WPool, which also provides a speed boost when multiplying these matrices to carry out the convolutions. If we do
not use the matrix-multiplication method for convolution, and
instead apply two-dimensional convolutions to each individual
image, the feature generation is slowed even more.
For the classiﬁer stage, on MNIST with M = 6400, the
runtime is approximately 150 seconds for D = 3 (there is a
small time penalty for smaller D, due to the larger dimension
of the input to the classiﬁer stage). Hence, the total run time
for MNIST with 40 ﬁlters and M = 6400 is in the order of 4
minutes to achieve a correct classiﬁcation rate above 99.5%.
With fewer ﬁlters and smaller M, it is simple to achieve over
99.2% in a minute or less.
For SVHN and CIFAR-10 where we scaled up to M =
40000, the run time bottleneck is the classiﬁer, due to the
O(KM 2) runtime complexity. We found it necessary to use a
PC with more RAM (peak usage was approximately 70 GB)
for M > 20000. In the case of M = 40000, the network
was trained in under an hour on CIFAR-10, while SVHN took
about 8-9 hours. Results within a few percent of our best,
however, can be obtained in far less time.
DISCUSSION AND CONCLUSIONS
As stated in the introduction, the purpose of this paper
is to highlight the potential beneﬁts of the method presented,
namely that it can attain excellent results with a rapid training
speed and low implementation complexity, whilst only suffering from reduced performance relative to state-of-the-art on
particularly hard problems.
In terms of efﬁcacy on classiﬁcation tasks, as shown in
Table III, our best result (0.37% error rate) surpasses the best
ever reported performance for classiﬁcation of the MNIST test
set when no augmentation of the training set is done. We have
also achieved, to our knowledge, the best performance reported
in the literature for the NORB-small database, surpassing the
previous best by about 0.3%.
For SVHN, our best result is within ∼2% of state-of-theart. It is highly likely that using ﬁlters trained on the SVHN
database rather than on Imagenet would reduce this gap, given
the structured nature of digits, as opposed to the more complex
nature of Imagenet images. Another avenue for closing the gap
on state-of-the-art using the same ﬁlters would be to increase
M and decrease D, thus resulting in more features and more
classiﬁer hidden units. Although we increased M to 40000,
we did not observe saturation in the error rate as we increased
M to this point.
For CIFAR-10, it is less clear what is lacking in our method
in comparison with the gap of about 14% to state-of-the-art
methods. We note that CIFAR-10 has relatively few training
points, and we observed that the gap between classiﬁcation performance on the actual training set, in comparison with the test
set, can be up to 20%. This suggests that designing enhanced
methods of regularisation (e.g. methods similar to dropout in
the convolutional stage, or data augmentation) are necessary to
ensure our method can achieve good performance on CIFAR-
10. Another possibility is to use a nonlinearity in the classiﬁer
stage that ensures the hidden-layer responses reﬂect higher
order correlations than possible from the squaring function
we used. However, we expect that training the convolutional
ﬁlters in Stage 1 so that they extract features that are more
discriminative for the speciﬁc dataset will be the most likely
enhancement for improving results on CIFAR-10.
Finally, we note that there exist iterative approaches for
training the classiﬁer component of Stage 2 using least
squares regression, and without training the input weights—
see, e.g., . These methods can be easily adapted for
use with the convolutional front-end, if, for example, additional
batches of training data become available, or if the problem
involves online learning.
In closing, following acceptance of this paper, we became
aware of a newly published paper that combines convolutional
feature extraction with least squares regression training of classiﬁer weights to obtain good results for the NORB dataset .
The three main differences between the method of the current
paper and the method of are as follows. First, we used
a hidden layer in our classiﬁer stage, whereas solves for
output weights using least squares regression applied to the
output of the pooling stage. Second, we used a variety of
methods for the convolutional ﬁlter weights, whereas uses
orthogonalised random weights only. Third, we downsample
following pooling, whereas does not do so.
ACKNOWLEDGMENT
Mark D. McDonnell’s contribution was by supported by
an Australian Research Fellowship from the Australian Research Council (project number DP1093425). We gratefully
acknowledge Prof David Kearney and Dr Victor Stamatescu
from University of South Australia and Dr Sebastien Wong
of DSTO, Australia, for useful discussions and provision of
computing resources. We also acknowledge discussions with
Prof Philip De Chazal of University of Sydney.