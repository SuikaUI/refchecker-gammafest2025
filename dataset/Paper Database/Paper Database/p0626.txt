THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
Object Detection with Deep Learning: A Review
Zhong-Qiu Zhao, Member, IEEE, Peng Zheng,
Shou-tao Xu, and Xindong Wu, Fellow, IEEE
Abstract—Due to object detection’s close relationship with
video analysis and image understanding, it has attracted much
research attention in recent years. Traditional object detection
methods are built on handcrafted features and shallow trainable
architectures. Their performance easily stagnates by constructing
complex ensembles which combine multiple low-level image
features with high-level context from object detectors and scene
classiﬁers. With the rapid development in deep learning, more
powerful tools, which are able to learn semantic, high-level,
deeper features, are introduced to address the problems existing
in traditional architectures. These models behave differently
in network architecture, training strategy and optimization
function, etc. In this paper, we provide a review on deep
learning based object detection frameworks. Our review begins
with a brief introduction on the history of deep learning and
its representative tool, namely Convolutional Neural Network
(CNN). Then we focus on typical generic object detection
architectures along with some modiﬁcations and useful tricks
to improve detection performance further. As distinct speciﬁc
detection tasks exhibit different characteristics, we also brieﬂy
survey several speciﬁc tasks, including salient object detection,
face detection and pedestrian detection. Experimental analyses
are also provided to compare various methods and draw some
meaningful conclusions. Finally, several promising directions and
tasks are provided to serve as guidelines for future work in
both object detection and relevant neural network based learning
Index Terms—deep learning, object detection, neural network
I. INTRODUCTION
O gain a complete image understanding, we should not
only concentrate on classifying different images, but
also try to precisely estimate the concepts and locations of
objects contained in each image. This task is referred as object
detection [S1], which usually consists of different subtasks
such as face detection [S2], pedestrian detection [S2]
and skeleton detection [S3]. As one of the fundamental
computer vision problems, object detection is able to provide
valuable information for semantic understanding of images
and videos, and is related to many applications, including
image classiﬁcation , , human behavior analysis [S4],
face recognition [S5] and autonomous driving , .
Meanwhile, Inheriting from neural networks and related learning systems, the progress in these ﬁelds will develop neural
network algorithms, and will also have great impacts on object
detection techniques which can be considered as learning
systems. – [S6]. However, due to large variations in
viewpoints, poses, occlusions and lighting conditions, it’s difﬁcult to perfectly accomplish object detection with an additional
Zhong-Qiu Zhao, Peng Zheng and Shou-Tao Xu are with the College of
Computer Science and Information Engineering, Hefei University of Technology, China. Xindong Wu is with the School of Computing and Informatics,
University of Louisiana at Lafayette, USA.
Manuscript received August xx, 2017; revised xx xx, 2017.
object localization task. So much attention has been attracted
to this ﬁeld in recent years – .
The problem deﬁnition of object detection is to determine
where objects are located in a given image (object localization)
and which category each object belongs to (object classiﬁcation). So the pipeline of traditional object detection models
can be mainly divided into three stages: informative region
selection, feature extraction and classiﬁcation.
Informative region selection. As different objects may appear
in any positions of the image and have different aspect ratios
or sizes, it is a natural choice to scan the whole image with a
multi-scale sliding window. Although this exhaustive strategy
can ﬁnd out all possible positions of the objects, its shortcomings are also obvious. Due to a large number of candidate
windows, it is computationally expensive and produces too
many redundant windows. However, if only a ﬁxed number of
sliding window templates are applied, unsatisfactory regions
may be produced.
Feature extraction. To recognize different objects, we need
to extract visual features which can provide a semantic and
robust representation. SIFT , HOG and Haar-like 
features are the representative ones. This is due to the fact
that these features can produce representations associated with
complex cells in human brain . However, due to the diversity of appearances, illumination conditions and backgrounds,
it’s difﬁcult to manually design a robust feature descriptor to
perfectly describe all kinds of objects.
Classiﬁcation. Besides, a classiﬁer is needed to distinguish
a target object from all the other categories and to make the
representations more hierarchical, semantic and informative
for visual recognition. Usually, the Supported Vector Machine
(SVM) , AdaBoost and Deformable Part-based Model
(DPM) are good choices. Among these classiﬁers, the
DPM is a ﬂexible model by combining object parts with
deformation cost to handle severe deformations. In DPM, with
the aid of a graphical model, carefully designed low-level
features and kinematically inspired part decompositions are
combined. And discriminative learning of graphical models
allows for building high-precision part-based models for a
variety of object classes.
Based on these discriminant local feature descriptors and
shallow learnable architectures, state of the art results have
been obtained on PASCAL VOC object detection competition
 and real-time embedded systems have been obtained with
a low burden on hardware. However, small gains are obtained
during 2010-2012 by only building ensemble systems and
employing minor variants of successful methods . This fact
is due to the following reasons: 1) The generation of candidate
bounding boxes with a sliding window strategy is redundant,
inefﬁcient and inaccurate. 2) The semantic gap cannot be
 
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
Pedestrian
Salient object
Generic object
Bounding box
regression
Local contrast
Segmentation
Multi-feature
Boosting forest
Multi-scale
Fig. 1. The application domains of object detection.
bridged by the combination of manually engineered low-level
descriptors and discriminatively-trained shallow models.
Thanks to the emergency of Deep Neural Networks (DNNs)
 [S7], a more signiﬁcant gain is obtained with the introduction of Regions with CNN features (R-CNN) . DNNs, or
the most representative CNNs, act in a quite different way from
traditional approaches. They have deeper architectures with the
capacity to learn more complex features than the shallow ones.
Also the expressivity and robust training algorithms allow to
learn informative object representations without the need to
design features manually .
Since the proposal of R-CNN, a great deal of improved
models have been suggested, including Fast R-CNN which
jointly optimizes classiﬁcation and bounding box regression
tasks , Faster R-CNN which takes an additional subnetwork to generate region proposals and YOLO which
accomplishes object detection via a ﬁxed-grid regression .
All of them bring different degrees of detection performance
improvements over the primary R-CNN and make real-time
and accurate object detection become more achievable.
In this paper, a systematic review is provided to summarise
representative models and their different characteristics in
several application domains, including generic object detection , , , salient object detection , , face
detection – and pedestrian detection , . Their
relationships are depicted in Figure 1. Based on basic CNN architectures, generic object detection is achieved with bounding
box regression, while salient object detection is accomplished
with local contrast enhancement and pixel-level segmentation.
Face detection and pedestrian detection are closely related
to generic object detection and mainly accomplished with
multi-scale adaption and multi-feature fusion/boosting forest,
respectively. The dotted lines indicate that the corresponding
domains are associated with each other under certain conditions. It should be noticed that the covered domains are
diversiﬁed. Pedestrian and face images have regular structures,
while general objects and scene images have more complex
variations in geometric structures and layouts. Therefore,
different deep models are required by various images.
There has been a relevant pioneer effort which mainly
focuses on relevant software tools to implement deep learning
techniques for image classiﬁcation and object detection, but
pays little attention on detailing speciﬁc algorithms. Different
from it, our work not only reviews deep learning based object
detection models and algorithms covering different application domains in detail, but also provides their corresponding
experimental comparisons and meaningful analyses.
The rest of this paper is organized as follows. In Section
2, a brief introduction on the history of deep learning and the
basic architecture of CNN is provided. Generic object detection architectures are presented in Section 3. Then reviews
of CNN applied in several speciﬁc tasks, including salient
object detection, face detection and pedestrian detection, are
exhibited in Section 4-6, respectively. Several promising future
directions are proposed in Section 7. At last, some concluding
remarks are presented in Section 8.
II. A BRIEF OVERVIEW OF DEEP LEARNING
Prior to overview on deep learning based object detection
approaches, we provide a review on the history of deep
learning along with an introduction on the basic architecture
and advantages of CNN.
A. The History: Birth, Decline and Prosperity
Deep models can be referred to as neural networks with
deep structures. The history of neural networks can date back
to 1940s , and the original intention was to simulate the
human brain system to solve general learning problems in a
principled way. It was popular in 1980s and 1990s with the
proposal of back-propagation algorithm by Hinton et al. .
However, due to the overﬁtting of training, lack of large scale
training data, limited computation power and insigniﬁcance
in performance compared with other machine learning tools,
neural networks fell out of fashion in early 2000s.
Deep learning has become popular since 2006 [S7] with
a break through in speech recognition . The recovery of
deep learning can be attributed to the following factors.
• The emergence of large scale annotated training data, such
as ImageNet , to fully exhibit its very large learning
• Fast development of high performance parallel computing
systems, such as GPU clusters;
• Signiﬁcant advances in the design of network structures
and training strategies. With unsupervised and layerwise
pre-training guided by Auto-Encoder (AE) or Restricted Boltzmann Machine (RBM) , a good initialization is provided. With dropout and data augmentation, the
overﬁtting problem in training has been relieved , .
With batch normalization (BN), the training of very deep
neural networks becomes quite efﬁcient . Meanwhile,
various network structures, such as AlexNet , Overfeat
 , GoogLeNet , VGG and ResNet , have
been extensively studied to improve the performance.
What prompts deep learning to have a huge impact on the
entire academic community? It may owe to the contribution of
Hinton’s group, whose continuous efforts have demonstrated
that deep learning would bring a revolutionary breakthrough
on grand challenges rather than just obvious improvements on
small datasets. Their success results from training a large CNN
on 1.2 million labeled images together with a few techniques
 (e.g., ReLU operation and ‘dropout’ regularization).
B. Architecture and Advantages of CNN
CNN is the most representative model of deep learning .
A typical CNN architecture, which is referred to as VGG16,
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
can be found in Fig. S1. Each layer of CNN is known as a
feature map. The feature map of the input layer is a 3D matrix
of pixel intensities for different color channels (e.g. RGB). The
feature map of any internal layer is an induced multi-channel
image, whose ‘pixel’ can be viewed as a speciﬁc feature. Every
neuron is connected with a small portion of adjacent neurons
from the previous layer (receptive ﬁeld). Different types of
transformations , , can be conducted on feature
maps, such as ﬁltering and pooling. Filtering (convolution)
operation convolutes a ﬁlter matrix (learned weights) with
the values of a receptive ﬁeld of neurons and takes a nonlinear function (such as sigmoid , ReLU) to obtain ﬁnal
responses. Pooling operation, such as max pooling, average
pooling, L2-pooling and local contrast normalization ,
summaries the responses of a receptive ﬁeld into one value
to produce more robust feature descriptions.
With an interleave between convolution and pooling, an
initial feature hierarchy is constructed, which can be ﬁne-tuned
in a supervised manner by adding several fully connected (FC)
layers to adapt to different visual tasks. According to the tasks
involved, the ﬁnal layer with different activation functions 
is added to get a speciﬁc conditional probability for each
output neuron. And the whole network can be optimized on
an objective function (e.g. mean squared error or cross-entropy
loss) via the stochastic gradient descent (SGD) method. The
typical VGG16 has totally 13 convolutional (conv) layers, 3
fully connected layers, 3 max-pooling layers and a softmax
classiﬁcation layer. The conv feature maps are produced by
convoluting 3*3 ﬁlter windows, and feature map resolutions
are reduced with 2 stride max-pooling layers. An arbitrary test
image of the same size as training samples can be processed
with the trained network. Re-scaling or cropping operations
may be needed if different sizes are provided .
The advantages of CNN against traditional methods can be
summarised as follows.
• Hierarchical feature representation, which is the multilevel representations from pixel to high-level semantic features learned by a hierarchical multi-stage structure ,
 , can be learned from data automatically and hidden
factors of input data can be disentangled through multi-level
nonlinear mappings.
• Compared with traditional shallow models, a deeper
architecture provides an exponentially increased expressive
capability.
• The architecture of CNN provides an opportunity to
jointly optimize several related tasks together (e.g. Fast R-
CNN combines classiﬁcation and bounding box regression
into a multi-task leaning manner).
• Beneﬁtting from the large learning capacity of deep
CNNs, some classical computer vision challenges can be
recast as high-dimensional data transform problems and
solved from a different viewpoint.
Due to these advantages, CNN has been widely applied
into many research ﬁelds, such as image super-resolution
reconstruction , , image classiﬁcation , , image retrieval , , face recognition [S5], pedestrian
detection – and video analysis , .
III. GENERIC OBJECT DETECTION
Generic object detection aims at locating and classifying
existing objects in any one image, and labeling them with
rectangular bounding boxes to show the conﬁdences of existence. The frameworks of generic object detection methods
can mainly be categorized into two types (see Figure 2).
One follows traditional object detection pipeline, generating
region proposals at ﬁrst and then classifying each proposal into
different object categories. The other regards object detection
as a regression or classiﬁcation problem, adopting a uniﬁed
framework to achieve ﬁnal results (categories and locations)
directly. The region proposal based methods mainly include
R-CNN , SPP-net , Fast R-CNN , Faster R-CNN
 , R-FCN , FPN and Mask R-CNN , some of
which are correlated with each other (e.g. SPP-net modiﬁes R-
CNN with a SPP layer). The regression/classiﬁcation based
methods mainly includes MultiBox , AttentionNet ,
G-CNN , YOLO , SSD , YOLOv2 , DSSD
 and DSOD . The correlations between these two
pipelines are bridged by the anchors introduced in Faster R-
CNN. Details of these methods are as follows.
A. Region Proposal Based Framework
The region proposal based framework, a two-step process,
matches the attentional mechanism of human brain to some
extent, which gives a coarse scan of the whole scenario ﬁrstly
and then focuses on regions of interest. Among the pre-related
works , , , the most representative one is Overfeat
 . This model inserts CNN into sliding window method,
which predicts bounding boxes directly from locations of
the topmost feature map after obtaining the conﬁdences of
underlying object categories.
1) R-CNN: It is of signiﬁcance to improve the quality of
candidate bounding boxes and to take a deep architecture to
extract high-level features. To solve these problems, R-CNN
 was proposed by Ross Girshick in 2014 and obtained a
mean average precision (mAP) of 53.3% with more than 30%
improvement over the previous best result (DPM HSC ) on
PASCAL VOC 2012. Figure 3 shows the ﬂowchart of R-CNN,
which can be divided into three stages as follows.
Region proposal generation. The R-CNN adopts selective
search to generate about 2k region proposals for each
image. The selective search method relies on simple bottom-up
grouping and saliency cues to provide more accurate candidate
boxes of arbitrary sizes quickly and to reduce the searching
space in object detection , .
CNN based deep feature extraction. In this stage, each
region proposal is warped or cropped into a ﬁxed resolution
and the CNN module in is utilized to extract a 4096dimensional feature as the ﬁnal representation. Due to large
learning capacity, dominant expressive power and hierarchical
structure of CNNs, a high-level, semantic and robust feature
representation for each region proposal can be obtained.
Classiﬁcation and localization. With pre-trained categoryspeciﬁc linear SVMs for multiple classes, different region proposals are scored on a set of positive regions and background
(negative) regions. The scored regions are then adjusted with
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
Generic object
Region proposal
Regression/
Classification
Mask R-CNN
AttentionNet
Segmentation
Joint Grid
regression
Multi-scale
regression
Stem block
Dense block
Deconv layers
Fig. 2. Two types of frameworks: region proposal based and regression/classiﬁcation based. SPP: Spatial Pyramid Pooling , FRCN: Faster R-CNN ,
RPN: Region Proposal Network , FCN: Fully Convolutional Network , BN: Batch Normalization , Deconv layers: Deconvolution layers 
2. Extract region
proposals (~2k)
3. Compute
CNN features
aeroplane? no.
person? yes.
tvmonitor? no.
4. Classify
warped region
R-CNN: Regions with CNN features
Fig. 3. The ﬂowchart of R-CNN , which consists of 3 stages: (1) extracts
bottom-up region proposals, (2) computes features for each proposal using a
CNN, and then (3) classiﬁes each region with class-speciﬁc linear SVMs.
bounding box regression and ﬁltered with a greedy nonmaximum suppression (NMS) to produce ﬁnal bounding boxes
for preserved object locations.
When there are scarce or insufﬁcient labeled data, pretraining is usually conducted. Instead of unsupervised pretraining , R-CNN ﬁrstly conducts supervised pre-training
on ILSVRC, a very large auxiliary dataset, and then takes a
domain-speciﬁc ﬁne-tuning. This scheme has been adopted by
most of subsequent approaches , .
In spite of its improvements over traditional methods and
signiﬁcance in bringing CNN into practical object detection,
there are still some disadvantages.
• Due to the existence of FC layers, the CNN requires a
ﬁxed-size (e.g., 227×227) input image, which directly leads
to the re-computation of the whole CNN for each evaluated
region, taking a great deal of time in the testing period.
• Training of R-CNN is a multi-stage pipeline. At ﬁrst,
a convolutional network (ConvNet) on object proposals is
ﬁne-tuned. Then the softmax classiﬁer learned by ﬁnetuning is replaced by SVMs to ﬁt in with ConvNet features.
Finally, bounding-box regressors are trained.
• Training is expensive in space and time. Features are
extracted from different region proposals and stored on the
disk. It will take a long time to process a relatively small
training set with very deep networks, such as VGG16. At the
same time, the storage memory required by these features
should also be a matter of concern.
• Although selective search can generate region proposals
with relatively high recalls, the obtained region proposals
are still redundant and this procedure is time-consuming
(around 2 seconds to extract 2k region proposals).
To solve these problems, many methods have been proposed. GOP takes a much faster geodesic based segmentation to replace traditional graph cuts. MCG searches
different scales of the image for multiple hierarchical segmentations and combinatorially groups different regions to produce
proposals. Instead of extracting visually distinct segments,
the edge boxes method adopts the idea that objects are
more likely to exist in bounding boxes with fewer contours
straggling their boundaries. Also some researches tried to
re-rank or reﬁne pre-extracted region proposals to remove
unnecessary ones and obtained a limited number of valuable
ones, such as DeepBox and SharpMask .
In addition, there are some improvements to solve the
problem of inaccurate localization. Zhang et al. utilized
a bayesian optimization based search algorithm to guide
the regressions of different bounding boxes sequentially, and
trained class-speciﬁc CNN classiﬁers with a structured loss
to penalize the localization inaccuracy explicitly. Saurabh
Gupta et al. improved object detection for RGB-D images
with semantically rich image and depth features , and
learned a new geocentric embedding for depth images to
encode each pixel. The combination of object detectors and
superpixel classiﬁcation framework gains a promising result
on semantic scene segmentation task. Ouyang et al. proposed
a deformable deep CNN (DeepID-Net) which introduces
a novel deformation constrained pooling (def-pooling) layer
to impose geometric penalty on the deformation of various
object parts and makes an ensemble of models with different
settings. Lenc et al. provided an analysis on the role
of proposal generation in CNN-based detectors and tried to
replace this stage with a constant and trivial region generation
scheme. The goal is achieved by biasing sampling to match
the statistics of the ground truth bounding boxes with K-means
clustering. However, more candidate boxes are required to
achieve comparable results to those of R-CNN.
2) SPP-net: FC layers must take a ﬁxed-size input. That’s
why R-CNN chooses to warp or crop each region proposal
into the same size. However, the object may exist partly in
the cropped region and unwanted geometric distortion may be
produced due to the warping operation. These content losses or
distortions will reduce recognition accuracy, especially when
the scales of objects vary.
To solve this problem, He et al. took the theory of spatial
pyramid matching (SPM) , into consideration and
proposed a novel CNN architecture named SPP-net . SPM
takes several ﬁner to coarser scales to partition the image into
a number of divisions and aggregates quantized local features
into mid-level representations.
The architecture of SPP-net for object detection can be
found in Figure 4. Different from R-CNN, SPP-net reuses
feature maps of the 5-th conv layer (conv5) to project region
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
spatial pyramid
pooling layer
feature maps of conv5
convolutional layers
fixed-length representation
input image
fully-connected layers (fc6, fc7)
Fig. 4. The architecture of SPP-net for object detection .
feature map
projection
RoI feature
For each RoI
Fig. 5. The architecture of Fast R-CNN .
proposals of arbitrary sizes to ﬁxed-length feature vectors. The
feasibility of the reusability of these feature maps is due to
the fact that the feature maps not only involve the strength of
local responses, but also have relationships with their spatial
positions . The layer after the ﬁnal conv layer is referred
to as spatial pyramid pooling layer (SPP layer). If the number
of feature maps in conv5 is 256, taking a 3-level pyramid,
the ﬁnal feature vector for each region proposal obtained after
SPP layer has a dimension of 256 × (12 + 22 + 42) = 5376.
SPP-net not only gains better results with correct estimation
of different region proposals in their corresponding scales, but
also improves detection efﬁciency in testing period with the
sharing of computation cost before SPP layer among different
proposals.
3) Fast R-CNN: Although SPP-net has achieved impressive
improvements in both accuracy and efﬁciency over R-CNN,
it still has some notable drawbacks. SPP-net takes almost
the same multi-stage pipeline as R-CNN, including feature
extraction, network ﬁne-tuning, SVM training and boundingbox regressor ﬁtting. So an additional expense on storage space
is still required. Additionally, the conv layers preceding the
SPP layer cannot be updated with the ﬁne-tuning algorithm
introduced in . As a result, an accuracy drop of very deep
networks is unsurprising. To this end, Girshick introduced
a multi-task loss on classiﬁcation and bounding box regression
and proposed a novel CNN architecture named Fast R-CNN.
The architecture of Fast R-CNN is exhibited in Figure 5.
Similar to SPP-net, the whole image is processed with conv
layers to produce feature maps. Then, a ﬁxed-length feature
vector is extracted from each region proposal with a region of
interest (RoI) pooling layer. The RoI pooling layer is a special
case of the SPP layer, which has only one pyramid level. Each
feature vector is then fed into a sequence of FC layers before
ﬁnally branching into two sibling output layers. One output
layer is responsible for producing softmax probabilities for
all C + 1 categories (C object classes plus one ‘background’
class) and the other output layer encodes reﬁned boundingbox positions with four real-valued numbers. All parameters
in these procedures (except the generation of region proposals)
are optimized via a multi-task loss in an end-to-end way.
The multi-tasks loss L is deﬁned as below to jointly train
classiﬁcation and bounding-box regression,
L(p, u, tu, v) = Lcls(p, u) + λ[u ≥1]Lloc(tu, v)
where Lcls(p, u) = −log pu calculates the log loss for ground
truth class u and pu is driven from the discrete probability
distribution p = (p0, · · · , pC) over the C +1 outputs from the
last FC layer. Lloc(tu, v) is deﬁned over the predicted offsets
h) and ground-truth bounding-box regression
targets v = (vx, vy, vw, vh), where x, y, w, h denote the two
coordinates of the box center, width, and height, respectively.
Each tu adopts the parameter settings in to specify an
object proposal with a log-space height/width shift and scaleinvariant translation. The Iverson bracket indicator function
[u ≥1] is employed to omit all background RoIs. To provide
more robustness against outliers and eliminate the sensitivity
in exploding gradients, a smooth L1 loss is adopted to ﬁt
bounding-box regressors as below
Lloc(tu, v) =
smoothL1(tu
smoothL1(x) =
if |x| < 1
To accelerate the pipeline of Fast R-CNN, another two tricks
are of necessity. On one hand, if training samples (i.e. RoIs)
come from different images, back-propagation through the
SPP layer becomes highly inefﬁcient. Fast R-CNN samples
mini-batches hierarchically, namely N images sampled randomly at ﬁrst and then R/N RoIs sampled in each image,
where R represents the number of RoIs. Critically, computation and memory are shared by RoIs from the same image in
the forward and backward pass. On the other hand, much time
is spent in computing the FC layers during the forward pass
 . The truncated Singular Value Decomposition (SVD) 
can be utilized to compress large FC layers and to accelerate
the testing procedure.
In the Fast R-CNN, regardless of region proposal generation, the training of all network layers can be processed in
a single-stage with a multi-task loss. It saves the additional
expense on storage space, and improves both accuracy and
efﬁciency with more reasonable training schemes.
4) Faster R-CNN: Despite the attempt to generate candidate boxes with biased sampling , state-of-the-art object
detection networks mainly rely on additional methods, such as
selective search and Edgebox, to generate a candidate pool of
isolated region proposals. Region proposal computation is also
a bottleneck in improving efﬁciency. To solve this problem,
Ren et al. introduced an additional Region Proposal Network
(RPN) , , which acts in a nearly cost-free way by
sharing full-image conv features with detection network.
RPN is achieved with a fully-convolutional network, which
has the ability to predict object bounds and scores at each
position simultaneously. Similar to , RPN takes an image
of arbitrary size to generate a set of rectangular object proposals. RPN operates on a speciﬁc conv layer with the preceding
layers shared with object detection network.
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
conv feature map
intermediate layer
4k coordinates
sliding window
k anchor boxes
The RPN in Faster R-CNN . K predeﬁned anchor boxes are
convoluted with each sliding window to produce ﬁxed-length vectors which
are taken by cls and reg layer to obtain corresponding outputs.
The architecture of RPN is shown in Figure 6. The network
slides over the conv feature map and fully connects to an
n × n spatial window. A low dimensional vector (512-d for
VGG16) is obtained in each sliding window and fed into two
sibling FC layers, namely box-classiﬁcation layer (cls) and
box-regression layer (reg). This architecture is implemented
with an n × n conv layer followed by two sibling 1 × 1 conv
layers. To increase non-linearity, ReLU is applied to the output
of the n × n conv layer.
The regressions towards true bounding boxes are achieved
by comparing proposals relative to reference boxes (anchors).
In the Faster R-CNN, anchors of 3 scales and 3 aspect ratios
are adopted. The loss function is similar to (1).
L(pi, ti) =
Lcls(pi, p∗
i Lreg(ti, t∗
where pi shows the predicted probability of the i-th anchor
being an object. The ground truth label p∗
i is 1 if the anchor is
positive, otherwise 0. ti stores 4 parameterized coordinates of
the predicted bounding box while t∗
i is related to the groundtruth box overlapping with a positive anchor. Lcls is a binary
log loss and Lreg is a smoothed L1 loss similar to (2). These
two terms are normalized with the mini-batch size (Ncls)
and the number of anchor locations (Nreg), respectively. In
the form of fully-convolutional networks, Faster R-CNN can
be trained end-to-end by back-propagation and SGD in an
alternate training manner.
With the proposal of Faster R-CNN, region proposal based
CNN architectures for object detection can really be trained
in an end-to-end way. Also a frame rate of 5 FPS (Frame
Per Second) on a GPU is achieved with state-of-the-art object
detection accuracy on PASCAL VOC 2007 and 2012. However, the alternate training algorithm is very time-consuming
and RPN produces object-like regions (including backgrounds)
instead of object instances and is not skilled in dealing with
objects with extreme scales or shapes.
5) R-FCN: Divided by the RoI pooling layer, a prevalent
family , of deep networks for object detection are
composed of two subnetworks: a shared fully convolutional
subnetwork (independent of RoIs) and an unshared RoI-wise
subnetwork. This decomposition originates from pioneering
classiﬁcation architectures (e.g. AlexNet and VGG16 )
which consist of a convolutional subnetwork and several FC
layers separated by a speciﬁc spatial pooling layer.
Recent state-of-the-art image classiﬁcation networks, such
as Residual Nets (ResNets) and GoogLeNets , ,
are fully convolutional. To adapt to these architectures, it’s
(a) Featurized image pyramid
(b) Single feature map
(d) Feature Pyramid Network
(c) Pyramidal feature hierarchy
Fig. 7. The main concern of FPN . (a) It is slow to use an image pyramid
to build a feature pyramid. (b) Only single scale features is adopted for faster
detection. (c) An alternative to the featurized image pyramid is to reuse the
pyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both
(b) and (c). Blue outlines indicate feature maps and thicker outlines denote
semantically stronger features.
natural to construct a fully convolutional object detection network without RoI-wise subnetwork. However, it turns out to be
inferior with such a naive solution . This inconsistence is
due to the dilemma of respecting translation variance in object
detection compared with increasing translation invariance in
image classiﬁcation. In other words, shifting an object inside
an image should be indiscriminative in image classiﬁcation
while any translation of an object in a bounding box may
be meaningful in object detection. A manual insertion of
the RoI pooling layer into convolutions can break down
translation invariance at the expense of additional unshared
region-wise layers. So Li et al. proposed a region-based
fully convolutional networks (R-FCN, Fig. S2).
Different from Faster R-CNN, for each category, the last
conv layer of R-FCN produces a total of k2 position-sensitive
score maps with a ﬁxed grid of k × k ﬁrstly and a positionsensitive RoI pooling layer is then appended to aggregate the
responses from these score maps. Finally, in each RoI, k2
position-sensitive scores are averaged to produce a C + 1-d
vector and softmax responses across categories are computed.
Another 4k2-d conv layer is appended to obtain class-agnostic
bounding boxes.
With R-FCN, more powerful classiﬁcation networks can be
adopted to accomplish object detection in a fully-convolutional
architecture by sharing nearly all the layers, and state-of-theart results are obtained on both PASCAL VOC and Microsoft
COCO datasets at a test speed of 170ms per image.
6) FPN: Feature pyramids built upon image pyramids
(featurized image pyramids) have been widely applied in
many object detection systems to improve scale invariance
 , (Figure 7(a)). However, training time and memory
consumption increase rapidly. To this end, some techniques
take only a single input scale to represent high-level semantics
and increase the robustness to scale changes (Figure 7(b)),
and image pyramids are built at test time which results in
an inconsistency between train/test-time inferences , .
The in-network feature hierarchy in a deep ConvNet produces
feature maps of different spatial resolutions while introduces
large semantic gaps caused by different depths (Figure 7(c)).
To avoid using low-level features, pioneer works , 
usually build the pyramid starting from middle layers or
just sum transformed feature responses, missing the higher-
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
Fig. 8. The Mask R-CNN framework for instance segmentation .
resolution maps of the feature hierarchy.
Different from these approaches, FPN holds an architecture with a bottom-up pathway, a top-down pathway
and several lateral connections to combine low-resolution and
semantically strong features with high-resolution and semantically weak features (Figure 7(d)). The bottom-up pathway,
which is the basic forward backbone ConvNet, produces a
feature hierarchy by downsampling the corresponding feature
maps with a stride of 2. The layers owning the same size of
output maps are grouped into the same network stage and the
output of the last layer of each stage is chosen as the reference
set of feature maps to build the following top-down pathway.
To build the top-down pathway, feature maps from higher
network stages are upsampled at ﬁrst and then enhanced with
those of the same spatial size from the bottom-up pathway
via lateral connections. A 1 × 1 conv layer is appended to
the upsampled map to reduce channel dimensions and the
mergence is achieved by element-wise addition. Finally, a 3×3
convolution is also appended to each merged map to reduce
the aliasing effect of upsampling and the ﬁnal feature map is
generated. This process is iterated until the ﬁnest resolution
map is generated.
As feature pyramid can extract rich semantics from all
levels and be trained end-to-end with all scales, state-of-theart representation can be obtained without sacriﬁcing speed
and memory. Meanwhile, FPN is independent of the backbone
CNN architectures and can be applied to different stages of
object detection (e.g. region proposal generation) and to many
other computer vision tasks (e.g. instance segmentation).
7) Mask R-CNN: Instance segmentation is a challenging task which requires detecting all objects in an image and
segmenting each instance (semantic segmentation ). These
two tasks are usually regarded as two independent processes.
And the multi-task scheme will create spurious edge and
exhibit systematic errors on overlapping instances . To
solve this problem, parallel to the existing branches in Faster
R-CNN for classiﬁcation and bounding box regression, the
Mask R-CNN adds a branch to predict segmentation
masks in a pixel-to-pixel manner (Figure 8).
Different from the other two branches which are inevitably
collapsed into short output vectors by FC layers, the segmentation mask branch encodes an m × m mask to maintain the
explicit object spatial layout. This kind of fully convolutional
representation requires fewer parameters but is more accurate
than that of . Formally, besides the two losses in (1) for
classiﬁcation and bounding box regression, an additional loss
for segmentation mask branch is deﬁned to reach a multi-task
loss. An this loss is only associated with ground-truth class
and relies on the classiﬁcation branch to predict the category.
Because RoI pooling, the core operation in Faster R-CNN,
performs a coarse spatial quantization for feature extraction,
misalignment is introduced between the RoI and the features.
It affects classiﬁcation little because of its robustness to small
translations. However, it has a large negative effect on pixelto-pixel mask prediction. To solve this problem, Mask R-CNN
adopts a simple and quantization-free layer, namely RoIAlign,
to preserve the explicit per-pixel spatial correspondence faithfully. RoIAlign is achieved by replacing the harsh quantization
of RoI pooling with bilinear interpolation , computing the
exact values of the input features at four regularly sampled
locations in each RoI bin. In spite of its simplicity, this
seemingly minor change improves mask accuracy greatly,
especially under strict localization metrics.
Given the Faster R-CNN framework, the mask branch only
adds a small computational burden and its cooperation with
other tasks provides complementary information for object
detection. As a result, Mask R-CNN is simple to implement
with promising instance segmentation and object detection
results. In a word, Mask R-CNN is a ﬂexible and efﬁcient
framework for instance-level recognition, which can be easily
generalized to other tasks (e.g. human pose estimation [S4])
with minimal modiﬁcation.
8) Multi-task Learning, Multi-scale Representation and
Contextual Modelling:
Although the Faster R-CNN gets
promising results with several hundred proposals, it still struggles in small-size object detection and localization, mainly due
to the coarseness of its feature maps and limited information
provided in particular candidate boxes. The phenomenon is
more obvious on the Microsoft COCO dataset which consists
of objects at a broad range of scales, less prototypical images,
and requires more precise localization. To tackle these problems, it is of necessity to accomplish object detection with
multi-task learning , multi-scale representation and
context modelling to combine complementary information from multiple sources.
Multi-task Learning learns a useful representation for
multiple correlated tasks from the same input , .
Brahmbhatt et al. introduced conv features trained for object segmentation and ‘stuff’ (amorphous categories such as
ground and water) to guide accurate object detection of small
objects (StuffNet) . Dai et al. presented Multitask
Network Cascades of three networks, namely class-agnostic
region proposal generation, pixel-level instance segmentation
and regional instance classiﬁcation. Li et al. incorporated the
weakly-supervised object segmentation cues and region-based
object detection into a multi-stage architecture to fully exploit
the learned segmentation features .
Multi-scale Representation combines activations from
multiple layers with skip-layer connections to provide semantic information of different spatial resolutions . Cai et
al. proposed the MS-CNN to ease the inconsistency
between the sizes of objects and receptive ﬁelds with multiple
scale-independent output layers. Yang et al. investigated two
strategies, namely scale-dependent pooling (SDP) and layerwise cascaded rejection classiﬁers (CRC), to exploit appropriate scale-dependent conv features . Kong et al. proposed
the HyperNet to calculate the shared features between RPN
and object detection network by aggregating and compressing
hierarchical feature maps from different resolutions into a
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
uniform space .
Contextual Modelling improves detection performance by
exploiting features from or around RoIs of different support
regions and resolutions to deal with occlusions and local
similarities . Zhu et al. proposed the SegDeepM to exploit
object segmentation which reduces the dependency on initial
candidate boxes with Markov Random Field . Moysset
et al. took advantage of 4 directional 2D-LSTMs to
convey global context between different local regions and reduced trainable parameters with local parameter-sharing .
Zeng et al. proposed a novel GBD-Net by introducing gated
functions to control message transmission between different
support regions .
The Combination incorporates different components above
into the same model to improve detection performance further.
Gidaris et al. proposed the Multi-Region CNN (MR-CNN)
model to capture different aspects of an object, the
distinct appearances of various object parts and semantic
segmentation-aware features. To obtain contextual and multiscale representations, Bell et al. proposed the Inside-Outside
Net (ION) by exploiting information both inside and outside
the RoI with spatial recurrent neural networks and
skip pooling . Zagoruyko et al. proposed the MultiPath
architecture by introducing three modiﬁcations to the Fast
R-CNN , including multi-scale skip connections ,
a modiﬁed foveal structure and a novel loss function
summing different IoU losses.
9) Thinking in Deep Learning based Object Detection:
Apart from the above approaches, there are still many important factors for continued progress.
There is a large imbalance between the number of annotated
objects and background examples. To address this problem,
Shrivastava et al. proposed an effective online mining algorithm (OHEM) for automatic selection of the hard examples, which leads to a more effective and efﬁcient training.
Instead of concentrating on feature extraction, Ren et al.
made a detailed analysis on object classiﬁers , and
found that it is of particular importance for object detection
to construct a deep and convolutional per-region classiﬁer
carefully, especially for ResNets and GoogLeNets .
Traditional CNN framework for object detection is not
skilled in handling signiﬁcant scale variation, occlusion or
truncation, especially when only 2D object detection is involved. To address this problem, Xiang et al. proposed a
novel subcategory-aware region proposal network , which
guides the generation of region proposals with subcategory
information related to object poses and jointly optimize object
detection and subcategory classiﬁcation.
Ouyang et al. found that the samples from different classes
follow a longtailed distribution , which indicates that different classes with distinct numbers of samples have different
degrees of impacts on feature learning. To this end, objects are
ﬁrstly clustered into visually similar class groups, and then a
hierarchical feature learning scheme is adopted to learn deep
representations for each group separately.
In order to minimize computational cost and achieve the
state-of-the-art performance, with the ‘deep and thin’ design
principle and following the pipeline of Fast R-CNN, Hong et
al. proposed the architecture of PVANET , which adopts
some building blocks including concatenated ReLU ,
Inception , and HyperNet to reduce the expense on
multi-scale feature extraction and trains the network with batch
normalization , residual connections , and learning
rate scheduling based on plateau detection . The PVANET
achieves the state-of-the-art performance and can be processed
in real time on Titan X GPU (21 FPS).
B. Regression/Classiﬁcation Based Framework
Region proposal based frameworks are composed of several correlated stages, including region proposal generation,
feature extraction with CNN, classiﬁcation and bounding box
regression, which are usually trained separately. Even in recent
end-to-end module Faster R-CNN, an alternative training is
still required to obtain shared convolution parameters between
RPN and detection network. As a result, the time spent in
handling different components becomes the bottleneck in realtime application.
frameworks
regression/classiﬁcation, mapping straightly from image pixels
to bounding box coordinates and class probabilities, can
reduce time expense. We ﬁrstly reviews some pioneer CNN
models, and then focus on two signiﬁcant frameworks,
namely You only look once (YOLO) and Single Shot
MultiBox Detector (SSD) .
1) Pioneer Works: Previous to YOLO and SSD, many
researchers have already tried to model object detection as
a regression or classiﬁcation task.
Szegedy et al. formulated object detection task as a DNNbased regression , generating a binary mask for the
test image and extracting detections with a simple bounding
box inference. However, the model has difﬁculty in handling
overlapping objects, and bounding boxes generated by direct
upsampling is far from perfect.
Pinheiro et al. proposed a CNN model with two branches:
one generates class agnostic segmentation masks and the
other predicts the likelihood of a given patch centered on
an object . Inference is efﬁcient since class scores and
segmentation can be obtained in a single model with most of
the CNN operations shared.
Erhan et al. proposed regression based MultiBox to produce
scored class-agnostic region proposals , . A uniﬁed
loss was introduced to bias both localization and conﬁdences
of multiple components to predict the coordinates of classagnostic bounding boxes. However, a large quantity of additional parameters are introduced to the ﬁnal layer.
Yoo et al. adopted an iterative classiﬁcation approach to
handle object detection and proposed an impressive end-toend CNN architecture named AttentionNet . Starting from
the top-left (TL) and bottom-right (BR) corner of an image,
AttentionNet points to a target object by generating quantized
weak directions and converges to an accurate object boundary box with an ensemble of iterative predictions. However,
the model becomes quite inefﬁcient when handling multiple
categories with a progressive two-step procedure.
Najibi et al. proposed a proposal-free iterative grid based
object detector (G-CNN), which models object detection as
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
Fig. 9. Main idea of YOLO .
ﬁnding a path from a ﬁxed grid to boxes tightly surrounding
the objects . Starting with a ﬁxed multi-scale bounding box
grid, G-CNN trains a regressor to move and scale elements of
the grid towards objects iteratively. However, G-CNN has a
difﬁculty in dealing with small or highly overlapping objects.
2) YOLO: Redmon et al. proposed a novel framework
called YOLO, which makes use of the whole topmost feature
map to predict both conﬁdences for multiple categories and
bounding boxes. The basic idea of YOLO is exhibited in
Figure 9. YOLO divides the input image into an S × S grid and
each grid cell is responsible for predicting the object centered
in that grid cell. Each grid cell predicts B bounding boxes
and their corresponding conﬁdence scores. Formally, conﬁdence scores are deﬁned as Pr(Object) ∗IOU truth
pred , which
indicates how likely there exist objects (Pr(Object) ≥0) and
shows conﬁdences of its prediction (IOU truth
pred ). At the same
time, regardless of the number of boxes, C conditional class
probabilities (Pr(Classi|Object)) should also be predicted in
each grid cell. It should be noticed that only the contribution
from the grid cell containing an object is calculated.
At test time, class-speciﬁc conﬁdence scores for each box
are achieved by multiplying the individual box conﬁdence
predictions with the conditional class probabilities as follows.
Pr(Object) ∗IOU truth
pred ∗Pr(Classi|Object)
= Pr(Classi) ∗IOU truth
where the existing probability of class-speciﬁc objects in the
box and the ﬁtness between the predicted box and the object
are both taken into consideration.
During training, the following loss function is optimized,
(xi −ˆxi)2 + (yi −ˆyi)2
(pi(c) −ˆpi(c))2
In a certain cell i, (xi, yi) denote the center of the box relative
to the bounds of the grid cell, (wi, hi) are the normalized width
and height relative to the image size, Ci represents conﬁdence
scores, 1obj
indicates the existence of objects and 1obj
that the prediction is conducted by the jth bounding box
predictor. Note that only when an object is present in that grid
cell, the loss function penalizes classiﬁcation errors. Similarly,
when the predictor is ‘responsible’ for the ground truth box
(i.e. the highest IoU of any predictor in that grid cell is
achieved), bounding box coordinate errors are penalized.
The YOLO consists of 24 conv layers and 2 FC layers,
of which some conv layers construct ensembles of inception
modules with 1 × 1 reduction layers followed by 3 × 3 conv
layers. The network can process images in real-time at 45
FPS and a simpliﬁed version Fast YOLO can reach 155 FPS
with better results than other real-time detectors. Furthermore,
YOLO produces fewer false positives on background, which
makes the cooperation with Fast R-CNN become possible. An
improved version, YOLOv2, was later proposed in , which
adopts several impressive strategies, such as BN, anchor boxes,
dimension cluster and multi-scale training.
3) SSD: YOLO has a difﬁculty in dealing with small
objects in groups, which is caused by strong spatial constraints
imposed on bounding box predictions . Meanwhile, YOLO
struggles to generalize to objects in new/unusual aspect ratios/
conﬁgurations and produces relatively coarse features due to
multiple downsampling operations.
Aiming at these problems, Liu et al. proposed a Single Shot
MultiBox Detector (SSD) , which was inspired by the
anchors adopted in MultiBox , RPN and multi-scale
representation . Given a speciﬁc feature map, instead of
ﬁxed grids adopted in YOLO, the SSD takes advantage of a set
of default anchor boxes with different aspect ratios and scales
to discretize the output space of bounding boxes. To handle
objects with various sizes, the network fuses predictions from
multiple feature maps with different resolutions .
The architecture of SSD is demonstrated in Figure 10. Given
the VGG16 backbone architecture, SSD adds several feature
layers to the end of the network, which are responsible for
predicting the offsets to default boxes with different scales and
aspect ratios and their associated conﬁdences. The network is
trained with a weighted sum of localization loss (e.g. Smooth
L1) and conﬁdence loss (e.g. Softmax), which is similar to
(1). Final detection results are obtained by conducting NMS
on multi-scale reﬁned bounding boxes.
Integrating with hard negative mining, data augmentation
and a larger number of carefully chosen default anchors,
SSD signiﬁcantly outperforms the Faster R-CNN in terms of
accuracy on PASCAL VOC and COCO, while being three
times faster. The SSD300 (input image size is 300×300) runs
at 59 FPS, which is more accurate and efﬁcient than YOLO.
However, SSD is not skilled at dealing with small objects,
which can be relieved by adopting better feature extractor
backbone (e.g. ResNet101), adding deconvolution layers with
skip connections to introduce additional large-scale context
 and designing better network structure (e.g. Stem Block
and Dense Block) .
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
Fig. 10. The architecture of SSD 300 . SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor
boxes and their associated conﬁdences. Final detection results are obtained by conducting NMS on multi-scale reﬁned bounding boxes.
C. Experimental Evaluation
We compare various object detection methods on three
benchmark datasets, including PASCAL VOC 2007 ,
PASCAL VOC 2012 and Microsoft COCO . The
evaluated approaches include R-CNN , SPP-net , Fast
R-CNN , NOC , Bayes , MR-CNN&S-CNN
 , Faster R-CNN , HyperNet , ION , MS-
GR , StuffNet , SSD300 , SSD512 , OHEM
 , SDP+CRC , GCNN , SubCNN , GBD-Net
 , PVANET , YOLO , YOLOv2 , R-FCN
 , FPN , Mask R-CNN , DSSD and DSOD
 . If no speciﬁc instructions for the adopted framework
are provided, the utilized model is a VGG16 pretrained
on 1000-way ImageNet classiﬁcation task . Due to the
limitation of paper length, we only provide an overview, including proposal, learning method, loss function, programming
language and platform, of the prominent architectures in Table
I. Detailed experimental settings, which can be found in the
original papers, are missed. In addition to the comparisons of
detection accuracy, another comparison is provided to evaluate
their test consumption on PASCAL VOC 2007.
1) PASCAL VOC 2007/2012: PASCAL VOC 2007 and
2012 datasets consist of 20 categories. The evaluation terms
are Average Precision (AP) in each single category and mean
Average Precision (mAP) across all the 20 categories. Comparative results are exhibited in Table II and III, from which
the following remarks can be obtained.
• If incorporated with a proper way, more powerful backbone CNN models can deﬁnitely improve object detection
performance (the comparison among R-CNN with AlexNet,
R-CNN with VGG16 and SPP-net with ZF-Net ).
• With the introduction of SPP layer (SPP-net), end-toend multi-task architecture (FRCN) and RPN (Faster R-
CNN), object detection performance is improved gradually
and apparently.
• Due to large quantities of trainable parameters, in order to
obtain multi-level robust features, data augmentation is very
important for deep learning based models (Faster R-CNN
with ‘07’ ,‘07+12’ and ‘07+12+coco’).
• Apart from basic models, there are still many other factors
affecting object detection performance, such as multi-scale
and multi-region feature extraction (e.g. MR-CNN), modi-
ﬁed classiﬁcation networks (e.g. NOC), additional information from other correlated tasks (e.g. StuffNet, HyperNet),
multi-scale representation (e.g. ION) and mining of hard
negative samples (e.g. OHEM).
• As YOLO is not skilled in producing object localizations
of high IoU, it obtains a very poor result on VOC 2012.
However, with the complementary information from Fast
R-CNN (YOLO+FRCN) and the aid of other strategies,
such as anchor boxes, BN and ﬁne grained features, the
localization errors are corrected (YOLOv2).
• By combining many recent tricks and modelling the whole
network as a fully convolutional one, R-FCN achieves a
more obvious improvement of detection performance over
other approaches.
2) Microsoft COCO: Microsoft COCO is composed of
300,000 fully segmented images, in which each image has
an average of 7 object instances from a total of 80 categories.
As there are a lot of less iconic objects with a broad range
of scales and a stricter requirement on object localization,
this dataset is more challenging than PASCAL 2012. Object
detection performance is evaluated by AP computed under
different degrees of IoUs and on different object sizes. The
results are shown in Table IV.
Besides similar remarks to those of PASCAL VOC, some
other conclusions can be drawn as follows from Table IV.
• Multi-scale training and test are beneﬁcial in improving object detection performance, which provide additional
information in different resolutions (R-FCN). FPN and
DSSD provide some better ways to build feature pyramids
to achieve multi-scale representation. The complementary
information from other related tasks is also helpful for
accurate object localization (Mask R-CNN with instance
segmentation task).
Faster R-CNN and R-FCN, perform better than regression/classﬁcation based approaches, namely YOLO and
SSD, due to the fact that quite a lot of localization errors
are produced by regression/classﬁcation based approaches.
• Context modelling is helpful to locate small objects,
which provides additional information by consulting nearby
objects and surroundings (GBD-Net and multi-path).
• Due to the existence of a large number of nonstandard
small objects, the results on this dataset are much worse
than those of VOC 2007/2012. With the introduction of
other powerful frameworks (e.g. ResNeXt ) and useful
strategies (e.g. multi-task learning , ), the performance can be improved.
• The success of DSOD in training from scratch stresses the
importance of network design to release the requirements
for perfect pre-trained classiﬁers on relevant tasks and large
numbers of annotated samples.
3) Timing Analysis: Timing analysis (Table V) is conducted
on Intel i7-6700K CPU with a single core and NVIDIA Titan
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
AN OVERVIEW OF PROMINENT GENERIC OBJECT DETECTION ARCHITECTURES.
Multi-scale Input
Learning Method
Loss Function
Softmax Layer
End-to-end Train
R-CNN 
Selective Search
Hinge loss (classiﬁcation),Bounding box regression
SPP-net 
Hinge loss (classiﬁcation),Bounding box regression
Fast RCNN 
Selective Search
Class Log loss+bounding box regression
Faster R-CNN 
Class Log loss+bounding box regression
Python/Matlab
R-FCN 
Class Log loss+bounding box regression
Mask R-CNN 
Class Log loss+bounding box regression
TensorFlow/Keras
+Semantic sigmoid loss
Synchronized SGD
Class Log loss+bounding box regression
TensorFlow
Class sum-squared error loss+bounding box regression
+object conﬁdence+background conﬁdence
Class softmax loss+bounding box regression
YOLOv2 
Class sum-squared error loss+bounding box regression
+object conﬁdence+background conﬁdence
* ‘+’ denotes that corresponding techniques are employed while ‘-’ denotes that this technique is not considered. It should be noticed that R-CNN and SPP-net can not be trained end-to-end with a multi-task loss while the
other architectures are based on multi-task joint training. As most of these architectures are re-implemented on different platforms with various programming languages, we only list the information associated with the versions
by the referenced authors.
COMPARATIVE RESULTS ON VOC 2007 TEST SET (%).
Trained on
R-CNN (Alex) 
R-CNN(VGG16) 
SPP-net(ZF) 
Bayes 
Fast R-CNN 
SDP+CRC 
SubCNN 
StuffNet30 
MR-CNN&S-CNN 
HyperNet 
MS-GR 
OHEM+Fast R-CNN 
Faster R-CNN 
Faster R-CNN 
Faster R-CNN 
07+12+COCO
SSD300 
07+12+COCO
SSD512 
07+12+COCO
* ‘07’: VOC2007 trainval, ‘07+12’: union of VOC2007 and VOC2012 trainval, ‘07+12+COCO’: trained on COCO trainval35k at ﬁrst and then ﬁne-tuned on 07+12. The S in ION ‘07+12+S’ denotes SBD segmentation labels.
COMPARATIVE RESULTS ON VOC 2012 TEST SET (%).
Trained on
R-CNN(Alex) 
R-CNN(VGG16) 
Bayes 
Fast R-CNN 
SutffNet30 
MR-CNN&S-CNN 
HyperNet 
OHEM+Fast R-CNN 
07++12+coco
Faster R-CNN 
Faster R-CNN 
07++12+coco
YOLO+Fast R-CNN 
YOLOv2 
07++12+coco
SSD300 
07++12+coco
SSD512 
07++12+coco
R-FCN (ResNet101) 
07++12+coco
* ‘07++12’: union of VOC2007 trainval and test and VOC2012 trainval. ‘07++12+COCO’: trained on COCO trainval35k at ﬁrst then ﬁne-tuned on 07++12.
COMPARATIVE RESULTS ON MICROSOFT COCO TEST DEV SET (%).
Trained on 0.5:0.95 0.5 0.75
Fast R-CNN 
39.9 19.4 4.1 20.0 35.8 21.3 29.4 30.1 7.3 32.1 52.0
43.2 23.6 6.4 24.1 38.3 23.2 32.7 33.5 10.1 37.7 53.6
NOC+FRCN(VGG16) 
NOC+FRCN(Google) 
NOC+FRCN (ResNet101) 
GBD-Net 
OHEM+FRCN 
42.5 22.2 5.0 23.7 34.6
OHEM+FRCN* 
44.4 24.8 7.1 26.4 37.9
OHEM+FRCN* 
45.9 26.1 7.4 27.7 38.5
Faster R-CNN 
45.3 23.5 7.7 26.4 37.1 23.8 34.0 34.6 12.0 38.5 54.4
YOLOv2 
trainval35k
44.0 19.2 5.0 22.4 35.5 20.7 31.6 33.3 9.8 36.5 54.4
SSD300 
trainval35k
41.2 23.4 5.3 23.2 39.6 22.5 33.2 35.3 9.6 37.6 56.5
SSD512 
trainval35k
46.5 27.8 9.0 28.9 41.9 24.8 37.5 39.8 14.0 43.5 59.0
R-FCN (ResNet101) 
10.8 32.8 45.0
R-FCN*(ResNet101) 
10.4 32.4 43.3
R-FCN**(ResNet101) 
14.3 35.5 44.2
Multi-path 
51.9 36.3 13.6 37.2 47.8 29.9 46.0 48.3 23.4 56.0 66.4
FPN (ResNet101) 
trainval35k
59.1 39.0 18.2 39.0 48.2
Mask (ResNet101+FPN) 
trainval35k
60.3 41.7 20.1 41.1 50.2
Mask (ResNeXt101+FPN) trainval35k
62.3 43.4 22.1 43.2 51.2
DSSD513 (ResNet101) 
trainval35k
53.3 35.2 13.0 35.4 51.1 28.9 43.5 46.2 21.8 49.1 66.4
DSOD300 
47.3 30.6 9.4 31.5 47.0 27.3 40.7 43.0 16.7 47.1 65.0
* FRCN*: Fast R-CNN with multi-scale training, R-FCN*: R-FCN with multi-scale training, R-FCN**: R-FCN
with multi-scale training and testing, Mask: Mask R-CNN.
X GPU. Except for ‘SS’ which is processed with CPU, the
other procedures related to CNN are all evaluated on GPU.
From Table V, we can draw some conclusions as follows.
• By computing CNN features on shared feature maps
(SPP-net), test consumption is reduced largely. Test time is
further reduced with the uniﬁed multi-task learning (FRCN)
and removal of additional region proposal generation stage
(Faster R-CNN). It’s also helpful to compress the parameters
of FC layers with SVD (PAVNET and FRCN).
COMPARISON OF TESTING CONSUMPTION ON VOC 07 TEST SET.
Trained on
Test time(sec/img)
SS+R-CNN 
SS+SPP-net 
SS+FRCN 
SDP+CRC 
SS+HyperNet* 
MR-CNN&S-CNN 
Faster R-CNN(VGG16) 
Faster R-CNN(ResNet101) 
SSD300 
SSD512 
R-FCN(ResNet101) 
07+12+coco
YOLOv2(544*544) 
DSSD321(ResNet101) 
DSOD300 
07+12+coco
PVANET+ 
07+12+coco
PVANET+(compress) 
07+12+coco
* SS: Selective Search , SS*: ‘fast mode’ Selective Search , HyperNet*: the speed up version of
HyperNet and PAVNET+ (compresss): PAVNET with additional bounding box voting and compressed fully
convolutional layers.
• It takes additional test time to extract multi-scale features and contextual information (ION and MR-RCNN&S-
• It takes more time to train a more complex and deeper
network (ResNet101 against VGG16) and this time consumption can be reduced by adding as many layers into
shared fully convolutional layers as possible (FRCN).
• Regression based models can usually be processed in real-
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
time at the cost of a drop in accuracy compared with region
proposal based models. Also, region proposal based models
can be modiﬁed into real-time systems with the introduction
of other tricks (PVANET), such as BN , residual
connections .
IV. SALIENT OBJECT DETECTION
Visual saliency detection, one of the most important and
challenging tasks in computer vision, aims to highlight the
most dominant object regions in an image. Numerous applications incorporate the visual saliency to improve their
performance, such as image cropping and segmentation
 , image retrieval and object detection .
Broadly, there are two branches of approaches in salient
object detection, namely bottom-up (BU) and top-down
(TD) . Local feature contrast plays the central role in BU
salient object detection, regardless of the semantic contents of
the scene. To learn local feature contrast, various local and
global features are extracted from pixels, e.g. edges ,
spatial information . However, high-level and multi-scale
semantic information cannot be explored with these low-level
features. As a result, low contrast salient maps instead of
salient objects are obtained. TD salient object detection is taskoriented and takes prior knowledge about object categories
to guide the generation of salient maps. Taking semantic
segmentation as an example, a saliency map is generated in the
segmentation to assign pixels to particular object categories via
a TD approach . In a word, TD saliency can be viewed
as a focus-of-attention mechanism, which prunes BU salient
points that are unlikely to be parts of the object .
A. Deep learning in Salient Object Detection
Due to the signiﬁcance for providing high-level and multiscale feature representation and the successful applications
in many correlated computer vision tasks, such as semantic
segmentation , edge detection and generic object
detection , it is feasible and necessary to extend CNN to
salient object detection.
The early work by Eleonora Vig et al. follows a
completely automatic data-driven approach to perform a largescale search for optimal features, namely an ensemble of deep
networks with different layers and parameters. To address the
problem of limited training data, Kummerer et al. proposed the
Deep Gaze by transferring from the AlexNet to generate
a high dimensional feature space and create a saliency map. A
similar architecture was proposed by Huang et al. to integrate
saliency prediction into pre-trained object recognition DNNs
 . The transfer is accomplished by ﬁne-tuning DNNs’
weights with an objective function based on the saliency
evaluation metrics, such as Similarity, KL-Divergence and
Normalized Scanpath Saliency.
Some works combined local and global visual clues to
improve salient object detection performance. Wang et al.
trained two independent deep CNNs (DNN-L and DNN-G)
to capture local information and global contrast and predicted
saliency maps by integrating both local estimation and global
search . Cholakkal et al. proposed a weakly supervised
saliency detection framework to combine visual saliency from
bottom-up and top-down saliency maps, and reﬁned the results
with a multi-scale superpixel-averaging . Zhao et al.
proposed a multi-context deep learning framework, which
utilizes a uniﬁed learning framework to model global and
local context jointly with the aid of superpixel segmentation
 . To predict saliency in videos, Bak et al. fused two
static saliency models, namely spatial stream net and temporal stream net, into a two-stream framework with a novel
empirically grounded data augmentation technique .
Complementary information from semantic segmentation
and context modeling is beneﬁcial. To learn internal representations of saliency efﬁciently, He et al. proposed a novel superpixelwise CNN approach called SuperCNN , in which
salient object detection is formulated as a binary labeling
problem. Based on a fully convolutional neural network, Li
et al. proposed a multi-task deep saliency model, in which
intrinsic correlations between saliency detection and semantic
segmentation are set up . However, due to the conv layers
with large receptive ﬁelds and pooling layers, blurry object
boundaries and coarse saliency maps are produced. Tang et
al. proposed a novel saliency detection framework (CRPSD)
 , which combines region-level saliency estimation and
pixel-level saliency prediction together with three closely
related CNNs. Li et al. proposed a deep contrast network
to combine segment-wise spatial pooling and pixel-level fully
convolutional streams .
The proper integration of multi-scale feature maps is also
of signiﬁcance for improving detection performance. Based
on Fast R-CNN, Wang et al. proposed the RegionNet by
performing salient object detection with end-to-end edge preserving and multi-scale contextual modelling . Liu et al.
 proposed a multi-resolution convolutional neural network
(Mr-CNN) to predict eye ﬁxations, which is achieved by
learning both bottom-up visual saliency and top-down visual
factors from raw image data simultaneously. Cornia et al.
proposed an architecture which combines features extracted at
different levels of the CNN . Li et al. proposed a multiscale deep CNN framework to extract three scales of deep
contrast features , namely the mean-subtracted region,
the bounding box of its immediate neighboring regions and
the masked entire image, from each candidate region.
It is efﬁcient and accurate to train a direct pixel-wise
CNN architecture to predict salient objects with the aids of
RNNs and deconvolution networks. Pan et al. formulated
saliency prediction as a minimization optimization on the
Euclidean distance between the predicted saliency map and
the ground truth and proposed two kinds of architectures
 : a shallow one trained from scratch and a deeper one
adapted from deconvoluted VGG network. As convolutionaldeconvolution networks are not expert in recognizing objects
of multiple scales, Kuen et al. proposed a recurrent attentional
convolutional-deconvolution network (RACDNN) with several
spatial transformer and recurrent network units to conquer
this problem . To fuse local, global and contextual
information of salient objects, Tang et al. developed a deeplysupervised recurrent convolutional neural network (DSRCNN)
to perform a full image-to-image saliency detection .
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
B. Experimental Evaluation
Four representative datasets, including ECSSD , HKU-
IS , PASCALS , and SOD , are used to
evaluate several state-of-the-art methods. ECSSD consists of
1000 structurally complex but semantically meaningful natural
images. HKU-IS is a large-scale dataset containing over 4000
challenging images. Most of these images have more than
one salient object and own low contrast. PASCALS is a
subset chosen from the validation set of PASCAL VOC 2010
segmentation dataset and is composed of 850 natural images.
The SOD dataset possesses 300 images containing multiple
salient objects. The training and validation sets for different
datasets are kept the same as those in .
Two standard metrics, namely F-measure and the mean
absolute error (MAE), are utilized to evaluate the quality of a
saliency map. Given precision and recall values pre-computed
on the union of generated binary mask B and ground truth Z,
F-measure is deﬁned as below
Fβ = (1 + β2)Presion × Recall
β2Presion + Recall
where β2 is set to 0.3 in order to stress the importance of the
precision value.
The MAE score is computed with the following equation
ˆS(i, j) = ˆZ(i, j)
where ˆZ and ˆS represent the ground truth and the continuous
saliency map, respectively. W and H are the width and
height of the salient area, respectively. This score stresses
the importance of successfully detected salient objects over
detected non-salient pixels .
The following approaches are evaluated: CHM , RC
 , DRFI , MC , MDF , LEGS , DSR
 , MTDNN , CRPSD , DCL , ELD ,
NLDF and DSSC . Among these methods, CHM,
RC and DRFI are classical ones with the best performance
 , while the other methods are all associated with CNN.
F-measure and MAE scores are shown in Table VI.
From Table VI, we can ﬁnd that CNN based methods
perform better than classic methods. MC and MDF combine
the information from local and global context to reach a
more accurate saliency. ELD refers to low-level handcrafted
features for complementary information. LEGS adopts generic
region proposals to provide initial salient regions, which may
be insufﬁcient for salient detection. DSR and MT act in
different ways by introducing recurrent network and semantic
segmentation, which provide insights for future improvements.
CPRSD, DCL, NLDF and DSSC are all based on multi-scale
representations and superpixel segmentation, which provide
robust salient regions and smooth boundaries. DCL, NLDF
and DSSC perform the best on these four datasets. DSSC
earns the best performance by modelling scale-to-scale shortconnections.
Overall, as CNN mainly provides salient information in
local regions, most of CNN based methods need to model
visual saliency along region boundaries with the aid of superpixel segmentation. Meanwhile, the extraction of multiscale deep CNN features is of signiﬁcance for measuring local
conspicuity. Finally, it’s necessary to strengthen local connections between different CNN layers and as well to utilize
complementary information from local and global context.
V. FACE DETECTION
Face detection is essential to many face applications and acts
as an important pre-processing procedure to face recognition
 – , face synthesis , and facial expression
analysis . Different from generic object detection, this
task is to recognize and locate face regions covering a very
large range of scales (30-300 pts vs. 10-1000 pts). At the same
time, faces have their unique object structural conﬁgurations
(e.g. the distribution of different face parts) and characteristics
(e.g. skin color). All these differences lead to special attention
to this task. However, large visual variations of faces, such as
occlusions, pose variations and illumination changes, impose
great challenges for this task in real applications.
The most famous face detector proposed by Viola and
Jones trains cascaded classiﬁers with Haar-Like features
and AdaBoost, achieving good performance with real-time
efﬁciency. However, this detector may degrade signiﬁcantly
in real-world applications due to larger visual variations of
human faces. Different from this cascade structure, Felzenszwalb et al. proposed a deformable part model (DPM) for face
detection . However, for these traditional face detection
methods, high computational expenses and large quantities
of annotations are required to achieve a reasonable result.
Besides, their performance is greatly restricted by manually
designed features and shallow architecture.
A. Deep learning in Face Detection
Recently, some CNN based face detection approaches have
been proposed – .As less accurate localization results from independent regressions of object coordinates, Yu
et al. proposed a novel IoU loss function for predicting
the four bounds of box jointly. Farfade et al. proposed a
Deep Dense Face Detector (DDFD) to conduct multi-view face
detection, which is able to detect faces in a wide range of orientations without requirement of pose/landmark annotations.
Yang et al. proposed a novel deep learning based face detection
framework , which collects the responses from local facial parts (e.g. eyes, nose and mouths) to address face detection
under severe occlusions and unconstrained pose variations.
Yang et al. proposed a scale-friendly detection network
named ScaleFace, which splits a large range of target scales
into smaller sub-ranges. Different specialized sub-networks are
constructed on these sub-scales and combined into a single
one to conduct end-to-end optimization. Hao et al. designed an
efﬁcient CNN to predict the scale distribution histogram of the
faces and took this histogram to guide the zoom-in and zoomout of the image . Since the faces are approximately
in uniform scale after zoom, compared with other state-ofthe-art baselines, better performance is achieved with less
computation cost. Besides, some generic detection frameworks
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
COMPARISON BETWEEN STATE OF THE ART METHODS.
DRFI 
LEGS 
MTDNN 
CRPSD 
NLDF 
DSSC 
* The bigger wFβ is or the smaller MAE is, the better the performance is.
are extended to face detection with different modiﬁcations, e.g.
Faster R-CNN , , .
Some authors trained CNNs with other complementary
tasks, such as 3D modelling and face landmarks, in a multitask learning manner. Huang et al. proposed a uniﬁed endto-end FCN framework called DenseBox to jointly conduct
face detection and landmark localization . Li et al.
 proposed a multi-task discriminative learning framework
which integrates a ConvNet with a ﬁxed 3D mean face model
in an end-to-end manner. In the framework, two issues are
addressed to transfer from generic object detection to face
detection, namely eliminating predeﬁned anchor boxes by a
3D mean face model and replacing RoI pooling layer with
a conﬁguration pooling layer. Zhang et al. proposed a
deep cascaded multi-task framework named MTCNN which
exploits the inherent correlations between face detection and
alignment in unconstrained environment to boost up detection
performance in a coarse-to-ﬁne manner.
Reducing computational expenses is of necessity in real applications. To achieve real-time detection on mobile platform,
Kalinovskii and Spitsyn proposed a new solution of frontal
face detection based on compact CNN cascades . This
method takes a cascade of three simple CNNs to generate,
classify and reﬁne candidate object positions progressively.
To reduce the effects of large pose variations, Chen et al.
proposed a cascaded CNN denoted by Supervised Transformer
Network . This network takes a multi-task RPN to predict
candidate face regions along with associated facial landmarks
simultaneously, and adopts a generic R-CNN to verify the
existence of valid faces. Yang et al. proposed a three-stage
cascade structure based on FCNs , while in each stage, a
multi-scale FCN is utilized to reﬁne the positions of possible
faces. Qin et al. proposed a uniﬁed framework which achieves
better results with the complementary information from different jointly trained CNNs .
B. Experimental Evaluation
The FDDB dataset has a total of 2,845 pictures in
which 5,171 faces are annotated with elliptical shape. Two
types of evaluations are used: the discrete score and continuous
score. By varying the threshold of the decision rule, the ROC
curve for the discrete scores can reﬂect the dependence of
the detected face fractions on the number of false alarms.
Compared with annotations, any detection with an IoU ratio
exceeding 0.5 is treated as positive. Each annotation is only
associated with one detection. The ROC curve for the continuous scores is the reﬂection of face localization quality.
The evaluated models cover DDFD , CascadeCNN
 , ACF-multiscale , Pico , HeadHunter ,
True positive rate
False positive
CascadeCNN
ACF-multiscale
HeadHunter
Joint Cascade
SURF-multiview
Viola-Jones
Face-R-CNN
(a) Discrete ROC curves
True positive rate
False positive
CascadeCNN
ACF-multiscale
HeadHunter
Joint Cascade
SURF-multiview
Viola-Jones
Face-R-CNN
(b) Continuous ROC curves
Fig. 11. The ROC curves of state-of-the-art methods on FDDB.
Joint Cascade , SURF-multiview , Viola-Jones ,
NPDFace , Faceness , CCF , MTCNN ,
Conv3D , Hyperface , UnitBox , LDCF+ [S2],
DeepIR , HR-ER , Face-R-CNN and Scale-
Face . ACF-multiscale, Pico, HeadHunter, Joint Cascade,
SURF-multiview, Viola-Jones, NPDFace and LDCF+ are built
on classic hand-crafted features while the rest methods are
based on deep CNN features. The ROC curves are shown in
Figure 11.
From Figure 11(a), in spite of relatively competitive results
produced by LDCF+, it can be observed that most of classic
methods perform with similar results and are outperformed
by CNN based methods by a signiﬁcant margin. From Figure
11(b), it can be observed that most of CNN based methods
earn similar true positive rates between 60% and 70% while
DeepIR and HR-ER perform much better than them. Among
classic methods, Joint Cascade is still competitive. As earlier
works, DDFD and CCF directly make use of generated feature
maps and obtain relatively poor results. CascadeCNN builds
cascaded CNNs to locate face regions, which is efﬁcient but inaccurate. Faceness combines the decisions from different part
detectors, resulting in precise face localizations while being
time-consuming. The outstanding performance of MTCNN,
Conv3D and Hyperface proves the effectiveness of multi-task
learning. HR-ER and ScaleFace adaptively detect faces of
different scales, and make a balance between accuracy and
efﬁciency. DeepIR and Face-R-CNN are two extensions of the
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
Faster R-CNN architecture to face detection, which validate
the signiﬁcance and effectiveness of Faster R-CNN. Unitbox
provides an alternative choice for performance improvements
by carefully designing optimization loss.
From these results, we can draw the conclusion that
CNN based methods are in the leading position. The performance can be improved by the following strategies: designing
novel optimization loss, modifying generic detection pipelines,
building meaningful network cascades, adapting scale-aware
detection and learning multi-task shared CNN features.
VI. PEDESTRIAN DETECTION
Recently, pedestrian detection has been intensively studied,
which has a close relationship to pedestrian tracking ,
 , person re-identiﬁcation , and robot navigation , . Prior to the recent progress in DCNN based
methods , , some researchers combined boosted
decision forests with hand-crafted features to obtain pedestrian
detectors – . At the same time, to explicitly model
the deformation and occlusion, part-based models and
explicit occlusion handling , are of concern.
As there are many pedestrian instances of small sizes
in typical scenarios of pedestrian detection (e.g. automatic
driving and intelligent surveillance), the application of RoI
pooling layer in generic object detection pipeline may result
in ‘plain’ features due to collapsing bins. In the meantime, the
main source of false predictions in pedestrian detection is the
confusion of hard background instances, which is in contrast
to the interference from multiple categories in generic object
detection. As a result, different conﬁgurations and components
are required to accomplish accurate pedestrian detection.
A. Deep learning in Pedestrian Detection
Although DCNNs have obtained excellent performance on
generic object detection , , none of these approaches
have achieved better results than the best hand-crafted feature
based method for a long time, even when part-based
information and occlusion handling are incorporated .
Thereby, some researches have been conducted to analyze the
reasons. Zhang et al. attempted to adapt generic Faster R-CNN
 to pedestrian detection . They modiﬁed the downstream classiﬁer by adding boosted forests to shared, highresolution conv feature maps and taking a RPN to handle small
instances and hard negative examples. To deal with complex
occlusions existing in pedestrian images, inspired by DPM
 , Tian et al. proposed a deep learning framework called
DeepParts , which makes decisions based an ensemble of
extensive part detectors. DeepParts has advantages in dealing
with weakly labeled data, low IoU positive proposals and
partial occlusion.
Other researchers also tried to combine complementary information from multiple data sources. CompACT-Deep adopts
a complexity-aware cascade to combine hand-crafted features
and ﬁne-tuned DCNNs . Based on Faster R-CNN, Liu et
al. proposed multi-spectral deep neural networks for pedestrian
detection to combine complementary information from color
and thermal images . Tian et al. proposed a taskassistant CNN (TA-CNN) to jointly learn multiple tasks with
DETAILED BREAKDOWN PERFORMANCE COMPARISONS OF
STATE-OF-THE-ART MODELS ON CALTECH PEDESTRIAN DATASET. ALL
NUMBERS ARE REPORTED IN L-AMR.
Reasonable
Checkerboards+ 
LDCF++[S2]
SCF+AlexNet 
SA-FastRCNN 
MS-CNN 
DeepParts 
CompACT-Deep 
RPN+BF 
F-DNN+SS 
multiple data sources and to combine pedestrian attributes
with semantic scene attributes together. Du et al. proposed
a deep neural network fusion architecture for fast and robust
pedestrian detection . Based on the candidate bounding
boxes generated with SSD detectors , multiple binary
classiﬁers are processed parallelly to conduct soft-rejection
based network fusion (SNF) by consulting their aggregated
degree of conﬁdences.
However, most of these approaches are much more sophisticated than the standard R-CNN framework. CompACT-Deep
consists of a variety of hand-crafted features, a small CNN
model and a large VGG16 model . DeepParts contains
45 ﬁne-tuned DCNN models, and a set of strategies, including
bounding box shifting handling and part selection, are required
to arrive at the reported results . So the modiﬁcation and
simpliﬁcation is of signiﬁcance to reduce the burden on both
software and hardware to satisfy real-time detection demand.
Tome et al. proposed a novel solution to adapt generic object
detection pipeline to pedestrian detection by optimizing most
of its stages . Hu et al. trained an ensemble of
boosted decision models by reusing the conv feature maps, and
a further improvement was gained with simple pixel labelling
and additional complementary hand-crafted features. Tome
et al. proposed a reduced memory region based deep
CNN architecture, which fuses regional responses from both
ACF detectors and SVM classiﬁers into R-CNN. Ribeiro et
al. addressed the problem of Human-Aware Navigation 
and proposed a vision-based person tracking system guided
by multiple camera sensors.
B. Experimental Evaluation
The evaluation is conducted on the most popular Caltech
Pedestrian dataset . The dataset was collected from the
videos of a vehicle driving through an urban environment
and consists of 250,000 frames with about 2300 unique
pedestrians and 350,000 annotated bounding boxes (BBs).
Three kinds of labels, namely ‘Person (clear identiﬁcations)’,
‘Person? (unclear identiﬁcations)’ and ‘People (large group of
individuals)’, are assigned to different BBs. The performance
is measured with the log-average miss rate (L-AMR) which
is computed evenly spaced in log-space in the range 10−2 to
1 by averaging miss rate at the rate of nine false positives
per image (FPPI) . According to the differences in the
height and visible part of the BBs, a total of 9 popular settings
are adopted to evaluate different properties of these models.
Details of these settings are as .
Evaluated methods include Checkerboards+ , LDCF++
[S2], SCF+AlexNet , SA-FastRCNN , MS-CNN
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
 , DeepParts , CompACT-Deep , RPN+BF
 and F-DNN+SS . The ﬁrst two methods are based
on hand-crafted features while the rest ones rely on deep CNN
features. All results are exhibited in Table VII. From this table,
we observe that different from other tasks, classic handcrafted
features can still earn competitive results with boosted decision
forests , ACF and HOG+LUV channels [S2]. As
an early attempt to adapt CNN to pedestrian detection, the
features generated by SCF+AlexNet are not so discriminant
and produce relatively poor results. Based on multiple CNNs,
DeepParts and CompACT-Deep accomplish detection tasks via
different strategies, namely local part integration and cascade
network. The responses from different local part detectors
make DeepParts robust to partial occlusions. However, due to
complexity, it is too time-consuming to achieve real-time detection. The multi-scale representation of MS-CNN improves
accuracy of pedestrian locations. SA-FastRCNN extends Fast
R-CNN to automatically detecting pedestrians according to
their different scales, which has trouble when there are partial
occlusions. RPN+BF combines the detectors produced by
Faster R-CNN with boosting decision forest to accurately
locate different pedestrians. F-DNN+SS, which is composed
of multiple parallel classiﬁers with soft rejections, performs
the best followed by RPN+BF, SA-FastRCNN and MS-CNN.
In short, CNN based methods can provide more accurate
candidate boxes and multi-level semantic information for
identifying and locating pedestrians. Meanwhile, handcrafted
features are complementary and can be combined with CNN
to achieve better results. The improvements over existing CNN
methods can be obtained by carefully designing the framework
and classiﬁers, extracting multi-scale and part based semantic
information and searching for complementary information
from other related tasks, such as segmentation.
VII. PROMISING FUTURE DIRECTIONS AND TASKS
In spite of rapid development and achieved promising
progress of object detection, there are still many open issues
for future work.
The ﬁrst one is small object detection such as occurring
in COCO dataset and in face detection task. To improve
localization accuracy on small objects under partial occlusions,
it is necessary to modify network architectures from the
following aspects.
• Multi-task joint optimization and multi-modal information fusion. Due to the correlations between different
tasks within and outside object detection, multi-task joint
optimization has already been studied by many researchers
 . However, apart from the tasks mentioned in
Subs. III-A8, it is desirable to think over the characteristics
of different sub-tasks of object detection (e.g. superpixel
semantic segmentation in salient object detection) and extend multi-task optimization to other applications such as
instance segmentation , multi-object tracking and
multi-person pose estimation [S4]. Besides, given a speciﬁc
application, the information from different modalities, such
as text , thermal data and images , can be
fused together to achieve a more discriminant network.
• Scale adaption. Objects usually exist in different scales,
which is more apparent in face detection and pedestrian
detection. To increase the robustness to scale changes, it
is demanded to train scale-invariant, multi-scale or scaleadaptive detectors. For scale-invariant detectors, more powerful backbone architectures (e.g. ResNext ), negative
sample mining , reverse connection and subcategory modelling are all beneﬁcial. For multi-scale
detectors, both the FPN which produces multi-scale
feature maps and Generative Adversarial Network 
which narrows representation differences between small objects and the large ones with a low-cost architecture provide
insights into generating meaningful feature pyramid. For
scale-adaptive detectors, it is useful to combine knowledge
graph , attentional mechanism , cascade network
 and scale distribution estimation to detect objects adaptively.
• Spatial correlations and contextual modelling. Spatial
distribution plays an important role in object detection. So
region proposal generation and grid regression are taken
to obtain probable object locations. However, the correlations between multiple proposals and object categories
are ignored. Besides, the global structure information is
abandoned by the position-sensitive score maps in R-FCN.
To solve these problems, we can refer to diverse subset
selection and sequential reasoning tasks for
possible solutions. It is also meaningful to mask salient parts
and couple them with the global structure in a joint-learning
manner .
The second one is to release the burden on manual labor and
accomplish real-time object detection, with the emergence of
large-scale image and video data. The following three aspects
can be taken into account.
• Cascade network. In a cascade network, a cascade of
detectors are built in different stages or layers , .
And easily distinguishable examples are rejected at shallow
layers so that features and classiﬁers at latter stages can
handle more difﬁcult samples with the aid of the decisions
from previous stages. However, current cascades are built in
a greedy manner, where previous stages in cascade are ﬁxed
when training a new stage. So the optimizations of different
CNNs are isolated, which stresses the necessity of end-toend optimization for CNN cascade. At the same time, it
is also a matter of concern to build contextual associated
cascade networks with existing layers.
• Unsupervised and weakly supervised learning. It’s
very time consuming to manually draw large quantities
of bounding boxes. To release this burden, semantic prior
 , unsupervised object discovery , multiple instance
learning and deep neural network prediction can
be integrated to make best use of image-level supervision to
assign object category tags to corresponding object regions
and reﬁne object boundaries. Furthermore, weakly annotations (e.g. center-click annotations ) are also helpful
for achieving high-quality detectors with modest annotation
efforts, especially aided by the mobile platform.
• Network optimization. Given speciﬁc applications and
platforms, it is signiﬁcant to make a balance among speed,
THIS PAPER HAS BEEN ACCEPTED BY IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS FOR PUBLICATION
memory and accuracy by selecting an optimal detection
architecture , . However, despite that detection
accuracy is reduced, it is more meaningful to learn compact
models with fewer number of parameters . And this
situation can be relieved by introducing better pre-training
schemes , knowledge distillation and hint learning . DSOD also provides a promising guideline to
train from scratch to bridge the gap between different image
sources and tasks .
The third one is to extend typical methods for 2D object detection to adapt 3D object detection and video object detection,
with the requirements from autonomous driving, intelligent
transportation and intelligent surveillance.
• 3D object detection. With the applications of 3D sensors
(e.g. LIDAR and camera), additional depth information can
be utilized to better understand the images in 2D and extend
the image-level knowledge to the real world. However,
seldom of these 3D-aware techniques aim to place correct
3D bounding boxes around detected objects. To achieve
better bounding results, multi-view representation and
3D proposal network may provide some guidelines to
encode depth information with the aid of inertial sensors
(accelerometer and gyrometer) .
• Video object detection. Temporal information across
different frames play an important role in understanding
the behaviors of different objects. However, the accuracy
suffers from degenerated object appearances (e.g., motion
blur and video defocus) in videos and the network is
usually not trained end-to-end. To this end, spatiotemporal
tubelets , optical ﬂow and LSTM should
be considered to fundamentally model object associations
between consecutive frames.
VIII. CONCLUSION
Due to its powerful learning ability and advantages in
dealing with occlusion, scale transformation and background
switches, deep learning based object detection has been a
research hotspot in recent years. This paper provides a detailed
review on deep learning based object detection frameworks
which handle different sub-problems, such as occlusion, clutter
and low resolution, with different degrees of modiﬁcations
on R-CNN. The review starts on generic object detection
pipelines which provide base architectures for other related
tasks. Then, three other common tasks, namely salient object
detection, face detection and pedestrian detection, are also
brieﬂy reviewed. Finally, we propose several promising future
directions to gain a thorough understanding of the object
detection landscape. This review is also meaningful for the
developments in neural networks and related learning systems,
which provides valuable insights and guidelines for future
ACKNOWLEDGMENTS
This research was supported by the National Natural Science Foundation of China (No.61672203 & 61375047 &
91746209), the National Key Research and Development Program of China (2016YFB1000901), and Anhui Natural Science Funds for Distinguished Young Scholar (No.170808J08).