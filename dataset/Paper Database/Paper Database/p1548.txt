Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models
Daniel W. Apley and Jingyu Zhu
Northwestern University, USA
Summary. In many supervised learning applications, understanding and visualizing the effects of
the predictor variables on the predicted response is of paramount importance. A shortcoming of
black box supervised learning models (e.g., complex trees, neural networks, boosted trees, random
forests, nearest neighbors, local kernel-weighted methods, support vector regression, etc.) in this
regard is their lack of interpretability or transparency. Partial dependence (PD) plots, which are the
most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they
require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to PD plots, we present a new visualization approach
that we term accumulated local effects (ALE) plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, ALE plots are far less computationally expensive than
Keywords: Functional ANOVA; Marginal plots; Partial dependence plots; Supervised learning; Visualization
Introduction
With the proliferation of larger and richer data sets in many predictive modeling application
domains, black box supervised learning models (e.g., complex trees, neural networks, boosted
trees, random forests, nearest neighbors, local kernel-weighted methods, support vector regression, etc.)
are increasingly commonly used in place of more transparent linear and logistic
regression models to capture nonlinear phenomena. However, one shortcoming of black box
supervised learning models is that they are diﬃcult to interpret in terms of understanding the
eﬀects of the predictor variables (aka predictors) on the predicted response. For many applications, understanding the eﬀects of the predictors is critically important. This is obviously the
case if the purpose of the predictive modeling is explanatory, such as identifying new disease
risk factors from electronic medical record databases. Even if the purpose is purely predictive,
understanding the eﬀects of the predictors may still be quite important. If the eﬀect of a predictor violates intuition (e.g., if it appears from the supervised learning model that the risk of
experiencing a cardiac event decreases as patients age), then this is either an indication that the
ﬁtted model is unreliable or that a surprising new phenomenon has been discovered. In addition,
predictive models must be transparent in many regulatory environments, e.g., to demonstrate
to regulators that consumer credit risk models do not penalize credit applicants based on age,
race, etc.
To be more concrete, suppose we have ﬁt a supervised learning model for approximating
E[Y |X = x] ≈f(x), where Y is a scalar response variable, X = (X1, X2, . . . , Xd) is a vector of
d predictors, and f(·) is the ﬁtted model that predicts Y (or the probability that Y falls into a
particular class, in the classiﬁcation setting) as a function of X. To simplify notation, we omit
symbol over f, with the understanding that it is ﬁtted from data. The training data
to which the model is ﬁt consists of n (d + 1)-variate observations {yi, xi = (xi,1, xi,2, . . . , xi,d) :
†Address for correspondence: Daniel W. Apley, Department of Industrial Engineering & Management
Sciences, Northwestern University, Evanston, IL 60208, USA.
E-mail: 
 
D. Apley and J. Zhu
Fig. 1. Illustration of the differences between the computation of (a) f1,P D(x1) and (b) f1,M(x1) at
i = 1, 2, . . . , n}. Throughout, we use upper case to denote a random variable and lower case to
denote speciﬁc or observed values of the random variable.
Our objective is to visualize and understand the “main eﬀects” dependence of f(x) =
f(x1, x2, . . . , xd) on each of the individual predictors, as well as the low-order “interaction effects” among pairs of predictors. Throughout the introduction we illustrate concepts for the
simple d = 2 case. The most popular approach for visualizing the eﬀects of the predictors is
partial dependence (PD) plots, introduced by Friedman . To understand the eﬀect of one
predictor (say X1) on the predicted response, a PD plot is a plot of the function
f1,PD(x1) ≡E[f(x1, X2)] =
p2(x2)f(x1, x2)dx2
versus x1, where p2(·) denotes the marginal distribution of X2. We use p(·) to denote the full
joint probability density of X, and use p·(·), p·|·(·|·), and p·,·(·, ·) to respectively denote the
marginal, conditional, and joint probability density functions of various elements of X, with the
subscripts indicating which elements. An estimate of (1), calculated pointwise in x1 for a range
of x1 values, is
ˆf1,PD(x1) ≡1
f(x1, xi,2).
Figure 1(a) illustrates how f1,PD(x1) is computed at a speciﬁc value x1 = 0.3 for a toy example
with n = 200 observations of (X1, X2) following a uniform distribution along the line segment
x2 = x1 but with independent N(0, 0.052) variables added to both predictors ,
for a similar example demonstrating the adverse consequences of extrapolation in PD plots).
Although we ignore the response variable for now, we return to this example in Section 4 and
ﬁt various models f(x) to these data. The salient point in Figure 1(a), which illustrates the
problem with PD plots, is that the integral in (1) is the weighted average of f(x1, X2) as X2
varies over its marginal distribution. This integral is over the entire vertical line segment in
Figure 1(a) and requires rather severe extrapolation beyond the envelope of the training data.
If one were to ﬁt a simple parametric model of the correct form (e.g., f(x) = β0 + β1x1 + β2x2
then this extrapolation might be reliable. However, by nature of its ﬂexibility, a nonparametric
supervised learning model like a regression tree cannot be expected to extrapolate reliably. As
we demonstrate later (see Figures 5—7), this renders the PD plot an unreliable indicator of the
eﬀect of x1.
Visualizing Supervised Learning Models
The extrapolation in Figure 1(a) that is required to calculate f1,PD(x1) occurs because the
marginal density p2(x2) is much less concentrated around the data than the conditional density
p2|1(x2|x1), due to the strong dependence between X2 and X1. Marginal plots (M plots) are
alternatives to PD plots that avoid such extrapolation by using the conditional density in place
of the marginal density. As illustrated in Figure 1(b), an M plot of the eﬀect of X1 is a plot of
the function
f1,M(x1) ≡E[f(X1, X2)|X1 = x1] =
p2|1(x2|x1)f(x1, x2)dx2
versus x1. A crude estimate of f1,M(x1) is
ˆf1,M(x1) ≡
f(x1, xi,2),
where N(x1) ⊂{1, 2, . . . , n} is the subset of row indices i for which xi,1 falls into some small,
appropriately selected neighborhood of x1, and n(x1) is the number of observations in the
neighborhood. Although more sophisticated kernel smoothing methods are typically used to
estimate f1,M(x1), we do not consider them here, because there is a more serious problem with
using f1,M(x1) to visualize the main eﬀect of X1 when X1 and X2 are dependent. Namely, using
f1,M(x1) is like regressing Y onto X1 while ignoring (i.e., marginalizing‡ over) the nuisance
variable X2.
Consequently, if Y depends on X1 and X2, f1,M(x1) will reﬂect both of their
eﬀects, a consequence of the omitted variable bias phenomenon in regression.
The main objective of this paper is to introduce a new method of assessing the main and
interaction eﬀects of the predictors in black box supervised learning models that avoids the
foregoing problems with PD plots and M plots. We refer to the approach as accumulated local
eﬀects (ALE) plots. For the case that d = 2 and f(·) is diﬀerentiable (the more general deﬁnition
is deferred until Section 2), we deﬁne the ALE main-eﬀect of X1 as
f1,ALE(x1) ≡
E[f1(X1, X2)|X1 = z1]dz1 −constant
p2|1(x2|z1)f1(z1, x2)dx2dz1 −constant,
where f1(x1, x2) ≡∂f(x1,x2)
represents the local eﬀect of x1 on f(·) at (x1, x2), and xmin,1 is
some value chosen near the lower bound of the eﬀective support of p1(·), e.g., just below the
smallest observation min{xi,1 : i = 1, 2, . . . , n}. Choice of xmin,1 is not important, as it only
aﬀects the vertical translation of the ALE plot of f1,ALE(x1) versus x1, and the constant in (5)
will be chosen to vertically center the plot.
The function f1,ALE(x1) can be interpreted as the accumulated local eﬀects of x1 in the following sense. In (5), we calculate the local eﬀect f1(x1, x2) of x1 at (x1 = z1, x2), then average this
local eﬀect across all values of x2 with weight p2|1(x2|z1), and then ﬁnally accumulate/integrate
this averaged local eﬀect over all values of z1 up to x1. As illustrated in Figure 2, when averaging the local eﬀect f1(x1, x2) across x2, the use of the conditional density p2|1(x2|x1), instead
of the marginal density p2(x2), avoids the extrapolation required in PD plots. The avoidance of
extrapolation is similar to M plots, which also use the conditional density p2|1(x2|x1). However,
by averaging (across x2) and accumulating (up to x1) the local eﬀects via (5), as opposed to
directly averaging f(·) via (3), ALE plots avoid the omitted nuisance variable bias that renders
M plots of little use for assessing the main eﬀects of the predictors. This relates closely to the
‡Regarding the terminology, plots of an estimate of f1,M(x1) are often referred to as “marginal plots”,
because ignoring X2 in this manner is equivalent to working with the joint distribution of (Y, X1) after
marginalizing across X2. Unfortunately, plots of ˆf1,P D(x1) are also sometimes referred to as “marginal
plots” (e.g., in the gbm package for ﬁtting boosted trees in R), presumably because the integral in (1) is
with respect to the marginal distribution p2(x2). In this paper, marginal plots will refer to how we have
deﬁned them above.
D. Apley and J. Zhu
Fig. 2. Illustration of the computation of f1,ALE(x1) at x1 = 0.3
use of paired diﬀerences to block out nuisance factors in more general statistical settings, which
we discuss in Section 5.3.
Methods also exist for visualizing the eﬀects of predictors by plotting a collection of curves,
rather than a single curve that represents some aggregate eﬀect. Consider the eﬀect of a single
predictor Xj, and let X\j denote the other predictors. Conditioning plots (coplots) ; Cleveland ), conditional response (CORE) plots ), and
individual conditional expectation (ICE) plots ) plot quantities like
f(xj, x\j) vs. xj for a collection of discrete values of x\j (CORE and ICE plots), or similarly
they plot E[f(xj, X\j)|X\j ∈Sk] vs. xj for each set Sk in some partition {Sk : k = 1, 2, . . .} of
the space of X\j. Such a collection of curves have more in common with interaction eﬀect plots
(as in Figure 10, later) than with main eﬀect plots, for which one desires, by deﬁnition, a single
aggregated curve.
The format of the remainder of the paper is as follows. In Section 2, we deﬁne the ALE
main eﬀects for individual predictors and the ALE second-order interaction eﬀects for pairs of
predictors. In Section 3 we present estimators of the ALE main and second-order interaction effects, which are conceptually straightforward and computationally eﬃcient (much more eﬃcient
than PD plots), and we prove their consistency.
We focus on main and second-order interaction eﬀects and discuss general higher-order eﬀects and their estimation in the appendices.
In Section 4 we give examples that illustrate the ALE plots and, in particular, how they can
produce correct results when PD plots are corrupted due to their reliance on extrapolation. In
Section 5, we discuss interpretation of ALE plots and a number of their desirable properties and
computational advantages, and we illustrate with a real data example. We also discuss their
relation to functional ANOVA decompositions for dependent variables ) that
have been developed to avoid the same extrapolation problem highlighted in Figure 1(a). ALE
plots are far more computationally eﬃcient and systematic to compute than functional ANOVA
decompositions; and they yield a fundamentally diﬀerent decomposition of f(x) that is better
suited for visualization of the eﬀects of the predictors. Section 6 concludes the paper. We also
provide as supplementary material an R package ALEPlot to implement ALE plots.
Visualizing Supervised Learning Models
Deﬁnition of ALE Main and Second-Order Effects
In this section we deﬁne the ALE main eﬀect functions for each predictor (Eq. (5) is a special
case for d = 2 and diﬀerentiable f(·)) and the ALE second-order eﬀect functions for each pair of
predictors. ALE plots are plots of estimates of these functions, and the estimators are deﬁned
in Section 3. We do not envision ALE plots being commonly used to visualize third- and highorder eﬀects, since high-order eﬀects are diﬃcult to interpret and usually not as predominant as
main and second-order eﬀects. For this reason, and to simplify notation, we focus on main and
second-order eﬀects and relegate the deﬁnition of higher-order ALE eﬀects to the appendices.
Throughout this section, we assume that p has compact support S, and the support of pj
is the interval Sj = [xmin,j, xmax,j] for each j ∈{1, 2, . . . , d}.
For each K = 1, 2, . . . , and
j ∈{1, 2, . . . , d}, let PK
k,j : k = 0, 1, . . . , K} be a partition of Sj into K intervals with
0,j = xmin,j and zK
K,j = xmax,j. Deﬁne δj,K ≡max{|zK
k−1,j| : k = 1, 2, . . . , K}, which
represents the ﬁneness of the partition. For any x ∈Sj, deﬁne kK
j (x) to be the index of the
interval of PK
into which x falls, i.e., x ∈(zK
k,j] for k = kK
j (x). Let X\j denote the
subset of d −1 predictors excluding Xj, i.e., X\j = (Xk : k = 1, 2, . . . , d; k ̸= j). The following
deﬁnition is a generalization of (5) for a function f(·) that is not necessarily diﬀerentiable and
for any d ≥2. The generalization essentially replaces the derivative and integral in (5) with
limiting forms of ﬁnite diﬀerences and summations, respectively.
Deﬁnition 1 (Uncentered ALE Main Eﬀect). Consider any j ∈{1, 2, . . . , d}, and suppose
the sequence of partitions {PK
: K = 1, 2, . . .} is such that limK→∞δj,K = 0. When f(·) and p
are such that the following limit exists and is independent of the particular sequence of partitions
: K = 1, 2, . . .} (see Theorem A.1 in Appendix A for suﬃcient conditions on the existence
and uniqueness of the limit), we deﬁne the uncentered ALE main (aka ﬁrst-order) eﬀect function
of Xj as (for xj ∈Sj)
gj,ALE(xj) ≡lim
k,j, X\j) −f(zK
k−1,j, X\j)|Xj ∈(zK
The following theorem, the proof of which is in Appendix A, states that for diﬀerentiable
f(·), the uncentered ALE main eﬀect of Xj in (6) has an equivalent but more revealing form
that is analogous to (5).
Theorem 1 (Uncentered ALE Main Eﬀect for diﬀerentiable f(·)). Let fj(xj, x\j) ≡
∂f(xj,x\j)
denote the partial derivative of f(x) with respect to xj when the derivative exists.
In Deﬁnition 1, suppose
(i) f(xj, x\j) is diﬀerentiable in xj on S,
(ii) fj(xj, x\j) is continuous in (xj, x\j) on S, and
(iii) E[fj(Xj, X\j)|Xj = zj] is continuous in zj on Sj.
Then, for each xj ∈Sj,
gj,ALE(xj) =
E[fj(Xj, X\j)|Xj = zj]dzj.
(End of Theorem 1)
The (centered) ALE main eﬀect of Xj, denoted by fj,ALE(xj), is deﬁned the same as
gj,ALE(xj) but centered so that fj,ALE(Xj) has a mean of zero with respect to the marginal
distribution of Xj. That is,
fj,ALE(xj) ≡gj,ALE(xj) −E[gj,ALE(Xj)]
= gj,ALE(xj) −
pj(zj)gj,ALE(zj)dzj.
D. Apley and J. Zhu
Remark 1. The ALE plot function fj,ALE(xj) attempts to quantify something quite similar to
the PD plot function fj,PD(xj) in (1) and can be interpreted in the same manner.
For example, ALE plots and PD plots both have a desirable additive recovery property. That is, if
j=1 fj(xj) is additive, then both fj,ALE(xj) and fj,PD(xj) are equal to the desired true
eﬀect fj(xj), up to an additive constant. Hence, a plot of fj,ALE(xj) vs. xj correctly reveals the
true eﬀect of Xj on f, no matter how black-box the function f is. If second-order interaction
eﬀects are present in f, a similar additive recovery property holds for the ALE second-order
interaction eﬀects that we deﬁne next (see Section 5.3 for a more general additive recovery property that applies to interactions of any order). In spite of the similarities in the characteristics
of f that they are designed to extract, the diﬀerences in fj,ALE(xj) and fj,PD(xj) lead to very
diﬀerent methods of estimation. As will be demonstrated in the later sections, the ALE plot
functions are estimated in a far more computationally eﬃcient manner that also avoids the extrapolation problem that renders PD plots unreliable with highly correlated predictors.
We next deﬁne the ALE second-order eﬀects. For each pair of indices {j, l} ⊆{1, 2, . . . , d},
let X\{j,l} denote the subset of d −2 predictors excluding {Xj, Xl}, i.e., X\{j,l} = (Xk : k =
1, 2, . . . , d; k ̸= j; k ̸= l). For general f(·), the uncentered ALE second-order eﬀect of (Xj, Xl)
is deﬁned similarly to (6), except that we replace the 1-D ﬁnite-diﬀerences by 2-D second-order
ﬁnite diﬀerences on the 2-D grid that is the Cartesian product of the 1-D partitions of Sj and
Sl, and the summation is over this 2-D grid.
Deﬁnition 2 (Uncentered ALE Second-Order Eﬀect). Consider any pair {j, l} ⊆{1, . . . , d}
and corresponding sequences of partitions {PK
: K = 1, 2, . . .} and {PK
: K = 1, 2, . . .} such
that limK→∞δj,K = limK→∞δl,K = 0. When f(·) and p are such that the following limit exists
and is independent of the particular sequences of partitions, we deﬁne the uncentered ALE
second-order eﬀect function of (Xj, Xl) as (for (xj, xl) ∈Sj × Sl)
h{j,l},ALE(xj, xl) ≡lim
(K, k, m; X\{j,l})|Xj ∈(zK
k,j], Xl ∈(zK
(K, k, m; x\{j,l}) = [f(zK
m,l, x\{j,l}) −f(zK
m,l, x\{j,l})]
m−1,l, x\{j,l}) −f(zK
m−1,l, x\{j,l})]
is the second-order ﬁnite diﬀerence of f(x) = f(xj, xl, x\{j,l}) with respect to (xj, xl) across cell
k,j] × (zK
m,l] of the 2-D grid that is the Cartesian product of PK
Analogous to Theorem 1, Theorem 2 (proved in Appendix A) states that for diﬀerentiable
f(·), the uncentered ALE second-order eﬀect of (Xj, Xl) in (9) has an equivalent integral form.
Theorem 2 (Uncentered ALE Second-Order Eﬀect for diﬀerentiable f(·)). Let
f{j,l}(xj, xl, x\{j,l}) ≡
∂2f(xj,xl,x\{j,l})
denote the second-order partial derivative of f(x) with
respect to xj and xl when the derivative exists. In Deﬁnition 2, suppose
(i) f(xj, xl, x\{j,l}) is diﬀerentiable in (xj, xl) on S,
(ii) f{j,l}(xj, xl, x\{j,l}) is continuous in (xj, xl, x\{j,l}) on S, and
(iii) E[f{j,l}(Xj, Xl, X\{j,l})|Xj = zj, Xl = zl] is continuous in (zj, zl) on Sj × Sl.
Then, for each (xj, xl) ∈Sj × Sl,
h{j,l},ALE(xj, xl) ≡
E[f{j,l}(Xj, Xl, X\{j,l})|Xj = zj, Xl = zl]dzjdzl.
Visualizing Supervised Learning Models
(End of Theorem 2)
The ALE second-order eﬀect of (Xj, Xl), denoted by f{j,l},ALE(xj, xl), is deﬁned the same
as h{j,l},ALE(xj, xl) but “doubly-centered” so that f{j,l},ALE(Xj, Xl) has a mean of zero with
respect to the marginal distribution of (Xj, Xl) and so that the ALE main eﬀects of Xj and Xl
on f{j,l},ALE(Xj, Xl) are both zero. The latter centering is accomplished by subtracting from
h{j,l},ALE(xj, xl) its uncentered ALE main eﬀects via
g{j,l},ALE(xj, xl) ≡h{j,l},ALE(xj, xl)
E[h{j,l},ALE(zK
k,j, Xl) −h{j,l},ALE(zK
k−1,j, Xl)|Xj ∈(zK
E[h{j,l},ALE(Xj, zK
k,l) −h{j,l},ALE(Xj, zK
k−1,l)|Xl ∈(zK
By Theorem 1, for diﬀerentiable f, (12) is equivalent to
g{j,l},ALE(xj, xl) ≡h{j,l},ALE(xj, xl) −
E[∂h{j,l},ALE(Xj, Xl)
|Xj = zj]dzj
E[∂h{j,l},ALE(Xj, Xl)
|Xl = zl]dzl
= h{j,l},ALE(xj, xl) −
pl|j(zl|zj)∂h{j,l},ALE(zj, zl)
pj|l(zj|zl)∂h{j,l},ALE(zj, zl)
The ﬁnal centering is accomplished by taking
f{j,l},ALE(xj, xl) ≡g{j,l},ALE(xj, xl) −E[g{j,l},ALE(Xj, Xl)]
= g{j,l},ALE(xj, xl) −
p{j,l}(zj, zl)g{j,l},ALE(zj, zl)dzjdzl.
It can be veriﬁed that f{j,l},ALE(xj, xl) is centered in the sense that the ALE main eﬀects of Xj
and Xl on f{j,l},ALE(xj, xl) are both zero (see Appendix C for a formal proof of a related but
more general result).
If we deﬁne the zero-order eﬀect for any function of X as its expected value with respect to p,
then we can view the ALE ﬁrst-order eﬀect of Xj as being obtained by ﬁrst calculating its uncentered ﬁrst-order eﬀect (6), and then for the resulting function, subtracting its zero-order eﬀect.
Likewise, the ALE second-order eﬀect of (Xj, Xl) is obtained by ﬁrst calculating the uncentered
second-order eﬀect (9), then for the resulting function, subtracting both of its ﬁrst-order eﬀects
of Xj and of Xl, and then for this resulting function, subtracting its zero-order eﬀect. The ALE
higher-order eﬀects are deﬁned analogously in Appendix B. The uncentered higher-order eﬀect
is ﬁrst calculated, and then all lower-order eﬀects are sequentially calculated and subtracted one
order at a time, until the ﬁnal result has all lower-order eﬀects that are identically zero.
Remark 2. In Appendix B we deﬁne ALE higher-order eﬀect functions fJ,ALE(xJ) for |J| > 2,
where |J| denotes the cardinality of the set of predictor indices J. Appendix C shows that this
leads to a functional-ANOVA-like decomposition of f via
fj,ALE(xj) +
f{j,l},ALE(xj, xl) +
J⊆{1,2,...,d},|J|≥3
fJ,ALE(xJ).
D. Apley and J. Zhu
This ALE decomposition has a certain orthogonality-like property, which we contrast with other
functional ANOVA decompositions in Section 5.5.
Remark 3. The ALE function deﬁnitions in this section apply to predictor distributions pj that
are continuous numerical with compact support. For discrete pj, one could consider modifying
(6) and (9) by using a ﬁxed ﬁnite partition whose interval endpoints coincide with the support
of pj. We do not develop this, however, because our focus is on estimation and interpretation of
the ALE eﬀects, and the estimators in the following section are meaningful for either continuous
or discrete pj. In the case that Xj is a nominal categorical predictor, one must decide how
to order its categories prior to estimating its ALE eﬀect (which requires diﬀerencing f across
neighboring categories of Xj). In Appendix E, we discuss a strategy for this that we have found
to work well in practice.
Estimation of fj,ALE(xj) and f{j,l},ALE(xj, xl)
In Appendix D we brieﬂy describe how to estimate the ALE higher-order eﬀect fJ,ALE(xJ) for
a general subset J ⊆{1, 2, . . . , d} of predictor indices. Our focus in this section is on estimating
the ﬁrst-order (|J| = 1) and second-order (|J| = 2) eﬀects, since these are the most common
and useful (i.e., interpretable).
As an overview, the estimate ˆfJ,ALE is obtained by computing estimates of the quantities in
Eqs. (6)—(14) for J = j (a single index) or for J = {j, l} (a pair of indices). For the estimates
we (i) replace the sequence of partitions in (6) (or (9)) by some appropriate ﬁxed partition of
the sample range of {xi,J : i = 1, . . . , n} and (ii) replace the conditional expectations in (6)
(or (9)) by sample averages across {xi,\J : i = 1, 2, . . . , n)}, conditioned on xi,J falling into
the corresponding interval/cell of the partition. In the preceding, xi,J = (xi,j : j ∈J) and
xi,\J = (xi,j : j = 1, 2, . . . , d; j ̸∈J) denote the ith observation of the subsets of predictors XJ
and X\J, respectively.
More speciﬁcally, for each j ∈{1, 2, . . . , d}, let {Nj(k) = (zk−1,j, zk,j] : k = 1, 2, . . . , K} be a
suﬃciently ﬁne partition of the sample range of {xi,j : i = 1, 2, . . . , n} into K intervals. Since the
estimator is computed for a ﬁxed K, we have omitted it as a superscript on the partition, with
the understanding that the partition implicitly depends on K. In all of our examples later in
the paper, we chose zk,j as the k
K quantile of the empirical distribution of {xi,j : i = 1, 2, . . . , n}
with z0,j chosen just below the smallest observation, and zK,j chosen as the largest observation.
Figure 3 illustrates the notation and concepts in computing the ALE main eﬀect estimator
ˆfj,ALE(xj) for the ﬁrst predictor j = 1 for the case of d = 2 predictors. For k = 1, 2, . . . , K, let
nj(k) denote the number of training observations that fall into the kth interval Nj(k), so that
k=1 nj(k) = n. For a particular value x of the predictor xj, let kj(x) denote the index of the
interval into which x falls, i.e., x ∈(zkj(x)−1,j, zkj(x),j].
For general d, to estimate the main eﬀect function fj,ALE(·) of a predictor Xj, we ﬁrst
compute an estimate of the uncentered eﬀect gj,ALE(·) deﬁned in (6), which is
ˆgj,ALE(x) =
{i:xi,j∈Nj(k)}
[f(zk,j, xi,\j) −f(zk−1,j, xi,\j)]
for each x ∈(z0,j, zK,j]. Analogous to (8), the ALE main eﬀect estimator ˆfj,ALE(·) is then
obtained by subtracting an estimate of E[gj,ALE(Xj)] from (15), i.e.,
ˆfj,ALE(x) = ˆgj,ALE(x) −1
ˆgj,ALE(xi,j) = ˆgj,ALE(x) −1
nj(k)ˆgj,ALE(zk,j).
To estimate the ALE second-order eﬀect of a pair of predictors (Xj, Xl), we partition the
sample range of {(xi,j, xi,l) : i = 1, 2, . . . , n} into a grid of K2 rectangular cells obtained as
the Cartesian product of the individual one-dimensional partitions.
Figure 4 illustrates the
Visualizing Supervised Learning Models
Fig. 3. Illustration of the notation and concepts in computing the ALE main effect estimator ˆfj,ALE(xj)
for j = 1 with d = 2 predictors. The bullets are a scatterplot of {(xi,1, xi,2) : i = 1, 2, . . . , n} for n = 30
training observations. The range of {xi,1 : i = 1, 2, . . . , n} is partitioned into K = 5 intervals {N1(k) =
(zk−1,1, zk,1] : k = 1, 2, . . . , 5} (in practice, K should usually be chosen much larger than 5). The numbers
of training observations falling into the 5 intervals are n1(1) = 4, n1(2) = 6, n1(3) = 6, n1(4) = 5, and
n1(5) = 9. The horizontal line segments shown in the N1(4) region are the segments across which the
ﬁnite differences f(z4,j, xi,\j) −f(z3,j, xi,\j) are calculated and then averaged in the inner summand of
Eq. (15) for k = 4 and j = 1.
notation and concepts. Let (k, m) (with k and m integers between 1 and K) denote the indices
into the grid of rectangular cells with k corresponding to xj and m corresponding to xl. In
analogy with Nj(k) and nj(k) deﬁned in the context of estimating fj,ALE(·), let N{j,l}(k, m) =
Nj(k) × Nl(m) = (zk−1,j, zk,j] × (zm−1,l, zm,l] denote the cell associated with indices (k, m), and
let n{j,l}(k, m) denote the number of training observations that fall into cell N{j,l}(k, m), so that
m=1 n{j,l}(k, m) = n.
To estimate f{j,l},ALE(xj, xl), we ﬁrst estimate the uncentered eﬀect h{j,l},ALE(xj, xl) deﬁned
ˆh{j,l},ALE(xj, xl) =
n{j,l}(k, m)
{i:xi,{j,l}∈N{j,l}(k,m)}
(K, k, m; xi,\{j,l})
for each (xj, xl) ∈(z0,j, zK,j] × (z0,l, zK,l]. In (17), ∆{j,l}
(K, k, m; xi,\{j,l}) is the second-order
ﬁnite diﬀerence deﬁned in (10), evaluated at the ith observation xi,\{j,l}, i.e.
(K, k, m; xi,\{j,l}) = [f(zk,j, zm,l, xi,\{j,l}) −f(zk−1,j, zm,l, xi,\{j,l})]
−[f(zk,j, zm−1,l, xi,\{j,l}) −f(zk−1,j, zm−1,l, xi,\{j,l})]
Analogous to (12), we next compute estimates of the ALE main eﬀects of Xj and Xl for the
function ˆh{j,l},ALE(xj, xl) and then subtract these from ˆh{j,l},ALE(xj, xl) to give an estimate of
D. Apley and J. Zhu
g{j,l},ALE(xj, xl):
ˆg{j,l},ALE(xj, xl)
=ˆh{j,l},ALE(xj, xl) −
{i:xi,j∈Nj(k)}
[ˆh{j,l},ALE(zk,j, xi,l) −ˆh{j,l},ALE(zk−1,j, xi,l)]
{i:xi,l∈Nl(m)}
[ˆh{j,l},ALE(xi,j, zm,l) −ˆh{j,l},ALE(xi,j, zm−1,l)]
=ˆh{j,l},ALE(xj, xl) −
nj,l(k, m)[ˆh{j,l},ALE(zk,j, zm,l) −ˆh{j,l},ALE(zk−1,j, zm,l)]
n{j,l}(k, m)[ˆh{j,l},ALE(zk,j, zm,l) −ˆh{j,l},ALE(zk,j, zm−1,l)].
Finally, analogous to (14), we estimate f{j,l},ALE(xj, xl) by subtracting an estimate of
Illustration of the notation used in computing the ALE second-order effect estimator
ˆf{j,l},ALE(xj, xl) for K = 5.
The ranges of {xi,j : i = 1, 2, . . . , n} and {xi,l : i = 1, 2, . . . , n}
are each partitioned into 5 intervals, and their Cartesian product forms the grid of rectangular cells
{N{j,l}(k, m) = Nj(k) × Nl(m) : k = 1, 2, . . . , 5; m = 1, 2, . . . , 5}. The cell with bold borders is the region
N{j,l}(4, 3). The second-order ﬁnite differences ∆{j,l}
(K, k, m; xi,\{j,l}) in Eq. (18) for (k, m) = (4, 3) are
calculated across the corners of this cell. In the inner summation of Eq. (17), these differences are then
averaged over the n{j,l}(4, 3) = 2 observations in region N{j,l}(4, 3).
Visualizing Supervised Learning Models
E[ˆg{j,l},ALE(Xj, Xl)] from (19), which gives
ˆf{j,l},ALE(xj, xl) = ˆg{j,l},ALE(xj, xl) −1
ˆg{j,l},ALE(xi,j, xi,l)
= ˆg{j,l},ALE(xj, xl) −1
n{j,l}(k, m)ˆg{j,l},ALE(zk,j, zm,l).
Theorems 3 and 4 in Appendix A show that, under mild conditions, (16) and (20) are
consistent estimators of the ALE main eﬀect (8) of Xj and ALE second-order eﬀect (14) of
(Xj, Xl), respectively.
ALE plots are plots of the ALE eﬀect estimates ˆfj,ALE(xj) and ˆf{j,l},ALE(xj, xl) versus the
predictors involved. ALE plots have substantial computational advantages over PD plot, which
we discuss in Section 5.4. In addition, ALE plots can produce reliable estimates of the main and
interaction eﬀects in situations where PD plots break down, which we illustrate with examples
in the next section, as well as an example on real data in Section 5.1.
Toy Examples Illustrating when ALE Plots are Reliable but PD Plots Break Down
Fig. 5. Depiction of the ﬁrst eight splits in the tree ﬁtted to the Example 1 data. The left panel is a
scatterplot of x2 vs. x1 showing splits corresponding to the truncated tree in the right panel.
Example 1. This example was introduced in Section 1. For this example, d = 2, n = 200, and
(X1, X2) follows a uniform distribution along a segment of the line x2 = x1 with independent
N(0, 0.052) variables added to both predictors. Figure 5 shows a scatter plot of X2 vs. X1. The
true response was generated according to the noiseless model Y = X1 + X2
2 for the 200 training
observations in Figure 5, to which we ﬁt a tree using the tree package of R ).
The tree was overgrown and then pruned back to have 100 leaf nodes, which was approximately
the optimal number of leaf nodes according to a cross-validation error sum of squares criterion.
Notice that the optimal size tree is relatively large, because the response here is a deterministic
function X1 + X2
2 of the predictors with no response observation error. The ﬁrst eight splits of
the ﬁtted tree f(x) are also depicted in Figure 5. Figure 6 shows main eﬀect PD plots, M plots,
D. Apley and J. Zhu
Fig. 6. For the tree model ﬁtted to the Example 1 data, plots of ˆfj,ALE(xj) (blue line with bullets),
ˆfj,P D(xj) (red dotted line), ˆfj,M(xj) (black dashed line), and the true main effect of Xj (black solid line)
for (a) j = 1, for which the true effect of X1 is linear, and (b) j = 2, for which the true effect of X2 is
quadratic. For both j = 1 and j = 2, ˆfj,ALE(xj) is much more accurate than either ˆfj,P D(xj) or ˆfj,M(xj).
and ALE plots for the full 100-node ﬁtted tree f(x), calculated via (2), (4), and (15)—(16),
respectively. For both j = 1 and j = 2, ˆfj,ALE(xj) is much more accurate than either ˆfj,PD(xj)
or ˆfj,M(xj). By inspection of the ﬁtted tree in Figure 5, it is clear why ˆfj,PD(xj) performs so
poorly in this example. For small x1 values like x1 ≈0.2, the PD plot estimate ˆf1,PD(x1 ≈0.2)
is much higher than it should be, because it is based on the extrapolated values of f(x) in the
upper-left corner of the scatter plot in Figure 5, which were substantially overestimated due to
the nature of the tree splits and the absence of any data in that region. For similar reasons,
ˆf2,PD(x2) for small x2 values is substantially underestimated because of the extrapolation in the
lower-right corner of the scatter plot in Figure 5. In contrast, by avoiding this extrapolation,
ˆf1,ALE(x1) and ˆf2,ALE(x2) are estimated quite accurately and are quite close to the true linear
(for x1) and quadratic (for x2) eﬀects, as seen in Figures 6(a) and 6(b), respectively.
Also notice that the M plots in Figures 6(a) and 6(b) perform very poorly. As expected,
because of the strong correlation between X1 and X2, ˆf1,M(x1) and ˆf2,M(x2) are quite close
to each other and are each combinations of the true eﬀects of X1 and X2. In the subsequent
examples, we do not further consider M plots.
Example 2. This example is a modiﬁcation of Example 1 having the same d = 2, n = 200, and
(X1, X2) following a uniform distribution along a segment of the line x2 = x1 with independent
N(0, 0.052) variables added to both predictors. However, the true response is now generated as
noisy observations according to the model Y = X1 + X2
2 + ε with ε ∼N(0, 0.12), and we ﬁt
a neural network model instead of a tree. For the neural network, we used the nnet package
of R ) with ten nodes in the single hidden layer, a linear output
activation function, and a decay/regularization parameter of 0.0001, all of which were chosen as
approximately optimal via multiple replicates of 10-fold cross-validation (the cross-validation r2
for this model varied between 0.965 and 0.975, depending on the data set generated, which is
quite close to the theoretical r2 value of 1 −Var(ε)
Var(Y ) = 0.972). We repeated the procedure in a
Monte Carlo simulation with 50 replicates, where on each replicate we generated a new training
data set of 200 observations and reﬁt the neural network model with the same tuning parameters
Visualizing Supervised Learning Models
Fig. 7. Comparison of (a) ˆf1,ALE(x1), (b) ˆf1,P D(x1), (c) ˆf2,ALE(x2), and (d) ˆf2,P D(x2) for neural network
models ﬁtted over 50 Monte Carlo replicates of the Example 2 data. In each panel, the black curve is
the true effect function (linear for X1 and quadratic for X2), and the gray curves are the estimated effect
functions over the 50 Monte Carlo replicates.
mentioned above. The estimated main eﬀect functions ˆfj,ALE(xj) and ˆfj,PD(xj) (for j = 1, 2)
over all 50 replicates are shown in Figure 7. For this example too, ˆfj,ALE(xj) is far superior to
ˆfj,PD(xj). On every replicate, ˆf1,ALE(x1) and ˆf2,ALE(x2) are quite close to the true linear and
quadratic eﬀects, respectively. In sharp contrast, ˆf1,PD(x1) and ˆf2,PD(x2) are so inaccurate on
many replicates that they are of little use in understanding the true eﬀects of X1 and X2.
Discussion
Illustration with a Bike-Sharing Real Data Example
We now show an example with a real, larger data set. The data are a compilation of the bikesharing rental counts from the Capital Bikeshare system (Washington D.C., USA) over the twoyear period 2011-2012, aggregated on an hourly basis, together with hourly weather and seasonal
information over the same time period. The data ﬁle can be found at 
ml/datasets/Bike+Sharing+Dataset ). There are n = 17393 cases/rows
in the training data set corresponding to 17393 hours of data. The response is the total number
of bike rental counts in each hour. We use the following d = 11 predictors: year , month (X2, treated as numerical: 1 = January, 2 =
February, . . ., 12 = December), hour (X3, treated as numerical: {0, 1, . . . , 23}), holiday (X4,
categorical: 0 = non-holiday, 1 = holiday), weekday (X5, treated as numerical: {0, 1, . . . , 6}
representing day of a week with 0 = Sunday), workingday (X6, categorical: 1 = neither weekend nor holiday, 0 = otherwise), weather situation (X7, treated as numerical: {1, 2, 3, 4}, smaller
values correspond to nicer weather situations), temp (X8, numerical: temperature in Celsius),
atemp (X9, numerical: feeling temperature in Celsius), hum (X10, numerical: humidity), windspeed (X11, numerical: wind speed). We do not use date and season in the data ﬁle as predictors
since they are dependent on the other predictors. Notice that the set of predictors are highly
correlated. For example, temperature and feeling temperature are highly correlated, and so are
month and temperature.
We ﬁt a neural network model using the R nnet package ) with
10 nodes in the single hidden layer (size = 10), a logistic output activation function (linout
= FALSE), and a regularization parameter of 0.05 (decay = 0.05).
These parameters are
approximately optimal according to multiple replicates of 3-fold cross validation (CV r2 ≈0.90).
For the ALE and PD plots, the function f(x) is the predicted hourly count of rental bikes from
the ﬁtted neural network model. Figure 8, Figure 9(a), and Figure 10 show ALE main and
interaction eﬀects plots for various predictors. We used K = 100 for both the main-eﬀect plots
and the interaction plots.
Fig. 8. For the bike-sharing example with f(x) a neural network model for predicting hourly bike rental
counts, ALE main-effect plots for month (X2, top left), hour-of-day (X3, top right), weather situation (X7,
bottom left), and wind speed (X11, bottom right) predictors. The zero-order effects have been included,
i.e., the plots are of fj,ALE(xj) + E[f(X)].
The ALE main-eﬀect plots are shown for month (X2), hour (X3), weather situation (X7),
and wind speed (X11) in Figure 8, and for feeling temperature (X9) in Figure 9(a). All of the
ALE main-eﬀect plots provide clear interpretations of the (main) eﬀects of the predictors. For
the eﬀect of month (X2), the number of rentals is lowest in January and gradually increases
Visualizing Supervised Learning Models
Fig. 9. For the bike-sharing data example with neural network predicted counts for f(x), ALE main-effect
plot (left panel) and PD main-effect plot (right panel) for feeling temperature (X9). Both plots include the
zero-order effect E[f(X)]. The two plots differ substantially, and the ALE plot seems to agree more with
intuition.
month-by-month until it peaks in September-October (months 9-10), after which it declines in
the winter months. For the eﬀect of hour of day (X3), the number of rentals increases until
it ﬁrst peaks at the morning rush hour around 8:00 am (hour 8), after which it decreases to
moderate levels over the late morning and early afternoon hours, and then peaks again at the
evening rush hour around 5:00-6:00 pm (hours 17−18). For the eﬀect of weather situation (X7),
the number of rentals monotonically decreases as the weather situation worsens. Recall that a
larger value of X7 corresponds to worse weather conditions. For the eﬀect of wind speed (X11),
the number of rentals also monotonically decreases as the wind speed increases. For the eﬀect
of atemp (X9, in Figure 9(a)), the number of rentals steadily increases as atemp (i.e., feeling
temperature) increases up until about 26 degrees Celsius (79 degrees Fahrenheit), after which it
steadily decreases. This makes perfect sense, since a feeling temperature of 26 degrees Celsius
might be considered as nearly optimal for comfortably biking around a city (note that feeling
temperature takes into factors such as humidity and breeze, so the actual temperature would
be somewhat lower), and feeling temperatures that are either substantially higher or lower than
this will make bike rental less appealing for many people. In comparison, Figure 9(b) shows the
PD main eﬀect plot for feeling temperature, which is substantially diﬀerent from the ALE main
eﬀect plot in Figure 9(a), even though the two are for the exact same ﬁtted neural network model.
The diﬀerence is due to the high correlation between feeling temperature and some of the other
predictors, and the resulting extrapolation that makes PD plots unreliable. In this case, the PD
plot indicates that the number of bike rentals monotonically increases as feeling temperature
increases, even at feeling temperatures over 40 degrees Celsius (104 Fahrenheit). The ALE plot
for feeling temperature in Figure 9(a), which indicates that bike rentals will decrease as feeling
temperature increases beyond the comfortable range, is in much better agreement with common
sense. In addition to providing better interpretability, the ALE plots are much faster to compute
than the PD plots (see Section 5.4).
Figure 10 shows two versions of the ALE second-order interaction eﬀect plot for the hour
and weather situation predictors ({X3, X7}), without and with the main eﬀects of X3 and X7
included. The latter (Figure 10(b)) plots E[f(X)]+ ˆf3,ALE(x3)+ ˆf7,ALE(x7)+ ˆf{3,7},ALE(x3, x7),
whereas the former (Figure 10(a)) plots only ˆf{3,7},ALE(x3, x7). Generally speaking, the latter
provides a clearer picture of the joint eﬀects of two predictors, whereas the former allows the
D. Apley and J. Zhu
Fig. 10. ALE second-order interaction plots for the predictors hour (X3) and weather situation (X7)
without (left panel) and with (right panel) the main effects of X3 and X7 included. The left panel plots
ˆf{3,7},ALE(x3, x7), and the right panel plots E[f(X)] + ˆf3,ALE(x3) + ˆf7,ALE(x7) + ˆf{3,7},ALE(x3, x7). The
numbers on the contours are the function values.
overall magnitude of the interaction eﬀect to be more easily assessed. Our ALEPlot R package
allows either to be plotted.
The interaction ALE plot in Figure 10 reveals an interesting relationship and is an important
supplement to the main eﬀects ALE plots for X3 and X7 in Figure 8. Consider the decrease in
bike rental counts that is due to larger weather situation values (i.e., less pleasant weather). If
there were no interactions, this decrease would be the same regardless of the hour or the levels
of the other predictors. But from Figure 10(a), there is clearly a strong interaction between X3
and X7, since the contour values vary over a range of about 110 units (from −50 to +60), which
is almost as large as the range for the main eﬀect ˆf7,ALE(x7) in Figure 8. The eﬀect of weather
situation, which is a decrease in rentals as weather situation increases, is clearly ampliﬁed during
the rush hour peaks, and in general at hours when the overall rental counts are expected to be
higher. One must be careful interpreting interaction plots without the main eﬀects included.
From Figure 10(a), at some hours (e.g., around hour 0, which is midnight) ˆf{3,7},ALE(x3, x7)
increases as weather situation increases. However, when the main eﬀects of X3 and X7 are
included as in Figure 10(b), it is clear that increasing weather situation decreases bike rentals
at any hour.
It makes sense that the eﬀects of weather situation are ampliﬁed by the eﬀects of hour
(on which the overall bike rental counts depend heavily), and this example illustrates how
visualizations like this can aid the model building process by suggesting modiﬁcations of the
model that one might consider. For example, Figure 10(b) indicates that the eﬀects of weather
situation and/or hour on rental counts might be better modeled as multiplicative.
The Wrong and Right Ways to Interpret ALE Plots (and PD Plots)
This section provides a word of caution on how not to interpret ALE plots when the predictors
are highly correlated, which also applies to interpreting PD plots. Reconsider Example 1, in
which X1 and X2 are highly correlated (see Figure 5), and the ALE and PD plots are as in
Figure 6. The wrong way to interpret the ALE plot is that it implies that if we ﬁx (say) x1 and
then vary x2 over its entire range, the response (of which f(x) is a predictive model) is expected
to vary as in Figure 6(b). And this interpretation is wrong even if f(x) is truly additive in the
predictors. Indeed, varying x2 over its entire range with x1 held ﬁxed would take x far outside
Visualizing Supervised Learning Models
Fig. 11. Illustration of the right way to interpret ALE plots for an example in which f(x) = Pd
j=1 fj(xj) is
additive with quadratic fj(xj) = x2
j, and K = 10 equally-spaced bins over the support were used.
The left panel shows the local effects of Xj within each bin (i.e., the summand in Eq. (6)). The local
effects are local and require no extrapolation outside the data envelope. The right plot is of gj,ALE(xj)
and can be viewed as piecing together (or accumulating) the local effects in a manner that facilitates
easier visualization of the underlying global effect.
the envelope of the training data to which f was ﬁt, as can be seen in Figure 5. f(x) is obviously
unreliable for this level of extrapolation, which was the main motivation for ALE plots, and we
have highly uncertain knowledge of the hypothetical values of the response far outside the data
However, ALE plots are still very useful if we interpret them correctly, and the correct
interpretation is illustrated in Figure 11. In this toy example, suppose f(x) = Pd
j=1 fj(xj) is
additive, the eﬀect of Xj is the quadratic function fj(xj) = x2
j, and Xj is highly correlated with
the other predictors. The left panel of Figure 11 shows the local eﬀects of Xj within each bin,
for K = 10 equally-spaced bins over the support . Here, the local eﬀect within a bin is
deﬁned as the summand in the Eq. (6) deﬁnition of gj,ALE(xj), which represents the average
change in f(X) as Xj changes from the left endpoint to the right endpoint of the bin.
The local eﬀects are exactly that – local – and require no extrapolation beyond the envelope
of the data, since the changes in f(X) are averaged across the conditional distribution of X\j,
given that Xj falls in that bin. Consequently, if the bin widths are not too small and f is not
too noisy, a local eﬀect plot like the one in the left panel of Figure 11 could be interpreted to
reveal the eﬀect of Xj on f.
However, the eﬀect of Xj is much easier to visualize if we accumulate the local eﬀects via Eq.
(6) and plot gj,ALE(xj) instead, as in the right panel of Figure 11. Aside from vertical centering,
this is exactly the ALE plot, and it is best viewed as a way of piecing together the local eﬀects
in a manner that provides easier visualization of the underlying global eﬀect of a predictor. The
additive recovery property discussed in the next subsection provides even stronger justiﬁcation
for this manner of piecing together the local eﬀects. Namely, if f(x) = Pd
j=1 fj(xj) is additive,
the ALE plot manner of piecing together the local eﬀects produces the correct global eﬀect
function fj,ALE(xj) = fj(xj). Of course, one must still keep in mind that the global eﬀect fj(xj)
may only hold when the set of predictors x jointly fall within the data envelope.
D. Apley and J. Zhu
Paired Differencing and Additive Recovery
The ALE functions have an attractive additive recovery property mentioned in Remark 1. Suppose f(x) = Pd
j=1 fj(xj) is an additive function of the individual predictors. Then it is straightforward to show that the ALE main eﬀects are fj,ALE(xj) = fj(xj) for (j = 1, 2, . . . , d), up
to an additive constant. That is, the ALE eﬀects recover the correct additive functions. More
generally, the following result states that higher-order ALE eﬀects fJ,ALE(xJ) have a similar
additive recovery property.
Additive recovery for ALE plots. Suppose f is of the form f(x) = P
J⊆{1,2,...,d},|J|≤k fJ(xJ)
for some 1 ≤k ≤d. That is, f has interactions of order k, but no higher-order interactions
than that. Then for every subset J with |J| = k, fJ,ALE(xJ) = fJ(xJ) + P
u⊂J hu(xu) for some
functions hu(xu) that are of strictly lower order than k. In other words, for every J with |J| = k,
the ALE eﬀect fJ,ALE(xJ) returns the correct k-order interaction fJ(xJ), since the presence of
strictly-lower-order functions do not alter the interpretation of a k-order interaction.
The proof of this additive recovery property for ALE plots follows directly from the decomposition theorem in Appendix C. It also follows that if the functions {fJ(xJ)} in the expression
for f(x) are adjusted so that each has no lower-order ALE eﬀects, then fJ,ALE(xJ) = fJ(xJ)
for each J ⊆{1, 2, . . . , d}. Although PD plots have a similar additive recovery property (see
below), M plots have no such property. For example, if f(x) = Pd
j=1 fj(xj), and the predictors
are dependent, then each fj,M(xj) may be a combination of the main eﬀects of many predictors.
As discussed previously, this can be viewed as the omitted variable bias in regression, whereby a
regression of Y on (say) X1, omitting a correlated nuisance variable X2 on which Y also depends,
will bias the eﬀect of X1 on Y .
Fig. 12. Illustration of how, when estimating f1,ALE(x1), the differences f(zk,1, xi,2) −f(zk−1,1, xi,2) and
f(zk′,1, xi,2)−f(zk′−1,1, xi,2) in (15) are paired differences that block out the nuisance variable X2. Here,
k = k1(0.3) and k′ = k1(0.8).
The mechanism by which ALE plots avoid this omitted nuisance variable bias is illustrated
in Figure 12, for the same example depicted in Figures 1, 2, and 5. First note that the M plot
functions in this example are severely biased, because f1,M(x1) averages the function f(x1, X2)
itself (as opposed to its derivative) with respect to the conditional distribution of X2|X1 = x1
(see (3) and (4)). For example, for f(x) = x1 + x2
2 considered in Figure 6, the M plot averaged function is f1,M(x1) = E[f(X1, X2)|X1 = x1] = x1 + E[X2
2|X1 = x1] ̸= x1, which is
biased by the functional dependence of f(x) on the correlated nuisance variable X2. In contrast
to averaging the function f itself, the ALE eﬀect f1,ALE(x1) estimated via (15)—(16) aver-
Visualizing Supervised Learning Models
ages only the local eﬀect of f represented by the paired diﬀerences f(zk,1, xi,2) −f(zk−1,1, xi,2)
in (15). As illustrated in Figure 12, this paired diﬀerencing is what blocks out the eﬀect of
the correlated nuisance variable X2.
Continuing the f(x) = x1 + x2
2 example, the paired
diﬀerences f(zk,1, xi,2) −f(zk−1,1, xi,2) = (zk,1 + x2
i,2) −(zk−1,1 + x2
i,2) = zk,1 −zk−1,1 for
the ALE plot completely block out the eﬀect of X2, so that the accumulated local eﬀect
k=1 [zk,1 −zk−1,1] = x1 + constant is correct.
Multiplicative recovery for ALE plots for independent subsets of predictors. Suppose the model is of the form f(x) = fJ(xJ)f\J(x\J) for some J ⊂{1, 2, . . . , d} with XJ independent of X\J. In this case it is straightforward to show that the ALE |J|-order interaction
eﬀect of XJ is fJ,ALE(xJ) = fJ(xJ)E[f\J(X\J)] + P
u⊂J hu(xu) for some lower-order functions
hu(xu). That is, the ALE |J|-order interaction eﬀect fJ,ALE(xJ) recovers the correct function
fJ(xJ), except for a multiplicative constant E[f\J(X\J)] and the additive presence of strictly
lower-order functions.
Comparison to PD plots. PD plots also have the same additive and multiplicative recovery
properties just discussed. Moreover, for f(x) = fJ(xJ)f\J(x\J), PD plots have multiplicative
recovery (up to a multiplicative constant) even when XJ and X\J are dependent ). Although it is probably desirable to have multiplicative recovery when XJ
and X\J are independent, it is unclear whether multiplicative recovery is even desirable if XJ
and X\J are dependent.
For example, suppose f(x) = x1x2 with X1 (J = 1) and X2 (\J = 2) standard normal random
variables with correlation coeﬃcient ρ. It is straightforward to show that f{1,2},ALE(x1, x2) =
2), f1,ALE(x1) =
1 −1), and f2,ALE(x2) =
2 −1), compared to
f{1,2},PD(x1, x2) = x1x2, f1,PD(x1) = 0, and f2,PD(x2) = 0. Because of the strong interaction, it
is essential to look at the second-order interaction eﬀects in order to understand the functional
dependence of f(·) on the predictors.
Both f{1,2},ALE(x1, x2) and f{1,2},PD(x1, x2) correctly
recover the interaction, up to lower-order functions of the individual predictors. Regarding the
main eﬀects, however, the picture is more ambiguous. First, it is important to note that with
a strong interaction and dependent predictors, it is unclear whether the main eﬀects are even
meaningful. And it is equally unclear whether the PD main eﬀect f1,PD(x1) = 0 is any more
or less meaningful than the ALE main eﬀect f1,ALE(x1) =
If X1 and X2 were
independent in this example, then it would probably be desirable to view the main eﬀects of X1
and X2 as zero, but in this case f1,PD(x1) = f1,ALE(x1) = 0 would actually be in agreement.
The situation is murkier with dependent predictors in this example. The local eﬀect ∂f(x)
of X1 depends strongly on the value of x2, in that it is ampliﬁed for larger |x2| and changes sign
if x2 changes sign. Thus, if ρ is large and positive, the local eﬀect of X1 is positive for x1 > 0
and negative for x1 < 0, which is the local eﬀect of a quadratic relationship. In this case one
might argue that the quadratic f1,ALE(x1) = 1
1 −1) is more revealing than f1,PD(x1) = 0.
However, the debate is largely academic, because when strong interactions are present the lowerorder eﬀects should not be interpreted in a vacuum.
Computational Advantages of ALE Plots over PD Plots
For general supervised learning models f(x), ALE plots have an enormous computational advantage over PD plots. Suppose we want to compute ˆfJ,ALE(xJ) for one subset J ⊆{1, 2, . . . , d} over
a grid in the xJ-space with K discrete locations for each variable. Computation of ˆfJ,ALE(xJ)
over this grid requires a total of 2|J| × n evaluations of the supervised learning model f(x)
(see (15)—(20) or (D.1) for |J| > 2). In comparison, computation of ˆfJ,PD(xJ) over this grid
requires a total of K|J| × n evaluations of f(x). For example, for K = 50, PD main eﬀects and
second-order interaction eﬀects require, respectively, 25 and 625 times more evaluations of f(x)
than the corresponding ALE eﬀects. Moreover, as we discuss in Appendix E, the evaluations of
f(x) can be easily vectorized (in R, for example, by appropriately calling the predict function
D. Apley and J. Zhu
that is built into most supervised learning packages in R).
Also notice that the number of evaluations of f(x) for ALE plots does not depend on K,
which is convenient. As n increases, the observations become denser, in which case we may want
the ﬁneness of the discretization to increase as well. If we choose K|J| ∝n (which results in the
same average number of observations per cell as n increases), then the number of evaluations of
f(x) is O(n) for ALE plots versus O(n2) for PD plots.
For the bike sharing example in Section 5.1, we implemented the ALE and PD plots using
our R package ALEPlot on a Windows™laptop with Intel(R) Core(TM) i7-7600U CPU @ 2.80
GHz processor. The ALE main-eﬀect plots and the ALE second-order interaction plots took
less than 1 second each. In comparison, with the same K = 100, the PD main-eﬀect plots took
about 5 seconds each, and the PD interaction plots (not shown in Section 5.1) took about 8
minutes each. The PD plot computational expense is proportional to K for main eﬀects and to
K2 for second-order interaction eﬀects, whereas the ALE plot computational expense is largely
independent of K. The ALE interaction plots were orders of magnitude faster to compute than
the PD interaction plots (less than 1 second vs. 8 minutes).
Relation to Functional ANOVA with Dependent Inputs
In the context of the closely-related problem of functional ANOVA with dependent input (i.e.,
predictor) variables, the extrapolation issue that motivated ALE plots has been previously considered. Hooker proposed a functional ANOVA decomposition of f(x) into component
functions {fJ,ANOV A(xJ) : J ⊆{1, 2, . . . , d}} by adopting the Stone approach of using
weighted integrals in the function approximation optimality criterion and in the component function orthogonality constraints. Hooker used p{1,2,...,d}(x) as a weighting function, which
indirectly avoids extrapolation of f(x) in regions in which there are no training observations,
because any such extrapolations are assigned little or no weight. The resulting ANOVA component functions are hierarchically orthogonal under the correlation inner product, in the sense
that fJ,ANOV A(XJ) and fu,ANOV A(Xu) are uncorrelated when u ⊂J. However, fJ,ANOV A(XJ)
and fu,ANOV A(Xu) are not uncorrelated for general u ̸= J.
In comparison, we show in Appendix C that the ALE decomposition
J⊆{1,2,...,d} fJ,ALE(xJ) mentioned in Remark 2 has the following orthogonality-like
property. For each J ⊆{1, 2, . . . , d}, let HJ(·) denote the operator that maps a function f to its
ALE eﬀect fJ,ALE, i.e., such that fJ,ALE = HJ(f) (see Appendix B for details). The collection
of operators {HJ : J ⊆{1, 2, . . . , d}} behaves like a collection of operators that project onto
orthogonal subspaces of an inner product space. Namely, if “◦” denotes the composite function
operator, then HJ ◦HJ(f) = HJ(f), and HJ ′ ◦HJ(f) = 0 for each J′ ⊆{1, 2, . . . , d} with
In other words, the ALE |J′|-order eﬀect of the predictors XJ ′ for the function fJ,ALE is
identically zero when J ̸= J′; and the ALE |J|-order eﬀect of the predictors XJ for the function
fJ,ALE is the same function fJ,ALE. For example, for any pair of predictors {Xj, Xl}, the ALE
main eﬀect of any predictor (including Xj or Xl) for the function f{j,l},ALE(xj, xl) is identically
zero. Thus each ALE second-order interaction eﬀect function has ALE main eﬀects that are
all identically zero. Likewise, each ALE main eﬀect function has ALE second-order interaction
eﬀects that are all identically zero. And for the function f{j,l},ALE(xj, xl), the ALE second-order
interaction eﬀect of any other pair of predictors is identically zero. Similarly, the ALE ﬁrst- and
second-order interaction eﬀects for any ALE third-order eﬀect function are all identically zero,
and vice-versa.
For the purpose of visualizing the eﬀects of the predictors on black box supervised learning
models, the correlation orthogonality in other functional ANOVA decompositions may be less
relevant and less useful than the ALE pseudo-orthogonality. As discussed in Roosen , if the
predictors are dependent, it may even be preferable to artiﬁcially impose a product p{1,2,...,d}(x)
in the functional ANOVA decomposition to avoid conﬂating direct and indirect eﬀects of a
predictor, and this will typically result in ANOVA component functions that are no longer
uncorrelated.
For example, suppose f(x) = x1 + x2, and X1 and X2 are correlated.
Visualizing Supervised Learning Models
functional ANOVA decomposition that gives uncorrelated main eﬀects will not give the correct
main eﬀects f1(x1) = x1 and f2(x2) = x2 that are needed to understand the true functional
dependence of f(x) on x1 and x2. In contrast, the ALE and PD main eﬀect functions are the
correct functions f1(x1) = x1 and f2(x2) = x2, up to an additive constant (see Section 5.3).
Functional ANOVA can be coerced into producing the correct main eﬀects f1,ANOV A(x1) = x1
and f2,ANOV A(x2) = x2 by artiﬁcially imposing a product distribution p{1,2}(x) = p1(x1)p2(x2),
but then the ANOVA component functions are no longer uncorrelated. Moreover, artiﬁcially
imposing a product p{1,2,...,d}(x) in functional ANOVA still leaves the extrapolation problem
that plagues PD plots and that motivated ALE plots and the work of Hooker .
In addition, practical implementation is far more cumbersome for functional ANOVA decompositions than for ALE (or PD) plots, for multiple reasons. First, p{1,2,...,d}(x) must be estimated
in the functional ANOVA approach of Hooker , which is problematic in high-dimensions.
In contrast, the ALE eﬀect estimators (15)—(20) involve summations over the training data
but require no estimate of p{1,2,...,d}(x). Second, each ALE plot eﬀect function can be computed
one-at-a-time using straightforward and computationally eﬃcient calculations that involve only
ﬁnite diﬀerencing, averaging, and summing. In contrast, the functional ANOVA component
functions must be computed simultaneously, which requires the solution of a complex system of
equations. Follow-up work in Li and Rabitz , Chastaing and Prieur , and Rahman
 improved the solution techniques, sometimes restricting the component ANOVA functions to be expansions in basis functions such as polynomials and splines, but these are more
restrictive (perhaps negating the beneﬁts of ﬁtting a black box supervised learning model in the
ﬁrst place), as well as more cumbersome and computationally expensive than ALE plots.
Conclusions
For visualizing the eﬀects of the predictor variables in black box supervised learning models, PD
plots are the most widely used method. The ALE plots that we have proposed in this paper are an
alternative that has two important advantages over PD plots. First, by design, ALE plots avoid
the extrapolation problem that can render PD plots unreliable when the predictors are highly
correlated (see Figures 6, 7, and 9). Second, ALE plots are substantially less computationally
expensive than PD plots, requiring only 2|J|×n evaluations of the supervised learning model f(x)
to compute each ˆfJ,ALE(xJ), compared to K|J| × n evaluations to compute each ˆfJ,PD(xJ). In
light of this, we suggest that ALE plots should be adopted as a standard visualization component
in supervised learning software. We have also provided, as supplementary material, an R package
ALEPlot to implement the ALE plots.