The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)
RepeatNet: A Repeat Aware Neural
Recommendation Machine for Session-Based Recommendation
Pengjie Ren,1,2 Zhumin Chen,1 Jing Li,1 Zhaochun Ren,3 Jun Ma,1 Maarten de Rijke2
1Shandong University, Jinan, China
2University of Amsterdam, Amsterdam, The Netherlands
3Data Science Lab, JD.com, Beijing, China
Recurrent neural networks for session-based recommendation have attracted a lot of attention recently because of
their promising performance. repeat consumption is a common phenomenon in many recommendation scenarios (e.g.,
e-commerce, music, and TV program recommendations),
where the same item is re-consumed repeatedly over time.
However, no previous studies have emphasized repeat consumption with neural networks. An effective neural approach
is needed to decide when to perform repeat recommendation. In this paper, we incorporate a repeat-explore mechanism into neural networks and propose a new model, called
RepeatNet, with an encoder-decoder structure. RepeatNet integrates a regular neural recommendation approach in the decoder with a new repeat recommendation mechanism that can
choose items from a user’s history and recommends them at
the right time. We report on extensive experiments on three
benchmark datasets. RepeatNet outperforms state-of-the-art
baselines on all three datasets in terms of MRR and Recall.
Furthermore, as the dataset size and the repeat ratio increase,
the improvements of RepeatNet over the baselines also increase, which demonstrates its advantage in handling repeat
recommendation scenarios.
Introduction
Session-based recommendations have received increasing
interest recently, due to their broad applicability in many online services (e.g., e-commerce, video watching, music listening) . Here, a session is a group of
interactions that take place within a given time frame. Sessions from a user can occur on the same day, or over several
days, weeks, or even months .
Conventional recommendation methods tackle sessionbased recommendations based on either the last interaction
or the last session. Zimdars, Chickering, and Meek 
and Shani, Heckerman, and Brafman investigate how
to extract sequential patterns to predict the next item using Markov models. Then, Chen et al. propose logistic Markov embeddings to learn the representations of
songs for playlist prediction. A major issue for these models is that the state space quickly becomes unmanageable
when trying to include all possible sequences of potential
Copyright c⃝2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
user selections over all items. Recurrent neural networks
(RNNs) have recently been used for the purpose of sessionbased recommendations and attracted signiﬁcant attention.
Hidasi et al. introduce RNNs with gated recurrent
units (GRUs) for session-based recommendation. They introduce a number of parallel RNN (p-RNN) architectures to
model sessions based on both clicks and features (images
and text) of clicked items . Quadrana et
al. personalize RNN models with cross-session information transfer and devise a Hierarchical RNN model
that relays and evolves latent hidden states of the RNNs
across user sessions. Li et al. introduce an attention mechanism into session-based recommendations and
outperform . Though the number of
studies that apply deep learning to session-based recommendation is increasing, none has emphasized so-called repeat
consumptions, which are a common phenomenon in many
recommendation scenarios (e.g., e-commerce, music, and
TV program recommendations), where the same item is reconsumed repeatedly over time.
Repeat consumption exists because people have regular
habits. For example, we all buy the same things repeatedly,
we eat at the same restaurants regularly, we listen to the same
songs and artists frequently . Table
1 shows the repeat consumption ratio for three benchmark
datasets that are commonly used in related studies . Repeat consumption not only
Table 1: Repeat ratio (%) on three benchmark datasets.
Validation
YOOCHOOSE 1/4
DIGINETICA
exists but also accounts for a large proportion of the interactions in some applications. In this paper, we investigate
repeat consumption by incorporating a repeat-explore mechanism into neural networks and propose a new model called
RepeatNet with an encoder-decoder structure. Unlike existing work that evaluates a score for each item using a single
decoder, RepeatNet evaluates the recommendation probabilities of each item with two decoders in a repeat mode and an
explore mode, respectively. In the repeat mode we recommend an old item from the user’s history while in the explore mode we recommend a new item. Speciﬁcally, we ﬁrst
encode each session into a representation. Then, we use a
repeat-explore mechanism to learn the switch probabilities
between repeat and explore modes. After that, we propose a
repeat recommendation decoder to learn the probabilities of
recommending old items in the repeat mode and an explore
recommendation decoder to learn the probabilities of recommending new items under the explore mode. Finally, we
determine the recommendation score for an item by combining the mode switch probabilities and the recommendation
probabilities of each item under the two modes in a probabilistic way. The mode prediction and item recommendation
are jointly learned in an end-to-end back-propagation training paradigm within a uniﬁed framework.
We carry out extensive experiments on three benchmark
datasets. The results show that RepeatNet outperforms stateof-the-art baselines on all three datasets in terms of MRR
and Recall. Furthermore, we ﬁnd that as the dataset size
and the repeat ratio increase, the improvements of Repeat-
Net over the baselines also increase, which demonstrates its
advantages in handling repeat recommendation scenarios.
To sum up, the main contributions in this paper are:
• We propose a novel deep learning-based model named
RepeatNet that takes into account the repeat consumption
phenomenon. To the best of our knowledge, we are the
ﬁrst to consider this in the context of session-based recommendation with a neural model.
• We introduce a repeat-explore mechanism for sessionbased recommendation to automatically learn the switch
probabilities between repeat and explore modes. Unlike
existing works that use a single decoder, we propose
two decoders to learn the recommendation probability for
each item in the two modes.
• We carry out extensive experiments and analyses on three
benchmark datasets. The results show that RepeatNet can
improve the performance of session-based recommendation over state-of-the-art methods by explicitly modeling
repeat consumption.
Related Work
We survey related work in two areas: session-based recommendations and repeat recommendations.
Session-based recommendation
Conventional methods for session-based recommendation
are usually based on Markov chains that predict the next
action given the last action. Zimdars, Chickering, and
Meek propose a sequential recommender based on
Markov chains and investigate how to extract sequential patterns to learn the next state using probabilistic decision-tree
models. Mobasher et al. study different sequential
patterns for recommendation and ﬁnd that contiguous sequential patterns are more suitable for sequential prediction
task than general sequential patterns. Shani, Heckerman, and
Brafman present a Markov decision process (MDP)
to provide recommendations in a session-based manner and
the simplest MDP boils down to ﬁrst-order Markov chains
where the next recommendation can simply be computed
through the transition probabilities between items. Yap, Li,
and Yu introduce a competence score measure in
personalized sequential pattern mining for next-item recommendations. Chen et al. model playlists as Markov
chains, and propose logistic Markov embeddings to learn
the representations of songs for playlists prediction. A major issue with applying Markov chains to the session-based
recommendation task is that the state space quickly becomes
unmanageable when trying to include all possible sequences
of potential user selections over all items.
RNNs have proved useful for sequential click prediction . Hidasi et al. apply
RNNs to session-based recommendation and achieve significant improvements over conventional methods. They utilize
session-parallel mini-batch training and employ rankingbased loss functions for learning the model. Later, they introduce a number of parallel RNN (p-RNN) architectures
to model sessions based on clicks and features (images and
text) of clicked items ; they propose alternative training strategies for p-RNNs that suit them better than standard training. Tan, Xu, and Liu propose two techniques to improve the performance of their
models, namely data augmentation and a method to account for shifts in the input data distribution. Jannach and
Ludewig show that a heuristics-based nearest neighbor scheme for sessions outperforms the model proposed by
Hidasi et al. in the large majority of the tested con-
ﬁgurations and datasets. Quadrana et al. propose a
way to personalize RNN models with cross-session information transfer and devise a Hierarchical RNN model that
relays end evolves latent hidden states of the RNNs across
user sessions. Li et al. explore a hybrid encoder with
an attention mechanism to model the user’s sequential behavior and intent to capture the user’s main purpose in the
current session.
Unlike the studies listed above, we emphasize the repeat
consumption phenomenon in our models.
Repeat recommendation
Anderson et al. study the patterns by which a user
consumes the same item repeatedly over time, in a wide variety of domains, ranging from check-ins at the same business location to re-watches of the same video. They ﬁnd
that recency of consumption is the strongest predictor of
repeat consumption. Chen, Wang, and Wang derive
four generic features that inﬂuence people’s short-term repeat consumption behavior. Then, they present two fast algorithms with linear and quadratic kernels to predict whether a
user will perform a short-term repeat consumption at a speciﬁc time given the context.
An important goal of a recommender system is to help
users discover new items. Besides that, many real-world systems utilize lists of recommendation for a different goal,
namely to remind users of items that they have viewed or
consumed in the past. Lerche, Jannach, and Ludewig 
investigate this through a live experiment, aiming to quantify the value of such reminders in recommendation lists.
Repeat-Explore Mechanism
Attentive Encoder
Repeat Rec Decoder
Explore Rec Decoder
Explore Rec
Repeat Mode
Explore Mode
ܲ݅ܫௌ= ܲݎܫௌܲ݅ݎ, ܫௌ+ ܲ݁ܫௌܲ(݅|݁, ܫௌ)
Figure 1: Overview of RepeatNet.
Benson, Kumar, and Tomkins identify two macroscopic behavior patterns of repeated consumptions. First, in
a given user’s lifetime, very few items live for a long time.
Second, the last consumptions of an item exhibit growing
inter-arrival gaps consistent with the notion of increasing
boredom leading up to eventual abandonment. The main
difference between our work and previous work on repeat
recommendations is that we are the ﬁrst to propose a neural recommendation model to explicitly emphasize repeat
consumption in both conventional and session-based recommendation tasks.
Given an action (e.g., clicking, shopping) session IS =
{i1, i2, . . . , iτ, . . . , it}, where iτ refers to an item, sessionbased recommendation tries to predict what the next event
would be, as shown in Eq. 1. Without loss of generality, we
take click actions as our running example in the paper:
P(it+1 | IS) ∼f(IS),
where P(it+1 | IS) denotes the probability of recommending it+1 given IS. Conventional methods usually model
f(IS) directly as a discriminant or probability function.
We propose RepeatNet to model P(it+1 | IS) from a probabilistic perspective by explicitly taking repeat consumption
into consideration, as shown in Eq. 2:
P(it+1 | IS) = P(r | IS)P(it+1 | r, IS) +
P(e | IS)P(it+1 | e, IS),
where r and e denote repeat mode and explore mode, respectively. Here, P(r | IS) and P(e | IS) represent the
probabilities of executing in repeat mode and explore mode,
respectively. P(it+1 | r, IS) and P(it+1 | e, IS) refer to the
probabilities of recommending it+1 in repeat mode and in
explore mode, respectively, given IS.
As illustrated in Fig. 1, RepeatNet consists of four main
components, a session encoder, a repeat-explore mechanism, a repeat recommendation decoder, and an explore
recommendation decoder. The session encoder encodes the
given session IS into latent representations H = {h1, h2,
. . . , hτ, . . . , ht}, where ht represents the session representation at timestamp t. The repeat-explore mechanism takes
H as input and predicts the probabilities of executing repeat mode or explore mode, corresponding to P(r | IS)
and P(e | IS) in Eq. 2. The repeat recommendation decoder takes H as input and predicts the repeat recommendation probabilities over clicked items in IS, corresponding
to P(it+1 | r, IS) in Eq. 2. The explore recommendation decoder takes H as input and predicts the explore recommendation probabilities over unclicked items in I −IS, where I
refers to all items, corresponding to P(it+1 | e, IS) in Eq. 2.
Session encoder
Like previous studies ,
we use a GRU to encode IS, where the GRU is deﬁned as:
zτ = σ(Wz[emb(iτ), hτ−1])
rτ = σ(Wr[emb(iτ), hτ−1])
hτ = tanh(Wh[emb(iτ), rτ ⊙hτ−1])
hτ = (1 −zτ) ⊙hτ−1 + zτ ⊙f
where Wz, Wr, and Wh are weight matrices; emb(iτ) is the
item embedding of iτ; σ denotes the sigmoid function. The
initial state of the GRU is set to zero vectors, i.e., h0 = 0.
After the session encoder, each session IS is encoded into
H = {h1, h2, . . . , hτ, . . . , ht}.
Repeat-explore mechanism
The repeat-explore mechanism can be seen as a binary classiﬁer that predicts the recommendation mode based on H =
{h1, h2, . . . , hτ, . . . , ht}. To this end, we ﬁrst apply an attention mechanism to
H to get a ﬁxed-length vector representation of IS. Speciﬁcally, we ﬁrst use the last hidden state ht to match with each
encoder hidden state hτ ∈H to get an importance score:
re tanh(Wreht + Urehτ),
where vre, Wre, and Ure are parameters. The importance
scores are then normalized to get the context vector for IS
as a weighted sum in Eq. 5:
τ=1 exp(ere
We then employ a softmax regression to transform cre
a mode probability distribution, corresponding to P(r | IS)
and P(e | IS) respectively, as shown in Eq. 6:
[P(r | IS), P(e | IS)] = softmax , we use a modiﬁcation of the attention model to
achieve this. The probability of re-clicking item iτ ∈IS is
computed as follows:
r tanh(Wrht + Urhτ)
P(i | r, IS) =
τ=1 exp(erτ )
if i ∈I −IS,
where vr, Wr, and Ur are parameters; P
τ) denotes
the sum of all occurrences of item i ∈IS, because the same
item might occur multiple times in different positions of IS.
Explore recommendation decoder
The explore recommendation decoder evaluates the probability of clicking a new item that does not exist in IS. To
better capture the user’s interest in session IS, we employ
an item-level attention mechanism that allows the decoder
to dynamically select and linearly combine different parts of
the input sequence :
e tanh(Weht + Uehτ)
τ=1 exp(eeτ)
where ve, We, and Ue are parameters. The factors αe
h determine which part of the input sequence should be emphasized
or ignored when making predictions. We then combine the
last hidden state and the attentive state into a hybrid representation cIS for IS, which is the concatenation of vectors
IS: cIS = [ht, ce
Finally, the probability of clicking item iτ ∈I −IS is
computed as follows:
if i ∈I −IS
P(i | e, IS) =
τ=1 exp(fτ)
e is the weight matrix and −∞means negative in-
ﬁnity. Since exp(−∞) = 0, we assume that if an item exists
in IS, then the probability of recommending it in the explore
mode is zero.
Objective function
Our goal is to maximize the output prediction probability
given the input session. Therefore, we optimize the negative
log-likelihood loss function as follows:
Lrec(θ) = −1
log P(iτ | IS),
where θ are all the parameters of RepeatNet, IS is the set of
all sessions in the training set, and P(iτ | IS) is the item
prediction probability as deﬁned in Eq. 2.
RepeatNet incorporates an extra repeat-explore mechanism to softly switch between repeat mode and explore
mode. We assume that if the next item exists in IS, then it is
generated under the repeat mode, otherwise explore mode.
Here, we can jointly train another mode prediction loss as
follows, which is also the negative log-likelihood loss:
1(iτ ∈IS) log P(r | IS) +
(1 −1(iτ ∈IS)) log P(e | IS),
where 1(iτ ∈IS) is an indicator function that equals 1 if
iτ ∈IS and 0 otherwise.
In the case of joint training, the ﬁnal loss is a linear combination of both losses:
L(θ) = Lrec(θ) + Lmode(θ).
All parameters of RepeatNet as well as the item embeddings are learned in an end-to-end back-propagation training
paradigm. Due to the full probability term in Eq. 2, the two
modes probabilities P(r | IS), P(e | IS) and the item prediction probabilities P(i | r, IS), P(i | e, IS) are basically
competing through a uniﬁed function.
Experiments
Datasets and evaluation metrics
We carry out experiments on three standard datasets,
i.e., YOOCHOOSE, DIGINETICA, and LASTFM. YOO-
CHOOSE and DIGINETICA are frequently used in sessionbased recommendation studies . Since they are both for e-commerce, we choose a
third dataset in a different domain, music, Last.fm.1 See Table 2. The splitting of the datasets are the same as and ﬁlter out sessions of length 1 and
items that appear less than 5 times. They note that the
1/4 version of the dataset is enough for the task and increasing the amount of data will not further improve the
performance.
• DIGINETICA3 is released by the CIKM Cup 2016. We
again follow and ﬁlter out sessions of
length 1 and items that appear less than 5 times.
• LASTFM4 is released by and widely used
in recommendation tasks . We use the
dataset for music artist recommendation; we keep the top
40,000 most popular artists and ﬁlter out sessions that are
longer than 50 or shorter than 2 items.
Recommender systems can only recommend a few items
at a time, the actual item a user might pick should be
amongst the ﬁrst few items of the list . Therefore, commonly used metrics are
MRR@20 and Recall@20 .
In this paper, we also report MRR@10 and Recall@10.
• Recall@k: The primary evaluation metric is Recall@k,
which is the proportion of cases when the desired item
is amongst the top-k items in all test cases.
• MRR@k: Another used metric is MRR@k (Mean Reciprocal Rank), which is the average of reciprocal ranks of
the desire items. The reciprocal rank is set to zero if the
rank is larger than k.
Table 2: Statistics of three datasets with drop ratio p = 0.5. We initialize model parameters randomly using
the Xavier method . We use Adam
as our optimizing algorithm. For the hyper-parameters of
the Adam optimizer, we set the learning rate α = 0.001,
two momentum parameters β1 = 0.9 and β2 = 0.999,
respectively, and ϵ = 10−8. We halve the learning rate α
every 3 rounds. We also apply gradient clipping with range [−5, 5] during training. To speed up the training and converge quickly, we use
mini-batch size 1024 by grid search. We test the model performance on the validation set for every epoch. The model
is written in Chainer and trained on a
GeForce GTX TitanX GPU.
Methods used for comparison
Conventional methods
We select the following conventional methods which are commonly used as baselines in
session based recommendations .
• POP: POP always recommends the most popular items
in the training set. It is frequently used as baselines in
recommender system domains .
• S-POP: S-POP recommends the most popular items of the
current session. Ties are broken using global popularity
values .
• Item-KNN: Items similar to the actual item are recommended by this baseline. Similarity is deﬁned as the cooccurrence number of two items in sessions divided by
the square root of the product of the number of sessions in
which either item occurs. Regularization is also included
to avoid coincidental high similarities between rarely visited items .
• BPR-MF: BPR-MF is a commonly
used matrix factorization method. We apply it to sessionbased recommendation by representing a new session
with the average latent factors of items that occurred in
the session so far.
• FPMC: FPMC is a state-of-the-art hybrid model for nextbasket recommendation. To adapt it to session-based recommendation, we ignore the user latent representations
when computing recommendation scores.
• PDP: Benson, Kumar, and Tomkins propose PDP
and claim that they are the ﬁrst to model sequential repeat
consumption. This is the only recommendation model that
considers sequential repeat consumption, to the best of
our knowledge.
Deep learning methods
No previous studies propose neural models that consider sequential repeat consumption. We
select recent state-of-the-art neural session based recommendation models as baselines.
• GRU4REC: GRU4REC uses
session-parallel mini-batch training process and also employs ranking-based loss functions for learning the model.
• Improved-GRU4REC: Improved GRU4REC improves GRU4REC with two techniques,
data augmentation and a method to account for shifts in
the input data distribution.
• GRU4REC-TOPK: Hidasi and Karatzoglou further improve GRU4REC with a top-k based ranking loss.
• NARM: NARM further improves
Improved-GRU4REC with a neural attention mechanism.
Table 3: Experimental results (%) on the three datasets.
DIGINETICA
Improved-GRU4REC 28.36
GRU4REC-TOPK
RepeatNet (no repeat) 30.02
Bold face indicates the best result in terms of the corresponding metric. Signiﬁcant improvements over the best baseline results are marked with † (t-test, p < .05). The scores reported in on the DIGINETICA dataset
differ because they did not sort the session items according to the “timeframe” ﬁeld, which ignores the sequential information. We run the code released by to obtain the results of GRU4REC, Improved-GRU4REC, GRU4REC-TOPK, and NARM.
Results and Analysis
The results of all methods are shown in Table 3. We run
the code released by 
to report the results of GRU4REC and NARM. We can get
several insights from Table 3. First, RepeatNet outperforms
both conventional methods and recent neural methods, including the strong baselines, GRU4REC-TOPK and NARM.
The improvement of RepeatNet over NARM is even larger
than the improvement of NARM over Improved-GRU4REC.
The improvements mean that explicitly modeling repeat
consumption is helpful, which gives RepeatNet more capabilities to model complex situations in session-based recommendations.
Second, as the repeat ratio increases, the performance
of RepeatNet increases generally. We reach this conclusion
based on the different improvements on YOOCHOOSE and
DIGINETICA. Both datasets are from the e-commerce domain but YOOCHOOSE has a higher repeat ratio.
Third, the performance of RepeatNet varies with different domains. Table 3 shows that RepeatNet has a bigger advantage in the music domain than in the e-commerce domain; we believe this is due to different characteristics of the
different domains. S-POP performs much better than Item-
KNN on LASTFM, which means that popularity is very important on LASTFM. However, Item-KNN performs much
better than S-POP on YOOCHOOSE, which means that collaborative ﬁltering is more important on YOOCHOOSE. Besides, the neural models have substantial gains over the conventional methods in all evaluation metrics on all datasets
generally. Similar conclusions have been formulated in other
recent studies .
Analysis of the repeat mechanism
Table 4: MRR@20 (%) of RepeatNet (with and without repeat mechanism) on repeat and non-repeat sessions.
With repeat
Rep: repeat sessions; Non-Rep: non-repeat sessions.
Generally, RepeatNet with repeat outperforms Repeat-
Net without repeat on all datasets, as shown in Table 3.
The results of RepeatNet (with and without repeat) on repeated and non-repeated sessions are shown in Table 4 and
5. We can see that the improvements of RepeatNet mainly
come from repeated sessions. Especially on DIGINTICA
and LASTFM, RepeatNet improves by 33.91% and 24.16%
respectively in terms of Recall@20 on repeated sessions.
However, RepeatNet drops a little on non-repeated sessions.
The results indicate that RepeatNet has more potential by
explicitly modeling repeat mode and explore mode. But it
also shows the limitation of RepeatNet that it seems inclined
to repeat recommendations too much if we let it learn the
mode probabilities totally from data. A mechanism should
be added to incorporate prior knowledge.
Table 5: Recall@20 (%) of RepeatNet (with and without repeat mechanism) on repeat and non-repeat sessions.
With repeat
Analysis of the attention vs repeat mechanism
Neural attention has shown its potential on many tasks and also on recommender systems recently . We compare
the results of RepeatNet with and without attention, with
and without repeat in Table 6 and 7. The results show that
both repeat and attention mechanisms can improve the results over Improved-GRU4REC. Importantly, the contributions of attention and repeat mechanisms are complementary
as the combination brings further improvements, on all metrics and datasets, demonstrating the need for both. Besides,
we can see that the attention mechanism helps to improve
Recall while the repeat mechanism helps to improve MRR.
Table 6: MRR@20 (%) of RepeatNet with attention vs with
RepeatNet YOOCHOOSE DIGINTICA LASTFM
No attention
Table 7: Recall@20 (%) of RepeatNet with attention vs with
RepeatNet YOOCHOOSE DIGINTICA LASTFM
No attention
Analysis of joint learning
Interestingly, if we jointly train the recommendation loss
Lrec and the mode prediction probability Lmode, the overall performance drops a little, as shown in Table 8. We believe that this is due to the following. First, Lrec is already a
good supervisor for learning the mode prediction. This conclusion can be drawn from Table 4 and 5 where it shows that
RepeatNet (with Lrec only) achieves large improvements on
repeated sessions. And the room left for improvement on repeated sessions is relatively small. Second, RepeatNet (with
Lrec only) is inclined to repeat recommendation. Adding
Lmode further exacerbates the situation. Besides, Lmode assumes that if the next item exists in IS, then it is generated
in repeat mode, which is not always reasonable.
Table 8: MRR@20 and Recall@20 (%) of RepeatNet with
and without joint learning.
Lrec + Lmode
Conclusion and Future Work
We propose RepeatNet with an encoder-decoder architecture
to address repeat consumption in the session-based recommendation task. By incorporating a repeat-explore mechanism into RNNs, RepeatNet can better capture the repeator-explore recommendation intent in a session. We conduct extensive experiments and analyses on three datasets
and demonstrate that RepeatNet outperforms state-of-the-art
methods in terms of MRR and Recall.
RepeatNet can be advanced and extended in several directions. First, prior knowledge of people can be incorporated
to inﬂuence repeat-explore mechanism. Second, more information (e.g., metadata, text) and more factors (e.g., collaborative ﬁltering) can be considered to further improve the
performance. Besides, variants of RepeatNet can be applied
to other recommendation tasks, such as content based recommendations.
Acknowledgments
This work is supported by the Natural Science Foundation
of China (61672324, 61672322), the Natural Science Foundation of Shandong province (2016ZRE27468), the Fundamental Research Funds of Shandong University, Ahold Delhaize, Amsterdam Data Science, the Bloomberg Research
Grant program, Elsevier, the European Community’s Seventh Framework Programme under grant
agreement nr 312827 (VOX-Pol), the Google Faculty Research Awards program, the Microsoft Research Ph.D. program, the Netherlands Institute for Sound and Vision, the
Netherlands Organisation for Scientiﬁc Research (NWO)
under project nrs CI-14-25, 652.002.001, 612.001.551, 652.-
001.003, and Yandex. All content represents the opinion of
the authors, which is not necessarily shared or endorsed by
their respective employers and/or sponsors.
To facilitate reproducibility of the results in this paper, we
are sharing the code used to run the experiments in this paper
at