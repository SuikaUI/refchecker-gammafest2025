Computer Speech and Language 
sets of filters using
back-propagation
David C. Plaut and Geoffrey
Cotnputc~r Scimw
Department,
University. Pittsburgh,
PA 15,713. C1.S.A.
A learning
procedure,
called back-propagation,
for layered networks
of deterministic,
neuron-like
units has been described
previously.
ability of the procedure
automatically
to discover useful internal
representations
makes it a powerful
tool for attacking
like speech recognition.
This paper describes further research
on the learning
and presents an example in which a network
learns a set of filters that enable it to discriminate
formant-like
patterns in the presence of noise. The generality
of the learning
is illustrated
by a second example in which a similar
learns an edge detection
task. The speed of learning
strongly dependent
on the shape of the surface formed by the error
measure in “weight
space”. Examples are given of the error surface for
a simple task and an acceleration method that speeds up descent in
weight space is illustrated. The main drawback of the learning
procedure is the way it scales as the size of the task and the network
increases. Some preliminary results on scaling are reported and it is
shown how the magnitude of the optimal weight changes depends on
the fan-in of the units. Additional results show how the amount of
interaction between the weights affects the learning speed. The paper is
concluded with a discussion of the difficulties that are likely to be
encounted in applying back-propagation to more realistic problems in
speech recognition.
and some promising
approaches
to overcoming
these difficulties.
1. Introduction
A major difficulty in designing systems for hard perceptual tasks like speech recognition
is in developing the appropriate sequence of representations, or filters, for converting the
information in the input into a form that is more useful for higher-level processes.
Rumelhart, Hinton & Williams (1986aJ) described a learning procedure, called hack-
prupagafim,
that discovers useful representations in layered networks of deterministic.
neuron-like units. The procedure repeatedly adjusts the weights on connections in the
network to minimize a measure of the difference between the actual output vector of the
network and the desired output vector given the current input vector. The power of this
procedure for learning representations makes it plausible to train
0085523081871010035 + 17 $03.00/0
 
a network,
to the one used in the first example, learns to map edge-like patterns
of varying
into a representation
of the orientation
and position
of the edge in p-0 space.
A major issue in the use of back-propagation
as a design tool is how long it takes a
to learn a task. The speed of learning is strongly
dependent on the shape of the
surface formed
by the error measure in “weight
This space has one dimension
for each weight in the network
and one additional
(height) that represents
overall error in the network’s
performance
for any given set of weights.
For many tasks.
that cause problems
for simple gradient
procedures.
Section 5 contains
examples of the shape of the error surface for a simple
task and illustrates
the advantages
of using an acceleration
method to speed up progress
a ravine without
causing divergent
“sloshing”
across the ravine.
The main drawback
of the learning procedure
is the way learning time scales as the
size of the task and the network
increases. In Section 6 we give some preliminary
on scaling and show how the magnitude
of the optimal
weight changes depends on the
of the units.
Additional
in Section
7 illustrate
the amount
interaction
the weights
affects the learning speed.
In the final section, we discuss
the difficulties
that are likely to be ecountered
attempting
to extend this approach
to real speech recognition,
and suggest a number of
approaches
for overcoming
these difficulties.
2. Back-propagation
2.1. The units
The total input, x,, to a unit j is a linear function
of the outputs
of the units. i, that are
to j and of the weights,
M;,, on these connections.
A unit has a real-valued
output, yj, (also called its state), that is a non-linear
its total input.
It is not necessary
to use the exact functions
given by equations
(1) and (2). Any input
that has a bounded derivative
will suffice. However,
the use of a linear
Learning sets of jilters
for combining
the inputs
to a unit before applying
the non-linearity
simplifies
the learning procedure.
2.2. Layered feed-forward
The simplest
form of the learning
is for networks
that are organized
sequential
layers of units, with
a layer of input units at the bottom,
any number
intermediate
layers, and a layer of output units at the top. Connections
are not allowed
a layer, or from higher to lower
layers. The only connections
from lower
layers to higher layers, but the layers need not be adjacent; connections
skip layers.
The network
is run in two stages: a forward
pass in which
the state of each unit in the
is set, and a backward
pass in which
the learning procedure
the forward
pass, an input vector is presented to the network
by setting the states of the
input units. Layers are then processed
sequentially,
at the bottom and working
The states of units in each successive
layer are determined
in parallel
applying equations
(1) and (2) to the connections
coming from units in lower layers. The
pass is complete once the states of the output units have been determined.
2.3. The learning procedure
The aim of the learning procedure
is to find a set of weights
such that, when the network
is presented
with each input vector,
the output
vector produced
by the network
same as (or sufficiently
close to) the desired ouput vector.
Given a fixed, finite set of
input-output
cases, the total error in the performance
of the network
with a particular
set of weights
can be computed
by comparing
the actual and desired output vectors for
every case. The error,
E, is defined by
E = $c&;,,
where c is an index over cases (input-output
pairs), j is an index over output units, ~7 is
the actual state of an output
unit, and d is its desired state.
The learning procedure
minimizes E by performing
gradient descent in weight space.
This requires
the partial derivative
respect to each weight
This derivative
the sum of the partial
derivatives
for each of the
input-output
cases. For a given case, the partial derivatives
of the error with respect to
each weight
are computed
a backward
pass that follows
the forward
The backward
pass starts
the output
units at the top of the network
successively
the layers, “back-propagating”
derivatives
each weight in the network.
To compute these derivatives
it is necessary first to compute
the error derivatives
with respect to the outputs
and inputs of the units.
First, the change in error is computed
with respect to the output 4; of each output unit
j. Differentiating
(3) for a particular
case, c, and suppressing
the index c gives
D. C. Plaut and G. E. Hinton
The chain rule can then be applied to compute
the change in error with
respect to the
input xj of each output
aux, ayj dx;
Substituting
the value of dyj/dxj (from
differentiation
of equation
It is now known
how a change in the total input to each output
unit will affect the
error. As this total input is simply a linear function
of the states of the units in the next
layer and the weights
on the connections
from those units, it is easy to compute
how changing
these states and weights
will affect the error.
The change in error with
respect to a weight,
wjj, from unit i to output
unit j, is
The effect of changing the output of unit i on the error caused by output unitj is simply
aE axi- aEu, .
ady, a-vi ax, ~1
As unit i may be connected
to a number
units, the total change in error
resulting from changing its output is just the sum of the changes in the error produced
each of those output units:
these steps of the backward
pass laid out graphically.
It has been
how aE/ay can be computed
for any unit in the penultimate
layer when given iiE/
ay for all units in the last layer. Therefore
this procedure
can be repeated to compute aE/
ay for successively
earlier layers, computing
for the weights
in the process.
of computation
for the backward
pass is of the same order as the
pass (it is linear in the number of connections)
and the form of the computation
is also similar.
In both cases, the units compute
a sum by multiplying
each incoming
by the weight on the connection
(see equations
1 and 6). In the backward
all the connections
are used backwards,
and ZJEjay plays the role that y plays in the
pass. The main difference is that in the forward
pass the sum is put through
non-linear
in the backward
pass it is simply multiplied
by y,(l -)I,).
One way of using aE/iiw
is to change the weights
after every input-output
case. This
has the advantage
that no separate
is required
the derivatives.
alternative
scheme, which was used in the research reported here, is to accumulate
Learning sets of Jilters
yj (1 - yj )
= xi aWaxj Wji
The steps involved
in computing
for the intermediate
a multilayer
The backward
pass starts
at the top of the Figure
downwards.
all the connections
are shown.
over all the input-output
cases (or over a large number
of them if it is not a finite set)
before changing the weights.
The advantage of this second method is that it allows
value of E in equation (3) to be monitored
for a given set of weights.
The simplest version of gradient descent is to change each weight
by a proportion,
of the accumulated
The speed of convergence
of this method can be improved
use of the second
derivatives,
but the algorithm
is much more complex and not as easily implemented
local computations
in parallel hardware.
The above method can be improved
cantly without
sacrificing
the simplicity
and locality by using an acceleration
the current
is used to modify
the velocity
of the point in weight
instead of its position.
where t is incremented
by 1 for each sweep through
the whole set of input-output
an epoch), and a is an exponential
decay factor
0 and 1 (called
that determines the relative contribution
of the current and past gradients to
the weight
change. We call a the momentum
because that has appropriate
connotations,
even though it is not a precise analogy. The correct analogy is to viscosity.
(7) can be viewed as describing
the behaviour
of a ball-bearing
rolling down
is immersed
in a liquid
determined
by a. A similar
acceleration
was used for estimating
Amari . Its effectiveness
is discussed
in Section 5.
The learning procedure
is entirely deterministic,
so if two units within
a layer start off
with the same connectivity
and weights,
there is nothing to make them ever differ from
each other. This symmetry
by starting
small random
D. C. Plaut and G. E. Hinton
3. Learning to discriminate noisy signals
Rumelhart et al. 
illustrated the performance of the learning procedure on many
different, simple tasks. A further example is given here which demonstrates that the
procedure can construct sets of filters that are good at discriminating between rather
similar signals in the presence of a lot of noise. An artificial task (suggested by Alex
Waibel) was used which was intended to resemble a task that arises in speech
recognition. The authors are currently working on extending this approach to real
speech data.
The input is a synthetic spectrogram that represents the energy in six different
frequency bands at nine different times. Figure 2 shows examples of spectrograms with
no random variation in the level of the signal or the background, as well as the same
spectrograms with added noise. The problem is to decide whether the signal is simply a
horizontal track or whether it rises at the beginning. There is variation in both the
frequency and onset time of the signal.
It is relatively easy to decide on the frequency of the horizontal part of the track. but it
is much harder to distinguish the “risers” from the “non-risers” because the noise in the
signal and background obscures the rise. To
make the distinction accurately, the
network needs to develop a set of filters that are carefully tuned to the critical
differences. The filters must cover the range of possible frequencies and onset times, and
when several different filters fit quite well, their outputs must be correctly weighted to
give the right answer. It should be noted, however, that due to effects such as time
warping, the corresponding problem in real speech data is considerably more difficult.
Possible ways of extending this approach to deal with more realistic data are described in
Section 8.
spectrograms
(a) without
and (b) with
Each horizontal
represents
in a particular
Learning sets offilters
The output layer
A layer of 24 hidden
units each of which
is connected
54 input units and to
both output units
A layer of 54 input
units whose
levels encode the
energy in six frequency
bands for nine time
The network
used for disciminating
like those in Figure
The network used comprised three layers, as shown in Fig. 3. Initially the network was
trained by repeatedly sweeping through a fixed set of 1000 examples, but it learned to use
the structure of the noise to help it discriminate the difficult cases, and so it did not
generalize well when tested on new examples in which the noise was different. It was
decided, therefore, to generate a new example every time so that, in the long run, there
were no spurious correlations between the noise and the signal. Because the network
lacks a strong a priori model of the nature of the task, it has no way of telling the
difference between a spurious correlation caused by using too small a sample and a
systematic correlation that reflects the structure of the task.
Examples were generated by the following procedures.
(1) Decide to generate a riser or a non-riser with equal probability.
(2) If it is a non-riser pick one of the six frequencies at random. If it is a riser pick one
of the four highest frequencies at random (the final frequency of a riser must be
one of these four because it must rise through two frequency bands at the
beginning).
(3) Pick one of five possible onset times at random.
(4) Give each of the input units a value of 0.4 if it is part of the signal and a value of
0.1 if it is part of the background. We now have a noise-free spectrogram of the
kind shown in Figure 2(a).
(5) Add independent Gaussian noise with a mean of 0 and standard deviation of 0.15
to each unit that is part of the signal. Add independent Gaussian noise with a
mean of 0 and standard deviation of 0.1 to the background. If any unit now has a
negative activity level, set its level to 0.
Learning sets ofjilters
The weights
were modified
after each block
of 25 examples.
For each weight,
values of dE/dw were summed for all 25 cases and the weight increment
after block t was
given by equation (7). For the first 25 blocks we used E = 0.005 and a = 0.5. After this the
changed rather slowly
and the values were raised to s = 0.07 and a = 0.99. It was
that it is generally
to use more conservative
values at the beginning
because the gradients
are initially very steep and the weights
tend to overshoot.
have settled down, they are near the bottom of a ravine in weight space, and high
values of a are required to speed progress
along the ravine and to damp out oscillations
across the ravine. The validity of interpreting
characteristics
space in terms of
structures
such as ravines is discussed
in Section 5.
In addition
to the weight
changes defined by equation
(7) each weight
decremented
by hw each time it was changed, where
h is a coefficient
that was set at
for this simulation.
This gives the weights
a very slight tendency
zero, eliminating
that are not doing any useful work.
that weights
SE/SW is near zero will keep shrinking
in magnitude.
Indeed, at equilibrium
the magnitude
of a weight will be proportional
to ~E/c?M’ and so it
will indicate how important
the weight is for performing
the task correctly.
This makes it
much easier to understand
the feature detectors
by the learning.
One way to
view the term 11~3 is as the derivative
of $hcc>‘, so the learning procedure
can be viewed as a
compromise
minimizing
E and minimizing
the sum of the squares
Figure 4 shows
the activity
levels of the units in all three layers for a number
examples chosen at random
after the network
has learned. Notice
that the network
about whether
the example is a riser or a non-riser,
but that in
difficult cases it tends to “hedge its bets”. This would provide more useful information
a higher level process
than a simple forced choice. Notice
also that for each example,
most of the units in the middle layer are firmly off.
The network
was trained for 10 000 blocks
of 25 examples each. After this amount of
experience the weights
are very stable and the performance
of the network
has ceased to
If the network
is forced to make a discrete decision by interpreting
active of the two output units as its response, it gives the “correct”
response 97.8% of the
time. This is better than a person can do using elaborate reasoning,
and it is probably
very close to the optimal
performance.
could be 100% correct
because the very same data can be generated by adding noise to two different underlying
signals, and hence it is not possible to recover the underlying
signal from the data with
certainty.
The best that can be done is to decide which category of signal is most likely to
have produced
the data and this will sometimes
not be the category from which
was actually derived. For example, with the signal and noise levels used in this example.
there is a probability
of about I .2% that the two crucial input units that form the rising
part of a riser will have a smaller combined
level than the two units that would
form part of a non-riser
with the same onset time and same final frequency.
This is only
one of several possible errors.
Figure 5 shows
the filters that were learned in the middle layer. The ones that have
positive weights
to the “riser”
unit have been arranged
at the top of the Figure.
Their weights
are mainly concentrated
on the part of the input that contains
the critical
information,
and between them they cover all the possible frequencies
and onset times.
D. C. Plaut and G. E. Hinton
levels of units in all three
for a number
that each filter covers
several different
cases and that each case is covered
several different filters. The set of filters form an “ecology”
in which each one fills a niche
that is left by the others. Using analytical
methods it would be very hard to design a set
of filters
this property,
even if the precise characteristics
of the process
generated the signals were explicitly
given. The difficulty
arises because the definition
a good set of filters is one for which
there exists a set of output weights
that allows
correct decision to be made as often as possible. The input weights
of the filters canot be
designed without
considering
the output
and an individual
filter cannot
designed without
considering
all the other filters. This means that the optimal value of
each weight depends on the value of every other weight.
The learning procedure
as a numerical
method of solving this analytically
intractable
design problem.
analytical
investigations
of optimal
filters are very
helpful in providing
understanding
of why some filters are the way they are, but we can
our understanding
of the variety and robustness
of possible filters by
numerical solutions,
particularly
when a number of different filters must work
for making
discriminations.
Therefore.
even if back-propagation
implausible
as a biological
learning mechanism,
it may still be useful for exploring
space of possible filter designs.
D. C. Plaut and G. E. Hinton
of the filters
by the middle
represented
by a square
size is proportional
to the magnitude
represents
the sign of the weight
for positive,
for negative).
4. Encoding edges in an image
To illustrate
the ability of the back-propagation
to discover
filters, we chose a task in which edges must be extracted from an intensity
array. The 225
input units implicitly
encode an edge by explicitly
the intensities
in a 15 x 15
image (see Fig. 6). The output
units explicitly
encode the edge: an active output
represents
an edge with
a particular
orientation
0 and a particular
perpendicular
distance p from the center of the image. Between the input and output units is a layer of
18 hidden units which
act as a narrow
channel (like the optic nerve). The
hidden units must learn to transmit
information
about the current
array in a
that makes it easy for the network
to produce
the correct
were interested
in what coding the learning procedure
All the possible
edges that could
in the image can be
represented
as points in the two-dimensional
space formed
by the parameters
in Fig. 7. To cover this space of possible edges with only 36 output units we used a
sampling method. The output units correspond
to a 6 x 6 grid of points in the
Learning sets of jilters
The activity
levels of all units
for a number
of different
of the network
is similar
to that used to recognize
spectrograms.
Representation
of an edge in terms
of its perpendicular
the origin
and its angle
the horizontal.
D. C. Plaut and G. E. Hinton
space. A given edge corresponds
to a point that typically fails to fall exactly on one of the
36 grid points. We could have chosen the nearest grid point to represent
the edge, but
this is an unstable representation
because a very small change in the edge can lead to a
big change in its representation.
A more robust method is to convolve
the point with a
and then to sample this blur at the grid points. The sample
values are the representation
of the edge. This method has the additional
advantage that
it represents
the orientation
and position
of an edge to a greater
separation
of the grid points.
To train the network,
a freshly generated edge was used on every trial. To generate an
edge, we first chose an orientation
between 0” and 360” and a distance from the center of
the image between
- 6 and 6. We then chose the intensities
d and 1 of the dark and light
sides of the edge: d was chosen at random
0 and 0.6 and then I was chosen
and 1.0. This guaranteed
an intensity
step of at least 0.4. Finally,
edge was smoothed
by setting the intensity
of each input unit to be:
] + e- r/n 25
where x is the perpendicular
distance of the pixel from the edge (x is positive if the pixel
lies on the light side of the edge). This corresponds,
almost exactly, to convolving
image with a Gaussian
Some of the tilters
by the middle
Learning sets of jilters
the network
perfectly.
to and from some of the hidden units are shown
in Fig. 8. Notice that the hidden
units do not look like edge detectors.
Each edge is detected by activity
in many hidden
units and each hidden unit contributes
to the detection
of many edges. Many
fields resemble
the “on-center-off-surround”
or “off-center-on-surround”
fields that are found for retinal ganglion cell. The receptive fields are not localized
small parts of the image because only one edge is present at a time, so there is no reason
not to combine information
from all over the image.
This section and the previous
one present
small examples
learning procedure
is able to construct
efficient sets of filters. Determining
the relevance
of these examples to problems
of more realistic size requires
a better understanding
the properties
of the learning procedure
that affect how the learning time scales as the
size of the network
and the task increase. The following
three sections
are devoted to
analyzing these properties.
5. Characteristics
of weight space
As mentioned
in Section
1, a useful way
to interpret
the operation
of the learning
is in terms of movement
down an error surface in a multi-dimensional
space. For a network
only two connections,
the characteristics
of the error surface
for a particular
task are relatively easy to imagine by analogy with actual surfaces which
curve through
three-dimensional
space. The error
suface can be described
being composed
of hills, valleys,
ridges, plateaus,
saddle points,
etc. In the
procedure,
the effects
of the weight-change
step (E) and momentum
parameters
have natural
interpretations
in terms of physical
among such
formations.
Unfortunately,
for more useful networks
or thousands
connections
it is not clear that these simple intuitions
about the characteristics
space are valid guides to determining
the parameters
of the learning procedure.
One way to depict some of the structure
of a high-dimensional
weight space is to plot
the error curves (i.e. cross-sections
of the error
along significant
directions
weight space and compare them to error curves along random directions.
The collection
of curves represents
surface “collapsed”
onto two dimensions.
While such a
graph gives a far from complete picture of weight space, it may give us a more direct way
to test the effects of different learning parameters
as well as clarify our interpretation
in weight space in terms of simple three-dimensional
constructs.
As an example. we present a few collapsed error surface graphs of a simple learning
:rt various
points in the search for a good set of weights.
The problem we will
is learning the association
of 20 pairs of random
binary vectors
of length 10.
The procedure
will operate on a three-layered
10 input units, 10 hidden
units, and 10 output units. Each input unit is connected
to each hidden unit, and each
hidden unit is connected to each output
unit. Taking into account the connections
a permanently
active unit to the hidden and output units (used to encode thresholds),
has a total of 220 connections.
This relatively
simple task was chosen because it
can be learned quickly
enough to allow extensive exploration
of the error surface. Error
surfaces of mom complex
tasks are qualitatively
Each curve in a graph
is generated
by: (1) choosing
a direction
(2) changing
the weights
in the network
representing
that direction;
and (3) plotting
the error produced
by the network
D. C. Plaut and G. E. Hinton
modified weight
values. In addition
to a number
directions
two significant
directions
(solid curves):
the direction
of maximum
and the direction
of the next weight step (integrated
gradient).
Each curve is labeled on
the right with its angle (in degrees) from the direction
of maximum
gradient. An asterisk
the current
space, and a vertical
the next position.
9-l 2 show
for the problem
above at points
throughout
the operation
of the learning procedure.
are presented for the first 10
epochs, as well as for epochs 25, 50, 7.5 and 107 (when
a solution
is reached).
example, s=O.l
and initially a=0.5.
the first few epochs, the procedure
repeatedly
reaches a minimum
use the influence
of the calculated
information
to change directions.
As momentum
contributes
to maintaining
along a particular
direction,
it is important
in these early stages that momentum
so as not to dominate the new gradient information.
The effect of having momentum
high at the start of learning will be illustrated
in later graphs. It is not until epoch 9 or 10
11) that continued
along the last weight-change
beneficial.
for epochs
Learning .sets ofJilters
for epochs
By epoch 25. the directions
of maximum
and integrated
practically
and monotonically
decreasing
over a relatively
long distance
space. In contrast,
in each of the random
directions
immediately
as we move away
from the current
point. The intuitive
interpretation
is that the learning
along the botom
ravine in weight
space. Because the correspondence
of the directions
of maximum
and integrated
increasing
speed up movement
the ravine without
causing divergent
oscillations
onto the walls of the ravine.
Accordingly,
(a) was increased to 0.95 at this point.
While the integrated
(and hence the direction
pointed along the bottom
of the ravine at epoch 50, the direction
of maximum
now points somewhat
the ravine. Without
the learning procedure
from side to side along the walls of the ravine. The high momentum
dampens this oscillatory
contribution
and maintains
movement along the most effective
direction.
This effect of momentum
becomes increasingly
stages of learning, as is evident at epoch 75 (Fig. 12). and finally at epoch 107. when a
is reached.
These graphs
suggest that momentum
be set initially
raised when the learning procedure
has settled on a stable direction
of movement.
D. C. Plaut and G. E. Hinton
for epochs
(solution)
for epochs
107 (solution).
Learning sets of jilters
to illustrate
the behavior
of the procedure
this rule is violated,
the collapsed
of the first
epochs of a run with
set initially
to 0.9 (instead
of 0.5). The first epoch is fine, since there is no
integrated
to affect the weight
change. However,
by epoch 3 the overly high
has caused the procedure
to overshoot
the minimum
of the original weight-
change direction
and increase the total error over the last position
space. This
usually means that hidden units have activities very near zero or one for all input vectors,
so they have very small error derivatives
and recover their sensitivity
very slowly.
In the first example run, almost 50 epochs were required to reduce the total error from
just over 50 to the solution
(near O-O), even with very high momentum
This suggests the possibility
of increasing
the size of each weight
step to speed up the
later stages of learning when high momentum
has essentially fixed the direction
change. In fact, increasing
E does significantly
reduce the number of epochs to solution.
as long as the weight step is not so large that the procedure
drastically
changes direction.
because a number of changes of direction
are required in the early stages of
the weight
not be too large initially.
14 illustrates
divergent behavior
that results at the beginning of a run with E set to 0.5 (instead of 0.1).
The first step drastically
overshoots
the minimum
along the direction
of maximum
Successive
are still too large to produce
for the first
high momentum
(a = 0.9).
D. C. Plaut and G. E. Hinton
for the first four
(E = 0.5).
the learning time scales
Small-scale
simulations
can only provide
into the behavior
of the learning
in larger networks
if there is information
about how the learning time scales.
Procedures
that are very fast for small examples
but scale exponentially
are of little
if the goal is to understand
learning in networks
with thousands
or millions of
units. There are many different
that can be scaled:
(1) the number of units used for the input and output vectors and the fraction
that are active in any one case;
(2) the number
of hidden layers;
(3) the number
of units in each hidden layer;
(4) the fan-in and fan-out
of the hidden units;
(5) the number of different input-output
pairs that must be learned. or the complexity
of the mapping
from input to output.
Learning sets of jilters
Much research remains to be done on the effects of most of these variables. This section
only addresses
the question
happens to the learning time when the number
hidden units or layers is increased
but the task and the input-output
encoding remain
If there is a fixed number of layers, we would
like the learning to go faster if the
has more hidden units per layer.
6.1. E.uperiments
Unfortunately,
two initial experiments
that increasing
the number
units or hidden layers slowed
the learning.
time is measured
number of sweeps through
the set of cases that are required to reach criterion.
time required
to simulate a larger network
on a serial tnachine is not counted.)
experiment,
on the identical
associations
of 20 pairs of random binary vectors
of length 10. Each network
of three layers, with
10 input units and 10 output
units. The first (called a l~l&lO
had 10 hidden units receiving input from all 10 input units and projecting
units; the second (called a 10-100-10
had 100 hidden units fully
interconnected
to both input and output units. Twenty
runs of each network
on the task
were carried out, with c=O.l
and a=0.8.
The results
of this first experiment
made it clear that the learning procedure
current form does not scale well with the addition of hidden units: the 10-l&10
took an average of 212 epochs to reach solution,
while the l&10&10
average of 531 epochs.
The second experiment
involved adding additional
layers of hidden units to a network
and seeing how the different networks
on the same task. The task was similar
to the one above, but only 10 pairs of vectors were used. Each network
has 10 input units
fully interconnected
to units in the first hidden layer. Each hidden layer had 10 units and
was fully interconnected
to the following
one, with the last connected to the 10 output
units. Networks
with one, two and four layers of hidden units were used, Twenty
each network
were carried
out, with E = 0.1 and a = 0.8.
The results
of the second experiment
were consistent
those of the first:
with a single hidden layer solved the task in an average of 100 epochs; with two
hidden layers it took
160 epochs on average, and with
four hidden layers it took an
average of 373 epochs to solve the task.
6.2. Unit splitting
There is one method
of introducing
more hidden units which
has no effect on the
performance
of the network.
Each hidden unit in the old network
is replaced
identical hidden units in the new network.
The input weights
of the new units are exactly
the same as for the old unit, so the activity
level of each new unit is exactly the same as
for the old one in all circumstances.
The output weights
of the new units are each l/n of
the output
of the old unit, and so their combined
effect on any other unit is
exactly the same as the effect of the single old unit. Figure
15 illustrates
this invariant
unit-splitting
operation.
To ensure that the old and new networks
remain equivalent
even after learning, it is necessary for the outgoing weights
of the new units to change by
1 in times as much as the outgoing weights
of the old unit. Therefore,
a different value of
D. C. Plaut and G. E. Hinton
have identical
input-output
tinctions.
input-output
is invariant
the operation
of splitting
intermediate
the outging
are also decreased
by the same factor.
E must be used for the incoming
and outgoing
and the E for a connection
from a hidden unit must be inversely
proportional
to the fan-in of the unit
receiving the connection.
6.3. Varying E with fan-in
The fact that it is possible to increase the number of hidden units and connections
by a factor
of n without
affecting the performance
of the learning procedure
suggests a way to improve
how well it scales. Critical
to the success of the unit-splitting
process is dividing the weight change step (E) by n for weights
on replicated connections.
This ensures that the weight
changes on incoming
connections
to a unit will cause the
same change in total input for a given amount
by the unit, even
connections
are contributing
to the input
equivalent procedure
in a normal network
be to set the effective weight
step for a
connection,
E,?, to be inversely
proportional
to the fan-in of the unit receiving input via
that connection.
Presumably
such a modification
also improve
the scaling of the
learning procedure
for networks
non-uniform
observations
of the operation
of the procedure
on different sized nets make
it clear that larger networks
higher fan-ins)
require a much smaller value of F, for
optimal learning than do smaller networks.
If the change in input to a unit is too large.
due to an overly ambitious
value of E, the output of the unit may overshoot
its optimal
value, requiring
an input change in the opposite direction
during the next epoch. Thus,
given the fan-in of units in a network,
setting E too high results in oscillatory
and poor learning performance.
if the effective E is reduced for connections
leading into units with many inputs but not reduced for other conections,
this oscillatory
can be avoided without
the learning of weights
on connections
input to units with
A close look at the details of the backward
pass of the learning procedure
clear why such a modification
be beneficial. Each connection
weight IV,, is changed
Learning sets qfjilters
in proportion
to the error attributed
to the output of unitj,
independent
of other inputs
unit j may receive.
A\v,= e - gSvj( 1 - ?;>,Y,.
Hence, the resulting
change in total input to unit j,
is proportional
to n, the fan-in of unit j.
In order to determine if varying E with fan-in would
the scaling performance
of the learning procedure,
the scaling experiment
the addition
of hidden units
to a single hidden layer was repeated using values of ~~~ inversely proportional
to the fan-
in of unit j. The constant of proportionality
was set at 1.0 so that the lo-lo&IO
had an effective E on the input connections
to the output units of 0.01, while the effective
c on the input connections
to the hidden units remained at 0.1. It was expected that these
more conservative
change steps would
any oscillatory
the learning performance.
The results bore out these expectations.
The average number of epochs to solution
the l&10&1
reduced from 531 to 121. By varying
of hidden units speeded up the learning by almost a factor of two, rather than
it down (recall from Section 6.1 that the 10-l&10
took 212 epochs on
this task).
This is not a solution
to the entire
scaling problem,
but it represents
significant
improvement
in the ability of the learning procedure
to handle large, complex
7. Reducing the interactions
between the weights
The previous
demonstrated
that, by varying
E inversely
interconnected
100 hidden units can learn a task nearly twice as fast as a
only 10 hidden units. While this manipulation
of E improves
scaling performance
of the learning procedure,
many presentations
of each environmen-
tal case are required
to learn most tasks, and larger networks
still generally take longer
to learn than do smaller ones. The above comparison
does not tell us what
particular
characteristics
of a network
most significantly
influence its learning speed, because at
least two important
factors are confounded:
(1) the number of hidden units, and (2) the
fan-in of the output
the learning speed is not necessarily
dependent on the number of units and
connections
in a network.
This can be seen by considering
similar to the 10~
IOO- 10 network.
but in which
the layers are not fully interconnected.
In particular,
hidden units are partitioned
into groups of 10, with each group receiving input from all
input units but only projecting
to a single output unit. For convenience, we will call this a
I O-~IOoflO~~lO network.
This structure
transforms
each 10 to 10 mapping
rrm’vpcwdmt
IO to 1 mappings,
and so reduces the amount of interaction
between weights
on connections
leading into the output layer.
D. C. Plaut and G. E. Hinton
7.1, Experiments
In order to investigate
the relative effects on learning
speed of the number
units, the fan-in of the output units, and the amount of interaction
between the weights,
the performances
lCLlOCLl0,
l&lOofl&10
on the task of learning
the association
pairs of random
of length 10. The results of the comparison
are summarized
in Table I.
As the Table shows,
the IO-10oflG10
solves the task much faster than the
both networks
have uniform
fan-in and the same number
of connections
from the hidden layer to the output
layer. The l&lOofl~lO
learns more quickly
because the states of units in each group
of 10 hidden units are
constrained
only by the desired state of a single output unit, whereas
the states of the 10
hidden units in the 10-l&10
must contribute
to the determination
of the states
of all 10 output
units. The reduced constraints
can be satisfied more quickly.
when E is varied so that the effects of fan-in differences
are eliminated,
l&lOof1&10
learns slightly
than the l(rlO&lO
even though
both networks
have the same number of hidden units and the l&100-10
much greater amount of interaction
between weights.
Thus a reduction
in the interaction
does not always
improve its performance.
The advantage of having an
additional
90 hidden units, some of which
may happen to detect features that are very
useful for determining
the state of the output
unit, seems to outweigh
the difficulty
caused by trying to make each of those feature detectors
adapt to ten different masters.
One might expect such a result for a task involving
highly related environmental
but it is somewhat
more surprising
for a task involving
random associations,
where there
is no systematic
in the environment
for the hidden units to encode. It appears
that, when the magnitudes
of weight changes are made sensitive to the number of sources
of error by varying E with fan-in, the learning procedure
is able to take advantage
additional
flexibility
by an increase in the interactions
the weights.
Comparison
of performance
of the l&1&10,
and l&lOofl&lO
on the task of learning
associations
10. Data was averaged over 20 runs with ~=0.1
in the fixed E cases, E,, = l.O/fan-in,
in the variable
E cases, and a = 0.8
(varying E has no effect on networks
with uniform
fan-in, and so the
average number
of epochs to solution
for these conditions
is placed in
parentheses).
of Average No. of epochs to solution
_ ~~__~~--~
l&lOofl~lO
Learning sets of jilters
7.2. Very fast learning with no generalization
Some insight into the effects of adding more hidden units can be gained by considering
the extreme case in which
the number of hidden units is an exponential
number of input units. Suppose that binary threshold
units are used and that the biases
and the weights
coming from the input units are fixed in such a way that exactly one
hidden unit is active for each possible input vector. Any possible mapping between input
and output
vectors in a single pass can now be learned. For each input vector, there is
one active hidden unit, and only the signs of the weights
from this hidden unit to the
output units need be set. If each hidden unit is called a “memory
and the signs
of its outgoing
are called its “contents”.
this is an exact model of a standard
random-access
This extreme case is a good illustration
of the trade-off
between speed of learning and
generalization.
It also suggests that if fast learning is required,
the number
units should be increased and the proportion
of them that are active decreased.
8. Applications
to speech recognition
It is clearly tempting
to try to apply the back-propagation
learning procedure
speech recognition,
because it has the ability to discover a hierarchy
of non-linear
automatically.
It seems more promising
than earlier connectionist
approaches
like Boltzmann
& Fallside,
1986) because it learns much
faster and produces much better final performance.
Several different research groups are
back-propagation
to real speech recognition
and so good data on its
effectiveness
soon be available.
We confine
here to a few cautionary
about the difficulties
that are likely to be encountered
in this application.
One approach
is to use. as input, a portion
of a spectrogram
that contains
phoneme which has already been segmented out and temporally
aligned . The example given earlier in this paper can be seen as an idealized
of this where
the temporal
is somewhat
The network
trained on many examples of various
phonemes, and then tested to see whether
identify the phonemes in new examples. The first problem with this approach is
that it is bound to require an immense amount of accurately
labeled training data to give
good generalization.
The network
requires thousands
of weights
to allow it to
capture the relevant structure,
and so the training data must contain many thousands
bits in order to constrain
these weights
sufficiently
to give good generalization.
number of bits in the training data does not significantly
exceed the number of degrees of
freedom in the model, the network can use a form of “rote-learning” or “table look-up”
to learn the training cases-it can find a setting of the weights that gives the correct
output for all the training examples without capturing the underlying regularities of the
task. Unfortunately,
each training example only contains a few bits because. for :I
supervised learning procedure like back-propagation, the information in an example is
the number of bits it takes to specify the correct output. So there is a severe practical
problem in getting enough data. The results of Prager et al. confirm that
generalization is poor when a general learning procedure is used to fit a model that has
more degrees of freedom than there are bits in the training set.
One reason why this first approach requires so much data is that there is very little
D. C. Plaur and G. E. Hinton
prior knowledge
built into the network.
If the network
starts with small random weights,
and if each unit in the first layer of hidden units receives input from every cell in the
spectrogram,
the network
learn just as well if the cells were put through
permutation
before being presented
as input. In other words,
the network
no prior expectations
that adjacent time frames or adjacent frequences
arc likely to be
more relevant
to each other than non-adjacent
ones. This lack of prior
expectation
means that the learning is searching
a huge space of possible filters most of which
almost certainly
useless. Many people have suggested that the amount of training
required could be reduced by using an architecture
that omits irrelevant
connections,
by starting
reasonable
hand-coded
feature detectors
instead of random
A second approach.
we feel is more promising
because it does not demand such
in the training
data. is to use an iterative
back-propagation
learning procedure.
This version,
is explained in Appendix
for networks
that have recurrent
connections
and are run for many time steps.
The network
receives new input at each time step, and it can either receive error-terms
each time step or receive the error after the final iteration.
Using an iterative network.
is no longer necessary
to turn many time frames into a single, spatially
laid out input
Instead, the network
can receive the time frames one at a time. Because the very
same set of weights
is applied at each time step, the network
that the same kind of event may occur at different
times. It does not have to learn to
the event separately
for each possible
time of occurrence
as it would
input was laid out spatially.
Thus, an important
of the task is built
into the iterative approach
and does not need to be learned. Kevin Lang (pers. comm.)
and Watrous
& Shastri have reported
some promising
initial experiments
of this approach.
8.1. Unsupervised
We suspect that back-propagation
best when it is preceded by a pre-coding
stage that uses unsupervised
connectionist
learning procedures
to reduce the bandwidth
of the data or to make important
features more explicit. There is a great deal of structure
in speech and it is far from clear that the best way to find this structure
is by attempting
to predict phoneme labels or other similar categories.
It is much easier to provide
training sets for an unsupervised
that simply looks for higher-order
statistical
in the input,
the data does not need to be labeled.
unsupervised
it may be possible to discover categories
like phonemes describe one such learning
procedure.
& Saramaki
(I 984) describe an interesting
application
unsupervised
procedure,
competitive
to the task of pre-coding.
It is also possible
to use a form of back-propagation
for unsupervised
structuring
the task as an “encoder”
problem . If the
output of a multilayer
is required to be the same as the input, the middle layers
to encode sufficient
information
to be able to
reconstruct
it as the output.
So the central
layer must form
an invertible
constricting
this layer to a small number of units, it is possible to force the network
produce a compact
invertible
code that can be used as the input to later processes.
sets ofjilters
method bears some resemblance to principle components analysis, but it works with
non-linear units and so can produce much more complex encodings, especially in a
network with many layers between the input and the output. Elman & Zipser 
have shown that this method can be used on the raw sample values of the speech wave to
produce a greatly compressed encoding that nevertheless contains enough information
to produce high-quality speech.
A promising variation of this approach is to use a set of adjacent time frames as input
and the next time frame as the required output. The activity levels of the hidden units
would then be “non-linear predictive coefficients”. It remains to be seen whether these
coefficients would be any more useful for recognition than LPC ones.
This research was supported by contract NOOOlLC86-K-00167
from the Office of Naval Research
and an R. K.
Fellowship
to David Plaut.