Department of Statistics Papers
Multidimensional Unfolding
 
Jan de Leeuw
Publication Date
2011-10-25
eScholarship.org
Powered by the California Digital Library
University of California
FROM JAN’S DESK
FROM JAN’S DESK
MULTIDIMENSIONAL UNFOLDING
JAN DE LEEUW
ABSTRACT. This is an entry for The Encyclopedia of Statistics in Behavioral Science, to be published by Wiley in 2005.
The unfolding model is a geometric model for preference and choice. It
locates individuals and alternatives as points in a joint space, and it says
that an individual will pick the alternative in the choice set closest to its ideal
point. Unfolding originated in the work of Coombs and his students. It
is perhaps the dominant model in both scaling of preferential choice and
attitude scaling.
The multidimensional unfolding technique computes solutions to the equations of unfolding model. It can be deﬁned as multidimensional scaling
of off-diagonal matrices. This means the data are dissimilarities between
n row objects and m column objects, collected in an n × m matrix 1. An
important example is preference data, where δi j indicates, for instance, how
much individual i dislikes object j. In unfolding we have many of the same
Date: April 3, 2004.
Key words and phrases. ﬁtting distances, multidimensional scaling, unfolding, choice
FROM JAN’S DESK
FROM JAN’S DESK
JAN DE LEEUW
distinctions as in general multidimensional scaling: there is unidimensional
and multidimensional unfolding, metric and nonmetric unfolding, and there
are many possible choices of loss functions that can be minimized.
First we will look at (metric) unfolding as deﬁning the system of equations
δi j = di j(X, Y), where X is the n × p conﬁguration matrix of row-points,
Y is the m × p conﬁguration matrix of column points, and
di j(X, Y) =
(xis −y js)2.
Clearly an equivalent system of algebraic equations is δ2
i j(X, Y), and
this system expands to
We can rewrite this in matrix form as 1(2) = ae′
m + enb′ −2XY ′, where a
and b contain the row and column sums of squares, and where e is used for
a vector with all elements equal to one. If we deﬁne the centering operators
Jn = In−ene′
n/n and Jm = Im−eme′
m/m, then we see that doubly centering
the matrix of squared dissimilarities gives the basic result
2 Jn1(2)Jm = ˜X ˜Y ′,
where ˜X = Jn X and ˜Y = JmY are centered versions of X and Y. For
our system of equations to be solvable, it is necessary that rank(H) ≤p.
Solving the system, or ﬁnding an approximate solution by using the singular
FROM JAN’S DESK
FROM JAN’S DESK
MULTIDIMENSIONAL UNFOLDING
value decomposition, gives us already an idea about X and Y, except that we
do not know the relative location and orientation of the two points clouds.
More precisely, if H = P Q′ is is full rank decomposition of H, then the
solutions X and Y of our system of equations δ2
i j(X, Y) can be written
in the form
X = (P + enα′)T,
Y = (Q + emβ′)(T ′)−1,
which leaves us with only the p(p + 2) unknowns in α, β, and T still to be
determined. By using the fact that the solution is invariant under translation
and rotation we can actually reduce this to 1
2 p(p + 3) parameters. One way
to ﬁnd these additional parameters is given in .
Instead of trying to ﬁnd an exact solution, if one actually exists, by algebraic
means, we can also deﬁne a multidimensional unfolding loss function and
minimize it. In the most basic and classical form, we have the Stress loss
wi j(δi j −di j(X, Y))2
This is identical to an ordinary multidimensional scaling problems where
the diagonal (row-row and column-column) weights are zero. Or, to put
it differently, in unfolding the dissimilarities between different row objects
FROM JAN’S DESK
FROM JAN’S DESK
JAN DE LEEUW
and different column objects are missing. Thus any multidimensional scaling program that can handle weights and missing data can be used to minimize this loss function. Details are in or [1, Part III]. One can also consider measuring loss using SStress, the sum of squared differences between
the squared dissimilarities and squared distances. This has been considered
in .
Social Psychology
Educational and Developmental Psychology
Clinical Psychology
Mathematical Psychology and Psychological Statistics
Experimental Psychology
Cultural Psychology and Psychology of Religion
Industrial Psychology
Test Construction and Validation
Physiological and Animal Psychology
TABLE 1. Nine Psychology Areas
We use an example from Roskam [9, p. 152]. The Department of Psychology at the University of Nijmegen has, or had, 9 different areas of
research and teaching. Each of the 39 psychologists working in the department ranked all 9 areas in order of relevance for their work. The areas
FROM JAN’S DESK
FROM JAN’S DESK
MULTIDIMENSIONAL UNFOLDING
are given in Table 1. We apply metric unfolding, in two dimensions, and
ﬁnd the solution in Figure 1.
dimension1
dimension2
FIGURE 1. Metric Unfolding Roskam Data
In this analysis we used the rank orders, more precisely the numbers 0 to
8. Thus, for good ﬁt, ﬁrst choices should coincide with ideal points. The
grouping of the 9 areas in the solution is quite natural.
In this case, and in many other cases, the problems we are analyzing suggest
that we really are interested in nonmetric unfolding. It is difﬁcult to think of
FROM JAN’S DESK
FROM JAN’S DESK
JAN DE LEEUW
actual applications of metric unfolding, except perhaps in the life and physical sciences. This does not mean that metric unfolding is uninteresting.
Most nonmetric unfolding algorithms solve metric unfolding subproblems,
and one can often make a case for metric unfolding as a robust form to solve
nonmetric unfolding problems.
The original techniques proposed by Coombs were purely nonmetric
and did not even lead to metric representations. In preference analysis,
the protypical area of application, we often only have ranking information.
Each individual ranks a number of candidates, or food samples, or investment opportunities.
The ranking information is row-conditional, which
means we cannot compare the ranks given by individual i to the ranks
given by individual k. The order is deﬁned only within rows. Metric data
are generally unconditional, because we can compare numbers both within
and between rows. Because of the paucity of information (only rank order, only row-conditional, only off-diagonal) the usual Kruskal approach to
nonmetric unfolding often leads to degenerate solutions, even after clever
renormalization and partitioning of the loss function . In Figure 2 we
give the solution minimizing
σ(X, Y, 1) =
j=1 wi j(δi j −di j(X, Y))2
j=1 wi j(δi j −δi⋆)2
FROM JAN’S DESK
FROM JAN’S DESK
MULTIDIMENSIONAL UNFOLDING
over X and Y and over those 1 whose rows are monotone with the ranks
given by the psychologists. Thus there is a separate monotone regression
computed for each of the 39 rows.
dimension1
dimension2
FIGURE 2. Nonmetric Unfolding Roskam Data
The solution is roughly the same as the metric one, but there is more clustering and clumping in the plot, and this makes the visual representation much
less clear. It is quite possible that continuing to iterate to higher precision
will lead to even more degeneracy. More recently Busing et al. have
FROM JAN’S DESK
FROM JAN’S DESK
JAN DE LEEUW
adapted the Kruskal approach to nonmetric unfolding by penalizing for the
ﬂatness of the monotone regression function.
One would expect even more problems when the data are not even rank
orders but just binary choices. Suppose n individuals have to choose one alternative from a set of m alternatives. The data can be coded as an indicator
matrix, which is an n × m binary matrix with exactly one unit element in
each row. The unfolding model says there are n points xi and m points y j in
Rp such that, if individual i picks alternative j, then ∥xi −y j∥≤∥xi −yℓ∥
for all ℓ= 1, . . . , m. More concisely, we use the m points y j to draw a
Voronoi diagram. This is illustrated in Figure 3 for six points in the plane.
FIGURE 3. A Voronoi Diagram
FROM JAN’S DESK
FROM JAN’S DESK
MULTIDIMENSIONAL UNFOLDING
There is one Voronoi cell for each the y j, and the cell (which can be bounded
on unbounded) contains exactly those points which are closer to y j than to
any of the other yℓ. The unfolding model says that individuals are in the
Voronoi cells of the objects they pick. This clearly leaves room for a lot of
indeterminacy in the actual placement of the points.
The situation becomes more favorable if we have more than one indicator
matrix, that is if each individual makes more than one choice. There is a
Voronoi diagram for each choice and individuals must be in the Voronoi
cells of the object they choose for each of the diagrams. Superimposing the
diagrams creates smaller and smaller regions that each individual must be
in, and the unfolding model requires the intersection of the Voronoi cells
determined by the choices of any individual to be nonempty.
It is perhaps simplest to apply this idea to binary choices. The Voronoi
cells in this case are half spaces deﬁned by hyperplanes dividing Rn in two
parts. All individuals choosing the ﬁrst of the two alternatives must be on
one side of the hyperplane, all others must be on the other side. There is a
hyperplane for each choice.
FROM JAN’S DESK
FROM JAN’S DESK
JAN DE LEEUW
FIGURE 4. Unfolding Binary Data
This is the nonmetric factor analysis model studied ﬁrst by Coombs and
Kao . It is illustrated in Figure 4.
The prototype here is roll call data . If 100 US senators vote on 20 issues,
then the unfolding model says that (for a representation in the plane) there
are 100 points and 20 lines, such that each issue-line separates the “aye”
and the “nay” voters for that issue. Unfolding in this case can be done by
correspondence analysis, or by maximum likelihood logit or probit techniques. We give an example, using 20 issues selected by Americans for
Democratic Action, and the 2000 US Senate.
FROM JAN’S DESK
FROM JAN’S DESK
MULTIDIMENSIONAL UNFOLDING
Roll call plot for senate
dimension 1
dimension 2
FIGURE 5. The 2000 US Senate