Vol.:(0123456789)
Machine Learning 113:4953â€“4975
 
Transforming variables toÂ central normality
JakobÂ Raymaekers1â€Š Â Â· PeterÂ J.Â Rousseeuw1
Received: 16 May 2020 / Revised: 28 November 2020 / Accepted: 12 February 2021 /
Published online: 21 March 2021
Â© The Author(s) 2021
Many real data sets contain numerical features (variables) whose distribution is far from
normal (Gaussian). Instead, their distribution is often skewed. In order to handle such data
it is customary to preprocess the variables to make them more normal. The Boxâ€“Cox and
Yeoâ€“Johnson transformations are well-known tools for this. However, the standard maximum likelihood estimator of their transformation parameter is highly sensitive to outliers,
and will often try to move outliers inward at the expense of the normality of the central
part of the data. We propose a modification of these transformations as well as an estimator of the transformation parameter that is robust to outliers, so the transformed data can
be approximately normal in the center and a few outliers may deviate from it. It compares
favorably to existing techniques in an extensive simulation study and on real data.
Keywordsâ€‚ Anomaly detectionÂ Â· Data preprocessingÂ Â· Feature transformationÂ Â· OutliersÂ Â·
Symmetrization
1â€‚ Introduction
In machine learning and statistics, some numerical data features may be very nonnormal
(nonGaussian) and asymmetric (skewed) which often complicates the next steps of the
analysis. Therefore it is customary to preprocess the data by transforming such features
in order to bring them closer to normality, after which it typically becomes easier to fit a
model or to make predictions. To be useful in practice, it must be possible to automate this
preprocessing step.
In order to transform a positive variable to give it a more normal distribution one often
resorts to a power transformation . The most often used function is
the Boxâ€“Cox (BC) power transform gíœ† studied by Box and Cox , given by
Editors: Tim Verdonck, Bart Baesens, MarÃ­a Ã“skarsdÃ³ttir and Seppe vanden Broucke.
* Peter J. Rousseeuw
 
Jakob Raymaekers
 
KU Leuven, Celestijnenlaan 200B, 3001Â Leuven, Belgium
Machine Learning 113:4953â€“4975
Here x stands for the observed feature, which is transformed to gíœ†(x) using a parameter íœ†â€Š.
A limitation of the family of BC transformations is that they are only applicable to positive
data. To remedy this, Yeo and Johnson proposed an alternative family of transformations that can deal with both positive and negative data. These Yeoâ€“Johnson (YJ) transformations híœ† are given by
and are also characterized by a parameter íœ†â€Š. FigureÂ 1 shows both of these transformations
for a range of íœ† values. In both families íœ†= 1 yields a linear relation. Transformations with
ğœ†< 1 compress the right tail of the distribution while expanding the left tail, making them
suitable for transforming right-skewed distributions towards symmetry. Similarly, transformations with ğœ†> 1 are designed to make left-skewed distributions more symmetrical.
Estimating the parameter íœ† for the BC or YJ transformation is typically done using maximum likelihood, under the assumption that the transformed variable follows a normal distribution. However, it is well known that maximum likelihood estimation is very sensitive
to outliers in the data, to the extent that a single outlier can have an arbitrarily large effect
on the estimate. In the setting of transformation to normality, outliers can yield transformations for which the bulk of the transformed data follows a very skewed distribution, so no
normality is attained at all. InÂ situations with outliers one would prefer to make the nonoutliers approximately normally distributed, while the outliers may stay outlying. So, our
goal is to achieve central normality, where the transformed data look roughly normal in
the center and a few outliers may deviate from it. Fitting such a transformation is not easy,
because a point that is outlying in the original data may not be outlying in the transformed
((1 + x)ğœ†âˆ’1)âˆ•ğœ†
if ğœ†â‰ 0 and x â‰¥0
log(1 + x)
if ğœ†= 0 and x â‰¥0
âˆ’((1 âˆ’x)2âˆ’ğœ†âˆ’1)âˆ•(2 âˆ’ğœ†)
if ğœ†â‰ 2 and x < 0
âˆ’log(1 âˆ’x)
if ğœ†= 2 and x < 0
Plot of gÎ»(x)
Plot of hÎ»(x)
Fig.â€¯1â€‚ â€‰The Boxâ€“Cox (left) and Yeoâ€“Johnson (right) transformations for several parameters íœ†
Machine Learning 113:4953â€“4975
data, and vice versa. The problem is that we do not know beforehand which points may
turn out to be outliers in the optimally transformed data.
Some proposals exist in the literature to make the estimation of the parameter íœ† in BC
more robust against outliers, mainly in the context of transforming the response variable
in a regression , but here we are not in
that setting. For the YJ transformation very few robust methods are available. In a trimmed maximum likelihood approach was explored, in which the objective is a trimmed sum of log likelihoods in which the lowest terms are discarded. We will
study this method in more detail later.
Note that both the BC and YJ transformations suffer from the complication that their
range depends on the parameter íœ†â€Š. In particular, for the BC transformation we have
whereas for the YJ transformation we have
So, for certain values of íœ† the range of the transformation is a half line. This is not without consequences. First, most well-known symmetric distributions are supported on the
entire line, so a perfect match is impossible. More importantly, we argue that this can make
outlier detection more difficult. Consider for instance the BC transformation with íœ†= âˆ’1
which has the range gâˆ’1(â„+
0 ) = (âˆ’âˆ, 1)â€Š. Suppose we transform a data set (x1, â€¦ , xn) to
(gâˆ’1(x1), â€¦ , gâˆ’1(xn))â€Š. If we let xn â†’âˆ making it an extremely clear outlier in the original
space, then gâˆ’1(xn) â†’1 in the transformed space. So a transformed outlier can be much
closer to the bulk of the transformed data than the original outlier was in the original data.
This is undesirable, since the outlier will be much harder to detect this way. This effect is
magnified if íœ† is estimated by maximum likelihood, since this estimator will try to accommodate all observations, including the outliers.
We illustrate this point using the TopGear dataset which contains information on 297 cars, scraped from the website of the British television show Top Gear.
We fit a Boxâ€“Cox transformation to the variable miles per gallon (MPG) which is
strictly positive. The left panel of Fig.Â 2 shows the normal QQ-plot of the MPG variable
before transformation. (That is, the horizontal axis contains as many quantiles from the
standard normal distribution as there are sorted data values on the vertical axis.) In this
plot the majority of the observations seem to roughly follow a normal distribution, that
is, many points in the QQ-plot lie close to a straight line. There are also three far outliers
at the top, which correspond to the Chevrolet Volt and Vauxhall Ampera (both with 235
MPG) and the BMW i3 (with 470 MPG). These cars are unusual because they derive most
of their power from a plug-in electric battery, whereas the majority of the cars in the data
set are gas-powered. The right panel of Fig.Â 2 shows the Boxâ€“Cox transformed data using
the maximum likelihood (ML) estimate Ì‚ğœ†= âˆ’0.11â€Š, indicating that the BC transformation
is fairly close to the log transform. We see that this transformation does not improve the
normality of the MPG variable. Instead it tries to bring the three outliers into the fold, at
(âˆ’1âˆ•ğœ†, âˆ)
(âˆ’âˆ, 1âˆ•ğœ†)
(âˆ’1âˆ•ğœ†âˆ’2, âˆ)
(âˆ’âˆ, 1âˆ•ğœ†)
Machine Learning 113:4953â€“4975
the expense of causing skewness in the central part of the transformed data and creating an
artificial outlier at the bottom.
The variable Weight shown in Fig.Â 3 illustrates a different effect. The original variable has one extreme and 4 intermediate outliers at the bottom. The extreme outlier is the
Peugeot 107, whose weight was erroneously listed as 210Â kg, and the next outlier is the tiny
Renault Twizy (410Â kg). Unlike the MPG variable the central part of these data is not very
normal, as those points in the QQ-plot do not line up so well. A transform that would make
the central part more straight would expose the outliers at the bottom more. But instead the
ML estimate is Ì‚ğœ†= 0.83 hence close to íœ†= 1 which would correspond to not transforming
Original MPG data
Theoretical Quantiles
Sample Quantiles
Boxâˆ’Cox transformed MPG using ML
Theoretical Quantiles
Sample Quantiles
Fig.â€¯2â€‚ â€‰Normal QQ-plot of the variable MPG in the Top Gear dataset (left) and the Boxâ€“Cox transformed
variable using the maximum likelihood estimate of íœ† (right). The ML estimate is heavily affected by the
three outliers at the top, causing it to create skewness in the central part of the transformed data
Original Weight data
Theoretical Quantiles
Sample Quantiles
Boxâˆ’Cox transformed Weight using ML
Theoretical Quantiles
Sample Quantiles
Fig.â€¯3â€‚ â€‰Normal QQ-plot of the variable Weight in the Top Gear dataset (left) and the transformed variable
using the ML estimate of íœ† (right). The transformation does not make the five outliers at the bottom stand
Machine Learning 113:4953â€“4975
the variable at all. Whereas the MPG variable should not be transformed much but is, the
Weight variable should be transformed but almost isnâ€™t.
In Sect.Â 2 we propose a new robust estimator for the parameter íœ†â€Š, and compare its sensitivity curve to those of other methods. SectionÂ 3 presents a simulation to study the performance of several estimators on clean and contaminated data. SectionÂ 4 illustrates the
proposed method on real data examples, and Sect.Â 6 concludes.
2â€‚ Methodology
2.1â€‚ Fitting aÂ transformation byÂ minimizing aÂ robust criterion
The most popular way of estimating the íœ† of the BC and YJ transformations is to use maximum likelihood (ML) under the assumption that the transformed variable follows a normal
distribution, as will briefly be summarized in Sect.Â 2.3. However, it is well known that ML
estimation is very sensitive to outliers in the data and other deviations from the assumed
model. We therefore propose a different way of estimating the transformation parameter of
a transformation.
Consider an ordered sample of univariate observations X = (x(1), â€¦ , x(n)) . Suppose we
want to estimate the parameter íœ† of a nonlinear function gíœ† such that gíœ†(x(1)), â€¦ , gíœ†(x(n))
come close to quantiles from the standard normal cumulative distribution function í›·â€Š. We
propose to estimate íœ† as:
Here Ì‚ğœ‡M is the Huber M-estimate of location of the gíœ†(x(i))â€Š, and Ì‚ğœM is their Huber M-estimate of scale. Both are standard robust univariate estimators . The
pi = (i âˆ’1âˆ•3)âˆ•(n + 1âˆ•3) are the usual equispaced probabilities that also yield the quantiles
in the QQ-plot ). The function íœŒ needs to be
positive, even and continuously differentiable. In least squares methods íœŒ(t) = t2â€Š, but in
our situation there can be large absolute residuals |
gğœ†(x(i))âˆ’Ì‚ğœ‡
âˆ’ğ›·âˆ’1(pi)| caused by outlying
values of gíœ†(x(i))â€Š. In order to obtain a robust method we need a bounded íœŒ function. We
propose to use the well-known Tukey bisquare íœŒ-function given by
The constant c is a tuning parameter, which we set to 0.5 by default here. See section A of
the supplementary material for a motivation of this choice.
To calculate Ì‚ğœ† numerically, we use the R function optimize() which relies on a combination of golden section search and successive parabolic interpolation to minimize the
objective of (5).
2.2â€‚ Rectified Boxâ€“Cox andÂ Yeoâ€“Johnson transformations
In this section we propose a modification of the classical BC and YJ transformations,
called the rectified BC and YJ transformations. They make a continuously differentiable
switch to a linear transformation in the tails of the BC and YJ functions. The purpose of
Ì‚ğœ†= argmin
(gğœ†(x(i)) âˆ’Ì‚ğœ‡M
1 âˆ’(1 âˆ’(xâˆ•c)2)3
if |x| > c.
Machine Learning 113:4953â€“4975
these modified transformations is to remedy two issues. First, the range of the classical BC
and YJ transformations depends on íœ† and is often only a half line. And second, as argued in
the introduction, the classical transformations often push outliers closer to the majority of
the data, which makes the outliers harder to detect. Instead the range of the proposed modified transformations is always the entire real line, and it becomes less likely that outliers
are masked by the transformation.
For ğœ†< 1â€Š, the BC transformation is designed to make right-skewed distributions more
symmetrical, and is bounded from above. In this case we define the rectified BC transformation as follows. Consider an upper constant Cu > 1â€Š. The rectified BC transformation ÌŠgğœ†
is defined as
Similarly, for ğœ†> 1 and a positive lower constant Cí“< 1 we put
For the YJ transformation we construct rectified counterparts in a similar fashion. For
ğœ†< 1 and a value Cu > 0 we define the rectified YJ transformation ÌŠhğœ†(x) as in (7) with gíœ†
replaced by híœ†:
Analogously, for ğœ†> 1 and Cí“< 0 we define ÌŠhğœ†(x) as in (8):
FigureÂ 4 shows such rectified BC and YJ transformations.
What are good choices of Cí“ and Cuâ€Š? Since the original data is often asymmetric, we
cannot just use a center (like the median) plus or minus a fixed number of (robust) standard deviations. Instead we set Cí“ equal to the first quartile of the original data, and for Cu
we take the third quartile. Other choices could be used, but more extreme quantiles would
yield a higher sensitivity to outliers.
2.3â€‚ Reweighted maximum likelihood
We now describe a reweighting scheme to increase the accuracy of the estimated Ì‚ğœ† while
preserving its robustness. For a data set x1, â€¦ , xn the classical maximum likelihood estimator for the Yeoâ€“Johnson transformation parameter íœ† is given by the íœ† which maximizes the
normal loglikelihood. After removing constant terms this can be written as:
ML,ğœ† is the maximum likelihood scale of the transformed data given by
gğœ†(Cu) + (x âˆ’Cu)g
if x > Cu.
{ gğœ†(Cí“) + (x âˆ’Cí“)g
hğœ†(Cu) + (x âˆ’Cu)h
if x > Cu.
{ hğœ†(Cí“) + (x âˆ’Cí“)h
ML = argmax
ML,ğœ†) + (ğœ†âˆ’1) sign (xi) log(|xi| + 1)
Machine Learning 113:4953â€“4975
The last term in (11) comes from the derivative of the YJ transformation. Criterion
(11) is sensitive to outliers since it depends on a classical variance and the unbounded
term log(1 + |xi|)â€Š. This can be remedied by using weights. Given a set of weights
W = (w1, â€¦ , wn) we define a weighted maximum likelihood (WML) estimator by
W,ğœ† now denotes the weighted variance of the transformed data:
If the weights appropriately downweight the outliers in the data, the WML criterion yields
a more robust estimate of the transformation parameter.
For the BC transform the reasoning is analogous, the only change being the final term that
comes from the derivative of the BC transform. This yields
In general, finding robust data weights is not an easy task. The problem is that the observed
data X = (x1, â€¦ , xn) can have a (very) skewed distribution and there is no straightforward
way to know which points will be outliers in the transformed data when íœ† is unknown. But
suppose that we have a rough initial estimate íœ†0 of íœ†â€Š. We can then transform the data with
íœ†0 yielding híœ†0(X) = (híœ†0(x1), â€¦ , híœ†0(xn))â€Š, which should be a lot more symmetric than the
(hğœ†(xi) âˆ’Ì‚ğœ‡ML,ğœ†)2
WML = argmax
W,ğœ†) + (ğœ†âˆ’1) sign (xi) log(1 + |xi|)
i=1 wi(hğœ†(xi) âˆ’Ì‚ğœ‡W,ğœ†)2
i=1 wi hğœ†(xi)
WML = argmax
W,ğœ†) + (ğœ†âˆ’1) log(xi)
Plot of gÂ°Î»(x)
Plot of hÂ°
Fig.â€¯4â€‚ â€‰The rectified Boxâ€“Cox (left) and Yeoâ€“Johnson (right) transformations for a range of parameters íœ†â€Š.
They look quite similar to the original transformations in Fig.Â 1 but contract less on the right when ğœ†< 1â€Š,
and contract less on the left when ğœ†> 1
Machine Learning 113:4953â€“4975
original data. We can therefore compute weights on híœ†0(X) using a classical weight function. Here we will use the â€œhard rejection ruleâ€ given by
with Ì‚ğœ‡ and Ì‚ğœ as in (5). With these weights we can compute a reweighted estimate Ì‚ğœ†1 by the
WML estimator in (13). Of course, the robustness of the reweighted estimator will depend
strongly on the robustness of the initial estimate íœ†0.
Note that the reweighting step can be iterated, yielding a multistep weighted ML estimator. In simulation studies we found that more than 2 reweighting steps provided no further
improvement in terms of accuracy (these results are not shown for brevity). We will always
use two reweighting steps from here onward.
2.4â€‚ The proposed method
Combining the above ideas, our proposed reweighted maximum likelihood (RewML)
method consists of the following steps:
â€¢ Step 1 Compute the initial estimate íœ†0 by maximizing the robust criterionÂ (5). When
fitting a Boxâ€“Cox transformation, plug in the rectified function ÌŠgğœ†â€Š. When fitting a Yeoâ€“
Johnson transformation, use the rectified function ÌŠhğœ†â€Š. Note that the rectified transforms
are only used in this first step.
â€¢ Step 2 Using íœ†0 as starting value, compute the reweighted ML estimate from (15) when
fitting the unrectified Boxâ€“Cox transform gíœ†â€Š, and from (13) when fitting the unrectified
Yeoâ€“Johnson transform híœ†.
â€¢ Step 3 Repeat step 2 once and stop.
2.5â€‚ Other estimators of
We will compare our proposal with two existing robust methods.
The first is the robustified ML estimator proposed by Carroll . The idea was to
replace the variance Ì‚ğœ2
ML,ğœ† in the ML formula (11) by a robust variance estimate of the
transformed data. Carrollâ€™s method was proposed for the BC transformation, but the idea
can be extended naturally to the estimation of the parameter of the YJ transformation. The
estimator is then given by
where Ì‚ğœM,ğœ† denotes the usual Huber M-estimate of scale of the transformed
data set (híœ†(x1), â€¦ , híœ†(xn)).
The second method is the maximum trimmed likelihood (MTL) estimator of VanÂ der
Veeken . Given a data set of size n, and a fixed number h that has to satisfy
2âŒ‰< h < nâ€Š, this method looks for the parameter Ì‚ğœ† which produces a subset of h consecutive observations which maximize the ML criterion (11).
if |hğœ†0(xi) âˆ’Ì‚ğœ‡| â©½ğ›·âˆ’1(0.995) Ì‚ğœ
if |hğœ†0(xi) âˆ’Ì‚ğœ‡| > ğ›·âˆ’1(0.995) Ì‚ğœ
Ì‚ğœ†Carroll = argmax
M,ğœ†) + (ğœ†âˆ’1) sign (xi) log(1 + |xi|)
Machine Learning 113:4953â€“4975
2.6â€‚ Sensitivity curves
In order to assess robustness against an outlier, stylized sensitivity curves were introduced on
page 96 of Andrews etÂ al. . For a given estimator T and a cumulative distribution function F they are constructed as follows:
1. Generate a stylized pseudo data set X0 of size n âˆ’1â€Š:
where the pi for i = 1, â€¦ , n âˆ’1 are equispaced probabilities that are symmetric about
1/2. We can for instance use pi = iâˆ•n.
2. Add to this stylized data set a variable point z to obtain
3. Calculate the sensitivity curve in z by
where z ranges over a grid chosen by the user. The purpose of the factor n is to put
sensitivity curves with different values of n on a similar scale.
The top panel of Fig.Â 5 shows the sensitivity curves for several estimators of the parameter íœ†
of the YJ transformation. We chose F = í›· so the true transformation parameter íœ† is 1, and
n = 100â€Š. The maximum likelihood estimator ML of (11) has an unbounded sensitivity curve,
which is undesirable as it means that a single outlier can move Ì‚ğœ† arbitrarily far away. The estimator of Carroll (17) has the same property, but is less affected in the sense that for a high |z|
the value of | SCn(z)| is smaller than for the ML estimator. The RewML estimator that we proposed in Sect.Â 2.4 has a sensitivity curve that lies close to that of the ML in the central region
of z, and becomes exactly zero for more extreme values of |z|. Such a sensitivity curve is called
redescending, meaning that it goes back to zero. Therefore a far outlier has little effect on
the resulting estimate. We also show MTL95, the trimmed likelihood estimator described in
Sect.Â 2.5 with hâˆ•n = 95%â€Š. Its sensitivity curve is also redescending, but in the central region it
is more erratic with several jumps.
The lower panel of Fig.Â 5 shows the sensitivity curves for the Boxâ€“Cox transformation
when the true parameter is íœ†= 0â€Š, i.e. the clean data follows a lognormal distribution F. We
now put log(z) on the horizontal axis, since this makes the plot more comparable to that for
Yeoâ€“Johnson in the top panel. Also here the ML and Carrollâ€™s estimator have an unbounded
sensitivity curve. Our RewML estimator has a redescending SC which again behaves similarly
to the classical ML for small | log(z)|â€Š, whereas the sensitivity to an extreme outlier is zero. The
maximal trimmed likelihood estimator MTL95 has large jumps reaching values over 40 in the
central region. Those peaks are not shown because the other curves would be hard to distinguish on that scale.
X0 = (x1, â€¦ , xnâˆ’1) =
Fâˆ’1(p1), â€¦ , Fâˆ’1(pnâˆ’1)
Xz = (x1, â€¦ , xnâˆ’1, z).
SCn(z) âˆ¶= n
T(Xz) âˆ’T(X0)
Machine Learning 113:4953â€“4975
3â€‚ Simulation
3.1â€‚ Compared methods
For the Boxâ€“Cox as well as the Yeoâ€“Johnson transformations we perform a simulation
study to compare the performance of several methods, including our proposal. The estimators under consideration are:
1. ML the classical maximum likelihood estimator given by (11), or by (15) with all wi = 1.
2. Carroll the robustified maximum likelihood estimator of Carroll given by (17).
3. MTL the maximum trimmed likelihood estimator of VanÂ der Veeken . The notation MTL90 stands for the version with hâˆ•n = 90%.
4. RewML the proposed reweighted maximum likelihood estimator described in Sect.Â 2.4.
Fig.â€¯5â€‚ â€‰Sensitivity curves of estimators of the parameter íœ† in the
Yeoâ€“Johnson (top) and Boxâ€“Cox
(bottom) transformations, with
sample size n = 100
Machine Learning 113:4953â€“4975
5. RewMLnr a variation on RewML in which the first step of Sect.Â 2.4 applies (5) to the
original Boxâ€“Cox or Yeoâ€“Johnson transform instead of their rectified versions. This is
not intended as a proposal, but included in order to show the advantage of rectification.
3.2â€‚ Data generation
We generate clean data sets as well as data with a fraction íœ€ of outliers. The clean data
are produced by generating a sample of size n from the standard normal distribution, after
which the inverse of the BC or YJ transformation with a given íœ† is applied. For contaminated data we replace a percentage íœ€ of the standard normal data by outliers at a fixed position before the inverse transformation is applied. For each such combination of íœ† and íœ€ we
generate m = 100 data sets.
To be more precise, the percentage íœ€ of contaminated points takes on the values 0, 0.05,
0.1, and 0.15, where íœ€= 0 corresponds to uncontaminated data. For the YJ transformation
we take the true transformation parameter íœ† equal to 0.5, 1.0, or 1.5. We chose these values
because for íœ† between 0 and 2 the range of YJ given by (4) is the entire real line, so the
inverse of YJ is defined for all real numbers. For the BC transformation we take íœ†= 0 for
which the range (3) is also the real line. For a given combination of íœ€ and íœ† the steps of the
data generation are:
1. Generate a sample Y = (y1, â€¦ , yn) from the standard normal distribution. Let k > 0 be
a positive parameter. Then replace a fraction íœ€ of the points in Y by k itself when íœ†â©½1â€Š,
and by âˆ’k when ğœ†> 1.
2. Apply the inverse BC transformation to Y, yielding the data set X given
byX = (gâˆ’1
íœ†(y1), â€¦ , gâˆ’1
íœ†(yn))â€Š. For YJ we put X = (hâˆ’1
íœ†(y1), â€¦ , hâˆ’1
3. Estimate íœ† from X using the methods described in Sect.Â 3.1.
The parameter k characterizing the position of the contamination is an integer that we let
range from 0 to 10.
We then estimate the bias and mean squared error (MSE) of each method by
where j = 1, â€¦ , m ranges over the generated data sets.
3.3â€‚ Results forÂ theÂ Yeoâ€“Johnson transformation
We first consider the effect of an increasing percentage of contamination on the different estimators. In this setting we fix the position of the contamination by setting
k = 10â€Š. (The results for k = 6 are qualitatively similar, as can be seen in section B of
the supplementary material.) FigureÂ 6 shows the bias and MSE of the estimators for
an increasing contamination percentage íœ€ on the horizontal axis. The results in the top
row are for data generated with íœ†= 0.5â€Š, whereas the middle row was generated with
íœ†= 1 and the bottom row with íœ†= 1.5â€Š. In all rows the classical ML and the Carroll
estimator have the largest bias and MSE, meaning they react strongly to far outliers,
as suggested by their unbounded sensitivity curves in Fig.Â 5. In contrast with this both
bias âˆ¶= ave n
j=1( Ì‚ğœ†j âˆ’ğœ†)
MSE âˆ¶= ave n
j=1( Ì‚ğœ†j âˆ’ğœ†)2
Machine Learning 113:4953â€“4975
Fig.â€¯6â€‚ â€‰Bias (left) and MSE (right) of the estimated Ì‚ğœ† of the Yeoâ€“Johnson transformation as a function of the
percentage íœ€ of outliers, when the location of the outliers is determined by setting k = 10â€Š. The true parameter íœ† used to generate the data is 0.5 in the top row, 1.0 in the middle row, and 1.5 in the bottom row
Machine Learning 113:4953â€“4975
RewML and RewMLnr perform much better as their bias and MSE are closer to zero.
Up to about 5% of outliers their curves are almost indistinguishable, but beyond that
RewML outperforms RewMLnr by a widening margin. This indicates that using the
rectified YJ transform in the first step of the estimator (see Sect.Â 2.4) is more robust
than using the plain YJ in that step, even though the goal of the entire 3-step procedure
RewML is to estimate the íœ† of the plain YJ transform.
In the same Fig.Â 6 we see the behavior of the maximum trimmed likelihood estimators
MTL95, MTL90 and MTL85. In the middle row íœ† is 1, and we see that MTL95, which fits
95% of the data, performs well when there are up to 5% of outliers and performs poorly
when there are over 5% of outliers. Analogously MTL90 performs well as long as there
are at most 10% of outliers, and so on. This is the intended behavior. But note that for íœ†â‰ 1
these estimators also have a substantial bias when the fraction of outliers is below what
they aim for, as can be seen in the top and bottom panels of Fig.Â 6. For instance MTL85
is biased when íœ€ is under 15%â€Š, even for íœ€= 0% when there are no outliers at all. So overall the MTL estimators only performed well when the percentage of trimming was equal
to 1 minus the percentage of outliers in the data. Since the true percentage of outliers is
almost never known in advance, it is not recommended to use the MTL method for variable
transformation.
Let us now investigate what happens if we keep the percentage of outliers fixed, say at
íœ€= 10%â€Š, but vary the position of the contamination by letting k = 0, 1, â€¦ , 10â€Š. FigureÂ 7
shows the resulting bias and MSE, with again íœ†= 0.5 in the top row, íœ†= 1 in the middle row, and íœ†= 1.5 in the bottom row. For k = 0 and k = 1 the ML, Carroll, RewML and
RewMLnr methods give similar results, since the contamination is close to the center so it
cannot be considered outlying. But as k increases the classical ML and the Carroll estimator become heavily affected by the outliers. On the other hand RewML and RewMLnr perform much better, and again RewML outperforms RewMLnr. Note that the bias of RewML
moves toward zero when k is large enough. We already noted this redescending behavior in
its sensitivity curve in Fig.Â 5.
The behavior of the maximum trimmed likelihood methods depends on the value of íœ†
used to generate the data. First focus on the middle row of Fig.Â 7 where íœ†= 1 so the clean
data is generated from the standard normal distribution. In that situation both MTL90 and
MTL85 behave well, whereas MTL95 can only withstand 5% of outliers and not the 10% generated here. One might expect the MTL method to work well as long as its h excludes at least
the number of outliers in the data. But in fact MTL85 does not behave so well when íœ† differs
from 1, as seen in the top and bottom panels of Fig.Â 7, where the bias remains substantial
even though the method expects 15% of outliers and there are only 10% of them. As in Fig.Â 6
this suggests that one needs to know the actual percentage of outliers in the data in order to
select the appropriate h for the MTL method, but that percentage is typically unknown.
3.4â€‚ Results forÂ theÂ Boxâ€“Cox transformation
When simulating data to apply the Boxâ€“Cox transformation to, the most natural choice of
íœ† is zero since this is the only value for which the range of BC is the entire real line. Therefore we can carry out the inverse BC transformation on any data set generated from a normal distribution, so the clean data follows a log-normal distribution. The top panel of Fig.Â 8
shows the bias and MSE for 10% of outliers with k = 1, â€¦ , 10â€Š. We see that the classical
ML and the estimator of Carroll are sensitive to outliers when k grows. Our reweighted
Machine Learning 113:4953â€“4975
method RewML performs much better. The RewMLnr method only differs from RewML
in that it uses the non-rectified BC transform in the first step, and does not do as well since
its bias goes back to zero at a slower rate.
Fig.â€¯7â€‚ â€‰Bias (left) and MSE (right) of the estimated Ì‚ğœ† of the Yeoâ€“Johnson transformation as a function of k
which determines how far the outliers are. Here the percentage of outliers is fixed at 10%â€Š. The true parameter íœ† used to generate the data is 0.5 in the top row, 1.0 in the middle row, and 1.5 in the bottom row
Machine Learning 113:4953â€“4975
The MTL estimators perform poorly here. The MTL95 version trims only 5% of the
data points so it cannot discard the 10% of outliers, leading to a behavior resembling that
of the classical ML. But also MTL90 and MTL85, which trim enough data points, obtain a
large bias which goes in the opposite direction, accompanied by a high MSE. The MSE of
MTL90 and MTL85 lie entirely above the plot area.
Finally, we consider a scenario with íœ†= 1â€Š. In that case the range of the Boxâ€“Cox transformation given by (3) is only (âˆ’1, +âˆ) so the transformed data cannot be normally distributed (which is already an issue for the justification of the classical maximum likelihood
method). But the transformed data can have a truncated normal distribution. In this special
setting we generated data from the normal distribution with mean 1 and standard deviation
1/3, and then truncated it to [0.01,Â 1.99] (keeping n = 100 points), so the clean data are
strictly positive and have a symmetric distribution around 1. In the bottom panel of Fig.Â 8
we see that the ML and Carroll estimators are not robust in this simulation setting. The
trimmed likelihood estimators also fail to deliver reasonable results, with curves that often
fall outside the plot area. On the other hand RewML still performs well, and again does
better than RewMLnr.
Fig.â€¯8â€‚ â€‰Bias (left) and MSE (right) of the estimated Ì‚ğœ† of the Boxâ€“Cox transformation as a function of k
which determines how far the outliers are. Here the percentage of outliers is fixed at 10%â€Š. The true íœ† used to
generate the data is 0 in the top row and 1 in the bottom row
Machine Learning 113:4953â€“4975
The simulation results for a fixed outlier position at k = 6 or k = 10 with contamination levels íœ€ from 0% to 15% can be found in section B of the supplementary material,
and are qualitatively similar to those for the YJ transform.
4â€‚ Empirical examples
4.1â€‚ Car data
Let us revisit the positive variable MPG from the TopGear data shown in the left panel
of Fig.Â 2. The majority of these data are already roughly normal, and three far outliers
at the top deviate from this pattern. Before applying a Boxâ€“Cox transformation we first
scale the variable so its median becomes 1. This makes the result invariant to the unit of
measurement, whether it is miles per gallon or, say, kilometers per liter. The maximum
likelihood estimator for Boxâ€“Cox tries to bring in the outliers and yields Ì‚ğœ†= âˆ’0.11â€Š,
which is close to íœ†= 0 corresponding to a logarithmic transformation. The resulting
transformed data in the left panel of Fig.Â 9 are quite skewed in the central part, so not
normal at all, which defeats the purpose of the transformation. This result is in sharp
contrast with our reweighted maximum likelihood (RewML) method which estimates
the transformation parameter as Ì‚ğœ†= 0.84â€Š. The resulting transformed data in the right
panel does achieve central normality.
The variable Weight in the left panel of Fig.Â 3 is not very normal in its center and
has some outliers at the bottom. The classical ML estimate is Ì‚ğœ†= 0.83â€Š, close to íœ†= 1
which would not transform the data at all, as we can see in the resulting left panel of
Fig.Â 10. In contrast, our RewML estimator obtains Ì‚ğœ†= 0.09 which substantially transforms the data, yielding the right panel of Fig.Â 10. There the central part of the data is
very close to normal, and the outliers at the bottom now stand out more, as they should.
Boxâˆ’Cox transformed MPG using ML
Theoretical Quantiles
Sample Quantiles
Boxâˆ’Cox transformed MPG using RewML
Theoretical Quantiles
Sample Quantiles
Fig.â€¯9â€‚ â€‰Normal QQ-plot of the Boxâ€“Cox transformed variable MPG using the ML estimate of íœ† (left) and
using the RewML estimate (right). The ML is strongly affected by the 3 outliers at the top, thereby transforming the central data away from normality. The RewML method achieves central normality and makes
the outliers stand out more
Machine Learning 113:4953â€“4975
4.2â€‚ Glass data
For a multivariate example we turn to the glass data from chemometrics, which has become something of
a benchmark. The data consists of n = 180 archeological glass samples, which were analyzed by spectroscopy. Our variables are the intensities measured at 500 wavelengths.
Many of these variables do not look normally distributed.
We first applied a Yeoâ€“Johnson transformation to each variable with Ì‚ğœ† obtained from
the nonrobust ML method of (11). For each variable we then standardized the transformed data h Ì‚ğœ†(xi) to (h Ì‚ğœ†(xi) âˆ’Ì‚ğœ‡ML, Ì‚ğœ†)âˆ•Ì‚ğœML, Ì‚ğœ† where Ì‚ğœ‡ML, Ì‚ğœ† and Ì‚ğœML, Ì‚ğœ† are given by (12).
This yields a standardized transformed data set with again 180 rows and 500 columns. In order to detect outliers in this matrix we compare each value to the interval
[âˆ’2.57, 2.57] which has a probability of exactly 99% for standard normal data. The top
panel of Fig.Â 11 is a heatmap of the standardized transformed data matrix where each
value within [âˆ’2.57, 2.57] is shown as yellow, values above 2.57 are red, and values
below âˆ’2.57 are blue. This heatmap is predominantly yellow because the ML method
tends to transform the data in a way that masks outliers, so not much structure is visible.
Next, we transformed each variable by Yeoâ€“Johnson with Ì‚ğœ† obtained by the
robust RewML method. The transformed variables were standardized accordingly to
(h Ì‚ğœ†(xi) âˆ’Ì‚ğœ‡W, Ì‚ğœ†)âˆ•Ì‚ğœW, Ì‚ğœ† where Ì‚ğœ‡W, Ì‚ğœ† and Ì‚ğœW, Ì‚ğœ† are given by (14) using the final weights in
(13). The resulting heatmap is in the bottom panel of Fig.Â 11. Here we see much more
structure, with red regions corresponding to glass samples with unusually high spectral
intensities at certain wavelengths. This is because the RewML method aims to make
the central part of each variable as normal as possible, which allows outliers to deviate from that central region. The resulting heatmap has a subject-matter interpretation
since wavelengths correspond to chemical elements. It indicates that some of the glass
samples (with row numbers between 22 and 30) have a higher concentration of phosphor, whereas rows 57â€“63 and 74â€“76 had an unusually high amount of calcium. The
Boxâˆ’Cox transformed Weight using ML
Theoretical Quantiles
Sample Quantiles
Boxâˆ’Cox transformed Weight using RewML
Theoretical Quantiles
Sample Quantiles
Fig.â€¯10â€‚ â€‰Normal QQ-plot of the Boxâ€“Cox transformed variable Weight using the ML estimate of íœ† (left)
and using the RewML estimate (right). The ML masks the five outliers at the bottom, whereas RewML
accentuates them and achieves central normality
Machine Learning 113:4953â€“4975
red zones in the bottom part of the heatmap were caused by the fact that the measuring
instrument was cleaned before recording the last 38 spectra.
4.3â€‚ DPOSS data
As a final example we consider data from the Digitized Palomar Sky Survey (DPOSS)
described by Djorgovski etÂ al. . We work with the dataset of 20000 stars available
as dposs in the R package cellWise . The data are measurements in three color bands, but for the purpose of illustration we restrict attention to
the color band with the fewest missing values. Selecting the completely observed rows
then yields a dataset of 11478 observations with 7 variables. Variables MAper, MTot and
MCore measure light intensity, and variables Area, IR2, csf and Ellip are measurements of
the size and shape of the images.
In order to analyze the data we first apply the YJ-transformation to each variable, with
the Ì‚ğœ† estimates obtained from RewML. We then perform cellwise robust PCA . We retained k = 4 components, explaining 97% of the variance. FigureÂ 12 is
the pairs plot of the robust scores. We clearly see a large cluster of regular points with
some outliers around it, and a smaller cluster of rather extreme outliers in red. The red
points correspond to the stars with a high value of MAper. The blue points are somewhat in
wavelengths
YJ transformed variables by ML
wavelengths
YJ transformed variables by RewML
Fig.â€¯11â€‚ â€‰Heatmap of the glass data after transforming each variable (column) by a Yeoâ€“Johnson transform
with parameter íœ† estimated by (top) the maximum likelihood method, and (bottom) the reweighted maximum likelihood method RewML. Yellow cells correspond to values in the central 99% range of the normal
distribution. Red cells indicate unusually high values, and blue cells have unusually low values (Color figure online)
Machine Learning 113:4953â€“4975
between the main cluster and the smaller cluster of extreme outliers. The two orange points
are celestial objects with an extreme value of Ellip.
The left panel of Fig.Â 13 contains the outlier map of this robust
PCA. Such an outlier map consists of two ingredients. The horizontal axis contains the
score distance of each object í±iâ€Š. This is the robust Mahalanobis distance of the orthogonal
projection of í±i on the subspace spanned by the k principal components, and can be computed as
where tij are the PCA scores and íœ†j is the j-th largest eigenvalue. The vertical axis shows
the orthogonal distance ODi of each point í±iâ€Š, which is the distance between í±i and its
projection on the k-dimensional subspace. We see that the red points form a large cluster
with extreme SDi as well as ODiâ€Š. The blue points are intermediate, and the two orange
points are still unusual but less extreme. Interestingly, when applying robust PCA to the
Fig.â€¯12â€‚ â€‰DPOSS data: pairs plot of the robust PCA scores after transforming the data with the robustly fitted
YJ-transformation
Machine Learning 113:4953â€“4975
untransformed data we obtain the right panel of Fig.Â 13 which shows a very different picture in which the red, blue and orange points are not clearly distinguished from the remainder. This outlier map appears to indicate a number of black points as most outlying, but in
fact they should not be considered outlying since they are characterized by high values of
Area and IR2 which are very right-skewed. When transforming these variables to central
normality, as in the left panel, the values that appeared to be outlying become part of the
regular tails of a symmetric distribution.
5â€‚ Discussion
In this discussion we address and clarify a few aspects of (robust) variable transformation.
Outliers in skewed data The question of what is an outlier in a general skewed distribution is a rather difficult one. One could flag the values below a lower cutoff or above an
upper cutoff, for instance given by quantiles of a distribution. But it is hard to robustly
determine cutoffs for a general skewed dataset or distribution. Our viewpoint is this. If
the data can be robustly transformed by BC or YJ towards a distribution that looks normal
except in the tails, then it easy to obtain cutoffs for the transformed data, e.g. by taking
their median plus or minus two median absolute deviations. And then, since the BC and
YJ transforms are monotone, these cutoffs can be transformed back to the original skewed
data or distribution. In this way we do not flag too many points. We have illustrated this for
lognormal data in section C of the supplementary material.
How many outliers can we deal with? In addition to sensitivity curves, which characterize the effect of a single outlier, it is interesting to find out how many adversely placed
outliers it takes to make the estimators fail badly. For RewML this turns out to be around
15% of the sample size, which is quite low compared to robust estimators of location or
scale. But this is unavoidable, as asymmetry is tied to the tails of the distribution. Empirically, skewness does not manifest itself much in the central part of the distribution, say
20% of mass above and below the median. On each side we have 30% of mass left, and if
the adversary is allowed to replace half of either portion (that is, 15% of the total mass) and
move it anywhere, they can modify the skewness a great deal.
Score distance
Orthogonal distance
Score distance
Orthogonal distance
Fig.â€¯13â€‚ â€‰DPOSS data: outlier map of robust PCA applied to the YJ-transformed variables (left), and applied
to the raw variables (right). The colors are those of Fig.Â 12 (Color figure online)
Machine Learning 113:4953â€“4975
Prestandardization In practice, one would typically apply some form of prestandardization to the data before trying to fit a transformation. For the YJ transformation, we can
simply prestandardize the dataset X = {x1, â€¦ , xn} by
where mad is the median absolute deviation from the median. This is what we did in the
glass and DPOSS examples. An advantage of this prestandardization is that the relevant
íœ† will typically be in the same range (say, from âˆ’4 to 6) for all variables in the data set,
which is convenient for the computation of Ì‚ğœ†â€Š. Before the BC transformation we cannot prestandardize by (19) since this would generate negative values. One option is to divide the
original xi by their median so they remain positive and have median 1. But if the resulting
data is tightly concentrated around 1, we may still require a huge positive or negative íœ† to
properly transform them by BC. Alternatively, we propose to prestandardize by
which performs a standardization on the log scale and then transforms back to the positive
scale. This again has the advantage that the range of íœ† can be kept fixed when computing Ì‚ğœ†â€Š,
making it as easy as applying the YJ transform after (19). A disadvantage is that the transformation parameter becomes harder to interpret, since e.g. a value of íœ†= 1 no longer corresponds to a linear transformation, but íœ†= 0 still corresponds to a logarithmic transform.
Tuning constants The proposed method has some tuning parameters, namely the constant c in (6), the cutoff í›·âˆ’1(0.995) in the reweighting step, and the rectification points.
The tuning of the constant c is addressed in section A of the supplementary material.
The second parameter is the constant í›·âˆ’1(0.995) â‰ˆ2.57 in the weights (16). This weight
function is commonly used in existing reweighting techniques in robust statistics. If the
transformed data is normally distributed we flag approximately 1% of values with this
cutoff, since P(|Z| > ğ›·âˆ’1(0.995)) = 0.01 if Z âˆ¼N(0, 1)â€Š. Choosing a higher cutoff results
in a higher efficiency, but at the cost of lower robustness. Generally, the estimates do not
depend too much on the choice of this cutoff as long as it is in a reasonably high range, say
from í›·âˆ’1(0.975) to í›·âˆ’1(0.995)â€Š. A final choice in the proposal are the â€œrectification pointsâ€
Cí“ and Cu for which we take the first and third quartiles of the data. Note that the constraints Cí“< 0 < Cu for YJ and Cí“< 1 < Cu for BC in (7)â€“(10) are satisfied automatically
when prestandardizing the data as described in the previous paragraph. It is worth noting
that these data-dependent choices of Cí“ and Cu do not create abrupt changes in the transformed data when moving through the range of possible values for íœ† because the rectified
transformations are continuous in íœ† by construction. For instance, passing from ğœ†< 1 to
ğœ†> 1 does not cause a jump in the transformed data because íœ†= 1 corresponds to a linear
transformation that is inherently rectified on both sides.
Models with transformed variables The ease or difficulty of interpreting a model in
the transformed variables depends on the model under consideration. For nonparametric
models it makes little difference. For parametric models the transformations can make the
model harder to interpret in some cases and easier in others, for instance where there is a
simple linear relation in the transformed variables instead of a model with higher-order
terms in the original variables. Also, the notion of leverage point in linear regression is
more easily interpretable with roughly normal regressors, as it is related to the (robust)
Mahalanobis distance of a point in regressor space.
Ìƒxi = xi âˆ’median (X)
(log(xi) âˆ’median (log X)
mad (log X)
Machine Learning 113:4953â€“4975
The effect of the sampling variability of íœ† on the inference in the resulting model is
also model dependent. We expect that it will typically lead to a somewhat higher variability in the estimation. However, as the BY and YJ transformations are continuous and
differentiable in íœ† and not very sensitive to small changes of íœ†â€Š, the increase in variability is
likely small. Moreover, if we apply BC or YJ transformations to predictor variables used in
CART or Random Forest, the predictions and feature importance metrics stay exactly the
same because the BC and YJ transformations are monotone.
6â€‚ Conclusion
In our view, a transformation to normality should fit the central part of the data well, and
not be determined by any outliers that may be present. This is why we aim for central normality, where the transformed data is close to normal (Gaussian) with the possible exception of some outliers that can remain further out. Fitting such a transformation is not easy,
because a point that appears to be outlying in the original data may not be outlying in the
transformed data, and vice versa.
To address this problem we introduced a combination of three ideas: a highly robust
objective function (5), the rectified Boxâ€“Cox and Yeoâ€“Johnson transforms in Sect.Â  2.2
which we use in our initial estimator only, and a reweighted maximum likelihood procedure for transformations. This combination turns out to be a powerful tool for this difficult
Preprocessing real data by this tool paves the way for applying subsequent methods,
such as anomaly detection and well-established model fitting and predictive techniques.
Supplementary Informationâ€‚ The online version contains supplementary material available at ( 
org/â€‹10.â€‹1007/â€‹s10994-â€‹021-â€‹05960-5).
Acknowledgementsâ€‚ This work was supported by Grant C16/15/068 of KU Leuven, Belgium. We thank the
referees for constructive comments improving the presentation.
Data availabilityâ€‚ The proposed method is available as the function transfo() in the R package cellWise on CRAN, which also includes the vignette
transfo_examples that reproduces all the examples in this paper. It makes uses of the R-packages
gridExtra , reshape2Â  , ggplot2 and scales
 .
Open Accessâ€‚ This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long
as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article
are included in the articleâ€™s Creative Commons licence, unless indicated otherwise in a credit line to the
material. If material is not included in the articleâ€™s Creative Commons licence and your intended use is not
permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit