Vol.:(0123456789)
Machine Learning 113:4953–4975
 
Transforming variables to central normality
Jakob Raymaekers1   · Peter J. Rousseeuw1
Received: 16 May 2020 / Revised: 28 November 2020 / Accepted: 12 February 2021 /
Published online: 21 March 2021
© The Author(s) 2021
Many real data sets contain numerical features (variables) whose distribution is far from
normal (Gaussian). Instead, their distribution is often skewed. In order to handle such data
it is customary to preprocess the variables to make them more normal. The Box–Cox and
Yeo–Johnson transformations are well-known tools for this. However, the standard maximum likelihood estimator of their transformation parameter is highly sensitive to outliers,
and will often try to move outliers inward at the expense of the normality of the central
part of the data. We propose a modification of these transformations as well as an estimator of the transformation parameter that is robust to outliers, so the transformed data can
be approximately normal in the center and a few outliers may deviate from it. It compares
favorably to existing techniques in an extensive simulation study and on real data.
Keywords  Anomaly detection · Data preprocessing · Feature transformation · Outliers ·
Symmetrization
1  Introduction
In machine learning and statistics, some numerical data features may be very nonnormal
(nonGaussian) and asymmetric (skewed) which often complicates the next steps of the
analysis. Therefore it is customary to preprocess the data by transforming such features
in order to bring them closer to normality, after which it typically becomes easier to fit a
model or to make predictions. To be useful in practice, it must be possible to automate this
preprocessing step.
In order to transform a positive variable to give it a more normal distribution one often
resorts to a power transformation . The most often used function is
the Box–Cox (BC) power transform g휆 studied by Box and Cox , given by
Editors: Tim Verdonck, Bart Baesens, María Óskarsdóttir and Seppe vanden Broucke.
* Peter J. Rousseeuw
 
Jakob Raymaekers
 
KU Leuven, Celestijnenlaan 200B, 3001 Leuven, Belgium
Machine Learning 113:4953–4975
Here x stands for the observed feature, which is transformed to g휆(x) using a parameter 휆 .
A limitation of the family of BC transformations is that they are only applicable to positive
data. To remedy this, Yeo and Johnson proposed an alternative family of transformations that can deal with both positive and negative data. These Yeo–Johnson (YJ) transformations h휆 are given by
and are also characterized by a parameter 휆 . Figure 1 shows both of these transformations
for a range of 휆 values. In both families 휆= 1 yields a linear relation. Transformations with
𝜆< 1 compress the right tail of the distribution while expanding the left tail, making them
suitable for transforming right-skewed distributions towards symmetry. Similarly, transformations with 𝜆> 1 are designed to make left-skewed distributions more symmetrical.
Estimating the parameter 휆 for the BC or YJ transformation is typically done using maximum likelihood, under the assumption that the transformed variable follows a normal distribution. However, it is well known that maximum likelihood estimation is very sensitive
to outliers in the data, to the extent that a single outlier can have an arbitrarily large effect
on the estimate. In the setting of transformation to normality, outliers can yield transformations for which the bulk of the transformed data follows a very skewed distribution, so no
normality is attained at all. In situations with outliers one would prefer to make the nonoutliers approximately normally distributed, while the outliers may stay outlying. So, our
goal is to achieve central normality, where the transformed data look roughly normal in
the center and a few outliers may deviate from it. Fitting such a transformation is not easy,
because a point that is outlying in the original data may not be outlying in the transformed
((1 + x)𝜆−1)∕𝜆
if 𝜆≠0 and x ≥0
log(1 + x)
if 𝜆= 0 and x ≥0
−((1 −x)2−𝜆−1)∕(2 −𝜆)
if 𝜆≠2 and x < 0
−log(1 −x)
if 𝜆= 2 and x < 0
Plot of gλ(x)
Plot of hλ(x)
Fig. 1   The Box–Cox (left) and Yeo–Johnson (right) transformations for several parameters 휆
Machine Learning 113:4953–4975
data, and vice versa. The problem is that we do not know beforehand which points may
turn out to be outliers in the optimally transformed data.
Some proposals exist in the literature to make the estimation of the parameter 휆 in BC
more robust against outliers, mainly in the context of transforming the response variable
in a regression , but here we are not in
that setting. For the YJ transformation very few robust methods are available. In a trimmed maximum likelihood approach was explored, in which the objective is a trimmed sum of log likelihoods in which the lowest terms are discarded. We will
study this method in more detail later.
Note that both the BC and YJ transformations suffer from the complication that their
range depends on the parameter 휆 . In particular, for the BC transformation we have
whereas for the YJ transformation we have
So, for certain values of 휆 the range of the transformation is a half line. This is not without consequences. First, most well-known symmetric distributions are supported on the
entire line, so a perfect match is impossible. More importantly, we argue that this can make
outlier detection more difficult. Consider for instance the BC transformation with 휆= −1
which has the range g−1(ℝ+
0 ) = (−∞, 1) . Suppose we transform a data set (x1, … , xn) to
(g−1(x1), … , g−1(xn)) . If we let xn →∞ making it an extremely clear outlier in the original
space, then g−1(xn) →1 in the transformed space. So a transformed outlier can be much
closer to the bulk of the transformed data than the original outlier was in the original data.
This is undesirable, since the outlier will be much harder to detect this way. This effect is
magnified if 휆 is estimated by maximum likelihood, since this estimator will try to accommodate all observations, including the outliers.
We illustrate this point using the TopGear dataset which contains information on 297 cars, scraped from the website of the British television show Top Gear.
We fit a Box–Cox transformation to the variable miles per gallon (MPG) which is
strictly positive. The left panel of Fig. 2 shows the normal QQ-plot of the MPG variable
before transformation. (That is, the horizontal axis contains as many quantiles from the
standard normal distribution as there are sorted data values on the vertical axis.) In this
plot the majority of the observations seem to roughly follow a normal distribution, that
is, many points in the QQ-plot lie close to a straight line. There are also three far outliers
at the top, which correspond to the Chevrolet Volt and Vauxhall Ampera (both with 235
MPG) and the BMW i3 (with 470 MPG). These cars are unusual because they derive most
of their power from a plug-in electric battery, whereas the majority of the cars in the data
set are gas-powered. The right panel of Fig. 2 shows the Box–Cox transformed data using
the maximum likelihood (ML) estimate ̂𝜆= −0.11 , indicating that the BC transformation
is fairly close to the log transform. We see that this transformation does not improve the
normality of the MPG variable. Instead it tries to bring the three outliers into the fold, at
(−1∕𝜆, ∞)
(−∞, 1∕𝜆)
(−1∕𝜆−2, ∞)
(−∞, 1∕𝜆)
Machine Learning 113:4953–4975
the expense of causing skewness in the central part of the transformed data and creating an
artificial outlier at the bottom.
The variable Weight shown in Fig. 3 illustrates a different effect. The original variable has one extreme and 4 intermediate outliers at the bottom. The extreme outlier is the
Peugeot 107, whose weight was erroneously listed as 210 kg, and the next outlier is the tiny
Renault Twizy (410 kg). Unlike the MPG variable the central part of these data is not very
normal, as those points in the QQ-plot do not line up so well. A transform that would make
the central part more straight would expose the outliers at the bottom more. But instead the
ML estimate is ̂𝜆= 0.83 hence close to 휆= 1 which would correspond to not transforming
Original MPG data
Theoretical Quantiles
Sample Quantiles
Box−Cox transformed MPG using ML
Theoretical Quantiles
Sample Quantiles
Fig. 2   Normal QQ-plot of the variable MPG in the Top Gear dataset (left) and the Box–Cox transformed
variable using the maximum likelihood estimate of 휆 (right). The ML estimate is heavily affected by the
three outliers at the top, causing it to create skewness in the central part of the transformed data
Original Weight data
Theoretical Quantiles
Sample Quantiles
Box−Cox transformed Weight using ML
Theoretical Quantiles
Sample Quantiles
Fig. 3   Normal QQ-plot of the variable Weight in the Top Gear dataset (left) and the transformed variable
using the ML estimate of 휆 (right). The transformation does not make the five outliers at the bottom stand
Machine Learning 113:4953–4975
the variable at all. Whereas the MPG variable should not be transformed much but is, the
Weight variable should be transformed but almost isn’t.
In Sect. 2 we propose a new robust estimator for the parameter 휆 , and compare its sensitivity curve to those of other methods. Section 3 presents a simulation to study the performance of several estimators on clean and contaminated data. Section 4 illustrates the
proposed method on real data examples, and Sect. 6 concludes.
2  Methodology
2.1  Fitting a transformation by minimizing a robust criterion
The most popular way of estimating the 휆 of the BC and YJ transformations is to use maximum likelihood (ML) under the assumption that the transformed variable follows a normal
distribution, as will briefly be summarized in Sect. 2.3. However, it is well known that ML
estimation is very sensitive to outliers in the data and other deviations from the assumed
model. We therefore propose a different way of estimating the transformation parameter of
a transformation.
Consider an ordered sample of univariate observations X = (x(1), … , x(n)) . Suppose we
want to estimate the parameter 휆 of a nonlinear function g휆 such that g휆(x(1)), … , g휆(x(n))
come close to quantiles from the standard normal cumulative distribution function 훷 . We
propose to estimate 휆 as:
Here ̂𝜇M is the Huber M-estimate of location of the g휆(x(i)) , and ̂𝜎M is their Huber M-estimate of scale. Both are standard robust univariate estimators . The
pi = (i −1∕3)∕(n + 1∕3) are the usual equispaced probabilities that also yield the quantiles
in the QQ-plot ). The function 휌 needs to be
positive, even and continuously differentiable. In least squares methods 휌(t) = t2 , but in
our situation there can be large absolute residuals |
g𝜆(x(i))−̂𝜇
−𝛷−1(pi)| caused by outlying
values of g휆(x(i)) . In order to obtain a robust method we need a bounded 휌 function. We
propose to use the well-known Tukey bisquare 휌-function given by
The constant c is a tuning parameter, which we set to 0.5 by default here. See section A of
the supplementary material for a motivation of this choice.
To calculate ̂𝜆 numerically, we use the R function optimize() which relies on a combination of golden section search and successive parabolic interpolation to minimize the
objective of (5).
2.2  Rectified Box–Cox and Yeo–Johnson transformations
In this section we propose a modification of the classical BC and YJ transformations,
called the rectified BC and YJ transformations. They make a continuously differentiable
switch to a linear transformation in the tails of the BC and YJ functions. The purpose of
̂𝜆= argmin
(g𝜆(x(i)) −̂𝜇M
1 −(1 −(x∕c)2)3
if |x| > c.
Machine Learning 113:4953–4975
these modified transformations is to remedy two issues. First, the range of the classical BC
and YJ transformations depends on 휆 and is often only a half line. And second, as argued in
the introduction, the classical transformations often push outliers closer to the majority of
the data, which makes the outliers harder to detect. Instead the range of the proposed modified transformations is always the entire real line, and it becomes less likely that outliers
are masked by the transformation.
For 𝜆< 1 , the BC transformation is designed to make right-skewed distributions more
symmetrical, and is bounded from above. In this case we define the rectified BC transformation as follows. Consider an upper constant Cu > 1 . The rectified BC transformation ̊g𝜆
is defined as
Similarly, for 𝜆> 1 and a positive lower constant C퓁< 1 we put
For the YJ transformation we construct rectified counterparts in a similar fashion. For
𝜆< 1 and a value Cu > 0 we define the rectified YJ transformation ̊h𝜆(x) as in (7) with g휆
replaced by h휆:
Analogously, for 𝜆> 1 and C퓁< 0 we define ̊h𝜆(x) as in (8):
Figure 4 shows such rectified BC and YJ transformations.
What are good choices of C퓁 and Cu ? Since the original data is often asymmetric, we
cannot just use a center (like the median) plus or minus a fixed number of (robust) standard deviations. Instead we set C퓁 equal to the first quartile of the original data, and for Cu
we take the third quartile. Other choices could be used, but more extreme quantiles would
yield a higher sensitivity to outliers.
2.3  Reweighted maximum likelihood
We now describe a reweighting scheme to increase the accuracy of the estimated ̂𝜆 while
preserving its robustness. For a data set x1, … , xn the classical maximum likelihood estimator for the Yeo–Johnson transformation parameter 휆 is given by the 휆 which maximizes the
normal loglikelihood. After removing constant terms this can be written as:
ML,𝜆 is the maximum likelihood scale of the transformed data given by
g𝜆(Cu) + (x −Cu)g
if x > Cu.
{ g𝜆(C퓁) + (x −C퓁)g
h𝜆(Cu) + (x −Cu)h
if x > Cu.
{ h𝜆(C퓁) + (x −C퓁)h
ML = argmax
ML,𝜆) + (𝜆−1) sign (xi) log(|xi| + 1)
Machine Learning 113:4953–4975
The last term in (11) comes from the derivative of the YJ transformation. Criterion
(11) is sensitive to outliers since it depends on a classical variance and the unbounded
term log(1 + |xi|) . This can be remedied by using weights. Given a set of weights
W = (w1, … , wn) we define a weighted maximum likelihood (WML) estimator by
W,𝜆 now denotes the weighted variance of the transformed data:
If the weights appropriately downweight the outliers in the data, the WML criterion yields
a more robust estimate of the transformation parameter.
For the BC transform the reasoning is analogous, the only change being the final term that
comes from the derivative of the BC transform. This yields
In general, finding robust data weights is not an easy task. The problem is that the observed
data X = (x1, … , xn) can have a (very) skewed distribution and there is no straightforward
way to know which points will be outliers in the transformed data when 휆 is unknown. But
suppose that we have a rough initial estimate 휆0 of 휆 . We can then transform the data with
휆0 yielding h휆0(X) = (h휆0(x1), … , h휆0(xn)) , which should be a lot more symmetric than the
(h𝜆(xi) −̂𝜇ML,𝜆)2
WML = argmax
W,𝜆) + (𝜆−1) sign (xi) log(1 + |xi|)
i=1 wi(h𝜆(xi) −̂𝜇W,𝜆)2
i=1 wi h𝜆(xi)
WML = argmax
W,𝜆) + (𝜆−1) log(xi)
Plot of g°λ(x)
Plot of h°
Fig. 4   The rectified Box–Cox (left) and Yeo–Johnson (right) transformations for a range of parameters 휆 .
They look quite similar to the original transformations in Fig. 1 but contract less on the right when 𝜆< 1 ,
and contract less on the left when 𝜆> 1
Machine Learning 113:4953–4975
original data. We can therefore compute weights on h휆0(X) using a classical weight function. Here we will use the “hard rejection rule” given by
with ̂𝜇 and ̂𝜎 as in (5). With these weights we can compute a reweighted estimate ̂𝜆1 by the
WML estimator in (13). Of course, the robustness of the reweighted estimator will depend
strongly on the robustness of the initial estimate 휆0.
Note that the reweighting step can be iterated, yielding a multistep weighted ML estimator. In simulation studies we found that more than 2 reweighting steps provided no further
improvement in terms of accuracy (these results are not shown for brevity). We will always
use two reweighting steps from here onward.
2.4  The proposed method
Combining the above ideas, our proposed reweighted maximum likelihood (RewML)
method consists of the following steps:
• Step 1 Compute the initial estimate 휆0 by maximizing the robust criterion (5). When
fitting a Box–Cox transformation, plug in the rectified function ̊g𝜆 . When fitting a Yeo–
Johnson transformation, use the rectified function ̊h𝜆 . Note that the rectified transforms
are only used in this first step.
• Step 2 Using 휆0 as starting value, compute the reweighted ML estimate from (15) when
fitting the unrectified Box–Cox transform g휆 , and from (13) when fitting the unrectified
Yeo–Johnson transform h휆.
• Step 3 Repeat step 2 once and stop.
2.5  Other estimators of
We will compare our proposal with two existing robust methods.
The first is the robustified ML estimator proposed by Carroll . The idea was to
replace the variance ̂𝜎2
ML,𝜆 in the ML formula (11) by a robust variance estimate of the
transformed data. Carroll’s method was proposed for the BC transformation, but the idea
can be extended naturally to the estimation of the parameter of the YJ transformation. The
estimator is then given by
where ̂𝜎M,𝜆 denotes the usual Huber M-estimate of scale of the transformed
data set (h휆(x1), … , h휆(xn)).
The second method is the maximum trimmed likelihood (MTL) estimator of Van der
Veeken . Given a data set of size n, and a fixed number h that has to satisfy
2⌉< h < n , this method looks for the parameter ̂𝜆 which produces a subset of h consecutive observations which maximize the ML criterion (11).
if |h𝜆0(xi) −̂𝜇| ⩽𝛷−1(0.995) ̂𝜎
if |h𝜆0(xi) −̂𝜇| > 𝛷−1(0.995) ̂𝜎
̂𝜆Carroll = argmax
M,𝜆) + (𝜆−1) sign (xi) log(1 + |xi|)
Machine Learning 113:4953–4975
2.6  Sensitivity curves
In order to assess robustness against an outlier, stylized sensitivity curves were introduced on
page 96 of Andrews et al. . For a given estimator T and a cumulative distribution function F they are constructed as follows:
1. Generate a stylized pseudo data set X0 of size n −1 :
where the pi for i = 1, … , n −1 are equispaced probabilities that are symmetric about
1/2. We can for instance use pi = i∕n.
2. Add to this stylized data set a variable point z to obtain
3. Calculate the sensitivity curve in z by
where z ranges over a grid chosen by the user. The purpose of the factor n is to put
sensitivity curves with different values of n on a similar scale.
The top panel of Fig. 5 shows the sensitivity curves for several estimators of the parameter 휆
of the YJ transformation. We chose F = 훷 so the true transformation parameter 휆 is 1, and
n = 100 . The maximum likelihood estimator ML of (11) has an unbounded sensitivity curve,
which is undesirable as it means that a single outlier can move ̂𝜆 arbitrarily far away. The estimator of Carroll (17) has the same property, but is less affected in the sense that for a high |z|
the value of | SCn(z)| is smaller than for the ML estimator. The RewML estimator that we proposed in Sect. 2.4 has a sensitivity curve that lies close to that of the ML in the central region
of z, and becomes exactly zero for more extreme values of |z|. Such a sensitivity curve is called
redescending, meaning that it goes back to zero. Therefore a far outlier has little effect on
the resulting estimate. We also show MTL95, the trimmed likelihood estimator described in
Sect. 2.5 with h∕n = 95% . Its sensitivity curve is also redescending, but in the central region it
is more erratic with several jumps.
The lower panel of Fig. 5 shows the sensitivity curves for the Box–Cox transformation
when the true parameter is 휆= 0 , i.e. the clean data follows a lognormal distribution F. We
now put log(z) on the horizontal axis, since this makes the plot more comparable to that for
Yeo–Johnson in the top panel. Also here the ML and Carroll’s estimator have an unbounded
sensitivity curve. Our RewML estimator has a redescending SC which again behaves similarly
to the classical ML for small | log(z)| , whereas the sensitivity to an extreme outlier is zero. The
maximal trimmed likelihood estimator MTL95 has large jumps reaching values over 40 in the
central region. Those peaks are not shown because the other curves would be hard to distinguish on that scale.
X0 = (x1, … , xn−1) =
F−1(p1), … , F−1(pn−1)
Xz = (x1, … , xn−1, z).
SCn(z) ∶= n
T(Xz) −T(X0)
Machine Learning 113:4953–4975
3  Simulation
3.1  Compared methods
For the Box–Cox as well as the Yeo–Johnson transformations we perform a simulation
study to compare the performance of several methods, including our proposal. The estimators under consideration are:
1. ML the classical maximum likelihood estimator given by (11), or by (15) with all wi = 1.
2. Carroll the robustified maximum likelihood estimator of Carroll given by (17).
3. MTL the maximum trimmed likelihood estimator of Van der Veeken . The notation MTL90 stands for the version with h∕n = 90%.
4. RewML the proposed reweighted maximum likelihood estimator described in Sect. 2.4.
Fig. 5   Sensitivity curves of estimators of the parameter 휆 in the
Yeo–Johnson (top) and Box–Cox
(bottom) transformations, with
sample size n = 100
Machine Learning 113:4953–4975
5. RewMLnr a variation on RewML in which the first step of Sect. 2.4 applies (5) to the
original Box–Cox or Yeo–Johnson transform instead of their rectified versions. This is
not intended as a proposal, but included in order to show the advantage of rectification.
3.2  Data generation
We generate clean data sets as well as data with a fraction 휀 of outliers. The clean data
are produced by generating a sample of size n from the standard normal distribution, after
which the inverse of the BC or YJ transformation with a given 휆 is applied. For contaminated data we replace a percentage 휀 of the standard normal data by outliers at a fixed position before the inverse transformation is applied. For each such combination of 휆 and 휀 we
generate m = 100 data sets.
To be more precise, the percentage 휀 of contaminated points takes on the values 0, 0.05,
0.1, and 0.15, where 휀= 0 corresponds to uncontaminated data. For the YJ transformation
we take the true transformation parameter 휆 equal to 0.5, 1.0, or 1.5. We chose these values
because for 휆 between 0 and 2 the range of YJ given by (4) is the entire real line, so the
inverse of YJ is defined for all real numbers. For the BC transformation we take 휆= 0 for
which the range (3) is also the real line. For a given combination of 휀 and 휆 the steps of the
data generation are:
1. Generate a sample Y = (y1, … , yn) from the standard normal distribution. Let k > 0 be
a positive parameter. Then replace a fraction 휀 of the points in Y by k itself when 휆⩽1 ,
and by −k when 𝜆> 1.
2. Apply the inverse BC transformation to Y, yielding the data set X given
byX = (g−1
휆(y1), … , g−1
휆(yn)) . For YJ we put X = (h−1
휆(y1), … , h−1
3. Estimate 휆 from X using the methods described in Sect. 3.1.
The parameter k characterizing the position of the contamination is an integer that we let
range from 0 to 10.
We then estimate the bias and mean squared error (MSE) of each method by
where j = 1, … , m ranges over the generated data sets.
3.3  Results for the Yeo–Johnson transformation
We first consider the effect of an increasing percentage of contamination on the different estimators. In this setting we fix the position of the contamination by setting
k = 10 . (The results for k = 6 are qualitatively similar, as can be seen in section B of
the supplementary material.) Figure 6 shows the bias and MSE of the estimators for
an increasing contamination percentage 휀 on the horizontal axis. The results in the top
row are for data generated with 휆= 0.5 , whereas the middle row was generated with
휆= 1 and the bottom row with 휆= 1.5 . In all rows the classical ML and the Carroll
estimator have the largest bias and MSE, meaning they react strongly to far outliers,
as suggested by their unbounded sensitivity curves in Fig. 5. In contrast with this both
bias ∶= ave n
j=1( ̂𝜆j −𝜆)
MSE ∶= ave n
j=1( ̂𝜆j −𝜆)2
Machine Learning 113:4953–4975
Fig. 6   Bias (left) and MSE (right) of the estimated ̂𝜆 of the Yeo–Johnson transformation as a function of the
percentage 휀 of outliers, when the location of the outliers is determined by setting k = 10 . The true parameter 휆 used to generate the data is 0.5 in the top row, 1.0 in the middle row, and 1.5 in the bottom row
Machine Learning 113:4953–4975
RewML and RewMLnr perform much better as their bias and MSE are closer to zero.
Up to about 5% of outliers their curves are almost indistinguishable, but beyond that
RewML outperforms RewMLnr by a widening margin. This indicates that using the
rectified YJ transform in the first step of the estimator (see Sect. 2.4) is more robust
than using the plain YJ in that step, even though the goal of the entire 3-step procedure
RewML is to estimate the 휆 of the plain YJ transform.
In the same Fig. 6 we see the behavior of the maximum trimmed likelihood estimators
MTL95, MTL90 and MTL85. In the middle row 휆 is 1, and we see that MTL95, which fits
95% of the data, performs well when there are up to 5% of outliers and performs poorly
when there are over 5% of outliers. Analogously MTL90 performs well as long as there
are at most 10% of outliers, and so on. This is the intended behavior. But note that for 휆≠1
these estimators also have a substantial bias when the fraction of outliers is below what
they aim for, as can be seen in the top and bottom panels of Fig. 6. For instance MTL85
is biased when 휀 is under 15% , even for 휀= 0% when there are no outliers at all. So overall the MTL estimators only performed well when the percentage of trimming was equal
to 1 minus the percentage of outliers in the data. Since the true percentage of outliers is
almost never known in advance, it is not recommended to use the MTL method for variable
transformation.
Let us now investigate what happens if we keep the percentage of outliers fixed, say at
휀= 10% , but vary the position of the contamination by letting k = 0, 1, … , 10 . Figure 7
shows the resulting bias and MSE, with again 휆= 0.5 in the top row, 휆= 1 in the middle row, and 휆= 1.5 in the bottom row. For k = 0 and k = 1 the ML, Carroll, RewML and
RewMLnr methods give similar results, since the contamination is close to the center so it
cannot be considered outlying. But as k increases the classical ML and the Carroll estimator become heavily affected by the outliers. On the other hand RewML and RewMLnr perform much better, and again RewML outperforms RewMLnr. Note that the bias of RewML
moves toward zero when k is large enough. We already noted this redescending behavior in
its sensitivity curve in Fig. 5.
The behavior of the maximum trimmed likelihood methods depends on the value of 휆
used to generate the data. First focus on the middle row of Fig. 7 where 휆= 1 so the clean
data is generated from the standard normal distribution. In that situation both MTL90 and
MTL85 behave well, whereas MTL95 can only withstand 5% of outliers and not the 10% generated here. One might expect the MTL method to work well as long as its h excludes at least
the number of outliers in the data. But in fact MTL85 does not behave so well when 휆 differs
from 1, as seen in the top and bottom panels of Fig. 7, where the bias remains substantial
even though the method expects 15% of outliers and there are only 10% of them. As in Fig. 6
this suggests that one needs to know the actual percentage of outliers in the data in order to
select the appropriate h for the MTL method, but that percentage is typically unknown.
3.4  Results for the Box–Cox transformation
When simulating data to apply the Box–Cox transformation to, the most natural choice of
휆 is zero since this is the only value for which the range of BC is the entire real line. Therefore we can carry out the inverse BC transformation on any data set generated from a normal distribution, so the clean data follows a log-normal distribution. The top panel of Fig. 8
shows the bias and MSE for 10% of outliers with k = 1, … , 10 . We see that the classical
ML and the estimator of Carroll are sensitive to outliers when k grows. Our reweighted
Machine Learning 113:4953–4975
method RewML performs much better. The RewMLnr method only differs from RewML
in that it uses the non-rectified BC transform in the first step, and does not do as well since
its bias goes back to zero at a slower rate.
Fig. 7   Bias (left) and MSE (right) of the estimated ̂𝜆 of the Yeo–Johnson transformation as a function of k
which determines how far the outliers are. Here the percentage of outliers is fixed at 10% . The true parameter 휆 used to generate the data is 0.5 in the top row, 1.0 in the middle row, and 1.5 in the bottom row
Machine Learning 113:4953–4975
The MTL estimators perform poorly here. The MTL95 version trims only 5% of the
data points so it cannot discard the 10% of outliers, leading to a behavior resembling that
of the classical ML. But also MTL90 and MTL85, which trim enough data points, obtain a
large bias which goes in the opposite direction, accompanied by a high MSE. The MSE of
MTL90 and MTL85 lie entirely above the plot area.
Finally, we consider a scenario with 휆= 1 . In that case the range of the Box–Cox transformation given by (3) is only (−1, +∞) so the transformed data cannot be normally distributed (which is already an issue for the justification of the classical maximum likelihood
method). But the transformed data can have a truncated normal distribution. In this special
setting we generated data from the normal distribution with mean 1 and standard deviation
1/3, and then truncated it to [0.01, 1.99] (keeping n = 100 points), so the clean data are
strictly positive and have a symmetric distribution around 1. In the bottom panel of Fig. 8
we see that the ML and Carroll estimators are not robust in this simulation setting. The
trimmed likelihood estimators also fail to deliver reasonable results, with curves that often
fall outside the plot area. On the other hand RewML still performs well, and again does
better than RewMLnr.
Fig. 8   Bias (left) and MSE (right) of the estimated ̂𝜆 of the Box–Cox transformation as a function of k
which determines how far the outliers are. Here the percentage of outliers is fixed at 10% . The true 휆 used to
generate the data is 0 in the top row and 1 in the bottom row
Machine Learning 113:4953–4975
The simulation results for a fixed outlier position at k = 6 or k = 10 with contamination levels 휀 from 0% to 15% can be found in section B of the supplementary material,
and are qualitatively similar to those for the YJ transform.
4  Empirical examples
4.1  Car data
Let us revisit the positive variable MPG from the TopGear data shown in the left panel
of Fig. 2. The majority of these data are already roughly normal, and three far outliers
at the top deviate from this pattern. Before applying a Box–Cox transformation we first
scale the variable so its median becomes 1. This makes the result invariant to the unit of
measurement, whether it is miles per gallon or, say, kilometers per liter. The maximum
likelihood estimator for Box–Cox tries to bring in the outliers and yields ̂𝜆= −0.11 ,
which is close to 휆= 0 corresponding to a logarithmic transformation. The resulting
transformed data in the left panel of Fig. 9 are quite skewed in the central part, so not
normal at all, which defeats the purpose of the transformation. This result is in sharp
contrast with our reweighted maximum likelihood (RewML) method which estimates
the transformation parameter as ̂𝜆= 0.84 . The resulting transformed data in the right
panel does achieve central normality.
The variable Weight in the left panel of Fig. 3 is not very normal in its center and
has some outliers at the bottom. The classical ML estimate is ̂𝜆= 0.83 , close to 휆= 1
which would not transform the data at all, as we can see in the resulting left panel of
Fig. 10. In contrast, our RewML estimator obtains ̂𝜆= 0.09 which substantially transforms the data, yielding the right panel of Fig. 10. There the central part of the data is
very close to normal, and the outliers at the bottom now stand out more, as they should.
Box−Cox transformed MPG using ML
Theoretical Quantiles
Sample Quantiles
Box−Cox transformed MPG using RewML
Theoretical Quantiles
Sample Quantiles
Fig. 9   Normal QQ-plot of the Box–Cox transformed variable MPG using the ML estimate of 휆 (left) and
using the RewML estimate (right). The ML is strongly affected by the 3 outliers at the top, thereby transforming the central data away from normality. The RewML method achieves central normality and makes
the outliers stand out more
Machine Learning 113:4953–4975
4.2  Glass data
For a multivariate example we turn to the glass data from chemometrics, which has become something of
a benchmark. The data consists of n = 180 archeological glass samples, which were analyzed by spectroscopy. Our variables are the intensities measured at 500 wavelengths.
Many of these variables do not look normally distributed.
We first applied a Yeo–Johnson transformation to each variable with ̂𝜆 obtained from
the nonrobust ML method of (11). For each variable we then standardized the transformed data h ̂𝜆(xi) to (h ̂𝜆(xi) −̂𝜇ML, ̂𝜆)∕̂𝜎ML, ̂𝜆 where ̂𝜇ML, ̂𝜆 and ̂𝜎ML, ̂𝜆 are given by (12).
This yields a standardized transformed data set with again 180 rows and 500 columns. In order to detect outliers in this matrix we compare each value to the interval
[−2.57, 2.57] which has a probability of exactly 99% for standard normal data. The top
panel of Fig. 11 is a heatmap of the standardized transformed data matrix where each
value within [−2.57, 2.57] is shown as yellow, values above 2.57 are red, and values
below −2.57 are blue. This heatmap is predominantly yellow because the ML method
tends to transform the data in a way that masks outliers, so not much structure is visible.
Next, we transformed each variable by Yeo–Johnson with ̂𝜆 obtained by the
robust RewML method. The transformed variables were standardized accordingly to
(h ̂𝜆(xi) −̂𝜇W, ̂𝜆)∕̂𝜎W, ̂𝜆 where ̂𝜇W, ̂𝜆 and ̂𝜎W, ̂𝜆 are given by (14) using the final weights in
(13). The resulting heatmap is in the bottom panel of Fig. 11. Here we see much more
structure, with red regions corresponding to glass samples with unusually high spectral
intensities at certain wavelengths. This is because the RewML method aims to make
the central part of each variable as normal as possible, which allows outliers to deviate from that central region. The resulting heatmap has a subject-matter interpretation
since wavelengths correspond to chemical elements. It indicates that some of the glass
samples (with row numbers between 22 and 30) have a higher concentration of phosphor, whereas rows 57–63 and 74–76 had an unusually high amount of calcium. The
Box−Cox transformed Weight using ML
Theoretical Quantiles
Sample Quantiles
Box−Cox transformed Weight using RewML
Theoretical Quantiles
Sample Quantiles
Fig. 10   Normal QQ-plot of the Box–Cox transformed variable Weight using the ML estimate of 휆 (left)
and using the RewML estimate (right). The ML masks the five outliers at the bottom, whereas RewML
accentuates them and achieves central normality
Machine Learning 113:4953–4975
red zones in the bottom part of the heatmap were caused by the fact that the measuring
instrument was cleaned before recording the last 38 spectra.
4.3  DPOSS data
As a final example we consider data from the Digitized Palomar Sky Survey (DPOSS)
described by Djorgovski et al. . We work with the dataset of 20000 stars available
as dposs in the R package cellWise . The data are measurements in three color bands, but for the purpose of illustration we restrict attention to
the color band with the fewest missing values. Selecting the completely observed rows
then yields a dataset of 11478 observations with 7 variables. Variables MAper, MTot and
MCore measure light intensity, and variables Area, IR2, csf and Ellip are measurements of
the size and shape of the images.
In order to analyze the data we first apply the YJ-transformation to each variable, with
the ̂𝜆 estimates obtained from RewML. We then perform cellwise robust PCA . We retained k = 4 components, explaining 97% of the variance. Figure 12 is
the pairs plot of the robust scores. We clearly see a large cluster of regular points with
some outliers around it, and a smaller cluster of rather extreme outliers in red. The red
points correspond to the stars with a high value of MAper. The blue points are somewhat in
wavelengths
YJ transformed variables by ML
wavelengths
YJ transformed variables by RewML
Fig. 11   Heatmap of the glass data after transforming each variable (column) by a Yeo–Johnson transform
with parameter 휆 estimated by (top) the maximum likelihood method, and (bottom) the reweighted maximum likelihood method RewML. Yellow cells correspond to values in the central 99% range of the normal
distribution. Red cells indicate unusually high values, and blue cells have unusually low values (Color figure online)
Machine Learning 113:4953–4975
between the main cluster and the smaller cluster of extreme outliers. The two orange points
are celestial objects with an extreme value of Ellip.
The left panel of Fig. 13 contains the outlier map of this robust
PCA. Such an outlier map consists of two ingredients. The horizontal axis contains the
score distance of each object 퐱i . This is the robust Mahalanobis distance of the orthogonal
projection of 퐱i on the subspace spanned by the k principal components, and can be computed as
where tij are the PCA scores and 휆j is the j-th largest eigenvalue. The vertical axis shows
the orthogonal distance ODi of each point 퐱i , which is the distance between 퐱i and its
projection on the k-dimensional subspace. We see that the red points form a large cluster
with extreme SDi as well as ODi . The blue points are intermediate, and the two orange
points are still unusual but less extreme. Interestingly, when applying robust PCA to the
Fig. 12   DPOSS data: pairs plot of the robust PCA scores after transforming the data with the robustly fitted
YJ-transformation
Machine Learning 113:4953–4975
untransformed data we obtain the right panel of Fig. 13 which shows a very different picture in which the red, blue and orange points are not clearly distinguished from the remainder. This outlier map appears to indicate a number of black points as most outlying, but in
fact they should not be considered outlying since they are characterized by high values of
Area and IR2 which are very right-skewed. When transforming these variables to central
normality, as in the left panel, the values that appeared to be outlying become part of the
regular tails of a symmetric distribution.
5  Discussion
In this discussion we address and clarify a few aspects of (robust) variable transformation.
Outliers in skewed data The question of what is an outlier in a general skewed distribution is a rather difficult one. One could flag the values below a lower cutoff or above an
upper cutoff, for instance given by quantiles of a distribution. But it is hard to robustly
determine cutoffs for a general skewed dataset or distribution. Our viewpoint is this. If
the data can be robustly transformed by BC or YJ towards a distribution that looks normal
except in the tails, then it easy to obtain cutoffs for the transformed data, e.g. by taking
their median plus or minus two median absolute deviations. And then, since the BC and
YJ transforms are monotone, these cutoffs can be transformed back to the original skewed
data or distribution. In this way we do not flag too many points. We have illustrated this for
lognormal data in section C of the supplementary material.
How many outliers can we deal with? In addition to sensitivity curves, which characterize the effect of a single outlier, it is interesting to find out how many adversely placed
outliers it takes to make the estimators fail badly. For RewML this turns out to be around
15% of the sample size, which is quite low compared to robust estimators of location or
scale. But this is unavoidable, as asymmetry is tied to the tails of the distribution. Empirically, skewness does not manifest itself much in the central part of the distribution, say
20% of mass above and below the median. On each side we have 30% of mass left, and if
the adversary is allowed to replace half of either portion (that is, 15% of the total mass) and
move it anywhere, they can modify the skewness a great deal.
Score distance
Orthogonal distance
Score distance
Orthogonal distance
Fig. 13   DPOSS data: outlier map of robust PCA applied to the YJ-transformed variables (left), and applied
to the raw variables (right). The colors are those of Fig. 12 (Color figure online)
Machine Learning 113:4953–4975
Prestandardization In practice, one would typically apply some form of prestandardization to the data before trying to fit a transformation. For the YJ transformation, we can
simply prestandardize the dataset X = {x1, … , xn} by
where mad is the median absolute deviation from the median. This is what we did in the
glass and DPOSS examples. An advantage of this prestandardization is that the relevant
휆 will typically be in the same range (say, from −4 to 6) for all variables in the data set,
which is convenient for the computation of ̂𝜆 . Before the BC transformation we cannot prestandardize by (19) since this would generate negative values. One option is to divide the
original xi by their median so they remain positive and have median 1. But if the resulting
data is tightly concentrated around 1, we may still require a huge positive or negative 휆 to
properly transform them by BC. Alternatively, we propose to prestandardize by
which performs a standardization on the log scale and then transforms back to the positive
scale. This again has the advantage that the range of 휆 can be kept fixed when computing ̂𝜆 ,
making it as easy as applying the YJ transform after (19). A disadvantage is that the transformation parameter becomes harder to interpret, since e.g. a value of 휆= 1 no longer corresponds to a linear transformation, but 휆= 0 still corresponds to a logarithmic transform.
Tuning constants The proposed method has some tuning parameters, namely the constant c in (6), the cutoff 훷−1(0.995) in the reweighting step, and the rectification points.
The tuning of the constant c is addressed in section A of the supplementary material.
The second parameter is the constant 훷−1(0.995) ≈2.57 in the weights (16). This weight
function is commonly used in existing reweighting techniques in robust statistics. If the
transformed data is normally distributed we flag approximately 1% of values with this
cutoff, since P(|Z| > 𝛷−1(0.995)) = 0.01 if Z ∼N(0, 1) . Choosing a higher cutoff results
in a higher efficiency, but at the cost of lower robustness. Generally, the estimates do not
depend too much on the choice of this cutoff as long as it is in a reasonably high range, say
from 훷−1(0.975) to 훷−1(0.995) . A final choice in the proposal are the “rectification points”
C퓁 and Cu for which we take the first and third quartiles of the data. Note that the constraints C퓁< 0 < Cu for YJ and C퓁< 1 < Cu for BC in (7)–(10) are satisfied automatically
when prestandardizing the data as described in the previous paragraph. It is worth noting
that these data-dependent choices of C퓁 and Cu do not create abrupt changes in the transformed data when moving through the range of possible values for 휆 because the rectified
transformations are continuous in 휆 by construction. For instance, passing from 𝜆< 1 to
𝜆> 1 does not cause a jump in the transformed data because 휆= 1 corresponds to a linear
transformation that is inherently rectified on both sides.
Models with transformed variables The ease or difficulty of interpreting a model in
the transformed variables depends on the model under consideration. For nonparametric
models it makes little difference. For parametric models the transformations can make the
model harder to interpret in some cases and easier in others, for instance where there is a
simple linear relation in the transformed variables instead of a model with higher-order
terms in the original variables. Also, the notion of leverage point in linear regression is
more easily interpretable with roughly normal regressors, as it is related to the (robust)
Mahalanobis distance of a point in regressor space.
̃xi = xi −median (X)
(log(xi) −median (log X)
mad (log X)
Machine Learning 113:4953–4975
The effect of the sampling variability of 휆 on the inference in the resulting model is
also model dependent. We expect that it will typically lead to a somewhat higher variability in the estimation. However, as the BY and YJ transformations are continuous and
differentiable in 휆 and not very sensitive to small changes of 휆 , the increase in variability is
likely small. Moreover, if we apply BC or YJ transformations to predictor variables used in
CART or Random Forest, the predictions and feature importance metrics stay exactly the
same because the BC and YJ transformations are monotone.
6  Conclusion
In our view, a transformation to normality should fit the central part of the data well, and
not be determined by any outliers that may be present. This is why we aim for central normality, where the transformed data is close to normal (Gaussian) with the possible exception of some outliers that can remain further out. Fitting such a transformation is not easy,
because a point that appears to be outlying in the original data may not be outlying in the
transformed data, and vice versa.
To address this problem we introduced a combination of three ideas: a highly robust
objective function (5), the rectified Box–Cox and Yeo–Johnson transforms in Sect.  2.2
which we use in our initial estimator only, and a reweighted maximum likelihood procedure for transformations. This combination turns out to be a powerful tool for this difficult
Preprocessing real data by this tool paves the way for applying subsequent methods,
such as anomaly detection and well-established model fitting and predictive techniques.
Supplementary Information  The online version contains supplementary material available at ( 
org/​10.​1007/​s10994-​021-​05960-5).
Acknowledgements  This work was supported by Grant C16/15/068 of KU Leuven, Belgium. We thank the
referees for constructive comments improving the presentation.
Data availability  The proposed method is available as the function transfo() in the R package cellWise on CRAN, which also includes the vignette
transfo_examples that reproduces all the examples in this paper. It makes uses of the R-packages
gridExtra , reshape2  , ggplot2 and scales
 .
Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License,
which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long
as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article
are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the
material. If material is not included in the article’s Creative Commons licence and your intended use is not
permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly
from the copyright holder. To view a copy of this licence, visit