Borrowing Treasures from the Wealthy: Deep Transfer Learning through
Selective Joint Fine-Tuning
Weifeng Ge
Department of Computer Science, The University of Hong Kong
Deep neural networks require a large amount of labeled
training data during supervised learning. However, collecting and labeling so much data might be infeasible in many
cases. In this paper, we introduce a deep transfer learning scheme, called selective joint ﬁne-tuning, for improving the performance of deep learning tasks with insufﬁcient
training data. In this scheme, a target learning task with
insufﬁcient training data is carried out simultaneously with
another source learning task with abundant training data.
However, the source learning task does not use all existing
training data. Our core idea is to identify and use a subset
of training images from the original source learning task
whose low-level characteristics are similar to those from
the target learning task, and jointly ﬁne-tune shared convolutional layers for both tasks. Speciﬁcally, we compute
descriptors from linear or nonlinear ﬁlter bank responses
on training images from both tasks, and use such descriptors to search for a desired subset of training samples for
the source learning task.
Experiments demonstrate that our deep transfer learning scheme achieves state-of-the-art performance on multiple visual classiﬁcation tasks with insufﬁcient training
data for deep learning. Such tasks include Caltech 256,
MIT Indoor 67, and ﬁne-grained classiﬁcation problems
(Oxford Flowers 102 and Stanford Dogs 120).
In comparison to ﬁne-tuning without a source domain, the proposed method can improve the classiﬁcation accuracy
by 2% - 10% using a single model.
Codes and models are available at 
Selective-Joint-Fine-tuning.
1. Introduction
Convolutional neural networks (CNNs) have become
deeper and larger to pursue increasingly better performance
on classiﬁcation and recognition tasks .
Looking at the successes of deep learning in computer
vison, we ﬁnd that a large amount of training or pretraining data is essential in training deep neural networks.
Large-scale image datasets, such as the ImageNet ILSVRC
dataset , Places , and MS COCO , have led to
a series of breakthroughs in visual recognition, including
image classiﬁcation , object detection , and semantic segmentation . Many other related visual tasks have
beneﬁted from these breakthroughs.
Nonetheless, researchers face a dilemma when using
deep convolutional neural networks to perform visual tasks
that do not have sufﬁcient training data. Training a deep
network with insufﬁcient data might even give rise to inferior performance in comparison to traditional classiﬁers fed
with handcrafted features. Fine-grained classiﬁcation problems, such as Oxford Flowers 102 and Stanford Dogs
120 , are such examples. The number of training samples in these datasets is far from being enough for training
large-scale deep neural networks, and the networks would
become overﬁt quickly.
Solving the overﬁtting problem for deep convolutional
neural networks on learning tasks without sufﬁcient training data is challenging . Transfer learning techniques
that apply knowledge learnt from one task to other related
tasks have been proven helpful . In the context of deep
learning, ﬁne-tuning a deep network pre-trained on the ImageNet or Places dataset is a common strategy to learn taskspeciﬁc deep features.This strategy is considered a simple
transfer learning technique for deep learning.
since the ratio between the number of learnable parameters
and the number of training samples still remains the same,
ﬁne-tuning needs to be terminated after a relatively small
number of iterations; otherwise, overﬁtting still occurs.
In this paper, we attempt to tackle the problem of training
deep neural networks for learning tasks that have insufﬁcient training data. We adopt the source-target joint training
methodology when ﬁne-tuning deep neural networks.
The original learning task without sufﬁcient training data
is called the target learning task, T t. To boost its performance, the target learning task is teamed up with another
learning task with rich training data. The latter is called
the source learning task, T s. Suppose the source learning
task has a large-scale training set Ds, and the target learning task has a small-scale training set Dt. Since the target
 
learning task is likely a specialized task, we envisage the
image signals in its dataset possess certain unique low-level
characteristics (e.g. fur textures in Stanford Dogs 120 ),
and the learned kernels in the convolutional layers of a deep
network need to grasp such characteristics in order to generate highly discriminative features. Thus supplying sufﬁcient training images with similar low-level characteristics
becomes the most important mission of the source learning task. Our core idea is to identify a subset of training
images from Ds whose low-level characteristics are similar to those from Dt, and then jointly ﬁne-tune a shared set
of convolutional layers for both source and target learning
tasks. The source learning task is ﬁne-tuned using the selected training images only. Hence, this process is called
selective joint ﬁne-tuning. The rationale behind this is that
the unique low-level characteristics of the images from Dt
might be overwhelmed if all images from Ds were taken as
training samples for the source learning task.
How do we select images from Ds that share similar
low-level characteristics as those from Dt? Since kernels
followed with nonlinear activation in a deep convolutional
neural network (CNN) are actually nonlinear spatial ﬁlters,
to ﬁnd sufﬁcient data for training high-quality kernels, we
use the responses from existing linear or nonlinear ﬁlter
banks to deﬁne similarity in low-level characteristics. Gabor ﬁlters form an example of a linear ﬁlter bank, and
the complete set of kernels from certain layers of a pretrained CNN form an example of a nonlinear ﬁlter bank. We
use histograms of ﬁlter bank responses as image descriptors
to search for images with similar low-level characteristics.
The motivation behind selecting images according to
their low-level characteristics is two fold. First, low-level
characteristics are extracted by kernels in the lower convolutional layers of a deep network. These lower convolutional layers form the foundation of an entire network,
and the quality of features extracted by these layers determines the quality of features at higher levels of the deep network. Sufﬁcient training images sharing similar low-level
characteristics could strength the kernels in these layers.
Second, images with similar low-level characteristics could
have very different high-level semantic contents. Therefore,
searching for images using low-level characteristics has less
restrictions and can return much more training images than
using high-level semantic contents.
The above source-target selective joint ﬁne-tuning
scheme is expected to beneﬁt the target learning task in two
different ways. First, since convolutional layers are shared
between the two learning tasks, the selected training samples for the source learning task prevent the deep network
from overﬁtting quickly. Second, since the selected training samples for the source learning task share similar lowlevel characteristics as those from the target learning task,
kernels in their shared convolutional layers can be trained
more robustly to generate highly discriminative features for
the target learning task.
The proposed source-target selective joint ﬁne-tuning
scheme is easy to implement. Experimental results demonstrate state-of-the-art performance on multiple visual classiﬁcation tasks with much less training samples than what
is required by recent deep learning architectures. These visual classiﬁcation tasks include ﬁne-grained classiﬁcation
on Stanford Dogs 120 and Oxford Flowers 102 ,
image classiﬁcation on Caltech 256 , and scene classi-
ﬁcation on MIT Indoor 67 .
In summary, this paper has the following contributions:
• We introduce a new deep transfer learning scheme, called
selective joint ﬁne-tuning, for improving the performance of
deep learning tasks with insufﬁcient training data. It is an
important step forward in the context of the widely adopted
strategy of ﬁne-tuning a pre-trained deep neural network.
• We develop a novel pipeline for implementing this deep
transfer learning scheme. Speciﬁcally, we compute descriptors from linear or nonlinear ﬁlter bank responses on training images from both tasks, and use such descriptors to
search for a desired subset of training samples for the source
learning task.
• Experiments demonstrate that our deep transfer learning
scheme achieves state-of-the-art performance on multiple
visual classiﬁcation tasks with insufﬁcient training data for
deep learning.
2. Related Work
Multi-Task Learning.
Multi-task learning (MTL) obtains shared feature representations or classiﬁers for related
tasks . In comparison to learning individual tasks independently, features and classiﬁers learned with MTL often have better generalization capability. In deep learning,
faster RCNN jointly learns object locations and labels
using shared convolutional layers but different loss functions for these two tasks. In , the same multi-scale convolutional architecture was used to predict depth, surface
normals and semantic labels. This indicates that convolutional neural networks can be adapted to different tasks easily. While previous work attempts to ﬁnd a shared
feature space that beneﬁts multiple learning tasks, the proposed joint training scheme in this paper focuses on learning
a shared feature space that improves the performance of the
target learning task only.
Feature Extraction and Fine-tuning. Off-the-shelf CNN
features have been proven to be powerful in various computer vision problems. Pre-training convolutional
neural networks on ImageNet or Places has been
the standard practice for other vision problems. However,
features learnt in pre-trained models are not tailored for the
Black Grouse
Barn Spider
(a) Source and Target Domain
Training Data
(b) Search k Nearest Neighbors
in Shallow Feature Space
(c) Deep Convolutional
Neural Networks
(d) Joint Optimization in
Different Label Spaces
Source Domain Data
Target Domain Data
Source Domain Loss Minimization
Target Domain Loss Minimization
Target Training Samples
in Shallow Feature Spaces
Source Training Samples
in Shallow Feature Spaces
Convolutional layers shared by the
source and target learning tasks
Linear classifier
Linear classifier
Figure 1. Pipeline of the proposed selective joint ﬁne-tuning. From left to right: (a) Datasets in the source domain and the target domain. (b)
Select nearest neighbors of each target domain training sample in the source domain via a low-level feature space. (c) Deep convolutional
neural network initialized with weights pre-trained on ImageNet or Places. (d) Jointly optimize the source and target cost functions in their
own label spaces.
target learning task. Fine-tuning pre-trained models 
has become a commonly used method to learn task-speciﬁc
features. The transfer ability of different convolutional layers in CNNs has been investigated in . However, for
tasks that do not have sufﬁcient training data, overﬁtting
occurs quickly during ﬁne-tuning. The proposed pipeline in
this paper not only alleviates overﬁtting, but also attempts
to ﬁnd a more discriminative feature space for the target
learning task.
Transfer Learning. Different from MTL, transfer learning (or domain adaptation) applies knowledge learnt in
one domain to other related tasks. Domain adaptation algorithms can be divided into three categories, including instance adaption , feature adaption , and model
adaption . Hong et al. transferred rich semantic information from source categories to target categories via
the attention model. Tzeng et al. performed feature
adaptation using a shared convolutional neural network by
transferring the class relationship in the source domain to
the target domain. To make our pipeline more ﬂexible, this
paper does not assume the source and target label spaces
are the same as in . Different from the work in 
which randomly resamples training classes or images in the
source domain, this paper conducts a special type of transfer
learning by selecting source training samples that are nearest neighbors of samples in the target domain in the space
of certain low-level image descriptor.
Krause et al. directly performed Google image
search using keywords associated with categories from the
target domain, and download a noisy collection of images
to form a training set. In our method, we search for nearest neighbors in a large-scale labeled dataset using lowlevel features instead of high-level semantic information. It
has been shown in that low-level features computed in
the bottom layers of a CNN encode very rich information,
which can completely reconstruct the original image. Our
experimental results show that nearest neighbor search using low-level features can outperform that using high-level
semantic information as in .
3. Selective Joint Fine-tuning
3.1. Overview
1 shows the overall pipeline for our proposed
source-target selective joint ﬁne-tuning scheme. Given a
target learning task T t that has insufﬁcient training data,
we perform selective joint ﬁne-tuning as follows. The entire training dataset associated with the target learning task
is called the target domain. The source domain is deﬁned
similarly.
Source Domain
The minimum requirement is that
the number of images in the source domain, Ds
i=1, should be large enough to train a deep convolutional neural network from scratch. Ideally, these training images should present diversiﬁed low-level characteristics.
That is, running a ﬁlter bank on them give rise
to as diversiﬁed responses as possible. There exist a few
large-scale visual recognition datasets that can serve as the
source domain, including ImageNet ILSVRC dataset ,
Places , and MS COCO .
Source Domain Training Images
: In our selective joint
ﬁne-tuning, we do not use all images in the source domain
as training images. Instead, for each image from the target
domain, we search a certain number of images with similar low-level characteristics from the source domain. Only
images returned from these searches are used as training
images for the source learning task in selective joint ﬁnetuning. We apply a ﬁlter bank to all images in both source
domain and target domain. Histograms of ﬁlter bank responses are used as image descriptors during search. We
associate an adaptive number of source domain images with
each target domain image. Hard training samples in the target domain might be associated with a larger number of
source domain images. Two ﬁlter banks are used in our
experiments. One is the Gabor ﬁlter bank, and the other
consists of kernels in the convolutional layers of AlexNet
pre-trained on ImageNet .
CNN Architecture
: Almost any existing deep convolutional neural network, such as AlexNet , VGGNet ,
and ResidualNet , can be used in our selective joint ﬁnetuning. We use the 152-layer residual network with identity
mappings as the CNN architecture in our experiments.
The entire residual network is shared by the source and target learning tasks. An extra output layer is added on top of
the residual network for each of the two learning tasks. This
output layer is not shared because the two learning tasks
may not share the same label space. The residual network
is pre-trained either on ImageNet or Places.
Source-Target Joint Fine-tuning
: Each task uses its
own cost function during selective joint ﬁne-tuning, and every training image only contributes to the cost function corresponding to the domain it comes from. The source domain images selected by the aforementioned searches are
used as training images for the source learning task only
while the entire target domain is used as the training set for
the target learning task only. Since the residual network
(with all its convolutional layers) is shared by these two
learning tasks, it is ﬁne-tuned by both training sets. And the
output layers on top of the residual network are ﬁne-tuned
by its corresponding training set only. Thus we conduct
end-to-end joint ﬁne-tuning to minimize the original loss
functions of the source learning task and the target learning
task simultaneously.
3.2. Similar Image Search
There is a unique step in our pipeline. For each image
from the target domain, we search a certain number of images with similar low-level characteristics from the source
domain. Only images returned from these searches are used
as training images for the source learning task in selective
joint ﬁne-tuning. We elaborate this image search step below.
Filter Bank
We use the responses to a ﬁlter bank to describe the low-level characteristics of an image. The ﬁrst
ﬁlter bank we use is the Gabor ﬁlter bank. Gabor ﬁlters are
commonly used for feature description, especially texture
description . Gabor ﬁlter responses are powerful lowlevel features for image and pattern analysis. We use the
parameter setting in as a reference. For each of the real
and imaginary parts, we use 24 convolutional kernels with
4 scales and 6 orientations. Thus there are 48 Gabor ﬁlters
Kernels in a deep convolutional neural network are actually spatial ﬁlters. When there is nonlinear activation following a kernel, the combination of the kernel and nonlinear
activation is essentially a nonlinear ﬁlter. A deep CNN can
extract low/middle/high level features at different convolutional layers . Convolutional layers close to the input
data focus on extract low-level features while those further
away from the input extract middle- and high-level features.
In fact, a subset of the kernels in the ﬁrst convolutional layer
of AlexNet trained on ImageNet exhibit oriented stripes,
similar to Gabor ﬁlters . When trained on a large-scale
diverse dataset, such as ImageNet, such kernels can be used
for describing generic low-level image characteristics. In
practice, we use all kernels (and their following nonlinear
activation) from the ﬁrst and second convolutional layers of
AlexNet pre-trained on ImageNet as our second choice of a
ﬁlter bank.
Image Descriptor
Let Ci(m, n) denote the response map
to the i-th convolutional kernel or Gabor ﬁlter in our ﬁlter
bank, and φi its histogram. To obtain more discriminative
histogram features, we ﬁrst obtain the upper bound hu
lower bound hl
i of the i-th response map by scanning the
entire target domain Dt. Then the interval hl
i is divided
into a set of small bins. We adaptively set the width of every histogram bin so that each of them contains a roughly
equal percentage of pixels. In this manner, we can avoid a
large percentage of pixels falling into the same bin. We concatenate the histograms of all ﬁlter response maps to form a
feature vector, φk =
φ1, φ2, , φD
, for image xk.
Nearest Neighbor Ranking
Given the histogram-based
descriptor of a training image xt
i in the target domain,
we search for its nearest neighbors in the source domain
Ds. Note that the number of kernels in different convolutional layers of AlexNet might be different. To ensure
equal weighting among different convolutional layers during nearest neighbor search, each histogram of kernel responses is normalized by the total number of kernels in the
corresponding layer. Thus the distance between the descriptor of a source image xs
j and that of a target image xt
computed as follows.
h ) + κ(φj,s
where wh = 1/Nh, Nh is the number of convolutional kernels in the corresponding layer, φi,t
are the hth histogram for images xt
j, and κ(·, ·) is the KLdivergence.
Hard Samples in the Target Domain
The labels of training samples in the target domain have varying degrees of
difﬁculty to satisfy. Intuitively, we would like to seek extra
help for those hard training samples in the target domain by
searching for more and more nearest neighbors in the source
domain. We propose an iterative scheme for this purpose.
We calculate the information entropy to measure the classi-
ﬁcation uncertainty of training samples in the target domain
after the m-th iteration as follows.
i,c log(pm
where C is the number of classes, pm
i,c is the probability
that the i-th training sample belongs to the c-th class after a
softmax layer in the m-th iteration.
Training samples that have high classiﬁcation uncertainty are considered hard training samples.
In the next
iteration, we increase the number of nearest neighbors of
the hard training samples as in Eq. (3.2), and continue ﬁnetuning the model trained in the current iteration. For a training sample xt
i in the target domain, the number of its nearest
neighbors in the next iteration is deﬁned as follows.
where σ0, σ1 and δ are constants, byt
i is predicted label of
is the number of nearest neighbors in the mth iteration. By changing the number of nearest neighbors
for samples in the target domain, the subset of the source
domain used as training data evolves over iterations, which
in turn gradually changes the feature representation learned
in the deep network. In the above equation, we typically set
δ = 0.1, σ0 = 4K0 and σ1 = 2K0, where K0 is the initial
number of nearest neighbors for all samples in the target
domain. In our experiments, we stop after ﬁve iterations.
In Table 1, we compare the effectiveness of Gabor ﬁlters and various combinations of kernels from AlexNet in
our selective joint ﬁne-tuning. In this experiment, we use
the 50-layer residual network with half of the convolutional kernels in the original architecture.
4. Experiments
4.1. Implementation
In all experiments, we use the 152-layer residual network
with identity mappings as the deep convolutional architecture, and conventional ﬁne-tuning performed on a pretrained network with the same architecture without using
Filter Bank
over all Accuracy(%)
Conv1-Conv2 in AlexNet
Conv1-Conv5 in AlexNet
Conv4-Conv5 in AlexNet
Gabor Filters
Fine-tuning w/o source domain
Table 1. A comparison of classiﬁcation performance on Oxford
Flowers 102 using various choices for the ﬁlter bank in selective
joint ﬁne-tuning.
any source datasets as our baseline. Note that the network
architecture we use is different from those used in most
published methods for the datasets we run experiments on,
and many existing methods adopt sophisticated parts models and feature encodings. The performance of such methods are still included in this paper to indicate that our simple
holistic method without incorporating parts models and feature encodings is capable of achieving state-of-the-art performance.
We use the pre-trained model released in to initialize the residual network. During selective joint ﬁne-turning,
source and target samples are mixed together in each minibatch. Once the data has passed the average pooling layer
in the residual network, we split the source and target samples, and send them to their corresponding softmax classi-
ﬁer layer respectively. Both the source and target classiﬁers
are initialized randomly.
We run all our experiments on a TITAN X GPU with
12GB memory. All training data is augmented as in 
ﬁrst, and we follow the training and testing settings in .
Every mini-batch can include 20 224×224 images using a
modiﬁed implementation of the residual network. We include randomly chosen samples from the target domain in a
mini-batch. Then for each of the chosen target sample, we
further include one of its retrieved nearest neighbors from
the source domain in the same mini-batch. We set the iter
size to 10 for each iteration in Caffe . The momentum parameter is set to 0.9 and the weight decay is 0.0001
in SGD. During selective joint ﬁne-tuning, the learning rate
starts from 0.01 and is divided by 10 after every 2400−5000
iterations in all the experiments. Most of the experiments
can ﬁnish in 16000 iterations.
4.2. Source Image Retrieval
We use the ImageNet ILSVRC 2012 training set as
the source domain for Stanford Dogs , Oxford Flowers , and Caltech 256 , and the combination of the
ImageNet and Places 205 training sets as the source
domain for MIT Indoor 67 . Fig. 2 shows the retrieved
1-st, 10-th, 20-th, 30-th, and 40-th nearest neighbors from
ImageNet or Places . It can be observed that corresponding source and target images share similar colors,
a.1 Chihuahua
a.2 Chihuahua
a.3 Beagle
a.5 Diaper
b.1 Pink Primrose
b.3 Capuchin
b.4 Measuring Cup
b.4 Butterfly
c.2 Horse Cart
c.4 Hard Disk
c.5 Snowmobile
d.1 Airport Inside
d.2 Restaurant
d.3 Restaurant
d.4 Butcher Shop
d.5 Coffee Mug
d.6 Grocery Store
e.2 Game Room
e.1 Airport Inside
e.3 Restaurant
e.4 Supermarket
e.5 Museum
Figure 2. Images in the source domain that have similar low-level characteristics with the target images. The ﬁrst column shows target
images from Stanford Dogs 120 , Oxford Flowers 102 , Caltech 256 , and MIT Indoor 67 . The following columns in rows
(a)-(d) are the corresponding 1st, 10-th, 20-th, 30-th and 40-th nearest images in ImageNet (source domain). The following columns in row
(e) are images retrieved from Places (source domain for MIT Indoor 67).
local patterns and global structures. Since low-level ﬁlter
bank responses do not encode strong semantic information,
the 50 nearest neighbors from a target domain include images from various and sometimes completely unrelated categories.
We ﬁnd out experimentally that there should be at least
200,000 retrieved images from the source domain. Too few
source images give rise to overﬁtting quickly. Therefore,
the initial number of retrieved nearest neighbors (K0) for
each target training sample is set to meet this requirement.
On the other hand, a surprising result is that setting K0 too
large would make the performance of the target learning
task drop signiﬁcantly. In our experiments, we set K0 to different values for Stanford Dogs (K0 = 100), Oxford Flowers (K0 = 300), Caltech 256 (K0 = 50 −100), and MIT
Indoor 67 (K0 = 100). Since there exists much overlap
among the nearest neighbors of different target samples, the
retrieved images typically do not cover the entire ImageNet
or Places datasets.
4.3. Fine-grained Object Recognition
Stanford Dogs 120. Stanford Dogs 120 contains 120
categories of dogs. There are 12000 images for training,
and 8580 images for testing. We do not use the parts information during selective joint ﬁne-tuning, and use the
commonly used mean class accuracy to evaluate the performance as in .
As shown in Table 2, the mean class accuracy achieved
by ﬁne-tuning the residual network using the training samples of this dataset only and without a source domain is
80.4%. It shows that the 152-layer residual network 
pre-trained on the ImageNet dataset has a strong generalization capability on this ﬁne-grained classiﬁcation task.
Using the entire ImageNet dataset during regular joint ﬁnetuning can improve the performance by 5.1%. When we
ﬁnally perform our proposed selective joint ﬁne-tuning using a subset of source domain images retrieved using histograms of low-level convolutional features, the performance is further improved to 90.2%, which is 9.8% higher
than the performance of conventional ﬁne-tuning without a
source domain and 4.3% higher than the result reported in
 , which expands the original target training set using
Google image search. This comparison demonstrates that
selective joint ﬁne-tuning can signiﬁcantly outperform conventional ﬁne-tuning.
Oxford Flowers 102. Oxford Flowers 102 consists of
102 ﬂower categories. 1020 images are used for training,
1020 for validation, and 6149 images are used for testing.
mean Acc(%)
HAR-CNN 
Local Alignment 
Multi scale metric learning 
MagNet 
Web Data + Original Data 
Training from scratch using target domain only
Selective joint training from scratch
Fine-tuning w/o source domain
Joint ﬁne-tuning with all source samples
Selective joint FT with random source samples
Selective joint FT w/o iterative NN retrieval
Selective joint FT with Gabor ﬁlter bank
Selective joint ﬁne-tuning
Selective joint FT with Model Fusion
Table 2. Classiﬁcation results on Stanford Dogs 120.
There are only 10 training images in each category.
As shown in Table 3, the mean class accuracy achieved
by conventional ﬁne-tuning using the training samples of
this dataset only and without a source domain is 92.3%. Selective joint ﬁne-tuning further improves the performance to
94.7%, 3.3% higher than previous best result from a single
network . To compare with previous state-of-the-art results obtained using an ensemble of different networks, we
also average the performance of multiple models obtained
during iterative source image retrieval for hard training samples in the target domain. Experiments show that the performance of our ensemble model is 95.8%, 1.3% higher than
previous best ensemble performance reported in . Note
mean Acc(%)
Multi-model Feature Concat 
MagNet 
VGG-19 + GoogleNet + AlexNet 
Training from scratch using target domain only
Selective joint training from scratch
Fine-tuning w/o source domain
Joint ﬁne-tuning with all source samples
Selective joint FT with random source samples
Selective joint FT w/o iterative NN retrieval
Selective joint FT with Gabor ﬁlter bank
Selective joint ﬁne-tuning
Selective joint FT with model fusion
VGG-19 + Part Constellation Model 
Selective joint FT with val set
Table 3. Classiﬁcation results on Oxford Flowers 102. The last two
rows compare performance using the validation set as additional
training data.
that Simon et al. used the validation set in this dataset
as additional training data. To verify the effectiveness of
our joint ﬁne-tuning strategy, we have also conducted experiments using this training setting and our result from a
single network outperforms that of by 1.7%.
4.4. General Object Recognition
Caltech 256. Caltech 256 has 256 object categories
and 1 background cluster class. In every category, there
are at least 80 images used for training, validation and testing. Researchers typically report results with the number
of training samples per class falling between 5 and 60. We
follow the testing procedure in to compare with stateof-the-art results.
We conduct four experiments with the number of training samples per class set to 15, 30, 45 and 60, respectively.
According to Table 4, in comparison to conventional ﬁnetuning without using a source domain, selective joint ﬁnetuning improves classiﬁcation accuracy in all four experiments, and the degree of improvement varies between 2.6%
and 4.1%. Performance improvement due to selective joint
ﬁne-tuning is more obvious when a smaller number of target
training image per class are used. This is because limited diversity in the target training data imposes a greater need to
seek help from the source domain. In most of these experiments, the classiﬁcation performance of our selective joint
ﬁne-tuning is also signiﬁcantly better than previous stateof-the-art results.
4.5. Scene Classiﬁcation
MIT Indoor 67. MIT Indoor 67 has 67 scene categories. In each category, there are 80 images for training
and 20 images for testing. Since MIT Indoor 67 is a scene
dataset, in addition to the ImageNet ILSVRC 2012 training
set , the Places-205 training set is also a potential
source domain. We compare three settings during slective
joint ﬁne-tuning: ImageNet as the source domain, Places as
the source domain, and the combination of both ImageNet
and Places as the source domain.
As shown in Table 5, the mean class accuracy of selective joint ﬁne-tuning with ImageNet as the source domain is 82.8%, 1.1% higher than that of conventional ﬁnetuning without using a source domain. Since ImageNet is
an object-centric dataset while MIT Indoor 67 is a scene
dataset, it is hard for training images in the target domain to
retrieve source domain images with similar low-level characteristics. But source images retrieved from ImageNet still
prevent the network from overﬁtting too heavily and help
achieve a performance gain. When the Places dataset serves
as the source domain, the mean class accuracy reaches
85.8%, which is 4.1% higher than the performance of ﬁnetuning without a source domain and 4.8% higher than previous best result from a single network . And the hybrid
mean Acc(%)
mean Acc(%)
mean Acc(%)
mean Acc(%)
Z. & F. Net 
VGG-19 
VGG-19 + GoogleNet +AlexNet 
VGG-19 + VGG-16 
Fine-tuning w/o source domain
Selective joint ﬁne-tuning
Table 4. Classiﬁcation results on Caltech 256.
mean Acc(%)
MetaObject-CNN 
MPP + DFSL 
VGG-19 + FV 
VGG-19 + GoogleNet 
Multi scale + multi model ensemble 
Fine-tuning w/o source domain
Selective joint FT with ImageNet(i)
Selective joint FT with Places(ii)
Selective joint FT with hybrid data(iii)
Average the output of (ii) and (iii)
Table 5. Classiﬁcation results on MIT Indoor 67.
source domain based on both ImageNet and Places does not
further improve the performance. Once averaging the output from the networks jointly ﬁne-tuned with Places and
the hybrid source domain, we obtain a classiﬁcation accuracy 0.9% higher than previous best result from an ensemble
model .
4.6. Ablation Study
We perform an ablation study on both Stanford Dogs 120
 and Oxford Flowers 102 by replacing or removing a single component from our pipeline. First, instead of
ﬁne-tuning, we perform training from scratch in two settings, one using the target domain only and the other using
selective joint training. Tables 2 and 3 show that while selective joint training obviously improves the performance,
it is still inferior than ﬁne-tuning pretrained networks. This
is because we only subsample a relatively small percentage
(20-30%) of the source data, which is still insufﬁcient to
train deep networks from scratch. Second, instead of using a
subset of retrieved training images from the source domain,
we simply use all training images in the source domain.
Joint ﬁne-tuning with the entire source domain decrease the
performance by 4.6% and 1.3% respectively. This demonstrates that using more training data from the source domain is not always better. On the contrary, using less but
more relevant data from the source domain is actually more
helpful. Third, instead of using a subset of retrieved training images, we use the same number of randomly chosen
training images from the source domain. Again, the performance drops by 4.7% and 1.5% respectively. Fourth, to validate the effectiveness of iteratively increasing the number
of retrieved images for hard training samples in the target
domain, we turn off this feature and only use the same number (K0) of retrieved images for all training samples in the
target domain. The performance drops by 1.9% and 0.5%
respectively. This indicates that our adaptive scheme for
hard samples is useful in improving the performance. Fifth,
we use convolutional kernels in the two bottom layers of a
pre-trained AlexNet as our ﬁlter bank. If we replace this ﬁlter bank with the Gabor ﬁlter bank, the overall performance
drops by 2.7% and 0.9% respectively, which indicates a ﬁlter bank learned from a diverse dataset could be more powerful than an analytically deﬁned one. Finally, if we perform conventional ﬁne-tuning without using a source domain, the performance drop becomes quite signiﬁcant and
reaches 9.8% and 2.4% respectively.
5. Conclusions
In this paper, we address deep learning tasks with insufﬁcient training data by introducing a new deep transfer learning scheme called selective joint ﬁne-tuning, which performs a target learning task with insufﬁcient training data
simultaneously with another source learning task with abundant training data. Different from previous work which directly adds extra training data to the target learning task,
our scheme borrows samples from a large-scale labeled
dataset for the source learning task, and do not require additional labeling effort beyond the existing datasets. Experiments show that our deep transfer learning scheme achieves
state-of-the-art performance on multiple visual classiﬁcation tasks with insufﬁcient training data for deep networks.
Nevertheless, how to ﬁnd the most suitable source domain
for a speciﬁc target learning task remains an open problem
for future investigation.
Acknowledgment
supported by Hong Kong Innovation and Technology Fund
(ITP/055/14LP).