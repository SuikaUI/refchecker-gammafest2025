EfÔ¨Åcient and Accurate Approximations of Nonlinear Convolutional Networks
Xiangyu Zhang1‚àó
Jianhua Zou1
Xiang Ming1‚àó
Kaiming He2
1Xi‚Äôan Jiaotong University
2Microsoft Research
This paper aims to accelerate the test-time computation
of deep convolutional neural networks (CNNs). Unlike existing methods that are designed for approximating linear
Ô¨Ålters or linear responses, our method takes the nonlinear
units into account. We minimize the reconstruction error
of the nonlinear responses, subject to a low-rank constraint
which helps to reduce the complexity of Ô¨Ålters. We develop
an effective solution to this constrained nonlinear optimization problem. An algorithm is also presented for reducing
the accumulated error when multiple layers are approximated. A whole-model speedup ratio of 4√ó is demonstrated
on a large network trained for ImageNet, while the top-5 error rate is only increased by 0.9%. Our accelerated model
has a comparably fast speed as the ‚ÄúAlexNet‚Äù , but is
4.7% more accurate.
1. Introduction
This paper addresses efÔ¨Åcient test-time computation of
deep convolutional neural networks (CNNs) . Since
the success of CNNs for large-scale image classiÔ¨Åcation, the accuracy of the newly developed CNNs has been continuously improving. However, the
computational cost of these networks (especially the more
accurate but larger models) also increases signiÔ¨Åcantly. The
expensive test-time evaluation of the models can make them
impractical in real-world systems. For example, a cloud service needs to process thousands of new requests per seconds; portable devices such as phones and tablets mostly
have CPUs or low-end GPUs only; some recognition tasks
like object detection are still time-consuming for
processing a single image even on a high-end GPU. For
these reasons and others, it is of practical importance to accelerate the test-time computation of CNNs.
There have been a few studies on approximating deep
CNNs for accelerating test-time evaluation . A
commonly used assumption is that the convolutional Ô¨Ålters
are approximately low-rank along certain dimensions. So
‚àóThis work is done when Xiangyu Zhang and Xiang Ming are interns
at Microsoft Research.
the original Ô¨Ålters can be approximately decomposed into
a series of smaller Ô¨Ålters, and the complexity is reduced.
These methods have shown promising speedup ratios on a
single or a few layers with some degradation of
The algorithms and approximations in the previous work
are developed for reconstructing linear Ô¨Ålters and
linear responses . However, the nonlinearity like the
RectiÔ¨Åed Linear Units (ReLU) is not involved in
their optimization. Ignoring the nonlinearity will impact
the quality of the approximated layers. Let us consider a
case that the Ô¨Ålters are approximated by reconstructing the
linear responses. Because the ReLU will follow, the model
accuracy is more sensitive to the reconstruction error of the
positive responses than to that of the negative responses.
Moreover, it is a challenging task of accelerating the
whole network (instead of just one or a very few layers).
The errors will be accumulated if several layers are approximated, especially when the model is deep. Actually, in the
recent work the approximations are applied on a single layer of large CNN models, such as those trained on
ImageNet . It is insufÔ¨Åcient for practical usage to
speedup one or a few layers, especially for the deeper models which have been shown very accurate .
In this paper, a method for accelerating nonlinear convolutional networks is proposed. It is based on minimizing
the reconstruction error of nonlinear responses, subject to a
low-rank constraint that can be used to reduce computation.
To solve the challenging constrained optimization problem,
we decompose it into two feasible subproblems and iteratively solve them. We further propose to minimize an asymmetric reconstruction error, which effectively reduces the
accumulated error of multiple approximated layers.
We evaluate our method on a 7-convolutional-layer
model trained on ImageNet. We investigate the cases of
accelerating each single layer and the whole model. Experiments show that our method is more accurate than the recent
method of Jaderberg et al.‚Äôs under the same speedup ratios. A whole-model speedup ratio of 4√ó is demonstrated,
and its degradation is merely 0.9%. When our model is accelerated to have a comparably fast speed as the ‚ÄúAlexNet‚Äù
 , our accuracy is 4.7% higher.
 
2. Approaches
2.1. Low-rank Approximation of Responses
Our observation is that the response at a position of a
convolutional feature map approximately lies on a low-rank
subspace. The low-rank decomposition can reduce the complexity.
To Ô¨Ånd the approximate low-rank subspace, we
minimize the reconstruction error of the responses.
More formally, we consider a convolutional layer with a
Ô¨Ålter size of k √ó k √ó c, where k is the spatial size of the
Ô¨Ålter and c is the number of input channels of this layer. To
compute a response, this Ô¨Ålter is applied on a k √ó k √ó c
volume of the layer input. We use x ‚ààRk2c+1 to denote a
vector that reshapes this volume (appending one as the last
entry for the bias). A response y ‚ààRd at a position of a
feature map is computed as:
where W is a d-by-(k2c+1) matrix, and d is the number
of Ô¨Ålters. Each row of W denotes the reshaped form of a
k √ó k √ó c Ô¨Ålter (appending the bias as the last entry). We
will address the nonlinear case later.
If the vector y is on a low-rank subspace, we can write
y = M(y ‚àí¬Øy) + ¬Øy, where M is a d-by-d matrix of a rank
d‚Ä≤ < d and ¬Øy is the mean vector of responses. Expanding
this equation, we can compute a response by:
y = MWx + b,
where b = ¬Øy ‚àíM¬Øy is a new bias. The rank-d‚Ä≤ matrix
M can be decomposed into two d-by-d‚Ä≤ matrices P and Q
such that M = PQ‚ä§. We denote W‚Ä≤ = Q‚ä§W as a d‚Ä≤-by-
(k2c+1) matrix, which is essentially a new set of d‚Ä≤ Ô¨Ålters.
Then we can compute (2) by:
y = PW‚Ä≤x + b.
The complexity of using Eqn.(3) is O(d‚Ä≤k2c) + O(dd‚Ä≤) ,
while the complexity of using Eqn.(1) is O(dk2c).
many typical models/layers, we usually have O(dd‚Ä≤) ‚â™
O(d‚Ä≤k2c), so the computation in Eqn.(3) will reduce the
complexity to about d‚Ä≤/d.
Fig. 1 illustrates how to use Eqn.(3) in a network. We
replace the original layer (given by W) by two layers (given
by W‚Ä≤ and P). The matrix W‚Ä≤ is actually d‚Ä≤ Ô¨Ålters whose
sizes are k √ó k √ó c. These Ô¨Ålters produce a d‚Ä≤-dimensional
feature map. On this feature map, the d-by-d‚Ä≤ matrix P can
be implemented as d Ô¨Ålters whose sizes are 1 √ó 1 √ó d‚Ä≤. So
P corresponds to a convolutional layer with a 1√ó1 spatial
support, which maps the d‚Ä≤-dimensional feature map to a
d-dimensional one. The usage of 1 √ó 1 spatial Ô¨Ålters to
adjust dimensions has been adopted for designing network
architectures . But in those papers, the 1 √ó 1 Ô¨Ålters
ùëë‚Ä≤ channels
Figure 1. Illustration of the approximation. (a) An original layer
with complexity O(dk2c). (b) An approximated layer with complexity reduced to O(d‚Ä≤k2c) + O(dd‚Ä≤).
are used to reduce dimensions, while in our case they restore
dimensions.
Note that the decomposition of M = PQ‚ä§can be arbitrary. It does not impact the value of y computed in Eqn.(3).
A simple decomposition is the Singular Vector Decomposition (SVD) : M = Ud‚Ä≤Sd‚Ä≤Vd‚Ä≤‚ä§, where Ud‚Ä≤ and Vd‚Ä≤ are
d-by-d‚Ä≤ column-orthogonal matrices and Sd‚Ä≤ is a d‚Ä≤-by-d‚Ä≤
diagonal matrix. Then we can obtain P = Ud‚Ä≤S1/2
Q = Vd‚Ä≤S1/2
In practice the low-rank assumption is an approximation,
and the computation in Eqn.(3) is approximate. To Ô¨Ånd an
approximate low-rank subspace, we optimize the following
‚à•(yi ‚àí¬Øy) ‚àíM(yi ‚àí¬Øy)‚à•2
rank(M) ‚â§d‚Ä≤.
Here yi is a response sampled from the feature maps in the
training set. This problem can be solved by SVD or actually Principal Component Analysis (PCA): let Y be the dby-n matrix concatenating n responses with the mean subtracted, compute the eigen-decomposition of the covariance
matrix YY‚ä§= USU‚ä§where U is an orthogonal matrix
and S is diagonal, and M = Ud‚Ä≤Ud‚Ä≤‚ä§where Ud‚Ä≤ are the
Ô¨Årst d‚Ä≤ eigenvectors. With the matrix M computed, we can
Ô¨Ånd P = Q = Ud‚Ä≤.
How good is the low-rank assumption of the responses?
We sample the responses from a CNN model (with 7 convolutional layers, detailed in Sec. 3) trained on ImageNet
 . For the responses of a convolutional layer (from 3,000
randomly sampled training images), we compute the eigenvalues of their covariance matrix and then plot the sum of
the largest eigenvalues (Fig. 2). We see that substantial energy is in a small portion of the largest eigenvectors. For
example, in the Conv2 layer (d = 256) the Ô¨Årst 128 eigenvectors contribute over 99.9% energy; in the Conv7 layer
PCA Accumulative Energy (%)
Figure 2. PCA accumulative energy of the responses in each layer, presented as the sum of largest d‚Ä≤ eigenvalues (relative to the total
energy when d‚Ä≤ = d). Here the Ô¨Ålter number d is 96 for Conv1, 256 for Conv2, and 512 for Conv3-7 (detailed in Table 1).
(d = 512), the Ô¨Årst 256 eigenvectors contribute over 95%
energy. This indicates that we can use a fraction of the Ô¨Ålters to precisely approximate the original Ô¨Ålters.
The low-rank behavior of the responses y is because
of the low-rank behaviors of the Ô¨Ålters W and the inputs
While the low-rank assumptions of Ô¨Ålters have been
adopted in recent work , we further adopt the lowrank assumptions of the Ô¨Ålter input x, which is a local volume and should have correlations. The responses y will
have lower rank than W and x, so the approximation can be
more precise. In our optimization (4), we directly address
the low-rank subspace of y.
2.2. The Nonlinear Case
Next we investigate the case of using nonlinear units.
We use r(¬∑) to denote the nonlinear operator. In this paper we focus on the RectiÔ¨Åed Linear Unit (ReLU) :
r(¬∑) = max(¬∑, 0). A nonlinear response is given by r(Wx)
or simply r(y). We minimize the reconstruction error of the
nonlinear responses:
‚à•r(yi) ‚àír(Myi + b)‚à•2
rank(M) ‚â§d‚Ä≤.
Here b is a new bias to be optimized, and r(My + b) =
r(MWx + b) is the nonlinear response computed by the
approximated Ô¨Ålters.
The above problem is challenging due to the nonlinearity
and the low-rank constraint. To Ô¨Ånd a feasible solution, we
relax it as:
‚à•r(yi) ‚àír(zi)‚à•2
2 + Œª‚à•zi ‚àí(Myi + b)‚à•2
rank(M) ‚â§d‚Ä≤.
Here {zi} is a set of auxiliary variables of the same size as
{yi}. Œª is a penalty parameter. If Œª ‚Üí‚àû, the solution
to (6) will converge to the solution to (5) . We adopt an
alternating solver, Ô¨Åxing {zi} and solving for M, b and vice
(i) The subproblem of M, b. In this case, {zi} are Ô¨Åxed. It
is easy to show b = ¬Øz ‚àíM¬Øy where ¬Øz is the sample mean of
{zi}. Substituting b into the objective function, we obtain
the problem involving M:
‚à•(zi ‚àí¬Øz) ‚àíM(yi ‚àí¬Øy)‚à•2
rank(M) ‚â§d‚Ä≤.
Let Z be the d-by-n matrix concatenating the vectors of
{zi ‚àí¬Øz}. We rewrite the above problem as:
M ‚à•Z ‚àíMY‚à•2
rank(M) ‚â§d‚Ä≤.
Here ‚à•¬∑ ‚à•F is the Frobenius norm. This optimization problem is a Reduced Rank Regression problem , and
it can be solved by a kind of Generalized Singular Vector
Decomposition (GSVD) . The solution is as follows. Let ÀÜM = ZY‚ä§(YY‚ä§)‚àí1. The GSVD is applied on ÀÜM
as ÀÜM = USV‚ä§, such that U is a d-by-d orthogonal matrix
satisfying U‚ä§U = Id where Id is a d-by-d identity matrix,
and V is a d-by-d matrix satisfying V‚ä§YY‚ä§V = Id (called
generalized orthogonality). Then the solution M to (8) is
given by M = Ud‚Ä≤Sd‚Ä≤Vd‚Ä≤‚ä§where Ud‚Ä≤ and Vd‚Ä≤ are the Ô¨Årst
d‚Ä≤ columns of U and V and Sd‚Ä≤ are the largest d‚Ä≤ singular
values. We can further show that if Z = Y (so the problem
in (7) becomes (4)), this solution degrades to computing the
eigen-decomposition of YY‚ä§.
(ii) The subproblem of {zi}. In this case, M and b are
Ô¨Åxed. Then in this subproblem each element zij of each
vector zi is independent of any other. So we solve a 1dimensional optimization problem as follows:
zij (r(yij) ‚àír(zij))2 + Œª(zij ‚àíy‚Ä≤
ij is the j-th entry of Myi + b. We can separately
consider zij ‚â•0 and zij < 0 and remove the ReLU operator. Then we can derive the solution as follows: let
ij = min(0, y‚Ä≤
ij = max(0, Œª ¬∑ y‚Ä≤
ij + r(yij)
then zij = z
ij gives a smaller value in (9) than z
and otherwise zij = z
Although we focus on the ReLU, our method is applicable for other types of nonlinearities. The subproblem in
(9) is a 1-dimensional nonlinear least squares problem, so
can be solved by gradient descent or simply line search. We
plan to study this issue in the future.
We alternatively solve (i) and (ii). The initialization is
given by the solution to the linear case (4). We warm up the
solver by setting the penalty parameter Œª = 0.01 and run
25 iterations. Then we increase the value of Œª. In theory, Œª
should be gradually increased to inÔ¨Ånity . But we Ô¨Ånd
that it is difÔ¨Åcult for the iterative solver to make progress if
Œª is too large. So we increase Œª to 1, run 25 more iterations,
and use the resulting M as our solution. Then we compute
P and Q by SVD on M.
2.3. Asymmetric Reconstruction for Multi-Layer
To accelerate a whole network, we apply the above
method sequentially on each layer, from the shallow layers to the deeper ones. If a previous layer is approximated,
its error can be accumulated when the next layer is approximated. We propose an asymmetric reconstruction method
to address this issue.
Let us consider a layer whose input feature map is
not precise due to the approximation of the previous
layer/layers. We denote the approximate input to the current layer as ÀÜx. For the training samples, we can still compute its non-approximate responses as y = Wx. So we can
optimize an ‚Äúasymmetric‚Äù version of (5):
‚à•r(Wxi) ‚àír(MWÀÜxi + b)‚à•2
rank(M) ‚â§d‚Ä≤.
Here in the Ô¨Årst term xi is the non-approximate input, while
in the second term ÀÜxi is the approximate input due to the
previous layer. We need not use ÀÜxi in the Ô¨Årst term, because r(Wxi) is the real outcome of the original network
and thus is more precise. On the other hand, we do not use
xi in the second term, because r(MWÀÜxi + b) is the actual operation of the approximated layer. This asymmetric
version can reduce the accumulative errors when multiple
layers are approximated. The optimization problem in (12)
can be solved using the same algorithm as for (5).
PCA Accumulative Energy (%)
‚àÜ Accuracy (%)
Figure 3. PCA accumulative energy and the accuracy rates (top-
5). Here the accuracy is evaluated using the linear solution (the
nonlinear solution has a similar trend). Each layer is evaluated
independently, with other layers not approximated. The accuracy
is shown as the difference to no approximation.
2.4. Rank Selection for Whole-Model Acceleration
In the above, the optimization is based on a target d‚Ä≤ of
each layer. d‚Ä≤ is the only parameter that determines the complexity of an accelerated layer. But given a desired speedup
ratio of the whole model, we need to determine the proper
rank d‚Ä≤ used for each layer.
Our strategy is based on an empirical observation that the
PCA energy is related to the classiÔ¨Åcation accuracy after approximations. To verify this observation, in Fig. 3 we show
the classiÔ¨Åcation accuracy (represented as the difference to
no approximation) vs. the PCA energy. Each point in this
Ô¨Ågure is empirically evaluated using a value of d‚Ä≤. 100%
energy means no approximation and thus no degradation of
classiÔ¨Åcation accuracy. Fig. 3 shows that the classiÔ¨Åcation
accuracy is roughly linear on the PCA energy.
To simultaneously determine the rank for each layer, we
further assume that the whole-model classiÔ¨Åcation accuracy
is roughly related to the product of the PCA energy of all
layers. More formally, we consider this objective function:
Here œÉl,a is the a-th largest eigenvalue of the layer l, and
a=1 œÉl,a is the PCA energy of the largest d‚Ä≤
l eigenvalues
in the layer l. The product Q
l is over all layers to be approximated. The objective E is assumed to be related to
the accuracy of the approximated whole network. Then we
optimize this problem:
Ô¨Ålter size
# channels
output size
complexity (%)
# of zeros
Table 1. The architecture of the model. Each convolutional layer is followed by ReLU. The Ô¨Ånal convolutional layer is followed by a spatial
pyramid pooling layer that have 4 levels ({6 √ó 6, 3 √ó 3, 2 √ó 2, 1 √ó 1}, totally 50 bins). The resulting 50 √ó 512-d is fed into the 4096-d
fc layer (fc6), followed by another 4096-d fc layer (fc7) and a 1000-way softmax layer. The convolutional complexity is the theoretical
time complexity, shown as relative numbers to the total convolutional complexity. The (relative) number of zeros is the calculated on the
responses of the layer, which shows the ‚Äúsparsity‚Äù of the layer.
Here dl is the original number of Ô¨Ålters in the layer l, and
Cl is the original time complexity of the layer l. So d‚Ä≤
is the complexity after the approximation. C is the total
complexity after the approximation, which is given by the
desired speedup ratio. This problem means that we want
to maximize the accumulated accuracy subject to the time
complexity constraint.
The problem in (14) is a combinatorial problem .
So we adopt a greedy strategy to solve it. We initialize
l as dl, and consider the set {œÉl,a}.
In each step we
remove an eigenvalue œÉl,d‚Ä≤
l from this set, chosen from a
certain layer l. The relative reduction of the objective is
‚ñ≥E/E = œÉl,d‚Ä≤/Pd‚Ä≤
a=1 œÉl,a, and the reduction of complexity is ‚ñ≥C =
dl Cl. Then we deÔ¨Åne a measure as ‚ñ≥E/E
The eigenvalue œÉl,d‚Ä≤
l that has the smallest value of this measure is removed. Intuitively, this measure favors a small reduction of ‚ñ≥E/E and a large reduction of complexity ‚ñ≥C.
This step is greedily iterated, until the constraint of the total
complexity is achieved.
2.5. Discussion
In our formulation, we focus on reducing the number of
Ô¨Ålters (from d to d‚Ä≤). There are algorithmic advantages of
operating on the ‚Äúd‚Äù dimension. Firstly, this dimension can
be easily controlled by the rank constraint rank(M) ‚â§d‚Ä≤.
This constraint enables closed-form solutions, e.g., PCA to
the problem (4) or GSVD to the subproblem (7). Secondly,
the optimized low-rank projection M can be exactly decomposed into low-dimensional Ô¨Ålters (P and Q) by SVD.
These simple and close-form solutions can produce good
results using a very small subset of training images (3,000
out of one million).
3. Experiments
We evaluate on the ‚ÄúSPPnet (Overfeat-7)‚Äù model ,
which is one of the state-of-the-art models for ImageNet
Large Scale Visual Recognition Challenge (ILSVRC) 2014
 . This model (detailed in Table 1) has a similar architecture to the Overfeat model , but has 7 convolutional
layers. A spatial pyramid pooling layer is used after the
last convolutional layer, which improves the classiÔ¨Åcation
accuracy. We train the model on the 1000-class dataset of
ImageNet 2012 , following the details in .
We evaluate the ‚Äútop-5 error‚Äù (or simply termed as ‚Äúerror‚Äù) using single-view testing.
The view is the center
224 √ó 224 region cropped from the resized image whose
shorter side is 256. The single-view error rate of the model
is 12.51% on the ImageNet validation set, and the increased
error rates of the approximated models are all based on this
number. For completeness, we report that this model has
11.1% error using 10-view test and 9.3% using 98-view test.
We use this model due to the following reasons. First,
its architecture is similar to many existing models (such as the Ô¨Årst/second layers and the cascade usage of 3√ó3 Ô¨Ålters), so we believe most observations should
be valid on other models. Second, on the other hand, this
model is deep (7-conv.) and the computation is more uniformly distributed among the layers (see ‚Äúcomplexity‚Äù in
Table 1). A similar behavior exhibits on the compelling
VGG-16/19 models . The uniformly distributed computation indicates that most layers should be accelerated for
an overall speedup.
For the training of the approximations as in (4), (6), and
(12), we randomly sample 3,000 images from the ImageNet
training set and use their responses as the training samples.
3.1. Single-Layer: Linear vs. Nonlinear
In this subsection we evaluate the single-layer performance. When evaluating a single approximated layer, the
rest layers are unchanged and not approximated.
speedup ratio (involving that single layer only) is shown as
the theoretical ratio computed by the complexity.
In Fig. 4 we compare the performance of our linear so-
Conv1 Speedup
Increase in Error (%)
Conv2 Speedup
Conv3 Speedup
Conv4 Speedup
Conv5 Speedup
Increase in Error (%)
Conv6 Speedup
Conv7 Speedup
Figure 4. Linear vs. Nonlinear: single-layer performance of accelerating Conv1 to Conv7. The speedup ratios are computed by the
theoretical complexity, but is nearly the same as the actual speedup ratios in our CPU/GPU implementation. The error rates are top-5
single-view, and shown as the increase of error rates compared with no approximation (smaller is better).
(b) 3‚àílayer (Conv2, 3 and 4)
(a) 2‚àílayer (Conv6 and 7)
Increase of Error (%)
(c) 3‚àílayer (Conv5, 6 and 7)
Asymmetric
Asymmetric
Asymmetric
Figure 5. Symmetric vs. Asymmetric: the cases of 2-layer and 3-layer approximation. The speedup is computed by the complexity of the
layers approximated. (a) Approximation of Conv6 & 7. (b) Approximation of Conv2, 3 & 4. (c) Approximation of Conv5, 6 & 7.
lution (4) and nonlinear solution (6). The performance is
displayed as increase of error rates (decrease of accuracy)
vs. the speedup ratio of that layer. Fig. 4 shows that the
nonlinear solution consistently performs better than the linear solution. In Table 1, we show the sparsity (the portion
of zero activations after ReLU) of each layer. A zero activation is due to the truncation of ReLU. The sparsity is over
60% for Conv2-7, indicating that the ReLU takes effect on a
substantial portion of activations. This explains the discrepancy between the linear and nonlinear solutions. Especially,
the Conv7 layer has a sparsity of 95%, so the advantage of
the nonlinear solution is more obvious.
Fig. 4 also shows that when accelerating only a single
layer by 2√ó, the increased error rates of our solutions are
rather marginal or ignorable. For the Conv2 layer, the error
rate is increased by < 0.1%; for the Conv3-7 layers, the
error rate is increased by < 0.2%.
We also notice that for Conv1, the degradation is ignorable on or below 2√ó speedup (1.8√ó corresponds to
d‚Ä≤ = 32). This can be explained by Fig. 2(a): the PCA
energy has almost no loss when d‚Ä≤ ‚â•32. But the degradation can grow quickly for larger speedup ratios, because in
this layer the channel number c = 3 is small and d‚Ä≤ needs
to be reduced drastically to achieve the speedup ratio. So in
the following, we will use d‚Ä≤ = 32 for Conv1.
3.2. Multi-Layer: Symmetric vs. Asymmetric
Next we evaluate the performance of asymmetric reconstruction as in the problem (12). We demonstrate approximating 2 layers or 3 layers. In the case of 2 layers, we show
the results of approximating Conv6 and 7; and in the case
of 3 layers, we show the results of approximating Conv5-7
Table 2. Whole-model acceleration with/without rank selection. The speedup ratios shown here involve all convolutional layers (Conv1-
Conv7). We Ô¨Åx d‚Ä≤ = 32 in Conv1. In the case of no rank selection, the speedup ratio of each other layer is the same. The solver is the
asymmetric version. Each column of Conv1-7 shows the rank d‚Ä≤ used, which is the number of Ô¨Ålters after approximation. The error rates
are top-5 single-view, and shown as the increase of error rates compared with no approximation (smaller is better).
or Conv2-4. The comparisons are consistently observed for
other cases of multi-layer.
We sequentially approximate the layers involved, from
a shallower one to a deeper one. In the asymmetric version (12), ÀÜx is from the output of the previous approximated layer (if any), and x is from the output of the previous non-approximate layer. In the symmetric version (5),
the response y = Mx where x is from the output of the
previous non-approximate layer. We have also tried another
symmetric version of y = MÀÜx where ÀÜx is from the output
of the previous approximated layer (if any), and found this
symmetric version is even worse.
Fig. 5 shows the comparisons between the symmetric
and asymmetric versions. The asymmetric solution has signiÔ¨Åcant improvement over the symmetric solution. For example, when only 3 layers are approximated simultaneously
(like Fig. 5 (c)), the improvement is over 1.0% when the
speedup is 4√ó. This indicates that the accumulative error
rate due to multi-layer approximation can be effectively reduced by the asymmetric version.
When more and all layers are approximated simultaneously (as below), if without the asymmetric solution, the
error rates will increase more drastically.
3.3. Whole-Model: with/without Rank Selection
In Table 2 we show the results of whole-model acceleration. The solver is the asymmetric version. For Conv1,
we Ô¨Åx d‚Ä≤ = 32. For other layers, when the rank selection is
not used, we adopt the same speedup ratio on each layer and
determine its desired rank d‚Ä≤ accordingly. When the rank selection is used, we apply it to select d‚Ä≤ for Conv2-7. Table 2
shows that the rank selection consistently outperforms the
counterpart without rank selection. The advantage of rank
selection is observed in both linear and nonlinear solutions.
In Table 2 we notice that the rank selection often chooses
a higher rank d‚Ä≤ (than the no rank selection) in Conv5-7.
For example, when the speedup is 3√ó, the rank selection
assigns d‚Ä≤ = 167 to Conv7, while this layer only requires
d‚Ä≤ = 153 to achieve 3√ó single-layer speedup of itself. This
can be explained by Fig. 2(c). The energy of Conv5-7 is less
concentrated, so these layers require higher ranks to achieve
good approximations.
3.4. Comparisons with Previous Work
We compare with Jaderberg et al.‚Äôs method , which
is a recent state-of-the-art solution to efÔ¨Åcient evaluation.
This method mainly operates on the spatial domain. It decomposes a k√ók spatial support into a cascade of k√ó1 and
1√ók spatial supports. This method focuses on the linear reconstruction error. The SGD solver is adopted for optimization. In the paper of , their method is only evaluated on
a single layer of a model trained for ImageNet.
Our comparisons are based on our re-implementation of
 . We use the Scheme 2 decomposition in and its
Ô¨Ålter reconstruction version, which is the one used for ImageNet as in . Our re-implementation of gives a
2√ó single-layer speedup on Conv2 and < 0.2% increase
of error. As a comparison, in it reports 0.5% increase
of error on Conv2 under a 2√ó single-layer speedup, evaluated on another Overfeat model . For whole-model
speedup, we adopt this method sequentially on Conv2-7 using the same speedup ratio. We do not apply this method on
Conv1, because this layer has a small fraction of complexity
while the spatial decomposition leads to considerable error
on this layer if using a speedup ratio similar to other layers.
In Fig. 6 we compare our method with Jaderberg et
al.‚Äôs for whole-model speedup.
The speedup ratios
are the theoretical complexity ratios involving all convolutional layers. Our method is the asymmetric version and
with rank selection (denoted as ‚Äúour asymmetric‚Äù). Fig. 6
shows that when the speedup ratios are large (4√ó and 5√ó),
our method outperforms Jaderberg et al.‚Äôs method signif-
Speedup Ratio
Increase of Error (%)
Jaderberg et al. (our impl.)
Our asymmetric
Our asymmetric (3d)
Figure 6. Comparisons with Jaderberg et al.‚Äôs spatial decomposition method . The error rates are top-5 single-view, and shown
as the increase of error rates compared with no approximation
(smaller is better).
icantly. For example, when the speedup ratio is 4√ó, the
increased error rate of our method is 4.2%, while Jaderberg
et al.‚Äôs is 6.0%. Jaderberg et al.‚Äôs result degrades quickly
when the speedup ratio is getting large, while ours degrades
more slowly. This is indicates the effects of our method for
reducing accumulative error. In our CPU implementation,
both methods have similar actual speedup ratios for a given
theoretical speedup, for example, 3.55√ó actual for 4√ó theoretical speedup. It is because the overhead for both methods
mainly comes from the fully-connected and other layers.
Because our asymmetric solution can effectively reduce
the accumulated error, we can approximate a layer by the
two methods simultaneously, and the asymmetric reconstruction of the next layer can reduce the error accumulated
by the two methods. As discussed in Sec. 2.5, our method
is based on the channel dimension (d), while Jaderberg et
al.‚Äôs method mainly exploits the decomposition of the two
spatial dimensions. These two mechanisms are complementary, so we conduct the following sequential strategy. The
Conv1 layer is approximated using our model only. Then
for the Conv2 layer, we Ô¨Årst apply our method. The approximated layer has d‚Ä≤ Ô¨Ålters whose sizes are k√ók√óc followed
by 1 √ó 1 Ô¨Ålters (as in Fig. 1(b)). Next we apply Jaderberg
et al.‚Äôs method to decompose the spatial support into a cascade of k √ó 1 and 1 √ó k Ô¨Ålters (Scheme 2 ). This gives a
3-dimensional approximation of Conv2. Then we apply our
method on Conv3. Now the asymmetric solver will take
the responses approximated by the two mechanisms as the
input, while the reconstruction target is still the responses
of the original network. So while Conv2 has been approximated twice, the asymmetric solver of Conv3 can partially
reduce the accumulated error. This process is sequentially
adopted in the layers that follow.
In Fig. 6 we show the results of this 3-dimensional decomposition strategy (denoted as ‚Äúour asymmetric (3d)‚Äù).
top-5 err.
top-5 err.
AlexNet 
(Overfeat-7)
our asym., 4√ó
our asym. (3d), 4√ó
Table 3. Comparisons of network performance. The top-5 error is
absolute values (not the increased number). The running time is
per view on a CPU (single thread, with SSE).
We set the speedup ratios of both mechanisms to be equal:
e.g., if the speedup ratio of the whole model is r√ó, then we
use ‚àör√ó for both. Fig. 6 shows that this strategy leads to
signiÔ¨Åcantly smaller increase of error. For example, when
the speedup is 5√ó, the error is increased by only 2.5%. This
is because the speedup ratio is accounted by all three dimensions, and the reduction of each dimension is lower. Our
asymmetric solver effectively controls the accumulative error even if the multiple layers are decomposed extensively.
Finally, we compare the accelerated whole model with
the well-known ‚ÄúAlexNet‚Äù . The comparison is based
on our re-implementation of AlexNet. The architecture is
the same as in except that the GPU splitting is ignored.
Besides the standard strategies used in , we train this
model using the 224√ó224 views cropped from resized images whose shorter edge is 256 . Our re-implementation
of this model has top-5 single-view error rate as 18.8% (10view top-5 16.0% and top-1 37.6%). This is better than the
one reported in 1.
Table 3 shows the comparisons on the accelerated models and AlexNet. The error rates in this table are the absolute
value (not the increased number). The time is the actual running time per view, on a C++ implementation and Intel i7
CPU (2.9GHz). The model accelerated by our asymmetric
solver (channel-only) has 16.7% error, and by our asymmetric solver (3d) has 14.1% error. This means that the accelerated model is 4.7% more accurate than AlexNet, while its
speed is nearly the same as AlexNet.
As a common practice , we also evaluate the 10-view
score of the models. Our accelerated model achieves 12.0%
error, which means only 0.9% increase of error with 4√ó
speedup (the original one has 11.1% 10-view error).
4. Conclusion and Future Work
On the core of our algorithm is the low-rank constraint.
While this constraint is designed for speedup in this work,
it can be considered as a regularizer on the convolutional
Ô¨Ålters. We plan to investigate this topic in the future.
1In the 10-view error is top-5 18.2% and top-1 40.7%.