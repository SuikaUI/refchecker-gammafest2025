Learning phase transitions by confusion
Evert P.L. van Nieuwenburg*, Ye-Hua Liu, and Sebastian D. Huber
Institute for Theoretical Physics, ETH Zurich, 8093 Z¨urich, Switzerland
Classifying phases of matter is a central problem in physics. For quantum mechanical systems,
this task can be daunting owing to the exponentially large Hilbert space. Thanks to the available
computing power and access to ever larger data sets, classiﬁcation problems are now routinely solved
using machine learning techniques. Here, we propose to use a neural network based approach to
ﬁnd phase transitions depending on the performance of the neural network after training it with
deliberately incorrectly labelled data. We demonstrate the success of this method on the topological
phase transition in the Kitaev chain, the thermal phase transition in the classical Ising model, and
the many-body-localization transition in a disordered quantum spin chain. Our method does not
depend on order parameters, knowledge of the topological content of the phases, or any other speciﬁcs
of the transition at hand. It therefore paves the way to a generic tool to identify unexplored phase
transitions.
INTRODUCTION
Machine learning as a tool for analyzing data is becoming more and more prevalent in an increasing number of
ﬁelds . This is due to a combination of availability of
large amounts of data, and the advances in hardware and
computational power (most notably through the use of
Two typical methods of machine learning can be distinguished, namely the unsupervised and supervised methods. In the former, the algorithm receives no input other
than the data and is asked, e.g., to extract features or
to cluster the samples. Such an unsupervised approach
was applied to the physical problem of identifying phase
transitions and order parameters, from images of classical conﬁgurations of Ising models . In the supervised
learning methods, the data has to be supplemented by a
labelling. Typical problems in this direction involve the
classiﬁcation of many samples, where each sample is assigned a class-label. The machine is trained to recognize
samples and predict their associated label, demonstrating
that it has learned by doing so via correctly predicting
samples it has not encountered before. This approach,
too, has been demonstrated on Ising models . Such approaches have also recently provided promising prospects
for strongly correlated fermions and the fermion
sign problem .
Last, we mention a slightly orthogonal approach based on reinforcement learning, where
the wavefunction was represented as a particular type
of artiﬁcial neural network . A variational approach
then allows for ﬁnding groundstates and performing timeevolution, and outperformed other state-of-the-art numerical methods for the two-dimensional Heisenberg spin
model. Some topological states have been shown to have
eﬃcient and exact representations as an artiﬁcial neural
network .
Conversely, concepts from physics have also found their
way into the ﬁeld of machine learning. Examples of this
are e.g. the relations between neural networks and statistical Ising models and renormalisation ﬂow , the use of
tensor network techniques to train them , and indeed
the very concept of phase transitions themselves .
Motivated by previous studies, we apply machinelearning techniques to the detection of phase transitions.
In contrast to the previous works, however, we focus on a
combination of supervised and unsupervised techniques.
In most cases namely, it is exactly the labelling that one
would like to ﬁnd out (i.e. classiﬁcation of phases). That
implies that a labelling is not known beforehand, and
hence supervised techniques are not directly applicable.
In this Letter we demonstrate that it is possible to ﬁnd
the correct labels, by purposefully mislabelling the data
and evaluating the performance of the machine learner.
We will base our method on neural networks, which are
capable of ﬁtting arbitrary non-linear functions . Indeed, if a linear feature extraction method worked, there
would have been no need to explicitly ﬁnd labels in the
ﬁrst place.
For quantum phase transitions, one tries to learn the
quantum-mechanical wavefunction |ψ⟩, which contains
exponentially many coeﬃcients with increasing system
size. As has been noted before , a similar problem exists in the ﬁeld of machine learning: the number of samples in a dataset has to increase exponentially with the
number of features one is trying to extract. To prevent
having to deal with exponentially large wavefunctions,
we pre-process the data in the form of the entanglement
spectrum (ES) , which has been shown to contain important information about |ψ⟩ , although care has to
be taken when interpreting phase transitions .
To justify the use of the ES, we note that recently
the quantum entanglement has taken up a major role
in the characterization of many-body quantum systems
 . In particular, the ES has been used as an important tool in, for example, ﬁngerprinting topological order
 , tensor network properties , quantum critical points, symmetry breaking phases , and even
many-body localization . Very recently, an experimental protocol for measuring the ES has been proposed
 . On the level of the ES, the information of phases is
not clearly identiﬁable as in the classical images, which
we will show in the following sections. However, patterns
in the ES suggest that learning and generalization is still
 
Neuron output
FIG. 1. (a) Evolution of the entanglement spectrum of the Kitaev chain as a function of the chemical potential µ. Here we plot
the largest four eigenvalues of the reduced density matrix ρA. The degeneracy structure is clearly observable. (b) Principal
component analysis of the entanglement spectrum. All data points are shown in the plane of the ﬁrst two principal components
y1 and y2. (c) Supervised learning with blanking. The shaded region is blanked out during the training phase, and the neural
network can still predict the correct transition point µ = −2t. (d) P(µ′
c), evolution of the accuracy of prediction, as a function
of the proposed critical point µ′
c, which shows the universal W-shape. See text for more details. (Parameters for training:
batch size Nb = 100, learning rate α = 0.075 and regularization l2 = 0.001.)
In the following, we will ﬁrst consider the Kitaev chain
as a demonstration of our method.
The Kitaev chain
serves as an excellent example since analytical results
are available, and the ES shows a clear distinction between the two phases of the model . We demonstrate
the generalizing power of the neural network by blanking
out the training data around the transition, and show
that it can still predict the transition accurately.
then purposefully mislabel the data, thereby confusing
the network, and introduce the characteristic shape of
the networks’ performance function. Next, this confusion
method is demonstrated on a classical two-dimensional
Ising model to show that it is not speciﬁc to the Kitaev model.
Last, we apply the proposed method to
the nontrivial task of ﬁnding the many-body-localization
(MBL) transition from many disordered samples of a onedimensional spin chain. We ﬁnish with a discussion and
We demonstrate the various machine-learning methods
on the model of the Kitaev chain:
i+1ˆci + ˆci+1ˆci + h.c.
where t > 0 controls the hopping and the pairing of spinless fermions alike and µ is a chemical potential. The
groundstate of this model has a quantum phase transition from a topologically trivial (|µ| > 2t) to a nontrivial
state (|µ| < 2t) as the chemical potential µ is tuned across
We use the ES to compress the quantum mechanical
wavefunction. The ES is deﬁned as follows. The whole
system is ﬁrst divided into two subsets A and B, after
which the reduced density matrix of subset A is calculated by partially tracing out the degrees of freedom in
ρA = TrB|ψ⟩⟨ψ|.
Denoting the eigenvalues of
ρA as λi, the entanglement spectrum is then deﬁned as
the set of numbers −ln λi.
It is important to remark
that various types of bipartition of the whole system into
subsets A and B exist, such as dividing the bulk in extensive disconnected parts , divisions in momentum
space or indeed even random partitioning . In
this work, we use the usual spatial bipartition into left
and right halves of the whole system.
As shown in Fig. 1a, the entanglement spectrum of the
Kitaev chain is clearly distinguishable in the two phases,
especially since the nontrivial phase has a degeneracy
structure as do all symmetry protected topological phases
This feature is clear also for human eyes, and a
machine-learning routine seems to be an overkill.
use this model for demonstration purposes and in the
following, we will apply the introduced methodology to
more complex models.
The data for machine learning
is chosen to be the largest 10 eigenvalues λi, for L = 20
with an equal partitioning LA = LB = 10, and for various
values of −4t ≤µ ≤0.
Unsupervised learning
First we perform unsupervised learning, using an
established method for feature extraction.
The entanglement spectra are interpreted as points in a 10dimensional space, and we use principal component analysis (PCA) to extract mutually orthogonal axes along
which most of the variance of the data can be observed.
PCA amounts to a linear transformation Y
where X is an N ×10 matrix containing the entanglement
spectra as rows (N = 104 is the number of samples).
The orthogonal matrix W has vectors representing the
principal components ωℓas its columns, which are determined through the eigenvalue equation XT Xωℓ= λℓωℓ.
The eigenvalues λℓare the singular values of the matrix
X, and are hence non-negative real numbers, and we normalize them s.t. P λℓ= 1. The result of PCA is shown
in Fig. 1(b), and it is indeed possible to cluster the spectra in to three sets: µ < −2t, µ = −2t, and µ > −2t.
Supervised learning
Although it is as a whole unnecessary, since the PCA
analysis already manages to extract the phases and the
transition point, we still train a feedforward neural network (NN) on the 10-dimensional inputs.
This is for
demonstration only, since we have in mind the application to models in which PCA is insuﬃcient.
We train the network with 80 hidden sigmoid neurons
in a single hidden layer, and 2 output neurons.
ﬁrst/second output neuron predicts the (not necessarily normalized) probability for the data to be in trivial/nontrivial phase, and the predicted phase is the phase
with the larger probability.
We use stochastic gradient descent and L2 regularization to try and minimize a
cross-entropy cost function (for details, see e.g. Ref. ).
The network easily learns to distinguish the spectra when
trained on a subset of the data points and asked to predict the others.
Arguably the most important objective of machinelearning in general is that of generalization. After all,
learning is demonstrated by being able to perform well on
examples that have not been encountered before. By having trained the network only on a subset of the data, and
having it correctly predict others, it has already demonstrated learning.
As another display of the generalizing power of the
network, we blank out the data in a width w around
µ = −2t and ask the network to interpolate and ﬁnd the
transition point. Figure 1c shows that the network has no
diﬃculties doing so even for w = 2t. We were able to go
up to widths w = 3t before training became unreliable.
Confusion scheme
The PCA as an unsupervised learning technique may
be applied without perfectly known information of the
system, but it is a linear analysis and is hence incapable
of extracting non-linear relationships among the data.
On the other hand, a NN is capably of ﬁtting any nonlinear function , but a training phase with correctly
labelled input-output pairs is needed. In the following,
we propose a scheme combining the two methods which
we refer to as a confusion scheme. This scheme is the
main result of this work.
Suppose all data lies in the parameter range (a, b), and
we know there exists a critical point a < c < b such that
the data could be classiﬁed into two groups. However, we
do not know the value of c. We propose a critical point
c′, and train a network that we call Nc′ by labelling all
data with parameters smaller than c′ with label 0 and
the others with label 1. Next, we evaluate the performance of Nc′ on the entire data set and refer to its total
performance, with respect to the proposed critical point
c′, as P(c′). We will show that the function P(c′) has a
universal W-shape, with the middle peak at the correct
critical point c. Applying this to the Kitaev model, we
can see from Fig. 1d that for −4t < µ < 0, the prediction
performance from confusion scheme has a W-shape with
the middle peak at µ = −2t.
The W-shape can be understood as follows. We assume
that the data has two diﬀerent structures in the regimes
below c and above c, and that the NN is able to ﬁnd and
distinguish them. We refer to these diﬀerent structures
as features. When we set c′ = a, the NN chooses to assign
label 1 to both features and thus correctly predicts 100%
of the data. A similar analysis applies to c′ = b, except
that every data point is assigned the label 0. When c′ = c
is the correct labelling, the NN will choose to assign the
right label to both sides of the critical point and again
performs perfectly.
When a < c′ < c, in the training
phase the NN sees data with the same feature in the
ranges from a to c′ and from c′ to c, but having diﬀerent
labels (hence the confusion). In this case it will choose to
learn the label of the majority data, and the performance
P(c′) = 1 −min (c −c′, c′ −a)
Similar analysis applies to c < c′ < b. This gives the
typical W-shape seen in Fig. 1d. Notice that if the point c
is not exactly centered between a and b, the W-shape will
be slightly distorted. Its middle peak always corresponds
to the correct labelling, but the depth of the minima will
diﬀer between the left and right.
phase transition in the two-dimensional classical Ising
FIG. 2. Position of the middle peak in the universal W-shape
deviates from T ′
c = Tc for L = 10 due to the ﬁnite-size eﬀect.
Here kBTc ≈2.27J is the exact transition temperature in
the thermodynamic limit.
For L = 20 the middle peak is
located exactly at T ′
c = Tc. (Parameters for training: batch
size Nb = 100, learning rate α = 0.02 and regularization
l2 = 0.005.)
model , which has been studied by both supervised
learning and unsupervised learning methods. Here
we train a NN (with L2 neurons in the input and hidden layers, and 2 neurons in the output layer) on the
L×L classical conﬁgurations sampled from Monte Carlo
simulation. As shown in Fig. 2, the W-shape again predicts the right transition temperature. Note the confusion scheme works better when the underlying feature in
the data is shaper, i.e. for the larger system size L = 20.
To conﬁrm that the confusion scheme indeed extracts
nontrivial features from the input data. We have checked
the performance curve from confusion scheme, when the
NN is trained on unstructured random data. We use a
ﬁctive parameter as a tuning parameter, but have completely unstructured (random) data as a function of it.
Hence, the network will not ﬁnd structure in the data,
and a correct labelling does not exist. The middle peak
of the characteristic W-shape disappears, turning it into
a V-shape.
We notice that the choice of the learning rate (α) and
regularization (l2) is essential for a successful training.
The use of regularization is expected to reduce overﬁtting and make the network less sensitive to small variations of the data, hence forcing it to rather learn its
structure . However, the confusion scheme depends
solely on the ability of ﬁnding the majority label for the
underlying structure in the data.
In this sense, over-
ﬁtting is not necessarily bad. Indeed we have observed
that training with a negative l2 may lead to an equally
good performance. We speculate that this is because a
negative l2 tries to quickly increase the weights, making it harder for the network to change its opinion about
data samples in later stages. If the initial training data
is uniformly sampled, meaning the majority data is indeed represented by a majority, the network will rapidly
P(h ′c) at h ′c only
25, 1e-06, 0.01
25, 1e-08, 0.01
100, 1e-06, 0.01
100, 1e-07, 0.01
100, 1e-08, 0.01
100, 1e-08, 25
(a) Principal component analysis of the random-
ﬁeld Heisenberg model.
Unlike in the Kitaev model or for
the Ising data , there is no clearly observable clustering.
(b) The characteristic W-shape of the performance curve on
the MBL data. The result shows that the network Nh′c for
c ≈3J performs best, indicating that this is the correct
labelling. The distinction between the thermalizing and nonthermalizing phase can hence be put at hc ≈3J, in agreement
with Ref. . (Parameters for training: batch size Nb = 100,
learning rate α = 10−8 and regularization l2 = 0.01.) (c) The
performance of network Nh′c, when evaluated at the point h′
only instead of on the full data, for various diﬀerent sets of
learning parameters (see legend). Clearly the performance of
the network is most independent of the exact training scheme
c ≈3J, showing a robustness of this correct labelling
against variations in training.
adjust its weights to this majority. Lastly, we mention
that trainings are performed in epochs. In each epoch
all training data is passed once in batches of size Nb in
a random order. The training is stopped when a clear
W-shape is formed.
Random-ﬁeld Heisenberg chain
We will now test our proposed scheme on an example
where we the exact location of the transition point is
now known . We study a case of interest in recent
literature, namely that of many-body localization. We
consider the following model:
Si · Si+1 +
where S denote spin-1/2 operators. The local ﬁelds hα
are drawn from a uniform box distribution with zero
mean and width hα
max = hmax
max = 0. The disorder allows us to generate many
samples at a ﬁxed set of model parameters, in analogy
to the diﬀerent conﬁgurations for a ﬁxed temperature in
the classical spin systems .
The model in equation (3) has a transition between
thermalizing and non-thermalizing (i.e. many-body localized) behavior, driven by the disorder strength hmax.
In particular, when varying hmax, both the energy level
statistics as well as the statistics of the entanglement
spectra change their nature .
For the case of the
energy levels, the gaps (level spacings) follow either a
Wigner-Dyson distribution for the thermalizing phase, or
a Poisson distribution for the localized phase; while for
the entanglement spectrum, the Wigner-Dyson distribution is replaced by a semi-Poisson distribution. Note that
the change of ES can already be seen from the statistics
in a single eigenstate .
We numerically obtain the entanglement spectrum for
the groundstate of the model in equation (3), for disorder
strengths between hmax = J and hmax = 5J. The transition was shown to happen around hmax ≈3J , but
we stress that our method does not rely on this knowledge. We would simply have started from a larger width
of points, and then systematically narrow it down to the
current range. At each value of hmax we generate 105
disorder realizations for system size L = 12 and calculate the entanglement spectrum for LA = LB = 6. These
26 = 64 levels are used as the input to the NN.
First, we try to use an unsupervised PCA to cluster
the data. This analysis shows that the ﬁrst two principal
components are dominant, with the other components
being of order 10−4 or less. However, a scatterplot of the
data when projected onto the ﬁrst two principal components (shown in Fig. 3a) does not reveal a clear clustering
of the spectra.
We therefore turn to train a shallow feedforward network on the entanglement spectra in order to use the
confusion scheme. Here we use a network with 64 input
neurons, 100 hidden neurons and 2 output neurons. The
results are shown in Fig. 3b. Also in this case, the characteristic W-shape is obtained and we detect the transition
at hc ≈3J. In addition to the previous cases, we also
consider explicitly the performance of the network Nh′c
c. We do this to conﬁrm that the labelling with h′
3J is indeed correct. We expect namely that the training of the network is most robust against changes in its
parameters for the correct labelling. In other words, we
may also look for the h′
c at which the training is most
independent of chosen conditions. As shown in Fig. 3c,
this point is also at h′
DISCUSSION
In this work we have explored the uses of machine
learning in identifying (quantum) phase transitions from
the overall performance of neural networks.
data for training we proposed to use the entanglement
spectrum of a quantum state, since it provides an excellent way of compressing the otherwise exponentially large
wavefunction. We have shown that by confusing the neural network, by purposefully mislabelling the data, we are
able to identify the phase transition between two phases
of matter without prior knowledge of them. We demonstrated this for three diﬀerent scenario’s. We expect our
method to be useful also in situations where it is not clear
whether two phases are identical, or whether intervening
phases exist in between other phases. If the underlying
data in such situations has a (hidden) pattern that distinguishes them, our machine learning approach may well be
able to ﬁnd it. Our method is not limited to applications
in condensed matter physics and phase transitions, but
might prove useful in any ﬁeld in which large amounts of
data can be classiﬁed using a priori unknown labels.
An interesting direction for future studies is the relaxation of the assumption that there are only two phases to
be distinguished. If there are multiple phase transitions
present in the data, the characteristic W-shape will be
modiﬁed, and its new shape (i.e. the number of peaks)
will signal the correct number of diﬀerent labels. Additionally, it may be possible to formulate this method in a
self-consistent way, with an adaptive labelling and having
the algorithm determine the correct labels by itself.
ACKNOWLEDGEMENTS
gratefully
acknowledge
support from the Swiss National Science Foundation
(SNSF). Y.-H.L. is supported by ERC Advanced Grant
acknowledges
discussions
with Maciej Koch-Janusz on extending the confusion
scheme to the case with multiple phases.
E.v.N. and
Y.-H.L. acknowledge helpful discussions with Giuseppe
Carleo, Juan Osorio and Lei Wang.
E.v.N and S.H.
thank Andreas Krause for useful discussion on machine
 M. I. Jordan and T. M. Mitchell, Science 349, 255 .
 L. Wang, Preprint at 
Carrasquilla
 .
 K. Ch’ng, J. Carrasquilla, R. G. Melko, and E. Khatami,
 
 L. Li, T. E. Baker, S. R. White, and K. Burke, Preprint
at .
 P. Broecker, J. Carrasquilla, R. G. Melko, and S. Trebst,
 
 .
 D.-L. Deng, X. Li,
and S. D. Sarma, Preprint at
 .
 .
 E. M. Stoudenmire and D. J. Schwab, Preprint at
 .
 L. Saitta and M. Sebag, Encyclopedia of Machine Learning pp. 767–773.
 S. O. Haykin, Neural Networks: A Comprehensive Foundation .
 H. Li and F. D. M. Haldane, Phys. Rev. Lett. 101, 010504
Laﬂorencie,
 .
 A. Chandran, V. Khemani, and S. L. Sondhi, Phys. Rev.
Lett. 113, 060501 .
 L. Amico, R. Fazio, A. Osterloh,
and V. Vedral, Rev.
Mod. Phys. 80, 517 .
 R. Thomale, A. Sterdyniak, N. Regnault,
Bernevig, Phys. Rev. Lett. 104, 180502 .
 X. L. Qi, H. Katsura, and A. W. W. Ludwig, Phys. Rev.
Lett. 108, 1 .
 A. M. Turner, F. Pollmann, and E. Berg, Phys. Rev. B
83, 075102 .
 J. I. Cirac, D. Poilblanc, N. Schuch, and F. Verstraete,
Phys. Rev. B 83, 245134 .
 N. Schuch, D. Poilblanc, J. I. Cirac, and D. P´erez-Garc´ıa,
Phys. Rev. Lett. 111, 090501 .
 P. Calabrese and A. Lefevre, Phys. Rev. A 78, 032329
 V. Alba, M. Haque, and A. M. L¨auchli, Phys. Rev. Lett.
108, 227201 .
 Z.-C. Yang, C. Chamon, A. Hamma, and E. R. Mucciolo,
Phys. Rev. Lett. 115, 267206 .
 S. D. Geraedts, R. Nandkishore, and N. Regnault, Phys.
Rev. B 93, 174202 .
 H. Pichler, G. Zhu, A. Seif, P. Zoller,
and M. Hafezi,
 
 A. Y. Kitaev, Physics-Uspekhi 44, 131 .
 T. H. Hsieh and L. Fu, Phys. Rev. Lett. 113, 106801
 R. Thomale, D. P. Arovas,
and B. A. Bernevig, Phys.
Rev. Lett. 105, 116805 .