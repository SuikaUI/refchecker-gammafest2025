published: 11 December 2019
doi: 10.3389/fncom.2019.00083
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Edited by:
Spyridon Bakas,
University of Pennsylvania,
United States
Reviewed by:
Arizona State University, United States
Guotai Wang,
University of Electronic Science and
Technology of China, China
*Correspondence:
Jakub Nalepa
 
Received: 30 April 2019
Accepted: 27 November 2019
Published: 11 December 2019
Nalepa J, Marcinkiewicz M and
Kawulok M Data Augmentation
for Brain-Tumor Segmentation: A
Front. Comput. Neurosci. 13:83.
doi: 10.3389/fncom.2019.00083
Data Augmentation for Brain-Tumor
Segmentation: A Review
Jakub Nalepa 1,2*, Michal Marcinkiewicz 3 and Michal Kawulok 2
1 Future Processing, Gliwice, Poland, 2 Silesian University of Technology, Gliwice, Poland, 3 Netguru, Poznan, Poland
Data augmentation is a popular technique which helps improve generalization capabilities
of deep neural networks, and can be perceived as implicit regularization. It plays a pivotal
role in scenarios in which the amount of high-quality ground-truth data is limited, and
acquiring new examples is costly and time-consuming. This is a very common problem in
medical image analysis, especially tumor delineation. In this paper, we review the current
advances in data-augmentation techniques applied to magnetic resonance images
of brain tumors. To better understand the practical aspects of such algorithms, we
investigate the papers submitted to the Multimodal Brain Tumor Segmentation Challenge
 , as the BraTS dataset became a standard benchmark for validating
existent and emerging brain-tumor detection and segmentation techniques. We verify
which data augmentation approaches were exploited and what was their impact on
the abilities of underlying supervised learners. Finally, we highlight the most promising
research directions to follow in order to synthesize high-quality artiﬁcial brain-tumor
examples which can boost the generalization abilities of deep models.
Keywords: MRI, image segmentation, data augmentation, deep learning, deep neural network
1. INTRODUCTION
Deep learning has established the state of the art in many sub-areas of computer vision and
pattern recognition , including medical imaging and medical image
analysis . Such techniques automatically discover the underlying data
representation to build high-quality models. Although it is possible to utilize generic priors and
exploit domain-speciﬁc knowledge to help improve representations, deep features can capture
very discriminative characteristics and explanatory factors of the data which could have been
omitted and/or unknown for human practitioners during the process of manual feature engineering
 .
In order to successfully build well-generalizing deep models, we need huge amount of
ground-truth data to avoid overﬁtting of such large-capacity learners, and “memorizing” training
sets . It has become a signiﬁcant obstacle which makes deep neural networks
quite challenging to apply in the medical image analysis ﬁeld where acquiring high-quality groundtruth data is time-consuming, expensive, and very human-dependent, especially in the context of
brain-tumor delineation from magnetic resonance imaging (MRI) . Additionally, the majority
of manually-annotated image sets are imbalanced—examples belonging to some speciﬁc classes
are often under-represented. To combat the problem of limited medical training sets, data
augmentation techniques, which generate synthetic training examples, are being actively developed
in the literature .
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
In this review paper, we analyze the brain-tumor segmentation
approaches available in the literature, and thoroughly investigate
which techniques have been utilized by the participants of the
Multimodal Brain Tumor Segmentation Challenge .
To the best of our knowledge, the dataset used for the BraTS
challenge is currently the largest and the most comprehensive
brain-tumor dataset utilized for validating existent and emerging
algorithms for detecting and segmenting brain tumors. Also, it is
heterogeneous in the sense that it includes both low- and highgrade lesions, and the included MRI scans have been acquired
at diﬀerent institutions (using diﬀerent MR scanners). We
discuss the brain-tumor data augmentation techniques already
available in the literature, and divide them into several groups
depending on their underlying concepts (section 2). Such MRI
data augmentation approaches have been applied to augment
other datasets as well, also acquired for diﬀerent organs .
In the BraTS challenge, the participants are given multimodal MRI data of brain-tumor patients (as already mentioned,
both low- and high-grade gliomas), alongside the corresponding
ground-truth multi-class segmentation (section 3). In this
dataset, diﬀerent sequences are co-registered to the same
anatomical template and interpolated to the same resolution
of 1 mm3. The task is to build a supervised learner which is
able to generalize well over the unseen data which is released
during the testing phase. In section 4, we summarize the
augmentation methods reported in 20 papers published in the
BraTS 2018 proceedings. Here, we focused on those papers
which explicitly mentioned that the data augmentation had been
utilized, and clearly stated what kind of data augmentation had
been applied. Although such augmentations are single-modal—
meaning that they operate over the MRI from a single sequence—
they can be easily applied to co-registered series, hence to
augment multi-modal tumor examples. Finally, the paper is
concluded in section 5, where we summarize the advantages
and disadvantages of the reviewed augmentation techniques, and
highlight the promising research directions which emerge from
(not only) BraTS.
2. DATA AUGMENTATION FOR
BRAIN-TUMOR SEGMENTATION
Data augmentation algorithms for brain-tumor segmentation
from MRI can be divided into the following main categories
(which we render in a taxonomy presented in Figure 1): the
algorithms exploiting various transformations of the original
data, including aﬃne image transformations (section 2.1),
elastic transformations (section 2.2), pixel-level transformations
(section 2.3), and various approaches for generating artiﬁcial
data (section 2.4). In the following subsections, we review
the approaches belonging to all groups of such augmentation
methods in more detail.
Traditionally, data augmentation approaches have been
applied to increase the size of training sets, in order to allow
large-capacity learners beneﬁt from more representative training
data . There is, however, a new trend in the
deep learning literature, in which examples are augmented on
the ﬂy (i.e., during the inference), in the test-time1 augmentation
process. In Figure 2, we present a ﬂowchart in which both
training- and test-time data augmentation is shown. Test-time
data augmentation can help increase the robustness of a trained
model by simulating the creation of a homogeneous ensemble,
where (n + 1) models (of the same type, and trained over
the same training data) vote for the ﬁnal class label of an
incoming test example, and n denotes the number of artiﬁciallygenerated samples, elaborated for the test example which is being
classiﬁed. The robustness of a deep model is often deﬁned as
its ability to correctly classify previously unseen examples—such
incoming examples are commonly “noisy” or slightly “perturbed”
when confronted with the original data, therefore they are more
challenging to classify and/or segment . Testtime data augmentation can be exploited for estimating the
level of uncertainty of deep networks during the inference—it
brings new exciting possibilities in the context of medical image
analysis, where quantifying the robustness and deep-network
reliability are crucial practical issues . This type
of data augmentation can utilize those methods which modify
an incoming example, e.g., by applying aﬃne, pixel-level or
elastic transformations in the case of brain-tumor segmentation
2.1. Data Augmentation Using Afﬁne Image
Transformations
approaches,
diﬀerent operations (rotation, zooming, cropping, ﬂipping,
translations)
examples . Shin et al.
pointed out that such traditional data augmentation techniques
fundamentally produce very correlated images , therefore can oﬀer very little improvements for the
deep-network training process and future generalization over the
unseen test data (such examples do not regularize the problem
suﬃciently). Additionally, they can also generate anatomically
incorrect examples, e.g., using rotation. Nevertheless, aﬃne
image transformations are trivial to implement (in both 2D and
3D), they are fairly ﬂexible (due to their hyper-parameters), and
are widely applied in the literature. In an example presented in
Figure 3, we can see that applying simple data augmentation
techniques can lead to a signiﬁcant increase in the number of
training samples.
2.1.1. Flip and Rotation
Random ﬂipping creates a mirror reﬂection of an original image
along one (or more) selected axis. Usually, natural images can
be ﬂipped along the horizontal axis, which is not the case for
the vertical one because up and down parts of an image are
not always “interchangeable.” A similar property holds for MRI
brain images—in the axial plane a brain has two hemispheres,
and the brain (in most cases) can be considered anatomically
symmetrical. Flipping along the horizontal axis swaps the left
1Test-time augmentation is also referred to as the inference-time and the online
data augmentation in the literature.
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
FIGURE 1 | Data augmentation for brain-tumor segmentation—a taxonomy.
FIGURE 2 | Flowchart presenting training- and test-time data augmentation. In the training-time data augmentation approach, we generate synthetic data to increase
the representativeness of a training set (and ultimately build better models), whereas in test-time augmentation, we beneﬁt from the ensemble-like technique, in which
multiple homogeneous classiﬁers vote for the ﬁnal class label for an incoming example by classifying this sample and a number of its augmented versions.
FIGURE 3 | Applying afﬁne and pixel-level (discussed in more detail in section 2.3) transformations can help signiﬁcantly increase the size (and potentially
representativeness) of training sets. In this example, we generate seven new images based on the original MRI (coupled with its ground truth in the bottom row).
hemisphere with the right one, and vice versa. This operation
can help various deep classiﬁers, especially those beneﬁtting
from the contextual tumor information, be invariant with respect
to their position within the brain which would be otherwise
diﬃcult for not representative training sets (e.g., containing
brain tumors located only in the left or right hemisphere).
Similarly, rotating an image by an angle α around the center
pixel can be exploited in this context. This operation is followed
by appropriate interpolation to ﬁt the original image size. The
rotation operation denoted as R is often coupled with zeropadding applied to the missing pixels:
cos α −sin α
2.1.2. Translation
The translation operation shifts the entire image by a given
number of pixels in a chosen direction, while applying padding
accordingly. It allows the network to not become focused on
features present mainly in one particular spatial region, but it
forces the model to learn spatially-invariant features instead. As
in the case of rotation—since the MRI scans of diﬀerent patients
available in training sets are often not co-registered—translation
of an image by a given number of pixels along a selected axis
(or axes) can create useful and viable images. However, this
procedure may not be “useful” for all deep architectures—
convolutional
convolutions
pooling operations, which are intrinsically spatially-invariant
 .
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
2.1.3. Scaling and Cropping
Introducing scaled versions of the original images into the
training set can help the deep network learn valuable deep
features independently of their original scale. This operation
S can be performed independently in diﬀerent directions (for
brevity, we have only two dimensions here):
and the scaling factors are given as sx and sy for the x and y
directions, respectively. As tumors vary in size, scaling can indeed
bring viable augmented images into a training set. Since various
deep architectures require images of the constant size, scaling
is commonly paired with cropping to maintain the original
image dimensions. Such augmented brain-tumor examples may
manifest tumoral features at diﬀerent scales. Also, cropping can
limit the ﬁeld of view only to those parts of the image which are
important .
2.1.4. Shearing
The shear transformation (H) displaces each point in an image
in a selected direction. This displacement is proportional to its
distance from the line which goes through the origin and is
parallel to this direction:
where hx and hy denote the shear coeﬃcient in the x and
y directions, respectively (as previously, we consider two
dimensions for readability). Although this operation can deform
shapes, it is rarely used to augment medical image data
because we often want to preserve original shape characteristics
 .
2.2. Data Augmentation Using Elastic
Image Transformations
Data augmentation algorithms based on unconstrained elastic
transformations of training examples can introduce shape
variations. They can bring lots of noise and damage into the
training set if the deformation ﬁeld is seriously varied—see an
example by Mok and Chung in which a widely-used
elastic transform produced a totally unrealistic synthetic MRI
scan of a human brain. If the simulated tumors were placed
in “unrealistic” positions, it would likely force the segmentation
engine to become invariant to contextual information and rather
focus on the lesion’s appearance features .
Although there are works which indicate that such aggressive
augmentation may deteriorate the performance of the models
in brain-tumor delineation , it is still an
open issue. Chaitanya et al. showed that visually nonrealistic synthetic examples can improve the segmentation of
cardiac MRI and noted that it is slightly counter-intuitive—it may
have occurred due to the inherent structural and deformationrelated characteristics of the cardiovascular system. Finally,
elastic transformations often beneﬁt from B-splines or random deformations
 .
Diﬀeomophic mappings play an important role in brain
imaging, as they are able to preserve topology and generate
biologically plausible deformations. In such transformations, the
diﬀeomorphism φ (also referred to as a diﬀeomorphic mapping) is
given in the spatial domain  of a source image I, and transforms
I to the target image J: I ◦φ−1(x, 1). The mapping is the solution
of the diﬀerential equation:
 φ(x, t), t
where φ(x, 0)
x, v is a time-dependent smooth velocity
ﬁeld, v :  × t →Rd, φ(x, t) is a geodesic path (d denotes
the dimensionality of the spatial domain ), and φ(x, t) :  ×
t →. In Nalepa et al. , we exploited the directly
manipulated free-form deformation, in which the velocity vector
ﬁelds are regularized using B-splines . The
d-dimensional update ﬁeld δvi1,...,id is
δvi1,...,id =
and B(·) are the B-spline basis functions, N denotes the
number of pixels in the domain of the reference image, r is the
spline order (in all dimensions), and ∂ξ
∂x is the gradient of the
spatial similarity metric at a pixel c. The B-spline functions act
as regularizers of the solution for each parametric dimension
 .
brain-tumor
diﬀeomorphic
registration
Figure 4—such
artiﬁcially-generated data signiﬁcantly improved the abilities
especially
transformations, as we showed in Nalepa et al. .
The generated (I′) images preserve topological information of
the original image data (I) with subtle changes to the tissue.
Diﬀeomorphic registration may be applied not only to images
exposing anatomical structures .
In Figure 5, we present examples of simple shapes which
underwent this transformation—the topological information is
clearly maintained in the generated images as well.
2.3. Data Augmentation Using Pixel-Level
Image Transformations
There exist augmentation techniques which do not alter
geometrical shape of an image (therefore, all geometrical features
remain unchanged during the augmentation process), but aﬀect
the pixel intensity values (either locally, or across the entire
image). Such operations can be especially useful in medical
image analysis, where diﬀerent training images are acquired
in diﬀerent locations and using diﬀerent scanners, hence can
be intrinsically heterogeneous in the pixel intensities, intensity
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
FIGURE 4 | Diffeomorphic image registration applied to example brain images allowed for obtaining visually-plausible generated images. For source (I), target (J), and
artiﬁcially generated (I′) images, we also present tumor masks overlayed over the corresponding original images (in yellow; rows with the o subscript), alongside a
zoomed part of a tumor (rows with the z superscript).
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
FIGURE 5 | Diffeomorphic image registration applied to basic shapes which underwent simple afﬁne registration (translation) before diffeomorphic mapping. Source
images (I) transformed to match the corresponding targets (J) still clearly expose their spatial characteristics (I′).
FIGURE 6 | Generative adversarial networks are aimed at generating fake data (by a generator; potentially using some available data characteristics) which is
indistinguishable from the original data by the discriminator. Therefore, the generator and discriminator compete with one another.
gradients or “saturation”2. During the pixel-level augmentation,
the pixel intensities are commonly perturbed using either random
or zero-mean Gaussian noise (with the standard deviation
corresponding to the appropriate data dimension), with a
given probability (the former operation is referred to as the
random intensity variation). Other pixel-level operations include
shifting and scaling of pixel-intensity values (and modifying the
image brightness), applying gamma correction and its multiple
variants ,
sharpening, blurring, and more . This kind
of data augmentation is often exploited for high-dimensional
data, as it can be conveniently applied to selected dimensions
 .
2.4. Data Augmentation by Generating
Artiﬁcial Data
To alleviate the problems related to the basic data augmentation
approaches (including the problem of generating correlated data
samples), various approaches toward generating artiﬁcial data
(GAD) have been proposed. Generative adversarial networks
(GANs), originally introduced in Goodfellow et al. , are
being exploited to augment medical datasets . The main objective of a GAN
(Figure 6) is to generate a new data example (by a generator)
which will be indistinguishable from the real data by the
2These variations can be however alleviated by appropriate data standardization.
discriminator (the generator competes with the discriminator,
and the overall optimization mimics the min-max game). Mok
and Chung proposed a new GAN architecture which utilizes a
coarse-to-ﬁne generator whose aim is to capture the manifold
of the training data and generate augmented examples . Adversarial networks have been also used
for semantic segmentation of brain tumors , brain-tumor detection , and image
synthesis of diﬀerent modalities . Although
GANs allow us to introduce invariance and robustness of deep
models with respect to not only aﬃne transforms (e.g., rotation,
scaling, or ﬂipping) but also to some shape and appearance
variations, convergence of the adversarial training and existence
of its equilibrium point remain the open issues. Finally, there
exist scenarios in which the generator renders multiple very
similar examples which cannot improve the generalization
of the system—it is known as the mode collapse problem
 .
An interesting approach for generating phantom image data
was exploited in Gholami et al. , where the authors utilized
a multi-species partial diﬀerential equations (PDE) growth model
of a tumor to generate synthetic lesions. However, such data
does not necessarily follow the correct intensity distribution
of a real MRI, hence it should be treated as a separate
modality, because using the artiﬁcial data which is sampled from
a very diﬀerent distribution may adversely aﬀect the overall
segmentation performance by “tricking” the underlying deep
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
model . The tumoral growth model itself
captured the time evolution of enhancing and necrotic tumor
concentrations together with the edema induced by a tumor.
Additionally, the deformation of a lesion was simulated by
incorporating the linear elasticity equations into the model. To
deal with the diﬀerent data distributions, the authors applied
CycleGAN for performing domain adaptation
(from the generated phantom data to the real BraTS MRI scans).
The experimental results showed that the domain adaptation was
able to generate images which were practically indistinguishable
from the real data, therefore could be safely included in the
training set.
A promising approach of combining training samples using
their linear combinations (referred to as mixup) was proposed
by Zhang et al. , and further enhanced for medical
image segmentation by Eaton-Rosen et al. in their mixmatch
algorithm , which additionally
introduced a technique of selecting training samples that
undergo linear combination. Since the medical image datasets
are often imbalanced (with the tumorous examples constituting
the minority class), training patches with highest “foreground
amounts” (i.e., the number of pixels annotated as tumorous)
are combined with those with the lowest concentration of
foreground. The authors showed that their approach can increase
performance in medical-image segmentation tasks, and related
its success to the mini-batch training. It is especially relevant in
the medical-image analysis, because the sizes of input scans are
usually large, hence the batches are small to keep the training
memory requirements feasible in practice. Such data-driven
augmentation techniques can also beneﬁt from growing groundtruth datasets (e.g., BraTS) which manifest large variability
of brain tumors, to generate even more synthetic examples.
Also, they could be potentially applied at test time to build an
ensemble-like model, if a training patch/image which matches
the test image being classiﬁed was eﬃciently selected from the
training set.
In this work, we analyzed the approaches which were exploited
by the BraTS 2018 participants to segment brain tumors from
MRI ,
and veriﬁed which augmentation scenarios were exploited in
these algorithms. All of those techniques have been trained over
the BraTS 2018 dataset consisting of MRI-DCE data of 285
patients with diagnosed gliomas: 210 patients with high-grade
glioblastomas (HGG), and 75 patients with low-grade gliomas
(LGG), and validated using the validation set of 66 previously
unseen patients (both LGG and HGG, however the grade has not
been revealed) . Each
study was manually annotated by one to four expert readers. The
data comes in four co-registered modalities: native pre-contrast
(T1), post-contrast T1-weighted (T1c), T2-weighted (T2), and T2
Fluid Attenuated Inversion Recovery (FLAIR). All the pixels have
one of four labels attached: healthy tissue, Gd-enhancing tumor
(ET), peritumoral edema (ED), the necrotic and non-enhancing
tumor core (NCR/NET). The scans were skull-stripped and
interpolated to the same shape (155, 240, 240 with the voxel size
of 1 mm3).
Importantly, this dataset manifests very heterogeneous image
quality, as the studies were acquired across diﬀerent institutions,
and using diﬀerent scanners. On the other hand, the delineation
procedure was clearly deﬁned which allowed for obtaining
similar ground-truth annotations across various readers. To this
end, the BraTS dataset—as the largest, most heterogeneous,
and carefully annotated set—has been established as a standard
brain-tumor dataset for quantifying the performance of existent
and emerging detection and segmentation approaches. This
heterogeneity is pivotal, as it captures a wide range of tumor
characteristics, and the models trained over BraTS are easily
applicable for segmenting other MRI scans .
experimentally,
U-Net-based
architecture using (a) BraTS
2019 training set (exclusively FLAIR sequences) and (b) our set
of 41 LGG (WHO II) brain-tumor patients who underwent the
MR imaging with a MAGNETOM Prisma 3T system (Siemens,
Erlangen, Germany) equipped with a maximum ﬁeld gradient
strength of 80 mT/m, and using a 20-channel quadrature head
coil. The MRI sequences were acquired in the axial plane with
a ﬁeld of view of 230 × 190 mm, matrix size 256 × 256 and 1
mm slice thickness with no slice gap. In particular, we exploited
exclusively FLAIR series with TE = 386 ms, TR = 5,000 ms, and
inversion time of 1,800 ms for segmentation of brain tumors.
These scans underwent the same pre-processing as applied in the
case of BraTS, however they were not segmented following
the same delineation protocol, hence the characteristics of the
manual segmentation likely diﬀer across (a) and (b). The
4-fold cross-validation showed that although the deep models
trained over (a) and (b) gave the statistically diﬀerent results
at p < 0.001, according to the two-tailed Wilcoxon test3, the
ensemble of models trained over (a) correctly detected 71.4%
(5/7 cases) of brain tumors in the WHO II test dataset, which
included seven patients kept aside while building an ensemble,
with the average whole-tumor DICE of 0.80, where DICE is
DICE(A, B) = 2 · |A ∩B|
|A| + |B| ,
where A and B are two segmentations, i.e., manual and
automated, 0 ≤DICE ≤1, and DICE = 1 means the perfect
segmentation score. On the other hand, a deep model trained
over the WHO II training set and used for segmenting the test
WHO II cases detected 85.7% tumors (6/7 patients) with the
average whole-tumor DICE = 0.84. This tiny experiment shows
that the segmentation engines trained over BraTS can capture
tumor characteristics which are manifested in MRI data acquired
and analyzed using diﬀerent protocols, and allow us to obtain
high-quality segmentation. Interestingly, if we train our ensemble
over the combined BraTS 2019 and WHO II training sets, we
3We tested the null hypothesis saying that applying the models trained exclusively
over the BraTS or our WHO II datasets leads to the same-quality segmentation.
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
FIGURE 7 | Two example low- and high-grade glioma patients from the BraTS 2018 dataset: red—GD-enhancing tumor (ET), green—peritumoral edema (ED), and
blue—necrotic and non-enhancing tumor core (NCR/NET); (A–D) show original images, whereas (A’–D’) present overlaid ground-truth masks.
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
TABLE 1 | Data augmentation techniques applied in the approaches validated within the BraTS 2018 challenge framework.
References
Pixel-wise
Albiol et al., 2019
VGG, Inception, Dense
3D afﬁne transformations
Benson et al., 2018*
CNN (encoder-decoder)
Carver et al., 2018
Chandra et al., 2018
V-Net, ResNet-18, FC-CRF Yes
Dai et al., 2018
Domain-adapted U-Net
Feng et al., 2018
Gholami et al., 2018*
Isensee et al., 2018
Kao et al., 2018
DeepMedic, 3D U-Net
Kermi et al., 2018
Lachinov et al., 2018*
Cascaded U-Net
Ma and Yang, 2018
McKinley et al., 2018
Shift, scale
Mehta and Arbel, 2018
Myronenko, 2018
CNN (encoder-decoder)
Nuechterlein and Mehta, 20183D-ESPNet
Puybareau et al., 2018
Rezaei et al., 2018†
Sun et al., 2018
CNN, DFKZ, 3D CNN
Wang et al., 2018*†
Number of methods utilizing this augmentation→15
Percentage (%) of methods utilizing this augmentation→75
The top-performing techniques (over the unseen test set) are annotated with green.
*The authors veriﬁed the impact of data augmentation of the generalization abilities of their deep models.
†The authors used both training- and test-time data augmentation.
will end up having the correct detection of 85.7% tumors (6/7
cases) with the average whole-tumor DICE of 0.76. We can
appreciate the fact that we were able to improve the detection,
but the segmentation quality slightly dropped, showing that the
detected case was challenging to segment. Finally, it is worth
mentioning that this experiment sheds only some light on the
eﬀectiveness of applying the deep models (or other data-driven
techniques) trained over BraTS for analyzing diﬀerent MRI
brain images. The manual delineation protocols were diﬀerent,
and the lack of inter-rater agreement may play pivotal role
in quantifying automated segmentation algorithms over such
diﬀerently acquired and analyzed image sets—it is unclear if
the diﬀerences result from the inter-rater disagreement of the
incorrect segmentation .
3.1. Example BraTS Images
Example BraTS 2018 images are rendered in Figure 7 (two
low-grade and two high-grade glioma patients), alongside the
corresponding multi-class ground-truth annotations. We can
appreciate that diﬀerent parts of the tumors are manifested in
diﬀerent modalities—e.g., necrotic and non-enhancing tumor
core is typically hypo-intense in T1-Gd when compared to
T1 . Therefore, multi-modal analysis appears
crucial to fully beneﬁt from the available image information.
4. BRAIN-TUMOR DATA AUGMENTATION
IN PRACTICE
4.1. BraTS 2018 Challenge
The BraTS challenge is aimed at evaluating the state-ofthe-art approaches toward accurate multi-class brain-tumor
segmentation from MRI. In this work, we review all published
methods which were evaluated within the framework of
the BraTS 2018 challenge—although 61 teams participated
in the testing phase , only 45 methods
were ﬁnally described and published in the post-conference
proceedings . We verify which augmentation
techniques were exploited to help boost generalization abilities
of the proposed supervised learners. We exclusively focus on 20
papers (44% of all manuscripts) in which the authors explicitly
stated that the augmentation had been used and report the type
of the applied augmentation.
In Table 1, we summarize all investigated brain-tumor
segmentation algorithms, and report the deep models utilized in
the corresponding works alongside the augmentation techniques.
In most of the cases, the authors followed the cross-validation
scenario, and divided the training set into multiple nonoverlapping folds. Then, separate models were trained over
such folds, and the authors ﬁnally formed an ensemble of
heterogeneous classiﬁers (trained over diﬀerent training data) to
segment previously unseen test brain-tumor images. Also, there
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
FIGURE 8 | The DICE values: (A) whote-tumor (WT), (B) tumor core (TC), and (C) enhancing tumor (ET), obtained using the investigated techniques over the BraTS
2018 validation set.
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
TABLE 2 | The impact of applying data augmentation on the average DICE scores.
Without augmentation
With augmentation
Change (in %)
References
Benson et al., 2018
Gholami et al., 2018
Lachinov et al., 2018*
Wang et al., 2018
For the methods reported by Lachinov et al. and Wang et al. , we analyzed the best-performing models.
*The authors veriﬁed the impact of data augmentation over the training set.
TABLE 3 | The impact of applying data augmentation on the average Hausdorff distance values (in mm).
Without augmentation
With augmentation
Change (in %)
References
Benson et al., 2018
Wang et al., 2018
For the method reported by Wang et al. , we analyzed the best-performing models. Note that Gholami et al. and Lachinov et al. did not present the Hausdorff
distances obtained using their approaches.
TABLE 4 | The fully convolutional neural networks proposed in Lorenzo et al. have been trained using a number of datasets with different preprocessing
and augmentations.
Feature centering
Vertical ﬂip
Horizontal ﬂip
Max. rotation (∡max)
Augmentation factor
In the prime versions, we applied elastic deformations. This table comes from our previous paper .
are approaches, e.g., by Albiol et al. , Chandra et al. ,
or Sun et al. , in which a variety of deep neural architectures
were used.
In the majority of investigated brain-tumor segmentation
techniques, the authors applied relatively simple training-time
data augmentation strategies—the combination of training- and
test-time augmentation was used only in two methods . In 75% of the analyzed approaches,
random ﬂipping was executed to increase the training set
size and provide anatomically correct brain images4. Similarly,
rotating and scaling MRI images was applied in 40% and
45% of techniques, respectively. Since modern deep network
architectures are commonly translation-invariant, this type of
aﬃne augmentation was used only in two works. Although
other augmentation strategies were not as popular as easyto-implement aﬃne transformations, it is worth noting that
the pixel-wise operations were utilized in all of the topperforming techniques ,
4Note that we do not count the algorithm proposed by Albiol et al. , because
the authors were not very speciﬁc about their augmentation strategies.
Isensee et al. , and McKinley et al. achieved the
ﬁrst, second, and third place across all segmentation algorithms5,
respectively). Additionally, Isensee et al. exploited elastic
transformations in their aggressive data augmentation procedure
which signiﬁcantly increased the size and representativeness of
their training sets, and ultimately allowed for outperforming
a number of other learners. Interestingly, the authors showed
that the state-of-the-art U-Net architecture can be extremely
competitive with other (much deeper and complex) models
if the data is appropriately curated. It, in turn, manifests the
importance of data representativeness and quality in the context
of robust medical image analysis.
In Figure 8, we visualize the DICE scores obtained using
almost all investigated methods . It is worth mentioning that the trend is fairly coherent for
all classes (whole tumor, tumor core, and enhancing tumor), and
the best-performing methods by Isensee et al. , McKinley
5For more detail on the validation and scoring procedures, see Bakas et al. .
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
et al. , and Myronenko consistently outperform
the other techniques in all cases. Although the success of these
approaches obviously lies not only in the applied augmentation
techniques, it is notable that the authors extensively beneﬁt from
generating additional synthetic data.
Albeit data augmentation is introduced in order to improve
the generalization capabilities of supervised learners, this impact
was veriﬁed only in four BraTS 2018 papers . Gholami et al. showed that their PDE-based
augmentation delivers very signiﬁcant improvement in the DICE
scores obtained for segmenting all parts of the tumors in the
multi-class classiﬁcation. The same performance boost (in the
DICE values obtained for each class) was reported by Lachinov
et al. . Finally, Wang et al. showed that the proposed
test-time data augmentation led to improving the performance of
their convolutional neural networks.
In Table 2, we gathered the DICE scores obtained with
and without the corresponding data augmentation, alongside
the change in DICE (reported in %; the larger the DICE
score becomes, the better segmentation has been obtained).
Interestingly, training-time data augmentation appeared to be
adversely aﬀecting the performance of the algorithm presented
by Benson et al. . On the other hand, the authors showed
that the Hausdorﬀdistance, being the maximum distance of
TABLE 5 | Five best-performing conﬁgurations of our fully convolutional neural
network according to the Friedman’s test (at p < 0.05) taking into account the
results elaborated for the WHO II validation set .
FIGURE 9 | Exploiting various augmentations and coupling them into an augmentation tree allow us to generate multiple versions of an original patch (or image) which
may be included in a training set. This ﬁgure is inspired by Lorenzo et al. .
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
all points from the segmented lesion to the corresponding
nearest point of the ground-truth segmentation , signiﬁcantly dropped, hence the maximum
segmentation error quantiﬁed by this metric was notably
reduced (the smaller the Hausdorﬀdistance becomes, the
better segmentation has been elaborated; Table 3). Test-time
data augmentation exploited by Wang et al. not only
decreased DICE for the whole-tumor segmentation, but also
caused the increase of the correspoding Hausdorﬀdistance.
TABLE 6 | The results, both (a) average, and (b) median DICE over our clinical
MRI data of low-grade glioma (WHO II) patients in the whole-tumor segmentation
task, for different augmentation scenarios.
Augmentation
Validation
DIR + Flip
DIR + Flip
The results come from our paper . The best results are boldfaced.
Therefore, applying it in the WT segmentation scenario led
to decreasing the abilities of the underlying models. Overall,
the vast majority of methods neither report nor analyze
the real impact of the incorporated augmentation techniques
on the classiﬁcation performance and/or inference time of
their deep models. Although we believe the authors did
investigate the advantages (and disadvantages) of their data
generation strategies (either experimentally or theoretically),
data augmentation is often used a standard tool which is
applied to any diﬃcult data (e.g., imbalanced, with highly
under-represented classes).
4.2. Beyond the BraTS Challenge
Although practically all brain-tumor segmentation algorithms
which emerge in the recent literature have been tested over
the BraTS datasets, we equipped our U-Nets with a battery of
augmentation techniques (summarized in Table 4) and veriﬁed
their impact over our clinical MRI data in Lorenzo et al.
 . In this experiment, we have focused on the wholetumor segmentation, as it was an intermediate step in the
automated dynamic contrast-enhanced MRI analysis, in which
perfusion parameters have been extracted for the entire tumor
volume. Additionally, this dataset was manually delineated by a
reader (8 years of experience) who highlighted the whole-tumor
areas only.
FIGURE 10 | Examples from our clinical dataset segmented using our deep network trained in the DIR+Flip setting: (A–C) are original images, (D–F) are
corresponding segmentations. Green color represents true positives, blue—false negatives, and red—false positives.
Frontiers in Computational Neuroscience | www.frontiersin.org
December 2019 | Volume 13 | Article 83
Nalepa et al.
Data Augmentation for Brain-Tumor Segmentation: A Review
We executed multi-step augmentation by applying both aﬃne
and elastic deformations of tumor examples, and increased the
cardinality of our training sets up to 16×. In Figure 9, we can
observe how executing simple aﬃne transformations leads to
new synthetic image patches. Since various augmentation
approaches
augmentation tree, the number of artiﬁcial examples can
be signiﬁcantly increased. The multi-fold cross-validation
experiments showed that introducing rotated training examples
was pivotal to boost the generalization abilities of underlying
deep models. To verify the statistical importance of the results,
we executed the Friedman’s ranking tests which revealed that
the horizontal ﬂip with additional rotation is crucial to build
well-generalizing deep learners in the patch-based segmentation
scenario (Table 5).
Similarly,
diﬀeomorphic
registration
(DIR) coupled with a recommendation algorithm6 to select
training image pairs for registration in the data augmentation
process . The proposed augmentation was
compared with random horizontal ﬂipping, and the experiments
indicated that the combined approach leads to statistically
signiﬁcant (Wilcoxon test at p < 0.01) improvements in DICE
(Table 6). In Figure 10, we have gathered example segmentations
obtained using our DIR+Flip deep model, alongside the
corresponding DICE values. Although the original network,
trained over the original training set would correctly detect and
segment large tumors (Figures 10A,B), it failed for relatively
small lesions which were under-represented in the training set
(Figure 10C). Similarly, synthesizing artiﬁcial training examples
helped improving the performance of our models in the case
of brain tumors located in the brain areas which have not
been originally included in the dataset (by applying rotation
and ﬂipping).
5. CONCLUSION
In this paper, we reviewed the state-of-the-art data augmentation
methods applied in the context of segmenting brain tumors
from MRI. We carefully investigated all BraTS 2018 papers
augmentation
techniques
these methods. Our investigation revealed that the aﬃne
transformations are still the most widely-used in practice, since
they are trivial to implement and can elaborate anatomicallycorrect brain-tumor examples. There are, however, augmentation
methods which combine various approaches, also including
elastic transformations. A very interesting research direction
encompasses algorithms which can generate artiﬁcial images
(e.g., based on the tumoral growth models) that not necessarily
follow real-life data distribution, but can be followed by
other techniques to ensure correctness of such phantoms.
6We used a recommendation algorithm for selecting source-target image pairs
that undergo registration. Such pairs should contain the training images which
capture lesions positioned in the same or close part of the brain, as the totally
diﬀerent images can easily render unrealistic brain-tumor examples. A potential
drawback of this recommendation technique is its time complexity which amounts
to O(||T||2), where ||T|| is the cardinality of the original training set.
The results showed that data augmentation was pivotal in the
best-performing BraTS algorithms, and Isensee et al. 
experimentally proved that well-known and widely-used fullyconvolutional neural networks can outperform other (perhaps
much more deeper and complex) learners, if the training
data is appropriately cleansed and curated. It clearly indicates
the importance of introducing eﬀective data augmentation
methods for medical image data, which beneﬁt from aﬃne
transformations (in 2D and 3D), pixel-wise modiﬁcations
and elastic transform to deal with the problem of limited
ground-truth data. In Table 7, we gather the advantages and
disadvantages of all groups of brain-tumor data augmentation
techniques analyzed in this review. Finally, these approaches
can be easily applied in both single- and multi-modal scans,
TABLE 7 | The pros and cons of state-of-the-art brain-tumor data
augmentation algorithms.
Transformation of original data
Advantages
Disadvantages
Afﬁne transformations
• Easy to implement and
understand
• Produce correlated images
• Operate in real-time due to low
time complexity
• Easily generate anatomically
incorrect examples (*)
• Applicable in training- and
• Deliver invariance with respect
to the lesion position, scale, and
Elastic transformations
• Can be applicable in trainingand test-time
• Not trivial to implement
• Can introduce variations in
• Often have high time complexity
• Easily generate anatomically
incorrect examples (*)
Pixel-wise transformations
• Easy to implement and
understand
• Cannot introduce changes in
• Operate in real-time due to low
time complexity
• Applicable in training- and
• Can simulate different
acquisition scenarios
Generation of artiﬁcial data
• Can synthesize realistic
• (Very) high time complexity
• (Potentially) applicable in
• GANs applicable in
training-time only
• Can introduce invariance with
respect to afﬁne transformations
and appearance variations
• Can easily render multiple
similar examples into training sets for brain-tumor detection and segmentation tasks is yet to be revealed.
usually by synthesizing artiﬁcial examples separately for each
image modality.
Although data augmentation became a pivotal part of virtually
all deep learning-powered methods for segmenting brain lesions
(due to the lack of very large, suﬃciently heterogeneous
and representative ground-truth sets, with BraTS being an
exception), there are still promising and unexplored research
pathways in the literature. We believe that hybridizing techniques
from various algorithmic groups, introducing more data-driven
augmentations, and applying them at training- and test-time
can further boost the performance of large-capacity learners.
Also, investigating the impact of including not necessarily
anatomically correct brain-tumor scans into training sets
remains an open issue (see the examples of anatomically incorrect
brain images which still manifest valid tumor characteristics
in Figure 11).
AUTHOR CONTRIBUTIONS
JN designed the study, performed the experiments, analyzed
data, and wrote the manuscript. MM provided selected
implementations and experimental results, and contributed to
writing of some parts of the initial version of the manuscript. MK
provided qualitative segmentation analysis and visualizations.
This work was supported by the Polish National Centre
for Research and Development under the Innomed Grant
(POIR.01.02.00-00-0030/15). JN was supported by the Silesian
University of Technology funds (The Rector’s Habilitation
Grant No. 02/020/RGH19/0185). The research undertaken in
this project led to developing Sens.AI—a tool for automated
segmentation of brain lesions from T2-FLAIR sequences (https://
sensai.eu). MK was supported by the Silesian University of
Technology funds (Grant No. 02/020/BK_18/0128).
ACKNOWLEDGMENTS
The authors are grateful to the Reviewers for their constructive
and valuable comments that helped improve the paper. JN thanks
Dana K. Mitchell for lots of inspiring discussions on (not only)
brain MRI analysis.
This paper is in memory of Dr. Grzegorz Nalepa, an
extraordinary scientist, pediatric hematologist/oncologist, and a
compassionate champion for kids at Riley Hospital for Children,
Indianapolis, USA, who helped countless patients and their
families through some of the most challenging moments of
their lives.