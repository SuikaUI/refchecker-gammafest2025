HAL Id: hal-00270806
 
 
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Testing for Homogeneity with Kernel Fisher
Discriminant Analysis
Zaid Harchaoui, Francis Bach, Eric Moulines
To cite this version:
Zaid Harchaoui, Francis Bach, Eric Moulines. Testing for Homogeneity with Kernel Fisher Discriminant Analysis. 2008. ￿hal-00270806￿
hal-00270806, version 1 - 7 Apr 2008
Testing for Homogeneity with KFDA
Testing for Homogeneity
with Kernel Fisher Discriminant Analysis
Za¨id Harchaoui
 
LTCI, TELECOM ParisTech & CNRS
46, rue Barrault
75634 Paris cedex 13, France
Francis R. Bach
 
INRIA - Willow Project-Team
Laboratoire d’Informatique de l’´Ecole Normale Sup´erieure
45, rue d’Ulm
75230 Paris, France
´Eric Moulines
 
LTCI, TELECOM ParisTech & CNRS
46, rue Barrault
75634 Paris cedex 13, France
We propose to investigate test statistics for testing homogeneity in reproducing kernel
Hilbert spaces. Asymptotic null distributions under null hypothesis are derived, and consistency under ﬁxed and local alternatives is assessed. Finally, experimental evidence of
the performance of the proposed approach on both artiﬁcial data and a speaker veriﬁcation
task is provided.
Keywords: statistical hypothesis testing, reproducing kernel Hilbert space, covariance operator
1. Introduction
An important problem in statistics and machine learning consists in testing whether the
distributions of two random variables are identical under the alternative that they may diﬀer
in some ways. More precisely, let {X(1)
1 , . . . , X(1)
n1 } and {X(2)
1 , . . . , X(2)
n2 } be independent
random variables taking values in an arbitrary input space X, with common distributions
P1 and P2, respectively. The problem consists in testing the null hypothesis of homogeneity
H0 : P1 = P2 , against the alternative HA : P1 ̸= P2.
This problem arises in many
applications, ranging from computational anatomy to speaker
segmentation . We shall allow the input space X to be quite general,
including for example ﬁnite-dimensional euclidean spaces but also function spaces, or more
sophisticated structures such as strings or graphs 
arising in applications such as bioinformatics .
Traditional approaches to this problem are based on cumulative distribution functions
(cdf), and use a certain distance between the empirical cdf obtained from the two samples.
Popular procedures are the two-sample Kolmogorov-Smirnov tests or the Cramer-Von Mises
tests , that have been frequently used to address these issues,
at least for low-dimensional data. Although these tests are popular due to their simplicity,
they are known to be insensitive to certain characteristics of the distributions, such as
densities containing high-frequency components or local features such as narrow bumps.
The low-power of the traditional cdf-based test statistics can be improved on by using test
statistics based on probability density estimators. Tests based on kernel density estimators
have been studied by Anderson et al. and Allen , using respectively the L2
and L1 distances between densities. More recently, the use of wavelet estimators has been
proposed and thoroughly analyzed. Adaptive versions of these tests, that is where smoothing
parameters for the density estimator are obtained from the data, have been considered
by Butucea and Tribouley .
Recently, Gretton et al. cast the two-sample homogeneity test in a kernel-based
framework, and have shown that their test statistics, coined Maximum Mean Discrepancy
(MMD) yields as a particular case the L2-distance between kernel density estimators. We
propose here to further enhance such an approach by directly incorporating the covariance
structure of the probability distributions into our test statistics, yielding in some sense to
a chi-square divergence between the two distributions. For discrete distributions, it is wellknown that such a normalization yield test statistics with greater power .
The paper is organized as follows.
In Section 2 and Section 3, we state the main
deﬁnitions and we build our test statistics upon kernel Fisher discriminant analysis. In
Section 4, we give the asymptotic distribution of our test statistic under the null hypothesis,
and establish the consistency and the limiting distribution of the test for both ﬁxed and
a class of local alternatives. In Section 5, we ﬁrst investigate the limiting power of our
test statistics against directional then non-directional sequences of local alternatives in a
particular setting, that is when P1 is the uniform distribution and P2 is a one-frequency
contamination of P1 on the Fourier basis and the reproducing kernel belongs to the class of
periodic spline kernels, and then compare our test statistics with the MMD test statistics in
terms of limiting power. In Section 6 we provide experimental evidence of the performance
of our test statistic on a speaker identiﬁcation task. Detailed proofs are presented in the
last sections.
2. Mean and covariance in reproducing kernel Hilbert spaces
We ﬁrst highlight the main assumptions on the reproducing kernel, and then introduce
operator-theoretic tools for deﬁning the mean element and the covariance operator associated with a reproducing kernel.
2.1 Reproducing kernel Hilbert spaces
Let (X, d) be a separable measurable metric space, and denote by X the associated σ-algebra.
Let X be X-valued random variable, with probability measure P, and the expectation with
respect to P is denoted by E. Consider a Hilbert space (H, ⟨·, ·⟩H) of functions from X to
R. The Hilbert space H is a reproducing kernel Hilbert space (RKHS) if at each x ∈X,
the point evaluation operator δx : H →R, which maps f ∈H to f(x) ∈R, is a bounded
linear functional. To each point x ∈X, there corresponds an element Φ(x) ∈H such that
Testing for Homogeneity with KFDA
⟨Φ(x), f⟩H = f(x) for all f ∈H, and ⟨Φ(x), Φ(y)⟩H = k(x, y), where k : X × X →R is
a positive deﬁnite kernel . In this situation, Φ(·) is the Aronszajn-map,
and we denote by ∥f∥H = ⟨f, f⟩1/2
the associated norm. It is assumed from now on that
H is a separable Hilbert space. Note that this is always the case if X is a separable metric
space and if the kernel is continuous . We make the following
two assumptions on the kernel:
(A1) The kernel k is bounded, that is |k|∞
= sup(x,y)∈X×X k(x, y) < ∞.
(A2) For all probability distributions P on (X, X), the RKHS associated with k(·, ·) is dense
Note that some of our results (such as the limiting distribution under the null distribution) are valid without assumption (A2), while consistency results against ﬁxed or local
alternatives do need (A2). Assumption (A2) is true in particular for the gaussian kernel
on Rd as shown in , and that X may be a discrete
space .
2.2 Mean element and covariance operator
We shall need some operator-theoretic tools , to deﬁne mean elements
and covariance operators. A linear operator T is said to be bounded if there is a number C
such that ∥Tf∥H ≤C ∥f∥H for all f ∈H. The operator-norm of T is then deﬁned as the
minimum of such numbers C, that is ∥T∥= sup∥f∥H≤1 ∥Tf∥H. Furthermore, a bounded
linear operator T is said to be Hilbert-Schmidt, if the Hilbert-Schmidt-norm ∥T∥HS =
p=1 ⟨Tep, Tep⟩H}1/2 is ﬁnite, where {ep}p≥1 is any complete orthonormal basis of H.
Note that ∥T∥HS is independent of the choice of the orthonormal basis. We shall make
frequent use of tensor product notations. The tensor product operator u ⊗v for u, v ∈H is
deﬁned for all f ∈H as (u ⊗v)f = ⟨v, f⟩H u.
We now introduce the mean element and covariance operator (see Blanchard et al.,
k1/2(x, x)P(dx) < ∞, the mean element µP is deﬁned as the unique element in
H satisfying for all functions f ∈H,
⟨µP, f⟩H = Pf def
If furthermore
k(x, x)P(dx) < ∞, then the covariance operator ΣP is deﬁned as the unique
linear operator onto H satisfying for all f, g ∈H,
(f −Pf)(g −Pg)dP ,
that is ⟨f, ΣPg⟩H is the covariance between f(X) and g(X) where X is distributed according
to P. Note that the mean element and covariance operator are well-deﬁned when (A1) is
Moreover, when assumption (A2) is satisﬁed, then the map from P 7→µP is
injective. Note also that the operator ΣP is a self-adjoint nonnegative trace-class operator.
In the sequel, the dependence of µP and ΣP in P is omitted whenever there is no risk of
confusion.
We now deﬁne what we later denote by Σ−1/2 in our proofs. For a compact operator
Σ, the range R(Σ1/2) of Σ1/2 is deﬁned as R(Σ1/2) = {Σ1/2f, f ∈H}, and may be characterized by R(Σ1/2) = {f ∈H, P∞
p=1 λp ⟨f, ep⟩2
H < ∞, f ⊥N(Σ1/2)}, where {λp, ep}p≥1
are the nonzero eigenvalues and eigenvectors of Σ, and N(Σ) = {f ∈H, Σf = 0} is
the null-space of Σ, that is functions which are constant in the support of P.
Deﬁning R−1(Σ1/2) = {g ∈H, g = P∞
⟨f, ep⟩H ep, f ∈R(Σ1/2)}, we observe that
Σ1/2 is a one-to-one mapping between R−1(Σ1/2) and R(Σ1/2). Thus, restricting the domain of Σ1/2 to R−1(Σ1/2), we may deﬁne its inverse for all f ∈R(Σ1/2) as Σ−1/2f =
⟨f, ep⟩H ep. The null space may be reduced to the null element (in particular
for the gaussian kernel), or may be inﬁnite-dimensional. Similarly, there may be inﬁnitely
many strictly positive eigenvalues (true nonparametric case) or ﬁnitely many (underlying
ﬁnite-dimensional problems).
Given a sample {X1, . . . , Xn}, the empirical estimates respectively of the mean element
and the covariance operator are then deﬁned as follows:
k(Xi, ·) ,
k(Xi, ·) ⊗k(Xi, ·) −ˆµ ⊗ˆµ .
By the reproducing property, they lead, on the one hand, to empirical means as from (3) we
have ⟨ˆµ, f⟩= n−1 Pn
i=1 f(Xi) for all f ∈H, and on the other hand, to empirical covariances
as from (4) we have ⟨f, ˆΣg⟩H = n−1 Pn
i=1 f(Xi)g(Xi)−{n−1 Pn
i=1 f(Xi)}{n−1 Pn
i=1 g(Xi)}
for all f, g ∈H.
3. KFDA-based test statistic
Our two-sample homogeneity test can be formulated as follows. Let {X(1)
1 , . . . , X(1)
1 , . . . , X(2)
n2 } two independent identically distributed samples (iid) respectively from P1
and P2, having mean and covariance operators given by (µ1, Σ1) and (µ2, Σ2). We build our
test statistics using a (regularized) kernelized version of the Fisher discriminant analysis.
Denote by ΣW
= (n1/n)Σ1 +(n2/n)Σ2 the pooled covariance operator, where n def
corresponding to the within-class covariance matrix in the ﬁnite-dimensional setting .
3.1 Maximum Kernel Fisher Discriminant Ratio
Let us denote ΣB
= (n1n2/n2)(µ2 −µ1) ⊗(µ2 −µ1) the between-class covariance operator.
For a = 1, 2, denote by (ˆµa, ˆΣa) respectively the empirical estimates of the mean element
and the covariance operator, deﬁned as previously stated in (3) and (4). Denote ˆΣW
(n1/n)ˆΣ1 +(n2/n)ˆΣ2 the empirical pooled covariance estimator, and ˆΣB
= (n1n2/n2)(ˆµ2 −
ˆµ1)⊗(ˆµ2 −ˆµ1) the empirical between-class covariance operator. Let {γn}n≥0 be a sequence
of strictly positive numbers. The maximum kernel Fisher discriminant ratio serves as a
Testing for Homogeneity with KFDA
basis of our test statistics:
f, (ˆΣW + γnI)f
(ˆΣW + γnI)−1/2(ˆµ2 −ˆµ1)
where I denotes the identity operator.
Note that if the input space is Euclidean, e.g.,
X = Rd, the kernel is linear k(x, y) = xT y and γn = 0, this quantity matches the so-called
Hotelling’s T 2-statistic in the two-sample case .
We shall make the following assumptions respectively on Σ1 and Σ2
(B1) For u = 1, 2, the eigenvalues {λp(Σu)}p≥1 satisfy P∞
(B2) For u = 1, 2, there are inﬁnitely many strictly positive eigenvalues {λp(Σu)}p≥1 of Σu.
The statistical analysis conducted in Section 4 shall demonstrate, in the case γn →0,
the need to respectively recenter and rescale (a standard statistical transformation known as
studentization) the maximum Fisher discriminant ratio, in order to get a theoretically wellgrounded test statistic. These roles, recentering and rescaling, will be played respectively by
d1(ΣW, γ) and d2(ΣW, γ), where for a given compact operator Σ with decreasing eigenvalues
λp, the quantity dr(Σ, γ) is deﬁned for all q ≥1 as
dr(Σ, γ) def
(λp + γ)−rλr
3.2 Computation of the test statistics
In practice the test statistics may be computed thanks to the kernel trick, adapted to
the kernel Fisher discriminant analysis as outlined in . Let us consider the two samples {X(1)
1 , . . . , X(1)
n1 } and {X(2)
n1 , . . . , X(2)
n2 }, with
n1 + n2 = n. Denote by G(u)
: Rnu 7→H, u = 1, 2, the linear operators which associates to
a vector α(u) = [α(u)
1 , . . . , α(u)
nu ]T the vector in H given by G(u)
n α(u) = Pnu
This operator may be presented in a matrix form
, ·), . . . , k(X(u)
We denote by Gn =
. We denote by K(u,v)
n , u, v ∈{0, 1}, the
Gram matrix given by K(u,v)
(i, j) def
) for i ∈{1, . . . , nu}, j ∈{1, . . . , nv}.
Deﬁne, for any integer ℓ, Pℓ= Iℓ−ℓ−11ℓ1T
ℓwhere 1ℓis the (ℓ×1) vector whose components
are all equal to one and Iℓis the (ℓ× ℓ) identity matrix and let Nn be given by
Finally, deﬁne the vector mn = (mn,i)1≤i≤n with mn,i = −n−1
for i = 1, . . . , n1 and
mn,i = n−1
for i = n1 + 1, . . . , n1 + n2. With the notations introduced above,
ˆµ2 −ˆµ1 = Gnmn ,
n )T , u = 1, 2 ,
ˆΣW = n−1GnNnNT
which implies that
ˆµ2 −ˆµ1, (ˆΣW + γI)−1(ˆµ2 −ˆµ1)
n(n−1GnNnNT
n + γI)−1Gnmn .
Then, using the matrix inversion lemma, we get
n(n−1GnNnNT
n + γI)−1Gnmn
I −n−1GnNn(γI + n−1NT
nGnNn)−1NT
nKnmn −n−1mT
nKnNn(γI + n−1NnKnNn)−1NnKnmn
Hence, the maximum kernel Fisher discriminant ratio may be computed from
(ˆΣW + γnI)−1/2(ˆµ2 −ˆµ1)
nKnmn −n−1mT
nKnNn(γI + n−1NnKnNn)−1NnKnmn
4. Main results
This discussion yields the following normalized test statistics:
(ˆΣW + γnI)−1/2ˆδ
H −d1(ˆΣW, γn)
2 d2(ˆΣW, γn)
In this paper, we ﬁrst consider the asymptotic behavior of bTn under the null hypothesis,
and against a ﬁxed alternative. This will establish that our nonparametric test procedure is
consistent. However, this is not enough, as it can be arbitrarily slow. We thus then consider
local alternatives.
For all our results, we consider two situations regarding the regularization parameter γn;
(a) a situation where γn is held ﬁxed, and in which the limiting distribution is somewhat
similar to the maximum mean discrepancy test statistics, and (b) a situation where γn tends
to zero slowly enough, and in which we obtain qualitatively diﬀerent results.
4.1 Limiting distribution under null hypothesis
Throughout this paper, we assume that the proportions n1/n and n2/n converge to strictly
positive numbers, that is
as n = n1 + n2 →∞,
with ρu > 0 for u = 1, 2 .
In this section, we derive the distribution of the test statistics under the null hypothesis
H0 : P1 = P2 of homogeneity, which implies µ1 = µ2 and Σ1 = Σ2 = ΣW.
consider the case where the regularization factor is held constant γn ≡γ. We denote
the convergence in distribution.
Testing for Homogeneity with KFDA
Theorem 1. Assume (A1-B1). Assume in addition that the probability distributions P1
and P2 are equal, i.e. P1 = P2 = P, and that γn ≡γ > 0. Then,
−→T∞(ΣW , γ) def
= 2−1/2 d−1
(λp(ΣW) + γ)−1λp(ΣW )(Z2
where {λp(ΣW)}p≥1 are the eigenvalues of the covariance operator ΣW , and d2(ΣW , γ) is
deﬁned in (6), and Zp, p ≥1 are independent standard normal variables.
If the number of non-vanishing eigenvalues is equal to p and if γ = 0, then the limiting
distribution coincides with the limiting distribution of the Hotelling T 2 for comparisons of
two p-dimensional vectors . The previous result is similar to what is obtained by Gretton et al.
 for the Maximum Mean Discrepancy test statistics (MMD), we obtain a weighted
sum of chi-squared distributions with summable weights. For a given level α ∈ , denote
by t1−α(ΣW, γ) the (1 −α)-quantile of the distribution of T∞(ΣW, γ). Then, the sequence
of test bTn(γ) ≥t1−α(ΣW , γ), is pointwise asymptotically level α to test homogeneity. Because in practice the covariance ΣW is unknown, it is not possible to compute the quantile
t1−α(ΣW , γ). Nevertheless, this quantile can still be consistently estimated by t1−α(ˆΣW , γ),
which can be obtained from the sample covariance matrix (see Proposition 24).
Corollary 2. The test bTn(γ) ≥t1−α(ˆΣW, γ) is pointwise asymptotically level α.
In practice, the quantile t1−α(ˆΣW, γ) can be numerically computed by inverse Laplace
transform .
For all γ > 0, the weights {(λp + γ)−1λp}p≥1 are summable.
However, if Assumption (B2) is satisﬁed, both d1,n(γ, ΣW ) and d1,n(γ, ΣW ) tend to inﬁnity when n →0. The
following theorem shows that if γn tends to zero slowly enough, then our test statistics is
asymptotically normal:
Theorem 3. Assume (A1), (B1-B2). Assume in addition that the probability distributions
P1 and P2 are equal, i.e. P1 = P2 = P and that the sequence {γn} is such that
2 (ΣW , γn)d1(ΣW , γn)γ−1
n n−1/2 →0 ,
−→N(0, 1) .
The proof of the theorem is postponed to Section 9. Under the assumptions of Theorem 3, the sequence of tests that rejects the null hypothesis when ˆTn(γn) ≥z1−α, where
z1−α is the (1 −α)-quantile of the standard normal distribution, is asymptotically level α.
Contrary to the case where γn ≡γ, the limiting distribution does not depend on the
reproducing kernel, nor on the sequence of regularization parameters {γ}n≥1. However,
notice that d−1
2 (ΣW , γn)d1(ΣW , γn)γ−1
n n−1/2 →0 requires that {γ}n≥1 goes to zero at a
slower rate than n−1/2. For instance, if the eigenvalues {λp}p≥1 decrease at a polynomial
rate, that is if there exists s > 0 such that we have λp = p−s for all p ≥1, then, by Lemma 20,
we have d1(ΣW , γn) ∼γ−1/s
and d2(ΣW , γn) ∼γ−1/2s
as n →∞. Therefore, the condition
2 (ΣW, γn)d1(ΣW, γn)γ−1
n n−1/2 →0 entails in this particular case that γ−1
= o(n2s/1+4s),
where the rate of decay s of the eigenvalues of the covariance operator ΣW , depends both
on the kernel and the underlying distribution P1 = P2 = P. Besides, it may seem surprising
that the limiting distribution is normal.
This is due to two facts.
First, we regularize
the sample covariance operator prior to inversion (being of ﬁnite rank, the inverse of ˆΣ
is obviously not deﬁned). Second, the problem is here truly inﬁnite dimensional, because
we have assumed that the eigenvalues are inﬁnite dimensional λp(ΣW ) > 0 for all p .
4.2 Limiting behavior against ﬁxed alternatives
We study the power of the test based on bTn(γn) under alternative hypotheses. The minimal
requirement is to prove that this sequence of tests is consistent. A sequence of tests of
constant level α is said to be consistent in power if the probability of accepting the null
hypothesis of homogeneity goes to zero as the sample size goes to inﬁnity under a ﬁxed
alternative. Recall that two probability P1 and P2 deﬁned on a measurable space (X, X)
are called singular if there exist two disjoint sets A and B in X whose union is X such that
P1 is zero on all measurable subsets of B while P2 is zero on all measurable subsets of B.
This is denoted by P1 ⊥P2.
When γn ≡γ or when γn →0, and P1 and P2 are not singular, then the following
proposition shows that the limits in both cases are ﬁnite, strictly positive and independent
of the kernel otherwise . The following result gives some useful insights on ∥Σ−1/2
(µ2 −µ1)∥H, the
population counterpart of ∥(ˆΣW + γnI)−1/2(ˆµ2 −ˆµ1)∥H on which our test statistics is based
Proposition 4. Assume (A1-A2). Let ν a measure dominating P1 and P2, and let p1 and
p2 the densities of P1 and P2 with respect to ν. The norm ∥Σ−1/2
(µ2−µ1)∥H is inﬁnite if and
only if P1 and P2 are mutually singular. If P1 and P2 are nonsingular, ∥Σ−1/2
(µ2 −µ1)∥H
is ﬁnite and is given by
ρ1p1 + ρ2p2
ρ1p1 + ρ2p2
It is equal to zero if the χ2-divergence is null, that is, if and only if P1 = P2.
By combining the two previous propositions, we therefore obtain the following consistency theorem:
Theorem 5. Assume (A1-A2). Let P1 and P2 be two distributions over (X, X), such that
P2 ̸= P1. If either γn ≡γ or γn + d−1
2 (Σ1, γn)d1(Σ1, γn)γ−1
n n−1/2 →0, then for any t > 0
PHA( bTn(γ) > t) →1 .
4.3 Limiting distribution against local alternatives
When the alternative is ﬁxed, any sensible test procedure will have a power that tends to
one as the sample size n tends to inﬁnity. This property is not suitable for comparing the
Testing for Homogeneity with KFDA
limiting power of diﬀerent test procedures. Several approaches are possible to answer this
question. One such approach is to consider sequences of local alternatives . Such alternatives tend to the null hypothesis as n →∞at a rate which
is such that the limiting distribution of sequence the test statistics under the sequence of
alternatives converge to a non-degenerate random variable. To compare two sequences of
tests for a given sequence of alternatives, one may then compute the ratio of the limiting
powers, and choose the test which has the largest power.
In our setting, let P1 denote a ﬁxed probability on (X, X) and let Pn
2 be a sequence of
probability on (X, X). The sequence Pn
2 depends on the sample size n and converge to P1 as
n goes to inﬁnity with respect to a certain distance. In the asymptotic analysis of our test
statistics against sequences of local alternatives, the χ2-divergence Dχ2 (P1 ∥Pn
2) is deﬁned
for all n as
2 absolutely continuous with respect to P1. Therefore, in the subsequent sections, we
shall make the following assumption:
(C) For any n, Pn
2 is absolutely continuous with respect to P1, and Dχ2 (P1 ∥Pn
n tends to inﬁnity.
The following theorem shows that under local alternatives, we get a series of shift in the
chi-squared distributions when γn ≡γ:
Theorem 6. Assume (A1), (B1), and (C). Assume in addition γn ≡γ > 0 and that
n = O(1), then
−→2−1/2d−1
(λp(Σ1) + γ)−1λp(Σ1){(Zp + an,p(γ))2 −1} ,
an,p(γ) = (n1n2/n)1/2 D
(Σ1 + γI)−1/2(µn
2 −µ1), ep
where {Zp}p≥1 are independent standard normal random variables, deﬁned on a common
probability space.
When the sequence of regularization parameters {γn}n≥1 tends to zero at a slower rate
than n−1/2, the test statistics is shown to be asymptotically normal, with the same limiting
variance as the one under the null hypothesis, but with a non-zero limiting mean, as detailed
in the next two results. While the former states the asymptotic normality under general
conditions, the latter highlights the fact that the asymptotic mean-shift in the limiting
distribution may be conveniently expressed from the limiting χ2-divergence of P1 and Pn
under additional smoothness assumptions on the spectrum of the covariance operator.
Theorem 7. Assume (A1), and (B1-2), and (C). Let {γn}n≥1 be a sequence such that
2 (Σ1, γn)d1(Σ1, γn)γ−1
n n−1/2 →0
2 (Σ1, γn)nη2
2 (Σ1, γn)d1(Σ1, γn)ηn →0 ,
where {ηn}n≥1 is deﬁned in (12). If the following limit exists,
(Σ1 + γnI)−1/2(µn
d2(Σ1, γn)
−→N(ρ1ρ2∆, 1) .
Corollary 8. Under the assumptions of Theorem 7, if there exists a > 0 such that
2 −µ1, Σ−1−a
and if the following limit exists,
n→∞d2(Σ1, γn)−1nη2
−→N(ρ1ρ2∆, 1) .
It is worthwhile to note that ρ1ρ2∆, the limiting mean-shift of our test statistics against
sequences of local alternatives does not depend on the choice of the reproducing kernel. This
means that, at least in the large-sample setting n →∞, the choice of the kernel is irrelevant,
provided that for some a > 0 we have
2 −µ1, Σ−1−a
H < ∞. Then, we get that
the sequences of local alternatives converge to the null at rate ηn = C d1/2
(Σ1, γn)n−1/2
for some constant C > 0, which is slower than the usual parametric rate n−1/2 since
d2(Σ1, γn) →∞as n →∞as shown in Lemma 18.
Note also that conditions of the
2 −µ1, Σ−1−α
H < ∞imply that the sequence of local alternatives are
limited to smooth enough densities pn
2 around p1.
5. Discussion
We illustrate now the behaviour of the limiting power of our test statistics against two
diﬀerent types of sequences of local alternatives. Then, we compare the power of our test
statistics against the power of the Maximum Mean Discrepancy test statistics proposed
by Gretton et al. . Finally, we highlights some links between testing for homogeneity
and supervised binary classiﬁcation.
5.1 Limiting power against local alternatives of KFDA
We have seen that our test statistics is consistent in power against ﬁxed alternatives, for
both regularization schemes γn ≡γ and γn →0. We shall now examine the behaviour of
the power of our test statistics, against diﬀerent types of sequences of local alternatives:
i) directional alternatives, ii) non-directional alternatives. For this purpose, we consider
a speciﬁc reproducing kernel, the periodic spline kernel, whose derivation is given below.
Indeed, when P1 is the uniform distribution on , and dP2/dP1 = 1 + ηcq with cq is a
one-component contamination on the Fourier basis, we may conveniently compute a closedform equivalent when n →∞of the eigenvalues of the covariance operator Σ1, and therefore
the power function of the test statistics.
Testing for Homogeneity with KFDA
Periodic spline kernel
The periodic spline kernel, described in , is deﬁned as follows. Any function f in L2(X), where X is taken as the torus R/2πZ,
may be expressed in the form of a Fourier series expansion f(t) = P∞
p=0 apcp(t) where
p, and for all ℓ≥1
c0(t) = 1X
c2ℓ−1(t) =
2 sin(2π(2ℓ−1)t)
2 cos(2π(2ℓ−1)t) .
Let us consider the family of RKHS deﬁned by Hm = {f : f ∈L2(X), P∞
with m > 1, where λp = (2πp)−2m for all p ≥1, whose norm is deﬁned for all f ∈L2(X) as
(2πp)−2ma2
Therefore, the associated reproducing kernel k(x, y) writes as
km(x, y) = 2
(2πp)−2mcp(x −y) = (−1)m−1
(2m)! B2m ((x −y) −⌊x −y⌋) ,
where B2m is the 2m-th Bernoulli polynomial.
The set {ep(t), p ≥1} is actually an orthonormal basis of H, where ep(t) def
for all p ≥1.
Let us consider P1 the uniform probability measure on .
ep −EP1[ep] ≡ep and µ1 ≡0, where µ1 is the mean element associated with P1. Hence,
{(λp, ep(t)), p ≥1} is an eigenbasis of Σ1 the covariance operator associated with P1, where
for all ℓ≥1
λ2ℓ−1 = (4πℓ)−2m
λ2ℓ= (4πℓ)−2m .
Note that the parameter m characterizes the RKHS Hm and its associated reproducing
kernel km(·, ·), and therefore controls the rate of decay of the eigenvalues of the covariance
operator Σ1. Indeed, by Lemma 20, we have d1(Σ1, γn) = C1 γ−1/2m
and d2(Σ1, γn) =
for some constants C1, C2 > 0 as n →∞.
Directional alternatives
Let us consider the limiting power of our test statistics in the
following setting:
H0 : P1 = Pn
A : P1 ̸= Pn
2, with Pn
2 such that dPn
2/dP1 = 1 + An−1/2cq ,
where P1 is the uniform probability measure on , and cq(t) is deﬁned in (17). In the
case γn ≡γ, given a signiﬁcance level α ∈(0, 1), the associated critical level t1−α is deﬁned
as satisfying
(λp(Σ1) + γ)−1λp(Σ1){Z2
p −1} > t1−α
Note that an,p(γ) = 0 for all p ≥1 (from Theorem 6) except for p = q where
n1n2/n2 (λq + γ)−1/2λ1/2
Figure 1: Evolution of power of KFDA as γ = 1, 10−1, . . . , 10−9, for q-th component alternatives with (from left to right) with q = 1, 5, 9.
In order to analyze the behaviour of the power for varying values of γ and for diﬀerent
values of q, we compute the limiting power, when taking m = 2 in the periodic reproducing
kernel, and for q = 1, 5, 9, and investigate the evolution of the power as a function of the
regularization parameter γ. As Figure 1 shows, our test statistics has trivial power, that is
equal to α, when γ ≫λq, while it reaches stricly nontrivial power as long as γ ≤λq. This
motivates the study of the decaying regularization scheme γn →0 of our test statistics, in
order to incorporate the γ →0 into our large-sample framework. In the next paragraph,
we shall demonstrate that the version of our test statistics with decaying regularization
parameter γn →0 reaches high power against a broader class of local alternatives, which we
call non-directional alternatives, where q ≡qn →∞, as opposed to directional alternatives
where q was kept constant. Yet, for having nontrivial power with the test statistics bT(γn)
against such sequences of local alternatives, the non-directional sequence of local alternatives
have to converge to the null at a slower rate than √n.
Non-directional alternatives
Now, we consider the limiting power of our test statistics
in the following setting:
H0 : P1 = Pn
A : P1 ̸= Pn
2, with Pn
2 such that dPn
2/dP1 = 1 + ηncqn ,
Assume P1 is the uniform probability measure on , and consider again the periodic
spline kernel of order 2m.
Take {qn}n≥1 a nonnegative nondecreasing sequence of integers.
Now, if the sequence of local alternatives is converging to the null at rate ηn =
(2∆)1/2q1/4
n n−1/2 for some ∆> 0, with qn = o(n1/1+4m) for our asymptotic analysis to
hold, then as long as γn ≡λqn = q−2m
bTn(γn) > z1−α
A (Z1 + ρ1ρ2∆> z1−α)
= 1 −Φ [z1−α −ρ1ρ2∆] .
Testing for Homogeneity with KFDA
Normal tails
 exp(−cp1/d)
 logd(1/γ)
logd/2(1/γ)
Polynomial tails
for any β > α
Table 1: examples of rate of convergence for the gaussian kernels for X = Rp
where we used Lemma 20 together with Theorem 7. On the other hand, if γ−1
then the limiting power is trivial and equal to α.
Back to the ﬁxed-regularization test statistics bTn(γ), we may also compute the limiting
power of bTn(γ) against the non-directional sequence of local alternatives deﬁned in (25)
by taking into account Remark 17 to use Theorem 6. Indeed, as n tends to inﬁnity, since
an,qn(γ) = (ρ1ρ2)1/2(λqn + γ)−1/2λqnηn, then the ﬁxed-regularization version bTn(γ) of the
test statistics has trivial power against non-directional alternatives.
Remark 9. We analyzed the limiting power of our test statistics in the speciﬁc case where
P1 is the uniform distribution on and the reproducing kernel belongs to the family of
periodic spline kernels. Yet, our ﬁndings carry over more general settings as illustrated by
Table 1. Indeed, for general distributions with polynomial decay in the tail and (nonperiodic)
gaussian kernels, the eigenvalues of the covariance operator still exhibit similar behaviour
as in the example treated above.
We now discuss the links between our procedure with the previously proposed Maximum
Mean Discrepancy (MMD) test statistics. We also highlight interesting links with supervised
kernel-based classiﬁcation.
5.2 Comparison with Maximum Mean Discrepancy
Our test statistics share many similarities with the Maximum Mean Discrepancy test statistics of Gretton et al. . In the case γn ≡γ , both have limiting null distribution which
may be expressed as an inﬁnite weighted mixture of chi-squared random variables. Yet,
while bT MMD
p −1) where bT MMD
denotes the test statistics used by MMD,
we have in our case bT KFDA
p=1(λp + γn)−1λp(Z2
p −1). Roughly speaking, the
test statistics based on KFDA uniformly weights the components associated with the ﬁrst
eigenvalues of the covariance operator, and downweights the remaining ones, which allows
to gain greater power for testing by focusing on the user-tunable number of components of
the covariance operator. On the other hand, the test statistics based on MMD is naturally
sensitive to diﬀerences lying on the ﬁrst components, and gets progressively less sensitive
to diﬀerences in higher components. Thus, our test statistics based on KFDA allows to
give equal weights to diﬀerences lying in (almost) all components, the eﬀective number of
components on the which the test statistics focus on being tuned via the regularization
parameter γn. These diﬀerences may be illustrated by considering the behavuour of MMD
against sequences of local alternatives respectively with ﬁxed-frequency and non-directional,
for periodic kernels.
Directional alternatives
Let us consider the setting deﬁned in (24). By a similar reasoning, we may also compute the limiting power of bT MMD
against directional sequences of
local alternatives, with a periodic spline kernel of order m = 2, for diﬀerent components
q = 1, 5, 9. Both test statistics KFDA and MMD
reach high power when the sequences
of local alternatives lies on the ﬁrst component.
However, the power of MMD tumbles
down for higher-order alternatives whereas the power of KFDA remains strictly nontrivial
for high-order alternatives as long as γ is suﬃciently small.
Figure 2: Comparison of the evolution of power of KFDA versus the power of MMD
γ = 1, 10−1, . . . , 10−9, for q-th component alternatives with (from left to right)
with q = 1, 5, 9.
Non-directional alternatives
Now, consider sequences of local alternatives as deﬁned
in (25). The test statistics MMD does not notice such alternatives. Therefore, MMD has
trivial power equal to α against non-directional alternatives.
5.3 Links with supervised classiﬁcation
When the sample sizes of each sample are equal, that is when n1 = n2, KFDA is known to be
equivalent to Kernel Ridge Regression (KRR), also referred to as smoothing spline regression
in statistics. In this case, KRR performs a kernel-based least-square regression ﬁt on the
labels, where the samples are respectively labelled −1 and +1. The recentering parameter
d1(Σ1, γn) in our procedure coincides with the so-called degrees of freedom in smoothing
spline regression, which were often advocated to provide a relevant measure of complexity for
model selection . In particular, since the mean-shift in the limiting normal
distribution against local alternatives is lower-bounded by nd−1
1 (Σ1, γn)⟨(µ2 −µ1), (Σ1 +
γnI)−1(µ2 −µ1)⟩, this suggests an algorithm for selecting γn and the kernel. For a ﬁxed
degree of freedom d1(Σ1, γn), maximizing the asymptotic mean-shift (which corresponds to
the class separation) is likely to yield greater power. As future work, we plan to investigate,
both theoretically and practically, the use of (single and multiple) kernel learning procedures
as developed by Bach et al. for maximizing the expected power of our test statistics
in speciﬁc applications.
6. Experiments
In this section, we investigate the experimental performances of our test statistic KFDA,
and compare it in terms of power against other nonparametric test statistics.
Testing for Homogeneity with KFDA
Figure 3: Comparison of ROC curves in a speaker veriﬁcation task
6.1 Speaker veriﬁcation
We conducted experiments in a speaker veriﬁcation task Bimbot et al. , on a subset
of 8 female speakers using data from the NIST 2004 Speaker Recognition Evaluation. We
refer the reader to for instance for details on the pre-processing
of data. The ﬁgure shows averaged results over all couples of speakers. For each couple
of speaker, at each run we took 3000 samples of each speaker and launched our KFDA-test
to decide whether samples come from the same speaker or not, and computed the type II
error by comparing the prediction to ground truth. We averaged the results for 100 runs
for each couple, and all couples of speaker. The level was set to α = 0.05, and the critical
values were computed by a bootstrap resampling procedure. Since the observations may
be considered dependent within the sequences, and independent between the sequences, we
used a ﬁxed-block variant of the boostrap, which consists in using boostrap samples built by
piecing together several boostrap samples drawn in each sequence. We performed the same
experiments for the Maximum Mean Discrepancy and the Tajvidi-Hall test statistic (TH).
We summed up the results by plotting the ROC-curve for all competing methods. Our
method reaches good empirical power for a small value of the prescribed level (1−β = 90%
for α = 0.05%). Maximum Mean Discrepancy also yields good empirical performance on
this task.
7. Conclusion
We proposed a well-calibrated kernel-based test statistic for testing the homogeneity of two
samples, built on the kernel Fisher discriminant analysis algorithm, for which we proved
that the asymptotic limit distribution under null hypothesis is standard normal distribution
when de regularization parameter decays to zero at a slower rate than n−1/2. Besides, our
test statistic can be readily computed from Gram matrices once a reproducing kernel is
deﬁned, and reaches nontrivial power aqgainst a large class of alternatives under mild conditions on the regularization parameter. Finally, our KFDA-test statistic yields competitive
performance for speaker identiﬁcation purposes.
8. Proof of some preliminary results
We preface the proof by some useful results relating the KFDA statistics to kernel independent quantities.
Proposition 10. Assume (A1)-(A2). Let P1 and P2 be two probability distributions on
(X, X), and denote by µ1, µ2 the associated mean (see (1)). Let Q be a probability dominating
P1 and P2, and let Σ be the associated covariance operator (see (2)). Then,
if and only if the vector (µ2 −µ1) ∈H belongs to the range of the square root Σ1/2. In
µ2 −µ1, Σ−1(µ2 −µ1)
Proof. Denote by {λk}k≥1 and {ek}k≥1 the strictly positive eigenvalues and the corresponding eigenvectors of the covariance operator Σ, respectively. For k ≥1, set
fk = λ−1/2
{ek −Qek} .
By construction, for any k, ℓ≥1,
λkδk,ℓ= ⟨ek, Σeℓ⟩H = ⟨ek −Qek, eℓ−Qeℓ⟩L2(Q) = λ1/2
⟨fk, fℓ⟩L2(Q) ,
where δk,ℓis Kronecker’s delta. Hence {fk}k≥1 is an orthonormal system of L2(Q). Note
that µ2 −µ1 belongs to the range of Σ1/2 if and only if
(a) ⟨µ2 −µ1, g⟩H = 0 for all g in the null space of Σ,
µ1 −µ2, Σ−1(µ1 −µ2)
⟨ep, (µ1 −µ2)⟩2
Consider ﬁrst condition (a). For any g ∈H, il follows from the deﬁnitions that
⟨µ2 −µ1, g⟩H =
(dP1 −dP2) g =
(dP1 −dP2) (g −Qg)
dQ , g −Qg
If g belongs to the null space of Σ, then ∥g−Qg∥L2(Q) = 0, and the previous relation implies
that ⟨µ2 −µ1, g⟩H = 0. Consider now (b).
⟨ep, (µ1 −µ2)⟩2
{dP1(x) −dP2(x)}ep(x)
Testing for Homogeneity with KFDA
In order to prove the equality, we simply notice that because of the density of the RKHS
in L2(Q), then {fk}k≥1 is a complete orthonormal basis of the space of functions L2
g ∈L2(Q) ,
(g −Qg)2dQ > 0
Lemma 11. Assume (A1)-(A2). Let P1 and P2 two probability distributions on (X, X) such
that P2 ≪P1.
Denote by Σ1 and Σ2 the associated covariance operators. Then, for any γ > 0,
Tr{(Σ1 + γI)−1(Σ2 −Σ1)}
≤2d2(Σ1, γ)
where d2(Σ1, γ) is deﬁned in (6).
Proof. Denote by {λk}k≥1 and {ek}k≥1 the strictly positive eigenvalues and the corresponding eigenvectors of the covariance operator Σ1. Note that ⟨ek, Σ1eℓ⟩= λkδk,ℓfor all k and
ℓ. Let us denote fk = λ−1/2
{ek −P1ek}. Then, we have ⟨fk, fℓ⟩L2(P1) = δk,ℓ. Note that
δk,ℓ−λ−1/2
⟨ek, Σ2eℓ⟩H
⟨µ2 −µ1, ek⟩H ⟨µ2 −µ1, eℓ⟩H
Then, using that (a + b)2 ≤2(a2 + b2), and (28) in Proposition 10 with Σ = Σ1, we obtain
Denote, for all p, q ≥1
ep, (Σ−1/2
By applying the H¨older inequality, and using (30), we get
Tr{(Σ1 + γI)−1(Σ2 −Σ1)}
ep, (Σ1 + γI)−1Σ1ep
ep, (Σ1 + γI)−1Σ1ep
≤2d2(Σ1, γ)
which completes the proof of (31).
Proposition 12. Assume (A1). Let {Xn
1 , . . . , Xn
n} be a triangular array of i.i.d random
variables, whose mean element and covariance operator are respectively (µn, Σn). If, for all
n all the eigenvalues λp(Σn) of Σn are non-negative, and if there exists C > 0 such that for
all n we have P∞
(Σn) < C, then P∞
p=1 |λp(ˆΣ −Σn)| = OP (n−1/2).
Proof. Lemma 21 shows that, for any orthonormal basis {ep}p≥1 in the RKHS H:
|λp(ˆΣ −Σn)| ≤
(ˆΣ −Σn)ep
We take the orthonormal family of eigenvectors {ep}p≥1 of the covariance operator Σn
(associated to the eigenvalues λp(Σn) ranked in decreasing order). Then, it suﬃces to show
(ˆΣ −Σn)ep
H = OP (n−1/2). Note that,
where ep,n = ep −En[ep(X1)] and
= k(Xi, ·)ep,n(Xi) −En {k(X1, ·)ep,n(X1)}
By the Minkowski inequality,
= A1 + A2 .
We consider these two terms separately. Consider ﬁrst A1. We have
1 = n−1En ∥ζp,n,i∥2
H ≤n−1En n
∥k(X1, ·)∥2
H |ep,n(X1)|2o
≤n−1|k|∞En 
|ep,n(X1)|2
Consider now A2. Since
i=1 k(Xi, ·)
H ≤|k|∞, we have
2 ≤n−1|k|∞En 
|ep,n(X1)|2
This shows, using the Minkowski inequality, that
(ˆΣ −Σn)ep
|ep,n(X1)|2 1/2 .
Since by assumption P∞
|ep,n(X1)|2 1/2 = P∞
(Σn) < ∞, the proof is concluded.
Testing for Homogeneity with KFDA
Corollary 13. Assume (A1). Let {X(1)
1,n1, . . . , X(1)
n1,n1} and {X(2)
1,n2, . . . , X(2)
n2,n2} be two triangular arrays, whose mean elements and covariance operators are respectively (µn
2), where n1/n →ρ1 and n2/n →ρ2 as n tends to inﬁnity. If supn⩾0
|λp(ˆΣW −ΣW)| = OP (n−1/2) .
In addition, we also have
HS = OP (n−1/2) .
Proof. Since ˆΣW −ΣW = n1n−1(ˆΣ1 −Σ1) + n2n−1(ˆΣ2 −Σn
(ˆΣW −ΣW)ep
(ˆΣ1 −Σ1)ep
and applying twice Proposition 12 leads to (34). Now, using that
|λp(ˆΣW −ΣW)| ,
then (35) follows as a direct consequence of (34).
9. Asymptotic approximation of the test statistics
The following proposition shows that in the asymptotic study of our test statistics, we can
replace most empirical quantities by population quantities. For ease of notation, we shall
denote µ2 −µ1 by δ. ˆµ2 −ˆµ1 by ˆδ.
Proposition 14. Assume (C). If
2 (Σ1, γn)d1(Σ1, γn)γ−1
n n−1/2 →0
2 (Σ1, γn)nη2
2 (Σ1, γn)d1(Σ1, γn)ηn →0 ,
then, bTn(γn) = ˜Tn(γn) + oP (1), where
˜Tn(γ) def
(Σ1 + γI)−1/2 ˆδ
H −d1(Σ1, γ)
2d2(Σ1, γ)
Proof. Notice that
|d2(ˆΣW , γn) −d2(Σ1, γn)| ≤|d2(ˆΣW, γn) −d2(ΣW , γn)| + |d2(ΣW, γn) −d2(Σ1, γn)| .
Then, on the one hand, using Eq. (77) for r = 2 in Lemma 23 with S = ΣW and ∆=
ˆΣW −ΣW and Eq. (34) in Corollary 13, we get
d2(ˆΣW , γn) −d2(ΣW , γn)
On the other hand, using Eq. (79) in Lemma 23 with S = Σ1 and ∆= n2n−1(Σn
Σ1), we get d2(ΣW , γn) −d2(Σ1, γn) = O(ηn).
Furthermore, similar reasoning, using
Eq. (77) and Eq. (78) again in Lemma 23 allows to prove that d−1
2 (Σ1, γn)d1(ˆΣW, γn) =
2 (Σ1, γn)d1(Σ1, γn) + oP (1). Next, we shall prove that
(ˆΣW + γnI)−1/2ˆδ
(Σ1 + γnI)−1/2ˆδ
(d1(Σ1, γn) + nη2
n n−1/2 + ηn)
Using straightforward algebra, we may write
(ˆΣW + γnI)−1/2ˆδ
(Σ1 + γnI)−1/2ˆδ
≤A1A2 {B1 + B2} ,
(Σ1 + γnI)−1/2ˆδ
(ˆΣW + γnI)−1/2(ˆΣW −ΣW)(Σ1 + γnI)−1/2
(ˆΣW + γnI)−1/2ˆδ
(ˆΣW + γnI)−1/2(Σn
2 −Σ1)(Σ1 + γnI)−1/2
We now prove that
1 = OP (n−1d1(Σ1, γn) + η2
2 = OP (n−1d1(Σ1, γn) + η2
We ﬁrst consider (40). Note that E
= δn ⊗δn + n−1
1 Σ1 + n−1
2, which yields
E∥(Σ1 + γnI)−1/2ˆδ∥2 = Tr
(Σ1 + γnI)−1E
δn, (Σ1 + γnI)−1δn
(Σ1 + γnI)−1Σ1
(Σ1 + γnI)−1 (Σn
Using Proposition 10 with Σ = Σ1 together with Assumption (C), we may write
δn, (Σ1 + γnI)−1δn
Next, applying Lemma 11, we obtain
Tr{(Σ1 + γnI)−1(Σn
= O(d2(Σ1, γn)ηn) ,
which yields
E∥(Σ1 + γnI)−1/2ˆδ∥2 = (n/n1n2)d1(Σ1, γn) {1 + O(ηn)} + O(η2
Finally, we get (40) by the Markov inequality. Now, to prove (41), it suﬃces to observe that
(ˆΣW + γnI)−1(Σ1 + γnI)
= 1+oP (1), and then conclude from (40). Next, using the upperbound
(Σ + γnI)−1/2
, and Corollary 13 which gives
HS = OP (n−1/2),
B1 = OP (γ−1
n n−1/2) .
Testing for Homogeneity with KFDA
Finally, under Assumption (C), using Eq. (30) in Lemma 11, we obtain
B2 = OP (ηn) .
The proof of (38) is concluded by plugging (40-41-44-45) into (39).
Remark 15. For the sake of generality, we proved the approximation result under the
assumptions γn + d−1
2 (Σ1, γn)d1(Σ1, γn)γ−1
n n−1/2 →0 on the one hand, d−1
2 (Σ1, γn)nη2
O(1) and d−1
2 (Σ1, γn)d1(Σ1, γn)ηn →0 on the other hand. However, in the case γn ≡γ, the
approximation is still valid if nη3
n →0, which allows to use this approximation to derive the
limiting power of our test statistics against non-directional sequences of local alternatives
as in (25).
10. Proof of Theorems 6-7
For ease of notation, in the subsequent proofs, we shall often omit Σ1 in quantities involving
it. Hence, from now on, λp, λq, d2 stand for λp(Σ1), λq, d2(Σ1, γ). Deﬁne
) −E[ep(X(1)
1 ≤i ≤n1 ,
i−n1) −E[ep(X(2)
n1 + 1 ≤i ≤n .
The following lemma gives formulas for the moments of Yn,p,i, used throughout the actual
proof of the main results.
Lemma 16. Consider {Yn,p,i}1≤i≤n,p≥1 and as deﬁned respectively in (46) . Then
E[Yn,p,iYn,q,i] = λ1/2
{δp,q + n1n−1εp,q}
n,p,i, Y 2
n,q,i) ≤Cn−2|k|∞λ1/2
(1 + εp,p)1/2(1 + εq,q)1/2 .
Proof. The ﬁrst expressions are proved by elementary calculations from
E[Yn,p,1Yn,q,1] = n2
n1nδp,qλp(Σ1)
E[Yn,p,1Yn,q,n1+1] = 0,
E[Yn,p,n1+1Yn,q,n1+1] = n1
ep, (Σ−1/2
Next, notice that, for all p ≥1, we have by the reproducing property and the the Cauchy-
Schwarz inequality
|ep(x)| = ⟨ep, k(x, ·)⟩H ≤∥ep∥H ∥k(x, ·)∥H ≤|k|1/2
which yields
n,p,i, Y 2
n,q,i] + E[Y 2
n,p,i]E[Y 2
≤CE1/2[Y 4
n,p,i]E1/2[Y 4
≤Cn−1|k|∞E1/2[Y 2
n,p,i]E1/2[Y 2
≤Cn−2|k|∞λ1/2
(1 + εp,p)1/2(1 + εq,q)1/2 .
10.1 Proof of Theorem 6
Proof. The proof is adapted from . By Proposition 14,
ˆVn,∞(γ) −d1(Σ1, γ)
2d2(Σ1, γ)
+ oP (1) ,
ˆVn,∞(γ) def
(λp + γ)−1
ˆδ −δn, ep
ˆVn,N(γ) def
(λp + γ)−1
Because {Yn,p,i} are zero mean, independent, Lemma 16-Eq. (47) shows that, as n goes
to inﬁnity, Pn
i=1 Cov(Yn,p,i, Yn,q,i) →λ1/2
δp,q . In addition, the Lyapunov condition
is satisﬁed, since using (48), Pn
n,p,i] ≤Cn−1λp.
We may thus apply the central
limit theorem for multivariate triangular arrays, which yields Sn,N
−→N(0, ΛN) where
Sn,N = (Sn,1, . . . , Sn,N) and (ΛN)p,q = δp,qλp, 1 ≤p, q ≤N.
Fix u and let ǫ > 0 be
Then, using the version of the continuous mapping theorem stated in , with the sequence of quadratic functions {gn}n≥1 deﬁned as
[ gn : TN = (T1, . . . , TN) 7→(TN + an)T [diag(α1, . . . , αN)](TN + an) ], we may write
|E[eiu ˆVn,N(γ)] −E[eiuVn,N(γ)]| ≤ǫ ,
with Vn,N(γ) def
p=1(λp + γ)−1λp(Zp + an,p)2, where {Zp}p≥1 are independent standard
normal random variables, deﬁned on a common probability space, and {an,p}p≥1 are deﬁned
in (13). Next, we prove that limN→∞lim supn→∞E[(ˆVn,∞(γ) −ˆVn,N(γ))2] = 0. By the
Rosenthal inequality , there exists a constant C such that
n,p] ≤C(n−1λp + λ2
p). The Minkowski inequality then leads to
E1/2[(ˆVn,∞(γ) −ˆVn,N(γ))2]
(λp + γ)−1 E1/2
(n−1/2 + λ1/2
(λp + γ)−1 ⟨δn, ep⟩2
(λp + γ)−1 ⟨δn, ep⟩2
Testing for Homogeneity with KFDA
Notice that, using (28) in Proposition 10 with Σ = Σ1, we have
(λp + γ)−1 ⟨δn, ep⟩2 ≤nγ−1λN+1
⟨δn, ep⟩2 ≤γ−1λN+1 nη2
which goes to zero uniformly in n as N →∞. Therefore, under Assumptions (B1) and (C),
we may choose N large enough so that
|E[eiu ˆVn,∞(γ)] −E[eiu ˆVn,N(γ)]| < ǫ .
Similar calculations allow to prove that E[(Vn,∞(γ) −Vn,N(γ))2] = o(1), which yields that
for all ǫ > 0, for a suﬃciently large N, we have
|E[eiuVn,∞(γ)] −E[eiuVn,N(γ)]| < ǫ .
Finally, combining (51) and (53) (54), by the triangular inequality, we have proved that,
for ǫ > 0, we may choose a suﬃciently large N, such that
|E[eiu ˆVn,∞(γ)] −E[eiuVn,∞(γ)]| < ǫ ,
and the proof is concluded by invoking L´evy’s continuity theorem .
Remark 17. For the sake of generality, we proved the result under the assumption that
However, if there exists a nonnegative nondecreasing sequence of integers
{qn}n≥1 such that for all n we have P∞
p=1(λp + γ)−1 ⟨δn, ep⟩2 = (λqn + γ)−1 ⟨δn, eqn⟩2, then
the truncation argument used in (52) is valid under a weaker assumption. In particular,
when considering non-directional sequences of local alternatives as in (25), it suﬃces to take
N →∞such that N −1qn = o(1), which for n suﬃciently large allows to get n P∞
γ)−1 ⟨δn, ep⟩2 = 0 in place of (52) in the proof. The rest of the proof follows similarly.
The following lemma highlights the main diﬀerence between the asymptotics respectively
when γn ≡γ and γn →0, which is that d1(Σ1, γn) →∞and d2(Σ1, γn) →∞in the case
γn →0, whereas they acted as irrelevant constants in the case γn ≡γ.
Lemma 18. If γn = o(1), then, d1(Σ1, γn) →∞, and d2(Σ1, γn) →∞, as n tends to
Proof. Since the function x 7→x/(x + γn) is monotone increasing, for any λ ≥γn, λ/(λ +
γn) ≥1/2. Therefore,
λp(Σ1) + γn
2# {k ≤n : λp(Σ1) ≥γn} ,
and the proof is concluded by noting that since γn →0, # {k : λp(Σ1) ≥γn} →∞, as n
tends to inﬁnity.
The quantities λp(Σ1), λq(Σ1), d1(Σ1, γn), d2(Σ1, γn) being pervasive in the subsequent
proofs, they shall be respectively be abbreviated as λp, λq, d1,n, d2,n.
Our test statistics
writes as ˜Tn = (
2d2,n)−1An with
(Σ1 + γnI)−1/2ˆδ
Using the quantities Sn,p and Yn,p,i deﬁned respectively in (49) and (46), An may be expressed as
(λp + γn)−1
(λp + γn)−1
Sn,p ⟨δn, ep⟩
δn, (Σ1 + γnI)−1δn
(λp + γn)−1 
Since, by Lemma 16 Eq. (47), ES2
n,p −λp = (n1/n)λpεp,p, where εp,p is deﬁned in (33), then,
by H¨older inequality, we obtain
(λp + γn)−1 
(λp + γn)−2 λ2
= O(d2,nηn) .
We now decompose
(λp + γn)−1
Sn,p ⟨δn, ep⟩
= Bn + 2Cn + 2Dn ,
where Bn and Cn and Dn are deﬁned as follows
n,p,i −EY 2
(λp + γn)−1
(λp + γn)−1
The proof is in three steps. We will ﬁrst show that Bn is negligible, then that Cn is negligible,
and ﬁnally establish a central limit theorem for Dn.
Testing for Homogeneity with KFDA
Step 1: Bn = oP (1). The proof amounts to compute the variance of this term. Since the
variables Yn,p,i and Yn,q,j are independent if i ̸= j, then Var(Bn) = Pn
i=1 vn,i, where
(λp + γn)−1{Y 2
n,p,i −E[Y 2
(λp + γn)−1(λq + γn)−1Cov(Y 2
n,p,i, Y 2
Using Lemma 16, Eq. (48), we get
vn,i ≤Cn−1
(λp + γn)−1λ1/2
(1 + εp,p)1/2
{1 + O(ηn)}
where the RHS above is indeed negligible, since by assumption we have γ−1
n n−1/2 →0 and
Step 2: Cn = oP (d2
2,n). Again, the proof essentially consists in computing the variance of
this term, and then conclude by the Markov inequality. As previously, since the variables
Yn,p,i and Yn,q,j are independent if i ̸= j, then Var(Cn) = Pn
i=1 un,i, where
(λp + γn)−2E[Y 2
n,p,i]n1n2
(λq + γn)−1(λq + γn)−1E[Yn,p,iYn,q,i]n1n2
⟨δn, ep⟩⟨δn, eq⟩.
Moreover, note that E[Y 2
n,p,i] ≤Cn−1λp, and under Assumption (C1)
(λp + γn)−2λp ⟨δn, ep⟩2
(λp + γn)−1 ⟨δn, ep⟩2 ≤d−1
δn, (Σ1 + γn)−1δn
Similarly, for p ̸= q we have |E[Yn,p,iYn,q,i]| ≤Cn−1λ1/2
|εp,q|, which implies that
(λp + γn)−1(λq + γn)−1λ1/2
| ⟨δn, ep⟩|| ⟨δn, eq⟩||εp,q|
(λp + γn)−2λp ⟨δn, ep⟩2
Step 3: d−1
−→N(0, 1/2). We use the central limit theorem (CLT) for triangular array
of martingale diﬀerence . For = 1, . . . , n, denote
(λp + γn)−1Yn,p,iMn,p,i−1 ,
and let Fn,i = σ (Yn,p,j, p ∈{1, . . . , n}, j ∈{0, . . . , i}). Note that, by construction, ξn,i is a
martingale increment, that is E [ξn,i | Fn,i−1] = 0. The ﬁrst step in the proof of the CLT is
to establish that
The second step of the proof is to establish the negligibility condition. We invoke , which requires to establish that max1≤i≤n |ξn,i|
(smallness) and E(max1≤i≤n ξ2
n,i) is bounded in n (tightness), where ξn,i is deﬁned in (60).
We will establish the two conditions simultaneously by checking that
Splitting the sum s2
n, between diagonal terms En, and oﬀ-diagonal terms Fn, we have
(λp + γn)−2
n,p,i−1E[Y 2
(λp + γn)−1(λq + γn)−1
Mn,p,i−1Nn,q,i−1E[Yn,p,iYn,q,i] .
Consider ﬁrst the diagonal terms En. We ﬁrst compute its mean. Note that E[N 2
n,p,j]. Using Lemma 16, we get
(λp + γn)−2
n,p,j]E[Y 2
2,nηn) + O(n−1)
Therefore, E[En] = 1/2 + o(1). Next, we check that En −E[En] = oP(1) is negligible. We
write En −E[En] = d−2
p=1(λp + γn)−2Qn,p, with
Testing for Homogeneity with KFDA
Using this notation,
Var[En] = d−4
(λp + γn)−4E[Q2
(λp + γn)−2(λq + γn)−2E[Qn,pQn,q] .
We will establish that
|E[Qn,pQn,q]| ≤C
q(δp,q + |εp,q|)2 + n−1λ3/2
Plugging this bound into (66) and using that λp/(λp + γn) ≤1 and d2,n →∞as n tends to
inﬁnity, yields under Assumption (B1)
2,n + n−1γ−1
2,nηn + n−1d−4
showing that Var[En] = o(1), and hence that En −E[En] = oP (1). To show (67), note ﬁrst
n,p,i −E[M2
n,p,i]}1≤i≤n is a Fn-adapted martingale. Denote by νn,p,i its increment
deﬁned recursively as follows: νn,p,1 = N 2
n,p,1 −E[N 2
n,p,1] and for i ≥1 as
νn,p,i = M2
n,p,i −E[M2
n,p,i−1 −E[N 2
n,p,i −E[Y 2
n,p,i] + 2Yn,p,iMn,p,i−1 .
Using the summation by part formula, Qn,p may be expressed as
Using Lemma 16, Eq. (47), we obtain for any 1 ≤p ≤q ≤n,
|E[Qn,pQn,q]| ≤
E[νn,p,iνn,q,i]
≤Cλpλq(1 + O(ηn))
E[νn,p,iνn,q,i]
E[νn,p,iνn,q,i] = Cov(Y 2
n,p,i, Y 2
n,q,i) + 4E {Yn,p,iYn,q,i} E {Mn,p,i−1Nn,q,i−1} .
First, applying Eq. (48) in Lemma 16 gives
n,p,i, Y 2
n,q,i) ≤Cn−1λ1/2
Since E[Mn,p,i−1Nn,q,i−1] = Pi−1
j=1 E[Yn,p,jYn,q,j], Lemma 16, Eq. (47) shows that
E[Yn,p,iYn,q,i]E[Mn,p,i−1Nn,q,i−1]
E[Yn,p,iYn,q,i]
E2[Yn,p,iYn,q,i]
≤Cλpλq(δp,q + |εp,q|)2 .
Eq. 67 follows by plugging (69) and (70) into (68). We ﬁnally consider Fn deﬁned in (64).
We will establish that Fn = oP(1). Using Lemma 16-Eq. (47),
n,p,i−1]E1/2[N 2
n,q,i−1] ≤Cλ1/2
and |E[Yn,p,iYn,q,i]| ≤Cn−1λ1/2
εp,q, the Minskovski inequality implies that
{E|Fn|2}1/2 ≤Cd−2
(λp + γn)−1(λq + γn)−1λpλqεp,q ≤Cηn ,
showing that Fn = o(1). This concludes the proof of Eq. (61).
We ﬁnally show Eq. (62). Since |Yn,p,i| ≤n−1/2|k|1/2
∞P-a.s we may bound
1≤i≤n |ξn,i| ≤Cd−1
(λp + γn)−1 max
1≤i≤n |Mn,p,i−1| .
Then, the Doob inequality implies that E1/2[max1≤i≤n |Mn,p,i−1|2] ≤E1/2[N 2
n,p,n−1] ≤
. Plugging this bound in (71), the Minkowski inequality
and the proof is concluded using the fact that γn + d−1
2 (Σ1, γn)d1(Σ1, γn)γ−1
n n−1/2 →0 and
Assumption (B1).
11. Proof of Theorem 5
Proof of Proposition 4. We denote by Σ = ρ1Σ1 + ρ2Σ2 + ρ1ρ2δ ⊗δ the covariance operator associated with the probability density p = ρ1p1 + ρ2p2, and δ = µ2 −µ1.
Proposition 10 applied to the probability densities p1, p2 and p = ρ1p1 + ρ2p2 shows that
ρ1p1+ρ2p2dρ. Thus
ρ2 (p1 −p)2 + ρ2
ρ1 (p2 −p)2
p dρ = 1 −
Testing for Homogeneity with KFDA
The previous inequality shows that ρ1ρ2
H < 1 is satisﬁed when
p1p2/pdρ ̸= 0.
Therefore, in this situation,
δ, (ρ1Σ1 + ρ2Σ2)−1δ
δ, (Σ −ρ1ρ2δ ⊗δ)−1δ
H (1 −ρ1ρ2
and the proof follows by combining the two latter equations.
Consider now the case where
p1p2/pdρ = 0, that is when the probability distribution
P1 and P2 are singular (for any set A ∈X such as P1(A) ̸= 0, P2(A) = 0 and vice-versa).
In that case,
δ, (ρ1Σ1 + ρ2Σ2)−1δ
H is inﬁnite.
Proof. We ﬁrst prove that
(ˆΣW + γnI)−1/2ˆδ
(ΣW + γ∞I)−1/2δ
= limn→∞γn. Using straightforward algebra, we may write
(ˆΣW + γnI)−1/2ˆδ
(ΣW + γ∞I)−1/2δ
≤C1 + C2 + C3 ,
(ΣW + γnI)−1/2ˆδ
(ˆΣW + γnI)−1/2ˆδ
(ˆΣW + γnI)−1/2(ˆΣW −ΣW ) (ΣW + γnI)−1/2
(ΣW + γnI)−1/2ˆδ
(ΣW + γnI)−1/2δ
(ΣW + γnI)−1/2δ
(ΣW + γ∞I)−1/2δ
First, prove that C1 = oP (1). Write C1 = A1A2B1. Using (with obvious changes) the
relation (42), the monotone convergence theorem yields
(ΣW + γnI)−1/2ˆδ
δ, (ΣW + γ∞I)−1δ
which gives A1 = OP (1). As for proving A2 = OP (1), using an argument similar to the
one used to derive Eq. (41), it suﬃces to observe that A2 = A1 + oP (1). Then, Eq. (35)
in Corollary 13 gives B1 = OP (γ−1
n n−1/2), which shows that C1 = A1A2B1 = oP (1). Next,
prove that C2 = oP (1). We may write
ˆδ −δ, (ΣW + γnI)−1δ
(ΣW + γnI)−1/2(ˆδ −δ)
(ΣW + γnI)−1/2
(ΣW + γnI)−1/2δ
H < ∞, and moreover ∥ˆδ −
δ∥H = OP (n−1/2), then we get C2 = OP (γ−1/2
n−1/2) = oP (1) Finally, prove that C3 = o(1)
Note that C3 = −P∞
n (λp +γn)−1λp ⟨δ, ep⟩2
H, where {λp} and {ep} denote respectively
the eigenvalues and eigenvectors of ΣW . Since [γ 7→(λp+γ)−1γ] is monotone, the monotone
convergence theorem shows that C3 = o(1).
Now, when P1 ̸= P2, Proposition 4 with P = ρ1P1 + ρ2P2 ensures that δ ∈R(Σ1/2
L2(P1) < ∞. Then, under assumption (A2), by injectivity of ΣW we have
δ ̸= 0. Hence, since ΣW is trace-class, we may apply Lemma 19 with α = 1, which yields
d−1(ΣW, γn) n →∞. Therefore, bTn(γn)
−→∞, and the proof ois concluded. Otherwise,
that is when
L2(P1) = ∞, we have bTn(γn)
Appendix A. Technical Lemmas
Lemma 19. Let {λp}p≥1 be a non-increasing sequence of non-negative numbers. Let α > 0.
Assume that P
p < ∞. Then, for any β ≥α,
p(λp + γ)−β ≤2
In addition, if limp→∞pλα
p = ∞, then for any β > 0,
p(λp + γ)−β = ∞.
Proof. For γ > 0, denote by qγ = supp≥1{p : λp > γ}. Then,
p(λp + γ)−β ≤γα
p (λp + γ)−α ≤γαqγ +
Since the sequence {λp} is non-increasing, the condition C def
p < ∞< ∞implies
p ≤C. Therefore, λp ≤C1/αp−1/α, which implies that for any p satisfying Cγ−α ≤
p, λp ≤γ, showing that qγ ≤Cγ−α. This establishes (74).
Since λ 7→λ(λ + γ)−1 is non-decreasing, for p ≤qγ, λp(λp + γ)−1 ≥(1/2). Therefore,
p(λp + γ)−β ≥(2)−βγαqγ. Since limp→∞pλα
p = ∞, this means that λp > 0 for
any p, which implies that limγ→0+ qγ = ∞. Therefore, limγ→0+ qγλα
qγ = limγ→0+ qγγα = ∞.
The proof follows.
Lemma 20. Let {λp}p≥1 be a non-increasing sequence of non-negative numbers. Assuse
there exists s > 0 such that λp = p−s for all p ≥1. Then,
(λp + γn)−rλr
(1 + vs)−rdv
(1 + o(1)) ,
Proof. First note that
(λp + γn)−rλr
(1 + γnλ−1
(1 + (γ1/s
n p)s)−r .
Testing for Homogeneity with KFDA
For all γ > 0, the function [u 7→(1 + (γ1/su)s)−r] is increasing and nonnegative. Therefore,
for all p ≥1 we may write
(1 + (γ1/su)s)−rdu
≤(1 + (γ1/sp)s)−r
(1 + (γ1/su)s)−rdu ,
Z γ1/s(p+1)
(1 + vs)−rdv
≤(1 + (γ1/sp)s)−r
(1 + vs)−rdv .
Hence, sussing on p over 1, . . . , N −1, we obtain
(1 + vs)−rdv
p=1(1 + (γ1/sp)s)−r
(1 + vs)−rdv .
Therefore, taking N →∞in such a way that γ1/sN →∞as γ →0, we ﬁnally get
(1 + (γ1/sp)s)−r = γ−1/s
(1 + vs)−rdv
(1 + o(1)) .
Lemma 21. Let A be a self-adjoint compact operator on H. Then, for any orthonormal
basis {ϕp}p≥1 of H,
Proof. Let {ψp}p≥1 be an orthonormal basis of H consisting of a sequence of eigenvectors
of A corresponding to the eigenvalues {λp(A)} of this latter operator, so that ⟨ψp, Aψp⟩H =
λp(A). Then,
⟨ψp, Aψp⟩H
⟨Aϕq, ψp⟩H
⟨Aϕq, ψp⟩H
Appendix B. Perturbation results on covariance operators
Lemma 22. Let A be a compact self-adjoint operator, with {λp}p≥1 the eigenvalues of A,
and {ep}p≥1 an orthonormal system of eigenvectors of A. Then, for all integer k > 1, using
the convention pk+1 = p1,
ep, (AB)kep
epj, Bepj+1
Proof. Let k be some integer, ﬁxed throughout the proof. The proof is by induction, that
is, we shall prove that, for all ℓ∈{1, . . . , k},
ep, (AB)kep
epj, Bepj+1
epℓ, (AB)k−ℓ+1ep1
First, for ℓ= 2, using that A∗ep1 = Aep1 = λp1ep1, and B∗ep1 = P∞
p2 ⟨ep1, Bep2⟩ep2, we
indeed have
ep1, AB(AB)k−1ep1
B∗ep1, (AB)k−1ep1
⟨ep1, Bep2⟩ep2, (AB)k−1ep1
λp1 ⟨ep1, Bep2⟩
ep2, (AB)k−1ep1
Assume the statement P(ℓ) is true, with ℓ< k −1. Let us now marginalize out, ﬁrst A
then B in (AB)k−ℓ+1, for the (ℓ+ 1)-th time, by summing over an index pℓ+1. Using the
same arguments as above, that is A∗epℓ= λpℓepℓand B∗epℓ= P∞
epℓ, Bepℓ+1
ep, (AB)kep
epj, Bepj+1
epℓ, AB(AB)k−ℓep1
epj, Bepj+1
B∗epℓ, (AB)k−ℓep1
epj, Bepj+1
epℓ, Bepℓ+1
epℓ+1, (AB)k−ℓep1
which proves P(ℓ+ 1).
The proof is concluded by a k-step induction, that is once A in (AB)k is eventually
marginalized out k-times and only the last term ⟨epk, Bep1⟩remains.
Lemma 23. Let γ > 0, and S a trace-class operator. Denote {λp}p≥1 and {ep}p≥1 respectively the positive eigenvalues and the corresponding eigenvectors of S. Consider dr(T, γ)
Testing for Homogeneity with KFDA
for r = 1, 2, with T a compact operator, as deﬁned in (6). If ∆is a trace-class perturbation
operator such that
(S + γI)−1∆
< 1, and ∥∆∥C1 = P∞
p=1 ∥∆ep∥< γ, then
|dr(S + ∆, γ) −dr(S, γ)| ≤
1 −γ−1 ∥∆∥C1
r = 1, 2 .
If d2(S, γ)
S−1/2∆S−1/2
HS < 1, then
|d1(S + ∆, γ) −d1(S, γ)| ≤
S−1/2∆S−1/2
1 −d2(S, γ)
S−1/2∆S−1/2
|d2(S + ∆, γ) −d2(S, γ)| ≤
S−1/2∆S−1/2
S−1/2∆S−1/2
 (S + γI)−1∆
< 1, then we may write
(S + ∆+ γI)−1(S + ∆) = (I + (S + γI)−1∆)−1(S + γI)−1(S + ∆)
(S + γI)−1∆
k (S + γI)−1(S + ∆)
= (S + γI)−1S +
(S + γI−1)∆
k  (S + γI)−1S −I
where the series converge in operator-norm. Since the trace is continuous in the space of
trace-class operators, and using
(S + γI)−1S −I
< 1, we get by linearity of the trace,
|d1(S + ∆, γ) −d1(S, γ)| =
(S + ∆+ γI)−1(S + ∆)
(S + γI)−1S
(S + γI)−1∆
(S + γI)−1S −I
n (S + γI)−1∆
Applying Lemma 22 with B = ∆, and A = (S + γI)−1, we obtain
n (S + γI)−1∆
 (S + γI)−1∆
(λpj + γ)−1
epj, ∆epj+1
Since, for all 1 ≤j ≤k, we have
epj, ∆epj+1
and (λpj +γ)−1 ≤γ−1, the upperbound in (80) is actually the sum of a geometric series whose ratio is γ−1 P∞
p=1 ∥∆ep∥=
γ−1 ∥∆∥C1, where γ−1 ∥∆∥C1 < 1 by assumption, which completes the proof of (77) when
r = 1. A similar reasoning as above allows to prove (77) when r = 2.
We now prove the second upper-bound (78). Using that
n (S + γI)−1∆
S1/2(S + γI)−1S1/2 
S−1/2∆S−1/2ok
we may apply Lemma 22 again, but with B = S−1/2∆S−1/2, and A = S1/2(S + γI)−1S1/2,
n (S + γI)−1∆
 (S + γI)−1∆
(λpj + γ)−1λpj
S−1/2∆S−1/2
Then, using that
 S−1/2∆S−1/2
 S−1/2∆S−1/2
, and applying H¨older
inequality, we obtain
n (S + γI)−1∆
(λp + γ)−2λ2
S−1/2∆S−1/2
S−1/2∆S−1/2
Finally, going back to (80), the upper-bound is actually the sum of a geometric series whose
ratio is d(S)
S−1/2∆S−1/2
HS, where d(S)
S−1/2∆S−1/2
HS < 1 by assumption, which
completes the proof of (78). As for (79), observe that
|d2(S + ∆, γ) −d2(S, γ)| ≤
(S + γI)−1∆
(S + γI)−1S −I
(S + γI)−1∆
S−1/2∆S−1/2o
where we used the inequality ∥AB∥HS ≤∥A∥HS ∥B∥HS, and
(S + γI)−1S −I
(S + γI)−1S
Appendix C. Miscellaneous proofs
Proposition 24. Assume (A1) and (B1).
Assume in addition that P1 = P2 = P.
γn ≡γ , then
P(T∞(ˆΣW, γ) ≤x) −P(T∞(ΣW , γ) ≤x)
where T∞(S, γ) for a trace-class operator S is deﬁned in (10).
Testing for Homogeneity with KFDA
Proof. First, deﬁne the random variables {Yn} and {Y } as follows
(ˆλp + γ)−1ˆλp(Z2
(λp + γ)−1λp(Z2
where {Zp}p≥1 are independent standard normal variables. Considering the random element
h ∈H, such that ⟨h, ep⟩H = Zp for all p ≥1, we may write
(ˆΣW + γI)−1/2 ˆΣ−1/2
H −d1,n(ˆΣW, γ) ,
(ΣW + γI)−1/2Σ−1/2
H −d1,n(ΣW, γ) .
Then, using Eq. (77) for r = 1 in Lemma 23 with S = ΣW, and Corollary 13 which gives
HS = OP (n−1/2), we get |Yn −Y | = OP (n−1/2), and hence that Yn
case γn ≡γ . Next, applying the Polya theorem gives the result
Supx |P(Yn ≤x) −P(Y ≤x)| →0 .
Appendix D. Eigenvalues of covariance operators
In this section, we give new general results regarding the decay of eigenvalues of covariance
operators. We assume that we have a bounded density p(x) on Rp with respect to the
Lebesgue measure, and a translation invariant kernel k(x −y) with positive integrable
Fourier transform. In this section, we consider eigenvalues of the second order moment
operator, which dominates the covariance operator.
From the proof of Proposition 10,
the eigenvalues of the second order moment operator are the eigenvalues of the following
operator from L2(Rp) to L2(Rp), deﬁned as
Rp p(x)1/2k(x −y)f(y)p(y)1/2dy
We let denote λn(p, K) the eigenvalues of this operator ranked in decreasing order.
We let denote T(p) the pointwise multiplication by p, deﬁned from L2(Rp) to L2(Rp).
We also denote C(k) the convolution operator by k. We thus get Q = T(p)1/2C(k)T(p)1/2.
Note that by taking Fourier transforms (P of p, and K of k), the eigenvalues are the same
as the one of T(K)1/2C(P)T(K)1/2 and thus p and K plays equivalent roles .
The following lemma, taken from Widom , gives an upperbound of the eigenvalues
in the situation where p and K are indicator functions:
Lemma 25. Let ε > 0. Then there exists δ > 0 such that, if p(x) is the indicator function
of [−1, 1] and K is the indicator function of [−γ, γ], with γ ⩽(1−ε)nπ/2, then λn(p, K) ⩽
This result is very useful because it is uniform in γ, as long as γ ⩽(1 −ε)nπ/2. We
now take ε = 1
2, and we thus get λn(1[−1,1], 1[−nπ/4,nπ/4]) ⩽e−nδ for some δ > 0.
We consider the tail behavior of p(x) and of the Fourier transform K(ω) of k, through
M(p, u) = max∥x∥∞⩾u p(x) and M(K, v) = max∥ω∥∞⩾v K(ω), where, for x = (x1, . . . , xp),
∥x∥∞= max1≤i≤p |xi|. We also let denote M0(K) and M0(p) the supremum of K and p
Proposition 26. For all (u, v) such that uv = nπ/4, then
λn(p, K) ⩽M(p, u)M0(K) + M(K, v)M0(p) + M0(K)M0(p)e−δn1/p
Proof. We divide twice Rp in two parts, the spatial version Rp = {x, ∥x∥∞⩽u} ∪
{x, ∥x∥∞> u} = Au ∪Bu and the Fourier version Rp = {ω, ∥ω∥∞⩽v} ∪{ω, ∥ω∥∞>
v} = Av ∪Bv. We have for all p and K,
λn(p, K) ⩽λn(p1Au, K) + λ1(p1Bu, K)
which is classical results for perturbation of eigenvalues By deﬁnition of M(p, u), we have
T(p1Bu) ≼M(p, u)I, and moreover C(k) ≼M0(K)I, which implies that λ1(p1Bu, K) ⩽
M(p, u)M0(K). We thus get
λn(p, K) ⩽λn(p1Au, K) + M(p, u)M0(K).
Similarly, we get
λn(p, K) ⩽λn(p1Au, K1Av) + M(p, u)M0(K) + M(K, v)M0(p).
We know that if two operators satisties A ≼B, then λn(A) ⩽λn(B), thus since T(p1Au) ≼
T(M0(p)1Au) and similarly for K, we get
λn(p, K) ⩽M0(K)M0(p)λn(1Au, 1Av) + M(p, u)M0(K) + M(K, v)M0(p)
By a simple change of variable, it easy to show that λn(1Au, 1Av) = λn(1A1, 1Avu) When
p = 1, we immediately have λn(1A1, 1Avu) ⩽e−δn.
When p > 1, then we notice that
the eigenfunctions and eigenvalue of the operators will be product of eigenfunctions and
eigenvectors of the univariate operators. That is, the eigenvalues are of the form µi1 · · · µip
where (i1, . . . , ip) are positive integer and µi ⩽e−δi are eigenvalues of the univariate operator. From the product formulation, we get that if n is equal to the number of partitions of a
certain integer k into p strictly positive integers, then λn ⩽e−δk. This number of partitions
is exactly equal to [(p −1)!(p −k)!]−1(k −1)! ⩽(k −1)p.
Thus, given any n, we can ﬁnd an integer k such that (k −1)p ⩽n, and we have
λn(1A1, 1Avu) ⩽e−δk. This leads to λn(1A1, 1Avu) ⩽e−δn1/p. The proposition follows.
We can now derive a number of corollaries:
Corollary 27. If p(x) is upper bounded by a constant times e−α∥x∥2 and K(ω) is upper bounded by a constant times e−β∥ω∥2, then there exists η > 0 such that λn(p, K) =
O(e−ηn1/p).
Proof. Take u = v =
Testing for Homogeneity with KFDA
Corollary 28. If p(x) is upper bounded by a constant times (1 + ∥x∥)−α (with α > p such
that we have integrability) and K(ω) is upper bounded by a constant times e−β∥ω∥2, then
λn(p, K) = O(
nα−η ) for any η > 0.
Proof. Take v proportional to nη/α.
Corollary 29. If p(x) is upper bounded by a constant times (1 + ∥x∥)−α (with α > p such
that we have integrability) and K(ω) is upper bounded by a constant times (1+∥x∥)−β (with
β > p such that we have integrability) , then λn(p, K) = O(n−αβ/(α+β)).
Proof. Take v proportional to nα/(α+β).