J. Math. Anal. Appl. 275 521–536
www.academicpress.com
Generalized Mann iterates for constructing
ﬁxed points in Hilbert spaces
Patrick L. Combettes a,∗and Teemu Pennanen b,1
a Laboratoire Jacques-Louis Lions, Université Pierre et Marie Curie–Paris 6, 75005 Paris, France
b Department of Management Science, Helsinki School of Economics, 00101 Helsinki, Finland
Received 19 October 2001
Submitted by W.A. Kirk
The mean iteration scheme originally proposed by Mann is extended to a broad class
of relaxed, inexact ﬁxed point algorithms in Hilbert spaces. Weak and strong convergence
results are established under general conditions on the underlying averaging process and
the type of operators involved. This analysis signiﬁcantly widens the range of applications
of mean iteration methods. Several examples are given.
2002 Elsevier Science (USA). All rights reserved.
1. Introduction
Let F be a ﬁrmly nonexpansive operator deﬁned from a real Hilbert space
(H,∥· ∥) into itself, i.e.,
∀(x,y) ∈H2
∥Fx −Fy∥2 ⩽∥x −y∥2 −
(F −Id)x −(F −Id)y
or, equivalently, 2F −Id is nonexpansive [16, Theorem 12.1]. It follows from a
classical result due to Opial [24, Theorem 3] that, for any initial point x0, the
* Corresponding author.
E-mail addresses: (P.L. Combettes), (T. Pennanen).
1 Work partially supported by a grant from the Conseil Régional du Limousin.
0022-247X/02/$ – see front matter 2002 Elsevier Science (USA). All rights reserved.
PII: S0022-247X(02)00221-4
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
sequence of successive approximations
xn+1 = Fxn
converge weakly to a ﬁxed point of F if such a point exists. The extension of this
result to the relaxed iterations
xn+1 = xn + λn
where 0 < λn < 2,
under the condition 
n⩾0 λn(2−λn) = +∞follows from [17, Corollary 3]. Now
T :domT = H →H |
∀(x,y) ∈H × FixT
⟨y −T x | x −T x⟩⩽0
where FixT denotes the ﬁxed point set of an operator T and ⟨·|·⟩the scalar
product of H. This class of operators includes ﬁrmly nonexpansive operators,
resolvents of maximal monotone operators, projection operators, subgradient
projection operators, operators of the form T = (Id + R)/2, where R is quasinonexpansive,as well as various combinations of those . The fact that F ∈T
suggests that (3) could be generalized to
xn+1 = xn + λn
where 0 < λn < 2 and Tn ∈T.
This iterative procedure was investigated in and further studied in . It
was shown that, under suitable conditions, the iterations (5) converge weakly to a
point in 
n⩾0 FixTn. These results provide a unifying framework for numerous
ﬁxed point algorithms, including in particular the serial scheme of for ﬁnding
a common ﬁxed point of a family of ﬁrmly nonexpansive operators and its
block-iterative generalizations , the proximal point algorithms of 
for ﬁnding a zero of a maximal monotone operator, the ﬁxed point scheme of
 for functional equations, the projection methods of for convex feasibility
problems, the subgradient projection methods of for systems of convex
inequalities, and operator splitting methods for variational inequalities .
In the above algorithms, the update xn+1 involves only the current iterate xn
and the past iterates (xj)0⩽j⩽n−1 are not exploited. In , Mann proposed a
simple modiﬁcation of the basic scheme (2) in which the updating rule incorporates the past history of the process. More precisely, his scheme for ﬁnding a
ﬁxed point of an operator T :H →H is governed by the recursion
xn+1 = T xn,
where xn denotes a convex combination of the points (xj)0⩽j⩽n, say xn =
j=0 αn,jxj. Further work on this type of iterative process for certain types of
operators was carried out in .
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
Most existing convergence results for the Mann iterates (6) require explicitly
(e.g., ) or implicitly (e.g., ) that the averaging matrix
A = [αn,j] be segmenting, i.e.,
(∀n ∈N) ∀j ∈{0,...,n}
αn+1,j = (1 −αn+1,n+1)αn,j.
This property implies that the points (xn)n⩾0 generated in (6) satisfy
xn+1 = αn+1,n+1xn+1 +
= αn+1,n+1xn+1 + (1 −αn+1,n+1)
= αn+1,n+1T xn + (1 −αn+1,n+1)xn.
In other words, one is really just applying (3) with a speciﬁc relaxation strategy,
λn = αn+1,n+1.
For that reason, (3) is commonly referred to as “Mann iterates” in the literature,
although it merely corresponds to a special case of (6). Under (7), convergence
results for (6) can be inferred from known results for (3). For instance, suppose
that T is a quasi-nonexpansive operator such that FixT ̸= ∅and T −Id is
demiclosed. Then any sequence (xn)n⩾0 conforming to (8) satisﬁes the following
properties: T xn−xn →0 and (xn)n⩾0 convergesweakly to a point in FixT under
either of the following conditions:
(i) limαn,n > 0 and limαn,n < 1 [11, Theorem 8].
n⩾0 αn,n(1 −αn,n) = +∞and T is nonexpansive [17, Corollary 3].
It therefore follows that the Mann sequence (xn)n⩾0 in (6) converges weakly to
a point in FixT (whereas the standard successive approximations xn+1 = T xn
do not converge in general in this case: take T = −Id and x0 ̸= 0). Let us
note that, under the segmenting condition (7), the value of αn,n ﬁxes those of
(αn,j)0⩽j⩽n−1. This condition is therefore very restrictive.
The goal of this paper is to introduce and analyze a common algorithmic framework encompassing and extending the above iterative methods. The algorithm
under consideration is the following inexact, Mann-like generalization of (5):
xn+1 = xn + λn
Tnxn + en −xn
where en ∈H, 0 < λn < 2, and Tn ∈T.
Here, en stands for the error made in the computation of Tnxn; incorporating such
errors provides a more realistic model of the actual implementation of the algo-
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
Fig. 1. An iteration of algorithm (10); xn+1 lies on the dashed-line segment.
rithm. Throughout, the convex combinations in (10) are deﬁned as
where (αn,j)n,j⩾0 are the entries of an inﬁnite lower triangular row stochastic
matrix A, i.e.,
j > n ⇒αn,j = 0,
j=0 αn,j = 1,
which satisﬁes the regularity condition
n→+∞αn,j = 0.
Our analysis will not rely on the segmenting condition (7) and will provide convergence results for the inexact, extended Mann iterations (10) for a wide range
of averaging schemes.
Figure 1 sheds some light on the geometrical structure of algorithm (10). At
iteration n, the points (xj)0⩽j⩽n are available. A convex combination xn of these
points is formed and an operator Tn ∈T is selected, such that FixTn contains the
solution set S of the underlying problem. If xn /∈FixTn, then, by (4),
x ∈H | ⟨x −Tnxn | xn −Tnxn⟩⩽0
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
is a closed afﬁne half-space containing FixTn and onto which Tnxn is the
projection of xn. The update xn+1 is a point on the open segment between xn
and its approximate reﬂection, 2(Tnxn + en) −xn, with respect to Hn. Thus, (10)
offers much more ﬂexibility in deﬁning the update than (5) and, thereby, may be
more advantageous in certain numerical applications. For instance, a problem that
has been reported in some applications of (5) to convex feasibility is a tendency
of its orbits to “zig-zag” . Acting on an average of past iterates rather than
on the latest one alone as in (5) naturally centers the iterations and mitigates zigzagging. Another numerical shortcoming of (5) that has been reported in operator
splitting applications is the “spiralling” of the orbits around the solution set ([12,
Section 7.1], ). The averaging taking place in (10) has the inherent ability to
avoid such undesirable convergence patterns.
The remainder of the paper is organized as follows. In Section 2, we introduce
a special type of averaging matrix A which will be suitable for studying algorithm (10). In Section 3, conditions for the weak and strong convergence of algorithm (10) to a point in 
n⩾0 FixTn are established. Applications are discussed
in Section 4.
2. Concentrating averaging matrices
Without further conditions on the averaging matrix A, algorithm (10) may fail
to converge. For instance, if we set αn,n−1 = 1 for n ⩾1, then, with λn ≡1,
Tn ≡Id, and x0 = 0, (10) becomes
In particular, if e0 ̸= 0 and en = 0 for n ⩾1, then xn = 0 for n even and xn = e0
for n odd. It will turn out that the following property of the averaging matrix
A prevents this kind of behavior. Henceforth, ℓ1 (respectively ℓ1
+) denotes the
class of summable sequences in R (respectively R+). Moreover, given a sequence
(ξn)n⩾0 in R, (ξn)n⩾0 denotes the sequence deﬁned through the same averaging
process as in (11).
Deﬁnition 2.1. A is concentrating if every sequence (ξn)n⩾0 in R+ such that
∃(εn)n⩾0 ∈ℓ1
ξn+1 ⩽ξn + εn
converges.
The following facts will be useful in checking whether a matrix is concentrating.
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
Lemma 2.2 [10, Lemma 3.1]. Let (ξn)n⩾0, (βn)n⩾0, and (εn)n⩾0 be sequences
in R+ such that (εn)n⩾0 ∈ℓ1 and
ξn+1 ⩽ξn −βn + εn.
Then (ξn)n⩾0 converges and (βn)n⩾0 ∈ℓ1.
Lemma 2.3. Let (ξn)n⩾0 be a sequence in R+ that satisﬁes (16) and set, for every
n ∈N, ˇξn = max0⩽j⩽n ξj. Then
(i) (ˇξn)n⩾0 converges.
(ii) (ξn)n⩾0 is bounded.
(iii) (ξn)n⩾0 is bounded.
Proof. (i) For every n ∈N, ξn+1 ⩽ξn + εn ⩽ˇξn + εn and therefore ˇξn+1 ⩽
ˇξn + εn. Hence, by Lemma 2.2, (ˇξn)n⩾0 converges.
(ii) and (iii) For every n ∈N, 0 ⩽ξn ⩽ˇξn and 0 ⩽ξn ⩽ˇξn, where (ˇξn)n⩾0 is
bounded by (i).
Our ﬁrst example is an immediate consequence of Lemma 2.2.
Example 2.4. If αn,n ≡1, then A is the identity matrix, which is concentrating.
In this case (10) reverts to (5) and we recover the standard T-class methods of 
The next example involves a relaxation of the segmenting condition (7).
Example 2.5. Set (∀n ∈N) τn = n
j=0 |αn+1,j −(1 −αn+1,n+1)αn,j|. Suppose
that (τn)n⩾0 ∈ℓ1 and that limαn,n > 0. Then A is concentrating.
Proof. Let (ξn)n⩾0 be a sequence in R+ satisfying (16). By Lemma 2.3(ii),
γ = supn⩾0 ξn < +∞. We have
ξ n+1 = αn+1,n+1ξn+1 +
αn+1,j −(1 −αn+1,n+1)αn,j
−αn+1,n+1(ξn −ξn+1 + εn) + αn+1,n+1εn
⩽ξn −αn+1,n+1(ξn −ξn+1 + εn) + (γ τn + εn),
where, by (16), ξn −ξn+1 + εn ⩾0. We thus get from Lemma 2.2 that (ξ n)n⩾0
converges and that (αn+1,n+1(ξn −ξn+1 + εn))n⩾0 ∈ℓ1. Hence, since limαn,n >
0, ξn −ξn+1 + εn →0 and (ξn)n⩾0 converges to the same limit as (ξ n)n⩾0.
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
An example of an averaging matrix satisfying the above conditions can be
constructed by choosing αn,n = α for n ⩾1, where α ∈]0,1[. Then (7) yields
∀j ∈{0,...,n}
for j = 0,
α(1 −α)n−j
for 1 ⩽j ⩽n.
The next example offers an alternative to the approximate segmenting condition used in Example 2.5.
Example 2.6. Set (∀j ∈N) τj = max{0,
n⩾j αn,j −1} and (∀n ∈N) Jn =
{j ∈N | αn,j > 0}. Suppose that 
j⩾0 τj < +∞, that
Jn+1 ⊂Jn ∪{n + 1},
and that there exists α ∈]0,1[ such that
(∀n ∈N) (∀j ∈Jn)
Then A is concentrating.
Proof. Let (ξn)n⩾0 be a sequence in R+ satisfying (16). Then it follows from
Lemma 2.3(ii) and (iii) that γ = supn⩾0 ξn < +∞and γ ′ = supn⩾0 ξn < +∞.
Now deﬁne (∀n ∈N) σn = n
j=0 αn,j|ξj −ξn|21/2 and ε′
n = 2γ ′εn + ε2
n)n⩾0 ∈ℓ1 and, by (16),
n+1 + 2ξnεn + ε2
and we infer from the assumptions that (σ 2
n )n⩾0 ∈ℓ1.
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
It follows from (16) that, for every n ⩾0, ξn+1 ⩽˜ξn + εn, where ˜ξn =
maxj∈Jn ξj . Consequently, by condition (18),
˜ξn+1 ⩽˜ξn + εn,
and (˜ξn)n⩾0 converges by Lemma 2.2. On the other hand, (19) and Jensen’s
inequality yield
|ξn −˜ξn| ⩽|ξn −ξn| + |˜ξn −ξn| ⩽1
αn,j|ξj −ξn| ⩽σn
Since σn →0, the convergence of (ξn)n⩾0 follows from that of (˜ξn)n⩾0.
As an example, take strictly positive numbers (ai)0⩽i⩽m such that m
i=0 ai = 1
and deﬁne the averaging matrix A by
(∀n ∈{0,...,m −1}) (∀j ∈{0,...,n})
if 0 ⩽j < n,
(∀n ⩾m) (∀j ∈{0,...,n})
if 0 ⩽j < n −m,
if n −m ⩽j ⩽n.
Then it is easily checked that the conditions of Example 2.6 are satisﬁed. More
general stationary averaging processes can be obtained by exploiting a root
condition from the theory of linear dynamical systems.
Example 2.7. Suppose there exist numbers (ai)0⩽i⩽m in R+ such that (21) holds
and the roots of the polynomial z
j=0 ajzm−j are all within the unit
disc, with exactly one root on its boundary. Then A is concentrating.
Proof. The claim follows from [27, Lemma 4].
The conditions of the previous example are frequently used in the numerical
integration literature; several speciﬁc examples can be found, for instance, in .
3. Convergence analysis
In this section we study the convergence of the generalized Mann iteration
scheme (10). Henceforth, W(yn)n⩾0 and S(yn)n⩾0 denote respectively the sets
of weak and strong cluster points of a sequence (yn)n⩾0 in H, whereas ⇀and →
denote respectively weak and strong convergence.
In the case of algorithm (3), a key property of the operator F to establish weak
convergenceto a point in FixF is the demiclosedness of F −Id at 0; i.e., whenever
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
yn ⇀y and Fyn −yn →0, then y = Fy . The following extended notion of
demiclosedness will prove pertinent to establish the weak convergence of (10).
Condition 3.1. For every bounded sequence (yn)n⩾0 in H,
Tnyn −yn →0
W(yn)n⩾0 ⊂
Likewise, to study the strong convergence of (3), a central property is the demicompactness of F at 0, i.e., every bounded sequence (yn)n⩾0 clusters strongly
whenever Fyn −yn →0 . For our purposes, a suitable extension of this
property will be
Condition 3.2. For every bounded sequence (yn)n⩾0 in H,
Tnyn −yn →0
S(yn)n⩾0 ̸= ∅.
The following two lemmas will also be required.
Lemma 3.3 [10, Proposition 2.3(ii)]. Let T ∈T and λ ∈ . Then
(∀y ∈H) (∀x ∈FixT )
y + λ(Ty −y) −x
⩽∥y −x∥2 −λ(2 −λ)∥Ty −y∥2.
Lemma 3.4 [20, Theorem 3.5.4]. Let (ξn)n⩾0 be a sequence in R. Then ξn →ξ
Our main convergence result can now be stated.
Theorem 3.5. Let (xn)n⩾0 be an arbitrary sequence generated by (10). Suppose that A is concentrating, that (Tn)n⩾0 satisﬁes Condition 3.1 with S =
n⩾0 Fix Tn ̸= ∅, that (λn)n⩾0 lies in [δ,2 −δ] for some δ ∈]0,1[, and that
(∥en∥)n⩾0 ∈ℓ1. Then:
(i) (xn)n⩾0 converges weakly to a point in S.
(ii) If (Tn)n⩾0 satisﬁes Condition 3.2, (xn)n⩾0 converges strongly to a point in S.
Proof. Take a point x ∈S. In view of (10), Lemma 3.3, and the convexity of ∥·∥,
∥xn+1 −x∥⩽
xn + λn(Tnxn −xn) −x
 + λn∥en∥
⩽∥xn −x∥+ 2∥en∥
αn,j∥xj −x∥+ 2∥en∥.
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
Therefore, since A is concentrating and (∥en∥)n⩾0 ∈ℓ1, (∥xn −x∥)n⩾0 converges
to some number ℓ(x). It then follows from Lemma 3.4 and (24) that
∥xn −x∥→ℓ(x).
Hence γ = 4 supn⩾0 ∥xn −x∥< +∞and the sequence (εn)n⩾0 deﬁned by
εn = γ ∥en∥+ 4∥en∥2
lies in ℓ1. Invoking Lemma 3.3, the convexity of ∥· ∥2, and the restrictions on
(λn)n⩾0, we obtain
∥xn+1 −x∥2 ⩽
xn + λn(Tnxn −xn) −x
 + λn∥en∥
⩽∥xn −x∥2 −λn(2 −λn)∥Tnxn −xn∥2
+ 2λn∥xn −x∥· ∥en∥+ λ2
αn,j∥xj −x∥2 −δ2∥Tnxn −xn∥2 + εn.
Consequently,
∥Tnxn −xn∥2
αn,j∥xj −x∥2 −∥xn+1 −x∥2 + εn
However, since (∥xn −x∥2)n⩾0 converges, Lemma 3.4 asserts that
αn,j∥xj −x∥2 −∥xn+1 −x∥2 →0.
It therefore follows from (27) that
Tnxn −xn →0.
Moreover, since
∥xn+1 −xn∥= λn∥Tnxn + en −xn∥
∥Tnxn −xn∥+ ∥en∥
(28) yields
xn+1 −xn →0.
(i) Take two points x and x′ in W(xn)n⩾0 ∩S. From (25), the sequences
(∥xn∥2 −2⟨xn | x⟩)n⩾0 and (∥xn∥2 −2⟨xn | x′⟩)n⩾0 converge and therefore so
does (⟨xn | x −x′⟩)n⩾0. Consequently, it must hold that ⟨x | x −x′⟩= ⟨x′ | x −x′⟩,
i.e., x = x′. Thus, the bounded sequence (xn)n⩾0 has at most one weak cluster
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
point in S. Since (22) and (28) imply that W(xn)n⩾0 ⊂S, we deduce that (xn)n⩾0
converges weakly to a point x ∈S. In view of (29), xn ⇀x.
(ii) It follows from (28) and (23) that S(xn)n⩾0 ̸= ∅. However, by (i), there
exists a point x ∈S such that xn ⇀x. Whence, S(xn)n⩾0 = {x} ⊂S and
therefore ℓ(x) = 0 in (25). We conclude xn →x.
As an immediate by-product of this theorem, we obtain convergence results
for the alternative averaging scheme
xj + λj(Tjxj + ej −xj)
where en ∈H, 0 < λn < 2, and Tn ∈T,
special cases of which have been investigated, for instance, in and . If the
Tns are resolvents of a maximal monotone operator, then (30) can be shown to
correspond to a linear multi-step method described in .
Corollary 3.6. Let (xn)n⩾0 be an arbitrary sequence generated by (30). Suppose that A is concentrating, that (Tn)n⩾0 satisﬁes Condition 3.1 with S =
n⩾0 Fix Tn ̸= ∅, that (λn)n⩾0 lies in [δ,2 −δ] for some δ ∈]0,1[, and that
(∥en∥)n⩾0 ∈ℓ1. Then:
(i) (xn)n⩾0 converges weakly to a point in S.
(ii) If (Tn)n⩾0 satisﬁes Condition 3.2, (xn)n⩾0 converges strongly to a point in S.
Proof. Deﬁne (∀j ∈N) yj = xj + λj(Tjxj + ej −xj). Then, by (30), for every
n ∈N, xn+1 = yn, whence yn+1 = yn + λn+1(Tn+1yn + en+1 −yn).
(i) By Theorem 3.5(i), yn ⇀x ∈S, i.e., (∀z ∈H) ⟨yn | z⟩→⟨x | z⟩. In turn,
Lemma 3.4 yields (∀z ∈H) ⟨yn | z⟩→⟨x | z⟩, i.e., xn ⇀x.
(ii) By Theorem 3.5(ii), yn →x ∈S and Lemma 3.4 yields n
j=0 αn,j ×
∥yj −x∥→0. Since (∀n ∈N) ∥xn+1 −x∥= ∥yn −x∥⩽n
j=0 αn,j∥yj −x∥,
we conclude xn →x.
4. Applications
Algorithm (5) covers essentially all Fejér-monotone methods [2, Proposition 2.7] and perturbed versions thereof . Theorem 3.5 provides convergence
results for the Mann-like extension of these methods described by (10). To demonstrate the wide range of applicability of these results, a few examples are detailed
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
4.1. Mean iterations for common ﬁxed points
Our ﬁrst application concerns the problem of ﬁnding a common ﬁxed point of
a ﬁnite family of operators (Ri)i∈I such that
Ri −Id is demiclosed at 0.
For every n ∈N, let (ωi,n)i∈I be weights in ]0,1] such that 
i∈I ωi,n = 1. It
follows from [10, Eq. (18)] that
ωi,nRix −x
ωi,n∥Rix −x∥2 = 0
Hence, the function
Ln :H →[1,+∞[:
i∈I ωi,n∥Rix −x∥2
i∈I ωi,nRix −x∥2
i∈I FixRi,
is well deﬁned.
We consider the extrapolated parallel algorithm
xn+1 = xn + λn
ωi,nRixn −xn
where en ∈H and 0 < λn < 2.
In the standard case when A is the identity matrix, this type of extrapolated
algorithm has been investigated at various levels of generality in .
It has been observed to enjoy fast convergence due to the large relaxation values
attainable through the extrapolation functions (Ln)n⩾0 but, in some cases, to be
subject to zig-zagging, which weakens its performance . As discussed in the
Introduction, the averaging process that takes place in (32) can effectively reduce
this phenomenon.
Corollary 4.1. Let (xn)n⩾0 be an arbitrary sequence generated by (32). Suppose
that A is concentrating, that 
i∈I Fix Ri ̸= ∅, that (λn)n⩾0 lies in [δ,2 −δ] for
some δ ∈]0,1[, that ζ = infn⩾0 mini∈I ωi,n > 0, and that (∥en∥)n⩾0 ∈ℓ1. Then:
(i) (xn)n⩾0 converges weakly to a point in 
i∈I FixRi.
(ii) If one of the operators in (Ri)i∈I is demicompact at 0, (xn)n⩾0 converges
strongly to a point in 
i∈I FixRi.
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
Proof. For every n ∈N, the operator Tn = Id + Ln(
i∈I ωi,nRi −Id) lies in
T and FixTn = 
i∈I FixRi [10, Proposition 2.4]. Hence, with (Tn)n⩾0 thus
deﬁned, algorithm (32) is immediately seen to be a particular realization of (10).
Therefore, to prove (i), it sufﬁces by Theorem 3.5 to check that Condition 3.1 is
satisﬁed. To this end, take a bounded sequence (yn)n⩾0 such that Tnyn −yn →0
and y ∈W(yn)n⩾0. Then we must show y ∈
i∈I Fix Ri.
i∈I FixRi and set β = supn⩾0 ∥yn −z∥. Then
∥Tnyn −yn∥⩾
ωi,nRiyn −yn
ωi,n∥Riyn −yn∥2
i∈I ∥Riyn −yn∥2,
where (33) follows from the inequality Ln(yn) ⩾1 and (34) from [10, Eq. (17)].
Consequently,
i∈I ∥Riyn −yn∥→0
and, since the operators (Ri −Id)i∈I are demiclosed at 0, we obtain y ∈
i∈I FixRi. Assertion (i) is thus proven.
To prove (ii) it sufﬁces to check that Condition 3.2 is satisﬁed, i.e., that
S(yn)n⩾0 ̸= ∅. Suppose that, for some j ∈I, Rj is demicompact at 0. Then,
by (36), Rjyn −yn →0 and, in turn, S(yn)n⩾0 ̸= ∅.
To illustrate this result, let us highlight speciﬁc applications.
Example 4.2 (ﬁrmly nonexpansive operators). (Ri)i∈I is a ﬁnite family of ﬁrmly
nonexpansive operators from H to H with domain H. Then, for each i ∈I,
Ri ∈T [2, Proposition 2.3] and Ri −Id is demiclosed [5, Lemma 4]. Corollary 4.1
therefore applies. In particular if, for every i ∈I, Ri is the projector relative to a
closed convex set Si, then (32) provides a new projection algorithm to ﬁnd a point
i∈I Si that reduces to Pierra’s method when A is the identity matrix,
en ≡0, ωi,n ≡ωi, and the range of the relaxation parameters (λn)n⩾0 is limited
Remark 4.3. In , an elliptic Cauchy problem was shown to be equivalent to a
ﬁxed point problem for a nonexpansive afﬁne operator T in a Hilbert space. This
problem was solved with the Mann iterative process (6) under the segmenting
condition (7). If we let R = (Id + T )/2, then R is a ﬁrmly nonexpansive operator
with FixR = FixT and Example 4.2 (with the single operator R) provides new
variants of the algorithm of beyond the segmenting condition.
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
Example 4.4 (demicontractions). For every i ∈I,
Ri = 1 −ki
Ti + 1 + ki
where Ti :domTi = H →H is demicontractive with constant ki ∈[0,1[, that is
(∀x ∈H) (∀y ∈FixTi)
∥Tix −y∥2 ⩽∥x −y∥2 + ki∥Tix −x∥2,
and Ti −Id is demiclosed at 0. Upon inserting (37) into (32), one obtains an
algorithm to ﬁnd a common ﬁxed point of (Ti)i∈I whose convergence properties
are given in Corollary 4.1. To see this, it sufﬁces to show that, for every i ∈I,
(a) FixRi = Fix Ti, (b) Ri −Id is demiclosed at 0, and (c) Ri ∈T. Properties (a)
and (b) are immediate from (37). To check (c), ﬁx x ∈H and y ∈FixRi. Then we
must show ∥Rix −x∥2 ⩽⟨y −x | Rix −x⟩. By (38), we have
∥Tix −x∥2 = ∥Tix −y∥2 + 2⟨y −x | Tix −x⟩−∥y −x∥2
⩽ki∥Tix −x∥2 + 2⟨y −x | Tix −x⟩.
∥Rix −x∥2 =
2∥Tix −x∥2 ⩽(1 −ki)⟨y −x | Tix −x⟩/2
= ⟨y −x | Rix −x⟩.
inequalities).
(fi)i∈I of continuous convex functions from H to R with nonempty level sets
(]−∞,0]))i∈I, we want to ﬁnd a point x ∈H such that
∥gi(x)∥2 gi(x)
if fi(x) > 0,
if fi(x) ⩽0,
where gi is a selection of the subdifferential ∂fi of fi. Then the operators (Ri)i∈I
lie in T [2, Proposition 2.3] and solving (41) is equivalent to ﬁnding one of
their common ﬁxed points. Moreover, if, for every i ∈I, ∂fi maps bounded sets
into bounded sets, then the operators (Ri −Id)i∈I are demiclosed at 0 (use the
same arguments as in the proof of [2, Corollary 6.10]) and Corollary 4.1 can
be invoked to solve (41). Here, Ri is demicompact at 0 if f −1
(]−∞,η]) is
boundedly compact (its intersection with any closed ball is compact) for some
η ∈]0,+∞[.
P.L. Combettes, T. Pennanen / J. Math. Anal. Appl. 275 521–536
4.2. Mean proximal iterations
We consider the standard problem of ﬁnding a zero of a set-valued maximal
monotone operator M :H →2H, i.e., a point in the set M−10. To solve this
problem, we propose the mean proximal algorithm
xn+1 = xn + λn
(Id + γnM)−1xn + en −xn
where en ∈H, 0 < λn < 2, and 0 < γn < +∞.
Corollary 4.6. Let (xn)n⩾0 be an arbitrary sequence generated by (43). Suppose
that A is concentrating, that 0 ∈ranM, that infn⩾0 γn > 0, that (λn)n⩾0 lies in
[δ,2 −δ] for some δ ∈]0,1[, and that (∥en∥)n⩾0 ∈ℓ1. Then:
(i) (xn)n⩾0 converges weakly to a point in M−10.
(ii) If domM is boundedly compact, (xn)n⩾0 converges strongly to a point in
Proof. For every n ∈N, set Tn = (Id+γnM)−1. Then the operators (Tn)n⩾0 lie in
T and, for every n ∈N, FixTn = M−10 [2, Proposition 2.3]. Therefore, to prove
(i), it sufﬁces by Theorem 3.5 to check that Condition 3.1 is satisﬁed. This can
be done by following the same arguments as in the proof of [2, Corollary 6.1].
Finally, the fact that the bounded compactness of domM in (ii) implies Condition 3.2 can be proved by proceeding as in the proof of [10, Theorem 6.9].
In particular, if A is the identity matrix, (43) relapses to the usual relaxed
proximal point algorithm. In this case, Corollary 4.6(i) can be found in [14,
Theorem 3], which itself contains Rockafellar’s classical result [32, Theorem 1]
for λn ≡1.