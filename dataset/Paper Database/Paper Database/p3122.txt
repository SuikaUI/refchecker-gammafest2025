DPGN: Distribution Propagation Graph Network for Few-shot Learning
Ling Yang1∗Liangliang Li2∗† Zilun Zhang2
Xinyu Zhou2 Erjin Zhou2 Yu Liu2
Northwestern Polytechnical University1
Megvii Technology2
 , 
 , {zxy, zej, liuyu}@megvii.com
Most graph-network-based meta-learning approaches
model instance-level relation of examples. We extend this
idea further to explicitly model the distribution-level relation of one example to all other examples in a 1-vs-N manner. We propose a novel approach named distribution propagation graph network (DPGN) for few-shot learning. It
conveys both the distribution-level relations and instancelevel relations in each few-shot learning task. To combine
the distribution-level relations and instance-level relations
for all examples, we construct a dual complete graph network which consists of a point graph and a distribution
graph with each node standing for an example. Equipped
with dual graph architecture, DPGN propagates label information from labeled examples to unlabeled examples within
several update generations.
In extensive experiments on
few-shot learning benchmarks, DPGN outperforms stateof-the-art results by a large margin in 5% ∼12% under
supervised settings and 7% ∼13% under semi-supervised
Code is available at 
1. Introduction
The success of deep learning is rooted in a large amount
of labeled data , while humans generalize well after
having seen few examples. The contradiction between these
two facts brings great attention to the research of few-shot
learning . Few-shot learning task aims at predicting
unlabeled data (query set) given a few labeled data (support
Fine-tuning is the defacto method in obtaining a
predictive model from a small training dataset in practice nowadays.
However, it suffers from overﬁtting issues . Meta-learning methods introduces the concept of episode to address the few-shot problem explicitly.
∗Contributed equally.
†Corresponding author.
difference
distribution
classification
distribution-level inference
instance-level inference
Figure 1: Our proposed DPGN adopts contrastive comparisons between each sample with support samples to
produce distribution representation. Then it incorporates
distribution-level comparisons with instance-level comparisons when classifying the query sample.
An episode is one round of model training, where in each
episode, only few examples (e.g., 1 or 5) are randomly sampled from each class in training data. Meta-learning methods adopt a trainer (also called meta-learner) which takes
the few-shot training data and outputs a classiﬁer. This process is called episodic training . Under the framework
of meta-learning, a diverse hypothesis was made to build an
efﬁcient meta-learner.
A rising trend in recent researches was to process the
training data with Graph Networks , which is a powerful model that generalizes many data structures (list, trees)
while introduces a combinatorial prior over data. Few-Shot
GNN is proposed to build a complete graph network
where each node feature is concatenated with the corresponding class label, then node features are updated via the
attention mechanism of graph network to propagate the label information. To further exploit intra-cluster similarity
and inter-cluster dissimilarity in the graph-based network,
EGNN demonstrates an edge-labeling graph neural
network under the episodic training framework. It is noted
that previous GNN studies in few-shot learning mainly fo-
 
cused on pair-wise relations like node labeling or edge labeling, and ignored a large number of substantial distribution relations. Additionally, other meta-learning approaches
claim to make use of the beneﬁts of global relations by
episodic training, but in an implicitly way.
As illustrated in Figure 1, ﬁrstly, we extract the instance
feature of support and query samples.
Then, we obtain
the distribution feature for each sample by calculating the
instance-level similarity over all support samples. To leverage both instance-level and distribution-level representation
of each example and process the representations at different levels independently, we propose a dual-graph architecture: a point graph (PG) and a distribution graph (DG).
Speciﬁcally, a PG generates a DG by gathering 1-vs-n relation on every example, while the DG reﬁnes the PG by
delivering distribution relations between each pair of examples. Such cyclic transformation adequately fuses instancelevel and distribution-level relations and multiple generations (rounds) of this Gather-Compare process concludes
our approach. Furthermore, it is easy to extend DPGN to
semi-supervised few-shot learning task where support set
containing both labeled and unlabeled samples for each
class. DPGN builds a bridge connection between labeled
and unlabeled samples in the form of similarity distribution,
which leads to a better propagation for label information in
semi-supervised few-shot classiﬁcation.
Our main contributions are summarized as follows:
• To the best of our knowledge, DPGN is the ﬁrst to
explicitly incorporate distribution propagation in
graph network for few-shot learning. The further ablation studies have demonstrated the effectiveness of
distribution relations.
• We devise the dual complete graph network that
combines instance-level and distribution-level relations. The cyclic update policy in this framework contributes to enhancing instance features with distribution information.
• Extensive experiments are conducted on four popular
benchmark datasets for few-shot learning. By comparing with all state-of-the-art methods, the DPGN
achieves a signiﬁcant improvement of 5%∼12% on
average in few-shot classiﬁcation accuracy. In semisupervised tasks, our algorithm outperforms existing
graph-based few-shot learning methods by 7%∼13 %.
2. Related Work
2.1. Graph Neural Network
Graph neural networks were ﬁrst designed for tasks on
processing graph-structured data . Graph neural networks mainly reﬁne the node representations by aggregating and transforming neighboring nodes recursively. Recent
approaches are proposed to exploit GNN in the
ﬁeld of few-shot learning task. TPN brings the transductive setting into graph-based few-shot learning, which
performs a Laplacian matrix to propagate labels from support set to query set in the graph. It also considers the similarity between support and query samples through the process of pairwise node features afﬁnities to propagate labels.
EGNN uses the similarity/dissimilarity between samples and dynamically update both node and edge features
for complicated interactions.
2.2. Metric Learning
Another category of few-shot learning approaches focus
on optimizing feature embeddings of input data using metric learning methods. Matching Networks produces a
weighted nearest neighbor classiﬁer through computing embedding distance between support and query set. Prototypical Networks ﬁrstly build a prototype representation
of each class in the embedding space. As an extension of
Prototypical Networks, IMF constructs inﬁnite mixture
prototypes by self-adaptation. RelationNet adopts a
distance metric network to learn pointwise relations in support and query samples.
2.3. Distribution Learning
Distribution Learning theory was ﬁrst introduced in 
to ﬁnd an efﬁcient algorithm that determines the distribution from which the samples are drawn. Different methods are proposed to efﬁciently estimate the target distributions. DLDL is one of the researches that
has assigned the discrete distribution instead of one-hot label for each instance in classiﬁcation and regression tasks.
CPNN takes both features and labels as the inputs and
produces the label distribution with only one hidden layer in
its framework. LDLFs devises a distribution learning
method based on the decision tree algorithm.
2.4. Meta Learning
Some few-shot approaches adopt a meta-learning framework that learns meta-level knowledge across batches of
tasks. MAML are gradient-based approaches that design
the meta-learner as an optimizer that could learn to update
the model parameters (e.g., all layers of a deep network)
within few optimization steps given novel examples. Reptile simpliﬁes the computation of meta-loss by incorporating an L2 loss which updates the meta-model parameters
towards the instance-speciﬁc adapted models. SNAIL 
learn a parameterized predictor to estimate the parameters
in models. MetaOptNet advocates the use of linear
classiﬁer instead of nearest-neighbor methods which can be
optimized as convex learning problems. LEO utilizes
an encoder-decoder architecture to mine the latent genera-
Distribution
generation 1
generation 2
generation
Prediction
Point Graph
Figure 2: The overall framework of DPGN. In this illustration, we take a 2way-1shot task as an example. The support and
query embeddings obtained from feature extractor are delivered to the dual complete graph (a point graph and a distribution
graph) for transductive propagation generation after generation. The green arrow represents a edge-to-node transformation
(P2D, described in Section 3.2.1) which aggregates instance similarities to construct distribution representations and the
blue arrow represents another edge-to-node transformation (D2P, described in Section 3.2.2) which aggregates distribution
similarities with instance features. DPGN makes the prediction for the query sample at the end of generation l.
tive representations and predicts high-dimensional parameters in extreme low-data regimes.
In this section, we ﬁrst provide the background of fewshot learning task, then introduce the proposed algorithm in
3.1. Problem Deﬁnition
The goal of few-shot learning tasks is to train a model
that can perform well in the case where only few samples
are given.
Each few-shot task has a support set S and a query
set Q. Given training data Dtrain, the support set S ⊂
Dtrain contains N classes with K samples for each class
(i.e., the N-way K-shot setting), it can be denoted as
S = {(x1, y1), (x2, y2), . . . , (xN ×K, yN ×K)}. The query
Dtrain has ¯T samples and can be denoted
{(xN×K+1, yN×K+1), . . . , (xN×K+ ¯
T , yN×K+ ¯
Speciﬁcally, in the training stage, data labels are provided
for both support set S and query set Q. Given testing data
Dtest, our goal is to train a classiﬁer that can map the query
sample from Q ∈Dtest to the corresponding label accurately with few support samples from S ∈Dtest. Labels of
support sets and query sets are mutually exclusive.
3.2. Distribution Propagation Graph Networks
In this section, we will explain the DPGN that we proposed for few-shot learning in detail. As shown in Figure
2. The DPGN consists of l generations and each generation
consists of a point graph Gp
l ) and a distribution graph Gd
l ). Firstly, the feature embeddings
of all samples are extracted by a convolutional backbone,
these embeddings are used to compute the instance similarities Ep
l . Secondly, the instance relations Ep
l are delivered
to construct the distribution graph Gd
l . The node features
l are initialized by aggregating Ep
l following the position
order in Gp
l and the edge features Ed
l stand for the distribution similarities between the node features V d
l . Finally,
the obtained Ed
l is delivered to Gp
l for constructing more
discriminative representations of nodes V p
l and we repeat
the above procedure generation by generation. A brief introduction of generation update for the DPGN can be expressed as Ep
l+1, where l
denotes the l-th generation.
For further explanation, we formulate V p
l as follows: V p
l,ij}, V d
l,ij} where i, j = 1, · · · T. T = N × K + ¯T
denotes the total number of examples in a training episode.
0,i is ﬁrst initialized by the output of the feature extractor
femb. For each sample xi:
0,i = femb(xi) ,
0,i ∈Rm and m denotes the dimension of the feature embedding.
Point-to-Distribution Aggregation
Point Similarity
Each edge in the point graph stands for
the instance (point) similarity and the edge ep
0,ij of the ﬁrst
generation is initialized as follows:
0,ij = fep
0,ij ∈R. fep
0 : Rm −→R is the encoding network
that transforms the instance similarity to a certain scale. fep
Distribution Graph
query node
Point Graph
query node
Figure 3: Details about P2D aggregation and D2P aggregation in DPGN. A 2way-1shot task is presented as
an example. MLP-1 is the FC-ReLU blocks mentioned in
P2D Aggregation and MLP-2 is the Conv-BN-ReLU blocks
mentioned in D2P Aggregation. The green arrow denotes
the P2D aggregation while the blue arrow denotes the D2P
aggregation. Both aggregation processes integrate the node
or edge features of their previous generation.
contains two Conv-BN-ReLU blocks with the parameter set θep
0 and a sigmoid layer.
For generation l > 0, given ep
l−1,ij, vp
l−1,i and vp
l,ij can be updated as follows:
l,ij = fep
l−1,j)2) · ep
In order to use edge information with a holistic view of the
l , a normalization operation is conducted on the
P2D Aggregation
After edge features Ep
l in point graph
l are produced or updated, the distribution graph Gd
l ) is the next to be constructed. As shown in Figure
l aims at integrating instance relations from the point
l and process the distribution-level relations. Each
distribution feature vd
l is a NK dimension feature
vector where the value in j-th entry represents the relation
between sample xi and sample xj and NK stands for the
total number of support samples in a task. For ﬁrst initialization:
j=1 δ(yi, yj)
if xi is labeled,
NK , · · · ,
otherwise,
0,i ∈RNK and f is the concatenation operator. δ(·)
is the Kronecker delta function which outputs one when
yi = yj and zero otherwise (yi and yj are labels).
For generations l > 0, the distribution node vd
l,i can be
updated as follows:
l,i = P2D(
where P2D : (RNK, RNK) −→RNK is the aggregation
network for distribution graph. P2D applies a concatenation operation between two features. Then, P2D performs
a transformation : R2NK −→RNK on the concatenated
features which is composed of a fully-connected layer and
ReLU , with the parameter set θvd
Distribution-to-Point Aggregation
Distribution Similarity
Each edge in distribution graph
stands for the similarity between distribution features of different samples. For generation l = 0, the distribution similarity ed
0,ij is initialized as follows:
0,ij = fed
0,ij ∈R. The encoding network fed
0 : RNK −→R
transforms the distribution similarity using two Conv-BN-
ReLU blocks with the parameter set θed
0 and a sigmoid layer
in the end. For generation l > 0, the update rule for ed
l is formulated as follows:
l,ij = fed
l,j)2) · ed
Also, we apply a normalization to ed
D2P Aggregation
As illustrated in Figure 3, the encoded
distribution information in Gd
l ﬂows back into the point
l at the end of each generation. Then node features vp
l captures the distribution relations through
aggregating all the node features in Gp
l with edge features
l,i as follows:
l,i = D2P(
l−1,j), vp
l,i ∈Rm and D2P : (Rm, Rm) −→Rm is the aggregation network for point graph in Gp
l with the parameter set
l . D2P concatenates the feature which is computed by
l−1,j) with the node features vp
l−1,i in previous generation and update the concatenated feature with
two Conv-BN-ReLU blocks. After this process, the node
features can integrate the distribution-level information into
the instance-level feature and prepares for computing instance similarities in the next generation.
3.3. Objective
The class prediction of each node can be computed by
feeding the corresponding edges in the ﬁnal generation l of
DPGN into softmax function:
P( ˆyi|xi) = Softmax(
l,ij · one-hot(yj)) ,
where P( ˆyi|xi) is the probability distribution over classes
given sample xi, and yj is the label of jth sample in the
support set. ep
l,ij stands for the edge feature in the point
graph at the ﬁnal generation.
Point Loss
It is noted that we make classiﬁcation predictions in the point graph for each sample. Therefore, the
point loss at generation l is deﬁned as follows:
l = LCE(P( ˆyi|xi), yi) ,
where LCE is the cross-entropy loss function, T stands
for the number of samples in each task (S, Q) ∈Dtrain.
P( ˆyi|xi) and yi are model probability predictions of sample xi and the ground-truth label respectively.
Distribution Loss
To facilitate the training process and
learn discriminative distribution features , we incorporate
the distribution loss which plays a signiﬁcant role in contributing to faster and better convergence. We deﬁne the
distribution loss for generation l as follows:
l = LCE(Softmax(
l,ij · one-hot(yj)), yi) ,
l,ij stands for the edge feature in the distribution
graph at generation l.
The total objective function is a weighted summation of
all the losses mentioned above:
where ˆl denotes total generations of DPGN and the weights
λp and λd of each loss are set to balance their importance.
In most of our experiments, λp and λd are set to 1.0 and 0.1
respectively.
4. Experiments
4.1. Datasets and Setups
We evaluate DPGN on four standard few-shot learning
benchmarks:
miniImageNet , tieredImageNet ,
CUB-200-2011 and CIFAR-FS . The miniImageNet
and tieredImageNet are the subsets of ImageNet .
CUB-200-2011 is initially designed for ﬁne-grained classiﬁcation and CIFAR-FS is a subset of CIFAR-100 for fewshot classiﬁcation.
As shown in Table 1, we list details
for images number, classes number, images resolution and
train/val/test splits following the criteria of previous works
 .
Table 1: Details for few-shot learning benchmarks.
Images Classes Train-val-test Resolution
miniImageNet
tieredImageNet 779165
351/97/160
CUB-200-2011 11788
Experiment Setups
Network Architecture
We use four popular networks for
fair comparison, which are ConvNet, ResNet12, ResNet18
and WRN that are used in EGNN , MetaOptNet ,
CloserLook and LEO respectively. ConvNet mainly
consists of four Conv-BN-ReLU blocks.
The last two
blocks also contain a dropout layer . ResNet12 and
ResNet18 are the same as the one described in . They
mainly have four blocks, which include one residual block
for ResNet12 and two residual blocks for ResNet18 respectively. WRN was ﬁrstly proposed in . It mainly has
three residual blocks and the depth of the network is set to
28 as in . The last features of all backbone networks
are processed by a global average pooling, then followed
by a fully-connected layer with batch normalization to
obtain a 128-dimensions instance embedding.
Training Schema
We perform data augmentation before
training, such as horizontal ﬂip, random crop, and color jitter (brightness, contrast, and saturation), which are mentioned in .
We randomly sample 28 meta-task
episodes in each iteration for meta-training. The Adam optimizer is used in all experiments with the initial learning
rate of 10−3. We decay the learning rate by 0.1 per 15000
iterations and set the weight decay to 10−5.
Evaluation Protocols
We evaluate DPGN in 5way-
1shot/5shot settings on standard few-shot learning datasets,
miniImageNet,
tieredImageNet,
CUB-200-2011
CIFAR-FS. We follow the evaluation process of previous
approaches . We randomly sample 10,000 tasks
then report the mean accuracy (in %) as well as the 95%
conﬁdence interval.
4.2. Experiment Results
Main Results
We compare the performance of DPGN
with several state-of-the-art models including graph and
non-graph methods.
For fair comparisons, we employ
DPGN on miniImageNet, tieredImageNet, CIFAR-FS and
CUB-200-2011 datasets, which is compared with other
methods in the same backbones.
As shown in Table 2,
3 and 4, the proposed DPGN is superior to other existing
methods and achieves the state-of-the-art performance, especially compared with the graph-based methods.
Table 2: Few-shot classiﬁcation accuracies on miniImageNet.
† denotes thatit is implemented by public code.
 and DPGN are tested in transduction.
Backbone 5way-1shot
5way-5shot
MatchingNet 
43.56±0.84 55.31± 0.73
ProtoNet 
49.42±0.78 68.20±0.66
RelationNet 
50.44±0.82 65.32±0.70
51.20±0.60 68.20±0.60
48.70±1.84 55.31±0.73
Dynamic 
56.20±0.86 71.94±0.57
50.33±0.36 66.41±0.63
55.51±0.86 69.86±0.65
Global 
53.21±0.40 72.34±0.32
Edge-label 
ConvNet 59.63±0.52† 76.34±0.48
66.01±0.36 82.83±0.41
61.76±0.08 77.59±0.12
61.07±0.15 76.75±0.11
67.24±0.51 83.72±0.44
CloserLook 
ResNet18 51.75±0.80 74.27±0.63
ResNet18 62.05±0.55 78.63±0.06
ResNet18 66.63±0.51 84.07±0.42
MetaGAN 
ResNet12 52.71±0.64 68.63±0.67
SNAIL 
ResNet12 55.71±0.99 68.88±0.92
TADAM 
ResNet12 58.50±0.30 76.70±0.30
Shot-Free 
ResNet12 59.04±0.43 77.64±0.39
Meta-Transfer ResNet12 61.20±1.80 75.53±0.80
ResNet12 62.96±0.02 78.49±0.02
TapNet 
ResNet12 61.65±0.15 76.36±0.10
Dense 
ResNet12 62.53±0.19 78.95±0.13
MetaOptNet 
ResNet12 62.64±0.61 78.63±0.46
ResNet12 67.77±0.32 84.60±0.43
Semi-supervised Few-shot Learning
We employ DPGN
on semi-supervised few-shot learning. Following ,
we use the same criteria to split miniImageNet dataset into
Table 3: Few-shot classiﬁcation accuracies on tieredImageNet. † denotes that it is implemented by public code. *
denotes that it is reported from . and DPGN are
tested in transduction.
5way-1shot
5way-5shot
51.67±1.81
70.30±1.75
ProtoNet* 
53.34±0.89
72.69±0.74
RelationNet* 
54.48±0.93
71.32±0.78
59.91±0.94
73.30±0.75
Edge-label 
ConvNet 63.52±0.52† 80.24±0.49
69.43±0.49
85.92±0.42
ResNet18 64.78±0.11
81.05±0.52
ResNet18 70.46±0.52
86.44±0.41
TapNet 
ResNet12 63.08±0.15
80.26±0.12
Meta-Transfer ResNet12 65.62±1.80† 80.61±0.90†
MetaOptNet 
ResNet12 65.81±0.74
81.75±0.53
Shot-Free 
ResNet12 66.87±0.43
82.64±0.39
ResNet12 72.45±0.51
87.24±0.39
Table 4: Few-shot classiﬁcation accuracies on CUB-200-
2011 and CIFAR-FS. * denotes that it is reported from 
or . DPGN are tested in transduction.
CUB-200-2011
5way-1shot 5way-5shot
ProtoNet* 
51.31±0.91 70.77±0.69
55.92±0.95 72.09±0.76
MatchingNet* 
61.16±0.89 72.86±0.70
RelationNet* 
62.45±0.98 76.11±0.69
CloserLook 
60.53±0.83 79.34±0.61
53.15±0.84 81.90±0.60
ConvNet 76.05±0.51 89.08±0.38
ResNet12 68.87±0.22 82.90±0.15
ResNet12 75.71±0.47 91.48±0.33
5way-1shot 5way-5shot
ProtoNet* 
RelationNet* 
Shot-Free 
MetaOptNet 
labeled and unlabeled parts with different ratios. For a 20%
labeled semi-supervised scenario, we split the support samples with a ratio of 0.2/0.8 for labeled and unlabeled data
in each class. In semi-supervised few-shot learning, DPGN
uses unlabeled support samples to explicitly construct similarity distributions over all other samples and the distributions work as a connection between queries and labeled sup-
Label ratio
Test accuracy
DPGN (ours)
Edge-label
Figure 4: Semi-supervised few-shot learning accuracy in
5way-10shot on miniImageNet. DPGN surpass TPN and
EGNN by a large margin consistently.
port samples, which could propagate label information from
labeled samples to queries sufﬁciently.
Trasductive/non-transductive experiments on
miniImageNet. “BN” means information is shared among
test examples using batch normalization. † denotes that it is
implemented by public code released by authors.
Transduction 5way-5shot
Reptile 
Edge-label 
Reptile 
RelationNet 
Edge-label 
In Figure 4, DPGN shows the superiority to exsisting
semi-supervised few-shot methods and the result demonstrates the effectiveness to exploit the relations between labeled and unlabeled data when the label ratio decreases.
Notably, DPGN surpasses TPN and EGNN by
11% ∼16% and 7% ∼13% respectively in few-shot average classiﬁcation accuracy on miniImageNet.
Transductive Propagation
To validate the effectiveness
of the transductive setting in our framework, we conduct the transductive and non-transductive experiments on
miniImageNet dataset in 5way-5shot setting. Table 5 shows
10way-1shot
10way-5shot
20way-1shot
RelationNet
Edge_label
DPGN (ours)
Figure 5: High-way few-shot classiﬁcation accuracies on
miniImageNet.
that the accuracy of DPGN increases by a large margin in
the transductive setting (comparing with non-transductive
Compared to TPN and EGNN which consider
instance-level features only, DPGN utilizes distribution
similarities between query samples and adopts dual graph
architecture to propagate label information in a sufﬁcient
High-way classiﬁcation
Furthermore, the performance
of DPGN in high-way few-shot scenarios is evaluated on
miniImageNet dataset and its results are shown in Figure 5.
The observed results show that DPGN not only exceeds the
powerful graph-based methods but also surpasses
the state-of-the-art non-graph methods signiﬁcantly. As the
number of ways increasing in few-shot tasks, it can broaden
the horizons of distribution utilization and make it possible
for DPGN to collect more abundant distribution-level information for queries.
4.3. Ablation Studies
Impact of Distribution Graph
The distribution graph
l works as an important component of DPGN by propagating distribution information, so it is necessary to investigate the effectiveness of Gd
l quantitatively. We design the
experiment by limiting the distribution similarities which
l for performing aggregation in each generation
during the inference process. Speciﬁcally, we mask out the
edge features Ed
l through keeping a different number of feature dimensions and set the value of rest dimensions to zero,
since zero gives no contribution. Figure 6 shows the result
for our experiment in 5way-1shot on miniImageNet. It is
obvious that test accuracy and the number of feature dimensions kept in Ed
l have positive correlations and accuracy increment (area in blue) decreases with more feature dimen-
previous value
increment in current setting
Figure 6: Effectiveness of Gd
l through keeping n dimensions in 5way-1shot on miniImageNet.
Generation number
Test accuracy
miniImageNet
tieredImageNet
CUB-200-2011
Figure 7: Generation number in DPGN on miniImageNet,
tieredImageNet, CUB-200-2011 and CIFAR-FS.
sions. Keeping dimensions from 0 to 5, DPGN boosts the
performance nearly by 10% in absolute value and the result
shows that the distribution graph has a great impact on our
framework.
Generation Numbers
DPGN has a cyclic architecture
that includes point graph and distribution graph, each graph
has node-update and edge-update modules respectively.
The total number of generations is an important ingredient for DPGN, so we perform experiments to obtain the
trend of test accuracy with different generation numbers in
DPGN on miniImageNet, tieredImageNet, CUB-200-2011,
and CIFAR-FS. In Figure 7, with the generation number
changing from 0 to 1, the test accuracy has a signiﬁcant
rise. When the generation number changes from 1 to 10, the
test accuracy increases by a small margin and the curve becomes to ﬂuctuate in the last several generations. Consider-
query image
query image
Figure 8: The visualization of edge prediction in each generation of DPGN. (a) to (f) denotes generation 1 to 6. The
dark denotes higher score and the shallow denotes lower
conﬁdence. The left axis stands for the index of 5 query
images and the bottom axis stands for 5 support class.
ing that more generations need more iterations to converge,
we choose generation 6 as a trade-off between the test accuracy and convergence time. Additionally, to visualize the
procedure of cyclic update, we choose a test scenario where
the ground truth classes of ﬁve query images are and visualize instance-level similarities which is used for
predictions of ﬁve query samples as shown in Figure 8. The
heatmap shows DPGN reﬁnes the instance-level similarity
matrix after several generations and makes the right predictions for ﬁve query samples in the ﬁnal generation. Notably,
DPGN not only contributes to predicting more accurately
but also enlarge the similarity distances between the samples in different classes through making instance features
more discriminative, which cleans the prediction heatmap.
5. Conclusion
In this paper, we have presented the Distribution Propagation Graph Network for few-shot learning, a dual
complete graph network that combines instance-level and
distribution-level relations in an explicit way equipped with
label propagation and transduction. The point and distribution losses are used to jointly update the parameters of
the DPGN with episodic training. Extensive experiments
demonstrate that our method outperforms recent state-ofthe-art algorithms by 5%∼12% in the supervised task and
7%∼13% in semi-supervised task on few-shot learning
benchmarks. For future work, we aim to focus on the highorder message propagation through encoding more complicated information which is linked with task-level relations.
6. Acknowledgement
This research was supported by National Key R&D Program of China (No. 2017YFA0700800).