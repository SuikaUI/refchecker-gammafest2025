Adversarial Feature Hallucination Networks for Few-Shot Learning
Kai Li1, Yulun Zhang1, Kunpeng Li1, Yun Fu1,2
1Department of Electrical and Computer Engineering, Northeastern University, Boston, USA
2Khoury College of Computer Science, Northeastern University, Boston, USA
{kaili,kunpengli,yunfu}@ece.neu.edu, 
The recent ﬂourish of deep learning in various tasks
is largely accredited to the rich and accessible labeled
data. Nonetheless, massive supervision remains a luxury
for many real applications, boosting great interest in labelscarce techniques such as few-shot learning (FSL), which
aims to learn concept of new classes with a few labeled samples. A natural approach to FSL is data augmentation and
many recent works have proved the feasibility by proposing
various data synthesis models. However, these models fail
to well secure the discriminability and diversity of the synthesized data and thus often produce undesirable results. In
this paper, we propose Adversarial Feature Hallucination
Networks (AFHN) which is based on conditional Wasserstein Generative Adversarial networks (cWGAN) and hallucinates diverse and discriminative features conditioned on
the few labeled samples. Two novel regularizers, i.e., the
classiﬁcation regularizer and the anti-collapse regularizer,
are incorporated into AFHN to encourage discriminability
and diversity of the synthesized features, respectively. Ablation study veriﬁes the effectiveness of the proposed cW-
GAN based feature hallucination framework and the proposed regularizers. Comparative results on three common
benchmark datasets substantiate the superiority of AFHN
to existing data augmentation based FSL approaches and
other state-of-the-art ones.
1. Introduction
The rich and accessible labeled data fuel the revolutionary success of deep learning . However, in
many speciﬁc real applications, only limited labeled data
are available.
This motivates the investigation of fewshot learning (FSL) where we need to learn concept of
new classes based on a few labeled samples. To combat
with deﬁciency of labeled data, some FSL methods resort
to enhance the discriminability of the feature representations such that a simple linear classiﬁer learned from a
few labeled samples can reach satisfactory classiﬁcation results . Another category of methods investigate techniques of quickly and effectively updating a deep
neural network with a few labeled data, either by learning a meta-network and the corresponding updating rules
 , or by learning a meta-learner model that generates some components of a classiﬁcation network directly
from the labeled samples . Alternatively, the
third group of methods address this problem with data augmentation by distorting the labeled images or synthesizing
new images/features based on the labeled ones .
Our proposed method falls into the data augmentation
based category.
The basic assumption of approaches in
this category is that the intra-class cross-sample relationship learned from seen (training) classes can be applied to
unseen (test) classes. Once the cross-sample relationship is
modeled and learned from seen classes, it can be applied
on the few labeled samples of unseen class to hallucinated
new ones. It is believed that the augmented samples can diversify the intra-class variance and thus help reach sharper
classiﬁcation boundaries . Whatever data augmentation
technique is used, it is critical to secure discriminability of
the augmented samples, as otherwise they shall cast catastrophic impact on the classiﬁer.
On the other hand, the
decision boundary of a classiﬁer can be determined precisely only when labeled samples exhibit sufﬁcient intraclass variance. Thus, diversity of the augmented samples is
also of a crucial role. This is in fact the essential motivation
of investigating data augmentation for FSL, as a few labeled
samples encapsulate limited intra-class variance.
Though various data augmentation based FSL methods
have been proposed recently, they fail to simultaneously
guarantee discriminability and diversity of the synthesized
samples. Some methods learn a ﬁnite set of transformation
mappings between samples in each base (label-rich) classes
and directly apply them to seed samples of novel (labelscarce) classes. However, the arbitrary mapping may destroy discriminability of the synthesized samples .
Other methods synthesize samples speciﬁcally for certain
tasks which regularize the synthesis process . Thus,
these methods can guarantee discriminability of the synthearXiv:2003.13193v2 [cs.CV] 27 Oct 2020
sized samples. But the task would constrain the synthesis
process and consequently the synthesized samples tend to
collapse into certain modes, thus failing to secure diversity.
To avoid limitations of the existing methods, we propose Adversarial Feature Hallucination Networks (AFHN)
which consists of a novel conditional Wasserstein Generative Adversarial Networks (cWGAN) based feature
synthesis framework and two novel regularizers.
many other data augmentation based FSL approaches that
perform data augmentation in the image space ,
our cWGAN based framework hallucinates new features
by using the features of the seed labeled samples as the
conditional context. To secure discriminability of the synthesized features, AFHN incorporates a novel classiﬁcation
regularizer that constrains the synthesized features being of
high correlation with features of real samples from the same
class while of low correlation with those from the different
classes. With this constraint, the generator is encouraged to
generate features encapsulating discriminative information
of the class used as the conditional context.
It is more complicated to ensure diversity of the synthesized features, as conditional GANs are notoriously susceptible to the mode collapse problem that only samples from
limited distribution modes are synthesized. This is caused
by the use of usually high dimensional and structured data
as the condition tends to make the generator ignore the latent code, which controls diversity. To avoid this problem,
we propose a novel anti-collapse regularizer which assigns
high penalty for the case where mode collapse likely occurs. It is derived from the observation that noise vectors
that are closer in the latent code space are more likely to be
collapsed into the same mode when mapped to the feature
space. We directly penalize the ratio of the dissimilarity of
the two synthesized feature vectors and the dissimilarity of
the two noise vectors generating them. With this constraint,
the generator is forced to explore minor distribution modes,
thus encouraging diversity of the synthesized features.
With discriminative and diverse features synthesized, we
can get highly effective classiﬁers and accordingly appealing recognition results. In summary, the contributions of
this paper are as follows: (1) We propose a novel cWGAN
based FSL framework which synthesizes fake features by
taking those of the few labeled samples as the conditional
context. (2) We propose two novel regularizers that guarantee discriminability and diversity of the synthesized features. (3) The proposed method reaches the state-of-the-art
performance on three common benchmark datasets.
2. Related Work
Regarding the perspective of addressing FSL, existing
algorithms can generally be divided into three categories.
The ﬁrst category of methods aim to enhance the discriminability of the feature representations extracted from images. To this goal, a number of methods resort to deep metric learning and learn deep embedding models that produce
discriminative feature for any given image .
The difference lies in the loss functions used. Other methods following this line focus on improving the deep metric
learning results by learning a separate similarity metric network , task dependent adaptive metric , patch-wise
similarity weighted metric , neural graph based metric
 , etc.
A more common category of algorithms address FSL by
enhancing ﬂexibility of a model such that it can be readily updated using a few labeled samples. These methods
utilize meta-learning, also called learning to learn, which
learns an algorithm (meta-learner) that outputs a model (the
learner) that can be applied on a new task when given some
information (meta-data) about that task. Following this line,
some approaches aim to optimize a meta-learned classiﬁcation model such that it can be easily ﬁne-tuned using a few
labeled data . Other approaches
adopt neural network generation and train a meta-learning
network which can adaptively generate entire or some components of a classiﬁcation neural network from a few labeled samples of novel classes . The generated neural network is supposed to be more effective to
classify unlabeled samples from the novel classes, as it is
generated from the labeled samples and encapsulates discriminative information about these classes.
The last category of methods combat deﬁciency of the labeled data directly with data augmentation. Some methods
try to employ additional samples by some forms of transfer learning from external data . More popular approaches perform data augmentation internally by applying
transformations on the labeled images or the corresponding
feature representations. Naively distorting the images with
common transformation techniques (e.g., adding Gaussian
perturbation, color jittering, etc.) is particularly risky as it
likely jeopardizes the discriminative content in the images.
This is undesirable for FSL as we only have a very limited number of images to be utilized; quality control of the
synthesizing results for any single image is crucial as otherwise the classiﬁer could be ruined by the low-quality images. Chen et al. propose a series of methods of performing quality-controlled image distortions by applying perturbation in the semantic feature space , shufﬂing image
patches and explicitly learning an image transformation
network . Performing data augmentation in the feature
space seems more promising as the feature variance directly
affects the classiﬁer. Many approaches with this idea have
been proposed by hallucinating new samples for novel class
based on seen classes , composing synthesized representations , and using GANs .
This paper proposes Adversarial Feature Hallucination
Networks (AFHN), a new GAN-based FSL model that augments labeled samples by synthesizing fake features conditioned on those of the labeled ones. AFHN signiﬁcantly
differs from the two existing GAN based models 
in the following aspects. First, AFHN builds upon Wasserstein GAN (WGAN) model which is known for more stable
performance, while adopt the conventional GAN
framework. Second, neither nor has a classiﬁcation regularizer. The most similar optimization objective
in is the one which optimizes the synthesized features
as the outlier class (relative to the real class), while that in
 is a cycle-consistency objective. We instead regularize the synthesized features of being high correlation with
real features from the same classes and low correlation with
those from the different classes. Third, After training the
generator, we learn a standard Softmax classiﬁer using the
synthesize features, while utilize them to enhance
existing FSL methods. Last, we further propose the novel
anti-collapse regularizer to encourage diversity of synthesized features, while do not.
AFHN also bears some similarity with an existing feature hallucination based FSL method . But apparently
we adopt the GAN framework which has the discriminator
to regularize the features produced by the generator, while
 uses the simple generative model. Besides, AFHN synthesizes new features to learn a standard Softmax classiﬁer
for new classes, while utilizes them to enhance existing FSL classiﬁer. Moreover, we aim to hallucinate diverse
features with the novel anti-collapse regularizer, while 
does not have such an objective.
3. Algorithm
In this section, we ﬁrst brieﬂy introduce Wasserstein
GAN and then elaborate the details of how we build the
proposed AFHN model upon it.
3.1. Wasserstein GAN
GAN is a recently proposed generative model that has
shown impressive performance on synthesizing realistic images. The generative process in GAN is modeled as a game
between two competitive models, the generator and the discriminator. The generator aims to generate from noise fake
samples as realistic as possible such that the discriminator
cannot tell whether they are real or fake. The discriminator
instead tries the best to make the correct judgment. This adversarial game pushes the generator to extensively explores
the data distribution and consequently produces more visually appealing samples than conventional generative models. However, it is known that GAN is highly unstable in
training. analyzes the convergence properties of the objective function of GAN and proposes the Wasserstein GAN
(WGAN) which utilizes the Wasserstein distance in the objective function and is shown of better theoretical properties than the vanilla GAN. We adopt the improved variant
of WGAN , which optimizes the following min-max
˜x∼Pg[D(˜x)] −
x∼Pr[D(x)]
ˆx∼Pˆx[(∥∇ˆxD(ˆx)∥2 −1)2],
where Pr is the data distribution and Pg is the model distribution deﬁned by ˜x ∼G(z), with z ∼p(z) randomly
sampled from noise distribution p. Pˆx is deﬁned by sampling uniformly along straight lines between pairs of points
sampled from the data distribution Pr and the generator distribution Pg, i.e., ˆx = αx + (1 −α)˜x with α ∼U(0, 1).
The ﬁrst two terms approximate the Wasserstein distance
and the third term penalizes the gradient norm of ˆx.
3.2. Adversarial Feature Hallucination Networks
Following the literature, we formally deﬁne FSL as follows: Given a distribution of tasks P(T ), a sample task
T ∼P(T ) is a tuple T = (ST , QT ) where the support set
ST = {{xi,j}K
j=1 contains K labeled samples from
each of the N classes. This is usually known as K-shot
N-way classiﬁcation. QT = {(xq, yq)}Q
q=1 is the query set
where the samples come from the same N classes as the
support set ST . The learning objective is to minimize the
classiﬁcation prediction risk of QT , according to ST .
The proposed AFHN approaches this problem by
proposing a general conditional WGAN based FSL framework and two novel regularization terms. Figure 1 illustrates the training pipeline.
FSL framework with conditional WGAN. For a typical
FSL task T = (ST , QT ), the feature extraction network F
produces a representation vector for each image. Specifically for an image from the support set (x, y) ∈ST , F
When there are multiple samples for class y, i.e., K > 1,
we simply average the feature vectors and take the averaged
vector as the prototype of class y . Conditioned on s,
we synthesize fake features for the class.
Unlike previous GAN models which sample a single random noise variable from some distribution, we sample two
noise variables z1 and z2 ∼N(0, 1). The generator G synthesizes fake feature ˜s1 (˜s2) taking as input z1 (z2) and the
class prototype s,
˜si = G(s, zi), i = 1, 2.
The generator G aims to synthesize ˜si to be as similar as
possible to s. The discriminator D tries to discern ˜si as fake
and s as real. Within the WGAN framework, the adversarial
training objective is as follows,
(x,y)∼ST [D(˜si, s)] −
(x,y)∼ST [D(s, s)]
(x,y)∼ST [(∥∇ˆsiD(ˆsi, s)∥2 −1)2], i = 1, 2.
𝑧 ~ 𝑁(0, 1)
Discriminator
Few-shot classifier
Figure 1. Framework of the proposed AFHN. AFHN takes as input a support set and a query set where images in the query set belongs
to the sampled classes in the support set. Each image in the support set is fed to the feature extraction network F, resulting the feature
embedding s. With s, feature generator G synthesizes two fake features ˜s1 and ˜s2, by combining s with two randomly sampled variables
z1 and z2. Discriminator D discriminates real feature s and fake features ˜s1 and ˜s2, resulting in the GAN loss LGAN. By analyzing the
relationship between (z1, z2) and (˜s1, ˜s2), we get the anti-collapse loss Lar. The proposed few-shot classiﬁer classiﬁes the features of the
query images based on the fake features ˜s1 and ˜s2. This results in the classiﬁcation loss Lcr.
Simply training the model with the above GAN loss
does not guarantee the generated features are well suited for
learning a discriminative classiﬁer because it neglects the
inter-class competing information among different classes.
Moreover, since the conditioned feature vectors are of high
dimension and structured, it is likely that the generator will
neglect the noise vectors and all synthesized features collapse to a single or few points in the feature space, i.e.,
the so-called mode collapse problem. To avoid these problems, we append the objective function with a classiﬁcation
regularization term and an anti-collapse regularization term,
aiming to encourage both diversity and discriminability of
the synthesized features.
Classiﬁcation regularizer. As our training objective is to
classify well samples in the query set QT , given the support
set ST , we encourage discriminability of the synthesized
features by requiring them serving well the classiﬁcation
task as the real features. Inspired by , we deﬁne a nonparametric FSL classiﬁer which calculates the possibility of
a query image (xq, yq) ∈QT of being the same class as
synthesized feature ˜si as
P(yq = y|xq) =
exp(cos(˜si, q))
j=1 exp(cos(˜sj
where q = F(xq). ˜sj
i is the synthesized feature for the j-th
class and cos(a, b) is the Cosine similarity of two vectors.
The adoption of Cosine similarity, rather than Euclidean
distance as in , is inspired by a recent FSL algorithm
 which proves that Cosine similarity can bound and reduce variance of the features and result in models of better
generalization.
With the proposed FSL classiﬁer, the classiﬁcation regularizer in a typical FSL task is deﬁned as follows:
(xq,yq)∼QT
y log[−P(yq = y|xq)]
for i = 1, 2. We can see that this regularizer explicitly
encourages the synthesized features to have high correlation with features from the same class (the conditional context), while low correlation with features from the different
classes. To achieve this, the synthesized features must encapsulate discriminative information about the conditioned
class and thus secure discriminability.
Anti-collapse regularizer.
GAN models are known for
suffering from the notorious mode collapse problem, especially conditional GANs where structured and highdimensional data (e.g., images) are usually used as the conditional contexts. As a consequence, the generator likely
ignores the latent code (noises) that accounts for diversity
and focuses only on the conditional contexts, which is undesirable. Speciﬁcally to our case, our goal is to augment
the few labeled samples in the feature space; when mode
collapse occurs, all synthesized features may collapse to a
single or a few points in the feature space, failing to diversify the labeled samples. Observing that noise vectors
that are closer in the latent code space are more likely to be
collapsed into the same mode when mapped to the feature
space, we directly penalize the ratio of the dissimilarity two
synthesized feature vectors and the dissimilarity of the two
noise vectors generating them.
Remember that we sample two random variables z1 and
z2. We generate two fake feature vectors ˜s1 and ˜s2 from
them. When z1 and z2 are closer, s1 and s2 are more likely
to be collapsed into the same mode. To mitigate this, we
Algorithm 1. Proposed FSL algorithm
Input: Training set Dt = {Xt, Yt}, parameters λ, α, and β.
Output: Feature extractor F, generator G, discriminator D.
1. Train F as a standard classiﬁcation task using Dt.
while not done do
// Fix G and update D.
2. Sample from Dt a batch of FSL tasks T d
For each T d
3. Sample a support set ST = {{xi,j}K
query set QT = {{xk,j}Q
4. Compute prototypes of the N classes P = {sj}N
where sj =
i=1 F(xi,j).
5. Sample N noise variables Z1 = {zj
variables Z2 = {zj
6. Generate fake feature sets ˜
j=1 according to Eq. (3).
7. Update D by maximizing Eq. (8).
// Fix D and update G.
8. Sample from Dt a batch of FSL tasks T g
For each T g
9. Execute steps 3 - 7.
10. Update G by minimizing Eq. (8).
deﬁne the anti-collapse regularization term as
h 1 −cos(˜s1,˜s2)
1 −cos(z1, z2)
We can observe that this term ampliﬁes the dissimilarity of
the two fake feature vectors when the latent codes generating them are of high similarity. With the case mode collapse
more likely occurs being assigned with higher penalty, the
generator is forced to mine minor modes in the feature space
during training. The discriminator will also handle fake features from the minor modes. Thus, it is expected that more
diverse features can be synthesized when applying the generator on novel classes.
With the above two regularization terms, we reach our
ﬁnal training objective as
Lcri + β 1
where α and β are two hyper-parameters. Algorithm 1 outlines the main training steps of the proposed method.
3.3. Classiﬁcation with Synthesized Samples
In the test stage, given an FSL task T ′ = (S′
randomly sampled from the test set that the classes have
no overlap with those in the training set, we ﬁrst augment the labeled support set S′
T with the learned generator G.
Then, we train a classiﬁer with the augmented
supported set.
The classiﬁer is used to classify samples from the query set Q′
Speciﬁcally, suppose after
data augmentation, we get an enlarged support set ˆS′
{(s1, y1), (s2, y2), · · · , (sN×K′, yN×K′} where K′ is the
number of samples synthesized for each class. With ˆS′
we train a standard Softmax classiﬁer fc as
log[−P(y|s; θ)],
where θ is the parameter of fc. With fc, we classify samples
4. Experiments
We evaluate AFHN on three common benchmark
datasets, namely, Mini-ImageNet , CUB and CI-
FAR100 . The Mini-ImageNet dataset is a subset of ImageNet. It has 60,000 images from 100 classes, 600 images for each class. We follow previous methods and use
the splits in for evaluation, i.e., 64, 16, 20 classes as
training, validation, and testing sets, respectively. The CUB
dataset is a ﬁne-grained dataset of totally 11,788 images
from 200 categories of birds. We use the split in and
100, 50, 50 classes for training, validation, and testing, respectively. The CIFAR-100 dataset contains 60,000 images
from 100 categories. We use the same data split as in .
In particular, 64, 16 and 20 classes are used for training,
validation and testing, respectively.
Following previous methods, we evaluate 5-way 1-shot
and 5-way 5-shot classiﬁcation tasks where each task instance involves classifying test images from 5 sampled
classes with 1 or 5 randomly sampled images for each class
as the support set. In order to reduce variance, we repeat
the evaluation task 600 times and report the mean of the
accuracy with a 95% conﬁdence interval.
4.1. Implementation Details
Following the previous data augmentation based methods , we use ResNet18 as our feature extraction network F. We implement the generator G as a twolayer MLP, with LeakyReLU activation for the ﬁrst layer
and ReLU activation for the second one. The dimension of
the hidden layer is 1024. The discriminator is also a twolayer MLP, with LeakyReLU as the activation function for
the ﬁrst layer. The dimension of the hidden layer is also
1024. The noise vectors z1 and z2 are drawn from a unit
Gaussian with the same dimensionality as the feature embeddings.
Following the data augmentation based FSL methods
 , we perform two-step training procedures. In the
ﬁrst step, we only train the feature extraction network F as
a multi-class classiﬁcation task using only the training split.
We use Adam optimizer with an initial learning rate 10−3
Table 1. Ablation study on the Mini-ImageNet dataset for the 5way 1-shot setting. cWGAN, CR, and AR represent the conditional WGAN framework, the classiﬁcation regularizer, and the
anti-collapse regularizer, respectively. The baseline result (52.73)
is obtained by applying the SVM classiﬁer directly on ResNet18
features without data augmentation. The result (55.65) with only
CR added is obtained from the synthesized features produced by
the generator without the discriminator and AR during training.
which decays to the half every 10 epochs. We train F with
100 epochs with batch size of 128. In the second training
stage, we train the generator and discriminator alternatively,
using features extracted by F and update G after every 5
updates of D. We also use Adam optimizer which has an
initial learning rate of 10−5 and decays to the half every 20
epochs for both G and D. We train the whole network with
100 epochs with 600 randomly sampled FSL tasks in each
epoch. For the hyper-parameters, we set λ = 10 as suggested by , and α = β = 1 for all the three datasets.
During the test stage, we synthesize 300 fake features for
each class.
The code is developed based on PyTorch.
4.2. Ablation Study
The proposed AFHN consists of the novel conditional
WGAN (cWGAN) based feature synthesize framework and
the two regularizers that encourage diversity and discriminability of the synthesized features, i.e., the Classiﬁcation
Regularier (CR) and Anti-collapse Regularizer (AR). To
evaluate the effectiveness and impact of these components,
we conduct ablation study on the Mini-ImageNet dataset for
the 5-way 1-shot setting. The results are shown in Table 1.
CR. This regularizer constrains the synthesized features to
have desirable classiﬁcation property such that we can train
from them a discriminative classiﬁer. We can see that when
it is used as the only regularization for the generator, it
raises the baseline result from 52.73 to 55.65. On the other
hand, when it is used along with cWGAN (the discriminator regularizes the generated features, resulting in the GAN
loss), it helps further boost the performance from 57.58 to
60.56. Therefore, in the both cases (with and without cW-
GAN), CR helps enhance discriminability of the synthesized features and leads to performance boost.
cWGAN. Compared with the baseline (without data augmentation), cWGAN helps raise the accuracy from 52.73
to 57.58. This is because the synthesized features enhance
the intra-class variance, which makes classiﬁcation decision
boundaries much sharper. Moreover, with CR as the regularizer, our cWGAN based generative model boosts the
performance of the naive generative model from 55.65 to
60.56. This further substantiates the effectiveness of the
proposed cWGAN framework.
The performance gain is
due to the adversarial game between the generator and the
discriminator, which enhances the generator’s capability of
modeling complex data distribution among training data.
The enhanced generator is therefore able to synthesize features of both higher diversity and discriminability.
As mentioned in the related work, one of the major differences of the proposed AFHN from the other feature hallucination based FSL method is that AFHN is an adversarial generative model while uses a naive generative
model. This study thus evidences the advantage of AFHN
over .
AR. AR aims to encourage the diversity of the synthesized
features by explicitly penalizing the case where mode collapses more likely occur. Table 1 shows that it further brings
about 2% performance gains, thus proving its effectiveness.
4.3. Comparative Results
Mini-Imagenet.
Mini-Imagenet is the most extensively
evaluated dataset. From Table 2 we can observe that AFHN
attains the new state-of-the-art, for both the 1-shot and 5shot setting. Compared with the other four data augmentation based methods, AFHN reaches signiﬁcant improvements: it beats ∆-encoder by more than 8% for the
5-shot setting and Dual TriNet by more than 3% for the
1-shot setting. Compared with MetaGAN which is also
based on GAN, AFHN achieves about 10% improvements
for both the 1-shot and 5-shot settings. Besides the signiﬁcant advantages over the peer data augmentation based
methods, AFHN also exhibits remarkable advantages over
the other two categories of methods. It beats the best metric
learning based method DCEM by about 3.5% for the 1shot setting. It also performs better than the state-of-the-art
meta-learning based algorithms. Compared with the baseline method, “ResNet18+SVM”, AFHN reaches about 10%
and 5% improvements for the 1-shot and 5-shot settings,
respectively. This substantiates the effectiveness of our proposed data augmentation techniques.
CUB. This is a ﬁne-grained bird dataset widely used for
ﬁne-grained classiﬁcation. Recently, it has been employed
for few-shot classiﬁcation evaluation. Thus, relatively less
results are reported on this dataset. From Table 3 we can see
that AFHN reaches comparable results with both the other
two data augmentation based methods Dual TriNet and ∆encoder.
It beats the best metric learning based method
SAML by 2.4% for the 5-shot setting, and performs
signiﬁcantly better than the meta-learning based methods.
Compared with the baseline, we only have a moderate improvement in the 1-shot setting and reach only a marginal
boost for the 5-shot setting. We speculate the reason is that
this dataset is relatively small, less than 60 images per class
ResNet18 + SVM (baseline)
52.73±1.44
73.31±0.81
Matching Net 
NeurIPS’16
43.56±0.84
55.31±0.73
PROTO Net 
NeurIPS’17
49.42±0.78
68.20±0.66
MM-Net 
53.37±0.48
66.97±0.35
50.33±0.36
66.41±0.63
RELATION NET 
50.44±0.82
65.32±0.70
51.24±0.74
71.02±0.64
55.51±0.86
69.86±0.65
55.22±0.84
71.55±0.66
57.69±0.20
73.03±0.16
58.71±0.62
77.28±0.46
48.70±1.84
63.11±0.92
META-LSTM 
43.44±0.77
60.60±0.71
SNAIL 
ResNet-256F
55.71±0.99
68.88±0.92
41.09±0.32
58.32±0.21
DFSVL 
55.95±0.89
73.00±0.68
META-SGD 
50.47±1.87
64.03±0.94
59.60±0.41
73.74±0.19
61.76±0.08
77.59±0.12
MetaGAN 
NeurIPS’18
52.71±0.64
68.63±0.67
Dual TriNet 
58.80±1.37
76.71±0.69
∆-encoder 
NeurIPS’18
IDeMe-Net 
59.14±0.86
74.63±0.74
AFHN (Proposed)
62.38±0.72
78.16±0.56
Table 2. Few-shot classiﬁcation accuracy on Mini-Imagenet. “MetricL”, “MetaL” and “DataAug” represent metric learning based category,
meta-learning based category and data augmentation based category, respectively. The “±” indicates 95% conﬁdence intervals over tasks.
The best results are in bold.
ResNet18 + SVM (baseline)
66.54±0.53
82.38±0.43
59.65±0.78
76.75±0.73
Matching Net 
NeurIPS’16
50.53±0.87
60.30±0.82
PROTO Net 
NeurIPS’17
53.15±0.84
81.90±0.60
69.33±0.22
81.56±0.15
49.28±0.90
58.30±0.80
META-LSTM 
META-SGD 
Dual TriNet 
63.41±0.64
78.43±0.64
∆-encoder 
NeurIPS’18
69.80±0.46
82.60±0.35
AFHN (Proposed)
70.53±1.01
83.95±0.63
68.32±0.93
81.45±0.87
Table 3. Few-shot classiﬁcation accuracy on CUB and CIFAR100. Please refer Table 2 for details.
on average; a large number of classes only have about 30
images. Due to the small scale of this dataset, the intra-class
variance is less signiﬁcant than that of the Mini-Imagenet
dataset, such that 5 labeled samples are sufﬁcient to capture
most of the intra-class variance. Performing data augmentation is less crucial than that for the other datasets.
CIFAR100. This dataset has the identical structure as the
Mini-ImageNet dataset. Table 3 shows that AFHN performs
the best over all the existing methods and the advantages
are sometimes signiﬁcant. AFHN beats Dual TriNet by 5%
and 3% for 1-shot and 5-shot respectively. Compared with
the best meta-learning based method, we get 7% and 4%
improvements for the 1-shot and 5-shot respectively. Compared with the baseline method, AFHN also reach remarkable gains. We reach about 10% and 5% improvements for
1-shot and 5-shot respectively. This great improvement convincingly substantiates the effectiveness of our GAN based
data augmentation method for solving the FSL problem.
cWGAN + CR
cWGAN + CR + AR
Figure 2. t-SNE visualization of synthesized feature embeddings. The real features are indicated by ⋆. Different colors represent
different classes.
500 1000 2000
Number of synthesized samples per class
Accuracy (%)
Figure 3. Impact of the number of synthesized samples for each
class on the Mini-ImageNet dataset.
In summary, among all the three datasets, we reach signiﬁcant improvements over existing state-of-the-art methods for two of them, while being comparable for the left
one. For all the datasets, our method reaches signiﬁcant
boost to the baseline method where there is no data augmentation. These experiments substantiate the effectiveness and
superiority of the proposed method.
4.4. Further Analysis
Impact of the number of synthesized features. Figure 3
shows the analysis on Mini-ImageNet about the recognition
accuracy with respect to the number of synthesized features
for each class during test. We can observe that the classiﬁcation accuracy keeps boosted with more features synthesized
at the beginning, and remains stable with even more synthesized samples. This is reasonable because the class variance
encapsulated by the few labeled samples has a upper bound;
data augmentation based on these labeled samples can enlarge the variance to some extent, but it is still bounded by
the few labeled samples themselves. When it reaches the
peak, the performance reasonably turns stable.
Visualization of synthesized features. We showed quantitatively in the ablation study that owing to the CR and
AR regularizers, we can generate diverse and discriminative
features which bring signiﬁcant performance gains. Here
we further study the effect of the two regularizers by showing the t-SNE visualization of the synthesized features. As
shown in Figure 2, the synthesized features of different
classes mix up together when using only cWGAN for augmentation. As analyzed before, cWGAN does not guarantee
synthesizing semantically meaningful features. The problem is substantially resolved when we train cWGAN with
CR. The synthesized features exhibit clear clustering structure, which helps train a discriminative classiﬁer. Furthermore, with AR added, the synthesized features still exhibit
favorable clustering structure. But taking a closer look of
the visualization, we can ﬁnd that the features synthesized
with AR added are more diverse than that without it: the
clusterings are less compact, stretched to larger regions, and
even contains some noises. This shows AR indeed helps diversify the synthesized features.
5. Conclusions
We introduce the Adversarial Feature Hallucination Networks (AFHN), a new data augmentation based few-shot
learning approach.
AFHN consists of a novel conditional Wasserstein GAN (cWGAN) based feature synthesis
framework, the classiﬁcation regularizer (CR) and the anticollapse regularizer (AR). Based on cWGAN, our framework synthesizes fake features for new classes by using the
features of the few labeled samples as the conditional context. CR secures feature discriminability by requiring the
synthesized features to be of high similarity with features of
the samples from the same classes, while of low similarity
with those from the different classes. AR aims to enhance
the diversity of the synthesized features by directly penalizing the cases where the mode collapse problem likely occurs.
The ablation study shows the effectiveness of the
cWGAN based feature synthesis framework, as well as the
two regularizers. Comparative results verify the superiority of AFHN to the existing data augmentation based FSL
approaches as well as other state-of-the-art ones.
Acknowledgement: This research is supported by the U.S.
Army Research Ofﬁce Award W911NF-17-1-0367.