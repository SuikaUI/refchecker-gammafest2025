Evolution of Neural Networks
for Classiﬁcation and Regression⋆
Miguel Rocha1, Paulo Cortez2, and Jos´e Neves1
1 Dep. Inform´atica, Universidade do Minho, 4710-057 Braga, PORTUGAL
 
 
 
2 Dep. Sistemas de Informa¸c˜ao, Univ. do Minho, 4800-058 Guimar˜aes, PORTUGAL
 
WWW home page: 
Abstract. Although Artiﬁcial Neural Networks (ANNs) are important
Data Mining techniques, the search for the optimal ANN is a challenging
task: the ANN should learn the input-output mapping without overﬁtting the data and training algorithms may get trapped in local minima.
The use of Evolutionary Computation (EC) is a promising alternative for
ANN optimization. This work presents two hybrid EC/ANN algorithms:
the ﬁrst evolves neural topologies while the latter performs simultaneous
optimization of architectures and weights. Sixteen real-world tasks were
used to test these strategies. Competitive results were achieved when
compared with a heuristic model selection and other Data Mining algorithms.
Keywords: Supervised Learning, Multilayer Perceptrons, Evolutionary
Algorithms, Lamarckian Optimization, Neural Network Ensembles.
Introduction
Artiﬁcial Neural Networks (ANNs) denote a set of connectionist models inspired
in the behavior of the human brain. In particular, the Multilayer Perceptron
(MLP) is the most popular ANN architecture, where neurons are grouped in
layers and only forward connections exist . This provides a powerful baselearner, with advantages such as nonlinear mapping and noise tolerance, increasingly used in Data Mining due to its good behavior in terms of predictive
knowledge .
The interest in MLPs was stimulated by the advent of the Backpropagation
algorithm in 1986 and since then, several fast gradient based variants have been
proposed (e.g., RPROP) . Yet, these training algorithms minimize an error
function by tuning the modiﬁable parameters of a ﬁxed architecture, which needs
to be set a priori. The MLP performance will be sensitive to this choice: a small
network will provide limited learning capabilities, while a large one will induce
generalization loss (i.e., overﬁtting).
⋆This work was supported by the FCT project POSI/EIA/59899/2004.
Thus, the correct design of the MLP topology is a complex and crucial task,
commonly addressed by trial-and-error procedures (e.g. exploring diﬀerent number of hidden nodes), in a blind search strategy, which only goes through a
small set of possible conﬁgurations. More elaborated methods have also been
proposed, such as pruning and constructive algorithms, although these
perform hill-climbing and are thus prone to local minima.
Evolutionary Computation (EC) is a good candidate for MLP design, since
it performs a global multi-point (or beam) search, quickly locating areas of high
quality, even when the search space is very complex. The combination of EC and
ANNs, called Evolutionary Neural Networks (ENNs), is a suitable candidate for
topology design, due to the error surface features :
– the number of nodes/connections is unbounded;
– changes are discrete; and
– the mapping from the structure to its performance is indirect: similar topologies may present diﬀerent performances, while distinct topologies may result
in similar behavior.
Indeed, the most common approach when using EC to optimize ANNs is to
evolve neural structures that are trained by gradient based algorithms. However, this presents some drawbacks since the gradient based algorithms used to
perform the MLP training are sensitive to parameter settings and to the initial
weights. Thus, very often, the error surface is rugged and the training process is
trapped in local minima. Furthermore, evolving neural structures without any
weight information will make it harder to access the real performance of a given
topology due to the noisy ﬁtness evaluation problem : diﬀerent random initial
weights may produce quite distinct ﬁtness values.
An alternative is to use the global search advantages of EC to simultaneously
evolve topologies and train the ANN, by considering genetic operators that work
both on the topology and on the values of the connection weights. Under this
context, the use of Lamarckian evolution is a promising approach, where EC
is combined with a local search procedure (e.g. tabu search or gradient based
algorithm), improving the ﬁtness of the individuals during their lifetime.
Ensembles are another promising Data Mining research ﬁeld, where several
models are combined to produce an answer . Often, it is possible to build
ensembles that are better than individual learners given that the errors made by
the individual models are uncorrelated, a condition easily met in ANN Ensembles
since training algorithms are stochastic in nature. Since ENNs already create a
population of ANNs during evolution, this strategy can be easily adapted to
ENNs with no computational eﬀort increase.
In this work, two ENN combinations are presented: the evolution of neural
topologies and the simultaneous optimization of ANN topologies and weights. In
both cases, a direct representation will be used as well as a structural mutation,
which adds or deletes connections or weights. In the second ENN, connection
weights are optimized through a Lamarckian evolution that uses a random mutation and a gradient-based algorithm (e.g. RPROP) for the local search. Both
evolutionary techniques will be tested in classiﬁcation and regression problems,
using single ANN and ensemble based models. Then, the results will be compared with a heuristic ANN selection procedure, as well with other Data Mining
The paper is organized as follows. First, a description is given on the neural
and evolutionary algorithms (Section 2). Then, in Section 3 the experiments
performed are described and the results analyzed. Finally, conclusions are drawn
in Section 4.
Learning Methods
Data Mining tasks
This work endorses two important Data Mining goals: classiﬁcation and regression tasks. The former requires a correct labeling between input attributes and
one of several predeﬁned classes (e.g., classifying cells for cancer diagnosis). The
latter deals with a mapping between a n-dimensional input vector and a realvalue variable (e.g., stock market prediction). The main diﬀerence is set in terms
of the output representation, since both tasks require supervised learning and
can be modeled by MLPs. Nevertheless, the performance metrics are quite distinct and some methods can only be applied to one of these goals (e.g. Decision
Trees) .
Two distinct accuracy measures will be adopted: the Percentage of Correctly
Classiﬁed Examples (PCCE), used in classiﬁcation tasks; and the Normalized
Root Mean Squared Error (NRMSE), applied in the regression ones. These measures are given by the equations:
1 , if Ti = Pi
i=1 Φ(i)/K × 100 (%)
i=1 (Ti −Pi)2/K
i=1 Ti/K × 100 (%)
where K denotes the number of examples; Pi, Ti the predicted and target values
for the i-th example. High PCCE values suggest good classiﬁers, while a good
regressor should present a low NRMSE.
Neural Networks
The MLPs used in this study make use of biases, sigmoid activation functions
and one hidden layer with a variable number of nodes. A diﬀerent approach was
followed for the regression tasks, since outputs may lie out of the logistic output
range ( ). Hence, shortcut connections and linear functions were applied on
the output neuron(s), to scale the range of the outputs (Fig. 1) . This solution
avoids the need of ﬁltering procedures, which may give rise to information loss.
Input Layer
Hidden Layer
Output Layer
connection
Fig. 1. A fully connected Multilayer Perceptron with one output neuron, bias and
shortcuts.
Before feeding the MLPs, the data was preprocessed with a 1-of-C encoding
(one binary variable per class) applied to the nominal attributes and all inputs
were rescaled within the range [−1, 1]. For example, the safety attribute from
the task car was encoded as: low →(1 -1 -1), med →(-1 1 -1) and high→(-1
-1 1). Regarding the outputs, the discrete variables were normalized within the
range (using also a 1-of-C encoding for the nominal attributes). Therefore,
the predicted class is given by the nearest class value to the node’s output, if one
single node is used (binary variable); otherwise the node with the highest output
value is considered. On the other hand, regression problems will be modeled by
one real-valued output, which directly represents the dependent target variable.
Heuristic model selection
In order to provide a basis for comparison with the ENNs developed in this
work, an Heuristic approach (HNN) to model selection was deﬁned by a simple
trial-and-error procedure, where N + 1 fully connected MLPs, with a number of
hidden nodes ranging from 0 to N, are compared.
For each MLP, the initial weights were randomly set within the range [−1, 1].
Next, the RPROP algorithm was selected for training, due to its faster
convergence and stability. This procedure is stopped after a maximum of 500
epochs or when the error slope was approaching zero. Then, the MLP with the
lowest validation error (computed over non training data) was selected.
A distinct approach is to use the trained MLPs (as before) in order to build an
ANN Ensemble. When building ensembles, two main issues need to be contemplated: the method used to select individual models to be part of the ensemble
and how to combine their outputs into a single value. Previous work by the
authors has shown that:
– Using individual ANNs with heterogeneous topologies, where a family of
MLPs with distinct structures (and therefore complexities) are combined,
outperforms other heuristics such as injecting randomness (i.e. training
MLPs with the same topology but distinct initial weights);
– The best way to combine the outputs of each individual MLP is to use a
simple average, where the output of the ensemble is given by:
Ei,j = (PN
k=0 Si,j,k)/(N + 1)
where Si,j,k denotes the j output node value for the i example, given by a
MLP with k hidden nodes; Ei,j is the ﬁnal ensemble output for the i example
and j output node. For classiﬁcation tasks, the interpretation of the outputs
is made over the averaged values (Ei,j).
Therefore, these two options will be used in this work, in a conﬁguration
denoted by Heuristic Neural Network Ensemble (HNNE), which is build of N +1
trained MLPs, with the number of hidden nodes varying from 0 to N. It is
important to note that this method does not imply extra computational work
when compared with the HNN conﬁguration.
Evolution of Neural Network Topologies
In this work, the ﬁrst ENN to be presented is the Topology-optimization Evolutionary Neural Network (TENN), where the aim is to evolve the optimal structure of a MLP to a given problem. Under this approach, a direct representation
was embraced, where the genotype is the whole MLP.
The data is divided into training and validation sets (TS and V S). Then, each
individual is evaluated by setting the weights of its connections to a random value
and training the MLP in its genotype, using the process described above, i.e., the
RPROP algorithm with 500 epochs. The value of the error of the trained MLP
in the validation set gives the ﬁtness of the individual (RMSE on V S). In both
classiﬁcation and regression tasks, the error metric used by the ﬁtness function is
the RMSE, since this is a smoother statistic than PCCE. Nevertheless, in the
analysis of the results (next section), the PCCE will be used in the classiﬁcation
datasets and the NMSE for the regression ones.
Regarding the initialization of the population, each individual is set by choosing a random number of hidden nodes between 0 and H. Then, each possible
connection is set with a probability of p. New individuals are created by the
application of a structural mutation operator, which works by adding or deleting a random number (generated between 1 and M) of nodes or connections
(Fig. 2.c and 2.d). This was the only operator used in the evolution of new solutions, since the crossover operator was discarded due to the fact that previous
experiments revealed no gain in its use. This behavior is probably related to
the permutation problem ; i.e., several genomes may encode the same MLP.
Nevertheless, this ENN is still potentially able to search through any kind of
MLP connectivity, ranging from linear models to complex nonlinear MLPs.
In TENN, a population of P individuals evolves for G generations and a
selection scheme involving the conversion of ﬁtness values into rankings and the
application of a roulette wheel scheme is used. In every generation, half of the
individuals are kept, and the remaining are bred using the genetic operator. The
best individual is always kept, i.e. an elitism of 1 is used. The whole TENN
procedure is given in by the pseudo-code:
Split the data into TS and V S
Ψi ←Initialize the population with P MLPs
Evaluate the initial population Ψi
WHILE (i < G)
Ai ←Select P/2 ancestors from Ψi for reproduction
Oi ←Apply the structural mutation over all elements of Ai
Evaluate the offspring Oi
Si ←Select P/2 −1 survivors from Ψi
Set the next generation (Ψi+1 ←best(Ψi) S Si
The evaluation function for a given population Ψ of size PΨ is given by:
WHILE (j ≤PΨ)
MLPj ←Select the j-th MLP from the population Ψ
Initialize the MLPj weights
Train the MLPj on TS with 500 RPROP epochs
Set the MLPj fitness as the RMSE on V S
A TENN Ensemble (TENNE) will be built using the best N individuals (the
ones with lower validation errors) obtained during the evolutionary process. The
ensemble’s output is computed as the average of the MLPs (Eq. 2). This step
does not imply any signiﬁcant extra computational eﬀort over TENN.
Simultaneous Evolution of Neural Network Topologies and
In the previous TENN approach, an a priori architecture needs to be set before
training. On the other hand, evolving neural structures without weight information will make harder the ﬁtness evaluation due to the noisy ﬁtness evaluation
problem. A possible solution to this problem is to to simultaneous evolve topologies and weights .
Therefore, a Simultaneous Evolutionary Neural Network (SENN) algorithm is
proposed. The direct representation used in TENNE is kept, and the evolutionary
algorithm uses two diﬀerent mutation operators (Fig. 2), each with an equal
probability of application (50%): the structural mutation presented above and a
macro mutation, which operates over the connection weight values by replacing
a random number (from 1 to M) of these with new randomly generated values
within the range [−1.0, 1.0].
a) Original MLP
b) After macro
d) After adding
connections
c) After node
Fig. 2. Example of the application of the mutation operators.
This algorithm will also be combined with a local optimization procedure,
under a Lamarckian evolution setting . In each generation, L epochs of the
RPROP learning algorithm are applied to every individual (MLP) in the population, using the examples in the training set. In past work , this Lamarckian
approach (with macro mutation) to training outperformed eight evolutionary
algorithms (using diﬀerent crossovers and mutations) and gradient-based algorithms (e.g. Backpropagation and RPROP).
The selection procedure, substitution rate and termination criteria are kept
from the TENN approach presented above. On the other hand, the initialization of the population also follows a similar process, completed by a step that
randomly initializes the weights within the range [−1.0; 1.0]. Thus, the SENN
procedure is given by following pseudo-code:
Split the data into the TS, FS and MS sets
Ψi ←Initialize the population and weights of the P MLPs
WHILE (i < G)
Train the MLPs from Ψi on TS with L RPROP epochs
Evaluate the current population Ψi
Ai ←Select P/2 ancestors from Ψi for reproduction
Oi ←Apply the macro (50%) or structural (50%) mutations on
all elements of Ai
Si ←Select P/2 −1 survivors from Ψi
Set the next generation (Ψi+1 ←best(Ψi) S Si
Regarding the ﬁtness function, a diﬀerent strategy was used here, since the
simultaneous evolution of weights and topologies is very sensitive to overﬁtting.
Thus, the validation set is subdivided into: a ﬁtness set (FS), used for the ﬁtness
evaluation (the RMSE is also used), and a model selection set (MS), used to
select the best individual at the end of the process. This evaluation function is
performed using the pseudo-code:
WHILE (j ≤PΨ)
MLPj ←Select the j-th MLP from the population Ψ
Set the MLPj fitness as the RMSE on FS
As before, a SENN Ensemble (SENNE) will be built using the best N individuals (in this case the ones with lower errors in the second validation set)
obtained during the evolutionary process.
Results and Discussion
In this work, sixteen real-world datasets were selected from the UCI repository, which is commonly used to benchmark learning algorithms . The main
features are listed in Table 1, namely: the number of numeric, binary and nominal (i.e. discrete with three or more distinct labels) input attributes, as well
as the number of examples and classes. The regression tasks are identiﬁed by
the symbol ℜ(last eight rows).
In the last decades, and due to increase attention given to the Data Mining
ﬁeld, several models and algorithms have been proposed, with each one presenting its own purposes and capabilities . In this work, ﬁve of the most popular
Data Mining models/algorithms will be selected, as basis of comparison with the
neural models, namely:
– a classiﬁcation Decision Tree based on the C4.5 algorithm (J48);
– a regression Decision Tree (M5 algorithm);
– a k-Nearest Neighbor (IB5);
– an Instance Based Algorithm (KStar); and
– a Support Vector Machine (SVM).
The ANN/EC experiments were conducted using a software package developed in JAVA by the authors, while the other Data Mining techniques were
computed using the WEKA environment with its default parameters . For
each model, 30 runs of a 5-fold cross-validation process (stratiﬁed in the classi-
ﬁcation tasks) were executed. This means that in each of these 150 experiments,
80% of the data is used for learning and 20% for testing.
For all setups, the learning data was divided into training (50% of the original
dataset) and validation sets (30%). In case of the SENN/SENNE algorithms,
Table 1. A summary of the datasets used.
Description
Ex. Classes
Num. Bin. Nom. (K)
balance balance scale weight and distance
BUPA liver disorders
car evaluation database
contraceptive method choice
dermat dermatology database
radar returns from the ionosphere
sonar classiﬁcation (rocks vs mines)
protein localization sites
abalone age of abalone
auto imports database
breast cancer time to recur
Cleveland heart disease database
housing housing prices in suburbs of Boston
Auto-Mpg (miles per gallon)
rise time of a servomechanism
Wisconsin prognostic breast cancer
the validation set was subdivided into ﬁtness (15% of the original dataset) and
model selection (15%) sets. Finally, the SENN/TENN parameters were set to
N = 20, P = 20, p = 50%, H = 10, L = 50, M = 5 and G = 20.
Tables 2 and 3 show the average of the errors measured over the 30 runs, for
each method and instance. The 95% t-student conﬁdence intervals are also
shown for the evolutionary approaches. The last row of each table averages the
global performance of each learning strategy, measured over all instances.
Some evidences are clearly shown from the results. In the classiﬁcation tasks,
it is obvious that the ANN-based learning models (the last six columns) are quite
competitive, outperforming the other Data Mining algorithms. In fact, the two
exceptions to this behavior are the dermatology and sonar instances, where
the SVM and KStar get the best results.
Regarding the single ANN approaches, both evolutionary algorithms (TENN
and SENN) excel the heuristic method (HNN), with a diﬀerence of 0.8% and 1%
in the average performance. When analysing each particular dataset, the TENN
(SENN) shows a signiﬁcant improvement in 3 (4) of the 8 tasks (p−value <
0.05). On the other hand, the ANN Ensemble-based versions (HNNE, TENNE
and SENNE) obtain better results when compared with the correspondent single
ANN ones, with improvements ranging from 1.2% to 1.5%. At the dataset level,
the improvements of TENNE/SENNE are signiﬁcant in 6/5 cases. Overall, the
evolutionary ensembles (TENNE and SENNE) clearly show the best accuracy
over all alternatives considered in this study.
For the regression tasks, the decision tree (M5P) is quite competitive, outperforming all the other Data Mining algorithms and ANN approaches except
Table 2. The classiﬁcation results (PCCE values, in %).
J48 IB5 KStar SVM HNN TENN
SENN HNNE TENNE SENNE
balance 78.3 87.6
95.2±0.4 96.4±0.5†
96.0±0.4 96.7±0.4‡
67.9±1.0 67.8±1.0
70.9±0.8‡ 70.4±0.8‡
97.3 98.3±0.2† 98.7±0.2†
98.9±0.1‡ 99.0±0.1
50.6 54.7±0.5† 53.6±0.5†
55.5±0.4 54.8±0.4‡
dermat 96.0 96.7
95.7±0.4 95.3±0.5
96.8±0.3‡ 96.5±0.4‡
89.8±0.7 90.5±0.7
92.8±0.5‡ 92.2±0.6‡
79.0±1.1 79.5±1.0
58.5 59.3±0.4† 59.7±0.5†
60.3±0.3‡ 60.3±0.4
† - Statistically signiﬁcant (p-value< 0.05) under pairwise comparison with HNN.
‡ - Statistically signiﬁcant under pairwise comparison with the non ensemble version.
TENNE and SENNE. However, as before, the evolutionary models excel the
heuristic one, with average improvements from 1.6% to 2.6%. The diﬀerences
are statistically signiﬁcant in 5 datasets for TENN and in 3 problems for SENN.
The ensembles also behave better than their single based ANN’s counterparts
(from 0.8% to 3.4% in average), presenting the best overall results. There is
a statistical diﬀerence for TENNE over TENN in 5 cases, while this number
increases to 7 when comparing SENNE over SENN.
Table 3. The regression results (NRMSE values, in %).
M5P IB5 KStar SVM HNN TENN
SENN HNNE TENNE SENNE
abalone 21.5 23.0
21.1±0.1 21.1±0.1
14.6±0.5 13.9±0.6
13.4±0.4‡ 12.6±0.5‡
43.0 42.5±0.5† 43.9±0.8
21.6±0.3 22.4±0.4
housing 17.5 22.2
19.0 17.6±0.4† 17.4±0.5†
16.0±0.3‡ 16.0±0.4‡
13.9 12.5±0.2† 12.8±0.3†
11.7±0.1‡ 11.9±0.2‡
60.6 45.5±2.8† 48.0±2.3†
40.0±2.6‡ 42.1±2.8‡
76.5 73.2±0.9† 77.0±1.5
70.9±0.6‡ 72.2±1.0‡
† - Statistically signiﬁcant under pairwise comparison with HNN.
‡ - Statistically signiﬁcant under pairwise comparison with the non ensemble version.
When comparing the average results of the best two methods, the TENNE
algorithm slightly outperforms SENNE both in classiﬁcation and regression.
Yet, in the vast majority of the datasets, the diﬀerences are not statistically
signiﬁcant. Since a similar level of accuracy was obtained, another dimension
will be used in the analysis of both alternatives: the computational eﬀort. In this
parameter, the evaluation clearly favors the SENN/SENNE algorithms, whose
computational burden is similar to the one required by the HNN/HNNE.
It should be noted that the computational load of each evolutionary approach
can be approximated by the number of epochs in the ANN training. All other
computational processes are negligible when compared with this measure. The
overhead of the TENN/TENNE is due to the ﬁtness evaluation process, where a
complete training of the ANN (500 epochs of the RPROP algorithm) is required.
In each generation, this procedure is applied to half of the population (the oﬀspring). In contrast, in the SENN/SENNE approaches, the evaluation process is
straightforward, involving only the calculation of the error metric. In this case,
the RPROP is only used in the lamarkcian operator, which is applied to all
individuals and implies 50 epochs. As a result, the TENN/TENNE algorithms
requires a computational time that is 500% higher than SENN/SENNE.
For demonstrative purposes, Fig. 3 plots the evolution of the best and average
ﬁtnesses of the population, for a given SENN simulation with the bupa task. The
ﬁgure shows a steady evolution, suggesting also diversity within the population,
since the two curves do not overlap. This example was executed in a Pentium
IV 3.4 GHz processor, demanding a computational time of 65 seconds. The
best evolved MLP contains a total of 9 hidden nodes, 7 bias and 4 shortcut
connections, denoting a strong nonlinearity for this task.
Similar work to the SENN approach has been reported in literature, namely
the EPNet system , which obtained interesting results. However, this approach was only applied to ﬁve UCI datasets where the best results are obtained
by low complexity MLPs (in some cases linear models). It is not surprising that,
since EPNet heavily promotes simple models, good results were obtained for
these cases. In this work, the majority of the problems demanded MLPs with
a much higher number of hidden nodes, where is it believed that the EPNet
system would not excel.
Conclusions
In this work, two evolutionary algorithms were proposed for ANN optimization:
the evolution of topologies and the simultaneous evolution of weights and topologies. The performances of these methods were favorably compared with other
Data Mining algorithms (e.g. Decision Trees or Support Vector Machines) and
with heuristic approaches to ANN model selection. The two evolutionary models were enhanced by considering ANN Ensembles, which combine the outputs
of the best ANNs obtained during evolution, in order to create more accurate
models. When taking both accuracy and computational eﬀort parameters into
consideration, the best available option is to consider the simultaneous evolution
strategy and to use the resulting ANN Ensemble as a prediction model.
The major contributions of this work were the following: i) in contrast with
previous studies, an exhaustive comparison is performed over the two main evo-
Fitness (RMSE)
Generation
Fig. 3. Example of the ﬁtness evolution for the bupa task.
lutionary approaches for ANN design and heuristic approaches to model selection, encompassing a total of 16 classiﬁcation and regression datasets, whose
underlying models denote diﬀerent degrees of nonlinearity; ii) the proposal and
evaluation of several methods to build ANN ensembles (HNNE, TENNE and
SENNE); iii) the splitting of the validation set into two components in simultaneous evolution, a feature that was necessary to boost its performance.
Although the considered genetic operators (structural and random mutations) are quite simple, they present a huge ﬂexibility. Indeed, the proposed
approach can be easily extended to other ANN architectures (e.g., Recurrent
Neural Networks ) or learning scenarios (e.g. Reinforcement Learning ).
In the former case, the RPROP local learning could be substituted by an appropriate learning algorithm (e.g. Backpropagation Through Time ), while in
the latter, the rewards from the environment can be used to implement the local
search heuristics.
In future work, the modeling of biological processes (fed-batch fermentations)
will be taken as a case study of this technology . In this task, a number
of variables evolves over time (e.g. glucose, biomass, weight of the fermenter,
dissolved oxygen, etc), and an accurate model to predict this behavior over time
is a major requirement to perform adequate optimization and online control of
the process.
One other direction in future work will be the development of more elaborated strategies to build ANN Ensembles, namely by designing ﬁtness functions
which reward specialization , with the aim that individual models can focus
in certain areas of the search space. Furthermore, the incorporation of other parameters of the ANN structure, such as the transfer function and the learning
algorithm (and speciﬁc parameters) to use in Lamarckian evolution, will also be
considered .