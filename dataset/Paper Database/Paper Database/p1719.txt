Elect. Comm. in Probab. 12 , 234–247
ELECTRONIC
COMMUNICATIONS
in PROBABILITY
ASYMPTOTIC DISTRIBUTION OF COORDINATES ON
HIGH DIMENSIONAL SPHERES
M. C. SPRUILL
School of Mathematics, Georgia Institute of Technology, Atlanta, GA 30332-016
email: 
Submitted January 5, 2007, accepted in ﬁnal form July 19, 2007
AMS 2000 Subject classiﬁcation: 60F17, 52A40, 28A75
Keywords: empiric distribution, dependent arrays, micro-canonical ensemble,
Minkowski area, isoperimetry.
The coordinates xi of a point x = (x1, x2, . . . , xn) chosen at random according to a uniform
distribution on the ℓ2(n)-sphere of radius n1/2 have approximately a normal distribution when
n is large. The coordinates xi of points uniformly distributed on the ℓ1(n)-sphere of radius n
have approximately a double exponential distribution. In these and all the ℓp(n), 1 ≤p ≤∞,
convergence of the distribution of coordinates as the dimension n increases is at the rate √n
and is described precisely in terms of weak convergence of a normalized empirical process to a
limiting Gaussian process, the sum of a Brownian bridge and a simple normal process.
Introduction
If Yn = (Y1n, . . . , Ynn) is chosen according to a uniform distribution on the sphere in n dimensions of radius √n then, computing the ratio of the surface area of a polar cap to the whole
sphere, one ﬁnds that the marginal probability density of Yjn/√n is
fn(s) = κn(1 −s2)(n−3)/2I(−1,1)(s),
where κn =
). Stirling’s approximation shows
n→∞κn(1 −v2
n )(n−3)/2I(−√n,∞)(v) =
2π e−v2/2,
so appealing to Scheﬀe’s theorem (see ) one has
n→∞P[Yjn ≤t] = lim
n )(n−3)/2 dv
and Yjn is asymptotically standard normal as the dimension increases. This is an elementary
aspect of a more comprehensive result attributed to Poincare; that the joint distribution of the
DOI: 10.1214/ECP.v12-1294
Coordinates
ﬁrst k coordinates of a vector uniformly distributed on the sphere S2,n(√n) is asymptotically
that of k independent normals as the dimension increases. Extensions have been made by
Diaconis and Freedman , Rachev and Ruschendorﬀ , and Stam in to convergence in
variation norm allowing also k to grow with n. In the authors study k = o(n) and relate
some history of the problem.
Attribution of the result to Poincare was not supported by
their investigations; the ﬁrst reference to the theorem on convergence of the ﬁrst k coordinates
they found was in the work of Borel . Borel’s interest, like ours, centers on the empiric
distribution (edf)
Fn(t) = #{Yin ≤t : i = 1, . . . , n}
The proportion of coordinates Yjn less than or equal to t ∈(−∞, ∞) is Fn(t). As pointed out
in , answers to Borel’s questions about Maxwell’s theorem are easy using modern methods.
If Z1, Z2, . . . are iid N(0, 1) and Rn = 1
i then it is well known that R−1/2
(Z1, . . . , Zn)
is uniform on S2,n(n1/2), so if the edf of Z1, Z2, . . . , Zn is Gn then since nGn(t) is binomial,
the weak law of large numbers shows that Gn(t)
p→Φ(t). By continuity of square-root and Φ
p→1 it follows, as indicated,
Fn(t) −Φ(t)
n t) −Φ(t)
n t) −Φ(R1/2
n t) + Φ(R1/2
n t) −Φ(t)
that the right-most term of the right hand side converges to 0 in probability. Finally, by the
Glivenko-Cantelli lemma (see equation (13.3) of ) it follows that the left-most term on the
right hand side tends to zero in probability. The argument yields asymptotic normality and,
assuming continuity, an aﬃrmative answer to the classical statistical mechanical question of
equivalence of ensembles: does one have equality of the expectations EG[k(Y )] =
and EU[k(Y )] =
k(y)dU(y) where, corresponding to the micro-canonical ensemble, U is the
uniform distribution on {y : H(y) = c2}, and G is the Gibbs’ distribution satisfying dG(y) =
e−aH(y)dy with a such that EG[H(Y )] =
H(y)dG(y) = c2, and H(y) the Hamiltonian? For
H(x) = cx2, if the functional gk(F) =
k(y)dF(y) is continuous, then the two are equivalent
modulo the choice of constants.
More generally, what can be said about the error in approximating the functional g(F)’s value
by g(Fn)? In the case of independence there are ready answers to questions about the rate
of convergence and the form of the error; for the edf Qn determined from n independent and
identically distributed univariate observations from Q, it is well known that the empiric process
Dn(t) = √n(Qn(t)−Q(t)), t ∈(−∞, ∞), converges weakly (Dn ⇒B◦Q) to a Gaussian process
as the sample size n increases. Here B is a Brownian bridge and it is seen that the rate of
convergence is √n with a Gaussian error. If the functional g is diﬀerentiable (see Serﬂing ),
then √n(g(Qn)−g(Q)) ⇒Dg(L), where Dg is the diﬀerential of g and L = B◦Q is the limiting
error process. The key question in the case of coordinates constrained to the sphere is: does
the process √n(Fn(t) −Φ(t)) converge weakly to a Gaussian process? The answer will be
shown here to be yes as will the answers to the analogous questions in each of the spaces
ℓp(n) if Φ is replaced in each case by an appropriate distribution. Even though the random
variables are dependent, convergence to a Gaussian process will occur at the rate √n. The
limiting stochastic process L(t) = B(Fp(t)) + tfp(t)
√p Z diﬀers from the limit in the iid case.
To state our result, for 1 ≤p < ∞, let 1
q = 1 and introduce the family of distributions Fp
Electronic Communications in Probability
on (−∞, ∞) whose probability densities with respect to Lebesgue measure are
fp(t) = p1/qe−|t|p/p
The space ℓp(n) is Rn with the norm ∥x∥p = (Pn
j=1 |xj|p)1/p where x = (x1, . . . , xn). The
sphere of “radius” r is Sp,n(r) = {x ∈Rn : ∥x∥p = r}. The ball of radius r is Bp,n(r) =
{x ∈Rn : ∥x∥p ≤r}. The convergence indicated by Dn ⇒D is so-called weak convergence
of probability measures deﬁned by limn→∞E[h(Dn)] = E[h(D)] for all bounded continuous h
and studied in, for example, . The following will be proven, where uniformly distributed in
the statement refers to σp,n deﬁned in section 3.
Theorem 1. Let p ∈[1, ∞) and Yn = (Y1n, . . . , Ynn) be uniformly distributed according to
σp,n on the sphere Sp,n(n1/p). There is a probability space on which are deﬁned a Brownian
bridge B and a standard normal random variable Z so that if Fn is as deﬁned in (1) then
√n(Fn(t) −Fp(t)) ⇒B(Fp(t)) + tfp(t)
as n →∞, where the indicated sum on the right hand side is a Gaussian process and
cov(B(Fp(t)), Z) = −tfp(t).
Idea of the proof of the theorem
Let Xn = (X1, . . . , Xn) where {X1, X2, . . . } are iid Fp random variables. Then the uniform
random vector Yn on the n-sphere of radius n1/p has the same distribution as n1/pXn
∥Xn∥p . Let
and Gn be the usual empirical distribution formed from the n iid random variables {Xi}n
Then the process of interest concerning (1) can be expressed probabilistically as
√n(Fn(t) −Fp(t))
d= √n((Gn(tψp(Xn)) −Fp(tψp(Xn))) + (Fp(tψp(Xn)) −Fp(t))).
It is well known that the process √n(Gn(t) −Fp(t)) converges weakly to B(Fp(t)), where B
is a Brownian bridge process. Noting that ψp(Xn)
p→1 as n →∞and that a simple Taylor’s
expansion of the second term yields that √n(Fp(tψp(Xn)) −Fp(t)) converges weakly to the
simple process tfp(t)
√p V , where V is a standard normal random variable, it can be seen that the
process in question, the empirical process based on an observation uniform on the n1/p-sphere
in ℓp(n), the emspherical process deﬁned by the left hand side of (5), converges weakly to a
zero mean Gaussian process
B(Fp(t)) + V tfp(t)
as the dimension n increases. The covariance of the two Gaussian summands will be shown to
cov(B(Fp(t)), sfp(s)
√p V ) = sfp(s)
√p (−tfp(t)).
Details of the uniform distribution σp,n of Theorem 1 on the spheres in ℓp(n) are given next.
Coordinates
Uniform distribution and Fp
The measure σp,n of Theorem 1 assigns to measurable subsets of Sp,n(1) their Minkowski
surface area, an intrinsic area in that it depends on geodesic distances on the surface. See .
The measure σp,n coincides on Sp,n(1), with measures which have appeared in the literature (see
 , , and ) in conjunction with the densities fp. In particular, it is shown that it coincides
with the measure µp,n deﬁned below (see (11)) which arose for Rachev and Ruschendorf 
in the disintegration of Vn.
The isoperimetric problem and solution
Let K ⊂Rn be a centrally symmetric closed bounded convex set with 0 as an internal point.
Then ρK(x) = inf{t : x ∈tK, t > 0} deﬁnes a Minkowski norm ∥x∥K = ρK(x) on Rn. The
only reasonable (Busemann ) n-dimensional volume measure in this Minkowski space is
translation invariant and must coincide with the (Lebesgue) volume measure Vn. One choice
for surface area is the Minkowski surface area σK, deﬁned for smooth convex bodies D by
σK(∂D) = lim
Vn(D + ǫK) −Vn(D)
For a more general class of sets M (see, for example, equation (18) of for details) the
Minkowski surface area can be shown to satisfy
∥u∥K0dσ2(u),
where σ2 is Euclidean surface area, u is the (Euclidean) unit normal to the surface ∂M, and
∥· ∥K0 is the norm in the dual space, also a Minkowski normed space in which the unit ball is
the polar reciprocal K0 = {x∗∈Rn :< x∗, x >≤1∀x ∈K} of K. Here < x, y >= Pn
It follows from the work of Busemann that among all solids M for which the left hand side
of (7) is ﬁxed, the solid maximizing the volume Vn is the polar reciprocal C0 of the set C of
∥u∥K0 . The latter is the unit sphere SK0(1) of the dual space (see also ). It follows
from (∂K0)0 = K that C0 = BK(1) = K, the unit ball. This solution also agrees in the case of
smooth convex sets with that from Minkowski’s ﬁrst inequality (see (15) of ); the solution
is the unit ball BK(1).
In the case of interest here ℓp(n), 1 ≤p < ∞; take K = Bp,n(1) and denote σK by σp. For the
sphere Sp,n(r) the Minkowski surface area satisﬁes
σp(Sp,n(r)) = lim
Vn(Bp,n(r) + ǫBp,n(1)) −Vn(Bp,n(r))
By homogeneity Vn(Bp,n(r)) = rnVn(Bp,n(1)) so one has σp(Sp,n(r)) = Vn(Bp,n(1)) drn
a formula due to Dirichlet (see ) the volume of Bp,n(1) is Vn(Bp,n(1)) =
p ) so the
Minkowski surface area of the radius r sphere in ℓp(n) is
σp(Sp,n(r)) = rn−1 2nΓn( 1
The simple formula (8) for σp(Sp,n(r)) should be contrasted with the Euclidean surface area
σ2(Sp,n(r)) for which there is no simple closed form. See .
Electronic Communications in Probability
Disintegration of Vn and Minkowski surface area
If f is smooth and D = {x : f(x) ≤c} is a compact convex centrally symmetric set with 0 as an
internal point and if g is a measurable function on ∂D then by (7) and ∥· ∥K0 = ∥· ∥q, one has
∂D g(x)dσp,n(x) =
∂D g(x)σ(x)dσ2(x). So dσp,n/dσ2 = ∥∇f(x)∥q/∥∇f(x)∥2. In particular,
for the surface ∂Bp,n(r) = Sp,n(r) = {x ∈Rn : f(x) = rp}, where f(x) = Pn
j=1 |xj|p, one has
a.e. (σ2), ∂f(x)
= p sgn(xj)|xj|p−1 = p sgn(xj)|xj|p/q, so for a.e. x ∈Sp,n(r)
j=1 |xj|qp/q)1/q
j=1 |xj|2p/q)1/2 =
j=1 |xj|2p/q .
For r > 0 ﬁxed, deﬁne the mapping Tr by Tr(v1, . . . , vn−1) = (v1, . . . , vn−1, (rp−Pn−1
This maps the region vi > 0, Pn−1
j < rp into the sphere Sp,n(r). It follows that
dσ2(v1, . . . , (rp −
j )1/p) = | ∂
Tr ∧· · · ∧
Tr|dv1 . . . dvn−1.
∂vj Tr = ej + cjen, where cj = −
i )1−1/p = −
(e1 + c1en) ∧(e2 + c2en)∧
∧(en−1 + cn−1en) =
e1,2,...,n−1
c1en,2,3,...,n−1 + c2e1,n,3,...,n−1 + · · · + cn−1e1,2,...,n−2,n,
it is seen that
j=1 |vj|2p/q
i=1 |vi|p)1/q .
From (10) and (9) it follows that the measure σp,n coincides with Rachev and Ruschendorf
’s measure µp,n deﬁned (see their equation (3.1)) on the portion of Sp,n(1) with all vi > 0
and analogously elsewhere by
IA(v1, . . . , vn−1, (1 −
j )(p−1)/p dv1 . . . dvn−1,
where U = {(v1, . . . , vn−1) : vi ≥0, Pn−1
j < 1}, and A is any measurable subset of Sp,n(1).
Minkowski uniformity under Fp
The probability P is uniform with respect to µ if P is absolutely continuous with respect to
µ and the R-N derivative f = dP
dµ is constant. The probability measure P is uniform on the
sphere Sp,n(1) if f is constant and the measure µ is surface area. If X1, . . . , Xn are iid Fp and
j=1 |Xj|p)1/p (X1, . . . , Xn)
Coordinates
then n1/pR is distributed uniformly with respect to Minkowski surface area on the sphere
Sp,n(n1/p). This follows from the literature and our calculations above but for a self contained proof consider for g : Rn
+ →R measurable the integral I =
g(v)dVn(v). Let T (v) =
i )1/p , . . . ,
i )1/p , (Pn
i )1/p). Here the domain of T is the region Pn
The range of T is {(u1, . . . , un−1, r) : ui ≥0, Pn−1
i ≤1, r ≥0}. Then T is invertible with
inverse T −1(u1, . . . , un−1, r) = (ru1, . . . , run−1, r(1 −Pn−1
i )1/p). Therefore
g(v1, . . . , vn)dv1 . . . dvn
g(ru1, . . . , run−1, r(1 −
i )1/p)|J(u1, . . . , un−1, r)|du1 . . . , dun−1dr
g(ru1, . . . , run, r(1 −
j)1/p)rn−1dµp,n(u)dr
(−1)2nrn−1
i )(p−1)/p . In particular, if f is the joint density of X1, . . . , Xn with respect to
Vn and M is a measurable subset of Sp,n(1), then letting A = R−1(M), one has the probability
P[(X1, . . . , Xn) ∈A]
f(ru1, . . . , run)rn−1dµp,n(u)dr
(2Γ(1/p))n
rn−1e−rp/pdrdσp,n(u)
p) σp,n(M).
Therefore, if X1, . . . , Xn are iid Fp and R is given in (12), then the density of R is uniform
with respect to σp,n.
Proof of the theorem for ℓp(n), 1 ≤p < ∞
The techniques of Billingsley on weak convergence of probability measures and uniform
integrability will be employed to prove Theorem 1.
Let (Ω, A, P) denote a probability space on which is deﬁned the sequence Uj ∼U(0, 1), j =
1, 2, . . . of independent random variables, identically distributed uniformly on the unit interval.
Fixing p ∈[1, ∞), one has that the iid Fp-distributed sequence of random variables X1, X2, . . .
can be expressed as Xj = F −1
(Uj). The usual empirical distribution based on the iid Xj is
n#{Xj ≤t} = 1
n#{Uj ≤Fp(t)} = Un(Fp(t)),
where Un is the empirical distribution, edf, of the iid uniforms. Suppressing the dependence
on ω ∈Ωfor both, deﬁne for each n = 1, 2, . . . the empirical process ∆n(u) = √n(Un(u) −u)
for u ∈ and (see also (4))
Vn = √n( 1
(Uj)|p −1).
Electronic Communications in Probability
The metric d0 of ( see Theorem 14.2) on D is employed. It is equivalent to the Skorohod
metric generating the same sigma ﬁeld D and D is a complete separable metric space under
The processes of basic interest are √n(Fn(t) −Fp(t)), t ∈(−∞, ∞). As commonly utilized in
the literature, the alternative parametrization relative to u ∈ is sometimes adopted below
in terms of which the basic process is expressed as
√n(Fn(F −1
In terms of this parametrization the processes concerning us are En(u) = √n(Gn(F −1
(u)ψp(Xn))−
(u))); these generate the same measures on (D , D) as the processes (13). Weak
convergence of the processes En will be proven.
Introduce for c > 0 the mappings φ(c, ·) deﬁned by φ(c, u) = Fp(cF −1
(u)), 0 < u < 1, φ(c, 1) =
1, and φ(c, 0) = 0. Then if
n (u) = ∆n(φ(( Vn
√n + 1)1/p, u)),
n (u) = √n
√n + 1)1/p, u) −φ(1, u)
one observes that
En(u) = E(1)
n (u) + E(2)
The following concerning product spaces will be used repeatedly. Take the metric d on the
product space M1 × M2, as
d((x1, y1), (x2, y2)) = max{d1(x1, x2), d2(y1, y2)},
where di is the metric on Mi.
Proposition 1. If (Xn(ω), Yn(ω)) are (Ω, A, P) to (M1 × M2, M1 × M2) measurable random
elements in a product M1 ×M2 of two complete separable metric spaces then weak convergence
of Xn ⇒X and Yn ⇒Y entails relative sequential compactness of the measures νn(·) =
P[(Xn, Yn) ∈·] on (M1 × M2, M1 × M2) with respect to weak convergence.
Proof: By assumption and Prohorov’s theorem (see Theorem 6.2 of ) it follows that the
sequences of marginal measures νX
n are both tight. Let ǫ > 0 be arbitrary, KX ∈M1 be
compact and satisfy P[ω ∈Ω: Xn(ω) ∈KX] ≥1 −ǫ/2 for all n and KY ∈M2 compact be
such that P[ω ∈Ω: Yn(ω) ∈KY ] ≥1 −ǫ/2 for all n. Then KX × KY ∈M1 × M2 is compact
(since it is is clearly complete and totally bounded under the metric (16) when - as they do
here - those properties of the sets KX and KY hold) and since
P[(Xn ∈KX) ∩(Yn ∈KY )] = 1 −P[(Xn ∈KX)c ∪(Yn ∈KY )c]
and P[(Xn ∈KX)c ∪(Yn ∈KY )c] ≤2 · ǫ/2, one has for all n
νn(KX × KY ) = P[(Xn, Yn) ∈KX × KY ] ≥1 −ǫ.
Thus the sequence of measures νn is tight and by Prohorov’s theorem (see Theorem 6.1 of )
it follows that there is a probability measure ¯ν on (M1 × M2, M1 × M2) and a subsequence
n′ so that νn′ ⇒¯ν.
It is shown next (see (5)) that √n(Gn(tψp(Xn)) −Fp(tψp(Xn))) ⇒B(Fp(t)).
Coordinates
Lemma 1. Let 1 ≤p < ∞. Then (see (14))
where B is a Brownian bridge process on .
Proof: The random time change argument of Billingsley , page 145 is used. There, the
set D0 ⊂D of non-decreasing functions φ : → is employed and here it is ﬁrst
argued that the functions φ(c, ·), for c > 0 ﬁxed are in D0. For u0 ∈(0, 1) one calculates the
derivative
duφ(c, u)|u=u0 = φu(c, u0) = cfp(cF −1
from which continuity of φ(c, ·) on (0, 1) follows. Consider un →1. Let 1 > ǫ > 0 be arbitrary
and a ∈(−∞, ∞) be such that Fp(t) > 1 −ǫ for t > a/2. Let N < ∞be such that n > N
entails F −1
(un) > a/c Then for n > N one has φ(c, un) ≥Fp(a) > 1 −ǫ = φ(c, 1) −ǫ. Since
φ(c, ·) is plainly increasing on (0, 1), for n > N one has|φ(c, 1) −φ(c, un)| < ǫ. Thus φ(c, ·)
is continuous at 1 and a similar argument shows it to be continuous at 0. It is therefore a
member of D0.
Next, consider the distance d0(φ(c, ·), φ(1, ·)). Details of its deﬁnition are in in material surrounding equation (14.17), but the only feature utilized here is that for x, y ∈C , d0(x, y) ≤
∥x −y∥∞. Denoting
∂cφ(c, u)|c=a by φc(a, u) one has for some ξ = ξu between c and 1
φ(c, u) −φ(1, u) = φc(ξ, u)(c −1) = fp(ξF −1
and since uniformly on compact sets c ∈[a, b] ⊂(0, ∞) one has sup−∞<x<∞|xfp(cx)| < B for
some B < ∞it follows that for |c −1| < δ < 1 one has
∥φ(c, ·) −φ(1, ·)∥∞≤Bδ.
Therefore, if Cn
p→1 then d0(φ(Cn, ·), φ(1, ·))
p→0. Since if X ∼Fp then |X|p ∼G(1/p, p),
the gamma distribution with mean 1 and variance p2/p = p, it follows from the ordinary CLT
d→N(0, 1). Thus the D-valued random element Φn = φ(( Vn
√n + 1)1/p, ·) satisﬁes
Φn ⇒φ(1, ·) = e(·), the identity.
As is well known, ∆n ⇒B, so if (∆n, Φn)
then as shown in (see material surrounding equation (17.7) there) and consulting (14),
= ∆n ◦Φn ⇒B ◦e = B.
Consider the measures νn on D × D whose marginals are (∆n, Φn) and let n′ be any subsequence. It follows from Proposition 1 that there is a probability measure ¯ν on D × D and a
further subsequence n′′ such that νn′′ ⇒¯ν. Here νn′′ has marginals (∆n′′, Φn′′) and so ¯ν must
be a measure whose marginals are (B, e); so (∆n′′, Φn′′)
→(B, e). It follows that E(1)
Since every subsequence has a further subsequence converging weakly to B, it must be that
Lemma 2 shows (see(5)) that
√n(Fp(tψp(Xn)) −Fp(t)) ⇒tfp(t)
Lemma 2. Let 1 ≤p < ∞. Then (see (15))
(·)fp(F −1
where Z ∼N(0, 1).
Electronic Communications in Probability
Proof: One has, for 1 ≤p < ∞
φ(c, u) −φ(1, u) = [φc(1, u) + ǫ(c, u)](c −1),
where for ﬁxed u ∈(0, 1), ǫ(c, u) →0 as c →1 and for δ suﬃciently small and uniformly on
|c −1| < δ, ∥ǫ(c, ·)∥∞< A for some A < ∞. With Cn = ( Vn
√n + 1)1/p it follows that
φc(1, u)√n[( Vn
√n + 1)1/p −1] + op(1)
Vn + op(1)
where Z ∼N(0, 1). □
Denote by µn the joint probability measure on D × D of (E(1)
n ). Applying Proposition 1
as in Lemma 1, there is a subsequence µn′ and a probability measure ¯µ on D × D whose
marginals, in light of Lemmas 1 and 2, must be (B, φc(1,·)
√p Z). It will be shown next that for
any such measure ¯µ, one has
cov(B(u), Z) = −F −1
p (u)fp(F −1
An arbitrary sequence {Vn}n≥1 of random variables is uniformly integrable (ui) if
|Vn(ω)|dP(ω) = 0.
The fact that if supn E[|Vn|1+ǫ] < ∞for some ǫ > 0 then {Vn} is ui will be employed as will
Theorem 5.4 of which states that if {Vn} is ui, and Vn ⇒V then limn→∞E[Vn] = E[V ]. It
is well known that in a Hilbert space (L2(Ω, A, P) here) a set is weakly sequentially compact
if and only if it is bounded and weakly closed (see Theorem 4.10.8 of ).
In the following it is more convenient to deal with the original Xj. It is assumed, without loss
of generality and for ease of notation, that the subsequence is the original n so µn ⇒¯µ.
Lemma 3. For ¯µ
cov(B ◦Fp(t), Z) = −tfp(t).
Proof: Fix t ∈(−∞, ∞) and let Cn = √n(Gn(t) −Fp(t)) and Dn = √n(Wn −1), where
j=1 |Xj|p. The expectations E[|CnDn|2] will be computed and it will be shown
that the supremum over n is ﬁnite. In particular, it will be demonstrated that E[C2
n−2(K1n2 + K2n) so that CnDn is ui. Deﬁne Ai = |Xi|p −1 and Bi = I(−∞,t](Xi) −Fp(t).
Note that E[Ai] = E[Bi] = 0, i = 1, . . . , n that A’s for diﬀerent indexes are independent and
the same applies to B’s. Furthermore, E[A2
pp2 = p and E[B2
i ] = Fp(t)(1 −Fp(t)). One
has (CnDn)2 =
i=1 Ai)2(Pn
j=1 Bj)2 so that C2
n is the sum of four terms S1, S2, S3, S4
u̸=v BuBv,
u̸=v AuAv,
u̸=v BuBv.
Coordinates
Consider ﬁrst S2. A typical term in the expansion will be A2
i BuBv, where u ̸= v. Only the ones
for which i equals u or v have expectations possibly diﬀering from 0, but if i = u then since
Bv is independent and 0 mean it too has expectation 0. Thus E[S2] = 0. The same argument
applies to E[S3]. In S4 we’ll have, using similar arguments, E[S4] = P
i̸=j E[AiBi]E[AjBj] =
(n2 −n)E[A1B1]E[A2B2]. In the case of S1 one has
1] + (n2 −n)E[A2
n E[|CnDn|2] = sup
n n−2(K1n2 + K2n) < ∞,
K1 = E[A1B1]E[A2B2] + E[A2
1] −E[A1B1]E[A2B2] −E[A2
It follows that CnDn is ui and limn→∞E[CnDn] = E[B ◦Fp(t)Z1] where Z1 ∼N(0, p). Noting
that for some K < ∞
|Fp(tw1/p) −Fp(t) −p−1tfp(t)(w −1)
) −Fp(t) −p−1tfp(t)(Wn −1)
] ≤nE[(Wn −1)4] = 3p2
and it is seen that ∥√n(Fp(tW 1/p
) −Fp(t) −p−1tfp(t)√n(Wn −1)∥2 →0. It follows now from
∥Cn∥2 = Fp(t)(1 −Fp(t)) and weak sequential compactness by passing to subsequences, that
√n(Fp(tW 1/p
) −Fp(t))] = E[B ◦Fp(t)Z].
On the other hand, by a direct computation,
E[√n(Gn(t) −Fp(t))(√n(Wn −1)]
nE[Gn(t)(Wn −1)]
E[I(−∞,t](Xi)(|Xj|p −1)]
E[I(−∞,t](Xi)(|Xi|p −1)]
E[I(−∞,t](X1)(|X1|p −1)]
|x|p p1/qe−|x|p/p
dx −Fp(t),
so that letting u = x and dv = xp−1e−xp/pdx one has
0 xpe−xp/pdx = −xe−xp/p|t
0 e−xp/pdx and hence
E[√n(Gn(t) −Fp(t))(√n(Wn −1)] = −tfp(t) + Fp(t) −Fp(t) = −tfp(t).
Electronic Communications in Probability
Emspheric p = 2
Figure 1: Comparison of covariance functions; empiric is Brownian bridge
Therefore,
E[B ◦Fp(t)Z] = −tfp(t).
A plot of a portion of the covariance function close to 0 appears in Figure 1 and a comparison
of variances on the same scale in Figure 2.
Figure 2: Comparison of variance functions for p = 2 : solid is Brownian bridge
Lemma 4. Let 1 ≤p < ∞be ﬁxed and En(u) = E(1)
n (u) + E(2)
n (u), 0 ≤u ≤1 (see equations
(14) and (15)). Then there is a Gaussian process E(u) = B(u) +
(u)fp(F −1
Z satisfying
(17) for which En ⇒E.
Coordinates
Proof: From what has been done so far it follows that for an arbitrary subsequence n′ of
n the measures µn′ on D × D which are the joint distributions of (E(1)
n ) have a further
subsequence n′′ and there is a probability measure ¯µ on D × D for which µn′′ ⇒¯µ. This
measure has marginals (B, φc(1,·)
√p Z) and the covariance of B(u) and Z is given by (17). Since
¯µ concentrates on C × C and θ(x, y) = x + y is continuous thereon, one has a probability
measure ¯η on D deﬁned for A ∈D by ¯η(A) = ¯µ(θ−1A) and the support of ¯η is contained in
C. It will now be argued that this measure ¯η is Gaussian. It is convenient to do this in terms
of the original Xj’s. Let X1, X2, . . . , be iid Fp, ﬁx −∞< t1 < t2 < tk < ∞, and consider the
random vectors W (n)(t) = (Wn(t1), . . . Wn(tk)), where
Wn(t) = √n 1
ψp(Xn)) −Fp(t)).
Since W (n′′)
d= (En′′(Fp(t1)), . . . , En′′(Fp(tk)))
L→(E(Fp(t1)), . . . , E(Fp(tk))) = W and since
E is continuous wp 1 and ψp(Xn)→1 one has also W (n′′)(t/ψp(Xn′′))
d→W. Noting that
Wn(tj/ψp(Xn))
√n(G(tj) −Fp(tj) −(Fp(tj/ψp(Xn) −Fp(tj)))
(I(−∞,tj](Xi) −tjfp(tj)
|Xi|p −Fp(tj) + tjfp(tj)
it is seen that W, being the limit in law of sums of iid well-behaved vectors, is a multivariate
normal. Furthermore, the limiting ﬁnite dimensional marginals do not depend on the subsequence. Therefore, the measure ¯η is unique and Gaussian and the claim has been proven.
Convergence also holds in the case p = ∞, where one can arrive at the correct statement and
conclusion purely formally by taking the limit as p →∞in the statement of Theorem 1; so F∞
is the uniform on [−1, 1], the random vector Yn = (Y1n, . . . , Ynn) ∈S∞,n(1), and for t ∈[−1, 1]
√n(Fn(t) −1 + t
I[−1,1](t)) ⇒B ◦F∞(t).
This follows from:
1. If ψ∞(Xn) = max{|X1|, . . . , |Xn|}, then ψ∞(Xn) ∈ and one has for 1 > v > 0, that
P[ψ∞(Xn) ≤v] = (
2dx)n = vn so ψ∞(Xn)
2. since for v > 0
P[n(ψ∞(Xn) −1) ≤−v] = (1 + −v
n )n →e−v,
the term in the limit process additional to the Brownian bridge part (the right-most term
in (5)) washes out and one has as limit simply the Brownian bridge B( 1+t
2 I[−1,1](t)).
Furthermore (see also ) the measure σ∞,n on S∞,n(1) coincides with ordinary Euclidean
Electronic Communications in Probability
Acknowledgment
Leonid Bunimovich introduced me to the question of coordinate distribution in ℓ2. Important modern references resulted from some of Christian Houdr´e’s suggested literature on the
isoperimetry problem in ℓp. Thanks also are hereby expressed to the referees and editors of
this journal for their careful attention to my paper and valuable comments.