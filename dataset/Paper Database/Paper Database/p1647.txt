HAL Id: inria-00548528
 
Submitted on 20 Dec 2010
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
A comparison of aﬀine region detectors
Krystian Mikolajczyk, Tinne Tuytelaars, Cordelia Schmid, Andrew Zisserman,
Jiri Matas, Frederik Schaffalitzky, Timor Kadir, Luc van Gool
To cite this version:
Krystian Mikolajczyk, Tinne Tuytelaars, Cordelia Schmid, Andrew Zisserman, Jiri Matas, et al.. A
comparison of aﬀine region detectors.
International Journal of Computer Vision, 2005, 65 (1/2),
pp.43–72. ￿10.1007/s11263-005-3848-x￿. ￿inria-00548528￿
A Comparison of Aﬃne Region Detectors
K. Mikolajczyk1, T. Tuytelaars2, C. Schmid4, A. Zisserman1,
J. Matas3, F. Schaﬀalitzky1, T. Kadir1, L. Van Gool2
1 University of Oxford, OX1 3PJ Oxford, United Kingdom
2 University of Leuven, Kasteelpark Arenberg 10, 3001 Leuven, Belgium
3 Czech Technical University, Karlovo Namesti 13, 121 35, Prague, Czech Republic
4 INRIA, GRAVIR-CNRS, 655, av. de l’Europe, 38330 Montbonnot, France
 , , ,
 , , ,
 , 
The paper gives a snapshot of the state of the art in aﬃne covariant region detectors, and
compares their performance on a set of test images under varying imaging conditions. Six
types of detectors are included: detectors based on aﬃne normalization around Harris and Hessian points , as proposed by Mikolajczyk and Schmid and by Schaﬀalitzky
and Zisserman; a detector of ‘maximally stable extremal regions’, proposed by Matas et
al. ; an edge-based region detector and a detector based on intensity extrema ,
proposed by Tuytelaars and Van Gool; and a detector of ‘salient regions’, proposed by Kadir,
Zisserman and Brady . The performance is measured against changes in viewpoint, scale,
illumination, defocus and image compression.
The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework.
Introduction
Detecting regions covariant with a class of transformations has now reached some maturity in the
computer vision literature. These regions have been used in quite varied applications including:
wide baseline matching for stereo pairs , reconstructing cameras for sets of disparate
views , image retrieval from large databases , model based recognition ,
object retrieval in video , visual data mining , texture recognition , shot location , robot localization and servoing , building panoramas , symmetry detection , and object categorization .
The requirement for these regions is that they should correspond to the same pre-image for
diﬀerent viewpoints, i.e., their shape is not ﬁxed but automatically adapts, based on the underlying image intensities, so that they are the projection of the same 3D surface patch. In
particular, consider images from two viewpoints and the geometric transformation between the
images induced by the viewpoint change. Regions detected after the viewpoint change should
be the same, modulo noise, as the transformed versions of the regions detected in the original
image – image transformation and region detection commute. As such, even though they have
often been called invariant regions in the literature (e.g., ), in principle they should
be termed covariant regions since they change covariantly with the transformation. The confusion probably arises from the fact that, even though the regions themselves are covariant, the
Figure 1: Class of transformations needed to cope with viewpoint changes. (a) First
viewpoint; (b,c) second viewpoint. Fixed size circular patches (a,b) clearly do not suﬃce to deal
with general viewpoint changes. What is needed is an anisotropic rescaling, i.e., an aﬃnity (c).
Bottom row shows close-up of the images with surface corresponding patches.
normalized image pattern they cover and the feature descriptors derived from them are typically
invariant.
Note, our use of the term ‘region’ simply refers to a set of pixels, i.e. any subset of the image.
This diﬀers from classical segmentation since the region boundaries do not have to correspond to
changes in image appearance such as colour or texture. All the detectors presented here produce
simply connected regions, but in general this need not be the case.
For viewpoint changes, the transformation of most interest is an aﬃnity. This is illustrated in
ﬁgure 1. Clearly, a region with ﬁxed shape (a circular example is shown in ﬁgure 1(a) and (b))
cannot cope with the geometric deformations caused by the change in viewpoint. We can observe
that the circle does not cover the same image content, i.e., the same physical surface. Instead, the
shape of the region has to be adaptive, or covariant with respect to aﬃnities (ﬁgure 1(c) – closeups shown in ﬁgure 1(d)–(f)). Indeed, an aﬃnity is suﬃcient to locally model image distortions
arising from viewpoint changes, provided that (1) the scene surface can be locally approximated
by a plane or in case of a rotating camera, and (2) perspective eﬀects are ignored, which are
typically small on a local scale anyway. Aside from the geometric deformations, also photometric
deformations need to be taken into account. These can be modeled by a linear transformation
of the intensities.
To further illustrate these issues, and how aﬃne covariant regions can be exploited to cope
with the geometric and photometric deformation between wide baseline images, consider the
example shown in ﬁgure 2. Unlike the example of ﬁgure 1 (where a circular region was chosen for
one viewpoint) the elliptical image regions here are detected independently in each viewpoint. As
is evident, the pre-image of these aﬃne covariant regions correspond to the same surface region.
Given such an aﬃne covariant region, it is then possible to normalize against the geometric and
photometric deformations (shown in ﬁgure 2(d)(e)) and to obtain a viewpoint and illumination
Aﬃne covariant regions oﬀer a solution to viewpoint and illumination
First row: one viewpoint; second row: other viewpoint.
(a) Original images, (b)
detected aﬃne covariant regions, (c) close-up of the detected regions. (d) Geometric normalization to circles.
The regions are the same up to rotation.
(e) Photometric and geometric
normalization. The slight residual diﬀerence in rotation is due to an estimation error.
invariant description of the intensity pattern within the region.
In a typical matching application, the regions are used as follows. First, a set of covariant
regions is detected in an image.
Often a large number, perhaps hundreds or thousands, of
possibly overlapping regions are obtained.
A vector descriptor is then associated with each
region, computed from the intensity pattern within the region. This descriptor is chosen to be
invariant to viewpoint changes and, to some extent, illumination changes, and to discriminate
between the regions. Correspondences may then be established with another image of the same
scene, by ﬁrst detecting and representing regions (independently) in the new image; and then
matching the regions based on their descriptors. By design the regions commute with viewpoint
change, so by design, corresponding regions in the two images will have similar (ideally identical)
vector descriptors. The beneﬁts are that correspondences can then be easily established and,
since there are multiple regions, the method is robust to partial occlusions.
This paper gives a snapshot of the state of the art in aﬃne covariant region detection. We will
describe and compare six methods of detecting these regions on images. These detectors have been
designed and implemented by a number of researchers and the comparison is carried out using
binaries supplied by the authors. The detectors are: (i) the ‘Harris-Aﬃne’ detector ;
(ii) the ‘Hessian-Aﬃne’ detector ; (iii) the ‘maximally stable extremal region’ detector (or
MSER, for short) ; (iv) an edge-based region detector (referred to as EBR); (v) an
intensity extrema-based region detector (referred to as IBR); and (vi) an entropy-based
region detector (referred to as salient regions).
To limit the scope of the paper we have not included methods for detecting regions which
are covariant only to similarity transformations (i.e., in particular scale), such as , or other methods of computing aﬃne invariant descriptors, such as image lines connecting
interest points or invariant vertical line segments . Also the detectors proposed
by Lindeberg and Baumberg have not been included, as they come very close to the
Harris-Aﬃne and Hessian-Aﬃne detectors.
The six detectors are described in section 2.
They are compared on the data set shown
in ﬁgure 9.
This data set includes structured and textured scenes as well as diﬀerent types
of transformations: viewpoint changes, scale changes, illumination changes, blur and JPEG
compression. It is described in more detail in section 3. Two types of comparisons are carried
out. First, in section 4, the repeatability of the detector is measured: how well does the detector
determine corresponding scene regions? This is measured by comparing the overlap between
the ground truth and detected regions, in a manner similar to the evaluation test used in ,
but with special attention paid to the eﬀect of the diﬀerent scales (region sizes) of the various
detectors’ output. Here, we also measure the accuracy of the regions shape, scale and localization.
Second, the distinctiveness of the detected regions is assessed: how distinguishable are the regions
detected? Following , we use the SIFT descriptor developed by Lowe , which is an 128dimensional vector, to describe the intensity pattern within the image regions. This descriptor
has been demonstrated to be superior to others used in literature on a number of measures .
Our intention is that the images and tests described here will be a benchmark against which
future aﬃne covariant region detectors can be assessed. The images, Matlab code to carry out
the performance tests, and binaries of the detectors are available from
 
Aﬃne covariant detectors
In this section we give a brief description of the six region detectors used in the comparison.
Section 2.1 describes the related methods Harris-Aﬃne and Hessian-Aﬃne. Sections 2.2 and 2.3
describe methods for detecting edge-based regions and intensity extrema-based regions. Finally,
sections 2.4 and 2.5 describe MSER and salient regions.
For the purpose of the comparisons the output region of all detector types are represented by
a common shape, which is an ellipse. Figures 3 and 4 show the ellipses for all detectors on one
pair of images. In order not to overload the images, only some of the corresponding regions that
were actually detected in both images have been shown. This selection is obtained by increasing
the threshold.
In fact, for most of the detectors the output shape is an ellipse. However, for two of the
detectors (edge-based regions and MSER) it is not, and information is lost by this representation,
as ellipses can only be matched up to a rotational degree of freedom. Examples of the original
regions detected by these two methods are given in ﬁgure 5. These are parallelogram-shaped
regions for the edge-based region detector, and arbitrarily shaped regions for the MSER detector.
In the following the representing ellipse is chosen to have the same ﬁrst and second moments as
the originally detected region, which is an aﬃne covariant construction method.
Detectors based on aﬃne normalization – Harris-Aﬃne & Hessian-
We describe here two related methods which detect interest points in scale-space, and then
determine an elliptical region for each point. Interest points are either detected with the Harris
detector or with a detector based on the Hessian matrix. In both cases scale-selection is based
on the Laplacian, and the shape of the elliptical region is determined with the second moment
matrix of the intensity gradient .
The second moment matrix, also called the auto-correlation matrix, is often used for feature
detection or for describing local image structures. Here it is used both in the Harris detector
and the elliptical shape estimation. This matrix describes the gradient distribution in a local
neighbourhood of a point:
M = µ(x, σI, σD) =
IxIy(x, σD)
IxIy(x, σD)
The local image derivatives are computed with Gaussian kernels of scale σD (diﬀerentiation
scale). The derivatives are then averaged in the neighbourhood of the point by smoothing with
a Gaussian window of scale σI (integration scale). The eigenvalues of this matrix represent two
(a) Harris-Aﬃne
(b) Hessian-Aﬃne
Figure 3: Regions generated by diﬀerent detectors on corresponding sub-parts of the
ﬁrst and third graﬃti images of ﬁgure 9(a). The ellipses show the original detection size.
(c) Salient regions
Figure 4: Regions generated by diﬀerent detectors continued.
Figure 5: Originally detected region shapes for the regions shown in ﬁgures 3(c)
Figure 6: Example of characteristic scales. Top row shows images taken with diﬀerent zoom.
Bottom row shows the responses of the Laplacian over scales. The characteristic scales are 10.1
and 3.9 for the left and right image, respectively. The ratio of scales corresponds to the scale
factor (2.5) between the two images. The radius of displayed regions in the top row is equal to
3 times the selected scales.
principal signal changes in a neighbourhood of the point. This property enables the extraction
of points, for which both curvatures are signiﬁcant, that is the signal change is signiﬁcant in orthogonal directions. Such points are stable in arbitrary lighting conditions and are representative
of an image. One of the most reliable interest point detectors, the Harris detector , is based
on this principle.
A similar idea is explored in the detector based on the Hessian matrix:
H = H(x, σD) =
 Ixx(x, σD)
Ixy(x, σD)
Ixy(x, σD)
Iyy(x, σD)
The second derivatives, which are used in this matrix give strong responses on blobs and
ridges. The regions are similar to those detected by a Laplacian operator (trace) but
a function based on the determinant of the Hessian matrix penalizes very long structures for
which the second derivative in one particular orientation is very small. A local maximum of the
determinant indicates the presence of a blob structure.
To deal with scale changes a scale selection method is applied. The idea is to select the
characteristic scale of a local structure, for which a given function attains an extremum over scales
(see ﬁgure 6). The selected scale is characteristic in the quantitative sense, since it measures the
scale at which there is maximum similarity between the feature detection operator and the local
image structures. The size of the region is therefore selected independently of image resolution
for each point. The Laplacian operator is used for scale selection in both detectors since it gave
the best results in the experimental comparison in .
Given the set of initial points extracted at their characteristic scales we can apply the iterative
estimation of elliptical aﬃne region .
The eigenvalues of the second moment matrix are
used to measure the aﬃne shape of the point neighbourhood. To determine the aﬃne shape,
we ﬁnd the transformation that projects the aﬃne pattern to the one with equal eigenvalues.
This transformation is given by the square root of the second moment matrix M 1/2.
neighbourhood of points xR and xL are normalized by transformations x′
R xR and x′
xL, respectively, the normalized regions are related by a simple rotation x′
R .
The matrices M ′
R computed in the normalized frames are equal to a rotation matrix
(see ﬁgure 7). Note that rotation preserves the eigenvalue ratio for an image patch, therefore, the
aﬃne deformation can be determined up to a rotation factor. This factor can be recovered by
other methods, for example normalization based on the dominant gradient orientation .
The estimation of aﬃne shape can be applied to any initial point given that the determinant
xL −→M −1/2
xR −→M −1/2
Figure 7: Diagram illustrating the aﬃne normalization using the second moment
matrices. Image coordinates are transformed with matrices M −1/2
and M −1/2
of the second moment matrix is larger than zero and the signal to noise ratio is insigniﬁcant for
this point. We can therefore use this technique to estimate the shape of initial regions provided
by the Harris and Hessian based detector.
The outline of the iterative region estimation:
1. Detect initial region with Harris or Hessian detector and select the scale.
2. Estimate the shape with the second moment matrix
3. Normalize the aﬃne region to the circular one
4. Go to step 2 if the eigenvalues of the second moment matrix for new point are not equal.
Examples of Harris-Aﬃne and Hessian-Aﬃne regions are displayed on ﬁgure 3(a) and 3(b).
An edge-based region detector
We describe here a method to detect aﬃne covariant regions in an image by exploiting the edges
present in the image. The rationale behind this is that edges are typically rather stable features,
that can be detected over a range of viewpoints, scales and/or illumination changes. Moreover,
by exploiting the edge geometry, the dimensionality of the problem can be signiﬁcantly reduced.
Indeed, as will be shown next, the 6D search problem over all possible aﬃnities (or 4D, once
the center point is ﬁxed) can further be reduced to a one-dimensional problem by exploiting the
nearby edges geometry. In practice, we start from a Harris corner point p and a nearby
edge, extracted with the Canny edge detector . To increase the robustness to scale changes,
these basic features are extracted at multiple scales. Two points p1 and p2 move away from the
corner in both directions along the edge, as shown in ﬁgure 8(a). Their relative speed is coupled
through the equality of relative aﬃne invariant parameters l1 and l2:
p −pi(si)|) dsi
with si an arbitrary curve parameter (in both directions), pi(1)(si) the ﬁrst derivative of pi(si)
with respect to si, abs() the absolute value and | . . . | the determinant. This condition prescribes
that the areas between the joint < p, p1 > and the edge and between the joint < p, p2 > and
Figure 8: Construction methods for EBR and IBR. (a) The edge-based region detector
starts from a corner point p and exploits nearby edge information; (b) The intensity extremabased region detector starts from an intensity extremum and studies the intensity pattern along
rays emanating from this point.
the edge remain identical. This is an aﬃne invariant criterion indeed. From now on, we simply
use l when referring to l1 = l2.
For each value l, the two points p1(l) and p2(l) together with the corner p deﬁne a parallelogram Ω(l): the parallelogram spanned by the vectors p1(l) −p and p2(l) −p. This yields a one
dimensional family of parallelogram-shaped regions as a function of l. From this 1D family we
select one (or a few) parallelogram for which the following photometric quantities of the texture
go through an extremum.
abs(|p1 −pg
abs(|p −pg
In(x, y)xpyq dxdy
pq the nth order, (p + q)th degree moment computed over the region Ω(l), pg the center
of gravity of the region, weighted with intensity I(x, y), and q the corner of the parallelogram
opposite to the corner point p (see ﬁgure 8(a)). The second factor in these formula has been
added to ensure invariance under an intensity oﬀset.
In the case of straight edges, the method described above cannot be applied, since l = 0
along the entire edge. Since intersections of two straight edges occur quite often, we cannot
simply neglect this case. To circumvent this problem, the two photometric quantities given in
Equation 4 are combined and locations where both functions reach a minimum value are taken
to ﬁx the parameters s1 and s2 along the straight edges. Moreover, instead of relying on the
correct detection of the Harris corner point, we can simply use the straight lines intersection
point instead. A more detailed explanation of this method can be found in . Examples
of detected regions are displayed in ﬁgure 5(b).
For easy comparison in the context of this paper, the parallelograms representing the invariant
regions are replaced by the enclosed ellipses, as shown in ﬁgure 4(b). However, in this way the
orientation-information is lost, so it should be avoided in a practical application, as discussed in
the beginning of section 2.
Intensity extrema-based region detector
Here we describe a method to detect aﬃne covariant regions that starts from intensity extrema
(detected at multiple scales), and explores the image around them in a radial way, delineating
regions of arbitrary shape, which are then replaced by ellipses.
More precisely, given a local extremum in intensity, the intensity function along rays emanating
from the extremum is studied, as shown in ﬁgure 8(b). The following function is evaluated along
abs(I(t) −I0)
0 abs(I(t)−I0)dt
with t an arbitrary parameter along the ray, I(t) the intensity at position t, I0 the intensity value
at the extremum and d a small number which has been added to prevent a division by zero. The
point for which this function reaches an extremum is invariant under aﬃne geometric and linear
photometric transformations (given the ray). Typically, a maximum is reached at positions where
the intensity suddenly increases or decreases. The function fI(t) is in itself already invariant.
Nevertheless, we select the points where this function reaches an extremum to make a robust
selection. Next, all points corresponding to maxima of fI(t) along rays originating from the
same local extremum are linked to enclose an aﬃne covariant region (see ﬁgure 8(b)). This often
irregularly-shaped region is replaced by an ellipse having the same shape moments up to the
second order. This ellipse-ﬁtting is again an aﬃne covariant construction. Examples of detected
regions are displayed in ﬁgure 4(a). More details about this method can be found in .
Maximally Stable Extremal region detector
A Maximally Stable Extremal Region (MSER) is a connected component of an appropriately
thresholded image. The word ‘extremal’ refers to the property that all pixels inside the MSER
have either higher (bright extremal regions) or lower (dark extremal regions) intensity than all the
pixels on its outer boundary. The ‘maximally stable’ in MSER describes the property optimized
in the threshold selection process.
The set of extremal regions E, i.e., the set of all connected components obtained by thresholding, has a number of desirable properties. Firstly, a monotonic change of image intensities leaves
E unchanged, since it depends only on the ordering of pixel intensities which is preserved under
monotonic transformation. This ensures that common photometric changes modelled locally as
linear or aﬃne leave E unaﬀected, even if the camera is non-linear (gamma-corrected).
Secondly, continuous geometric transformations preserve topology – pixels from a single connected
component are transformed to a single connected component. Thus after a geometric change locally approximated by an aﬃne transform, homography or even continuous non-linear warping, a
matching extremal region will be in the transformed set E′. Finally, there are no more extremal
regions than there are pixels in the image. So a set of regions was deﬁned that is preserved under
a broad class of geometric and photometric changes and yet has the same cardinality as e.g. the
set of ﬁxed-sized square windows commonly used in narrow-baseline matching.
Implementation details:
The enumeration of the set of extremal regions E is very eﬃcient,
almost linear in the number of image pixels. The enumeration proceeds as follows. First, pixels
are sorted by intensity. After sorting, pixels are marked in the image (either in decreasing or
increasing order) and the list of growing and merging connected components and their areas is
maintained using the union-ﬁnd algorithm . During the enumeration process, the area of
each connected component as a function of intensity is stored. Among the extremal regions, the
‘maximally stable’ ones are those corresponding to thresholds were the relative area change as a
function of relative change of threshold is at a local minimum. In other words, the MSER are the
parts of the image where local binarization is stable over a large range of thresholds. The deﬁnition of MSER stability based on relative area change is only aﬃne invariant (both photometrically
and geometrically). Consequently, the process of MSER detection is aﬃne covariant.
Detection of MSER is related to thresholding, since every extremal region is a connected
component of a thresholded image.
However, no global or ‘optimal’ threshold is sought, all
thresholds are tested and the stability of the connected components evaluated. The output of
the MSER detector is not a binarized image.
For some parts of the image, multiple stable
thresholds exist and a system of nested subsets is output in this case.
Finally we remark that the diﬀerent sets of extremal regions can be deﬁned just by changing
the ordering function. The MSER described in this section and used in the experiments should
be more precisely called intensity induced MSERs.
Salient region detector
This detector is based on the pdf of intensity values computed over an elliptical region. Detection
proceeds in two steps: ﬁrst, at each pixel the entropy of the pdf is evaluated over the three
parameter family of ellipses centred on that pixel. The set of entropy extrema over scale and the
corresponding ellipse parameters are recorded. These are candidate salient regions. Second, the
candidate salient regions over the entire image are ranked using the magnitude of the derivative
of the pdf with respect to scale. The top P ranked regions are retained.
In more detail, the elliptical region E centred on a pixel x is parameterized by its scale s
(which speciﬁes the major axis), its orientation θ (of the major axis), and the ratio of major to
minor axes λ. The pdf of intensities p(I) is computed over E. The entropy H is then given by
p(I) log p(I)
The set of extrema over scale in H is computed for the parameters s, θ, λ for each pixel of the
image. For each extrema the derivative of the pdf p(I; s, θ, λ) with s is computed as
|∂p(I; s, θ, λ)
and the saliency Y of the elliptical region is computed as Y = HW. The regions are ranked by
their saliency Y. Examples of detected regions are displayed in ﬁgure 4(c). More details about
this method can be found in .
The image data set
Figure 9 shows examples from the image sets used to evaluate the detectors.
Five diﬀerent
changes in imaging conditions are evaluated: viewpoint changes (a) & (b); scale changes (c) &
(d); image blur (e) & (f); JPEG compression (g); and illumination (h). In the cases of viewpoint
change, scale change and blur, the same change in imaging conditions is applied to two diﬀerent
scene types.
This means that the eﬀect of changing the image conditions can be separated
from the eﬀect of changing the scene type. One scene type contains homogeneous regions with
distinctive edge boundaries (e.g. graﬃti, buildings), and the other contains repeated textures of
diﬀerent forms. These will be referred to as structured versus textured scenes respectively.
In the viewpoint change test the camera varies from a fronto-parallel view to one with signiﬁcant foreshortening at approximately 60 degrees to the camera. The scale change and blur
sequences are acquired by varying the camera zoom and focus respectively. The scale changes by
about a factor of four. The light changes are introduced by varying the camera aperture. The
JPEG sequence is generated using a standard xv image browser with the image quality parameter
varying from 40% to 2%. Each of the test sequences contains 6 images with a gradual geometric
or photometric transformation. All images are of medium resolution (approximately 800 × 640
The images are either of planar scenes or the camera position is ﬁxed during acquisition, so
that in all cases the images are related by homographies (plane projective transformations). This
means that the mapping relating images is known (or can be computed), and this mapping is
used to determine ground truth matches for the aﬃne covariant detectors.
The homographies between the reference (left most) image and the other images in a particular
dataset are computed in two steps. First, a small number of point correspondences are selected
manually between the reference and other image. These correspondences are used to compute an
approximate homography between the images, and the other image is warped by this homography
so that it is roughly aligned with the reference image. Second, a standard small-baseline robust
homography estimation algorithm is used to compute an accurate residual homography between
the reference and warped image (using hundreds of automatically detected and matched interest
points) . The composition of these two homographies (approximate and residual) gives an
accurate homography between the reference and other image. The root-mean-square error is less
than 1 pixel for every image pair.
Of course, the homography could be computed directly and automatically using correspondences of the aﬃne covariant regions detected by any of the methods of section 2. The reason for
adopting this two step approach is to have an estimation method independent of all the detectors
that are being evaluated.
All the images as well as the computed homographies are available on the website.
Discussion
Before we compare the performance of the diﬀerent detectors in more detail in the next section,
a few more general observations can already be made, simply by examining the output of the
diﬀerent detectors for the images shown in ﬁgures 3 and 4.
For all our experiments (unless
explicitly mentioned), the same set of parameters are used for each detector. These parameters
are the default parameters given by the authors.
First of all, note that the ellipses in the left and right images of ﬁgures 3 and 4 do indeed
cover more or less the same scene regions. This is the key requirement for covariant operators,
and seems to be fulﬁlled for at least a subset of the detected regions for all detectors. Some other
key observations are summarized below.
Complexity and required computation time:
The computational complexity of the algorithm ﬁnding initial points in the Harris-Aﬃne and Hessian-Aﬃne detectors is O(n), where n
is the number of pixels. The complexity of the automatic scale selection and shape adaptation
algorithm is O((m + k)p), where p is the number of initial points, m is a number of investigated
scales in the automatic scale selection and k is a number of iterations in the shape adaptation
algorithm.
For the intensity extrema-based region detector, the algorithm ﬁnding intensity extrema is
O(n), where n is again the number of pixels. The complexity of constructing the actual region
around the intensity extrema is O(p), where p is the number of intensity extrema.
For the edge-based region detector, the algorithm ﬁnding initial corner points and the algorithm
ﬁnding edges in the image are both O(n), where n is again the number of pixels. The complexity
of constructing the actual region starting from the corners and edges is O(pd), where p is the
number of corners and d is the average number of edges nearby a corner.
For the salient region detector, the complexity of the ﬁrst step of the algorithm is O(nl),
where l is the number of ellipses investigated at each pixel (the three discretized parameters of
the ellipse shape). The complexity of the second step is O(e), where e is the number of extrema
detected in the ﬁrst step.
For the MSER detector, the computational complexity of the sorting step is O(n) if the range
of image values is small, e.g. the typical {0, . . . , 255}, since the sort can be implemented as
binsort. The complexity of the union-ﬁnd algorithm is O(n log log n), i.e., fast.
Computation times vary widely, as can be seen in table 1. The computation times mentioned
in this table have all been measured on a Pentium 4 2GHz Linux PC, for the leftmost image shown
in ﬁgure 9(a), which is 800 × 640 pixels. Even though the timings are for not heavily optimized
code and may change depending on the implementation as well as on the image content, we
believe the table gives a reasonable indication of typical computation times.
Figure 9: Data set. (a), (b) Viewpoint change, (c),(d) Zoom+rotation, (e),(f) Image blur, (g)
JPEG compression, (h) Light change. In the case of viewpoint change, scale change and blur,
the same change in imaging conditions is applied to two diﬀerent scene types: structured and
textured scenes. In the experimental comparisons, the left most image of each set is used as the
reference image.
run time (min:sec)
number of regions
Harris-Aﬃne
Hessian-Aﬃne
Salient Regions
Table 1: Computation times for the diﬀerent detectors for the leftmost image of ﬁgure 9(a) (size
Region density:
The various detectors generate very diﬀerent numbers of regions, c.f. table 1.
The number of regions also strongly depends on the scene type, e.g. for the MSER detector there
are about 2600 regions for the textured blur scene (ﬁgure 9(f)) and only 230 for the light change
scene (ﬁgure 9(h)). Similar behaviour can be observed for other detectors.
The variation in numbers between detector type is to be expected since the detectors respond
to diﬀerent features and the images contain diﬀerent numbers for a given feature type.
example, the edge-based region detector requires curves, and if none of suﬃcient length occur in
a particular image, then no regions of this type can be detected.
However, this variety is also a virtue: the diﬀerent detectors are complementary. Some respond
well to structured scenes (e.g. MSER and the edge-based regions), others to more textured scenes
(e.g. Harris-Aﬃne and Hessian-Aﬃne). We will return to this point in section 4.2.
Region size:
Also the size of the detected regions signiﬁcantly varies depending on the detector.
Typically, Harris-Aﬃne, Hessian-Aﬃne and MSER detect many very small regions, whereas the
other detectors only yield larger ones. This can also be seen in the examples shown in ﬁgures 3
and 4. Figure 10 shows histograms of region size for the diﬀerent region detectors. The size of
the regions is measured as the geometric average of the half-length of both axes of the ellipses,
which corresponds to the radius of a circular region with the same area. Larger regions typically
have a higher discriminative power, as they contain more information, which makes them easier
to match, at the cost of a higher risk of being occluded or not covering a planar part of the scene.
Also, as will be shown in the next section (cf. ﬁgure 11), large regions automatically have better
chances of overlapping other regions.
Distinguished regions versus measurement regions:
As a ﬁnal note, we would like to
draw the attention of the reader to the fact that given a detected aﬃne covariant region, it is
possible to associate with it any number of new aﬃne regions that are obtained by aﬃne covariant
constructions, such as scaling, taking the convex hull or ﬁtting an ellipse based on second order
moments. In this respect, one should make a distinction between a distinguished region and a
measurement region, as ﬁrst pointed out in , where the former refers to the set of pixels that
have eﬀectively contributed to the aﬃne detector response while the latter can be any region
obtained by an aﬃne covariant construction. Here, we focus on the original distinguished regions
(except for the ellipse ﬁtting for edge-based and MSER regions, to obtain the same shape for all
detectors), as they determine the intrinsic quality of a detector. In a practical matching setup
however, it may be advantageous to use a diﬀerent measurement region (see also section 5 and
the discussion on scale in next section).
Overlap comparison using homographies
The objective of this experiment is to measure the repeatability and accuracy of the detectors:
to what extent do the detected regions overlap exactly the same scene area (i.e., are the preimages identical)? How often are regions detected in one image without the corresponding region
Histogram of detected region size
average region size
number of detected regions
Histogram of detected region size
average region size
number of detected regions
Histogram of detected region size
average region size
number of detected regions
Harris-Aﬃne
Hessian-Aﬃne
Histogram of detected region size
average region size
number of detected regions
Histogram of detected region size
average region size
number of detected regions
Histogram of detected region size
average region size
number of detected regions
Salient Regions
Figure 10: Histograms of region size for the diﬀerent detectors for the reference image
of ﬁgure 9(a)). Note that the y axes do not have the same scalings in all cases.
being detected in another? Quantitative results are obtained by making these questions precise
(see below). The ground truth in all cases is provided by mapping the regions detected on the
images in a set to a reference image using homographies. The basic measure of accuracy and
repeatability we use is the relative amount of overlap between the detected region in the reference
image and the region detected in the other image, projected onto the reference image using the
homography relating the images. This gives a good indication of the chance that the region can
be matched correctly. In the tests the reference image is always the image of highest quality and
is shown as the leftmost image of each set in ﬁgure 9.
Two important parameters characterize the performance of a region detector:
1. the repeatability, i.e., the average number of corresponding regions detected in images under
diﬀerent geometric and photometric transformations, both in absolute and relative terms
(i.e., percentage-wise), and
2. the accuracy of localization and region estimation.
However, before describing the overlap test in more detail, it is necessary to discuss the eﬀect
of region size and region density, since these aﬀect the outcome of the overlap comparison.
A note on the eﬀect of region size:
Larger regions automatically have a better chance of
yielding good overlap scores. Simply rescaling the regions, i.e., using a diﬀerent measurement
region (e.g. doubling the size of all regions) suﬃces to boost the overlap performance of a region
detector. This can be understood as follows. Suppose the distinguished region is an ellipse,
and the measurement region is also an ellipse centered on the distinguished region but with an
arbitrary scaling s. Then from a geometrical point of view, varying the scaling deﬁnes a cone out
of the image plane (with elliptical cross-section), and with s a distance on the cone axis. In the
reference image there are two such cones – one from the distinguished region in that image, and
the other from the mapped distinguished region from the other image, as illustrated in ﬁgure 11.
Clearly as the scaling goes to zero there is no intersection of the cones, and as the scaling goes
to inﬁnity the relative amount of overlap, deﬁned as the ratio of the intersection to the union of
the ellipses approaches unity.
Figure 11: Rescaling regions has an eﬀect on their overlap.
To measure the intrinsic quality of a region detector, we need to deﬁne an overlap criterion
that is insensitive to such rescaling. Focusing on the original distinguished regions would unproportionally favour detectors with large distinguished regions. Instead, the solution adopted here
is to apply a scaling s that normalizes the reference region to a ﬁxed region size prior to computing the overlap measure. It should be noted though that this is only for reason of comparison
of diﬀerent detectors. It may result in increased or decreased repeatability scores compared to
what one might get in a typical matching experiment, where such normalization typically is not
performed (and is not desirable either).
A note on the eﬀect of region density:
Also the region density, i.e., the number of detected
regions per ﬁxed amount of pixel area, may have an eﬀect on the repeatability score of a detector.
Indeed, if only a few regions are detected, the thresholds can be set very sharply, resulting in very
stable regions, which typically perform better than average. At the other extreme, if the number
of regions becomes really huge, the image might get so cluttered with regions that some of them
may be matched by accident rather than by design. In the limit, one would get an (aﬃne) scale
space approach rather than an aﬃne covariant region detector.
One way out would be to tune the parameters of the detectors such that they all output a
similar number of regions. However, this is diﬃcult to achieve in practice, since the number
of detected regions also depends on the scene type. Moreover, it is not straightforward for all
detectors to come up with a single parameter that can be varied to obtain the desired number of
regions in a meaningful way, i.e., representing some kind of ‘quality measure’ for the regions. So
we use the default parameters supplied by the authors. To give an idea of the number of regions,
both absolute and relative repeatability scores are given. In addition, for several detectors, the
repeatability is computed versus the number of detected regions, which is reported in section 4.3.
Repeatability measure
Two regions are deemed to correspond if the overlap error, deﬁned as the error in the image
area covered by the regions, is suﬃciently small:
1 −Rµa ∩R(HT µbH)
(Rµa ∪RHT µbH) < ϵO
Figure 12: Overlap error ϵO. Examples of ellipses projected on the corresponding ellipse with
the ground truth transformation. (bottom) Overlap error for above displayed ellipses. Note that
the overlap error comes from diﬀerent size, orientation and position of the ellipses.
where Rµ represents the elliptic region deﬁned by xT µx = 1. H is the homography relating the
two images. The union of the regions is Rµa ∪R(HT µbH), and Rµa ∩R(HT µbH) is their intersection.
The area of the union and the intersection of the regions are computed numerically.
The repeatability score for a given pair of images is computed as the ratio between the
number of region-to-region correspondences and the smaller of the number of regions in the pair
of images. We take into account only the regions located in the part of the scene present in both
To compensate for the eﬀect of regions of diﬀerent sizes, as mentioned in the previous section,
we ﬁrst rescale the regions as follows. Based on the region detected in the reference image, we
determine the scale factor that transforms it into a region of normalized size (corresponding
to a radius 30, in our experiments).
Then, we apply this scale factor to both the region in
the reference image and the region detected in the other image which has been mapped onto
the reference image, before computing the actual overlap error as described above. The precise
procedure is given in the Matlab code on
 
Examples of the overlap errors are displayed in ﬁgure 12. Note that an overlap error of 20%
is very small as it corresponds to only 10% diﬀerence between the regions’ radius. Regions with
50% overlap error can still be matched successfully with a robust descriptor.
Repeatability under various transformations
In a ﬁrst set of experiments, we ﬁx the overlap error threshold to 40% and the normalized region
size to a radius of 30 pixels, and check the repeatability of the diﬀerent region detectors for
gradually increasing transformations, according to the image sets shown in ﬁgure 9. In other
words, we measure how the number of correspondences depends on the transformation between
the reference and other images in the set. Both the relative and actual number of corresponding
regions is recorded. In general we would like a detector to have a high repeatability score and a
large number of correspondences. This test allows to measure the robustness of the detectors to
changes in viewpoint, scale, illumination, etc.
The results of these tests are shown in ﬁgures 13-20(a) and (b). Figures 13-20(c) and (d)
show matching results, which are discussed in section 5. A detailed discussion is given below,
but we ﬁrst make some general comments. The ideal plot for repeatability would be a horizontal
line at 100%.
As can be seen in all cases, neither a horizontal line nor 100% are achieved.
Indeed the performance generally decreases with the severity of the transformation, and the best
performance achieved is 95% for JPEG compression (ﬁgure 19). The reasons for this lack of
100% performance are sometimes speciﬁc to detectors and scene types (discussed below), and
sometimes general – the transformation is outside the range for which the detector is designed,
e.g. discretization errors, noise, non-linear illumination changes, projective deformations etc. Also
the limited ‘range’ of the regions shape (size, skewness, ...) can partially explain this eﬀect. For
instance, in case of a zoomed out test image, only the large regions in the reference image will
survive the transformation, as the small regions will have become too small for accurate detection.
The same holds for other types of transformations: very elongated regions in the reference image
may become undetectable if the inferred aﬃne transformation stretches them even further but,
on the other hand, allow for very large viewpoint changes if the inferred aﬃne transformation
makes them rounder.
The left side of each ﬁgure typically represents small transformations. The repeatability score
obtained in this range indicates how well a given detector performs for the given scene type and
to what extent the detector is aﬀected by a small transformation of this scene. The invariance
of the detector under the studied transformation, on the other hand, is reﬂected in the slope of
the curves, i.e., how much does a given curve degrade with increasing transformations.
The absolute number of correspondences typically drops faster than the relative number. This
can be understood by the fact that in most cases larger transformations result in lower quality
images and/or smaller commonly visible parts between the reference image and the other image,
and hence a smaller number of regions are detected.
Viewpoint change:
The eﬀect of changing viewpoint for the structured graﬃti scene from
ﬁgure 9(a) are displayed in ﬁgure 13. Figure 13(a) shows the repeatability score and ﬁgure 13(b)
the absolute number of correspondences.
The results for images containing repeated texture
motifs (ﬁgure 9(b)) are displayed in ﬁgure 14. The best results are obtained with the MSER
detector for both scene types.
This is due to the high detection accuracy especially on the
homogeneous regions with distinctive boundaries. The repeatability score for a viewpoint change
of 20 degrees varies between 40% and 78% and decreases for large viewpoint angles to 10%−46%.
The largest number of corresponding regions is given by Hessian-Aﬃne (1300) detector followed
by Harris-Aﬃne (900) detector for the structured scene, and given by Harris-Aﬃne (1200), MSER
(1200) and EBR (1300) detectors for the textured scene. These numbers decrease to less than
200/400 for the structured/textured scene for large viewpoint angle.
Scale change:
Figure 15 shows the results for the structured scene from ﬁgure 9(c), while
ﬁgure 16 shows the results for the textured scene from ﬁgure 9(d). The main image transformation
is a scale change and in-plane rotation. The Hessian-Aﬃne detector performs best, followed by
MSER and Harris-Aﬃne detectors. This conﬁrms the high performance of the automatic scale
selection applied in both Hessian-Aﬃne and Harris-Aﬃne detectors. These plots clearly show
the sensitivity of the detectors to the scene type. For the textured scene, the edge-based region
detector gives very low repeatability scores (below 20%), whereas for the structured scene, its
results are similar to the other detectors, with score going from 60% down to 28%. The unstable
repeatability score of the salient region detector for the textured scene is due to the small number
of detected regions in this type of images.
Figures 17 and 18 show the results for the structured scene from ﬁgure 9(e) and the
textured one from ﬁgure 9(f), both undergoing increasing amounts of image blur. The results are
better than for viewpoint and scale changes, especially for the structured scene. All detectors have
nearly horizontal repeatability curves, showing a high level of invariance to image blur, except
for the MSER detector, which is clearly more sensitive to this type of transformation. This is
because the region boundaries become smooth, and the segmentation process is less accurate.
The number of corresponding regions detected on structured scene is much lower than for the
textured scene and it changes by a diﬀerent factor for diﬀerent detectors. This clearly shows that
the detectors respond to diﬀerent features. The repeatability for the EBR detector is very low
for the textured scene. This can be explained by the lack of stable edges, on which the region
extraction is based.
JPEG artifacts:
Figure 19 shows the score for the JPEG compression sequence from ﬁgure 9(g). For this type of structured scene (buildings), with large homogeneous areas and distinctive corners, Hessian-Aﬃne and Harris-Aﬃne are clearly best suited. The degradation under
increasing compression artefacts is similar for all detectors.
Light change:
Figure 20 shows the results for light changes for the images on ﬁgure 9(h).
All curves are nearly horizontal, showing good robustness to illumination changes, although the
MSER obtains the highest repeatability score for this type of scene. The absolute score shows how
a small transformation of this type of a scene can aﬀect the repeatability of diﬀerent detectors.
General conclusions:
For most experiments the MSER regions or Hessian-Aﬃne obtain the
best repeatability score and are followed by Harris-Aﬃne. Salient regions give relatively low
repeatability. For the edge-based region detector, it largely depends on the scene content, i.e.,
whether the image contains stable curves or not. The intensity extrema-based region detector
gives average scores. Results largely depend on the type of scene used for the experiments. Again,
this illustrates the complementarity of the various detectors. Depending on the application, a
combination of detectors is probably prudent.
Viewpoint changes are the most diﬃcult type of transformation to cope with, followed by scale
changes. All detectors behave more or less similar under the diﬀerent types of transformations,
except for the blur sequence of ﬁgure 17, where MSER performs signiﬁcantly worse than the
In the majority of the examples Hessian-Aﬃne and Harris-Aﬃne detector provide several
times more corresponding regions than the other detectors. The Hessian-Aﬃne detector almost
systematically outperforms the Harris-Aﬃne detector, and the same holds for MSER with respect
More detailed tests
To further validate our experimental setup and to obtain a deeper insight in what is actually
going on, a more detailed analysis is performed on one image pair with a viewpoint change of 40
degrees, namely the ﬁrst and third column of the graﬃti sequence shown in ﬁgure 9(a).
Accuracy of the detectors:
First, we test the eﬀect of our choice for the overlap error
threshold. This was ﬁxed to 40% in all the previous experiments. Choosing a lower threshold
results in more accurate regions, (see ﬁgure 12).
Figure 21(a) shows the repeatability score
as a function of the overlap error. Clearly, as the required overlap is relaxed, more regions are
qualiﬁed as corresponding, and the repeatability scores go up. The relative ordering of the various
detectors remains virtually the same, except for the Harris-Aﬃne and Hessian region detectors.
They improve their ranking with increasing overlap error, which means that these detectors are
less accurate than the others – at least for this type of scene.
Choice of normalized region size:
Next, we test the eﬀect of our choice of the normalized
region size. This was ﬁxed to a radius of 30 pixels in all the previous experiments. Figure 21(b)
shows how the repeatability scores vary as a function of the normalized region size, with the
overlap error threshold ﬁxed to 40%. The relative ordering of the diﬀerent detectors stays the
same, which indicates that our experimental setup is not very sensitive to the choice of the
normalized region size. With a larger normalized region size, we obtain lower overlap errors and
the curves increase slightly (see also ﬁgure 11).
Varying the region density:
For some detectors, it is possible to vary the number of detected
regions, simply by changing the value of one signiﬁcant parameter. This makes it possible to
compensate for the eﬀect that diﬀerent region densities might have on the repeatability scores
and compare diﬀerent detectors when they output similar number of regions. Figure 21(c) shows
that the repeatability of MSER (92%) and IBR (63%) is high for a small number of regions (70)
and decreases to 68% and 50% respectively for 350 detected regions, unlike the repeatability for
Hessian-Laplace, Harris-Laplace and salient regions which is low for a small number of regions
and increases for more than 300 regions. However, the rank of the detectors remains the same in
the range of available threshold settings, therefore the order of the detectors in the experiments
in the previous section is not aﬀected by the density of regions. Depending on the application and
the required number of regions one can set the appropriate threshold to optimize the performance.
Repeatability score as a function of region size:
Rather than normalizing all regions to
a ﬁxed region size prior to computing the overlap error, an alternative approach would be to
only compare regions of similar sizes. This results in a plot showing the repeatability scores for
diﬀerent detectors as a function of region size. Large regions typically yield higher repeatability
scores, not only because of their intrinsic stability, but also because they automatically yield
lower overlap errors. Figure 21(d) shows the repeatability with respect to detected region size.
MSER detector has the highest repeatability score and it is nearly the same for diﬀerent size
of the detected regions. The results for Hessian-Aﬃne, Harris-Aﬃne and IBR are similar. The
repeatability is low for small regions, then it increases for medium size regions and slightly
decreases for larger regions except that the score for Harris-Aﬃne decreases more rapidly. The
repeatability for EBR and salient regions is small for small and medium size regions and increases
for large regions. Note, that the repeatability for diﬀerent region size depends also on the type
of image transformation i.e., for large scale changes only the small regions from one image will
match with the large regions from the other one.
Matching experiments
In the previous section, the performance of the diﬀerent region detectors is evaluated from a
rather theoretical point of view, focusing on the overlap error and repeatability. In this section,
we follow a more practical approach. In a practical application, regions need to be matched or
clustered, and apart from the accuracy and repeatability of the detection, also the distinctiveness
of the region is important. We test how well the regions can be matched, looking at the number
of matches found as well as the ratio between correct matches and mismatches.
To this end, we compute a descriptor for the regions, and then check to what extent matching
with the descriptor gives the correct region match. Here we use the SIFT descriptor of Lowe .
This descriptor gave the best matching results in an evaluation of diﬀerent descriptors computed
on scale and aﬃne invariant regions . The descriptor is a 128 dimensional vector computed
from the spatial distribution of image gradients over a circular region. To this end, each elliptical
region is ﬁrst mapped to a circular region of 30 × 30 pixels, and rotated based on the dominant
gradient orientation, to compensate for the aﬃne geometric deformations, as shown in ﬁgure 2(e).
Note that unlike in section 4, this mapping concerns descriptors; the region size is coincidentally
the same (30 pixels).
Matching score
Again the measure is computed between a reference image and the other images in a set. The
matching score is computed in two steps.
1. A region match is deemed correct if the overlap error deﬁned in the previous section is
minimal and less than 40%, i.e., ϵO ≤0.4. This provides the ground truth for correct
matches. Only a single match is allowed for each region.
2. The matching score is computed as the ratio between the number of correct matches and the
smaller number of detected regions in the pair of images. A match is the nearest neighbour
in the descriptor space. The descriptors are compared with the Euclidean distance.
This test gives an idea on the distinctiveness of features. The results are rather indicative than
quantitative. If the matching results do not follow those of the repeatability test for a particular
feature type that means that the distinctiveness of these features diﬀers from the distinctiveness
of other detectors.
The eﬀect of rescaling the regions:
Here, the issue arises on what scale to compute the
descriptor for a given region. Indeed, rather than taking the original distinguished region, one
might also rescale the region ﬁrst, which typically leads to more discriminative power – certainly
for the small regions. Figure 22(c) shows how the matching score for the diﬀerent detectors varies
for diﬀerent scale factors. Typically, the curves go slightly up for larger measurement regions,
except for EBR and salient regions which attain their maximum score for scale factor of 2 and
3 respectively. However, except for EBR the relative ordering of the diﬀerent detectors remains
unaltered. For all our matching experiments, we selected a scale factor of 3.
It should be noted though that in a practical application a large scale factor can be more
detrimental, due to the higher risk of occlusions or non-planarities. Since in our experimental
setup all images are related by homographies, these eﬀects do not occur.
Matching under various transformations
Figures 13 - 20 (c) and (d) give the results of the matching experiment for the diﬀerent types
of transformations. These are basically the same plots as given in ﬁgures 13 - 20 (a) and (b)
but now focusing on regions that have actually been matched, rather than just corresponding
For most transformations, the plots look indeed very similar to (albeit a bit lower than)
the results obtained with the overlap error test. This indicates that the regions typically have
suﬃcient distinctiveness to be matched automatically. One should be careful though to generalize
these results because these might be statistically unreliable, e.g. for much larger number of
features in database retrieval.
Sometimes, the score or relative ordering of the detectors diﬀers signiﬁcantly from the overlap
error tests of the previous section (e.g. ﬁgures 17 and 20). This means that the regions found by
some detectors are not distinctive and many mismatches occur.
The ranking of the detectors changes in ﬁgure 20(c) comparing to ﬁgure 20(a) which means
the Harris-Aﬃne and Hessian-Aﬃne are less distinctive. These detectors ﬁnd several slightly
diﬀerent regions containing the same local structure all of which have a small overlap error.
Thus, the matched regions might have the overlap smaller than 40% but the minimum overlap
error is for a slightly diﬀerent region. In this way the matched regions are counted as incorrect.
The same change in ranking for Harris-Aﬃne and Hessian-Aﬃne can be observed on the results
for other transformations. However the rank of the ﬁgures (d) showing the number of matched
regions do not change with respect to the number of corresponding regions on ﬁgures (b).
The curves for ﬁgure 18(c) and (d) give the results for the textured scene shown in ﬁgure 9(f).
For this case, the matching scores are signiﬁcantly lower than the repeatability scores obtained
earlier. This can be explained by the fact that the scene contains many similar local structures,
that can hardly be distinguished.
Ratio between correct and false matches:
So far, we investigated the matching capability
of corresponding regions. In a typical matching application, what matters is the ratio between
correct matches and false matches, i.e., are the regions within a correct match more similar to
each other than two regions that do not correspond but accidentally look more or less similar ?
Here, the accuracy of the region detection plays a role, as does the variability of the intensity
patterns for all regions found by a detector i.e., the distinctiveness.
Figure 22(a) shows the
percentage of correct matches as a function of the number of matches. A match is the nearest
neighbour in the SIFT feature space. These curves were obtained by ranking the matches based
viewpoint angle
repeatability %
Harris−Affine
Hessian−Affine
viewpoint angle
#correspondences
Harris−Affine
Hessian−Affine
viewpoint angle
matching score %
Harris−Affine
Hessian−Affine
viewpoint angle
#correct matches
Harris−Affine
Hessian−Affine
Figure 13: Viewpoint change for the structured scene (Graﬃti sequence ﬁgure 9(a)).
(a) Repeatability score for viewpoint change (default settings- overlap 40%,normalized size=30
pixels). (b) Number of corresponding regions. (c) Matching score. (d) Number of correct nearest
neighbour matches.
on the distance between the nearest neighbours. To obtain the same number of matches for
diﬀerent detectors the threshold was individually changed for each region type.
As the threshold, therefore the number of matches increases (ﬁgure 22(a)), the number of
correct as well as false matches also increases, but the number of false matches increases faster,
hence the percentage of correct matches drops. For a good detector, a small threshold results in
almost exclusively correct matches. Figure 22(b) shows the absolute number of correct matches
with respect to the total number of matches. We observe that MSER and IBR provide a large
number of correct matches for a small descriptor threshold. Up to 100 matches more than 90%
are correct. This means one does not have to rely so heavily on semi-local or global consistency
checks to remove the false matches afterwords. Harris-Aﬃne and Hessian-Aﬃne obtain low score
but improve when the distance is larger.
Depending on the application, the number of matches a user is interested in may vary. If only
a very small number of matches is needed (e.g. for computing epipolar geometry), the MSER or
IBR detector is the best choice for this type of scene. Above 200 matches, Hessian-Aﬃne and
Harris-Aﬃne perform better – albeit at the cost of a large false positive rate.
viewpoint angle
repeatability %
Harris−Affine
Hessian−Affine
viewpoint angle
#correspondences
Harris−Affine
Hessian−Affine
viewpoint angle
matching score %
Harris−Affine
Hessian−Affine
viewpoint angle
#correct matches
Harris−Affine
Hessian−Affine
Figure 14: Viewpoint change for the textured scene (Wall sequence ﬁgure 9(b)). (a)
Repeatability score for viewpoint change (default settings). (b) Number of corresponding regions.
(c) Matching score. (d) Number of correct nearest neighbour matches.
scale changes
repeatability %
Harris−Affine
Hessian−Affine
scale changes
#correspondences
Harris−Affine
Hessian−Affine
scale changes
matching score %
Harris−Affine
Hessian−Affine
scale changes
#correct matches
Harris−Affine
Hessian−Affine
Figure 15: Scale change for the structured scene (Boat sequence ﬁgure 9(c)).
Repeatability score for scale change (default settings). (b) Number of corresponding regions. (c)
Matching score. (d) Number of correct nearest neighbour matches.
scale changes
repeatability %
Harris−Affine
Hessian−Affine
scale changes
#correspondences
Harris−Affine
Hessian−Affine
scale changes
matching score %
Harris−Affine
Hessian−Affine
scale changes
#correct matches
Harris−Affine
Hessian−Affine
Figure 16: Scale change for the textured scene (Bark sequence ﬁgure 9(d)). (a) Repeatability score for scale change (default settings). (b) Number of corresponding regions. (c)
Matching score. (d) Number of correct nearest neighbour matches.
increasing blur
repeatability %
Harris−Affine
Hessian−Affine
increasing blur
#correspondences
Harris−Affine
Hessian−Affine
increasing blur
matching score %
Harris−Affine
Hessian−Affine
increasing blur
#correct matches
Harris−Affine
Hessian−Affine
Figure 17: Blur for the structured scene (Bikes sequence ﬁgure 9(e)). (a) Repeatability
score for blur change (default settings). (b) Number of corresponding regions. (c) Matching
score. (d) Number of correct nearest neighbour matches.
increasing blur
repeatability %
Harris−Affine
Hessian−Affine
increasing blur
#correspondences
Harris−Affine
Hessian−Affine
increasing blur
matching score %
Harris−Affine
Hessian−Affine
increasing blur
#correct matches
Harris−Affine
Hessian−Affine
Figure 18: Blur for the textured scene (Trees sequence ﬁgure 9(f)). (a) Repeatability
score for blur change (default settings). (b) Number of corresponding regions. (c) Matching
score. (d) Number of correct nearest neighbour matches.
JPEG compression %
repeatability %
Harris−Affine
Hessian−Affine
JPEG compression %
#correspondences
Harris−Affine
Hessian−Affine
JPEG compression %
matching score %
Harris−Affine
Hessian−Affine
JPEG compression %
#correct matches
Harris−Affine
Hessian−Affine
Figure 19: JPEG compression (UBC sequence ﬁgure 9(g)). (a) Repeatability score for different JPEG compression (default settings). (b) Number of corresponding regions. (c) Matching
score. (d) Number of correct nearest neighbour matches.
decreasing light
repeatability %
Harris−Affine
Hessian−Affine
decreasing light
#correspondences
Harris−Affine
Hessian−Affine
decreasing light
matching score %
Harris−Affine
Hessian−Affine
decreasing light
#correct matches
Harris−Affine
Hessian−Affine
Figure 20: Illumination change (Leuven sequence ﬁgure 9(h)). (a) Repeatability score
for diﬀerent illumination (default settings). (b) Number of corresponding regions. (c) Matching
score. (d) Number of correct nearest neighbour matches.
overlap error %
repeatability %
Harris−Affine
Hessian−Affine
normalized region size
repeatability %
Harris−Affine
Hessian−Affine
number of detected regions
repeatability %
Harris−Affine
Hessian−Affine
region size
repeatability %
Harris−Affine
Hessian−Affine
Figure 21: Viewpoint change (Graﬃti image pair - 1st and 3rd column in ﬁgure 9(a)).
(a) Repeatability score for diﬀerent overlap error for one pair (normalized size=30 pixels) . (b) Repeatability score for diﬀerent normalized region size (overlap error < 40%). (c) Repeatability
score for diﬀerent number of detected regions (overlap error= 40%, normalized size=30 pixels).
(d) Repeatability score as a function of region size.
total number of matches
correct matches %
Harris−Affine
Hessian−Affine
total number of matches
correct matches
Harris−Affine
Hessian−Affine
region size factor
matching score %
Harris−Affine
Hessian−Affine
Figure 22: Viewpoint change (Graﬃti image pair - 1st and 3rd column in ﬁgure 9(a)).
(a) Percentage of correct matches versus total number of nearest neighbour matches. (b) Number
of correct matches versus total number of nearest neighbour matches. (c) Matching score for
diﬀerent size of measurement region. Region size factor is the ratio measurement / detected
region size.
Conclusions
In this paper we have presented the state of the art on aﬃne covariant region detectors and have
compared their performance. The comparison has shown that the performance of all presented
detectors declines slowly, with similar rates, as the change of viewpoint increases. There does
not exist one detector which outperforms the other detectors for all scene types and all types of
transformations. In many cases the highest score is obtained by the MSER detector, followed by
Hessian-Aﬃne. MSER performs well on images containing homogeneous regions with distinctive
boundaries. This also holds for IBR since both methods are designed for similar region types.
Hessian-Aﬃne and Harris-Aﬃne provide more regions than the other detectors, which is useful in
matching scenes with occlusion and clutter. EBR is suitable for scenes containing intersections
of edges. Salient regions obtained low scores in this evaluation but performed well in the context
of object class recognition .
The detectors are complementary, i.e., they extract regions with diﬀerent properties and the
overlap of these regions is small if not empty. Several detectors should be used simultaneously to
obtain the best performance. The output of diﬀerent detectors can be combined by concatenating
the respective matches. This increases the number of matches and therefore the robustness to
occlusion, at the expense of processing time. The choice of the optimal subset depends on the
context, for example on the required number of extracted regions and processing time. In general,
matching on descriptors alone is not suﬃcient (as some are mismatched), and further steps are
required to disambiguate matches . These steps depend on the application, but
generally use methods of geometric ﬁltering based on the local spatial arrangement of the regions,
or on multiple view geometric relations.
Another contribution of the paper is the carefully designed test protocol which is available
on the Internet together with the test data. This allows the evaluation of future detectors and
their comparison with those studied in the paper. Note that the criteria, as deﬁned here, are
only valid for planar scenes or in the case of camera rotation or zoom. Only in these cases is the
geometric relation between two images deﬁned by a homography. However, many 3D objects are
composed of smooth surfaces, which are planar in the small – that is, suﬃciently small patches
can be treated as being comprised of coplanar points. Naturally, regions are also detected at
depth and surface orientation discontinuities of 3D scenes. Evaluating the repeatability of such
regions is beyond the scope of this paper.
Research on covariant regions and their description is now well advanced – they are the building
blocks for general recognition systems – but more remains to be done. One direct generalization
is to apply the detectors to representations of the image other than intensity, for example ordering
functions such as ‘saturation’ or ‘projection on the red-blue direction in RGB space’ could be
used. Furthermore, aﬃne detectors for shape (object boundaries) or completely unstructured
textures should be developed. Finally, an important issue is how to design detectors for images
of an object class, where there is within-class variation in addition to aﬃne viewpoint changes,
and how to measure their repeatability .
Acknowledgements
This research was initiated and supported by the European FET Fifth Framework project VIBES.
K. Mikolajczyk was funded by an INRIA postdoctoral fellowship, T. Tuytelaars by the Fund for
Scientiﬁc Research-Flanders, J. Matas by the Czech Academy of Sciences project 1ET101210407
and T. Kadir by the European project CogViSys. F. Schaﬀalitzky is a fellow of Balliol College,