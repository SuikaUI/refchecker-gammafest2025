Image De-raining Using a Conditional Generative
Adversarial Network
He Zhang, Member, IEEE, Vishwanath Sindagi, Student Member, IEEE
Vishal M. Patel, Senior Member, IEEE
Abstract—Severe weather conditions such as rain and snow
adversely affect the visual quality of images captured under
such conditions thus rendering them useless for further usage
and sharing. In addition, such degraded images drastically affect
performance of vision systems. Hence, it is important to address
the problem of single image de-raining. However, the inherent
ill-posed nature of the problem presents several challenges. We
attempt to leverage powerful generative modeling capabilities
of the recently introduced Conditional Generative Adversarial
Networks (CGAN) by enforcing an additional constraint that
the de-rained image must be indistinguishable from its corresponding ground truth clean image. The adversarial loss from
GAN provides additional regularization and helps to achieve
superior results. In addition to presenting a new approach to
de-rain images, we introduce a new reﬁned loss function and
architectural novelties in the generator-discriminator pair for
achieving improved results. The loss function is aimed at reducing
artifacts introduced by GANs and ensure better visual quality.
The generator sub-network is constructed using the recently introduced densely connected networks, whereas the discriminator
is designed to leverage global and local information to decide if
an image is real/fake. Based on this, we propose a novel single
image de-raining method called Image De-raining Conditional
Generative Adversarial Network (ID-CGAN), which considers
quantitative, visual and also discriminative performance into the
objective function. Experiments evaluated on synthetic and real
images show that the proposed method outperforms many recent
state-of-the-art single image de-raining methods in terms of
quantitative and visual performance. Furthermore, experimental
results evaluated on object detection datasets using Faster-
RCNN also demonstrate the effectiveness of proposed method
in improving the detection performance on images degraded by
Index Terms—Generative adversarial network, single image
de-raining, de-snowing, perceptual loss
I. INTRODUCTION
It has been known that unpredictable impairments such as
illumination, noise and severe weather conditions, such as rain,
snow and haze, adversely inﬂuence the performance of many
computer vision algorithms such as tracking, detection and
segmentation. This is primarily due to the fact that most of
these state-of-the-art algorithms are trained using images that
are captured under well-controlled conditions. For example, it
can be observed from Fig. 1, that the presence of heavy rain
greatly degrade perceptual quality of the image, thus imposing
larger challenge for face detection and veriﬁcation algorithms
in such weather conditions. A possible method to address
He Zhang is with Adobe, San Jose, CA email: 
Vishwanath Sindagi and Vishal M. Patel are with the Department of
Electrical and Computer Engineering, Johns Hopkins University, Baltimore,
MD, USA. Email: {vsindag1, vpatel36}@jhu.edu
De-rained results
Fig. 1: Sample results of the proposed ID-CGAN method for single
image de-raining.
this issue is to include images captured under unconstrained
conditions during the training process of these algorithms.
However, it may not be practical to collect such large scale
datasets as training set. In addition, in this age of ubiquitous cellphone usage, images captured in the bad weather
conditions by cellphone cameras undergo degradations that
drastically affect the visual quality of images making the
images useless for sharing and usage. In order to improve the
overall quality of such degraded images for better visual appeal
and to ensure enhanced performance of vision algorithms, it
becomes essential to automatically remove these undesirable
artifacts due to severe weather conditions discussed above.
In this paper, we investigate the effectiveness of conditional
generative adversarial networks (GANs) in addressing this
issue, where a learned discriminator network is used as a
guidance to synthesize images free from weather-based degradations. Speciﬁcally, we propose a single image-based deraining algorithm using a conditional GAN framework for
visually enhancing images that have undergone degradations
due to rain.
Mathematically, a rainy image can be decomposed into two
separate images: one corresponding to rain streaks and the
other corresponding to the clean background image (see Fig.
 
Fig. 2: Rain streak removal from a single image. A rainy image (a)
can be viewed as the superposition of a clean background image (b)
and a rain streak image (c).
2). Hence, the input rainy image can be expressed as
x = y + w,
where y represents the clean background image and w represents the rain streaks. As a result, similar to image de-noising
and image separation problems – , image de-raining can
be viewed as the problem of separating two components from
a rainy image.
In the case of video-based de-raining, a common strategy
to solve (1) is to leverage additional temporal information,
such as methods proposed in – . However, temporal
information is not available for cases of single image deraining problem. In such cases, previous works have designed
appropriate prior in solving (1) such as sparsity prior –
 , Gaussian Mixture Model (GMM) prior and patchrank prior . Most recently, with the large scale synthesized
training samples released by de-raining researchers, Convolutional Neural Networks (CNNs) have been also successfully
applied to solve single image de-raining problem – .
By directly learning a non-linear mapping between the input
rainy image and its corresponding ground truth using a CNN
structure with some prior information enforced, CNN-based
methods are able to achieve superior visual performance.
Even though tremendous improvements have been achieved,
we note that these methods do not consider additional information into the optimization. Hence, to design a visually appealing de-raining algorithm, we must consider the following
information into the optimization framework:
(a) The criterion that performance of vision algorithms such
as detection and classiﬁcation should not be affected by
the presence of rain streaks should be considered in the
objective function. The inclusion of this discriminative
information ensures that the reconstructed image is indistinguishable from its original counterpart.
(b) Rather than concentrating only on the characterization
of rain-streaks, visual quality may also be considered
into the optimization function. This can ensure that the
de-rained image looks visually appealing without losing
important details.
(c) Some of the existing methods adopt off-line additional
image processing techniques to enhance the results ,
 . Instead, it would be better to use a more uniﬁed
structure to deal with the problem without any additional
processing.
In this work, these criteria are incorporated in a novel conditional GAN-based framework called Image De-raining Conditional Generative Adversarial Network (ID-CGAN) to address
the single image de-raining problem. We aim to leverage the
generative modeling capabilities of the recently introduced
CGANs. While existing CNN-based approaches minimize
only L2 error, these methods need additional regularization due
to the ill-posed nature of the problem. In this work, adversarial
loss from CGANs is used as additional regularizer leading
to superior results in terms of visual quality and quantitative
performance. The use of discriminator for classifying between
real/fake samples provides additional feedback, enabling the
generator to produce results that are visually similar to the
ground-truth clean samples (real samples). Inspired by the
recent success of GANs for pixel-level vision tasks such as
image generation , image inpainting and image superresolution , our network consists of two sub-networks:
densely-connected generator (G) and multi-scale discriminator
(D). The generator acts as a mapping function to translate
an input rainy image to de-rained image such that it fools
the discriminator, which is trained to distinguish rainy images
from images without rain. The discriminator is designed to
capture hierarchical context information through multi-scale
pooling. However, traditional GANs are not stable to
train and may introduce artifacts in the output image making
it visually unpleasant and artiﬁcial. To address this issue, we
introduce a new reﬁned perceptual loss to serve as an additional loss function to aid the proposed network in generating
visually pleasing outputs. Furthermore, to leverage different
scale information in determining whether the corresponding
de-rained image is real or fake, a multi-scale discriminator is
proposed. Sample results of the proposed ID-CGAN algorithm
are shown in Fig. 1. In summary, this paper makes the
following contributions:
1) A conditional GAN-based framework to address the challenging single image de-raining problem without the use
of any additional post-processing.
2) A densely-connected generator sub-network that is specifically designed for the single image de-raining task.
3) A multi-scale discriminator is proposed to leverage both
local and global information to determine whether the
corresponding de-rained image is real or fake.
4) Extensive experiments are conducted on publicly available and synthesized datasets to demonstrate the effectiveness of the proposed method in terms of visual quality
and quantitative performance. /
5) Lastly, effectiveness of the proposed method in improving
high-level object detection task is demonstrated on VOC
dataset . The detections are performed using Faster-
RCNN .
This paper is organized as follows. A brief background on
de-raining, GANs and perceptual loss is given in Section II.
The details of the proposed ID-CGAN method are given
in Section III. Experimental results on both synthetic and
real images are presented in Section IV. Finally, Section V
concludes the paper with a brief summary and discussion.
II. BACKGROUND
In this section, we brieﬂy review the literature for existing
single image de-raining methods and conditional GANs.
A. Single Image De-raining
As discussed in Section I, single image de-raining is an extremely challenging task due to its ill-posed nature. In addition,
the unavailability of temporal information, which could have
been used as additional constraints, also pose challenges to
solve the single image de-raining problem. Hence, in order to
generate optimal solutions to this problem, different kinds of
prior knowledge are enforced into the optimization framework.
In the following, we discuss sparsity-based methods, low-rank
method, gaussian mixture model methods and deep learning
methods in solving image de-raining problem.
Sparsity-based Methods: To overcome the issue of ill-posed
nature of (1), authors in employed two set of learned
dictionary atoms (ﬁlters) to sparsely represent clean background image and rain-streak separately. The separation is
performed in the high frequency part of the given rainy image,
assuming that the low-frequencies correspond to the clean
background image. An important assumption that is made in
this approach is that rain streaks in an image usually have
similar orientations. Similarly, Luo et al. in proposed a
discriminative approach that sparsely approximates the patches
of clean background and rain-streak components by discriminative codes over two set of learned dictionary atoms with
strong mutual exclusivity property. However, their method
generates artifacts around the rain-streak components in the
resulting images.
Low-rank Representation-based Methods Inspired by the
observation that rain streak components within an image share
similar patterns and orientations, Chen et al. proposed a low
patch-rank prior to capture these patterns. This is motivated by
the use of patch-rank for characterizing the texture component
in solving cartoon-texture image decomposition problem ,
 . Since the patch-rank may also capture some repetitive
texture patterns, this method removes important texture details
from the input image, due to which the results become blurred.
To address this issue, Zhang et al. recently proposed a convolutional coding-based method that uses a set of learned
convolutional low-rank ﬁlters to capture the rain pixels.
Gaussian Mixture Model-based Methods: Based on the assumptions that GMMs can accommodate multiple orientations
and scales of rain streaks, Li et al. in used the image
decomposition framework to propose patch-based GMM priors
to model background and rain streaks separately.
Deep Learning-based Methods
The success of convolutional neural networks in several
computer vision tasks – has inspired researchers to
develop CNN-based approaches for image de-raining –
 . These methods attempt to learn a non-linear function to
convert an input rainy image to a clean target image. Based
on the observation that both rain streaks and object details
remain only in the detail layer, Fu et al. employed a twostep procedure, where the input rainy image is decomposed
into a background-based layer and a detail layer separately.
Then, a CNN-based non-linear mapping is learned to remove
the rain streaks from the detail layer. Built on , Fu et al.
extended the network structure using Res-block in .
Yang et al. proposed a CNN structure that can jointly detect
and remove rain streaks. Most recently, several deep learning
methods have been explored for single image de-raining task
 – .
B. Generative Adversarial Networks
Generative Adversarial Networks (GANs) are a class
of methods to model a data distributions and consist of two
functions: the generator G, which translate a sample from a
random uniform distribution to the data distribution; the discriminator D which measure the probability whether a given
sample belongs to the data distribution or not. Based on a game
theoretic min-max principles, the generator and discriminator
are typically learned jointly by alternating the training of D
and G. Although GANs are able to generate visual appealing
images by preserving high frequency details, yet GANs still
face many unsolved challenges: in general they are notoriously
difﬁcult to train and GANs easily suffer from modal collapse.
Recently, researchers have explored various aspects of GANs
such as leveraging another conditional variable , training
improvements and use of task speciﬁc cost function .
Also, an alternative viewpoint for the discriminator function
is explored by Zhao et al. where they deviate from
the traditional probabilistic interpretation of the discriminator
The success of GANs in synthesizing visually appealing
images has inspired researchers to explore the use of GANs
in other related such as text-to-image synthesis – ,
single image super-resolution , domain adaption , face
synthesis and other related applications – .
III. PROPOSED METHOD
Instead of solving (1) in a decomposition framework, we
aim to directly learn a mapping from an input rainy image to
a de-rained (background) image by constructing a conditional
GAN-based deep network called ID-CGAN. The proposed
network is composed of three important parts (generator,
discriminator and perceptual loss function) that serve distinct
purposes. Similar to traditional GANs , , the proposed
method contains two sub-networks: a generator sub-network
G and a discriminator sub-network D. The generator subnetwork G is a densely-connected symmetric deep CNN
network with appropriate skip connections as shown in the
top part in Fig. 3. Its primary goal is to synthesize a derained image from an image that is degraded by rain (input
rainy image). The multi-scale discriminator sub-network D,
as shown in the bottom part in Fig. 3, serves to distinguish
‘fake’ de-rained image (synthesized by the generator) from
corresponding ground truth ‘real’ image. It can also be viewed
as a guidance for the generator G. Since GANs are known to
be unstable to train which results in artifacts in the output
image synthesized by G, we deﬁne a reﬁned perceptual loss
functions to address this issue. Additionally, this new reﬁned
loss function ensures that the generated (de-rained) images
are visually appealing. In what follows, we elaborate these
modules in further detail, starting with the GAN objective
function followed by details of the network architecture for
the generator/discriminator and the overall loss function.
No addition pre-
(or post) processing
End-to-end mapping
Consider discriminative performance
in the optimization
Consider visual performance
in the optimization
Not Patch-based
Time efﬁciency
JORDER 
TABLE I: Compared to the existing methods, our ID-CGAN has several desirable properties: 1. No additional image processing. 2. Include
discriminative factor into optimization. 3. Consider visual performance into optimization.
Fig. 3: An overview of the proposed ID-CGAN method for single image de-raining. The network consists of two sub-networks: generator
G and discriminator D.
A. GAN Objective Function
The objective of a generative adversarial network is based
on the mini-max game, where the idea is to learn a generator
G that synthesizes samples similar to the data distribution such
that the discriminator is not able to distinguish between the
synthesize samples and real samples. At the same time, the
goal is also to learn a good discriminator D such that it is
able to distinguish between synthesized and real samples. To
achieve this, the proposed method alternatively updates G and
D following the structure proposed in , . Note that in
our work, we use a conditional variant of GAN, where the
generator G, learns to generate a mapping from a condition
variable. Given an input rainy image x, conditional GAN
learns a non-linear function to synthesize the output image
y by conditioning on the input image x:
Ex∼pdata(x),[log(1 −D(x, G(x)))]+
Ex∼pdata(x,y)[log D(x, y))].
B. Generator with Symmetric Structure
As the goal of single image de-raining is to generate pixellevel de-rained image, the generator should be able to remove
rain streaks as much as possible without loosing any detail
information of the background image. So the key part lies in
designing a good structure to generate de-rained image.
Existing methods for solving (1), such as sparse codingbased methods , , , , neural network-based
methods and CNN-based methods have all adopted a
symmetric (encoding-decoding) structure. For example, sparse
coding-based methods use a learned or pre-deﬁned synthesis
dictionaries to decode the input noisy image into sparse coef-
ﬁcient map. Then another set of analysis dictionaries are used
to transfer the coefﬁcients to desired clear output. Usually, the
input rainy image is transferred to a speciﬁc domain for effective separation of background image and undesired component
(rain-streak). After separation, the background image (in the
Architecture
Input, num c=3
3x3 Convolution, BN, ReLu, MaxP
D(4 layers)+Td, num c=128
D(6 layers)+Td, num c=256
D(8 layers)+Tn, num c=512
D(8 layers)+Tn, num c=128
D(6 layers)+Tu, num c=120
D(4 layers)+Tu, num c=64
D(4 layers)+Tu, num c=64
D(4 layers)+Tn, num c=16
3x3 Conv, Tanh, num c=3
Ourput, num c=3
TABLE II: Network architecture for generator.
new domain) has to be transferred back to the original domain
which requires the use of a symmetric process.
Following these methods, a symmetric structure is adopted
to form the generator sub-network. The generator G directly
learns an end-to-end mapping from input rainy image to its
corresponding ground truth. In contrast to the existing adversarial networks for image-to-image translation that use U-Net
 , or ResNet blocks , in their generators, we
use the recently introduced densely connected blocks .
These dense blocks enable strong gradient ﬂow and result in
improved parameter efﬁciency. Furthermore, we introduce skip
connections across the dense blocks to efﬁciently leverage features from different levels and guarantee better convergence.
The jth dense block Dj is represented as:
Dj = cat[Dj,1, Dj,2, ..., Dj,6],
where Dj,i represents the features from the ith layer in
dense block Dj and each layer in a dense block consists of
three consecutive operations, batch normalization (BN), leaky
rectiﬁed linear units (LReLU) and a 3×3 convolution.
Each dense block is followed by a transition block (T),
functioning as up-sampling (Tu), down-sampling (Td) or
no-sampling operation (Tn). To make the network efﬁcient in
training and have better convergence performance, symmetric
skip connections are included into the proposed generator subnetwork, similar to . The generator network is as follows:
CBLP(64)-D(256)-Td(128)-D(512)-Td(256)-D(1024)-
Tn(512)-D(768)-Tn(128)-D(640)-Tu(120)-D(384)-
Tu(64)-D(192)-Tu(64)-D(32)-Tn(16)-C(3)-Tanh
where, CBLP is a set of convolutional layers followed
by batch normalization, leaky ReLU activation and pooling
module, and the number inside braces indicates the number
of channels for the output feature maps of each block. Details
of the architecture is also shown in Table II.
C. Multi-scale Discriminator
From the point of view of a GAN framework, the goal of
de-raining an input rainy image is not only to make the derained result visually appealing and quantitatively comparable
to the ground truth, but also to ensure that the de-rained result
is indistinguishable from the ground truth image. Therefore,
a learned discriminator sub-network is designed to classify if
each input image is real or fake. Previous methods have
demonstrated the effectiveness of leveraging an efﬁcient patchdiscriminator in generating high quality results. For example,
Isola et al adopt a 70× 70 patch discriminator, where
70 × 70 indicates the receptive ﬁeld of the discriminator.
Though such a single scale (eg. 70× 70) patch-discriminator
is able to achieve visually pleasing results, however, it is still
not capable enough to capture the global context information,
resulting in insufﬁcient estimation. As shown in the zoomedin part of the Fig. 6 (e), it can be observed that certain tiny
details are still missing in the de-rained results using a single
scale discriminator. For example, it can be observed from the
second row of Fig. 6 that the front mirror of truck is largely
being removed in the de-rained results. This is probably due to
the fact that the receptive ﬁeld size in the discriminator is 70×
70 and no additional surrounding context is provided. Hence,
we argue that it is important to leverage a more powerful
discriminator that captures both local and global information
to decide whether it is real or fake.
To effectively address this issue, a novel multi-scale discriminator is proposed in this paper. This is inspired by the
usage of multi-scale features in objection detection and
semantic segmentation . Similar to the structure that was
proposed in , a convolutional layer with batch normalization and PReLU activation are used as a basis throughout the
discriminator network. Then, a multi-scale pooling module,
which pools features at different scales, is stacked at the end of
the discriminator. The pooled features are then upsampled and
concatenated, followed by a 1×1 convolution and a sigmoid
function to produce a probability score normalized between
 . By using features at different scales, we explicitly incorporate global hierarchical context into the discriminator. The
proposed discriminator sub-network D is shown in the bottom
part of Fig. 3. And details of the multi-scale discriminator is
shown in Table III.
D. Reﬁned Perceptual Loss
As discussed earlier, GANs are known to be unstable to
train and they may produce noisy or incomprehensible results
via the guided generator. A probable reason is that the new
input may not come from the same distribution of the training
samples. As illustrated in Fig. 5(c), it can be clearly observed
that there are some artifacts introduced by the normal GAN
structure. This greatly inﬂuences the visual performance of
the output image. A possible solution to address this issue
is to introduce perceptual loss into the network. Recently,
loss function measured on the difference of high-level feature
representation, such as loss measured on certain layers in
CNN , has demonstrated much better visual performance
than the per-pixel loss used in traditional CNNs. However, in
Fig. 4: Sample images from real-world rainy dataset.
Architecture
Input, num c=6
3x3 Convolution, BN, ReLu, MaxP,
3x3 Convolution, BN, ReLu, MaxP,
3x3 Convolution, BN, ReLu, MaxP,
3x3 Convolution, BN, ReLu, MaxP,
Four-level Poling Module, num c=72
Output, num c=72
TABLE III: Network architecture for the discriminator.
many cases it fails to preserve color and texture information
 . Also, it does not achieve good quantitative performance
simultaneously. To ensure that the results have good visual
and quantitative scores along with good discriminatory performance, we propose a new reﬁned loss function. Speciﬁcally,
we combine pixel-to-pixel Euclidean loss, perceptual loss 
and adversarial loss together with appropriate weights to form
our new reﬁned loss function. The new loss function is then
deﬁned as follows:
LRP = LE + λaLA + λpLP ,
where LA represents adversarial loss (loss from the discriminator D), LP is perceptual loss and LE is normal per-pixel
loss function such as Euclidean loss. Here, λp and λa are
pre-deﬁned weights for perceptual loss and adversarial loss,
respectively. If we set both λp and λa to be 0, then the
network reduces to a normal CNN conﬁguration, which aims
to minimize only the Euclidean loss between output image and
ground truth. If λp is set to 0, then the network reduces to a
normal GAN. If λa set to 0, then the network reduces to the
structure proposed in .
The three loss functions LP , LE and LA are deﬁned as
follows. Given an image pair {x, yb} with C channels, width
W and height H (i.e. C × W × H), where x is the input
image and yb is the corresponding ground truth, the per-pixel
Euclidean loss is deﬁned as:
∥φE(x)c,w,h −(yb)c,w,h∥2
where φE is the learned network G for generating the derained output. Suppose the outputs of certain high-level layer
are with size Ci × Wi × Hi. Similarly, the perceptual loss is
∥V (φE(x))c,w,h −V (yb)c,w,h∥2
where V represents a non-linear CNN transformation. Similar
to the idea proposed in , we aim to minimize the distance
between high-level features. In our method, we compute the
feature loss at layer relu2 2 in VGG-16 model .1
Given a set of N de-rained images generated from the
generator {φE(x)}N
i=1, the entropy loss from the discriminator
to guide the generator is deﬁned as:
log(D(φE(x))).
IV. EXPERIMENTS AND RESULTS
In this section, we present details of the experiments and
quality measures used to evaluate the proposed ID-CGAN
method. We also discuss the dataset and training details
followed by comparison of the proposed method against a set
of baseline methods and recent state-of-the-art approaches.
A. Experimental Details
1) Synthetic dataset: Due to the lack of availability of
large size datasets for training and evaluation of single image
de-raining, we synthesized a new set of training and testing
samples in our experiments. The training set consists of a
total of 700 images, where 500 images are randomly chosen
from the ﬁrst 800 images in the UCID dataset and 200
images are randomly chosen from the BSD-500’s training set
 . The test set consists of a total of 100 images, where
50 images are randomly chosen from the last 500 images
in the UCID dataset and 50 images are randomly chosen
from the test-set of the BSD-500 dataset . After the
train and test sets are created, we add rain-streaks to these
images by following the guidelines mentioned in using
Photoshop2. It is ensured that rain pixels of different intensities
and orientations are added to generate a diverse training and
test set. Note that the images with rain form the set of observed
images and the corresponding clean images form the set of
1 val.prototxt
2 
Fig. 5: Qualitative comparisons for different baseline conﬁgurations of the proposed method. (a) Input image, (b) GEN, (c) GEN-CGAN-S,
(d) GEN-P, (e) GEN-PS, (f) ID-CGAN and (g) Target image.
Fig. 6: Qualitative comparisons for different baseline conﬁgurations of the proposed method. (a) Input image, (b) GEN, (c) GEN-CGAN-S,
(d) GEN-P, (e) GEN-CGAN-PS, (f) ID-CGAN and (g) Target image.
ground truth images. All the training and test samples are
resized to 256×256.
2) Real-world rainy images dataset: In order to demonstrate the effectiveness of the proposed method on real-world
data, we created a dataset of 50 rainy images downloaded from
the Internet. While creating this dataset, we took all possible
care to ensure that the images collected were diverse in terms
of content as well as intensity and orientation of the rain pixels.
A few sample images from this dataset are shown in Fig. 4.
This dataset is used for evaluation (test) purpose only.
3) Quality measures: The following measures are used to
evaluate the performance of different methods: Peak Signal to
Noise Ratio (PSNR), Structural Similarity Index (SSIM) ,
Universal Quality Index (UQI) and Visual Information
Fidelity (VIF) . Similar to previous methods , all of
these quantitative measures are calculated using the luminance
channel. Since we do not have ground truth reference images
for the real dataset, the performance of the proposed and other
methods on the real dataset is evaluated visually.
B. Model Details and Parameters
The entire network is trained on a Nvidia Titan-X GPU
using the torch framework . We used a batch size of 1
and number of training iterations of 100k. Adam algorithm
 with a learning rate of 2×10−3 is used. During training,
GEN-CGAN-S
GEN-CGAN-PS
TABLE IV: Quantitative comparison baseline conﬁgurations.
we set λa = 6.6 × 10−3 and λp = 1. All the parameters are
set via cross-validation. A low value for λa is used so as to
ensure that the adversarial loss does not dominate the other
C. Comparison with Baseline Conﬁgurations
In order to demonstrate the signiﬁcance of different modules
in the proposed method, we compare the performance of
following baseline conﬁgurations:
• GEN: Generator G is trained using per-pixel Euclidean loss
by setting λa and λp to zero in (4). This amounts to a
traditional CNN architecture with Euclidean loss.
• GEN-CGAN-S: Generator G trained using per-pixel Euclidean loss and Adversarial loss from a single-scale discriminator D (no multi-scale pooling). λp is set to zero in
JORDER 
TABLE V: Quantitative comparisons with state-of-the-art methods evaluated on using four different criterions.
• GEN-P: Generator G is trained using per-pixel Euclidean
loss and perceptual loss. λa is set to zero in (4).
• GEN-CGAN-PS: Generator G is trained using per-pixel
Euclidean loss, perceptual loss and adversarial loss from a
single scale discriminator.
• ID-CGAN: Generator G is trained using per-pixel Euclidean
loss, perceptual loss and adversarial loss from multi-scale
discriminator D.
All four conﬁgurations along with ID-CGAN are learned
using training images from the synthetic training dataset.
Results of quantitative performance, using the measures discussed earlier on test images from the synthetic dataset, are
shown in Table IV. Sample results for the above baseline
conﬁgurations on test images from real dataset are shown
in Fig. 5 and Fig. 6. It can be observed from Fig. 5(c),
that the introduction of adversarial loss improves the visual
quality over the traditional CNN architectures, however, it also
introduces certain artifacts. The use of perceptual loss along
with adversarial loss from a single scale discriminator reduces
these artifacts while producing sharper results. However, part
of the texture details are still missing in the de-rained results
(Fig. 5(e)) such as the edge of the left back part of the car
(shown in third row in Fig. 5) and the structure of truck’s
front mirror (shown in second row in Fig. 6). Finally, the use
of adversarial loss from multi-scale discriminator along with
other loss functions (ID-CGAN) results in recovery of these
texture details and achieve the best results. Quantitative results
shown in Table V also demonstrate the effectiveness of the
each module.
D. Comparison with State-of-the-art Methods
We compare the performance of the proposed ID-CGAN
method with the following recent state-of-the-art methods for
single image de-raining:
• SPM: Sparse dictionary-based method (TIP ’12)
• DSC: Discriminative sparse coding-based method 
(ICCV ’15)
• PRM: PRM prior-based method (CVPR ’16)
• GMM: GMM-based method (ICCV ’13)
• CNN: CNN-based method (TIP ’17)
• CCR: Convolutional-coding based method (WACV ’17)
• DDN: Deep Detail Network method (CVPR ’17)
• JORDER: CNN-based method (CVPR ’17)
• PAN: GAN-based method (TIP ’18)
1) Evaluation on synthetic dataset: In the ﬁrst set of
experiments, we evaluate the proposed method and compare its
quantitative and qualitative performance against several stateof-the-art approaches on test images from the synthetic dataset.
As the ground truth is available for the these test images, we
calculate the quantitative measures such as PSNR, SSIM, UQI
and VIF. Table V shows the comparison of results based on
these metrics. This table clear demonstrates that the proposed
ID-CGAN method is able to achieve superior quantitative
performance as compared to the recent methods in terms of
all the metrics stated earlier.
Fig. 7 illustrates the qualitative improvements on two sample images from the synthetic dataset, achieved due to the
use of the proposed method. Note that we selectively sample
difﬁcult images to show that our method performs well in
difﬁcult conditions. While PRM is able to remove the
rain-streaks, it produces blurred results which are not visually
appealing. The other compared methods are able to either
reduce the intensity of rain or remove the streaks in parts,
however, they fail to completely remove the rain-streaks. In
contrast to the other methods, the proposed method is able
to successfully remove majority of the rain streaks while
maintaining the details of the de-rained images.
2) Evaluation on Real Rainy Images: We also evaluated
the performance of the proposed method and recent state-ofthe-art methods on real-world rainy test images. The de-rained
results for all the methods on two sample input rainy images
are shown in Fig. 8. For better visual comparison, we show
zoomed versions of the two speciﬁc regions-of-interest below
the de-rained results. By looking at these regions-of-interest,
we can clearly observe that DSC tends to add artifacts on
the de-rained images. Even though the other methods GMM
 , CNN , CCR , DNN and JORDER are
able to achieve good visual performance, rain drops are still
visible in the zoomed regions-of-interest. In comparison, the
proposed method is able to remove most of the rain drops
while maintaining the details of the background image. One
may observe that the proposed method leaves out a few rainstreaks in the output images. This is because the two image
samples represent relatively difﬁcult cases for de-raining.
However, the proposed method is able to achieve better results
compared to state-of-the-art methods. Additional comparisons
are provided in Fig. 9. It can be seen that the proposed method
achieves better results among all the methods. In addition,
more de-rained results on different rainy images, shown in
Fig. 11, demonstrate that the proposed method successfully
removes rain streaks.
3) Evaluation on Object Detection Results: Single image
de-raining algorithms can be used as a pre-processing step
to improve the performance of other high level vision tasks
such as face recognition and object detection . In order to
demonstrate the performance improvement obtained after deraining using the proposed IDCGAN method, we evaluated
Faster-RCNN on VOC 2007 dataset . First, the VOC
JORDER 
Ground Truth
JORDER 
Ground Truth
Fig. 7: Qualitative comparison of rain-streak removal on two sample images from synthetic dataset.
2007 dataset is artiﬁcially degraded with rain streaks similar
to Section IV A. Due to the degradations, object detection
performance using Faster-RCNN results in poor performance.
Next, the degraded images are processed by ID-CGAN method
to remove the rain streaks and the de-rained images are fed
to the Faster-RCNN method. We present the mean average
precision (mAP) for the entire VOC dataset in Table IV. It may
be noted that Faster-RCNN on degraded images results in a
low average precision, however, the performance is boosted by
78% when the images undergo de-raining using the proposed
ID-CGAN method.
Sample detection results for Faster-RCNN on real-world
rainy and de-rained images are shown in Fig. 10. The degradations result in total failure of Faster-RCNN on these images,
however, after being processed by ID-CGAN, the same detection method is able successfully detect different objects in the
4) Computation times: Table ?? compares the running
time of several state-of-the-art methods. All baseline methods
are implemented using MATLAB or MATLAB wrapper. Our
method is implemented in Torch. It can be observed that all
GPU-based CNN methods – are computationally more
efﬁcient. The proposed ID-CGAN is able achieve the fastest
time3 as compared to these methods. On an average, ID-
CGAN in GPU can process and image of size 500 × 500
in about 0.3s.
V. CONCLUSION
In this paper, we proposed a conditional GAN-based algorithm for the removal of rain streaks form a single image.
In comparison to the existing approaches which attempt to
solve the de-raining problem in an image decomposition
framework by using prior information, we investigated the use
3ID-CGAN is running as fast as Fu et al .
JORDER 
JORDER 
JORODR 
JORDER 
Fig. 8: Qualitative comparison of rain-streak removal on two sample real images.
JORDER 
JORDER 
Fig. 9: Qualitative comparison of rain-streak removal on two sample real images.
Fig. 10: Real-world examples of object detection (Faster-RCNN ) improvements obtained by the proposed ID-CGAN. Left: Detection
results on rainy images; Right: Detection results on de-rained images. The detection performance is boosted when ID-CGAN is used as a
pre-processing step.
Fig. 11: Additional de-rained results using the proposed ID-CGAN
method on real-world dataset. Left: Input; Right: Derained results.
of generative modeling for synthesizing de-rained image from
a given input rainy image. For improved stability in training
and reducing artifacts introduced by GANs in the output
images, we proposed the use of a new reﬁned loss function in
the GAN optimization framework. In addition, a multi-scale
discriminator is proposed to leverage features from different
scales to determine whether the de-rained image is real or
fake. Extensive experiments are conducted on synthetic and
real-world dataset to evaluate the performance of the proposed
method. Comparison with several recent methods demonstrates
that our approach achieves signiﬁcant improvements in terms
of different metrics. Moreover, a detailed ablation study is
conducted to clearly illustrate improvements obtained due
to different modules in the proposed method. Furthermore,
experimental results evaluated on objection detection using
Faster-RCNN demonstrated signiﬁcant improvements in detection performance when ID-CGAN method is used as a preprocessing step.
ACKNOWLEDGMENT
This work was supported by an ARO grant W911NF-16-1-
JORDER 
Our De-rained
TABLE VI: Object detection performance using Faster-RCNN on
VOC 2007 dataset.